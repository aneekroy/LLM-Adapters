{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 51012,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005881055649489083,
      "grad_norm": 0.6301401853561401,
      "learning_rate": 2.6999999999999996e-05,
      "loss": 2.7684,
      "step": 10
    },
    {
      "epoch": 0.0011762111298978166,
      "grad_norm": 0.8666848540306091,
      "learning_rate": 5.6999999999999996e-05,
      "loss": 2.5636,
      "step": 20
    },
    {
      "epoch": 0.001764316694846725,
      "grad_norm": 0.6717386841773987,
      "learning_rate": 8.699999999999999e-05,
      "loss": 2.3778,
      "step": 30
    },
    {
      "epoch": 0.002352422259795633,
      "grad_norm": 1.114404320716858,
      "learning_rate": 0.000117,
      "loss": 2.0339,
      "step": 40
    },
    {
      "epoch": 0.002940527824744542,
      "grad_norm": 0.6937846541404724,
      "learning_rate": 0.000147,
      "loss": 1.5482,
      "step": 50
    },
    {
      "epoch": 0.00352863338969345,
      "grad_norm": 0.9048432111740112,
      "learning_rate": 0.00017699999999999997,
      "loss": 1.4429,
      "step": 60
    },
    {
      "epoch": 0.004116738954642358,
      "grad_norm": 0.6477141976356506,
      "learning_rate": 0.00020699999999999996,
      "loss": 1.4133,
      "step": 70
    },
    {
      "epoch": 0.004704844519591266,
      "grad_norm": 0.5115437507629395,
      "learning_rate": 0.000237,
      "loss": 1.2968,
      "step": 80
    },
    {
      "epoch": 0.005292950084540175,
      "grad_norm": 0.5443816781044006,
      "learning_rate": 0.000267,
      "loss": 1.254,
      "step": 90
    },
    {
      "epoch": 0.005881055649489084,
      "grad_norm": 0.5772690773010254,
      "learning_rate": 0.00029699999999999996,
      "loss": 1.3044,
      "step": 100
    },
    {
      "epoch": 0.006469161214437992,
      "grad_norm": 0.5492121577262878,
      "learning_rate": 0.00029994696731615333,
      "loss": 1.2002,
      "step": 110
    },
    {
      "epoch": 0.0070572667793869,
      "grad_norm": 0.4956986606121063,
      "learning_rate": 0.0002998880421118793,
      "loss": 1.2537,
      "step": 120
    },
    {
      "epoch": 0.007645372344335808,
      "grad_norm": 0.5730493068695068,
      "learning_rate": 0.0002998291169076052,
      "loss": 1.2343,
      "step": 130
    },
    {
      "epoch": 0.008233477909284716,
      "grad_norm": 0.7993683218955994,
      "learning_rate": 0.0002997701917033312,
      "loss": 1.1764,
      "step": 140
    },
    {
      "epoch": 0.008821583474233625,
      "grad_norm": 0.6307101845741272,
      "learning_rate": 0.0002997112664990572,
      "loss": 1.2651,
      "step": 150
    },
    {
      "epoch": 0.009409689039182533,
      "grad_norm": 0.6157280802726746,
      "learning_rate": 0.00029965234129478315,
      "loss": 1.1848,
      "step": 160
    },
    {
      "epoch": 0.009997794604131442,
      "grad_norm": 0.46344056725502014,
      "learning_rate": 0.00029959341609050907,
      "loss": 1.2376,
      "step": 170
    },
    {
      "epoch": 0.01058590016908035,
      "grad_norm": 0.4073779582977295,
      "learning_rate": 0.00029953449088623504,
      "loss": 1.1424,
      "step": 180
    },
    {
      "epoch": 0.011174005734029258,
      "grad_norm": 0.5658795237541199,
      "learning_rate": 0.000299475565681961,
      "loss": 1.3046,
      "step": 190
    },
    {
      "epoch": 0.011762111298978167,
      "grad_norm": 0.6266144514083862,
      "learning_rate": 0.00029941664047768694,
      "loss": 1.1857,
      "step": 200
    },
    {
      "epoch": 0.012350216863927075,
      "grad_norm": 0.6461713314056396,
      "learning_rate": 0.0002993577152734129,
      "loss": 1.1758,
      "step": 210
    },
    {
      "epoch": 0.012938322428875984,
      "grad_norm": 0.5478423237800598,
      "learning_rate": 0.0002992987900691389,
      "loss": 1.2218,
      "step": 220
    },
    {
      "epoch": 0.013526427993824891,
      "grad_norm": 0.6636168360710144,
      "learning_rate": 0.00029923986486486486,
      "loss": 1.209,
      "step": 230
    },
    {
      "epoch": 0.0141145335587738,
      "grad_norm": 0.6623058915138245,
      "learning_rate": 0.0002991809396605908,
      "loss": 1.1457,
      "step": 240
    },
    {
      "epoch": 0.014702639123722707,
      "grad_norm": 0.4787333309650421,
      "learning_rate": 0.00029912201445631676,
      "loss": 1.1813,
      "step": 250
    },
    {
      "epoch": 0.015290744688671616,
      "grad_norm": 0.4725334644317627,
      "learning_rate": 0.00029906308925204273,
      "loss": 1.1723,
      "step": 260
    },
    {
      "epoch": 0.015878850253620524,
      "grad_norm": 0.6301475167274475,
      "learning_rate": 0.00029900416404776865,
      "loss": 1.1652,
      "step": 270
    },
    {
      "epoch": 0.016466955818569433,
      "grad_norm": 0.4706641435623169,
      "learning_rate": 0.00029894523884349463,
      "loss": 1.0956,
      "step": 280
    },
    {
      "epoch": 0.017055061383518342,
      "grad_norm": 0.43386340141296387,
      "learning_rate": 0.0002988863136392206,
      "loss": 0.9593,
      "step": 290
    },
    {
      "epoch": 0.01764316694846725,
      "grad_norm": 0.4983191192150116,
      "learning_rate": 0.0002988273884349466,
      "loss": 1.1213,
      "step": 300
    },
    {
      "epoch": 0.018231272513416157,
      "grad_norm": 0.5294252634048462,
      "learning_rate": 0.0002987684632306725,
      "loss": 1.1761,
      "step": 310
    },
    {
      "epoch": 0.018819378078365066,
      "grad_norm": 0.4020210802555084,
      "learning_rate": 0.0002987095380263985,
      "loss": 1.1353,
      "step": 320
    },
    {
      "epoch": 0.019407483643313975,
      "grad_norm": 0.7036600112915039,
      "learning_rate": 0.00029865061282212445,
      "loss": 1.047,
      "step": 330
    },
    {
      "epoch": 0.019995589208262884,
      "grad_norm": 0.5745941996574402,
      "learning_rate": 0.00029859168761785037,
      "loss": 1.138,
      "step": 340
    },
    {
      "epoch": 0.020583694773211793,
      "grad_norm": 0.4839319884777069,
      "learning_rate": 0.00029853276241357634,
      "loss": 1.2099,
      "step": 350
    },
    {
      "epoch": 0.0211718003381607,
      "grad_norm": 0.49032062292099,
      "learning_rate": 0.0002984738372093023,
      "loss": 1.1664,
      "step": 360
    },
    {
      "epoch": 0.021759905903109607,
      "grad_norm": 0.4772025942802429,
      "learning_rate": 0.0002984149120050283,
      "loss": 1.1751,
      "step": 370
    },
    {
      "epoch": 0.022348011468058517,
      "grad_norm": 0.4385848641395569,
      "learning_rate": 0.0002983559868007542,
      "loss": 1.1159,
      "step": 380
    },
    {
      "epoch": 0.022936117033007426,
      "grad_norm": 0.42331698536872864,
      "learning_rate": 0.0002982970615964802,
      "loss": 1.1571,
      "step": 390
    },
    {
      "epoch": 0.023524222597956335,
      "grad_norm": 0.4509694278240204,
      "learning_rate": 0.0002982381363922061,
      "loss": 1.0972,
      "step": 400
    },
    {
      "epoch": 0.02411232816290524,
      "grad_norm": 0.4545334577560425,
      "learning_rate": 0.0002981792111879321,
      "loss": 1.1671,
      "step": 410
    },
    {
      "epoch": 0.02470043372785415,
      "grad_norm": 0.5003778338432312,
      "learning_rate": 0.00029812028598365806,
      "loss": 1.107,
      "step": 420
    },
    {
      "epoch": 0.02528853929280306,
      "grad_norm": 0.4597870111465454,
      "learning_rate": 0.000298061360779384,
      "loss": 1.2263,
      "step": 430
    },
    {
      "epoch": 0.025876644857751967,
      "grad_norm": 0.471441388130188,
      "learning_rate": 0.00029800243557510995,
      "loss": 1.2067,
      "step": 440
    },
    {
      "epoch": 0.026464750422700876,
      "grad_norm": 0.4598173201084137,
      "learning_rate": 0.00029794351037083593,
      "loss": 1.0516,
      "step": 450
    },
    {
      "epoch": 0.027052855987649782,
      "grad_norm": 0.4474995732307434,
      "learning_rate": 0.00029788458516656185,
      "loss": 1.0867,
      "step": 460
    },
    {
      "epoch": 0.02764096155259869,
      "grad_norm": 0.4516012370586395,
      "learning_rate": 0.0002978256599622878,
      "loss": 1.0608,
      "step": 470
    },
    {
      "epoch": 0.0282290671175476,
      "grad_norm": 0.46654921770095825,
      "learning_rate": 0.0002977667347580138,
      "loss": 1.187,
      "step": 480
    },
    {
      "epoch": 0.02881717268249651,
      "grad_norm": 0.531394898891449,
      "learning_rate": 0.0002977078095537398,
      "loss": 1.2049,
      "step": 490
    },
    {
      "epoch": 0.029405278247445415,
      "grad_norm": 0.5915147662162781,
      "learning_rate": 0.0002976488843494657,
      "loss": 1.1139,
      "step": 500
    },
    {
      "epoch": 0.029993383812394324,
      "grad_norm": 0.4127674102783203,
      "learning_rate": 0.00029758995914519167,
      "loss": 1.1337,
      "step": 510
    },
    {
      "epoch": 0.030581489377343233,
      "grad_norm": 0.4452025294303894,
      "learning_rate": 0.00029753103394091764,
      "loss": 1.1567,
      "step": 520
    },
    {
      "epoch": 0.031169594942292142,
      "grad_norm": 0.5554621815681458,
      "learning_rate": 0.00029747210873664356,
      "loss": 1.1582,
      "step": 530
    },
    {
      "epoch": 0.03175770050724105,
      "grad_norm": 0.48620620369911194,
      "learning_rate": 0.00029741318353236954,
      "loss": 1.1249,
      "step": 540
    },
    {
      "epoch": 0.03234580607218996,
      "grad_norm": 0.5056248307228088,
      "learning_rate": 0.0002973542583280955,
      "loss": 1.1265,
      "step": 550
    },
    {
      "epoch": 0.032933911637138866,
      "grad_norm": 0.4482632875442505,
      "learning_rate": 0.0002972953331238215,
      "loss": 1.2304,
      "step": 560
    },
    {
      "epoch": 0.03352201720208777,
      "grad_norm": 0.44630199670791626,
      "learning_rate": 0.0002972364079195474,
      "loss": 1.1548,
      "step": 570
    },
    {
      "epoch": 0.034110122767036684,
      "grad_norm": 0.4924348294734955,
      "learning_rate": 0.0002971774827152734,
      "loss": 1.1589,
      "step": 580
    },
    {
      "epoch": 0.03469822833198559,
      "grad_norm": 0.5460498332977295,
      "learning_rate": 0.00029711855751099936,
      "loss": 1.2088,
      "step": 590
    },
    {
      "epoch": 0.0352863338969345,
      "grad_norm": 0.3942244350910187,
      "learning_rate": 0.0002970596323067253,
      "loss": 1.1289,
      "step": 600
    },
    {
      "epoch": 0.03587443946188341,
      "grad_norm": 0.5067312121391296,
      "learning_rate": 0.00029700070710245125,
      "loss": 1.1111,
      "step": 610
    },
    {
      "epoch": 0.03646254502683231,
      "grad_norm": 0.44432276487350464,
      "learning_rate": 0.00029694178189817723,
      "loss": 1.1233,
      "step": 620
    },
    {
      "epoch": 0.037050650591781226,
      "grad_norm": 0.3717876970767975,
      "learning_rate": 0.0002968828566939032,
      "loss": 1.1476,
      "step": 630
    },
    {
      "epoch": 0.03763875615673013,
      "grad_norm": 0.41797980666160583,
      "learning_rate": 0.0002968239314896291,
      "loss": 1.1333,
      "step": 640
    },
    {
      "epoch": 0.038226861721679044,
      "grad_norm": 0.4768736958503723,
      "learning_rate": 0.0002967650062853551,
      "loss": 1.1119,
      "step": 650
    },
    {
      "epoch": 0.03881496728662795,
      "grad_norm": 0.3702704608440399,
      "learning_rate": 0.0002967060810810811,
      "loss": 1.15,
      "step": 660
    },
    {
      "epoch": 0.039403072851576855,
      "grad_norm": 0.4271191358566284,
      "learning_rate": 0.000296647155876807,
      "loss": 1.0728,
      "step": 670
    },
    {
      "epoch": 0.03999117841652577,
      "grad_norm": 0.43507686257362366,
      "learning_rate": 0.00029658823067253297,
      "loss": 1.0701,
      "step": 680
    },
    {
      "epoch": 0.04057928398147467,
      "grad_norm": 0.4252344071865082,
      "learning_rate": 0.00029652930546825894,
      "loss": 1.1445,
      "step": 690
    },
    {
      "epoch": 0.041167389546423586,
      "grad_norm": 0.5367803573608398,
      "learning_rate": 0.0002964703802639849,
      "loss": 1.1465,
      "step": 700
    },
    {
      "epoch": 0.04175549511137249,
      "grad_norm": 0.42044878005981445,
      "learning_rate": 0.00029641145505971084,
      "loss": 1.1164,
      "step": 710
    },
    {
      "epoch": 0.0423436006763214,
      "grad_norm": 0.43292269110679626,
      "learning_rate": 0.0002963525298554368,
      "loss": 1.0583,
      "step": 720
    },
    {
      "epoch": 0.04293170624127031,
      "grad_norm": 0.47497788071632385,
      "learning_rate": 0.0002962936046511628,
      "loss": 1.078,
      "step": 730
    },
    {
      "epoch": 0.043519811806219215,
      "grad_norm": 0.46685120463371277,
      "learning_rate": 0.0002962346794468887,
      "loss": 1.0648,
      "step": 740
    },
    {
      "epoch": 0.04410791737116813,
      "grad_norm": 0.4488048255443573,
      "learning_rate": 0.0002961757542426147,
      "loss": 1.1806,
      "step": 750
    },
    {
      "epoch": 0.04469602293611703,
      "grad_norm": 0.4170013964176178,
      "learning_rate": 0.00029611682903834066,
      "loss": 1.2487,
      "step": 760
    },
    {
      "epoch": 0.04528412850106594,
      "grad_norm": 0.436572790145874,
      "learning_rate": 0.00029605790383406663,
      "loss": 1.1736,
      "step": 770
    },
    {
      "epoch": 0.04587223406601485,
      "grad_norm": 0.3733289837837219,
      "learning_rate": 0.00029599897862979255,
      "loss": 1.0703,
      "step": 780
    },
    {
      "epoch": 0.04646033963096376,
      "grad_norm": 0.4595854878425598,
      "learning_rate": 0.00029594005342551853,
      "loss": 1.0563,
      "step": 790
    },
    {
      "epoch": 0.04704844519591267,
      "grad_norm": 0.39430680871009827,
      "learning_rate": 0.0002958811282212445,
      "loss": 1.0944,
      "step": 800
    },
    {
      "epoch": 0.047636550760861575,
      "grad_norm": 0.40343520045280457,
      "learning_rate": 0.0002958222030169704,
      "loss": 1.1382,
      "step": 810
    },
    {
      "epoch": 0.04822465632581048,
      "grad_norm": 0.4650798439979553,
      "learning_rate": 0.0002957632778126964,
      "loss": 1.0745,
      "step": 820
    },
    {
      "epoch": 0.04881276189075939,
      "grad_norm": 0.37783417105674744,
      "learning_rate": 0.0002957043526084223,
      "loss": 1.0668,
      "step": 830
    },
    {
      "epoch": 0.0494008674557083,
      "grad_norm": 0.5120459794998169,
      "learning_rate": 0.0002956454274041483,
      "loss": 1.0777,
      "step": 840
    },
    {
      "epoch": 0.04998897302065721,
      "grad_norm": 0.3728084862232208,
      "learning_rate": 0.00029558650219987427,
      "loss": 1.0893,
      "step": 850
    },
    {
      "epoch": 0.05057707858560612,
      "grad_norm": 0.634568452835083,
      "learning_rate": 0.0002955275769956002,
      "loss": 1.2098,
      "step": 860
    },
    {
      "epoch": 0.05116518415055502,
      "grad_norm": 0.45141392946243286,
      "learning_rate": 0.00029546865179132616,
      "loss": 1.0432,
      "step": 870
    },
    {
      "epoch": 0.051753289715503935,
      "grad_norm": 0.32568129897117615,
      "learning_rate": 0.00029540972658705214,
      "loss": 1.154,
      "step": 880
    },
    {
      "epoch": 0.05234139528045284,
      "grad_norm": 0.44802847504615784,
      "learning_rate": 0.0002953508013827781,
      "loss": 1.1393,
      "step": 890
    },
    {
      "epoch": 0.05292950084540175,
      "grad_norm": 0.5106037259101868,
      "learning_rate": 0.00029529187617850403,
      "loss": 1.2015,
      "step": 900
    },
    {
      "epoch": 0.05351760641035066,
      "grad_norm": 0.43189722299575806,
      "learning_rate": 0.00029523295097423,
      "loss": 1.1585,
      "step": 910
    },
    {
      "epoch": 0.054105711975299564,
      "grad_norm": 0.44759032130241394,
      "learning_rate": 0.000295174025769956,
      "loss": 1.1274,
      "step": 920
    },
    {
      "epoch": 0.05469381754024848,
      "grad_norm": 0.4455403983592987,
      "learning_rate": 0.0002951151005656819,
      "loss": 1.0759,
      "step": 930
    },
    {
      "epoch": 0.05528192310519738,
      "grad_norm": 0.41390976309776306,
      "learning_rate": 0.0002950561753614079,
      "loss": 1.1205,
      "step": 940
    },
    {
      "epoch": 0.05587002867014629,
      "grad_norm": 0.37219157814979553,
      "learning_rate": 0.00029499725015713385,
      "loss": 1.094,
      "step": 950
    },
    {
      "epoch": 0.0564581342350952,
      "grad_norm": 0.41518479585647583,
      "learning_rate": 0.00029493832495285983,
      "loss": 1.2455,
      "step": 960
    },
    {
      "epoch": 0.057046239800044106,
      "grad_norm": 0.3772057890892029,
      "learning_rate": 0.00029487939974858575,
      "loss": 1.1404,
      "step": 970
    },
    {
      "epoch": 0.05763434536499302,
      "grad_norm": 0.4419359862804413,
      "learning_rate": 0.0002948204745443117,
      "loss": 1.144,
      "step": 980
    },
    {
      "epoch": 0.058222450929941924,
      "grad_norm": 0.4240175187587738,
      "learning_rate": 0.0002947615493400377,
      "loss": 1.2056,
      "step": 990
    },
    {
      "epoch": 0.05881055649489083,
      "grad_norm": 0.41166192293167114,
      "learning_rate": 0.0002947026241357636,
      "loss": 1.0694,
      "step": 1000
    },
    {
      "epoch": 0.05939866205983974,
      "grad_norm": 0.3956306576728821,
      "learning_rate": 0.0002946436989314896,
      "loss": 1.0615,
      "step": 1010
    },
    {
      "epoch": 0.05998676762478865,
      "grad_norm": 0.39384523034095764,
      "learning_rate": 0.00029458477372721557,
      "loss": 1.071,
      "step": 1020
    },
    {
      "epoch": 0.06057487318973756,
      "grad_norm": 0.37343671917915344,
      "learning_rate": 0.00029452584852294154,
      "loss": 1.1251,
      "step": 1030
    },
    {
      "epoch": 0.061162978754686466,
      "grad_norm": 0.6738592982292175,
      "learning_rate": 0.00029446692331866746,
      "loss": 1.143,
      "step": 1040
    },
    {
      "epoch": 0.06175108431963537,
      "grad_norm": 0.45311203598976135,
      "learning_rate": 0.00029440799811439344,
      "loss": 1.111,
      "step": 1050
    },
    {
      "epoch": 0.062339189884584284,
      "grad_norm": 0.372273713350296,
      "learning_rate": 0.0002943490729101194,
      "loss": 1.0703,
      "step": 1060
    },
    {
      "epoch": 0.0629272954495332,
      "grad_norm": 0.3498716950416565,
      "learning_rate": 0.00029429014770584533,
      "loss": 1.0963,
      "step": 1070
    },
    {
      "epoch": 0.0635154010144821,
      "grad_norm": 0.4094342589378357,
      "learning_rate": 0.0002942312225015713,
      "loss": 1.1023,
      "step": 1080
    },
    {
      "epoch": 0.06410350657943101,
      "grad_norm": 0.34400051832199097,
      "learning_rate": 0.0002941722972972973,
      "loss": 1.104,
      "step": 1090
    },
    {
      "epoch": 0.06469161214437992,
      "grad_norm": 0.35877230763435364,
      "learning_rate": 0.00029411337209302326,
      "loss": 0.978,
      "step": 1100
    },
    {
      "epoch": 0.06527971770932882,
      "grad_norm": 0.42020076513290405,
      "learning_rate": 0.0002940544468887492,
      "loss": 1.1467,
      "step": 1110
    },
    {
      "epoch": 0.06586782327427773,
      "grad_norm": 0.3924144506454468,
      "learning_rate": 0.00029399552168447515,
      "loss": 1.1087,
      "step": 1120
    },
    {
      "epoch": 0.06645592883922664,
      "grad_norm": 0.45876118540763855,
      "learning_rate": 0.00029393659648020113,
      "loss": 1.1186,
      "step": 1130
    },
    {
      "epoch": 0.06704403440417554,
      "grad_norm": 0.3957752287387848,
      "learning_rate": 0.00029387767127592705,
      "loss": 1.098,
      "step": 1140
    },
    {
      "epoch": 0.06763213996912446,
      "grad_norm": 0.38257545232772827,
      "learning_rate": 0.000293818746071653,
      "loss": 1.0673,
      "step": 1150
    },
    {
      "epoch": 0.06822024553407337,
      "grad_norm": 0.4532524347305298,
      "learning_rate": 0.000293759820867379,
      "loss": 1.1695,
      "step": 1160
    },
    {
      "epoch": 0.06880835109902228,
      "grad_norm": 0.4249649941921234,
      "learning_rate": 0.00029370089566310497,
      "loss": 1.1033,
      "step": 1170
    },
    {
      "epoch": 0.06939645666397118,
      "grad_norm": 0.4050876498222351,
      "learning_rate": 0.0002936419704588309,
      "loss": 1.1634,
      "step": 1180
    },
    {
      "epoch": 0.06998456222892009,
      "grad_norm": 0.4182823896408081,
      "learning_rate": 0.00029358304525455687,
      "loss": 1.143,
      "step": 1190
    },
    {
      "epoch": 0.070572667793869,
      "grad_norm": 0.44158241152763367,
      "learning_rate": 0.00029352412005028284,
      "loss": 1.0537,
      "step": 1200
    },
    {
      "epoch": 0.0711607733588179,
      "grad_norm": 0.4288628101348877,
      "learning_rate": 0.00029346519484600876,
      "loss": 1.0988,
      "step": 1210
    },
    {
      "epoch": 0.07174887892376682,
      "grad_norm": 0.40149974822998047,
      "learning_rate": 0.00029340626964173474,
      "loss": 1.11,
      "step": 1220
    },
    {
      "epoch": 0.07233698448871573,
      "grad_norm": 0.411335825920105,
      "learning_rate": 0.0002933473444374607,
      "loss": 1.1616,
      "step": 1230
    },
    {
      "epoch": 0.07292509005366463,
      "grad_norm": 0.4490196704864502,
      "learning_rate": 0.00029328841923318663,
      "loss": 1.2397,
      "step": 1240
    },
    {
      "epoch": 0.07351319561861354,
      "grad_norm": 0.3882448673248291,
      "learning_rate": 0.0002932294940289126,
      "loss": 1.1544,
      "step": 1250
    },
    {
      "epoch": 0.07410130118356245,
      "grad_norm": 0.4035611152648926,
      "learning_rate": 0.00029317056882463853,
      "loss": 1.0576,
      "step": 1260
    },
    {
      "epoch": 0.07468940674851136,
      "grad_norm": 0.40731239318847656,
      "learning_rate": 0.0002931116436203645,
      "loss": 0.9949,
      "step": 1270
    },
    {
      "epoch": 0.07527751231346026,
      "grad_norm": 0.46896296739578247,
      "learning_rate": 0.0002930527184160905,
      "loss": 1.1376,
      "step": 1280
    },
    {
      "epoch": 0.07586561787840918,
      "grad_norm": 0.4198344945907593,
      "learning_rate": 0.00029299379321181645,
      "loss": 1.0552,
      "step": 1290
    },
    {
      "epoch": 0.07645372344335809,
      "grad_norm": 0.4793877899646759,
      "learning_rate": 0.0002929348680075424,
      "loss": 1.0646,
      "step": 1300
    },
    {
      "epoch": 0.07704182900830699,
      "grad_norm": 0.40739452838897705,
      "learning_rate": 0.00029287594280326835,
      "loss": 1.1074,
      "step": 1310
    },
    {
      "epoch": 0.0776299345732559,
      "grad_norm": 0.4312345087528229,
      "learning_rate": 0.0002928170175989943,
      "loss": 1.1258,
      "step": 1320
    },
    {
      "epoch": 0.07821804013820481,
      "grad_norm": 0.40024542808532715,
      "learning_rate": 0.00029275809239472024,
      "loss": 1.126,
      "step": 1330
    },
    {
      "epoch": 0.07880614570315371,
      "grad_norm": 0.41153132915496826,
      "learning_rate": 0.0002926991671904462,
      "loss": 1.1262,
      "step": 1340
    },
    {
      "epoch": 0.07939425126810262,
      "grad_norm": 0.493206650018692,
      "learning_rate": 0.0002926402419861722,
      "loss": 1.1472,
      "step": 1350
    },
    {
      "epoch": 0.07998235683305153,
      "grad_norm": 0.3821393847465515,
      "learning_rate": 0.00029258131678189817,
      "loss": 1.1018,
      "step": 1360
    },
    {
      "epoch": 0.08057046239800045,
      "grad_norm": 0.38259661197662354,
      "learning_rate": 0.0002925223915776241,
      "loss": 1.1526,
      "step": 1370
    },
    {
      "epoch": 0.08115856796294935,
      "grad_norm": 0.4143788516521454,
      "learning_rate": 0.00029246346637335006,
      "loss": 1.1833,
      "step": 1380
    },
    {
      "epoch": 0.08174667352789826,
      "grad_norm": 0.4138440191745758,
      "learning_rate": 0.00029240454116907604,
      "loss": 1.0944,
      "step": 1390
    },
    {
      "epoch": 0.08233477909284717,
      "grad_norm": 0.46727001667022705,
      "learning_rate": 0.00029234561596480196,
      "loss": 1.1574,
      "step": 1400
    },
    {
      "epoch": 0.08292288465779607,
      "grad_norm": 0.3885822594165802,
      "learning_rate": 0.00029228669076052793,
      "loss": 1.0243,
      "step": 1410
    },
    {
      "epoch": 0.08351099022274498,
      "grad_norm": 0.37271764874458313,
      "learning_rate": 0.0002922277655562539,
      "loss": 1.0965,
      "step": 1420
    },
    {
      "epoch": 0.0840990957876939,
      "grad_norm": 0.3486955165863037,
      "learning_rate": 0.0002921688403519799,
      "loss": 1.0847,
      "step": 1430
    },
    {
      "epoch": 0.0846872013526428,
      "grad_norm": 0.3919849991798401,
      "learning_rate": 0.0002921099151477058,
      "loss": 1.0253,
      "step": 1440
    },
    {
      "epoch": 0.0852753069175917,
      "grad_norm": 0.3528934419155121,
      "learning_rate": 0.0002920509899434318,
      "loss": 1.0519,
      "step": 1450
    },
    {
      "epoch": 0.08586341248254062,
      "grad_norm": 0.3449113070964813,
      "learning_rate": 0.00029199206473915775,
      "loss": 1.0607,
      "step": 1460
    },
    {
      "epoch": 0.08645151804748953,
      "grad_norm": 0.3627431392669678,
      "learning_rate": 0.00029193313953488367,
      "loss": 1.1086,
      "step": 1470
    },
    {
      "epoch": 0.08703962361243843,
      "grad_norm": 0.3521270155906677,
      "learning_rate": 0.00029187421433060965,
      "loss": 1.0832,
      "step": 1480
    },
    {
      "epoch": 0.08762772917738734,
      "grad_norm": 0.34419921040534973,
      "learning_rate": 0.0002918152891263356,
      "loss": 1.1081,
      "step": 1490
    },
    {
      "epoch": 0.08821583474233625,
      "grad_norm": 0.3976770043373108,
      "learning_rate": 0.0002917563639220616,
      "loss": 1.1384,
      "step": 1500
    },
    {
      "epoch": 0.08880394030728515,
      "grad_norm": 0.395351767539978,
      "learning_rate": 0.0002916974387177875,
      "loss": 1.135,
      "step": 1510
    },
    {
      "epoch": 0.08939204587223407,
      "grad_norm": 0.3606332838535309,
      "learning_rate": 0.0002916385135135135,
      "loss": 1.0826,
      "step": 1520
    },
    {
      "epoch": 0.08998015143718298,
      "grad_norm": 0.4767404794692993,
      "learning_rate": 0.00029157958830923947,
      "loss": 1.1123,
      "step": 1530
    },
    {
      "epoch": 0.09056825700213188,
      "grad_norm": 0.40864261984825134,
      "learning_rate": 0.0002915206631049654,
      "loss": 1.0102,
      "step": 1540
    },
    {
      "epoch": 0.09115636256708079,
      "grad_norm": 0.3601161241531372,
      "learning_rate": 0.00029146173790069136,
      "loss": 1.1461,
      "step": 1550
    },
    {
      "epoch": 0.0917444681320297,
      "grad_norm": 0.3751782774925232,
      "learning_rate": 0.00029140281269641734,
      "loss": 1.1291,
      "step": 1560
    },
    {
      "epoch": 0.0923325736969786,
      "grad_norm": 0.3435978889465332,
      "learning_rate": 0.0002913438874921433,
      "loss": 1.1452,
      "step": 1570
    },
    {
      "epoch": 0.09292067926192751,
      "grad_norm": 0.3776666224002838,
      "learning_rate": 0.00029128496228786923,
      "loss": 1.1412,
      "step": 1580
    },
    {
      "epoch": 0.09350878482687643,
      "grad_norm": 0.3686293363571167,
      "learning_rate": 0.0002912260370835952,
      "loss": 1.0338,
      "step": 1590
    },
    {
      "epoch": 0.09409689039182534,
      "grad_norm": 0.3356236219406128,
      "learning_rate": 0.0002911671118793212,
      "loss": 1.0923,
      "step": 1600
    },
    {
      "epoch": 0.09468499595677424,
      "grad_norm": 0.36502835154533386,
      "learning_rate": 0.0002911081866750471,
      "loss": 1.0522,
      "step": 1610
    },
    {
      "epoch": 0.09527310152172315,
      "grad_norm": 0.3784879446029663,
      "learning_rate": 0.0002910492614707731,
      "loss": 1.1488,
      "step": 1620
    },
    {
      "epoch": 0.09586120708667206,
      "grad_norm": 0.4185827374458313,
      "learning_rate": 0.00029099033626649905,
      "loss": 1.0773,
      "step": 1630
    },
    {
      "epoch": 0.09644931265162096,
      "grad_norm": 0.3433593809604645,
      "learning_rate": 0.000290931411062225,
      "loss": 1.0473,
      "step": 1640
    },
    {
      "epoch": 0.09703741821656987,
      "grad_norm": 0.3217140734195709,
      "learning_rate": 0.00029087248585795095,
      "loss": 1.1414,
      "step": 1650
    },
    {
      "epoch": 0.09762552378151879,
      "grad_norm": 0.34208330512046814,
      "learning_rate": 0.00029081356065367687,
      "loss": 1.0959,
      "step": 1660
    },
    {
      "epoch": 0.09821362934646768,
      "grad_norm": 0.4215960204601288,
      "learning_rate": 0.00029075463544940284,
      "loss": 1.0521,
      "step": 1670
    },
    {
      "epoch": 0.0988017349114166,
      "grad_norm": 0.3484281599521637,
      "learning_rate": 0.0002906957102451288,
      "loss": 1.0995,
      "step": 1680
    },
    {
      "epoch": 0.09938984047636551,
      "grad_norm": 0.4282454550266266,
      "learning_rate": 0.0002906367850408548,
      "loss": 1.0791,
      "step": 1690
    },
    {
      "epoch": 0.09997794604131442,
      "grad_norm": 0.38426753878593445,
      "learning_rate": 0.0002905778598365807,
      "loss": 1.1568,
      "step": 1700
    },
    {
      "epoch": 0.10056605160626332,
      "grad_norm": 0.31963881850242615,
      "learning_rate": 0.0002905189346323067,
      "loss": 1.0753,
      "step": 1710
    },
    {
      "epoch": 0.10115415717121223,
      "grad_norm": 0.3615850806236267,
      "learning_rate": 0.00029046000942803266,
      "loss": 1.0262,
      "step": 1720
    },
    {
      "epoch": 0.10174226273616115,
      "grad_norm": 0.3732593059539795,
      "learning_rate": 0.0002904010842237586,
      "loss": 1.2466,
      "step": 1730
    },
    {
      "epoch": 0.10233036830111004,
      "grad_norm": 0.396153062582016,
      "learning_rate": 0.00029034215901948456,
      "loss": 1.0731,
      "step": 1740
    },
    {
      "epoch": 0.10291847386605896,
      "grad_norm": 0.4071430563926697,
      "learning_rate": 0.00029028323381521053,
      "loss": 1.0984,
      "step": 1750
    },
    {
      "epoch": 0.10350657943100787,
      "grad_norm": 0.34861162304878235,
      "learning_rate": 0.0002902243086109365,
      "loss": 0.9746,
      "step": 1760
    },
    {
      "epoch": 0.10409468499595677,
      "grad_norm": 0.39686620235443115,
      "learning_rate": 0.00029016538340666243,
      "loss": 1.0798,
      "step": 1770
    },
    {
      "epoch": 0.10468279056090568,
      "grad_norm": 0.3493978977203369,
      "learning_rate": 0.0002901064582023884,
      "loss": 1.0835,
      "step": 1780
    },
    {
      "epoch": 0.1052708961258546,
      "grad_norm": 0.3660849928855896,
      "learning_rate": 0.0002900475329981144,
      "loss": 1.1786,
      "step": 1790
    },
    {
      "epoch": 0.1058590016908035,
      "grad_norm": 0.3317444324493408,
      "learning_rate": 0.0002899886077938403,
      "loss": 1.0595,
      "step": 1800
    },
    {
      "epoch": 0.1064471072557524,
      "grad_norm": 0.38788217306137085,
      "learning_rate": 0.00028992968258956627,
      "loss": 1.1394,
      "step": 1810
    },
    {
      "epoch": 0.10703521282070132,
      "grad_norm": 0.4235611855983734,
      "learning_rate": 0.00028987075738529225,
      "loss": 1.0291,
      "step": 1820
    },
    {
      "epoch": 0.10762331838565023,
      "grad_norm": 0.4006715416908264,
      "learning_rate": 0.0002898118321810182,
      "loss": 1.04,
      "step": 1830
    },
    {
      "epoch": 0.10821142395059913,
      "grad_norm": 0.4059234857559204,
      "learning_rate": 0.00028975290697674414,
      "loss": 1.1108,
      "step": 1840
    },
    {
      "epoch": 0.10879952951554804,
      "grad_norm": 0.3552429974079132,
      "learning_rate": 0.0002896939817724701,
      "loss": 1.1446,
      "step": 1850
    },
    {
      "epoch": 0.10938763508049695,
      "grad_norm": 0.3340567946434021,
      "learning_rate": 0.0002896350565681961,
      "loss": 1.1522,
      "step": 1860
    },
    {
      "epoch": 0.10997574064544585,
      "grad_norm": 0.3639146089553833,
      "learning_rate": 0.000289576131363922,
      "loss": 1.1628,
      "step": 1870
    },
    {
      "epoch": 0.11056384621039476,
      "grad_norm": 0.3678780794143677,
      "learning_rate": 0.000289517206159648,
      "loss": 1.1061,
      "step": 1880
    },
    {
      "epoch": 0.11115195177534368,
      "grad_norm": 0.5176407694816589,
      "learning_rate": 0.00028945828095537396,
      "loss": 1.0537,
      "step": 1890
    },
    {
      "epoch": 0.11174005734029258,
      "grad_norm": 0.39405593276023865,
      "learning_rate": 0.00028939935575109994,
      "loss": 1.1253,
      "step": 1900
    },
    {
      "epoch": 0.11232816290524149,
      "grad_norm": 0.3478938937187195,
      "learning_rate": 0.00028934043054682586,
      "loss": 0.9668,
      "step": 1910
    },
    {
      "epoch": 0.1129162684701904,
      "grad_norm": 0.39177465438842773,
      "learning_rate": 0.00028928150534255183,
      "loss": 1.1113,
      "step": 1920
    },
    {
      "epoch": 0.11350437403513931,
      "grad_norm": 0.3384564220905304,
      "learning_rate": 0.0002892225801382778,
      "loss": 1.0545,
      "step": 1930
    },
    {
      "epoch": 0.11409247960008821,
      "grad_norm": 0.3770425319671631,
      "learning_rate": 0.00028916365493400373,
      "loss": 1.0647,
      "step": 1940
    },
    {
      "epoch": 0.11468058516503712,
      "grad_norm": 0.37233930826187134,
      "learning_rate": 0.0002891047297297297,
      "loss": 1.0261,
      "step": 1950
    },
    {
      "epoch": 0.11526869072998604,
      "grad_norm": 0.4479830265045166,
      "learning_rate": 0.0002890458045254557,
      "loss": 1.122,
      "step": 1960
    },
    {
      "epoch": 0.11585679629493494,
      "grad_norm": 0.360519677400589,
      "learning_rate": 0.00028898687932118165,
      "loss": 1.0473,
      "step": 1970
    },
    {
      "epoch": 0.11644490185988385,
      "grad_norm": 0.38182321190834045,
      "learning_rate": 0.00028892795411690757,
      "loss": 1.055,
      "step": 1980
    },
    {
      "epoch": 0.11703300742483276,
      "grad_norm": 0.33445656299591064,
      "learning_rate": 0.00028886902891263355,
      "loss": 1.0171,
      "step": 1990
    },
    {
      "epoch": 0.11762111298978166,
      "grad_norm": 0.4253181219100952,
      "learning_rate": 0.0002888101037083595,
      "loss": 1.0284,
      "step": 2000
    },
    {
      "epoch": 0.11820921855473057,
      "grad_norm": 0.34886473417282104,
      "learning_rate": 0.00028875117850408544,
      "loss": 1.1595,
      "step": 2010
    },
    {
      "epoch": 0.11879732411967948,
      "grad_norm": 0.33041319251060486,
      "learning_rate": 0.0002886922532998114,
      "loss": 1.1065,
      "step": 2020
    },
    {
      "epoch": 0.1193854296846284,
      "grad_norm": 0.33517229557037354,
      "learning_rate": 0.0002886333280955374,
      "loss": 1.0001,
      "step": 2030
    },
    {
      "epoch": 0.1199735352495773,
      "grad_norm": 0.3744744062423706,
      "learning_rate": 0.00028857440289126337,
      "loss": 1.0291,
      "step": 2040
    },
    {
      "epoch": 0.12056164081452621,
      "grad_norm": 0.3837451934814453,
      "learning_rate": 0.0002885154776869893,
      "loss": 1.0685,
      "step": 2050
    },
    {
      "epoch": 0.12114974637947512,
      "grad_norm": 0.4016897678375244,
      "learning_rate": 0.00028845655248271526,
      "loss": 1.1503,
      "step": 2060
    },
    {
      "epoch": 0.12173785194442402,
      "grad_norm": 0.4057416617870331,
      "learning_rate": 0.0002883976272784412,
      "loss": 1.0267,
      "step": 2070
    },
    {
      "epoch": 0.12232595750937293,
      "grad_norm": 0.34350404143333435,
      "learning_rate": 0.00028833870207416716,
      "loss": 1.1097,
      "step": 2080
    },
    {
      "epoch": 0.12291406307432184,
      "grad_norm": 0.4420211613178253,
      "learning_rate": 0.00028827977686989313,
      "loss": 1.1322,
      "step": 2090
    },
    {
      "epoch": 0.12350216863927074,
      "grad_norm": 0.33641430735588074,
      "learning_rate": 0.00028822085166561905,
      "loss": 1.0399,
      "step": 2100
    },
    {
      "epoch": 0.12409027420421966,
      "grad_norm": 0.4001871347427368,
      "learning_rate": 0.000288161926461345,
      "loss": 1.0032,
      "step": 2110
    },
    {
      "epoch": 0.12467837976916857,
      "grad_norm": 0.35401201248168945,
      "learning_rate": 0.000288103001257071,
      "loss": 1.102,
      "step": 2120
    },
    {
      "epoch": 0.12526648533411747,
      "grad_norm": 0.4077368378639221,
      "learning_rate": 0.0002880440760527969,
      "loss": 1.1586,
      "step": 2130
    },
    {
      "epoch": 0.1258545908990664,
      "grad_norm": 0.3750026822090149,
      "learning_rate": 0.0002879851508485229,
      "loss": 1.0388,
      "step": 2140
    },
    {
      "epoch": 0.1264426964640153,
      "grad_norm": 0.4107149541378021,
      "learning_rate": 0.00028792622564424887,
      "loss": 0.9928,
      "step": 2150
    },
    {
      "epoch": 0.1270308020289642,
      "grad_norm": 0.35290610790252686,
      "learning_rate": 0.00028786730043997485,
      "loss": 1.058,
      "step": 2160
    },
    {
      "epoch": 0.12761890759391312,
      "grad_norm": 0.38815563917160034,
      "learning_rate": 0.00028780837523570077,
      "loss": 1.0495,
      "step": 2170
    },
    {
      "epoch": 0.12820701315886202,
      "grad_norm": 0.3572966158390045,
      "learning_rate": 0.00028774945003142674,
      "loss": 1.0506,
      "step": 2180
    },
    {
      "epoch": 0.12879511872381091,
      "grad_norm": 0.3822269141674042,
      "learning_rate": 0.0002876905248271527,
      "loss": 0.9924,
      "step": 2190
    },
    {
      "epoch": 0.12938322428875984,
      "grad_norm": 0.34148305654525757,
      "learning_rate": 0.00028763159962287864,
      "loss": 1.1299,
      "step": 2200
    },
    {
      "epoch": 0.12997132985370874,
      "grad_norm": 0.37369129061698914,
      "learning_rate": 0.0002875726744186046,
      "loss": 1.1163,
      "step": 2210
    },
    {
      "epoch": 0.13055943541865764,
      "grad_norm": 0.3918123245239258,
      "learning_rate": 0.0002875137492143306,
      "loss": 1.1217,
      "step": 2220
    },
    {
      "epoch": 0.13114754098360656,
      "grad_norm": 0.4035830497741699,
      "learning_rate": 0.00028745482401005656,
      "loss": 1.0303,
      "step": 2230
    },
    {
      "epoch": 0.13173564654855546,
      "grad_norm": 0.35373640060424805,
      "learning_rate": 0.0002873958988057825,
      "loss": 1.0811,
      "step": 2240
    },
    {
      "epoch": 0.13232375211350436,
      "grad_norm": 0.33787107467651367,
      "learning_rate": 0.00028733697360150846,
      "loss": 1.079,
      "step": 2250
    },
    {
      "epoch": 0.1329118576784533,
      "grad_norm": 0.4219416081905365,
      "learning_rate": 0.00028727804839723443,
      "loss": 1.0254,
      "step": 2260
    },
    {
      "epoch": 0.1334999632434022,
      "grad_norm": 0.3223458528518677,
      "learning_rate": 0.00028721912319296035,
      "loss": 0.9979,
      "step": 2270
    },
    {
      "epoch": 0.13408806880835109,
      "grad_norm": 0.35340648889541626,
      "learning_rate": 0.0002871601979886863,
      "loss": 1.1595,
      "step": 2280
    },
    {
      "epoch": 0.1346761743733,
      "grad_norm": 0.36841022968292236,
      "learning_rate": 0.0002871012727844123,
      "loss": 1.101,
      "step": 2290
    },
    {
      "epoch": 0.1352642799382489,
      "grad_norm": 0.38630878925323486,
      "learning_rate": 0.0002870423475801383,
      "loss": 1.1216,
      "step": 2300
    },
    {
      "epoch": 0.13585238550319784,
      "grad_norm": 0.36327826976776123,
      "learning_rate": 0.0002869834223758642,
      "loss": 1.1654,
      "step": 2310
    },
    {
      "epoch": 0.13644049106814674,
      "grad_norm": 0.36676645278930664,
      "learning_rate": 0.00028692449717159017,
      "loss": 1.0823,
      "step": 2320
    },
    {
      "epoch": 0.13702859663309563,
      "grad_norm": 0.3388923108577728,
      "learning_rate": 0.00028686557196731615,
      "loss": 1.0309,
      "step": 2330
    },
    {
      "epoch": 0.13761670219804456,
      "grad_norm": 0.378446489572525,
      "learning_rate": 0.00028680664676304207,
      "loss": 1.0289,
      "step": 2340
    },
    {
      "epoch": 0.13820480776299346,
      "grad_norm": 0.3109877109527588,
      "learning_rate": 0.00028674772155876804,
      "loss": 1.1029,
      "step": 2350
    },
    {
      "epoch": 0.13879291332794236,
      "grad_norm": 0.3754013478755951,
      "learning_rate": 0.000286688796354494,
      "loss": 1.0995,
      "step": 2360
    },
    {
      "epoch": 0.13938101889289128,
      "grad_norm": 0.36246752738952637,
      "learning_rate": 0.00028662987115022,
      "loss": 1.0961,
      "step": 2370
    },
    {
      "epoch": 0.13996912445784018,
      "grad_norm": 0.4641627073287964,
      "learning_rate": 0.0002865709459459459,
      "loss": 1.0524,
      "step": 2380
    },
    {
      "epoch": 0.14055723002278908,
      "grad_norm": 0.3570825755596161,
      "learning_rate": 0.0002865120207416719,
      "loss": 1.1141,
      "step": 2390
    },
    {
      "epoch": 0.141145335587738,
      "grad_norm": 0.38178059458732605,
      "learning_rate": 0.00028645309553739786,
      "loss": 1.0632,
      "step": 2400
    },
    {
      "epoch": 0.1417334411526869,
      "grad_norm": 0.3691385090351105,
      "learning_rate": 0.0002863941703331238,
      "loss": 1.0662,
      "step": 2410
    },
    {
      "epoch": 0.1423215467176358,
      "grad_norm": 0.3558020293712616,
      "learning_rate": 0.00028633524512884976,
      "loss": 1.158,
      "step": 2420
    },
    {
      "epoch": 0.14290965228258473,
      "grad_norm": 0.3838139772415161,
      "learning_rate": 0.00028627631992457573,
      "loss": 1.1089,
      "step": 2430
    },
    {
      "epoch": 0.14349775784753363,
      "grad_norm": 0.3412465453147888,
      "learning_rate": 0.0002862173947203017,
      "loss": 1.0734,
      "step": 2440
    },
    {
      "epoch": 0.14408586341248253,
      "grad_norm": 0.4252605140209198,
      "learning_rate": 0.0002861584695160276,
      "loss": 1.1002,
      "step": 2450
    },
    {
      "epoch": 0.14467396897743146,
      "grad_norm": 0.3738976716995239,
      "learning_rate": 0.0002860995443117536,
      "loss": 1.0548,
      "step": 2460
    },
    {
      "epoch": 0.14526207454238035,
      "grad_norm": 0.3834873139858246,
      "learning_rate": 0.0002860406191074796,
      "loss": 1.0435,
      "step": 2470
    },
    {
      "epoch": 0.14585018010732925,
      "grad_norm": 0.3680359125137329,
      "learning_rate": 0.0002859816939032055,
      "loss": 1.0492,
      "step": 2480
    },
    {
      "epoch": 0.14643828567227818,
      "grad_norm": 0.37317922711372375,
      "learning_rate": 0.00028592276869893147,
      "loss": 1.0782,
      "step": 2490
    },
    {
      "epoch": 0.14702639123722708,
      "grad_norm": 0.4690002202987671,
      "learning_rate": 0.0002858638434946574,
      "loss": 1.0368,
      "step": 2500
    },
    {
      "epoch": 0.147614496802176,
      "grad_norm": 0.4295642375946045,
      "learning_rate": 0.00028580491829038337,
      "loss": 1.0877,
      "step": 2510
    },
    {
      "epoch": 0.1482026023671249,
      "grad_norm": 0.3870314061641693,
      "learning_rate": 0.00028574599308610934,
      "loss": 1.0718,
      "step": 2520
    },
    {
      "epoch": 0.1487907079320738,
      "grad_norm": 0.36963358521461487,
      "learning_rate": 0.00028568706788183526,
      "loss": 1.1213,
      "step": 2530
    },
    {
      "epoch": 0.14937881349702273,
      "grad_norm": 0.3560134470462799,
      "learning_rate": 0.00028562814267756124,
      "loss": 1.1472,
      "step": 2540
    },
    {
      "epoch": 0.14996691906197163,
      "grad_norm": 0.3443005084991455,
      "learning_rate": 0.0002855692174732872,
      "loss": 1.0707,
      "step": 2550
    },
    {
      "epoch": 0.15055502462692053,
      "grad_norm": 0.44806042313575745,
      "learning_rate": 0.0002855102922690132,
      "loss": 1.0802,
      "step": 2560
    },
    {
      "epoch": 0.15114313019186945,
      "grad_norm": 0.3693299889564514,
      "learning_rate": 0.0002854513670647391,
      "loss": 0.982,
      "step": 2570
    },
    {
      "epoch": 0.15173123575681835,
      "grad_norm": 0.3421916365623474,
      "learning_rate": 0.0002853924418604651,
      "loss": 1.0772,
      "step": 2580
    },
    {
      "epoch": 0.15231934132176725,
      "grad_norm": 0.48673367500305176,
      "learning_rate": 0.00028533351665619106,
      "loss": 1.0673,
      "step": 2590
    },
    {
      "epoch": 0.15290744688671618,
      "grad_norm": 0.37379297614097595,
      "learning_rate": 0.000285274591451917,
      "loss": 1.1404,
      "step": 2600
    },
    {
      "epoch": 0.15349555245166507,
      "grad_norm": 0.3981846570968628,
      "learning_rate": 0.00028521566624764295,
      "loss": 1.0776,
      "step": 2610
    },
    {
      "epoch": 0.15408365801661397,
      "grad_norm": 0.3931783437728882,
      "learning_rate": 0.0002851567410433689,
      "loss": 1.0897,
      "step": 2620
    },
    {
      "epoch": 0.1546717635815629,
      "grad_norm": 0.3983454704284668,
      "learning_rate": 0.0002850978158390949,
      "loss": 1.0299,
      "step": 2630
    },
    {
      "epoch": 0.1552598691465118,
      "grad_norm": 0.3568795621395111,
      "learning_rate": 0.0002850388906348208,
      "loss": 1.0654,
      "step": 2640
    },
    {
      "epoch": 0.1558479747114607,
      "grad_norm": 0.44731640815734863,
      "learning_rate": 0.0002849799654305468,
      "loss": 1.0735,
      "step": 2650
    },
    {
      "epoch": 0.15643608027640962,
      "grad_norm": 0.35215267539024353,
      "learning_rate": 0.00028492104022627277,
      "loss": 1.0536,
      "step": 2660
    },
    {
      "epoch": 0.15702418584135852,
      "grad_norm": 0.423831045627594,
      "learning_rate": 0.0002848621150219987,
      "loss": 0.9668,
      "step": 2670
    },
    {
      "epoch": 0.15761229140630742,
      "grad_norm": 0.39006730914115906,
      "learning_rate": 0.00028480318981772467,
      "loss": 1.1902,
      "step": 2680
    },
    {
      "epoch": 0.15820039697125635,
      "grad_norm": 0.3817098140716553,
      "learning_rate": 0.00028474426461345064,
      "loss": 0.9905,
      "step": 2690
    },
    {
      "epoch": 0.15878850253620524,
      "grad_norm": 0.3641454875469208,
      "learning_rate": 0.0002846853394091766,
      "loss": 1.0972,
      "step": 2700
    },
    {
      "epoch": 0.15937660810115414,
      "grad_norm": 0.3691389262676239,
      "learning_rate": 0.00028462641420490254,
      "loss": 1.0895,
      "step": 2710
    },
    {
      "epoch": 0.15996471366610307,
      "grad_norm": 0.34205424785614014,
      "learning_rate": 0.0002845674890006285,
      "loss": 1.0729,
      "step": 2720
    },
    {
      "epoch": 0.16055281923105197,
      "grad_norm": 0.3923344016075134,
      "learning_rate": 0.0002845085637963545,
      "loss": 1.0177,
      "step": 2730
    },
    {
      "epoch": 0.1611409247960009,
      "grad_norm": 0.3494298458099365,
      "learning_rate": 0.0002844496385920804,
      "loss": 1.0954,
      "step": 2740
    },
    {
      "epoch": 0.1617290303609498,
      "grad_norm": 0.35264089703559875,
      "learning_rate": 0.0002843907133878064,
      "loss": 1.1237,
      "step": 2750
    },
    {
      "epoch": 0.1623171359258987,
      "grad_norm": 0.3668877184391022,
      "learning_rate": 0.00028433178818353236,
      "loss": 1.0042,
      "step": 2760
    },
    {
      "epoch": 0.16290524149084762,
      "grad_norm": 0.3602745234966278,
      "learning_rate": 0.00028427286297925833,
      "loss": 1.1064,
      "step": 2770
    },
    {
      "epoch": 0.16349334705579652,
      "grad_norm": 0.36614325642585754,
      "learning_rate": 0.00028421393777498425,
      "loss": 1.1015,
      "step": 2780
    },
    {
      "epoch": 0.16408145262074542,
      "grad_norm": 0.35096514225006104,
      "learning_rate": 0.0002841550125707102,
      "loss": 1.0958,
      "step": 2790
    },
    {
      "epoch": 0.16466955818569434,
      "grad_norm": 0.38875606656074524,
      "learning_rate": 0.0002840960873664362,
      "loss": 1.0379,
      "step": 2800
    },
    {
      "epoch": 0.16525766375064324,
      "grad_norm": 0.3953165113925934,
      "learning_rate": 0.0002840371621621621,
      "loss": 1.063,
      "step": 2810
    },
    {
      "epoch": 0.16584576931559214,
      "grad_norm": 0.35875165462493896,
      "learning_rate": 0.0002839782369578881,
      "loss": 1.0391,
      "step": 2820
    },
    {
      "epoch": 0.16643387488054107,
      "grad_norm": 0.3384517431259155,
      "learning_rate": 0.00028391931175361407,
      "loss": 1.0748,
      "step": 2830
    },
    {
      "epoch": 0.16702198044548996,
      "grad_norm": 0.4023648500442505,
      "learning_rate": 0.00028386038654934005,
      "loss": 0.9492,
      "step": 2840
    },
    {
      "epoch": 0.16761008601043886,
      "grad_norm": 0.30413421988487244,
      "learning_rate": 0.00028380146134506597,
      "loss": 1.09,
      "step": 2850
    },
    {
      "epoch": 0.1681981915753878,
      "grad_norm": 0.38912978768348694,
      "learning_rate": 0.00028374253614079194,
      "loss": 1.1385,
      "step": 2860
    },
    {
      "epoch": 0.1687862971403367,
      "grad_norm": 0.39484551548957825,
      "learning_rate": 0.0002836836109365179,
      "loss": 1.1688,
      "step": 2870
    },
    {
      "epoch": 0.1693744027052856,
      "grad_norm": 0.4086456000804901,
      "learning_rate": 0.00028362468573224384,
      "loss": 1.0758,
      "step": 2880
    },
    {
      "epoch": 0.1699625082702345,
      "grad_norm": 0.3487234115600586,
      "learning_rate": 0.0002835657605279698,
      "loss": 1.0446,
      "step": 2890
    },
    {
      "epoch": 0.1705506138351834,
      "grad_norm": 0.35590001940727234,
      "learning_rate": 0.0002835068353236958,
      "loss": 1.051,
      "step": 2900
    },
    {
      "epoch": 0.1711387194001323,
      "grad_norm": 0.4164336919784546,
      "learning_rate": 0.0002834479101194217,
      "loss": 0.9544,
      "step": 2910
    },
    {
      "epoch": 0.17172682496508124,
      "grad_norm": 0.42634254693984985,
      "learning_rate": 0.0002833889849151477,
      "loss": 1.0912,
      "step": 2920
    },
    {
      "epoch": 0.17231493053003014,
      "grad_norm": 0.4024883806705475,
      "learning_rate": 0.0002833300597108736,
      "loss": 1.0255,
      "step": 2930
    },
    {
      "epoch": 0.17290303609497906,
      "grad_norm": 0.35621920228004456,
      "learning_rate": 0.0002832711345065996,
      "loss": 1.0512,
      "step": 2940
    },
    {
      "epoch": 0.17349114165992796,
      "grad_norm": 0.35232415795326233,
      "learning_rate": 0.00028321220930232555,
      "loss": 1.1378,
      "step": 2950
    },
    {
      "epoch": 0.17407924722487686,
      "grad_norm": 0.35269689559936523,
      "learning_rate": 0.0002831532840980515,
      "loss": 1.0441,
      "step": 2960
    },
    {
      "epoch": 0.17466735278982579,
      "grad_norm": 0.3697231411933899,
      "learning_rate": 0.00028309435889377745,
      "loss": 1.0846,
      "step": 2970
    },
    {
      "epoch": 0.17525545835477468,
      "grad_norm": 0.36466458439826965,
      "learning_rate": 0.0002830354336895034,
      "loss": 1.0037,
      "step": 2980
    },
    {
      "epoch": 0.17584356391972358,
      "grad_norm": 0.32982975244522095,
      "learning_rate": 0.0002829765084852294,
      "loss": 0.9758,
      "step": 2990
    },
    {
      "epoch": 0.1764316694846725,
      "grad_norm": 0.3748198449611664,
      "learning_rate": 0.0002829175832809553,
      "loss": 1.0969,
      "step": 3000
    },
    {
      "epoch": 0.1770197750496214,
      "grad_norm": 0.3602713346481323,
      "learning_rate": 0.0002828586580766813,
      "loss": 1.0339,
      "step": 3010
    },
    {
      "epoch": 0.1776078806145703,
      "grad_norm": 0.3732072114944458,
      "learning_rate": 0.00028279973287240727,
      "loss": 1.1381,
      "step": 3020
    },
    {
      "epoch": 0.17819598617951923,
      "grad_norm": 0.37597233057022095,
      "learning_rate": 0.00028274080766813324,
      "loss": 1.1085,
      "step": 3030
    },
    {
      "epoch": 0.17878409174446813,
      "grad_norm": 0.3556745946407318,
      "learning_rate": 0.00028268188246385916,
      "loss": 1.0865,
      "step": 3040
    },
    {
      "epoch": 0.17937219730941703,
      "grad_norm": 0.33320197463035583,
      "learning_rate": 0.00028262295725958514,
      "loss": 1.0612,
      "step": 3050
    },
    {
      "epoch": 0.17996030287436596,
      "grad_norm": 0.38252758979797363,
      "learning_rate": 0.0002825640320553111,
      "loss": 1.0451,
      "step": 3060
    },
    {
      "epoch": 0.18054840843931486,
      "grad_norm": 0.37447428703308105,
      "learning_rate": 0.00028250510685103703,
      "loss": 0.9816,
      "step": 3070
    },
    {
      "epoch": 0.18113651400426375,
      "grad_norm": 0.3749387264251709,
      "learning_rate": 0.000282446181646763,
      "loss": 1.0808,
      "step": 3080
    },
    {
      "epoch": 0.18172461956921268,
      "grad_norm": 0.314657598733902,
      "learning_rate": 0.000282387256442489,
      "loss": 1.1113,
      "step": 3090
    },
    {
      "epoch": 0.18231272513416158,
      "grad_norm": 0.3888534903526306,
      "learning_rate": 0.00028232833123821496,
      "loss": 1.1214,
      "step": 3100
    },
    {
      "epoch": 0.18290083069911048,
      "grad_norm": 0.37910693883895874,
      "learning_rate": 0.0002822694060339409,
      "loss": 1.084,
      "step": 3110
    },
    {
      "epoch": 0.1834889362640594,
      "grad_norm": 0.3419681191444397,
      "learning_rate": 0.00028221048082966685,
      "loss": 1.1081,
      "step": 3120
    },
    {
      "epoch": 0.1840770418290083,
      "grad_norm": 0.36931395530700684,
      "learning_rate": 0.0002821515556253928,
      "loss": 1.0534,
      "step": 3130
    },
    {
      "epoch": 0.1846651473939572,
      "grad_norm": 0.3794580399990082,
      "learning_rate": 0.00028209263042111875,
      "loss": 1.1032,
      "step": 3140
    },
    {
      "epoch": 0.18525325295890613,
      "grad_norm": 0.32978180050849915,
      "learning_rate": 0.0002820337052168447,
      "loss": 1.0782,
      "step": 3150
    },
    {
      "epoch": 0.18584135852385503,
      "grad_norm": 0.31731361150741577,
      "learning_rate": 0.0002819747800125707,
      "loss": 1.0826,
      "step": 3160
    },
    {
      "epoch": 0.18642946408880395,
      "grad_norm": 0.3553670346736908,
      "learning_rate": 0.00028191585480829667,
      "loss": 1.0322,
      "step": 3170
    },
    {
      "epoch": 0.18701756965375285,
      "grad_norm": 0.37481555342674255,
      "learning_rate": 0.0002818569296040226,
      "loss": 1.1208,
      "step": 3180
    },
    {
      "epoch": 0.18760567521870175,
      "grad_norm": 0.38438835740089417,
      "learning_rate": 0.00028179800439974857,
      "loss": 1.1019,
      "step": 3190
    },
    {
      "epoch": 0.18819378078365068,
      "grad_norm": 0.4402426481246948,
      "learning_rate": 0.00028173907919547454,
      "loss": 1.1081,
      "step": 3200
    },
    {
      "epoch": 0.18878188634859958,
      "grad_norm": 0.35581353306770325,
      "learning_rate": 0.00028168015399120046,
      "loss": 1.0914,
      "step": 3210
    },
    {
      "epoch": 0.18936999191354847,
      "grad_norm": 0.3318317234516144,
      "learning_rate": 0.00028162122878692644,
      "loss": 1.1594,
      "step": 3220
    },
    {
      "epoch": 0.1899580974784974,
      "grad_norm": 0.3796766400337219,
      "learning_rate": 0.0002815623035826524,
      "loss": 1.0877,
      "step": 3230
    },
    {
      "epoch": 0.1905462030434463,
      "grad_norm": 0.31843724846839905,
      "learning_rate": 0.0002815033783783784,
      "loss": 1.1051,
      "step": 3240
    },
    {
      "epoch": 0.1911343086083952,
      "grad_norm": 0.44270333647727966,
      "learning_rate": 0.0002814444531741043,
      "loss": 1.0896,
      "step": 3250
    },
    {
      "epoch": 0.19172241417334412,
      "grad_norm": 0.3404698371887207,
      "learning_rate": 0.0002813855279698303,
      "loss": 1.1676,
      "step": 3260
    },
    {
      "epoch": 0.19231051973829302,
      "grad_norm": 0.34197255969047546,
      "learning_rate": 0.00028132660276555625,
      "loss": 1.1197,
      "step": 3270
    },
    {
      "epoch": 0.19289862530324192,
      "grad_norm": 0.38519740104675293,
      "learning_rate": 0.0002812676775612822,
      "loss": 1.1329,
      "step": 3280
    },
    {
      "epoch": 0.19348673086819085,
      "grad_norm": 0.34409648180007935,
      "learning_rate": 0.00028120875235700815,
      "loss": 1.0802,
      "step": 3290
    },
    {
      "epoch": 0.19407483643313975,
      "grad_norm": 0.3456099033355713,
      "learning_rate": 0.0002811498271527341,
      "loss": 1.1357,
      "step": 3300
    },
    {
      "epoch": 0.19466294199808865,
      "grad_norm": 0.34350186586380005,
      "learning_rate": 0.0002810909019484601,
      "loss": 1.018,
      "step": 3310
    },
    {
      "epoch": 0.19525104756303757,
      "grad_norm": 0.31979137659072876,
      "learning_rate": 0.000281031976744186,
      "loss": 1.0399,
      "step": 3320
    },
    {
      "epoch": 0.19583915312798647,
      "grad_norm": 0.3488866686820984,
      "learning_rate": 0.00028097305153991194,
      "loss": 1.0027,
      "step": 3330
    },
    {
      "epoch": 0.19642725869293537,
      "grad_norm": 0.3462633490562439,
      "learning_rate": 0.0002809141263356379,
      "loss": 1.1304,
      "step": 3340
    },
    {
      "epoch": 0.1970153642578843,
      "grad_norm": 0.324211984872818,
      "learning_rate": 0.0002808552011313639,
      "loss": 1.0413,
      "step": 3350
    },
    {
      "epoch": 0.1976034698228332,
      "grad_norm": 0.33488065004348755,
      "learning_rate": 0.00028079627592708987,
      "loss": 1.11,
      "step": 3360
    },
    {
      "epoch": 0.19819157538778212,
      "grad_norm": 0.45456787943840027,
      "learning_rate": 0.0002807373507228158,
      "loss": 0.975,
      "step": 3370
    },
    {
      "epoch": 0.19877968095273102,
      "grad_norm": 0.4898355305194855,
      "learning_rate": 0.00028067842551854176,
      "loss": 1.1327,
      "step": 3380
    },
    {
      "epoch": 0.19936778651767992,
      "grad_norm": 0.3092983365058899,
      "learning_rate": 0.00028061950031426774,
      "loss": 1.061,
      "step": 3390
    },
    {
      "epoch": 0.19995589208262884,
      "grad_norm": 0.362764447927475,
      "learning_rate": 0.00028056057510999366,
      "loss": 1.043,
      "step": 3400
    },
    {
      "epoch": 0.20054399764757774,
      "grad_norm": 0.3585885465145111,
      "learning_rate": 0.00028050164990571963,
      "loss": 1.0705,
      "step": 3410
    },
    {
      "epoch": 0.20113210321252664,
      "grad_norm": 0.34486299753189087,
      "learning_rate": 0.0002804427247014456,
      "loss": 1.0354,
      "step": 3420
    },
    {
      "epoch": 0.20172020877747557,
      "grad_norm": 0.3106212019920349,
      "learning_rate": 0.0002803837994971716,
      "loss": 1.0737,
      "step": 3430
    },
    {
      "epoch": 0.20230831434242447,
      "grad_norm": 0.34058213233947754,
      "learning_rate": 0.0002803248742928975,
      "loss": 1.1031,
      "step": 3440
    },
    {
      "epoch": 0.20289641990737337,
      "grad_norm": 0.3920165002346039,
      "learning_rate": 0.0002802659490886235,
      "loss": 0.9666,
      "step": 3450
    },
    {
      "epoch": 0.2034845254723223,
      "grad_norm": 0.3460952341556549,
      "learning_rate": 0.00028020702388434945,
      "loss": 1.0788,
      "step": 3460
    },
    {
      "epoch": 0.2040726310372712,
      "grad_norm": 0.3511543273925781,
      "learning_rate": 0.00028014809868007537,
      "loss": 1.2037,
      "step": 3470
    },
    {
      "epoch": 0.2046607366022201,
      "grad_norm": 0.36378541588783264,
      "learning_rate": 0.00028008917347580135,
      "loss": 1.0414,
      "step": 3480
    },
    {
      "epoch": 0.20524884216716902,
      "grad_norm": 0.34807395935058594,
      "learning_rate": 0.0002800302482715273,
      "loss": 1.1601,
      "step": 3490
    },
    {
      "epoch": 0.20583694773211791,
      "grad_norm": 0.3902975022792816,
      "learning_rate": 0.0002799713230672533,
      "loss": 1.0617,
      "step": 3500
    },
    {
      "epoch": 0.2064250532970668,
      "grad_norm": 0.3802992105484009,
      "learning_rate": 0.0002799123978629792,
      "loss": 1.1346,
      "step": 3510
    },
    {
      "epoch": 0.20701315886201574,
      "grad_norm": 0.40992334485054016,
      "learning_rate": 0.0002798534726587052,
      "loss": 1.0462,
      "step": 3520
    },
    {
      "epoch": 0.20760126442696464,
      "grad_norm": 0.35537534952163696,
      "learning_rate": 0.00027979454745443116,
      "loss": 1.062,
      "step": 3530
    },
    {
      "epoch": 0.20818936999191354,
      "grad_norm": 0.3860933780670166,
      "learning_rate": 0.0002797356222501571,
      "loss": 1.0698,
      "step": 3540
    },
    {
      "epoch": 0.20877747555686246,
      "grad_norm": 0.41460105776786804,
      "learning_rate": 0.00027967669704588306,
      "loss": 1.0806,
      "step": 3550
    },
    {
      "epoch": 0.20936558112181136,
      "grad_norm": 0.36657625436782837,
      "learning_rate": 0.00027961777184160903,
      "loss": 1.1224,
      "step": 3560
    },
    {
      "epoch": 0.20995368668676026,
      "grad_norm": 0.30363956093788147,
      "learning_rate": 0.000279558846637335,
      "loss": 1.0749,
      "step": 3570
    },
    {
      "epoch": 0.2105417922517092,
      "grad_norm": 0.31056132912635803,
      "learning_rate": 0.00027949992143306093,
      "loss": 1.0834,
      "step": 3580
    },
    {
      "epoch": 0.21112989781665809,
      "grad_norm": 0.3490445017814636,
      "learning_rate": 0.0002794409962287869,
      "loss": 1.1659,
      "step": 3590
    },
    {
      "epoch": 0.211718003381607,
      "grad_norm": 0.40983837842941284,
      "learning_rate": 0.0002793820710245129,
      "loss": 1.0785,
      "step": 3600
    },
    {
      "epoch": 0.2123061089465559,
      "grad_norm": 0.3999720513820648,
      "learning_rate": 0.0002793231458202388,
      "loss": 1.0125,
      "step": 3610
    },
    {
      "epoch": 0.2128942145115048,
      "grad_norm": 0.3558674156665802,
      "learning_rate": 0.0002792642206159648,
      "loss": 1.085,
      "step": 3620
    },
    {
      "epoch": 0.21348232007645374,
      "grad_norm": 0.3250555694103241,
      "learning_rate": 0.00027920529541169075,
      "loss": 1.0261,
      "step": 3630
    },
    {
      "epoch": 0.21407042564140263,
      "grad_norm": 0.39124807715415955,
      "learning_rate": 0.0002791463702074167,
      "loss": 1.0772,
      "step": 3640
    },
    {
      "epoch": 0.21465853120635153,
      "grad_norm": 0.380469411611557,
      "learning_rate": 0.00027908744500314264,
      "loss": 0.9645,
      "step": 3650
    },
    {
      "epoch": 0.21524663677130046,
      "grad_norm": 0.4100155830383301,
      "learning_rate": 0.0002790285197988686,
      "loss": 1.1087,
      "step": 3660
    },
    {
      "epoch": 0.21583474233624936,
      "grad_norm": 0.3266269266605377,
      "learning_rate": 0.0002789695945945946,
      "loss": 1.1149,
      "step": 3670
    },
    {
      "epoch": 0.21642284790119826,
      "grad_norm": 0.32149699330329895,
      "learning_rate": 0.0002789106693903205,
      "loss": 1.1565,
      "step": 3680
    },
    {
      "epoch": 0.21701095346614718,
      "grad_norm": 0.35594967007637024,
      "learning_rate": 0.0002788517441860465,
      "loss": 1.105,
      "step": 3690
    },
    {
      "epoch": 0.21759905903109608,
      "grad_norm": 0.3504832983016968,
      "learning_rate": 0.00027879281898177246,
      "loss": 1.0861,
      "step": 3700
    },
    {
      "epoch": 0.21818716459604498,
      "grad_norm": 0.3478836417198181,
      "learning_rate": 0.00027873389377749844,
      "loss": 1.0291,
      "step": 3710
    },
    {
      "epoch": 0.2187752701609939,
      "grad_norm": 0.39852818846702576,
      "learning_rate": 0.00027867496857322436,
      "loss": 1.0021,
      "step": 3720
    },
    {
      "epoch": 0.2193633757259428,
      "grad_norm": 0.3641613721847534,
      "learning_rate": 0.00027861604336895033,
      "loss": 1.066,
      "step": 3730
    },
    {
      "epoch": 0.2199514812908917,
      "grad_norm": 0.3572084307670593,
      "learning_rate": 0.0002785571181646763,
      "loss": 1.0854,
      "step": 3740
    },
    {
      "epoch": 0.22053958685584063,
      "grad_norm": 0.3599865436553955,
      "learning_rate": 0.00027849819296040223,
      "loss": 0.945,
      "step": 3750
    },
    {
      "epoch": 0.22112769242078953,
      "grad_norm": 0.3505467176437378,
      "learning_rate": 0.0002784392677561282,
      "loss": 1.0242,
      "step": 3760
    },
    {
      "epoch": 0.22171579798573843,
      "grad_norm": 0.34660884737968445,
      "learning_rate": 0.0002783803425518541,
      "loss": 1.0399,
      "step": 3770
    },
    {
      "epoch": 0.22230390355068735,
      "grad_norm": 0.3837994635105133,
      "learning_rate": 0.0002783214173475801,
      "loss": 1.0997,
      "step": 3780
    },
    {
      "epoch": 0.22289200911563625,
      "grad_norm": 0.35161688923835754,
      "learning_rate": 0.0002782624921433061,
      "loss": 0.9526,
      "step": 3790
    },
    {
      "epoch": 0.22348011468058515,
      "grad_norm": 0.4637252986431122,
      "learning_rate": 0.000278203566939032,
      "loss": 1.2029,
      "step": 3800
    },
    {
      "epoch": 0.22406822024553408,
      "grad_norm": 0.3376275897026062,
      "learning_rate": 0.00027814464173475797,
      "loss": 1.0873,
      "step": 3810
    },
    {
      "epoch": 0.22465632581048298,
      "grad_norm": 0.4189087152481079,
      "learning_rate": 0.00027808571653048394,
      "loss": 1.1729,
      "step": 3820
    },
    {
      "epoch": 0.2252444313754319,
      "grad_norm": 0.34726569056510925,
      "learning_rate": 0.0002780267913262099,
      "loss": 0.9654,
      "step": 3830
    },
    {
      "epoch": 0.2258325369403808,
      "grad_norm": 0.39916253089904785,
      "learning_rate": 0.00027796786612193584,
      "loss": 0.9837,
      "step": 3840
    },
    {
      "epoch": 0.2264206425053297,
      "grad_norm": 0.34912824630737305,
      "learning_rate": 0.0002779089409176618,
      "loss": 1.0662,
      "step": 3850
    },
    {
      "epoch": 0.22700874807027863,
      "grad_norm": 0.3964875042438507,
      "learning_rate": 0.0002778500157133878,
      "loss": 1.0294,
      "step": 3860
    },
    {
      "epoch": 0.22759685363522753,
      "grad_norm": 0.41518932580947876,
      "learning_rate": 0.0002777910905091137,
      "loss": 1.0673,
      "step": 3870
    },
    {
      "epoch": 0.22818495920017642,
      "grad_norm": 0.34171584248542786,
      "learning_rate": 0.0002777321653048397,
      "loss": 1.0895,
      "step": 3880
    },
    {
      "epoch": 0.22877306476512535,
      "grad_norm": 0.3960545063018799,
      "learning_rate": 0.00027767324010056566,
      "loss": 1.0729,
      "step": 3890
    },
    {
      "epoch": 0.22936117033007425,
      "grad_norm": 0.3723655641078949,
      "learning_rate": 0.00027761431489629163,
      "loss": 1.0776,
      "step": 3900
    },
    {
      "epoch": 0.22994927589502315,
      "grad_norm": 0.2976587116718292,
      "learning_rate": 0.00027755538969201755,
      "loss": 1.0613,
      "step": 3910
    },
    {
      "epoch": 0.23053738145997207,
      "grad_norm": 0.3622182309627533,
      "learning_rate": 0.00027749646448774353,
      "loss": 1.0204,
      "step": 3920
    },
    {
      "epoch": 0.23112548702492097,
      "grad_norm": 0.35373643040657043,
      "learning_rate": 0.0002774375392834695,
      "loss": 0.9928,
      "step": 3930
    },
    {
      "epoch": 0.23171359258986987,
      "grad_norm": 0.344637930393219,
      "learning_rate": 0.0002773786140791954,
      "loss": 1.0325,
      "step": 3940
    },
    {
      "epoch": 0.2323016981548188,
      "grad_norm": 0.4012328088283539,
      "learning_rate": 0.0002773196888749214,
      "loss": 1.0747,
      "step": 3950
    },
    {
      "epoch": 0.2328898037197677,
      "grad_norm": 0.37453922629356384,
      "learning_rate": 0.0002772607636706474,
      "loss": 1.056,
      "step": 3960
    },
    {
      "epoch": 0.2334779092847166,
      "grad_norm": 0.3232728838920593,
      "learning_rate": 0.00027720183846637335,
      "loss": 1.093,
      "step": 3970
    },
    {
      "epoch": 0.23406601484966552,
      "grad_norm": 0.3599105477333069,
      "learning_rate": 0.00027714291326209927,
      "loss": 1.0627,
      "step": 3980
    },
    {
      "epoch": 0.23465412041461442,
      "grad_norm": 0.4009389281272888,
      "learning_rate": 0.00027708398805782524,
      "loss": 1.1056,
      "step": 3990
    },
    {
      "epoch": 0.23524222597956332,
      "grad_norm": 0.3552418649196625,
      "learning_rate": 0.0002770250628535512,
      "loss": 1.0476,
      "step": 4000
    },
    {
      "epoch": 0.23583033154451225,
      "grad_norm": 0.35968437790870667,
      "learning_rate": 0.0002769720301697046,
      "loss": 1.0087,
      "step": 4010
    },
    {
      "epoch": 0.23641843710946114,
      "grad_norm": 0.4254089593887329,
      "learning_rate": 0.0002769131049654305,
      "loss": 0.9918,
      "step": 4020
    },
    {
      "epoch": 0.23700654267441007,
      "grad_norm": 0.36238208413124084,
      "learning_rate": 0.00027685417976115647,
      "loss": 1.0604,
      "step": 4030
    },
    {
      "epoch": 0.23759464823935897,
      "grad_norm": 0.3535977900028229,
      "learning_rate": 0.00027679525455688244,
      "loss": 1.0573,
      "step": 4040
    },
    {
      "epoch": 0.23818275380430787,
      "grad_norm": 0.37878093123435974,
      "learning_rate": 0.0002767363293526084,
      "loss": 1.0907,
      "step": 4050
    },
    {
      "epoch": 0.2387708593692568,
      "grad_norm": 0.34966567158699036,
      "learning_rate": 0.00027667740414833434,
      "loss": 1.135,
      "step": 4060
    },
    {
      "epoch": 0.2393589649342057,
      "grad_norm": 0.3471282720565796,
      "learning_rate": 0.0002766184789440603,
      "loss": 1.0436,
      "step": 4070
    },
    {
      "epoch": 0.2399470704991546,
      "grad_norm": 0.3060363531112671,
      "learning_rate": 0.0002765595537397863,
      "loss": 1.0356,
      "step": 4080
    },
    {
      "epoch": 0.24053517606410352,
      "grad_norm": 0.32712721824645996,
      "learning_rate": 0.0002765006285355122,
      "loss": 1.0373,
      "step": 4090
    },
    {
      "epoch": 0.24112328162905242,
      "grad_norm": 0.37871578335762024,
      "learning_rate": 0.0002764417033312382,
      "loss": 1.0944,
      "step": 4100
    },
    {
      "epoch": 0.24171138719400131,
      "grad_norm": 0.3896784484386444,
      "learning_rate": 0.00027638277812696416,
      "loss": 1.099,
      "step": 4110
    },
    {
      "epoch": 0.24229949275895024,
      "grad_norm": 0.31884488463401794,
      "learning_rate": 0.00027632385292269013,
      "loss": 1.0005,
      "step": 4120
    },
    {
      "epoch": 0.24288759832389914,
      "grad_norm": 0.33985936641693115,
      "learning_rate": 0.00027626492771841605,
      "loss": 1.0237,
      "step": 4130
    },
    {
      "epoch": 0.24347570388884804,
      "grad_norm": 0.36620378494262695,
      "learning_rate": 0.00027620600251414203,
      "loss": 1.073,
      "step": 4140
    },
    {
      "epoch": 0.24406380945379696,
      "grad_norm": 0.3608217239379883,
      "learning_rate": 0.000276147077309868,
      "loss": 1.0772,
      "step": 4150
    },
    {
      "epoch": 0.24465191501874586,
      "grad_norm": 0.3663639724254608,
      "learning_rate": 0.0002760881521055939,
      "loss": 1.0206,
      "step": 4160
    },
    {
      "epoch": 0.24524002058369476,
      "grad_norm": 0.37609896063804626,
      "learning_rate": 0.0002760292269013199,
      "loss": 1.0531,
      "step": 4170
    },
    {
      "epoch": 0.2458281261486437,
      "grad_norm": 0.3183635175228119,
      "learning_rate": 0.0002759703016970459,
      "loss": 1.0935,
      "step": 4180
    },
    {
      "epoch": 0.2464162317135926,
      "grad_norm": 0.3328031003475189,
      "learning_rate": 0.00027591137649277185,
      "loss": 1.0271,
      "step": 4190
    },
    {
      "epoch": 0.24700433727854149,
      "grad_norm": 0.33522453904151917,
      "learning_rate": 0.00027585245128849777,
      "loss": 1.0903,
      "step": 4200
    },
    {
      "epoch": 0.2475924428434904,
      "grad_norm": 0.34149840474128723,
      "learning_rate": 0.00027579352608422374,
      "loss": 1.035,
      "step": 4210
    },
    {
      "epoch": 0.2481805484084393,
      "grad_norm": 0.3610455095767975,
      "learning_rate": 0.0002757346008799497,
      "loss": 1.1101,
      "step": 4220
    },
    {
      "epoch": 0.2487686539733882,
      "grad_norm": 0.3121917247772217,
      "learning_rate": 0.00027567567567567564,
      "loss": 1.1425,
      "step": 4230
    },
    {
      "epoch": 0.24935675953833714,
      "grad_norm": 0.32553404569625854,
      "learning_rate": 0.0002756167504714016,
      "loss": 1.058,
      "step": 4240
    },
    {
      "epoch": 0.24994486510328603,
      "grad_norm": 0.39874526858329773,
      "learning_rate": 0.00027555782526712754,
      "loss": 1.0342,
      "step": 4250
    },
    {
      "epoch": 0.25053297066823493,
      "grad_norm": 0.336509644985199,
      "learning_rate": 0.0002754989000628535,
      "loss": 1.0315,
      "step": 4260
    },
    {
      "epoch": 0.25112107623318386,
      "grad_norm": 0.32006213068962097,
      "learning_rate": 0.0002754399748585795,
      "loss": 1.0905,
      "step": 4270
    },
    {
      "epoch": 0.2517091817981328,
      "grad_norm": 0.34897613525390625,
      "learning_rate": 0.0002753810496543054,
      "loss": 1.1127,
      "step": 4280
    },
    {
      "epoch": 0.25229728736308166,
      "grad_norm": 0.32680460810661316,
      "learning_rate": 0.0002753221244500314,
      "loss": 1.0213,
      "step": 4290
    },
    {
      "epoch": 0.2528853929280306,
      "grad_norm": 0.4101792573928833,
      "learning_rate": 0.00027526319924575735,
      "loss": 1.0519,
      "step": 4300
    },
    {
      "epoch": 0.2534734984929795,
      "grad_norm": 0.3036856949329376,
      "learning_rate": 0.00027520427404148333,
      "loss": 1.149,
      "step": 4310
    },
    {
      "epoch": 0.2540616040579284,
      "grad_norm": 0.3140403628349304,
      "learning_rate": 0.00027514534883720925,
      "loss": 1.0193,
      "step": 4320
    },
    {
      "epoch": 0.2546497096228773,
      "grad_norm": 0.4411742091178894,
      "learning_rate": 0.0002750864236329352,
      "loss": 1.0911,
      "step": 4330
    },
    {
      "epoch": 0.25523781518782623,
      "grad_norm": 0.35693061351776123,
      "learning_rate": 0.0002750274984286612,
      "loss": 1.1593,
      "step": 4340
    },
    {
      "epoch": 0.2558259207527751,
      "grad_norm": 0.3119974434375763,
      "learning_rate": 0.0002749685732243871,
      "loss": 1.0269,
      "step": 4350
    },
    {
      "epoch": 0.25641402631772403,
      "grad_norm": 0.3536623418331146,
      "learning_rate": 0.0002749096480201131,
      "loss": 1.0782,
      "step": 4360
    },
    {
      "epoch": 0.25700213188267296,
      "grad_norm": 0.3606469929218292,
      "learning_rate": 0.00027485072281583907,
      "loss": 1.098,
      "step": 4370
    },
    {
      "epoch": 0.25759023744762183,
      "grad_norm": 0.3848024308681488,
      "learning_rate": 0.00027479179761156504,
      "loss": 1.0231,
      "step": 4380
    },
    {
      "epoch": 0.25817834301257075,
      "grad_norm": 0.3167300522327423,
      "learning_rate": 0.00027473287240729096,
      "loss": 1.0726,
      "step": 4390
    },
    {
      "epoch": 0.2587664485775197,
      "grad_norm": 0.39505189657211304,
      "learning_rate": 0.00027467394720301694,
      "loss": 1.0829,
      "step": 4400
    },
    {
      "epoch": 0.25935455414246855,
      "grad_norm": 0.3674067258834839,
      "learning_rate": 0.0002746150219987429,
      "loss": 1.0523,
      "step": 4410
    },
    {
      "epoch": 0.2599426597074175,
      "grad_norm": 0.32160744071006775,
      "learning_rate": 0.00027455609679446883,
      "loss": 1.0562,
      "step": 4420
    },
    {
      "epoch": 0.2605307652723664,
      "grad_norm": 0.3670947849750519,
      "learning_rate": 0.0002744971715901948,
      "loss": 1.1294,
      "step": 4430
    },
    {
      "epoch": 0.2611188708373153,
      "grad_norm": 0.31499776244163513,
      "learning_rate": 0.0002744382463859208,
      "loss": 1.0195,
      "step": 4440
    },
    {
      "epoch": 0.2617069764022642,
      "grad_norm": 0.31653547286987305,
      "learning_rate": 0.00027437932118164676,
      "loss": 1.0355,
      "step": 4450
    },
    {
      "epoch": 0.26229508196721313,
      "grad_norm": 0.3225306272506714,
      "learning_rate": 0.0002743203959773727,
      "loss": 1.0612,
      "step": 4460
    },
    {
      "epoch": 0.262883187532162,
      "grad_norm": 0.4000687301158905,
      "learning_rate": 0.00027426147077309865,
      "loss": 1.0914,
      "step": 4470
    },
    {
      "epoch": 0.2634712930971109,
      "grad_norm": 0.3650432527065277,
      "learning_rate": 0.00027420254556882463,
      "loss": 1.0146,
      "step": 4480
    },
    {
      "epoch": 0.26405939866205985,
      "grad_norm": 0.3410583734512329,
      "learning_rate": 0.00027414362036455055,
      "loss": 0.9997,
      "step": 4490
    },
    {
      "epoch": 0.2646475042270087,
      "grad_norm": 0.4302600026130676,
      "learning_rate": 0.0002740846951602765,
      "loss": 1.0312,
      "step": 4500
    },
    {
      "epoch": 0.26523560979195765,
      "grad_norm": 0.3281654119491577,
      "learning_rate": 0.0002740257699560025,
      "loss": 0.9491,
      "step": 4510
    },
    {
      "epoch": 0.2658237153569066,
      "grad_norm": 0.29879075288772583,
      "learning_rate": 0.0002739668447517285,
      "loss": 1.0308,
      "step": 4520
    },
    {
      "epoch": 0.26641182092185545,
      "grad_norm": 0.3832491338253021,
      "learning_rate": 0.0002739079195474544,
      "loss": 1.1474,
      "step": 4530
    },
    {
      "epoch": 0.2669999264868044,
      "grad_norm": 0.3671109676361084,
      "learning_rate": 0.00027384899434318037,
      "loss": 1.1414,
      "step": 4540
    },
    {
      "epoch": 0.2675880320517533,
      "grad_norm": 0.3321380615234375,
      "learning_rate": 0.00027379006913890634,
      "loss": 1.0566,
      "step": 4550
    },
    {
      "epoch": 0.26817613761670217,
      "grad_norm": 0.39011913537979126,
      "learning_rate": 0.00027373114393463226,
      "loss": 1.0465,
      "step": 4560
    },
    {
      "epoch": 0.2687642431816511,
      "grad_norm": 0.33617934584617615,
      "learning_rate": 0.00027367221873035824,
      "loss": 1.0989,
      "step": 4570
    },
    {
      "epoch": 0.2693523487466,
      "grad_norm": 0.36883848905563354,
      "learning_rate": 0.0002736132935260842,
      "loss": 1.0199,
      "step": 4580
    },
    {
      "epoch": 0.26994045431154895,
      "grad_norm": 0.3444705009460449,
      "learning_rate": 0.0002735543683218102,
      "loss": 1.0972,
      "step": 4590
    },
    {
      "epoch": 0.2705285598764978,
      "grad_norm": 0.3609585464000702,
      "learning_rate": 0.0002734954431175361,
      "loss": 1.02,
      "step": 4600
    },
    {
      "epoch": 0.27111666544144675,
      "grad_norm": 0.39164042472839355,
      "learning_rate": 0.0002734365179132621,
      "loss": 0.9903,
      "step": 4610
    },
    {
      "epoch": 0.2717047710063957,
      "grad_norm": 0.3805946111679077,
      "learning_rate": 0.00027337759270898806,
      "loss": 1.0869,
      "step": 4620
    },
    {
      "epoch": 0.27229287657134454,
      "grad_norm": 0.3331334590911865,
      "learning_rate": 0.000273318667504714,
      "loss": 1.0579,
      "step": 4630
    },
    {
      "epoch": 0.27288098213629347,
      "grad_norm": 0.3255905508995056,
      "learning_rate": 0.00027325974230043995,
      "loss": 0.978,
      "step": 4640
    },
    {
      "epoch": 0.2734690877012424,
      "grad_norm": 0.3771885633468628,
      "learning_rate": 0.00027320081709616593,
      "loss": 1.0822,
      "step": 4650
    },
    {
      "epoch": 0.27405719326619127,
      "grad_norm": 0.3178049325942993,
      "learning_rate": 0.0002731418918918919,
      "loss": 1.0221,
      "step": 4660
    },
    {
      "epoch": 0.2746452988311402,
      "grad_norm": 0.34553325176239014,
      "learning_rate": 0.0002730829666876178,
      "loss": 1.0919,
      "step": 4670
    },
    {
      "epoch": 0.2752334043960891,
      "grad_norm": 0.30323290824890137,
      "learning_rate": 0.00027302404148334374,
      "loss": 1.1014,
      "step": 4680
    },
    {
      "epoch": 0.275821509961038,
      "grad_norm": 0.31681573390960693,
      "learning_rate": 0.0002729651162790697,
      "loss": 1.0217,
      "step": 4690
    },
    {
      "epoch": 0.2764096155259869,
      "grad_norm": 0.33138808608055115,
      "learning_rate": 0.0002729061910747957,
      "loss": 1.0953,
      "step": 4700
    },
    {
      "epoch": 0.27699772109093584,
      "grad_norm": 0.38350093364715576,
      "learning_rate": 0.00027284726587052167,
      "loss": 1.0288,
      "step": 4710
    },
    {
      "epoch": 0.2775858266558847,
      "grad_norm": 0.3390055298805237,
      "learning_rate": 0.0002727883406662476,
      "loss": 1.038,
      "step": 4720
    },
    {
      "epoch": 0.27817393222083364,
      "grad_norm": 0.375936895608902,
      "learning_rate": 0.00027272941546197356,
      "loss": 1.0275,
      "step": 4730
    },
    {
      "epoch": 0.27876203778578257,
      "grad_norm": 0.35149699449539185,
      "learning_rate": 0.00027267049025769954,
      "loss": 1.0418,
      "step": 4740
    },
    {
      "epoch": 0.27935014335073144,
      "grad_norm": 0.37235212326049805,
      "learning_rate": 0.00027261156505342546,
      "loss": 1.0947,
      "step": 4750
    },
    {
      "epoch": 0.27993824891568037,
      "grad_norm": 0.2921561300754547,
      "learning_rate": 0.00027255263984915143,
      "loss": 1.0829,
      "step": 4760
    },
    {
      "epoch": 0.2805263544806293,
      "grad_norm": 0.37757155299186707,
      "learning_rate": 0.0002724937146448774,
      "loss": 1.1383,
      "step": 4770
    },
    {
      "epoch": 0.28111446004557816,
      "grad_norm": 0.34258463978767395,
      "learning_rate": 0.0002724347894406034,
      "loss": 1.074,
      "step": 4780
    },
    {
      "epoch": 0.2817025656105271,
      "grad_norm": 0.3925888240337372,
      "learning_rate": 0.0002723758642363293,
      "loss": 1.0648,
      "step": 4790
    },
    {
      "epoch": 0.282290671175476,
      "grad_norm": 0.3371596932411194,
      "learning_rate": 0.0002723169390320553,
      "loss": 1.1608,
      "step": 4800
    },
    {
      "epoch": 0.2828787767404249,
      "grad_norm": 0.35802021622657776,
      "learning_rate": 0.00027225801382778125,
      "loss": 1.05,
      "step": 4810
    },
    {
      "epoch": 0.2834668823053738,
      "grad_norm": 0.3296296298503876,
      "learning_rate": 0.0002721990886235072,
      "loss": 1.0147,
      "step": 4820
    },
    {
      "epoch": 0.28405498787032274,
      "grad_norm": 0.3123762011528015,
      "learning_rate": 0.00027214016341923315,
      "loss": 1.0514,
      "step": 4830
    },
    {
      "epoch": 0.2846430934352716,
      "grad_norm": 0.3314218521118164,
      "learning_rate": 0.0002720812382149591,
      "loss": 1.1825,
      "step": 4840
    },
    {
      "epoch": 0.28523119900022054,
      "grad_norm": 0.3485172986984253,
      "learning_rate": 0.0002720223130106851,
      "loss": 1.0658,
      "step": 4850
    },
    {
      "epoch": 0.28581930456516946,
      "grad_norm": 0.3215785026550293,
      "learning_rate": 0.000271963387806411,
      "loss": 1.1425,
      "step": 4860
    },
    {
      "epoch": 0.28640741013011833,
      "grad_norm": 0.35765695571899414,
      "learning_rate": 0.000271904462602137,
      "loss": 1.0526,
      "step": 4870
    },
    {
      "epoch": 0.28699551569506726,
      "grad_norm": 0.32364925742149353,
      "learning_rate": 0.00027184553739786297,
      "loss": 1.002,
      "step": 4880
    },
    {
      "epoch": 0.2875836212600162,
      "grad_norm": 0.33644047379493713,
      "learning_rate": 0.0002717866121935889,
      "loss": 1.0645,
      "step": 4890
    },
    {
      "epoch": 0.28817172682496506,
      "grad_norm": 0.3546588718891144,
      "learning_rate": 0.00027172768698931486,
      "loss": 0.9861,
      "step": 4900
    },
    {
      "epoch": 0.288759832389914,
      "grad_norm": 0.362063467502594,
      "learning_rate": 0.00027166876178504084,
      "loss": 1.0138,
      "step": 4910
    },
    {
      "epoch": 0.2893479379548629,
      "grad_norm": 0.3323010802268982,
      "learning_rate": 0.0002716098365807668,
      "loss": 1.1012,
      "step": 4920
    },
    {
      "epoch": 0.2899360435198118,
      "grad_norm": 0.3255186975002289,
      "learning_rate": 0.00027155091137649273,
      "loss": 1.0275,
      "step": 4930
    },
    {
      "epoch": 0.2905241490847607,
      "grad_norm": 0.3213694393634796,
      "learning_rate": 0.0002714919861722187,
      "loss": 1.0486,
      "step": 4940
    },
    {
      "epoch": 0.29111225464970963,
      "grad_norm": 0.32852277159690857,
      "learning_rate": 0.0002714330609679447,
      "loss": 1.1203,
      "step": 4950
    },
    {
      "epoch": 0.2917003602146585,
      "grad_norm": 0.37314265966415405,
      "learning_rate": 0.0002713741357636706,
      "loss": 1.0566,
      "step": 4960
    },
    {
      "epoch": 0.29228846577960743,
      "grad_norm": 0.35050836205482483,
      "learning_rate": 0.0002713152105593966,
      "loss": 1.1051,
      "step": 4970
    },
    {
      "epoch": 0.29287657134455636,
      "grad_norm": 0.3328794836997986,
      "learning_rate": 0.00027125628535512255,
      "loss": 1.0458,
      "step": 4980
    },
    {
      "epoch": 0.29346467690950523,
      "grad_norm": 0.38377806544303894,
      "learning_rate": 0.00027119736015084853,
      "loss": 1.0805,
      "step": 4990
    },
    {
      "epoch": 0.29405278247445416,
      "grad_norm": 0.41014423966407776,
      "learning_rate": 0.00027113843494657445,
      "loss": 1.1282,
      "step": 5000
    },
    {
      "epoch": 0.2946408880394031,
      "grad_norm": 0.35221734642982483,
      "learning_rate": 0.0002710795097423004,
      "loss": 1.1447,
      "step": 5010
    },
    {
      "epoch": 0.295228993604352,
      "grad_norm": 0.3900274634361267,
      "learning_rate": 0.0002710205845380264,
      "loss": 1.0083,
      "step": 5020
    },
    {
      "epoch": 0.2958170991693009,
      "grad_norm": 0.3722454011440277,
      "learning_rate": 0.0002709616593337523,
      "loss": 0.9999,
      "step": 5030
    },
    {
      "epoch": 0.2964052047342498,
      "grad_norm": 0.33228087425231934,
      "learning_rate": 0.0002709027341294783,
      "loss": 1.0683,
      "step": 5040
    },
    {
      "epoch": 0.29699331029919873,
      "grad_norm": 0.34436070919036865,
      "learning_rate": 0.00027084380892520427,
      "loss": 1.0528,
      "step": 5050
    },
    {
      "epoch": 0.2975814158641476,
      "grad_norm": 0.35832181572914124,
      "learning_rate": 0.00027078488372093024,
      "loss": 1.0347,
      "step": 5060
    },
    {
      "epoch": 0.29816952142909653,
      "grad_norm": 0.3008611500263214,
      "learning_rate": 0.00027072595851665616,
      "loss": 1.1026,
      "step": 5070
    },
    {
      "epoch": 0.29875762699404546,
      "grad_norm": 0.35711702704429626,
      "learning_rate": 0.00027066703331238214,
      "loss": 1.1014,
      "step": 5080
    },
    {
      "epoch": 0.2993457325589943,
      "grad_norm": 0.41569000482559204,
      "learning_rate": 0.00027060810810810806,
      "loss": 1.1168,
      "step": 5090
    },
    {
      "epoch": 0.29993383812394325,
      "grad_norm": 0.30087006092071533,
      "learning_rate": 0.00027054918290383403,
      "loss": 0.9986,
      "step": 5100
    },
    {
      "epoch": 0.3005219436888922,
      "grad_norm": 0.36034855246543884,
      "learning_rate": 0.00027049025769956,
      "loss": 1.17,
      "step": 5110
    },
    {
      "epoch": 0.30111004925384105,
      "grad_norm": 0.3817436993122101,
      "learning_rate": 0.00027043133249528593,
      "loss": 1.1422,
      "step": 5120
    },
    {
      "epoch": 0.30169815481879,
      "grad_norm": 0.3085518777370453,
      "learning_rate": 0.0002703724072910119,
      "loss": 1.0354,
      "step": 5130
    },
    {
      "epoch": 0.3022862603837389,
      "grad_norm": 0.3384864032268524,
      "learning_rate": 0.0002703134820867379,
      "loss": 1.1044,
      "step": 5140
    },
    {
      "epoch": 0.3028743659486878,
      "grad_norm": 0.3232147991657257,
      "learning_rate": 0.0002702545568824638,
      "loss": 1.1173,
      "step": 5150
    },
    {
      "epoch": 0.3034624715136367,
      "grad_norm": 0.31654277443885803,
      "learning_rate": 0.0002701956316781898,
      "loss": 1.0404,
      "step": 5160
    },
    {
      "epoch": 0.3040505770785856,
      "grad_norm": 0.3525785803794861,
      "learning_rate": 0.00027013670647391575,
      "loss": 1.0536,
      "step": 5170
    },
    {
      "epoch": 0.3046386826435345,
      "grad_norm": 0.3004358112812042,
      "learning_rate": 0.0002700777812696417,
      "loss": 1.0049,
      "step": 5180
    },
    {
      "epoch": 0.3052267882084834,
      "grad_norm": 0.3087332844734192,
      "learning_rate": 0.00027001885606536764,
      "loss": 1.1176,
      "step": 5190
    },
    {
      "epoch": 0.30581489377343235,
      "grad_norm": 0.32578137516975403,
      "learning_rate": 0.0002699599308610936,
      "loss": 1.0599,
      "step": 5200
    },
    {
      "epoch": 0.3064029993383812,
      "grad_norm": 0.3846634030342102,
      "learning_rate": 0.0002699010056568196,
      "loss": 0.9985,
      "step": 5210
    },
    {
      "epoch": 0.30699110490333015,
      "grad_norm": 0.3338776230812073,
      "learning_rate": 0.0002698420804525455,
      "loss": 1.0313,
      "step": 5220
    },
    {
      "epoch": 0.3075792104682791,
      "grad_norm": 0.328497976064682,
      "learning_rate": 0.0002697831552482715,
      "loss": 1.1126,
      "step": 5230
    },
    {
      "epoch": 0.30816731603322794,
      "grad_norm": 0.3765682280063629,
      "learning_rate": 0.00026972423004399746,
      "loss": 1.03,
      "step": 5240
    },
    {
      "epoch": 0.30875542159817687,
      "grad_norm": 0.32622039318084717,
      "learning_rate": 0.00026966530483972344,
      "loss": 1.0849,
      "step": 5250
    },
    {
      "epoch": 0.3093435271631258,
      "grad_norm": 0.3905145227909088,
      "learning_rate": 0.00026960637963544936,
      "loss": 0.9751,
      "step": 5260
    },
    {
      "epoch": 0.30993163272807467,
      "grad_norm": 0.31710344552993774,
      "learning_rate": 0.00026954745443117533,
      "loss": 0.9772,
      "step": 5270
    },
    {
      "epoch": 0.3105197382930236,
      "grad_norm": 0.3541526794433594,
      "learning_rate": 0.0002694885292269013,
      "loss": 0.954,
      "step": 5280
    },
    {
      "epoch": 0.3111078438579725,
      "grad_norm": 0.30555659532546997,
      "learning_rate": 0.00026942960402262723,
      "loss": 1.1505,
      "step": 5290
    },
    {
      "epoch": 0.3116959494229214,
      "grad_norm": 0.29946577548980713,
      "learning_rate": 0.0002693706788183532,
      "loss": 1.1147,
      "step": 5300
    },
    {
      "epoch": 0.3122840549878703,
      "grad_norm": 0.358296662569046,
      "learning_rate": 0.0002693117536140792,
      "loss": 1.1622,
      "step": 5310
    },
    {
      "epoch": 0.31287216055281925,
      "grad_norm": 0.3298180401325226,
      "learning_rate": 0.00026925282840980515,
      "loss": 1.0506,
      "step": 5320
    },
    {
      "epoch": 0.3134602661177681,
      "grad_norm": 0.3591659963130951,
      "learning_rate": 0.0002691939032055311,
      "loss": 1.1183,
      "step": 5330
    },
    {
      "epoch": 0.31404837168271704,
      "grad_norm": 0.3942638039588928,
      "learning_rate": 0.00026913497800125705,
      "loss": 1.0642,
      "step": 5340
    },
    {
      "epoch": 0.31463647724766597,
      "grad_norm": 0.32332444190979004,
      "learning_rate": 0.000269076052796983,
      "loss": 1.0773,
      "step": 5350
    },
    {
      "epoch": 0.31522458281261484,
      "grad_norm": 0.3958848714828491,
      "learning_rate": 0.00026901712759270894,
      "loss": 1.0475,
      "step": 5360
    },
    {
      "epoch": 0.31581268837756377,
      "grad_norm": 0.33055174350738525,
      "learning_rate": 0.0002689582023884349,
      "loss": 1.0209,
      "step": 5370
    },
    {
      "epoch": 0.3164007939425127,
      "grad_norm": 0.3652138113975525,
      "learning_rate": 0.0002688992771841609,
      "loss": 1.0716,
      "step": 5380
    },
    {
      "epoch": 0.31698889950746156,
      "grad_norm": 0.3288179934024811,
      "learning_rate": 0.00026884035197988687,
      "loss": 1.1468,
      "step": 5390
    },
    {
      "epoch": 0.3175770050724105,
      "grad_norm": 0.3106636106967926,
      "learning_rate": 0.0002687814267756128,
      "loss": 0.9483,
      "step": 5400
    },
    {
      "epoch": 0.3181651106373594,
      "grad_norm": 0.3695886731147766,
      "learning_rate": 0.00026872250157133876,
      "loss": 1.0484,
      "step": 5410
    },
    {
      "epoch": 0.3187532162023083,
      "grad_norm": 0.3043193817138672,
      "learning_rate": 0.00026866357636706474,
      "loss": 1.0614,
      "step": 5420
    },
    {
      "epoch": 0.3193413217672572,
      "grad_norm": 0.30987316370010376,
      "learning_rate": 0.00026860465116279066,
      "loss": 1.1026,
      "step": 5430
    },
    {
      "epoch": 0.31992942733220614,
      "grad_norm": 0.3339443504810333,
      "learning_rate": 0.00026854572595851663,
      "loss": 1.028,
      "step": 5440
    },
    {
      "epoch": 0.32051753289715507,
      "grad_norm": 0.3543473482131958,
      "learning_rate": 0.0002684868007542426,
      "loss": 0.9852,
      "step": 5450
    },
    {
      "epoch": 0.32110563846210394,
      "grad_norm": 0.35773777961730957,
      "learning_rate": 0.0002684278755499686,
      "loss": 1.0244,
      "step": 5460
    },
    {
      "epoch": 0.32169374402705286,
      "grad_norm": 0.3499491512775421,
      "learning_rate": 0.0002683689503456945,
      "loss": 1.129,
      "step": 5470
    },
    {
      "epoch": 0.3222818495920018,
      "grad_norm": 0.36125895380973816,
      "learning_rate": 0.0002683100251414205,
      "loss": 1.0901,
      "step": 5480
    },
    {
      "epoch": 0.32286995515695066,
      "grad_norm": 0.38178595900535583,
      "learning_rate": 0.00026825109993714645,
      "loss": 1.0389,
      "step": 5490
    },
    {
      "epoch": 0.3234580607218996,
      "grad_norm": 0.3247810900211334,
      "learning_rate": 0.0002681921747328724,
      "loss": 1.0355,
      "step": 5500
    },
    {
      "epoch": 0.3240461662868485,
      "grad_norm": 0.3145785927772522,
      "learning_rate": 0.00026813324952859835,
      "loss": 1.041,
      "step": 5510
    },
    {
      "epoch": 0.3246342718517974,
      "grad_norm": 0.36696723103523254,
      "learning_rate": 0.00026807432432432427,
      "loss": 0.9571,
      "step": 5520
    },
    {
      "epoch": 0.3252223774167463,
      "grad_norm": 0.32616284489631653,
      "learning_rate": 0.00026801539912005024,
      "loss": 1.1011,
      "step": 5530
    },
    {
      "epoch": 0.32581048298169524,
      "grad_norm": 0.3233625888824463,
      "learning_rate": 0.0002679564739157762,
      "loss": 1.0565,
      "step": 5540
    },
    {
      "epoch": 0.3263985885466441,
      "grad_norm": 0.39313697814941406,
      "learning_rate": 0.00026789754871150214,
      "loss": 1.0485,
      "step": 5550
    },
    {
      "epoch": 0.32698669411159303,
      "grad_norm": 0.32174864411354065,
      "learning_rate": 0.0002678386235072281,
      "loss": 1.1333,
      "step": 5560
    },
    {
      "epoch": 0.32757479967654196,
      "grad_norm": 0.35569077730178833,
      "learning_rate": 0.0002677796983029541,
      "loss": 1.0183,
      "step": 5570
    },
    {
      "epoch": 0.32816290524149083,
      "grad_norm": 0.32811349630355835,
      "learning_rate": 0.00026772077309868006,
      "loss": 1.1257,
      "step": 5580
    },
    {
      "epoch": 0.32875101080643976,
      "grad_norm": 0.33575505018234253,
      "learning_rate": 0.000267661847894406,
      "loss": 1.0982,
      "step": 5590
    },
    {
      "epoch": 0.3293391163713887,
      "grad_norm": 0.33867785334587097,
      "learning_rate": 0.00026760292269013196,
      "loss": 0.9604,
      "step": 5600
    },
    {
      "epoch": 0.32992722193633756,
      "grad_norm": 0.3381858766078949,
      "learning_rate": 0.00026754399748585793,
      "loss": 1.0551,
      "step": 5610
    },
    {
      "epoch": 0.3305153275012865,
      "grad_norm": 0.378312349319458,
      "learning_rate": 0.00026748507228158385,
      "loss": 1.1798,
      "step": 5620
    },
    {
      "epoch": 0.3311034330662354,
      "grad_norm": 0.3127136826515198,
      "learning_rate": 0.00026742614707730983,
      "loss": 1.0421,
      "step": 5630
    },
    {
      "epoch": 0.3316915386311843,
      "grad_norm": 0.3683975040912628,
      "learning_rate": 0.0002673672218730358,
      "loss": 1.0616,
      "step": 5640
    },
    {
      "epoch": 0.3322796441961332,
      "grad_norm": 0.34755611419677734,
      "learning_rate": 0.0002673082966687618,
      "loss": 1.0459,
      "step": 5650
    },
    {
      "epoch": 0.33286774976108213,
      "grad_norm": 0.32370710372924805,
      "learning_rate": 0.0002672493714644877,
      "loss": 0.964,
      "step": 5660
    },
    {
      "epoch": 0.333455855326031,
      "grad_norm": 0.30656012892723083,
      "learning_rate": 0.0002671904462602137,
      "loss": 1.0721,
      "step": 5670
    },
    {
      "epoch": 0.33404396089097993,
      "grad_norm": 0.3462313115596771,
      "learning_rate": 0.00026713152105593965,
      "loss": 1.083,
      "step": 5680
    },
    {
      "epoch": 0.33463206645592886,
      "grad_norm": 0.37853482365608215,
      "learning_rate": 0.00026707259585166557,
      "loss": 1.0411,
      "step": 5690
    },
    {
      "epoch": 0.3352201720208777,
      "grad_norm": 0.33870816230773926,
      "learning_rate": 0.00026701367064739154,
      "loss": 1.0224,
      "step": 5700
    },
    {
      "epoch": 0.33580827758582665,
      "grad_norm": 0.39802485704421997,
      "learning_rate": 0.0002669547454431175,
      "loss": 1.0253,
      "step": 5710
    },
    {
      "epoch": 0.3363963831507756,
      "grad_norm": 0.34257060289382935,
      "learning_rate": 0.0002668958202388435,
      "loss": 1.1371,
      "step": 5720
    },
    {
      "epoch": 0.33698448871572445,
      "grad_norm": 0.36409592628479004,
      "learning_rate": 0.0002668368950345694,
      "loss": 1.093,
      "step": 5730
    },
    {
      "epoch": 0.3375725942806734,
      "grad_norm": 0.32104939222335815,
      "learning_rate": 0.0002667779698302954,
      "loss": 1.0774,
      "step": 5740
    },
    {
      "epoch": 0.3381606998456223,
      "grad_norm": 0.3104189336299896,
      "learning_rate": 0.00026671904462602136,
      "loss": 1.1048,
      "step": 5750
    },
    {
      "epoch": 0.3387488054105712,
      "grad_norm": 0.36907312273979187,
      "learning_rate": 0.0002666601194217473,
      "loss": 0.9744,
      "step": 5760
    },
    {
      "epoch": 0.3393369109755201,
      "grad_norm": 0.39653727412223816,
      "learning_rate": 0.00026660119421747326,
      "loss": 1.0925,
      "step": 5770
    },
    {
      "epoch": 0.339925016540469,
      "grad_norm": 0.3276975154876709,
      "learning_rate": 0.00026654226901319923,
      "loss": 1.0045,
      "step": 5780
    },
    {
      "epoch": 0.3405131221054179,
      "grad_norm": 0.3412836790084839,
      "learning_rate": 0.0002664833438089252,
      "loss": 1.0923,
      "step": 5790
    },
    {
      "epoch": 0.3411012276703668,
      "grad_norm": 0.31431442499160767,
      "learning_rate": 0.00026642441860465113,
      "loss": 0.9548,
      "step": 5800
    },
    {
      "epoch": 0.34168933323531575,
      "grad_norm": 0.33156099915504456,
      "learning_rate": 0.0002663654934003771,
      "loss": 1.1533,
      "step": 5810
    },
    {
      "epoch": 0.3422774388002646,
      "grad_norm": 0.3577512502670288,
      "learning_rate": 0.0002663065681961031,
      "loss": 1.0225,
      "step": 5820
    },
    {
      "epoch": 0.34286554436521355,
      "grad_norm": 0.3407970368862152,
      "learning_rate": 0.000266247642991829,
      "loss": 1.049,
      "step": 5830
    },
    {
      "epoch": 0.3434536499301625,
      "grad_norm": 0.4390730559825897,
      "learning_rate": 0.00026618871778755497,
      "loss": 0.9562,
      "step": 5840
    },
    {
      "epoch": 0.34404175549511135,
      "grad_norm": 0.3239665925502777,
      "learning_rate": 0.00026612979258328095,
      "loss": 0.9662,
      "step": 5850
    },
    {
      "epoch": 0.34462986106006027,
      "grad_norm": 0.34853240847587585,
      "learning_rate": 0.0002660708673790069,
      "loss": 1.0704,
      "step": 5860
    },
    {
      "epoch": 0.3452179666250092,
      "grad_norm": 0.34433090686798096,
      "learning_rate": 0.00026601194217473284,
      "loss": 1.0253,
      "step": 5870
    },
    {
      "epoch": 0.3458060721899581,
      "grad_norm": 0.34410083293914795,
      "learning_rate": 0.0002659530169704588,
      "loss": 1.084,
      "step": 5880
    },
    {
      "epoch": 0.346394177754907,
      "grad_norm": 0.3776593804359436,
      "learning_rate": 0.0002658940917661848,
      "loss": 1.13,
      "step": 5890
    },
    {
      "epoch": 0.3469822833198559,
      "grad_norm": 0.3266339600086212,
      "learning_rate": 0.0002658351665619107,
      "loss": 1.0113,
      "step": 5900
    },
    {
      "epoch": 0.34757038888480485,
      "grad_norm": 0.3309793770313263,
      "learning_rate": 0.0002657762413576367,
      "loss": 1.049,
      "step": 5910
    },
    {
      "epoch": 0.3481584944497537,
      "grad_norm": 0.36193475127220154,
      "learning_rate": 0.00026571731615336266,
      "loss": 1.1017,
      "step": 5920
    },
    {
      "epoch": 0.34874660001470265,
      "grad_norm": 0.29084259271621704,
      "learning_rate": 0.0002656583909490886,
      "loss": 0.9715,
      "step": 5930
    },
    {
      "epoch": 0.34933470557965157,
      "grad_norm": 0.31213995814323425,
      "learning_rate": 0.00026559946574481456,
      "loss": 1.0343,
      "step": 5940
    },
    {
      "epoch": 0.34992281114460044,
      "grad_norm": 0.31731414794921875,
      "learning_rate": 0.0002655405405405405,
      "loss": 1.1149,
      "step": 5950
    },
    {
      "epoch": 0.35051091670954937,
      "grad_norm": 0.3423238694667816,
      "learning_rate": 0.00026548161533626645,
      "loss": 1.0722,
      "step": 5960
    },
    {
      "epoch": 0.3510990222744983,
      "grad_norm": 0.35781043767929077,
      "learning_rate": 0.00026542269013199243,
      "loss": 1.1106,
      "step": 5970
    },
    {
      "epoch": 0.35168712783944717,
      "grad_norm": 0.35036736726760864,
      "learning_rate": 0.0002653637649277184,
      "loss": 1.0472,
      "step": 5980
    },
    {
      "epoch": 0.3522752334043961,
      "grad_norm": 0.3512951731681824,
      "learning_rate": 0.0002653048397234443,
      "loss": 1.0903,
      "step": 5990
    },
    {
      "epoch": 0.352863338969345,
      "grad_norm": 0.35774970054626465,
      "learning_rate": 0.0002652459145191703,
      "loss": 1.0945,
      "step": 6000
    },
    {
      "epoch": 0.3534514445342939,
      "grad_norm": 0.3351399302482605,
      "learning_rate": 0.00026519288183532365,
      "loss": 1.0989,
      "step": 6010
    },
    {
      "epoch": 0.3540395500992428,
      "grad_norm": 0.3599112629890442,
      "learning_rate": 0.00026513395663104963,
      "loss": 1.0327,
      "step": 6020
    },
    {
      "epoch": 0.35462765566419174,
      "grad_norm": 0.30304357409477234,
      "learning_rate": 0.00026507503142677555,
      "loss": 1.0116,
      "step": 6030
    },
    {
      "epoch": 0.3552157612291406,
      "grad_norm": 0.32058125734329224,
      "learning_rate": 0.0002650161062225015,
      "loss": 1.0961,
      "step": 6040
    },
    {
      "epoch": 0.35580386679408954,
      "grad_norm": 0.39785948395729065,
      "learning_rate": 0.0002649571810182275,
      "loss": 1.0578,
      "step": 6050
    },
    {
      "epoch": 0.35639197235903847,
      "grad_norm": 0.32175907492637634,
      "learning_rate": 0.00026489825581395347,
      "loss": 1.1525,
      "step": 6060
    },
    {
      "epoch": 0.35698007792398734,
      "grad_norm": 0.3464099168777466,
      "learning_rate": 0.0002648393306096794,
      "loss": 1.1709,
      "step": 6070
    },
    {
      "epoch": 0.35756818348893626,
      "grad_norm": 0.39362993836402893,
      "learning_rate": 0.00026478040540540537,
      "loss": 1.054,
      "step": 6080
    },
    {
      "epoch": 0.3581562890538852,
      "grad_norm": 0.3822583556175232,
      "learning_rate": 0.00026472148020113134,
      "loss": 1.0623,
      "step": 6090
    },
    {
      "epoch": 0.35874439461883406,
      "grad_norm": 0.3583880364894867,
      "learning_rate": 0.00026466255499685726,
      "loss": 1.0047,
      "step": 6100
    },
    {
      "epoch": 0.359332500183783,
      "grad_norm": 0.3176627457141876,
      "learning_rate": 0.00026460362979258324,
      "loss": 1.0736,
      "step": 6110
    },
    {
      "epoch": 0.3599206057487319,
      "grad_norm": 0.29895514249801636,
      "learning_rate": 0.0002645447045883092,
      "loss": 1.0112,
      "step": 6120
    },
    {
      "epoch": 0.3605087113136808,
      "grad_norm": 0.3633062541484833,
      "learning_rate": 0.0002644857793840352,
      "loss": 0.9549,
      "step": 6130
    },
    {
      "epoch": 0.3610968168786297,
      "grad_norm": 0.3605278730392456,
      "learning_rate": 0.0002644268541797611,
      "loss": 1.0314,
      "step": 6140
    },
    {
      "epoch": 0.36168492244357864,
      "grad_norm": 0.3174442946910858,
      "learning_rate": 0.0002643679289754871,
      "loss": 1.0198,
      "step": 6150
    },
    {
      "epoch": 0.3622730280085275,
      "grad_norm": 0.36002641916275024,
      "learning_rate": 0.00026430900377121306,
      "loss": 1.022,
      "step": 6160
    },
    {
      "epoch": 0.36286113357347644,
      "grad_norm": 0.3352866768836975,
      "learning_rate": 0.000264250078566939,
      "loss": 0.9895,
      "step": 6170
    },
    {
      "epoch": 0.36344923913842536,
      "grad_norm": 0.31035763025283813,
      "learning_rate": 0.00026419115336266495,
      "loss": 1.0745,
      "step": 6180
    },
    {
      "epoch": 0.36403734470337423,
      "grad_norm": 0.3355978727340698,
      "learning_rate": 0.00026413222815839093,
      "loss": 1.0236,
      "step": 6190
    },
    {
      "epoch": 0.36462545026832316,
      "grad_norm": 0.37670770287513733,
      "learning_rate": 0.0002640733029541169,
      "loss": 1.0452,
      "step": 6200
    },
    {
      "epoch": 0.3652135558332721,
      "grad_norm": 0.39939847588539124,
      "learning_rate": 0.0002640143777498428,
      "loss": 1.1961,
      "step": 6210
    },
    {
      "epoch": 0.36580166139822096,
      "grad_norm": 0.3068031370639801,
      "learning_rate": 0.0002639554525455688,
      "loss": 1.0303,
      "step": 6220
    },
    {
      "epoch": 0.3663897669631699,
      "grad_norm": 0.35729852318763733,
      "learning_rate": 0.00026389652734129477,
      "loss": 1.177,
      "step": 6230
    },
    {
      "epoch": 0.3669778725281188,
      "grad_norm": 0.34759777784347534,
      "learning_rate": 0.00026383760213702075,
      "loss": 1.0493,
      "step": 6240
    },
    {
      "epoch": 0.3675659780930677,
      "grad_norm": 0.362613320350647,
      "learning_rate": 0.00026377867693274667,
      "loss": 1.0269,
      "step": 6250
    },
    {
      "epoch": 0.3681540836580166,
      "grad_norm": 0.35207706689834595,
      "learning_rate": 0.00026371975172847264,
      "loss": 1.0172,
      "step": 6260
    },
    {
      "epoch": 0.36874218922296553,
      "grad_norm": 0.3417227268218994,
      "learning_rate": 0.0002636608265241986,
      "loss": 1.041,
      "step": 6270
    },
    {
      "epoch": 0.3693302947879144,
      "grad_norm": 0.34523651003837585,
      "learning_rate": 0.00026360190131992454,
      "loss": 1.1755,
      "step": 6280
    },
    {
      "epoch": 0.36991840035286333,
      "grad_norm": 0.4282139539718628,
      "learning_rate": 0.0002635429761156505,
      "loss": 1.1014,
      "step": 6290
    },
    {
      "epoch": 0.37050650591781226,
      "grad_norm": 0.36233285069465637,
      "learning_rate": 0.0002634840509113765,
      "loss": 1.0455,
      "step": 6300
    },
    {
      "epoch": 0.3710946114827612,
      "grad_norm": 0.31390535831451416,
      "learning_rate": 0.00026342512570710246,
      "loss": 0.9999,
      "step": 6310
    },
    {
      "epoch": 0.37168271704771005,
      "grad_norm": 0.3246980905532837,
      "learning_rate": 0.0002633662005028284,
      "loss": 1.0039,
      "step": 6320
    },
    {
      "epoch": 0.372270822612659,
      "grad_norm": 0.3680986166000366,
      "learning_rate": 0.00026330727529855436,
      "loss": 1.14,
      "step": 6330
    },
    {
      "epoch": 0.3728589281776079,
      "grad_norm": 0.3992152512073517,
      "learning_rate": 0.00026324835009428033,
      "loss": 1.0923,
      "step": 6340
    },
    {
      "epoch": 0.3734470337425568,
      "grad_norm": 0.3244057893753052,
      "learning_rate": 0.00026318942489000625,
      "loss": 1.0449,
      "step": 6350
    },
    {
      "epoch": 0.3740351393075057,
      "grad_norm": 0.391814261674881,
      "learning_rate": 0.00026313049968573223,
      "loss": 0.9562,
      "step": 6360
    },
    {
      "epoch": 0.37462324487245463,
      "grad_norm": 0.3021530508995056,
      "learning_rate": 0.0002630715744814582,
      "loss": 1.0476,
      "step": 6370
    },
    {
      "epoch": 0.3752113504374035,
      "grad_norm": 0.3259018063545227,
      "learning_rate": 0.0002630126492771842,
      "loss": 1.0155,
      "step": 6380
    },
    {
      "epoch": 0.3757994560023524,
      "grad_norm": 0.3342035412788391,
      "learning_rate": 0.0002629537240729101,
      "loss": 1.1105,
      "step": 6390
    },
    {
      "epoch": 0.37638756156730135,
      "grad_norm": 0.3529188632965088,
      "learning_rate": 0.00026289479886863607,
      "loss": 1.0182,
      "step": 6400
    },
    {
      "epoch": 0.3769756671322502,
      "grad_norm": 0.35972732305526733,
      "learning_rate": 0.00026283587366436205,
      "loss": 1.0223,
      "step": 6410
    },
    {
      "epoch": 0.37756377269719915,
      "grad_norm": 0.34397092461586,
      "learning_rate": 0.00026277694846008797,
      "loss": 0.9733,
      "step": 6420
    },
    {
      "epoch": 0.3781518782621481,
      "grad_norm": 0.4040443003177643,
      "learning_rate": 0.0002627180232558139,
      "loss": 1.0367,
      "step": 6430
    },
    {
      "epoch": 0.37873998382709695,
      "grad_norm": 0.33527871966362,
      "learning_rate": 0.00026265909805153986,
      "loss": 1.0155,
      "step": 6440
    },
    {
      "epoch": 0.3793280893920459,
      "grad_norm": 0.27521467208862305,
      "learning_rate": 0.00026260017284726584,
      "loss": 1.0604,
      "step": 6450
    },
    {
      "epoch": 0.3799161949569948,
      "grad_norm": 0.3579181432723999,
      "learning_rate": 0.0002625412476429918,
      "loss": 1.2225,
      "step": 6460
    },
    {
      "epoch": 0.3805043005219437,
      "grad_norm": 0.32657718658447266,
      "learning_rate": 0.00026248232243871773,
      "loss": 1.0851,
      "step": 6470
    },
    {
      "epoch": 0.3810924060868926,
      "grad_norm": 0.36870914697647095,
      "learning_rate": 0.0002624233972344437,
      "loss": 1.0998,
      "step": 6480
    },
    {
      "epoch": 0.3816805116518415,
      "grad_norm": 0.33802324533462524,
      "learning_rate": 0.0002623644720301697,
      "loss": 1.0713,
      "step": 6490
    },
    {
      "epoch": 0.3822686172167904,
      "grad_norm": 0.34023115038871765,
      "learning_rate": 0.0002623055468258956,
      "loss": 1.0283,
      "step": 6500
    },
    {
      "epoch": 0.3828567227817393,
      "grad_norm": 0.32630735635757446,
      "learning_rate": 0.0002622466216216216,
      "loss": 1.1494,
      "step": 6510
    },
    {
      "epoch": 0.38344482834668825,
      "grad_norm": 0.3427028954029083,
      "learning_rate": 0.00026218769641734755,
      "loss": 1.1561,
      "step": 6520
    },
    {
      "epoch": 0.3840329339116371,
      "grad_norm": 0.33476370573043823,
      "learning_rate": 0.00026212877121307353,
      "loss": 1.1766,
      "step": 6530
    },
    {
      "epoch": 0.38462103947658605,
      "grad_norm": 0.4063611328601837,
      "learning_rate": 0.00026206984600879945,
      "loss": 1.0985,
      "step": 6540
    },
    {
      "epoch": 0.385209145041535,
      "grad_norm": 0.3475991189479828,
      "learning_rate": 0.0002620109208045254,
      "loss": 1.0793,
      "step": 6550
    },
    {
      "epoch": 0.38579725060648384,
      "grad_norm": 0.31628528237342834,
      "learning_rate": 0.0002619519956002514,
      "loss": 1.0916,
      "step": 6560
    },
    {
      "epoch": 0.38638535617143277,
      "grad_norm": 0.40580663084983826,
      "learning_rate": 0.0002618930703959773,
      "loss": 1.0741,
      "step": 6570
    },
    {
      "epoch": 0.3869734617363817,
      "grad_norm": 0.33273938298225403,
      "learning_rate": 0.0002618341451917033,
      "loss": 1.0647,
      "step": 6580
    },
    {
      "epoch": 0.38756156730133057,
      "grad_norm": 0.39248669147491455,
      "learning_rate": 0.00026177521998742927,
      "loss": 0.9828,
      "step": 6590
    },
    {
      "epoch": 0.3881496728662795,
      "grad_norm": 0.38490715622901917,
      "learning_rate": 0.00026171629478315524,
      "loss": 1.0178,
      "step": 6600
    },
    {
      "epoch": 0.3887377784312284,
      "grad_norm": 0.3659142553806305,
      "learning_rate": 0.00026165736957888116,
      "loss": 1.0226,
      "step": 6610
    },
    {
      "epoch": 0.3893258839961773,
      "grad_norm": 0.32115018367767334,
      "learning_rate": 0.00026159844437460714,
      "loss": 1.052,
      "step": 6620
    },
    {
      "epoch": 0.3899139895611262,
      "grad_norm": 0.2902972996234894,
      "learning_rate": 0.0002615395191703331,
      "loss": 1.0194,
      "step": 6630
    },
    {
      "epoch": 0.39050209512607514,
      "grad_norm": 0.32735612988471985,
      "learning_rate": 0.0002614805939660591,
      "loss": 1.0148,
      "step": 6640
    },
    {
      "epoch": 0.391090200691024,
      "grad_norm": 0.3971503674983978,
      "learning_rate": 0.000261421668761785,
      "loss": 1.043,
      "step": 6650
    },
    {
      "epoch": 0.39167830625597294,
      "grad_norm": 0.323654443025589,
      "learning_rate": 0.000261362743557511,
      "loss": 0.9855,
      "step": 6660
    },
    {
      "epoch": 0.39226641182092187,
      "grad_norm": 0.34860435128211975,
      "learning_rate": 0.00026130381835323696,
      "loss": 1.0141,
      "step": 6670
    },
    {
      "epoch": 0.39285451738587074,
      "grad_norm": 0.3414376378059387,
      "learning_rate": 0.0002612448931489629,
      "loss": 1.1062,
      "step": 6680
    },
    {
      "epoch": 0.39344262295081966,
      "grad_norm": 0.35057058930397034,
      "learning_rate": 0.00026118596794468885,
      "loss": 1.0127,
      "step": 6690
    },
    {
      "epoch": 0.3940307285157686,
      "grad_norm": 0.31182152032852173,
      "learning_rate": 0.0002611270427404148,
      "loss": 1.0382,
      "step": 6700
    },
    {
      "epoch": 0.39461883408071746,
      "grad_norm": 0.40092891454696655,
      "learning_rate": 0.0002610681175361408,
      "loss": 1.0316,
      "step": 6710
    },
    {
      "epoch": 0.3952069396456664,
      "grad_norm": 0.31512174010276794,
      "learning_rate": 0.0002610091923318667,
      "loss": 1.0135,
      "step": 6720
    },
    {
      "epoch": 0.3957950452106153,
      "grad_norm": 0.351473867893219,
      "learning_rate": 0.0002609502671275927,
      "loss": 1.092,
      "step": 6730
    },
    {
      "epoch": 0.39638315077556424,
      "grad_norm": 0.328549325466156,
      "learning_rate": 0.00026089134192331867,
      "loss": 1.1144,
      "step": 6740
    },
    {
      "epoch": 0.3969712563405131,
      "grad_norm": 0.3460352122783661,
      "learning_rate": 0.0002608324167190446,
      "loss": 1.0537,
      "step": 6750
    },
    {
      "epoch": 0.39755936190546204,
      "grad_norm": 0.3749367594718933,
      "learning_rate": 0.00026077349151477057,
      "loss": 1.1026,
      "step": 6760
    },
    {
      "epoch": 0.39814746747041097,
      "grad_norm": 0.3355403244495392,
      "learning_rate": 0.00026071456631049654,
      "loss": 1.1605,
      "step": 6770
    },
    {
      "epoch": 0.39873557303535984,
      "grad_norm": 0.29899993538856506,
      "learning_rate": 0.0002606556411062225,
      "loss": 0.9497,
      "step": 6780
    },
    {
      "epoch": 0.39932367860030876,
      "grad_norm": 0.34274715185165405,
      "learning_rate": 0.00026059671590194844,
      "loss": 1.0438,
      "step": 6790
    },
    {
      "epoch": 0.3999117841652577,
      "grad_norm": 0.3189675807952881,
      "learning_rate": 0.0002605377906976744,
      "loss": 1.1312,
      "step": 6800
    },
    {
      "epoch": 0.40049988973020656,
      "grad_norm": 0.381056547164917,
      "learning_rate": 0.0002604788654934004,
      "loss": 1.1683,
      "step": 6810
    },
    {
      "epoch": 0.4010879952951555,
      "grad_norm": 0.34852075576782227,
      "learning_rate": 0.0002604199402891263,
      "loss": 1.052,
      "step": 6820
    },
    {
      "epoch": 0.4016761008601044,
      "grad_norm": 0.33289235830307007,
      "learning_rate": 0.0002603610150848523,
      "loss": 1.1269,
      "step": 6830
    },
    {
      "epoch": 0.4022642064250533,
      "grad_norm": 0.3374880254268646,
      "learning_rate": 0.00026030208988057826,
      "loss": 1.0215,
      "step": 6840
    },
    {
      "epoch": 0.4028523119900022,
      "grad_norm": 0.30113470554351807,
      "learning_rate": 0.0002602431646763042,
      "loss": 0.9891,
      "step": 6850
    },
    {
      "epoch": 0.40344041755495114,
      "grad_norm": 0.3231077492237091,
      "learning_rate": 0.00026018423947203015,
      "loss": 1.1525,
      "step": 6860
    },
    {
      "epoch": 0.4040285231199,
      "grad_norm": 0.29078805446624756,
      "learning_rate": 0.00026012531426775607,
      "loss": 1.0428,
      "step": 6870
    },
    {
      "epoch": 0.40461662868484893,
      "grad_norm": 0.31891629099845886,
      "learning_rate": 0.00026006638906348205,
      "loss": 1.1056,
      "step": 6880
    },
    {
      "epoch": 0.40520473424979786,
      "grad_norm": 0.398138165473938,
      "learning_rate": 0.000260007463859208,
      "loss": 1.0197,
      "step": 6890
    },
    {
      "epoch": 0.40579283981474673,
      "grad_norm": 0.3271045386791229,
      "learning_rate": 0.00025994853865493394,
      "loss": 1.0984,
      "step": 6900
    },
    {
      "epoch": 0.40638094537969566,
      "grad_norm": 0.3258526027202606,
      "learning_rate": 0.0002598896134506599,
      "loss": 1.0023,
      "step": 6910
    },
    {
      "epoch": 0.4069690509446446,
      "grad_norm": 0.3223864734172821,
      "learning_rate": 0.0002598306882463859,
      "loss": 0.9763,
      "step": 6920
    },
    {
      "epoch": 0.40755715650959345,
      "grad_norm": 0.3064817488193512,
      "learning_rate": 0.00025977176304211187,
      "loss": 1.0651,
      "step": 6930
    },
    {
      "epoch": 0.4081452620745424,
      "grad_norm": 0.31489449739456177,
      "learning_rate": 0.0002597128378378378,
      "loss": 1.1027,
      "step": 6940
    },
    {
      "epoch": 0.4087333676394913,
      "grad_norm": 0.3598993420600891,
      "learning_rate": 0.00025965391263356376,
      "loss": 0.9789,
      "step": 6950
    },
    {
      "epoch": 0.4093214732044402,
      "grad_norm": 0.3263813853263855,
      "learning_rate": 0.00025959498742928974,
      "loss": 1.0732,
      "step": 6960
    },
    {
      "epoch": 0.4099095787693891,
      "grad_norm": 0.3242092430591583,
      "learning_rate": 0.00025953606222501566,
      "loss": 1.0878,
      "step": 6970
    },
    {
      "epoch": 0.41049768433433803,
      "grad_norm": 0.2858439087867737,
      "learning_rate": 0.00025947713702074163,
      "loss": 1.0711,
      "step": 6980
    },
    {
      "epoch": 0.4110857898992869,
      "grad_norm": 0.37128549814224243,
      "learning_rate": 0.0002594182118164676,
      "loss": 1.1569,
      "step": 6990
    },
    {
      "epoch": 0.41167389546423583,
      "grad_norm": 0.32385385036468506,
      "learning_rate": 0.0002593592866121936,
      "loss": 0.9653,
      "step": 7000
    },
    {
      "epoch": 0.41226200102918475,
      "grad_norm": 0.380783349275589,
      "learning_rate": 0.0002593003614079195,
      "loss": 1.1173,
      "step": 7010
    },
    {
      "epoch": 0.4128501065941336,
      "grad_norm": 0.33616966009140015,
      "learning_rate": 0.0002592414362036455,
      "loss": 1.0719,
      "step": 7020
    },
    {
      "epoch": 0.41343821215908255,
      "grad_norm": 0.3313792049884796,
      "learning_rate": 0.00025918251099937145,
      "loss": 1.0332,
      "step": 7030
    },
    {
      "epoch": 0.4140263177240315,
      "grad_norm": 0.40473246574401855,
      "learning_rate": 0.0002591235857950974,
      "loss": 0.9971,
      "step": 7040
    },
    {
      "epoch": 0.41461442328898035,
      "grad_norm": 0.3171842098236084,
      "learning_rate": 0.00025906466059082335,
      "loss": 0.9852,
      "step": 7050
    },
    {
      "epoch": 0.4152025288539293,
      "grad_norm": 0.3303419351577759,
      "learning_rate": 0.0002590057353865493,
      "loss": 1.0541,
      "step": 7060
    },
    {
      "epoch": 0.4157906344188782,
      "grad_norm": 0.32523998618125916,
      "learning_rate": 0.0002589468101822753,
      "loss": 1.0834,
      "step": 7070
    },
    {
      "epoch": 0.4163787399838271,
      "grad_norm": 0.298444539308548,
      "learning_rate": 0.0002588878849780012,
      "loss": 1.1506,
      "step": 7080
    },
    {
      "epoch": 0.416966845548776,
      "grad_norm": 0.3257317841053009,
      "learning_rate": 0.0002588289597737272,
      "loss": 0.9664,
      "step": 7090
    },
    {
      "epoch": 0.4175549511137249,
      "grad_norm": 0.3456861972808838,
      "learning_rate": 0.00025877003456945317,
      "loss": 1.066,
      "step": 7100
    },
    {
      "epoch": 0.4181430566786738,
      "grad_norm": 0.36009520292282104,
      "learning_rate": 0.00025871110936517914,
      "loss": 0.9862,
      "step": 7110
    },
    {
      "epoch": 0.4187311622436227,
      "grad_norm": 0.32440146803855896,
      "learning_rate": 0.00025865218416090506,
      "loss": 0.9967,
      "step": 7120
    },
    {
      "epoch": 0.41931926780857165,
      "grad_norm": 0.31767696142196655,
      "learning_rate": 0.00025859325895663104,
      "loss": 1.0769,
      "step": 7130
    },
    {
      "epoch": 0.4199073733735205,
      "grad_norm": 0.32560041546821594,
      "learning_rate": 0.000258534333752357,
      "loss": 1.0545,
      "step": 7140
    },
    {
      "epoch": 0.42049547893846945,
      "grad_norm": 0.3569432497024536,
      "learning_rate": 0.00025847540854808293,
      "loss": 0.9662,
      "step": 7150
    },
    {
      "epoch": 0.4210835845034184,
      "grad_norm": 0.34979352355003357,
      "learning_rate": 0.0002584164833438089,
      "loss": 1.0654,
      "step": 7160
    },
    {
      "epoch": 0.42167169006836724,
      "grad_norm": 0.3947981894016266,
      "learning_rate": 0.0002583575581395349,
      "loss": 0.987,
      "step": 7170
    },
    {
      "epoch": 0.42225979563331617,
      "grad_norm": 0.31831297278404236,
      "learning_rate": 0.00025829863293526086,
      "loss": 1.0442,
      "step": 7180
    },
    {
      "epoch": 0.4228479011982651,
      "grad_norm": 0.33927932381629944,
      "learning_rate": 0.0002582397077309868,
      "loss": 1.0004,
      "step": 7190
    },
    {
      "epoch": 0.423436006763214,
      "grad_norm": 0.33908987045288086,
      "learning_rate": 0.00025818078252671275,
      "loss": 1.045,
      "step": 7200
    },
    {
      "epoch": 0.4240241123281629,
      "grad_norm": 0.3019404709339142,
      "learning_rate": 0.0002581218573224387,
      "loss": 0.9037,
      "step": 7210
    },
    {
      "epoch": 0.4246122178931118,
      "grad_norm": 0.3272733688354492,
      "learning_rate": 0.00025806293211816465,
      "loss": 1.1217,
      "step": 7220
    },
    {
      "epoch": 0.42520032345806075,
      "grad_norm": 0.31047022342681885,
      "learning_rate": 0.0002580040069138906,
      "loss": 0.9989,
      "step": 7230
    },
    {
      "epoch": 0.4257884290230096,
      "grad_norm": 0.35883980989456177,
      "learning_rate": 0.0002579450817096166,
      "loss": 1.0571,
      "step": 7240
    },
    {
      "epoch": 0.42637653458795854,
      "grad_norm": 0.353562593460083,
      "learning_rate": 0.00025788615650534257,
      "loss": 1.027,
      "step": 7250
    },
    {
      "epoch": 0.42696464015290747,
      "grad_norm": 0.29941895604133606,
      "learning_rate": 0.0002578272313010685,
      "loss": 0.9923,
      "step": 7260
    },
    {
      "epoch": 0.42755274571785634,
      "grad_norm": 0.2891441583633423,
      "learning_rate": 0.0002577683060967944,
      "loss": 1.0589,
      "step": 7270
    },
    {
      "epoch": 0.42814085128280527,
      "grad_norm": 0.3786044120788574,
      "learning_rate": 0.0002577093808925204,
      "loss": 1.0166,
      "step": 7280
    },
    {
      "epoch": 0.4287289568477542,
      "grad_norm": 0.33537545800209045,
      "learning_rate": 0.00025765045568824636,
      "loss": 0.997,
      "step": 7290
    },
    {
      "epoch": 0.42931706241270307,
      "grad_norm": 0.3518073856830597,
      "learning_rate": 0.0002575915304839723,
      "loss": 1.0185,
      "step": 7300
    },
    {
      "epoch": 0.429905167977652,
      "grad_norm": 0.3296126425266266,
      "learning_rate": 0.00025753260527969826,
      "loss": 1.044,
      "step": 7310
    },
    {
      "epoch": 0.4304932735426009,
      "grad_norm": 0.31169241666793823,
      "learning_rate": 0.00025747368007542423,
      "loss": 1.1121,
      "step": 7320
    },
    {
      "epoch": 0.4310813791075498,
      "grad_norm": 0.29706069827079773,
      "learning_rate": 0.0002574147548711502,
      "loss": 1.1335,
      "step": 7330
    },
    {
      "epoch": 0.4316694846724987,
      "grad_norm": 0.36089858412742615,
      "learning_rate": 0.0002573558296668761,
      "loss": 1.0512,
      "step": 7340
    },
    {
      "epoch": 0.43225759023744764,
      "grad_norm": 0.3351339101791382,
      "learning_rate": 0.0002572969044626021,
      "loss": 1.0498,
      "step": 7350
    },
    {
      "epoch": 0.4328456958023965,
      "grad_norm": 0.3417644500732422,
      "learning_rate": 0.0002572379792583281,
      "loss": 1.0309,
      "step": 7360
    },
    {
      "epoch": 0.43343380136734544,
      "grad_norm": 0.36982426047325134,
      "learning_rate": 0.000257179054054054,
      "loss": 0.9764,
      "step": 7370
    },
    {
      "epoch": 0.43402190693229437,
      "grad_norm": 0.3308740556240082,
      "learning_rate": 0.00025712012884977997,
      "loss": 1.1452,
      "step": 7380
    },
    {
      "epoch": 0.43461001249724324,
      "grad_norm": 0.3554490804672241,
      "learning_rate": 0.00025706120364550595,
      "loss": 1.0826,
      "step": 7390
    },
    {
      "epoch": 0.43519811806219216,
      "grad_norm": 0.3479733467102051,
      "learning_rate": 0.0002570022784412319,
      "loss": 1.014,
      "step": 7400
    },
    {
      "epoch": 0.4357862236271411,
      "grad_norm": 0.380109965801239,
      "learning_rate": 0.00025694335323695784,
      "loss": 0.941,
      "step": 7410
    },
    {
      "epoch": 0.43637432919208996,
      "grad_norm": 0.36414802074432373,
      "learning_rate": 0.0002568844280326838,
      "loss": 1.112,
      "step": 7420
    },
    {
      "epoch": 0.4369624347570389,
      "grad_norm": 0.35757893323898315,
      "learning_rate": 0.0002568255028284098,
      "loss": 1.1294,
      "step": 7430
    },
    {
      "epoch": 0.4375505403219878,
      "grad_norm": 0.33437296748161316,
      "learning_rate": 0.0002567665776241357,
      "loss": 1.0003,
      "step": 7440
    },
    {
      "epoch": 0.4381386458869367,
      "grad_norm": 0.3136492967605591,
      "learning_rate": 0.0002567076524198617,
      "loss": 0.995,
      "step": 7450
    },
    {
      "epoch": 0.4387267514518856,
      "grad_norm": 0.32980111241340637,
      "learning_rate": 0.00025664872721558766,
      "loss": 0.9315,
      "step": 7460
    },
    {
      "epoch": 0.43931485701683454,
      "grad_norm": 0.3269745707511902,
      "learning_rate": 0.00025658980201131364,
      "loss": 1.0916,
      "step": 7470
    },
    {
      "epoch": 0.4399029625817834,
      "grad_norm": 0.354611873626709,
      "learning_rate": 0.00025653087680703956,
      "loss": 1.0587,
      "step": 7480
    },
    {
      "epoch": 0.44049106814673233,
      "grad_norm": 0.3079547882080078,
      "learning_rate": 0.00025647195160276553,
      "loss": 1.0897,
      "step": 7490
    },
    {
      "epoch": 0.44107917371168126,
      "grad_norm": 0.38146188855171204,
      "learning_rate": 0.0002564130263984915,
      "loss": 1.1059,
      "step": 7500
    },
    {
      "epoch": 0.44166727927663013,
      "grad_norm": 0.3081369400024414,
      "learning_rate": 0.0002563541011942175,
      "loss": 1.0734,
      "step": 7510
    },
    {
      "epoch": 0.44225538484157906,
      "grad_norm": 0.36824238300323486,
      "learning_rate": 0.0002562951759899434,
      "loss": 1.0004,
      "step": 7520
    },
    {
      "epoch": 0.442843490406528,
      "grad_norm": 0.3296247720718384,
      "learning_rate": 0.0002562362507856694,
      "loss": 1.0276,
      "step": 7530
    },
    {
      "epoch": 0.44343159597147686,
      "grad_norm": 0.34385713934898376,
      "learning_rate": 0.00025617732558139535,
      "loss": 1.0192,
      "step": 7540
    },
    {
      "epoch": 0.4440197015364258,
      "grad_norm": 0.32852572202682495,
      "learning_rate": 0.00025611840037712127,
      "loss": 0.9735,
      "step": 7550
    },
    {
      "epoch": 0.4446078071013747,
      "grad_norm": 0.34254661202430725,
      "learning_rate": 0.00025605947517284725,
      "loss": 1.0452,
      "step": 7560
    },
    {
      "epoch": 0.4451959126663236,
      "grad_norm": 0.3870809078216553,
      "learning_rate": 0.0002560005499685732,
      "loss": 1.0239,
      "step": 7570
    },
    {
      "epoch": 0.4457840182312725,
      "grad_norm": 0.3530063331127167,
      "learning_rate": 0.0002559416247642992,
      "loss": 1.0024,
      "step": 7580
    },
    {
      "epoch": 0.44637212379622143,
      "grad_norm": 0.33130118250846863,
      "learning_rate": 0.0002558826995600251,
      "loss": 1.0987,
      "step": 7590
    },
    {
      "epoch": 0.4469602293611703,
      "grad_norm": 0.3035010099411011,
      "learning_rate": 0.0002558237743557511,
      "loss": 1.0649,
      "step": 7600
    },
    {
      "epoch": 0.44754833492611923,
      "grad_norm": 0.36123254895210266,
      "learning_rate": 0.00025576484915147707,
      "loss": 1.0876,
      "step": 7610
    },
    {
      "epoch": 0.44813644049106816,
      "grad_norm": 0.34023168683052063,
      "learning_rate": 0.000255705923947203,
      "loss": 1.0359,
      "step": 7620
    },
    {
      "epoch": 0.4487245460560171,
      "grad_norm": 0.3250979483127594,
      "learning_rate": 0.00025564699874292896,
      "loss": 1.0777,
      "step": 7630
    },
    {
      "epoch": 0.44931265162096595,
      "grad_norm": 0.3152918815612793,
      "learning_rate": 0.00025558807353865494,
      "loss": 1.0299,
      "step": 7640
    },
    {
      "epoch": 0.4499007571859149,
      "grad_norm": 0.3094918131828308,
      "learning_rate": 0.0002555291483343809,
      "loss": 1.0696,
      "step": 7650
    },
    {
      "epoch": 0.4504888627508638,
      "grad_norm": 0.3720642328262329,
      "learning_rate": 0.00025547022313010683,
      "loss": 1.0105,
      "step": 7660
    },
    {
      "epoch": 0.4510769683158127,
      "grad_norm": 0.31900879740715027,
      "learning_rate": 0.0002554112979258328,
      "loss": 1.0054,
      "step": 7670
    },
    {
      "epoch": 0.4516650738807616,
      "grad_norm": 0.3753970265388489,
      "learning_rate": 0.0002553523727215588,
      "loss": 0.987,
      "step": 7680
    },
    {
      "epoch": 0.45225317944571053,
      "grad_norm": 0.36823558807373047,
      "learning_rate": 0.0002552934475172847,
      "loss": 1.126,
      "step": 7690
    },
    {
      "epoch": 0.4528412850106594,
      "grad_norm": 0.31789523363113403,
      "learning_rate": 0.0002552345223130106,
      "loss": 1.024,
      "step": 7700
    },
    {
      "epoch": 0.4534293905756083,
      "grad_norm": 0.3358728587627411,
      "learning_rate": 0.0002551755971087366,
      "loss": 0.9947,
      "step": 7710
    },
    {
      "epoch": 0.45401749614055725,
      "grad_norm": 0.35169872641563416,
      "learning_rate": 0.00025511667190446257,
      "loss": 1.0591,
      "step": 7720
    },
    {
      "epoch": 0.4546056017055061,
      "grad_norm": 0.3435027599334717,
      "learning_rate": 0.00025505774670018855,
      "loss": 1.1306,
      "step": 7730
    },
    {
      "epoch": 0.45519370727045505,
      "grad_norm": 0.3619226813316345,
      "learning_rate": 0.00025499882149591447,
      "loss": 0.9844,
      "step": 7740
    },
    {
      "epoch": 0.455781812835404,
      "grad_norm": 0.3033200204372406,
      "learning_rate": 0.00025493989629164044,
      "loss": 1.053,
      "step": 7750
    },
    {
      "epoch": 0.45636991840035285,
      "grad_norm": 0.3556632697582245,
      "learning_rate": 0.0002548809710873664,
      "loss": 1.0277,
      "step": 7760
    },
    {
      "epoch": 0.4569580239653018,
      "grad_norm": 0.3636208772659302,
      "learning_rate": 0.00025482204588309234,
      "loss": 1.1092,
      "step": 7770
    },
    {
      "epoch": 0.4575461295302507,
      "grad_norm": 0.3213832378387451,
      "learning_rate": 0.0002547631206788183,
      "loss": 1.0511,
      "step": 7780
    },
    {
      "epoch": 0.45813423509519957,
      "grad_norm": 0.31111520528793335,
      "learning_rate": 0.0002547041954745443,
      "loss": 1.046,
      "step": 7790
    },
    {
      "epoch": 0.4587223406601485,
      "grad_norm": 0.38065704703330994,
      "learning_rate": 0.00025464527027027026,
      "loss": 0.9084,
      "step": 7800
    },
    {
      "epoch": 0.4593104462250974,
      "grad_norm": 0.32802140712738037,
      "learning_rate": 0.0002545863450659962,
      "loss": 1.0304,
      "step": 7810
    },
    {
      "epoch": 0.4598985517900463,
      "grad_norm": 0.31417492032051086,
      "learning_rate": 0.00025452741986172216,
      "loss": 1.0158,
      "step": 7820
    },
    {
      "epoch": 0.4604866573549952,
      "grad_norm": 0.3014402687549591,
      "learning_rate": 0.00025446849465744813,
      "loss": 1.0874,
      "step": 7830
    },
    {
      "epoch": 0.46107476291994415,
      "grad_norm": 0.3197631239891052,
      "learning_rate": 0.00025440956945317405,
      "loss": 0.9902,
      "step": 7840
    },
    {
      "epoch": 0.461662868484893,
      "grad_norm": 0.3447346091270447,
      "learning_rate": 0.0002543506442489,
      "loss": 1.0579,
      "step": 7850
    },
    {
      "epoch": 0.46225097404984195,
      "grad_norm": 0.366913378238678,
      "learning_rate": 0.000254291719044626,
      "loss": 0.9708,
      "step": 7860
    },
    {
      "epoch": 0.46283907961479087,
      "grad_norm": 0.3580063283443451,
      "learning_rate": 0.000254232793840352,
      "loss": 0.9489,
      "step": 7870
    },
    {
      "epoch": 0.46342718517973974,
      "grad_norm": 0.2961018681526184,
      "learning_rate": 0.0002541738686360779,
      "loss": 1.0679,
      "step": 7880
    },
    {
      "epoch": 0.46401529074468867,
      "grad_norm": 0.3415501117706299,
      "learning_rate": 0.00025411494343180387,
      "loss": 1.0747,
      "step": 7890
    },
    {
      "epoch": 0.4646033963096376,
      "grad_norm": 0.3544521629810333,
      "learning_rate": 0.00025405601822752985,
      "loss": 1.0557,
      "step": 7900
    },
    {
      "epoch": 0.46519150187458647,
      "grad_norm": 0.3563128411769867,
      "learning_rate": 0.0002539970930232558,
      "loss": 1.1408,
      "step": 7910
    },
    {
      "epoch": 0.4657796074395354,
      "grad_norm": 0.35090920329093933,
      "learning_rate": 0.00025393816781898174,
      "loss": 1.039,
      "step": 7920
    },
    {
      "epoch": 0.4663677130044843,
      "grad_norm": 0.3148064911365509,
      "learning_rate": 0.0002538792426147077,
      "loss": 1.0032,
      "step": 7930
    },
    {
      "epoch": 0.4669558185694332,
      "grad_norm": 0.3074811100959778,
      "learning_rate": 0.0002538203174104337,
      "loss": 1.0194,
      "step": 7940
    },
    {
      "epoch": 0.4675439241343821,
      "grad_norm": 0.3355798125267029,
      "learning_rate": 0.0002537613922061596,
      "loss": 0.9612,
      "step": 7950
    },
    {
      "epoch": 0.46813202969933104,
      "grad_norm": 0.3487861454486847,
      "learning_rate": 0.0002537024670018856,
      "loss": 1.0276,
      "step": 7960
    },
    {
      "epoch": 0.4687201352642799,
      "grad_norm": 0.3089461922645569,
      "learning_rate": 0.00025364354179761156,
      "loss": 1.0114,
      "step": 7970
    },
    {
      "epoch": 0.46930824082922884,
      "grad_norm": 0.31787800788879395,
      "learning_rate": 0.00025358461659333754,
      "loss": 1.0189,
      "step": 7980
    },
    {
      "epoch": 0.46989634639417777,
      "grad_norm": 0.3335334360599518,
      "learning_rate": 0.00025352569138906346,
      "loss": 0.9762,
      "step": 7990
    },
    {
      "epoch": 0.47048445195912664,
      "grad_norm": 0.340578556060791,
      "learning_rate": 0.00025346676618478943,
      "loss": 1.065,
      "step": 8000
    },
    {
      "epoch": 0.47107255752407556,
      "grad_norm": NaN,
      "learning_rate": 0.0002534078409805154,
      "loss": 1.0954,
      "step": 8010
    },
    {
      "epoch": 0.4716606630890245,
      "grad_norm": 0.3344525694847107,
      "learning_rate": 0.00025335480829666876,
      "loss": 1.0315,
      "step": 8020
    },
    {
      "epoch": 0.47224876865397336,
      "grad_norm": 0.34153100848197937,
      "learning_rate": 0.0002532958830923947,
      "loss": 1.0394,
      "step": 8030
    },
    {
      "epoch": 0.4728368742189223,
      "grad_norm": 0.35672974586486816,
      "learning_rate": 0.00025323695788812066,
      "loss": 0.983,
      "step": 8040
    },
    {
      "epoch": 0.4734249797838712,
      "grad_norm": 0.3380025625228882,
      "learning_rate": 0.00025317803268384663,
      "loss": 1.0488,
      "step": 8050
    },
    {
      "epoch": 0.47401308534882014,
      "grad_norm": 0.36482590436935425,
      "learning_rate": 0.0002531191074795726,
      "loss": 1.0509,
      "step": 8060
    },
    {
      "epoch": 0.474601190913769,
      "grad_norm": 0.3911086618900299,
      "learning_rate": 0.0002530601822752985,
      "loss": 1.0039,
      "step": 8070
    },
    {
      "epoch": 0.47518929647871794,
      "grad_norm": 0.3243265151977539,
      "learning_rate": 0.0002530012570710245,
      "loss": 1.1886,
      "step": 8080
    },
    {
      "epoch": 0.47577740204366686,
      "grad_norm": 0.3384384512901306,
      "learning_rate": 0.0002529423318667505,
      "loss": 1.0438,
      "step": 8090
    },
    {
      "epoch": 0.47636550760861573,
      "grad_norm": 0.3114083409309387,
      "learning_rate": 0.0002528834066624764,
      "loss": 1.027,
      "step": 8100
    },
    {
      "epoch": 0.47695361317356466,
      "grad_norm": 0.3094269931316376,
      "learning_rate": 0.00025282448145820237,
      "loss": 1.0352,
      "step": 8110
    },
    {
      "epoch": 0.4775417187385136,
      "grad_norm": 0.3519774079322815,
      "learning_rate": 0.00025276555625392835,
      "loss": 1.0969,
      "step": 8120
    },
    {
      "epoch": 0.47812982430346246,
      "grad_norm": 0.30151405930519104,
      "learning_rate": 0.0002527066310496543,
      "loss": 1.0459,
      "step": 8130
    },
    {
      "epoch": 0.4787179298684114,
      "grad_norm": 0.31866225600242615,
      "learning_rate": 0.00025264770584538024,
      "loss": 1.0326,
      "step": 8140
    },
    {
      "epoch": 0.4793060354333603,
      "grad_norm": 0.3204851746559143,
      "learning_rate": 0.0002525887806411062,
      "loss": 1.0299,
      "step": 8150
    },
    {
      "epoch": 0.4798941409983092,
      "grad_norm": 0.33878299593925476,
      "learning_rate": 0.0002525298554368322,
      "loss": 1.1008,
      "step": 8160
    },
    {
      "epoch": 0.4804822465632581,
      "grad_norm": 0.36644676327705383,
      "learning_rate": 0.0002524709302325581,
      "loss": 1.0424,
      "step": 8170
    },
    {
      "epoch": 0.48107035212820703,
      "grad_norm": 0.31027403473854065,
      "learning_rate": 0.0002524120050282841,
      "loss": 1.0127,
      "step": 8180
    },
    {
      "epoch": 0.4816584576931559,
      "grad_norm": 0.3115144371986389,
      "learning_rate": 0.00025235307982401,
      "loss": 1.0766,
      "step": 8190
    },
    {
      "epoch": 0.48224656325810483,
      "grad_norm": 0.3728862702846527,
      "learning_rate": 0.000252294154619736,
      "loss": 0.9853,
      "step": 8200
    },
    {
      "epoch": 0.48283466882305376,
      "grad_norm": 0.33557966351509094,
      "learning_rate": 0.00025223522941546196,
      "loss": 0.9794,
      "step": 8210
    },
    {
      "epoch": 0.48342277438800263,
      "grad_norm": 0.37026065587997437,
      "learning_rate": 0.0002521763042111879,
      "loss": 1.0742,
      "step": 8220
    },
    {
      "epoch": 0.48401087995295156,
      "grad_norm": 0.3282525837421417,
      "learning_rate": 0.00025211737900691385,
      "loss": 1.0337,
      "step": 8230
    },
    {
      "epoch": 0.4845989855179005,
      "grad_norm": 0.294232040643692,
      "learning_rate": 0.0002520584538026398,
      "loss": 0.9792,
      "step": 8240
    },
    {
      "epoch": 0.48518709108284935,
      "grad_norm": 0.3371100425720215,
      "learning_rate": 0.0002519995285983658,
      "loss": 1.0579,
      "step": 8250
    },
    {
      "epoch": 0.4857751966477983,
      "grad_norm": 0.363458514213562,
      "learning_rate": 0.0002519406033940917,
      "loss": 1.106,
      "step": 8260
    },
    {
      "epoch": 0.4863633022127472,
      "grad_norm": 0.34114694595336914,
      "learning_rate": 0.0002518816781898177,
      "loss": 1.0288,
      "step": 8270
    },
    {
      "epoch": 0.4869514077776961,
      "grad_norm": 0.33799266815185547,
      "learning_rate": 0.00025182275298554367,
      "loss": 1.0707,
      "step": 8280
    },
    {
      "epoch": 0.487539513342645,
      "grad_norm": 0.33746853470802307,
      "learning_rate": 0.0002517638277812696,
      "loss": 1.0415,
      "step": 8290
    },
    {
      "epoch": 0.48812761890759393,
      "grad_norm": 0.3346860408782959,
      "learning_rate": 0.00025170490257699557,
      "loss": 1.029,
      "step": 8300
    },
    {
      "epoch": 0.4887157244725428,
      "grad_norm": 0.3451192080974579,
      "learning_rate": 0.00025164597737272154,
      "loss": 1.0448,
      "step": 8310
    },
    {
      "epoch": 0.4893038300374917,
      "grad_norm": 0.32359185814857483,
      "learning_rate": 0.0002515870521684475,
      "loss": 1.0709,
      "step": 8320
    },
    {
      "epoch": 0.48989193560244065,
      "grad_norm": 0.32075271010398865,
      "learning_rate": 0.00025152812696417344,
      "loss": 1.0261,
      "step": 8330
    },
    {
      "epoch": 0.4904800411673895,
      "grad_norm": 0.2910362184047699,
      "learning_rate": 0.0002514692017598994,
      "loss": 0.8862,
      "step": 8340
    },
    {
      "epoch": 0.49106814673233845,
      "grad_norm": 0.3309563994407654,
      "learning_rate": 0.0002514102765556254,
      "loss": 1.0213,
      "step": 8350
    },
    {
      "epoch": 0.4916562522972874,
      "grad_norm": 0.39823034405708313,
      "learning_rate": 0.0002513513513513513,
      "loss": 1.0918,
      "step": 8360
    },
    {
      "epoch": 0.49224435786223625,
      "grad_norm": 0.29987815022468567,
      "learning_rate": 0.0002512924261470773,
      "loss": 1.0496,
      "step": 8370
    },
    {
      "epoch": 0.4928324634271852,
      "grad_norm": 0.3153943121433258,
      "learning_rate": 0.00025123350094280326,
      "loss": 1.0535,
      "step": 8380
    },
    {
      "epoch": 0.4934205689921341,
      "grad_norm": 0.3379216492176056,
      "learning_rate": 0.00025117457573852923,
      "loss": 1.0917,
      "step": 8390
    },
    {
      "epoch": 0.49400867455708297,
      "grad_norm": 0.31701481342315674,
      "learning_rate": 0.00025111565053425515,
      "loss": 1.0881,
      "step": 8400
    },
    {
      "epoch": 0.4945967801220319,
      "grad_norm": 0.33057448267936707,
      "learning_rate": 0.0002510567253299811,
      "loss": 1.0289,
      "step": 8410
    },
    {
      "epoch": 0.4951848856869808,
      "grad_norm": 0.3420124650001526,
      "learning_rate": 0.0002509978001257071,
      "loss": 1.0719,
      "step": 8420
    },
    {
      "epoch": 0.4957729912519297,
      "grad_norm": 0.34574753046035767,
      "learning_rate": 0.000250938874921433,
      "loss": 1.0016,
      "step": 8430
    },
    {
      "epoch": 0.4963610968168786,
      "grad_norm": 0.35470691323280334,
      "learning_rate": 0.000250879949717159,
      "loss": 1.0526,
      "step": 8440
    },
    {
      "epoch": 0.49694920238182755,
      "grad_norm": 0.320971816778183,
      "learning_rate": 0.00025082102451288497,
      "loss": 1.0696,
      "step": 8450
    },
    {
      "epoch": 0.4975373079467764,
      "grad_norm": 0.32328101992607117,
      "learning_rate": 0.00025076209930861094,
      "loss": 1.0401,
      "step": 8460
    },
    {
      "epoch": 0.49812541351172535,
      "grad_norm": 0.3223609924316406,
      "learning_rate": 0.00025070317410433687,
      "loss": 1.1405,
      "step": 8470
    },
    {
      "epoch": 0.49871351907667427,
      "grad_norm": 0.3539333641529083,
      "learning_rate": 0.00025064424890006284,
      "loss": 1.1109,
      "step": 8480
    },
    {
      "epoch": 0.4993016246416232,
      "grad_norm": 0.32482630014419556,
      "learning_rate": 0.0002505853236957888,
      "loss": 1.1169,
      "step": 8490
    },
    {
      "epoch": 0.49988973020657207,
      "grad_norm": 0.4100240170955658,
      "learning_rate": 0.00025052639849151474,
      "loss": 1.0514,
      "step": 8500
    },
    {
      "epoch": 0.5004778357715209,
      "grad_norm": 0.27606871724128723,
      "learning_rate": 0.0002504674732872407,
      "loss": 1.0042,
      "step": 8510
    },
    {
      "epoch": 0.5010659413364699,
      "grad_norm": 0.3561316430568695,
      "learning_rate": 0.0002504085480829667,
      "loss": 1.0294,
      "step": 8520
    },
    {
      "epoch": 0.5016540469014188,
      "grad_norm": 0.350577712059021,
      "learning_rate": 0.00025034962287869266,
      "loss": 1.0068,
      "step": 8530
    },
    {
      "epoch": 0.5022421524663677,
      "grad_norm": 0.34743785858154297,
      "learning_rate": 0.0002502906976744186,
      "loss": 1.0693,
      "step": 8540
    },
    {
      "epoch": 0.5028302580313166,
      "grad_norm": 0.33529630303382874,
      "learning_rate": 0.00025023177247014456,
      "loss": 1.0309,
      "step": 8550
    },
    {
      "epoch": 0.5034183635962656,
      "grad_norm": 0.3048957288265228,
      "learning_rate": 0.00025017284726587053,
      "loss": 1.0044,
      "step": 8560
    },
    {
      "epoch": 0.5040064691612144,
      "grad_norm": 0.3006167411804199,
      "learning_rate": 0.00025011392206159645,
      "loss": 1.1146,
      "step": 8570
    },
    {
      "epoch": 0.5045945747261633,
      "grad_norm": 0.28935864567756653,
      "learning_rate": 0.0002500549968573224,
      "loss": 1.0843,
      "step": 8580
    },
    {
      "epoch": 0.5051826802911122,
      "grad_norm": 0.3601384460926056,
      "learning_rate": 0.0002499960716530484,
      "loss": 1.0433,
      "step": 8590
    },
    {
      "epoch": 0.5057707858560612,
      "grad_norm": 0.35219499468803406,
      "learning_rate": 0.0002499371464487744,
      "loss": 0.9621,
      "step": 8600
    },
    {
      "epoch": 0.5063588914210101,
      "grad_norm": 0.32255491614341736,
      "learning_rate": 0.0002498782212445003,
      "loss": 1.0419,
      "step": 8610
    },
    {
      "epoch": 0.506946996985959,
      "grad_norm": 0.3555779457092285,
      "learning_rate": 0.0002498192960402262,
      "loss": 0.9694,
      "step": 8620
    },
    {
      "epoch": 0.5075351025509078,
      "grad_norm": 0.3876092731952667,
      "learning_rate": 0.0002497603708359522,
      "loss": 1.0383,
      "step": 8630
    },
    {
      "epoch": 0.5081232081158568,
      "grad_norm": 0.28334254026412964,
      "learning_rate": 0.00024970144563167817,
      "loss": 1.0627,
      "step": 8640
    },
    {
      "epoch": 0.5087113136808057,
      "grad_norm": 0.512872576713562,
      "learning_rate": 0.00024964252042740414,
      "loss": 1.1193,
      "step": 8650
    },
    {
      "epoch": 0.5092994192457546,
      "grad_norm": 0.3210139870643616,
      "learning_rate": 0.00024958359522313006,
      "loss": 0.9799,
      "step": 8660
    },
    {
      "epoch": 0.5098875248107035,
      "grad_norm": 0.31847333908081055,
      "learning_rate": 0.00024952467001885604,
      "loss": 0.9832,
      "step": 8670
    },
    {
      "epoch": 0.5104756303756525,
      "grad_norm": 0.3184986114501953,
      "learning_rate": 0.000249465744814582,
      "loss": 1.0167,
      "step": 8680
    },
    {
      "epoch": 0.5110637359406013,
      "grad_norm": 0.32047563791275024,
      "learning_rate": 0.00024940681961030793,
      "loss": 1.0857,
      "step": 8690
    },
    {
      "epoch": 0.5116518415055502,
      "grad_norm": 0.3244185745716095,
      "learning_rate": 0.0002493478944060339,
      "loss": 0.9323,
      "step": 8700
    },
    {
      "epoch": 0.5122399470704991,
      "grad_norm": 0.34178826212882996,
      "learning_rate": 0.0002492889692017599,
      "loss": 1.09,
      "step": 8710
    },
    {
      "epoch": 0.5128280526354481,
      "grad_norm": 0.3511996269226074,
      "learning_rate": 0.00024923004399748585,
      "loss": 0.9493,
      "step": 8720
    },
    {
      "epoch": 0.513416158200397,
      "grad_norm": 0.3141157329082489,
      "learning_rate": 0.0002491711187932118,
      "loss": 1.0816,
      "step": 8730
    },
    {
      "epoch": 0.5140042637653459,
      "grad_norm": 0.3242158591747284,
      "learning_rate": 0.00024911219358893775,
      "loss": 1.1071,
      "step": 8740
    },
    {
      "epoch": 0.5145923693302948,
      "grad_norm": 0.34310591220855713,
      "learning_rate": 0.0002490532683846637,
      "loss": 0.9328,
      "step": 8750
    },
    {
      "epoch": 0.5151804748952437,
      "grad_norm": 0.3521094024181366,
      "learning_rate": 0.00024899434318038965,
      "loss": 1.1717,
      "step": 8760
    },
    {
      "epoch": 0.5157685804601926,
      "grad_norm": 0.31291142106056213,
      "learning_rate": 0.0002489354179761156,
      "loss": 1.1599,
      "step": 8770
    },
    {
      "epoch": 0.5163566860251415,
      "grad_norm": 0.31912311911582947,
      "learning_rate": 0.0002488764927718416,
      "loss": 1.0332,
      "step": 8780
    },
    {
      "epoch": 0.5169447915900904,
      "grad_norm": 0.3583078384399414,
      "learning_rate": 0.00024881756756756757,
      "loss": 1.1229,
      "step": 8790
    },
    {
      "epoch": 0.5175328971550394,
      "grad_norm": 0.32117509841918945,
      "learning_rate": 0.0002487586423632935,
      "loss": 1.1638,
      "step": 8800
    },
    {
      "epoch": 0.5181210027199883,
      "grad_norm": 0.3126143515110016,
      "learning_rate": 0.00024869971715901946,
      "loss": 1.0919,
      "step": 8810
    },
    {
      "epoch": 0.5187091082849371,
      "grad_norm": 0.3174304962158203,
      "learning_rate": 0.00024864079195474544,
      "loss": 1.0759,
      "step": 8820
    },
    {
      "epoch": 0.519297213849886,
      "grad_norm": 0.36914950609207153,
      "learning_rate": 0.00024858186675047136,
      "loss": 1.1453,
      "step": 8830
    },
    {
      "epoch": 0.519885319414835,
      "grad_norm": 0.30860984325408936,
      "learning_rate": 0.00024852294154619734,
      "loss": 1.0003,
      "step": 8840
    },
    {
      "epoch": 0.5204734249797839,
      "grad_norm": 0.33536410331726074,
      "learning_rate": 0.0002484640163419233,
      "loss": 1.0738,
      "step": 8850
    },
    {
      "epoch": 0.5210615305447328,
      "grad_norm": 0.3339655101299286,
      "learning_rate": 0.0002484050911376493,
      "loss": 1.0638,
      "step": 8860
    },
    {
      "epoch": 0.5216496361096817,
      "grad_norm": 0.29325875639915466,
      "learning_rate": 0.0002483461659333752,
      "loss": 1.1137,
      "step": 8870
    },
    {
      "epoch": 0.5222377416746306,
      "grad_norm": 0.3245059549808502,
      "learning_rate": 0.0002482872407291012,
      "loss": 1.0365,
      "step": 8880
    },
    {
      "epoch": 0.5228258472395795,
      "grad_norm": 0.36886340379714966,
      "learning_rate": 0.00024822831552482715,
      "loss": 1.0028,
      "step": 8890
    },
    {
      "epoch": 0.5234139528045284,
      "grad_norm": 0.3263726830482483,
      "learning_rate": 0.0002481693903205531,
      "loss": 0.9402,
      "step": 8900
    },
    {
      "epoch": 0.5240020583694773,
      "grad_norm": 0.3314315378665924,
      "learning_rate": 0.00024811046511627905,
      "loss": 1.0252,
      "step": 8910
    },
    {
      "epoch": 0.5245901639344263,
      "grad_norm": 0.3387935757637024,
      "learning_rate": 0.000248051539912005,
      "loss": 1.0238,
      "step": 8920
    },
    {
      "epoch": 0.5251782694993752,
      "grad_norm": 0.40339136123657227,
      "learning_rate": 0.000247992614707731,
      "loss": 1.057,
      "step": 8930
    },
    {
      "epoch": 0.525766375064324,
      "grad_norm": 0.4146227240562439,
      "learning_rate": 0.0002479336895034569,
      "loss": 1.0105,
      "step": 8940
    },
    {
      "epoch": 0.5263544806292729,
      "grad_norm": 0.31119534373283386,
      "learning_rate": 0.0002478747642991829,
      "loss": 1.0427,
      "step": 8950
    },
    {
      "epoch": 0.5269425861942219,
      "grad_norm": 0.34290003776550293,
      "learning_rate": 0.00024781583909490887,
      "loss": 1.0138,
      "step": 8960
    },
    {
      "epoch": 0.5275306917591708,
      "grad_norm": 0.32186803221702576,
      "learning_rate": 0.0002477569138906348,
      "loss": 1.0565,
      "step": 8970
    },
    {
      "epoch": 0.5281187973241197,
      "grad_norm": 0.3174428939819336,
      "learning_rate": 0.00024769798868636076,
      "loss": 0.9319,
      "step": 8980
    },
    {
      "epoch": 0.5287069028890686,
      "grad_norm": 0.319748193025589,
      "learning_rate": 0.00024763906348208674,
      "loss": 1.0349,
      "step": 8990
    },
    {
      "epoch": 0.5292950084540174,
      "grad_norm": 0.3278958797454834,
      "learning_rate": 0.0002475801382778127,
      "loss": 1.0454,
      "step": 9000
    },
    {
      "epoch": 0.5298831140189664,
      "grad_norm": 0.3936189115047455,
      "learning_rate": 0.00024752121307353863,
      "loss": 0.9987,
      "step": 9010
    },
    {
      "epoch": 0.5304712195839153,
      "grad_norm": 0.28662770986557007,
      "learning_rate": 0.0002474622878692646,
      "loss": 1.0932,
      "step": 9020
    },
    {
      "epoch": 0.5310593251488642,
      "grad_norm": 0.37003758549690247,
      "learning_rate": 0.00024740336266499053,
      "loss": 1.0866,
      "step": 9030
    },
    {
      "epoch": 0.5316474307138132,
      "grad_norm": 0.3126535415649414,
      "learning_rate": 0.0002473444374607165,
      "loss": 1.1598,
      "step": 9040
    },
    {
      "epoch": 0.5322355362787621,
      "grad_norm": 0.32055363059043884,
      "learning_rate": 0.0002472855122564425,
      "loss": 1.0018,
      "step": 9050
    },
    {
      "epoch": 0.5328236418437109,
      "grad_norm": 0.2987554967403412,
      "learning_rate": 0.0002472265870521684,
      "loss": 1.0914,
      "step": 9060
    },
    {
      "epoch": 0.5334117474086598,
      "grad_norm": 0.29826611280441284,
      "learning_rate": 0.0002471676618478944,
      "loss": 1.1307,
      "step": 9070
    },
    {
      "epoch": 0.5339998529736087,
      "grad_norm": 0.3316572606563568,
      "learning_rate": 0.00024710873664362035,
      "loss": 1.1324,
      "step": 9080
    },
    {
      "epoch": 0.5345879585385577,
      "grad_norm": 0.3460550904273987,
      "learning_rate": 0.00024704981143934627,
      "loss": 0.9071,
      "step": 9090
    },
    {
      "epoch": 0.5351760641035066,
      "grad_norm": 0.29260268807411194,
      "learning_rate": 0.00024699088623507224,
      "loss": 1.1081,
      "step": 9100
    },
    {
      "epoch": 0.5357641696684555,
      "grad_norm": 0.363668292760849,
      "learning_rate": 0.0002469319610307982,
      "loss": 1.1364,
      "step": 9110
    },
    {
      "epoch": 0.5363522752334043,
      "grad_norm": 0.3631923794746399,
      "learning_rate": 0.0002468730358265242,
      "loss": 1.0836,
      "step": 9120
    },
    {
      "epoch": 0.5369403807983533,
      "grad_norm": 0.3799728751182556,
      "learning_rate": 0.0002468141106222501,
      "loss": 0.9991,
      "step": 9130
    },
    {
      "epoch": 0.5375284863633022,
      "grad_norm": 0.36455488204956055,
      "learning_rate": 0.0002467551854179761,
      "loss": 1.0654,
      "step": 9140
    },
    {
      "epoch": 0.5381165919282511,
      "grad_norm": 0.35207417607307434,
      "learning_rate": 0.00024669626021370206,
      "loss": 1.1143,
      "step": 9150
    },
    {
      "epoch": 0.5387046974932,
      "grad_norm": 0.33545732498168945,
      "learning_rate": 0.000246637335009428,
      "loss": 0.9779,
      "step": 9160
    },
    {
      "epoch": 0.539292803058149,
      "grad_norm": 0.37577754259109497,
      "learning_rate": 0.00024657840980515396,
      "loss": 1.0454,
      "step": 9170
    },
    {
      "epoch": 0.5398809086230979,
      "grad_norm": 0.3506116271018982,
      "learning_rate": 0.00024651948460087993,
      "loss": 1.0417,
      "step": 9180
    },
    {
      "epoch": 0.5404690141880467,
      "grad_norm": 0.37733012437820435,
      "learning_rate": 0.0002464605593966059,
      "loss": 1.072,
      "step": 9190
    },
    {
      "epoch": 0.5410571197529956,
      "grad_norm": 0.3702929615974426,
      "learning_rate": 0.00024640163419233183,
      "loss": 1.1136,
      "step": 9200
    },
    {
      "epoch": 0.5416452253179446,
      "grad_norm": 0.3619152009487152,
      "learning_rate": 0.0002463427089880578,
      "loss": 1.0324,
      "step": 9210
    },
    {
      "epoch": 0.5422333308828935,
      "grad_norm": 0.30696260929107666,
      "learning_rate": 0.0002462837837837838,
      "loss": 1.1221,
      "step": 9220
    },
    {
      "epoch": 0.5428214364478424,
      "grad_norm": 0.31267857551574707,
      "learning_rate": 0.0002462248585795097,
      "loss": 1.0288,
      "step": 9230
    },
    {
      "epoch": 0.5434095420127913,
      "grad_norm": 0.26784130930900574,
      "learning_rate": 0.0002461659333752357,
      "loss": 1.0459,
      "step": 9240
    },
    {
      "epoch": 0.5439976475777402,
      "grad_norm": 0.3465924859046936,
      "learning_rate": 0.00024610700817096165,
      "loss": 1.0542,
      "step": 9250
    },
    {
      "epoch": 0.5445857531426891,
      "grad_norm": 0.3121328353881836,
      "learning_rate": 0.0002460480829666876,
      "loss": 1.1147,
      "step": 9260
    },
    {
      "epoch": 0.545173858707638,
      "grad_norm": 0.34306976199150085,
      "learning_rate": 0.00024598915776241354,
      "loss": 1.0784,
      "step": 9270
    },
    {
      "epoch": 0.5457619642725869,
      "grad_norm": 0.3712669312953949,
      "learning_rate": 0.0002459302325581395,
      "loss": 0.9928,
      "step": 9280
    },
    {
      "epoch": 0.5463500698375359,
      "grad_norm": 0.36131608486175537,
      "learning_rate": 0.0002458713073538655,
      "loss": 1.1448,
      "step": 9290
    },
    {
      "epoch": 0.5469381754024848,
      "grad_norm": 0.3553740084171295,
      "learning_rate": 0.0002458123821495914,
      "loss": 0.9626,
      "step": 9300
    },
    {
      "epoch": 0.5475262809674336,
      "grad_norm": 0.3103846609592438,
      "learning_rate": 0.0002457534569453174,
      "loss": 1.0712,
      "step": 9310
    },
    {
      "epoch": 0.5481143865323825,
      "grad_norm": 0.41920289397239685,
      "learning_rate": 0.00024569453174104336,
      "loss": 1.0186,
      "step": 9320
    },
    {
      "epoch": 0.5487024920973315,
      "grad_norm": 0.3061476945877075,
      "learning_rate": 0.00024563560653676934,
      "loss": 1.027,
      "step": 9330
    },
    {
      "epoch": 0.5492905976622804,
      "grad_norm": 0.30720892548561096,
      "learning_rate": 0.00024557668133249526,
      "loss": 1.117,
      "step": 9340
    },
    {
      "epoch": 0.5498787032272293,
      "grad_norm": 0.4039391875267029,
      "learning_rate": 0.00024551775612822123,
      "loss": 1.0443,
      "step": 9350
    },
    {
      "epoch": 0.5504668087921782,
      "grad_norm": 0.37421417236328125,
      "learning_rate": 0.0002454588309239472,
      "loss": 1.0398,
      "step": 9360
    },
    {
      "epoch": 0.5510549143571271,
      "grad_norm": 0.2611134350299835,
      "learning_rate": 0.00024539990571967313,
      "loss": 1.0357,
      "step": 9370
    },
    {
      "epoch": 0.551643019922076,
      "grad_norm": 0.3368493914604187,
      "learning_rate": 0.0002453409805153991,
      "loss": 1.1085,
      "step": 9380
    },
    {
      "epoch": 0.5522311254870249,
      "grad_norm": 0.3359927535057068,
      "learning_rate": 0.0002452820553111251,
      "loss": 1.0481,
      "step": 9390
    },
    {
      "epoch": 0.5528192310519738,
      "grad_norm": 0.35394230484962463,
      "learning_rate": 0.00024522313010685105,
      "loss": 1.0077,
      "step": 9400
    },
    {
      "epoch": 0.5534073366169228,
      "grad_norm": 0.32916566729545593,
      "learning_rate": 0.000245164204902577,
      "loss": 1.0222,
      "step": 9410
    },
    {
      "epoch": 0.5539954421818717,
      "grad_norm": 0.3696168065071106,
      "learning_rate": 0.00024510527969830295,
      "loss": 0.9972,
      "step": 9420
    },
    {
      "epoch": 0.5545835477468205,
      "grad_norm": 0.3018307089805603,
      "learning_rate": 0.0002450463544940289,
      "loss": 1.1025,
      "step": 9430
    },
    {
      "epoch": 0.5551716533117694,
      "grad_norm": 0.33433422446250916,
      "learning_rate": 0.00024498742928975484,
      "loss": 1.0501,
      "step": 9440
    },
    {
      "epoch": 0.5557597588767184,
      "grad_norm": 0.3364710211753845,
      "learning_rate": 0.0002449285040854808,
      "loss": 1.0195,
      "step": 9450
    },
    {
      "epoch": 0.5563478644416673,
      "grad_norm": 0.3090519607067108,
      "learning_rate": 0.00024486957888120674,
      "loss": 1.0279,
      "step": 9460
    },
    {
      "epoch": 0.5569359700066162,
      "grad_norm": 0.39319971203804016,
      "learning_rate": 0.0002448106536769327,
      "loss": 1.0572,
      "step": 9470
    },
    {
      "epoch": 0.5575240755715651,
      "grad_norm": 0.2995067536830902,
      "learning_rate": 0.0002447517284726587,
      "loss": 1.0676,
      "step": 9480
    },
    {
      "epoch": 0.558112181136514,
      "grad_norm": 0.33077001571655273,
      "learning_rate": 0.0002446928032683846,
      "loss": 1.1356,
      "step": 9490
    },
    {
      "epoch": 0.5587002867014629,
      "grad_norm": 0.33915022015571594,
      "learning_rate": 0.0002446338780641106,
      "loss": 1.1605,
      "step": 9500
    },
    {
      "epoch": 0.5592883922664118,
      "grad_norm": 0.3167703449726105,
      "learning_rate": 0.00024457495285983656,
      "loss": 1.0749,
      "step": 9510
    },
    {
      "epoch": 0.5598764978313607,
      "grad_norm": 0.39409270882606506,
      "learning_rate": 0.00024451602765556253,
      "loss": 1.1393,
      "step": 9520
    },
    {
      "epoch": 0.5604646033963097,
      "grad_norm": 0.3478277325630188,
      "learning_rate": 0.00024445710245128845,
      "loss": 1.0418,
      "step": 9530
    },
    {
      "epoch": 0.5610527089612586,
      "grad_norm": 0.3741080164909363,
      "learning_rate": 0.00024439817724701443,
      "loss": 1.0637,
      "step": 9540
    },
    {
      "epoch": 0.5616408145262074,
      "grad_norm": 0.3238665461540222,
      "learning_rate": 0.0002443392520427404,
      "loss": 1.0603,
      "step": 9550
    },
    {
      "epoch": 0.5622289200911563,
      "grad_norm": 0.3610852360725403,
      "learning_rate": 0.0002442803268384663,
      "loss": 1.144,
      "step": 9560
    },
    {
      "epoch": 0.5628170256561053,
      "grad_norm": 0.3498801589012146,
      "learning_rate": 0.0002442214016341923,
      "loss": 1.2159,
      "step": 9570
    },
    {
      "epoch": 0.5634051312210542,
      "grad_norm": 0.3239791989326477,
      "learning_rate": 0.0002441624764299183,
      "loss": 1.0026,
      "step": 9580
    },
    {
      "epoch": 0.5639932367860031,
      "grad_norm": 0.35527369379997253,
      "learning_rate": 0.00024410355122564422,
      "loss": 1.116,
      "step": 9590
    },
    {
      "epoch": 0.564581342350952,
      "grad_norm": 0.3805204927921295,
      "learning_rate": 0.00024404462602137017,
      "loss": 1.1192,
      "step": 9600
    },
    {
      "epoch": 0.565169447915901,
      "grad_norm": 0.31656378507614136,
      "learning_rate": 0.00024398570081709614,
      "loss": 1.0275,
      "step": 9610
    },
    {
      "epoch": 0.5657575534808498,
      "grad_norm": 0.3395709693431854,
      "learning_rate": 0.0002439267756128221,
      "loss": 1.0215,
      "step": 9620
    },
    {
      "epoch": 0.5663456590457987,
      "grad_norm": 0.3639974594116211,
      "learning_rate": 0.00024386785040854807,
      "loss": 1.0821,
      "step": 9630
    },
    {
      "epoch": 0.5669337646107476,
      "grad_norm": 0.32313668727874756,
      "learning_rate": 0.00024380892520427401,
      "loss": 1.0184,
      "step": 9640
    },
    {
      "epoch": 0.5675218701756966,
      "grad_norm": 0.35888174176216125,
      "learning_rate": 0.00024375,
      "loss": 0.9818,
      "step": 9650
    },
    {
      "epoch": 0.5681099757406455,
      "grad_norm": 0.32653385400772095,
      "learning_rate": 0.00024369107479572594,
      "loss": 1.0522,
      "step": 9660
    },
    {
      "epoch": 0.5686980813055944,
      "grad_norm": 0.3333784341812134,
      "learning_rate": 0.00024363214959145188,
      "loss": 0.9828,
      "step": 9670
    },
    {
      "epoch": 0.5692861868705432,
      "grad_norm": 0.3034907579421997,
      "learning_rate": 0.00024357322438717786,
      "loss": 1.0642,
      "step": 9680
    },
    {
      "epoch": 0.5698742924354921,
      "grad_norm": 0.3304423391819,
      "learning_rate": 0.0002435142991829038,
      "loss": 0.9479,
      "step": 9690
    },
    {
      "epoch": 0.5704623980004411,
      "grad_norm": 0.3766768276691437,
      "learning_rate": 0.00024345537397862978,
      "loss": 1.0412,
      "step": 9700
    },
    {
      "epoch": 0.57105050356539,
      "grad_norm": 0.3408749997615814,
      "learning_rate": 0.00024339644877435573,
      "loss": 1.0741,
      "step": 9710
    },
    {
      "epoch": 0.5716386091303389,
      "grad_norm": 0.3026256859302521,
      "learning_rate": 0.0002433375235700817,
      "loss": 1.1125,
      "step": 9720
    },
    {
      "epoch": 0.5722267146952879,
      "grad_norm": 0.2932858467102051,
      "learning_rate": 0.00024327859836580765,
      "loss": 1.0976,
      "step": 9730
    },
    {
      "epoch": 0.5728148202602367,
      "grad_norm": 0.35932672023773193,
      "learning_rate": 0.00024321967316153363,
      "loss": 1.0077,
      "step": 9740
    },
    {
      "epoch": 0.5734029258251856,
      "grad_norm": 0.3166603147983551,
      "learning_rate": 0.00024316074795725957,
      "loss": 1.0748,
      "step": 9750
    },
    {
      "epoch": 0.5739910313901345,
      "grad_norm": 0.34924229979515076,
      "learning_rate": 0.00024310182275298552,
      "loss": 1.0764,
      "step": 9760
    },
    {
      "epoch": 0.5745791369550834,
      "grad_norm": 0.38375362753868103,
      "learning_rate": 0.0002430428975487115,
      "loss": 1.0368,
      "step": 9770
    },
    {
      "epoch": 0.5751672425200324,
      "grad_norm": 0.312300443649292,
      "learning_rate": 0.00024298397234443744,
      "loss": 1.0763,
      "step": 9780
    },
    {
      "epoch": 0.5757553480849813,
      "grad_norm": 0.34971150755882263,
      "learning_rate": 0.00024292504714016342,
      "loss": 1.0378,
      "step": 9790
    },
    {
      "epoch": 0.5763434536499301,
      "grad_norm": 0.3059251308441162,
      "learning_rate": 0.00024286612193588937,
      "loss": 1.1914,
      "step": 9800
    },
    {
      "epoch": 0.576931559214879,
      "grad_norm": 0.32290807366371155,
      "learning_rate": 0.00024280719673161534,
      "loss": 1.0469,
      "step": 9810
    },
    {
      "epoch": 0.577519664779828,
      "grad_norm": 0.335554301738739,
      "learning_rate": 0.0002427482715273413,
      "loss": 1.0165,
      "step": 9820
    },
    {
      "epoch": 0.5781077703447769,
      "grad_norm": 0.3531686067581177,
      "learning_rate": 0.00024268934632306724,
      "loss": 1.0973,
      "step": 9830
    },
    {
      "epoch": 0.5786958759097258,
      "grad_norm": 0.3184836208820343,
      "learning_rate": 0.0002426304211187932,
      "loss": 1.0895,
      "step": 9840
    },
    {
      "epoch": 0.5792839814746747,
      "grad_norm": 0.2960170805454254,
      "learning_rate": 0.00024257149591451916,
      "loss": 1.0306,
      "step": 9850
    },
    {
      "epoch": 0.5798720870396236,
      "grad_norm": 0.3083387017250061,
      "learning_rate": 0.00024251257071024508,
      "loss": 1.0139,
      "step": 9860
    },
    {
      "epoch": 0.5804601926045725,
      "grad_norm": 0.3293408751487732,
      "learning_rate": 0.00024245364550597105,
      "loss": 1.0116,
      "step": 9870
    },
    {
      "epoch": 0.5810482981695214,
      "grad_norm": 0.345039427280426,
      "learning_rate": 0.000242394720301697,
      "loss": 1.0445,
      "step": 9880
    },
    {
      "epoch": 0.5816364037344703,
      "grad_norm": 0.3378322124481201,
      "learning_rate": 0.00024233579509742298,
      "loss": 1.0896,
      "step": 9890
    },
    {
      "epoch": 0.5822245092994193,
      "grad_norm": 0.325835257768631,
      "learning_rate": 0.00024227686989314892,
      "loss": 1.0266,
      "step": 9900
    },
    {
      "epoch": 0.5828126148643682,
      "grad_norm": 0.33075520396232605,
      "learning_rate": 0.0002422179446888749,
      "loss": 0.9741,
      "step": 9910
    },
    {
      "epoch": 0.583400720429317,
      "grad_norm": 0.32678747177124023,
      "learning_rate": 0.00024215901948460085,
      "loss": 1.0329,
      "step": 9920
    },
    {
      "epoch": 0.5839888259942659,
      "grad_norm": 0.3205433785915375,
      "learning_rate": 0.0002421000942803268,
      "loss": 1.0436,
      "step": 9930
    },
    {
      "epoch": 0.5845769315592149,
      "grad_norm": 0.5574682950973511,
      "learning_rate": 0.00024204116907605277,
      "loss": 1.0696,
      "step": 9940
    },
    {
      "epoch": 0.5851650371241638,
      "grad_norm": 0.3315344750881195,
      "learning_rate": 0.00024198224387177872,
      "loss": 1.1341,
      "step": 9950
    },
    {
      "epoch": 0.5857531426891127,
      "grad_norm": 0.35923147201538086,
      "learning_rate": 0.0002419233186675047,
      "loss": 1.0277,
      "step": 9960
    },
    {
      "epoch": 0.5863412482540616,
      "grad_norm": 0.31941789388656616,
      "learning_rate": 0.00024186439346323064,
      "loss": 1.1218,
      "step": 9970
    },
    {
      "epoch": 0.5869293538190105,
      "grad_norm": 0.3134666979312897,
      "learning_rate": 0.0002418054682589566,
      "loss": 1.0443,
      "step": 9980
    },
    {
      "epoch": 0.5875174593839594,
      "grad_norm": 0.33097201585769653,
      "learning_rate": 0.00024174654305468256,
      "loss": 0.9853,
      "step": 9990
    },
    {
      "epoch": 0.5881055649489083,
      "grad_norm": 0.32694733142852783,
      "learning_rate": 0.0002416876178504085,
      "loss": 1.0618,
      "step": 10000
    },
    {
      "epoch": 0.5886936705138572,
      "grad_norm": 0.2996409833431244,
      "learning_rate": 0.00024162869264613448,
      "loss": 1.0157,
      "step": 10010
    },
    {
      "epoch": 0.5892817760788062,
      "grad_norm": 0.30325353145599365,
      "learning_rate": 0.00024157565996228784,
      "loss": 1.0669,
      "step": 10020
    },
    {
      "epoch": 0.5898698816437551,
      "grad_norm": 0.34650421142578125,
      "learning_rate": 0.0002415167347580138,
      "loss": 1.016,
      "step": 10030
    },
    {
      "epoch": 0.590457987208704,
      "grad_norm": 0.3602699935436249,
      "learning_rate": 0.00024145780955373976,
      "loss": 1.0425,
      "step": 10040
    },
    {
      "epoch": 0.5910460927736528,
      "grad_norm": 0.3815881013870239,
      "learning_rate": 0.0002413988843494657,
      "loss": 1.0243,
      "step": 10050
    },
    {
      "epoch": 0.5916341983386018,
      "grad_norm": 0.31757086515426636,
      "learning_rate": 0.00024133995914519168,
      "loss": 1.0407,
      "step": 10060
    },
    {
      "epoch": 0.5922223039035507,
      "grad_norm": 0.33143380284309387,
      "learning_rate": 0.00024128103394091763,
      "loss": 1.0527,
      "step": 10070
    },
    {
      "epoch": 0.5928104094684996,
      "grad_norm": 0.3507579267024994,
      "learning_rate": 0.0002412221087366436,
      "loss": 1.0785,
      "step": 10080
    },
    {
      "epoch": 0.5933985150334485,
      "grad_norm": 0.32976409792900085,
      "learning_rate": 0.00024116318353236955,
      "loss": 0.9161,
      "step": 10090
    },
    {
      "epoch": 0.5939866205983975,
      "grad_norm": 0.35557323694229126,
      "learning_rate": 0.0002411042583280955,
      "loss": 1.1385,
      "step": 10100
    },
    {
      "epoch": 0.5945747261633463,
      "grad_norm": 0.35407721996307373,
      "learning_rate": 0.00024104533312382148,
      "loss": 1.0832,
      "step": 10110
    },
    {
      "epoch": 0.5951628317282952,
      "grad_norm": 0.3408515155315399,
      "learning_rate": 0.00024098640791954742,
      "loss": 1.13,
      "step": 10120
    },
    {
      "epoch": 0.5957509372932441,
      "grad_norm": 0.3081120550632477,
      "learning_rate": 0.0002409274827152734,
      "loss": 1.037,
      "step": 10130
    },
    {
      "epoch": 0.5963390428581931,
      "grad_norm": 0.3057849407196045,
      "learning_rate": 0.00024086855751099935,
      "loss": 1.0433,
      "step": 10140
    },
    {
      "epoch": 0.596927148423142,
      "grad_norm": 0.34790197014808655,
      "learning_rate": 0.00024080963230672532,
      "loss": 0.9936,
      "step": 10150
    },
    {
      "epoch": 0.5975152539880909,
      "grad_norm": 0.34273090958595276,
      "learning_rate": 0.00024075070710245127,
      "loss": 1.0496,
      "step": 10160
    },
    {
      "epoch": 0.5981033595530397,
      "grad_norm": 0.3290517032146454,
      "learning_rate": 0.00024069178189817722,
      "loss": 0.9907,
      "step": 10170
    },
    {
      "epoch": 0.5986914651179887,
      "grad_norm": 0.3051109313964844,
      "learning_rate": 0.0002406328566939032,
      "loss": 0.9988,
      "step": 10180
    },
    {
      "epoch": 0.5992795706829376,
      "grad_norm": 0.2870694696903229,
      "learning_rate": 0.00024057393148962914,
      "loss": 1.0182,
      "step": 10190
    },
    {
      "epoch": 0.5998676762478865,
      "grad_norm": 0.33658045530319214,
      "learning_rate": 0.00024051500628535511,
      "loss": 1.024,
      "step": 10200
    },
    {
      "epoch": 0.6004557818128354,
      "grad_norm": 0.3334030508995056,
      "learning_rate": 0.00024045608108108106,
      "loss": 1.1006,
      "step": 10210
    },
    {
      "epoch": 0.6010438873777844,
      "grad_norm": 0.3611794114112854,
      "learning_rate": 0.00024039715587680704,
      "loss": 1.0594,
      "step": 10220
    },
    {
      "epoch": 0.6016319929427332,
      "grad_norm": 0.3406950831413269,
      "learning_rate": 0.00024033823067253298,
      "loss": 1.1313,
      "step": 10230
    },
    {
      "epoch": 0.6022200985076821,
      "grad_norm": 0.3415292501449585,
      "learning_rate": 0.00024027930546825893,
      "loss": 1.0829,
      "step": 10240
    },
    {
      "epoch": 0.602808204072631,
      "grad_norm": 0.3145488202571869,
      "learning_rate": 0.0002402203802639849,
      "loss": 0.9385,
      "step": 10250
    },
    {
      "epoch": 0.60339630963758,
      "grad_norm": 0.36486494541168213,
      "learning_rate": 0.00024016145505971085,
      "loss": 1.0786,
      "step": 10260
    },
    {
      "epoch": 0.6039844152025289,
      "grad_norm": 0.30585941672325134,
      "learning_rate": 0.00024010252985543683,
      "loss": 1.1122,
      "step": 10270
    },
    {
      "epoch": 0.6045725207674778,
      "grad_norm": 0.3201292157173157,
      "learning_rate": 0.00024004360465116278,
      "loss": 1.032,
      "step": 10280
    },
    {
      "epoch": 0.6051606263324266,
      "grad_norm": 0.3565978407859802,
      "learning_rate": 0.00023998467944688875,
      "loss": 1.1003,
      "step": 10290
    },
    {
      "epoch": 0.6057487318973755,
      "grad_norm": 0.4398830831050873,
      "learning_rate": 0.0002399257542426147,
      "loss": 1.0137,
      "step": 10300
    },
    {
      "epoch": 0.6063368374623245,
      "grad_norm": 0.3581782579421997,
      "learning_rate": 0.00023986682903834065,
      "loss": 0.9689,
      "step": 10310
    },
    {
      "epoch": 0.6069249430272734,
      "grad_norm": 0.3179527819156647,
      "learning_rate": 0.00023980790383406662,
      "loss": 1.121,
      "step": 10320
    },
    {
      "epoch": 0.6075130485922223,
      "grad_norm": 0.3201282024383545,
      "learning_rate": 0.00023974897862979257,
      "loss": 1.1183,
      "step": 10330
    },
    {
      "epoch": 0.6081011541571713,
      "grad_norm": 0.2954225242137909,
      "learning_rate": 0.00023969005342551854,
      "loss": 1.0869,
      "step": 10340
    },
    {
      "epoch": 0.6086892597221201,
      "grad_norm": 0.35765889286994934,
      "learning_rate": 0.0002396311282212445,
      "loss": 1.0257,
      "step": 10350
    },
    {
      "epoch": 0.609277365287069,
      "grad_norm": 0.30156993865966797,
      "learning_rate": 0.00023957220301697047,
      "loss": 1.0788,
      "step": 10360
    },
    {
      "epoch": 0.6098654708520179,
      "grad_norm": 0.3402600884437561,
      "learning_rate": 0.00023951327781269639,
      "loss": 1.0089,
      "step": 10370
    },
    {
      "epoch": 0.6104535764169668,
      "grad_norm": 0.38944122195243835,
      "learning_rate": 0.00023945435260842233,
      "loss": 1.0476,
      "step": 10380
    },
    {
      "epoch": 0.6110416819819158,
      "grad_norm": 0.3263886272907257,
      "learning_rate": 0.0002393954274041483,
      "loss": 1.0131,
      "step": 10390
    },
    {
      "epoch": 0.6116297875468647,
      "grad_norm": 0.34473446011543274,
      "learning_rate": 0.00023933650219987426,
      "loss": 1.0527,
      "step": 10400
    },
    {
      "epoch": 0.6122178931118135,
      "grad_norm": 0.36266961693763733,
      "learning_rate": 0.00023927757699560023,
      "loss": 1.0849,
      "step": 10410
    },
    {
      "epoch": 0.6128059986767624,
      "grad_norm": 0.2916710674762726,
      "learning_rate": 0.00023921865179132618,
      "loss": 0.9977,
      "step": 10420
    },
    {
      "epoch": 0.6133941042417114,
      "grad_norm": 0.3593098223209381,
      "learning_rate": 0.00023915972658705213,
      "loss": 0.9916,
      "step": 10430
    },
    {
      "epoch": 0.6139822098066603,
      "grad_norm": 0.34178727865219116,
      "learning_rate": 0.0002391008013827781,
      "loss": 1.0299,
      "step": 10440
    },
    {
      "epoch": 0.6145703153716092,
      "grad_norm": 0.31616392731666565,
      "learning_rate": 0.00023904187617850405,
      "loss": 1.0577,
      "step": 10450
    },
    {
      "epoch": 0.6151584209365581,
      "grad_norm": 0.31793341040611267,
      "learning_rate": 0.00023898295097423002,
      "loss": 1.0997,
      "step": 10460
    },
    {
      "epoch": 0.6157465265015071,
      "grad_norm": 0.3691152334213257,
      "learning_rate": 0.00023892402576995597,
      "loss": 1.1365,
      "step": 10470
    },
    {
      "epoch": 0.6163346320664559,
      "grad_norm": 0.3696058392524719,
      "learning_rate": 0.00023886510056568195,
      "loss": 1.1014,
      "step": 10480
    },
    {
      "epoch": 0.6169227376314048,
      "grad_norm": 0.3579210937023163,
      "learning_rate": 0.0002388061753614079,
      "loss": 1.0418,
      "step": 10490
    },
    {
      "epoch": 0.6175108431963537,
      "grad_norm": 0.35694313049316406,
      "learning_rate": 0.00023874725015713384,
      "loss": 0.9754,
      "step": 10500
    },
    {
      "epoch": 0.6180989487613027,
      "grad_norm": 0.35352715849876404,
      "learning_rate": 0.00023868832495285982,
      "loss": 1.0907,
      "step": 10510
    },
    {
      "epoch": 0.6186870543262516,
      "grad_norm": 0.3622708022594452,
      "learning_rate": 0.00023862939974858576,
      "loss": 1.0207,
      "step": 10520
    },
    {
      "epoch": 0.6192751598912005,
      "grad_norm": 0.3247079849243164,
      "learning_rate": 0.00023857047454431174,
      "loss": 1.0674,
      "step": 10530
    },
    {
      "epoch": 0.6198632654561493,
      "grad_norm": 0.3806007504463196,
      "learning_rate": 0.00023851154934003769,
      "loss": 1.1194,
      "step": 10540
    },
    {
      "epoch": 0.6204513710210983,
      "grad_norm": 0.28105270862579346,
      "learning_rate": 0.00023845262413576366,
      "loss": 1.0042,
      "step": 10550
    },
    {
      "epoch": 0.6210394765860472,
      "grad_norm": 0.32099124789237976,
      "learning_rate": 0.0002383936989314896,
      "loss": 1.0595,
      "step": 10560
    },
    {
      "epoch": 0.6216275821509961,
      "grad_norm": 0.3582892119884491,
      "learning_rate": 0.00023833477372721556,
      "loss": 1.029,
      "step": 10570
    },
    {
      "epoch": 0.622215687715945,
      "grad_norm": 0.3475419878959656,
      "learning_rate": 0.00023827584852294153,
      "loss": 1.0321,
      "step": 10580
    },
    {
      "epoch": 0.622803793280894,
      "grad_norm": 0.3146158754825592,
      "learning_rate": 0.00023821692331866748,
      "loss": 0.9768,
      "step": 10590
    },
    {
      "epoch": 0.6233918988458428,
      "grad_norm": 0.34329426288604736,
      "learning_rate": 0.00023815799811439345,
      "loss": 1.1098,
      "step": 10600
    },
    {
      "epoch": 0.6239800044107917,
      "grad_norm": 0.3680782616138458,
      "learning_rate": 0.0002380990729101194,
      "loss": 1.0344,
      "step": 10610
    },
    {
      "epoch": 0.6245681099757406,
      "grad_norm": 0.3136894404888153,
      "learning_rate": 0.00023804014770584538,
      "loss": 1.021,
      "step": 10620
    },
    {
      "epoch": 0.6251562155406896,
      "grad_norm": 0.3474947512149811,
      "learning_rate": 0.00023798122250157132,
      "loss": 0.9706,
      "step": 10630
    },
    {
      "epoch": 0.6257443211056385,
      "grad_norm": 0.3332025408744812,
      "learning_rate": 0.00023792229729729727,
      "loss": 1.0849,
      "step": 10640
    },
    {
      "epoch": 0.6263324266705874,
      "grad_norm": 0.3260226249694824,
      "learning_rate": 0.00023786337209302325,
      "loss": 1.0805,
      "step": 10650
    },
    {
      "epoch": 0.6269205322355362,
      "grad_norm": 0.29082587361335754,
      "learning_rate": 0.0002378044468887492,
      "loss": 1.0389,
      "step": 10660
    },
    {
      "epoch": 0.6275086378004852,
      "grad_norm": 0.2987142503261566,
      "learning_rate": 0.00023774552168447517,
      "loss": 0.9848,
      "step": 10670
    },
    {
      "epoch": 0.6280967433654341,
      "grad_norm": 0.35913705825805664,
      "learning_rate": 0.00023768659648020112,
      "loss": 1.0721,
      "step": 10680
    },
    {
      "epoch": 0.628684848930383,
      "grad_norm": 0.33001044392585754,
      "learning_rate": 0.0002376276712759271,
      "loss": 1.038,
      "step": 10690
    },
    {
      "epoch": 0.6292729544953319,
      "grad_norm": 0.3208729922771454,
      "learning_rate": 0.00023756874607165304,
      "loss": 1.0619,
      "step": 10700
    },
    {
      "epoch": 0.6298610600602809,
      "grad_norm": 0.3623923659324646,
      "learning_rate": 0.00023750982086737899,
      "loss": 1.1566,
      "step": 10710
    },
    {
      "epoch": 0.6304491656252297,
      "grad_norm": 0.3394668400287628,
      "learning_rate": 0.00023745089566310496,
      "loss": 1.0125,
      "step": 10720
    },
    {
      "epoch": 0.6310372711901786,
      "grad_norm": 0.3312571048736572,
      "learning_rate": 0.0002373919704588309,
      "loss": 0.965,
      "step": 10730
    },
    {
      "epoch": 0.6316253767551275,
      "grad_norm": 0.34635889530181885,
      "learning_rate": 0.00023733304525455688,
      "loss": 1.1184,
      "step": 10740
    },
    {
      "epoch": 0.6322134823200765,
      "grad_norm": 0.3408621847629547,
      "learning_rate": 0.00023727412005028283,
      "loss": 1.0484,
      "step": 10750
    },
    {
      "epoch": 0.6328015878850254,
      "grad_norm": 0.3691810667514801,
      "learning_rate": 0.0002372151948460088,
      "loss": 1.1108,
      "step": 10760
    },
    {
      "epoch": 0.6333896934499743,
      "grad_norm": 0.36929240822792053,
      "learning_rate": 0.00023715626964173475,
      "loss": 1.0733,
      "step": 10770
    },
    {
      "epoch": 0.6339777990149231,
      "grad_norm": 0.30533045530319214,
      "learning_rate": 0.0002370973444374607,
      "loss": 0.9806,
      "step": 10780
    },
    {
      "epoch": 0.634565904579872,
      "grad_norm": 0.33516690135002136,
      "learning_rate": 0.00023703841923318665,
      "loss": 1.0315,
      "step": 10790
    },
    {
      "epoch": 0.635154010144821,
      "grad_norm": 0.3078573942184448,
      "learning_rate": 0.0002369794940289126,
      "loss": 1.1945,
      "step": 10800
    },
    {
      "epoch": 0.6357421157097699,
      "grad_norm": 0.37752565741539,
      "learning_rate": 0.00023692056882463854,
      "loss": 1.0555,
      "step": 10810
    },
    {
      "epoch": 0.6363302212747188,
      "grad_norm": 0.3298223912715912,
      "learning_rate": 0.00023686164362036452,
      "loss": 1.0828,
      "step": 10820
    },
    {
      "epoch": 0.6369183268396678,
      "grad_norm": 0.39074042439460754,
      "learning_rate": 0.00023680271841609047,
      "loss": 0.9648,
      "step": 10830
    },
    {
      "epoch": 0.6375064324046166,
      "grad_norm": 0.3210831582546234,
      "learning_rate": 0.00023674379321181644,
      "loss": 0.9257,
      "step": 10840
    },
    {
      "epoch": 0.6380945379695655,
      "grad_norm": 0.3727746605873108,
      "learning_rate": 0.0002366848680075424,
      "loss": 1.0842,
      "step": 10850
    },
    {
      "epoch": 0.6386826435345144,
      "grad_norm": 0.344746470451355,
      "learning_rate": 0.00023662594280326836,
      "loss": 1.0883,
      "step": 10860
    },
    {
      "epoch": 0.6392707490994634,
      "grad_norm": 0.34006497263908386,
      "learning_rate": 0.0002365670175989943,
      "loss": 1.0086,
      "step": 10870
    },
    {
      "epoch": 0.6398588546644123,
      "grad_norm": 0.2958259880542755,
      "learning_rate": 0.00023650809239472029,
      "loss": 1.0694,
      "step": 10880
    },
    {
      "epoch": 0.6404469602293612,
      "grad_norm": 0.3426581621170044,
      "learning_rate": 0.00023644916719044623,
      "loss": 1.0041,
      "step": 10890
    },
    {
      "epoch": 0.6410350657943101,
      "grad_norm": 0.39892059564590454,
      "learning_rate": 0.00023639024198617218,
      "loss": 1.0416,
      "step": 10900
    },
    {
      "epoch": 0.641623171359259,
      "grad_norm": 0.3138001263141632,
      "learning_rate": 0.00023633131678189816,
      "loss": 1.02,
      "step": 10910
    },
    {
      "epoch": 0.6422112769242079,
      "grad_norm": 0.3075653910636902,
      "learning_rate": 0.0002362723915776241,
      "loss": 1.093,
      "step": 10920
    },
    {
      "epoch": 0.6427993824891568,
      "grad_norm": 0.3650808036327362,
      "learning_rate": 0.00023621346637335008,
      "loss": 0.986,
      "step": 10930
    },
    {
      "epoch": 0.6433874880541057,
      "grad_norm": 0.3143604099750519,
      "learning_rate": 0.00023615454116907603,
      "loss": 0.995,
      "step": 10940
    },
    {
      "epoch": 0.6439755936190547,
      "grad_norm": 0.334454208612442,
      "learning_rate": 0.000236095615964802,
      "loss": 1.0292,
      "step": 10950
    },
    {
      "epoch": 0.6445636991840036,
      "grad_norm": 0.36304593086242676,
      "learning_rate": 0.00023603669076052795,
      "loss": 1.0333,
      "step": 10960
    },
    {
      "epoch": 0.6451518047489524,
      "grad_norm": 0.3820332884788513,
      "learning_rate": 0.0002359777655562539,
      "loss": 0.974,
      "step": 10970
    },
    {
      "epoch": 0.6457399103139013,
      "grad_norm": 0.30843421816825867,
      "learning_rate": 0.00023591884035197987,
      "loss": 1.0467,
      "step": 10980
    },
    {
      "epoch": 0.6463280158788502,
      "grad_norm": 0.3664749562740326,
      "learning_rate": 0.00023585991514770582,
      "loss": 1.0397,
      "step": 10990
    },
    {
      "epoch": 0.6469161214437992,
      "grad_norm": 0.31909552216529846,
      "learning_rate": 0.0002358009899434318,
      "loss": 1.0916,
      "step": 11000
    },
    {
      "epoch": 0.6475042270087481,
      "grad_norm": 0.39953312277793884,
      "learning_rate": 0.00023574206473915774,
      "loss": 0.9759,
      "step": 11010
    },
    {
      "epoch": 0.648092332573697,
      "grad_norm": 0.3562053442001343,
      "learning_rate": 0.00023568313953488372,
      "loss": 1.1173,
      "step": 11020
    },
    {
      "epoch": 0.6486804381386458,
      "grad_norm": 0.35929590463638306,
      "learning_rate": 0.00023562421433060966,
      "loss": 1.0435,
      "step": 11030
    },
    {
      "epoch": 0.6492685437035948,
      "grad_norm": 0.3725811541080475,
      "learning_rate": 0.0002355652891263356,
      "loss": 1.1125,
      "step": 11040
    },
    {
      "epoch": 0.6498566492685437,
      "grad_norm": 0.3399063050746918,
      "learning_rate": 0.00023550636392206159,
      "loss": 1.0302,
      "step": 11050
    },
    {
      "epoch": 0.6504447548334926,
      "grad_norm": 0.352439820766449,
      "learning_rate": 0.00023544743871778753,
      "loss": 1.0231,
      "step": 11060
    },
    {
      "epoch": 0.6510328603984415,
      "grad_norm": 0.3308781087398529,
      "learning_rate": 0.0002353885135135135,
      "loss": 1.0982,
      "step": 11070
    },
    {
      "epoch": 0.6516209659633905,
      "grad_norm": 0.3788602352142334,
      "learning_rate": 0.00023532958830923946,
      "loss": 1.1133,
      "step": 11080
    },
    {
      "epoch": 0.6522090715283393,
      "grad_norm": 0.32343775033950806,
      "learning_rate": 0.00023527066310496543,
      "loss": 1.0511,
      "step": 11090
    },
    {
      "epoch": 0.6527971770932882,
      "grad_norm": 0.32951733469963074,
      "learning_rate": 0.00023521173790069138,
      "loss": 1.0174,
      "step": 11100
    },
    {
      "epoch": 0.6533852826582371,
      "grad_norm": 0.337712824344635,
      "learning_rate": 0.00023515281269641733,
      "loss": 1.1283,
      "step": 11110
    },
    {
      "epoch": 0.6539733882231861,
      "grad_norm": 0.31907573342323303,
      "learning_rate": 0.0002350938874921433,
      "loss": 1.0476,
      "step": 11120
    },
    {
      "epoch": 0.654561493788135,
      "grad_norm": 0.3420347571372986,
      "learning_rate": 0.00023503496228786925,
      "loss": 1.0761,
      "step": 11130
    },
    {
      "epoch": 0.6551495993530839,
      "grad_norm": 0.3595360815525055,
      "learning_rate": 0.00023497603708359522,
      "loss": 1.1052,
      "step": 11140
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 0.33959323167800903,
      "learning_rate": 0.00023491711187932117,
      "loss": 1.0046,
      "step": 11150
    },
    {
      "epoch": 0.6563258104829817,
      "grad_norm": 0.302423357963562,
      "learning_rate": 0.00023485818667504714,
      "loss": 1.0131,
      "step": 11160
    },
    {
      "epoch": 0.6569139160479306,
      "grad_norm": 0.33591851592063904,
      "learning_rate": 0.0002347992614707731,
      "loss": 1.1257,
      "step": 11170
    },
    {
      "epoch": 0.6575020216128795,
      "grad_norm": 0.4004807770252228,
      "learning_rate": 0.00023474033626649904,
      "loss": 1.0297,
      "step": 11180
    },
    {
      "epoch": 0.6580901271778284,
      "grad_norm": 0.31042757630348206,
      "learning_rate": 0.00023468141106222501,
      "loss": 1.2079,
      "step": 11190
    },
    {
      "epoch": 0.6586782327427774,
      "grad_norm": 0.39031463861465454,
      "learning_rate": 0.00023462248585795096,
      "loss": 0.9721,
      "step": 11200
    },
    {
      "epoch": 0.6592663383077262,
      "grad_norm": 0.36105939745903015,
      "learning_rate": 0.00023456356065367688,
      "loss": 1.0681,
      "step": 11210
    },
    {
      "epoch": 0.6598544438726751,
      "grad_norm": 0.33932244777679443,
      "learning_rate": 0.00023450463544940286,
      "loss": 1.1188,
      "step": 11220
    },
    {
      "epoch": 0.660442549437624,
      "grad_norm": 0.2950565218925476,
      "learning_rate": 0.0002344457102451288,
      "loss": 1.0888,
      "step": 11230
    },
    {
      "epoch": 0.661030655002573,
      "grad_norm": 0.3489660322666168,
      "learning_rate": 0.00023438678504085478,
      "loss": 1.0677,
      "step": 11240
    },
    {
      "epoch": 0.6616187605675219,
      "grad_norm": 0.3320390284061432,
      "learning_rate": 0.00023432785983658073,
      "loss": 1.0278,
      "step": 11250
    },
    {
      "epoch": 0.6622068661324708,
      "grad_norm": 0.36373576521873474,
      "learning_rate": 0.0002342689346323067,
      "loss": 1.085,
      "step": 11260
    },
    {
      "epoch": 0.6627949716974196,
      "grad_norm": 0.27966129779815674,
      "learning_rate": 0.00023421000942803265,
      "loss": 1.186,
      "step": 11270
    },
    {
      "epoch": 0.6633830772623686,
      "grad_norm": 0.36733493208885193,
      "learning_rate": 0.00023415108422375862,
      "loss": 1.0489,
      "step": 11280
    },
    {
      "epoch": 0.6639711828273175,
      "grad_norm": 0.3351941704750061,
      "learning_rate": 0.00023409215901948457,
      "loss": 1.0803,
      "step": 11290
    },
    {
      "epoch": 0.6645592883922664,
      "grad_norm": 0.31541526317596436,
      "learning_rate": 0.00023403323381521052,
      "loss": 1.1123,
      "step": 11300
    },
    {
      "epoch": 0.6651473939572153,
      "grad_norm": 0.3123857378959656,
      "learning_rate": 0.0002339743086109365,
      "loss": 1.0487,
      "step": 11310
    },
    {
      "epoch": 0.6657354995221643,
      "grad_norm": 0.36859366297721863,
      "learning_rate": 0.00023391538340666244,
      "loss": 1.0796,
      "step": 11320
    },
    {
      "epoch": 0.6663236050871132,
      "grad_norm": 0.35724830627441406,
      "learning_rate": 0.00023385645820238842,
      "loss": 1.0735,
      "step": 11330
    },
    {
      "epoch": 0.666911710652062,
      "grad_norm": 0.27625489234924316,
      "learning_rate": 0.00023379753299811436,
      "loss": 1.0433,
      "step": 11340
    },
    {
      "epoch": 0.6674998162170109,
      "grad_norm": 0.32490479946136475,
      "learning_rate": 0.00023373860779384034,
      "loss": 1.0733,
      "step": 11350
    },
    {
      "epoch": 0.6680879217819599,
      "grad_norm": 0.367217481136322,
      "learning_rate": 0.0002336796825895663,
      "loss": 0.9514,
      "step": 11360
    },
    {
      "epoch": 0.6686760273469088,
      "grad_norm": 0.31737393140792847,
      "learning_rate": 0.00023362075738529224,
      "loss": 1.0448,
      "step": 11370
    },
    {
      "epoch": 0.6692641329118577,
      "grad_norm": 0.3287685811519623,
      "learning_rate": 0.0002335618321810182,
      "loss": 1.0607,
      "step": 11380
    },
    {
      "epoch": 0.6698522384768066,
      "grad_norm": 0.3811560273170471,
      "learning_rate": 0.00023350290697674416,
      "loss": 1.1508,
      "step": 11390
    },
    {
      "epoch": 0.6704403440417555,
      "grad_norm": 0.296783983707428,
      "learning_rate": 0.00023344398177247013,
      "loss": 0.97,
      "step": 11400
    },
    {
      "epoch": 0.6710284496067044,
      "grad_norm": 0.34790661931037903,
      "learning_rate": 0.00023338505656819608,
      "loss": 1.0319,
      "step": 11410
    },
    {
      "epoch": 0.6716165551716533,
      "grad_norm": 0.3553442656993866,
      "learning_rate": 0.00023332613136392205,
      "loss": 1.0218,
      "step": 11420
    },
    {
      "epoch": 0.6722046607366022,
      "grad_norm": 0.3439214527606964,
      "learning_rate": 0.000233267206159648,
      "loss": 1.0949,
      "step": 11430
    },
    {
      "epoch": 0.6727927663015512,
      "grad_norm": 0.3057499825954437,
      "learning_rate": 0.00023320828095537395,
      "loss": 0.995,
      "step": 11440
    },
    {
      "epoch": 0.6733808718665001,
      "grad_norm": 0.33369261026382446,
      "learning_rate": 0.00023314935575109992,
      "loss": 1.0319,
      "step": 11450
    },
    {
      "epoch": 0.6739689774314489,
      "grad_norm": 0.3618963062763214,
      "learning_rate": 0.00023309043054682587,
      "loss": 1.0395,
      "step": 11460
    },
    {
      "epoch": 0.6745570829963978,
      "grad_norm": 0.3164079189300537,
      "learning_rate": 0.00023303150534255185,
      "loss": 1.1011,
      "step": 11470
    },
    {
      "epoch": 0.6751451885613468,
      "grad_norm": 0.3381073474884033,
      "learning_rate": 0.0002329725801382778,
      "loss": 1.0523,
      "step": 11480
    },
    {
      "epoch": 0.6757332941262957,
      "grad_norm": 0.3262271285057068,
      "learning_rate": 0.00023291365493400377,
      "loss": 0.9857,
      "step": 11490
    },
    {
      "epoch": 0.6763213996912446,
      "grad_norm": 0.34878799319267273,
      "learning_rate": 0.00023285472972972972,
      "loss": 1.1059,
      "step": 11500
    },
    {
      "epoch": 0.6769095052561935,
      "grad_norm": 0.37783753871917725,
      "learning_rate": 0.00023279580452545566,
      "loss": 1.0023,
      "step": 11510
    },
    {
      "epoch": 0.6774976108211423,
      "grad_norm": 0.3326636552810669,
      "learning_rate": 0.00023273687932118164,
      "loss": 1.0195,
      "step": 11520
    },
    {
      "epoch": 0.6780857163860913,
      "grad_norm": 0.3248157799243927,
      "learning_rate": 0.0002326779541169076,
      "loss": 1.0255,
      "step": 11530
    },
    {
      "epoch": 0.6786738219510402,
      "grad_norm": 0.34533312916755676,
      "learning_rate": 0.00023261902891263356,
      "loss": 1.0686,
      "step": 11540
    },
    {
      "epoch": 0.6792619275159891,
      "grad_norm": 0.29470095038414,
      "learning_rate": 0.0002325601037083595,
      "loss": 1.0894,
      "step": 11550
    },
    {
      "epoch": 0.679850033080938,
      "grad_norm": 0.37812066078186035,
      "learning_rate": 0.00023250117850408548,
      "loss": 1.0904,
      "step": 11560
    },
    {
      "epoch": 0.680438138645887,
      "grad_norm": 0.3935694694519043,
      "learning_rate": 0.00023244225329981143,
      "loss": 1.0113,
      "step": 11570
    },
    {
      "epoch": 0.6810262442108358,
      "grad_norm": 0.34875988960266113,
      "learning_rate": 0.00023238332809553738,
      "loss": 1.0557,
      "step": 11580
    },
    {
      "epoch": 0.6816143497757847,
      "grad_norm": 0.36889898777008057,
      "learning_rate": 0.00023232440289126335,
      "loss": 1.0857,
      "step": 11590
    },
    {
      "epoch": 0.6822024553407336,
      "grad_norm": 0.3724684417247772,
      "learning_rate": 0.0002322654776869893,
      "loss": 0.9869,
      "step": 11600
    },
    {
      "epoch": 0.6827905609056826,
      "grad_norm": 0.30557864904403687,
      "learning_rate": 0.00023220655248271528,
      "loss": 1.1471,
      "step": 11610
    },
    {
      "epoch": 0.6833786664706315,
      "grad_norm": 0.2850455939769745,
      "learning_rate": 0.0002321476272784412,
      "loss": 0.9181,
      "step": 11620
    },
    {
      "epoch": 0.6839667720355804,
      "grad_norm": 0.3232835531234741,
      "learning_rate": 0.00023208870207416714,
      "loss": 0.889,
      "step": 11630
    },
    {
      "epoch": 0.6845548776005292,
      "grad_norm": 0.2871328890323639,
      "learning_rate": 0.00023202977686989312,
      "loss": 1.036,
      "step": 11640
    },
    {
      "epoch": 0.6851429831654782,
      "grad_norm": 0.3074128329753876,
      "learning_rate": 0.00023197085166561907,
      "loss": 1.0586,
      "step": 11650
    },
    {
      "epoch": 0.6857310887304271,
      "grad_norm": 0.41531097888946533,
      "learning_rate": 0.00023191192646134504,
      "loss": 1.0463,
      "step": 11660
    },
    {
      "epoch": 0.686319194295376,
      "grad_norm": 0.29555708169937134,
      "learning_rate": 0.000231853001257071,
      "loss": 1.0525,
      "step": 11670
    },
    {
      "epoch": 0.686907299860325,
      "grad_norm": 0.33164942264556885,
      "learning_rate": 0.00023179407605279696,
      "loss": 0.9648,
      "step": 11680
    },
    {
      "epoch": 0.6874954054252739,
      "grad_norm": 0.3366042375564575,
      "learning_rate": 0.0002317351508485229,
      "loss": 1.0567,
      "step": 11690
    },
    {
      "epoch": 0.6880835109902227,
      "grad_norm": 0.3156481981277466,
      "learning_rate": 0.00023167622564424886,
      "loss": 0.9839,
      "step": 11700
    },
    {
      "epoch": 0.6886716165551716,
      "grad_norm": 0.3790990710258484,
      "learning_rate": 0.00023161730043997483,
      "loss": 0.9647,
      "step": 11710
    },
    {
      "epoch": 0.6892597221201205,
      "grad_norm": 0.36984091997146606,
      "learning_rate": 0.00023155837523570078,
      "loss": 1.0801,
      "step": 11720
    },
    {
      "epoch": 0.6898478276850695,
      "grad_norm": 0.32335785031318665,
      "learning_rate": 0.00023149945003142676,
      "loss": 1.0848,
      "step": 11730
    },
    {
      "epoch": 0.6904359332500184,
      "grad_norm": 0.2950368821620941,
      "learning_rate": 0.0002314405248271527,
      "loss": 1.0822,
      "step": 11740
    },
    {
      "epoch": 0.6910240388149673,
      "grad_norm": 0.34259480237960815,
      "learning_rate": 0.00023138159962287868,
      "loss": 1.0314,
      "step": 11750
    },
    {
      "epoch": 0.6916121443799162,
      "grad_norm": 0.34185662865638733,
      "learning_rate": 0.00023132267441860463,
      "loss": 1.0572,
      "step": 11760
    },
    {
      "epoch": 0.6922002499448651,
      "grad_norm": 0.3223971426486969,
      "learning_rate": 0.00023126374921433057,
      "loss": 1.0301,
      "step": 11770
    },
    {
      "epoch": 0.692788355509814,
      "grad_norm": 0.37332162261009216,
      "learning_rate": 0.00023120482401005655,
      "loss": 1.0512,
      "step": 11780
    },
    {
      "epoch": 0.6933764610747629,
      "grad_norm": 0.35330265760421753,
      "learning_rate": 0.0002311458988057825,
      "loss": 0.9854,
      "step": 11790
    },
    {
      "epoch": 0.6939645666397118,
      "grad_norm": 0.341996967792511,
      "learning_rate": 0.00023108697360150847,
      "loss": 1.09,
      "step": 11800
    },
    {
      "epoch": 0.6945526722046608,
      "grad_norm": 0.34954097867012024,
      "learning_rate": 0.00023102804839723442,
      "loss": 1.0396,
      "step": 11810
    },
    {
      "epoch": 0.6951407777696097,
      "grad_norm": 0.3329828083515167,
      "learning_rate": 0.0002309691231929604,
      "loss": 1.0153,
      "step": 11820
    },
    {
      "epoch": 0.6957288833345585,
      "grad_norm": 0.3441820442676544,
      "learning_rate": 0.00023091019798868634,
      "loss": 1.0443,
      "step": 11830
    },
    {
      "epoch": 0.6963169888995074,
      "grad_norm": 0.32925480604171753,
      "learning_rate": 0.0002308512727844123,
      "loss": 1.0052,
      "step": 11840
    },
    {
      "epoch": 0.6969050944644564,
      "grad_norm": 0.31330204010009766,
      "learning_rate": 0.00023079234758013826,
      "loss": 1.0856,
      "step": 11850
    },
    {
      "epoch": 0.6974932000294053,
      "grad_norm": 0.3389941453933716,
      "learning_rate": 0.0002307334223758642,
      "loss": 1.045,
      "step": 11860
    },
    {
      "epoch": 0.6980813055943542,
      "grad_norm": 0.3289443552494049,
      "learning_rate": 0.0002306744971715902,
      "loss": 1.109,
      "step": 11870
    },
    {
      "epoch": 0.6986694111593031,
      "grad_norm": 0.34615883231163025,
      "learning_rate": 0.00023061557196731613,
      "loss": 1.0458,
      "step": 11880
    },
    {
      "epoch": 0.699257516724252,
      "grad_norm": 0.34443965554237366,
      "learning_rate": 0.0002305566467630421,
      "loss": 1.0182,
      "step": 11890
    },
    {
      "epoch": 0.6998456222892009,
      "grad_norm": 0.3435761630535126,
      "learning_rate": 0.00023049772155876806,
      "loss": 0.9704,
      "step": 11900
    },
    {
      "epoch": 0.7004337278541498,
      "grad_norm": 0.32689356803894043,
      "learning_rate": 0.000230438796354494,
      "loss": 1.0812,
      "step": 11910
    },
    {
      "epoch": 0.7010218334190987,
      "grad_norm": 0.34831246733665466,
      "learning_rate": 0.00023037987115021998,
      "loss": 1.0892,
      "step": 11920
    },
    {
      "epoch": 0.7016099389840477,
      "grad_norm": 0.33250001072883606,
      "learning_rate": 0.00023032094594594593,
      "loss": 1.0344,
      "step": 11930
    },
    {
      "epoch": 0.7021980445489966,
      "grad_norm": 0.3467240631580353,
      "learning_rate": 0.0002302620207416719,
      "loss": 0.9953,
      "step": 11940
    },
    {
      "epoch": 0.7027861501139454,
      "grad_norm": 0.36230549216270447,
      "learning_rate": 0.00023020309553739785,
      "loss": 1.0507,
      "step": 11950
    },
    {
      "epoch": 0.7033742556788943,
      "grad_norm": 0.32775139808654785,
      "learning_rate": 0.00023014417033312382,
      "loss": 1.0647,
      "step": 11960
    },
    {
      "epoch": 0.7039623612438433,
      "grad_norm": 0.333306223154068,
      "learning_rate": 0.00023008524512884977,
      "loss": 1.0264,
      "step": 11970
    },
    {
      "epoch": 0.7045504668087922,
      "grad_norm": 0.2994380295276642,
      "learning_rate": 0.00023002631992457572,
      "loss": 1.0126,
      "step": 11980
    },
    {
      "epoch": 0.7051385723737411,
      "grad_norm": 0.3459926247596741,
      "learning_rate": 0.0002299673947203017,
      "loss": 1.0791,
      "step": 11990
    },
    {
      "epoch": 0.70572667793869,
      "grad_norm": 0.29073888063430786,
      "learning_rate": 0.00022990846951602764,
      "loss": 1.068,
      "step": 12000
    },
    {
      "epoch": 0.7063147835036389,
      "grad_norm": 0.3119853436946869,
      "learning_rate": 0.00022984954431175362,
      "loss": 1.0665,
      "step": 12010
    },
    {
      "epoch": 0.7069028890685878,
      "grad_norm": 0.3323444426059723,
      "learning_rate": 0.00022979651162790697,
      "loss": 1.0831,
      "step": 12020
    },
    {
      "epoch": 0.7074909946335367,
      "grad_norm": 0.3204491436481476,
      "learning_rate": 0.00022973758642363292,
      "loss": 1.0207,
      "step": 12030
    },
    {
      "epoch": 0.7080791001984856,
      "grad_norm": 0.3473706543445587,
      "learning_rate": 0.0002296786612193589,
      "loss": 1.0294,
      "step": 12040
    },
    {
      "epoch": 0.7086672057634346,
      "grad_norm": 0.3088231682777405,
      "learning_rate": 0.00022961973601508484,
      "loss": 0.9725,
      "step": 12050
    },
    {
      "epoch": 0.7092553113283835,
      "grad_norm": 0.3345648944377899,
      "learning_rate": 0.0002295608108108108,
      "loss": 1.1091,
      "step": 12060
    },
    {
      "epoch": 0.7098434168933323,
      "grad_norm": 0.3745521903038025,
      "learning_rate": 0.00022950188560653676,
      "loss": 0.9985,
      "step": 12070
    },
    {
      "epoch": 0.7104315224582812,
      "grad_norm": 0.35054531693458557,
      "learning_rate": 0.0002294429604022627,
      "loss": 1.0993,
      "step": 12080
    },
    {
      "epoch": 0.7110196280232302,
      "grad_norm": 0.29618126153945923,
      "learning_rate": 0.0002293840351979887,
      "loss": 1.0457,
      "step": 12090
    },
    {
      "epoch": 0.7116077335881791,
      "grad_norm": 0.2976970970630646,
      "learning_rate": 0.00022932510999371463,
      "loss": 1.0868,
      "step": 12100
    },
    {
      "epoch": 0.712195839153128,
      "grad_norm": 0.3224283456802368,
      "learning_rate": 0.0002292661847894406,
      "loss": 0.982,
      "step": 12110
    },
    {
      "epoch": 0.7127839447180769,
      "grad_norm": 0.32792824506759644,
      "learning_rate": 0.00022920725958516656,
      "loss": 1.0189,
      "step": 12120
    },
    {
      "epoch": 0.7133720502830257,
      "grad_norm": 0.3145402669906616,
      "learning_rate": 0.00022914833438089248,
      "loss": 1.1136,
      "step": 12130
    },
    {
      "epoch": 0.7139601558479747,
      "grad_norm": 0.33222106099128723,
      "learning_rate": 0.00022908940917661845,
      "loss": 1.1471,
      "step": 12140
    },
    {
      "epoch": 0.7145482614129236,
      "grad_norm": 0.3460688591003418,
      "learning_rate": 0.0002290304839723444,
      "loss": 1.1413,
      "step": 12150
    },
    {
      "epoch": 0.7151363669778725,
      "grad_norm": 0.35311853885650635,
      "learning_rate": 0.00022897155876807037,
      "loss": 0.9995,
      "step": 12160
    },
    {
      "epoch": 0.7157244725428215,
      "grad_norm": 0.36951035261154175,
      "learning_rate": 0.00022891263356379632,
      "loss": 1.0293,
      "step": 12170
    },
    {
      "epoch": 0.7163125781077704,
      "grad_norm": 0.30896058678627014,
      "learning_rate": 0.00022885370835952227,
      "loss": 1.0796,
      "step": 12180
    },
    {
      "epoch": 0.7169006836727193,
      "grad_norm": 0.31745585799217224,
      "learning_rate": 0.00022879478315524824,
      "loss": 1.091,
      "step": 12190
    },
    {
      "epoch": 0.7174887892376681,
      "grad_norm": 0.3598427474498749,
      "learning_rate": 0.0002287358579509742,
      "loss": 0.9989,
      "step": 12200
    },
    {
      "epoch": 0.718076894802617,
      "grad_norm": 0.32933109998703003,
      "learning_rate": 0.00022867693274670017,
      "loss": 1.1062,
      "step": 12210
    },
    {
      "epoch": 0.718665000367566,
      "grad_norm": 0.389278382062912,
      "learning_rate": 0.00022861800754242611,
      "loss": 1.0856,
      "step": 12220
    },
    {
      "epoch": 0.7192531059325149,
      "grad_norm": 0.3424201011657715,
      "learning_rate": 0.0002285590823381521,
      "loss": 1.1134,
      "step": 12230
    },
    {
      "epoch": 0.7198412114974638,
      "grad_norm": 0.302461713552475,
      "learning_rate": 0.00022850015713387804,
      "loss": 1.0496,
      "step": 12240
    },
    {
      "epoch": 0.7204293170624128,
      "grad_norm": 0.3264782130718231,
      "learning_rate": 0.00022844123192960398,
      "loss": 1.013,
      "step": 12250
    },
    {
      "epoch": 0.7210174226273616,
      "grad_norm": 0.35063636302948,
      "learning_rate": 0.00022838230672532996,
      "loss": 1.082,
      "step": 12260
    },
    {
      "epoch": 0.7216055281923105,
      "grad_norm": 0.33430394530296326,
      "learning_rate": 0.0002283233815210559,
      "loss": 1.1241,
      "step": 12270
    },
    {
      "epoch": 0.7221936337572594,
      "grad_norm": 0.34289076924324036,
      "learning_rate": 0.00022826445631678188,
      "loss": 1.0171,
      "step": 12280
    },
    {
      "epoch": 0.7227817393222083,
      "grad_norm": 0.3404757082462311,
      "learning_rate": 0.00022820553111250783,
      "loss": 1.1422,
      "step": 12290
    },
    {
      "epoch": 0.7233698448871573,
      "grad_norm": 0.3584369421005249,
      "learning_rate": 0.0002281466059082338,
      "loss": 0.9726,
      "step": 12300
    },
    {
      "epoch": 0.7239579504521062,
      "grad_norm": 0.35024240612983704,
      "learning_rate": 0.00022808768070395975,
      "loss": 1.0847,
      "step": 12310
    },
    {
      "epoch": 0.724546056017055,
      "grad_norm": 0.3523390591144562,
      "learning_rate": 0.0002280287554996857,
      "loss": 1.0484,
      "step": 12320
    },
    {
      "epoch": 0.7251341615820039,
      "grad_norm": 0.3607694208621979,
      "learning_rate": 0.00022796983029541167,
      "loss": 1.0506,
      "step": 12330
    },
    {
      "epoch": 0.7257222671469529,
      "grad_norm": 0.31463801860809326,
      "learning_rate": 0.00022791090509113762,
      "loss": 1.0829,
      "step": 12340
    },
    {
      "epoch": 0.7263103727119018,
      "grad_norm": 0.3664943277835846,
      "learning_rate": 0.0002278519798868636,
      "loss": 0.947,
      "step": 12350
    },
    {
      "epoch": 0.7268984782768507,
      "grad_norm": 0.3502577841281891,
      "learning_rate": 0.00022779305468258954,
      "loss": 1.1353,
      "step": 12360
    },
    {
      "epoch": 0.7274865838417996,
      "grad_norm": 0.3185376524925232,
      "learning_rate": 0.00022773412947831552,
      "loss": 0.9871,
      "step": 12370
    },
    {
      "epoch": 0.7280746894067485,
      "grad_norm": 0.3368106186389923,
      "learning_rate": 0.00022767520427404147,
      "loss": 1.07,
      "step": 12380
    },
    {
      "epoch": 0.7286627949716974,
      "grad_norm": 0.3419119119644165,
      "learning_rate": 0.00022761627906976741,
      "loss": 1.0522,
      "step": 12390
    },
    {
      "epoch": 0.7292509005366463,
      "grad_norm": 0.3286425769329071,
      "learning_rate": 0.0002275573538654934,
      "loss": 0.992,
      "step": 12400
    },
    {
      "epoch": 0.7298390061015952,
      "grad_norm": 0.3150603175163269,
      "learning_rate": 0.00022749842866121934,
      "loss": 1.0187,
      "step": 12410
    },
    {
      "epoch": 0.7304271116665442,
      "grad_norm": 0.34922897815704346,
      "learning_rate": 0.0002274395034569453,
      "loss": 1.0532,
      "step": 12420
    },
    {
      "epoch": 0.7310152172314931,
      "grad_norm": 0.41816994547843933,
      "learning_rate": 0.00022738057825267126,
      "loss": 1.1083,
      "step": 12430
    },
    {
      "epoch": 0.7316033227964419,
      "grad_norm": 0.4327884018421173,
      "learning_rate": 0.00022732165304839723,
      "loss": 1.0226,
      "step": 12440
    },
    {
      "epoch": 0.7321914283613908,
      "grad_norm": 0.3317195177078247,
      "learning_rate": 0.00022726272784412318,
      "loss": 1.0499,
      "step": 12450
    },
    {
      "epoch": 0.7327795339263398,
      "grad_norm": 0.3522748649120331,
      "learning_rate": 0.00022720380263984913,
      "loss": 1.1224,
      "step": 12460
    },
    {
      "epoch": 0.7333676394912887,
      "grad_norm": 0.3491578698158264,
      "learning_rate": 0.0002271448774355751,
      "loss": 1.1279,
      "step": 12470
    },
    {
      "epoch": 0.7339557450562376,
      "grad_norm": 0.3285716474056244,
      "learning_rate": 0.00022708595223130105,
      "loss": 1.0601,
      "step": 12480
    },
    {
      "epoch": 0.7345438506211865,
      "grad_norm": 0.36052876710891724,
      "learning_rate": 0.00022702702702702703,
      "loss": 0.9145,
      "step": 12490
    },
    {
      "epoch": 0.7351319561861354,
      "grad_norm": 0.33965688943862915,
      "learning_rate": 0.00022696810182275297,
      "loss": 1.0429,
      "step": 12500
    },
    {
      "epoch": 0.7357200617510843,
      "grad_norm": 0.3390500247478485,
      "learning_rate": 0.00022690917661847895,
      "loss": 1.0295,
      "step": 12510
    },
    {
      "epoch": 0.7363081673160332,
      "grad_norm": 0.3511352837085724,
      "learning_rate": 0.0002268502514142049,
      "loss": 1.0296,
      "step": 12520
    },
    {
      "epoch": 0.7368962728809821,
      "grad_norm": 0.2877874970436096,
      "learning_rate": 0.00022679132620993084,
      "loss": 0.9659,
      "step": 12530
    },
    {
      "epoch": 0.7374843784459311,
      "grad_norm": 0.3148576319217682,
      "learning_rate": 0.00022673240100565682,
      "loss": 1.0519,
      "step": 12540
    },
    {
      "epoch": 0.73807248401088,
      "grad_norm": 0.3257160186767578,
      "learning_rate": 0.00022667347580138274,
      "loss": 1.0958,
      "step": 12550
    },
    {
      "epoch": 0.7386605895758288,
      "grad_norm": 0.3266076445579529,
      "learning_rate": 0.00022661455059710871,
      "loss": 1.1382,
      "step": 12560
    },
    {
      "epoch": 0.7392486951407777,
      "grad_norm": 0.31010526418685913,
      "learning_rate": 0.00022655562539283466,
      "loss": 1.0993,
      "step": 12570
    },
    {
      "epoch": 0.7398368007057267,
      "grad_norm": 0.33418864011764526,
      "learning_rate": 0.0002264967001885606,
      "loss": 1.0185,
      "step": 12580
    },
    {
      "epoch": 0.7404249062706756,
      "grad_norm": 0.36286991834640503,
      "learning_rate": 0.00022643777498428658,
      "loss": 1.0516,
      "step": 12590
    },
    {
      "epoch": 0.7410130118356245,
      "grad_norm": 0.3492390513420105,
      "learning_rate": 0.00022637884978001253,
      "loss": 1.023,
      "step": 12600
    },
    {
      "epoch": 0.7416011174005734,
      "grad_norm": 0.32262861728668213,
      "learning_rate": 0.0002263199245757385,
      "loss": 0.9519,
      "step": 12610
    },
    {
      "epoch": 0.7421892229655224,
      "grad_norm": 0.33228644728660583,
      "learning_rate": 0.00022626099937146445,
      "loss": 1.073,
      "step": 12620
    },
    {
      "epoch": 0.7427773285304712,
      "grad_norm": 0.3238873779773712,
      "learning_rate": 0.00022620207416719043,
      "loss": 1.0328,
      "step": 12630
    },
    {
      "epoch": 0.7433654340954201,
      "grad_norm": 0.3282049000263214,
      "learning_rate": 0.00022614314896291638,
      "loss": 1.0768,
      "step": 12640
    },
    {
      "epoch": 0.743953539660369,
      "grad_norm": 0.319724440574646,
      "learning_rate": 0.00022608422375864232,
      "loss": 1.0509,
      "step": 12650
    },
    {
      "epoch": 0.744541645225318,
      "grad_norm": 0.35460805892944336,
      "learning_rate": 0.0002260252985543683,
      "loss": 1.0478,
      "step": 12660
    },
    {
      "epoch": 0.7451297507902669,
      "grad_norm": 0.30466774106025696,
      "learning_rate": 0.00022596637335009425,
      "loss": 1.0458,
      "step": 12670
    },
    {
      "epoch": 0.7457178563552158,
      "grad_norm": 0.34185993671417236,
      "learning_rate": 0.00022590744814582022,
      "loss": 0.9732,
      "step": 12680
    },
    {
      "epoch": 0.7463059619201646,
      "grad_norm": 0.35625913739204407,
      "learning_rate": 0.00022584852294154617,
      "loss": 1.1246,
      "step": 12690
    },
    {
      "epoch": 0.7468940674851136,
      "grad_norm": 0.35288530588150024,
      "learning_rate": 0.00022578959773727214,
      "loss": 1.0797,
      "step": 12700
    },
    {
      "epoch": 0.7474821730500625,
      "grad_norm": 0.3085477650165558,
      "learning_rate": 0.0002257306725329981,
      "loss": 1.0084,
      "step": 12710
    },
    {
      "epoch": 0.7480702786150114,
      "grad_norm": 0.3145731985569,
      "learning_rate": 0.00022567174732872404,
      "loss": 1.1554,
      "step": 12720
    },
    {
      "epoch": 0.7486583841799603,
      "grad_norm": 0.36635690927505493,
      "learning_rate": 0.00022561282212445001,
      "loss": 1.0879,
      "step": 12730
    },
    {
      "epoch": 0.7492464897449093,
      "grad_norm": 0.3779779374599457,
      "learning_rate": 0.00022555389692017596,
      "loss": 1.0724,
      "step": 12740
    },
    {
      "epoch": 0.7498345953098581,
      "grad_norm": 0.3476412892341614,
      "learning_rate": 0.00022549497171590194,
      "loss": 1.0657,
      "step": 12750
    },
    {
      "epoch": 0.750422700874807,
      "grad_norm": 0.3169519603252411,
      "learning_rate": 0.00022543604651162788,
      "loss": 1.016,
      "step": 12760
    },
    {
      "epoch": 0.7510108064397559,
      "grad_norm": 0.29096734523773193,
      "learning_rate": 0.00022537712130735386,
      "loss": 0.995,
      "step": 12770
    },
    {
      "epoch": 0.7515989120047049,
      "grad_norm": 0.3264258801937103,
      "learning_rate": 0.0002253181961030798,
      "loss": 1.106,
      "step": 12780
    },
    {
      "epoch": 0.7521870175696538,
      "grad_norm": 0.32279136776924133,
      "learning_rate": 0.00022525927089880575,
      "loss": 1.0661,
      "step": 12790
    },
    {
      "epoch": 0.7527751231346027,
      "grad_norm": 0.35961154103279114,
      "learning_rate": 0.00022520034569453173,
      "loss": 0.9729,
      "step": 12800
    },
    {
      "epoch": 0.7533632286995515,
      "grad_norm": 0.3243245482444763,
      "learning_rate": 0.00022514142049025768,
      "loss": 1.1528,
      "step": 12810
    },
    {
      "epoch": 0.7539513342645005,
      "grad_norm": 0.2994406223297119,
      "learning_rate": 0.00022508249528598365,
      "loss": 1.0073,
      "step": 12820
    },
    {
      "epoch": 0.7545394398294494,
      "grad_norm": 0.29646560549736023,
      "learning_rate": 0.0002250235700817096,
      "loss": 1.0424,
      "step": 12830
    },
    {
      "epoch": 0.7551275453943983,
      "grad_norm": 0.3280830383300781,
      "learning_rate": 0.00022496464487743557,
      "loss": 1.0034,
      "step": 12840
    },
    {
      "epoch": 0.7557156509593472,
      "grad_norm": 0.3511027991771698,
      "learning_rate": 0.00022490571967316152,
      "loss": 1.0893,
      "step": 12850
    },
    {
      "epoch": 0.7563037565242962,
      "grad_norm": 0.3586220443248749,
      "learning_rate": 0.00022484679446888747,
      "loss": 0.9972,
      "step": 12860
    },
    {
      "epoch": 0.756891862089245,
      "grad_norm": 0.39992114901542664,
      "learning_rate": 0.00022478786926461344,
      "loss": 0.9922,
      "step": 12870
    },
    {
      "epoch": 0.7574799676541939,
      "grad_norm": 0.32923340797424316,
      "learning_rate": 0.0002247289440603394,
      "loss": 1.0983,
      "step": 12880
    },
    {
      "epoch": 0.7580680732191428,
      "grad_norm": 0.38778236508369446,
      "learning_rate": 0.00022467001885606537,
      "loss": 1.0677,
      "step": 12890
    },
    {
      "epoch": 0.7586561787840918,
      "grad_norm": 0.331729918718338,
      "learning_rate": 0.0002246110936517913,
      "loss": 1.1115,
      "step": 12900
    },
    {
      "epoch": 0.7592442843490407,
      "grad_norm": 0.31684410572052,
      "learning_rate": 0.0002245521684475173,
      "loss": 1.09,
      "step": 12910
    },
    {
      "epoch": 0.7598323899139896,
      "grad_norm": 0.29745060205459595,
      "learning_rate": 0.00022449324324324324,
      "loss": 0.9866,
      "step": 12920
    },
    {
      "epoch": 0.7604204954789384,
      "grad_norm": 0.33824825286865234,
      "learning_rate": 0.00022443431803896918,
      "loss": 1.0407,
      "step": 12930
    },
    {
      "epoch": 0.7610086010438873,
      "grad_norm": 0.32310470938682556,
      "learning_rate": 0.00022437539283469516,
      "loss": 1.1431,
      "step": 12940
    },
    {
      "epoch": 0.7615967066088363,
      "grad_norm": 0.3861705958843231,
      "learning_rate": 0.0002243164676304211,
      "loss": 0.9964,
      "step": 12950
    },
    {
      "epoch": 0.7621848121737852,
      "grad_norm": 0.31635382771492004,
      "learning_rate": 0.00022425754242614705,
      "loss": 1.072,
      "step": 12960
    },
    {
      "epoch": 0.7627729177387341,
      "grad_norm": 0.39551305770874023,
      "learning_rate": 0.000224198617221873,
      "loss": 1.1652,
      "step": 12970
    },
    {
      "epoch": 0.763361023303683,
      "grad_norm": 0.3591243624687195,
      "learning_rate": 0.00022413969201759895,
      "loss": 1.1245,
      "step": 12980
    },
    {
      "epoch": 0.7639491288686319,
      "grad_norm": 0.30460458993911743,
      "learning_rate": 0.00022408076681332492,
      "loss": 1.0608,
      "step": 12990
    },
    {
      "epoch": 0.7645372344335808,
      "grad_norm": 0.29127395153045654,
      "learning_rate": 0.00022402184160905087,
      "loss": 1.0124,
      "step": 13000
    },
    {
      "epoch": 0.7651253399985297,
      "grad_norm": 0.3862419128417969,
      "learning_rate": 0.00022396291640477685,
      "loss": 0.9696,
      "step": 13010
    },
    {
      "epoch": 0.7657134455634786,
      "grad_norm": 0.31501731276512146,
      "learning_rate": 0.0002239039912005028,
      "loss": 0.9422,
      "step": 13020
    },
    {
      "epoch": 0.7663015511284276,
      "grad_norm": 0.3500342071056366,
      "learning_rate": 0.00022384506599622877,
      "loss": 1.0901,
      "step": 13030
    },
    {
      "epoch": 0.7668896566933765,
      "grad_norm": 0.31164947152137756,
      "learning_rate": 0.00022378614079195472,
      "loss": 1.1241,
      "step": 13040
    },
    {
      "epoch": 0.7674777622583254,
      "grad_norm": 0.31678974628448486,
      "learning_rate": 0.00022372721558768066,
      "loss": 1.0613,
      "step": 13050
    },
    {
      "epoch": 0.7680658678232742,
      "grad_norm": 0.3075961470603943,
      "learning_rate": 0.00022366829038340664,
      "loss": 0.965,
      "step": 13060
    },
    {
      "epoch": 0.7686539733882232,
      "grad_norm": 0.32165542244911194,
      "learning_rate": 0.00022360936517913259,
      "loss": 1.0393,
      "step": 13070
    },
    {
      "epoch": 0.7692420789531721,
      "grad_norm": 0.29547154903411865,
      "learning_rate": 0.00022355043997485856,
      "loss": 0.9988,
      "step": 13080
    },
    {
      "epoch": 0.769830184518121,
      "grad_norm": 0.3628067970275879,
      "learning_rate": 0.0002234915147705845,
      "loss": 1.0437,
      "step": 13090
    },
    {
      "epoch": 0.77041829008307,
      "grad_norm": 0.37488898634910583,
      "learning_rate": 0.00022343258956631048,
      "loss": 0.9705,
      "step": 13100
    },
    {
      "epoch": 0.7710063956480189,
      "grad_norm": 0.33305874466896057,
      "learning_rate": 0.00022337366436203643,
      "loss": 1.0498,
      "step": 13110
    },
    {
      "epoch": 0.7715945012129677,
      "grad_norm": 0.38883817195892334,
      "learning_rate": 0.00022331473915776238,
      "loss": 1.0098,
      "step": 13120
    },
    {
      "epoch": 0.7721826067779166,
      "grad_norm": 0.33020105957984924,
      "learning_rate": 0.00022325581395348835,
      "loss": 1.0481,
      "step": 13130
    },
    {
      "epoch": 0.7727707123428655,
      "grad_norm": 0.3029105067253113,
      "learning_rate": 0.0002231968887492143,
      "loss": 0.9688,
      "step": 13140
    },
    {
      "epoch": 0.7733588179078145,
      "grad_norm": 0.33223986625671387,
      "learning_rate": 0.00022313796354494028,
      "loss": 1.1035,
      "step": 13150
    },
    {
      "epoch": 0.7739469234727634,
      "grad_norm": 0.29712823033332825,
      "learning_rate": 0.00022307903834066622,
      "loss": 1.0856,
      "step": 13160
    },
    {
      "epoch": 0.7745350290377123,
      "grad_norm": 0.31213098764419556,
      "learning_rate": 0.0002230201131363922,
      "loss": 1.0386,
      "step": 13170
    },
    {
      "epoch": 0.7751231346026611,
      "grad_norm": 0.33717361092567444,
      "learning_rate": 0.00022296118793211815,
      "loss": 1.0685,
      "step": 13180
    },
    {
      "epoch": 0.7757112401676101,
      "grad_norm": 0.41076165437698364,
      "learning_rate": 0.0002229022627278441,
      "loss": 1.0623,
      "step": 13190
    },
    {
      "epoch": 0.776299345732559,
      "grad_norm": 0.32853612303733826,
      "learning_rate": 0.00022284333752357007,
      "loss": 1.1857,
      "step": 13200
    },
    {
      "epoch": 0.7768874512975079,
      "grad_norm": 0.31549763679504395,
      "learning_rate": 0.00022278441231929602,
      "loss": 1.0487,
      "step": 13210
    },
    {
      "epoch": 0.7774755568624568,
      "grad_norm": 0.3375073969364166,
      "learning_rate": 0.000222725487115022,
      "loss": 0.9898,
      "step": 13220
    },
    {
      "epoch": 0.7780636624274058,
      "grad_norm": 0.2987048327922821,
      "learning_rate": 0.00022266656191074794,
      "loss": 0.9488,
      "step": 13230
    },
    {
      "epoch": 0.7786517679923546,
      "grad_norm": 0.29881569743156433,
      "learning_rate": 0.0002226076367064739,
      "loss": 1.0238,
      "step": 13240
    },
    {
      "epoch": 0.7792398735573035,
      "grad_norm": 0.37746575474739075,
      "learning_rate": 0.00022254871150219986,
      "loss": 1.0963,
      "step": 13250
    },
    {
      "epoch": 0.7798279791222524,
      "grad_norm": 0.2998966872692108,
      "learning_rate": 0.0002224897862979258,
      "loss": 0.9473,
      "step": 13260
    },
    {
      "epoch": 0.7804160846872014,
      "grad_norm": 0.32552245259284973,
      "learning_rate": 0.00022243086109365178,
      "loss": 1.0483,
      "step": 13270
    },
    {
      "epoch": 0.7810041902521503,
      "grad_norm": 0.34651708602905273,
      "learning_rate": 0.00022237193588937773,
      "loss": 0.9946,
      "step": 13280
    },
    {
      "epoch": 0.7815922958170992,
      "grad_norm": 0.3209000527858734,
      "learning_rate": 0.0002223130106851037,
      "loss": 0.9922,
      "step": 13290
    },
    {
      "epoch": 0.782180401382048,
      "grad_norm": 0.3336249887943268,
      "learning_rate": 0.00022225408548082965,
      "loss": 1.0934,
      "step": 13300
    },
    {
      "epoch": 0.782768506946997,
      "grad_norm": 0.32171908020973206,
      "learning_rate": 0.00022219516027655563,
      "loss": 1.0704,
      "step": 13310
    },
    {
      "epoch": 0.7833566125119459,
      "grad_norm": 0.33535531163215637,
      "learning_rate": 0.00022213623507228158,
      "loss": 1.0092,
      "step": 13320
    },
    {
      "epoch": 0.7839447180768948,
      "grad_norm": 0.3439755141735077,
      "learning_rate": 0.00022207730986800752,
      "loss": 1.0857,
      "step": 13330
    },
    {
      "epoch": 0.7845328236418437,
      "grad_norm": 0.37299656867980957,
      "learning_rate": 0.0002220183846637335,
      "loss": 1.0193,
      "step": 13340
    },
    {
      "epoch": 0.7851209292067927,
      "grad_norm": 0.3683205246925354,
      "learning_rate": 0.00022195945945945945,
      "loss": 1.1253,
      "step": 13350
    },
    {
      "epoch": 0.7857090347717415,
      "grad_norm": 0.3650701940059662,
      "learning_rate": 0.00022190053425518542,
      "loss": 1.0292,
      "step": 13360
    },
    {
      "epoch": 0.7862971403366904,
      "grad_norm": 0.31525102257728577,
      "learning_rate": 0.00022184160905091137,
      "loss": 1.0224,
      "step": 13370
    },
    {
      "epoch": 0.7868852459016393,
      "grad_norm": 0.33762383460998535,
      "learning_rate": 0.0002217826838466373,
      "loss": 1.0368,
      "step": 13380
    },
    {
      "epoch": 0.7874733514665883,
      "grad_norm": 0.306342214345932,
      "learning_rate": 0.00022172375864236326,
      "loss": 1.0765,
      "step": 13390
    },
    {
      "epoch": 0.7880614570315372,
      "grad_norm": 0.3824257254600525,
      "learning_rate": 0.0002216648334380892,
      "loss": 1.0762,
      "step": 13400
    },
    {
      "epoch": 0.7886495625964861,
      "grad_norm": 0.3139113783836365,
      "learning_rate": 0.00022160590823381519,
      "loss": 1.0582,
      "step": 13410
    },
    {
      "epoch": 0.7892376681614349,
      "grad_norm": 0.328466534614563,
      "learning_rate": 0.00022154698302954113,
      "loss": 1.023,
      "step": 13420
    },
    {
      "epoch": 0.7898257737263839,
      "grad_norm": 0.3105615973472595,
      "learning_rate": 0.0002214880578252671,
      "loss": 1.1166,
      "step": 13430
    },
    {
      "epoch": 0.7904138792913328,
      "grad_norm": 0.3182862102985382,
      "learning_rate": 0.00022142913262099306,
      "loss": 1.1243,
      "step": 13440
    },
    {
      "epoch": 0.7910019848562817,
      "grad_norm": 0.3261635899543762,
      "learning_rate": 0.000221370207416719,
      "loss": 1.1004,
      "step": 13450
    },
    {
      "epoch": 0.7915900904212306,
      "grad_norm": 0.3660684823989868,
      "learning_rate": 0.00022131128221244498,
      "loss": 1.0654,
      "step": 13460
    },
    {
      "epoch": 0.7921781959861796,
      "grad_norm": 0.3589000999927521,
      "learning_rate": 0.00022125235700817093,
      "loss": 1.165,
      "step": 13470
    },
    {
      "epoch": 0.7927663015511285,
      "grad_norm": 0.2749186158180237,
      "learning_rate": 0.0002211934318038969,
      "loss": 1.0757,
      "step": 13480
    },
    {
      "epoch": 0.7933544071160773,
      "grad_norm": 0.35569214820861816,
      "learning_rate": 0.00022113450659962285,
      "loss": 1.0823,
      "step": 13490
    },
    {
      "epoch": 0.7939425126810262,
      "grad_norm": 0.3291437327861786,
      "learning_rate": 0.00022107558139534882,
      "loss": 1.0256,
      "step": 13500
    },
    {
      "epoch": 0.7945306182459752,
      "grad_norm": 0.3625227212905884,
      "learning_rate": 0.00022101665619107477,
      "loss": 0.9958,
      "step": 13510
    },
    {
      "epoch": 0.7951187238109241,
      "grad_norm": 0.34718024730682373,
      "learning_rate": 0.00022095773098680072,
      "loss": 1.0377,
      "step": 13520
    },
    {
      "epoch": 0.795706829375873,
      "grad_norm": 0.3326761722564697,
      "learning_rate": 0.0002208988057825267,
      "loss": 1.0756,
      "step": 13530
    },
    {
      "epoch": 0.7962949349408219,
      "grad_norm": 0.36540859937667847,
      "learning_rate": 0.00022083988057825264,
      "loss": 1.0927,
      "step": 13540
    },
    {
      "epoch": 0.7968830405057707,
      "grad_norm": 0.33518272638320923,
      "learning_rate": 0.00022078095537397862,
      "loss": 1.0719,
      "step": 13550
    },
    {
      "epoch": 0.7974711460707197,
      "grad_norm": 0.3381972312927246,
      "learning_rate": 0.00022072203016970456,
      "loss": 0.9372,
      "step": 13560
    },
    {
      "epoch": 0.7980592516356686,
      "grad_norm": 0.365575909614563,
      "learning_rate": 0.00022066310496543054,
      "loss": 1.0216,
      "step": 13570
    },
    {
      "epoch": 0.7986473572006175,
      "grad_norm": 0.351839154958725,
      "learning_rate": 0.00022060417976115649,
      "loss": 1.0229,
      "step": 13580
    },
    {
      "epoch": 0.7992354627655665,
      "grad_norm": 0.3517919182777405,
      "learning_rate": 0.00022054525455688243,
      "loss": 1.0983,
      "step": 13590
    },
    {
      "epoch": 0.7998235683305154,
      "grad_norm": 0.3228565454483032,
      "learning_rate": 0.0002204863293526084,
      "loss": 1.0618,
      "step": 13600
    },
    {
      "epoch": 0.8004116738954642,
      "grad_norm": 0.34240666031837463,
      "learning_rate": 0.00022042740414833436,
      "loss": 1.0696,
      "step": 13610
    },
    {
      "epoch": 0.8009997794604131,
      "grad_norm": 0.3409767746925354,
      "learning_rate": 0.00022036847894406033,
      "loss": 1.0175,
      "step": 13620
    },
    {
      "epoch": 0.801587885025362,
      "grad_norm": 0.35539618134498596,
      "learning_rate": 0.00022030955373978628,
      "loss": 1.0881,
      "step": 13630
    },
    {
      "epoch": 0.802175990590311,
      "grad_norm": 0.3341321647167206,
      "learning_rate": 0.00022025062853551225,
      "loss": 0.9731,
      "step": 13640
    },
    {
      "epoch": 0.8027640961552599,
      "grad_norm": 0.3278583884239197,
      "learning_rate": 0.0002201917033312382,
      "loss": 1.0775,
      "step": 13650
    },
    {
      "epoch": 0.8033522017202088,
      "grad_norm": 0.32424473762512207,
      "learning_rate": 0.00022013277812696415,
      "loss": 1.0963,
      "step": 13660
    },
    {
      "epoch": 0.8039403072851576,
      "grad_norm": 0.39165782928466797,
      "learning_rate": 0.00022007385292269012,
      "loss": 1.0263,
      "step": 13670
    },
    {
      "epoch": 0.8045284128501066,
      "grad_norm": 0.32294484972953796,
      "learning_rate": 0.00022001492771841607,
      "loss": 1.0512,
      "step": 13680
    },
    {
      "epoch": 0.8051165184150555,
      "grad_norm": 0.2853296101093292,
      "learning_rate": 0.00021995600251414204,
      "loss": 1.0953,
      "step": 13690
    },
    {
      "epoch": 0.8057046239800044,
      "grad_norm": 0.32103198766708374,
      "learning_rate": 0.000219897077309868,
      "loss": 1.0395,
      "step": 13700
    },
    {
      "epoch": 0.8062927295449533,
      "grad_norm": 0.3218367397785187,
      "learning_rate": 0.00021983815210559397,
      "loss": 1.0604,
      "step": 13710
    },
    {
      "epoch": 0.8068808351099023,
      "grad_norm": 0.3080589175224304,
      "learning_rate": 0.00021977922690131991,
      "loss": 1.0821,
      "step": 13720
    },
    {
      "epoch": 0.8074689406748511,
      "grad_norm": 0.3061177134513855,
      "learning_rate": 0.00021972030169704586,
      "loss": 1.035,
      "step": 13730
    },
    {
      "epoch": 0.8080570462398,
      "grad_norm": 0.37079814076423645,
      "learning_rate": 0.00021966137649277184,
      "loss": 1.0328,
      "step": 13740
    },
    {
      "epoch": 0.8086451518047489,
      "grad_norm": 0.3475204110145569,
      "learning_rate": 0.00021960245128849778,
      "loss": 1.0821,
      "step": 13750
    },
    {
      "epoch": 0.8092332573696979,
      "grad_norm": 0.3107881546020508,
      "learning_rate": 0.00021954352608422376,
      "loss": 1.0118,
      "step": 13760
    },
    {
      "epoch": 0.8098213629346468,
      "grad_norm": 0.33297157287597656,
      "learning_rate": 0.0002194846008799497,
      "loss": 1.0124,
      "step": 13770
    },
    {
      "epoch": 0.8104094684995957,
      "grad_norm": 0.38896986842155457,
      "learning_rate": 0.00021942567567567568,
      "loss": 0.9642,
      "step": 13780
    },
    {
      "epoch": 0.8109975740645445,
      "grad_norm": 0.31285712122917175,
      "learning_rate": 0.00021936675047140163,
      "loss": 0.9885,
      "step": 13790
    },
    {
      "epoch": 0.8115856796294935,
      "grad_norm": 0.31945469975471497,
      "learning_rate": 0.00021930782526712755,
      "loss": 1.0166,
      "step": 13800
    },
    {
      "epoch": 0.8121737851944424,
      "grad_norm": 0.33236265182495117,
      "learning_rate": 0.00021924890006285352,
      "loss": 1.0606,
      "step": 13810
    },
    {
      "epoch": 0.8127618907593913,
      "grad_norm": 0.3178788423538208,
      "learning_rate": 0.00021918997485857947,
      "loss": 1.1527,
      "step": 13820
    },
    {
      "epoch": 0.8133499963243402,
      "grad_norm": 0.41734349727630615,
      "learning_rate": 0.00021913104965430545,
      "loss": 1.043,
      "step": 13830
    },
    {
      "epoch": 0.8139381018892892,
      "grad_norm": 0.3594609797000885,
      "learning_rate": 0.0002190721244500314,
      "loss": 1.0095,
      "step": 13840
    },
    {
      "epoch": 0.814526207454238,
      "grad_norm": 0.3569348156452179,
      "learning_rate": 0.00021901319924575734,
      "loss": 1.0756,
      "step": 13850
    },
    {
      "epoch": 0.8151143130191869,
      "grad_norm": 0.34378868341445923,
      "learning_rate": 0.00021895427404148332,
      "loss": 1.0683,
      "step": 13860
    },
    {
      "epoch": 0.8157024185841358,
      "grad_norm": 0.2970857620239258,
      "learning_rate": 0.00021889534883720927,
      "loss": 1.0331,
      "step": 13870
    },
    {
      "epoch": 0.8162905241490848,
      "grad_norm": 0.3104506731033325,
      "learning_rate": 0.00021883642363293524,
      "loss": 1.0961,
      "step": 13880
    },
    {
      "epoch": 0.8168786297140337,
      "grad_norm": 0.3495126962661743,
      "learning_rate": 0.0002187774984286612,
      "loss": 1.121,
      "step": 13890
    },
    {
      "epoch": 0.8174667352789826,
      "grad_norm": 0.37432533502578735,
      "learning_rate": 0.00021871857322438716,
      "loss": 1.1264,
      "step": 13900
    },
    {
      "epoch": 0.8180548408439315,
      "grad_norm": 0.35773736238479614,
      "learning_rate": 0.0002186596480201131,
      "loss": 1.0799,
      "step": 13910
    },
    {
      "epoch": 0.8186429464088804,
      "grad_norm": 0.34821292757987976,
      "learning_rate": 0.00021860072281583906,
      "loss": 1.0145,
      "step": 13920
    },
    {
      "epoch": 0.8192310519738293,
      "grad_norm": 0.3244125247001648,
      "learning_rate": 0.00021854179761156503,
      "loss": 1.0572,
      "step": 13930
    },
    {
      "epoch": 0.8198191575387782,
      "grad_norm": 0.34042301774024963,
      "learning_rate": 0.00021848287240729098,
      "loss": 1.0603,
      "step": 13940
    },
    {
      "epoch": 0.8204072631037271,
      "grad_norm": 0.3548874855041504,
      "learning_rate": 0.00021842394720301695,
      "loss": 1.0095,
      "step": 13950
    },
    {
      "epoch": 0.8209953686686761,
      "grad_norm": 0.36234238743782043,
      "learning_rate": 0.0002183650219987429,
      "loss": 1.0319,
      "step": 13960
    },
    {
      "epoch": 0.821583474233625,
      "grad_norm": 0.33984139561653137,
      "learning_rate": 0.00021830609679446888,
      "loss": 0.942,
      "step": 13970
    },
    {
      "epoch": 0.8221715797985738,
      "grad_norm": 0.3694455027580261,
      "learning_rate": 0.00021824717159019482,
      "loss": 1.1154,
      "step": 13980
    },
    {
      "epoch": 0.8227596853635227,
      "grad_norm": 0.36654379963874817,
      "learning_rate": 0.00021818824638592077,
      "loss": 1.0463,
      "step": 13990
    },
    {
      "epoch": 0.8233477909284717,
      "grad_norm": 0.33293306827545166,
      "learning_rate": 0.00021812932118164675,
      "loss": 1.0925,
      "step": 14000
    },
    {
      "epoch": 0.8239358964934206,
      "grad_norm": 0.3391299843788147,
      "learning_rate": 0.0002180703959773727,
      "loss": 0.918,
      "step": 14010
    },
    {
      "epoch": 0.8245240020583695,
      "grad_norm": 0.36795976758003235,
      "learning_rate": 0.00021801736329352605,
      "loss": 1.0864,
      "step": 14020
    },
    {
      "epoch": 0.8251121076233184,
      "grad_norm": 0.3121790587902069,
      "learning_rate": 0.00021795843808925203,
      "loss": 1.015,
      "step": 14030
    },
    {
      "epoch": 0.8257002131882673,
      "grad_norm": 0.38020819425582886,
      "learning_rate": 0.00021789951288497797,
      "loss": 1.1013,
      "step": 14040
    },
    {
      "epoch": 0.8262883187532162,
      "grad_norm": 0.3122849762439728,
      "learning_rate": 0.00021784058768070395,
      "loss": 1.1024,
      "step": 14050
    },
    {
      "epoch": 0.8268764243181651,
      "grad_norm": 0.3241370618343353,
      "learning_rate": 0.0002177816624764299,
      "loss": 0.984,
      "step": 14060
    },
    {
      "epoch": 0.827464529883114,
      "grad_norm": 0.31415224075317383,
      "learning_rate": 0.00021772273727215584,
      "loss": 1.0033,
      "step": 14070
    },
    {
      "epoch": 0.828052635448063,
      "grad_norm": 0.3475119173526764,
      "learning_rate": 0.00021766381206788182,
      "loss": 1.1109,
      "step": 14080
    },
    {
      "epoch": 0.8286407410130119,
      "grad_norm": 0.3363587260246277,
      "learning_rate": 0.00021760488686360777,
      "loss": 1.1006,
      "step": 14090
    },
    {
      "epoch": 0.8292288465779607,
      "grad_norm": 0.34557461738586426,
      "learning_rate": 0.00021754596165933374,
      "loss": 0.9941,
      "step": 14100
    },
    {
      "epoch": 0.8298169521429096,
      "grad_norm": 0.38404789566993713,
      "learning_rate": 0.0002174870364550597,
      "loss": 1.1244,
      "step": 14110
    },
    {
      "epoch": 0.8304050577078586,
      "grad_norm": 0.3798271119594574,
      "learning_rate": 0.00021742811125078566,
      "loss": 1.1908,
      "step": 14120
    },
    {
      "epoch": 0.8309931632728075,
      "grad_norm": 0.29869407415390015,
      "learning_rate": 0.0002173691860465116,
      "loss": 1.0367,
      "step": 14130
    },
    {
      "epoch": 0.8315812688377564,
      "grad_norm": 0.334696888923645,
      "learning_rate": 0.00021731026084223758,
      "loss": 1.109,
      "step": 14140
    },
    {
      "epoch": 0.8321693744027053,
      "grad_norm": 0.3434125781059265,
      "learning_rate": 0.00021725133563796353,
      "loss": 1.0314,
      "step": 14150
    },
    {
      "epoch": 0.8327574799676541,
      "grad_norm": 0.32685723900794983,
      "learning_rate": 0.00021719241043368948,
      "loss": 0.9805,
      "step": 14160
    },
    {
      "epoch": 0.8333455855326031,
      "grad_norm": 0.3087541162967682,
      "learning_rate": 0.00021713348522941545,
      "loss": 1.0943,
      "step": 14170
    },
    {
      "epoch": 0.833933691097552,
      "grad_norm": 0.3582649230957031,
      "learning_rate": 0.0002170745600251414,
      "loss": 1.0339,
      "step": 14180
    },
    {
      "epoch": 0.8345217966625009,
      "grad_norm": 0.3606986999511719,
      "learning_rate": 0.00021701563482086738,
      "loss": 0.9354,
      "step": 14190
    },
    {
      "epoch": 0.8351099022274499,
      "grad_norm": 0.3390083909034729,
      "learning_rate": 0.00021695670961659332,
      "loss": 1.061,
      "step": 14200
    },
    {
      "epoch": 0.8356980077923988,
      "grad_norm": 0.34984731674194336,
      "learning_rate": 0.0002168977844123193,
      "loss": 1.0032,
      "step": 14210
    },
    {
      "epoch": 0.8362861133573476,
      "grad_norm": 0.3303617238998413,
      "learning_rate": 0.00021683885920804525,
      "loss": 1.0588,
      "step": 14220
    },
    {
      "epoch": 0.8368742189222965,
      "grad_norm": 0.3418929874897003,
      "learning_rate": 0.0002167799340037712,
      "loss": 0.9956,
      "step": 14230
    },
    {
      "epoch": 0.8374623244872454,
      "grad_norm": 0.3457988500595093,
      "learning_rate": 0.00021672100879949717,
      "loss": 1.0478,
      "step": 14240
    },
    {
      "epoch": 0.8380504300521944,
      "grad_norm": 0.3161003291606903,
      "learning_rate": 0.00021666208359522312,
      "loss": 1.0977,
      "step": 14250
    },
    {
      "epoch": 0.8386385356171433,
      "grad_norm": 0.3306731879711151,
      "learning_rate": 0.0002166031583909491,
      "loss": 1.071,
      "step": 14260
    },
    {
      "epoch": 0.8392266411820922,
      "grad_norm": 0.4075222909450531,
      "learning_rate": 0.00021654423318667504,
      "loss": 1.1512,
      "step": 14270
    },
    {
      "epoch": 0.839814746747041,
      "grad_norm": 0.3011418282985687,
      "learning_rate": 0.00021648530798240101,
      "loss": 1.1684,
      "step": 14280
    },
    {
      "epoch": 0.84040285231199,
      "grad_norm": 0.37361952662467957,
      "learning_rate": 0.00021642638277812696,
      "loss": 1.0294,
      "step": 14290
    },
    {
      "epoch": 0.8409909578769389,
      "grad_norm": 0.3526269197463989,
      "learning_rate": 0.0002163674575738529,
      "loss": 1.0674,
      "step": 14300
    },
    {
      "epoch": 0.8415790634418878,
      "grad_norm": 0.3702561557292938,
      "learning_rate": 0.00021630853236957886,
      "loss": 1.1034,
      "step": 14310
    },
    {
      "epoch": 0.8421671690068367,
      "grad_norm": 0.3154326379299164,
      "learning_rate": 0.0002162496071653048,
      "loss": 0.9808,
      "step": 14320
    },
    {
      "epoch": 0.8427552745717857,
      "grad_norm": 0.28826257586479187,
      "learning_rate": 0.00021619068196103075,
      "loss": 1.0091,
      "step": 14330
    },
    {
      "epoch": 0.8433433801367345,
      "grad_norm": 0.3531170189380646,
      "learning_rate": 0.00021613175675675673,
      "loss": 1.1788,
      "step": 14340
    },
    {
      "epoch": 0.8439314857016834,
      "grad_norm": 0.3240622878074646,
      "learning_rate": 0.00021607283155248267,
      "loss": 1.0454,
      "step": 14350
    },
    {
      "epoch": 0.8445195912666323,
      "grad_norm": 0.28094765543937683,
      "learning_rate": 0.00021601390634820865,
      "loss": 0.9844,
      "step": 14360
    },
    {
      "epoch": 0.8451076968315813,
      "grad_norm": 0.31160226464271545,
      "learning_rate": 0.0002159549811439346,
      "loss": 1.0696,
      "step": 14370
    },
    {
      "epoch": 0.8456958023965302,
      "grad_norm": 0.3314354121685028,
      "learning_rate": 0.00021589605593966057,
      "loss": 1.0305,
      "step": 14380
    },
    {
      "epoch": 0.8462839079614791,
      "grad_norm": 0.36776718497276306,
      "learning_rate": 0.00021583713073538652,
      "loss": 0.9971,
      "step": 14390
    },
    {
      "epoch": 0.846872013526428,
      "grad_norm": 0.3265286982059479,
      "learning_rate": 0.00021577820553111247,
      "loss": 1.1325,
      "step": 14400
    },
    {
      "epoch": 0.8474601190913769,
      "grad_norm": 0.36574244499206543,
      "learning_rate": 0.00021571928032683844,
      "loss": 1.0408,
      "step": 14410
    },
    {
      "epoch": 0.8480482246563258,
      "grad_norm": 0.31394001841545105,
      "learning_rate": 0.0002156603551225644,
      "loss": 1.0132,
      "step": 14420
    },
    {
      "epoch": 0.8486363302212747,
      "grad_norm": 0.31264546513557434,
      "learning_rate": 0.00021560142991829036,
      "loss": 1.0293,
      "step": 14430
    },
    {
      "epoch": 0.8492244357862236,
      "grad_norm": 0.31452473998069763,
      "learning_rate": 0.0002155425047140163,
      "loss": 0.9642,
      "step": 14440
    },
    {
      "epoch": 0.8498125413511726,
      "grad_norm": 0.39089518785476685,
      "learning_rate": 0.0002154835795097423,
      "loss": 1.0236,
      "step": 14450
    },
    {
      "epoch": 0.8504006469161215,
      "grad_norm": 0.38996484875679016,
      "learning_rate": 0.00021542465430546823,
      "loss": 1.1358,
      "step": 14460
    },
    {
      "epoch": 0.8509887524810703,
      "grad_norm": 0.3064448833465576,
      "learning_rate": 0.00021536572910119418,
      "loss": 1.0825,
      "step": 14470
    },
    {
      "epoch": 0.8515768580460192,
      "grad_norm": 0.35647720098495483,
      "learning_rate": 0.00021530680389692016,
      "loss": 1.0115,
      "step": 14480
    },
    {
      "epoch": 0.8521649636109682,
      "grad_norm": 0.32881006598472595,
      "learning_rate": 0.0002152478786926461,
      "loss": 1.033,
      "step": 14490
    },
    {
      "epoch": 0.8527530691759171,
      "grad_norm": 0.2925114929676056,
      "learning_rate": 0.00021518895348837208,
      "loss": 1.0885,
      "step": 14500
    },
    {
      "epoch": 0.853341174740866,
      "grad_norm": 0.3546042740345001,
      "learning_rate": 0.00021513002828409803,
      "loss": 1.055,
      "step": 14510
    },
    {
      "epoch": 0.8539292803058149,
      "grad_norm": 0.31872305274009705,
      "learning_rate": 0.000215071103079824,
      "loss": 1.0115,
      "step": 14520
    },
    {
      "epoch": 0.8545173858707638,
      "grad_norm": 0.34387534856796265,
      "learning_rate": 0.00021501217787554995,
      "loss": 1.0266,
      "step": 14530
    },
    {
      "epoch": 0.8551054914357127,
      "grad_norm": 0.39237791299819946,
      "learning_rate": 0.00021495325267127592,
      "loss": 1.0472,
      "step": 14540
    },
    {
      "epoch": 0.8556935970006616,
      "grad_norm": 0.3606635630130768,
      "learning_rate": 0.00021489432746700187,
      "loss": 1.0727,
      "step": 14550
    },
    {
      "epoch": 0.8562817025656105,
      "grad_norm": 0.328975111246109,
      "learning_rate": 0.00021483540226272782,
      "loss": 0.9439,
      "step": 14560
    },
    {
      "epoch": 0.8568698081305595,
      "grad_norm": 0.3289596438407898,
      "learning_rate": 0.0002147764770584538,
      "loss": 1.0188,
      "step": 14570
    },
    {
      "epoch": 0.8574579136955084,
      "grad_norm": 0.3119949698448181,
      "learning_rate": 0.00021471755185417974,
      "loss": 1.0199,
      "step": 14580
    },
    {
      "epoch": 0.8580460192604572,
      "grad_norm": 0.34914860129356384,
      "learning_rate": 0.00021465862664990572,
      "loss": 1.0538,
      "step": 14590
    },
    {
      "epoch": 0.8586341248254061,
      "grad_norm": 0.3262300491333008,
      "learning_rate": 0.00021459970144563166,
      "loss": 1.0372,
      "step": 14600
    },
    {
      "epoch": 0.8592222303903551,
      "grad_norm": 0.3314288258552551,
      "learning_rate": 0.00021454077624135764,
      "loss": 1.0861,
      "step": 14610
    },
    {
      "epoch": 0.859810335955304,
      "grad_norm": 0.3417331874370575,
      "learning_rate": 0.0002144818510370836,
      "loss": 1.0629,
      "step": 14620
    },
    {
      "epoch": 0.8603984415202529,
      "grad_norm": 0.30649301409721375,
      "learning_rate": 0.00021442292583280953,
      "loss": 1.0943,
      "step": 14630
    },
    {
      "epoch": 0.8609865470852018,
      "grad_norm": 0.357417494058609,
      "learning_rate": 0.0002143640006285355,
      "loss": 1.0032,
      "step": 14640
    },
    {
      "epoch": 0.8615746526501507,
      "grad_norm": 0.31396058201789856,
      "learning_rate": 0.00021430507542426146,
      "loss": 1.0387,
      "step": 14650
    },
    {
      "epoch": 0.8621627582150996,
      "grad_norm": 0.32742729783058167,
      "learning_rate": 0.00021424615021998743,
      "loss": 1.1088,
      "step": 14660
    },
    {
      "epoch": 0.8627508637800485,
      "grad_norm": 0.30162596702575684,
      "learning_rate": 0.00021418722501571338,
      "loss": 1.023,
      "step": 14670
    },
    {
      "epoch": 0.8633389693449974,
      "grad_norm": 0.3292255401611328,
      "learning_rate": 0.00021412829981143935,
      "loss": 1.129,
      "step": 14680
    },
    {
      "epoch": 0.8639270749099464,
      "grad_norm": 0.34424376487731934,
      "learning_rate": 0.0002140693746071653,
      "loss": 1.0282,
      "step": 14690
    },
    {
      "epoch": 0.8645151804748953,
      "grad_norm": 0.3700040280818939,
      "learning_rate": 0.00021401044940289125,
      "loss": 1.0272,
      "step": 14700
    },
    {
      "epoch": 0.8651032860398441,
      "grad_norm": 0.2928077280521393,
      "learning_rate": 0.00021395152419861722,
      "loss": 1.0922,
      "step": 14710
    },
    {
      "epoch": 0.865691391604793,
      "grad_norm": 0.29715004563331604,
      "learning_rate": 0.00021389259899434314,
      "loss": 1.0193,
      "step": 14720
    },
    {
      "epoch": 0.866279497169742,
      "grad_norm": 0.3461441099643707,
      "learning_rate": 0.0002138336737900691,
      "loss": 1.033,
      "step": 14730
    },
    {
      "epoch": 0.8668676027346909,
      "grad_norm": 0.32879483699798584,
      "learning_rate": 0.00021377474858579507,
      "loss": 1.0904,
      "step": 14740
    },
    {
      "epoch": 0.8674557082996398,
      "grad_norm": 0.31530535221099854,
      "learning_rate": 0.00021371582338152101,
      "loss": 1.101,
      "step": 14750
    },
    {
      "epoch": 0.8680438138645887,
      "grad_norm": 0.33679232001304626,
      "learning_rate": 0.000213656898177247,
      "loss": 0.9532,
      "step": 14760
    },
    {
      "epoch": 0.8686319194295375,
      "grad_norm": 0.3391202986240387,
      "learning_rate": 0.00021359797297297294,
      "loss": 1.0626,
      "step": 14770
    },
    {
      "epoch": 0.8692200249944865,
      "grad_norm": 0.3370535373687744,
      "learning_rate": 0.0002135390477686989,
      "loss": 0.9806,
      "step": 14780
    },
    {
      "epoch": 0.8698081305594354,
      "grad_norm": 0.3399554193019867,
      "learning_rate": 0.00021348012256442486,
      "loss": 1.0786,
      "step": 14790
    },
    {
      "epoch": 0.8703962361243843,
      "grad_norm": 0.3593713641166687,
      "learning_rate": 0.0002134211973601508,
      "loss": 1.039,
      "step": 14800
    },
    {
      "epoch": 0.8709843416893333,
      "grad_norm": 0.3388674855232239,
      "learning_rate": 0.00021336227215587678,
      "loss": 1.0633,
      "step": 14810
    },
    {
      "epoch": 0.8715724472542822,
      "grad_norm": 0.34644633531570435,
      "learning_rate": 0.00021330334695160273,
      "loss": 0.9681,
      "step": 14820
    },
    {
      "epoch": 0.8721605528192311,
      "grad_norm": 0.29836463928222656,
      "learning_rate": 0.0002132444217473287,
      "loss": 1.1258,
      "step": 14830
    },
    {
      "epoch": 0.8727486583841799,
      "grad_norm": 0.33874350786209106,
      "learning_rate": 0.00021318549654305465,
      "loss": 1.0343,
      "step": 14840
    },
    {
      "epoch": 0.8733367639491288,
      "grad_norm": 0.3262680172920227,
      "learning_rate": 0.00021312657133878063,
      "loss": 1.1208,
      "step": 14850
    },
    {
      "epoch": 0.8739248695140778,
      "grad_norm": 0.29239770770072937,
      "learning_rate": 0.00021306764613450657,
      "loss": 1.051,
      "step": 14860
    },
    {
      "epoch": 0.8745129750790267,
      "grad_norm": 0.33983883261680603,
      "learning_rate": 0.00021300872093023252,
      "loss": 0.9835,
      "step": 14870
    },
    {
      "epoch": 0.8751010806439756,
      "grad_norm": 0.3017096519470215,
      "learning_rate": 0.0002129497957259585,
      "loss": 1.0447,
      "step": 14880
    },
    {
      "epoch": 0.8756891862089246,
      "grad_norm": 0.3880551755428314,
      "learning_rate": 0.00021289087052168444,
      "loss": 1.0244,
      "step": 14890
    },
    {
      "epoch": 0.8762772917738734,
      "grad_norm": 0.33220118284225464,
      "learning_rate": 0.00021283194531741042,
      "loss": 1.0529,
      "step": 14900
    },
    {
      "epoch": 0.8768653973388223,
      "grad_norm": 0.39268001914024353,
      "learning_rate": 0.00021277302011313637,
      "loss": 1.0853,
      "step": 14910
    },
    {
      "epoch": 0.8774535029037712,
      "grad_norm": 0.33693641424179077,
      "learning_rate": 0.00021271409490886234,
      "loss": 1.0771,
      "step": 14920
    },
    {
      "epoch": 0.8780416084687201,
      "grad_norm": 0.3070867955684662,
      "learning_rate": 0.0002126551697045883,
      "loss": 1.0244,
      "step": 14930
    },
    {
      "epoch": 0.8786297140336691,
      "grad_norm": 0.3573172092437744,
      "learning_rate": 0.00021259624450031424,
      "loss": 1.0375,
      "step": 14940
    },
    {
      "epoch": 0.879217819598618,
      "grad_norm": 0.37526482343673706,
      "learning_rate": 0.0002125373192960402,
      "loss": 1.0582,
      "step": 14950
    },
    {
      "epoch": 0.8798059251635668,
      "grad_norm": 0.32203519344329834,
      "learning_rate": 0.00021247839409176616,
      "loss": 1.0398,
      "step": 14960
    },
    {
      "epoch": 0.8803940307285157,
      "grad_norm": 0.31201961636543274,
      "learning_rate": 0.00021241946888749213,
      "loss": 1.1039,
      "step": 14970
    },
    {
      "epoch": 0.8809821362934647,
      "grad_norm": 0.4108127951622009,
      "learning_rate": 0.00021236054368321808,
      "loss": 1.0791,
      "step": 14980
    },
    {
      "epoch": 0.8815702418584136,
      "grad_norm": 0.3368382453918457,
      "learning_rate": 0.00021230161847894406,
      "loss": 1.0622,
      "step": 14990
    },
    {
      "epoch": 0.8821583474233625,
      "grad_norm": 0.3644947409629822,
      "learning_rate": 0.00021224269327467,
      "loss": 1.0461,
      "step": 15000
    },
    {
      "epoch": 0.8827464529883114,
      "grad_norm": 0.28889918327331543,
      "learning_rate": 0.00021218376807039598,
      "loss": 1.0519,
      "step": 15010
    },
    {
      "epoch": 0.8833345585532603,
      "grad_norm": 0.336911141872406,
      "learning_rate": 0.00021212484286612193,
      "loss": 0.9326,
      "step": 15020
    },
    {
      "epoch": 0.8839226641182092,
      "grad_norm": 0.3394138813018799,
      "learning_rate": 0.00021206591766184787,
      "loss": 1.0191,
      "step": 15030
    },
    {
      "epoch": 0.8845107696831581,
      "grad_norm": 0.3127237856388092,
      "learning_rate": 0.00021200699245757385,
      "loss": 0.9733,
      "step": 15040
    },
    {
      "epoch": 0.885098875248107,
      "grad_norm": 0.3707282543182373,
      "learning_rate": 0.0002119480672532998,
      "loss": 1.0186,
      "step": 15050
    },
    {
      "epoch": 0.885686980813056,
      "grad_norm": 0.37086984515190125,
      "learning_rate": 0.00021188914204902577,
      "loss": 1.045,
      "step": 15060
    },
    {
      "epoch": 0.8862750863780049,
      "grad_norm": 0.3326617181301117,
      "learning_rate": 0.00021183021684475172,
      "loss": 1.0748,
      "step": 15070
    },
    {
      "epoch": 0.8868631919429537,
      "grad_norm": 0.3468395471572876,
      "learning_rate": 0.0002117712916404777,
      "loss": 1.0597,
      "step": 15080
    },
    {
      "epoch": 0.8874512975079026,
      "grad_norm": 0.38094186782836914,
      "learning_rate": 0.00021171236643620364,
      "loss": 1.0858,
      "step": 15090
    },
    {
      "epoch": 0.8880394030728516,
      "grad_norm": 0.32913610339164734,
      "learning_rate": 0.0002116534412319296,
      "loss": 1.0223,
      "step": 15100
    },
    {
      "epoch": 0.8886275086378005,
      "grad_norm": 0.2949933111667633,
      "learning_rate": 0.00021159451602765556,
      "loss": 1.0823,
      "step": 15110
    },
    {
      "epoch": 0.8892156142027494,
      "grad_norm": 0.3930502235889435,
      "learning_rate": 0.0002115355908233815,
      "loss": 1.0743,
      "step": 15120
    },
    {
      "epoch": 0.8898037197676983,
      "grad_norm": 0.3062503933906555,
      "learning_rate": 0.00021147666561910749,
      "loss": 1.0486,
      "step": 15130
    },
    {
      "epoch": 0.8903918253326472,
      "grad_norm": 0.33903875946998596,
      "learning_rate": 0.0002114177404148334,
      "loss": 1.0135,
      "step": 15140
    },
    {
      "epoch": 0.8909799308975961,
      "grad_norm": 0.3222808241844177,
      "learning_rate": 0.00021135881521055935,
      "loss": 1.0223,
      "step": 15150
    },
    {
      "epoch": 0.891568036462545,
      "grad_norm": 0.30841538310050964,
      "learning_rate": 0.00021129989000628533,
      "loss": 1.0957,
      "step": 15160
    },
    {
      "epoch": 0.8921561420274939,
      "grad_norm": 0.349257230758667,
      "learning_rate": 0.00021124096480201128,
      "loss": 1.0802,
      "step": 15170
    },
    {
      "epoch": 0.8927442475924429,
      "grad_norm": 0.3220928907394409,
      "learning_rate": 0.00021118203959773725,
      "loss": 1.1266,
      "step": 15180
    },
    {
      "epoch": 0.8933323531573918,
      "grad_norm": 0.31528234481811523,
      "learning_rate": 0.0002111231143934632,
      "loss": 1.087,
      "step": 15190
    },
    {
      "epoch": 0.8939204587223406,
      "grad_norm": 0.3485638499259949,
      "learning_rate": 0.00021106418918918915,
      "loss": 1.1152,
      "step": 15200
    },
    {
      "epoch": 0.8945085642872895,
      "grad_norm": 0.3084618151187897,
      "learning_rate": 0.00021100526398491512,
      "loss": 1.0297,
      "step": 15210
    },
    {
      "epoch": 0.8950966698522385,
      "grad_norm": 0.4025724232196808,
      "learning_rate": 0.00021094633878064107,
      "loss": 1.0704,
      "step": 15220
    },
    {
      "epoch": 0.8956847754171874,
      "grad_norm": 0.3243289589881897,
      "learning_rate": 0.00021088741357636704,
      "loss": 1.0041,
      "step": 15230
    },
    {
      "epoch": 0.8962728809821363,
      "grad_norm": 0.35837844014167786,
      "learning_rate": 0.000210828488372093,
      "loss": 0.9813,
      "step": 15240
    },
    {
      "epoch": 0.8968609865470852,
      "grad_norm": 0.35690435767173767,
      "learning_rate": 0.00021076956316781897,
      "loss": 1.1266,
      "step": 15250
    },
    {
      "epoch": 0.8974490921120342,
      "grad_norm": 0.37343689799308777,
      "learning_rate": 0.00021071063796354491,
      "loss": 0.99,
      "step": 15260
    },
    {
      "epoch": 0.898037197676983,
      "grad_norm": 0.31833598017692566,
      "learning_rate": 0.00021065171275927086,
      "loss": 1.0421,
      "step": 15270
    },
    {
      "epoch": 0.8986253032419319,
      "grad_norm": 0.28176915645599365,
      "learning_rate": 0.00021059278755499684,
      "loss": 1.0626,
      "step": 15280
    },
    {
      "epoch": 0.8992134088068808,
      "grad_norm": 0.36389654874801636,
      "learning_rate": 0.00021053386235072278,
      "loss": 1.1123,
      "step": 15290
    },
    {
      "epoch": 0.8998015143718298,
      "grad_norm": 0.3305434286594391,
      "learning_rate": 0.00021047493714644876,
      "loss": 1.0457,
      "step": 15300
    },
    {
      "epoch": 0.9003896199367787,
      "grad_norm": 0.3716282546520233,
      "learning_rate": 0.0002104160119421747,
      "loss": 1.0391,
      "step": 15310
    },
    {
      "epoch": 0.9009777255017276,
      "grad_norm": 0.3483240306377411,
      "learning_rate": 0.00021035708673790068,
      "loss": 1.0832,
      "step": 15320
    },
    {
      "epoch": 0.9015658310666764,
      "grad_norm": 0.29704058170318604,
      "learning_rate": 0.00021029816153362663,
      "loss": 0.9903,
      "step": 15330
    },
    {
      "epoch": 0.9021539366316254,
      "grad_norm": 0.3310352563858032,
      "learning_rate": 0.00021023923632935258,
      "loss": 1.0318,
      "step": 15340
    },
    {
      "epoch": 0.9027420421965743,
      "grad_norm": 0.36893153190612793,
      "learning_rate": 0.00021018031112507855,
      "loss": 1.047,
      "step": 15350
    },
    {
      "epoch": 0.9033301477615232,
      "grad_norm": 0.3452814221382141,
      "learning_rate": 0.0002101213859208045,
      "loss": 1.0852,
      "step": 15360
    },
    {
      "epoch": 0.9039182533264721,
      "grad_norm": 0.2996656000614166,
      "learning_rate": 0.00021006246071653047,
      "loss": 1.0566,
      "step": 15370
    },
    {
      "epoch": 0.9045063588914211,
      "grad_norm": 0.3649332821369171,
      "learning_rate": 0.00021000353551225642,
      "loss": 1.0096,
      "step": 15380
    },
    {
      "epoch": 0.9050944644563699,
      "grad_norm": 0.33166560530662537,
      "learning_rate": 0.0002099446103079824,
      "loss": 1.021,
      "step": 15390
    },
    {
      "epoch": 0.9056825700213188,
      "grad_norm": 0.30239373445510864,
      "learning_rate": 0.00020988568510370834,
      "loss": 1.1359,
      "step": 15400
    },
    {
      "epoch": 0.9062706755862677,
      "grad_norm": 0.3586159348487854,
      "learning_rate": 0.00020982675989943432,
      "loss": 1.1196,
      "step": 15410
    },
    {
      "epoch": 0.9068587811512167,
      "grad_norm": 0.3705204725265503,
      "learning_rate": 0.00020976783469516027,
      "loss": 1.027,
      "step": 15420
    },
    {
      "epoch": 0.9074468867161656,
      "grad_norm": 0.4480561912059784,
      "learning_rate": 0.0002097089094908862,
      "loss": 1.0179,
      "step": 15430
    },
    {
      "epoch": 0.9080349922811145,
      "grad_norm": 0.37831857800483704,
      "learning_rate": 0.0002096499842866122,
      "loss": 1.0677,
      "step": 15440
    },
    {
      "epoch": 0.9086230978460633,
      "grad_norm": 0.3374165892601013,
      "learning_rate": 0.00020959105908233814,
      "loss": 1.0176,
      "step": 15450
    },
    {
      "epoch": 0.9092112034110122,
      "grad_norm": 0.36556264758110046,
      "learning_rate": 0.0002095321338780641,
      "loss": 1.1014,
      "step": 15460
    },
    {
      "epoch": 0.9097993089759612,
      "grad_norm": 0.3728230893611908,
      "learning_rate": 0.00020947320867379006,
      "loss": 1.0491,
      "step": 15470
    },
    {
      "epoch": 0.9103874145409101,
      "grad_norm": 0.27956417202949524,
      "learning_rate": 0.00020941428346951603,
      "loss": 1.0174,
      "step": 15480
    },
    {
      "epoch": 0.910975520105859,
      "grad_norm": 0.3186468482017517,
      "learning_rate": 0.00020935535826524198,
      "loss": 1.0461,
      "step": 15490
    },
    {
      "epoch": 0.911563625670808,
      "grad_norm": 0.30724042654037476,
      "learning_rate": 0.00020929643306096793,
      "loss": 1.0569,
      "step": 15500
    },
    {
      "epoch": 0.9121517312357568,
      "grad_norm": 0.3604380488395691,
      "learning_rate": 0.0002092375078566939,
      "loss": 1.0392,
      "step": 15510
    },
    {
      "epoch": 0.9127398368007057,
      "grad_norm": 0.34287071228027344,
      "learning_rate": 0.00020917858265241985,
      "loss": 1.0159,
      "step": 15520
    },
    {
      "epoch": 0.9133279423656546,
      "grad_norm": 0.36329418420791626,
      "learning_rate": 0.00020911965744814583,
      "loss": 1.0493,
      "step": 15530
    },
    {
      "epoch": 0.9139160479306035,
      "grad_norm": 0.2955712378025055,
      "learning_rate": 0.00020906073224387177,
      "loss": 0.9965,
      "step": 15540
    },
    {
      "epoch": 0.9145041534955525,
      "grad_norm": 0.3582795262336731,
      "learning_rate": 0.00020900180703959775,
      "loss": 1.0963,
      "step": 15550
    },
    {
      "epoch": 0.9150922590605014,
      "grad_norm": 0.34314054250717163,
      "learning_rate": 0.00020894288183532367,
      "loss": 1.0655,
      "step": 15560
    },
    {
      "epoch": 0.9156803646254502,
      "grad_norm": 0.30225446820259094,
      "learning_rate": 0.00020888395663104962,
      "loss": 1.0517,
      "step": 15570
    },
    {
      "epoch": 0.9162684701903991,
      "grad_norm": 0.3356943428516388,
      "learning_rate": 0.0002088250314267756,
      "loss": 1.1309,
      "step": 15580
    },
    {
      "epoch": 0.9168565757553481,
      "grad_norm": 0.2903801500797272,
      "learning_rate": 0.00020876610622250154,
      "loss": 1.1247,
      "step": 15590
    },
    {
      "epoch": 0.917444681320297,
      "grad_norm": 0.36694803833961487,
      "learning_rate": 0.00020870718101822749,
      "loss": 0.9968,
      "step": 15600
    },
    {
      "epoch": 0.9180327868852459,
      "grad_norm": 0.3069133162498474,
      "learning_rate": 0.00020864825581395346,
      "loss": 1.1109,
      "step": 15610
    },
    {
      "epoch": 0.9186208924501948,
      "grad_norm": 0.3313067555427551,
      "learning_rate": 0.0002085893306096794,
      "loss": 1.0726,
      "step": 15620
    },
    {
      "epoch": 0.9192089980151437,
      "grad_norm": 0.3904404938220978,
      "learning_rate": 0.00020853040540540538,
      "loss": 0.9854,
      "step": 15630
    },
    {
      "epoch": 0.9197971035800926,
      "grad_norm": 0.34367141127586365,
      "learning_rate": 0.00020847148020113133,
      "loss": 1.0179,
      "step": 15640
    },
    {
      "epoch": 0.9203852091450415,
      "grad_norm": 0.3031176030635834,
      "learning_rate": 0.0002084125549968573,
      "loss": 0.9861,
      "step": 15650
    },
    {
      "epoch": 0.9209733147099904,
      "grad_norm": 0.38525068759918213,
      "learning_rate": 0.00020835362979258325,
      "loss": 1.049,
      "step": 15660
    },
    {
      "epoch": 0.9215614202749394,
      "grad_norm": 0.35627490282058716,
      "learning_rate": 0.0002082947045883092,
      "loss": 1.0648,
      "step": 15670
    },
    {
      "epoch": 0.9221495258398883,
      "grad_norm": 0.373233437538147,
      "learning_rate": 0.00020823577938403518,
      "loss": 1.0829,
      "step": 15680
    },
    {
      "epoch": 0.9227376314048372,
      "grad_norm": 0.3179006576538086,
      "learning_rate": 0.00020817685417976112,
      "loss": 1.0054,
      "step": 15690
    },
    {
      "epoch": 0.923325736969786,
      "grad_norm": 0.3718336224555969,
      "learning_rate": 0.0002081179289754871,
      "loss": 1.0062,
      "step": 15700
    },
    {
      "epoch": 0.923913842534735,
      "grad_norm": 0.3580162525177002,
      "learning_rate": 0.00020805900377121305,
      "loss": 1.0385,
      "step": 15710
    },
    {
      "epoch": 0.9245019480996839,
      "grad_norm": 0.3415461778640747,
      "learning_rate": 0.00020800007856693902,
      "loss": 1.1334,
      "step": 15720
    },
    {
      "epoch": 0.9250900536646328,
      "grad_norm": 0.31723204255104065,
      "learning_rate": 0.00020794115336266497,
      "loss": 0.9584,
      "step": 15730
    },
    {
      "epoch": 0.9256781592295817,
      "grad_norm": 0.35055816173553467,
      "learning_rate": 0.00020788222815839092,
      "loss": 1.108,
      "step": 15740
    },
    {
      "epoch": 0.9262662647945307,
      "grad_norm": 0.33763763308525085,
      "learning_rate": 0.0002078233029541169,
      "loss": 0.9748,
      "step": 15750
    },
    {
      "epoch": 0.9268543703594795,
      "grad_norm": 0.37767234444618225,
      "learning_rate": 0.00020776437774984284,
      "loss": 1.0086,
      "step": 15760
    },
    {
      "epoch": 0.9274424759244284,
      "grad_norm": 0.38990238308906555,
      "learning_rate": 0.0002077054525455688,
      "loss": 1.1275,
      "step": 15770
    },
    {
      "epoch": 0.9280305814893773,
      "grad_norm": 0.34439796209335327,
      "learning_rate": 0.00020764652734129476,
      "loss": 1.0728,
      "step": 15780
    },
    {
      "epoch": 0.9286186870543263,
      "grad_norm": 0.4073192775249481,
      "learning_rate": 0.00020758760213702074,
      "loss": 0.9993,
      "step": 15790
    },
    {
      "epoch": 0.9292067926192752,
      "grad_norm": 0.3382709324359894,
      "learning_rate": 0.00020752867693274668,
      "loss": 1.0019,
      "step": 15800
    },
    {
      "epoch": 0.9297948981842241,
      "grad_norm": 0.360167920589447,
      "learning_rate": 0.00020746975172847263,
      "loss": 1.1472,
      "step": 15810
    },
    {
      "epoch": 0.9303830037491729,
      "grad_norm": 0.294215589761734,
      "learning_rate": 0.0002074108265241986,
      "loss": 1.0987,
      "step": 15820
    },
    {
      "epoch": 0.9309711093141219,
      "grad_norm": 0.3353247344493866,
      "learning_rate": 0.00020735190131992455,
      "loss": 1.1205,
      "step": 15830
    },
    {
      "epoch": 0.9315592148790708,
      "grad_norm": 0.34294307231903076,
      "learning_rate": 0.00020729297611565053,
      "loss": 1.0642,
      "step": 15840
    },
    {
      "epoch": 0.9321473204440197,
      "grad_norm": 0.3388543128967285,
      "learning_rate": 0.00020723405091137648,
      "loss": 1.0473,
      "step": 15850
    },
    {
      "epoch": 0.9327354260089686,
      "grad_norm": 0.3416450619697571,
      "learning_rate": 0.00020717512570710245,
      "loss": 1.0847,
      "step": 15860
    },
    {
      "epoch": 0.9333235315739176,
      "grad_norm": 0.30306199193000793,
      "learning_rate": 0.0002071162005028284,
      "loss": 1.1124,
      "step": 15870
    },
    {
      "epoch": 0.9339116371388664,
      "grad_norm": 0.31903284788131714,
      "learning_rate": 0.00020705727529855437,
      "loss": 1.0707,
      "step": 15880
    },
    {
      "epoch": 0.9344997427038153,
      "grad_norm": 0.3601856231689453,
      "learning_rate": 0.00020699835009428032,
      "loss": 1.0693,
      "step": 15890
    },
    {
      "epoch": 0.9350878482687642,
      "grad_norm": 0.3166513741016388,
      "learning_rate": 0.00020693942489000627,
      "loss": 1.0812,
      "step": 15900
    },
    {
      "epoch": 0.9356759538337132,
      "grad_norm": 0.29708218574523926,
      "learning_rate": 0.00020688049968573224,
      "loss": 0.9708,
      "step": 15910
    },
    {
      "epoch": 0.9362640593986621,
      "grad_norm": 0.35544708371162415,
      "learning_rate": 0.0002068215744814582,
      "loss": 1.0501,
      "step": 15920
    },
    {
      "epoch": 0.936852164963611,
      "grad_norm": 0.3181348145008087,
      "learning_rate": 0.00020676264927718416,
      "loss": 1.0205,
      "step": 15930
    },
    {
      "epoch": 0.9374402705285598,
      "grad_norm": 0.3537976145744324,
      "learning_rate": 0.0002067037240729101,
      "loss": 1.0517,
      "step": 15940
    },
    {
      "epoch": 0.9380283760935088,
      "grad_norm": 0.3403588533401489,
      "learning_rate": 0.0002066447988686361,
      "loss": 0.996,
      "step": 15950
    },
    {
      "epoch": 0.9386164816584577,
      "grad_norm": 0.30390921235084534,
      "learning_rate": 0.00020658587366436203,
      "loss": 1.0491,
      "step": 15960
    },
    {
      "epoch": 0.9392045872234066,
      "grad_norm": 0.3366653621196747,
      "learning_rate": 0.00020652694846008798,
      "loss": 1.0587,
      "step": 15970
    },
    {
      "epoch": 0.9397926927883555,
      "grad_norm": 0.2926473021507263,
      "learning_rate": 0.00020646802325581393,
      "loss": 0.9769,
      "step": 15980
    },
    {
      "epoch": 0.9403807983533045,
      "grad_norm": 0.39133092761039734,
      "learning_rate": 0.00020640909805153988,
      "loss": 0.9244,
      "step": 15990
    },
    {
      "epoch": 0.9409689039182533,
      "grad_norm": 0.3563808500766754,
      "learning_rate": 0.00020635017284726583,
      "loss": 0.974,
      "step": 16000
    },
    {
      "epoch": 0.9415570094832022,
      "grad_norm": 0.3340171277523041,
      "learning_rate": 0.0002062912476429918,
      "loss": 0.9793,
      "step": 16010
    },
    {
      "epoch": 0.9421451150481511,
      "grad_norm": NaN,
      "learning_rate": 0.00020623232243871775,
      "loss": 1.0814,
      "step": 16020
    },
    {
      "epoch": 0.9427332206131,
      "grad_norm": 0.343409925699234,
      "learning_rate": 0.00020617928975487116,
      "loss": 1.0722,
      "step": 16030
    },
    {
      "epoch": 0.943321326178049,
      "grad_norm": 0.31837716698646545,
      "learning_rate": 0.0002061203645505971,
      "loss": 0.9713,
      "step": 16040
    },
    {
      "epoch": 0.9439094317429979,
      "grad_norm": 0.35212138295173645,
      "learning_rate": 0.00020606143934632305,
      "loss": 1.0253,
      "step": 16050
    },
    {
      "epoch": 0.9444975373079467,
      "grad_norm": 0.354254812002182,
      "learning_rate": 0.000206002514142049,
      "loss": 0.9324,
      "step": 16060
    },
    {
      "epoch": 0.9450856428728956,
      "grad_norm": 0.327457457780838,
      "learning_rate": 0.00020594358893777495,
      "loss": 1.0468,
      "step": 16070
    },
    {
      "epoch": 0.9456737484378446,
      "grad_norm": 0.3487199544906616,
      "learning_rate": 0.00020588466373350092,
      "loss": 1.1196,
      "step": 16080
    },
    {
      "epoch": 0.9462618540027935,
      "grad_norm": 0.30777454376220703,
      "learning_rate": 0.00020582573852922687,
      "loss": 1.0882,
      "step": 16090
    },
    {
      "epoch": 0.9468499595677424,
      "grad_norm": 0.3594619631767273,
      "learning_rate": 0.00020576681332495282,
      "loss": 1.0655,
      "step": 16100
    },
    {
      "epoch": 0.9474380651326914,
      "grad_norm": 0.3529719114303589,
      "learning_rate": 0.0002057078881206788,
      "loss": 1.0438,
      "step": 16110
    },
    {
      "epoch": 0.9480261706976403,
      "grad_norm": 0.30015814304351807,
      "learning_rate": 0.00020564896291640474,
      "loss": 1.0116,
      "step": 16120
    },
    {
      "epoch": 0.9486142762625891,
      "grad_norm": 0.34352371096611023,
      "learning_rate": 0.00020559003771213072,
      "loss": 1.1188,
      "step": 16130
    },
    {
      "epoch": 0.949202381827538,
      "grad_norm": 0.33240190148353577,
      "learning_rate": 0.00020553111250785666,
      "loss": 1.0139,
      "step": 16140
    },
    {
      "epoch": 0.949790487392487,
      "grad_norm": 0.4384898245334625,
      "learning_rate": 0.00020547218730358264,
      "loss": 1.0835,
      "step": 16150
    },
    {
      "epoch": 0.9503785929574359,
      "grad_norm": 0.32126298546791077,
      "learning_rate": 0.00020541326209930859,
      "loss": 1.061,
      "step": 16160
    },
    {
      "epoch": 0.9509666985223848,
      "grad_norm": 0.368007630109787,
      "learning_rate": 0.00020535433689503453,
      "loss": 0.9854,
      "step": 16170
    },
    {
      "epoch": 0.9515548040873337,
      "grad_norm": 0.28984400629997253,
      "learning_rate": 0.0002052954116907605,
      "loss": 0.9729,
      "step": 16180
    },
    {
      "epoch": 0.9521429096522825,
      "grad_norm": 0.35942381620407104,
      "learning_rate": 0.00020523648648648646,
      "loss": 1.0628,
      "step": 16190
    },
    {
      "epoch": 0.9527310152172315,
      "grad_norm": 0.32816019654273987,
      "learning_rate": 0.00020517756128221243,
      "loss": 1.0786,
      "step": 16200
    },
    {
      "epoch": 0.9533191207821804,
      "grad_norm": 0.2960497736930847,
      "learning_rate": 0.00020511863607793838,
      "loss": 1.0726,
      "step": 16210
    },
    {
      "epoch": 0.9539072263471293,
      "grad_norm": 0.3185883164405823,
      "learning_rate": 0.00020505971087366435,
      "loss": 1.0138,
      "step": 16220
    },
    {
      "epoch": 0.9544953319120782,
      "grad_norm": 0.3717302083969116,
      "learning_rate": 0.0002050007856693903,
      "loss": 1.0968,
      "step": 16230
    },
    {
      "epoch": 0.9550834374770272,
      "grad_norm": 0.3387612998485565,
      "learning_rate": 0.00020494186046511625,
      "loss": 1.0337,
      "step": 16240
    },
    {
      "epoch": 0.955671543041976,
      "grad_norm": 0.31234508752822876,
      "learning_rate": 0.00020488293526084222,
      "loss": 1.1182,
      "step": 16250
    },
    {
      "epoch": 0.9562596486069249,
      "grad_norm": 0.3551212251186371,
      "learning_rate": 0.00020482401005656817,
      "loss": 1.0789,
      "step": 16260
    },
    {
      "epoch": 0.9568477541718738,
      "grad_norm": 0.4218168556690216,
      "learning_rate": 0.00020476508485229415,
      "loss": 0.9929,
      "step": 16270
    },
    {
      "epoch": 0.9574358597368228,
      "grad_norm": 0.30187949538230896,
      "learning_rate": 0.0002047061596480201,
      "loss": 1.1103,
      "step": 16280
    },
    {
      "epoch": 0.9580239653017717,
      "grad_norm": 0.3406587243080139,
      "learning_rate": 0.00020464723444374607,
      "loss": 1.0434,
      "step": 16290
    },
    {
      "epoch": 0.9586120708667206,
      "grad_norm": 0.30987924337387085,
      "learning_rate": 0.00020458830923947202,
      "loss": 0.982,
      "step": 16300
    },
    {
      "epoch": 0.9592001764316694,
      "grad_norm": 0.3291209042072296,
      "learning_rate": 0.00020452938403519796,
      "loss": 1.0499,
      "step": 16310
    },
    {
      "epoch": 0.9597882819966184,
      "grad_norm": 0.3920324444770813,
      "learning_rate": 0.00020447045883092394,
      "loss": 1.0449,
      "step": 16320
    },
    {
      "epoch": 0.9603763875615673,
      "grad_norm": 0.35634055733680725,
      "learning_rate": 0.00020441153362664989,
      "loss": 1.075,
      "step": 16330
    },
    {
      "epoch": 0.9609644931265162,
      "grad_norm": 0.2755732834339142,
      "learning_rate": 0.00020435260842237586,
      "loss": 0.9112,
      "step": 16340
    },
    {
      "epoch": 0.9615525986914651,
      "grad_norm": 0.3203467130661011,
      "learning_rate": 0.0002042936832181018,
      "loss": 1.0254,
      "step": 16350
    },
    {
      "epoch": 0.9621407042564141,
      "grad_norm": 0.3058554232120514,
      "learning_rate": 0.00020423475801382778,
      "loss": 1.1483,
      "step": 16360
    },
    {
      "epoch": 0.9627288098213629,
      "grad_norm": 0.3399289846420288,
      "learning_rate": 0.00020417583280955373,
      "loss": 1.0903,
      "step": 16370
    },
    {
      "epoch": 0.9633169153863118,
      "grad_norm": 0.3569810688495636,
      "learning_rate": 0.00020411690760527968,
      "loss": 1.0729,
      "step": 16380
    },
    {
      "epoch": 0.9639050209512607,
      "grad_norm": 0.36124446988105774,
      "learning_rate": 0.00020405798240100565,
      "loss": 1.0133,
      "step": 16390
    },
    {
      "epoch": 0.9644931265162097,
      "grad_norm": 0.3544175326824188,
      "learning_rate": 0.0002039990571967316,
      "loss": 1.0689,
      "step": 16400
    },
    {
      "epoch": 0.9650812320811586,
      "grad_norm": 0.38560834527015686,
      "learning_rate": 0.00020394013199245757,
      "loss": 1.0071,
      "step": 16410
    },
    {
      "epoch": 0.9656693376461075,
      "grad_norm": 0.3760964274406433,
      "learning_rate": 0.00020388120678818352,
      "loss": 1.1254,
      "step": 16420
    },
    {
      "epoch": 0.9662574432110563,
      "grad_norm": 0.34587934613227844,
      "learning_rate": 0.0002038222815839095,
      "loss": 0.9792,
      "step": 16430
    },
    {
      "epoch": 0.9668455487760053,
      "grad_norm": 0.31935158371925354,
      "learning_rate": 0.00020376335637963544,
      "loss": 1.1121,
      "step": 16440
    },
    {
      "epoch": 0.9674336543409542,
      "grad_norm": 0.3099399507045746,
      "learning_rate": 0.0002037044311753614,
      "loss": 0.9543,
      "step": 16450
    },
    {
      "epoch": 0.9680217599059031,
      "grad_norm": 0.3132788836956024,
      "learning_rate": 0.00020364550597108737,
      "loss": 0.9658,
      "step": 16460
    },
    {
      "epoch": 0.968609865470852,
      "grad_norm": 0.3256374001502991,
      "learning_rate": 0.00020358658076681331,
      "loss": 1.0938,
      "step": 16470
    },
    {
      "epoch": 0.969197971035801,
      "grad_norm": 0.3537837266921997,
      "learning_rate": 0.00020352765556253926,
      "loss": 1.108,
      "step": 16480
    },
    {
      "epoch": 0.9697860766007498,
      "grad_norm": 0.30175530910491943,
      "learning_rate": 0.0002034687303582652,
      "loss": 1.0092,
      "step": 16490
    },
    {
      "epoch": 0.9703741821656987,
      "grad_norm": 0.2969346344470978,
      "learning_rate": 0.00020340980515399116,
      "loss": 1.081,
      "step": 16500
    },
    {
      "epoch": 0.9709622877306476,
      "grad_norm": 0.32352414727211,
      "learning_rate": 0.00020335087994971713,
      "loss": 0.9582,
      "step": 16510
    },
    {
      "epoch": 0.9715503932955966,
      "grad_norm": 0.2948017120361328,
      "learning_rate": 0.00020329195474544308,
      "loss": 1.0585,
      "step": 16520
    },
    {
      "epoch": 0.9721384988605455,
      "grad_norm": 0.34994226694107056,
      "learning_rate": 0.00020323302954116906,
      "loss": 1.0031,
      "step": 16530
    },
    {
      "epoch": 0.9727266044254944,
      "grad_norm": 0.3341574966907501,
      "learning_rate": 0.000203174104336895,
      "loss": 1.1287,
      "step": 16540
    },
    {
      "epoch": 0.9733147099904433,
      "grad_norm": 0.3625199794769287,
      "learning_rate": 0.00020311517913262098,
      "loss": 1.1177,
      "step": 16550
    },
    {
      "epoch": 0.9739028155553922,
      "grad_norm": 0.33796244859695435,
      "learning_rate": 0.00020305625392834693,
      "loss": 0.9836,
      "step": 16560
    },
    {
      "epoch": 0.9744909211203411,
      "grad_norm": 0.29304757714271545,
      "learning_rate": 0.00020299732872407287,
      "loss": 0.9572,
      "step": 16570
    },
    {
      "epoch": 0.97507902668529,
      "grad_norm": 0.30054551362991333,
      "learning_rate": 0.00020293840351979885,
      "loss": 1.0389,
      "step": 16580
    },
    {
      "epoch": 0.9756671322502389,
      "grad_norm": 0.32563358545303345,
      "learning_rate": 0.0002028794783155248,
      "loss": 1.1382,
      "step": 16590
    },
    {
      "epoch": 0.9762552378151879,
      "grad_norm": 0.3218577802181244,
      "learning_rate": 0.00020282055311125077,
      "loss": 1.0373,
      "step": 16600
    },
    {
      "epoch": 0.9768433433801368,
      "grad_norm": 0.3413528800010681,
      "learning_rate": 0.00020276162790697672,
      "loss": 1.0226,
      "step": 16610
    },
    {
      "epoch": 0.9774314489450856,
      "grad_norm": 0.3454200327396393,
      "learning_rate": 0.0002027027027027027,
      "loss": 1.0261,
      "step": 16620
    },
    {
      "epoch": 0.9780195545100345,
      "grad_norm": 0.31131842732429504,
      "learning_rate": 0.00020264377749842864,
      "loss": 1.0484,
      "step": 16630
    },
    {
      "epoch": 0.9786076600749835,
      "grad_norm": 0.33037251234054565,
      "learning_rate": 0.0002025848522941546,
      "loss": 1.0966,
      "step": 16640
    },
    {
      "epoch": 0.9791957656399324,
      "grad_norm": 0.3324500024318695,
      "learning_rate": 0.00020252592708988056,
      "loss": 1.1028,
      "step": 16650
    },
    {
      "epoch": 0.9797838712048813,
      "grad_norm": 0.3176199197769165,
      "learning_rate": 0.0002024670018856065,
      "loss": 1.0123,
      "step": 16660
    },
    {
      "epoch": 0.9803719767698302,
      "grad_norm": 0.32344165444374084,
      "learning_rate": 0.00020240807668133248,
      "loss": 1.0319,
      "step": 16670
    },
    {
      "epoch": 0.980960082334779,
      "grad_norm": 0.28998154401779175,
      "learning_rate": 0.00020234915147705843,
      "loss": 1.0635,
      "step": 16680
    },
    {
      "epoch": 0.981548187899728,
      "grad_norm": 0.34681400656700134,
      "learning_rate": 0.0002022902262727844,
      "loss": 1.0837,
      "step": 16690
    },
    {
      "epoch": 0.9821362934646769,
      "grad_norm": 0.2856605052947998,
      "learning_rate": 0.00020223130106851035,
      "loss": 0.9554,
      "step": 16700
    },
    {
      "epoch": 0.9827243990296258,
      "grad_norm": 0.3689197599887848,
      "learning_rate": 0.0002021723758642363,
      "loss": 0.9583,
      "step": 16710
    },
    {
      "epoch": 0.9833125045945748,
      "grad_norm": 0.3298247754573822,
      "learning_rate": 0.00020211345065996228,
      "loss": 1.1403,
      "step": 16720
    },
    {
      "epoch": 0.9839006101595237,
      "grad_norm": 0.3018088936805725,
      "learning_rate": 0.00020205452545568822,
      "loss": 1.0346,
      "step": 16730
    },
    {
      "epoch": 0.9844887157244725,
      "grad_norm": 0.40592721104621887,
      "learning_rate": 0.0002019956002514142,
      "loss": 1.138,
      "step": 16740
    },
    {
      "epoch": 0.9850768212894214,
      "grad_norm": 0.34767329692840576,
      "learning_rate": 0.00020193667504714015,
      "loss": 1.0456,
      "step": 16750
    },
    {
      "epoch": 0.9856649268543703,
      "grad_norm": 0.3820033669471741,
      "learning_rate": 0.00020187774984286612,
      "loss": 1.0065,
      "step": 16760
    },
    {
      "epoch": 0.9862530324193193,
      "grad_norm": 0.35872575640678406,
      "learning_rate": 0.00020181882463859207,
      "loss": 1.0664,
      "step": 16770
    },
    {
      "epoch": 0.9868411379842682,
      "grad_norm": 0.3308049738407135,
      "learning_rate": 0.00020175989943431802,
      "loss": 1.0755,
      "step": 16780
    },
    {
      "epoch": 0.9874292435492171,
      "grad_norm": 0.30654168128967285,
      "learning_rate": 0.000201700974230044,
      "loss": 1.1026,
      "step": 16790
    },
    {
      "epoch": 0.9880173491141659,
      "grad_norm": 0.36139801144599915,
      "learning_rate": 0.00020164204902576994,
      "loss": 0.9985,
      "step": 16800
    },
    {
      "epoch": 0.9886054546791149,
      "grad_norm": 0.3443264365196228,
      "learning_rate": 0.00020158312382149591,
      "loss": 1.0111,
      "step": 16810
    },
    {
      "epoch": 0.9891935602440638,
      "grad_norm": 0.39434948563575745,
      "learning_rate": 0.00020152419861722186,
      "loss": 1.0523,
      "step": 16820
    },
    {
      "epoch": 0.9897816658090127,
      "grad_norm": 0.35768184065818787,
      "learning_rate": 0.00020146527341294784,
      "loss": 1.0772,
      "step": 16830
    },
    {
      "epoch": 0.9903697713739616,
      "grad_norm": 0.3667212426662445,
      "learning_rate": 0.00020140634820867378,
      "loss": 1.015,
      "step": 16840
    },
    {
      "epoch": 0.9909578769389106,
      "grad_norm": 0.327752023935318,
      "learning_rate": 0.00020134742300439973,
      "loss": 1.061,
      "step": 16850
    },
    {
      "epoch": 0.9915459825038594,
      "grad_norm": 0.31812697649002075,
      "learning_rate": 0.0002012884978001257,
      "loss": 1.0082,
      "step": 16860
    },
    {
      "epoch": 0.9921340880688083,
      "grad_norm": 0.35861140489578247,
      "learning_rate": 0.00020122957259585165,
      "loss": 1.0428,
      "step": 16870
    },
    {
      "epoch": 0.9927221936337572,
      "grad_norm": 0.3587864637374878,
      "learning_rate": 0.00020117064739157763,
      "loss": 1.0786,
      "step": 16880
    },
    {
      "epoch": 0.9933102991987062,
      "grad_norm": 0.3393045663833618,
      "learning_rate": 0.00020111172218730358,
      "loss": 0.9746,
      "step": 16890
    },
    {
      "epoch": 0.9938984047636551,
      "grad_norm": 0.35651493072509766,
      "learning_rate": 0.0002010527969830295,
      "loss": 0.989,
      "step": 16900
    },
    {
      "epoch": 0.994486510328604,
      "grad_norm": 0.3185540437698364,
      "learning_rate": 0.00020099387177875547,
      "loss": 1.0609,
      "step": 16910
    },
    {
      "epoch": 0.9950746158935528,
      "grad_norm": 0.3816555142402649,
      "learning_rate": 0.00020093494657448142,
      "loss": 0.9779,
      "step": 16920
    },
    {
      "epoch": 0.9956627214585018,
      "grad_norm": 0.30217209458351135,
      "learning_rate": 0.0002008760213702074,
      "loss": 0.9947,
      "step": 16930
    },
    {
      "epoch": 0.9962508270234507,
      "grad_norm": 0.30008596181869507,
      "learning_rate": 0.00020081709616593334,
      "loss": 1.059,
      "step": 16940
    },
    {
      "epoch": 0.9968389325883996,
      "grad_norm": 0.35126635432243347,
      "learning_rate": 0.00020075817096165932,
      "loss": 1.0624,
      "step": 16950
    },
    {
      "epoch": 0.9974270381533485,
      "grad_norm": 0.361714243888855,
      "learning_rate": 0.00020069924575738526,
      "loss": 1.0147,
      "step": 16960
    },
    {
      "epoch": 0.9980151437182975,
      "grad_norm": 0.3063908517360687,
      "learning_rate": 0.0002006403205531112,
      "loss": 0.9879,
      "step": 16970
    },
    {
      "epoch": 0.9986032492832464,
      "grad_norm": 0.3125292658805847,
      "learning_rate": 0.0002005813953488372,
      "loss": 0.981,
      "step": 16980
    },
    {
      "epoch": 0.9991913548481952,
      "grad_norm": 0.32226982712745667,
      "learning_rate": 0.00020052247014456313,
      "loss": 1.008,
      "step": 16990
    },
    {
      "epoch": 0.9997794604131441,
      "grad_norm": 0.31783702969551086,
      "learning_rate": 0.0002004635449402891,
      "loss": 1.0572,
      "step": 17000
    },
    {
      "epoch": 1.0003528633389693,
      "grad_norm": 0.3301459550857544,
      "learning_rate": 0.00020040461973601506,
      "loss": 0.9128,
      "step": 17010
    },
    {
      "epoch": 1.0009409689039181,
      "grad_norm": 0.35476556420326233,
      "learning_rate": 0.00020034569453174103,
      "loss": 1.0916,
      "step": 17020
    },
    {
      "epoch": 1.0015290744688672,
      "grad_norm": 0.3053843379020691,
      "learning_rate": 0.00020028676932746698,
      "loss": 1.0285,
      "step": 17030
    },
    {
      "epoch": 1.002117180033816,
      "grad_norm": 0.37515097856521606,
      "learning_rate": 0.00020022784412319293,
      "loss": 1.0266,
      "step": 17040
    },
    {
      "epoch": 1.002705285598765,
      "grad_norm": 0.33495596051216125,
      "learning_rate": 0.0002001689189189189,
      "loss": 1.0278,
      "step": 17050
    },
    {
      "epoch": 1.0032933911637139,
      "grad_norm": 0.35173603892326355,
      "learning_rate": 0.00020010999371464485,
      "loss": 0.9999,
      "step": 17060
    },
    {
      "epoch": 1.003881496728663,
      "grad_norm": 0.32669150829315186,
      "learning_rate": 0.00020005106851037082,
      "loss": 1.0267,
      "step": 17070
    },
    {
      "epoch": 1.0044696022936117,
      "grad_norm": 0.29251280426979065,
      "learning_rate": 0.00019999214330609677,
      "loss": 0.9655,
      "step": 17080
    },
    {
      "epoch": 1.0050577078585605,
      "grad_norm": 0.36655184626579285,
      "learning_rate": 0.00019993321810182275,
      "loss": 1.1593,
      "step": 17090
    },
    {
      "epoch": 1.0056458134235096,
      "grad_norm": 0.34612876176834106,
      "learning_rate": 0.0001998742928975487,
      "loss": 1.0232,
      "step": 17100
    },
    {
      "epoch": 1.0062339189884584,
      "grad_norm": 0.29874032735824585,
      "learning_rate": 0.00019981536769327464,
      "loss": 0.9619,
      "step": 17110
    },
    {
      "epoch": 1.0068220245534074,
      "grad_norm": 0.32856234908103943,
      "learning_rate": 0.00019975644248900062,
      "loss": 0.9205,
      "step": 17120
    },
    {
      "epoch": 1.0074101301183562,
      "grad_norm": 0.33349376916885376,
      "learning_rate": 0.00019969751728472656,
      "loss": 1.0582,
      "step": 17130
    },
    {
      "epoch": 1.007998235683305,
      "grad_norm": 0.36838579177856445,
      "learning_rate": 0.00019963859208045254,
      "loss": 1.095,
      "step": 17140
    },
    {
      "epoch": 1.008586341248254,
      "grad_norm": 0.3913862407207489,
      "learning_rate": 0.0001995796668761785,
      "loss": 1.0098,
      "step": 17150
    },
    {
      "epoch": 1.009174446813203,
      "grad_norm": 0.34346434473991394,
      "learning_rate": 0.00019952074167190446,
      "loss": 1.0983,
      "step": 17160
    },
    {
      "epoch": 1.009762552378152,
      "grad_norm": 0.3409370183944702,
      "learning_rate": 0.0001994618164676304,
      "loss": 1.1538,
      "step": 17170
    },
    {
      "epoch": 1.0103506579431007,
      "grad_norm": 0.3319326937198639,
      "learning_rate": 0.00019940289126335636,
      "loss": 1.0361,
      "step": 17180
    },
    {
      "epoch": 1.0109387635080498,
      "grad_norm": 0.3680270314216614,
      "learning_rate": 0.00019934396605908233,
      "loss": 1.1112,
      "step": 17190
    },
    {
      "epoch": 1.0115268690729986,
      "grad_norm": 0.3300327658653259,
      "learning_rate": 0.00019928504085480828,
      "loss": 0.9997,
      "step": 17200
    },
    {
      "epoch": 1.0121149746379474,
      "grad_norm": 0.3310889005661011,
      "learning_rate": 0.00019922611565053425,
      "loss": 0.9734,
      "step": 17210
    },
    {
      "epoch": 1.0127030802028965,
      "grad_norm": 0.35488128662109375,
      "learning_rate": 0.0001991671904462602,
      "loss": 1.07,
      "step": 17220
    },
    {
      "epoch": 1.0132911857678453,
      "grad_norm": 0.3791426122188568,
      "learning_rate": 0.00019910826524198618,
      "loss": 0.9295,
      "step": 17230
    },
    {
      "epoch": 1.0138792913327943,
      "grad_norm": 0.33098554611206055,
      "learning_rate": 0.00019904934003771212,
      "loss": 1.0165,
      "step": 17240
    },
    {
      "epoch": 1.0144673968977431,
      "grad_norm": 0.3120339512825012,
      "learning_rate": 0.00019899041483343807,
      "loss": 1.0547,
      "step": 17250
    },
    {
      "epoch": 1.0150555024626922,
      "grad_norm": 0.3334653079509735,
      "learning_rate": 0.00019893148962916405,
      "loss": 1.0732,
      "step": 17260
    },
    {
      "epoch": 1.015643608027641,
      "grad_norm": 0.3575448989868164,
      "learning_rate": 0.00019887256442489,
      "loss": 1.0068,
      "step": 17270
    },
    {
      "epoch": 1.0162317135925898,
      "grad_norm": 0.3307514190673828,
      "learning_rate": 0.00019881363922061597,
      "loss": 0.9656,
      "step": 17280
    },
    {
      "epoch": 1.0168198191575388,
      "grad_norm": 0.31972160935401917,
      "learning_rate": 0.00019875471401634192,
      "loss": 1.0366,
      "step": 17290
    },
    {
      "epoch": 1.0174079247224876,
      "grad_norm": 0.3871074616909027,
      "learning_rate": 0.0001986957888120679,
      "loss": 0.9743,
      "step": 17300
    },
    {
      "epoch": 1.0179960302874367,
      "grad_norm": 0.32115426659584045,
      "learning_rate": 0.00019863686360779384,
      "loss": 1.1391,
      "step": 17310
    },
    {
      "epoch": 1.0185841358523855,
      "grad_norm": 0.3328655958175659,
      "learning_rate": 0.00019857793840351976,
      "loss": 1.0155,
      "step": 17320
    },
    {
      "epoch": 1.0191722414173343,
      "grad_norm": 0.3472786247730255,
      "learning_rate": 0.00019851901319924573,
      "loss": 1.0595,
      "step": 17330
    },
    {
      "epoch": 1.0197603469822833,
      "grad_norm": 0.3534031808376312,
      "learning_rate": 0.00019846008799497168,
      "loss": 0.9943,
      "step": 17340
    },
    {
      "epoch": 1.0203484525472322,
      "grad_norm": 0.30733686685562134,
      "learning_rate": 0.00019840116279069766,
      "loss": 0.9448,
      "step": 17350
    },
    {
      "epoch": 1.0209365581121812,
      "grad_norm": 0.3030608594417572,
      "learning_rate": 0.0001983422375864236,
      "loss": 1.0721,
      "step": 17360
    },
    {
      "epoch": 1.02152466367713,
      "grad_norm": 0.309298038482666,
      "learning_rate": 0.00019828331238214955,
      "loss": 1.051,
      "step": 17370
    },
    {
      "epoch": 1.022112769242079,
      "grad_norm": 0.34817948937416077,
      "learning_rate": 0.00019822438717787553,
      "loss": 1.0396,
      "step": 17380
    },
    {
      "epoch": 1.0227008748070279,
      "grad_norm": 0.3768700659275055,
      "learning_rate": 0.00019816546197360147,
      "loss": 1.0966,
      "step": 17390
    },
    {
      "epoch": 1.0232889803719767,
      "grad_norm": 0.3509993851184845,
      "learning_rate": 0.00019810653676932745,
      "loss": 1.1104,
      "step": 17400
    },
    {
      "epoch": 1.0238770859369257,
      "grad_norm": 0.3057616055011749,
      "learning_rate": 0.0001980476115650534,
      "loss": 1.0804,
      "step": 17410
    },
    {
      "epoch": 1.0244651915018745,
      "grad_norm": 0.3144806921482086,
      "learning_rate": 0.00019798868636077937,
      "loss": 0.9836,
      "step": 17420
    },
    {
      "epoch": 1.0250532970668236,
      "grad_norm": 0.322094202041626,
      "learning_rate": 0.00019792976115650532,
      "loss": 1.1049,
      "step": 17430
    },
    {
      "epoch": 1.0256414026317724,
      "grad_norm": 0.3549480140209198,
      "learning_rate": 0.00019787083595223127,
      "loss": 1.0489,
      "step": 17440
    },
    {
      "epoch": 1.0262295081967212,
      "grad_norm": 0.3327786326408386,
      "learning_rate": 0.00019781191074795724,
      "loss": 0.9862,
      "step": 17450
    },
    {
      "epoch": 1.0268176137616702,
      "grad_norm": 0.34147393703460693,
      "learning_rate": 0.0001977529855436832,
      "loss": 1.0366,
      "step": 17460
    },
    {
      "epoch": 1.027405719326619,
      "grad_norm": 0.3676181137561798,
      "learning_rate": 0.00019769406033940916,
      "loss": 1.1349,
      "step": 17470
    },
    {
      "epoch": 1.027993824891568,
      "grad_norm": 0.348557710647583,
      "learning_rate": 0.0001976351351351351,
      "loss": 1.0471,
      "step": 17480
    },
    {
      "epoch": 1.028581930456517,
      "grad_norm": 0.39602336287498474,
      "learning_rate": 0.00019757620993086109,
      "loss": 1.0627,
      "step": 17490
    },
    {
      "epoch": 1.029170036021466,
      "grad_norm": 0.3353917896747589,
      "learning_rate": 0.00019751728472658703,
      "loss": 1.084,
      "step": 17500
    },
    {
      "epoch": 1.0297581415864148,
      "grad_norm": 0.33567532896995544,
      "learning_rate": 0.00019745835952231298,
      "loss": 1.1217,
      "step": 17510
    },
    {
      "epoch": 1.0303462471513636,
      "grad_norm": 0.3407614827156067,
      "learning_rate": 0.00019739943431803896,
      "loss": 1.0444,
      "step": 17520
    },
    {
      "epoch": 1.0309343527163126,
      "grad_norm": 0.41665148735046387,
      "learning_rate": 0.0001973405091137649,
      "loss": 1.0438,
      "step": 17530
    },
    {
      "epoch": 1.0315224582812614,
      "grad_norm": 0.3968219459056854,
      "learning_rate": 0.00019728158390949088,
      "loss": 0.9432,
      "step": 17540
    },
    {
      "epoch": 1.0321105638462105,
      "grad_norm": 0.4041583240032196,
      "learning_rate": 0.00019722265870521683,
      "loss": 0.9443,
      "step": 17550
    },
    {
      "epoch": 1.0326986694111593,
      "grad_norm": 0.3697609603404999,
      "learning_rate": 0.0001971637335009428,
      "loss": 1.0838,
      "step": 17560
    },
    {
      "epoch": 1.033286774976108,
      "grad_norm": 0.3769422471523285,
      "learning_rate": 0.00019710480829666875,
      "loss": 1.0027,
      "step": 17570
    },
    {
      "epoch": 1.0338748805410571,
      "grad_norm": 0.34400269389152527,
      "learning_rate": 0.0001970458830923947,
      "loss": 0.9837,
      "step": 17580
    },
    {
      "epoch": 1.034462986106006,
      "grad_norm": 0.37163400650024414,
      "learning_rate": 0.00019698695788812067,
      "loss": 0.957,
      "step": 17590
    },
    {
      "epoch": 1.035051091670955,
      "grad_norm": 0.4499106705188751,
      "learning_rate": 0.00019692803268384662,
      "loss": 0.9887,
      "step": 17600
    },
    {
      "epoch": 1.0356391972359038,
      "grad_norm": 0.31317541003227234,
      "learning_rate": 0.0001968691074795726,
      "loss": 0.9358,
      "step": 17610
    },
    {
      "epoch": 1.0362273028008528,
      "grad_norm": 0.29276496171951294,
      "learning_rate": 0.00019681018227529854,
      "loss": 0.9402,
      "step": 17620
    },
    {
      "epoch": 1.0368154083658017,
      "grad_norm": 0.3130515515804291,
      "learning_rate": 0.00019675125707102452,
      "loss": 1.1775,
      "step": 17630
    },
    {
      "epoch": 1.0374035139307505,
      "grad_norm": 0.354906290769577,
      "learning_rate": 0.00019669233186675046,
      "loss": 1.0928,
      "step": 17640
    },
    {
      "epoch": 1.0379916194956995,
      "grad_norm": 0.3445342779159546,
      "learning_rate": 0.0001966334066624764,
      "loss": 0.9838,
      "step": 17650
    },
    {
      "epoch": 1.0385797250606483,
      "grad_norm": 0.30665963888168335,
      "learning_rate": 0.00019657448145820239,
      "loss": 1.0873,
      "step": 17660
    },
    {
      "epoch": 1.0391678306255974,
      "grad_norm": 0.3260977566242218,
      "learning_rate": 0.00019651555625392833,
      "loss": 1.0626,
      "step": 17670
    },
    {
      "epoch": 1.0397559361905462,
      "grad_norm": 0.3238944411277771,
      "learning_rate": 0.0001964566310496543,
      "loss": 1.0857,
      "step": 17680
    },
    {
      "epoch": 1.0403440417554952,
      "grad_norm": 0.29281651973724365,
      "learning_rate": 0.00019639770584538026,
      "loss": 0.9842,
      "step": 17690
    },
    {
      "epoch": 1.040932147320444,
      "grad_norm": 0.34902599453926086,
      "learning_rate": 0.00019633878064110623,
      "loss": 1.1248,
      "step": 17700
    },
    {
      "epoch": 1.0415202528853928,
      "grad_norm": 0.3293711245059967,
      "learning_rate": 0.00019627985543683218,
      "loss": 0.9927,
      "step": 17710
    },
    {
      "epoch": 1.0421083584503419,
      "grad_norm": 0.349878191947937,
      "learning_rate": 0.00019622093023255813,
      "loss": 0.9566,
      "step": 17720
    },
    {
      "epoch": 1.0426964640152907,
      "grad_norm": 0.3420850932598114,
      "learning_rate": 0.0001961620050282841,
      "loss": 0.9762,
      "step": 17730
    },
    {
      "epoch": 1.0432845695802397,
      "grad_norm": 0.35367658734321594,
      "learning_rate": 0.00019610307982401002,
      "loss": 0.9744,
      "step": 17740
    },
    {
      "epoch": 1.0438726751451886,
      "grad_norm": 0.42462578415870667,
      "learning_rate": 0.000196044154619736,
      "loss": 0.9822,
      "step": 17750
    },
    {
      "epoch": 1.0444607807101374,
      "grad_norm": 0.3191755414009094,
      "learning_rate": 0.00019598522941546194,
      "loss": 0.9292,
      "step": 17760
    },
    {
      "epoch": 1.0450488862750864,
      "grad_norm": 0.3766307830810547,
      "learning_rate": 0.0001959263042111879,
      "loss": 0.9783,
      "step": 17770
    },
    {
      "epoch": 1.0456369918400352,
      "grad_norm": 0.3620375394821167,
      "learning_rate": 0.00019586737900691387,
      "loss": 0.9941,
      "step": 17780
    },
    {
      "epoch": 1.0462250974049843,
      "grad_norm": 0.31996703147888184,
      "learning_rate": 0.00019580845380263981,
      "loss": 0.9685,
      "step": 17790
    },
    {
      "epoch": 1.046813202969933,
      "grad_norm": 0.2972654104232788,
      "learning_rate": 0.0001957495285983658,
      "loss": 1.0688,
      "step": 17800
    },
    {
      "epoch": 1.0474013085348821,
      "grad_norm": 0.32918500900268555,
      "learning_rate": 0.00019569060339409174,
      "loss": 0.936,
      "step": 17810
    },
    {
      "epoch": 1.047989414099831,
      "grad_norm": 0.3336625099182129,
      "learning_rate": 0.0001956316781898177,
      "loss": 1.0498,
      "step": 17820
    },
    {
      "epoch": 1.0485775196647797,
      "grad_norm": 0.3199909031391144,
      "learning_rate": 0.00019557275298554366,
      "loss": 0.9813,
      "step": 17830
    },
    {
      "epoch": 1.0491656252297288,
      "grad_norm": 0.39515089988708496,
      "learning_rate": 0.0001955138277812696,
      "loss": 1.0502,
      "step": 17840
    },
    {
      "epoch": 1.0497537307946776,
      "grad_norm": 0.359260618686676,
      "learning_rate": 0.00019545490257699558,
      "loss": 0.9762,
      "step": 17850
    },
    {
      "epoch": 1.0503418363596266,
      "grad_norm": 0.3383365869522095,
      "learning_rate": 0.00019539597737272153,
      "loss": 1.023,
      "step": 17860
    },
    {
      "epoch": 1.0509299419245755,
      "grad_norm": 0.307062029838562,
      "learning_rate": 0.0001953370521684475,
      "loss": 1.0699,
      "step": 17870
    },
    {
      "epoch": 1.0515180474895243,
      "grad_norm": 0.34191185235977173,
      "learning_rate": 0.00019527812696417345,
      "loss": 0.9855,
      "step": 17880
    },
    {
      "epoch": 1.0521061530544733,
      "grad_norm": 0.327615350484848,
      "learning_rate": 0.00019521920175989943,
      "loss": 1.0724,
      "step": 17890
    },
    {
      "epoch": 1.0526942586194221,
      "grad_norm": 0.4268221855163574,
      "learning_rate": 0.00019516027655562537,
      "loss": 1.0173,
      "step": 17900
    },
    {
      "epoch": 1.0532823641843712,
      "grad_norm": 0.322458952665329,
      "learning_rate": 0.00019510135135135132,
      "loss": 1.0962,
      "step": 17910
    },
    {
      "epoch": 1.05387046974932,
      "grad_norm": 0.3365767002105713,
      "learning_rate": 0.0001950424261470773,
      "loss": 1.1322,
      "step": 17920
    },
    {
      "epoch": 1.054458575314269,
      "grad_norm": 0.33728331327438354,
      "learning_rate": 0.00019498350094280324,
      "loss": 1.0284,
      "step": 17930
    },
    {
      "epoch": 1.0550466808792178,
      "grad_norm": 0.33041080832481384,
      "learning_rate": 0.00019492457573852922,
      "loss": 1.0885,
      "step": 17940
    },
    {
      "epoch": 1.0556347864441666,
      "grad_norm": 0.3102232813835144,
      "learning_rate": 0.00019486565053425517,
      "loss": 0.9976,
      "step": 17950
    },
    {
      "epoch": 1.0562228920091157,
      "grad_norm": 0.3419567346572876,
      "learning_rate": 0.00019480672532998114,
      "loss": 1.023,
      "step": 17960
    },
    {
      "epoch": 1.0568109975740645,
      "grad_norm": 0.2975638508796692,
      "learning_rate": 0.0001947478001257071,
      "loss": 1.048,
      "step": 17970
    },
    {
      "epoch": 1.0573991031390135,
      "grad_norm": 0.34785550832748413,
      "learning_rate": 0.00019468887492143304,
      "loss": 1.0412,
      "step": 17980
    },
    {
      "epoch": 1.0579872087039623,
      "grad_norm": 0.33393627405166626,
      "learning_rate": 0.000194629949717159,
      "loss": 1.0782,
      "step": 17990
    },
    {
      "epoch": 1.0585753142689112,
      "grad_norm": 0.3702943027019501,
      "learning_rate": 0.00019457102451288496,
      "loss": 0.9935,
      "step": 18000
    },
    {
      "epoch": 1.0591634198338602,
      "grad_norm": 0.3369457423686981,
      "learning_rate": 0.00019451209930861093,
      "loss": 1.0553,
      "step": 18010
    },
    {
      "epoch": 1.059751525398809,
      "grad_norm": 0.35751453042030334,
      "learning_rate": 0.00019445317410433688,
      "loss": 0.9103,
      "step": 18020
    },
    {
      "epoch": 1.060339630963758,
      "grad_norm": 0.36088335514068604,
      "learning_rate": 0.00019440014142049024,
      "loss": 0.9407,
      "step": 18030
    },
    {
      "epoch": 1.0609277365287069,
      "grad_norm": 0.35751405358314514,
      "learning_rate": 0.0001943412162162162,
      "loss": 1.0461,
      "step": 18040
    },
    {
      "epoch": 1.061515842093656,
      "grad_norm": 0.347574919462204,
      "learning_rate": 0.00019428229101194216,
      "loss": 0.8741,
      "step": 18050
    },
    {
      "epoch": 1.0621039476586047,
      "grad_norm": 0.32668623328208923,
      "learning_rate": 0.0001942233658076681,
      "loss": 1.0373,
      "step": 18060
    },
    {
      "epoch": 1.0626920532235535,
      "grad_norm": 0.3394410014152527,
      "learning_rate": 0.00019416444060339408,
      "loss": 1.0134,
      "step": 18070
    },
    {
      "epoch": 1.0632801587885026,
      "grad_norm": 0.3820042908191681,
      "learning_rate": 0.00019410551539912003,
      "loss": 0.9902,
      "step": 18080
    },
    {
      "epoch": 1.0638682643534514,
      "grad_norm": 0.3451758921146393,
      "learning_rate": 0.000194046590194846,
      "loss": 1.0661,
      "step": 18090
    },
    {
      "epoch": 1.0644563699184004,
      "grad_norm": 0.3723766505718231,
      "learning_rate": 0.00019398766499057195,
      "loss": 0.9621,
      "step": 18100
    },
    {
      "epoch": 1.0650444754833492,
      "grad_norm": 0.36811161041259766,
      "learning_rate": 0.00019392873978629793,
      "loss": 1.0215,
      "step": 18110
    },
    {
      "epoch": 1.0656325810482983,
      "grad_norm": 0.35380640625953674,
      "learning_rate": 0.00019386981458202387,
      "loss": 1.0035,
      "step": 18120
    },
    {
      "epoch": 1.066220686613247,
      "grad_norm": 0.3514632284641266,
      "learning_rate": 0.00019381088937774982,
      "loss": 1.0487,
      "step": 18130
    },
    {
      "epoch": 1.066808792178196,
      "grad_norm": 0.2973841726779938,
      "learning_rate": 0.0001937519641734758,
      "loss": 1.0691,
      "step": 18140
    },
    {
      "epoch": 1.067396897743145,
      "grad_norm": 0.33416977524757385,
      "learning_rate": 0.00019369303896920174,
      "loss": 0.9692,
      "step": 18150
    },
    {
      "epoch": 1.0679850033080938,
      "grad_norm": 0.2895018756389618,
      "learning_rate": 0.00019363411376492772,
      "loss": 0.9841,
      "step": 18160
    },
    {
      "epoch": 1.0685731088730428,
      "grad_norm": 0.4021310806274414,
      "learning_rate": 0.00019357518856065367,
      "loss": 1.0994,
      "step": 18170
    },
    {
      "epoch": 1.0691612144379916,
      "grad_norm": 0.3539924621582031,
      "learning_rate": 0.00019351626335637964,
      "loss": 1.0295,
      "step": 18180
    },
    {
      "epoch": 1.0697493200029404,
      "grad_norm": 0.3437977135181427,
      "learning_rate": 0.0001934573381521056,
      "loss": 0.9804,
      "step": 18190
    },
    {
      "epoch": 1.0703374255678895,
      "grad_norm": 0.34148478507995605,
      "learning_rate": 0.00019339841294783154,
      "loss": 0.9709,
      "step": 18200
    },
    {
      "epoch": 1.0709255311328383,
      "grad_norm": 0.35830023884773254,
      "learning_rate": 0.0001933394877435575,
      "loss": 0.9545,
      "step": 18210
    },
    {
      "epoch": 1.0715136366977873,
      "grad_norm": 0.3767963945865631,
      "learning_rate": 0.00019328056253928346,
      "loss": 0.956,
      "step": 18220
    },
    {
      "epoch": 1.0721017422627361,
      "grad_norm": 0.33508941531181335,
      "learning_rate": 0.00019322163733500943,
      "loss": 1.1229,
      "step": 18230
    },
    {
      "epoch": 1.0726898478276852,
      "grad_norm": 0.42343300580978394,
      "learning_rate": 0.00019316271213073535,
      "loss": 1.0634,
      "step": 18240
    },
    {
      "epoch": 1.073277953392634,
      "grad_norm": 0.3222399950027466,
      "learning_rate": 0.0001931037869264613,
      "loss": 0.9644,
      "step": 18250
    },
    {
      "epoch": 1.0738660589575828,
      "grad_norm": 0.3783910274505615,
      "learning_rate": 0.00019304486172218728,
      "loss": 1.0398,
      "step": 18260
    },
    {
      "epoch": 1.0744541645225318,
      "grad_norm": 0.37179046869277954,
      "learning_rate": 0.00019298593651791322,
      "loss": 1.0104,
      "step": 18270
    },
    {
      "epoch": 1.0750422700874807,
      "grad_norm": 0.34283003211021423,
      "learning_rate": 0.0001929270113136392,
      "loss": 1.0404,
      "step": 18280
    },
    {
      "epoch": 1.0756303756524297,
      "grad_norm": 0.38722991943359375,
      "learning_rate": 0.00019286808610936515,
      "loss": 0.9633,
      "step": 18290
    },
    {
      "epoch": 1.0762184812173785,
      "grad_norm": 0.37857675552368164,
      "learning_rate": 0.00019280916090509112,
      "loss": 1.0742,
      "step": 18300
    },
    {
      "epoch": 1.0768065867823273,
      "grad_norm": 0.3331270217895508,
      "learning_rate": 0.00019275023570081707,
      "loss": 1.0204,
      "step": 18310
    },
    {
      "epoch": 1.0773946923472764,
      "grad_norm": 0.3071032762527466,
      "learning_rate": 0.00019269131049654302,
      "loss": 1.044,
      "step": 18320
    },
    {
      "epoch": 1.0779827979122252,
      "grad_norm": 0.31239771842956543,
      "learning_rate": 0.000192632385292269,
      "loss": 0.9871,
      "step": 18330
    },
    {
      "epoch": 1.0785709034771742,
      "grad_norm": 0.3526100814342499,
      "learning_rate": 0.00019257346008799494,
      "loss": 1.0048,
      "step": 18340
    },
    {
      "epoch": 1.079159009042123,
      "grad_norm": 0.3515399992465973,
      "learning_rate": 0.0001925145348837209,
      "loss": 1.0641,
      "step": 18350
    },
    {
      "epoch": 1.079747114607072,
      "grad_norm": 0.3238844573497772,
      "learning_rate": 0.00019245560967944686,
      "loss": 1.0958,
      "step": 18360
    },
    {
      "epoch": 1.0803352201720209,
      "grad_norm": 0.3310815095901489,
      "learning_rate": 0.00019239668447517284,
      "loss": 1.0128,
      "step": 18370
    },
    {
      "epoch": 1.0809233257369697,
      "grad_norm": 0.37234756350517273,
      "learning_rate": 0.00019233775927089878,
      "loss": 1.0446,
      "step": 18380
    },
    {
      "epoch": 1.0815114313019187,
      "grad_norm": 0.33125585317611694,
      "learning_rate": 0.00019227883406662473,
      "loss": 1.0199,
      "step": 18390
    },
    {
      "epoch": 1.0820995368668676,
      "grad_norm": 0.33595436811447144,
      "learning_rate": 0.0001922199088623507,
      "loss": 1.0373,
      "step": 18400
    },
    {
      "epoch": 1.0826876424318166,
      "grad_norm": 0.3062257468700409,
      "learning_rate": 0.00019216098365807665,
      "loss": 1.0483,
      "step": 18410
    },
    {
      "epoch": 1.0832757479967654,
      "grad_norm": 0.3794252574443817,
      "learning_rate": 0.00019210205845380263,
      "loss": 1.0684,
      "step": 18420
    },
    {
      "epoch": 1.0838638535617142,
      "grad_norm": 0.31460070610046387,
      "learning_rate": 0.00019204313324952858,
      "loss": 1.0052,
      "step": 18430
    },
    {
      "epoch": 1.0844519591266633,
      "grad_norm": 0.30809468030929565,
      "learning_rate": 0.00019198420804525455,
      "loss": 1.0921,
      "step": 18440
    },
    {
      "epoch": 1.085040064691612,
      "grad_norm": 0.3728621006011963,
      "learning_rate": 0.0001919252828409805,
      "loss": 1.009,
      "step": 18450
    },
    {
      "epoch": 1.085628170256561,
      "grad_norm": 0.31701329350471497,
      "learning_rate": 0.00019186635763670645,
      "loss": 1.0895,
      "step": 18460
    },
    {
      "epoch": 1.08621627582151,
      "grad_norm": 0.3210003674030304,
      "learning_rate": 0.00019180743243243242,
      "loss": 0.9005,
      "step": 18470
    },
    {
      "epoch": 1.086804381386459,
      "grad_norm": 0.371517151594162,
      "learning_rate": 0.00019174850722815837,
      "loss": 0.9869,
      "step": 18480
    },
    {
      "epoch": 1.0873924869514078,
      "grad_norm": 0.4502947926521301,
      "learning_rate": 0.00019168958202388434,
      "loss": 1.0478,
      "step": 18490
    },
    {
      "epoch": 1.0879805925163566,
      "grad_norm": 0.3511929214000702,
      "learning_rate": 0.0001916306568196103,
      "loss": 1.0443,
      "step": 18500
    },
    {
      "epoch": 1.0885686980813056,
      "grad_norm": 0.37375086545944214,
      "learning_rate": 0.00019157173161533627,
      "loss": 1.0055,
      "step": 18510
    },
    {
      "epoch": 1.0891568036462544,
      "grad_norm": 0.3811854124069214,
      "learning_rate": 0.0001915128064110622,
      "loss": 0.9365,
      "step": 18520
    },
    {
      "epoch": 1.0897449092112035,
      "grad_norm": 0.3618122935295105,
      "learning_rate": 0.00019145388120678816,
      "loss": 0.9933,
      "step": 18530
    },
    {
      "epoch": 1.0903330147761523,
      "grad_norm": 0.32552891969680786,
      "learning_rate": 0.00019139495600251414,
      "loss": 1.0791,
      "step": 18540
    },
    {
      "epoch": 1.0909211203411013,
      "grad_norm": 0.3455032706260681,
      "learning_rate": 0.00019133603079824008,
      "loss": 1.098,
      "step": 18550
    },
    {
      "epoch": 1.0915092259060502,
      "grad_norm": 0.3186795115470886,
      "learning_rate": 0.00019127710559396606,
      "loss": 0.9469,
      "step": 18560
    },
    {
      "epoch": 1.092097331470999,
      "grad_norm": 0.32399675250053406,
      "learning_rate": 0.000191218180389692,
      "loss": 1.125,
      "step": 18570
    },
    {
      "epoch": 1.092685437035948,
      "grad_norm": 0.3619092106819153,
      "learning_rate": 0.00019115925518541798,
      "loss": 1.176,
      "step": 18580
    },
    {
      "epoch": 1.0932735426008968,
      "grad_norm": 0.3411148190498352,
      "learning_rate": 0.00019110032998114393,
      "loss": 1.0796,
      "step": 18590
    },
    {
      "epoch": 1.0938616481658459,
      "grad_norm": 0.3046376705169678,
      "learning_rate": 0.00019104140477686988,
      "loss": 0.9284,
      "step": 18600
    },
    {
      "epoch": 1.0944497537307947,
      "grad_norm": 0.3383946418762207,
      "learning_rate": 0.00019098247957259585,
      "loss": 1.1497,
      "step": 18610
    },
    {
      "epoch": 1.0950378592957435,
      "grad_norm": 0.3513421416282654,
      "learning_rate": 0.0001909235543683218,
      "loss": 1.0538,
      "step": 18620
    },
    {
      "epoch": 1.0956259648606925,
      "grad_norm": 0.3328641355037689,
      "learning_rate": 0.00019086462916404777,
      "loss": 0.9352,
      "step": 18630
    },
    {
      "epoch": 1.0962140704256413,
      "grad_norm": 0.3572659492492676,
      "learning_rate": 0.00019080570395977372,
      "loss": 1.0605,
      "step": 18640
    },
    {
      "epoch": 1.0968021759905904,
      "grad_norm": 0.3739888668060303,
      "learning_rate": 0.0001907467787554997,
      "loss": 1.0133,
      "step": 18650
    },
    {
      "epoch": 1.0973902815555392,
      "grad_norm": 0.35421258211135864,
      "learning_rate": 0.00019068785355122562,
      "loss": 1.1024,
      "step": 18660
    },
    {
      "epoch": 1.0979783871204882,
      "grad_norm": 0.3631093502044678,
      "learning_rate": 0.00019062892834695156,
      "loss": 1.0247,
      "step": 18670
    },
    {
      "epoch": 1.098566492685437,
      "grad_norm": 0.36453941464424133,
      "learning_rate": 0.00019057000314267754,
      "loss": 1.0777,
      "step": 18680
    },
    {
      "epoch": 1.0991545982503859,
      "grad_norm": 0.3677118420600891,
      "learning_rate": 0.00019051107793840349,
      "loss": 1.0652,
      "step": 18690
    },
    {
      "epoch": 1.099742703815335,
      "grad_norm": 0.3319052755832672,
      "learning_rate": 0.00019045215273412946,
      "loss": 1.0446,
      "step": 18700
    },
    {
      "epoch": 1.1003308093802837,
      "grad_norm": 0.3128156363964081,
      "learning_rate": 0.0001903932275298554,
      "loss": 0.9863,
      "step": 18710
    },
    {
      "epoch": 1.1009189149452328,
      "grad_norm": 0.33275654911994934,
      "learning_rate": 0.00019033430232558136,
      "loss": 1.0605,
      "step": 18720
    },
    {
      "epoch": 1.1015070205101816,
      "grad_norm": 0.3542141318321228,
      "learning_rate": 0.00019027537712130733,
      "loss": 1.0915,
      "step": 18730
    },
    {
      "epoch": 1.1020951260751304,
      "grad_norm": 0.3408583104610443,
      "learning_rate": 0.00019021645191703328,
      "loss": 0.9778,
      "step": 18740
    },
    {
      "epoch": 1.1026832316400794,
      "grad_norm": 0.32256999611854553,
      "learning_rate": 0.00019015752671275925,
      "loss": 1.0128,
      "step": 18750
    },
    {
      "epoch": 1.1032713372050282,
      "grad_norm": 0.3532918691635132,
      "learning_rate": 0.0001900986015084852,
      "loss": 1.0547,
      "step": 18760
    },
    {
      "epoch": 1.1038594427699773,
      "grad_norm": 0.3380075991153717,
      "learning_rate": 0.00019003967630421118,
      "loss": 1.0623,
      "step": 18770
    },
    {
      "epoch": 1.104447548334926,
      "grad_norm": 0.3223486542701721,
      "learning_rate": 0.00018998075109993712,
      "loss": 1.0534,
      "step": 18780
    },
    {
      "epoch": 1.1050356538998751,
      "grad_norm": 0.3984256386756897,
      "learning_rate": 0.00018992182589566307,
      "loss": 1.0128,
      "step": 18790
    },
    {
      "epoch": 1.105623759464824,
      "grad_norm": 0.3505503237247467,
      "learning_rate": 0.00018986290069138905,
      "loss": 0.98,
      "step": 18800
    },
    {
      "epoch": 1.1062118650297728,
      "grad_norm": 0.360026091337204,
      "learning_rate": 0.000189803975487115,
      "loss": 0.9121,
      "step": 18810
    },
    {
      "epoch": 1.1067999705947218,
      "grad_norm": 0.32685279846191406,
      "learning_rate": 0.00018974505028284097,
      "loss": 1.0427,
      "step": 18820
    },
    {
      "epoch": 1.1073880761596706,
      "grad_norm": 0.2886371314525604,
      "learning_rate": 0.00018968612507856692,
      "loss": 0.9844,
      "step": 18830
    },
    {
      "epoch": 1.1079761817246196,
      "grad_norm": 0.3560764491558075,
      "learning_rate": 0.0001896271998742929,
      "loss": 1.07,
      "step": 18840
    },
    {
      "epoch": 1.1085642872895685,
      "grad_norm": 0.3582213222980499,
      "learning_rate": 0.00018956827467001884,
      "loss": 0.9305,
      "step": 18850
    },
    {
      "epoch": 1.1091523928545173,
      "grad_norm": 0.3739594519138336,
      "learning_rate": 0.00018950934946574479,
      "loss": 1.0396,
      "step": 18860
    },
    {
      "epoch": 1.1097404984194663,
      "grad_norm": 0.3570129871368408,
      "learning_rate": 0.00018945042426147076,
      "loss": 1.1051,
      "step": 18870
    },
    {
      "epoch": 1.1103286039844151,
      "grad_norm": 0.365114688873291,
      "learning_rate": 0.0001893914990571967,
      "loss": 1.0047,
      "step": 18880
    },
    {
      "epoch": 1.1109167095493642,
      "grad_norm": 0.36316153407096863,
      "learning_rate": 0.00018933257385292268,
      "loss": 0.9552,
      "step": 18890
    },
    {
      "epoch": 1.111504815114313,
      "grad_norm": 0.3282708525657654,
      "learning_rate": 0.00018927364864864863,
      "loss": 1.0051,
      "step": 18900
    },
    {
      "epoch": 1.112092920679262,
      "grad_norm": 0.3595121204853058,
      "learning_rate": 0.0001892147234443746,
      "loss": 1.0408,
      "step": 18910
    },
    {
      "epoch": 1.1126810262442108,
      "grad_norm": 0.329509437084198,
      "learning_rate": 0.00018915579824010055,
      "loss": 1.0818,
      "step": 18920
    },
    {
      "epoch": 1.1132691318091597,
      "grad_norm": 0.3559158444404602,
      "learning_rate": 0.0001890968730358265,
      "loss": 1.0056,
      "step": 18930
    },
    {
      "epoch": 1.1138572373741087,
      "grad_norm": 0.3162948191165924,
      "learning_rate": 0.00018903794783155247,
      "loss": 1.0396,
      "step": 18940
    },
    {
      "epoch": 1.1144453429390575,
      "grad_norm": 0.3969731330871582,
      "learning_rate": 0.00018897902262727842,
      "loss": 1.1233,
      "step": 18950
    },
    {
      "epoch": 1.1150334485040065,
      "grad_norm": 0.33723655343055725,
      "learning_rate": 0.0001889200974230044,
      "loss": 1.085,
      "step": 18960
    },
    {
      "epoch": 1.1156215540689554,
      "grad_norm": 0.35914644598960876,
      "learning_rate": 0.00018886117221873034,
      "loss": 0.9779,
      "step": 18970
    },
    {
      "epoch": 1.1162096596339044,
      "grad_norm": 0.3726477026939392,
      "learning_rate": 0.00018880224701445632,
      "loss": 1.0308,
      "step": 18980
    },
    {
      "epoch": 1.1167977651988532,
      "grad_norm": 0.3108329772949219,
      "learning_rate": 0.00018874332181018227,
      "loss": 0.9667,
      "step": 18990
    },
    {
      "epoch": 1.117385870763802,
      "grad_norm": 0.32242488861083984,
      "learning_rate": 0.00018868439660590821,
      "loss": 1.0003,
      "step": 19000
    },
    {
      "epoch": 1.117973976328751,
      "grad_norm": 0.3474632799625397,
      "learning_rate": 0.0001886254714016342,
      "loss": 1.0276,
      "step": 19010
    },
    {
      "epoch": 1.1185620818936999,
      "grad_norm": 0.36535143852233887,
      "learning_rate": 0.00018856654619736014,
      "loss": 1.0087,
      "step": 19020
    },
    {
      "epoch": 1.119150187458649,
      "grad_norm": 0.3783193528652191,
      "learning_rate": 0.0001885076209930861,
      "loss": 1.0442,
      "step": 19030
    },
    {
      "epoch": 1.1197382930235977,
      "grad_norm": 0.32460471987724304,
      "learning_rate": 0.00018844869578881206,
      "loss": 1.0528,
      "step": 19040
    },
    {
      "epoch": 1.1203263985885465,
      "grad_norm": 0.31489887833595276,
      "learning_rate": 0.00018838977058453803,
      "loss": 0.9815,
      "step": 19050
    },
    {
      "epoch": 1.1209145041534956,
      "grad_norm": 0.33332735300064087,
      "learning_rate": 0.00018833084538026398,
      "loss": 0.9679,
      "step": 19060
    },
    {
      "epoch": 1.1215026097184444,
      "grad_norm": 0.3100507855415344,
      "learning_rate": 0.00018827192017598993,
      "loss": 1.1132,
      "step": 19070
    },
    {
      "epoch": 1.1220907152833934,
      "grad_norm": 0.3480791747570038,
      "learning_rate": 0.00018821299497171588,
      "loss": 0.9503,
      "step": 19080
    },
    {
      "epoch": 1.1226788208483423,
      "grad_norm": 0.3905664384365082,
      "learning_rate": 0.00018815406976744183,
      "loss": 1.0223,
      "step": 19090
    },
    {
      "epoch": 1.1232669264132913,
      "grad_norm": 0.3586256802082062,
      "learning_rate": 0.0001880951445631678,
      "loss": 1.0577,
      "step": 19100
    },
    {
      "epoch": 1.12385503197824,
      "grad_norm": 0.31913408637046814,
      "learning_rate": 0.00018803621935889375,
      "loss": 1.0458,
      "step": 19110
    },
    {
      "epoch": 1.124443137543189,
      "grad_norm": 0.3924711346626282,
      "learning_rate": 0.0001879772941546197,
      "loss": 1.0239,
      "step": 19120
    },
    {
      "epoch": 1.125031243108138,
      "grad_norm": 0.3634001910686493,
      "learning_rate": 0.00018791836895034567,
      "loss": 1.0284,
      "step": 19130
    },
    {
      "epoch": 1.1256193486730868,
      "grad_norm": 0.33698245882987976,
      "learning_rate": 0.00018785944374607162,
      "loss": 0.9787,
      "step": 19140
    },
    {
      "epoch": 1.1262074542380358,
      "grad_norm": 0.37244123220443726,
      "learning_rate": 0.0001878005185417976,
      "loss": 1.0289,
      "step": 19150
    },
    {
      "epoch": 1.1267955598029846,
      "grad_norm": 0.3552241027355194,
      "learning_rate": 0.00018774159333752354,
      "loss": 1.0011,
      "step": 19160
    },
    {
      "epoch": 1.1273836653679337,
      "grad_norm": 0.34220826625823975,
      "learning_rate": 0.00018768266813324951,
      "loss": 1.0823,
      "step": 19170
    },
    {
      "epoch": 1.1279717709328825,
      "grad_norm": 0.3634170889854431,
      "learning_rate": 0.00018762374292897546,
      "loss": 1.0614,
      "step": 19180
    },
    {
      "epoch": 1.1285598764978313,
      "grad_norm": 0.3165680766105652,
      "learning_rate": 0.0001875648177247014,
      "loss": 1.0524,
      "step": 19190
    },
    {
      "epoch": 1.1291479820627803,
      "grad_norm": 0.40751636028289795,
      "learning_rate": 0.00018750589252042738,
      "loss": 0.9925,
      "step": 19200
    },
    {
      "epoch": 1.1297360876277291,
      "grad_norm": 0.3350670337677002,
      "learning_rate": 0.00018744696731615333,
      "loss": 1.1021,
      "step": 19210
    },
    {
      "epoch": 1.1303241931926782,
      "grad_norm": 0.3964845538139343,
      "learning_rate": 0.0001873880421118793,
      "loss": 1.0529,
      "step": 19220
    },
    {
      "epoch": 1.130912298757627,
      "grad_norm": 0.3399260342121124,
      "learning_rate": 0.00018732911690760525,
      "loss": 0.987,
      "step": 19230
    },
    {
      "epoch": 1.1315004043225758,
      "grad_norm": 0.4331289231777191,
      "learning_rate": 0.00018727019170333123,
      "loss": 1.0349,
      "step": 19240
    },
    {
      "epoch": 1.1320885098875249,
      "grad_norm": 0.3477139472961426,
      "learning_rate": 0.00018721126649905718,
      "loss": 1.0585,
      "step": 19250
    },
    {
      "epoch": 1.1326766154524737,
      "grad_norm": 0.39303532242774963,
      "learning_rate": 0.00018715234129478312,
      "loss": 0.9578,
      "step": 19260
    },
    {
      "epoch": 1.1332647210174227,
      "grad_norm": 0.3025900721549988,
      "learning_rate": 0.0001870934160905091,
      "loss": 1.0251,
      "step": 19270
    },
    {
      "epoch": 1.1338528265823715,
      "grad_norm": 0.38134342432022095,
      "learning_rate": 0.00018703449088623505,
      "loss": 0.9761,
      "step": 19280
    },
    {
      "epoch": 1.1344409321473203,
      "grad_norm": 0.3283628225326538,
      "learning_rate": 0.00018697556568196102,
      "loss": 0.9676,
      "step": 19290
    },
    {
      "epoch": 1.1350290377122694,
      "grad_norm": 0.382307231426239,
      "learning_rate": 0.00018691664047768697,
      "loss": 1.0392,
      "step": 19300
    },
    {
      "epoch": 1.1356171432772182,
      "grad_norm": 0.36147549748420715,
      "learning_rate": 0.00018685771527341294,
      "loss": 1.1023,
      "step": 19310
    },
    {
      "epoch": 1.1362052488421672,
      "grad_norm": 0.3373827338218689,
      "learning_rate": 0.0001867987900691389,
      "loss": 0.9449,
      "step": 19320
    },
    {
      "epoch": 1.136793354407116,
      "grad_norm": 0.3432009816169739,
      "learning_rate": 0.00018673986486486484,
      "loss": 1.0782,
      "step": 19330
    },
    {
      "epoch": 1.137381459972065,
      "grad_norm": 0.37006863951683044,
      "learning_rate": 0.00018668093966059081,
      "loss": 1.07,
      "step": 19340
    },
    {
      "epoch": 1.137969565537014,
      "grad_norm": 0.38666802644729614,
      "learning_rate": 0.00018662201445631676,
      "loss": 0.9917,
      "step": 19350
    },
    {
      "epoch": 1.1385576711019627,
      "grad_norm": 0.44549641013145447,
      "learning_rate": 0.00018656308925204274,
      "loss": 0.9034,
      "step": 19360
    },
    {
      "epoch": 1.1391457766669117,
      "grad_norm": 0.349732905626297,
      "learning_rate": 0.00018650416404776868,
      "loss": 1.0024,
      "step": 19370
    },
    {
      "epoch": 1.1397338822318606,
      "grad_norm": 0.3967360556125641,
      "learning_rate": 0.00018644523884349466,
      "loss": 0.9805,
      "step": 19380
    },
    {
      "epoch": 1.1403219877968096,
      "grad_norm": 0.36778104305267334,
      "learning_rate": 0.0001863863136392206,
      "loss": 1.1363,
      "step": 19390
    },
    {
      "epoch": 1.1409100933617584,
      "grad_norm": 0.39141419529914856,
      "learning_rate": 0.00018632738843494655,
      "loss": 1.1142,
      "step": 19400
    },
    {
      "epoch": 1.1414981989267075,
      "grad_norm": 0.34787091612815857,
      "learning_rate": 0.00018626846323067253,
      "loss": 1.0311,
      "step": 19410
    },
    {
      "epoch": 1.1420863044916563,
      "grad_norm": 0.34636735916137695,
      "learning_rate": 0.00018620953802639848,
      "loss": 1.0629,
      "step": 19420
    },
    {
      "epoch": 1.142674410056605,
      "grad_norm": 0.3943689167499542,
      "learning_rate": 0.00018615061282212445,
      "loss": 1.0506,
      "step": 19430
    },
    {
      "epoch": 1.1432625156215541,
      "grad_norm": 0.3396618962287903,
      "learning_rate": 0.0001860916876178504,
      "loss": 1.0697,
      "step": 19440
    },
    {
      "epoch": 1.143850621186503,
      "grad_norm": 0.3883373439311981,
      "learning_rate": 0.00018603276241357637,
      "loss": 0.9716,
      "step": 19450
    },
    {
      "epoch": 1.144438726751452,
      "grad_norm": 0.3663337230682373,
      "learning_rate": 0.00018597383720930232,
      "loss": 1.0023,
      "step": 19460
    },
    {
      "epoch": 1.1450268323164008,
      "grad_norm": 0.3406410217285156,
      "learning_rate": 0.00018591491200502827,
      "loss": 0.9739,
      "step": 19470
    },
    {
      "epoch": 1.1456149378813496,
      "grad_norm": 0.34500357508659363,
      "learning_rate": 0.00018585598680075424,
      "loss": 1.1111,
      "step": 19480
    },
    {
      "epoch": 1.1462030434462986,
      "grad_norm": 0.3962405025959015,
      "learning_rate": 0.0001857970615964802,
      "loss": 1.0558,
      "step": 19490
    },
    {
      "epoch": 1.1467911490112475,
      "grad_norm": 0.3467184603214264,
      "learning_rate": 0.00018573813639220614,
      "loss": 0.9775,
      "step": 19500
    },
    {
      "epoch": 1.1473792545761965,
      "grad_norm": 0.33622321486473083,
      "learning_rate": 0.0001856792111879321,
      "loss": 1.0241,
      "step": 19510
    },
    {
      "epoch": 1.1479673601411453,
      "grad_norm": 0.36114394664764404,
      "learning_rate": 0.00018562028598365803,
      "loss": 1.0551,
      "step": 19520
    },
    {
      "epoch": 1.1485554657060941,
      "grad_norm": 0.35352176427841187,
      "learning_rate": 0.000185561360779384,
      "loss": 0.9828,
      "step": 19530
    },
    {
      "epoch": 1.1491435712710432,
      "grad_norm": 0.3139616847038269,
      "learning_rate": 0.00018550243557510996,
      "loss": 1.1214,
      "step": 19540
    },
    {
      "epoch": 1.149731676835992,
      "grad_norm": 0.3530470132827759,
      "learning_rate": 0.00018544351037083593,
      "loss": 1.0657,
      "step": 19550
    },
    {
      "epoch": 1.150319782400941,
      "grad_norm": 0.33016225695610046,
      "learning_rate": 0.00018538458516656188,
      "loss": 0.9904,
      "step": 19560
    },
    {
      "epoch": 1.1509078879658898,
      "grad_norm": 0.28693515062332153,
      "learning_rate": 0.00018532565996228785,
      "loss": 1.0196,
      "step": 19570
    },
    {
      "epoch": 1.1514959935308389,
      "grad_norm": 0.3509535491466522,
      "learning_rate": 0.0001852667347580138,
      "loss": 1.0154,
      "step": 19580
    },
    {
      "epoch": 1.1520840990957877,
      "grad_norm": 0.39895185828208923,
      "learning_rate": 0.00018520780955373975,
      "loss": 1.0159,
      "step": 19590
    },
    {
      "epoch": 1.1526722046607367,
      "grad_norm": 0.3719369173049927,
      "learning_rate": 0.00018514888434946572,
      "loss": 0.9918,
      "step": 19600
    },
    {
      "epoch": 1.1532603102256855,
      "grad_norm": 0.376097708940506,
      "learning_rate": 0.00018508995914519167,
      "loss": 1.0499,
      "step": 19610
    },
    {
      "epoch": 1.1538484157906344,
      "grad_norm": 0.33779841661453247,
      "learning_rate": 0.00018503103394091765,
      "loss": 1.0148,
      "step": 19620
    },
    {
      "epoch": 1.1544365213555834,
      "grad_norm": 0.3264074921607971,
      "learning_rate": 0.0001849721087366436,
      "loss": 0.9919,
      "step": 19630
    },
    {
      "epoch": 1.1550246269205322,
      "grad_norm": 0.31610795855522156,
      "learning_rate": 0.00018491318353236957,
      "loss": 1.0229,
      "step": 19640
    },
    {
      "epoch": 1.1556127324854812,
      "grad_norm": 0.32579466700553894,
      "learning_rate": 0.00018485425832809552,
      "loss": 1.0477,
      "step": 19650
    },
    {
      "epoch": 1.15620083805043,
      "grad_norm": 0.35636335611343384,
      "learning_rate": 0.00018479533312382146,
      "loss": 1.0719,
      "step": 19660
    },
    {
      "epoch": 1.1567889436153789,
      "grad_norm": 0.31810951232910156,
      "learning_rate": 0.00018473640791954744,
      "loss": 1.0507,
      "step": 19670
    },
    {
      "epoch": 1.157377049180328,
      "grad_norm": 0.36358582973480225,
      "learning_rate": 0.0001846774827152734,
      "loss": 1.0872,
      "step": 19680
    },
    {
      "epoch": 1.1579651547452767,
      "grad_norm": 0.3423285484313965,
      "learning_rate": 0.00018461855751099936,
      "loss": 0.9588,
      "step": 19690
    },
    {
      "epoch": 1.1585532603102258,
      "grad_norm": 0.3259040117263794,
      "learning_rate": 0.0001845596323067253,
      "loss": 0.9597,
      "step": 19700
    },
    {
      "epoch": 1.1591413658751746,
      "grad_norm": 0.35483279824256897,
      "learning_rate": 0.00018450070710245128,
      "loss": 0.9738,
      "step": 19710
    },
    {
      "epoch": 1.1597294714401234,
      "grad_norm": 0.3441406786441803,
      "learning_rate": 0.00018444178189817723,
      "loss": 0.9986,
      "step": 19720
    },
    {
      "epoch": 1.1603175770050724,
      "grad_norm": 0.3404577374458313,
      "learning_rate": 0.00018438285669390318,
      "loss": 1.0459,
      "step": 19730
    },
    {
      "epoch": 1.1609056825700212,
      "grad_norm": 0.3737490475177765,
      "learning_rate": 0.00018432393148962915,
      "loss": 0.9633,
      "step": 19740
    },
    {
      "epoch": 1.1614937881349703,
      "grad_norm": 0.3185754120349884,
      "learning_rate": 0.0001842650062853551,
      "loss": 1.0574,
      "step": 19750
    },
    {
      "epoch": 1.162081893699919,
      "grad_norm": 0.358087956905365,
      "learning_rate": 0.00018420608108108108,
      "loss": 0.9884,
      "step": 19760
    },
    {
      "epoch": 1.1626699992648681,
      "grad_norm": 0.3357989490032196,
      "learning_rate": 0.00018414715587680702,
      "loss": 1.0129,
      "step": 19770
    },
    {
      "epoch": 1.163258104829817,
      "grad_norm": 0.3274150788784027,
      "learning_rate": 0.000184088230672533,
      "loss": 1.1446,
      "step": 19780
    },
    {
      "epoch": 1.1638462103947658,
      "grad_norm": 0.3458513617515564,
      "learning_rate": 0.00018402930546825895,
      "loss": 0.9924,
      "step": 19790
    },
    {
      "epoch": 1.1644343159597148,
      "grad_norm": 0.33606499433517456,
      "learning_rate": 0.0001839703802639849,
      "loss": 1.058,
      "step": 19800
    },
    {
      "epoch": 1.1650224215246636,
      "grad_norm": 0.3443149924278259,
      "learning_rate": 0.00018391145505971087,
      "loss": 1.0559,
      "step": 19810
    },
    {
      "epoch": 1.1656105270896127,
      "grad_norm": 0.3744921088218689,
      "learning_rate": 0.00018385252985543682,
      "loss": 1.071,
      "step": 19820
    },
    {
      "epoch": 1.1661986326545615,
      "grad_norm": 0.33131951093673706,
      "learning_rate": 0.0001837936046511628,
      "loss": 0.9883,
      "step": 19830
    },
    {
      "epoch": 1.1667867382195105,
      "grad_norm": 0.32882654666900635,
      "learning_rate": 0.00018373467944688874,
      "loss": 1.0212,
      "step": 19840
    },
    {
      "epoch": 1.1673748437844593,
      "grad_norm": 0.3584596514701843,
      "learning_rate": 0.00018367575424261471,
      "loss": 0.9851,
      "step": 19850
    },
    {
      "epoch": 1.1679629493494081,
      "grad_norm": 0.3405468165874481,
      "learning_rate": 0.00018361682903834066,
      "loss": 1.0457,
      "step": 19860
    },
    {
      "epoch": 1.1685510549143572,
      "grad_norm": 0.36773914098739624,
      "learning_rate": 0.0001835579038340666,
      "loss": 1.0806,
      "step": 19870
    },
    {
      "epoch": 1.169139160479306,
      "grad_norm": 0.354452908039093,
      "learning_rate": 0.00018349897862979258,
      "loss": 0.9726,
      "step": 19880
    },
    {
      "epoch": 1.169727266044255,
      "grad_norm": 0.36042413115501404,
      "learning_rate": 0.00018344005342551853,
      "loss": 0.9865,
      "step": 19890
    },
    {
      "epoch": 1.1703153716092038,
      "grad_norm": 0.3398341238498688,
      "learning_rate": 0.0001833811282212445,
      "loss": 1.0676,
      "step": 19900
    },
    {
      "epoch": 1.1709034771741527,
      "grad_norm": 0.3769535720348358,
      "learning_rate": 0.00018332220301697045,
      "loss": 1.0014,
      "step": 19910
    },
    {
      "epoch": 1.1714915827391017,
      "grad_norm": 0.37156742811203003,
      "learning_rate": 0.00018326327781269637,
      "loss": 1.0883,
      "step": 19920
    },
    {
      "epoch": 1.1720796883040505,
      "grad_norm": 0.3213960528373718,
      "learning_rate": 0.00018320435260842235,
      "loss": 0.9649,
      "step": 19930
    },
    {
      "epoch": 1.1726677938689996,
      "grad_norm": 0.3624028265476227,
      "learning_rate": 0.0001831454274041483,
      "loss": 1.0468,
      "step": 19940
    },
    {
      "epoch": 1.1732558994339484,
      "grad_norm": 0.3313309848308563,
      "learning_rate": 0.00018308650219987427,
      "loss": 0.9606,
      "step": 19950
    },
    {
      "epoch": 1.1738440049988972,
      "grad_norm": 0.3429575562477112,
      "learning_rate": 0.00018302757699560022,
      "loss": 1.0968,
      "step": 19960
    },
    {
      "epoch": 1.1744321105638462,
      "grad_norm": 0.3552027642726898,
      "learning_rate": 0.0001829686517913262,
      "loss": 0.9732,
      "step": 19970
    },
    {
      "epoch": 1.175020216128795,
      "grad_norm": 0.3193657696247101,
      "learning_rate": 0.00018290972658705214,
      "loss": 1.0161,
      "step": 19980
    },
    {
      "epoch": 1.175608321693744,
      "grad_norm": 0.3353422284126282,
      "learning_rate": 0.0001828508013827781,
      "loss": 1.0147,
      "step": 19990
    },
    {
      "epoch": 1.176196427258693,
      "grad_norm": 0.3448340892791748,
      "learning_rate": 0.00018279187617850406,
      "loss": 1.0673,
      "step": 20000
    },
    {
      "epoch": 1.176784532823642,
      "grad_norm": 0.3251631557941437,
      "learning_rate": 0.00018273295097423,
      "loss": 0.9806,
      "step": 20010
    },
    {
      "epoch": 1.1773726383885907,
      "grad_norm": 0.37861523032188416,
      "learning_rate": 0.00018267402576995599,
      "loss": 1.1321,
      "step": 20020
    },
    {
      "epoch": 1.1779607439535398,
      "grad_norm": 0.32528162002563477,
      "learning_rate": 0.00018262099308610934,
      "loss": 1.0321,
      "step": 20030
    },
    {
      "epoch": 1.1785488495184886,
      "grad_norm": 0.369454026222229,
      "learning_rate": 0.0001825620678818353,
      "loss": 1.0355,
      "step": 20040
    },
    {
      "epoch": 1.1791369550834374,
      "grad_norm": 0.3486360013484955,
      "learning_rate": 0.00018250314267756126,
      "loss": 1.0814,
      "step": 20050
    },
    {
      "epoch": 1.1797250606483864,
      "grad_norm": 0.4172245264053345,
      "learning_rate": 0.0001824442174732872,
      "loss": 1.0102,
      "step": 20060
    },
    {
      "epoch": 1.1803131662133353,
      "grad_norm": 0.4014687240123749,
      "learning_rate": 0.00018238529226901316,
      "loss": 1.0343,
      "step": 20070
    },
    {
      "epoch": 1.1809012717782843,
      "grad_norm": 0.32086730003356934,
      "learning_rate": 0.00018232636706473913,
      "loss": 1.018,
      "step": 20080
    },
    {
      "epoch": 1.1814893773432331,
      "grad_norm": 0.3038152754306793,
      "learning_rate": 0.00018226744186046508,
      "loss": 1.0333,
      "step": 20090
    },
    {
      "epoch": 1.182077482908182,
      "grad_norm": 0.4042797386646271,
      "learning_rate": 0.00018220851665619106,
      "loss": 1.0557,
      "step": 20100
    },
    {
      "epoch": 1.182665588473131,
      "grad_norm": 0.340887188911438,
      "learning_rate": 0.000182149591451917,
      "loss": 1.1636,
      "step": 20110
    },
    {
      "epoch": 1.1832536940380798,
      "grad_norm": 0.35700806975364685,
      "learning_rate": 0.00018209066624764298,
      "loss": 1.0786,
      "step": 20120
    },
    {
      "epoch": 1.1838417996030288,
      "grad_norm": 0.3456757664680481,
      "learning_rate": 0.00018203174104336893,
      "loss": 0.998,
      "step": 20130
    },
    {
      "epoch": 1.1844299051679776,
      "grad_norm": 0.3359755575656891,
      "learning_rate": 0.00018197281583909487,
      "loss": 1.0713,
      "step": 20140
    },
    {
      "epoch": 1.1850180107329265,
      "grad_norm": 0.3496888279914856,
      "learning_rate": 0.00018191389063482085,
      "loss": 1.0363,
      "step": 20150
    },
    {
      "epoch": 1.1856061162978755,
      "grad_norm": 0.3354261815547943,
      "learning_rate": 0.0001818549654305468,
      "loss": 1.0558,
      "step": 20160
    },
    {
      "epoch": 1.1861942218628243,
      "grad_norm": 0.35720014572143555,
      "learning_rate": 0.00018179604022627277,
      "loss": 1.0099,
      "step": 20170
    },
    {
      "epoch": 1.1867823274277733,
      "grad_norm": 0.35717588663101196,
      "learning_rate": 0.00018173711502199872,
      "loss": 1.0311,
      "step": 20180
    },
    {
      "epoch": 1.1873704329927222,
      "grad_norm": 0.4011683762073517,
      "learning_rate": 0.0001816781898177247,
      "loss": 0.9236,
      "step": 20190
    },
    {
      "epoch": 1.1879585385576712,
      "grad_norm": 0.32435402274131775,
      "learning_rate": 0.00018161926461345064,
      "loss": 1.0602,
      "step": 20200
    },
    {
      "epoch": 1.18854664412262,
      "grad_norm": 0.33088698983192444,
      "learning_rate": 0.00018156033940917662,
      "loss": 1.0082,
      "step": 20210
    },
    {
      "epoch": 1.1891347496875688,
      "grad_norm": 0.3643750548362732,
      "learning_rate": 0.00018150141420490256,
      "loss": 1.1379,
      "step": 20220
    },
    {
      "epoch": 1.1897228552525179,
      "grad_norm": 0.3318164348602295,
      "learning_rate": 0.0001814424890006285,
      "loss": 1.0927,
      "step": 20230
    },
    {
      "epoch": 1.1903109608174667,
      "grad_norm": 0.32312101125717163,
      "learning_rate": 0.00018138356379635449,
      "loss": 1.0655,
      "step": 20240
    },
    {
      "epoch": 1.1908990663824157,
      "grad_norm": 0.33093270659446716,
      "learning_rate": 0.00018132463859208043,
      "loss": 1.0345,
      "step": 20250
    },
    {
      "epoch": 1.1914871719473645,
      "grad_norm": 0.3559545576572418,
      "learning_rate": 0.0001812657133878064,
      "loss": 0.9836,
      "step": 20260
    },
    {
      "epoch": 1.1920752775123136,
      "grad_norm": 0.33043962717056274,
      "learning_rate": 0.00018120678818353236,
      "loss": 0.9998,
      "step": 20270
    },
    {
      "epoch": 1.1926633830772624,
      "grad_norm": 0.3185589909553528,
      "learning_rate": 0.00018114786297925833,
      "loss": 0.9703,
      "step": 20280
    },
    {
      "epoch": 1.1932514886422112,
      "grad_norm": 0.3666468560695648,
      "learning_rate": 0.00018108893777498428,
      "loss": 1.1133,
      "step": 20290
    },
    {
      "epoch": 1.1938395942071602,
      "grad_norm": 0.35977619886398315,
      "learning_rate": 0.00018103001257071023,
      "loss": 1.0088,
      "step": 20300
    },
    {
      "epoch": 1.194427699772109,
      "grad_norm": 0.332046240568161,
      "learning_rate": 0.0001809710873664362,
      "loss": 0.9931,
      "step": 20310
    },
    {
      "epoch": 1.195015805337058,
      "grad_norm": 0.3424686789512634,
      "learning_rate": 0.00018091216216216215,
      "loss": 1.0073,
      "step": 20320
    },
    {
      "epoch": 1.195603910902007,
      "grad_norm": 0.36176225543022156,
      "learning_rate": 0.00018085323695788812,
      "loss": 1.0524,
      "step": 20330
    },
    {
      "epoch": 1.1961920164669557,
      "grad_norm": 0.3337569236755371,
      "learning_rate": 0.00018079431175361407,
      "loss": 1.0431,
      "step": 20340
    },
    {
      "epoch": 1.1967801220319048,
      "grad_norm": 0.34512758255004883,
      "learning_rate": 0.00018073538654934005,
      "loss": 1.0691,
      "step": 20350
    },
    {
      "epoch": 1.1973682275968536,
      "grad_norm": 0.3675364553928375,
      "learning_rate": 0.000180676461345066,
      "loss": 1.0051,
      "step": 20360
    },
    {
      "epoch": 1.1979563331618026,
      "grad_norm": 0.3372431695461273,
      "learning_rate": 0.00018061753614079194,
      "loss": 0.9683,
      "step": 20370
    },
    {
      "epoch": 1.1985444387267514,
      "grad_norm": 0.37654048204421997,
      "learning_rate": 0.00018055861093651792,
      "loss": 0.9887,
      "step": 20380
    },
    {
      "epoch": 1.1991325442917002,
      "grad_norm": 0.36124128103256226,
      "learning_rate": 0.00018049968573224386,
      "loss": 1.0306,
      "step": 20390
    },
    {
      "epoch": 1.1997206498566493,
      "grad_norm": 0.34451282024383545,
      "learning_rate": 0.00018044076052796984,
      "loss": 1.0746,
      "step": 20400
    },
    {
      "epoch": 1.200308755421598,
      "grad_norm": 0.3193647563457489,
      "learning_rate": 0.00018038183532369579,
      "loss": 0.9837,
      "step": 20410
    },
    {
      "epoch": 1.2008968609865471,
      "grad_norm": 0.40451911091804504,
      "learning_rate": 0.0001803229101194217,
      "loss": 1.0157,
      "step": 20420
    },
    {
      "epoch": 1.201484966551496,
      "grad_norm": 0.3060506284236908,
      "learning_rate": 0.00018026398491514768,
      "loss": 0.9849,
      "step": 20430
    },
    {
      "epoch": 1.202073072116445,
      "grad_norm": 0.31688347458839417,
      "learning_rate": 0.00018020505971087363,
      "loss": 1.0944,
      "step": 20440
    },
    {
      "epoch": 1.2026611776813938,
      "grad_norm": 0.34599539637565613,
      "learning_rate": 0.0001801461345065996,
      "loss": 0.982,
      "step": 20450
    },
    {
      "epoch": 1.2032492832463428,
      "grad_norm": 0.4129278361797333,
      "learning_rate": 0.00018008720930232555,
      "loss": 1.0699,
      "step": 20460
    },
    {
      "epoch": 1.2038373888112917,
      "grad_norm": 0.36698397994041443,
      "learning_rate": 0.0001800282840980515,
      "loss": 1.011,
      "step": 20470
    },
    {
      "epoch": 1.2044254943762405,
      "grad_norm": 0.3613888621330261,
      "learning_rate": 0.00017996935889377747,
      "loss": 1.0674,
      "step": 20480
    },
    {
      "epoch": 1.2050135999411895,
      "grad_norm": 0.3866070508956909,
      "learning_rate": 0.00017991043368950342,
      "loss": 1.0057,
      "step": 20490
    },
    {
      "epoch": 1.2056017055061383,
      "grad_norm": 0.3254486620426178,
      "learning_rate": 0.0001798515084852294,
      "loss": 1.0313,
      "step": 20500
    },
    {
      "epoch": 1.2061898110710874,
      "grad_norm": 0.31909191608428955,
      "learning_rate": 0.00017979258328095534,
      "loss": 1.1278,
      "step": 20510
    },
    {
      "epoch": 1.2067779166360362,
      "grad_norm": 0.3620762228965759,
      "learning_rate": 0.00017973365807668132,
      "loss": 1.0341,
      "step": 20520
    },
    {
      "epoch": 1.207366022200985,
      "grad_norm": 0.37701234221458435,
      "learning_rate": 0.00017967473287240727,
      "loss": 0.9772,
      "step": 20530
    },
    {
      "epoch": 1.207954127765934,
      "grad_norm": 0.35346198081970215,
      "learning_rate": 0.00017961580766813321,
      "loss": 1.0133,
      "step": 20540
    },
    {
      "epoch": 1.2085422333308828,
      "grad_norm": 0.3212798237800598,
      "learning_rate": 0.0001795568824638592,
      "loss": 1.0437,
      "step": 20550
    },
    {
      "epoch": 1.2091303388958319,
      "grad_norm": 0.2932887673377991,
      "learning_rate": 0.00017949795725958514,
      "loss": 1.0578,
      "step": 20560
    },
    {
      "epoch": 1.2097184444607807,
      "grad_norm": 0.4404144883155823,
      "learning_rate": 0.0001794390320553111,
      "loss": 0.9242,
      "step": 20570
    },
    {
      "epoch": 1.2103065500257295,
      "grad_norm": 0.3276449739933014,
      "learning_rate": 0.00017938010685103706,
      "loss": 0.9683,
      "step": 20580
    },
    {
      "epoch": 1.2108946555906785,
      "grad_norm": 0.36364126205444336,
      "learning_rate": 0.00017932118164676303,
      "loss": 1.0996,
      "step": 20590
    },
    {
      "epoch": 1.2114827611556274,
      "grad_norm": 0.31683412194252014,
      "learning_rate": 0.00017926225644248898,
      "loss": 1.0716,
      "step": 20600
    },
    {
      "epoch": 1.2120708667205764,
      "grad_norm": 0.3294946253299713,
      "learning_rate": 0.00017920333123821493,
      "loss": 0.9564,
      "step": 20610
    },
    {
      "epoch": 1.2126589722855252,
      "grad_norm": 0.3577755391597748,
      "learning_rate": 0.0001791444060339409,
      "loss": 0.9949,
      "step": 20620
    },
    {
      "epoch": 1.2132470778504743,
      "grad_norm": 0.34262222051620483,
      "learning_rate": 0.00017908548082966685,
      "loss": 1.0765,
      "step": 20630
    },
    {
      "epoch": 1.213835183415423,
      "grad_norm": 0.337417334318161,
      "learning_rate": 0.00017902655562539283,
      "loss": 1.067,
      "step": 20640
    },
    {
      "epoch": 1.2144232889803719,
      "grad_norm": 0.35078009963035583,
      "learning_rate": 0.00017896763042111877,
      "loss": 1.0378,
      "step": 20650
    },
    {
      "epoch": 1.215011394545321,
      "grad_norm": 0.40182560682296753,
      "learning_rate": 0.00017890870521684475,
      "loss": 1.1231,
      "step": 20660
    },
    {
      "epoch": 1.2155995001102697,
      "grad_norm": 0.3525107502937317,
      "learning_rate": 0.0001788497800125707,
      "loss": 1.046,
      "step": 20670
    },
    {
      "epoch": 1.2161876056752188,
      "grad_norm": 0.3559602200984955,
      "learning_rate": 0.00017879085480829667,
      "loss": 1.0599,
      "step": 20680
    },
    {
      "epoch": 1.2167757112401676,
      "grad_norm": 0.3422512114048004,
      "learning_rate": 0.00017873192960402262,
      "loss": 1.1051,
      "step": 20690
    },
    {
      "epoch": 1.2173638168051166,
      "grad_norm": 0.3590051531791687,
      "learning_rate": 0.00017867300439974857,
      "loss": 1.0465,
      "step": 20700
    },
    {
      "epoch": 1.2179519223700654,
      "grad_norm": 0.3470859229564667,
      "learning_rate": 0.00017861407919547454,
      "loss": 1.0495,
      "step": 20710
    },
    {
      "epoch": 1.2185400279350143,
      "grad_norm": 0.3511834442615509,
      "learning_rate": 0.0001785551539912005,
      "loss": 1.0358,
      "step": 20720
    },
    {
      "epoch": 1.2191281334999633,
      "grad_norm": 0.38899722695350647,
      "learning_rate": 0.00017849622878692646,
      "loss": 1.0104,
      "step": 20730
    },
    {
      "epoch": 1.2197162390649121,
      "grad_norm": 0.3940136134624481,
      "learning_rate": 0.0001784373035826524,
      "loss": 1.1257,
      "step": 20740
    },
    {
      "epoch": 1.2203043446298611,
      "grad_norm": 0.34340548515319824,
      "learning_rate": 0.00017837837837837839,
      "loss": 1.0297,
      "step": 20750
    },
    {
      "epoch": 1.22089245019481,
      "grad_norm": 0.35762327909469604,
      "learning_rate": 0.00017831945317410433,
      "loss": 0.9925,
      "step": 20760
    },
    {
      "epoch": 1.2214805557597588,
      "grad_norm": 0.39373794198036194,
      "learning_rate": 0.00017826052796983028,
      "loss": 1.0175,
      "step": 20770
    },
    {
      "epoch": 1.2220686613247078,
      "grad_norm": 0.32705801725387573,
      "learning_rate": 0.00017820160276555626,
      "loss": 1.0445,
      "step": 20780
    },
    {
      "epoch": 1.2226567668896566,
      "grad_norm": 0.365542471408844,
      "learning_rate": 0.0001781426775612822,
      "loss": 0.9688,
      "step": 20790
    },
    {
      "epoch": 1.2232448724546057,
      "grad_norm": 0.38727307319641113,
      "learning_rate": 0.00017808375235700818,
      "loss": 1.0645,
      "step": 20800
    },
    {
      "epoch": 1.2238329780195545,
      "grad_norm": 0.39495646953582764,
      "learning_rate": 0.00017802482715273413,
      "loss": 1.0238,
      "step": 20810
    },
    {
      "epoch": 1.2244210835845033,
      "grad_norm": 0.33680132031440735,
      "learning_rate": 0.0001779659019484601,
      "loss": 0.995,
      "step": 20820
    },
    {
      "epoch": 1.2250091891494523,
      "grad_norm": 0.3542320132255554,
      "learning_rate": 0.00017790697674418605,
      "loss": 1.0763,
      "step": 20830
    },
    {
      "epoch": 1.2255972947144012,
      "grad_norm": 0.3420284688472748,
      "learning_rate": 0.00017784805153991197,
      "loss": 1.0075,
      "step": 20840
    },
    {
      "epoch": 1.2261854002793502,
      "grad_norm": 0.33142077922821045,
      "learning_rate": 0.00017778912633563794,
      "loss": 1.0445,
      "step": 20850
    },
    {
      "epoch": 1.226773505844299,
      "grad_norm": 0.33166617155075073,
      "learning_rate": 0.0001777302011313639,
      "loss": 1.0326,
      "step": 20860
    },
    {
      "epoch": 1.227361611409248,
      "grad_norm": 0.3959124684333801,
      "learning_rate": 0.00017767127592708984,
      "loss": 1.0501,
      "step": 20870
    },
    {
      "epoch": 1.2279497169741969,
      "grad_norm": 0.33011406660079956,
      "learning_rate": 0.0001776123507228158,
      "loss": 0.9541,
      "step": 20880
    },
    {
      "epoch": 1.228537822539146,
      "grad_norm": 0.3157108724117279,
      "learning_rate": 0.00017755342551854176,
      "loss": 1.0372,
      "step": 20890
    },
    {
      "epoch": 1.2291259281040947,
      "grad_norm": 0.33246049284935,
      "learning_rate": 0.00017749450031426774,
      "loss": 1.0119,
      "step": 20900
    },
    {
      "epoch": 1.2297140336690435,
      "grad_norm": 0.3698597550392151,
      "learning_rate": 0.00017743557510999368,
      "loss": 1.0269,
      "step": 20910
    },
    {
      "epoch": 1.2303021392339926,
      "grad_norm": 0.3647659420967102,
      "learning_rate": 0.00017737664990571966,
      "loss": 0.9799,
      "step": 20920
    },
    {
      "epoch": 1.2308902447989414,
      "grad_norm": 0.3100378215312958,
      "learning_rate": 0.0001773177247014456,
      "loss": 1.0388,
      "step": 20930
    },
    {
      "epoch": 1.2314783503638904,
      "grad_norm": 0.3066404163837433,
      "learning_rate": 0.00017725879949717155,
      "loss": 1.0503,
      "step": 20940
    },
    {
      "epoch": 1.2320664559288392,
      "grad_norm": 0.3000946640968323,
      "learning_rate": 0.00017719987429289753,
      "loss": 1.0296,
      "step": 20950
    },
    {
      "epoch": 1.232654561493788,
      "grad_norm": 0.3080751895904541,
      "learning_rate": 0.00017714094908862348,
      "loss": 1.0739,
      "step": 20960
    },
    {
      "epoch": 1.233242667058737,
      "grad_norm": 0.337875097990036,
      "learning_rate": 0.00017708202388434945,
      "loss": 1.0835,
      "step": 20970
    },
    {
      "epoch": 1.233830772623686,
      "grad_norm": 0.34671658277511597,
      "learning_rate": 0.0001770230986800754,
      "loss": 1.0079,
      "step": 20980
    },
    {
      "epoch": 1.234418878188635,
      "grad_norm": 0.4368517994880676,
      "learning_rate": 0.00017696417347580137,
      "loss": 1.0869,
      "step": 20990
    },
    {
      "epoch": 1.2350069837535838,
      "grad_norm": 0.3191502094268799,
      "learning_rate": 0.00017690524827152732,
      "loss": 0.9668,
      "step": 21000
    },
    {
      "epoch": 1.2355950893185326,
      "grad_norm": 0.3392443060874939,
      "learning_rate": 0.00017684632306725327,
      "loss": 1.0619,
      "step": 21010
    },
    {
      "epoch": 1.2361831948834816,
      "grad_norm": 0.37518271803855896,
      "learning_rate": 0.00017678739786297924,
      "loss": 0.8841,
      "step": 21020
    },
    {
      "epoch": 1.2367713004484304,
      "grad_norm": 0.34687694907188416,
      "learning_rate": 0.0001767284726587052,
      "loss": 0.9766,
      "step": 21030
    },
    {
      "epoch": 1.2373594060133795,
      "grad_norm": 0.343555748462677,
      "learning_rate": 0.00017666954745443117,
      "loss": 1.0139,
      "step": 21040
    },
    {
      "epoch": 1.2379475115783283,
      "grad_norm": 0.40595772862434387,
      "learning_rate": 0.0001766106222501571,
      "loss": 1.0156,
      "step": 21050
    },
    {
      "epoch": 1.2385356171432773,
      "grad_norm": 0.3641795516014099,
      "learning_rate": 0.0001765516970458831,
      "loss": 0.9839,
      "step": 21060
    },
    {
      "epoch": 1.2391237227082261,
      "grad_norm": 0.40528246760368347,
      "learning_rate": 0.00017649277184160904,
      "loss": 0.9433,
      "step": 21070
    },
    {
      "epoch": 1.239711828273175,
      "grad_norm": 0.34946638345718384,
      "learning_rate": 0.000176433846637335,
      "loss": 1.0478,
      "step": 21080
    },
    {
      "epoch": 1.240299933838124,
      "grad_norm": 0.3481043577194214,
      "learning_rate": 0.00017637492143306096,
      "loss": 0.9538,
      "step": 21090
    },
    {
      "epoch": 1.2408880394030728,
      "grad_norm": 0.3133760988712311,
      "learning_rate": 0.0001763159962287869,
      "loss": 1.0272,
      "step": 21100
    },
    {
      "epoch": 1.2414761449680218,
      "grad_norm": 0.3525311052799225,
      "learning_rate": 0.00017625707102451288,
      "loss": 0.9575,
      "step": 21110
    },
    {
      "epoch": 1.2420642505329706,
      "grad_norm": 0.37829113006591797,
      "learning_rate": 0.00017619814582023883,
      "loss": 0.921,
      "step": 21120
    },
    {
      "epoch": 1.2426523560979197,
      "grad_norm": 0.3575631380081177,
      "learning_rate": 0.0001761392206159648,
      "loss": 1.1559,
      "step": 21130
    },
    {
      "epoch": 1.2432404616628685,
      "grad_norm": 0.3602418899536133,
      "learning_rate": 0.00017608029541169075,
      "loss": 1.0657,
      "step": 21140
    },
    {
      "epoch": 1.2438285672278173,
      "grad_norm": 0.37311047315597534,
      "learning_rate": 0.00017602137020741672,
      "loss": 0.9351,
      "step": 21150
    },
    {
      "epoch": 1.2444166727927664,
      "grad_norm": 0.437702476978302,
      "learning_rate": 0.00017596244500314267,
      "loss": 0.99,
      "step": 21160
    },
    {
      "epoch": 1.2450047783577152,
      "grad_norm": 0.33737918734550476,
      "learning_rate": 0.00017590351979886862,
      "loss": 1.0875,
      "step": 21170
    },
    {
      "epoch": 1.2455928839226642,
      "grad_norm": 0.3313477039337158,
      "learning_rate": 0.0001758445945945946,
      "loss": 1.0468,
      "step": 21180
    },
    {
      "epoch": 1.246180989487613,
      "grad_norm": 0.35743334889411926,
      "learning_rate": 0.00017578566939032054,
      "loss": 0.98,
      "step": 21190
    },
    {
      "epoch": 1.2467690950525618,
      "grad_norm": 0.28947922587394714,
      "learning_rate": 0.00017572674418604652,
      "loss": 1.0482,
      "step": 21200
    },
    {
      "epoch": 1.2473572006175109,
      "grad_norm": 0.3855493664741516,
      "learning_rate": 0.00017566781898177247,
      "loss": 1.0357,
      "step": 21210
    },
    {
      "epoch": 1.2479453061824597,
      "grad_norm": 0.35302817821502686,
      "learning_rate": 0.00017560889377749844,
      "loss": 0.9845,
      "step": 21220
    },
    {
      "epoch": 1.2485334117474087,
      "grad_norm": 0.45080944895744324,
      "learning_rate": 0.0001755499685732244,
      "loss": 0.9794,
      "step": 21230
    },
    {
      "epoch": 1.2491215173123575,
      "grad_norm": 0.35070136189460754,
      "learning_rate": 0.00017549104336895034,
      "loss": 0.9531,
      "step": 21240
    },
    {
      "epoch": 1.2497096228773064,
      "grad_norm": 0.3379538357257843,
      "learning_rate": 0.0001754321181646763,
      "loss": 1.0774,
      "step": 21250
    },
    {
      "epoch": 1.2502977284422554,
      "grad_norm": 0.3116826117038727,
      "learning_rate": 0.00017537319296040223,
      "loss": 0.9547,
      "step": 21260
    },
    {
      "epoch": 1.2508858340072042,
      "grad_norm": 0.3421974778175354,
      "learning_rate": 0.00017531426775612818,
      "loss": 1.0199,
      "step": 21270
    },
    {
      "epoch": 1.2514739395721532,
      "grad_norm": 0.3587269186973572,
      "learning_rate": 0.00017525534255185415,
      "loss": 1.0581,
      "step": 21280
    },
    {
      "epoch": 1.252062045137102,
      "grad_norm": 0.3194705545902252,
      "learning_rate": 0.0001751964173475801,
      "loss": 1.0278,
      "step": 21290
    },
    {
      "epoch": 1.2526501507020509,
      "grad_norm": 0.35452863574028015,
      "learning_rate": 0.00017513749214330608,
      "loss": 1.0703,
      "step": 21300
    },
    {
      "epoch": 1.253238256267,
      "grad_norm": 0.36151397228240967,
      "learning_rate": 0.00017507856693903202,
      "loss": 1.0706,
      "step": 21310
    },
    {
      "epoch": 1.253826361831949,
      "grad_norm": 0.3295113146305084,
      "learning_rate": 0.000175019641734758,
      "loss": 1.0172,
      "step": 21320
    },
    {
      "epoch": 1.2544144673968978,
      "grad_norm": 0.34008097648620605,
      "learning_rate": 0.00017496071653048395,
      "loss": 0.9859,
      "step": 21330
    },
    {
      "epoch": 1.2550025729618466,
      "grad_norm": 0.3584437668323517,
      "learning_rate": 0.0001749017913262099,
      "loss": 1.0128,
      "step": 21340
    },
    {
      "epoch": 1.2555906785267956,
      "grad_norm": 0.3596284091472626,
      "learning_rate": 0.00017484286612193587,
      "loss": 0.9429,
      "step": 21350
    },
    {
      "epoch": 1.2561787840917444,
      "grad_norm": 0.3946214020252228,
      "learning_rate": 0.00017478394091766182,
      "loss": 1.0197,
      "step": 21360
    },
    {
      "epoch": 1.2567668896566935,
      "grad_norm": 0.34488150477409363,
      "learning_rate": 0.0001747250157133878,
      "loss": 0.9717,
      "step": 21370
    },
    {
      "epoch": 1.2573549952216423,
      "grad_norm": 0.3183024525642395,
      "learning_rate": 0.00017466609050911374,
      "loss": 1.0572,
      "step": 21380
    },
    {
      "epoch": 1.257943100786591,
      "grad_norm": 0.36380329728126526,
      "learning_rate": 0.0001746071653048397,
      "loss": 1.0937,
      "step": 21390
    },
    {
      "epoch": 1.2585312063515401,
      "grad_norm": 0.3334023356437683,
      "learning_rate": 0.00017454824010056566,
      "loss": 1.0014,
      "step": 21400
    },
    {
      "epoch": 1.259119311916489,
      "grad_norm": 0.32776838541030884,
      "learning_rate": 0.0001744893148962916,
      "loss": 0.988,
      "step": 21410
    },
    {
      "epoch": 1.259707417481438,
      "grad_norm": 0.3635835647583008,
      "learning_rate": 0.00017443038969201758,
      "loss": 1.0205,
      "step": 21420
    },
    {
      "epoch": 1.2602955230463868,
      "grad_norm": 0.3838821053504944,
      "learning_rate": 0.00017437146448774353,
      "loss": 0.9924,
      "step": 21430
    },
    {
      "epoch": 1.2608836286113356,
      "grad_norm": 0.3460281789302826,
      "learning_rate": 0.0001743125392834695,
      "loss": 1.035,
      "step": 21440
    },
    {
      "epoch": 1.2614717341762847,
      "grad_norm": 0.3221007287502289,
      "learning_rate": 0.00017425361407919545,
      "loss": 1.034,
      "step": 21450
    },
    {
      "epoch": 1.2620598397412335,
      "grad_norm": 0.3372153639793396,
      "learning_rate": 0.00017419468887492143,
      "loss": 0.925,
      "step": 21460
    },
    {
      "epoch": 1.2626479453061825,
      "grad_norm": 0.35903626680374146,
      "learning_rate": 0.00017413576367064737,
      "loss": 1.0316,
      "step": 21470
    },
    {
      "epoch": 1.2632360508711313,
      "grad_norm": 0.36298197507858276,
      "learning_rate": 0.00017407683846637335,
      "loss": 1.0716,
      "step": 21480
    },
    {
      "epoch": 1.2638241564360801,
      "grad_norm": 0.40277600288391113,
      "learning_rate": 0.0001740179132620993,
      "loss": 1.007,
      "step": 21490
    },
    {
      "epoch": 1.2644122620010292,
      "grad_norm": 0.3332656919956207,
      "learning_rate": 0.00017395898805782524,
      "loss": 1.0211,
      "step": 21500
    },
    {
      "epoch": 1.2650003675659782,
      "grad_norm": 0.40207672119140625,
      "learning_rate": 0.00017390006285355122,
      "loss": 0.9938,
      "step": 21510
    },
    {
      "epoch": 1.265588473130927,
      "grad_norm": 0.33240175247192383,
      "learning_rate": 0.00017384113764927717,
      "loss": 1.0527,
      "step": 21520
    },
    {
      "epoch": 1.2661765786958759,
      "grad_norm": 0.3186228573322296,
      "learning_rate": 0.00017378221244500314,
      "loss": 0.9954,
      "step": 21530
    },
    {
      "epoch": 1.266764684260825,
      "grad_norm": 0.42545005679130554,
      "learning_rate": 0.0001737232872407291,
      "loss": 1.0188,
      "step": 21540
    },
    {
      "epoch": 1.2673527898257737,
      "grad_norm": 0.3340122699737549,
      "learning_rate": 0.00017366436203645506,
      "loss": 1.0432,
      "step": 21550
    },
    {
      "epoch": 1.2679408953907227,
      "grad_norm": 0.3642629384994507,
      "learning_rate": 0.000173605436832181,
      "loss": 0.9656,
      "step": 21560
    },
    {
      "epoch": 1.2685290009556716,
      "grad_norm": 0.38000813126564026,
      "learning_rate": 0.00017354651162790696,
      "loss": 0.9718,
      "step": 21570
    },
    {
      "epoch": 1.2691171065206204,
      "grad_norm": 0.3225587010383606,
      "learning_rate": 0.00017348758642363293,
      "loss": 0.9305,
      "step": 21580
    },
    {
      "epoch": 1.2697052120855694,
      "grad_norm": 0.3255215883255005,
      "learning_rate": 0.00017342866121935888,
      "loss": 0.9397,
      "step": 21590
    },
    {
      "epoch": 1.2702933176505182,
      "grad_norm": 0.33911094069480896,
      "learning_rate": 0.00017336973601508486,
      "loss": 1.0805,
      "step": 21600
    },
    {
      "epoch": 1.2708814232154673,
      "grad_norm": 0.35549208521842957,
      "learning_rate": 0.0001733108108108108,
      "loss": 1.0399,
      "step": 21610
    },
    {
      "epoch": 1.271469528780416,
      "grad_norm": 0.3234094977378845,
      "learning_rate": 0.00017325188560653678,
      "loss": 0.9444,
      "step": 21620
    },
    {
      "epoch": 1.272057634345365,
      "grad_norm": 0.3512588143348694,
      "learning_rate": 0.00017319296040226273,
      "loss": 0.9824,
      "step": 21630
    },
    {
      "epoch": 1.272645739910314,
      "grad_norm": 0.3770892918109894,
      "learning_rate": 0.00017313403519798867,
      "loss": 1.1144,
      "step": 21640
    },
    {
      "epoch": 1.2732338454752627,
      "grad_norm": 0.41609442234039307,
      "learning_rate": 0.00017307510999371465,
      "loss": 1.0125,
      "step": 21650
    },
    {
      "epoch": 1.2738219510402118,
      "grad_norm": 0.3569420278072357,
      "learning_rate": 0.0001730161847894406,
      "loss": 0.9897,
      "step": 21660
    },
    {
      "epoch": 1.2744100566051606,
      "grad_norm": 0.3181958496570587,
      "learning_rate": 0.00017295725958516657,
      "loss": 1.0783,
      "step": 21670
    },
    {
      "epoch": 1.2749981621701094,
      "grad_norm": 0.3765166401863098,
      "learning_rate": 0.0001728983343808925,
      "loss": 1.0317,
      "step": 21680
    },
    {
      "epoch": 1.2755862677350585,
      "grad_norm": 0.3548137843608856,
      "learning_rate": 0.00017283940917661844,
      "loss": 0.9931,
      "step": 21690
    },
    {
      "epoch": 1.2761743733000073,
      "grad_norm": 0.32856231927871704,
      "learning_rate": 0.00017278048397234441,
      "loss": 0.9933,
      "step": 21700
    },
    {
      "epoch": 1.2767624788649563,
      "grad_norm": 0.4015820026397705,
      "learning_rate": 0.00017272155876807036,
      "loss": 0.99,
      "step": 21710
    },
    {
      "epoch": 1.2773505844299051,
      "grad_norm": 0.3391590416431427,
      "learning_rate": 0.00017266263356379634,
      "loss": 1.0083,
      "step": 21720
    },
    {
      "epoch": 1.277938689994854,
      "grad_norm": 0.3774256408214569,
      "learning_rate": 0.00017260370835952228,
      "loss": 1.0136,
      "step": 21730
    },
    {
      "epoch": 1.278526795559803,
      "grad_norm": 0.2828402519226074,
      "learning_rate": 0.00017254478315524823,
      "loss": 1.0079,
      "step": 21740
    },
    {
      "epoch": 1.279114901124752,
      "grad_norm": 0.3240427076816559,
      "learning_rate": 0.0001724858579509742,
      "loss": 1.0885,
      "step": 21750
    },
    {
      "epoch": 1.2797030066897008,
      "grad_norm": 0.32747402787208557,
      "learning_rate": 0.00017242693274670015,
      "loss": 0.9893,
      "step": 21760
    },
    {
      "epoch": 1.2802911122546496,
      "grad_norm": 0.35374751687049866,
      "learning_rate": 0.00017236800754242613,
      "loss": 0.9863,
      "step": 21770
    },
    {
      "epoch": 1.2808792178195987,
      "grad_norm": 0.3306727111339569,
      "learning_rate": 0.00017230908233815208,
      "loss": 1.0907,
      "step": 21780
    },
    {
      "epoch": 1.2814673233845475,
      "grad_norm": 0.37135937809944153,
      "learning_rate": 0.00017225015713387805,
      "loss": 1.1025,
      "step": 21790
    },
    {
      "epoch": 1.2820554289494965,
      "grad_norm": 0.3196341395378113,
      "learning_rate": 0.000172191231929604,
      "loss": 1.0787,
      "step": 21800
    },
    {
      "epoch": 1.2826435345144453,
      "grad_norm": 0.43845945596694946,
      "learning_rate": 0.00017213230672532995,
      "loss": 0.9725,
      "step": 21810
    },
    {
      "epoch": 1.2832316400793942,
      "grad_norm": 0.34181633591651917,
      "learning_rate": 0.00017207338152105592,
      "loss": 1.0176,
      "step": 21820
    },
    {
      "epoch": 1.2838197456443432,
      "grad_norm": 0.35463565587997437,
      "learning_rate": 0.00017201445631678187,
      "loss": 1.0606,
      "step": 21830
    },
    {
      "epoch": 1.284407851209292,
      "grad_norm": 0.32328498363494873,
      "learning_rate": 0.00017195553111250784,
      "loss": 0.9639,
      "step": 21840
    },
    {
      "epoch": 1.284995956774241,
      "grad_norm": 0.3329913318157196,
      "learning_rate": 0.0001718966059082338,
      "loss": 1.0353,
      "step": 21850
    },
    {
      "epoch": 1.2855840623391899,
      "grad_norm": 0.3324337899684906,
      "learning_rate": 0.00017183768070395977,
      "loss": 1.0072,
      "step": 21860
    },
    {
      "epoch": 1.2861721679041387,
      "grad_norm": 0.329178124666214,
      "learning_rate": 0.00017177875549968571,
      "loss": 0.9558,
      "step": 21870
    },
    {
      "epoch": 1.2867602734690877,
      "grad_norm": 0.37961849570274353,
      "learning_rate": 0.00017171983029541166,
      "loss": 1.0248,
      "step": 21880
    },
    {
      "epoch": 1.2873483790340365,
      "grad_norm": 0.346913605928421,
      "learning_rate": 0.00017166090509113764,
      "loss": 0.9526,
      "step": 21890
    },
    {
      "epoch": 1.2879364845989856,
      "grad_norm": 0.3355092406272888,
      "learning_rate": 0.00017160197988686358,
      "loss": 0.9926,
      "step": 21900
    },
    {
      "epoch": 1.2885245901639344,
      "grad_norm": 0.29472678899765015,
      "learning_rate": 0.00017154305468258956,
      "loss": 0.9996,
      "step": 21910
    },
    {
      "epoch": 1.2891126957288832,
      "grad_norm": 0.33820652961730957,
      "learning_rate": 0.0001714841294783155,
      "loss": 1.0139,
      "step": 21920
    },
    {
      "epoch": 1.2897008012938322,
      "grad_norm": 0.34954795241355896,
      "learning_rate": 0.00017142520427404148,
      "loss": 0.9976,
      "step": 21930
    },
    {
      "epoch": 1.2902889068587813,
      "grad_norm": 0.3905593752861023,
      "learning_rate": 0.00017136627906976743,
      "loss": 1.0185,
      "step": 21940
    },
    {
      "epoch": 1.29087701242373,
      "grad_norm": 0.36583003401756287,
      "learning_rate": 0.0001713073538654934,
      "loss": 1.1043,
      "step": 21950
    },
    {
      "epoch": 1.291465117988679,
      "grad_norm": 0.3295050263404846,
      "learning_rate": 0.00017124842866121935,
      "loss": 0.9964,
      "step": 21960
    },
    {
      "epoch": 1.292053223553628,
      "grad_norm": 0.3904248774051666,
      "learning_rate": 0.0001711895034569453,
      "loss": 0.9713,
      "step": 21970
    },
    {
      "epoch": 1.2926413291185768,
      "grad_norm": 0.33559441566467285,
      "learning_rate": 0.00017113057825267127,
      "loss": 1.012,
      "step": 21980
    },
    {
      "epoch": 1.2932294346835258,
      "grad_norm": 0.3245697617530823,
      "learning_rate": 0.00017107165304839722,
      "loss": 0.9357,
      "step": 21990
    },
    {
      "epoch": 1.2938175402484746,
      "grad_norm": 0.3422049582004547,
      "learning_rate": 0.0001710127278441232,
      "loss": 1.0194,
      "step": 22000
    },
    {
      "epoch": 1.2944056458134234,
      "grad_norm": 0.3593516945838928,
      "learning_rate": 0.00017095380263984914,
      "loss": 1.0544,
      "step": 22010
    },
    {
      "epoch": 1.2949937513783725,
      "grad_norm": 0.33050185441970825,
      "learning_rate": 0.00017089487743557512,
      "loss": 1.058,
      "step": 22020
    },
    {
      "epoch": 1.2955818569433213,
      "grad_norm": 0.4000053405761719,
      "learning_rate": 0.00017083595223130107,
      "loss": 1.1088,
      "step": 22030
    },
    {
      "epoch": 1.2961699625082703,
      "grad_norm": 0.3879542052745819,
      "learning_rate": 0.00017078291954745442,
      "loss": 0.9833,
      "step": 22040
    },
    {
      "epoch": 1.2967580680732191,
      "grad_norm": 0.3280361294746399,
      "learning_rate": 0.00017072399434318037,
      "loss": 1.1012,
      "step": 22050
    },
    {
      "epoch": 1.297346173638168,
      "grad_norm": 0.3621399700641632,
      "learning_rate": 0.00017066506913890634,
      "loss": 0.9644,
      "step": 22060
    },
    {
      "epoch": 1.297934279203117,
      "grad_norm": 0.32923609018325806,
      "learning_rate": 0.0001706061439346323,
      "loss": 1.0819,
      "step": 22070
    },
    {
      "epoch": 1.2985223847680658,
      "grad_norm": 0.33565545082092285,
      "learning_rate": 0.00017054721873035827,
      "loss": 1.0159,
      "step": 22080
    },
    {
      "epoch": 1.2991104903330148,
      "grad_norm": 0.36542749404907227,
      "learning_rate": 0.00017048829352608421,
      "loss": 1.059,
      "step": 22090
    },
    {
      "epoch": 1.2996985958979637,
      "grad_norm": 0.33681803941726685,
      "learning_rate": 0.0001704293683218102,
      "loss": 1.0332,
      "step": 22100
    },
    {
      "epoch": 1.3002867014629125,
      "grad_norm": 0.3864671289920807,
      "learning_rate": 0.00017037044311753614,
      "loss": 1.0326,
      "step": 22110
    },
    {
      "epoch": 1.3008748070278615,
      "grad_norm": 0.345933198928833,
      "learning_rate": 0.00017031151791326208,
      "loss": 0.9496,
      "step": 22120
    },
    {
      "epoch": 1.3014629125928103,
      "grad_norm": 0.34090423583984375,
      "learning_rate": 0.00017025259270898806,
      "loss": 1.0504,
      "step": 22130
    },
    {
      "epoch": 1.3020510181577594,
      "grad_norm": 0.3304266035556793,
      "learning_rate": 0.000170193667504714,
      "loss": 1.0512,
      "step": 22140
    },
    {
      "epoch": 1.3026391237227082,
      "grad_norm": 0.34790509939193726,
      "learning_rate": 0.00017013474230043998,
      "loss": 0.9633,
      "step": 22150
    },
    {
      "epoch": 1.303227229287657,
      "grad_norm": 0.35584011673927307,
      "learning_rate": 0.00017007581709616593,
      "loss": 1.066,
      "step": 22160
    },
    {
      "epoch": 1.303815334852606,
      "grad_norm": 0.33409249782562256,
      "learning_rate": 0.0001700168918918919,
      "loss": 0.9808,
      "step": 22170
    },
    {
      "epoch": 1.304403440417555,
      "grad_norm": 0.3983939290046692,
      "learning_rate": 0.00016995796668761782,
      "loss": 1.0361,
      "step": 22180
    },
    {
      "epoch": 1.3049915459825039,
      "grad_norm": 0.34290772676467896,
      "learning_rate": 0.00016989904148334377,
      "loss": 1.0449,
      "step": 22190
    },
    {
      "epoch": 1.3055796515474527,
      "grad_norm": 0.33580636978149414,
      "learning_rate": 0.00016984011627906975,
      "loss": 0.9985,
      "step": 22200
    },
    {
      "epoch": 1.3061677571124017,
      "grad_norm": 0.345763623714447,
      "learning_rate": 0.0001697811910747957,
      "loss": 1.212,
      "step": 22210
    },
    {
      "epoch": 1.3067558626773506,
      "grad_norm": 0.39837217330932617,
      "learning_rate": 0.00016972226587052167,
      "loss": 1.0263,
      "step": 22220
    },
    {
      "epoch": 1.3073439682422996,
      "grad_norm": 0.3250557780265808,
      "learning_rate": 0.00016966334066624762,
      "loss": 1.0092,
      "step": 22230
    },
    {
      "epoch": 1.3079320738072484,
      "grad_norm": 0.3983432948589325,
      "learning_rate": 0.00016960441546197356,
      "loss": 1.111,
      "step": 22240
    },
    {
      "epoch": 1.3085201793721972,
      "grad_norm": 0.3710528016090393,
      "learning_rate": 0.00016954549025769954,
      "loss": 1.0253,
      "step": 22250
    },
    {
      "epoch": 1.3091082849371463,
      "grad_norm": 0.41260799765586853,
      "learning_rate": 0.0001694865650534255,
      "loss": 0.9978,
      "step": 22260
    },
    {
      "epoch": 1.309696390502095,
      "grad_norm": 0.3755528926849365,
      "learning_rate": 0.00016942763984915146,
      "loss": 1.0406,
      "step": 22270
    },
    {
      "epoch": 1.3102844960670441,
      "grad_norm": 0.33454078435897827,
      "learning_rate": 0.0001693687146448774,
      "loss": 1.0343,
      "step": 22280
    },
    {
      "epoch": 1.310872601631993,
      "grad_norm": 0.3867446184158325,
      "learning_rate": 0.00016930978944060338,
      "loss": 0.9602,
      "step": 22290
    },
    {
      "epoch": 1.3114607071969417,
      "grad_norm": 0.3737047016620636,
      "learning_rate": 0.00016925086423632933,
      "loss": 1.0282,
      "step": 22300
    },
    {
      "epoch": 1.3120488127618908,
      "grad_norm": 0.35491320490837097,
      "learning_rate": 0.00016919193903205528,
      "loss": 1.0836,
      "step": 22310
    },
    {
      "epoch": 1.3126369183268396,
      "grad_norm": 0.3307535946369171,
      "learning_rate": 0.00016913301382778125,
      "loss": 1.0392,
      "step": 22320
    },
    {
      "epoch": 1.3132250238917886,
      "grad_norm": 0.3328978717327118,
      "learning_rate": 0.0001690740886235072,
      "loss": 0.9664,
      "step": 22330
    },
    {
      "epoch": 1.3138131294567374,
      "grad_norm": 0.30462002754211426,
      "learning_rate": 0.00016901516341923318,
      "loss": 1.0312,
      "step": 22340
    },
    {
      "epoch": 1.3144012350216863,
      "grad_norm": 0.3799833059310913,
      "learning_rate": 0.00016895623821495912,
      "loss": 0.9479,
      "step": 22350
    },
    {
      "epoch": 1.3149893405866353,
      "grad_norm": 0.3442729711532593,
      "learning_rate": 0.0001688973130106851,
      "loss": 1.0276,
      "step": 22360
    },
    {
      "epoch": 1.3155774461515843,
      "grad_norm": 0.3241092264652252,
      "learning_rate": 0.00016883838780641105,
      "loss": 1.0909,
      "step": 22370
    },
    {
      "epoch": 1.3161655517165332,
      "grad_norm": 0.3357059359550476,
      "learning_rate": 0.000168779462602137,
      "loss": 0.9998,
      "step": 22380
    },
    {
      "epoch": 1.316753657281482,
      "grad_norm": 0.36062973737716675,
      "learning_rate": 0.00016872053739786297,
      "loss": 0.9937,
      "step": 22390
    },
    {
      "epoch": 1.317341762846431,
      "grad_norm": 0.3608020842075348,
      "learning_rate": 0.00016866161219358892,
      "loss": 0.9378,
      "step": 22400
    },
    {
      "epoch": 1.3179298684113798,
      "grad_norm": 0.35658177733421326,
      "learning_rate": 0.0001686026869893149,
      "loss": 1.0287,
      "step": 22410
    },
    {
      "epoch": 1.3185179739763289,
      "grad_norm": 0.3138873279094696,
      "learning_rate": 0.00016854376178504084,
      "loss": 1.0424,
      "step": 22420
    },
    {
      "epoch": 1.3191060795412777,
      "grad_norm": 0.3774104714393616,
      "learning_rate": 0.00016848483658076681,
      "loss": 0.9208,
      "step": 22430
    },
    {
      "epoch": 1.3196941851062265,
      "grad_norm": 0.33499690890312195,
      "learning_rate": 0.00016842591137649276,
      "loss": 1.0116,
      "step": 22440
    },
    {
      "epoch": 1.3202822906711755,
      "grad_norm": 0.33902719616889954,
      "learning_rate": 0.0001683669861722187,
      "loss": 1.0112,
      "step": 22450
    },
    {
      "epoch": 1.3208703962361243,
      "grad_norm": 0.3836866617202759,
      "learning_rate": 0.00016830806096794468,
      "loss": 0.9531,
      "step": 22460
    },
    {
      "epoch": 1.3214585018010734,
      "grad_norm": 0.3282046616077423,
      "learning_rate": 0.00016824913576367063,
      "loss": 1.003,
      "step": 22470
    },
    {
      "epoch": 1.3220466073660222,
      "grad_norm": 0.3632875084877014,
      "learning_rate": 0.0001681902105593966,
      "loss": 1.012,
      "step": 22480
    },
    {
      "epoch": 1.322634712930971,
      "grad_norm": 0.35654720664024353,
      "learning_rate": 0.00016813128535512255,
      "loss": 1.0522,
      "step": 22490
    },
    {
      "epoch": 1.32322281849592,
      "grad_norm": 0.3590754568576813,
      "learning_rate": 0.00016807236015084853,
      "loss": 1.1203,
      "step": 22500
    },
    {
      "epoch": 1.3238109240608689,
      "grad_norm": 0.35096275806427,
      "learning_rate": 0.00016801343494657448,
      "loss": 1.0081,
      "step": 22510
    },
    {
      "epoch": 1.324399029625818,
      "grad_norm": 0.3208027184009552,
      "learning_rate": 0.00016795450974230042,
      "loss": 1.073,
      "step": 22520
    },
    {
      "epoch": 1.3249871351907667,
      "grad_norm": 0.3795671761035919,
      "learning_rate": 0.0001678955845380264,
      "loss": 1.1244,
      "step": 22530
    },
    {
      "epoch": 1.3255752407557155,
      "grad_norm": 0.3160175383090973,
      "learning_rate": 0.00016783665933375235,
      "loss": 1.0608,
      "step": 22540
    },
    {
      "epoch": 1.3261633463206646,
      "grad_norm": 0.33214330673217773,
      "learning_rate": 0.00016777773412947832,
      "loss": 0.9561,
      "step": 22550
    },
    {
      "epoch": 1.3267514518856134,
      "grad_norm": 0.36126336455345154,
      "learning_rate": 0.00016771880892520427,
      "loss": 1.0093,
      "step": 22560
    },
    {
      "epoch": 1.3273395574505624,
      "grad_norm": 0.34478214383125305,
      "learning_rate": 0.00016765988372093024,
      "loss": 1.0372,
      "step": 22570
    },
    {
      "epoch": 1.3279276630155112,
      "grad_norm": 0.348820298910141,
      "learning_rate": 0.0001676009585166562,
      "loss": 1.0513,
      "step": 22580
    },
    {
      "epoch": 1.32851576858046,
      "grad_norm": 0.3433401882648468,
      "learning_rate": 0.00016754203331238214,
      "loss": 1.0589,
      "step": 22590
    },
    {
      "epoch": 1.329103874145409,
      "grad_norm": 0.4364696443080902,
      "learning_rate": 0.0001674831081081081,
      "loss": 0.9672,
      "step": 22600
    },
    {
      "epoch": 1.3296919797103581,
      "grad_norm": 0.3399813771247864,
      "learning_rate": 0.00016742418290383403,
      "loss": 0.9731,
      "step": 22610
    },
    {
      "epoch": 1.330280085275307,
      "grad_norm": 0.36181309819221497,
      "learning_rate": 0.00016736525769956,
      "loss": 1.06,
      "step": 22620
    },
    {
      "epoch": 1.3308681908402558,
      "grad_norm": 0.3526007831096649,
      "learning_rate": 0.00016730633249528596,
      "loss": 0.9628,
      "step": 22630
    },
    {
      "epoch": 1.3314562964052048,
      "grad_norm": 0.37836071848869324,
      "learning_rate": 0.0001672474072910119,
      "loss": 1.0099,
      "step": 22640
    },
    {
      "epoch": 1.3320444019701536,
      "grad_norm": 0.3476807475090027,
      "learning_rate": 0.00016718848208673788,
      "loss": 0.9388,
      "step": 22650
    },
    {
      "epoch": 1.3326325075351027,
      "grad_norm": 0.32137778401374817,
      "learning_rate": 0.00016712955688246383,
      "loss": 1.0413,
      "step": 22660
    },
    {
      "epoch": 1.3332206131000515,
      "grad_norm": 0.34923669695854187,
      "learning_rate": 0.0001670706316781898,
      "loss": 0.982,
      "step": 22670
    },
    {
      "epoch": 1.3338087186650003,
      "grad_norm": 0.3855333626270294,
      "learning_rate": 0.00016701170647391575,
      "loss": 0.9338,
      "step": 22680
    },
    {
      "epoch": 1.3343968242299493,
      "grad_norm": 0.34408774971961975,
      "learning_rate": 0.00016695278126964172,
      "loss": 1.0456,
      "step": 22690
    },
    {
      "epoch": 1.3349849297948981,
      "grad_norm": 0.3454543352127075,
      "learning_rate": 0.00016689385606536767,
      "loss": 1.153,
      "step": 22700
    },
    {
      "epoch": 1.3355730353598472,
      "grad_norm": 0.3289482593536377,
      "learning_rate": 0.00016683493086109362,
      "loss": 1.0063,
      "step": 22710
    },
    {
      "epoch": 1.336161140924796,
      "grad_norm": 0.3369739055633545,
      "learning_rate": 0.0001667760056568196,
      "loss": 1.0497,
      "step": 22720
    },
    {
      "epoch": 1.3367492464897448,
      "grad_norm": 0.386803537607193,
      "learning_rate": 0.00016671708045254554,
      "loss": 1.0602,
      "step": 22730
    },
    {
      "epoch": 1.3373373520546938,
      "grad_norm": 0.38053011894226074,
      "learning_rate": 0.00016665815524827152,
      "loss": 1.0254,
      "step": 22740
    },
    {
      "epoch": 1.3379254576196427,
      "grad_norm": 0.3569447100162506,
      "learning_rate": 0.00016659923004399746,
      "loss": 1.079,
      "step": 22750
    },
    {
      "epoch": 1.3385135631845917,
      "grad_norm": 0.33365702629089355,
      "learning_rate": 0.00016654030483972344,
      "loss": 1.0334,
      "step": 22760
    },
    {
      "epoch": 1.3391016687495405,
      "grad_norm": 0.37581369280815125,
      "learning_rate": 0.00016648137963544939,
      "loss": 0.9744,
      "step": 22770
    },
    {
      "epoch": 1.3396897743144893,
      "grad_norm": 0.31354957818984985,
      "learning_rate": 0.00016642245443117533,
      "loss": 1.1299,
      "step": 22780
    },
    {
      "epoch": 1.3402778798794384,
      "grad_norm": 0.2908093333244324,
      "learning_rate": 0.0001663635292269013,
      "loss": 1.1055,
      "step": 22790
    },
    {
      "epoch": 1.3408659854443874,
      "grad_norm": 0.4397112727165222,
      "learning_rate": 0.00016630460402262726,
      "loss": 1.0063,
      "step": 22800
    },
    {
      "epoch": 1.3414540910093362,
      "grad_norm": 0.3483932316303253,
      "learning_rate": 0.00016624567881835323,
      "loss": 0.9353,
      "step": 22810
    },
    {
      "epoch": 1.342042196574285,
      "grad_norm": 0.3264854848384857,
      "learning_rate": 0.00016618675361407918,
      "loss": 1.0844,
      "step": 22820
    },
    {
      "epoch": 1.342630302139234,
      "grad_norm": 0.2999517321586609,
      "learning_rate": 0.00016612782840980515,
      "loss": 0.988,
      "step": 22830
    },
    {
      "epoch": 1.3432184077041829,
      "grad_norm": 0.3362576961517334,
      "learning_rate": 0.0001660689032055311,
      "loss": 0.9587,
      "step": 22840
    },
    {
      "epoch": 1.343806513269132,
      "grad_norm": 0.3772180378437042,
      "learning_rate": 0.00016600997800125705,
      "loss": 1.0214,
      "step": 22850
    },
    {
      "epoch": 1.3443946188340807,
      "grad_norm": 0.3347642719745636,
      "learning_rate": 0.00016595105279698302,
      "loss": 1.0047,
      "step": 22860
    },
    {
      "epoch": 1.3449827243990295,
      "grad_norm": 0.3170306980609894,
      "learning_rate": 0.00016589212759270897,
      "loss": 0.9929,
      "step": 22870
    },
    {
      "epoch": 1.3455708299639786,
      "grad_norm": 0.3962932825088501,
      "learning_rate": 0.00016583320238843495,
      "loss": 0.9389,
      "step": 22880
    },
    {
      "epoch": 1.3461589355289274,
      "grad_norm": 0.3117310404777527,
      "learning_rate": 0.0001657742771841609,
      "loss": 1.0659,
      "step": 22890
    },
    {
      "epoch": 1.3467470410938764,
      "grad_norm": 0.34644344449043274,
      "learning_rate": 0.00016571535197988687,
      "loss": 1.0392,
      "step": 22900
    },
    {
      "epoch": 1.3473351466588253,
      "grad_norm": 0.3299046456813812,
      "learning_rate": 0.00016565642677561282,
      "loss": 0.9367,
      "step": 22910
    },
    {
      "epoch": 1.347923252223774,
      "grad_norm": 0.37247827649116516,
      "learning_rate": 0.00016559750157133876,
      "loss": 0.9956,
      "step": 22920
    },
    {
      "epoch": 1.348511357788723,
      "grad_norm": 0.3393561840057373,
      "learning_rate": 0.00016553857636706474,
      "loss": 1.1173,
      "step": 22930
    },
    {
      "epoch": 1.349099463353672,
      "grad_norm": 0.31136780977249146,
      "learning_rate": 0.00016547965116279069,
      "loss": 1.1038,
      "step": 22940
    },
    {
      "epoch": 1.349687568918621,
      "grad_norm": 0.34079214930534363,
      "learning_rate": 0.00016542072595851666,
      "loss": 1.0446,
      "step": 22950
    },
    {
      "epoch": 1.3502756744835698,
      "grad_norm": 0.3206827938556671,
      "learning_rate": 0.0001653618007542426,
      "loss": 1.0289,
      "step": 22960
    },
    {
      "epoch": 1.3508637800485186,
      "grad_norm": 0.3561250567436218,
      "learning_rate": 0.00016530287554996858,
      "loss": 1.0627,
      "step": 22970
    },
    {
      "epoch": 1.3514518856134676,
      "grad_norm": 0.3712453842163086,
      "learning_rate": 0.00016524395034569453,
      "loss": 1.0331,
      "step": 22980
    },
    {
      "epoch": 1.3520399911784164,
      "grad_norm": 0.34283512830734253,
      "learning_rate": 0.00016518502514142048,
      "loss": 0.9972,
      "step": 22990
    },
    {
      "epoch": 1.3526280967433655,
      "grad_norm": 0.369966059923172,
      "learning_rate": 0.00016512609993714645,
      "loss": 1.073,
      "step": 23000
    },
    {
      "epoch": 1.3532162023083143,
      "grad_norm": 0.3470115661621094,
      "learning_rate": 0.0001650671747328724,
      "loss": 1.0799,
      "step": 23010
    },
    {
      "epoch": 1.3538043078732631,
      "grad_norm": 0.33416518568992615,
      "learning_rate": 0.00016500824952859835,
      "loss": 1.128,
      "step": 23020
    },
    {
      "epoch": 1.3543924134382122,
      "grad_norm": 0.4122907817363739,
      "learning_rate": 0.0001649493243243243,
      "loss": 1.0656,
      "step": 23030
    },
    {
      "epoch": 1.3549805190031612,
      "grad_norm": 0.3539762496948242,
      "learning_rate": 0.00016489039912005024,
      "loss": 0.9639,
      "step": 23040
    },
    {
      "epoch": 1.35556862456811,
      "grad_norm": 0.36985814571380615,
      "learning_rate": 0.00016483147391577622,
      "loss": 1.0274,
      "step": 23050
    },
    {
      "epoch": 1.3561567301330588,
      "grad_norm": 0.37729644775390625,
      "learning_rate": 0.00016477254871150217,
      "loss": 0.9061,
      "step": 23060
    },
    {
      "epoch": 1.3567448356980079,
      "grad_norm": 0.37072622776031494,
      "learning_rate": 0.00016471362350722814,
      "loss": 1.0468,
      "step": 23070
    },
    {
      "epoch": 1.3573329412629567,
      "grad_norm": 0.3999411165714264,
      "learning_rate": 0.0001646546983029541,
      "loss": 1.0613,
      "step": 23080
    },
    {
      "epoch": 1.3579210468279057,
      "grad_norm": 0.3505893647670746,
      "learning_rate": 0.00016459577309868006,
      "loss": 0.9092,
      "step": 23090
    },
    {
      "epoch": 1.3585091523928545,
      "grad_norm": 0.3344140350818634,
      "learning_rate": 0.000164536847894406,
      "loss": 0.9571,
      "step": 23100
    },
    {
      "epoch": 1.3590972579578033,
      "grad_norm": 0.32083287835121155,
      "learning_rate": 0.00016447792269013196,
      "loss": 1.0049,
      "step": 23110
    },
    {
      "epoch": 1.3596853635227524,
      "grad_norm": 0.3645947575569153,
      "learning_rate": 0.00016441899748585793,
      "loss": 1.0813,
      "step": 23120
    },
    {
      "epoch": 1.3602734690877012,
      "grad_norm": 0.346981018781662,
      "learning_rate": 0.00016436007228158388,
      "loss": 1.0253,
      "step": 23130
    },
    {
      "epoch": 1.3608615746526502,
      "grad_norm": 0.31810250878334045,
      "learning_rate": 0.00016430114707730986,
      "loss": 1.0316,
      "step": 23140
    },
    {
      "epoch": 1.361449680217599,
      "grad_norm": 0.3884196877479553,
      "learning_rate": 0.0001642422218730358,
      "loss": 1.056,
      "step": 23150
    },
    {
      "epoch": 1.3620377857825479,
      "grad_norm": 0.3800853490829468,
      "learning_rate": 0.00016418329666876178,
      "loss": 0.8974,
      "step": 23160
    },
    {
      "epoch": 1.362625891347497,
      "grad_norm": 0.36449557542800903,
      "learning_rate": 0.00016412437146448773,
      "loss": 0.9439,
      "step": 23170
    },
    {
      "epoch": 1.3632139969124457,
      "grad_norm": 0.3218843936920166,
      "learning_rate": 0.00016406544626021367,
      "loss": 0.9753,
      "step": 23180
    },
    {
      "epoch": 1.3638021024773948,
      "grad_norm": 0.3670317828655243,
      "learning_rate": 0.00016400652105593965,
      "loss": 0.9297,
      "step": 23190
    },
    {
      "epoch": 1.3643902080423436,
      "grad_norm": 0.35717692971229553,
      "learning_rate": 0.0001639475958516656,
      "loss": 0.9556,
      "step": 23200
    },
    {
      "epoch": 1.3649783136072924,
      "grad_norm": 0.3153073489665985,
      "learning_rate": 0.00016388867064739157,
      "loss": 0.9802,
      "step": 23210
    },
    {
      "epoch": 1.3655664191722414,
      "grad_norm": 0.33022621273994446,
      "learning_rate": 0.00016382974544311752,
      "loss": 1.0802,
      "step": 23220
    },
    {
      "epoch": 1.3661545247371905,
      "grad_norm": 0.3613029420375824,
      "learning_rate": 0.0001637708202388435,
      "loss": 1.029,
      "step": 23230
    },
    {
      "epoch": 1.3667426303021393,
      "grad_norm": 0.3656368851661682,
      "learning_rate": 0.00016371189503456944,
      "loss": 1.0361,
      "step": 23240
    },
    {
      "epoch": 1.367330735867088,
      "grad_norm": 0.3541760742664337,
      "learning_rate": 0.0001636529698302954,
      "loss": 1.0041,
      "step": 23250
    },
    {
      "epoch": 1.3679188414320371,
      "grad_norm": 0.3769182562828064,
      "learning_rate": 0.00016359404462602136,
      "loss": 1.0355,
      "step": 23260
    },
    {
      "epoch": 1.368506946996986,
      "grad_norm": 0.36910930275917053,
      "learning_rate": 0.0001635351194217473,
      "loss": 1.037,
      "step": 23270
    },
    {
      "epoch": 1.369095052561935,
      "grad_norm": 0.3744880259037018,
      "learning_rate": 0.00016347619421747329,
      "loss": 0.9817,
      "step": 23280
    },
    {
      "epoch": 1.3696831581268838,
      "grad_norm": 0.32340675592422485,
      "learning_rate": 0.00016341726901319923,
      "loss": 1.0828,
      "step": 23290
    },
    {
      "epoch": 1.3702712636918326,
      "grad_norm": 0.32741257548332214,
      "learning_rate": 0.0001633583438089252,
      "loss": 0.9785,
      "step": 23300
    },
    {
      "epoch": 1.3708593692567816,
      "grad_norm": 0.3742581605911255,
      "learning_rate": 0.00016329941860465116,
      "loss": 1.0325,
      "step": 23310
    },
    {
      "epoch": 1.3714474748217305,
      "grad_norm": 0.37888866662979126,
      "learning_rate": 0.0001632404934003771,
      "loss": 1.0935,
      "step": 23320
    },
    {
      "epoch": 1.3720355803866795,
      "grad_norm": 0.3390244245529175,
      "learning_rate": 0.00016318156819610308,
      "loss": 1.0175,
      "step": 23330
    },
    {
      "epoch": 1.3726236859516283,
      "grad_norm": 0.3694898784160614,
      "learning_rate": 0.00016312264299182903,
      "loss": 0.9442,
      "step": 23340
    },
    {
      "epoch": 1.3732117915165771,
      "grad_norm": 0.36802059412002563,
      "learning_rate": 0.000163063717787555,
      "loss": 1.0207,
      "step": 23350
    },
    {
      "epoch": 1.3737998970815262,
      "grad_norm": 0.3022995591163635,
      "learning_rate": 0.00016300479258328095,
      "loss": 0.9745,
      "step": 23360
    },
    {
      "epoch": 1.374388002646475,
      "grad_norm": 0.3347220718860626,
      "learning_rate": 0.00016294586737900692,
      "loss": 1.0081,
      "step": 23370
    },
    {
      "epoch": 1.374976108211424,
      "grad_norm": 0.34657758474349976,
      "learning_rate": 0.00016288694217473287,
      "loss": 0.9528,
      "step": 23380
    },
    {
      "epoch": 1.3755642137763728,
      "grad_norm": 0.3650955855846405,
      "learning_rate": 0.00016282801697045882,
      "loss": 1.0272,
      "step": 23390
    },
    {
      "epoch": 1.3761523193413217,
      "grad_norm": 0.3492852449417114,
      "learning_rate": 0.0001627690917661848,
      "loss": 1.1268,
      "step": 23400
    },
    {
      "epoch": 1.3767404249062707,
      "grad_norm": 0.4101478159427643,
      "learning_rate": 0.00016271016656191074,
      "loss": 0.9626,
      "step": 23410
    },
    {
      "epoch": 1.3773285304712195,
      "grad_norm": 0.35969141125679016,
      "learning_rate": 0.00016265124135763672,
      "loss": 1.0305,
      "step": 23420
    },
    {
      "epoch": 1.3779166360361685,
      "grad_norm": 0.33710816502571106,
      "learning_rate": 0.00016259231615336266,
      "loss": 0.9611,
      "step": 23430
    },
    {
      "epoch": 1.3785047416011174,
      "grad_norm": 0.31244996190071106,
      "learning_rate": 0.00016253339094908858,
      "loss": 0.9684,
      "step": 23440
    },
    {
      "epoch": 1.3790928471660662,
      "grad_norm": 0.30731144547462463,
      "learning_rate": 0.00016247446574481456,
      "loss": 1.0705,
      "step": 23450
    },
    {
      "epoch": 1.3796809527310152,
      "grad_norm": 0.3864540755748749,
      "learning_rate": 0.0001624155405405405,
      "loss": 1.027,
      "step": 23460
    },
    {
      "epoch": 1.3802690582959642,
      "grad_norm": 0.3155273199081421,
      "learning_rate": 0.00016235661533626648,
      "loss": 1.0275,
      "step": 23470
    },
    {
      "epoch": 1.380857163860913,
      "grad_norm": 0.3369208872318268,
      "learning_rate": 0.00016229769013199243,
      "loss": 0.9791,
      "step": 23480
    },
    {
      "epoch": 1.3814452694258619,
      "grad_norm": 0.36110201478004456,
      "learning_rate": 0.0001622387649277184,
      "loss": 1.0954,
      "step": 23490
    },
    {
      "epoch": 1.382033374990811,
      "grad_norm": 0.366502970457077,
      "learning_rate": 0.00016217983972344435,
      "loss": 1.0429,
      "step": 23500
    },
    {
      "epoch": 1.3826214805557597,
      "grad_norm": 0.36606302857398987,
      "learning_rate": 0.0001621209145191703,
      "loss": 0.9093,
      "step": 23510
    },
    {
      "epoch": 1.3832095861207088,
      "grad_norm": 0.34365877509117126,
      "learning_rate": 0.00016206198931489627,
      "loss": 1.0091,
      "step": 23520
    },
    {
      "epoch": 1.3837976916856576,
      "grad_norm": 0.31490394473075867,
      "learning_rate": 0.00016200306411062222,
      "loss": 0.9958,
      "step": 23530
    },
    {
      "epoch": 1.3843857972506064,
      "grad_norm": 0.34720370173454285,
      "learning_rate": 0.0001619441389063482,
      "loss": 1.0707,
      "step": 23540
    },
    {
      "epoch": 1.3849739028155554,
      "grad_norm": 0.3582633137702942,
      "learning_rate": 0.00016188521370207414,
      "loss": 0.9687,
      "step": 23550
    },
    {
      "epoch": 1.3855620083805043,
      "grad_norm": 0.3799933195114136,
      "learning_rate": 0.00016182628849780012,
      "loss": 1.0572,
      "step": 23560
    },
    {
      "epoch": 1.3861501139454533,
      "grad_norm": 0.348621666431427,
      "learning_rate": 0.00016176736329352607,
      "loss": 0.9764,
      "step": 23570
    },
    {
      "epoch": 1.386738219510402,
      "grad_norm": 0.34721776843070984,
      "learning_rate": 0.000161708438089252,
      "loss": 1.0191,
      "step": 23580
    },
    {
      "epoch": 1.387326325075351,
      "grad_norm": 0.36609846353530884,
      "learning_rate": 0.000161649512884978,
      "loss": 1.0571,
      "step": 23590
    },
    {
      "epoch": 1.3879144306403,
      "grad_norm": 0.36890390515327454,
      "learning_rate": 0.00016159058768070394,
      "loss": 0.9439,
      "step": 23600
    },
    {
      "epoch": 1.3885025362052488,
      "grad_norm": 0.3262481689453125,
      "learning_rate": 0.0001615316624764299,
      "loss": 1.1135,
      "step": 23610
    },
    {
      "epoch": 1.3890906417701978,
      "grad_norm": 0.36751049757003784,
      "learning_rate": 0.00016147273727215586,
      "loss": 1.0592,
      "step": 23620
    },
    {
      "epoch": 1.3896787473351466,
      "grad_norm": 0.39983606338500977,
      "learning_rate": 0.00016141381206788183,
      "loss": 1.0544,
      "step": 23630
    },
    {
      "epoch": 1.3902668529000954,
      "grad_norm": 0.40012598037719727,
      "learning_rate": 0.00016135488686360778,
      "loss": 0.9705,
      "step": 23640
    },
    {
      "epoch": 1.3908549584650445,
      "grad_norm": 0.4565107524394989,
      "learning_rate": 0.00016129596165933373,
      "loss": 0.9338,
      "step": 23650
    },
    {
      "epoch": 1.3914430640299935,
      "grad_norm": 0.37778550386428833,
      "learning_rate": 0.0001612370364550597,
      "loss": 1.0784,
      "step": 23660
    },
    {
      "epoch": 1.3920311695949423,
      "grad_norm": 0.4091256856918335,
      "learning_rate": 0.00016117811125078565,
      "loss": 1.0789,
      "step": 23670
    },
    {
      "epoch": 1.3926192751598911,
      "grad_norm": 0.36593180894851685,
      "learning_rate": 0.00016111918604651163,
      "loss": 1.006,
      "step": 23680
    },
    {
      "epoch": 1.3932073807248402,
      "grad_norm": 0.34088972210884094,
      "learning_rate": 0.00016106026084223757,
      "loss": 0.9972,
      "step": 23690
    },
    {
      "epoch": 1.393795486289789,
      "grad_norm": 0.2980150580406189,
      "learning_rate": 0.00016100133563796355,
      "loss": 1.0681,
      "step": 23700
    },
    {
      "epoch": 1.394383591854738,
      "grad_norm": 0.35099974274635315,
      "learning_rate": 0.0001609424104336895,
      "loss": 0.905,
      "step": 23710
    },
    {
      "epoch": 1.3949716974196869,
      "grad_norm": 0.3337480425834656,
      "learning_rate": 0.00016088348522941544,
      "loss": 0.9383,
      "step": 23720
    },
    {
      "epoch": 1.3955598029846357,
      "grad_norm": 0.37547731399536133,
      "learning_rate": 0.00016082456002514142,
      "loss": 0.9809,
      "step": 23730
    },
    {
      "epoch": 1.3961479085495847,
      "grad_norm": 0.3297570049762726,
      "learning_rate": 0.00016076563482086737,
      "loss": 1.0086,
      "step": 23740
    },
    {
      "epoch": 1.3967360141145335,
      "grad_norm": 0.399348646402359,
      "learning_rate": 0.00016070670961659334,
      "loss": 0.9995,
      "step": 23750
    },
    {
      "epoch": 1.3973241196794826,
      "grad_norm": 0.36240488290786743,
      "learning_rate": 0.0001606477844123193,
      "loss": 1.0414,
      "step": 23760
    },
    {
      "epoch": 1.3979122252444314,
      "grad_norm": 0.3432638347148895,
      "learning_rate": 0.00016058885920804526,
      "loss": 1.1866,
      "step": 23770
    },
    {
      "epoch": 1.3985003308093802,
      "grad_norm": 0.33396443724632263,
      "learning_rate": 0.0001605299340037712,
      "loss": 1.0835,
      "step": 23780
    },
    {
      "epoch": 1.3990884363743292,
      "grad_norm": 0.368817538022995,
      "learning_rate": 0.00016047100879949716,
      "loss": 1.0568,
      "step": 23790
    },
    {
      "epoch": 1.399676541939278,
      "grad_norm": 0.3213929533958435,
      "learning_rate": 0.00016041208359522313,
      "loss": 1.0145,
      "step": 23800
    },
    {
      "epoch": 1.400264647504227,
      "grad_norm": 0.3188202679157257,
      "learning_rate": 0.00016035315839094908,
      "loss": 1.0776,
      "step": 23810
    },
    {
      "epoch": 1.400852753069176,
      "grad_norm": 0.3411685526371002,
      "learning_rate": 0.00016029423318667505,
      "loss": 1.0177,
      "step": 23820
    },
    {
      "epoch": 1.4014408586341247,
      "grad_norm": 0.3922530710697174,
      "learning_rate": 0.000160235307982401,
      "loss": 1.011,
      "step": 23830
    },
    {
      "epoch": 1.4020289641990737,
      "grad_norm": 0.34810227155685425,
      "learning_rate": 0.00016017638277812698,
      "loss": 1.0564,
      "step": 23840
    },
    {
      "epoch": 1.4026170697640226,
      "grad_norm": 0.35646334290504456,
      "learning_rate": 0.00016011745757385292,
      "loss": 1.0132,
      "step": 23850
    },
    {
      "epoch": 1.4032051753289716,
      "grad_norm": 0.31491801142692566,
      "learning_rate": 0.00016005853236957885,
      "loss": 1.0609,
      "step": 23860
    },
    {
      "epoch": 1.4037932808939204,
      "grad_norm": 0.33637917041778564,
      "learning_rate": 0.00015999960716530482,
      "loss": 1.071,
      "step": 23870
    },
    {
      "epoch": 1.4043813864588692,
      "grad_norm": 0.3621973991394043,
      "learning_rate": 0.00015994068196103077,
      "loss": 1.0029,
      "step": 23880
    },
    {
      "epoch": 1.4049694920238183,
      "grad_norm": 0.36186683177948,
      "learning_rate": 0.00015988175675675674,
      "loss": 1.047,
      "step": 23890
    },
    {
      "epoch": 1.4055575975887673,
      "grad_norm": 0.3864920139312744,
      "learning_rate": 0.0001598228315524827,
      "loss": 1.0141,
      "step": 23900
    },
    {
      "epoch": 1.4061457031537161,
      "grad_norm": 0.3717442750930786,
      "learning_rate": 0.00015976390634820864,
      "loss": 1.1335,
      "step": 23910
    },
    {
      "epoch": 1.406733808718665,
      "grad_norm": 0.3538595736026764,
      "learning_rate": 0.0001597049811439346,
      "loss": 1.0238,
      "step": 23920
    },
    {
      "epoch": 1.407321914283614,
      "grad_norm": 0.36642393469810486,
      "learning_rate": 0.00015964605593966056,
      "loss": 0.9659,
      "step": 23930
    },
    {
      "epoch": 1.4079100198485628,
      "grad_norm": 0.3742246925830841,
      "learning_rate": 0.00015958713073538653,
      "loss": 0.9717,
      "step": 23940
    },
    {
      "epoch": 1.4084981254135118,
      "grad_norm": 0.3376002907752991,
      "learning_rate": 0.00015952820553111248,
      "loss": 1.0947,
      "step": 23950
    },
    {
      "epoch": 1.4090862309784606,
      "grad_norm": 0.3900580406188965,
      "learning_rate": 0.00015946928032683846,
      "loss": 0.9773,
      "step": 23960
    },
    {
      "epoch": 1.4096743365434095,
      "grad_norm": 0.39643484354019165,
      "learning_rate": 0.0001594103551225644,
      "loss": 0.9948,
      "step": 23970
    },
    {
      "epoch": 1.4102624421083585,
      "grad_norm": 0.3368888795375824,
      "learning_rate": 0.00015935142991829035,
      "loss": 1.0301,
      "step": 23980
    },
    {
      "epoch": 1.4108505476733073,
      "grad_norm": 0.384789377450943,
      "learning_rate": 0.00015929250471401633,
      "loss": 1.0116,
      "step": 23990
    },
    {
      "epoch": 1.4114386532382563,
      "grad_norm": 0.36725038290023804,
      "learning_rate": 0.00015923357950974227,
      "loss": 1.0584,
      "step": 24000
    },
    {
      "epoch": 1.4120267588032052,
      "grad_norm": 0.3307896554470062,
      "learning_rate": 0.00015917465430546825,
      "loss": 1.0215,
      "step": 24010
    },
    {
      "epoch": 1.412614864368154,
      "grad_norm": 0.38407161831855774,
      "learning_rate": 0.0001591157291011942,
      "loss": 1.078,
      "step": 24020
    },
    {
      "epoch": 1.413202969933103,
      "grad_norm": 0.37283945083618164,
      "learning_rate": 0.00015905680389692017,
      "loss": 0.9578,
      "step": 24030
    },
    {
      "epoch": 1.4137910754980518,
      "grad_norm": 0.3361665606498718,
      "learning_rate": 0.00015900377121307353,
      "loss": 1.0788,
      "step": 24040
    },
    {
      "epoch": 1.4143791810630009,
      "grad_norm": 0.3492462933063507,
      "learning_rate": 0.00015894484600879948,
      "loss": 0.9792,
      "step": 24050
    },
    {
      "epoch": 1.4149672866279497,
      "grad_norm": 0.3818630278110504,
      "learning_rate": 0.00015888592080452542,
      "loss": 0.972,
      "step": 24060
    },
    {
      "epoch": 1.4155553921928985,
      "grad_norm": 0.34218475222587585,
      "learning_rate": 0.0001588269956002514,
      "loss": 0.9452,
      "step": 24070
    },
    {
      "epoch": 1.4161434977578475,
      "grad_norm": 0.39267805218696594,
      "learning_rate": 0.00015876807039597735,
      "loss": 1.0499,
      "step": 24080
    },
    {
      "epoch": 1.4167316033227966,
      "grad_norm": 0.39971861243247986,
      "learning_rate": 0.00015870914519170332,
      "loss": 1.1195,
      "step": 24090
    },
    {
      "epoch": 1.4173197088877454,
      "grad_norm": 0.361451119184494,
      "learning_rate": 0.00015865021998742927,
      "loss": 1.0574,
      "step": 24100
    },
    {
      "epoch": 1.4179078144526942,
      "grad_norm": 0.3556424379348755,
      "learning_rate": 0.00015859129478315524,
      "loss": 0.9801,
      "step": 24110
    },
    {
      "epoch": 1.4184959200176432,
      "grad_norm": 0.3547901511192322,
      "learning_rate": 0.0001585323695788812,
      "loss": 1.0662,
      "step": 24120
    },
    {
      "epoch": 1.419084025582592,
      "grad_norm": 0.43670395016670227,
      "learning_rate": 0.00015847344437460714,
      "loss": 1.0222,
      "step": 24130
    },
    {
      "epoch": 1.419672131147541,
      "grad_norm": 0.37242597341537476,
      "learning_rate": 0.0001584145191703331,
      "loss": 1.0493,
      "step": 24140
    },
    {
      "epoch": 1.42026023671249,
      "grad_norm": 0.32169032096862793,
      "learning_rate": 0.00015835559396605906,
      "loss": 1.0603,
      "step": 24150
    },
    {
      "epoch": 1.4208483422774387,
      "grad_norm": 0.33582767844200134,
      "learning_rate": 0.00015829666876178503,
      "loss": 1.0796,
      "step": 24160
    },
    {
      "epoch": 1.4214364478423878,
      "grad_norm": 0.35925981402397156,
      "learning_rate": 0.00015823774355751098,
      "loss": 0.9704,
      "step": 24170
    },
    {
      "epoch": 1.4220245534073366,
      "grad_norm": 0.3466480076313019,
      "learning_rate": 0.00015817881835323696,
      "loss": 0.8711,
      "step": 24180
    },
    {
      "epoch": 1.4226126589722856,
      "grad_norm": 0.3916873335838318,
      "learning_rate": 0.0001581198931489629,
      "loss": 1.0245,
      "step": 24190
    },
    {
      "epoch": 1.4232007645372344,
      "grad_norm": 0.3959715962409973,
      "learning_rate": 0.00015806096794468885,
      "loss": 0.9841,
      "step": 24200
    },
    {
      "epoch": 1.4237888701021832,
      "grad_norm": 0.3440030515193939,
      "learning_rate": 0.00015800204274041483,
      "loss": 1.067,
      "step": 24210
    },
    {
      "epoch": 1.4243769756671323,
      "grad_norm": 0.3554309904575348,
      "learning_rate": 0.00015794311753614078,
      "loss": 1.0829,
      "step": 24220
    },
    {
      "epoch": 1.424965081232081,
      "grad_norm": 0.3493784964084625,
      "learning_rate": 0.00015788419233186675,
      "loss": 1.0565,
      "step": 24230
    },
    {
      "epoch": 1.4255531867970301,
      "grad_norm": 0.32882246375083923,
      "learning_rate": 0.0001578252671275927,
      "loss": 1.0752,
      "step": 24240
    },
    {
      "epoch": 1.426141292361979,
      "grad_norm": 0.3979480564594269,
      "learning_rate": 0.00015776634192331867,
      "loss": 1.1021,
      "step": 24250
    },
    {
      "epoch": 1.4267293979269278,
      "grad_norm": 0.31292444467544556,
      "learning_rate": 0.00015770741671904462,
      "loss": 1.054,
      "step": 24260
    },
    {
      "epoch": 1.4273175034918768,
      "grad_norm": 0.34920111298561096,
      "learning_rate": 0.00015764849151477057,
      "loss": 0.9616,
      "step": 24270
    },
    {
      "epoch": 1.4279056090568256,
      "grad_norm": 0.3607979118824005,
      "learning_rate": 0.00015758956631049654,
      "loss": 1.0737,
      "step": 24280
    },
    {
      "epoch": 1.4284937146217747,
      "grad_norm": 0.35691705346107483,
      "learning_rate": 0.0001575306411062225,
      "loss": 1.0801,
      "step": 24290
    },
    {
      "epoch": 1.4290818201867235,
      "grad_norm": 0.41174760460853577,
      "learning_rate": 0.00015747171590194846,
      "loss": 1.0167,
      "step": 24300
    },
    {
      "epoch": 1.4296699257516723,
      "grad_norm": 0.3496910035610199,
      "learning_rate": 0.0001574127906976744,
      "loss": 1.0507,
      "step": 24310
    },
    {
      "epoch": 1.4302580313166213,
      "grad_norm": 0.35333067178726196,
      "learning_rate": 0.0001573538654934004,
      "loss": 0.971,
      "step": 24320
    },
    {
      "epoch": 1.4308461368815704,
      "grad_norm": 0.35673606395721436,
      "learning_rate": 0.00015729494028912633,
      "loss": 1.0614,
      "step": 24330
    },
    {
      "epoch": 1.4314342424465192,
      "grad_norm": 0.37092822790145874,
      "learning_rate": 0.0001572360150848523,
      "loss": 1.0305,
      "step": 24340
    },
    {
      "epoch": 1.432022348011468,
      "grad_norm": 0.37554797530174255,
      "learning_rate": 0.00015717708988057826,
      "loss": 1.034,
      "step": 24350
    },
    {
      "epoch": 1.432610453576417,
      "grad_norm": 0.34657660126686096,
      "learning_rate": 0.00015711816467630418,
      "loss": 0.9986,
      "step": 24360
    },
    {
      "epoch": 1.4331985591413658,
      "grad_norm": 0.47857943177223206,
      "learning_rate": 0.00015705923947203015,
      "loss": 1.0415,
      "step": 24370
    },
    {
      "epoch": 1.4337866647063149,
      "grad_norm": 0.3574732840061188,
      "learning_rate": 0.0001570003142677561,
      "loss": 1.0917,
      "step": 24380
    },
    {
      "epoch": 1.4343747702712637,
      "grad_norm": 0.3540315330028534,
      "learning_rate": 0.00015694138906348205,
      "loss": 0.9916,
      "step": 24390
    },
    {
      "epoch": 1.4349628758362125,
      "grad_norm": 0.34522539377212524,
      "learning_rate": 0.00015688246385920802,
      "loss": 1.0859,
      "step": 24400
    },
    {
      "epoch": 1.4355509814011616,
      "grad_norm": 0.35084763169288635,
      "learning_rate": 0.00015682353865493397,
      "loss": 1.0328,
      "step": 24410
    },
    {
      "epoch": 1.4361390869661104,
      "grad_norm": 0.30083659291267395,
      "learning_rate": 0.00015676461345065994,
      "loss": 0.8917,
      "step": 24420
    },
    {
      "epoch": 1.4367271925310594,
      "grad_norm": 0.3565019965171814,
      "learning_rate": 0.0001567056882463859,
      "loss": 1.0096,
      "step": 24430
    },
    {
      "epoch": 1.4373152980960082,
      "grad_norm": 0.336548775434494,
      "learning_rate": 0.00015664676304211187,
      "loss": 1.0806,
      "step": 24440
    },
    {
      "epoch": 1.437903403660957,
      "grad_norm": 0.37079885601997375,
      "learning_rate": 0.00015658783783783781,
      "loss": 0.997,
      "step": 24450
    },
    {
      "epoch": 1.438491509225906,
      "grad_norm": 0.38398000597953796,
      "learning_rate": 0.00015652891263356376,
      "loss": 1.0553,
      "step": 24460
    },
    {
      "epoch": 1.4390796147908549,
      "grad_norm": 0.33671244978904724,
      "learning_rate": 0.00015646998742928974,
      "loss": 1.0437,
      "step": 24470
    },
    {
      "epoch": 1.439667720355804,
      "grad_norm": 0.31804725527763367,
      "learning_rate": 0.00015641106222501568,
      "loss": 1.0212,
      "step": 24480
    },
    {
      "epoch": 1.4402558259207527,
      "grad_norm": 0.37522491812705994,
      "learning_rate": 0.00015635213702074166,
      "loss": 1.071,
      "step": 24490
    },
    {
      "epoch": 1.4408439314857016,
      "grad_norm": 0.3576429486274719,
      "learning_rate": 0.0001562932118164676,
      "loss": 1.0479,
      "step": 24500
    },
    {
      "epoch": 1.4414320370506506,
      "grad_norm": 0.4311547875404358,
      "learning_rate": 0.00015623428661219358,
      "loss": 0.985,
      "step": 24510
    },
    {
      "epoch": 1.4420201426155996,
      "grad_norm": 0.3843821883201599,
      "learning_rate": 0.00015617536140791953,
      "loss": 1.0426,
      "step": 24520
    },
    {
      "epoch": 1.4426082481805484,
      "grad_norm": 0.34915032982826233,
      "learning_rate": 0.00015611643620364548,
      "loss": 1.0371,
      "step": 24530
    },
    {
      "epoch": 1.4431963537454973,
      "grad_norm": 0.3158363699913025,
      "learning_rate": 0.00015605751099937145,
      "loss": 1.073,
      "step": 24540
    },
    {
      "epoch": 1.4437844593104463,
      "grad_norm": 0.3576527535915375,
      "learning_rate": 0.0001559985857950974,
      "loss": 1.0153,
      "step": 24550
    },
    {
      "epoch": 1.4443725648753951,
      "grad_norm": 0.3823189437389374,
      "learning_rate": 0.00015593966059082337,
      "loss": 1.0047,
      "step": 24560
    },
    {
      "epoch": 1.4449606704403442,
      "grad_norm": 0.38642987608909607,
      "learning_rate": 0.00015588073538654932,
      "loss": 1.0088,
      "step": 24570
    },
    {
      "epoch": 1.445548776005293,
      "grad_norm": 0.4032566249370575,
      "learning_rate": 0.0001558218101822753,
      "loss": 1.023,
      "step": 24580
    },
    {
      "epoch": 1.4461368815702418,
      "grad_norm": 0.33685198426246643,
      "learning_rate": 0.00015576288497800124,
      "loss": 1.0644,
      "step": 24590
    },
    {
      "epoch": 1.4467249871351908,
      "grad_norm": 0.33467310667037964,
      "learning_rate": 0.0001557039597737272,
      "loss": 0.9363,
      "step": 24600
    },
    {
      "epoch": 1.4473130927001396,
      "grad_norm": 0.33860841393470764,
      "learning_rate": 0.00015564503456945317,
      "loss": 0.9984,
      "step": 24610
    },
    {
      "epoch": 1.4479011982650887,
      "grad_norm": 0.3579542338848114,
      "learning_rate": 0.00015558610936517911,
      "loss": 1.0758,
      "step": 24620
    },
    {
      "epoch": 1.4484893038300375,
      "grad_norm": 0.3431657552719116,
      "learning_rate": 0.0001555271841609051,
      "loss": 1.0186,
      "step": 24630
    },
    {
      "epoch": 1.4490774093949863,
      "grad_norm": 0.3887501358985901,
      "learning_rate": 0.00015546825895663104,
      "loss": 1.0205,
      "step": 24640
    },
    {
      "epoch": 1.4496655149599353,
      "grad_norm": 0.35845455527305603,
      "learning_rate": 0.000155409333752357,
      "loss": 0.9468,
      "step": 24650
    },
    {
      "epoch": 1.4502536205248842,
      "grad_norm": 0.3281766474246979,
      "learning_rate": 0.00015535040854808296,
      "loss": 1.1232,
      "step": 24660
    },
    {
      "epoch": 1.4508417260898332,
      "grad_norm": 0.3515635132789612,
      "learning_rate": 0.0001552914833438089,
      "loss": 1.0825,
      "step": 24670
    },
    {
      "epoch": 1.451429831654782,
      "grad_norm": 0.3394002914428711,
      "learning_rate": 0.00015523255813953488,
      "loss": 1.0255,
      "step": 24680
    },
    {
      "epoch": 1.4520179372197308,
      "grad_norm": 0.3459037244319916,
      "learning_rate": 0.00015517363293526083,
      "loss": 1.0657,
      "step": 24690
    },
    {
      "epoch": 1.4526060427846799,
      "grad_norm": 0.3642634451389313,
      "learning_rate": 0.0001551147077309868,
      "loss": 1.1458,
      "step": 24700
    },
    {
      "epoch": 1.4531941483496287,
      "grad_norm": 0.32402732968330383,
      "learning_rate": 0.00015505578252671275,
      "loss": 1.0115,
      "step": 24710
    },
    {
      "epoch": 1.4537822539145777,
      "grad_norm": 0.31217920780181885,
      "learning_rate": 0.00015499685732243873,
      "loss": 1.0335,
      "step": 24720
    },
    {
      "epoch": 1.4543703594795265,
      "grad_norm": 0.3721821904182434,
      "learning_rate": 0.00015493793211816467,
      "loss": 1.0291,
      "step": 24730
    },
    {
      "epoch": 1.4549584650444753,
      "grad_norm": 0.3092610538005829,
      "learning_rate": 0.00015487900691389062,
      "loss": 1.0135,
      "step": 24740
    },
    {
      "epoch": 1.4555465706094244,
      "grad_norm": 0.35018882155418396,
      "learning_rate": 0.0001548200817096166,
      "loss": 1.1064,
      "step": 24750
    },
    {
      "epoch": 1.4561346761743734,
      "grad_norm": 0.3549729883670807,
      "learning_rate": 0.00015476115650534254,
      "loss": 1.0685,
      "step": 24760
    },
    {
      "epoch": 1.4567227817393222,
      "grad_norm": 0.3491494953632355,
      "learning_rate": 0.00015470223130106852,
      "loss": 1.0037,
      "step": 24770
    },
    {
      "epoch": 1.457310887304271,
      "grad_norm": 0.3539985716342926,
      "learning_rate": 0.00015464330609679444,
      "loss": 1.0553,
      "step": 24780
    },
    {
      "epoch": 1.45789899286922,
      "grad_norm": 0.3645873963832855,
      "learning_rate": 0.0001545843808925204,
      "loss": 1.0765,
      "step": 24790
    },
    {
      "epoch": 1.458487098434169,
      "grad_norm": 0.3003985583782196,
      "learning_rate": 0.00015452545568824636,
      "loss": 1.0119,
      "step": 24800
    },
    {
      "epoch": 1.459075203999118,
      "grad_norm": 0.36327916383743286,
      "learning_rate": 0.0001544665304839723,
      "loss": 1.0919,
      "step": 24810
    },
    {
      "epoch": 1.4596633095640668,
      "grad_norm": 0.3352181613445282,
      "learning_rate": 0.00015440760527969828,
      "loss": 1.181,
      "step": 24820
    },
    {
      "epoch": 1.4602514151290156,
      "grad_norm": 0.31178078055381775,
      "learning_rate": 0.00015434868007542423,
      "loss": 0.999,
      "step": 24830
    },
    {
      "epoch": 1.4608395206939646,
      "grad_norm": 0.3186665177345276,
      "learning_rate": 0.0001542897548711502,
      "loss": 1.0453,
      "step": 24840
    },
    {
      "epoch": 1.4614276262589134,
      "grad_norm": 0.32163673639297485,
      "learning_rate": 0.00015423082966687615,
      "loss": 1.0604,
      "step": 24850
    },
    {
      "epoch": 1.4620157318238625,
      "grad_norm": 0.33115577697753906,
      "learning_rate": 0.0001541719044626021,
      "loss": 0.9885,
      "step": 24860
    },
    {
      "epoch": 1.4626038373888113,
      "grad_norm": 0.3893609344959259,
      "learning_rate": 0.00015411297925832808,
      "loss": 0.9964,
      "step": 24870
    },
    {
      "epoch": 1.46319194295376,
      "grad_norm": 0.3175526559352875,
      "learning_rate": 0.00015405405405405402,
      "loss": 0.9486,
      "step": 24880
    },
    {
      "epoch": 1.4637800485187091,
      "grad_norm": 0.3148840367794037,
      "learning_rate": 0.00015399512884978,
      "loss": 0.99,
      "step": 24890
    },
    {
      "epoch": 1.464368154083658,
      "grad_norm": 0.31371253728866577,
      "learning_rate": 0.00015393620364550595,
      "loss": 1.0704,
      "step": 24900
    },
    {
      "epoch": 1.464956259648607,
      "grad_norm": 0.39165106415748596,
      "learning_rate": 0.00015387727844123192,
      "loss": 1.1275,
      "step": 24910
    },
    {
      "epoch": 1.4655443652135558,
      "grad_norm": 0.3576233685016632,
      "learning_rate": 0.00015381835323695787,
      "loss": 1.0249,
      "step": 24920
    },
    {
      "epoch": 1.4661324707785046,
      "grad_norm": 0.36467984318733215,
      "learning_rate": 0.00015375942803268382,
      "loss": 1.0639,
      "step": 24930
    },
    {
      "epoch": 1.4667205763434537,
      "grad_norm": 0.3397107720375061,
      "learning_rate": 0.0001537005028284098,
      "loss": 1.0,
      "step": 24940
    },
    {
      "epoch": 1.4673086819084027,
      "grad_norm": 0.33672720193862915,
      "learning_rate": 0.00015364157762413574,
      "loss": 0.9962,
      "step": 24950
    },
    {
      "epoch": 1.4678967874733515,
      "grad_norm": 0.3538486659526825,
      "learning_rate": 0.00015358265241986171,
      "loss": 1.0229,
      "step": 24960
    },
    {
      "epoch": 1.4684848930383003,
      "grad_norm": 0.3755020797252655,
      "learning_rate": 0.00015352372721558766,
      "loss": 0.9712,
      "step": 24970
    },
    {
      "epoch": 1.4690729986032494,
      "grad_norm": 0.3658936619758606,
      "learning_rate": 0.00015346480201131364,
      "loss": 1.0434,
      "step": 24980
    },
    {
      "epoch": 1.4696611041681982,
      "grad_norm": 0.3008204698562622,
      "learning_rate": 0.00015340587680703958,
      "loss": 0.9295,
      "step": 24990
    },
    {
      "epoch": 1.4702492097331472,
      "grad_norm": 0.3271540701389313,
      "learning_rate": 0.00015334695160276553,
      "loss": 1.0074,
      "step": 25000
    },
    {
      "epoch": 1.470837315298096,
      "grad_norm": 0.3742002248764038,
      "learning_rate": 0.0001532880263984915,
      "loss": 1.0223,
      "step": 25010
    },
    {
      "epoch": 1.4714254208630448,
      "grad_norm": 0.3422985076904297,
      "learning_rate": 0.00015322910119421745,
      "loss": 1.0451,
      "step": 25020
    },
    {
      "epoch": 1.4720135264279939,
      "grad_norm": 0.3712857663631439,
      "learning_rate": 0.00015317017598994343,
      "loss": 1.0237,
      "step": 25030
    },
    {
      "epoch": 1.4726016319929427,
      "grad_norm": 0.33892548084259033,
      "learning_rate": 0.00015311125078566938,
      "loss": 1.0359,
      "step": 25040
    },
    {
      "epoch": 1.4731897375578917,
      "grad_norm": 0.3682754635810852,
      "learning_rate": 0.00015305232558139535,
      "loss": 1.0283,
      "step": 25050
    },
    {
      "epoch": 1.4737778431228405,
      "grad_norm": 0.35185152292251587,
      "learning_rate": 0.0001529934003771213,
      "loss": 0.9265,
      "step": 25060
    },
    {
      "epoch": 1.4743659486877894,
      "grad_norm": 0.3242577910423279,
      "learning_rate": 0.00015293447517284725,
      "loss": 1.1578,
      "step": 25070
    },
    {
      "epoch": 1.4749540542527384,
      "grad_norm": 0.3058888614177704,
      "learning_rate": 0.00015287554996857322,
      "loss": 1.049,
      "step": 25080
    },
    {
      "epoch": 1.4755421598176872,
      "grad_norm": 0.3302425444126129,
      "learning_rate": 0.00015281662476429917,
      "loss": 0.9593,
      "step": 25090
    },
    {
      "epoch": 1.4761302653826363,
      "grad_norm": 0.34021204710006714,
      "learning_rate": 0.00015275769956002514,
      "loss": 1.0077,
      "step": 25100
    },
    {
      "epoch": 1.476718370947585,
      "grad_norm": 0.3802245855331421,
      "learning_rate": 0.0001526987743557511,
      "loss": 1.1136,
      "step": 25110
    },
    {
      "epoch": 1.4773064765125339,
      "grad_norm": 0.34978559613227844,
      "learning_rate": 0.00015263984915147707,
      "loss": 1.0005,
      "step": 25120
    },
    {
      "epoch": 1.477894582077483,
      "grad_norm": 0.3251127004623413,
      "learning_rate": 0.00015258092394720301,
      "loss": 1.0791,
      "step": 25130
    },
    {
      "epoch": 1.4784826876424317,
      "grad_norm": 0.33019572496414185,
      "learning_rate": 0.00015252199874292896,
      "loss": 1.0548,
      "step": 25140
    },
    {
      "epoch": 1.4790707932073808,
      "grad_norm": 0.33003219962120056,
      "learning_rate": 0.00015246307353865494,
      "loss": 1.0477,
      "step": 25150
    },
    {
      "epoch": 1.4796588987723296,
      "grad_norm": 0.3703914284706116,
      "learning_rate": 0.00015240414833438088,
      "loss": 1.0075,
      "step": 25160
    },
    {
      "epoch": 1.4802470043372784,
      "grad_norm": 0.34275588393211365,
      "learning_rate": 0.00015234522313010686,
      "loss": 1.0447,
      "step": 25170
    },
    {
      "epoch": 1.4808351099022274,
      "grad_norm": 0.4123154580593109,
      "learning_rate": 0.0001522862979258328,
      "loss": 1.0553,
      "step": 25180
    },
    {
      "epoch": 1.4814232154671765,
      "grad_norm": 0.34909147024154663,
      "learning_rate": 0.00015222737272155878,
      "loss": 0.9618,
      "step": 25190
    },
    {
      "epoch": 1.4820113210321253,
      "grad_norm": 0.3385658264160156,
      "learning_rate": 0.0001521684475172847,
      "loss": 1.0849,
      "step": 25200
    },
    {
      "epoch": 1.482599426597074,
      "grad_norm": 0.298293799161911,
      "learning_rate": 0.00015210952231301065,
      "loss": 1.1022,
      "step": 25210
    },
    {
      "epoch": 1.4831875321620231,
      "grad_norm": 0.35795658826828003,
      "learning_rate": 0.00015205059710873662,
      "loss": 0.9794,
      "step": 25220
    },
    {
      "epoch": 1.483775637726972,
      "grad_norm": 0.3507162630558014,
      "learning_rate": 0.00015199167190446257,
      "loss": 0.9317,
      "step": 25230
    },
    {
      "epoch": 1.484363743291921,
      "grad_norm": 0.36439698934555054,
      "learning_rate": 0.00015193274670018855,
      "loss": 0.9731,
      "step": 25240
    },
    {
      "epoch": 1.4849518488568698,
      "grad_norm": 0.38709700107574463,
      "learning_rate": 0.0001518738214959145,
      "loss": 1.0271,
      "step": 25250
    },
    {
      "epoch": 1.4855399544218186,
      "grad_norm": 0.3426320552825928,
      "learning_rate": 0.00015181489629164044,
      "loss": 1.0785,
      "step": 25260
    },
    {
      "epoch": 1.4861280599867677,
      "grad_norm": 0.3521478772163391,
      "learning_rate": 0.00015175597108736642,
      "loss": 1.0238,
      "step": 25270
    },
    {
      "epoch": 1.4867161655517165,
      "grad_norm": 0.33334192633628845,
      "learning_rate": 0.00015169704588309236,
      "loss": 1.1117,
      "step": 25280
    },
    {
      "epoch": 1.4873042711166655,
      "grad_norm": 0.3709181845188141,
      "learning_rate": 0.00015163812067881834,
      "loss": 1.0537,
      "step": 25290
    },
    {
      "epoch": 1.4878923766816143,
      "grad_norm": 0.3173593580722809,
      "learning_rate": 0.00015157919547454429,
      "loss": 0.9832,
      "step": 25300
    },
    {
      "epoch": 1.4884804822465632,
      "grad_norm": 0.34687715768814087,
      "learning_rate": 0.00015152027027027026,
      "loss": 1.0465,
      "step": 25310
    },
    {
      "epoch": 1.4890685878115122,
      "grad_norm": 0.3370822072029114,
      "learning_rate": 0.0001514613450659962,
      "loss": 1.0116,
      "step": 25320
    },
    {
      "epoch": 1.489656693376461,
      "grad_norm": 0.35694801807403564,
      "learning_rate": 0.00015140241986172216,
      "loss": 0.9778,
      "step": 25330
    },
    {
      "epoch": 1.49024479894141,
      "grad_norm": 0.39337193965911865,
      "learning_rate": 0.00015134349465744813,
      "loss": 0.9953,
      "step": 25340
    },
    {
      "epoch": 1.4908329045063589,
      "grad_norm": 0.3749016523361206,
      "learning_rate": 0.00015128456945317408,
      "loss": 1.1037,
      "step": 25350
    },
    {
      "epoch": 1.4914210100713077,
      "grad_norm": 0.33766472339630127,
      "learning_rate": 0.00015122564424890005,
      "loss": 1.0196,
      "step": 25360
    },
    {
      "epoch": 1.4920091156362567,
      "grad_norm": 0.3787684142589569,
      "learning_rate": 0.000151166719044626,
      "loss": 0.9875,
      "step": 25370
    },
    {
      "epoch": 1.4925972212012057,
      "grad_norm": 0.3562096357345581,
      "learning_rate": 0.00015110779384035198,
      "loss": 1.0349,
      "step": 25380
    },
    {
      "epoch": 1.4931853267661546,
      "grad_norm": 0.4081413149833679,
      "learning_rate": 0.00015104886863607792,
      "loss": 1.0295,
      "step": 25390
    },
    {
      "epoch": 1.4937734323311034,
      "grad_norm": 0.3533773124217987,
      "learning_rate": 0.00015098994343180387,
      "loss": 0.9986,
      "step": 25400
    },
    {
      "epoch": 1.4943615378960524,
      "grad_norm": 0.36933550238609314,
      "learning_rate": 0.00015093101822752985,
      "loss": 1.072,
      "step": 25410
    },
    {
      "epoch": 1.4949496434610012,
      "grad_norm": 0.38499462604522705,
      "learning_rate": 0.0001508720930232558,
      "loss": 1.0333,
      "step": 25420
    },
    {
      "epoch": 1.4955377490259503,
      "grad_norm": 0.32370030879974365,
      "learning_rate": 0.00015081316781898177,
      "loss": 1.0455,
      "step": 25430
    },
    {
      "epoch": 1.496125854590899,
      "grad_norm": 0.36920878291130066,
      "learning_rate": 0.00015075424261470772,
      "loss": 0.9846,
      "step": 25440
    },
    {
      "epoch": 1.496713960155848,
      "grad_norm": 0.34459495544433594,
      "learning_rate": 0.0001506953174104337,
      "loss": 0.9969,
      "step": 25450
    },
    {
      "epoch": 1.497302065720797,
      "grad_norm": 0.327000230550766,
      "learning_rate": 0.00015063639220615964,
      "loss": 0.9924,
      "step": 25460
    },
    {
      "epoch": 1.4978901712857458,
      "grad_norm": 0.34400779008865356,
      "learning_rate": 0.00015057746700188559,
      "loss": 1.0508,
      "step": 25470
    },
    {
      "epoch": 1.4984782768506948,
      "grad_norm": 0.345580130815506,
      "learning_rate": 0.00015051854179761156,
      "loss": 0.9642,
      "step": 25480
    },
    {
      "epoch": 1.4990663824156436,
      "grad_norm": 0.3529553711414337,
      "learning_rate": 0.0001504596165933375,
      "loss": 1.0334,
      "step": 25490
    },
    {
      "epoch": 1.4996544879805924,
      "grad_norm": 0.3869370222091675,
      "learning_rate": 0.00015040069138906348,
      "loss": 0.9417,
      "step": 25500
    },
    {
      "epoch": 1.5002425935455415,
      "grad_norm": 0.37179628014564514,
      "learning_rate": 0.00015034176618478943,
      "loss": 0.9969,
      "step": 25510
    },
    {
      "epoch": 1.5008306991104905,
      "grad_norm": 0.3928183615207672,
      "learning_rate": 0.0001502828409805154,
      "loss": 0.991,
      "step": 25520
    },
    {
      "epoch": 1.5014188046754393,
      "grad_norm": 0.357713520526886,
      "learning_rate": 0.00015022391577624135,
      "loss": 1.0417,
      "step": 25530
    },
    {
      "epoch": 1.5020069102403881,
      "grad_norm": 0.34529685974121094,
      "learning_rate": 0.0001501649905719673,
      "loss": 1.0,
      "step": 25540
    },
    {
      "epoch": 1.502595015805337,
      "grad_norm": 0.35772308707237244,
      "learning_rate": 0.00015010606536769328,
      "loss": 1.0055,
      "step": 25550
    },
    {
      "epoch": 1.503183121370286,
      "grad_norm": 0.36746153235435486,
      "learning_rate": 0.00015004714016341922,
      "loss": 1.0347,
      "step": 25560
    },
    {
      "epoch": 1.503771226935235,
      "grad_norm": 0.3200569748878479,
      "learning_rate": 0.00014998821495914517,
      "loss": 0.9426,
      "step": 25570
    },
    {
      "epoch": 1.5043593325001838,
      "grad_norm": 0.3458486795425415,
      "learning_rate": 0.00014992928975487115,
      "loss": 0.9574,
      "step": 25580
    },
    {
      "epoch": 1.5049474380651326,
      "grad_norm": 0.3565143644809723,
      "learning_rate": 0.0001498703645505971,
      "loss": 1.0379,
      "step": 25590
    },
    {
      "epoch": 1.5055355436300815,
      "grad_norm": 0.3107713460922241,
      "learning_rate": 0.00014981143934632304,
      "loss": 0.9862,
      "step": 25600
    },
    {
      "epoch": 1.5061236491950305,
      "grad_norm": 0.4026002585887909,
      "learning_rate": 0.00014975251414204902,
      "loss": 0.9828,
      "step": 25610
    },
    {
      "epoch": 1.5067117547599795,
      "grad_norm": 0.33840635418891907,
      "learning_rate": 0.00014969358893777496,
      "loss": 1.0446,
      "step": 25620
    },
    {
      "epoch": 1.5072998603249284,
      "grad_norm": 0.4085220992565155,
      "learning_rate": 0.00014963466373350094,
      "loss": 1.0075,
      "step": 25630
    },
    {
      "epoch": 1.5078879658898772,
      "grad_norm": 0.338303804397583,
      "learning_rate": 0.00014957573852922689,
      "loss": 1.0709,
      "step": 25640
    },
    {
      "epoch": 1.508476071454826,
      "grad_norm": 0.32407692074775696,
      "learning_rate": 0.00014951681332495286,
      "loss": 0.9866,
      "step": 25650
    },
    {
      "epoch": 1.509064177019775,
      "grad_norm": 0.3972952365875244,
      "learning_rate": 0.0001494578881206788,
      "loss": 0.9446,
      "step": 25660
    },
    {
      "epoch": 1.509652282584724,
      "grad_norm": 0.3512197732925415,
      "learning_rate": 0.00014939896291640476,
      "loss": 1.0533,
      "step": 25670
    },
    {
      "epoch": 1.5102403881496729,
      "grad_norm": 0.3583589494228363,
      "learning_rate": 0.00014934003771213073,
      "loss": 1.1286,
      "step": 25680
    },
    {
      "epoch": 1.5108284937146217,
      "grad_norm": 0.3001783788204193,
      "learning_rate": 0.00014928111250785668,
      "loss": 0.9652,
      "step": 25690
    },
    {
      "epoch": 1.5114165992795707,
      "grad_norm": 0.3512403070926666,
      "learning_rate": 0.00014922218730358265,
      "loss": 0.9679,
      "step": 25700
    },
    {
      "epoch": 1.5120047048445195,
      "grad_norm": 0.36043718457221985,
      "learning_rate": 0.0001491632620993086,
      "loss": 1.061,
      "step": 25710
    },
    {
      "epoch": 1.5125928104094686,
      "grad_norm": 0.37454935908317566,
      "learning_rate": 0.00014910433689503455,
      "loss": 1.0493,
      "step": 25720
    },
    {
      "epoch": 1.5131809159744174,
      "grad_norm": 0.4157048463821411,
      "learning_rate": 0.0001490454116907605,
      "loss": 0.9861,
      "step": 25730
    },
    {
      "epoch": 1.5137690215393662,
      "grad_norm": 0.3770911395549774,
      "learning_rate": 0.00014898648648648647,
      "loss": 0.9554,
      "step": 25740
    },
    {
      "epoch": 1.5143571271043152,
      "grad_norm": 0.33735114336013794,
      "learning_rate": 0.00014892756128221242,
      "loss": 1.0013,
      "step": 25750
    },
    {
      "epoch": 1.5149452326692643,
      "grad_norm": 0.3510373532772064,
      "learning_rate": 0.0001488686360779384,
      "loss": 0.9615,
      "step": 25760
    },
    {
      "epoch": 1.515533338234213,
      "grad_norm": 0.31969714164733887,
      "learning_rate": 0.00014880971087366434,
      "loss": 1.0221,
      "step": 25770
    },
    {
      "epoch": 1.516121443799162,
      "grad_norm": 0.35724908113479614,
      "learning_rate": 0.00014875078566939032,
      "loss": 1.0093,
      "step": 25780
    },
    {
      "epoch": 1.5167095493641107,
      "grad_norm": 0.3563688099384308,
      "learning_rate": 0.00014869186046511626,
      "loss": 1.0424,
      "step": 25790
    },
    {
      "epoch": 1.5172976549290598,
      "grad_norm": 0.33974093198776245,
      "learning_rate": 0.0001486329352608422,
      "loss": 1.0299,
      "step": 25800
    },
    {
      "epoch": 1.5178857604940088,
      "grad_norm": 0.6550647020339966,
      "learning_rate": 0.00014857401005656819,
      "loss": 1.0561,
      "step": 25810
    },
    {
      "epoch": 1.5184738660589576,
      "grad_norm": 0.36218029260635376,
      "learning_rate": 0.00014851508485229413,
      "loss": 0.9583,
      "step": 25820
    },
    {
      "epoch": 1.5190619716239064,
      "grad_norm": 0.40476179122924805,
      "learning_rate": 0.0001484561596480201,
      "loss": 1.0146,
      "step": 25830
    },
    {
      "epoch": 1.5196500771888553,
      "grad_norm": 0.397590696811676,
      "learning_rate": 0.00014839723444374606,
      "loss": 0.8753,
      "step": 25840
    },
    {
      "epoch": 1.5202381827538043,
      "grad_norm": 0.3226390779018402,
      "learning_rate": 0.00014833830923947203,
      "loss": 1.0521,
      "step": 25850
    },
    {
      "epoch": 1.5208262883187533,
      "grad_norm": 0.3860856592655182,
      "learning_rate": 0.00014827938403519798,
      "loss": 0.9749,
      "step": 25860
    },
    {
      "epoch": 1.5214143938837021,
      "grad_norm": 0.3724854588508606,
      "learning_rate": 0.00014822045883092393,
      "loss": 1.0492,
      "step": 25870
    },
    {
      "epoch": 1.522002499448651,
      "grad_norm": 0.3166047930717468,
      "learning_rate": 0.0001481615336266499,
      "loss": 1.0092,
      "step": 25880
    },
    {
      "epoch": 1.5225906050136,
      "grad_norm": 0.3316105008125305,
      "learning_rate": 0.00014810260842237585,
      "loss": 1.0919,
      "step": 25890
    },
    {
      "epoch": 1.5231787105785488,
      "grad_norm": 0.398543119430542,
      "learning_rate": 0.00014804368321810182,
      "loss": 1.0335,
      "step": 25900
    },
    {
      "epoch": 1.5237668161434978,
      "grad_norm": 0.3678348958492279,
      "learning_rate": 0.00014798475801382777,
      "loss": 0.9785,
      "step": 25910
    },
    {
      "epoch": 1.5243549217084467,
      "grad_norm": 0.32650208473205566,
      "learning_rate": 0.00014792583280955375,
      "loss": 1.1118,
      "step": 25920
    },
    {
      "epoch": 1.5249430272733955,
      "grad_norm": 0.3417334258556366,
      "learning_rate": 0.00014786690760527967,
      "loss": 0.9851,
      "step": 25930
    },
    {
      "epoch": 1.5255311328383445,
      "grad_norm": 0.3314702808856964,
      "learning_rate": 0.00014780798240100564,
      "loss": 1.0097,
      "step": 25940
    },
    {
      "epoch": 1.5261192384032936,
      "grad_norm": 0.32426968216896057,
      "learning_rate": 0.0001477490571967316,
      "loss": 1.0725,
      "step": 25950
    },
    {
      "epoch": 1.5267073439682424,
      "grad_norm": 0.4407394230365753,
      "learning_rate": 0.00014769013199245756,
      "loss": 1.0667,
      "step": 25960
    },
    {
      "epoch": 1.5272954495331912,
      "grad_norm": 0.3831948935985565,
      "learning_rate": 0.0001476312067881835,
      "loss": 0.9313,
      "step": 25970
    },
    {
      "epoch": 1.52788355509814,
      "grad_norm": 0.3638412058353424,
      "learning_rate": 0.00014757228158390949,
      "loss": 1.028,
      "step": 25980
    },
    {
      "epoch": 1.528471660663089,
      "grad_norm": 0.35073259472846985,
      "learning_rate": 0.00014751335637963543,
      "loss": 0.9693,
      "step": 25990
    },
    {
      "epoch": 1.529059766228038,
      "grad_norm": 0.41599711775779724,
      "learning_rate": 0.00014745443117536138,
      "loss": 0.9623,
      "step": 26000
    },
    {
      "epoch": 1.529647871792987,
      "grad_norm": 0.39332419633865356,
      "learning_rate": 0.00014739550597108736,
      "loss": 1.0582,
      "step": 26010
    },
    {
      "epoch": 1.5302359773579357,
      "grad_norm": 0.37632590532302856,
      "learning_rate": 0.0001473365807668133,
      "loss": 0.9421,
      "step": 26020
    },
    {
      "epoch": 1.5308240829228845,
      "grad_norm": 0.37986770272254944,
      "learning_rate": 0.00014727765556253928,
      "loss": 1.0039,
      "step": 26030
    },
    {
      "epoch": 1.5314121884878336,
      "grad_norm": 0.3153214454650879,
      "learning_rate": 0.00014722462287869263,
      "loss": 0.9938,
      "step": 26040
    },
    {
      "epoch": 1.5320002940527826,
      "grad_norm": 0.38498368859291077,
      "learning_rate": 0.00014716569767441858,
      "loss": 0.9381,
      "step": 26050
    },
    {
      "epoch": 1.5325883996177314,
      "grad_norm": 0.3457787334918976,
      "learning_rate": 0.00014710677247014456,
      "loss": 0.9593,
      "step": 26060
    },
    {
      "epoch": 1.5331765051826802,
      "grad_norm": 0.3258185386657715,
      "learning_rate": 0.0001470478472658705,
      "loss": 1.0516,
      "step": 26070
    },
    {
      "epoch": 1.533764610747629,
      "grad_norm": 0.34432104229927063,
      "learning_rate": 0.00014698892206159648,
      "loss": 1.1142,
      "step": 26080
    },
    {
      "epoch": 1.534352716312578,
      "grad_norm": 0.4558851420879364,
      "learning_rate": 0.00014692999685732243,
      "loss": 1.032,
      "step": 26090
    },
    {
      "epoch": 1.5349408218775271,
      "grad_norm": 0.4085523188114166,
      "learning_rate": 0.00014687107165304837,
      "loss": 1.1069,
      "step": 26100
    },
    {
      "epoch": 1.535528927442476,
      "grad_norm": 0.35115525126457214,
      "learning_rate": 0.00014681214644877435,
      "loss": 1.0729,
      "step": 26110
    },
    {
      "epoch": 1.5361170330074247,
      "grad_norm": 0.3813783824443817,
      "learning_rate": 0.0001467532212445003,
      "loss": 0.9607,
      "step": 26120
    },
    {
      "epoch": 1.5367051385723738,
      "grad_norm": 0.40081775188446045,
      "learning_rate": 0.00014669429604022627,
      "loss": 0.9599,
      "step": 26130
    },
    {
      "epoch": 1.5372932441373226,
      "grad_norm": 0.3474396765232086,
      "learning_rate": 0.00014663537083595222,
      "loss": 1.0328,
      "step": 26140
    },
    {
      "epoch": 1.5378813497022716,
      "grad_norm": 0.32305288314819336,
      "learning_rate": 0.0001465764456316782,
      "loss": 0.9869,
      "step": 26150
    },
    {
      "epoch": 1.5384694552672205,
      "grad_norm": 0.3562493920326233,
      "learning_rate": 0.00014651752042740414,
      "loss": 1.0445,
      "step": 26160
    },
    {
      "epoch": 1.5390575608321693,
      "grad_norm": 0.31287282705307007,
      "learning_rate": 0.0001464585952231301,
      "loss": 1.0001,
      "step": 26170
    },
    {
      "epoch": 1.5396456663971183,
      "grad_norm": 0.3079422116279602,
      "learning_rate": 0.00014639967001885606,
      "loss": 0.976,
      "step": 26180
    },
    {
      "epoch": 1.5402337719620673,
      "grad_norm": 0.3345564603805542,
      "learning_rate": 0.000146340744814582,
      "loss": 1.0281,
      "step": 26190
    },
    {
      "epoch": 1.5408218775270162,
      "grad_norm": 0.3564794957637787,
      "learning_rate": 0.00014628181961030799,
      "loss": 0.9583,
      "step": 26200
    },
    {
      "epoch": 1.541409983091965,
      "grad_norm": 0.3599005937576294,
      "learning_rate": 0.00014622289440603393,
      "loss": 0.9977,
      "step": 26210
    },
    {
      "epoch": 1.5419980886569138,
      "grad_norm": 0.40351465344429016,
      "learning_rate": 0.00014616396920175988,
      "loss": 1.0213,
      "step": 26220
    },
    {
      "epoch": 1.5425861942218628,
      "grad_norm": 0.3482559025287628,
      "learning_rate": 0.00014610504399748583,
      "loss": 0.9759,
      "step": 26230
    },
    {
      "epoch": 1.5431742997868119,
      "grad_norm": 0.38278427720069885,
      "learning_rate": 0.0001460461187932118,
      "loss": 0.9439,
      "step": 26240
    },
    {
      "epoch": 1.5437624053517607,
      "grad_norm": 0.31109967827796936,
      "learning_rate": 0.00014598719358893775,
      "loss": 1.022,
      "step": 26250
    },
    {
      "epoch": 1.5443505109167095,
      "grad_norm": 0.33254843950271606,
      "learning_rate": 0.00014592826838466373,
      "loss": 1.0176,
      "step": 26260
    },
    {
      "epoch": 1.5449386164816583,
      "grad_norm": 0.3933364748954773,
      "learning_rate": 0.00014586934318038967,
      "loss": 0.9366,
      "step": 26270
    },
    {
      "epoch": 1.5455267220466073,
      "grad_norm": 0.3579438328742981,
      "learning_rate": 0.00014581041797611565,
      "loss": 1.0912,
      "step": 26280
    },
    {
      "epoch": 1.5461148276115564,
      "grad_norm": 0.3458821177482605,
      "learning_rate": 0.0001457514927718416,
      "loss": 0.997,
      "step": 26290
    },
    {
      "epoch": 1.5467029331765052,
      "grad_norm": 0.3177313506603241,
      "learning_rate": 0.00014569256756756754,
      "loss": 1.0312,
      "step": 26300
    },
    {
      "epoch": 1.547291038741454,
      "grad_norm": 0.388115793466568,
      "learning_rate": 0.00014563364236329352,
      "loss": 1.0405,
      "step": 26310
    },
    {
      "epoch": 1.547879144306403,
      "grad_norm": 0.3396031856536865,
      "learning_rate": 0.00014557471715901947,
      "loss": 0.9988,
      "step": 26320
    },
    {
      "epoch": 1.5484672498713519,
      "grad_norm": 0.4149866998195648,
      "learning_rate": 0.00014551579195474544,
      "loss": 0.8906,
      "step": 26330
    },
    {
      "epoch": 1.549055355436301,
      "grad_norm": 0.3808273673057556,
      "learning_rate": 0.0001454568667504714,
      "loss": 0.98,
      "step": 26340
    },
    {
      "epoch": 1.5496434610012497,
      "grad_norm": 0.36122840642929077,
      "learning_rate": 0.00014539794154619736,
      "loss": 1.0326,
      "step": 26350
    },
    {
      "epoch": 1.5502315665661985,
      "grad_norm": 0.34509655833244324,
      "learning_rate": 0.0001453390163419233,
      "loss": 0.9716,
      "step": 26360
    },
    {
      "epoch": 1.5508196721311476,
      "grad_norm": 0.3224962651729584,
      "learning_rate": 0.00014528009113764926,
      "loss": 1.0918,
      "step": 26370
    },
    {
      "epoch": 1.5514077776960966,
      "grad_norm": 0.34944871068000793,
      "learning_rate": 0.00014522116593337523,
      "loss": 1.1273,
      "step": 26380
    },
    {
      "epoch": 1.5519958832610454,
      "grad_norm": 0.332276850938797,
      "learning_rate": 0.00014516224072910118,
      "loss": 1.0367,
      "step": 26390
    },
    {
      "epoch": 1.5525839888259942,
      "grad_norm": 0.37913429737091064,
      "learning_rate": 0.00014510331552482716,
      "loss": 1.0534,
      "step": 26400
    },
    {
      "epoch": 1.553172094390943,
      "grad_norm": 0.3755055069923401,
      "learning_rate": 0.0001450443903205531,
      "loss": 1.0743,
      "step": 26410
    },
    {
      "epoch": 1.553760199955892,
      "grad_norm": 0.3604620695114136,
      "learning_rate": 0.00014498546511627908,
      "loss": 1.0519,
      "step": 26420
    },
    {
      "epoch": 1.5543483055208411,
      "grad_norm": 0.36406251788139343,
      "learning_rate": 0.000144926539912005,
      "loss": 1.0058,
      "step": 26430
    },
    {
      "epoch": 1.55493641108579,
      "grad_norm": 0.3770683705806732,
      "learning_rate": 0.00014486761470773097,
      "loss": 1.0572,
      "step": 26440
    },
    {
      "epoch": 1.5555245166507388,
      "grad_norm": 0.38303470611572266,
      "learning_rate": 0.00014480868950345692,
      "loss": 1.0627,
      "step": 26450
    },
    {
      "epoch": 1.5561126222156876,
      "grad_norm": 0.30880433320999146,
      "learning_rate": 0.0001447497642991829,
      "loss": 1.0951,
      "step": 26460
    },
    {
      "epoch": 1.5567007277806366,
      "grad_norm": 0.3796963393688202,
      "learning_rate": 0.00014469083909490884,
      "loss": 1.0726,
      "step": 26470
    },
    {
      "epoch": 1.5572888333455857,
      "grad_norm": 0.341668039560318,
      "learning_rate": 0.00014463191389063482,
      "loss": 1.0245,
      "step": 26480
    },
    {
      "epoch": 1.5578769389105345,
      "grad_norm": 0.3469761312007904,
      "learning_rate": 0.00014457298868636077,
      "loss": 1.0027,
      "step": 26490
    },
    {
      "epoch": 1.5584650444754833,
      "grad_norm": 0.34357336163520813,
      "learning_rate": 0.0001445140634820867,
      "loss": 1.084,
      "step": 26500
    },
    {
      "epoch": 1.559053150040432,
      "grad_norm": 0.3445056974887848,
      "learning_rate": 0.0001444551382778127,
      "loss": 0.997,
      "step": 26510
    },
    {
      "epoch": 1.5596412556053811,
      "grad_norm": 0.3478359878063202,
      "learning_rate": 0.00014439621307353864,
      "loss": 1.0181,
      "step": 26520
    },
    {
      "epoch": 1.5602293611703302,
      "grad_norm": 0.41054481267929077,
      "learning_rate": 0.0001443372878692646,
      "loss": 0.9632,
      "step": 26530
    },
    {
      "epoch": 1.560817466735279,
      "grad_norm": 0.37108486890792847,
      "learning_rate": 0.00014427836266499056,
      "loss": 0.9077,
      "step": 26540
    },
    {
      "epoch": 1.5614055723002278,
      "grad_norm": 0.3641875684261322,
      "learning_rate": 0.00014421943746071653,
      "loss": 1.0104,
      "step": 26550
    },
    {
      "epoch": 1.5619936778651768,
      "grad_norm": 0.33283671736717224,
      "learning_rate": 0.00014416051225644248,
      "loss": 1.0229,
      "step": 26560
    },
    {
      "epoch": 1.5625817834301257,
      "grad_norm": 0.3534127175807953,
      "learning_rate": 0.00014410158705216843,
      "loss": 1.0429,
      "step": 26570
    },
    {
      "epoch": 1.5631698889950747,
      "grad_norm": 0.33327239751815796,
      "learning_rate": 0.0001440426618478944,
      "loss": 1.0385,
      "step": 26580
    },
    {
      "epoch": 1.5637579945600235,
      "grad_norm": 0.34881359338760376,
      "learning_rate": 0.00014398373664362035,
      "loss": 1.0553,
      "step": 26590
    },
    {
      "epoch": 1.5643461001249723,
      "grad_norm": 0.32235851883888245,
      "learning_rate": 0.00014392481143934632,
      "loss": 0.9927,
      "step": 26600
    },
    {
      "epoch": 1.5649342056899214,
      "grad_norm": 0.32894450426101685,
      "learning_rate": 0.00014386588623507227,
      "loss": 1.0556,
      "step": 26610
    },
    {
      "epoch": 1.5655223112548704,
      "grad_norm": 0.3854081928730011,
      "learning_rate": 0.00014380696103079825,
      "loss": 1.0368,
      "step": 26620
    },
    {
      "epoch": 1.5661104168198192,
      "grad_norm": 0.3232531249523163,
      "learning_rate": 0.0001437480358265242,
      "loss": 1.0665,
      "step": 26630
    },
    {
      "epoch": 1.566698522384768,
      "grad_norm": 0.3238396644592285,
      "learning_rate": 0.00014368911062225014,
      "loss": 1.1023,
      "step": 26640
    },
    {
      "epoch": 1.5672866279497168,
      "grad_norm": 0.39844438433647156,
      "learning_rate": 0.0001436301854179761,
      "loss": 1.0499,
      "step": 26650
    },
    {
      "epoch": 1.5678747335146659,
      "grad_norm": 0.381982684135437,
      "learning_rate": 0.00014357126021370206,
      "loss": 1.0022,
      "step": 26660
    },
    {
      "epoch": 1.568462839079615,
      "grad_norm": 0.3673485517501831,
      "learning_rate": 0.000143512335009428,
      "loss": 1.0249,
      "step": 26670
    },
    {
      "epoch": 1.5690509446445637,
      "grad_norm": 0.3778415024280548,
      "learning_rate": 0.00014345340980515396,
      "loss": 1.0304,
      "step": 26680
    },
    {
      "epoch": 1.5696390502095126,
      "grad_norm": 0.3878897726535797,
      "learning_rate": 0.00014339448460087994,
      "loss": 1.0661,
      "step": 26690
    },
    {
      "epoch": 1.5702271557744614,
      "grad_norm": 0.2855464816093445,
      "learning_rate": 0.00014333555939660588,
      "loss": 1.0276,
      "step": 26700
    },
    {
      "epoch": 1.5708152613394104,
      "grad_norm": 0.35826021432876587,
      "learning_rate": 0.00014327663419233186,
      "loss": 1.0001,
      "step": 26710
    },
    {
      "epoch": 1.5714033669043594,
      "grad_norm": 0.3333914577960968,
      "learning_rate": 0.0001432177089880578,
      "loss": 0.9521,
      "step": 26720
    },
    {
      "epoch": 1.5719914724693083,
      "grad_norm": 0.4754515290260315,
      "learning_rate": 0.00014315878378378378,
      "loss": 0.9777,
      "step": 26730
    },
    {
      "epoch": 1.572579578034257,
      "grad_norm": 0.33466973900794983,
      "learning_rate": 0.00014309985857950973,
      "loss": 0.9964,
      "step": 26740
    },
    {
      "epoch": 1.5731676835992061,
      "grad_norm": 0.3589799702167511,
      "learning_rate": 0.0001430409333752357,
      "loss": 1.0307,
      "step": 26750
    },
    {
      "epoch": 1.573755789164155,
      "grad_norm": 0.3992599844932556,
      "learning_rate": 0.00014298200817096165,
      "loss": 0.9439,
      "step": 26760
    },
    {
      "epoch": 1.574343894729104,
      "grad_norm": 0.33954012393951416,
      "learning_rate": 0.0001429230829666876,
      "loss": 1.0819,
      "step": 26770
    },
    {
      "epoch": 1.5749320002940528,
      "grad_norm": 0.3883202373981476,
      "learning_rate": 0.00014286415776241357,
      "loss": 0.9837,
      "step": 26780
    },
    {
      "epoch": 1.5755201058590016,
      "grad_norm": 0.3931784927845001,
      "learning_rate": 0.00014280523255813952,
      "loss": 1.0262,
      "step": 26790
    },
    {
      "epoch": 1.5761082114239506,
      "grad_norm": 0.37719419598579407,
      "learning_rate": 0.0001427463073538655,
      "loss": 0.9974,
      "step": 26800
    },
    {
      "epoch": 1.5766963169888997,
      "grad_norm": 0.3116258680820465,
      "learning_rate": 0.00014268738214959144,
      "loss": 1.1199,
      "step": 26810
    },
    {
      "epoch": 1.5772844225538485,
      "grad_norm": 0.38520896434783936,
      "learning_rate": 0.00014262845694531742,
      "loss": 0.9868,
      "step": 26820
    },
    {
      "epoch": 1.5778725281187973,
      "grad_norm": 0.35979586839675903,
      "learning_rate": 0.00014256953174104336,
      "loss": 0.9735,
      "step": 26830
    },
    {
      "epoch": 1.5784606336837461,
      "grad_norm": 0.37300124764442444,
      "learning_rate": 0.0001425106065367693,
      "loss": 1.0281,
      "step": 26840
    },
    {
      "epoch": 1.5790487392486952,
      "grad_norm": 0.3135339319705963,
      "learning_rate": 0.00014245168133249526,
      "loss": 0.9842,
      "step": 26850
    },
    {
      "epoch": 1.5796368448136442,
      "grad_norm": 0.37511807680130005,
      "learning_rate": 0.00014239275612822123,
      "loss": 0.9205,
      "step": 26860
    },
    {
      "epoch": 1.580224950378593,
      "grad_norm": 0.3430726230144501,
      "learning_rate": 0.00014233383092394718,
      "loss": 0.9948,
      "step": 26870
    },
    {
      "epoch": 1.5808130559435418,
      "grad_norm": 0.3539557456970215,
      "learning_rate": 0.00014227490571967313,
      "loss": 1.0037,
      "step": 26880
    },
    {
      "epoch": 1.5814011615084906,
      "grad_norm": 0.3253609240055084,
      "learning_rate": 0.0001422159805153991,
      "loss": 0.9525,
      "step": 26890
    },
    {
      "epoch": 1.5819892670734397,
      "grad_norm": 0.35060209035873413,
      "learning_rate": 0.00014215705531112505,
      "loss": 1.0675,
      "step": 26900
    },
    {
      "epoch": 1.5825773726383887,
      "grad_norm": 0.3369283974170685,
      "learning_rate": 0.00014209813010685103,
      "loss": 1.0635,
      "step": 26910
    },
    {
      "epoch": 1.5831654782033375,
      "grad_norm": 0.3884388208389282,
      "learning_rate": 0.00014203920490257697,
      "loss": 0.9813,
      "step": 26920
    },
    {
      "epoch": 1.5837535837682863,
      "grad_norm": 0.3585618734359741,
      "learning_rate": 0.00014198027969830295,
      "loss": 0.9707,
      "step": 26930
    },
    {
      "epoch": 1.5843416893332352,
      "grad_norm": 0.3582281768321991,
      "learning_rate": 0.0001419213544940289,
      "loss": 0.9477,
      "step": 26940
    },
    {
      "epoch": 1.5849297948981842,
      "grad_norm": 0.3863653242588043,
      "learning_rate": 0.00014186242928975487,
      "loss": 1.0863,
      "step": 26950
    },
    {
      "epoch": 1.5855179004631332,
      "grad_norm": 0.3699488639831543,
      "learning_rate": 0.00014180350408548082,
      "loss": 1.0802,
      "step": 26960
    },
    {
      "epoch": 1.586106006028082,
      "grad_norm": 0.3444786071777344,
      "learning_rate": 0.00014174457888120677,
      "loss": 1.0931,
      "step": 26970
    },
    {
      "epoch": 1.5866941115930309,
      "grad_norm": 0.3899393081665039,
      "learning_rate": 0.00014168565367693274,
      "loss": 1.0166,
      "step": 26980
    },
    {
      "epoch": 1.58728221715798,
      "grad_norm": 0.4052623212337494,
      "learning_rate": 0.0001416267284726587,
      "loss": 1.0386,
      "step": 26990
    },
    {
      "epoch": 1.5878703227229287,
      "grad_norm": 0.34023383259773254,
      "learning_rate": 0.00014156780326838466,
      "loss": 1.0267,
      "step": 27000
    },
    {
      "epoch": 1.5884584282878778,
      "grad_norm": 0.36752408742904663,
      "learning_rate": 0.0001415088780641106,
      "loss": 1.0522,
      "step": 27010
    },
    {
      "epoch": 1.5890465338528266,
      "grad_norm": 0.32734033465385437,
      "learning_rate": 0.0001414499528598366,
      "loss": 1.0517,
      "step": 27020
    },
    {
      "epoch": 1.5896346394177754,
      "grad_norm": 0.32907184958457947,
      "learning_rate": 0.00014139102765556253,
      "loss": 1.0632,
      "step": 27030
    },
    {
      "epoch": 1.5902227449827244,
      "grad_norm": 0.35871806740760803,
      "learning_rate": 0.00014133210245128848,
      "loss": 1.1203,
      "step": 27040
    },
    {
      "epoch": 1.5908108505476735,
      "grad_norm": 0.41521063446998596,
      "learning_rate": 0.00014127317724701446,
      "loss": 1.0812,
      "step": 27050
    },
    {
      "epoch": 1.5913989561126223,
      "grad_norm": 0.33533984422683716,
      "learning_rate": 0.0001412142520427404,
      "loss": 0.9287,
      "step": 27060
    },
    {
      "epoch": 1.591987061677571,
      "grad_norm": 0.3690626621246338,
      "learning_rate": 0.00014115532683846635,
      "loss": 1.0578,
      "step": 27070
    },
    {
      "epoch": 1.59257516724252,
      "grad_norm": 0.35903459787368774,
      "learning_rate": 0.0001410964016341923,
      "loss": 1.0155,
      "step": 27080
    },
    {
      "epoch": 1.593163272807469,
      "grad_norm": 0.3858313262462616,
      "learning_rate": 0.00014103747642991827,
      "loss": 1.046,
      "step": 27090
    },
    {
      "epoch": 1.593751378372418,
      "grad_norm": 0.37016505002975464,
      "learning_rate": 0.00014097855122564422,
      "loss": 0.9969,
      "step": 27100
    },
    {
      "epoch": 1.5943394839373668,
      "grad_norm": 0.36094287037849426,
      "learning_rate": 0.0001409196260213702,
      "loss": 1.0173,
      "step": 27110
    },
    {
      "epoch": 1.5949275895023156,
      "grad_norm": 0.331570565700531,
      "learning_rate": 0.00014086070081709614,
      "loss": 0.9939,
      "step": 27120
    },
    {
      "epoch": 1.5955156950672644,
      "grad_norm": 0.33010226488113403,
      "learning_rate": 0.00014080177561282212,
      "loss": 0.9661,
      "step": 27130
    },
    {
      "epoch": 1.5961038006322135,
      "grad_norm": 0.38211119174957275,
      "learning_rate": 0.00014074285040854807,
      "loss": 0.9874,
      "step": 27140
    },
    {
      "epoch": 1.5966919061971625,
      "grad_norm": 0.38249966502189636,
      "learning_rate": 0.00014068392520427404,
      "loss": 1.0082,
      "step": 27150
    },
    {
      "epoch": 1.5972800117621113,
      "grad_norm": 0.32864439487457275,
      "learning_rate": 0.000140625,
      "loss": 1.015,
      "step": 27160
    },
    {
      "epoch": 1.5978681173270601,
      "grad_norm": 0.32656607031822205,
      "learning_rate": 0.00014056607479572594,
      "loss": 1.1337,
      "step": 27170
    },
    {
      "epoch": 1.5984562228920092,
      "grad_norm": 0.3906714618206024,
      "learning_rate": 0.0001405071495914519,
      "loss": 1.0176,
      "step": 27180
    },
    {
      "epoch": 1.599044328456958,
      "grad_norm": 0.3374478220939636,
      "learning_rate": 0.00014044822438717786,
      "loss": 0.9554,
      "step": 27190
    },
    {
      "epoch": 1.599632434021907,
      "grad_norm": 0.3707432150840759,
      "learning_rate": 0.00014038929918290383,
      "loss": 0.9763,
      "step": 27200
    },
    {
      "epoch": 1.6002205395868558,
      "grad_norm": 0.33600085973739624,
      "learning_rate": 0.00014033037397862978,
      "loss": 1.0126,
      "step": 27210
    },
    {
      "epoch": 1.6008086451518047,
      "grad_norm": 0.5152122378349304,
      "learning_rate": 0.00014027144877435576,
      "loss": 1.0675,
      "step": 27220
    },
    {
      "epoch": 1.6013967507167537,
      "grad_norm": 0.3409930467605591,
      "learning_rate": 0.0001402125235700817,
      "loss": 0.9306,
      "step": 27230
    },
    {
      "epoch": 1.6019848562817027,
      "grad_norm": 0.3557894229888916,
      "learning_rate": 0.00014015359836580765,
      "loss": 1.0767,
      "step": 27240
    },
    {
      "epoch": 1.6025729618466515,
      "grad_norm": 0.36824852228164673,
      "learning_rate": 0.00014009467316153363,
      "loss": 1.1338,
      "step": 27250
    },
    {
      "epoch": 1.6031610674116004,
      "grad_norm": 0.3951340317726135,
      "learning_rate": 0.00014003574795725957,
      "loss": 1.0533,
      "step": 27260
    },
    {
      "epoch": 1.6037491729765492,
      "grad_norm": 0.33153003454208374,
      "learning_rate": 0.00013997682275298552,
      "loss": 1.0078,
      "step": 27270
    },
    {
      "epoch": 1.6043372785414982,
      "grad_norm": 0.3718516528606415,
      "learning_rate": 0.00013991789754871147,
      "loss": 0.9955,
      "step": 27280
    },
    {
      "epoch": 1.6049253841064473,
      "grad_norm": 0.37194395065307617,
      "learning_rate": 0.00013985897234443744,
      "loss": 1.0044,
      "step": 27290
    },
    {
      "epoch": 1.605513489671396,
      "grad_norm": 0.3669256865978241,
      "learning_rate": 0.0001398000471401634,
      "loss": 1.1615,
      "step": 27300
    },
    {
      "epoch": 1.6061015952363449,
      "grad_norm": 0.3578917682170868,
      "learning_rate": 0.00013974112193588937,
      "loss": 0.9584,
      "step": 27310
    },
    {
      "epoch": 1.6066897008012937,
      "grad_norm": 0.34306448698043823,
      "learning_rate": 0.00013968219673161531,
      "loss": 0.9868,
      "step": 27320
    },
    {
      "epoch": 1.6072778063662427,
      "grad_norm": 0.3805358409881592,
      "learning_rate": 0.0001396232715273413,
      "loss": 1.0436,
      "step": 27330
    },
    {
      "epoch": 1.6078659119311918,
      "grad_norm": 0.3143596351146698,
      "learning_rate": 0.00013956434632306724,
      "loss": 1.0879,
      "step": 27340
    },
    {
      "epoch": 1.6084540174961406,
      "grad_norm": 0.3562489151954651,
      "learning_rate": 0.0001395054211187932,
      "loss": 0.9995,
      "step": 27350
    },
    {
      "epoch": 1.6090421230610894,
      "grad_norm": 0.3567286729812622,
      "learning_rate": 0.00013944649591451916,
      "loss": 1.0748,
      "step": 27360
    },
    {
      "epoch": 1.6096302286260382,
      "grad_norm": 0.4124416410923004,
      "learning_rate": 0.0001393875707102451,
      "loss": 0.9649,
      "step": 27370
    },
    {
      "epoch": 1.6102183341909873,
      "grad_norm": 0.3626733422279358,
      "learning_rate": 0.00013932864550597108,
      "loss": 1.1036,
      "step": 27380
    },
    {
      "epoch": 1.6108064397559363,
      "grad_norm": 0.3191625773906708,
      "learning_rate": 0.00013926972030169703,
      "loss": 1.0815,
      "step": 27390
    },
    {
      "epoch": 1.611394545320885,
      "grad_norm": 0.32776182889938354,
      "learning_rate": 0.000139210795097423,
      "loss": 0.9685,
      "step": 27400
    },
    {
      "epoch": 1.611982650885834,
      "grad_norm": 0.37238648533821106,
      "learning_rate": 0.00013915186989314895,
      "loss": 1.0011,
      "step": 27410
    },
    {
      "epoch": 1.612570756450783,
      "grad_norm": 0.336819589138031,
      "learning_rate": 0.00013909294468887493,
      "loss": 0.9828,
      "step": 27420
    },
    {
      "epoch": 1.6131588620157318,
      "grad_norm": 0.31935346126556396,
      "learning_rate": 0.00013903401948460087,
      "loss": 0.9421,
      "step": 27430
    },
    {
      "epoch": 1.6137469675806808,
      "grad_norm": 0.3316940665245056,
      "learning_rate": 0.00013897509428032682,
      "loss": 0.924,
      "step": 27440
    },
    {
      "epoch": 1.6143350731456296,
      "grad_norm": 0.3899064064025879,
      "learning_rate": 0.0001389161690760528,
      "loss": 0.9689,
      "step": 27450
    },
    {
      "epoch": 1.6149231787105784,
      "grad_norm": 0.3392362594604492,
      "learning_rate": 0.00013885724387177874,
      "loss": 1.0402,
      "step": 27460
    },
    {
      "epoch": 1.6155112842755275,
      "grad_norm": 0.3107779324054718,
      "learning_rate": 0.00013879831866750472,
      "loss": 1.0106,
      "step": 27470
    },
    {
      "epoch": 1.6160993898404765,
      "grad_norm": 0.3723389506340027,
      "learning_rate": 0.00013873939346323064,
      "loss": 0.9948,
      "step": 27480
    },
    {
      "epoch": 1.6166874954054253,
      "grad_norm": 0.35577163100242615,
      "learning_rate": 0.00013868046825895661,
      "loss": 1.0657,
      "step": 27490
    },
    {
      "epoch": 1.6172756009703741,
      "grad_norm": 0.36244598031044006,
      "learning_rate": 0.00013862154305468256,
      "loss": 1.0567,
      "step": 27500
    },
    {
      "epoch": 1.617863706535323,
      "grad_norm": 0.3403986990451813,
      "learning_rate": 0.00013856261785040854,
      "loss": 0.9988,
      "step": 27510
    },
    {
      "epoch": 1.618451812100272,
      "grad_norm": 0.37427982687950134,
      "learning_rate": 0.00013850369264613448,
      "loss": 1.0295,
      "step": 27520
    },
    {
      "epoch": 1.619039917665221,
      "grad_norm": 0.3404519557952881,
      "learning_rate": 0.00013844476744186046,
      "loss": 1.0609,
      "step": 27530
    },
    {
      "epoch": 1.6196280232301699,
      "grad_norm": 0.3823225498199463,
      "learning_rate": 0.0001383858422375864,
      "loss": 1.1162,
      "step": 27540
    },
    {
      "epoch": 1.6202161287951187,
      "grad_norm": 0.40552008152008057,
      "learning_rate": 0.00013832691703331238,
      "loss": 0.9166,
      "step": 27550
    },
    {
      "epoch": 1.6208042343600675,
      "grad_norm": 0.3393058478832245,
      "learning_rate": 0.00013826799182903833,
      "loss": 1.0977,
      "step": 27560
    },
    {
      "epoch": 1.6213923399250165,
      "grad_norm": 0.34788283705711365,
      "learning_rate": 0.00013820906662476428,
      "loss": 0.9551,
      "step": 27570
    },
    {
      "epoch": 1.6219804454899656,
      "grad_norm": 0.4214043915271759,
      "learning_rate": 0.00013815014142049025,
      "loss": 1.0508,
      "step": 27580
    },
    {
      "epoch": 1.6225685510549144,
      "grad_norm": 0.35335561633110046,
      "learning_rate": 0.0001380912162162162,
      "loss": 1.0547,
      "step": 27590
    },
    {
      "epoch": 1.6231566566198632,
      "grad_norm": 0.3311766982078552,
      "learning_rate": 0.00013803229101194217,
      "loss": 1.1187,
      "step": 27600
    },
    {
      "epoch": 1.6237447621848122,
      "grad_norm": 0.3810930550098419,
      "learning_rate": 0.00013797336580766812,
      "loss": 0.9851,
      "step": 27610
    },
    {
      "epoch": 1.624332867749761,
      "grad_norm": 0.3262854516506195,
      "learning_rate": 0.0001379144406033941,
      "loss": 0.9643,
      "step": 27620
    },
    {
      "epoch": 1.62492097331471,
      "grad_norm": 0.32952195405960083,
      "learning_rate": 0.00013785551539912004,
      "loss": 1.0208,
      "step": 27630
    },
    {
      "epoch": 1.625509078879659,
      "grad_norm": 0.36664772033691406,
      "learning_rate": 0.000137796590194846,
      "loss": 0.9889,
      "step": 27640
    },
    {
      "epoch": 1.6260971844446077,
      "grad_norm": 0.31815382838249207,
      "learning_rate": 0.00013773766499057197,
      "loss": 1.0937,
      "step": 27650
    },
    {
      "epoch": 1.6266852900095568,
      "grad_norm": 0.3459019362926483,
      "learning_rate": 0.00013767873978629791,
      "loss": 1.069,
      "step": 27660
    },
    {
      "epoch": 1.6272733955745058,
      "grad_norm": 0.36854204535484314,
      "learning_rate": 0.0001376198145820239,
      "loss": 1.0798,
      "step": 27670
    },
    {
      "epoch": 1.6278615011394546,
      "grad_norm": 0.37339603900909424,
      "learning_rate": 0.00013756088937774984,
      "loss": 1.0888,
      "step": 27680
    },
    {
      "epoch": 1.6284496067044034,
      "grad_norm": 0.38217392563819885,
      "learning_rate": 0.00013750196417347578,
      "loss": 1.0415,
      "step": 27690
    },
    {
      "epoch": 1.6290377122693522,
      "grad_norm": 0.3343300223350525,
      "learning_rate": 0.00013744303896920173,
      "loss": 1.0562,
      "step": 27700
    },
    {
      "epoch": 1.6296258178343013,
      "grad_norm": 0.41220396757125854,
      "learning_rate": 0.0001373841137649277,
      "loss": 1.0147,
      "step": 27710
    },
    {
      "epoch": 1.6302139233992503,
      "grad_norm": 0.38370320200920105,
      "learning_rate": 0.00013732518856065365,
      "loss": 1.0423,
      "step": 27720
    },
    {
      "epoch": 1.6308020289641991,
      "grad_norm": 0.3503250181674957,
      "learning_rate": 0.00013726626335637963,
      "loss": 1.066,
      "step": 27730
    },
    {
      "epoch": 1.631390134529148,
      "grad_norm": 0.3449796140193939,
      "learning_rate": 0.00013720733815210558,
      "loss": 1.0393,
      "step": 27740
    },
    {
      "epoch": 1.6319782400940968,
      "grad_norm": 0.34272804856300354,
      "learning_rate": 0.00013714841294783152,
      "loss": 1.0524,
      "step": 27750
    },
    {
      "epoch": 1.6325663456590458,
      "grad_norm": 0.3458411991596222,
      "learning_rate": 0.0001370894877435575,
      "loss": 1.0866,
      "step": 27760
    },
    {
      "epoch": 1.6331544512239948,
      "grad_norm": 0.3696419596672058,
      "learning_rate": 0.00013703056253928345,
      "loss": 0.9878,
      "step": 27770
    },
    {
      "epoch": 1.6337425567889436,
      "grad_norm": 0.3513305187225342,
      "learning_rate": 0.00013697163733500942,
      "loss": 0.9919,
      "step": 27780
    },
    {
      "epoch": 1.6343306623538925,
      "grad_norm": 0.33817604184150696,
      "learning_rate": 0.00013691271213073537,
      "loss": 1.0542,
      "step": 27790
    },
    {
      "epoch": 1.6349187679188413,
      "grad_norm": 0.3412715196609497,
      "learning_rate": 0.00013685378692646134,
      "loss": 1.0104,
      "step": 27800
    },
    {
      "epoch": 1.6355068734837903,
      "grad_norm": 0.42966681718826294,
      "learning_rate": 0.0001367948617221873,
      "loss": 0.9739,
      "step": 27810
    },
    {
      "epoch": 1.6360949790487394,
      "grad_norm": 0.35621246695518494,
      "learning_rate": 0.00013673593651791327,
      "loss": 0.9655,
      "step": 27820
    },
    {
      "epoch": 1.6366830846136882,
      "grad_norm": 0.42224445939064026,
      "learning_rate": 0.0001366770113136392,
      "loss": 0.9039,
      "step": 27830
    },
    {
      "epoch": 1.637271190178637,
      "grad_norm": 0.37786224484443665,
      "learning_rate": 0.00013661808610936516,
      "loss": 1.038,
      "step": 27840
    },
    {
      "epoch": 1.637859295743586,
      "grad_norm": 0.34084898233413696,
      "learning_rate": 0.00013655916090509114,
      "loss": 0.9815,
      "step": 27850
    },
    {
      "epoch": 1.6384474013085348,
      "grad_norm": 0.4028019905090332,
      "learning_rate": 0.00013650023570081708,
      "loss": 0.9885,
      "step": 27860
    },
    {
      "epoch": 1.6390355068734839,
      "grad_norm": 0.4674728512763977,
      "learning_rate": 0.00013644131049654306,
      "loss": 0.955,
      "step": 27870
    },
    {
      "epoch": 1.6396236124384327,
      "grad_norm": 0.3383948802947998,
      "learning_rate": 0.000136382385292269,
      "loss": 1.0038,
      "step": 27880
    },
    {
      "epoch": 1.6402117180033815,
      "grad_norm": 0.352094441652298,
      "learning_rate": 0.00013632346008799498,
      "loss": 1.0043,
      "step": 27890
    },
    {
      "epoch": 1.6407998235683305,
      "grad_norm": 0.4330936074256897,
      "learning_rate": 0.0001362645348837209,
      "loss": 0.9943,
      "step": 27900
    },
    {
      "epoch": 1.6413879291332796,
      "grad_norm": 0.38723769783973694,
      "learning_rate": 0.00013620560967944688,
      "loss": 1.0115,
      "step": 27910
    },
    {
      "epoch": 1.6419760346982284,
      "grad_norm": 0.35655367374420166,
      "learning_rate": 0.00013614668447517282,
      "loss": 1.0679,
      "step": 27920
    },
    {
      "epoch": 1.6425641402631772,
      "grad_norm": 0.3216688930988312,
      "learning_rate": 0.0001360877592708988,
      "loss": 0.9218,
      "step": 27930
    },
    {
      "epoch": 1.643152245828126,
      "grad_norm": 0.3571297526359558,
      "learning_rate": 0.00013602883406662475,
      "loss": 0.9222,
      "step": 27940
    },
    {
      "epoch": 1.643740351393075,
      "grad_norm": 0.32340526580810547,
      "learning_rate": 0.0001359699088623507,
      "loss": 0.9682,
      "step": 27950
    },
    {
      "epoch": 1.644328456958024,
      "grad_norm": 0.408610463142395,
      "learning_rate": 0.00013591098365807667,
      "loss": 0.9755,
      "step": 27960
    },
    {
      "epoch": 1.644916562522973,
      "grad_norm": 0.3126434087753296,
      "learning_rate": 0.00013585205845380262,
      "loss": 1.0677,
      "step": 27970
    },
    {
      "epoch": 1.6455046680879217,
      "grad_norm": 0.368192195892334,
      "learning_rate": 0.0001357931332495286,
      "loss": 0.9506,
      "step": 27980
    },
    {
      "epoch": 1.6460927736528705,
      "grad_norm": 0.31199002265930176,
      "learning_rate": 0.00013573420804525454,
      "loss": 0.9904,
      "step": 27990
    },
    {
      "epoch": 1.6466808792178196,
      "grad_norm": 0.3354726731777191,
      "learning_rate": 0.0001356752828409805,
      "loss": 0.9578,
      "step": 28000
    },
    {
      "epoch": 1.6472689847827686,
      "grad_norm": 0.3790351450443268,
      "learning_rate": 0.00013561635763670646,
      "loss": 1.0601,
      "step": 28010
    },
    {
      "epoch": 1.6478570903477174,
      "grad_norm": 0.45873144268989563,
      "learning_rate": 0.00013555743243243244,
      "loss": 1.0136,
      "step": 28020
    },
    {
      "epoch": 1.6484451959126663,
      "grad_norm": 0.40227705240249634,
      "learning_rate": 0.00013549850722815838,
      "loss": 0.9764,
      "step": 28030
    },
    {
      "epoch": 1.649033301477615,
      "grad_norm": 0.37154045701026917,
      "learning_rate": 0.00013543958202388433,
      "loss": 1.0991,
      "step": 28040
    },
    {
      "epoch": 1.649621407042564,
      "grad_norm": 0.3826456665992737,
      "learning_rate": 0.0001353865493400377,
      "loss": 0.94,
      "step": 28050
    },
    {
      "epoch": 1.6502095126075131,
      "grad_norm": 0.33968281745910645,
      "learning_rate": 0.00013532762413576366,
      "loss": 1.0796,
      "step": 28060
    },
    {
      "epoch": 1.650797618172462,
      "grad_norm": 0.3602653741836548,
      "learning_rate": 0.0001352686989314896,
      "loss": 0.9676,
      "step": 28070
    },
    {
      "epoch": 1.6513857237374108,
      "grad_norm": 0.36366981267929077,
      "learning_rate": 0.00013520977372721558,
      "loss": 0.9724,
      "step": 28080
    },
    {
      "epoch": 1.6519738293023598,
      "grad_norm": 0.35329458117485046,
      "learning_rate": 0.00013515084852294153,
      "loss": 0.9592,
      "step": 28090
    },
    {
      "epoch": 1.6525619348673088,
      "grad_norm": 0.31909921765327454,
      "learning_rate": 0.0001350919233186675,
      "loss": 1.0613,
      "step": 28100
    },
    {
      "epoch": 1.6531500404322577,
      "grad_norm": 0.34015774726867676,
      "learning_rate": 0.00013503299811439345,
      "loss": 0.9259,
      "step": 28110
    },
    {
      "epoch": 1.6537381459972065,
      "grad_norm": 0.34607774019241333,
      "learning_rate": 0.0001349740729101194,
      "loss": 0.9371,
      "step": 28120
    },
    {
      "epoch": 1.6543262515621553,
      "grad_norm": 0.3148321807384491,
      "learning_rate": 0.00013491514770584538,
      "loss": 1.0079,
      "step": 28130
    },
    {
      "epoch": 1.6549143571271043,
      "grad_norm": 0.3966764807701111,
      "learning_rate": 0.00013485622250157132,
      "loss": 1.0176,
      "step": 28140
    },
    {
      "epoch": 1.6555024626920534,
      "grad_norm": 0.3546880781650543,
      "learning_rate": 0.0001347972972972973,
      "loss": 0.948,
      "step": 28150
    },
    {
      "epoch": 1.6560905682570022,
      "grad_norm": 0.350326806306839,
      "learning_rate": 0.00013473837209302325,
      "loss": 0.9959,
      "step": 28160
    },
    {
      "epoch": 1.656678673821951,
      "grad_norm": 0.3269682824611664,
      "learning_rate": 0.00013467944688874922,
      "loss": 0.9717,
      "step": 28170
    },
    {
      "epoch": 1.6572667793868998,
      "grad_norm": 0.3111501932144165,
      "learning_rate": 0.00013462052168447517,
      "loss": 1.0085,
      "step": 28180
    },
    {
      "epoch": 1.6578548849518489,
      "grad_norm": 0.3288746774196625,
      "learning_rate": 0.00013456159648020112,
      "loss": 0.999,
      "step": 28190
    },
    {
      "epoch": 1.6584429905167979,
      "grad_norm": 0.3873496949672699,
      "learning_rate": 0.00013450267127592706,
      "loss": 1.0367,
      "step": 28200
    },
    {
      "epoch": 1.6590310960817467,
      "grad_norm": 0.33733266592025757,
      "learning_rate": 0.00013444374607165304,
      "loss": 0.9807,
      "step": 28210
    },
    {
      "epoch": 1.6596192016466955,
      "grad_norm": 0.40355128049850464,
      "learning_rate": 0.00013438482086737899,
      "loss": 1.0851,
      "step": 28220
    },
    {
      "epoch": 1.6602073072116443,
      "grad_norm": 0.367983341217041,
      "learning_rate": 0.00013432589566310496,
      "loss": 0.9508,
      "step": 28230
    },
    {
      "epoch": 1.6607954127765934,
      "grad_norm": 0.3230224847793579,
      "learning_rate": 0.0001342669704588309,
      "loss": 0.9824,
      "step": 28240
    },
    {
      "epoch": 1.6613835183415424,
      "grad_norm": 0.3741146922111511,
      "learning_rate": 0.00013420804525455686,
      "loss": 1.0438,
      "step": 28250
    },
    {
      "epoch": 1.6619716239064912,
      "grad_norm": 0.3373245298862457,
      "learning_rate": 0.00013414912005028283,
      "loss": 0.9982,
      "step": 28260
    },
    {
      "epoch": 1.66255972947144,
      "grad_norm": 0.3606819212436676,
      "learning_rate": 0.00013409019484600878,
      "loss": 1.0041,
      "step": 28270
    },
    {
      "epoch": 1.663147835036389,
      "grad_norm": 0.36665597558021545,
      "learning_rate": 0.00013403126964173475,
      "loss": 0.9736,
      "step": 28280
    },
    {
      "epoch": 1.663735940601338,
      "grad_norm": 0.35689038038253784,
      "learning_rate": 0.0001339723444374607,
      "loss": 1.0314,
      "step": 28290
    },
    {
      "epoch": 1.664324046166287,
      "grad_norm": 0.3665476441383362,
      "learning_rate": 0.00013391341923318668,
      "loss": 0.9952,
      "step": 28300
    },
    {
      "epoch": 1.6649121517312357,
      "grad_norm": 0.36844485998153687,
      "learning_rate": 0.00013385449402891262,
      "loss": 0.9888,
      "step": 28310
    },
    {
      "epoch": 1.6655002572961846,
      "grad_norm": 0.36045119166374207,
      "learning_rate": 0.00013379556882463857,
      "loss": 0.94,
      "step": 28320
    },
    {
      "epoch": 1.6660883628611336,
      "grad_norm": 0.3329153060913086,
      "learning_rate": 0.00013373664362036455,
      "loss": 0.9782,
      "step": 28330
    },
    {
      "epoch": 1.6666764684260826,
      "grad_norm": 0.37294232845306396,
      "learning_rate": 0.0001336777184160905,
      "loss": 1.0176,
      "step": 28340
    },
    {
      "epoch": 1.6672645739910315,
      "grad_norm": 0.3825145363807678,
      "learning_rate": 0.00013361879321181647,
      "loss": 0.9911,
      "step": 28350
    },
    {
      "epoch": 1.6678526795559803,
      "grad_norm": 0.3986307680606842,
      "learning_rate": 0.00013355986800754242,
      "loss": 0.9561,
      "step": 28360
    },
    {
      "epoch": 1.668440785120929,
      "grad_norm": 0.3982667028903961,
      "learning_rate": 0.0001335009428032684,
      "loss": 1.0696,
      "step": 28370
    },
    {
      "epoch": 1.6690288906858781,
      "grad_norm": 0.422095388174057,
      "learning_rate": 0.00013344201759899434,
      "loss": 1.0533,
      "step": 28380
    },
    {
      "epoch": 1.6696169962508272,
      "grad_norm": 0.36432701349258423,
      "learning_rate": 0.00013338309239472029,
      "loss": 1.0538,
      "step": 28390
    },
    {
      "epoch": 1.670205101815776,
      "grad_norm": 0.37420859932899475,
      "learning_rate": 0.00013332416719044623,
      "loss": 0.966,
      "step": 28400
    },
    {
      "epoch": 1.6707932073807248,
      "grad_norm": 0.3792787194252014,
      "learning_rate": 0.0001332652419861722,
      "loss": 1.0574,
      "step": 28410
    },
    {
      "epoch": 1.6713813129456736,
      "grad_norm": 0.3757590353488922,
      "learning_rate": 0.00013320631678189816,
      "loss": 1.0246,
      "step": 28420
    },
    {
      "epoch": 1.6719694185106226,
      "grad_norm": 0.36873507499694824,
      "learning_rate": 0.00013314739157762413,
      "loss": 1.069,
      "step": 28430
    },
    {
      "epoch": 1.6725575240755717,
      "grad_norm": 0.39823171496391296,
      "learning_rate": 0.00013308846637335008,
      "loss": 1.0502,
      "step": 28440
    },
    {
      "epoch": 1.6731456296405205,
      "grad_norm": 0.38574716448783875,
      "learning_rate": 0.00013302954116907603,
      "loss": 1.0453,
      "step": 28450
    },
    {
      "epoch": 1.6737337352054693,
      "grad_norm": 0.37436744570732117,
      "learning_rate": 0.000132970615964802,
      "loss": 0.9745,
      "step": 28460
    },
    {
      "epoch": 1.6743218407704181,
      "grad_norm": 0.3782295882701874,
      "learning_rate": 0.00013291169076052795,
      "loss": 1.0403,
      "step": 28470
    },
    {
      "epoch": 1.6749099463353672,
      "grad_norm": 0.3268773555755615,
      "learning_rate": 0.00013285276555625392,
      "loss": 1.1216,
      "step": 28480
    },
    {
      "epoch": 1.6754980519003162,
      "grad_norm": 0.33834269642829895,
      "learning_rate": 0.00013279384035197987,
      "loss": 0.9816,
      "step": 28490
    },
    {
      "epoch": 1.676086157465265,
      "grad_norm": 0.3773658573627472,
      "learning_rate": 0.00013273491514770585,
      "loss": 0.9883,
      "step": 28500
    },
    {
      "epoch": 1.6766742630302138,
      "grad_norm": 0.37614160776138306,
      "learning_rate": 0.0001326759899434318,
      "loss": 1.0948,
      "step": 28510
    },
    {
      "epoch": 1.6772623685951629,
      "grad_norm": 0.40971216559410095,
      "learning_rate": 0.00013261706473915774,
      "loss": 1.06,
      "step": 28520
    },
    {
      "epoch": 1.677850474160112,
      "grad_norm": 0.3149292469024658,
      "learning_rate": 0.00013255813953488372,
      "loss": 0.942,
      "step": 28530
    },
    {
      "epoch": 1.6784385797250607,
      "grad_norm": 0.36829668283462524,
      "learning_rate": 0.00013249921433060966,
      "loss": 0.9521,
      "step": 28540
    },
    {
      "epoch": 1.6790266852900095,
      "grad_norm": 0.35780367255210876,
      "learning_rate": 0.00013244028912633564,
      "loss": 1.0424,
      "step": 28550
    },
    {
      "epoch": 1.6796147908549584,
      "grad_norm": 0.3871500790119171,
      "learning_rate": 0.00013238136392206159,
      "loss": 1.0044,
      "step": 28560
    },
    {
      "epoch": 1.6802028964199074,
      "grad_norm": 0.3635707199573517,
      "learning_rate": 0.00013232243871778756,
      "loss": 0.96,
      "step": 28570
    },
    {
      "epoch": 1.6807910019848564,
      "grad_norm": 0.3702815771102905,
      "learning_rate": 0.0001322635135135135,
      "loss": 1.08,
      "step": 28580
    },
    {
      "epoch": 1.6813791075498052,
      "grad_norm": 0.32057011127471924,
      "learning_rate": 0.00013220458830923946,
      "loss": 1.0276,
      "step": 28590
    },
    {
      "epoch": 1.681967213114754,
      "grad_norm": 0.3371724486351013,
      "learning_rate": 0.00013214566310496543,
      "loss": 1.0059,
      "step": 28600
    },
    {
      "epoch": 1.6825553186797029,
      "grad_norm": 0.3181372582912445,
      "learning_rate": 0.00013208673790069138,
      "loss": 0.9671,
      "step": 28610
    },
    {
      "epoch": 1.683143424244652,
      "grad_norm": 0.3460101783275604,
      "learning_rate": 0.00013202781269641733,
      "loss": 1.0871,
      "step": 28620
    },
    {
      "epoch": 1.683731529809601,
      "grad_norm": 0.32320868968963623,
      "learning_rate": 0.0001319688874921433,
      "loss": 1.082,
      "step": 28630
    },
    {
      "epoch": 1.6843196353745498,
      "grad_norm": 0.3600608706474304,
      "learning_rate": 0.00013190996228786925,
      "loss": 0.9911,
      "step": 28640
    },
    {
      "epoch": 1.6849077409394986,
      "grad_norm": 0.3568195700645447,
      "learning_rate": 0.0001318510370835952,
      "loss": 1.087,
      "step": 28650
    },
    {
      "epoch": 1.6854958465044474,
      "grad_norm": 0.3616385757923126,
      "learning_rate": 0.00013179211187932117,
      "loss": 0.966,
      "step": 28660
    },
    {
      "epoch": 1.6860839520693964,
      "grad_norm": 0.3682524859905243,
      "learning_rate": 0.00013173318667504712,
      "loss": 0.9937,
      "step": 28670
    },
    {
      "epoch": 1.6866720576343455,
      "grad_norm": 0.3693683445453644,
      "learning_rate": 0.0001316742614707731,
      "loss": 0.9903,
      "step": 28680
    },
    {
      "epoch": 1.6872601631992943,
      "grad_norm": 0.3731309771537781,
      "learning_rate": 0.00013161533626649904,
      "loss": 0.9849,
      "step": 28690
    },
    {
      "epoch": 1.687848268764243,
      "grad_norm": 0.384964257478714,
      "learning_rate": 0.00013155641106222502,
      "loss": 1.0403,
      "step": 28700
    },
    {
      "epoch": 1.6884363743291921,
      "grad_norm": 0.34652000665664673,
      "learning_rate": 0.00013149748585795096,
      "loss": 1.0176,
      "step": 28710
    },
    {
      "epoch": 1.689024479894141,
      "grad_norm": 0.31585168838500977,
      "learning_rate": 0.0001314385606536769,
      "loss": 0.9893,
      "step": 28720
    },
    {
      "epoch": 1.68961258545909,
      "grad_norm": 0.3120078146457672,
      "learning_rate": 0.00013137963544940289,
      "loss": 0.988,
      "step": 28730
    },
    {
      "epoch": 1.6902006910240388,
      "grad_norm": 0.2903827428817749,
      "learning_rate": 0.00013132071024512883,
      "loss": 0.9743,
      "step": 28740
    },
    {
      "epoch": 1.6907887965889876,
      "grad_norm": 0.36687347292900085,
      "learning_rate": 0.0001312617850408548,
      "loss": 0.9657,
      "step": 28750
    },
    {
      "epoch": 1.6913769021539367,
      "grad_norm": 0.34227436780929565,
      "learning_rate": 0.00013120285983658076,
      "loss": 1.0652,
      "step": 28760
    },
    {
      "epoch": 1.6919650077188857,
      "grad_norm": 0.43220198154449463,
      "learning_rate": 0.00013114393463230673,
      "loss": 1.1205,
      "step": 28770
    },
    {
      "epoch": 1.6925531132838345,
      "grad_norm": 0.3672402799129486,
      "learning_rate": 0.00013108500942803268,
      "loss": 0.9739,
      "step": 28780
    },
    {
      "epoch": 1.6931412188487833,
      "grad_norm": 0.31235823035240173,
      "learning_rate": 0.00013102608422375863,
      "loss": 0.9524,
      "step": 28790
    },
    {
      "epoch": 1.6937293244137321,
      "grad_norm": 0.35162413120269775,
      "learning_rate": 0.0001309671590194846,
      "loss": 1.1547,
      "step": 28800
    },
    {
      "epoch": 1.6943174299786812,
      "grad_norm": 0.3745265305042267,
      "learning_rate": 0.00013090823381521055,
      "loss": 1.0675,
      "step": 28810
    },
    {
      "epoch": 1.6949055355436302,
      "grad_norm": 0.3291720151901245,
      "learning_rate": 0.0001308493086109365,
      "loss": 1.1073,
      "step": 28820
    },
    {
      "epoch": 1.695493641108579,
      "grad_norm": 0.4636184573173523,
      "learning_rate": 0.00013079038340666247,
      "loss": 0.9929,
      "step": 28830
    },
    {
      "epoch": 1.6960817466735278,
      "grad_norm": 0.3501957654953003,
      "learning_rate": 0.00013073145820238842,
      "loss": 0.9884,
      "step": 28840
    },
    {
      "epoch": 1.6966698522384767,
      "grad_norm": 0.3324242830276489,
      "learning_rate": 0.00013067253299811437,
      "loss": 1.0303,
      "step": 28850
    },
    {
      "epoch": 1.6972579578034257,
      "grad_norm": 0.3604033589363098,
      "learning_rate": 0.00013061360779384034,
      "loss": 1.123,
      "step": 28860
    },
    {
      "epoch": 1.6978460633683747,
      "grad_norm": 0.3575410544872284,
      "learning_rate": 0.0001305546825895663,
      "loss": 0.9907,
      "step": 28870
    },
    {
      "epoch": 1.6984341689333236,
      "grad_norm": 0.35390231013298035,
      "learning_rate": 0.00013049575738529226,
      "loss": 0.9771,
      "step": 28880
    },
    {
      "epoch": 1.6990222744982724,
      "grad_norm": 0.33619046211242676,
      "learning_rate": 0.0001304368321810182,
      "loss": 1.0459,
      "step": 28890
    },
    {
      "epoch": 1.6996103800632212,
      "grad_norm": 0.3612377643585205,
      "learning_rate": 0.00013037790697674419,
      "loss": 1.0287,
      "step": 28900
    },
    {
      "epoch": 1.7001984856281702,
      "grad_norm": 0.3483540713787079,
      "learning_rate": 0.00013031898177247013,
      "loss": 1.019,
      "step": 28910
    },
    {
      "epoch": 1.7007865911931193,
      "grad_norm": 0.3672637343406677,
      "learning_rate": 0.00013026005656819608,
      "loss": 1.0977,
      "step": 28920
    },
    {
      "epoch": 1.701374696758068,
      "grad_norm": 0.3077772855758667,
      "learning_rate": 0.00013020113136392206,
      "loss": 1.0499,
      "step": 28930
    },
    {
      "epoch": 1.7019628023230169,
      "grad_norm": 0.3320320248603821,
      "learning_rate": 0.000130142206159648,
      "loss": 1.022,
      "step": 28940
    },
    {
      "epoch": 1.702550907887966,
      "grad_norm": 0.37496164441108704,
      "learning_rate": 0.00013008328095537398,
      "loss": 0.9593,
      "step": 28950
    },
    {
      "epoch": 1.703139013452915,
      "grad_norm": 0.31016603112220764,
      "learning_rate": 0.00013002435575109993,
      "loss": 0.9087,
      "step": 28960
    },
    {
      "epoch": 1.7037271190178638,
      "grad_norm": 0.391732394695282,
      "learning_rate": 0.0001299654305468259,
      "loss": 1.0132,
      "step": 28970
    },
    {
      "epoch": 1.7043152245828126,
      "grad_norm": 0.34027719497680664,
      "learning_rate": 0.00012990650534255185,
      "loss": 1.0226,
      "step": 28980
    },
    {
      "epoch": 1.7049033301477614,
      "grad_norm": 0.3349382281303406,
      "learning_rate": 0.0001298475801382778,
      "loss": 1.0693,
      "step": 28990
    },
    {
      "epoch": 1.7054914357127104,
      "grad_norm": 0.37027621269226074,
      "learning_rate": 0.00012978865493400377,
      "loss": 1.0009,
      "step": 29000
    },
    {
      "epoch": 1.7060795412776595,
      "grad_norm": 0.3647538423538208,
      "learning_rate": 0.00012972972972972972,
      "loss": 1.0198,
      "step": 29010
    },
    {
      "epoch": 1.7066676468426083,
      "grad_norm": 0.34244805574417114,
      "learning_rate": 0.0001296708045254557,
      "loss": 0.9763,
      "step": 29020
    },
    {
      "epoch": 1.7072557524075571,
      "grad_norm": 0.41989436745643616,
      "learning_rate": 0.00012961187932118164,
      "loss": 1.0959,
      "step": 29030
    },
    {
      "epoch": 1.707843857972506,
      "grad_norm": 0.408358097076416,
      "learning_rate": 0.0001295529541169076,
      "loss": 1.0876,
      "step": 29040
    },
    {
      "epoch": 1.708431963537455,
      "grad_norm": 0.3966359496116638,
      "learning_rate": 0.00012949402891263354,
      "loss": 1.0017,
      "step": 29050
    },
    {
      "epoch": 1.709020069102404,
      "grad_norm": 0.33407241106033325,
      "learning_rate": 0.0001294351037083595,
      "loss": 1.0347,
      "step": 29060
    },
    {
      "epoch": 1.7096081746673528,
      "grad_norm": 0.3753260374069214,
      "learning_rate": 0.00012937617850408546,
      "loss": 0.9146,
      "step": 29070
    },
    {
      "epoch": 1.7101962802323016,
      "grad_norm": 0.37031522393226624,
      "learning_rate": 0.00012931725329981143,
      "loss": 0.974,
      "step": 29080
    },
    {
      "epoch": 1.7107843857972505,
      "grad_norm": 0.37275341153144836,
      "learning_rate": 0.00012925832809553738,
      "loss": 0.9401,
      "step": 29090
    },
    {
      "epoch": 1.7113724913621995,
      "grad_norm": 0.336701363325119,
      "learning_rate": 0.00012919940289126335,
      "loss": 1.0033,
      "step": 29100
    },
    {
      "epoch": 1.7119605969271485,
      "grad_norm": 0.37924516201019287,
      "learning_rate": 0.0001291404776869893,
      "loss": 0.9026,
      "step": 29110
    },
    {
      "epoch": 1.7125487024920973,
      "grad_norm": 0.33636024594306946,
      "learning_rate": 0.00012908155248271525,
      "loss": 1.0416,
      "step": 29120
    },
    {
      "epoch": 1.7131368080570462,
      "grad_norm": 0.3826577067375183,
      "learning_rate": 0.00012902262727844122,
      "loss": 0.957,
      "step": 29130
    },
    {
      "epoch": 1.7137249136219952,
      "grad_norm": 0.386339008808136,
      "learning_rate": 0.00012896370207416717,
      "loss": 1.0098,
      "step": 29140
    },
    {
      "epoch": 1.714313019186944,
      "grad_norm": 0.3459145724773407,
      "learning_rate": 0.00012890477686989315,
      "loss": 1.0303,
      "step": 29150
    },
    {
      "epoch": 1.714901124751893,
      "grad_norm": 0.3994544744491577,
      "learning_rate": 0.0001288458516656191,
      "loss": 1.0413,
      "step": 29160
    },
    {
      "epoch": 1.7154892303168419,
      "grad_norm": 0.38842830061912537,
      "learning_rate": 0.00012878692646134507,
      "loss": 0.9552,
      "step": 29170
    },
    {
      "epoch": 1.7160773358817907,
      "grad_norm": 0.36539819836616516,
      "learning_rate": 0.00012872800125707102,
      "loss": 1.0614,
      "step": 29180
    },
    {
      "epoch": 1.7166654414467397,
      "grad_norm": 0.3455975353717804,
      "learning_rate": 0.00012866907605279696,
      "loss": 0.933,
      "step": 29190
    },
    {
      "epoch": 1.7172535470116888,
      "grad_norm": 0.3552533686161041,
      "learning_rate": 0.00012861015084852294,
      "loss": 0.9915,
      "step": 29200
    },
    {
      "epoch": 1.7178416525766376,
      "grad_norm": 0.3643229603767395,
      "learning_rate": 0.0001285512256442489,
      "loss": 1.1013,
      "step": 29210
    },
    {
      "epoch": 1.7184297581415864,
      "grad_norm": 0.36269518733024597,
      "learning_rate": 0.00012849230043997486,
      "loss": 1.0441,
      "step": 29220
    },
    {
      "epoch": 1.7190178637065352,
      "grad_norm": 0.33321401476860046,
      "learning_rate": 0.0001284333752357008,
      "loss": 1.0181,
      "step": 29230
    },
    {
      "epoch": 1.7196059692714842,
      "grad_norm": 0.3486897349357605,
      "learning_rate": 0.00012837445003142676,
      "loss": 1.0372,
      "step": 29240
    },
    {
      "epoch": 1.7201940748364333,
      "grad_norm": 0.37709853053092957,
      "learning_rate": 0.0001283155248271527,
      "loss": 1.0099,
      "step": 29250
    },
    {
      "epoch": 1.720782180401382,
      "grad_norm": 0.350093811750412,
      "learning_rate": 0.00012825659962287868,
      "loss": 1.0833,
      "step": 29260
    },
    {
      "epoch": 1.721370285966331,
      "grad_norm": 0.3163035809993744,
      "learning_rate": 0.00012819767441860463,
      "loss": 0.9562,
      "step": 29270
    },
    {
      "epoch": 1.7219583915312797,
      "grad_norm": 0.388931006193161,
      "learning_rate": 0.0001281387492143306,
      "loss": 1.0884,
      "step": 29280
    },
    {
      "epoch": 1.7225464970962288,
      "grad_norm": 0.33168119192123413,
      "learning_rate": 0.00012807982401005655,
      "loss": 0.9085,
      "step": 29290
    },
    {
      "epoch": 1.7231346026611778,
      "grad_norm": 0.34823888540267944,
      "learning_rate": 0.00012802089880578252,
      "loss": 1.0894,
      "step": 29300
    },
    {
      "epoch": 1.7237227082261266,
      "grad_norm": 0.36021360754966736,
      "learning_rate": 0.00012796197360150847,
      "loss": 1.0329,
      "step": 29310
    },
    {
      "epoch": 1.7243108137910754,
      "grad_norm": 0.35682186484336853,
      "learning_rate": 0.00012790304839723442,
      "loss": 1.0792,
      "step": 29320
    },
    {
      "epoch": 1.7248989193560242,
      "grad_norm": 0.3649616241455078,
      "learning_rate": 0.0001278441231929604,
      "loss": 1.0225,
      "step": 29330
    },
    {
      "epoch": 1.7254870249209733,
      "grad_norm": 0.34197425842285156,
      "learning_rate": 0.00012778519798868634,
      "loss": 1.0197,
      "step": 29340
    },
    {
      "epoch": 1.7260751304859223,
      "grad_norm": 0.3476351499557495,
      "learning_rate": 0.00012772627278441232,
      "loss": 1.0662,
      "step": 29350
    },
    {
      "epoch": 1.7266632360508711,
      "grad_norm": 0.38105085492134094,
      "learning_rate": 0.00012766734758013826,
      "loss": 1.0058,
      "step": 29360
    },
    {
      "epoch": 1.72725134161582,
      "grad_norm": 0.36470210552215576,
      "learning_rate": 0.00012760842237586424,
      "loss": 1.0556,
      "step": 29370
    },
    {
      "epoch": 1.727839447180769,
      "grad_norm": 0.37789052724838257,
      "learning_rate": 0.0001275494971715902,
      "loss": 0.9271,
      "step": 29380
    },
    {
      "epoch": 1.728427552745718,
      "grad_norm": 0.339603990316391,
      "learning_rate": 0.00012749057196731613,
      "loss": 0.9748,
      "step": 29390
    },
    {
      "epoch": 1.7290156583106668,
      "grad_norm": 0.36559170484542847,
      "learning_rate": 0.0001274316467630421,
      "loss": 0.9784,
      "step": 29400
    },
    {
      "epoch": 1.7296037638756157,
      "grad_norm": 0.3772423267364502,
      "learning_rate": 0.00012737272155876806,
      "loss": 1.0551,
      "step": 29410
    },
    {
      "epoch": 1.7301918694405645,
      "grad_norm": 0.36423006653785706,
      "learning_rate": 0.00012731379635449403,
      "loss": 1.0956,
      "step": 29420
    },
    {
      "epoch": 1.7307799750055135,
      "grad_norm": 0.3611447811126709,
      "learning_rate": 0.00012725487115021998,
      "loss": 0.9533,
      "step": 29430
    },
    {
      "epoch": 1.7313680805704625,
      "grad_norm": 0.3399893641471863,
      "learning_rate": 0.00012719594594594595,
      "loss": 1.0579,
      "step": 29440
    },
    {
      "epoch": 1.7319561861354114,
      "grad_norm": 0.35717102885246277,
      "learning_rate": 0.00012713702074167187,
      "loss": 0.8973,
      "step": 29450
    },
    {
      "epoch": 1.7325442917003602,
      "grad_norm": 0.32093754410743713,
      "learning_rate": 0.00012707809553739785,
      "loss": 0.9498,
      "step": 29460
    },
    {
      "epoch": 1.733132397265309,
      "grad_norm": 0.33832743763923645,
      "learning_rate": 0.0001270191703331238,
      "loss": 1.0767,
      "step": 29470
    },
    {
      "epoch": 1.733720502830258,
      "grad_norm": 0.38811418414115906,
      "learning_rate": 0.00012696024512884977,
      "loss": 0.9891,
      "step": 29480
    },
    {
      "epoch": 1.734308608395207,
      "grad_norm": 0.3926917016506195,
      "learning_rate": 0.00012690131992457572,
      "loss": 1.0558,
      "step": 29490
    },
    {
      "epoch": 1.7348967139601559,
      "grad_norm": 0.3362366259098053,
      "learning_rate": 0.0001268423947203017,
      "loss": 1.0042,
      "step": 29500
    },
    {
      "epoch": 1.7354848195251047,
      "grad_norm": 0.40980231761932373,
      "learning_rate": 0.00012678346951602764,
      "loss": 1.0503,
      "step": 29510
    },
    {
      "epoch": 1.7360729250900535,
      "grad_norm": 0.3666894733905792,
      "learning_rate": 0.0001267245443117536,
      "loss": 1.1024,
      "step": 29520
    },
    {
      "epoch": 1.7366610306550025,
      "grad_norm": 0.38994789123535156,
      "learning_rate": 0.00012666561910747956,
      "loss": 1.1321,
      "step": 29530
    },
    {
      "epoch": 1.7372491362199516,
      "grad_norm": 0.3517436981201172,
      "learning_rate": 0.0001266066939032055,
      "loss": 1.0487,
      "step": 29540
    },
    {
      "epoch": 1.7378372417849004,
      "grad_norm": 0.392385333776474,
      "learning_rate": 0.0001265477686989315,
      "loss": 1.0053,
      "step": 29550
    },
    {
      "epoch": 1.7384253473498492,
      "grad_norm": 0.3353509306907654,
      "learning_rate": 0.00012648884349465743,
      "loss": 0.9958,
      "step": 29560
    },
    {
      "epoch": 1.7390134529147983,
      "grad_norm": 0.42398297786712646,
      "learning_rate": 0.0001264299182903834,
      "loss": 1.1014,
      "step": 29570
    },
    {
      "epoch": 1.739601558479747,
      "grad_norm": 0.3526543974876404,
      "learning_rate": 0.00012637099308610936,
      "loss": 1.0506,
      "step": 29580
    },
    {
      "epoch": 1.740189664044696,
      "grad_norm": 0.3203542232513428,
      "learning_rate": 0.0001263120678818353,
      "loss": 0.9931,
      "step": 29590
    },
    {
      "epoch": 1.740777769609645,
      "grad_norm": 0.3657482862472534,
      "learning_rate": 0.00012625314267756128,
      "loss": 0.9788,
      "step": 29600
    },
    {
      "epoch": 1.7413658751745937,
      "grad_norm": 0.33070680499076843,
      "learning_rate": 0.00012619421747328723,
      "loss": 1.0521,
      "step": 29610
    },
    {
      "epoch": 1.7419539807395428,
      "grad_norm": 0.3740617334842682,
      "learning_rate": 0.0001261352922690132,
      "loss": 0.9996,
      "step": 29620
    },
    {
      "epoch": 1.7425420863044918,
      "grad_norm": 0.4476335048675537,
      "learning_rate": 0.00012607636706473915,
      "loss": 1.0253,
      "step": 29630
    },
    {
      "epoch": 1.7431301918694406,
      "grad_norm": 0.3841564953327179,
      "learning_rate": 0.00012601744186046512,
      "loss": 1.0831,
      "step": 29640
    },
    {
      "epoch": 1.7437182974343894,
      "grad_norm": 0.30545225739479065,
      "learning_rate": 0.00012595851665619107,
      "loss": 1.0694,
      "step": 29650
    },
    {
      "epoch": 1.7443064029993383,
      "grad_norm": 0.3170417249202728,
      "learning_rate": 0.00012589959145191702,
      "loss": 0.9586,
      "step": 29660
    },
    {
      "epoch": 1.7448945085642873,
      "grad_norm": 0.36123543977737427,
      "learning_rate": 0.00012584066624764297,
      "loss": 1.0867,
      "step": 29670
    },
    {
      "epoch": 1.7454826141292363,
      "grad_norm": 0.3423078954219818,
      "learning_rate": 0.00012578174104336894,
      "loss": 1.0281,
      "step": 29680
    },
    {
      "epoch": 1.7460707196941851,
      "grad_norm": 0.3464491665363312,
      "learning_rate": 0.0001257228158390949,
      "loss": 1.0148,
      "step": 29690
    },
    {
      "epoch": 1.746658825259134,
      "grad_norm": 0.37695878744125366,
      "learning_rate": 0.00012566389063482086,
      "loss": 0.9649,
      "step": 29700
    },
    {
      "epoch": 1.7472469308240828,
      "grad_norm": 0.35511314868927,
      "learning_rate": 0.0001256049654305468,
      "loss": 1.0042,
      "step": 29710
    },
    {
      "epoch": 1.7478350363890318,
      "grad_norm": 0.3224305510520935,
      "learning_rate": 0.00012554604022627276,
      "loss": 1.0531,
      "step": 29720
    },
    {
      "epoch": 1.7484231419539809,
      "grad_norm": 0.38037100434303284,
      "learning_rate": 0.00012548711502199873,
      "loss": 0.9634,
      "step": 29730
    },
    {
      "epoch": 1.7490112475189297,
      "grad_norm": 0.3730316758155823,
      "learning_rate": 0.00012542818981772468,
      "loss": 1.0549,
      "step": 29740
    },
    {
      "epoch": 1.7495993530838785,
      "grad_norm": 0.34895071387290955,
      "learning_rate": 0.00012536926461345066,
      "loss": 1.0154,
      "step": 29750
    },
    {
      "epoch": 1.7501874586488273,
      "grad_norm": 0.41465866565704346,
      "learning_rate": 0.0001253103394091766,
      "loss": 0.9808,
      "step": 29760
    },
    {
      "epoch": 1.7507755642137763,
      "grad_norm": 0.40446919202804565,
      "learning_rate": 0.00012525141420490258,
      "loss": 0.9794,
      "step": 29770
    },
    {
      "epoch": 1.7513636697787254,
      "grad_norm": 0.3593003749847412,
      "learning_rate": 0.00012519248900062853,
      "loss": 1.0494,
      "step": 29780
    },
    {
      "epoch": 1.7519517753436742,
      "grad_norm": 0.35909393429756165,
      "learning_rate": 0.00012513356379635447,
      "loss": 0.9901,
      "step": 29790
    },
    {
      "epoch": 1.752539880908623,
      "grad_norm": 0.36666586995124817,
      "learning_rate": 0.00012507463859208045,
      "loss": 1.1133,
      "step": 29800
    },
    {
      "epoch": 1.753127986473572,
      "grad_norm": 0.3578716516494751,
      "learning_rate": 0.0001250157133878064,
      "loss": 1.0282,
      "step": 29810
    },
    {
      "epoch": 1.753716092038521,
      "grad_norm": 0.3469257652759552,
      "learning_rate": 0.00012495678818353237,
      "loss": 1.0646,
      "step": 29820
    },
    {
      "epoch": 1.75430419760347,
      "grad_norm": 0.35395652055740356,
      "learning_rate": 0.00012489786297925832,
      "loss": 0.9466,
      "step": 29830
    },
    {
      "epoch": 1.7548923031684187,
      "grad_norm": 0.3363160490989685,
      "learning_rate": 0.0001248389377749843,
      "loss": 1.0685,
      "step": 29840
    },
    {
      "epoch": 1.7554804087333675,
      "grad_norm": 0.4055118262767792,
      "learning_rate": 0.00012478001257071024,
      "loss": 1.0629,
      "step": 29850
    },
    {
      "epoch": 1.7560685142983166,
      "grad_norm": 0.3299819529056549,
      "learning_rate": 0.0001247210873664362,
      "loss": 0.928,
      "step": 29860
    },
    {
      "epoch": 1.7566566198632656,
      "grad_norm": 0.39800578355789185,
      "learning_rate": 0.00012466216216216214,
      "loss": 0.9763,
      "step": 29870
    },
    {
      "epoch": 1.7572447254282144,
      "grad_norm": 0.362192302942276,
      "learning_rate": 0.0001246032369578881,
      "loss": 1.0258,
      "step": 29880
    },
    {
      "epoch": 1.7578328309931632,
      "grad_norm": 0.37389466166496277,
      "learning_rate": 0.00012454431175361406,
      "loss": 1.0131,
      "step": 29890
    },
    {
      "epoch": 1.758420936558112,
      "grad_norm": 0.3380078673362732,
      "learning_rate": 0.00012448538654934003,
      "loss": 1.0425,
      "step": 29900
    },
    {
      "epoch": 1.759009042123061,
      "grad_norm": 0.34931713342666626,
      "learning_rate": 0.00012442646134506598,
      "loss": 1.0261,
      "step": 29910
    },
    {
      "epoch": 1.7595971476880101,
      "grad_norm": 0.4001452326774597,
      "learning_rate": 0.00012436753614079193,
      "loss": 1.0298,
      "step": 29920
    },
    {
      "epoch": 1.760185253252959,
      "grad_norm": 0.36835938692092896,
      "learning_rate": 0.0001243086109365179,
      "loss": 1.1625,
      "step": 29930
    },
    {
      "epoch": 1.7607733588179078,
      "grad_norm": 0.38329192996025085,
      "learning_rate": 0.00012424968573224385,
      "loss": 1.1134,
      "step": 29940
    },
    {
      "epoch": 1.7613614643828566,
      "grad_norm": 0.3065031170845032,
      "learning_rate": 0.00012419076052796983,
      "loss": 1.1202,
      "step": 29950
    },
    {
      "epoch": 1.7619495699478056,
      "grad_norm": 0.325051486492157,
      "learning_rate": 0.00012413183532369577,
      "loss": 0.9807,
      "step": 29960
    },
    {
      "epoch": 1.7625376755127546,
      "grad_norm": 0.3591424822807312,
      "learning_rate": 0.00012407291011942175,
      "loss": 1.0183,
      "step": 29970
    },
    {
      "epoch": 1.7631257810777035,
      "grad_norm": 0.3811368942260742,
      "learning_rate": 0.0001240139849151477,
      "loss": 1.0154,
      "step": 29980
    },
    {
      "epoch": 1.7637138866426523,
      "grad_norm": 0.3681144118309021,
      "learning_rate": 0.00012395505971087364,
      "loss": 1.1371,
      "step": 29990
    },
    {
      "epoch": 1.7643019922076013,
      "grad_norm": 0.3302520215511322,
      "learning_rate": 0.00012389613450659962,
      "loss": 1.0605,
      "step": 30000
    },
    {
      "epoch": 1.7648900977725501,
      "grad_norm": 0.33086472749710083,
      "learning_rate": 0.00012383720930232557,
      "loss": 1.0149,
      "step": 30010
    },
    {
      "epoch": 1.7654782033374992,
      "grad_norm": 0.38324907422065735,
      "learning_rate": 0.00012377828409805154,
      "loss": 1.0891,
      "step": 30020
    },
    {
      "epoch": 1.766066308902448,
      "grad_norm": 0.4283851087093353,
      "learning_rate": 0.0001237193588937775,
      "loss": 1.0289,
      "step": 30030
    },
    {
      "epoch": 1.7666544144673968,
      "grad_norm": 0.4035763144493103,
      "learning_rate": 0.00012366043368950346,
      "loss": 0.9785,
      "step": 30040
    },
    {
      "epoch": 1.7672425200323458,
      "grad_norm": 0.353915810585022,
      "learning_rate": 0.0001236015084852294,
      "loss": 1.0567,
      "step": 30050
    },
    {
      "epoch": 1.7678306255972949,
      "grad_norm": 0.33583560585975647,
      "learning_rate": 0.00012354847580138277,
      "loss": 1.012,
      "step": 30060
    },
    {
      "epoch": 1.7684187311622437,
      "grad_norm": 0.37185654044151306,
      "learning_rate": 0.00012348955059710871,
      "loss": 1.009,
      "step": 30070
    },
    {
      "epoch": 1.7690068367271925,
      "grad_norm": 0.42189276218414307,
      "learning_rate": 0.0001234306253928347,
      "loss": 0.9406,
      "step": 30080
    },
    {
      "epoch": 1.7695949422921413,
      "grad_norm": 0.3394095301628113,
      "learning_rate": 0.00012337170018856064,
      "loss": 1.008,
      "step": 30090
    },
    {
      "epoch": 1.7701830478570904,
      "grad_norm": 0.30651432275772095,
      "learning_rate": 0.0001233127749842866,
      "loss": 0.9746,
      "step": 30100
    },
    {
      "epoch": 1.7707711534220394,
      "grad_norm": 0.37837424874305725,
      "learning_rate": 0.00012325384978001256,
      "loss": 0.909,
      "step": 30110
    },
    {
      "epoch": 1.7713592589869882,
      "grad_norm": 0.3210141062736511,
      "learning_rate": 0.00012319492457573853,
      "loss": 1.048,
      "step": 30120
    },
    {
      "epoch": 1.771947364551937,
      "grad_norm": 0.31285277009010315,
      "learning_rate": 0.00012313599937146448,
      "loss": 0.9396,
      "step": 30130
    },
    {
      "epoch": 1.7725354701168858,
      "grad_norm": 0.3768308460712433,
      "learning_rate": 0.00012307707416719043,
      "loss": 1.0425,
      "step": 30140
    },
    {
      "epoch": 1.7731235756818349,
      "grad_norm": 0.32643213868141174,
      "learning_rate": 0.0001230181489629164,
      "loss": 0.9181,
      "step": 30150
    },
    {
      "epoch": 1.773711681246784,
      "grad_norm": 0.36479562520980835,
      "learning_rate": 0.00012295922375864235,
      "loss": 0.9732,
      "step": 30160
    },
    {
      "epoch": 1.7742997868117327,
      "grad_norm": 0.3195275366306305,
      "learning_rate": 0.0001229002985543683,
      "loss": 0.988,
      "step": 30170
    },
    {
      "epoch": 1.7748878923766815,
      "grad_norm": 0.43148717284202576,
      "learning_rate": 0.00012284137335009427,
      "loss": 1.0534,
      "step": 30180
    },
    {
      "epoch": 1.7754759979416304,
      "grad_norm": 0.37967386841773987,
      "learning_rate": 0.00012278244814582022,
      "loss": 1.0221,
      "step": 30190
    },
    {
      "epoch": 1.7760641035065794,
      "grad_norm": 0.33805209398269653,
      "learning_rate": 0.00012272352294154617,
      "loss": 1.1017,
      "step": 30200
    },
    {
      "epoch": 1.7766522090715284,
      "grad_norm": 0.31357473134994507,
      "learning_rate": 0.00012266459773727214,
      "loss": 0.9297,
      "step": 30210
    },
    {
      "epoch": 1.7772403146364772,
      "grad_norm": 0.3839379847049713,
      "learning_rate": 0.0001226056725329981,
      "loss": 1.0277,
      "step": 30220
    },
    {
      "epoch": 1.777828420201426,
      "grad_norm": 0.3391871452331543,
      "learning_rate": 0.00012254674732872407,
      "loss": 1.0208,
      "step": 30230
    },
    {
      "epoch": 1.778416525766375,
      "grad_norm": 0.3057996928691864,
      "learning_rate": 0.00012248782212445001,
      "loss": 1.1351,
      "step": 30240
    },
    {
      "epoch": 1.7790046313313241,
      "grad_norm": 0.3849540650844574,
      "learning_rate": 0.000122428896920176,
      "loss": 1.0236,
      "step": 30250
    },
    {
      "epoch": 1.779592736896273,
      "grad_norm": 0.33565208315849304,
      "learning_rate": 0.00012236997171590194,
      "loss": 1.0265,
      "step": 30260
    },
    {
      "epoch": 1.7801808424612218,
      "grad_norm": 0.3904728889465332,
      "learning_rate": 0.00012231104651162788,
      "loss": 1.0843,
      "step": 30270
    },
    {
      "epoch": 1.7807689480261706,
      "grad_norm": 0.36036697030067444,
      "learning_rate": 0.00012225212130735386,
      "loss": 1.0295,
      "step": 30280
    },
    {
      "epoch": 1.7813570535911196,
      "grad_norm": 0.42061537504196167,
      "learning_rate": 0.0001221931961030798,
      "loss": 1.0568,
      "step": 30290
    },
    {
      "epoch": 1.7819451591560687,
      "grad_norm": 0.3590044379234314,
      "learning_rate": 0.00012213427089880578,
      "loss": 0.9872,
      "step": 30300
    },
    {
      "epoch": 1.7825332647210175,
      "grad_norm": 0.3447684645652771,
      "learning_rate": 0.00012207534569453173,
      "loss": 1.0049,
      "step": 30310
    },
    {
      "epoch": 1.7831213702859663,
      "grad_norm": 0.3296189308166504,
      "learning_rate": 0.00012201642049025769,
      "loss": 1.0354,
      "step": 30320
    },
    {
      "epoch": 1.783709475850915,
      "grad_norm": 0.32570913434028625,
      "learning_rate": 0.00012195749528598365,
      "loss": 1.0931,
      "step": 30330
    },
    {
      "epoch": 1.7842975814158641,
      "grad_norm": 0.3256068825721741,
      "learning_rate": 0.00012189857008170961,
      "loss": 0.9541,
      "step": 30340
    },
    {
      "epoch": 1.7848856869808132,
      "grad_norm": 0.36982327699661255,
      "learning_rate": 0.00012183964487743557,
      "loss": 0.9756,
      "step": 30350
    },
    {
      "epoch": 1.785473792545762,
      "grad_norm": 0.4082585275173187,
      "learning_rate": 0.00012178071967316154,
      "loss": 0.9222,
      "step": 30360
    },
    {
      "epoch": 1.7860618981107108,
      "grad_norm": 0.36929717659950256,
      "learning_rate": 0.00012172179446888747,
      "loss": 0.9881,
      "step": 30370
    },
    {
      "epoch": 1.7866500036756596,
      "grad_norm": 0.3650018870830536,
      "learning_rate": 0.00012166286926461343,
      "loss": 0.9881,
      "step": 30380
    },
    {
      "epoch": 1.7872381092406087,
      "grad_norm": 0.3546287417411804,
      "learning_rate": 0.00012160394406033939,
      "loss": 1.0937,
      "step": 30390
    },
    {
      "epoch": 1.7878262148055577,
      "grad_norm": 0.3427996039390564,
      "learning_rate": 0.00012154501885606535,
      "loss": 0.9648,
      "step": 30400
    },
    {
      "epoch": 1.7884143203705065,
      "grad_norm": 0.32818087935447693,
      "learning_rate": 0.00012148609365179131,
      "loss": 0.9725,
      "step": 30410
    },
    {
      "epoch": 1.7890024259354553,
      "grad_norm": 0.3475096523761749,
      "learning_rate": 0.00012142716844751728,
      "loss": 1.0696,
      "step": 30420
    },
    {
      "epoch": 1.7895905315004044,
      "grad_norm": 0.3326198160648346,
      "learning_rate": 0.00012136824324324322,
      "loss": 1.0318,
      "step": 30430
    },
    {
      "epoch": 1.7901786370653532,
      "grad_norm": 0.37257644534111023,
      "learning_rate": 0.00012130931803896918,
      "loss": 1.0517,
      "step": 30440
    },
    {
      "epoch": 1.7907667426303022,
      "grad_norm": 0.3670153021812439,
      "learning_rate": 0.00012125039283469515,
      "loss": 0.9799,
      "step": 30450
    },
    {
      "epoch": 1.791354848195251,
      "grad_norm": 0.31402966380119324,
      "learning_rate": 0.0001211914676304211,
      "loss": 1.0068,
      "step": 30460
    },
    {
      "epoch": 1.7919429537601999,
      "grad_norm": 0.332554429769516,
      "learning_rate": 0.00012113254242614707,
      "loss": 1.0573,
      "step": 30470
    },
    {
      "epoch": 1.792531059325149,
      "grad_norm": 0.3225191533565521,
      "learning_rate": 0.00012107361722187303,
      "loss": 1.0585,
      "step": 30480
    },
    {
      "epoch": 1.793119164890098,
      "grad_norm": 0.35743477940559387,
      "learning_rate": 0.00012101469201759899,
      "loss": 1.0692,
      "step": 30490
    },
    {
      "epoch": 1.7937072704550467,
      "grad_norm": 0.3330630660057068,
      "learning_rate": 0.00012095576681332495,
      "loss": 1.0081,
      "step": 30500
    },
    {
      "epoch": 1.7942953760199956,
      "grad_norm": 0.33224600553512573,
      "learning_rate": 0.0001208968416090509,
      "loss": 0.9818,
      "step": 30510
    },
    {
      "epoch": 1.7948834815849444,
      "grad_norm": 0.3308488726615906,
      "learning_rate": 0.00012083791640477686,
      "loss": 1.0307,
      "step": 30520
    },
    {
      "epoch": 1.7954715871498934,
      "grad_norm": 0.3799363076686859,
      "learning_rate": 0.00012077899120050282,
      "loss": 1.0338,
      "step": 30530
    },
    {
      "epoch": 1.7960596927148424,
      "grad_norm": 0.36385780572891235,
      "learning_rate": 0.00012072006599622878,
      "loss": 1.0765,
      "step": 30540
    },
    {
      "epoch": 1.7966477982797913,
      "grad_norm": 0.4042145609855652,
      "learning_rate": 0.00012066114079195474,
      "loss": 0.9727,
      "step": 30550
    },
    {
      "epoch": 1.79723590384474,
      "grad_norm": 0.3867086172103882,
      "learning_rate": 0.0001206022155876807,
      "loss": 0.9417,
      "step": 30560
    },
    {
      "epoch": 1.797824009409689,
      "grad_norm": 0.3628942370414734,
      "learning_rate": 0.00012054329038340667,
      "loss": 1.0285,
      "step": 30570
    },
    {
      "epoch": 1.798412114974638,
      "grad_norm": 0.38915809988975525,
      "learning_rate": 0.0001204843651791326,
      "loss": 1.0213,
      "step": 30580
    },
    {
      "epoch": 1.799000220539587,
      "grad_norm": 0.3953523635864258,
      "learning_rate": 0.00012042543997485856,
      "loss": 1.0125,
      "step": 30590
    },
    {
      "epoch": 1.7995883261045358,
      "grad_norm": 0.35171520709991455,
      "learning_rate": 0.00012036651477058452,
      "loss": 0.9766,
      "step": 30600
    },
    {
      "epoch": 1.8001764316694846,
      "grad_norm": 0.3561643064022064,
      "learning_rate": 0.00012030758956631048,
      "loss": 0.9364,
      "step": 30610
    },
    {
      "epoch": 1.8007645372344334,
      "grad_norm": 0.3527878522872925,
      "learning_rate": 0.00012024866436203644,
      "loss": 1.0208,
      "step": 30620
    },
    {
      "epoch": 1.8013526427993825,
      "grad_norm": 0.371671199798584,
      "learning_rate": 0.00012018973915776239,
      "loss": 1.0269,
      "step": 30630
    },
    {
      "epoch": 1.8019407483643315,
      "grad_norm": 0.42219892144203186,
      "learning_rate": 0.00012013081395348835,
      "loss": 1.0028,
      "step": 30640
    },
    {
      "epoch": 1.8025288539292803,
      "grad_norm": 0.44106408953666687,
      "learning_rate": 0.00012007188874921431,
      "loss": 0.9312,
      "step": 30650
    },
    {
      "epoch": 1.8031169594942291,
      "grad_norm": 0.3723376989364624,
      "learning_rate": 0.00012001296354494028,
      "loss": 0.982,
      "step": 30660
    },
    {
      "epoch": 1.8037050650591782,
      "grad_norm": 0.34422892332077026,
      "learning_rate": 0.00011995403834066624,
      "loss": 1.0594,
      "step": 30670
    },
    {
      "epoch": 1.8042931706241272,
      "grad_norm": 0.3716738224029541,
      "learning_rate": 0.0001198951131363922,
      "loss": 1.0858,
      "step": 30680
    },
    {
      "epoch": 1.804881276189076,
      "grad_norm": 0.3260582387447357,
      "learning_rate": 0.00011983618793211816,
      "loss": 1.1462,
      "step": 30690
    },
    {
      "epoch": 1.8054693817540248,
      "grad_norm": 0.3454870581626892,
      "learning_rate": 0.00011977726272784412,
      "loss": 0.968,
      "step": 30700
    },
    {
      "epoch": 1.8060574873189736,
      "grad_norm": 0.3686186373233795,
      "learning_rate": 0.00011971833752357007,
      "loss": 1.0093,
      "step": 30710
    },
    {
      "epoch": 1.8066455928839227,
      "grad_norm": 0.3758677542209625,
      "learning_rate": 0.00011965941231929603,
      "loss": 0.9572,
      "step": 30720
    },
    {
      "epoch": 1.8072336984488717,
      "grad_norm": 0.3437823951244354,
      "learning_rate": 0.00011960048711502199,
      "loss": 0.9034,
      "step": 30730
    },
    {
      "epoch": 1.8078218040138205,
      "grad_norm": 0.4234622120857239,
      "learning_rate": 0.00011954156191074795,
      "loss": 1.0838,
      "step": 30740
    },
    {
      "epoch": 1.8084099095787693,
      "grad_norm": 0.41361239552497864,
      "learning_rate": 0.00011948263670647391,
      "loss": 1.1556,
      "step": 30750
    },
    {
      "epoch": 1.8089980151437182,
      "grad_norm": 0.3276383578777313,
      "learning_rate": 0.00011942371150219987,
      "loss": 0.9674,
      "step": 30760
    },
    {
      "epoch": 1.8095861207086672,
      "grad_norm": 0.35625913739204407,
      "learning_rate": 0.00011936478629792584,
      "loss": 1.0675,
      "step": 30770
    },
    {
      "epoch": 1.8101742262736162,
      "grad_norm": 0.3398924767971039,
      "learning_rate": 0.00011930586109365178,
      "loss": 1.0048,
      "step": 30780
    },
    {
      "epoch": 1.810762331838565,
      "grad_norm": 0.3612164258956909,
      "learning_rate": 0.00011924693588937773,
      "loss": 0.9108,
      "step": 30790
    },
    {
      "epoch": 1.8113504374035139,
      "grad_norm": 0.3660553991794586,
      "learning_rate": 0.00011918801068510369,
      "loss": 1.003,
      "step": 30800
    },
    {
      "epoch": 1.8119385429684627,
      "grad_norm": 0.3476606011390686,
      "learning_rate": 0.00011912908548082965,
      "loss": 1.0211,
      "step": 30810
    },
    {
      "epoch": 1.8125266485334117,
      "grad_norm": 0.3453685939311981,
      "learning_rate": 0.00011907016027655561,
      "loss": 1.0759,
      "step": 30820
    },
    {
      "epoch": 1.8131147540983608,
      "grad_norm": 0.33445850014686584,
      "learning_rate": 0.00011901123507228156,
      "loss": 0.9923,
      "step": 30830
    },
    {
      "epoch": 1.8137028596633096,
      "grad_norm": 0.3145131766796112,
      "learning_rate": 0.00011895230986800752,
      "loss": 1.0214,
      "step": 30840
    },
    {
      "epoch": 1.8142909652282584,
      "grad_norm": 0.33291324973106384,
      "learning_rate": 0.00011889338466373348,
      "loss": 1.0606,
      "step": 30850
    },
    {
      "epoch": 1.8148790707932074,
      "grad_norm": 0.3170919418334961,
      "learning_rate": 0.00011883445945945945,
      "loss": 1.0496,
      "step": 30860
    },
    {
      "epoch": 1.8154671763581562,
      "grad_norm": 0.34172871708869934,
      "learning_rate": 0.00011877553425518541,
      "loss": 0.9825,
      "step": 30870
    },
    {
      "epoch": 1.8160552819231053,
      "grad_norm": 0.37458866834640503,
      "learning_rate": 0.00011871660905091137,
      "loss": 1.0118,
      "step": 30880
    },
    {
      "epoch": 1.816643387488054,
      "grad_norm": 0.32741591334342957,
      "learning_rate": 0.00011865768384663733,
      "loss": 0.9868,
      "step": 30890
    },
    {
      "epoch": 1.817231493053003,
      "grad_norm": 0.3179832994937897,
      "learning_rate": 0.00011859875864236329,
      "loss": 1.016,
      "step": 30900
    },
    {
      "epoch": 1.817819598617952,
      "grad_norm": 0.41886386275291443,
      "learning_rate": 0.00011853983343808924,
      "loss": 0.993,
      "step": 30910
    },
    {
      "epoch": 1.818407704182901,
      "grad_norm": 0.3710983991622925,
      "learning_rate": 0.0001184809082338152,
      "loss": 0.9611,
      "step": 30920
    },
    {
      "epoch": 1.8189958097478498,
      "grad_norm": 0.37001341581344604,
      "learning_rate": 0.00011842198302954116,
      "loss": 0.9788,
      "step": 30930
    },
    {
      "epoch": 1.8195839153127986,
      "grad_norm": 0.42465126514434814,
      "learning_rate": 0.00011836305782526712,
      "loss": 1.0244,
      "step": 30940
    },
    {
      "epoch": 1.8201720208777474,
      "grad_norm": 0.4378671944141388,
      "learning_rate": 0.00011830413262099308,
      "loss": 1.0098,
      "step": 30950
    },
    {
      "epoch": 1.8207601264426965,
      "grad_norm": 0.3269287645816803,
      "learning_rate": 0.00011824520741671904,
      "loss": 1.0459,
      "step": 30960
    },
    {
      "epoch": 1.8213482320076455,
      "grad_norm": 0.3488832414150238,
      "learning_rate": 0.000118186282212445,
      "loss": 1.0814,
      "step": 30970
    },
    {
      "epoch": 1.8219363375725943,
      "grad_norm": 0.3720921277999878,
      "learning_rate": 0.00011812735700817095,
      "loss": 0.9479,
      "step": 30980
    },
    {
      "epoch": 1.8225244431375431,
      "grad_norm": 0.39286714792251587,
      "learning_rate": 0.00011806843180389691,
      "loss": 1.0752,
      "step": 30990
    },
    {
      "epoch": 1.823112548702492,
      "grad_norm": 0.35552990436553955,
      "learning_rate": 0.00011800950659962286,
      "loss": 1.0253,
      "step": 31000
    },
    {
      "epoch": 1.823700654267441,
      "grad_norm": 0.3277584910392761,
      "learning_rate": 0.00011795058139534882,
      "loss": 1.0137,
      "step": 31010
    },
    {
      "epoch": 1.82428875983239,
      "grad_norm": 0.3295068144798279,
      "learning_rate": 0.00011789165619107478,
      "loss": 1.1348,
      "step": 31020
    },
    {
      "epoch": 1.8248768653973388,
      "grad_norm": 0.3527720868587494,
      "learning_rate": 0.00011783273098680073,
      "loss": 0.8543,
      "step": 31030
    },
    {
      "epoch": 1.8254649709622877,
      "grad_norm": 0.3777305483818054,
      "learning_rate": 0.0001177738057825267,
      "loss": 1.1182,
      "step": 31040
    },
    {
      "epoch": 1.8260530765272365,
      "grad_norm": 0.33369266986846924,
      "learning_rate": 0.00011771488057825265,
      "loss": 0.9917,
      "step": 31050
    },
    {
      "epoch": 1.8266411820921855,
      "grad_norm": 0.4132450222969055,
      "learning_rate": 0.00011765595537397862,
      "loss": 0.9707,
      "step": 31060
    },
    {
      "epoch": 1.8272292876571345,
      "grad_norm": 0.32882240414619446,
      "learning_rate": 0.00011759703016970458,
      "loss": 1.0189,
      "step": 31070
    },
    {
      "epoch": 1.8278173932220834,
      "grad_norm": 0.38431355357170105,
      "learning_rate": 0.00011753810496543054,
      "loss": 1.0966,
      "step": 31080
    },
    {
      "epoch": 1.8284054987870322,
      "grad_norm": 0.46574148535728455,
      "learning_rate": 0.0001174791797611565,
      "loss": 0.9893,
      "step": 31090
    },
    {
      "epoch": 1.8289936043519812,
      "grad_norm": 0.3180488646030426,
      "learning_rate": 0.00011742025455688246,
      "loss": 0.9852,
      "step": 31100
    },
    {
      "epoch": 1.8295817099169303,
      "grad_norm": 0.3683098554611206,
      "learning_rate": 0.00011736132935260841,
      "loss": 0.9982,
      "step": 31110
    },
    {
      "epoch": 1.830169815481879,
      "grad_norm": 0.3828071653842926,
      "learning_rate": 0.00011730240414833437,
      "loss": 1.0294,
      "step": 31120
    },
    {
      "epoch": 1.8307579210468279,
      "grad_norm": 0.3368627429008484,
      "learning_rate": 0.00011724347894406033,
      "loss": 0.9726,
      "step": 31130
    },
    {
      "epoch": 1.8313460266117767,
      "grad_norm": 0.33580487966537476,
      "learning_rate": 0.00011718455373978629,
      "loss": 1.0135,
      "step": 31140
    },
    {
      "epoch": 1.8319341321767257,
      "grad_norm": 0.386463463306427,
      "learning_rate": 0.00011712562853551225,
      "loss": 1.1199,
      "step": 31150
    },
    {
      "epoch": 1.8325222377416748,
      "grad_norm": 0.30527693033218384,
      "learning_rate": 0.00011706670333123821,
      "loss": 0.9561,
      "step": 31160
    },
    {
      "epoch": 1.8331103433066236,
      "grad_norm": 0.3600352704524994,
      "learning_rate": 0.00011700777812696418,
      "loss": 0.9871,
      "step": 31170
    },
    {
      "epoch": 1.8336984488715724,
      "grad_norm": 0.3716374635696411,
      "learning_rate": 0.00011694885292269012,
      "loss": 1.0577,
      "step": 31180
    },
    {
      "epoch": 1.8342865544365212,
      "grad_norm": 0.3931129276752472,
      "learning_rate": 0.00011688992771841608,
      "loss": 1.0281,
      "step": 31190
    },
    {
      "epoch": 1.8348746600014703,
      "grad_norm": 0.3735862076282501,
      "learning_rate": 0.00011683100251414205,
      "loss": 1.0813,
      "step": 31200
    },
    {
      "epoch": 1.8354627655664193,
      "grad_norm": 0.3531584143638611,
      "learning_rate": 0.00011677207730986799,
      "loss": 1.0095,
      "step": 31210
    },
    {
      "epoch": 1.8360508711313681,
      "grad_norm": 0.3435062766075134,
      "learning_rate": 0.00011671315210559395,
      "loss": 1.0098,
      "step": 31220
    },
    {
      "epoch": 1.836638976696317,
      "grad_norm": 0.3288135528564453,
      "learning_rate": 0.0001166542269013199,
      "loss": 0.9787,
      "step": 31230
    },
    {
      "epoch": 1.8372270822612657,
      "grad_norm": 0.36677539348602295,
      "learning_rate": 0.00011659530169704586,
      "loss": 0.9651,
      "step": 31240
    },
    {
      "epoch": 1.8378151878262148,
      "grad_norm": 0.32202064990997314,
      "learning_rate": 0.00011653637649277182,
      "loss": 0.9689,
      "step": 31250
    },
    {
      "epoch": 1.8384032933911638,
      "grad_norm": 0.38747304677963257,
      "learning_rate": 0.00011647745128849779,
      "loss": 0.9047,
      "step": 31260
    },
    {
      "epoch": 1.8389913989561126,
      "grad_norm": 0.3667500913143158,
      "learning_rate": 0.00011641852608422375,
      "loss": 0.9383,
      "step": 31270
    },
    {
      "epoch": 1.8395795045210614,
      "grad_norm": 0.3742503821849823,
      "learning_rate": 0.00011635960087994971,
      "loss": 0.9905,
      "step": 31280
    },
    {
      "epoch": 1.8401676100860105,
      "grad_norm": 0.3513867259025574,
      "learning_rate": 0.00011630067567567567,
      "loss": 1.0167,
      "step": 31290
    },
    {
      "epoch": 1.8407557156509593,
      "grad_norm": 0.347731351852417,
      "learning_rate": 0.00011624175047140163,
      "loss": 1.0216,
      "step": 31300
    },
    {
      "epoch": 1.8413438212159083,
      "grad_norm": 0.34411197900772095,
      "learning_rate": 0.00011618282526712758,
      "loss": 1.0018,
      "step": 31310
    },
    {
      "epoch": 1.8419319267808572,
      "grad_norm": 0.3234246075153351,
      "learning_rate": 0.00011612390006285354,
      "loss": 1.0258,
      "step": 31320
    },
    {
      "epoch": 1.842520032345806,
      "grad_norm": 0.36731329560279846,
      "learning_rate": 0.0001160649748585795,
      "loss": 1.0226,
      "step": 31330
    },
    {
      "epoch": 1.843108137910755,
      "grad_norm": 0.3718308210372925,
      "learning_rate": 0.00011600604965430546,
      "loss": 1.0585,
      "step": 31340
    },
    {
      "epoch": 1.843696243475704,
      "grad_norm": 0.3193581700325012,
      "learning_rate": 0.00011594712445003142,
      "loss": 1.0663,
      "step": 31350
    },
    {
      "epoch": 1.8442843490406529,
      "grad_norm": 0.3489864766597748,
      "learning_rate": 0.00011588819924575738,
      "loss": 0.9272,
      "step": 31360
    },
    {
      "epoch": 1.8448724546056017,
      "grad_norm": 0.3063570261001587,
      "learning_rate": 0.00011582927404148335,
      "loss": 1.0615,
      "step": 31370
    },
    {
      "epoch": 1.8454605601705505,
      "grad_norm": 0.3733421862125397,
      "learning_rate": 0.00011577034883720929,
      "loss": 1.0105,
      "step": 31380
    },
    {
      "epoch": 1.8460486657354995,
      "grad_norm": 0.4507680833339691,
      "learning_rate": 0.00011571142363293525,
      "loss": 1.0051,
      "step": 31390
    },
    {
      "epoch": 1.8466367713004486,
      "grad_norm": 0.35312575101852417,
      "learning_rate": 0.00011565249842866122,
      "loss": 1.0,
      "step": 31400
    },
    {
      "epoch": 1.8472248768653974,
      "grad_norm": 0.3661899268627167,
      "learning_rate": 0.00011559357322438718,
      "loss": 1.0018,
      "step": 31410
    },
    {
      "epoch": 1.8478129824303462,
      "grad_norm": 0.34151846170425415,
      "learning_rate": 0.00011553464802011312,
      "loss": 1.0502,
      "step": 31420
    },
    {
      "epoch": 1.848401087995295,
      "grad_norm": 0.3912332057952881,
      "learning_rate": 0.00011547572281583907,
      "loss": 1.0208,
      "step": 31430
    },
    {
      "epoch": 1.848989193560244,
      "grad_norm": 0.35603222250938416,
      "learning_rate": 0.00011541679761156503,
      "loss": 0.972,
      "step": 31440
    },
    {
      "epoch": 1.849577299125193,
      "grad_norm": 0.34476977586746216,
      "learning_rate": 0.000115357872407291,
      "loss": 1.0088,
      "step": 31450
    },
    {
      "epoch": 1.850165404690142,
      "grad_norm": 0.35370364785194397,
      "learning_rate": 0.00011529894720301696,
      "loss": 1.139,
      "step": 31460
    },
    {
      "epoch": 1.8507535102550907,
      "grad_norm": 0.38209789991378784,
      "learning_rate": 0.00011524002199874292,
      "loss": 1.1235,
      "step": 31470
    },
    {
      "epoch": 1.8513416158200395,
      "grad_norm": 0.31592369079589844,
      "learning_rate": 0.00011518109679446888,
      "loss": 0.8914,
      "step": 31480
    },
    {
      "epoch": 1.8519297213849886,
      "grad_norm": 0.3631497025489807,
      "learning_rate": 0.00011512217159019484,
      "loss": 1.0334,
      "step": 31490
    },
    {
      "epoch": 1.8525178269499376,
      "grad_norm": 0.3472576439380646,
      "learning_rate": 0.00011506324638592079,
      "loss": 1.0016,
      "step": 31500
    },
    {
      "epoch": 1.8531059325148864,
      "grad_norm": 0.36617758870124817,
      "learning_rate": 0.00011500432118164675,
      "loss": 0.883,
      "step": 31510
    },
    {
      "epoch": 1.8536940380798352,
      "grad_norm": 0.355451762676239,
      "learning_rate": 0.00011494539597737271,
      "loss": 1.0995,
      "step": 31520
    },
    {
      "epoch": 1.8542821436447843,
      "grad_norm": 0.40240079164505005,
      "learning_rate": 0.00011488647077309867,
      "loss": 0.9704,
      "step": 31530
    },
    {
      "epoch": 1.8548702492097333,
      "grad_norm": 0.34264910221099854,
      "learning_rate": 0.00011482754556882463,
      "loss": 1.0327,
      "step": 31540
    },
    {
      "epoch": 1.8554583547746821,
      "grad_norm": 0.3741994798183441,
      "learning_rate": 0.00011476862036455059,
      "loss": 1.0032,
      "step": 31550
    },
    {
      "epoch": 1.856046460339631,
      "grad_norm": 0.36785104870796204,
      "learning_rate": 0.00011470969516027655,
      "loss": 1.0459,
      "step": 31560
    },
    {
      "epoch": 1.8566345659045798,
      "grad_norm": 0.36097919940948486,
      "learning_rate": 0.00011465076995600251,
      "loss": 1.0171,
      "step": 31570
    },
    {
      "epoch": 1.8572226714695288,
      "grad_norm": 0.28366413712501526,
      "learning_rate": 0.00011459184475172846,
      "loss": 1.0259,
      "step": 31580
    },
    {
      "epoch": 1.8578107770344778,
      "grad_norm": 0.3597811460494995,
      "learning_rate": 0.00011453291954745442,
      "loss": 1.0622,
      "step": 31590
    },
    {
      "epoch": 1.8583988825994266,
      "grad_norm": 0.3728032410144806,
      "learning_rate": 0.00011447399434318038,
      "loss": 0.9946,
      "step": 31600
    },
    {
      "epoch": 1.8589869881643755,
      "grad_norm": 0.3424491584300995,
      "learning_rate": 0.00011441506913890635,
      "loss": 1.0396,
      "step": 31610
    },
    {
      "epoch": 1.8595750937293243,
      "grad_norm": 0.33957672119140625,
      "learning_rate": 0.00011435614393463231,
      "loss": 1.0342,
      "step": 31620
    },
    {
      "epoch": 1.8601631992942733,
      "grad_norm": 0.33788347244262695,
      "learning_rate": 0.00011429721873035824,
      "loss": 1.0531,
      "step": 31630
    },
    {
      "epoch": 1.8607513048592224,
      "grad_norm": 0.34804847836494446,
      "learning_rate": 0.0001142382935260842,
      "loss": 1.0427,
      "step": 31640
    },
    {
      "epoch": 1.8613394104241712,
      "grad_norm": 0.36607134342193604,
      "learning_rate": 0.00011417936832181016,
      "loss": 1.03,
      "step": 31650
    },
    {
      "epoch": 1.86192751598912,
      "grad_norm": 0.33365124464035034,
      "learning_rate": 0.00011412044311753612,
      "loss": 1.0589,
      "step": 31660
    },
    {
      "epoch": 1.8625156215540688,
      "grad_norm": 0.37076759338378906,
      "learning_rate": 0.00011406151791326209,
      "loss": 0.964,
      "step": 31670
    },
    {
      "epoch": 1.8631037271190178,
      "grad_norm": 0.37024474143981934,
      "learning_rate": 0.00011400259270898805,
      "loss": 0.9721,
      "step": 31680
    },
    {
      "epoch": 1.8636918326839669,
      "grad_norm": 0.39523786306381226,
      "learning_rate": 0.00011394366750471401,
      "loss": 1.1273,
      "step": 31690
    },
    {
      "epoch": 1.8642799382489157,
      "grad_norm": 0.33975499868392944,
      "learning_rate": 0.00011388474230043996,
      "loss": 0.9951,
      "step": 31700
    },
    {
      "epoch": 1.8648680438138645,
      "grad_norm": 0.3919663429260254,
      "learning_rate": 0.00011382581709616592,
      "loss": 0.9832,
      "step": 31710
    },
    {
      "epoch": 1.8654561493788135,
      "grad_norm": 0.3238614797592163,
      "learning_rate": 0.00011376689189189188,
      "loss": 1.1017,
      "step": 31720
    },
    {
      "epoch": 1.8660442549437624,
      "grad_norm": 0.34273266792297363,
      "learning_rate": 0.00011370796668761784,
      "loss": 1.1092,
      "step": 31730
    },
    {
      "epoch": 1.8666323605087114,
      "grad_norm": 0.3555426597595215,
      "learning_rate": 0.0001136490414833438,
      "loss": 0.9582,
      "step": 31740
    },
    {
      "epoch": 1.8672204660736602,
      "grad_norm": 0.34905266761779785,
      "learning_rate": 0.00011359011627906976,
      "loss": 1.0523,
      "step": 31750
    },
    {
      "epoch": 1.867808571638609,
      "grad_norm": 0.3315693736076355,
      "learning_rate": 0.00011353119107479572,
      "loss": 0.9916,
      "step": 31760
    },
    {
      "epoch": 1.868396677203558,
      "grad_norm": 0.3799024522304535,
      "learning_rate": 0.00011347226587052168,
      "loss": 1.0682,
      "step": 31770
    },
    {
      "epoch": 1.868984782768507,
      "grad_norm": 0.3661997616291046,
      "learning_rate": 0.00011341334066624763,
      "loss": 1.0519,
      "step": 31780
    },
    {
      "epoch": 1.869572888333456,
      "grad_norm": 0.3662422001361847,
      "learning_rate": 0.0001133544154619736,
      "loss": 1.0681,
      "step": 31790
    },
    {
      "epoch": 1.8701609938984047,
      "grad_norm": 0.3118511736392975,
      "learning_rate": 0.00011329549025769955,
      "loss": 0.9997,
      "step": 31800
    },
    {
      "epoch": 1.8707490994633535,
      "grad_norm": 0.3368658125400543,
      "learning_rate": 0.00011323656505342552,
      "loss": 1.1128,
      "step": 31810
    },
    {
      "epoch": 1.8713372050283026,
      "grad_norm": 0.32979145646095276,
      "learning_rate": 0.00011317763984915148,
      "loss": 1.012,
      "step": 31820
    },
    {
      "epoch": 1.8719253105932516,
      "grad_norm": 0.3541216552257538,
      "learning_rate": 0.00011311871464487744,
      "loss": 1.046,
      "step": 31830
    },
    {
      "epoch": 1.8725134161582004,
      "grad_norm": 0.35709887742996216,
      "learning_rate": 0.00011305978944060337,
      "loss": 1.0784,
      "step": 31840
    },
    {
      "epoch": 1.8731015217231493,
      "grad_norm": 0.3065369129180908,
      "learning_rate": 0.00011300086423632933,
      "loss": 1.0557,
      "step": 31850
    },
    {
      "epoch": 1.873689627288098,
      "grad_norm": 0.3637891411781311,
      "learning_rate": 0.0001129419390320553,
      "loss": 1.0406,
      "step": 31860
    },
    {
      "epoch": 1.874277732853047,
      "grad_norm": 0.3158796429634094,
      "learning_rate": 0.00011288301382778126,
      "loss": 1.0223,
      "step": 31870
    },
    {
      "epoch": 1.8748658384179961,
      "grad_norm": 0.37987932562828064,
      "learning_rate": 0.00011282408862350722,
      "loss": 1.1509,
      "step": 31880
    },
    {
      "epoch": 1.875453943982945,
      "grad_norm": 0.3475913405418396,
      "learning_rate": 0.00011276516341923318,
      "loss": 0.9754,
      "step": 31890
    },
    {
      "epoch": 1.8760420495478938,
      "grad_norm": 0.3654301166534424,
      "learning_rate": 0.00011270623821495913,
      "loss": 0.994,
      "step": 31900
    },
    {
      "epoch": 1.8766301551128426,
      "grad_norm": 0.3697878420352936,
      "learning_rate": 0.00011264731301068509,
      "loss": 1.0028,
      "step": 31910
    },
    {
      "epoch": 1.8772182606777916,
      "grad_norm": 0.3558148443698883,
      "learning_rate": 0.00011258838780641105,
      "loss": 0.9302,
      "step": 31920
    },
    {
      "epoch": 1.8778063662427407,
      "grad_norm": 0.37998536229133606,
      "learning_rate": 0.00011252946260213701,
      "loss": 1.0943,
      "step": 31930
    },
    {
      "epoch": 1.8783944718076895,
      "grad_norm": 0.3691084384918213,
      "learning_rate": 0.00011247053739786297,
      "loss": 1.1089,
      "step": 31940
    },
    {
      "epoch": 1.8789825773726383,
      "grad_norm": 0.39091867208480835,
      "learning_rate": 0.00011241161219358893,
      "loss": 1.0724,
      "step": 31950
    },
    {
      "epoch": 1.8795706829375873,
      "grad_norm": 0.40338003635406494,
      "learning_rate": 0.00011235268698931489,
      "loss": 1.0558,
      "step": 31960
    },
    {
      "epoch": 1.8801587885025364,
      "grad_norm": 0.3606337308883667,
      "learning_rate": 0.00011229376178504085,
      "loss": 1.0192,
      "step": 31970
    },
    {
      "epoch": 1.8807468940674852,
      "grad_norm": 0.43291687965393066,
      "learning_rate": 0.0001122348365807668,
      "loss": 0.9852,
      "step": 31980
    },
    {
      "epoch": 1.881334999632434,
      "grad_norm": 0.3778044879436493,
      "learning_rate": 0.00011217591137649276,
      "loss": 1.0149,
      "step": 31990
    },
    {
      "epoch": 1.8819231051973828,
      "grad_norm": 0.3652268350124359,
      "learning_rate": 0.00011211698617221872,
      "loss": 1.1506,
      "step": 32000
    },
    {
      "epoch": 1.8825112107623319,
      "grad_norm": 0.3586070239543915,
      "learning_rate": 0.00011205806096794469,
      "loss": 1.0567,
      "step": 32010
    },
    {
      "epoch": 1.883099316327281,
      "grad_norm": 0.35016492009162903,
      "learning_rate": 0.00011199913576367065,
      "loss": 1.0356,
      "step": 32020
    },
    {
      "epoch": 1.8836874218922297,
      "grad_norm": 0.3829323649406433,
      "learning_rate": 0.00011194021055939661,
      "loss": 1.0146,
      "step": 32030
    },
    {
      "epoch": 1.8842755274571785,
      "grad_norm": 0.34393781423568726,
      "learning_rate": 0.00011188128535512254,
      "loss": 1.0852,
      "step": 32040
    },
    {
      "epoch": 1.8848636330221273,
      "grad_norm": 0.3294260799884796,
      "learning_rate": 0.0001118223601508485,
      "loss": 0.996,
      "step": 32050
    },
    {
      "epoch": 1.8854517385870764,
      "grad_norm": 0.38363465666770935,
      "learning_rate": 0.00011176932746700187,
      "loss": 1.0321,
      "step": 32060
    },
    {
      "epoch": 1.8860398441520254,
      "grad_norm": 0.32352569699287415,
      "learning_rate": 0.00011171040226272783,
      "loss": 1.0579,
      "step": 32070
    },
    {
      "epoch": 1.8866279497169742,
      "grad_norm": 0.34717345237731934,
      "learning_rate": 0.0001116514770584538,
      "loss": 0.9541,
      "step": 32080
    },
    {
      "epoch": 1.887216055281923,
      "grad_norm": 0.3491412103176117,
      "learning_rate": 0.00011159255185417976,
      "loss": 1.0728,
      "step": 32090
    },
    {
      "epoch": 1.8878041608468719,
      "grad_norm": 0.32088804244995117,
      "learning_rate": 0.00011153362664990572,
      "loss": 1.0184,
      "step": 32100
    },
    {
      "epoch": 1.888392266411821,
      "grad_norm": 0.3776129186153412,
      "learning_rate": 0.00011147470144563168,
      "loss": 1.0104,
      "step": 32110
    },
    {
      "epoch": 1.88898037197677,
      "grad_norm": 0.33541956543922424,
      "learning_rate": 0.00011141577624135764,
      "loss": 1.0903,
      "step": 32120
    },
    {
      "epoch": 1.8895684775417187,
      "grad_norm": 0.4313697814941406,
      "learning_rate": 0.00011135685103708357,
      "loss": 0.9642,
      "step": 32130
    },
    {
      "epoch": 1.8901565831066676,
      "grad_norm": 0.3566366732120514,
      "learning_rate": 0.00011129792583280953,
      "loss": 0.9781,
      "step": 32140
    },
    {
      "epoch": 1.8907446886716166,
      "grad_norm": 0.4187815189361572,
      "learning_rate": 0.0001112390006285355,
      "loss": 0.9869,
      "step": 32150
    },
    {
      "epoch": 1.8913327942365654,
      "grad_norm": 0.32009705901145935,
      "learning_rate": 0.00011118007542426146,
      "loss": 1.1095,
      "step": 32160
    },
    {
      "epoch": 1.8919208998015145,
      "grad_norm": 0.3663318157196045,
      "learning_rate": 0.00011112115021998742,
      "loss": 1.0467,
      "step": 32170
    },
    {
      "epoch": 1.8925090053664633,
      "grad_norm": 0.3635147213935852,
      "learning_rate": 0.00011106222501571338,
      "loss": 1.0672,
      "step": 32180
    },
    {
      "epoch": 1.893097110931412,
      "grad_norm": 0.3218417167663574,
      "learning_rate": 0.00011100329981143933,
      "loss": 1.0066,
      "step": 32190
    },
    {
      "epoch": 1.8936852164963611,
      "grad_norm": 0.35376355051994324,
      "learning_rate": 0.00011094437460716529,
      "loss": 0.9901,
      "step": 32200
    },
    {
      "epoch": 1.8942733220613102,
      "grad_norm": 0.40837693214416504,
      "learning_rate": 0.00011088544940289125,
      "loss": 0.9753,
      "step": 32210
    },
    {
      "epoch": 1.894861427626259,
      "grad_norm": 0.3662349581718445,
      "learning_rate": 0.00011082652419861721,
      "loss": 1.0319,
      "step": 32220
    },
    {
      "epoch": 1.8954495331912078,
      "grad_norm": 0.3277644217014313,
      "learning_rate": 0.00011076759899434317,
      "loss": 0.9856,
      "step": 32230
    },
    {
      "epoch": 1.8960376387561566,
      "grad_norm": 0.33380964398384094,
      "learning_rate": 0.00011070867379006913,
      "loss": 1.0353,
      "step": 32240
    },
    {
      "epoch": 1.8966257443211056,
      "grad_norm": 0.31009799242019653,
      "learning_rate": 0.0001106497485857951,
      "loss": 1.0521,
      "step": 32250
    },
    {
      "epoch": 1.8972138498860547,
      "grad_norm": 0.4077991843223572,
      "learning_rate": 0.00011059082338152104,
      "loss": 1.0535,
      "step": 32260
    },
    {
      "epoch": 1.8978019554510035,
      "grad_norm": 0.3411177694797516,
      "learning_rate": 0.000110531898177247,
      "loss": 0.9804,
      "step": 32270
    },
    {
      "epoch": 1.8983900610159523,
      "grad_norm": 0.32897940278053284,
      "learning_rate": 0.00011047297297297296,
      "loss": 0.9601,
      "step": 32280
    },
    {
      "epoch": 1.8989781665809011,
      "grad_norm": 0.37105661630630493,
      "learning_rate": 0.00011041404776869893,
      "loss": 1.1454,
      "step": 32290
    },
    {
      "epoch": 1.8995662721458502,
      "grad_norm": 0.3929522931575775,
      "learning_rate": 0.00011035512256442489,
      "loss": 0.9795,
      "step": 32300
    },
    {
      "epoch": 1.9001543777107992,
      "grad_norm": 0.37816518545150757,
      "learning_rate": 0.00011029619736015085,
      "loss": 1.0161,
      "step": 32310
    },
    {
      "epoch": 1.900742483275748,
      "grad_norm": 0.3415912985801697,
      "learning_rate": 0.00011023727215587681,
      "loss": 1.1418,
      "step": 32320
    },
    {
      "epoch": 1.9013305888406968,
      "grad_norm": 0.3394257724285126,
      "learning_rate": 0.00011017834695160277,
      "loss": 1.0587,
      "step": 32330
    },
    {
      "epoch": 1.9019186944056456,
      "grad_norm": 0.3243708610534668,
      "learning_rate": 0.0001101194217473287,
      "loss": 0.9492,
      "step": 32340
    },
    {
      "epoch": 1.9025067999705947,
      "grad_norm": 0.38688817620277405,
      "learning_rate": 0.00011006049654305467,
      "loss": 1.0054,
      "step": 32350
    },
    {
      "epoch": 1.9030949055355437,
      "grad_norm": 0.3841874599456787,
      "learning_rate": 0.00011000157133878063,
      "loss": 1.029,
      "step": 32360
    },
    {
      "epoch": 1.9036830111004925,
      "grad_norm": 0.3472031354904175,
      "learning_rate": 0.00010994264613450659,
      "loss": 1.1285,
      "step": 32370
    },
    {
      "epoch": 1.9042711166654414,
      "grad_norm": 0.37660011649131775,
      "learning_rate": 0.00010988372093023255,
      "loss": 1.0398,
      "step": 32380
    },
    {
      "epoch": 1.9048592222303904,
      "grad_norm": 0.34525424242019653,
      "learning_rate": 0.0001098247957259585,
      "loss": 0.9264,
      "step": 32390
    },
    {
      "epoch": 1.9054473277953394,
      "grad_norm": 0.3331548273563385,
      "learning_rate": 0.00010976587052168446,
      "loss": 1.0786,
      "step": 32400
    },
    {
      "epoch": 1.9060354333602882,
      "grad_norm": 0.38255250453948975,
      "learning_rate": 0.00010970694531741042,
      "loss": 0.9878,
      "step": 32410
    },
    {
      "epoch": 1.906623538925237,
      "grad_norm": 0.38411903381347656,
      "learning_rate": 0.00010964802011313638,
      "loss": 1.0131,
      "step": 32420
    },
    {
      "epoch": 1.9072116444901859,
      "grad_norm": 0.31435176730155945,
      "learning_rate": 0.00010958909490886234,
      "loss": 1.0765,
      "step": 32430
    },
    {
      "epoch": 1.907799750055135,
      "grad_norm": 0.35429081320762634,
      "learning_rate": 0.0001095301697045883,
      "loss": 0.9551,
      "step": 32440
    },
    {
      "epoch": 1.908387855620084,
      "grad_norm": 0.37095507979393005,
      "learning_rate": 0.00010947124450031426,
      "loss": 0.9689,
      "step": 32450
    },
    {
      "epoch": 1.9089759611850328,
      "grad_norm": 0.3744267225265503,
      "learning_rate": 0.00010941231929604021,
      "loss": 0.9296,
      "step": 32460
    },
    {
      "epoch": 1.9095640667499816,
      "grad_norm": 0.3891134262084961,
      "learning_rate": 0.00010935339409176617,
      "loss": 0.9968,
      "step": 32470
    },
    {
      "epoch": 1.9101521723149304,
      "grad_norm": 0.3791251480579376,
      "learning_rate": 0.00010929446888749213,
      "loss": 0.9803,
      "step": 32480
    },
    {
      "epoch": 1.9107402778798794,
      "grad_norm": 0.351466566324234,
      "learning_rate": 0.0001092355436832181,
      "loss": 1.0811,
      "step": 32490
    },
    {
      "epoch": 1.9113283834448285,
      "grad_norm": 0.36099404096603394,
      "learning_rate": 0.00010917661847894406,
      "loss": 1.0416,
      "step": 32500
    },
    {
      "epoch": 1.9119164890097773,
      "grad_norm": 0.35734856128692627,
      "learning_rate": 0.00010911769327467002,
      "loss": 1.0202,
      "step": 32510
    },
    {
      "epoch": 1.912504594574726,
      "grad_norm": 0.37232258915901184,
      "learning_rate": 0.00010905876807039598,
      "loss": 1.0328,
      "step": 32520
    },
    {
      "epoch": 1.913092700139675,
      "grad_norm": 0.35357043147087097,
      "learning_rate": 0.00010899984286612194,
      "loss": 1.0571,
      "step": 32530
    },
    {
      "epoch": 1.913680805704624,
      "grad_norm": 0.3629661500453949,
      "learning_rate": 0.00010894091766184789,
      "loss": 1.0527,
      "step": 32540
    },
    {
      "epoch": 1.914268911269573,
      "grad_norm": 0.4251629710197449,
      "learning_rate": 0.00010888199245757384,
      "loss": 0.9387,
      "step": 32550
    },
    {
      "epoch": 1.9148570168345218,
      "grad_norm": 0.33226385712623596,
      "learning_rate": 0.0001088230672532998,
      "loss": 0.9916,
      "step": 32560
    },
    {
      "epoch": 1.9154451223994706,
      "grad_norm": 0.3577357828617096,
      "learning_rate": 0.00010876414204902576,
      "loss": 1.0692,
      "step": 32570
    },
    {
      "epoch": 1.9160332279644197,
      "grad_norm": 0.41888922452926636,
      "learning_rate": 0.00010870521684475172,
      "loss": 0.9259,
      "step": 32580
    },
    {
      "epoch": 1.9166213335293685,
      "grad_norm": 0.3917703330516815,
      "learning_rate": 0.00010864629164047767,
      "loss": 1.0583,
      "step": 32590
    },
    {
      "epoch": 1.9172094390943175,
      "grad_norm": 0.36566880345344543,
      "learning_rate": 0.00010858736643620363,
      "loss": 1.0928,
      "step": 32600
    },
    {
      "epoch": 1.9177975446592663,
      "grad_norm": 0.3865605890750885,
      "learning_rate": 0.00010852844123192959,
      "loss": 1.0958,
      "step": 32610
    },
    {
      "epoch": 1.9183856502242151,
      "grad_norm": 0.3534482717514038,
      "learning_rate": 0.00010846951602765555,
      "loss": 1.0176,
      "step": 32620
    },
    {
      "epoch": 1.9189737557891642,
      "grad_norm": 0.40110695362091064,
      "learning_rate": 0.00010841059082338151,
      "loss": 1.0118,
      "step": 32630
    },
    {
      "epoch": 1.9195618613541132,
      "grad_norm": 0.3739053010940552,
      "learning_rate": 0.00010835166561910747,
      "loss": 1.128,
      "step": 32640
    },
    {
      "epoch": 1.920149966919062,
      "grad_norm": 0.39786016941070557,
      "learning_rate": 0.00010829274041483343,
      "loss": 1.0175,
      "step": 32650
    },
    {
      "epoch": 1.9207380724840108,
      "grad_norm": 0.3541963994503021,
      "learning_rate": 0.00010823381521055938,
      "loss": 1.087,
      "step": 32660
    },
    {
      "epoch": 1.9213261780489597,
      "grad_norm": 0.3529263138771057,
      "learning_rate": 0.00010817489000628534,
      "loss": 1.0564,
      "step": 32670
    },
    {
      "epoch": 1.9219142836139087,
      "grad_norm": 0.3681371510028839,
      "learning_rate": 0.0001081159648020113,
      "loss": 1.0429,
      "step": 32680
    },
    {
      "epoch": 1.9225023891788577,
      "grad_norm": 0.3518370985984802,
      "learning_rate": 0.00010805703959773727,
      "loss": 0.9329,
      "step": 32690
    },
    {
      "epoch": 1.9230904947438066,
      "grad_norm": 0.424062579870224,
      "learning_rate": 0.00010799811439346323,
      "loss": 0.9027,
      "step": 32700
    },
    {
      "epoch": 1.9236786003087554,
      "grad_norm": 0.3399587869644165,
      "learning_rate": 0.00010793918918918919,
      "loss": 1.0589,
      "step": 32710
    },
    {
      "epoch": 1.9242667058737042,
      "grad_norm": 0.3594941198825836,
      "learning_rate": 0.00010788026398491515,
      "loss": 0.9184,
      "step": 32720
    },
    {
      "epoch": 1.9248548114386532,
      "grad_norm": 0.37775057554244995,
      "learning_rate": 0.0001078213387806411,
      "loss": 1.0277,
      "step": 32730
    },
    {
      "epoch": 1.9254429170036023,
      "grad_norm": 0.36330562829971313,
      "learning_rate": 0.00010776241357636706,
      "loss": 1.0012,
      "step": 32740
    },
    {
      "epoch": 1.926031022568551,
      "grad_norm": 0.335817813873291,
      "learning_rate": 0.00010770348837209302,
      "loss": 1.0371,
      "step": 32750
    },
    {
      "epoch": 1.9266191281335,
      "grad_norm": 0.35850048065185547,
      "learning_rate": 0.00010764456316781897,
      "loss": 1.0265,
      "step": 32760
    },
    {
      "epoch": 1.9272072336984487,
      "grad_norm": 0.385446697473526,
      "learning_rate": 0.00010758563796354493,
      "loss": 0.9942,
      "step": 32770
    },
    {
      "epoch": 1.9277953392633977,
      "grad_norm": 0.3542042076587677,
      "learning_rate": 0.00010752671275927089,
      "loss": 1.0601,
      "step": 32780
    },
    {
      "epoch": 1.9283834448283468,
      "grad_norm": 0.40623849630355835,
      "learning_rate": 0.00010746778755499684,
      "loss": 0.9658,
      "step": 32790
    },
    {
      "epoch": 1.9289715503932956,
      "grad_norm": 0.46784359216690063,
      "learning_rate": 0.0001074088623507228,
      "loss": 0.9879,
      "step": 32800
    },
    {
      "epoch": 1.9295596559582444,
      "grad_norm": 0.32169264554977417,
      "learning_rate": 0.00010734993714644876,
      "loss": 0.8326,
      "step": 32810
    },
    {
      "epoch": 1.9301477615231935,
      "grad_norm": 0.37087905406951904,
      "learning_rate": 0.00010729101194217472,
      "loss": 1.0188,
      "step": 32820
    },
    {
      "epoch": 1.9307358670881425,
      "grad_norm": 0.39277052879333496,
      "learning_rate": 0.00010723208673790068,
      "loss": 0.9988,
      "step": 32830
    },
    {
      "epoch": 1.9313239726530913,
      "grad_norm": 0.3957362174987793,
      "learning_rate": 0.00010717316153362664,
      "loss": 1.0897,
      "step": 32840
    },
    {
      "epoch": 1.9319120782180401,
      "grad_norm": 0.33966901898384094,
      "learning_rate": 0.0001071142363293526,
      "loss": 0.9747,
      "step": 32850
    },
    {
      "epoch": 1.932500183782989,
      "grad_norm": 0.3753831088542938,
      "learning_rate": 0.00010705531112507855,
      "loss": 1.0115,
      "step": 32860
    },
    {
      "epoch": 1.933088289347938,
      "grad_norm": 0.3645501136779785,
      "learning_rate": 0.00010699638592080451,
      "loss": 0.9721,
      "step": 32870
    },
    {
      "epoch": 1.933676394912887,
      "grad_norm": 0.33812716603279114,
      "learning_rate": 0.00010693746071653047,
      "loss": 0.9764,
      "step": 32880
    },
    {
      "epoch": 1.9342645004778358,
      "grad_norm": 0.3990471363067627,
      "learning_rate": 0.00010687853551225644,
      "loss": 1.1656,
      "step": 32890
    },
    {
      "epoch": 1.9348526060427846,
      "grad_norm": 0.3661632537841797,
      "learning_rate": 0.0001068196103079824,
      "loss": 0.9868,
      "step": 32900
    },
    {
      "epoch": 1.9354407116077335,
      "grad_norm": 0.3547300398349762,
      "learning_rate": 0.00010676068510370836,
      "loss": 1.0194,
      "step": 32910
    },
    {
      "epoch": 1.9360288171726825,
      "grad_norm": 0.36668238043785095,
      "learning_rate": 0.00010670175989943432,
      "loss": 1.0077,
      "step": 32920
    },
    {
      "epoch": 1.9366169227376315,
      "grad_norm": 0.3740617632865906,
      "learning_rate": 0.00010664283469516027,
      "loss": 1.0273,
      "step": 32930
    },
    {
      "epoch": 1.9372050283025803,
      "grad_norm": 0.34308111667633057,
      "learning_rate": 0.00010658390949088623,
      "loss": 1.0102,
      "step": 32940
    },
    {
      "epoch": 1.9377931338675292,
      "grad_norm": 0.3250194191932678,
      "learning_rate": 0.00010652498428661219,
      "loss": 0.974,
      "step": 32950
    },
    {
      "epoch": 1.938381239432478,
      "grad_norm": 0.4140893518924713,
      "learning_rate": 0.00010646605908233815,
      "loss": 0.9782,
      "step": 32960
    },
    {
      "epoch": 1.938969344997427,
      "grad_norm": 0.35518184304237366,
      "learning_rate": 0.0001064071338780641,
      "loss": 1.0217,
      "step": 32970
    },
    {
      "epoch": 1.939557450562376,
      "grad_norm": 0.33138808608055115,
      "learning_rate": 0.00010634820867379006,
      "loss": 1.0487,
      "step": 32980
    },
    {
      "epoch": 1.9401455561273249,
      "grad_norm": 0.39406847953796387,
      "learning_rate": 0.000106289283469516,
      "loss": 0.8948,
      "step": 32990
    },
    {
      "epoch": 1.9407336616922737,
      "grad_norm": 0.31414517760276794,
      "learning_rate": 0.00010623035826524197,
      "loss": 1.1273,
      "step": 33000
    },
    {
      "epoch": 1.9413217672572227,
      "grad_norm": 0.37826472520828247,
      "learning_rate": 0.00010617143306096793,
      "loss": 0.9934,
      "step": 33010
    },
    {
      "epoch": 1.9419098728221715,
      "grad_norm": 0.3693677484989166,
      "learning_rate": 0.00010611250785669389,
      "loss": 0.9551,
      "step": 33020
    },
    {
      "epoch": 1.9424979783871206,
      "grad_norm": 0.3316132426261902,
      "learning_rate": 0.00010605358265241985,
      "loss": 1.0623,
      "step": 33030
    },
    {
      "epoch": 1.9430860839520694,
      "grad_norm": 0.3447326123714447,
      "learning_rate": 0.00010599465744814581,
      "loss": 0.9679,
      "step": 33040
    },
    {
      "epoch": 1.9436741895170182,
      "grad_norm": 0.3291204571723938,
      "learning_rate": 0.00010593573224387177,
      "loss": 0.9327,
      "step": 33050
    },
    {
      "epoch": 1.9442622950819672,
      "grad_norm": 0.30126145482063293,
      "learning_rate": 0.00010587680703959772,
      "loss": 0.9593,
      "step": 33060
    },
    {
      "epoch": 1.9448504006469163,
      "grad_norm": 0.3559046983718872,
      "learning_rate": 0.00010581788183532368,
      "loss": 0.9664,
      "step": 33070
    },
    {
      "epoch": 1.945438506211865,
      "grad_norm": 0.37059682607650757,
      "learning_rate": 0.00010575895663104964,
      "loss": 1.0206,
      "step": 33080
    },
    {
      "epoch": 1.946026611776814,
      "grad_norm": 0.33312058448791504,
      "learning_rate": 0.0001057000314267756,
      "loss": 0.948,
      "step": 33090
    },
    {
      "epoch": 1.9466147173417627,
      "grad_norm": 0.35304534435272217,
      "learning_rate": 0.00010564110622250157,
      "loss": 1.0472,
      "step": 33100
    },
    {
      "epoch": 1.9472028229067118,
      "grad_norm": 0.3869187831878662,
      "learning_rate": 0.00010558218101822753,
      "loss": 1.0994,
      "step": 33110
    },
    {
      "epoch": 1.9477909284716608,
      "grad_norm": 0.38560229539871216,
      "learning_rate": 0.00010552325581395349,
      "loss": 1.096,
      "step": 33120
    },
    {
      "epoch": 1.9483790340366096,
      "grad_norm": 0.38177308440208435,
      "learning_rate": 0.00010546433060967944,
      "loss": 0.978,
      "step": 33130
    },
    {
      "epoch": 1.9489671396015584,
      "grad_norm": 0.3689734935760498,
      "learning_rate": 0.0001054054054054054,
      "loss": 0.9365,
      "step": 33140
    },
    {
      "epoch": 1.9495552451665072,
      "grad_norm": 0.33013299107551575,
      "learning_rate": 0.00010534648020113136,
      "loss": 1.067,
      "step": 33150
    },
    {
      "epoch": 1.9501433507314563,
      "grad_norm": 0.33026403188705444,
      "learning_rate": 0.00010528755499685732,
      "loss": 1.0433,
      "step": 33160
    },
    {
      "epoch": 1.9507314562964053,
      "grad_norm": 0.33772385120391846,
      "learning_rate": 0.00010522862979258328,
      "loss": 1.0513,
      "step": 33170
    },
    {
      "epoch": 1.9513195618613541,
      "grad_norm": 0.37204572558403015,
      "learning_rate": 0.00010516970458830923,
      "loss": 1.0775,
      "step": 33180
    },
    {
      "epoch": 1.951907667426303,
      "grad_norm": 0.3756186366081238,
      "learning_rate": 0.00010511077938403518,
      "loss": 0.9686,
      "step": 33190
    },
    {
      "epoch": 1.9524957729912518,
      "grad_norm": 0.34491974115371704,
      "learning_rate": 0.00010505185417976114,
      "loss": 1.0673,
      "step": 33200
    },
    {
      "epoch": 1.9530838785562008,
      "grad_norm": 0.30318138003349304,
      "learning_rate": 0.0001049929289754871,
      "loss": 1.0796,
      "step": 33210
    },
    {
      "epoch": 1.9536719841211498,
      "grad_norm": 0.4037652313709259,
      "learning_rate": 0.00010493400377121306,
      "loss": 1.0276,
      "step": 33220
    },
    {
      "epoch": 1.9542600896860987,
      "grad_norm": 0.3582938611507416,
      "learning_rate": 0.00010487507856693902,
      "loss": 1.0146,
      "step": 33230
    },
    {
      "epoch": 1.9548481952510475,
      "grad_norm": 0.34054237604141235,
      "learning_rate": 0.00010481615336266498,
      "loss": 0.9884,
      "step": 33240
    },
    {
      "epoch": 1.9554363008159965,
      "grad_norm": 0.40794992446899414,
      "learning_rate": 0.00010475722815839094,
      "loss": 0.9635,
      "step": 33250
    },
    {
      "epoch": 1.9560244063809455,
      "grad_norm": 0.3648630678653717,
      "learning_rate": 0.00010469830295411689,
      "loss": 0.9922,
      "step": 33260
    },
    {
      "epoch": 1.9566125119458944,
      "grad_norm": 0.37382903695106506,
      "learning_rate": 0.00010463937774984285,
      "loss": 1.1285,
      "step": 33270
    },
    {
      "epoch": 1.9572006175108432,
      "grad_norm": 0.37472257018089294,
      "learning_rate": 0.00010458045254556881,
      "loss": 1.0713,
      "step": 33280
    },
    {
      "epoch": 1.957788723075792,
      "grad_norm": 0.36330968141555786,
      "learning_rate": 0.00010452152734129477,
      "loss": 0.9835,
      "step": 33290
    },
    {
      "epoch": 1.958376828640741,
      "grad_norm": 0.3286409378051758,
      "learning_rate": 0.00010446260213702074,
      "loss": 0.9851,
      "step": 33300
    },
    {
      "epoch": 1.95896493420569,
      "grad_norm": 0.4251982271671295,
      "learning_rate": 0.0001044036769327467,
      "loss": 1.1016,
      "step": 33310
    },
    {
      "epoch": 1.9595530397706389,
      "grad_norm": 0.3532177805900574,
      "learning_rate": 0.00010434475172847266,
      "loss": 1.0407,
      "step": 33320
    },
    {
      "epoch": 1.9601411453355877,
      "grad_norm": 0.3674749433994293,
      "learning_rate": 0.0001042858265241986,
      "loss": 0.9828,
      "step": 33330
    },
    {
      "epoch": 1.9607292509005365,
      "grad_norm": 0.311917781829834,
      "learning_rate": 0.00010422690131992457,
      "loss": 1.0497,
      "step": 33340
    },
    {
      "epoch": 1.9613173564654856,
      "grad_norm": 0.31791409850120544,
      "learning_rate": 0.00010416797611565053,
      "loss": 0.9942,
      "step": 33350
    },
    {
      "epoch": 1.9619054620304346,
      "grad_norm": 0.3726382255554199,
      "learning_rate": 0.00010410905091137649,
      "loss": 1.0657,
      "step": 33360
    },
    {
      "epoch": 1.9624935675953834,
      "grad_norm": 0.3534354269504547,
      "learning_rate": 0.00010405012570710245,
      "loss": 1.0415,
      "step": 33370
    },
    {
      "epoch": 1.9630816731603322,
      "grad_norm": 0.4234212338924408,
      "learning_rate": 0.00010399120050282841,
      "loss": 0.9337,
      "step": 33380
    },
    {
      "epoch": 1.963669778725281,
      "grad_norm": 0.3643425703048706,
      "learning_rate": 0.00010393227529855435,
      "loss": 1.0118,
      "step": 33390
    },
    {
      "epoch": 1.96425788429023,
      "grad_norm": 0.37721702456474304,
      "learning_rate": 0.00010387335009428031,
      "loss": 1.077,
      "step": 33400
    },
    {
      "epoch": 1.964845989855179,
      "grad_norm": 0.3625536262989044,
      "learning_rate": 0.00010381442489000627,
      "loss": 1.1302,
      "step": 33410
    },
    {
      "epoch": 1.965434095420128,
      "grad_norm": 0.3670025169849396,
      "learning_rate": 0.00010375549968573223,
      "loss": 1.0027,
      "step": 33420
    },
    {
      "epoch": 1.9660222009850767,
      "grad_norm": 0.35115018486976624,
      "learning_rate": 0.00010369657448145819,
      "loss": 0.9721,
      "step": 33430
    },
    {
      "epoch": 1.9666103065500258,
      "grad_norm": 0.36050528287887573,
      "learning_rate": 0.00010363764927718415,
      "loss": 1.0286,
      "step": 33440
    },
    {
      "epoch": 1.9671984121149746,
      "grad_norm": 0.34673479199409485,
      "learning_rate": 0.00010357872407291011,
      "loss": 0.9264,
      "step": 33450
    },
    {
      "epoch": 1.9677865176799236,
      "grad_norm": 0.3713052570819855,
      "learning_rate": 0.00010351979886863606,
      "loss": 1.0948,
      "step": 33460
    },
    {
      "epoch": 1.9683746232448724,
      "grad_norm": 0.4002484679222107,
      "learning_rate": 0.00010346087366436202,
      "loss": 1.0608,
      "step": 33470
    },
    {
      "epoch": 1.9689627288098213,
      "grad_norm": 0.35185906291007996,
      "learning_rate": 0.00010340194846008798,
      "loss": 0.9432,
      "step": 33480
    },
    {
      "epoch": 1.9695508343747703,
      "grad_norm": 0.3978027105331421,
      "learning_rate": 0.00010334302325581394,
      "loss": 0.9611,
      "step": 33490
    },
    {
      "epoch": 1.9701389399397193,
      "grad_norm": 0.34133845567703247,
      "learning_rate": 0.0001032840980515399,
      "loss": 0.983,
      "step": 33500
    },
    {
      "epoch": 1.9707270455046682,
      "grad_norm": 0.3662980794906616,
      "learning_rate": 0.00010322517284726587,
      "loss": 1.0581,
      "step": 33510
    },
    {
      "epoch": 1.971315151069617,
      "grad_norm": 0.3727848529815674,
      "learning_rate": 0.00010316624764299183,
      "loss": 1.071,
      "step": 33520
    },
    {
      "epoch": 1.9719032566345658,
      "grad_norm": 0.3724727928638458,
      "learning_rate": 0.00010310732243871778,
      "loss": 1.0782,
      "step": 33530
    },
    {
      "epoch": 1.9724913621995148,
      "grad_norm": 0.36292871832847595,
      "learning_rate": 0.00010304839723444374,
      "loss": 1.049,
      "step": 33540
    },
    {
      "epoch": 1.9730794677644639,
      "grad_norm": 0.34051358699798584,
      "learning_rate": 0.0001029894720301697,
      "loss": 0.9447,
      "step": 33550
    },
    {
      "epoch": 1.9736675733294127,
      "grad_norm": 0.33960816264152527,
      "learning_rate": 0.00010293054682589566,
      "loss": 0.9946,
      "step": 33560
    },
    {
      "epoch": 1.9742556788943615,
      "grad_norm": 0.3455972373485565,
      "learning_rate": 0.00010287162162162162,
      "loss": 0.9631,
      "step": 33570
    },
    {
      "epoch": 1.9748437844593103,
      "grad_norm": 0.35228249430656433,
      "learning_rate": 0.00010281269641734758,
      "loss": 1.0054,
      "step": 33580
    },
    {
      "epoch": 1.9754318900242593,
      "grad_norm": 0.3348499536514282,
      "learning_rate": 0.00010275377121307352,
      "loss": 1.0226,
      "step": 33590
    },
    {
      "epoch": 1.9760199955892084,
      "grad_norm": 0.3649054765701294,
      "learning_rate": 0.00010269484600879948,
      "loss": 1.0312,
      "step": 33600
    },
    {
      "epoch": 1.9766081011541572,
      "grad_norm": 0.3688194751739502,
      "learning_rate": 0.00010263592080452544,
      "loss": 0.9826,
      "step": 33610
    },
    {
      "epoch": 1.977196206719106,
      "grad_norm": 0.3897623121738434,
      "learning_rate": 0.0001025769956002514,
      "loss": 0.9942,
      "step": 33620
    },
    {
      "epoch": 1.9777843122840548,
      "grad_norm": 0.3623385429382324,
      "learning_rate": 0.00010251807039597736,
      "loss": 1.0513,
      "step": 33630
    },
    {
      "epoch": 1.9783724178490039,
      "grad_norm": 0.3345963954925537,
      "learning_rate": 0.00010245914519170332,
      "loss": 0.9852,
      "step": 33640
    },
    {
      "epoch": 1.978960523413953,
      "grad_norm": 0.377872109413147,
      "learning_rate": 0.00010240021998742928,
      "loss": 1.0537,
      "step": 33650
    },
    {
      "epoch": 1.9795486289789017,
      "grad_norm": 0.37553995847702026,
      "learning_rate": 0.00010234129478315523,
      "loss": 1.0538,
      "step": 33660
    },
    {
      "epoch": 1.9801367345438505,
      "grad_norm": 0.46441733837127686,
      "learning_rate": 0.00010228236957888119,
      "loss": 1.0906,
      "step": 33670
    },
    {
      "epoch": 1.9807248401087996,
      "grad_norm": 0.36810532212257385,
      "learning_rate": 0.00010222344437460715,
      "loss": 0.9571,
      "step": 33680
    },
    {
      "epoch": 1.9813129456737484,
      "grad_norm": 0.37654468417167664,
      "learning_rate": 0.00010216451917033311,
      "loss": 1.0276,
      "step": 33690
    },
    {
      "epoch": 1.9819010512386974,
      "grad_norm": 0.37268340587615967,
      "learning_rate": 0.00010210559396605908,
      "loss": 1.0514,
      "step": 33700
    },
    {
      "epoch": 1.9824891568036462,
      "grad_norm": 0.4117443859577179,
      "learning_rate": 0.00010204666876178504,
      "loss": 1.0584,
      "step": 33710
    },
    {
      "epoch": 1.983077262368595,
      "grad_norm": 0.34904465079307556,
      "learning_rate": 0.000101987743557511,
      "loss": 1.014,
      "step": 33720
    },
    {
      "epoch": 1.983665367933544,
      "grad_norm": 0.32960575819015503,
      "learning_rate": 0.00010192881835323695,
      "loss": 1.0606,
      "step": 33730
    },
    {
      "epoch": 1.9842534734984931,
      "grad_norm": 0.30822500586509705,
      "learning_rate": 0.0001018698931489629,
      "loss": 0.9892,
      "step": 33740
    },
    {
      "epoch": 1.984841579063442,
      "grad_norm": 0.37562811374664307,
      "learning_rate": 0.00010181096794468887,
      "loss": 1.0271,
      "step": 33750
    },
    {
      "epoch": 1.9854296846283908,
      "grad_norm": 0.38176172971725464,
      "learning_rate": 0.00010175204274041483,
      "loss": 0.9935,
      "step": 33760
    },
    {
      "epoch": 1.9860177901933396,
      "grad_norm": 0.367824912071228,
      "learning_rate": 0.00010169311753614079,
      "loss": 1.0748,
      "step": 33770
    },
    {
      "epoch": 1.9866058957582886,
      "grad_norm": 0.48549023270606995,
      "learning_rate": 0.00010163419233186675,
      "loss": 1.0437,
      "step": 33780
    },
    {
      "epoch": 1.9871940013232376,
      "grad_norm": 0.347505122423172,
      "learning_rate": 0.00010157526712759271,
      "loss": 1.0927,
      "step": 33790
    },
    {
      "epoch": 1.9877821068881865,
      "grad_norm": 0.3798636198043823,
      "learning_rate": 0.00010151634192331865,
      "loss": 0.9518,
      "step": 33800
    },
    {
      "epoch": 1.9883702124531353,
      "grad_norm": 0.39210355281829834,
      "learning_rate": 0.00010145741671904461,
      "loss": 0.8879,
      "step": 33810
    },
    {
      "epoch": 1.988958318018084,
      "grad_norm": 0.3745344579219818,
      "learning_rate": 0.00010139849151477057,
      "loss": 1.0472,
      "step": 33820
    },
    {
      "epoch": 1.9895464235830331,
      "grad_norm": 0.26152753829956055,
      "learning_rate": 0.00010133956631049653,
      "loss": 0.9932,
      "step": 33830
    },
    {
      "epoch": 1.9901345291479822,
      "grad_norm": 0.3217948079109192,
      "learning_rate": 0.00010128064110622249,
      "loss": 1.0501,
      "step": 33840
    },
    {
      "epoch": 1.990722634712931,
      "grad_norm": 0.38941720128059387,
      "learning_rate": 0.00010122171590194845,
      "loss": 1.0474,
      "step": 33850
    },
    {
      "epoch": 1.9913107402778798,
      "grad_norm": 0.3659362494945526,
      "learning_rate": 0.0001011627906976744,
      "loss": 1.058,
      "step": 33860
    },
    {
      "epoch": 1.9918988458428288,
      "grad_norm": 0.32248786091804504,
      "learning_rate": 0.00010110386549340036,
      "loss": 1.0428,
      "step": 33870
    },
    {
      "epoch": 1.9924869514077777,
      "grad_norm": 0.35924193263053894,
      "learning_rate": 0.00010104494028912632,
      "loss": 1.0514,
      "step": 33880
    },
    {
      "epoch": 1.9930750569727267,
      "grad_norm": 0.3667766749858856,
      "learning_rate": 0.00010098601508485228,
      "loss": 0.9652,
      "step": 33890
    },
    {
      "epoch": 1.9936631625376755,
      "grad_norm": 0.342854380607605,
      "learning_rate": 0.00010092708988057825,
      "loss": 1.0959,
      "step": 33900
    },
    {
      "epoch": 1.9942512681026243,
      "grad_norm": 0.3237900137901306,
      "learning_rate": 0.0001008681646763042,
      "loss": 0.9574,
      "step": 33910
    },
    {
      "epoch": 1.9948393736675734,
      "grad_norm": 0.3422260284423828,
      "learning_rate": 0.00010080923947203017,
      "loss": 1.0198,
      "step": 33920
    },
    {
      "epoch": 1.9954274792325224,
      "grad_norm": 0.38810521364212036,
      "learning_rate": 0.00010075031426775612,
      "loss": 1.0371,
      "step": 33930
    },
    {
      "epoch": 1.9960155847974712,
      "grad_norm": 0.38556426763534546,
      "learning_rate": 0.00010069138906348208,
      "loss": 1.0684,
      "step": 33940
    },
    {
      "epoch": 1.99660369036242,
      "grad_norm": 0.32180947065353394,
      "learning_rate": 0.00010063246385920804,
      "loss": 1.0117,
      "step": 33950
    },
    {
      "epoch": 1.9971917959273688,
      "grad_norm": 0.3722313642501831,
      "learning_rate": 0.000100573538654934,
      "loss": 0.9521,
      "step": 33960
    },
    {
      "epoch": 1.9977799014923179,
      "grad_norm": 0.3496752083301544,
      "learning_rate": 0.00010051461345065996,
      "loss": 1.0287,
      "step": 33970
    },
    {
      "epoch": 1.998368007057267,
      "grad_norm": 0.3233623802661896,
      "learning_rate": 0.00010045568824638592,
      "loss": 1.004,
      "step": 33980
    },
    {
      "epoch": 1.9989561126222157,
      "grad_norm": 0.35751473903656006,
      "learning_rate": 0.00010039676304211188,
      "loss": 1.056,
      "step": 33990
    },
    {
      "epoch": 1.9995442181871645,
      "grad_norm": 0.38050079345703125,
      "learning_rate": 0.00010033783783783783,
      "loss": 1.0309,
      "step": 34000
    },
    {
      "epoch": 2.00011762111299,
      "grad_norm": 0.3361454904079437,
      "learning_rate": 0.00010027891263356378,
      "loss": 0.9604,
      "step": 34010
    },
    {
      "epoch": 2.0007057266779387,
      "grad_norm": 0.3769259750843048,
      "learning_rate": 0.00010021998742928974,
      "loss": 1.0506,
      "step": 34020
    },
    {
      "epoch": 2.0012938322428875,
      "grad_norm": 0.37119948863983154,
      "learning_rate": 0.0001001610622250157,
      "loss": 1.0017,
      "step": 34030
    },
    {
      "epoch": 2.0018819378078363,
      "grad_norm": 0.3850937783718109,
      "learning_rate": 0.00010010213702074166,
      "loss": 1.0544,
      "step": 34040
    },
    {
      "epoch": 2.0024700433727856,
      "grad_norm": 0.33740562200546265,
      "learning_rate": 0.00010004321181646762,
      "loss": 0.9633,
      "step": 34050
    },
    {
      "epoch": 2.0030581489377344,
      "grad_norm": NaN,
      "learning_rate": 9.998428661219357e-05,
      "loss": 1.0683,
      "step": 34060
    },
    {
      "epoch": 2.003646254502683,
      "grad_norm": 0.39689338207244873,
      "learning_rate": 9.993125392834695e-05,
      "loss": 1.012,
      "step": 34070
    },
    {
      "epoch": 2.004234360067632,
      "grad_norm": 0.36519941687583923,
      "learning_rate": 9.987232872407291e-05,
      "loss": 1.0651,
      "step": 34080
    },
    {
      "epoch": 2.0048224656325813,
      "grad_norm": 0.3904041051864624,
      "learning_rate": 9.981340351979886e-05,
      "loss": 1.0435,
      "step": 34090
    },
    {
      "epoch": 2.00541057119753,
      "grad_norm": 0.35035791993141174,
      "learning_rate": 9.975447831552481e-05,
      "loss": 0.9926,
      "step": 34100
    },
    {
      "epoch": 2.005998676762479,
      "grad_norm": 0.35680708289146423,
      "learning_rate": 9.969555311125077e-05,
      "loss": 1.0291,
      "step": 34110
    },
    {
      "epoch": 2.0065867823274277,
      "grad_norm": 0.34832754731178284,
      "learning_rate": 9.963662790697673e-05,
      "loss": 1.0153,
      "step": 34120
    },
    {
      "epoch": 2.0071748878923765,
      "grad_norm": 0.3804228901863098,
      "learning_rate": 9.957770270270269e-05,
      "loss": 0.946,
      "step": 34130
    },
    {
      "epoch": 2.007762993457326,
      "grad_norm": 0.3817605674266815,
      "learning_rate": 9.951877749842865e-05,
      "loss": 1.0941,
      "step": 34140
    },
    {
      "epoch": 2.0083510990222746,
      "grad_norm": 0.3580937683582306,
      "learning_rate": 9.94598522941546e-05,
      "loss": 0.9313,
      "step": 34150
    },
    {
      "epoch": 2.0089392045872234,
      "grad_norm": 0.3141365945339203,
      "learning_rate": 9.940092708988056e-05,
      "loss": 1.0001,
      "step": 34160
    },
    {
      "epoch": 2.0095273101521722,
      "grad_norm": 0.3585048317909241,
      "learning_rate": 9.934200188560652e-05,
      "loss": 0.9886,
      "step": 34170
    },
    {
      "epoch": 2.010115415717121,
      "grad_norm": 0.42648518085479736,
      "learning_rate": 9.928307668133249e-05,
      "loss": 0.9899,
      "step": 34180
    },
    {
      "epoch": 2.0107035212820703,
      "grad_norm": 0.3040113151073456,
      "learning_rate": 9.922415147705845e-05,
      "loss": 0.9955,
      "step": 34190
    },
    {
      "epoch": 2.011291626847019,
      "grad_norm": 0.39523833990097046,
      "learning_rate": 9.916522627278441e-05,
      "loss": 0.9036,
      "step": 34200
    },
    {
      "epoch": 2.011879732411968,
      "grad_norm": 0.37070053815841675,
      "learning_rate": 9.910630106851037e-05,
      "loss": 1.0014,
      "step": 34210
    },
    {
      "epoch": 2.0124678379769168,
      "grad_norm": 0.3350638747215271,
      "learning_rate": 9.904737586423632e-05,
      "loss": 1.0205,
      "step": 34220
    },
    {
      "epoch": 2.0130559435418656,
      "grad_norm": 0.38820019364356995,
      "learning_rate": 9.898845065996228e-05,
      "loss": 0.9935,
      "step": 34230
    },
    {
      "epoch": 2.013644049106815,
      "grad_norm": 0.39090269804000854,
      "learning_rate": 9.892952545568824e-05,
      "loss": 0.9315,
      "step": 34240
    },
    {
      "epoch": 2.0142321546717636,
      "grad_norm": 0.3632262945175171,
      "learning_rate": 9.88706002514142e-05,
      "loss": 1.117,
      "step": 34250
    },
    {
      "epoch": 2.0148202602367125,
      "grad_norm": 0.34314897656440735,
      "learning_rate": 9.881167504714016e-05,
      "loss": 1.0087,
      "step": 34260
    },
    {
      "epoch": 2.0154083658016613,
      "grad_norm": 0.3213823735713959,
      "learning_rate": 9.875274984286612e-05,
      "loss": 0.9661,
      "step": 34270
    },
    {
      "epoch": 2.01599647136661,
      "grad_norm": 0.44598665833473206,
      "learning_rate": 9.869382463859208e-05,
      "loss": 1.0258,
      "step": 34280
    },
    {
      "epoch": 2.0165845769315593,
      "grad_norm": 0.32593318819999695,
      "learning_rate": 9.863489943431803e-05,
      "loss": 1.0054,
      "step": 34290
    },
    {
      "epoch": 2.017172682496508,
      "grad_norm": 0.392015278339386,
      "learning_rate": 9.857597423004399e-05,
      "loss": 1.0627,
      "step": 34300
    },
    {
      "epoch": 2.017760788061457,
      "grad_norm": 0.3427535593509674,
      "learning_rate": 9.851704902576994e-05,
      "loss": 0.9547,
      "step": 34310
    },
    {
      "epoch": 2.018348893626406,
      "grad_norm": 0.3630708158016205,
      "learning_rate": 9.84581238214959e-05,
      "loss": 1.0089,
      "step": 34320
    },
    {
      "epoch": 2.018936999191355,
      "grad_norm": 0.38931331038475037,
      "learning_rate": 9.839919861722186e-05,
      "loss": 0.9071,
      "step": 34330
    },
    {
      "epoch": 2.019525104756304,
      "grad_norm": 0.38720113039016724,
      "learning_rate": 9.834027341294782e-05,
      "loss": 0.9786,
      "step": 34340
    },
    {
      "epoch": 2.0201132103212527,
      "grad_norm": 0.32967910170555115,
      "learning_rate": 9.828134820867377e-05,
      "loss": 1.0294,
      "step": 34350
    },
    {
      "epoch": 2.0207013158862015,
      "grad_norm": 0.33400431275367737,
      "learning_rate": 9.822242300439973e-05,
      "loss": 0.9398,
      "step": 34360
    },
    {
      "epoch": 2.0212894214511503,
      "grad_norm": 0.3990192413330078,
      "learning_rate": 9.81634978001257e-05,
      "loss": 1.0168,
      "step": 34370
    },
    {
      "epoch": 2.0218775270160996,
      "grad_norm": 0.35930368304252625,
      "learning_rate": 9.810457259585166e-05,
      "loss": 0.9739,
      "step": 34380
    },
    {
      "epoch": 2.0224656325810484,
      "grad_norm": 0.3728949725627899,
      "learning_rate": 9.804564739157762e-05,
      "loss": 1.013,
      "step": 34390
    },
    {
      "epoch": 2.023053738145997,
      "grad_norm": 0.3366044759750366,
      "learning_rate": 9.798672218730358e-05,
      "loss": 0.9411,
      "step": 34400
    },
    {
      "epoch": 2.023641843710946,
      "grad_norm": 0.39782264828681946,
      "learning_rate": 9.792779698302954e-05,
      "loss": 0.9165,
      "step": 34410
    },
    {
      "epoch": 2.024229949275895,
      "grad_norm": 0.3772971034049988,
      "learning_rate": 9.786887177875549e-05,
      "loss": 0.9038,
      "step": 34420
    },
    {
      "epoch": 2.024818054840844,
      "grad_norm": 0.3971233069896698,
      "learning_rate": 9.780994657448145e-05,
      "loss": 1.0028,
      "step": 34430
    },
    {
      "epoch": 2.025406160405793,
      "grad_norm": 0.35393571853637695,
      "learning_rate": 9.775102137020741e-05,
      "loss": 0.9168,
      "step": 34440
    },
    {
      "epoch": 2.0259942659707417,
      "grad_norm": 0.37734928727149963,
      "learning_rate": 9.769209616593337e-05,
      "loss": 0.9716,
      "step": 34450
    },
    {
      "epoch": 2.0265823715356905,
      "grad_norm": 0.39811474084854126,
      "learning_rate": 9.763317096165933e-05,
      "loss": 1.006,
      "step": 34460
    },
    {
      "epoch": 2.0271704771006394,
      "grad_norm": 0.3725973963737488,
      "learning_rate": 9.757424575738529e-05,
      "loss": 0.9694,
      "step": 34470
    },
    {
      "epoch": 2.0277585826655886,
      "grad_norm": 0.35304972529411316,
      "learning_rate": 9.751532055311125e-05,
      "loss": 0.9927,
      "step": 34480
    },
    {
      "epoch": 2.0283466882305374,
      "grad_norm": 0.4361490309238434,
      "learning_rate": 9.74563953488372e-05,
      "loss": 0.9779,
      "step": 34490
    },
    {
      "epoch": 2.0289347937954862,
      "grad_norm": 0.4376175105571747,
      "learning_rate": 9.739747014456316e-05,
      "loss": 0.9984,
      "step": 34500
    },
    {
      "epoch": 2.029522899360435,
      "grad_norm": 0.34551894664764404,
      "learning_rate": 9.733854494028912e-05,
      "loss": 1.0217,
      "step": 34510
    },
    {
      "epoch": 2.0301110049253843,
      "grad_norm": 0.37025895714759827,
      "learning_rate": 9.727961973601507e-05,
      "loss": 0.9129,
      "step": 34520
    },
    {
      "epoch": 2.030699110490333,
      "grad_norm": 0.34446612000465393,
      "learning_rate": 9.722069453174103e-05,
      "loss": 1.0107,
      "step": 34530
    },
    {
      "epoch": 2.031287216055282,
      "grad_norm": 0.3637993037700653,
      "learning_rate": 9.7161769327467e-05,
      "loss": 0.9731,
      "step": 34540
    },
    {
      "epoch": 2.0318753216202308,
      "grad_norm": 0.3550746440887451,
      "learning_rate": 9.710284412319294e-05,
      "loss": 0.8958,
      "step": 34550
    },
    {
      "epoch": 2.0324634271851796,
      "grad_norm": 0.3675632178783417,
      "learning_rate": 9.70439189189189e-05,
      "loss": 0.9894,
      "step": 34560
    },
    {
      "epoch": 2.033051532750129,
      "grad_norm": 0.3394962251186371,
      "learning_rate": 9.698499371464486e-05,
      "loss": 0.9547,
      "step": 34570
    },
    {
      "epoch": 2.0336396383150777,
      "grad_norm": 0.3836996257305145,
      "learning_rate": 9.692606851037082e-05,
      "loss": 0.9878,
      "step": 34580
    },
    {
      "epoch": 2.0342277438800265,
      "grad_norm": 0.3670666813850403,
      "learning_rate": 9.686714330609679e-05,
      "loss": 0.9394,
      "step": 34590
    },
    {
      "epoch": 2.0348158494449753,
      "grad_norm": 0.3433150351047516,
      "learning_rate": 9.680821810182275e-05,
      "loss": 0.988,
      "step": 34600
    },
    {
      "epoch": 2.035403955009924,
      "grad_norm": 0.3346148133277893,
      "learning_rate": 9.674929289754871e-05,
      "loss": 0.9577,
      "step": 34610
    },
    {
      "epoch": 2.0359920605748734,
      "grad_norm": 0.4002562463283539,
      "learning_rate": 9.669036769327466e-05,
      "loss": 0.9739,
      "step": 34620
    },
    {
      "epoch": 2.036580166139822,
      "grad_norm": 0.34057390689849854,
      "learning_rate": 9.663144248900062e-05,
      "loss": 1.0097,
      "step": 34630
    },
    {
      "epoch": 2.037168271704771,
      "grad_norm": 0.3600614368915558,
      "learning_rate": 9.657251728472658e-05,
      "loss": 0.965,
      "step": 34640
    },
    {
      "epoch": 2.03775637726972,
      "grad_norm": 0.40222570300102234,
      "learning_rate": 9.651359208045254e-05,
      "loss": 1.0217,
      "step": 34650
    },
    {
      "epoch": 2.0383444828346686,
      "grad_norm": 0.32726654410362244,
      "learning_rate": 9.64546668761785e-05,
      "loss": 0.9807,
      "step": 34660
    },
    {
      "epoch": 2.038932588399618,
      "grad_norm": 0.38607552647590637,
      "learning_rate": 9.639574167190446e-05,
      "loss": 1.0588,
      "step": 34670
    },
    {
      "epoch": 2.0395206939645667,
      "grad_norm": 0.3919824957847595,
      "learning_rate": 9.633681646763042e-05,
      "loss": 0.8589,
      "step": 34680
    },
    {
      "epoch": 2.0401087995295155,
      "grad_norm": 0.3383572995662689,
      "learning_rate": 9.627789126335637e-05,
      "loss": 1.0258,
      "step": 34690
    },
    {
      "epoch": 2.0406969050944643,
      "grad_norm": 0.34816235303878784,
      "learning_rate": 9.621896605908233e-05,
      "loss": 1.0889,
      "step": 34700
    },
    {
      "epoch": 2.041285010659413,
      "grad_norm": 0.36499854922294617,
      "learning_rate": 9.61600408548083e-05,
      "loss": 1.0095,
      "step": 34710
    },
    {
      "epoch": 2.0418731162243624,
      "grad_norm": 0.33535805344581604,
      "learning_rate": 9.610111565053425e-05,
      "loss": 1.0097,
      "step": 34720
    },
    {
      "epoch": 2.042461221789311,
      "grad_norm": 0.40166622400283813,
      "learning_rate": 9.60421904462602e-05,
      "loss": 1.0314,
      "step": 34730
    },
    {
      "epoch": 2.04304932735426,
      "grad_norm": 0.40951257944107056,
      "learning_rate": 9.598326524198616e-05,
      "loss": 1.0036,
      "step": 34740
    },
    {
      "epoch": 2.043637432919209,
      "grad_norm": 0.4190656542778015,
      "learning_rate": 9.592434003771211e-05,
      "loss": 1.0374,
      "step": 34750
    },
    {
      "epoch": 2.044225538484158,
      "grad_norm": 0.36874890327453613,
      "learning_rate": 9.586541483343807e-05,
      "loss": 0.9903,
      "step": 34760
    },
    {
      "epoch": 2.044813644049107,
      "grad_norm": 0.3441307544708252,
      "learning_rate": 9.580648962916403e-05,
      "loss": 0.9188,
      "step": 34770
    },
    {
      "epoch": 2.0454017496140557,
      "grad_norm": 0.40949299931526184,
      "learning_rate": 9.574756442489e-05,
      "loss": 0.9977,
      "step": 34780
    },
    {
      "epoch": 2.0459898551790046,
      "grad_norm": 0.3574064373970032,
      "learning_rate": 9.568863922061596e-05,
      "loss": 1.0225,
      "step": 34790
    },
    {
      "epoch": 2.0465779607439534,
      "grad_norm": 0.3300991654396057,
      "learning_rate": 9.562971401634192e-05,
      "loss": 0.9835,
      "step": 34800
    },
    {
      "epoch": 2.0471660663089026,
      "grad_norm": 0.35296630859375,
      "learning_rate": 9.557078881206788e-05,
      "loss": 0.9799,
      "step": 34810
    },
    {
      "epoch": 2.0477541718738514,
      "grad_norm": 0.36116254329681396,
      "learning_rate": 9.551186360779383e-05,
      "loss": 0.9944,
      "step": 34820
    },
    {
      "epoch": 2.0483422774388003,
      "grad_norm": 0.3782866597175598,
      "learning_rate": 9.545293840351979e-05,
      "loss": 0.9462,
      "step": 34830
    },
    {
      "epoch": 2.048930383003749,
      "grad_norm": 0.3876466155052185,
      "learning_rate": 9.539401319924575e-05,
      "loss": 1.0141,
      "step": 34840
    },
    {
      "epoch": 2.049518488568698,
      "grad_norm": 0.3654608726501465,
      "learning_rate": 9.533508799497171e-05,
      "loss": 0.942,
      "step": 34850
    },
    {
      "epoch": 2.050106594133647,
      "grad_norm": 0.38772913813591003,
      "learning_rate": 9.527616279069767e-05,
      "loss": 1.0298,
      "step": 34860
    },
    {
      "epoch": 2.050694699698596,
      "grad_norm": 0.37137362360954285,
      "learning_rate": 9.521723758642363e-05,
      "loss": 1.0124,
      "step": 34870
    },
    {
      "epoch": 2.051282805263545,
      "grad_norm": 0.32582053542137146,
      "learning_rate": 9.515831238214959e-05,
      "loss": 1.1374,
      "step": 34880
    },
    {
      "epoch": 2.0518709108284936,
      "grad_norm": 0.31844452023506165,
      "learning_rate": 9.509938717787554e-05,
      "loss": 0.9742,
      "step": 34890
    },
    {
      "epoch": 2.0524590163934424,
      "grad_norm": 0.39516645669937134,
      "learning_rate": 9.50404619736015e-05,
      "loss": 1.002,
      "step": 34900
    },
    {
      "epoch": 2.0530471219583917,
      "grad_norm": 0.3575883209705353,
      "learning_rate": 9.498153676932746e-05,
      "loss": 1.0073,
      "step": 34910
    },
    {
      "epoch": 2.0536352275233405,
      "grad_norm": 0.41051408648490906,
      "learning_rate": 9.492261156505342e-05,
      "loss": 0.886,
      "step": 34920
    },
    {
      "epoch": 2.0542233330882893,
      "grad_norm": 0.37685132026672363,
      "learning_rate": 9.486368636077939e-05,
      "loss": 1.1038,
      "step": 34930
    },
    {
      "epoch": 2.054811438653238,
      "grad_norm": 0.40449488162994385,
      "learning_rate": 9.480476115650533e-05,
      "loss": 0.9816,
      "step": 34940
    },
    {
      "epoch": 2.0553995442181874,
      "grad_norm": 0.394782155752182,
      "learning_rate": 9.474583595223128e-05,
      "loss": 0.8744,
      "step": 34950
    },
    {
      "epoch": 2.055987649783136,
      "grad_norm": 0.3432568907737732,
      "learning_rate": 9.468691074795724e-05,
      "loss": 1.098,
      "step": 34960
    },
    {
      "epoch": 2.056575755348085,
      "grad_norm": 0.3296269178390503,
      "learning_rate": 9.46279855436832e-05,
      "loss": 1.046,
      "step": 34970
    },
    {
      "epoch": 2.057163860913034,
      "grad_norm": 0.3754832148551941,
      "learning_rate": 9.456906033940916e-05,
      "loss": 0.9787,
      "step": 34980
    },
    {
      "epoch": 2.0577519664779826,
      "grad_norm": 0.3343832492828369,
      "learning_rate": 9.451013513513513e-05,
      "loss": 0.9984,
      "step": 34990
    },
    {
      "epoch": 2.058340072042932,
      "grad_norm": 0.37603816390037537,
      "learning_rate": 9.445120993086109e-05,
      "loss": 1.0034,
      "step": 35000
    },
    {
      "epoch": 2.0589281776078807,
      "grad_norm": 0.3498879373073578,
      "learning_rate": 9.439228472658705e-05,
      "loss": 1.0915,
      "step": 35010
    },
    {
      "epoch": 2.0595162831728295,
      "grad_norm": 0.38798588514328003,
      "learning_rate": 9.4333359522313e-05,
      "loss": 1.011,
      "step": 35020
    },
    {
      "epoch": 2.0601043887377783,
      "grad_norm": 0.4108501076698303,
      "learning_rate": 9.427443431803896e-05,
      "loss": 1.0553,
      "step": 35030
    },
    {
      "epoch": 2.060692494302727,
      "grad_norm": 0.3619403541088104,
      "learning_rate": 9.421550911376492e-05,
      "loss": 1.043,
      "step": 35040
    },
    {
      "epoch": 2.0612805998676764,
      "grad_norm": 0.3561926484107971,
      "learning_rate": 9.415658390949088e-05,
      "loss": 1.0351,
      "step": 35050
    },
    {
      "epoch": 2.0618687054326252,
      "grad_norm": 0.4402959942817688,
      "learning_rate": 9.409765870521684e-05,
      "loss": 1.0381,
      "step": 35060
    },
    {
      "epoch": 2.062456810997574,
      "grad_norm": 0.37314948439598083,
      "learning_rate": 9.40387335009428e-05,
      "loss": 1.0224,
      "step": 35070
    },
    {
      "epoch": 2.063044916562523,
      "grad_norm": 0.3954274356365204,
      "learning_rate": 9.397980829666876e-05,
      "loss": 1.0751,
      "step": 35080
    },
    {
      "epoch": 2.0636330221274717,
      "grad_norm": 0.41685107350349426,
      "learning_rate": 9.392088309239471e-05,
      "loss": 0.9887,
      "step": 35090
    },
    {
      "epoch": 2.064221127692421,
      "grad_norm": 0.3599672317504883,
      "learning_rate": 9.386195788812067e-05,
      "loss": 0.9961,
      "step": 35100
    },
    {
      "epoch": 2.0648092332573698,
      "grad_norm": 0.40968063473701477,
      "learning_rate": 9.380303268384663e-05,
      "loss": 1.047,
      "step": 35110
    },
    {
      "epoch": 2.0653973388223186,
      "grad_norm": 0.3763284981250763,
      "learning_rate": 9.37441074795726e-05,
      "loss": 0.9222,
      "step": 35120
    },
    {
      "epoch": 2.0659854443872674,
      "grad_norm": 0.3488815724849701,
      "learning_rate": 9.368518227529856e-05,
      "loss": 1.0221,
      "step": 35130
    },
    {
      "epoch": 2.066573549952216,
      "grad_norm": 0.38586336374282837,
      "learning_rate": 9.36262570710245e-05,
      "loss": 0.9769,
      "step": 35140
    },
    {
      "epoch": 2.0671616555171655,
      "grad_norm": 0.34056293964385986,
      "learning_rate": 9.356733186675045e-05,
      "loss": 1.0202,
      "step": 35150
    },
    {
      "epoch": 2.0677497610821143,
      "grad_norm": 0.3327057361602783,
      "learning_rate": 9.350840666247641e-05,
      "loss": 0.9546,
      "step": 35160
    },
    {
      "epoch": 2.068337866647063,
      "grad_norm": 0.4277088940143585,
      "learning_rate": 9.344948145820237e-05,
      "loss": 1.0095,
      "step": 35170
    },
    {
      "epoch": 2.068925972212012,
      "grad_norm": 0.3976280391216278,
      "learning_rate": 9.339055625392833e-05,
      "loss": 0.9731,
      "step": 35180
    },
    {
      "epoch": 2.069514077776961,
      "grad_norm": 0.3225848972797394,
      "learning_rate": 9.33316310496543e-05,
      "loss": 1.021,
      "step": 35190
    },
    {
      "epoch": 2.07010218334191,
      "grad_norm": 0.3641155958175659,
      "learning_rate": 9.327270584538026e-05,
      "loss": 0.9756,
      "step": 35200
    },
    {
      "epoch": 2.070690288906859,
      "grad_norm": 0.3544680178165436,
      "learning_rate": 9.321378064110622e-05,
      "loss": 0.9302,
      "step": 35210
    },
    {
      "epoch": 2.0712783944718076,
      "grad_norm": 0.35035440325737,
      "learning_rate": 9.315485543683217e-05,
      "loss": 0.9609,
      "step": 35220
    },
    {
      "epoch": 2.0718665000367564,
      "grad_norm": 0.3561136722564697,
      "learning_rate": 9.309593023255813e-05,
      "loss": 1.0637,
      "step": 35230
    },
    {
      "epoch": 2.0724546056017057,
      "grad_norm": 0.3986060321331024,
      "learning_rate": 9.303700502828409e-05,
      "loss": 0.9795,
      "step": 35240
    },
    {
      "epoch": 2.0730427111666545,
      "grad_norm": 0.40219640731811523,
      "learning_rate": 9.297807982401005e-05,
      "loss": 0.996,
      "step": 35250
    },
    {
      "epoch": 2.0736308167316033,
      "grad_norm": 0.33577534556388855,
      "learning_rate": 9.291915461973601e-05,
      "loss": 0.9346,
      "step": 35260
    },
    {
      "epoch": 2.074218922296552,
      "grad_norm": 0.34071746468544006,
      "learning_rate": 9.286022941546197e-05,
      "loss": 0.9471,
      "step": 35270
    },
    {
      "epoch": 2.074807027861501,
      "grad_norm": 0.3313722610473633,
      "learning_rate": 9.280130421118793e-05,
      "loss": 1.035,
      "step": 35280
    },
    {
      "epoch": 2.07539513342645,
      "grad_norm": 0.35608622431755066,
      "learning_rate": 9.274237900691388e-05,
      "loss": 1.0277,
      "step": 35290
    },
    {
      "epoch": 2.075983238991399,
      "grad_norm": 0.42251405119895935,
      "learning_rate": 9.268345380263984e-05,
      "loss": 1.0263,
      "step": 35300
    },
    {
      "epoch": 2.076571344556348,
      "grad_norm": 0.3829123377799988,
      "learning_rate": 9.26245285983658e-05,
      "loss": 0.9327,
      "step": 35310
    },
    {
      "epoch": 2.0771594501212967,
      "grad_norm": 0.3864680230617523,
      "learning_rate": 9.256560339409176e-05,
      "loss": 0.9202,
      "step": 35320
    },
    {
      "epoch": 2.0777475556862455,
      "grad_norm": 0.3573298156261444,
      "learning_rate": 9.250667818981772e-05,
      "loss": 1.0098,
      "step": 35330
    },
    {
      "epoch": 2.0783356612511947,
      "grad_norm": 0.3380829691886902,
      "learning_rate": 9.244775298554369e-05,
      "loss": 0.9006,
      "step": 35340
    },
    {
      "epoch": 2.0789237668161435,
      "grad_norm": 0.38087916374206543,
      "learning_rate": 9.238882778126962e-05,
      "loss": 0.9544,
      "step": 35350
    },
    {
      "epoch": 2.0795118723810924,
      "grad_norm": 0.4053654670715332,
      "learning_rate": 9.232990257699558e-05,
      "loss": 1.1314,
      "step": 35360
    },
    {
      "epoch": 2.080099977946041,
      "grad_norm": 0.34637850522994995,
      "learning_rate": 9.227097737272154e-05,
      "loss": 0.9408,
      "step": 35370
    },
    {
      "epoch": 2.0806880835109904,
      "grad_norm": 0.3790070116519928,
      "learning_rate": 9.22120521684475e-05,
      "loss": 0.9852,
      "step": 35380
    },
    {
      "epoch": 2.0812761890759393,
      "grad_norm": 0.36038631200790405,
      "learning_rate": 9.215312696417347e-05,
      "loss": 1.053,
      "step": 35390
    },
    {
      "epoch": 2.081864294640888,
      "grad_norm": 0.3788948357105255,
      "learning_rate": 9.209420175989943e-05,
      "loss": 1.0351,
      "step": 35400
    },
    {
      "epoch": 2.082452400205837,
      "grad_norm": 0.3776477873325348,
      "learning_rate": 9.203527655562539e-05,
      "loss": 1.0811,
      "step": 35410
    },
    {
      "epoch": 2.0830405057707857,
      "grad_norm": 0.3437257707118988,
      "learning_rate": 9.197635135135134e-05,
      "loss": 1.0564,
      "step": 35420
    },
    {
      "epoch": 2.083628611335735,
      "grad_norm": 0.3306732177734375,
      "learning_rate": 9.19174261470773e-05,
      "loss": 1.0846,
      "step": 35430
    },
    {
      "epoch": 2.0842167169006838,
      "grad_norm": 0.3543509542942047,
      "learning_rate": 9.185850094280326e-05,
      "loss": 1.0384,
      "step": 35440
    },
    {
      "epoch": 2.0848048224656326,
      "grad_norm": 0.36942458152770996,
      "learning_rate": 9.179957573852922e-05,
      "loss": 1.0467,
      "step": 35450
    },
    {
      "epoch": 2.0853929280305814,
      "grad_norm": 0.34845080971717834,
      "learning_rate": 9.174065053425518e-05,
      "loss": 0.9866,
      "step": 35460
    },
    {
      "epoch": 2.08598103359553,
      "grad_norm": 0.393383651971817,
      "learning_rate": 9.168172532998114e-05,
      "loss": 0.9431,
      "step": 35470
    },
    {
      "epoch": 2.0865691391604795,
      "grad_norm": 0.3654276132583618,
      "learning_rate": 9.16228001257071e-05,
      "loss": 0.9813,
      "step": 35480
    },
    {
      "epoch": 2.0871572447254283,
      "grad_norm": 0.3839554786682129,
      "learning_rate": 9.156387492143305e-05,
      "loss": 0.9476,
      "step": 35490
    },
    {
      "epoch": 2.087745350290377,
      "grad_norm": 0.35931670665740967,
      "learning_rate": 9.150494971715901e-05,
      "loss": 0.9475,
      "step": 35500
    },
    {
      "epoch": 2.088333455855326,
      "grad_norm": 0.3543607294559479,
      "learning_rate": 9.144602451288497e-05,
      "loss": 0.9414,
      "step": 35510
    },
    {
      "epoch": 2.0889215614202747,
      "grad_norm": 0.3536403477191925,
      "learning_rate": 9.138709930861093e-05,
      "loss": 0.9337,
      "step": 35520
    },
    {
      "epoch": 2.089509666985224,
      "grad_norm": 0.34091469645500183,
      "learning_rate": 9.13281741043369e-05,
      "loss": 1.0571,
      "step": 35530
    },
    {
      "epoch": 2.090097772550173,
      "grad_norm": 0.4054940640926361,
      "learning_rate": 9.126924890006286e-05,
      "loss": 1.0375,
      "step": 35540
    },
    {
      "epoch": 2.0906858781151216,
      "grad_norm": 0.3819165825843811,
      "learning_rate": 9.121032369578882e-05,
      "loss": 1.0402,
      "step": 35550
    },
    {
      "epoch": 2.0912739836800704,
      "grad_norm": 0.38608062267303467,
      "learning_rate": 9.115139849151475e-05,
      "loss": 0.9241,
      "step": 35560
    },
    {
      "epoch": 2.0918620892450193,
      "grad_norm": 0.3809472918510437,
      "learning_rate": 9.109247328724071e-05,
      "loss": 1.0004,
      "step": 35570
    },
    {
      "epoch": 2.0924501948099685,
      "grad_norm": 0.3683457374572754,
      "learning_rate": 9.103354808296667e-05,
      "loss": 1.0361,
      "step": 35580
    },
    {
      "epoch": 2.0930383003749173,
      "grad_norm": 0.3997715413570404,
      "learning_rate": 9.097462287869263e-05,
      "loss": 1.011,
      "step": 35590
    },
    {
      "epoch": 2.093626405939866,
      "grad_norm": 0.4065631628036499,
      "learning_rate": 9.09156976744186e-05,
      "loss": 1.0149,
      "step": 35600
    },
    {
      "epoch": 2.094214511504815,
      "grad_norm": 0.4024195969104767,
      "learning_rate": 9.085677247014456e-05,
      "loss": 0.9827,
      "step": 35610
    },
    {
      "epoch": 2.0948026170697642,
      "grad_norm": 0.3923671543598175,
      "learning_rate": 9.07978472658705e-05,
      "loss": 1.0108,
      "step": 35620
    },
    {
      "epoch": 2.095390722634713,
      "grad_norm": 0.4272162616252899,
      "learning_rate": 9.073892206159647e-05,
      "loss": 0.9635,
      "step": 35630
    },
    {
      "epoch": 2.095978828199662,
      "grad_norm": 0.38664329051971436,
      "learning_rate": 9.067999685732243e-05,
      "loss": 1.0376,
      "step": 35640
    },
    {
      "epoch": 2.0965669337646107,
      "grad_norm": 0.34800705313682556,
      "learning_rate": 9.062107165304839e-05,
      "loss": 0.9424,
      "step": 35650
    },
    {
      "epoch": 2.0971550393295595,
      "grad_norm": 0.354030966758728,
      "learning_rate": 9.056214644877435e-05,
      "loss": 0.9306,
      "step": 35660
    },
    {
      "epoch": 2.0977431448945087,
      "grad_norm": 0.37238889932632446,
      "learning_rate": 9.050322124450031e-05,
      "loss": 1.0078,
      "step": 35670
    },
    {
      "epoch": 2.0983312504594576,
      "grad_norm": 0.3958697021007538,
      "learning_rate": 9.044429604022627e-05,
      "loss": 1.0039,
      "step": 35680
    },
    {
      "epoch": 2.0989193560244064,
      "grad_norm": 0.36600422859191895,
      "learning_rate": 9.038537083595222e-05,
      "loss": 0.9304,
      "step": 35690
    },
    {
      "epoch": 2.099507461589355,
      "grad_norm": 0.34906327724456787,
      "learning_rate": 9.032644563167818e-05,
      "loss": 1.127,
      "step": 35700
    },
    {
      "epoch": 2.100095567154304,
      "grad_norm": 0.3617803752422333,
      "learning_rate": 9.026752042740414e-05,
      "loss": 1.027,
      "step": 35710
    },
    {
      "epoch": 2.1006836727192533,
      "grad_norm": 0.38185298442840576,
      "learning_rate": 9.02085952231301e-05,
      "loss": 1.0545,
      "step": 35720
    },
    {
      "epoch": 2.101271778284202,
      "grad_norm": 0.3511275053024292,
      "learning_rate": 9.014967001885606e-05,
      "loss": 1.0549,
      "step": 35730
    },
    {
      "epoch": 2.101859883849151,
      "grad_norm": 0.3509127199649811,
      "learning_rate": 9.009074481458203e-05,
      "loss": 0.9834,
      "step": 35740
    },
    {
      "epoch": 2.1024479894140997,
      "grad_norm": 0.3187914788722992,
      "learning_rate": 9.003181961030799e-05,
      "loss": 0.9386,
      "step": 35750
    },
    {
      "epoch": 2.1030360949790485,
      "grad_norm": 0.36376067996025085,
      "learning_rate": 8.997289440603393e-05,
      "loss": 1.0157,
      "step": 35760
    },
    {
      "epoch": 2.103624200543998,
      "grad_norm": 0.41321587562561035,
      "learning_rate": 8.991396920175988e-05,
      "loss": 1.0198,
      "step": 35770
    },
    {
      "epoch": 2.1042123061089466,
      "grad_norm": 0.44302406907081604,
      "learning_rate": 8.985504399748584e-05,
      "loss": 0.9927,
      "step": 35780
    },
    {
      "epoch": 2.1048004116738954,
      "grad_norm": 0.38834503293037415,
      "learning_rate": 8.97961187932118e-05,
      "loss": 1.0557,
      "step": 35790
    },
    {
      "epoch": 2.1053885172388442,
      "grad_norm": 0.3719600439071655,
      "learning_rate": 8.973719358893777e-05,
      "loss": 0.9731,
      "step": 35800
    },
    {
      "epoch": 2.1059766228037935,
      "grad_norm": 0.43590840697288513,
      "learning_rate": 8.967826838466373e-05,
      "loss": 0.9242,
      "step": 35810
    },
    {
      "epoch": 2.1065647283687423,
      "grad_norm": 0.36416563391685486,
      "learning_rate": 8.961934318038967e-05,
      "loss": 1.0382,
      "step": 35820
    },
    {
      "epoch": 2.107152833933691,
      "grad_norm": 0.3410950005054474,
      "learning_rate": 8.956041797611564e-05,
      "loss": 0.9701,
      "step": 35830
    },
    {
      "epoch": 2.10774093949864,
      "grad_norm": 0.38378819823265076,
      "learning_rate": 8.95014927718416e-05,
      "loss": 0.9912,
      "step": 35840
    },
    {
      "epoch": 2.1083290450635888,
      "grad_norm": 0.3847517669200897,
      "learning_rate": 8.944256756756756e-05,
      "loss": 0.8813,
      "step": 35850
    },
    {
      "epoch": 2.108917150628538,
      "grad_norm": 0.38194382190704346,
      "learning_rate": 8.938364236329352e-05,
      "loss": 0.9857,
      "step": 35860
    },
    {
      "epoch": 2.109505256193487,
      "grad_norm": 0.3421277105808258,
      "learning_rate": 8.932471715901948e-05,
      "loss": 1.0024,
      "step": 35870
    },
    {
      "epoch": 2.1100933617584356,
      "grad_norm": 0.363158255815506,
      "learning_rate": 8.926579195474544e-05,
      "loss": 0.9944,
      "step": 35880
    },
    {
      "epoch": 2.1106814673233845,
      "grad_norm": 0.40677666664123535,
      "learning_rate": 8.920686675047139e-05,
      "loss": 1.071,
      "step": 35890
    },
    {
      "epoch": 2.1112695728883333,
      "grad_norm": 0.37435612082481384,
      "learning_rate": 8.914794154619735e-05,
      "loss": 1.0581,
      "step": 35900
    },
    {
      "epoch": 2.1118576784532825,
      "grad_norm": 0.37061789631843567,
      "learning_rate": 8.908901634192331e-05,
      "loss": 1.142,
      "step": 35910
    },
    {
      "epoch": 2.1124457840182314,
      "grad_norm": 0.4348873794078827,
      "learning_rate": 8.903009113764927e-05,
      "loss": 1.0267,
      "step": 35920
    },
    {
      "epoch": 2.11303388958318,
      "grad_norm": 0.38930627703666687,
      "learning_rate": 8.897116593337523e-05,
      "loss": 1.0552,
      "step": 35930
    },
    {
      "epoch": 2.113621995148129,
      "grad_norm": 0.3639257252216339,
      "learning_rate": 8.89122407291012e-05,
      "loss": 0.8984,
      "step": 35940
    },
    {
      "epoch": 2.114210100713078,
      "grad_norm": 0.3051701486110687,
      "learning_rate": 8.885331552482716e-05,
      "loss": 0.9446,
      "step": 35950
    },
    {
      "epoch": 2.114798206278027,
      "grad_norm": 0.36170634627342224,
      "learning_rate": 8.87943903205531e-05,
      "loss": 0.9716,
      "step": 35960
    },
    {
      "epoch": 2.115386311842976,
      "grad_norm": 0.36164113879203796,
      "learning_rate": 8.873546511627907e-05,
      "loss": 1.066,
      "step": 35970
    },
    {
      "epoch": 2.1159744174079247,
      "grad_norm": 0.38517189025878906,
      "learning_rate": 8.867653991200501e-05,
      "loss": 1.0481,
      "step": 35980
    },
    {
      "epoch": 2.1165625229728735,
      "grad_norm": 0.35901787877082825,
      "learning_rate": 8.861761470773097e-05,
      "loss": 1.0263,
      "step": 35990
    },
    {
      "epoch": 2.1171506285378223,
      "grad_norm": 0.3656112551689148,
      "learning_rate": 8.855868950345694e-05,
      "loss": 0.9793,
      "step": 36000
    },
    {
      "epoch": 2.1177387341027716,
      "grad_norm": 0.3604167401790619,
      "learning_rate": 8.84997642991829e-05,
      "loss": 0.9696,
      "step": 36010
    },
    {
      "epoch": 2.1183268396677204,
      "grad_norm": 0.39633405208587646,
      "learning_rate": 8.844083909490884e-05,
      "loss": 0.9924,
      "step": 36020
    },
    {
      "epoch": 2.118914945232669,
      "grad_norm": 0.42121803760528564,
      "learning_rate": 8.83819138906348e-05,
      "loss": 0.9276,
      "step": 36030
    },
    {
      "epoch": 2.119503050797618,
      "grad_norm": 0.3570890724658966,
      "learning_rate": 8.832298868636077e-05,
      "loss": 1.0512,
      "step": 36040
    },
    {
      "epoch": 2.1200911563625673,
      "grad_norm": 0.344797283411026,
      "learning_rate": 8.826406348208673e-05,
      "loss": 0.9835,
      "step": 36050
    },
    {
      "epoch": 2.120679261927516,
      "grad_norm": 0.3412124216556549,
      "learning_rate": 8.820513827781269e-05,
      "loss": 0.967,
      "step": 36060
    },
    {
      "epoch": 2.121267367492465,
      "grad_norm": 0.36571410298347473,
      "learning_rate": 8.815210559396604e-05,
      "loss": 1.0202,
      "step": 36070
    },
    {
      "epoch": 2.1218554730574137,
      "grad_norm": 0.3621833622455597,
      "learning_rate": 8.8093180389692e-05,
      "loss": 1.0583,
      "step": 36080
    },
    {
      "epoch": 2.1224435786223625,
      "grad_norm": 0.4314742088317871,
      "learning_rate": 8.803425518541797e-05,
      "loss": 1.0523,
      "step": 36090
    },
    {
      "epoch": 2.123031684187312,
      "grad_norm": 0.39503511786460876,
      "learning_rate": 8.797532998114393e-05,
      "loss": 1.125,
      "step": 36100
    },
    {
      "epoch": 2.1236197897522606,
      "grad_norm": 0.3692614734172821,
      "learning_rate": 8.791640477686988e-05,
      "loss": 0.9871,
      "step": 36110
    },
    {
      "epoch": 2.1242078953172094,
      "grad_norm": 0.41074374318122864,
      "learning_rate": 8.785747957259584e-05,
      "loss": 0.9386,
      "step": 36120
    },
    {
      "epoch": 2.1247960008821583,
      "grad_norm": 0.4900466799736023,
      "learning_rate": 8.77985543683218e-05,
      "loss": 0.9733,
      "step": 36130
    },
    {
      "epoch": 2.125384106447107,
      "grad_norm": 0.4007475674152374,
      "learning_rate": 8.773962916404776e-05,
      "loss": 1.1203,
      "step": 36140
    },
    {
      "epoch": 2.1259722120120563,
      "grad_norm": 0.4634423553943634,
      "learning_rate": 8.768070395977372e-05,
      "loss": 1.0503,
      "step": 36150
    },
    {
      "epoch": 2.126560317577005,
      "grad_norm": 0.3814338147640228,
      "learning_rate": 8.762177875549968e-05,
      "loss": 0.9646,
      "step": 36160
    },
    {
      "epoch": 2.127148423141954,
      "grad_norm": 0.3330982029438019,
      "learning_rate": 8.756285355122564e-05,
      "loss": 0.9589,
      "step": 36170
    },
    {
      "epoch": 2.1277365287069028,
      "grad_norm": 0.3756183683872223,
      "learning_rate": 8.750392834695159e-05,
      "loss": 1.0104,
      "step": 36180
    },
    {
      "epoch": 2.1283246342718516,
      "grad_norm": 0.35702458024024963,
      "learning_rate": 8.744500314267755e-05,
      "loss": 0.9433,
      "step": 36190
    },
    {
      "epoch": 2.128912739836801,
      "grad_norm": 0.3547680079936981,
      "learning_rate": 8.738607793840351e-05,
      "loss": 1.0336,
      "step": 36200
    },
    {
      "epoch": 2.1295008454017497,
      "grad_norm": 0.3640573024749756,
      "learning_rate": 8.732715273412947e-05,
      "loss": 0.9995,
      "step": 36210
    },
    {
      "epoch": 2.1300889509666985,
      "grad_norm": 0.4293625056743622,
      "learning_rate": 8.726822752985544e-05,
      "loss": 0.9568,
      "step": 36220
    },
    {
      "epoch": 2.1306770565316473,
      "grad_norm": 0.34213754534721375,
      "learning_rate": 8.72093023255814e-05,
      "loss": 1.102,
      "step": 36230
    },
    {
      "epoch": 2.1312651620965966,
      "grad_norm": 0.3647196292877197,
      "learning_rate": 8.715037712130736e-05,
      "loss": 1.0032,
      "step": 36240
    },
    {
      "epoch": 2.1318532676615454,
      "grad_norm": 0.3751947283744812,
      "learning_rate": 8.70914519170333e-05,
      "loss": 0.9564,
      "step": 36250
    },
    {
      "epoch": 2.132441373226494,
      "grad_norm": 0.3925042450428009,
      "learning_rate": 8.703252671275927e-05,
      "loss": 1.0072,
      "step": 36260
    },
    {
      "epoch": 2.133029478791443,
      "grad_norm": 0.3593811094760895,
      "learning_rate": 8.697360150848523e-05,
      "loss": 0.9424,
      "step": 36270
    },
    {
      "epoch": 2.133617584356392,
      "grad_norm": 0.36542269587516785,
      "learning_rate": 8.691467630421118e-05,
      "loss": 0.9235,
      "step": 36280
    },
    {
      "epoch": 2.134205689921341,
      "grad_norm": 0.3827506899833679,
      "learning_rate": 8.685575109993714e-05,
      "loss": 1.009,
      "step": 36290
    },
    {
      "epoch": 2.13479379548629,
      "grad_norm": 0.3711852729320526,
      "learning_rate": 8.679682589566308e-05,
      "loss": 0.9902,
      "step": 36300
    },
    {
      "epoch": 2.1353819010512387,
      "grad_norm": 0.3542975187301636,
      "learning_rate": 8.673790069138905e-05,
      "loss": 0.9755,
      "step": 36310
    },
    {
      "epoch": 2.1359700066161875,
      "grad_norm": 0.3851590156555176,
      "learning_rate": 8.667897548711501e-05,
      "loss": 1.03,
      "step": 36320
    },
    {
      "epoch": 2.1365581121811363,
      "grad_norm": 0.3514776825904846,
      "learning_rate": 8.662005028284097e-05,
      "loss": 1.0269,
      "step": 36330
    },
    {
      "epoch": 2.1371462177460856,
      "grad_norm": 0.38507065176963806,
      "learning_rate": 8.656112507856693e-05,
      "loss": 0.8786,
      "step": 36340
    },
    {
      "epoch": 2.1377343233110344,
      "grad_norm": 0.3515297770500183,
      "learning_rate": 8.650219987429289e-05,
      "loss": 1.1806,
      "step": 36350
    },
    {
      "epoch": 2.1383224288759832,
      "grad_norm": 0.36733272671699524,
      "learning_rate": 8.644327467001885e-05,
      "loss": 0.968,
      "step": 36360
    },
    {
      "epoch": 2.138910534440932,
      "grad_norm": 0.3392082154750824,
      "learning_rate": 8.638434946574481e-05,
      "loss": 0.9685,
      "step": 36370
    },
    {
      "epoch": 2.139498640005881,
      "grad_norm": 0.5090042948722839,
      "learning_rate": 8.632542426147076e-05,
      "loss": 1.0466,
      "step": 36380
    },
    {
      "epoch": 2.14008674557083,
      "grad_norm": 0.3849683403968811,
      "learning_rate": 8.626649905719672e-05,
      "loss": 1.0296,
      "step": 36390
    },
    {
      "epoch": 2.140674851135779,
      "grad_norm": 0.4258640706539154,
      "learning_rate": 8.620757385292268e-05,
      "loss": 1.0403,
      "step": 36400
    },
    {
      "epoch": 2.1412629567007277,
      "grad_norm": 0.4065786898136139,
      "learning_rate": 8.614864864864864e-05,
      "loss": 1.0348,
      "step": 36410
    },
    {
      "epoch": 2.1418510622656766,
      "grad_norm": 0.4183654189109802,
      "learning_rate": 8.60897234443746e-05,
      "loss": 0.9653,
      "step": 36420
    },
    {
      "epoch": 2.1424391678306254,
      "grad_norm": 0.4110995829105377,
      "learning_rate": 8.603079824010057e-05,
      "loss": 1.0035,
      "step": 36430
    },
    {
      "epoch": 2.1430272733955746,
      "grad_norm": 0.40231502056121826,
      "learning_rate": 8.597187303582653e-05,
      "loss": 0.9785,
      "step": 36440
    },
    {
      "epoch": 2.1436153789605235,
      "grad_norm": 0.3358253538608551,
      "learning_rate": 8.591294783155248e-05,
      "loss": 0.9357,
      "step": 36450
    },
    {
      "epoch": 2.1442034845254723,
      "grad_norm": 0.3562166094779968,
      "learning_rate": 8.585402262727844e-05,
      "loss": 0.967,
      "step": 36460
    },
    {
      "epoch": 2.144791590090421,
      "grad_norm": 0.35065194964408875,
      "learning_rate": 8.57950974230044e-05,
      "loss": 1.0137,
      "step": 36470
    },
    {
      "epoch": 2.1453796956553703,
      "grad_norm": 0.32685112953186035,
      "learning_rate": 8.573617221873036e-05,
      "loss": 0.9353,
      "step": 36480
    },
    {
      "epoch": 2.145967801220319,
      "grad_norm": 0.34341123700141907,
      "learning_rate": 8.56772470144563e-05,
      "loss": 0.9877,
      "step": 36490
    },
    {
      "epoch": 2.146555906785268,
      "grad_norm": 0.4026148319244385,
      "learning_rate": 8.561832181018225e-05,
      "loss": 0.9695,
      "step": 36500
    },
    {
      "epoch": 2.147144012350217,
      "grad_norm": 0.3677126169204712,
      "learning_rate": 8.555939660590822e-05,
      "loss": 1.05,
      "step": 36510
    },
    {
      "epoch": 2.1477321179151656,
      "grad_norm": 0.3919200301170349,
      "learning_rate": 8.550047140163418e-05,
      "loss": 0.9944,
      "step": 36520
    },
    {
      "epoch": 2.148320223480115,
      "grad_norm": 0.39154648780822754,
      "learning_rate": 8.544154619736014e-05,
      "loss": 1.062,
      "step": 36530
    },
    {
      "epoch": 2.1489083290450637,
      "grad_norm": 0.36539050936698914,
      "learning_rate": 8.53826209930861e-05,
      "loss": 1.0536,
      "step": 36540
    },
    {
      "epoch": 2.1494964346100125,
      "grad_norm": 0.3436116874217987,
      "learning_rate": 8.532369578881206e-05,
      "loss": 1.1125,
      "step": 36550
    },
    {
      "epoch": 2.1500845401749613,
      "grad_norm": 0.3744369149208069,
      "learning_rate": 8.526477058453802e-05,
      "loss": 0.9928,
      "step": 36560
    },
    {
      "epoch": 2.15067264573991,
      "grad_norm": 0.39158353209495544,
      "learning_rate": 8.520584538026398e-05,
      "loss": 0.9765,
      "step": 36570
    },
    {
      "epoch": 2.1512607513048594,
      "grad_norm": 0.3389238119125366,
      "learning_rate": 8.514692017598993e-05,
      "loss": 1.0056,
      "step": 36580
    },
    {
      "epoch": 2.151848856869808,
      "grad_norm": 0.39198246598243713,
      "learning_rate": 8.508799497171589e-05,
      "loss": 1.014,
      "step": 36590
    },
    {
      "epoch": 2.152436962434757,
      "grad_norm": 0.32903099060058594,
      "learning_rate": 8.502906976744185e-05,
      "loss": 1.0345,
      "step": 36600
    },
    {
      "epoch": 2.153025067999706,
      "grad_norm": 0.3793531358242035,
      "learning_rate": 8.497014456316781e-05,
      "loss": 0.9616,
      "step": 36610
    },
    {
      "epoch": 2.1536131735646546,
      "grad_norm": 0.3647383749485016,
      "learning_rate": 8.491121935889378e-05,
      "loss": 1.014,
      "step": 36620
    },
    {
      "epoch": 2.154201279129604,
      "grad_norm": 0.34921616315841675,
      "learning_rate": 8.485229415461974e-05,
      "loss": 1.0615,
      "step": 36630
    },
    {
      "epoch": 2.1547893846945527,
      "grad_norm": 0.3235081136226654,
      "learning_rate": 8.47933689503457e-05,
      "loss": 1.0331,
      "step": 36640
    },
    {
      "epoch": 2.1553774902595015,
      "grad_norm": 0.37881287932395935,
      "learning_rate": 8.473444374607165e-05,
      "loss": 0.9898,
      "step": 36650
    },
    {
      "epoch": 2.1559655958244504,
      "grad_norm": 0.3909841775894165,
      "learning_rate": 8.46755185417976e-05,
      "loss": 1.0112,
      "step": 36660
    },
    {
      "epoch": 2.1565537013893996,
      "grad_norm": 0.41832301020622253,
      "learning_rate": 8.461659333752357e-05,
      "loss": 0.9755,
      "step": 36670
    },
    {
      "epoch": 2.1571418069543484,
      "grad_norm": 0.33868545293807983,
      "learning_rate": 8.455766813324953e-05,
      "loss": 0.952,
      "step": 36680
    },
    {
      "epoch": 2.1577299125192972,
      "grad_norm": 0.3573039472103119,
      "learning_rate": 8.449874292897549e-05,
      "loss": 0.9991,
      "step": 36690
    },
    {
      "epoch": 2.158318018084246,
      "grad_norm": 0.414060115814209,
      "learning_rate": 8.443981772470142e-05,
      "loss": 0.8977,
      "step": 36700
    },
    {
      "epoch": 2.158906123649195,
      "grad_norm": 0.3322283625602722,
      "learning_rate": 8.438089252042739e-05,
      "loss": 1.046,
      "step": 36710
    },
    {
      "epoch": 2.159494229214144,
      "grad_norm": 0.3959721326828003,
      "learning_rate": 8.432196731615335e-05,
      "loss": 0.999,
      "step": 36720
    },
    {
      "epoch": 2.160082334779093,
      "grad_norm": 0.37379956245422363,
      "learning_rate": 8.426304211187931e-05,
      "loss": 0.9935,
      "step": 36730
    },
    {
      "epoch": 2.1606704403440418,
      "grad_norm": 0.3628588020801544,
      "learning_rate": 8.420411690760527e-05,
      "loss": 1.0132,
      "step": 36740
    },
    {
      "epoch": 2.1612585459089906,
      "grad_norm": 0.37004756927490234,
      "learning_rate": 8.414519170333123e-05,
      "loss": 1.067,
      "step": 36750
    },
    {
      "epoch": 2.1618466514739394,
      "grad_norm": 0.372213751077652,
      "learning_rate": 8.408626649905719e-05,
      "loss": 1.0419,
      "step": 36760
    },
    {
      "epoch": 2.1624347570388887,
      "grad_norm": 0.37319841980934143,
      "learning_rate": 8.402734129478315e-05,
      "loss": 1.0011,
      "step": 36770
    },
    {
      "epoch": 2.1630228626038375,
      "grad_norm": 0.4031963646411896,
      "learning_rate": 8.39684160905091e-05,
      "loss": 1.0504,
      "step": 36780
    },
    {
      "epoch": 2.1636109681687863,
      "grad_norm": 0.35814693570137024,
      "learning_rate": 8.390949088623506e-05,
      "loss": 1.0387,
      "step": 36790
    },
    {
      "epoch": 2.164199073733735,
      "grad_norm": 0.39397507905960083,
      "learning_rate": 8.385056568196102e-05,
      "loss": 0.9769,
      "step": 36800
    },
    {
      "epoch": 2.164787179298684,
      "grad_norm": 0.37494590878486633,
      "learning_rate": 8.379164047768698e-05,
      "loss": 1.1541,
      "step": 36810
    },
    {
      "epoch": 2.165375284863633,
      "grad_norm": 0.37468791007995605,
      "learning_rate": 8.373271527341294e-05,
      "loss": 1.052,
      "step": 36820
    },
    {
      "epoch": 2.165963390428582,
      "grad_norm": 0.35664576292037964,
      "learning_rate": 8.36737900691389e-05,
      "loss": 0.9041,
      "step": 36830
    },
    {
      "epoch": 2.166551495993531,
      "grad_norm": 0.41149383783340454,
      "learning_rate": 8.361486486486487e-05,
      "loss": 1.0499,
      "step": 36840
    },
    {
      "epoch": 2.1671396015584796,
      "grad_norm": 0.39614352583885193,
      "learning_rate": 8.355593966059081e-05,
      "loss": 0.9834,
      "step": 36850
    },
    {
      "epoch": 2.1677277071234284,
      "grad_norm": 0.3795802593231201,
      "learning_rate": 8.349701445631678e-05,
      "loss": 0.9697,
      "step": 36860
    },
    {
      "epoch": 2.1683158126883777,
      "grad_norm": 0.3638112246990204,
      "learning_rate": 8.343808925204274e-05,
      "loss": 0.9755,
      "step": 36870
    },
    {
      "epoch": 2.1689039182533265,
      "grad_norm": 0.35652515292167664,
      "learning_rate": 8.33791640477687e-05,
      "loss": 1.06,
      "step": 36880
    },
    {
      "epoch": 2.1694920238182753,
      "grad_norm": 0.4223596155643463,
      "learning_rate": 8.332023884349466e-05,
      "loss": 1.102,
      "step": 36890
    },
    {
      "epoch": 2.170080129383224,
      "grad_norm": 0.3949556052684784,
      "learning_rate": 8.32613136392206e-05,
      "loss": 0.9785,
      "step": 36900
    },
    {
      "epoch": 2.1706682349481734,
      "grad_norm": 0.3747965395450592,
      "learning_rate": 8.320238843494656e-05,
      "loss": 0.9799,
      "step": 36910
    },
    {
      "epoch": 2.171256340513122,
      "grad_norm": 0.384928435087204,
      "learning_rate": 8.314346323067252e-05,
      "loss": 1.0092,
      "step": 36920
    },
    {
      "epoch": 2.171844446078071,
      "grad_norm": 0.4001144468784332,
      "learning_rate": 8.308453802639848e-05,
      "loss": 1.0867,
      "step": 36930
    },
    {
      "epoch": 2.17243255164302,
      "grad_norm": 0.364999383687973,
      "learning_rate": 8.302561282212444e-05,
      "loss": 1.0144,
      "step": 36940
    },
    {
      "epoch": 2.1730206572079687,
      "grad_norm": 0.3334293067455292,
      "learning_rate": 8.29666876178504e-05,
      "loss": 1.0286,
      "step": 36950
    },
    {
      "epoch": 2.173608762772918,
      "grad_norm": 0.3630879521369934,
      "learning_rate": 8.290776241357636e-05,
      "loss": 1.0293,
      "step": 36960
    },
    {
      "epoch": 2.1741968683378667,
      "grad_norm": 0.36419156193733215,
      "learning_rate": 8.284883720930232e-05,
      "loss": 0.9808,
      "step": 36970
    },
    {
      "epoch": 2.1747849739028156,
      "grad_norm": 0.41057899594306946,
      "learning_rate": 8.278991200502827e-05,
      "loss": 1.1109,
      "step": 36980
    },
    {
      "epoch": 2.1753730794677644,
      "grad_norm": 0.4383115768432617,
      "learning_rate": 8.273098680075423e-05,
      "loss": 1.0889,
      "step": 36990
    },
    {
      "epoch": 2.175961185032713,
      "grad_norm": 0.3594508469104767,
      "learning_rate": 8.267206159648019e-05,
      "loss": 0.9454,
      "step": 37000
    },
    {
      "epoch": 2.1765492905976624,
      "grad_norm": 0.3512227535247803,
      "learning_rate": 8.261313639220615e-05,
      "loss": 0.9679,
      "step": 37010
    },
    {
      "epoch": 2.1771373961626113,
      "grad_norm": 0.40400874614715576,
      "learning_rate": 8.255421118793211e-05,
      "loss": 1.001,
      "step": 37020
    },
    {
      "epoch": 2.17772550172756,
      "grad_norm": 0.40158042311668396,
      "learning_rate": 8.249528598365808e-05,
      "loss": 0.9219,
      "step": 37030
    },
    {
      "epoch": 2.178313607292509,
      "grad_norm": 0.3971676826477051,
      "learning_rate": 8.243636077938404e-05,
      "loss": 1.0618,
      "step": 37040
    },
    {
      "epoch": 2.1789017128574577,
      "grad_norm": 0.31730416417121887,
      "learning_rate": 8.237743557510998e-05,
      "loss": 1.0447,
      "step": 37050
    },
    {
      "epoch": 2.179489818422407,
      "grad_norm": 0.36071065068244934,
      "learning_rate": 8.231851037083595e-05,
      "loss": 1.0943,
      "step": 37060
    },
    {
      "epoch": 2.180077923987356,
      "grad_norm": 0.3424389362335205,
      "learning_rate": 8.225958516656191e-05,
      "loss": 1.1119,
      "step": 37070
    },
    {
      "epoch": 2.1806660295523046,
      "grad_norm": 0.4135705232620239,
      "learning_rate": 8.220065996228787e-05,
      "loss": 1.0278,
      "step": 37080
    },
    {
      "epoch": 2.1812541351172534,
      "grad_norm": 0.33253028988838196,
      "learning_rate": 8.214173475801383e-05,
      "loss": 1.007,
      "step": 37090
    },
    {
      "epoch": 2.1818422406822027,
      "grad_norm": 0.38666781783103943,
      "learning_rate": 8.208280955373979e-05,
      "loss": 1.0563,
      "step": 37100
    },
    {
      "epoch": 2.1824303462471515,
      "grad_norm": 0.36324170231819153,
      "learning_rate": 8.202388434946572e-05,
      "loss": 0.9479,
      "step": 37110
    },
    {
      "epoch": 2.1830184518121003,
      "grad_norm": 0.41013482213020325,
      "learning_rate": 8.196495914519169e-05,
      "loss": 1.0259,
      "step": 37120
    },
    {
      "epoch": 2.183606557377049,
      "grad_norm": 0.4018977880477905,
      "learning_rate": 8.190603394091765e-05,
      "loss": 1.0486,
      "step": 37130
    },
    {
      "epoch": 2.184194662941998,
      "grad_norm": 0.36819693446159363,
      "learning_rate": 8.184710873664361e-05,
      "loss": 0.983,
      "step": 37140
    },
    {
      "epoch": 2.184782768506947,
      "grad_norm": 0.35409048199653625,
      "learning_rate": 8.178818353236957e-05,
      "loss": 1.0975,
      "step": 37150
    },
    {
      "epoch": 2.185370874071896,
      "grad_norm": 0.3716367185115814,
      "learning_rate": 8.172925832809553e-05,
      "loss": 0.9868,
      "step": 37160
    },
    {
      "epoch": 2.185958979636845,
      "grad_norm": 0.32755202054977417,
      "learning_rate": 8.167033312382149e-05,
      "loss": 1.0122,
      "step": 37170
    },
    {
      "epoch": 2.1865470852017936,
      "grad_norm": 0.3534010946750641,
      "learning_rate": 8.161140791954744e-05,
      "loss": 1.009,
      "step": 37180
    },
    {
      "epoch": 2.1871351907667425,
      "grad_norm": 0.3955795466899872,
      "learning_rate": 8.15524827152734e-05,
      "loss": 1.0194,
      "step": 37190
    },
    {
      "epoch": 2.1877232963316917,
      "grad_norm": 0.3223321735858917,
      "learning_rate": 8.149355751099936e-05,
      "loss": 1.0808,
      "step": 37200
    },
    {
      "epoch": 2.1883114018966405,
      "grad_norm": 0.4131771922111511,
      "learning_rate": 8.143463230672532e-05,
      "loss": 0.9745,
      "step": 37210
    },
    {
      "epoch": 2.1888995074615893,
      "grad_norm": 0.38807806372642517,
      "learning_rate": 8.137570710245128e-05,
      "loss": 0.9724,
      "step": 37220
    },
    {
      "epoch": 2.189487613026538,
      "grad_norm": 0.3805127441883087,
      "learning_rate": 8.131678189817725e-05,
      "loss": 0.9361,
      "step": 37230
    },
    {
      "epoch": 2.190075718591487,
      "grad_norm": 0.35912713408470154,
      "learning_rate": 8.125785669390321e-05,
      "loss": 0.9728,
      "step": 37240
    },
    {
      "epoch": 2.1906638241564362,
      "grad_norm": 0.34648752212524414,
      "learning_rate": 8.119893148962915e-05,
      "loss": 1.0167,
      "step": 37250
    },
    {
      "epoch": 2.191251929721385,
      "grad_norm": 0.37343811988830566,
      "learning_rate": 8.114000628535512e-05,
      "loss": 1.04,
      "step": 37260
    },
    {
      "epoch": 2.191840035286334,
      "grad_norm": 0.3853645622730255,
      "learning_rate": 8.108108108108108e-05,
      "loss": 1.1455,
      "step": 37270
    },
    {
      "epoch": 2.1924281408512827,
      "grad_norm": 0.3552083969116211,
      "learning_rate": 8.102215587680704e-05,
      "loss": 1.0149,
      "step": 37280
    },
    {
      "epoch": 2.1930162464162315,
      "grad_norm": 0.4046938419342041,
      "learning_rate": 8.0963230672533e-05,
      "loss": 0.9516,
      "step": 37290
    },
    {
      "epoch": 2.1936043519811808,
      "grad_norm": 0.33838945627212524,
      "learning_rate": 8.090430546825896e-05,
      "loss": 1.0742,
      "step": 37300
    },
    {
      "epoch": 2.1941924575461296,
      "grad_norm": 0.4011485278606415,
      "learning_rate": 8.084538026398492e-05,
      "loss": 0.9837,
      "step": 37310
    },
    {
      "epoch": 2.1947805631110784,
      "grad_norm": 0.36988380551338196,
      "learning_rate": 8.078645505971086e-05,
      "loss": 1.0505,
      "step": 37320
    },
    {
      "epoch": 2.195368668676027,
      "grad_norm": 0.38857218623161316,
      "learning_rate": 8.072752985543682e-05,
      "loss": 0.9832,
      "step": 37330
    },
    {
      "epoch": 2.1959567742409765,
      "grad_norm": 0.3484189212322235,
      "learning_rate": 8.066860465116278e-05,
      "loss": 0.9069,
      "step": 37340
    },
    {
      "epoch": 2.1965448798059253,
      "grad_norm": 0.36432597041130066,
      "learning_rate": 8.060967944688874e-05,
      "loss": 1.0404,
      "step": 37350
    },
    {
      "epoch": 2.197132985370874,
      "grad_norm": 0.37717971205711365,
      "learning_rate": 8.05507542426147e-05,
      "loss": 1.0504,
      "step": 37360
    },
    {
      "epoch": 2.197721090935823,
      "grad_norm": 0.3097587525844574,
      "learning_rate": 8.049182903834065e-05,
      "loss": 0.978,
      "step": 37370
    },
    {
      "epoch": 2.1983091965007717,
      "grad_norm": 0.33365657925605774,
      "learning_rate": 8.043290383406661e-05,
      "loss": 0.9891,
      "step": 37380
    },
    {
      "epoch": 2.198897302065721,
      "grad_norm": 0.39925599098205566,
      "learning_rate": 8.037397862979257e-05,
      "loss": 0.9356,
      "step": 37390
    },
    {
      "epoch": 2.19948540763067,
      "grad_norm": 0.40314435958862305,
      "learning_rate": 8.031505342551853e-05,
      "loss": 0.9479,
      "step": 37400
    },
    {
      "epoch": 2.2000735131956186,
      "grad_norm": 0.36565861105918884,
      "learning_rate": 8.025612822124449e-05,
      "loss": 1.0844,
      "step": 37410
    },
    {
      "epoch": 2.2006616187605674,
      "grad_norm": 0.37028199434280396,
      "learning_rate": 8.019720301697045e-05,
      "loss": 0.9764,
      "step": 37420
    },
    {
      "epoch": 2.2012497243255162,
      "grad_norm": 0.38858985900878906,
      "learning_rate": 8.013827781269642e-05,
      "loss": 0.9365,
      "step": 37430
    },
    {
      "epoch": 2.2018378298904655,
      "grad_norm": 0.34240394830703735,
      "learning_rate": 8.007935260842238e-05,
      "loss": 1.0308,
      "step": 37440
    },
    {
      "epoch": 2.2024259354554143,
      "grad_norm": 0.34934911131858826,
      "learning_rate": 8.002042740414832e-05,
      "loss": 1.0493,
      "step": 37450
    },
    {
      "epoch": 2.203014041020363,
      "grad_norm": 0.36955249309539795,
      "learning_rate": 7.996150219987429e-05,
      "loss": 0.9898,
      "step": 37460
    },
    {
      "epoch": 2.203602146585312,
      "grad_norm": 0.40501928329467773,
      "learning_rate": 7.990257699560025e-05,
      "loss": 0.9917,
      "step": 37470
    },
    {
      "epoch": 2.2041902521502608,
      "grad_norm": 0.3755263388156891,
      "learning_rate": 7.984365179132621e-05,
      "loss": 0.8699,
      "step": 37480
    },
    {
      "epoch": 2.20477835771521,
      "grad_norm": 0.45160502195358276,
      "learning_rate": 7.978472658705217e-05,
      "loss": 1.0101,
      "step": 37490
    },
    {
      "epoch": 2.205366463280159,
      "grad_norm": 0.31679314374923706,
      "learning_rate": 7.972580138277813e-05,
      "loss": 1.0171,
      "step": 37500
    },
    {
      "epoch": 2.2059545688451077,
      "grad_norm": 0.43980592489242554,
      "learning_rate": 7.966687617850409e-05,
      "loss": 1.0396,
      "step": 37510
    },
    {
      "epoch": 2.2065426744100565,
      "grad_norm": 0.38974931836128235,
      "learning_rate": 7.960795097423004e-05,
      "loss": 0.9395,
      "step": 37520
    },
    {
      "epoch": 2.2071307799750057,
      "grad_norm": 0.3934083878993988,
      "learning_rate": 7.954902576995599e-05,
      "loss": 1.0179,
      "step": 37530
    },
    {
      "epoch": 2.2077188855399545,
      "grad_norm": 0.3862577974796295,
      "learning_rate": 7.949010056568195e-05,
      "loss": 1.0592,
      "step": 37540
    },
    {
      "epoch": 2.2083069911049034,
      "grad_norm": 0.3930305540561676,
      "learning_rate": 7.943117536140791e-05,
      "loss": 0.991,
      "step": 37550
    },
    {
      "epoch": 2.208895096669852,
      "grad_norm": 0.41920897364616394,
      "learning_rate": 7.937225015713387e-05,
      "loss": 1.046,
      "step": 37560
    },
    {
      "epoch": 2.209483202234801,
      "grad_norm": 0.39566731452941895,
      "learning_rate": 7.931332495285982e-05,
      "loss": 1.0932,
      "step": 37570
    },
    {
      "epoch": 2.2100713077997503,
      "grad_norm": 0.3323931097984314,
      "learning_rate": 7.925439974858578e-05,
      "loss": 1.0479,
      "step": 37580
    },
    {
      "epoch": 2.210659413364699,
      "grad_norm": 0.353613018989563,
      "learning_rate": 7.919547454431174e-05,
      "loss": 0.9614,
      "step": 37590
    },
    {
      "epoch": 2.211247518929648,
      "grad_norm": 0.36891281604766846,
      "learning_rate": 7.91365493400377e-05,
      "loss": 1.1147,
      "step": 37600
    },
    {
      "epoch": 2.2118356244945967,
      "grad_norm": 0.3690737783908844,
      "learning_rate": 7.907762413576366e-05,
      "loss": 1.0386,
      "step": 37610
    },
    {
      "epoch": 2.2124237300595455,
      "grad_norm": 0.39862245321273804,
      "learning_rate": 7.901869893148962e-05,
      "loss": 0.9493,
      "step": 37620
    },
    {
      "epoch": 2.2130118356244948,
      "grad_norm": 0.42305564880371094,
      "learning_rate": 7.895977372721559e-05,
      "loss": 0.9746,
      "step": 37630
    },
    {
      "epoch": 2.2135999411894436,
      "grad_norm": 0.35668420791625977,
      "learning_rate": 7.890084852294155e-05,
      "loss": 1.0023,
      "step": 37640
    },
    {
      "epoch": 2.2141880467543924,
      "grad_norm": 0.3808746635913849,
      "learning_rate": 7.88419233186675e-05,
      "loss": 0.972,
      "step": 37650
    },
    {
      "epoch": 2.214776152319341,
      "grad_norm": 0.4152831435203552,
      "learning_rate": 7.878299811439346e-05,
      "loss": 0.9553,
      "step": 37660
    },
    {
      "epoch": 2.21536425788429,
      "grad_norm": 0.34581097960472107,
      "learning_rate": 7.872407291011942e-05,
      "loss": 0.96,
      "step": 37670
    },
    {
      "epoch": 2.2159523634492393,
      "grad_norm": 0.42617377638816833,
      "learning_rate": 7.866514770584538e-05,
      "loss": 1.0272,
      "step": 37680
    },
    {
      "epoch": 2.216540469014188,
      "grad_norm": 0.36029332876205444,
      "learning_rate": 7.860622250157134e-05,
      "loss": 1.0723,
      "step": 37690
    },
    {
      "epoch": 2.217128574579137,
      "grad_norm": 0.40198659896850586,
      "learning_rate": 7.85472972972973e-05,
      "loss": 1.0074,
      "step": 37700
    },
    {
      "epoch": 2.2177166801440857,
      "grad_norm": 0.33607879281044006,
      "learning_rate": 7.848837209302326e-05,
      "loss": 0.9974,
      "step": 37710
    },
    {
      "epoch": 2.2183047857090346,
      "grad_norm": 0.3977805972099304,
      "learning_rate": 7.842944688874921e-05,
      "loss": 0.9875,
      "step": 37720
    },
    {
      "epoch": 2.218892891273984,
      "grad_norm": 0.37223291397094727,
      "learning_rate": 7.837052168447517e-05,
      "loss": 0.9757,
      "step": 37730
    },
    {
      "epoch": 2.2194809968389326,
      "grad_norm": 0.3539528548717499,
      "learning_rate": 7.831159648020112e-05,
      "loss": 1.0094,
      "step": 37740
    },
    {
      "epoch": 2.2200691024038814,
      "grad_norm": 0.3967481255531311,
      "learning_rate": 7.825267127592708e-05,
      "loss": 0.886,
      "step": 37750
    },
    {
      "epoch": 2.2206572079688303,
      "grad_norm": 0.3784666359424591,
      "learning_rate": 7.819374607165304e-05,
      "loss": 0.9836,
      "step": 37760
    },
    {
      "epoch": 2.2212453135337795,
      "grad_norm": 0.38321974873542786,
      "learning_rate": 7.813482086737899e-05,
      "loss": 0.9587,
      "step": 37770
    },
    {
      "epoch": 2.2218334190987283,
      "grad_norm": 0.37778353691101074,
      "learning_rate": 7.807589566310495e-05,
      "loss": 1.087,
      "step": 37780
    },
    {
      "epoch": 2.222421524663677,
      "grad_norm": 0.34752196073532104,
      "learning_rate": 7.801697045883091e-05,
      "loss": 0.9083,
      "step": 37790
    },
    {
      "epoch": 2.223009630228626,
      "grad_norm": 0.35947662591934204,
      "learning_rate": 7.795804525455687e-05,
      "loss": 1.036,
      "step": 37800
    },
    {
      "epoch": 2.223597735793575,
      "grad_norm": 0.39200663566589355,
      "learning_rate": 7.789912005028283e-05,
      "loss": 1.0652,
      "step": 37810
    },
    {
      "epoch": 2.224185841358524,
      "grad_norm": 0.3543272316455841,
      "learning_rate": 7.78401948460088e-05,
      "loss": 1.0628,
      "step": 37820
    },
    {
      "epoch": 2.224773946923473,
      "grad_norm": 0.37816283106803894,
      "learning_rate": 7.778126964173475e-05,
      "loss": 1.1176,
      "step": 37830
    },
    {
      "epoch": 2.2253620524884217,
      "grad_norm": 0.3609936237335205,
      "learning_rate": 7.772234443746072e-05,
      "loss": 0.9704,
      "step": 37840
    },
    {
      "epoch": 2.2259501580533705,
      "grad_norm": 0.39609554409980774,
      "learning_rate": 7.766341923318666e-05,
      "loss": 1.0147,
      "step": 37850
    },
    {
      "epoch": 2.2265382636183193,
      "grad_norm": 0.3852391541004181,
      "learning_rate": 7.760449402891262e-05,
      "loss": 0.9982,
      "step": 37860
    },
    {
      "epoch": 2.2271263691832686,
      "grad_norm": 0.37250539660453796,
      "learning_rate": 7.754556882463859e-05,
      "loss": 1.0464,
      "step": 37870
    },
    {
      "epoch": 2.2277144747482174,
      "grad_norm": 0.33956238627433777,
      "learning_rate": 7.748664362036455e-05,
      "loss": 0.9483,
      "step": 37880
    },
    {
      "epoch": 2.228302580313166,
      "grad_norm": 0.40673646330833435,
      "learning_rate": 7.742771841609051e-05,
      "loss": 1.0135,
      "step": 37890
    },
    {
      "epoch": 2.228890685878115,
      "grad_norm": 0.3959093391895294,
      "learning_rate": 7.736879321181647e-05,
      "loss": 0.949,
      "step": 37900
    },
    {
      "epoch": 2.229478791443064,
      "grad_norm": 0.3657641112804413,
      "learning_rate": 7.730986800754243e-05,
      "loss": 1.0838,
      "step": 37910
    },
    {
      "epoch": 2.230066897008013,
      "grad_norm": 0.3531484007835388,
      "learning_rate": 7.725094280326838e-05,
      "loss": 1.0129,
      "step": 37920
    },
    {
      "epoch": 2.230655002572962,
      "grad_norm": 0.3596988916397095,
      "learning_rate": 7.719201759899434e-05,
      "loss": 0.9287,
      "step": 37930
    },
    {
      "epoch": 2.2312431081379107,
      "grad_norm": 0.3829471170902252,
      "learning_rate": 7.71330923947203e-05,
      "loss": 1.0337,
      "step": 37940
    },
    {
      "epoch": 2.2318312137028595,
      "grad_norm": 0.36945840716362,
      "learning_rate": 7.707416719044625e-05,
      "loss": 1.0489,
      "step": 37950
    },
    {
      "epoch": 2.232419319267809,
      "grad_norm": 0.33613836765289307,
      "learning_rate": 7.701524198617221e-05,
      "loss": 1.0189,
      "step": 37960
    },
    {
      "epoch": 2.2330074248327576,
      "grad_norm": 0.42158442735671997,
      "learning_rate": 7.695631678189816e-05,
      "loss": 1.0459,
      "step": 37970
    },
    {
      "epoch": 2.2335955303977064,
      "grad_norm": 0.3694537281990051,
      "learning_rate": 7.689739157762412e-05,
      "loss": 1.0751,
      "step": 37980
    },
    {
      "epoch": 2.2341836359626552,
      "grad_norm": 0.42846304178237915,
      "learning_rate": 7.683846637335008e-05,
      "loss": 1.0047,
      "step": 37990
    },
    {
      "epoch": 2.234771741527604,
      "grad_norm": 0.3437451720237732,
      "learning_rate": 7.677954116907604e-05,
      "loss": 1.0916,
      "step": 38000
    },
    {
      "epoch": 2.2353598470925533,
      "grad_norm": 0.3807920217514038,
      "learning_rate": 7.6720615964802e-05,
      "loss": 0.9337,
      "step": 38010
    },
    {
      "epoch": 2.235947952657502,
      "grad_norm": 0.36154744029045105,
      "learning_rate": 7.666169076052796e-05,
      "loss": 1.0601,
      "step": 38020
    },
    {
      "epoch": 2.236536058222451,
      "grad_norm": 0.37565523386001587,
      "learning_rate": 7.660276555625392e-05,
      "loss": 0.9972,
      "step": 38030
    },
    {
      "epoch": 2.2371241637873998,
      "grad_norm": 0.412291020154953,
      "learning_rate": 7.654384035197989e-05,
      "loss": 1.0405,
      "step": 38040
    },
    {
      "epoch": 2.2377122693523486,
      "grad_norm": 0.37123414874076843,
      "learning_rate": 7.648491514770583e-05,
      "loss": 1.0009,
      "step": 38050
    },
    {
      "epoch": 2.238300374917298,
      "grad_norm": 0.33206963539123535,
      "learning_rate": 7.64259899434318e-05,
      "loss": 0.8763,
      "step": 38060
    },
    {
      "epoch": 2.2388884804822466,
      "grad_norm": NaN,
      "learning_rate": 7.636706473915776e-05,
      "loss": 0.9713,
      "step": 38070
    },
    {
      "epoch": 2.2394765860471955,
      "grad_norm": 0.404087096452713,
      "learning_rate": 7.631403205531111e-05,
      "loss": 0.9847,
      "step": 38080
    },
    {
      "epoch": 2.2400646916121443,
      "grad_norm": 0.3460196852684021,
      "learning_rate": 7.625510685103707e-05,
      "loss": 1.0032,
      "step": 38090
    },
    {
      "epoch": 2.240652797177093,
      "grad_norm": 0.39338406920433044,
      "learning_rate": 7.619618164676303e-05,
      "loss": 0.9946,
      "step": 38100
    },
    {
      "epoch": 2.2412409027420424,
      "grad_norm": 0.4198448956012726,
      "learning_rate": 7.6137256442489e-05,
      "loss": 1.0953,
      "step": 38110
    },
    {
      "epoch": 2.241829008306991,
      "grad_norm": 0.4151395857334137,
      "learning_rate": 7.607833123821496e-05,
      "loss": 1.01,
      "step": 38120
    },
    {
      "epoch": 2.24241711387194,
      "grad_norm": 0.37685349583625793,
      "learning_rate": 7.60194060339409e-05,
      "loss": 1.0166,
      "step": 38130
    },
    {
      "epoch": 2.243005219436889,
      "grad_norm": 0.3148682117462158,
      "learning_rate": 7.596048082966687e-05,
      "loss": 0.9343,
      "step": 38140
    },
    {
      "epoch": 2.2435933250018376,
      "grad_norm": 0.46403250098228455,
      "learning_rate": 7.590155562539283e-05,
      "loss": 1.0537,
      "step": 38150
    },
    {
      "epoch": 2.244181430566787,
      "grad_norm": 0.3630557060241699,
      "learning_rate": 7.584263042111879e-05,
      "loss": 0.9666,
      "step": 38160
    },
    {
      "epoch": 2.2447695361317357,
      "grad_norm": 0.4555101692676544,
      "learning_rate": 7.578370521684475e-05,
      "loss": 1.0853,
      "step": 38170
    },
    {
      "epoch": 2.2453576416966845,
      "grad_norm": 0.44331735372543335,
      "learning_rate": 7.572478001257071e-05,
      "loss": 1.0472,
      "step": 38180
    },
    {
      "epoch": 2.2459457472616333,
      "grad_norm": 0.3714655041694641,
      "learning_rate": 7.566585480829667e-05,
      "loss": 0.9931,
      "step": 38190
    },
    {
      "epoch": 2.2465338528265826,
      "grad_norm": 0.33190980553627014,
      "learning_rate": 7.560692960402263e-05,
      "loss": 0.9622,
      "step": 38200
    },
    {
      "epoch": 2.2471219583915314,
      "grad_norm": 0.3478620946407318,
      "learning_rate": 7.554800439974858e-05,
      "loss": 0.9039,
      "step": 38210
    },
    {
      "epoch": 2.24771006395648,
      "grad_norm": 0.4011863172054291,
      "learning_rate": 7.548907919547454e-05,
      "loss": 1.0391,
      "step": 38220
    },
    {
      "epoch": 2.248298169521429,
      "grad_norm": 0.4020701050758362,
      "learning_rate": 7.54301539912005e-05,
      "loss": 1.0077,
      "step": 38230
    },
    {
      "epoch": 2.248886275086378,
      "grad_norm": 0.34712690114974976,
      "learning_rate": 7.537122878692646e-05,
      "loss": 1.0816,
      "step": 38240
    },
    {
      "epoch": 2.249474380651327,
      "grad_norm": 0.3815256953239441,
      "learning_rate": 7.531230358265241e-05,
      "loss": 0.985,
      "step": 38250
    },
    {
      "epoch": 2.250062486216276,
      "grad_norm": 0.35223498940467834,
      "learning_rate": 7.525337837837836e-05,
      "loss": 0.9537,
      "step": 38260
    },
    {
      "epoch": 2.2506505917812247,
      "grad_norm": 0.4004324972629547,
      "learning_rate": 7.519445317410432e-05,
      "loss": 0.916,
      "step": 38270
    },
    {
      "epoch": 2.2512386973461735,
      "grad_norm": 0.33709901571273804,
      "learning_rate": 7.513552796983028e-05,
      "loss": 0.9941,
      "step": 38280
    },
    {
      "epoch": 2.2518268029111224,
      "grad_norm": 0.43432825803756714,
      "learning_rate": 7.507660276555624e-05,
      "loss": 1.0576,
      "step": 38290
    },
    {
      "epoch": 2.2524149084760716,
      "grad_norm": 0.3417045474052429,
      "learning_rate": 7.50176775612822e-05,
      "loss": 0.9294,
      "step": 38300
    },
    {
      "epoch": 2.2530030140410204,
      "grad_norm": 0.3961404860019684,
      "learning_rate": 7.495875235700816e-05,
      "loss": 1.0053,
      "step": 38310
    },
    {
      "epoch": 2.2535911196059693,
      "grad_norm": 0.4193451702594757,
      "learning_rate": 7.489982715273413e-05,
      "loss": 1.1095,
      "step": 38320
    },
    {
      "epoch": 2.254179225170918,
      "grad_norm": 0.34901177883148193,
      "learning_rate": 7.484090194846007e-05,
      "loss": 0.9705,
      "step": 38330
    },
    {
      "epoch": 2.2547673307358673,
      "grad_norm": 0.4753080904483795,
      "learning_rate": 7.478197674418603e-05,
      "loss": 0.9594,
      "step": 38340
    },
    {
      "epoch": 2.255355436300816,
      "grad_norm": 0.3765251338481903,
      "learning_rate": 7.4723051539912e-05,
      "loss": 0.9657,
      "step": 38350
    },
    {
      "epoch": 2.255943541865765,
      "grad_norm": 0.33405420184135437,
      "learning_rate": 7.466412633563796e-05,
      "loss": 1.0779,
      "step": 38360
    },
    {
      "epoch": 2.2565316474307138,
      "grad_norm": 0.39225661754608154,
      "learning_rate": 7.460520113136392e-05,
      "loss": 0.9741,
      "step": 38370
    },
    {
      "epoch": 2.2571197529956626,
      "grad_norm": 0.35558784008026123,
      "learning_rate": 7.454627592708988e-05,
      "loss": 0.9887,
      "step": 38380
    },
    {
      "epoch": 2.257707858560612,
      "grad_norm": 0.3679301142692566,
      "learning_rate": 7.448735072281584e-05,
      "loss": 1.0079,
      "step": 38390
    },
    {
      "epoch": 2.2582959641255607,
      "grad_norm": 0.3748186230659485,
      "learning_rate": 7.442842551854179e-05,
      "loss": 0.9199,
      "step": 38400
    },
    {
      "epoch": 2.2588840696905095,
      "grad_norm": 0.3506608307361603,
      "learning_rate": 7.436950031426775e-05,
      "loss": 0.9056,
      "step": 38410
    },
    {
      "epoch": 2.2594721752554583,
      "grad_norm": 0.3677774667739868,
      "learning_rate": 7.431057510999371e-05,
      "loss": 1.0275,
      "step": 38420
    },
    {
      "epoch": 2.260060280820407,
      "grad_norm": 0.37244677543640137,
      "learning_rate": 7.425164990571966e-05,
      "loss": 1.0423,
      "step": 38430
    },
    {
      "epoch": 2.2606483863853564,
      "grad_norm": 0.46125346422195435,
      "learning_rate": 7.419272470144562e-05,
      "loss": 1.0314,
      "step": 38440
    },
    {
      "epoch": 2.261236491950305,
      "grad_norm": 0.35445839166641235,
      "learning_rate": 7.413379949717158e-05,
      "loss": 1.0132,
      "step": 38450
    },
    {
      "epoch": 2.261824597515254,
      "grad_norm": 0.35964441299438477,
      "learning_rate": 7.407487429289754e-05,
      "loss": 1.03,
      "step": 38460
    },
    {
      "epoch": 2.262412703080203,
      "grad_norm": 0.37733903527259827,
      "learning_rate": 7.40159490886235e-05,
      "loss": 0.9933,
      "step": 38470
    },
    {
      "epoch": 2.2630008086451516,
      "grad_norm": 0.370637446641922,
      "learning_rate": 7.395702388434946e-05,
      "loss": 1.0294,
      "step": 38480
    },
    {
      "epoch": 2.263588914210101,
      "grad_norm": 0.3729720413684845,
      "learning_rate": 7.389809868007543e-05,
      "loss": 0.9831,
      "step": 38490
    },
    {
      "epoch": 2.2641770197750497,
      "grad_norm": 0.30926352739334106,
      "learning_rate": 7.383917347580139e-05,
      "loss": 1.0156,
      "step": 38500
    },
    {
      "epoch": 2.2647651253399985,
      "grad_norm": 0.447756290435791,
      "learning_rate": 7.378024827152733e-05,
      "loss": 1.1217,
      "step": 38510
    },
    {
      "epoch": 2.2653532309049473,
      "grad_norm": 0.37687402963638306,
      "learning_rate": 7.37213230672533e-05,
      "loss": 0.9776,
      "step": 38520
    },
    {
      "epoch": 2.265941336469896,
      "grad_norm": 0.40362516045570374,
      "learning_rate": 7.366239786297924e-05,
      "loss": 1.0332,
      "step": 38530
    },
    {
      "epoch": 2.2665294420348454,
      "grad_norm": 0.3799434006214142,
      "learning_rate": 7.36034726587052e-05,
      "loss": 0.974,
      "step": 38540
    },
    {
      "epoch": 2.2671175475997942,
      "grad_norm": 0.37775230407714844,
      "learning_rate": 7.354454745443117e-05,
      "loss": 1.0229,
      "step": 38550
    },
    {
      "epoch": 2.267705653164743,
      "grad_norm": 0.37962278723716736,
      "learning_rate": 7.348562225015713e-05,
      "loss": 1.0062,
      "step": 38560
    },
    {
      "epoch": 2.268293758729692,
      "grad_norm": 0.41426071524620056,
      "learning_rate": 7.342669704588309e-05,
      "loss": 1.0334,
      "step": 38570
    },
    {
      "epoch": 2.2688818642946407,
      "grad_norm": 0.388204962015152,
      "learning_rate": 7.336777184160905e-05,
      "loss": 1.0161,
      "step": 38580
    },
    {
      "epoch": 2.26946996985959,
      "grad_norm": 0.4000314772129059,
      "learning_rate": 7.330884663733501e-05,
      "loss": 1.0468,
      "step": 38590
    },
    {
      "epoch": 2.2700580754245387,
      "grad_norm": 0.3588949739933014,
      "learning_rate": 7.324992143306097e-05,
      "loss": 1.0083,
      "step": 38600
    },
    {
      "epoch": 2.2706461809894876,
      "grad_norm": 0.4025312066078186,
      "learning_rate": 7.319099622878692e-05,
      "loss": 0.9471,
      "step": 38610
    },
    {
      "epoch": 2.2712342865544364,
      "grad_norm": 0.44562268257141113,
      "learning_rate": 7.313207102451288e-05,
      "loss": 1.0513,
      "step": 38620
    },
    {
      "epoch": 2.271822392119385,
      "grad_norm": 0.35029929876327515,
      "learning_rate": 7.307314582023883e-05,
      "loss": 0.8019,
      "step": 38630
    },
    {
      "epoch": 2.2724104976843345,
      "grad_norm": 0.3579314649105072,
      "learning_rate": 7.301422061596479e-05,
      "loss": 0.9902,
      "step": 38640
    },
    {
      "epoch": 2.2729986032492833,
      "grad_norm": 0.3837129473686218,
      "learning_rate": 7.295529541169075e-05,
      "loss": 1.05,
      "step": 38650
    },
    {
      "epoch": 2.273586708814232,
      "grad_norm": 0.39653387665748596,
      "learning_rate": 7.289637020741671e-05,
      "loss": 0.9725,
      "step": 38660
    },
    {
      "epoch": 2.274174814379181,
      "grad_norm": 0.3475390374660492,
      "learning_rate": 7.283744500314267e-05,
      "loss": 1.0301,
      "step": 38670
    },
    {
      "epoch": 2.27476291994413,
      "grad_norm": 0.37484312057495117,
      "learning_rate": 7.277851979886863e-05,
      "loss": 0.9837,
      "step": 38680
    },
    {
      "epoch": 2.275351025509079,
      "grad_norm": 0.38176625967025757,
      "learning_rate": 7.27195945945946e-05,
      "loss": 1.0164,
      "step": 38690
    },
    {
      "epoch": 2.275939131074028,
      "grad_norm": 0.39308497309684753,
      "learning_rate": 7.266066939032054e-05,
      "loss": 1.0071,
      "step": 38700
    },
    {
      "epoch": 2.2765272366389766,
      "grad_norm": 0.35346174240112305,
      "learning_rate": 7.26017441860465e-05,
      "loss": 0.9663,
      "step": 38710
    },
    {
      "epoch": 2.2771153422039254,
      "grad_norm": 0.418739914894104,
      "learning_rate": 7.254281898177247e-05,
      "loss": 1.0722,
      "step": 38720
    },
    {
      "epoch": 2.2777034477688747,
      "grad_norm": 0.3759552836418152,
      "learning_rate": 7.248389377749841e-05,
      "loss": 1.0024,
      "step": 38730
    },
    {
      "epoch": 2.2782915533338235,
      "grad_norm": 0.3133223056793213,
      "learning_rate": 7.242496857322437e-05,
      "loss": 1.0036,
      "step": 38740
    },
    {
      "epoch": 2.2788796588987723,
      "grad_norm": 0.3772808015346527,
      "learning_rate": 7.236604336895034e-05,
      "loss": 1.0319,
      "step": 38750
    },
    {
      "epoch": 2.279467764463721,
      "grad_norm": 0.4276670217514038,
      "learning_rate": 7.23071181646763e-05,
      "loss": 0.9876,
      "step": 38760
    },
    {
      "epoch": 2.2800558700286704,
      "grad_norm": 0.4122319221496582,
      "learning_rate": 7.224819296040226e-05,
      "loss": 0.9047,
      "step": 38770
    },
    {
      "epoch": 2.280643975593619,
      "grad_norm": 0.392497718334198,
      "learning_rate": 7.218926775612822e-05,
      "loss": 0.9707,
      "step": 38780
    },
    {
      "epoch": 2.281232081158568,
      "grad_norm": 0.4147508144378662,
      "learning_rate": 7.213034255185418e-05,
      "loss": 0.999,
      "step": 38790
    },
    {
      "epoch": 2.281820186723517,
      "grad_norm": 0.37219545245170593,
      "learning_rate": 7.207141734758013e-05,
      "loss": 1.0151,
      "step": 38800
    },
    {
      "epoch": 2.2824082922884656,
      "grad_norm": 0.3948727250099182,
      "learning_rate": 7.201249214330609e-05,
      "loss": 0.979,
      "step": 38810
    },
    {
      "epoch": 2.282996397853415,
      "grad_norm": 0.4321069121360779,
      "learning_rate": 7.195356693903205e-05,
      "loss": 0.997,
      "step": 38820
    },
    {
      "epoch": 2.2835845034183637,
      "grad_norm": 0.3737233579158783,
      "learning_rate": 7.1894641734758e-05,
      "loss": 1.0536,
      "step": 38830
    },
    {
      "epoch": 2.2841726089833125,
      "grad_norm": 0.35562512278556824,
      "learning_rate": 7.183571653048396e-05,
      "loss": 1.1016,
      "step": 38840
    },
    {
      "epoch": 2.2847607145482614,
      "grad_norm": 0.43867582082748413,
      "learning_rate": 7.177679132620992e-05,
      "loss": 1.0561,
      "step": 38850
    },
    {
      "epoch": 2.28534882011321,
      "grad_norm": 0.3581627905368805,
      "learning_rate": 7.171786612193588e-05,
      "loss": 0.9891,
      "step": 38860
    },
    {
      "epoch": 2.2859369256781594,
      "grad_norm": 0.3908814787864685,
      "learning_rate": 7.165894091766184e-05,
      "loss": 1.0068,
      "step": 38870
    },
    {
      "epoch": 2.2865250312431082,
      "grad_norm": 0.4002227485179901,
      "learning_rate": 7.16000157133878e-05,
      "loss": 0.9602,
      "step": 38880
    },
    {
      "epoch": 2.287113136808057,
      "grad_norm": 0.36657193303108215,
      "learning_rate": 7.154109050911377e-05,
      "loss": 0.9155,
      "step": 38890
    },
    {
      "epoch": 2.287701242373006,
      "grad_norm": 0.4682648181915283,
      "learning_rate": 7.148216530483971e-05,
      "loss": 0.9597,
      "step": 38900
    },
    {
      "epoch": 2.2882893479379547,
      "grad_norm": 0.3976679742336273,
      "learning_rate": 7.142324010056567e-05,
      "loss": 1.0129,
      "step": 38910
    },
    {
      "epoch": 2.288877453502904,
      "grad_norm": 0.33583536744117737,
      "learning_rate": 7.136431489629164e-05,
      "loss": 1.0771,
      "step": 38920
    },
    {
      "epoch": 2.2894655590678528,
      "grad_norm": 0.4490334093570709,
      "learning_rate": 7.130538969201758e-05,
      "loss": 0.9974,
      "step": 38930
    },
    {
      "epoch": 2.2900536646328016,
      "grad_norm": 0.37679746747016907,
      "learning_rate": 7.124646448774354e-05,
      "loss": 0.9464,
      "step": 38940
    },
    {
      "epoch": 2.2906417701977504,
      "grad_norm": 0.35221120715141296,
      "learning_rate": 7.11875392834695e-05,
      "loss": 1.0461,
      "step": 38950
    },
    {
      "epoch": 2.291229875762699,
      "grad_norm": 0.3873271346092224,
      "learning_rate": 7.112861407919547e-05,
      "loss": 0.9904,
      "step": 38960
    },
    {
      "epoch": 2.2918179813276485,
      "grad_norm": 0.42008763551712036,
      "learning_rate": 7.106968887492143e-05,
      "loss": 0.9612,
      "step": 38970
    },
    {
      "epoch": 2.2924060868925973,
      "grad_norm": 0.39409446716308594,
      "learning_rate": 7.101076367064739e-05,
      "loss": 1.0286,
      "step": 38980
    },
    {
      "epoch": 2.292994192457546,
      "grad_norm": 0.38071659207344055,
      "learning_rate": 7.095183846637335e-05,
      "loss": 1.1219,
      "step": 38990
    },
    {
      "epoch": 2.293582298022495,
      "grad_norm": 0.34501388669013977,
      "learning_rate": 7.08929132620993e-05,
      "loss": 0.9998,
      "step": 39000
    },
    {
      "epoch": 2.2941704035874437,
      "grad_norm": 0.4231052100658417,
      "learning_rate": 7.083398805782526e-05,
      "loss": 0.9828,
      "step": 39010
    },
    {
      "epoch": 2.294758509152393,
      "grad_norm": 0.4342377483844757,
      "learning_rate": 7.077506285355122e-05,
      "loss": 1.0699,
      "step": 39020
    },
    {
      "epoch": 2.295346614717342,
      "grad_norm": 0.38753628730773926,
      "learning_rate": 7.071613764927717e-05,
      "loss": 0.9621,
      "step": 39030
    },
    {
      "epoch": 2.2959347202822906,
      "grad_norm": 0.43711259961128235,
      "learning_rate": 7.065721244500313e-05,
      "loss": 0.9838,
      "step": 39040
    },
    {
      "epoch": 2.2965228258472394,
      "grad_norm": 0.4032125174999237,
      "learning_rate": 7.059828724072909e-05,
      "loss": 0.9581,
      "step": 39050
    },
    {
      "epoch": 2.2971109314121883,
      "grad_norm": 0.3736952543258667,
      "learning_rate": 7.053936203645505e-05,
      "loss": 1.0482,
      "step": 39060
    },
    {
      "epoch": 2.2976990369771375,
      "grad_norm": 0.37177953124046326,
      "learning_rate": 7.048043683218101e-05,
      "loss": 0.971,
      "step": 39070
    },
    {
      "epoch": 2.2982871425420863,
      "grad_norm": 0.3523291051387787,
      "learning_rate": 7.042151162790697e-05,
      "loss": 0.9658,
      "step": 39080
    },
    {
      "epoch": 2.298875248107035,
      "grad_norm": 0.4058360159397125,
      "learning_rate": 7.036258642363294e-05,
      "loss": 0.9771,
      "step": 39090
    },
    {
      "epoch": 2.299463353671984,
      "grad_norm": 0.4346550703048706,
      "learning_rate": 7.030366121935888e-05,
      "loss": 1.0471,
      "step": 39100
    },
    {
      "epoch": 2.300051459236933,
      "grad_norm": 0.38565605878829956,
      "learning_rate": 7.024473601508484e-05,
      "loss": 1.0286,
      "step": 39110
    },
    {
      "epoch": 2.300639564801882,
      "grad_norm": 0.38949379324913025,
      "learning_rate": 7.01858108108108e-05,
      "loss": 0.9894,
      "step": 39120
    },
    {
      "epoch": 2.301227670366831,
      "grad_norm": 0.32266882061958313,
      "learning_rate": 7.012688560653675e-05,
      "loss": 0.9355,
      "step": 39130
    },
    {
      "epoch": 2.3018157759317797,
      "grad_norm": 0.3938552439212799,
      "learning_rate": 7.006796040226271e-05,
      "loss": 1.0066,
      "step": 39140
    },
    {
      "epoch": 2.3024038814967285,
      "grad_norm": 0.3981664478778839,
      "learning_rate": 7.000903519798868e-05,
      "loss": 0.9931,
      "step": 39150
    },
    {
      "epoch": 2.3029919870616777,
      "grad_norm": 0.3113463819026947,
      "learning_rate": 6.995010999371464e-05,
      "loss": 1.0671,
      "step": 39160
    },
    {
      "epoch": 2.3035800926266266,
      "grad_norm": 0.41046836972236633,
      "learning_rate": 6.98911847894406e-05,
      "loss": 1.0107,
      "step": 39170
    },
    {
      "epoch": 2.3041681981915754,
      "grad_norm": 0.3215433955192566,
      "learning_rate": 6.983225958516656e-05,
      "loss": 1.0469,
      "step": 39180
    },
    {
      "epoch": 2.304756303756524,
      "grad_norm": 0.38025572896003723,
      "learning_rate": 6.977333438089252e-05,
      "loss": 1.0074,
      "step": 39190
    },
    {
      "epoch": 2.3053444093214734,
      "grad_norm": 0.431219220161438,
      "learning_rate": 6.971440917661847e-05,
      "loss": 0.9473,
      "step": 39200
    },
    {
      "epoch": 2.3059325148864223,
      "grad_norm": 0.4217194616794586,
      "learning_rate": 6.965548397234443e-05,
      "loss": 0.979,
      "step": 39210
    },
    {
      "epoch": 2.306520620451371,
      "grad_norm": 0.36935555934906006,
      "learning_rate": 6.959655876807039e-05,
      "loss": 1.0043,
      "step": 39220
    },
    {
      "epoch": 2.30710872601632,
      "grad_norm": 0.40762385725975037,
      "learning_rate": 6.953763356379635e-05,
      "loss": 1.0337,
      "step": 39230
    },
    {
      "epoch": 2.3076968315812687,
      "grad_norm": 0.3992079496383667,
      "learning_rate": 6.94787083595223e-05,
      "loss": 1.0533,
      "step": 39240
    },
    {
      "epoch": 2.308284937146218,
      "grad_norm": 0.3527950644493103,
      "learning_rate": 6.941978315524826e-05,
      "loss": 0.9368,
      "step": 39250
    },
    {
      "epoch": 2.308873042711167,
      "grad_norm": 0.4653284251689911,
      "learning_rate": 6.936085795097422e-05,
      "loss": 1.0334,
      "step": 39260
    },
    {
      "epoch": 2.3094611482761156,
      "grad_norm": 0.44879597425460815,
      "learning_rate": 6.930193274670018e-05,
      "loss": 0.9458,
      "step": 39270
    },
    {
      "epoch": 2.3100492538410644,
      "grad_norm": 0.4625910520553589,
      "learning_rate": 6.924300754242614e-05,
      "loss": 0.9673,
      "step": 39280
    },
    {
      "epoch": 2.3106373594060132,
      "grad_norm": 0.4131387770175934,
      "learning_rate": 6.91840823381521e-05,
      "loss": 0.9432,
      "step": 39290
    },
    {
      "epoch": 2.3112254649709625,
      "grad_norm": 0.3753945827484131,
      "learning_rate": 6.912515713387805e-05,
      "loss": 1.0616,
      "step": 39300
    },
    {
      "epoch": 2.3118135705359113,
      "grad_norm": 0.39948517084121704,
      "learning_rate": 6.906623192960401e-05,
      "loss": 0.9416,
      "step": 39310
    },
    {
      "epoch": 2.31240167610086,
      "grad_norm": 0.41510120034217834,
      "learning_rate": 6.900730672532997e-05,
      "loss": 1.1293,
      "step": 39320
    },
    {
      "epoch": 2.312989781665809,
      "grad_norm": 0.3548029363155365,
      "learning_rate": 6.894838152105594e-05,
      "loss": 0.9836,
      "step": 39330
    },
    {
      "epoch": 2.3135778872307577,
      "grad_norm": 0.3763059675693512,
      "learning_rate": 6.888945631678188e-05,
      "loss": 0.9688,
      "step": 39340
    },
    {
      "epoch": 2.314165992795707,
      "grad_norm": 0.4106729328632355,
      "learning_rate": 6.883053111250784e-05,
      "loss": 1.0297,
      "step": 39350
    },
    {
      "epoch": 2.314754098360656,
      "grad_norm": 0.355133056640625,
      "learning_rate": 6.87716059082338e-05,
      "loss": 0.9897,
      "step": 39360
    },
    {
      "epoch": 2.3153422039256046,
      "grad_norm": 0.3915221095085144,
      "learning_rate": 6.871268070395977e-05,
      "loss": 0.9942,
      "step": 39370
    },
    {
      "epoch": 2.3159303094905535,
      "grad_norm": 0.4049055576324463,
      "learning_rate": 6.865375549968573e-05,
      "loss": 0.9792,
      "step": 39380
    },
    {
      "epoch": 2.3165184150555023,
      "grad_norm": 0.41032150387763977,
      "learning_rate": 6.859483029541169e-05,
      "loss": 0.9333,
      "step": 39390
    },
    {
      "epoch": 2.3171065206204515,
      "grad_norm": 0.37320223450660706,
      "learning_rate": 6.853590509113764e-05,
      "loss": 0.9919,
      "step": 39400
    },
    {
      "epoch": 2.3176946261854003,
      "grad_norm": 0.37979137897491455,
      "learning_rate": 6.84769798868636e-05,
      "loss": 0.9572,
      "step": 39410
    },
    {
      "epoch": 2.318282731750349,
      "grad_norm": 0.43799278140068054,
      "learning_rate": 6.841805468258956e-05,
      "loss": 0.9424,
      "step": 39420
    },
    {
      "epoch": 2.318870837315298,
      "grad_norm": 0.4227069914340973,
      "learning_rate": 6.835912947831552e-05,
      "loss": 0.9226,
      "step": 39430
    },
    {
      "epoch": 2.319458942880247,
      "grad_norm": 0.35762763023376465,
      "learning_rate": 6.830020427404148e-05,
      "loss": 1.0262,
      "step": 39440
    },
    {
      "epoch": 2.320047048445196,
      "grad_norm": 0.3848508894443512,
      "learning_rate": 6.824127906976743e-05,
      "loss": 0.9378,
      "step": 39450
    },
    {
      "epoch": 2.320635154010145,
      "grad_norm": 0.37423697113990784,
      "learning_rate": 6.818235386549339e-05,
      "loss": 0.9766,
      "step": 39460
    },
    {
      "epoch": 2.3212232595750937,
      "grad_norm": 0.3113398551940918,
      "learning_rate": 6.812342866121935e-05,
      "loss": 1.047,
      "step": 39470
    },
    {
      "epoch": 2.3218113651400425,
      "grad_norm": 0.3576924800872803,
      "learning_rate": 6.806450345694531e-05,
      "loss": 0.9377,
      "step": 39480
    },
    {
      "epoch": 2.3223994707049913,
      "grad_norm": 0.4009740948677063,
      "learning_rate": 6.800557825267127e-05,
      "loss": 1.1084,
      "step": 39490
    },
    {
      "epoch": 2.3229875762699406,
      "grad_norm": 0.38427969813346863,
      "learning_rate": 6.794665304839722e-05,
      "loss": 1.0113,
      "step": 39500
    },
    {
      "epoch": 2.3235756818348894,
      "grad_norm": 0.3723314106464386,
      "learning_rate": 6.788772784412318e-05,
      "loss": 0.9569,
      "step": 39510
    },
    {
      "epoch": 2.324163787399838,
      "grad_norm": 0.3795701861381531,
      "learning_rate": 6.782880263984914e-05,
      "loss": 1.0296,
      "step": 39520
    },
    {
      "epoch": 2.324751892964787,
      "grad_norm": 0.3831966519355774,
      "learning_rate": 6.77698774355751e-05,
      "loss": 0.9899,
      "step": 39530
    },
    {
      "epoch": 2.3253399985297363,
      "grad_norm": 0.3666419982910156,
      "learning_rate": 6.771095223130107e-05,
      "loss": 1.0585,
      "step": 39540
    },
    {
      "epoch": 2.325928104094685,
      "grad_norm": 0.36906665563583374,
      "learning_rate": 6.765202702702701e-05,
      "loss": 0.9452,
      "step": 39550
    },
    {
      "epoch": 2.326516209659634,
      "grad_norm": 0.3800193667411804,
      "learning_rate": 6.759310182275298e-05,
      "loss": 1.0175,
      "step": 39560
    },
    {
      "epoch": 2.3271043152245827,
      "grad_norm": 0.4257373809814453,
      "learning_rate": 6.753417661847894e-05,
      "loss": 0.9793,
      "step": 39570
    },
    {
      "epoch": 2.3276924207895315,
      "grad_norm": 0.352088987827301,
      "learning_rate": 6.74752514142049e-05,
      "loss": 0.9446,
      "step": 39580
    },
    {
      "epoch": 2.328280526354481,
      "grad_norm": 0.3611021339893341,
      "learning_rate": 6.741632620993086e-05,
      "loss": 0.9773,
      "step": 39590
    },
    {
      "epoch": 2.3288686319194296,
      "grad_norm": 0.34727156162261963,
      "learning_rate": 6.735740100565681e-05,
      "loss": 1.0745,
      "step": 39600
    },
    {
      "epoch": 2.3294567374843784,
      "grad_norm": 0.3633574843406677,
      "learning_rate": 6.729847580138277e-05,
      "loss": 0.9173,
      "step": 39610
    },
    {
      "epoch": 2.3300448430493272,
      "grad_norm": 0.3426189124584198,
      "learning_rate": 6.723955059710873e-05,
      "loss": 0.9717,
      "step": 39620
    },
    {
      "epoch": 2.3306329486142765,
      "grad_norm": 0.4253324568271637,
      "learning_rate": 6.718062539283469e-05,
      "loss": 1.0069,
      "step": 39630
    },
    {
      "epoch": 2.3312210541792253,
      "grad_norm": 0.4022383987903595,
      "learning_rate": 6.712170018856065e-05,
      "loss": 1.0658,
      "step": 39640
    },
    {
      "epoch": 2.331809159744174,
      "grad_norm": 0.4081665873527527,
      "learning_rate": 6.706277498428661e-05,
      "loss": 1.0862,
      "step": 39650
    },
    {
      "epoch": 2.332397265309123,
      "grad_norm": 0.36436355113983154,
      "learning_rate": 6.700384978001256e-05,
      "loss": 0.9878,
      "step": 39660
    },
    {
      "epoch": 2.3329853708740718,
      "grad_norm": 0.4264126121997833,
      "learning_rate": 6.694492457573852e-05,
      "loss": 1.0851,
      "step": 39670
    },
    {
      "epoch": 2.333573476439021,
      "grad_norm": 0.384782075881958,
      "learning_rate": 6.688599937146448e-05,
      "loss": 0.9333,
      "step": 39680
    },
    {
      "epoch": 2.33416158200397,
      "grad_norm": 0.4026111364364624,
      "learning_rate": 6.682707416719044e-05,
      "loss": 1.035,
      "step": 39690
    },
    {
      "epoch": 2.3347496875689187,
      "grad_norm": 0.4516589343547821,
      "learning_rate": 6.676814896291639e-05,
      "loss": 1.0008,
      "step": 39700
    },
    {
      "epoch": 2.3353377931338675,
      "grad_norm": 0.41845962405204773,
      "learning_rate": 6.670922375864235e-05,
      "loss": 1.0171,
      "step": 39710
    },
    {
      "epoch": 2.3359258986988163,
      "grad_norm": 0.379934161901474,
      "learning_rate": 6.665029855436831e-05,
      "loss": 0.9074,
      "step": 39720
    },
    {
      "epoch": 2.3365140042637655,
      "grad_norm": 0.37877923250198364,
      "learning_rate": 6.659137335009428e-05,
      "loss": 0.9776,
      "step": 39730
    },
    {
      "epoch": 2.3371021098287144,
      "grad_norm": 0.41217878460884094,
      "learning_rate": 6.653244814582024e-05,
      "loss": 1.005,
      "step": 39740
    },
    {
      "epoch": 2.337690215393663,
      "grad_norm": 0.3480764627456665,
      "learning_rate": 6.64735229415462e-05,
      "loss": 0.9645,
      "step": 39750
    },
    {
      "epoch": 2.338278320958612,
      "grad_norm": 0.3423563241958618,
      "learning_rate": 6.641459773727215e-05,
      "loss": 0.998,
      "step": 39760
    },
    {
      "epoch": 2.338866426523561,
      "grad_norm": 0.42927002906799316,
      "learning_rate": 6.635567253299811e-05,
      "loss": 0.9698,
      "step": 39770
    },
    {
      "epoch": 2.33945453208851,
      "grad_norm": 0.3744545578956604,
      "learning_rate": 6.629674732872407e-05,
      "loss": 1.0277,
      "step": 39780
    },
    {
      "epoch": 2.340042637653459,
      "grad_norm": 0.34007468819618225,
      "learning_rate": 6.623782212445003e-05,
      "loss": 0.9177,
      "step": 39790
    },
    {
      "epoch": 2.3406307432184077,
      "grad_norm": 0.403256356716156,
      "learning_rate": 6.617889692017598e-05,
      "loss": 0.9761,
      "step": 39800
    },
    {
      "epoch": 2.3412188487833565,
      "grad_norm": 0.4342447817325592,
      "learning_rate": 6.611997171590194e-05,
      "loss": 1.0322,
      "step": 39810
    },
    {
      "epoch": 2.3418069543483053,
      "grad_norm": 0.4178664982318878,
      "learning_rate": 6.60610465116279e-05,
      "loss": 0.9526,
      "step": 39820
    },
    {
      "epoch": 2.3423950599132546,
      "grad_norm": 0.34862494468688965,
      "learning_rate": 6.600212130735386e-05,
      "loss": 1.0742,
      "step": 39830
    },
    {
      "epoch": 2.3429831654782034,
      "grad_norm": 0.355130136013031,
      "learning_rate": 6.594319610307982e-05,
      "loss": 1.0161,
      "step": 39840
    },
    {
      "epoch": 2.343571271043152,
      "grad_norm": 0.4257957637310028,
      "learning_rate": 6.588427089880578e-05,
      "loss": 1.0186,
      "step": 39850
    },
    {
      "epoch": 2.344159376608101,
      "grad_norm": 0.4134558141231537,
      "learning_rate": 6.582534569453174e-05,
      "loss": 1.0435,
      "step": 39860
    },
    {
      "epoch": 2.34474748217305,
      "grad_norm": 0.3681051433086395,
      "learning_rate": 6.576642049025769e-05,
      "loss": 0.9444,
      "step": 39870
    },
    {
      "epoch": 2.345335587737999,
      "grad_norm": 0.37934768199920654,
      "learning_rate": 6.570749528598365e-05,
      "loss": 1.0206,
      "step": 39880
    },
    {
      "epoch": 2.345923693302948,
      "grad_norm": 0.3686491847038269,
      "learning_rate": 6.564857008170961e-05,
      "loss": 1.0321,
      "step": 39890
    },
    {
      "epoch": 2.3465117988678967,
      "grad_norm": 0.34898725152015686,
      "learning_rate": 6.558964487743556e-05,
      "loss": 1.0295,
      "step": 39900
    },
    {
      "epoch": 2.3470999044328456,
      "grad_norm": 0.34351032972335815,
      "learning_rate": 6.553071967316152e-05,
      "loss": 1.0287,
      "step": 39910
    },
    {
      "epoch": 2.3476880099977944,
      "grad_norm": 0.44393715262413025,
      "learning_rate": 6.547179446888748e-05,
      "loss": 1.0261,
      "step": 39920
    },
    {
      "epoch": 2.3482761155627436,
      "grad_norm": 0.36776965856552124,
      "learning_rate": 6.541286926461345e-05,
      "loss": 0.9829,
      "step": 39930
    },
    {
      "epoch": 2.3488642211276924,
      "grad_norm": 0.33529946208000183,
      "learning_rate": 6.53539440603394e-05,
      "loss": 1.0001,
      "step": 39940
    },
    {
      "epoch": 2.3494523266926413,
      "grad_norm": 0.3244617283344269,
      "learning_rate": 6.529501885606537e-05,
      "loss": 0.8701,
      "step": 39950
    },
    {
      "epoch": 2.35004043225759,
      "grad_norm": 0.31892144680023193,
      "learning_rate": 6.523609365179133e-05,
      "loss": 1.0401,
      "step": 39960
    },
    {
      "epoch": 2.3506285378225393,
      "grad_norm": 0.42773571610450745,
      "learning_rate": 6.517716844751728e-05,
      "loss": 1.0398,
      "step": 39970
    },
    {
      "epoch": 2.351216643387488,
      "grad_norm": 0.4073984622955322,
      "learning_rate": 6.511824324324324e-05,
      "loss": 0.9846,
      "step": 39980
    },
    {
      "epoch": 2.351804748952437,
      "grad_norm": 0.3820739984512329,
      "learning_rate": 6.50593180389692e-05,
      "loss": 1.0371,
      "step": 39990
    },
    {
      "epoch": 2.352392854517386,
      "grad_norm": 0.42521655559539795,
      "learning_rate": 6.500039283469515e-05,
      "loss": 1.0074,
      "step": 40000
    },
    {
      "epoch": 2.3529809600823346,
      "grad_norm": 0.3765609860420227,
      "learning_rate": 6.494146763042111e-05,
      "loss": 0.9953,
      "step": 40010
    },
    {
      "epoch": 2.353569065647284,
      "grad_norm": 0.3213924169540405,
      "learning_rate": 6.488254242614707e-05,
      "loss": 1.0664,
      "step": 40020
    },
    {
      "epoch": 2.3541571712122327,
      "grad_norm": 0.4123045802116394,
      "learning_rate": 6.482361722187303e-05,
      "loss": 1.0135,
      "step": 40030
    },
    {
      "epoch": 2.3547452767771815,
      "grad_norm": 0.3624420762062073,
      "learning_rate": 6.476469201759899e-05,
      "loss": 0.9708,
      "step": 40040
    },
    {
      "epoch": 2.3553333823421303,
      "grad_norm": 0.349342942237854,
      "learning_rate": 6.470576681332495e-05,
      "loss": 1.0414,
      "step": 40050
    },
    {
      "epoch": 2.3559214879070796,
      "grad_norm": 0.38916537165641785,
      "learning_rate": 6.464684160905091e-05,
      "loss": 1.0318,
      "step": 40060
    },
    {
      "epoch": 2.3565095934720284,
      "grad_norm": 0.3417682349681854,
      "learning_rate": 6.458791640477686e-05,
      "loss": 1.0807,
      "step": 40070
    },
    {
      "epoch": 2.357097699036977,
      "grad_norm": 0.33005523681640625,
      "learning_rate": 6.453488372093023e-05,
      "loss": 1.085,
      "step": 40080
    },
    {
      "epoch": 2.357685804601926,
      "grad_norm": 0.3343586027622223,
      "learning_rate": 6.447595851665618e-05,
      "loss": 1.0254,
      "step": 40090
    },
    {
      "epoch": 2.358273910166875,
      "grad_norm": 0.35057100653648376,
      "learning_rate": 6.441703331238214e-05,
      "loss": 0.9854,
      "step": 40100
    },
    {
      "epoch": 2.358862015731824,
      "grad_norm": 0.3697534203529358,
      "learning_rate": 6.43581081081081e-05,
      "loss": 1.0081,
      "step": 40110
    },
    {
      "epoch": 2.359450121296773,
      "grad_norm": 0.42332395911216736,
      "learning_rate": 6.429918290383406e-05,
      "loss": 0.9794,
      "step": 40120
    },
    {
      "epoch": 2.3600382268617217,
      "grad_norm": 0.39266088604927063,
      "learning_rate": 6.424025769956002e-05,
      "loss": 0.9852,
      "step": 40130
    },
    {
      "epoch": 2.3606263324266705,
      "grad_norm": 0.3479563593864441,
      "learning_rate": 6.418133249528598e-05,
      "loss": 0.9992,
      "step": 40140
    },
    {
      "epoch": 2.3612144379916193,
      "grad_norm": 0.4051280915737152,
      "learning_rate": 6.412240729101195e-05,
      "loss": 1.0184,
      "step": 40150
    },
    {
      "epoch": 2.3618025435565686,
      "grad_norm": 0.4208100736141205,
      "learning_rate": 6.406348208673789e-05,
      "loss": 1.0477,
      "step": 40160
    },
    {
      "epoch": 2.3623906491215174,
      "grad_norm": 0.35562169551849365,
      "learning_rate": 6.400455688246385e-05,
      "loss": 1.0538,
      "step": 40170
    },
    {
      "epoch": 2.3629787546864662,
      "grad_norm": 0.3379301130771637,
      "learning_rate": 6.394563167818982e-05,
      "loss": 1.1267,
      "step": 40180
    },
    {
      "epoch": 2.363566860251415,
      "grad_norm": 0.389643132686615,
      "learning_rate": 6.388670647391576e-05,
      "loss": 1.0168,
      "step": 40190
    },
    {
      "epoch": 2.364154965816364,
      "grad_norm": 0.3805403411388397,
      "learning_rate": 6.382778126964172e-05,
      "loss": 0.9531,
      "step": 40200
    },
    {
      "epoch": 2.364743071381313,
      "grad_norm": 0.4078199565410614,
      "learning_rate": 6.376885606536769e-05,
      "loss": 1.0097,
      "step": 40210
    },
    {
      "epoch": 2.365331176946262,
      "grad_norm": 0.3462417423725128,
      "learning_rate": 6.370993086109365e-05,
      "loss": 1.0349,
      "step": 40220
    },
    {
      "epoch": 2.3659192825112108,
      "grad_norm": 0.3567469120025635,
      "learning_rate": 6.365100565681961e-05,
      "loss": 1.0271,
      "step": 40230
    },
    {
      "epoch": 2.3665073880761596,
      "grad_norm": 0.3710134029388428,
      "learning_rate": 6.359208045254557e-05,
      "loss": 1.0228,
      "step": 40240
    },
    {
      "epoch": 2.3670954936411084,
      "grad_norm": 0.3430696129798889,
      "learning_rate": 6.353315524827153e-05,
      "loss": 1.0274,
      "step": 40250
    },
    {
      "epoch": 2.3676835992060576,
      "grad_norm": 0.4211600422859192,
      "learning_rate": 6.347423004399748e-05,
      "loss": 1.1547,
      "step": 40260
    },
    {
      "epoch": 2.3682717047710065,
      "grad_norm": 0.3406979739665985,
      "learning_rate": 6.341530483972344e-05,
      "loss": 0.9978,
      "step": 40270
    },
    {
      "epoch": 2.3688598103359553,
      "grad_norm": 0.4113385081291199,
      "learning_rate": 6.33563796354494e-05,
      "loss": 1.0185,
      "step": 40280
    },
    {
      "epoch": 2.369447915900904,
      "grad_norm": 0.3524216413497925,
      "learning_rate": 6.329745443117535e-05,
      "loss": 0.9985,
      "step": 40290
    },
    {
      "epoch": 2.370036021465853,
      "grad_norm": 0.3760763704776764,
      "learning_rate": 6.323852922690131e-05,
      "loss": 0.8921,
      "step": 40300
    },
    {
      "epoch": 2.370624127030802,
      "grad_norm": 0.3865000903606415,
      "learning_rate": 6.317960402262727e-05,
      "loss": 0.9772,
      "step": 40310
    },
    {
      "epoch": 2.371212232595751,
      "grad_norm": 0.3514677584171295,
      "learning_rate": 6.312067881835323e-05,
      "loss": 0.9886,
      "step": 40320
    },
    {
      "epoch": 2.3718003381607,
      "grad_norm": 0.3774223029613495,
      "learning_rate": 6.306175361407919e-05,
      "loss": 1.0354,
      "step": 40330
    },
    {
      "epoch": 2.3723884437256486,
      "grad_norm": 0.38124290108680725,
      "learning_rate": 6.300282840980515e-05,
      "loss": 0.9863,
      "step": 40340
    },
    {
      "epoch": 2.3729765492905974,
      "grad_norm": 0.3604116141796112,
      "learning_rate": 6.294390320553112e-05,
      "loss": 1.1103,
      "step": 40350
    },
    {
      "epoch": 2.3735646548555467,
      "grad_norm": 0.3349013030529022,
      "learning_rate": 6.288497800125706e-05,
      "loss": 1.0447,
      "step": 40360
    },
    {
      "epoch": 2.3741527604204955,
      "grad_norm": 0.3781963288784027,
      "learning_rate": 6.282605279698302e-05,
      "loss": 0.9874,
      "step": 40370
    },
    {
      "epoch": 2.3747408659854443,
      "grad_norm": 0.4041118919849396,
      "learning_rate": 6.276712759270899e-05,
      "loss": 0.984,
      "step": 40380
    },
    {
      "epoch": 2.375328971550393,
      "grad_norm": 0.39221376180648804,
      "learning_rate": 6.270820238843493e-05,
      "loss": 1.0792,
      "step": 40390
    },
    {
      "epoch": 2.3759170771153424,
      "grad_norm": 0.3914555013179779,
      "learning_rate": 6.26492771841609e-05,
      "loss": 0.9884,
      "step": 40400
    },
    {
      "epoch": 2.376505182680291,
      "grad_norm": 0.430093914270401,
      "learning_rate": 6.259035197988686e-05,
      "loss": 0.9825,
      "step": 40410
    },
    {
      "epoch": 2.37709328824524,
      "grad_norm": 0.37137433886528015,
      "learning_rate": 6.253142677561282e-05,
      "loss": 0.9216,
      "step": 40420
    },
    {
      "epoch": 2.377681393810189,
      "grad_norm": 0.37940776348114014,
      "learning_rate": 6.247250157133878e-05,
      "loss": 0.9188,
      "step": 40430
    },
    {
      "epoch": 2.3782694993751377,
      "grad_norm": 0.38740772008895874,
      "learning_rate": 6.241357636706474e-05,
      "loss": 0.9657,
      "step": 40440
    },
    {
      "epoch": 2.378857604940087,
      "grad_norm": 0.39487311244010925,
      "learning_rate": 6.23546511627907e-05,
      "loss": 0.9932,
      "step": 40450
    },
    {
      "epoch": 2.3794457105050357,
      "grad_norm": 0.35366106033325195,
      "learning_rate": 6.229572595851665e-05,
      "loss": 0.9894,
      "step": 40460
    },
    {
      "epoch": 2.3800338160699845,
      "grad_norm": 0.36390191316604614,
      "learning_rate": 6.223680075424261e-05,
      "loss": 0.9533,
      "step": 40470
    },
    {
      "epoch": 2.3806219216349334,
      "grad_norm": 0.39800381660461426,
      "learning_rate": 6.217787554996857e-05,
      "loss": 1.0146,
      "step": 40480
    },
    {
      "epoch": 2.3812100271998826,
      "grad_norm": 0.3676307201385498,
      "learning_rate": 6.211895034569452e-05,
      "loss": 0.8523,
      "step": 40490
    },
    {
      "epoch": 2.3817981327648314,
      "grad_norm": 0.36295610666275024,
      "learning_rate": 6.206002514142048e-05,
      "loss": 1.0537,
      "step": 40500
    },
    {
      "epoch": 2.3823862383297802,
      "grad_norm": 0.3958817422389984,
      "learning_rate": 6.200109993714644e-05,
      "loss": 0.9608,
      "step": 40510
    },
    {
      "epoch": 2.382974343894729,
      "grad_norm": 0.4109421670436859,
      "learning_rate": 6.19421747328724e-05,
      "loss": 0.992,
      "step": 40520
    },
    {
      "epoch": 2.383562449459678,
      "grad_norm": 0.37488624453544617,
      "learning_rate": 6.188324952859836e-05,
      "loss": 1.065,
      "step": 40530
    },
    {
      "epoch": 2.384150555024627,
      "grad_norm": 0.3869525194168091,
      "learning_rate": 6.182432432432432e-05,
      "loss": 0.9882,
      "step": 40540
    },
    {
      "epoch": 2.384738660589576,
      "grad_norm": 0.35243865847587585,
      "learning_rate": 6.176539912005029e-05,
      "loss": 1.0427,
      "step": 40550
    },
    {
      "epoch": 2.3853267661545248,
      "grad_norm": 0.558646559715271,
      "learning_rate": 6.170647391577623e-05,
      "loss": 0.9467,
      "step": 40560
    },
    {
      "epoch": 2.3859148717194736,
      "grad_norm": 0.33404409885406494,
      "learning_rate": 6.16475487115022e-05,
      "loss": 0.9143,
      "step": 40570
    },
    {
      "epoch": 2.3865029772844224,
      "grad_norm": 0.3555719554424286,
      "learning_rate": 6.158862350722816e-05,
      "loss": 0.9772,
      "step": 40580
    },
    {
      "epoch": 2.3870910828493717,
      "grad_norm": 0.3558866083621979,
      "learning_rate": 6.15296983029541e-05,
      "loss": 1.036,
      "step": 40590
    },
    {
      "epoch": 2.3876791884143205,
      "grad_norm": 0.3625774681568146,
      "learning_rate": 6.147077309868006e-05,
      "loss": 0.9863,
      "step": 40600
    },
    {
      "epoch": 2.3882672939792693,
      "grad_norm": 0.4133852422237396,
      "learning_rate": 6.141184789440603e-05,
      "loss": 1.1529,
      "step": 40610
    },
    {
      "epoch": 2.388855399544218,
      "grad_norm": 0.40270760655403137,
      "learning_rate": 6.135292269013199e-05,
      "loss": 0.9895,
      "step": 40620
    },
    {
      "epoch": 2.389443505109167,
      "grad_norm": 0.4916229248046875,
      "learning_rate": 6.129399748585795e-05,
      "loss": 0.9529,
      "step": 40630
    },
    {
      "epoch": 2.390031610674116,
      "grad_norm": 0.46161195635795593,
      "learning_rate": 6.123507228158391e-05,
      "loss": 0.9996,
      "step": 40640
    },
    {
      "epoch": 2.390619716239065,
      "grad_norm": 0.381258100271225,
      "learning_rate": 6.117614707730987e-05,
      "loss": 0.9004,
      "step": 40650
    },
    {
      "epoch": 2.391207821804014,
      "grad_norm": 0.3947226107120514,
      "learning_rate": 6.111722187303582e-05,
      "loss": 1.0359,
      "step": 40660
    },
    {
      "epoch": 2.3917959273689626,
      "grad_norm": 0.3662395477294922,
      "learning_rate": 6.105829666876178e-05,
      "loss": 1.0155,
      "step": 40670
    },
    {
      "epoch": 2.3923840329339114,
      "grad_norm": 0.444431334733963,
      "learning_rate": 6.099937146448774e-05,
      "loss": 1.0989,
      "step": 40680
    },
    {
      "epoch": 2.3929721384988607,
      "grad_norm": 0.3519737720489502,
      "learning_rate": 6.0940446260213694e-05,
      "loss": 0.9859,
      "step": 40690
    },
    {
      "epoch": 2.3935602440638095,
      "grad_norm": 0.34762367606163025,
      "learning_rate": 6.0881521055939656e-05,
      "loss": 1.0296,
      "step": 40700
    },
    {
      "epoch": 2.3941483496287583,
      "grad_norm": 0.40073490142822266,
      "learning_rate": 6.082259585166561e-05,
      "loss": 1.0805,
      "step": 40710
    },
    {
      "epoch": 2.394736455193707,
      "grad_norm": 0.36383646726608276,
      "learning_rate": 6.076367064739157e-05,
      "loss": 1.0216,
      "step": 40720
    },
    {
      "epoch": 2.395324560758656,
      "grad_norm": 0.33970534801483154,
      "learning_rate": 6.070474544311753e-05,
      "loss": 1.0588,
      "step": 40730
    },
    {
      "epoch": 2.3959126663236052,
      "grad_norm": 0.3716100752353668,
      "learning_rate": 6.064582023884349e-05,
      "loss": 1.0574,
      "step": 40740
    },
    {
      "epoch": 2.396500771888554,
      "grad_norm": 0.36397430300712585,
      "learning_rate": 6.058689503456945e-05,
      "loss": 1.0036,
      "step": 40750
    },
    {
      "epoch": 2.397088877453503,
      "grad_norm": 0.38789281249046326,
      "learning_rate": 6.052796983029541e-05,
      "loss": 0.9332,
      "step": 40760
    },
    {
      "epoch": 2.3976769830184517,
      "grad_norm": 0.4321713149547577,
      "learning_rate": 6.046904462602137e-05,
      "loss": 1.0448,
      "step": 40770
    },
    {
      "epoch": 2.3982650885834005,
      "grad_norm": 0.3517861068248749,
      "learning_rate": 6.0410119421747325e-05,
      "loss": 1.0744,
      "step": 40780
    },
    {
      "epoch": 2.3988531941483497,
      "grad_norm": 0.35970234870910645,
      "learning_rate": 6.035119421747328e-05,
      "loss": 1.0105,
      "step": 40790
    },
    {
      "epoch": 2.3994412997132986,
      "grad_norm": 0.34935253858566284,
      "learning_rate": 6.029226901319924e-05,
      "loss": 1.0433,
      "step": 40800
    },
    {
      "epoch": 2.4000294052782474,
      "grad_norm": 0.3955813944339752,
      "learning_rate": 6.0233343808925195e-05,
      "loss": 1.0337,
      "step": 40810
    },
    {
      "epoch": 2.400617510843196,
      "grad_norm": 0.44645068049430847,
      "learning_rate": 6.0174418604651156e-05,
      "loss": 1.0431,
      "step": 40820
    },
    {
      "epoch": 2.4012056164081454,
      "grad_norm": 0.35636141896247864,
      "learning_rate": 6.011549340037712e-05,
      "loss": 1.1252,
      "step": 40830
    },
    {
      "epoch": 2.4017937219730943,
      "grad_norm": 0.3863353133201599,
      "learning_rate": 6.005656819610307e-05,
      "loss": 0.9964,
      "step": 40840
    },
    {
      "epoch": 2.402381827538043,
      "grad_norm": 0.37726891040802,
      "learning_rate": 5.999764299182903e-05,
      "loss": 0.9888,
      "step": 40850
    },
    {
      "epoch": 2.402969933102992,
      "grad_norm": 0.3986726403236389,
      "learning_rate": 5.9938717787554994e-05,
      "loss": 1.0513,
      "step": 40860
    },
    {
      "epoch": 2.4035580386679407,
      "grad_norm": 0.37962043285369873,
      "learning_rate": 5.9879792583280955e-05,
      "loss": 0.998,
      "step": 40870
    },
    {
      "epoch": 2.40414614423289,
      "grad_norm": 0.36903616786003113,
      "learning_rate": 5.982086737900691e-05,
      "loss": 1.0483,
      "step": 40880
    },
    {
      "epoch": 2.404734249797839,
      "grad_norm": 0.42994067072868347,
      "learning_rate": 5.9761942174732864e-05,
      "loss": 0.9977,
      "step": 40890
    },
    {
      "epoch": 2.4053223553627876,
      "grad_norm": 0.38114359974861145,
      "learning_rate": 5.9703016970458825e-05,
      "loss": 1.0212,
      "step": 40900
    },
    {
      "epoch": 2.4059104609277364,
      "grad_norm": 0.4114084541797638,
      "learning_rate": 5.964409176618478e-05,
      "loss": 0.9806,
      "step": 40910
    },
    {
      "epoch": 2.4064985664926857,
      "grad_norm": 0.37207815051078796,
      "learning_rate": 5.958516656191074e-05,
      "loss": 1.049,
      "step": 40920
    },
    {
      "epoch": 2.4070866720576345,
      "grad_norm": 0.44000425934791565,
      "learning_rate": 5.95262413576367e-05,
      "loss": 0.9981,
      "step": 40930
    },
    {
      "epoch": 2.4076747776225833,
      "grad_norm": 0.3886933922767639,
      "learning_rate": 5.9467316153362657e-05,
      "loss": 1.1506,
      "step": 40940
    },
    {
      "epoch": 2.408262883187532,
      "grad_norm": 0.3529481887817383,
      "learning_rate": 5.940839094908862e-05,
      "loss": 1.0322,
      "step": 40950
    },
    {
      "epoch": 2.408850988752481,
      "grad_norm": 0.42534366250038147,
      "learning_rate": 5.934946574481458e-05,
      "loss": 1.0345,
      "step": 40960
    },
    {
      "epoch": 2.40943909431743,
      "grad_norm": 0.36992475390434265,
      "learning_rate": 5.929054054054054e-05,
      "loss": 1.109,
      "step": 40970
    },
    {
      "epoch": 2.410027199882379,
      "grad_norm": 0.342152863740921,
      "learning_rate": 5.9231615336266495e-05,
      "loss": 1.0449,
      "step": 40980
    },
    {
      "epoch": 2.410615305447328,
      "grad_norm": 0.39577701687812805,
      "learning_rate": 5.9172690131992456e-05,
      "loss": 1.068,
      "step": 40990
    },
    {
      "epoch": 2.4112034110122766,
      "grad_norm": 0.3642795979976654,
      "learning_rate": 5.911376492771841e-05,
      "loss": 0.9794,
      "step": 41000
    },
    {
      "epoch": 2.4117915165772255,
      "grad_norm": 0.3722016513347626,
      "learning_rate": 5.9054839723444365e-05,
      "loss": 1.0255,
      "step": 41010
    },
    {
      "epoch": 2.4123796221421747,
      "grad_norm": 0.4100465476512909,
      "learning_rate": 5.8995914519170326e-05,
      "loss": 1.0594,
      "step": 41020
    },
    {
      "epoch": 2.4129677277071235,
      "grad_norm": 0.3771568238735199,
      "learning_rate": 5.893698931489629e-05,
      "loss": 0.9158,
      "step": 41030
    },
    {
      "epoch": 2.4135558332720723,
      "grad_norm": 0.37106889486312866,
      "learning_rate": 5.887806411062224e-05,
      "loss": 0.9742,
      "step": 41040
    },
    {
      "epoch": 2.414143938837021,
      "grad_norm": 0.36442577838897705,
      "learning_rate": 5.88191389063482e-05,
      "loss": 0.9864,
      "step": 41050
    },
    {
      "epoch": 2.41473204440197,
      "grad_norm": 0.4166611135005951,
      "learning_rate": 5.8760213702074164e-05,
      "loss": 1.0408,
      "step": 41060
    },
    {
      "epoch": 2.4153201499669192,
      "grad_norm": 0.3829500079154968,
      "learning_rate": 5.8701288497800125e-05,
      "loss": 0.9796,
      "step": 41070
    },
    {
      "epoch": 2.415908255531868,
      "grad_norm": 0.35046812891960144,
      "learning_rate": 5.864236329352608e-05,
      "loss": 1.0557,
      "step": 41080
    },
    {
      "epoch": 2.416496361096817,
      "grad_norm": 0.4123656749725342,
      "learning_rate": 5.858343808925204e-05,
      "loss": 1.0339,
      "step": 41090
    },
    {
      "epoch": 2.4170844666617657,
      "grad_norm": 0.32760685682296753,
      "learning_rate": 5.8524512884977995e-05,
      "loss": 1.0744,
      "step": 41100
    },
    {
      "epoch": 2.4176725722267145,
      "grad_norm": 0.4083939492702484,
      "learning_rate": 5.846558768070395e-05,
      "loss": 0.9856,
      "step": 41110
    },
    {
      "epoch": 2.4182606777916638,
      "grad_norm": 0.36362165212631226,
      "learning_rate": 5.840666247642991e-05,
      "loss": 1.0072,
      "step": 41120
    },
    {
      "epoch": 2.4188487833566126,
      "grad_norm": 0.3830595314502716,
      "learning_rate": 5.834773727215587e-05,
      "loss": 1.0329,
      "step": 41130
    },
    {
      "epoch": 2.4194368889215614,
      "grad_norm": 0.4153725802898407,
      "learning_rate": 5.8288812067881826e-05,
      "loss": 0.9822,
      "step": 41140
    },
    {
      "epoch": 2.42002499448651,
      "grad_norm": 0.4114004373550415,
      "learning_rate": 5.822988686360779e-05,
      "loss": 1.0176,
      "step": 41150
    },
    {
      "epoch": 2.420613100051459,
      "grad_norm": 0.360525518655777,
      "learning_rate": 5.817096165933375e-05,
      "loss": 1.0015,
      "step": 41160
    },
    {
      "epoch": 2.4212012056164083,
      "grad_norm": 0.34425994753837585,
      "learning_rate": 5.811203645505971e-05,
      "loss": 1.0593,
      "step": 41170
    },
    {
      "epoch": 2.421789311181357,
      "grad_norm": 0.39924269914627075,
      "learning_rate": 5.8053111250785664e-05,
      "loss": 1.002,
      "step": 41180
    },
    {
      "epoch": 2.422377416746306,
      "grad_norm": 0.3886869549751282,
      "learning_rate": 5.7994186046511626e-05,
      "loss": 1.0467,
      "step": 41190
    },
    {
      "epoch": 2.4229655223112547,
      "grad_norm": 0.447038859128952,
      "learning_rate": 5.793526084223759e-05,
      "loss": 0.988,
      "step": 41200
    },
    {
      "epoch": 2.4235536278762035,
      "grad_norm": 0.43851202726364136,
      "learning_rate": 5.7876335637963534e-05,
      "loss": 1.0408,
      "step": 41210
    },
    {
      "epoch": 2.424141733441153,
      "grad_norm": 0.37798380851745605,
      "learning_rate": 5.7817410433689496e-05,
      "loss": 1.0688,
      "step": 41220
    },
    {
      "epoch": 2.4247298390061016,
      "grad_norm": 0.37435412406921387,
      "learning_rate": 5.775848522941546e-05,
      "loss": 0.9786,
      "step": 41230
    },
    {
      "epoch": 2.4253179445710504,
      "grad_norm": 0.39804768562316895,
      "learning_rate": 5.769956002514141e-05,
      "loss": 0.9654,
      "step": 41240
    },
    {
      "epoch": 2.4259060501359992,
      "grad_norm": 0.39228901267051697,
      "learning_rate": 5.764063482086737e-05,
      "loss": 1.0953,
      "step": 41250
    },
    {
      "epoch": 2.4264941557009485,
      "grad_norm": 0.36678409576416016,
      "learning_rate": 5.7581709616593334e-05,
      "loss": 1.0074,
      "step": 41260
    },
    {
      "epoch": 2.4270822612658973,
      "grad_norm": 0.41959407925605774,
      "learning_rate": 5.7522784412319295e-05,
      "loss": 1.0378,
      "step": 41270
    },
    {
      "epoch": 2.427670366830846,
      "grad_norm": 0.4216648042201996,
      "learning_rate": 5.746385920804525e-05,
      "loss": 0.9794,
      "step": 41280
    },
    {
      "epoch": 2.428258472395795,
      "grad_norm": 0.46149173378944397,
      "learning_rate": 5.740493400377121e-05,
      "loss": 0.9488,
      "step": 41290
    },
    {
      "epoch": 2.4288465779607438,
      "grad_norm": 0.38752663135528564,
      "learning_rate": 5.734600879949717e-05,
      "loss": 0.9992,
      "step": 41300
    },
    {
      "epoch": 2.429434683525693,
      "grad_norm": 0.39006170630455017,
      "learning_rate": 5.728708359522312e-05,
      "loss": 1.0495,
      "step": 41310
    },
    {
      "epoch": 2.430022789090642,
      "grad_norm": 0.3455735146999359,
      "learning_rate": 5.722815839094908e-05,
      "loss": 0.985,
      "step": 41320
    },
    {
      "epoch": 2.4306108946555907,
      "grad_norm": 0.37911269068717957,
      "learning_rate": 5.716923318667504e-05,
      "loss": 1.0051,
      "step": 41330
    },
    {
      "epoch": 2.4311990002205395,
      "grad_norm": 0.34761953353881836,
      "learning_rate": 5.7110307982400996e-05,
      "loss": 0.9841,
      "step": 41340
    },
    {
      "epoch": 2.4317871057854887,
      "grad_norm": 0.4063988924026489,
      "learning_rate": 5.705138277812696e-05,
      "loss": 0.9912,
      "step": 41350
    },
    {
      "epoch": 2.4323752113504375,
      "grad_norm": 0.3561123311519623,
      "learning_rate": 5.699245757385292e-05,
      "loss": 0.9842,
      "step": 41360
    },
    {
      "epoch": 2.4329633169153864,
      "grad_norm": 0.38450512290000916,
      "learning_rate": 5.693353236957888e-05,
      "loss": 1.1298,
      "step": 41370
    },
    {
      "epoch": 2.433551422480335,
      "grad_norm": 0.34087812900543213,
      "learning_rate": 5.6874607165304834e-05,
      "loss": 1.0134,
      "step": 41380
    },
    {
      "epoch": 2.434139528045284,
      "grad_norm": 0.5242703557014465,
      "learning_rate": 5.6815681961030795e-05,
      "loss": 0.9635,
      "step": 41390
    },
    {
      "epoch": 2.4347276336102333,
      "grad_norm": 0.35788050293922424,
      "learning_rate": 5.6756756756756757e-05,
      "loss": 1.0048,
      "step": 41400
    },
    {
      "epoch": 2.435315739175182,
      "grad_norm": 0.38166314363479614,
      "learning_rate": 5.669783155248271e-05,
      "loss": 1.0423,
      "step": 41410
    },
    {
      "epoch": 2.435903844740131,
      "grad_norm": 0.46737906336784363,
      "learning_rate": 5.6638906348208665e-05,
      "loss": 0.9563,
      "step": 41420
    },
    {
      "epoch": 2.4364919503050797,
      "grad_norm": 0.4007360339164734,
      "learning_rate": 5.6579981143934627e-05,
      "loss": 1.024,
      "step": 41430
    },
    {
      "epoch": 2.4370800558700285,
      "grad_norm": 0.3676513731479645,
      "learning_rate": 5.652105593966058e-05,
      "loss": 1.1073,
      "step": 41440
    },
    {
      "epoch": 2.4376681614349778,
      "grad_norm": 0.38545313477516174,
      "learning_rate": 5.646213073538654e-05,
      "loss": 0.9726,
      "step": 41450
    },
    {
      "epoch": 2.4382562669999266,
      "grad_norm": 0.3732075095176697,
      "learning_rate": 5.6403205531112503e-05,
      "loss": 1.0309,
      "step": 41460
    },
    {
      "epoch": 2.4388443725648754,
      "grad_norm": 0.3843722939491272,
      "learning_rate": 5.6344280326838465e-05,
      "loss": 0.9375,
      "step": 41470
    },
    {
      "epoch": 2.4394324781298242,
      "grad_norm": 0.36214885115623474,
      "learning_rate": 5.628535512256442e-05,
      "loss": 0.9892,
      "step": 41480
    },
    {
      "epoch": 2.440020583694773,
      "grad_norm": 0.31597670912742615,
      "learning_rate": 5.622642991829038e-05,
      "loss": 0.9538,
      "step": 41490
    },
    {
      "epoch": 2.4406086892597223,
      "grad_norm": 0.3337475657463074,
      "learning_rate": 5.616750471401634e-05,
      "loss": 0.9936,
      "step": 41500
    },
    {
      "epoch": 2.441196794824671,
      "grad_norm": 0.4362499713897705,
      "learning_rate": 5.6108579509742296e-05,
      "loss": 0.9901,
      "step": 41510
    },
    {
      "epoch": 2.44178490038962,
      "grad_norm": 0.37343576550483704,
      "learning_rate": 5.604965430546825e-05,
      "loss": 0.9068,
      "step": 41520
    },
    {
      "epoch": 2.4423730059545687,
      "grad_norm": 0.38049110770225525,
      "learning_rate": 5.599072910119421e-05,
      "loss": 1.0037,
      "step": 41530
    },
    {
      "epoch": 2.4429611115195176,
      "grad_norm": 0.36638352274894714,
      "learning_rate": 5.5931803896920166e-05,
      "loss": 1.0802,
      "step": 41540
    },
    {
      "epoch": 2.443549217084467,
      "grad_norm": 0.40716084837913513,
      "learning_rate": 5.587287869264613e-05,
      "loss": 1.0302,
      "step": 41550
    },
    {
      "epoch": 2.4441373226494156,
      "grad_norm": 0.360441118478775,
      "learning_rate": 5.581395348837209e-05,
      "loss": 0.9624,
      "step": 41560
    },
    {
      "epoch": 2.4447254282143644,
      "grad_norm": 0.4097503125667572,
      "learning_rate": 5.575502828409805e-05,
      "loss": 1.0134,
      "step": 41570
    },
    {
      "epoch": 2.4453135337793133,
      "grad_norm": 0.3536948561668396,
      "learning_rate": 5.5696103079824004e-05,
      "loss": 1.0281,
      "step": 41580
    },
    {
      "epoch": 2.445901639344262,
      "grad_norm": 0.4034007489681244,
      "learning_rate": 5.5637177875549965e-05,
      "loss": 1.0073,
      "step": 41590
    },
    {
      "epoch": 2.4464897449092113,
      "grad_norm": 0.419023722410202,
      "learning_rate": 5.5578252671275926e-05,
      "loss": 1.0158,
      "step": 41600
    },
    {
      "epoch": 2.44707785047416,
      "grad_norm": 0.4293012022972107,
      "learning_rate": 5.551932746700188e-05,
      "loss": 0.9488,
      "step": 41610
    },
    {
      "epoch": 2.447665956039109,
      "grad_norm": 0.37558236718177795,
      "learning_rate": 5.546040226272784e-05,
      "loss": 0.9832,
      "step": 41620
    },
    {
      "epoch": 2.448254061604058,
      "grad_norm": 0.3756943345069885,
      "learning_rate": 5.5401477058453796e-05,
      "loss": 1.0854,
      "step": 41630
    },
    {
      "epoch": 2.4488421671690066,
      "grad_norm": 0.3629535436630249,
      "learning_rate": 5.534255185417975e-05,
      "loss": 1.0257,
      "step": 41640
    },
    {
      "epoch": 2.449430272733956,
      "grad_norm": 0.41545194387435913,
      "learning_rate": 5.528362664990571e-05,
      "loss": 0.9384,
      "step": 41650
    },
    {
      "epoch": 2.4500183782989047,
      "grad_norm": 0.3572874069213867,
      "learning_rate": 5.522470144563167e-05,
      "loss": 1.0349,
      "step": 41660
    },
    {
      "epoch": 2.4506064838638535,
      "grad_norm": 0.4208691120147705,
      "learning_rate": 5.5165776241357634e-05,
      "loss": 1.0506,
      "step": 41670
    },
    {
      "epoch": 2.4511945894288023,
      "grad_norm": 0.39506062865257263,
      "learning_rate": 5.510685103708359e-05,
      "loss": 0.9042,
      "step": 41680
    },
    {
      "epoch": 2.4517826949937516,
      "grad_norm": 0.41598808765411377,
      "learning_rate": 5.504792583280955e-05,
      "loss": 0.9716,
      "step": 41690
    },
    {
      "epoch": 2.4523708005587004,
      "grad_norm": 0.4036056399345398,
      "learning_rate": 5.498900062853551e-05,
      "loss": 0.9909,
      "step": 41700
    },
    {
      "epoch": 2.452958906123649,
      "grad_norm": 0.3818858563899994,
      "learning_rate": 5.4930075424261466e-05,
      "loss": 1.1255,
      "step": 41710
    },
    {
      "epoch": 2.453547011688598,
      "grad_norm": 0.34761497378349304,
      "learning_rate": 5.487115021998743e-05,
      "loss": 1.058,
      "step": 41720
    },
    {
      "epoch": 2.454135117253547,
      "grad_norm": 0.45057442784309387,
      "learning_rate": 5.481222501571338e-05,
      "loss": 1.0506,
      "step": 41730
    },
    {
      "epoch": 2.454723222818496,
      "grad_norm": 0.47954702377319336,
      "learning_rate": 5.4753299811439336e-05,
      "loss": 0.9143,
      "step": 41740
    },
    {
      "epoch": 2.455311328383445,
      "grad_norm": 0.4008083641529083,
      "learning_rate": 5.46943746071653e-05,
      "loss": 0.9543,
      "step": 41750
    },
    {
      "epoch": 2.4558994339483937,
      "grad_norm": 0.40114107728004456,
      "learning_rate": 5.463544940289126e-05,
      "loss": 0.9775,
      "step": 41760
    },
    {
      "epoch": 2.4564875395133425,
      "grad_norm": 0.36420702934265137,
      "learning_rate": 5.457652419861722e-05,
      "loss": 1.0853,
      "step": 41770
    },
    {
      "epoch": 2.457075645078292,
      "grad_norm": 0.3831574618816376,
      "learning_rate": 5.4517598994343174e-05,
      "loss": 0.9704,
      "step": 41780
    },
    {
      "epoch": 2.4576637506432406,
      "grad_norm": 0.3826654553413391,
      "learning_rate": 5.4458673790069135e-05,
      "loss": 0.9843,
      "step": 41790
    },
    {
      "epoch": 2.4582518562081894,
      "grad_norm": 0.4409656226634979,
      "learning_rate": 5.4399748585795096e-05,
      "loss": 1.0633,
      "step": 41800
    },
    {
      "epoch": 2.4588399617731382,
      "grad_norm": 0.3536848723888397,
      "learning_rate": 5.434082338152105e-05,
      "loss": 1.067,
      "step": 41810
    },
    {
      "epoch": 2.459428067338087,
      "grad_norm": 0.3682482838630676,
      "learning_rate": 5.428189817724701e-05,
      "loss": 1.0154,
      "step": 41820
    },
    {
      "epoch": 2.4600161729030363,
      "grad_norm": 0.381151020526886,
      "learning_rate": 5.422297297297297e-05,
      "loss": 1.013,
      "step": 41830
    },
    {
      "epoch": 2.460604278467985,
      "grad_norm": 0.3592909276485443,
      "learning_rate": 5.416404776869892e-05,
      "loss": 1.0597,
      "step": 41840
    },
    {
      "epoch": 2.461192384032934,
      "grad_norm": 0.38613468408584595,
      "learning_rate": 5.410512256442488e-05,
      "loss": 1.0025,
      "step": 41850
    },
    {
      "epoch": 2.4617804895978828,
      "grad_norm": 0.3636337220668793,
      "learning_rate": 5.404619736015084e-05,
      "loss": 1.0023,
      "step": 41860
    },
    {
      "epoch": 2.4623685951628316,
      "grad_norm": 0.4085087776184082,
      "learning_rate": 5.3987272155876804e-05,
      "loss": 0.9622,
      "step": 41870
    },
    {
      "epoch": 2.462956700727781,
      "grad_norm": 0.33930957317352295,
      "learning_rate": 5.392834695160276e-05,
      "loss": 0.9623,
      "step": 41880
    },
    {
      "epoch": 2.4635448062927296,
      "grad_norm": 0.3658113181591034,
      "learning_rate": 5.386942174732872e-05,
      "loss": 0.9558,
      "step": 41890
    },
    {
      "epoch": 2.4641329118576785,
      "grad_norm": 0.4348566234111786,
      "learning_rate": 5.381049654305468e-05,
      "loss": 0.9968,
      "step": 41900
    },
    {
      "epoch": 2.4647210174226273,
      "grad_norm": 0.37671518325805664,
      "learning_rate": 5.3751571338780635e-05,
      "loss": 1.071,
      "step": 41910
    },
    {
      "epoch": 2.465309122987576,
      "grad_norm": 0.37784045934677124,
      "learning_rate": 5.3692646134506597e-05,
      "loss": 1.0753,
      "step": 41920
    },
    {
      "epoch": 2.4658972285525254,
      "grad_norm": 0.38788285851478577,
      "learning_rate": 5.363372093023256e-05,
      "loss": 1.0756,
      "step": 41930
    },
    {
      "epoch": 2.466485334117474,
      "grad_norm": 0.38716623187065125,
      "learning_rate": 5.3574795725958505e-05,
      "loss": 0.9786,
      "step": 41940
    },
    {
      "epoch": 2.467073439682423,
      "grad_norm": 0.4529068171977997,
      "learning_rate": 5.351587052168447e-05,
      "loss": 0.9825,
      "step": 41950
    },
    {
      "epoch": 2.467661545247372,
      "grad_norm": 0.36467304825782776,
      "learning_rate": 5.345694531741043e-05,
      "loss": 1.0302,
      "step": 41960
    },
    {
      "epoch": 2.4682496508123206,
      "grad_norm": 0.43112823367118835,
      "learning_rate": 5.339802011313639e-05,
      "loss": 1.0507,
      "step": 41970
    },
    {
      "epoch": 2.46883775637727,
      "grad_norm": 0.4191754162311554,
      "learning_rate": 5.3339094908862343e-05,
      "loss": 1.034,
      "step": 41980
    },
    {
      "epoch": 2.4694258619422187,
      "grad_norm": 0.3522661328315735,
      "learning_rate": 5.3280169704588305e-05,
      "loss": 0.9773,
      "step": 41990
    },
    {
      "epoch": 2.4700139675071675,
      "grad_norm": 0.37545111775398254,
      "learning_rate": 5.3221244500314266e-05,
      "loss": 0.9715,
      "step": 42000
    },
    {
      "epoch": 2.4706020730721163,
      "grad_norm": 0.43489885330200195,
      "learning_rate": 5.316231929604022e-05,
      "loss": 0.9976,
      "step": 42010
    },
    {
      "epoch": 2.471190178637065,
      "grad_norm": 0.46961739659309387,
      "learning_rate": 5.310339409176618e-05,
      "loss": 1.0588,
      "step": 42020
    },
    {
      "epoch": 2.4717782842020144,
      "grad_norm": 0.47357359528541565,
      "learning_rate": 5.304446888749214e-05,
      "loss": 1.0331,
      "step": 42030
    },
    {
      "epoch": 2.472366389766963,
      "grad_norm": 0.38008856773376465,
      "learning_rate": 5.2985543683218104e-05,
      "loss": 0.9269,
      "step": 42040
    },
    {
      "epoch": 2.472954495331912,
      "grad_norm": 0.39401233196258545,
      "learning_rate": 5.292661847894405e-05,
      "loss": 0.9388,
      "step": 42050
    },
    {
      "epoch": 2.473542600896861,
      "grad_norm": 0.44375166296958923,
      "learning_rate": 5.286769327467001e-05,
      "loss": 0.9787,
      "step": 42060
    },
    {
      "epoch": 2.4741307064618097,
      "grad_norm": 0.4246489703655243,
      "learning_rate": 5.2808768070395974e-05,
      "loss": 0.9456,
      "step": 42070
    },
    {
      "epoch": 2.474718812026759,
      "grad_norm": 0.38838991522789,
      "learning_rate": 5.2755735386549336e-05,
      "loss": 0.9587,
      "step": 42080
    },
    {
      "epoch": 2.4753069175917077,
      "grad_norm": 0.3843376040458679,
      "learning_rate": 5.26968101822753e-05,
      "loss": 1.0372,
      "step": 42090
    },
    {
      "epoch": 2.4758950231566565,
      "grad_norm": 0.4214411675930023,
      "learning_rate": 5.263788497800125e-05,
      "loss": 0.9532,
      "step": 42100
    },
    {
      "epoch": 2.4764831287216054,
      "grad_norm": 0.42499393224716187,
      "learning_rate": 5.257895977372721e-05,
      "loss": 0.9608,
      "step": 42110
    },
    {
      "epoch": 2.4770712342865546,
      "grad_norm": 0.33844512701034546,
      "learning_rate": 5.2520034569453174e-05,
      "loss": 0.8913,
      "step": 42120
    },
    {
      "epoch": 2.4776593398515034,
      "grad_norm": 0.41258132457733154,
      "learning_rate": 5.246110936517912e-05,
      "loss": 1.0719,
      "step": 42130
    },
    {
      "epoch": 2.4782474454164523,
      "grad_norm": 0.39844977855682373,
      "learning_rate": 5.240218416090508e-05,
      "loss": 1.0273,
      "step": 42140
    },
    {
      "epoch": 2.478835550981401,
      "grad_norm": 0.36641815304756165,
      "learning_rate": 5.2343258956631044e-05,
      "loss": 1.0074,
      "step": 42150
    },
    {
      "epoch": 2.47942365654635,
      "grad_norm": 0.35264766216278076,
      "learning_rate": 5.2284333752357006e-05,
      "loss": 1.0617,
      "step": 42160
    },
    {
      "epoch": 2.480011762111299,
      "grad_norm": 0.5239335298538208,
      "learning_rate": 5.222540854808296e-05,
      "loss": 0.9062,
      "step": 42170
    },
    {
      "epoch": 2.480599867676248,
      "grad_norm": 0.3993597626686096,
      "learning_rate": 5.216648334380892e-05,
      "loss": 0.9445,
      "step": 42180
    },
    {
      "epoch": 2.4811879732411968,
      "grad_norm": 0.40794381499290466,
      "learning_rate": 5.210755813953488e-05,
      "loss": 0.9464,
      "step": 42190
    },
    {
      "epoch": 2.4817760788061456,
      "grad_norm": 0.35243329405784607,
      "learning_rate": 5.204863293526084e-05,
      "loss": 0.9692,
      "step": 42200
    },
    {
      "epoch": 2.482364184371095,
      "grad_norm": 0.4017269015312195,
      "learning_rate": 5.19897077309868e-05,
      "loss": 1.122,
      "step": 42210
    },
    {
      "epoch": 2.4829522899360437,
      "grad_norm": 0.4142912030220032,
      "learning_rate": 5.193078252671276e-05,
      "loss": 1.1156,
      "step": 42220
    },
    {
      "epoch": 2.4835403955009925,
      "grad_norm": 0.3857523202896118,
      "learning_rate": 5.187185732243872e-05,
      "loss": 1.0571,
      "step": 42230
    },
    {
      "epoch": 2.4841285010659413,
      "grad_norm": 0.3744346499443054,
      "learning_rate": 5.181293211816467e-05,
      "loss": 0.9614,
      "step": 42240
    },
    {
      "epoch": 2.48471660663089,
      "grad_norm": 0.40932273864746094,
      "learning_rate": 5.175400691389063e-05,
      "loss": 0.9624,
      "step": 42250
    },
    {
      "epoch": 2.4853047121958394,
      "grad_norm": 0.4060951769351959,
      "learning_rate": 5.1695081709616584e-05,
      "loss": 1.0199,
      "step": 42260
    },
    {
      "epoch": 2.485892817760788,
      "grad_norm": 0.4196202754974365,
      "learning_rate": 5.1636156505342545e-05,
      "loss": 0.9503,
      "step": 42270
    },
    {
      "epoch": 2.486480923325737,
      "grad_norm": 0.37051403522491455,
      "learning_rate": 5.1577231301068506e-05,
      "loss": 1.0136,
      "step": 42280
    },
    {
      "epoch": 2.487069028890686,
      "grad_norm": 0.353994220495224,
      "learning_rate": 5.151830609679447e-05,
      "loss": 0.9386,
      "step": 42290
    },
    {
      "epoch": 2.4876571344556346,
      "grad_norm": 0.4701818823814392,
      "learning_rate": 5.145938089252042e-05,
      "loss": 0.9978,
      "step": 42300
    },
    {
      "epoch": 2.488245240020584,
      "grad_norm": 0.34818705916404724,
      "learning_rate": 5.140045568824638e-05,
      "loss": 0.9942,
      "step": 42310
    },
    {
      "epoch": 2.4888333455855327,
      "grad_norm": 0.3676397502422333,
      "learning_rate": 5.1341530483972344e-05,
      "loss": 0.9414,
      "step": 42320
    },
    {
      "epoch": 2.4894214511504815,
      "grad_norm": 0.39616668224334717,
      "learning_rate": 5.1282605279698305e-05,
      "loss": 1.2024,
      "step": 42330
    },
    {
      "epoch": 2.4900095567154303,
      "grad_norm": 0.3967369496822357,
      "learning_rate": 5.122368007542425e-05,
      "loss": 1.0328,
      "step": 42340
    },
    {
      "epoch": 2.490597662280379,
      "grad_norm": 0.4234454035758972,
      "learning_rate": 5.1164754871150214e-05,
      "loss": 0.9635,
      "step": 42350
    },
    {
      "epoch": 2.4911857678453284,
      "grad_norm": 0.36981332302093506,
      "learning_rate": 5.110582966687617e-05,
      "loss": 1.0611,
      "step": 42360
    },
    {
      "epoch": 2.4917738734102772,
      "grad_norm": 0.3471890985965729,
      "learning_rate": 5.104690446260213e-05,
      "loss": 1.0444,
      "step": 42370
    },
    {
      "epoch": 2.492361978975226,
      "grad_norm": 0.34369999170303345,
      "learning_rate": 5.098797925832809e-05,
      "loss": 0.8818,
      "step": 42380
    },
    {
      "epoch": 2.492950084540175,
      "grad_norm": 0.3662683069705963,
      "learning_rate": 5.092905405405405e-05,
      "loss": 0.9287,
      "step": 42390
    },
    {
      "epoch": 2.4935381901051237,
      "grad_norm": 0.3926120400428772,
      "learning_rate": 5.0870128849780007e-05,
      "loss": 0.9339,
      "step": 42400
    },
    {
      "epoch": 2.494126295670073,
      "grad_norm": 0.350229948759079,
      "learning_rate": 5.081120364550597e-05,
      "loss": 1.0345,
      "step": 42410
    },
    {
      "epoch": 2.4947144012350218,
      "grad_norm": 0.35503143072128296,
      "learning_rate": 5.075227844123193e-05,
      "loss": 1.0341,
      "step": 42420
    },
    {
      "epoch": 2.4953025067999706,
      "grad_norm": 0.40386486053466797,
      "learning_rate": 5.069335323695789e-05,
      "loss": 1.0238,
      "step": 42430
    },
    {
      "epoch": 2.4958906123649194,
      "grad_norm": 0.45497363805770874,
      "learning_rate": 5.063442803268384e-05,
      "loss": 1.015,
      "step": 42440
    },
    {
      "epoch": 2.496478717929868,
      "grad_norm": 0.3631156086921692,
      "learning_rate": 5.05755028284098e-05,
      "loss": 1.0602,
      "step": 42450
    },
    {
      "epoch": 2.4970668234948175,
      "grad_norm": 0.3924984931945801,
      "learning_rate": 5.0516577624135753e-05,
      "loss": 0.9611,
      "step": 42460
    },
    {
      "epoch": 2.4976549290597663,
      "grad_norm": 0.36773398518562317,
      "learning_rate": 5.0457652419861715e-05,
      "loss": 0.9294,
      "step": 42470
    },
    {
      "epoch": 2.498243034624715,
      "grad_norm": 0.37985220551490784,
      "learning_rate": 5.0398727215587676e-05,
      "loss": 1.0116,
      "step": 42480
    },
    {
      "epoch": 2.498831140189664,
      "grad_norm": 0.4206356406211853,
      "learning_rate": 5.033980201131364e-05,
      "loss": 1.0134,
      "step": 42490
    },
    {
      "epoch": 2.4994192457546127,
      "grad_norm": 0.34760770201683044,
      "learning_rate": 5.028087680703959e-05,
      "loss": 0.997,
      "step": 42500
    },
    {
      "epoch": 2.500007351319562,
      "grad_norm": 0.38457736372947693,
      "learning_rate": 5.022195160276555e-05,
      "loss": 0.9947,
      "step": 42510
    },
    {
      "epoch": 2.500595456884511,
      "grad_norm": 0.3833104372024536,
      "learning_rate": 5.0163026398491514e-05,
      "loss": 0.9048,
      "step": 42520
    },
    {
      "epoch": 2.5011835624494596,
      "grad_norm": 0.36509808897972107,
      "learning_rate": 5.0104101194217475e-05,
      "loss": 0.9377,
      "step": 42530
    },
    {
      "epoch": 2.5017716680144084,
      "grad_norm": 0.38900548219680786,
      "learning_rate": 5.004517598994343e-05,
      "loss": 0.9113,
      "step": 42540
    },
    {
      "epoch": 2.5023597735793572,
      "grad_norm": 0.39677804708480835,
      "learning_rate": 4.9986250785669384e-05,
      "loss": 0.9554,
      "step": 42550
    },
    {
      "epoch": 2.5029478791443065,
      "grad_norm": 0.4102211892604828,
      "learning_rate": 4.992732558139534e-05,
      "loss": 0.9841,
      "step": 42560
    },
    {
      "epoch": 2.5035359847092553,
      "grad_norm": 0.41070589423179626,
      "learning_rate": 4.98684003771213e-05,
      "loss": 0.9426,
      "step": 42570
    },
    {
      "epoch": 2.504124090274204,
      "grad_norm": 0.353488028049469,
      "learning_rate": 4.980947517284726e-05,
      "loss": 1.0241,
      "step": 42580
    },
    {
      "epoch": 2.5047121958391534,
      "grad_norm": 0.3927994668483734,
      "learning_rate": 4.975054996857322e-05,
      "loss": 1.0081,
      "step": 42590
    },
    {
      "epoch": 2.5053003014041018,
      "grad_norm": 0.3557378649711609,
      "learning_rate": 4.9691624764299176e-05,
      "loss": 1.0004,
      "step": 42600
    },
    {
      "epoch": 2.505888406969051,
      "grad_norm": 0.39554619789123535,
      "learning_rate": 4.963269956002514e-05,
      "loss": 1.1047,
      "step": 42610
    },
    {
      "epoch": 2.506476512534,
      "grad_norm": 0.3443203568458557,
      "learning_rate": 4.95737743557511e-05,
      "loss": 1.0219,
      "step": 42620
    },
    {
      "epoch": 2.5070646180989486,
      "grad_norm": 0.3444654643535614,
      "learning_rate": 4.951484915147706e-05,
      "loss": 1.068,
      "step": 42630
    },
    {
      "epoch": 2.507652723663898,
      "grad_norm": 0.3798837661743164,
      "learning_rate": 4.9455923947203014e-05,
      "loss": 1.0334,
      "step": 42640
    },
    {
      "epoch": 2.5082408292288467,
      "grad_norm": 0.3415796756744385,
      "learning_rate": 4.939699874292897e-05,
      "loss": 0.961,
      "step": 42650
    },
    {
      "epoch": 2.5088289347937955,
      "grad_norm": 0.37115296721458435,
      "learning_rate": 4.933807353865492e-05,
      "loss": 1.0078,
      "step": 42660
    },
    {
      "epoch": 2.5094170403587444,
      "grad_norm": 0.34684062004089355,
      "learning_rate": 4.9279148334380884e-05,
      "loss": 0.9359,
      "step": 42670
    },
    {
      "epoch": 2.510005145923693,
      "grad_norm": 0.3378710448741913,
      "learning_rate": 4.9220223130106846e-05,
      "loss": 0.9163,
      "step": 42680
    },
    {
      "epoch": 2.5105932514886424,
      "grad_norm": 0.3996760845184326,
      "learning_rate": 4.916129792583281e-05,
      "loss": 0.9898,
      "step": 42690
    },
    {
      "epoch": 2.5111813570535912,
      "grad_norm": 0.45575302839279175,
      "learning_rate": 4.910237272155876e-05,
      "loss": 0.9576,
      "step": 42700
    },
    {
      "epoch": 2.51176946261854,
      "grad_norm": 0.34451624751091003,
      "learning_rate": 4.904344751728472e-05,
      "loss": 0.9587,
      "step": 42710
    },
    {
      "epoch": 2.512357568183489,
      "grad_norm": 0.3293044865131378,
      "learning_rate": 4.8984522313010684e-05,
      "loss": 1.0339,
      "step": 42720
    },
    {
      "epoch": 2.5129456737484377,
      "grad_norm": 0.3751841187477112,
      "learning_rate": 4.8925597108736645e-05,
      "loss": 1.0462,
      "step": 42730
    },
    {
      "epoch": 2.513533779313387,
      "grad_norm": 0.340425044298172,
      "learning_rate": 4.88666719044626e-05,
      "loss": 0.9532,
      "step": 42740
    },
    {
      "epoch": 2.5141218848783358,
      "grad_norm": 0.3673895597457886,
      "learning_rate": 4.880774670018856e-05,
      "loss": 1.065,
      "step": 42750
    },
    {
      "epoch": 2.5147099904432846,
      "grad_norm": 0.3817406892776489,
      "learning_rate": 4.874882149591451e-05,
      "loss": 0.9961,
      "step": 42760
    },
    {
      "epoch": 2.5152980960082334,
      "grad_norm": 0.43098655343055725,
      "learning_rate": 4.868989629164047e-05,
      "loss": 0.9934,
      "step": 42770
    },
    {
      "epoch": 2.515886201573182,
      "grad_norm": 0.43723064661026,
      "learning_rate": 4.863097108736643e-05,
      "loss": 1.0522,
      "step": 42780
    },
    {
      "epoch": 2.5164743071381315,
      "grad_norm": 0.3771350085735321,
      "learning_rate": 4.857204588309239e-05,
      "loss": 0.9531,
      "step": 42790
    },
    {
      "epoch": 2.5170624127030803,
      "grad_norm": 0.4265438914299011,
      "learning_rate": 4.8513120678818346e-05,
      "loss": 0.9125,
      "step": 42800
    },
    {
      "epoch": 2.517650518268029,
      "grad_norm": 0.3682960569858551,
      "learning_rate": 4.845419547454431e-05,
      "loss": 0.9938,
      "step": 42810
    },
    {
      "epoch": 2.518238623832978,
      "grad_norm": 0.3848274350166321,
      "learning_rate": 4.839527027027027e-05,
      "loss": 0.9635,
      "step": 42820
    },
    {
      "epoch": 2.5188267293979267,
      "grad_norm": 0.40891233086586,
      "learning_rate": 4.833634506599623e-05,
      "loss": 0.9809,
      "step": 42830
    },
    {
      "epoch": 2.519414834962876,
      "grad_norm": 0.39655154943466187,
      "learning_rate": 4.8277419861722184e-05,
      "loss": 1.0151,
      "step": 42840
    },
    {
      "epoch": 2.520002940527825,
      "grad_norm": 0.3096795380115509,
      "learning_rate": 4.8218494657448145e-05,
      "loss": 0.9775,
      "step": 42850
    },
    {
      "epoch": 2.5205910460927736,
      "grad_norm": 0.4077451825141907,
      "learning_rate": 4.815956945317409e-05,
      "loss": 0.9593,
      "step": 42860
    },
    {
      "epoch": 2.5211791516577224,
      "grad_norm": 0.3683348596096039,
      "learning_rate": 4.8100644248900054e-05,
      "loss": 0.9339,
      "step": 42870
    },
    {
      "epoch": 2.5217672572226713,
      "grad_norm": 0.3279736042022705,
      "learning_rate": 4.8041719044626015e-05,
      "loss": 1.0508,
      "step": 42880
    },
    {
      "epoch": 2.5223553627876205,
      "grad_norm": 0.47554337978363037,
      "learning_rate": 4.7982793840351977e-05,
      "loss": 0.9623,
      "step": 42890
    },
    {
      "epoch": 2.5229434683525693,
      "grad_norm": 0.37860745191574097,
      "learning_rate": 4.792386863607793e-05,
      "loss": 1.0461,
      "step": 42900
    },
    {
      "epoch": 2.523531573917518,
      "grad_norm": 0.41728195548057556,
      "learning_rate": 4.786494343180389e-05,
      "loss": 1.0956,
      "step": 42910
    },
    {
      "epoch": 2.524119679482467,
      "grad_norm": 0.37696993350982666,
      "learning_rate": 4.780601822752985e-05,
      "loss": 1.0346,
      "step": 42920
    },
    {
      "epoch": 2.5247077850474158,
      "grad_norm": 0.3713766634464264,
      "learning_rate": 4.7747093023255815e-05,
      "loss": 0.9971,
      "step": 42930
    },
    {
      "epoch": 2.525295890612365,
      "grad_norm": 0.40718188881874084,
      "learning_rate": 4.768816781898177e-05,
      "loss": 1.0932,
      "step": 42940
    },
    {
      "epoch": 2.525883996177314,
      "grad_norm": 0.3789708614349365,
      "learning_rate": 4.762924261470773e-05,
      "loss": 0.9663,
      "step": 42950
    },
    {
      "epoch": 2.5264721017422627,
      "grad_norm": 0.4139104187488556,
      "learning_rate": 4.757031741043369e-05,
      "loss": 0.9834,
      "step": 42960
    },
    {
      "epoch": 2.5270602073072115,
      "grad_norm": 0.40048158168792725,
      "learning_rate": 4.751139220615964e-05,
      "loss": 1.0831,
      "step": 42970
    },
    {
      "epoch": 2.5276483128721603,
      "grad_norm": 0.3441958427429199,
      "learning_rate": 4.74524670018856e-05,
      "loss": 1.0363,
      "step": 42980
    },
    {
      "epoch": 2.5282364184371096,
      "grad_norm": 0.39157259464263916,
      "learning_rate": 4.739354179761156e-05,
      "loss": 1.0979,
      "step": 42990
    },
    {
      "epoch": 2.5288245240020584,
      "grad_norm": 0.37371328473091125,
      "learning_rate": 4.7334616593337516e-05,
      "loss": 1.063,
      "step": 43000
    },
    {
      "epoch": 2.529412629567007,
      "grad_norm": 0.348911315202713,
      "learning_rate": 4.727569138906348e-05,
      "loss": 0.884,
      "step": 43010
    },
    {
      "epoch": 2.5300007351319564,
      "grad_norm": 0.35118934512138367,
      "learning_rate": 4.721676618478944e-05,
      "loss": 0.9398,
      "step": 43020
    },
    {
      "epoch": 2.530588840696905,
      "grad_norm": 0.3388121724128723,
      "learning_rate": 4.71578409805154e-05,
      "loss": 1.0345,
      "step": 43030
    },
    {
      "epoch": 2.531176946261854,
      "grad_norm": 0.40669116377830505,
      "learning_rate": 4.7098915776241354e-05,
      "loss": 0.9292,
      "step": 43040
    },
    {
      "epoch": 2.531765051826803,
      "grad_norm": 0.4154651463031769,
      "learning_rate": 4.7039990571967315e-05,
      "loss": 1.0678,
      "step": 43050
    },
    {
      "epoch": 2.5323531573917517,
      "grad_norm": 0.389619380235672,
      "learning_rate": 4.6981065367693276e-05,
      "loss": 0.9502,
      "step": 43060
    },
    {
      "epoch": 2.532941262956701,
      "grad_norm": 0.3990038335323334,
      "learning_rate": 4.6922140163419224e-05,
      "loss": 0.9703,
      "step": 43070
    },
    {
      "epoch": 2.53352936852165,
      "grad_norm": 0.4731845259666443,
      "learning_rate": 4.6863214959145185e-05,
      "loss": 0.9088,
      "step": 43080
    },
    {
      "epoch": 2.5341174740865986,
      "grad_norm": 0.332761287689209,
      "learning_rate": 4.6804289754871146e-05,
      "loss": 0.9303,
      "step": 43090
    },
    {
      "epoch": 2.5347055796515474,
      "grad_norm": 0.356182336807251,
      "learning_rate": 4.67453645505971e-05,
      "loss": 0.9586,
      "step": 43100
    },
    {
      "epoch": 2.5352936852164962,
      "grad_norm": 0.3974674940109253,
      "learning_rate": 4.668643934632306e-05,
      "loss": 1.0373,
      "step": 43110
    },
    {
      "epoch": 2.5358817907814455,
      "grad_norm": 0.362471342086792,
      "learning_rate": 4.662751414204902e-05,
      "loss": 0.9782,
      "step": 43120
    },
    {
      "epoch": 2.5364698963463943,
      "grad_norm": 0.35468852519989014,
      "learning_rate": 4.6568588937774984e-05,
      "loss": 0.9281,
      "step": 43130
    },
    {
      "epoch": 2.537058001911343,
      "grad_norm": 0.417346715927124,
      "learning_rate": 4.650966373350094e-05,
      "loss": 0.9758,
      "step": 43140
    },
    {
      "epoch": 2.537646107476292,
      "grad_norm": 0.37926435470581055,
      "learning_rate": 4.64507385292269e-05,
      "loss": 0.9713,
      "step": 43150
    },
    {
      "epoch": 2.5382342130412407,
      "grad_norm": 0.3840422034263611,
      "learning_rate": 4.639181332495286e-05,
      "loss": 1.0029,
      "step": 43160
    },
    {
      "epoch": 2.53882231860619,
      "grad_norm": 0.40216392278671265,
      "learning_rate": 4.6332888120678816e-05,
      "loss": 1.0326,
      "step": 43170
    },
    {
      "epoch": 2.539410424171139,
      "grad_norm": 0.38707423210144043,
      "learning_rate": 4.627396291640477e-05,
      "loss": 0.9902,
      "step": 43180
    },
    {
      "epoch": 2.5399985297360876,
      "grad_norm": 0.3807802200317383,
      "learning_rate": 4.621503771213073e-05,
      "loss": 1.0365,
      "step": 43190
    },
    {
      "epoch": 2.5405866353010365,
      "grad_norm": 0.39560410380363464,
      "learning_rate": 4.6156112507856686e-05,
      "loss": 0.9481,
      "step": 43200
    },
    {
      "epoch": 2.5411747408659853,
      "grad_norm": 0.36958181858062744,
      "learning_rate": 4.609718730358265e-05,
      "loss": 0.8884,
      "step": 43210
    },
    {
      "epoch": 2.5417628464309345,
      "grad_norm": 0.4027670919895172,
      "learning_rate": 4.603826209930861e-05,
      "loss": 0.9631,
      "step": 43220
    },
    {
      "epoch": 2.5423509519958833,
      "grad_norm": 0.39206916093826294,
      "learning_rate": 4.597933689503457e-05,
      "loss": 0.9977,
      "step": 43230
    },
    {
      "epoch": 2.542939057560832,
      "grad_norm": 0.44733190536499023,
      "learning_rate": 4.5920411690760524e-05,
      "loss": 1.0032,
      "step": 43240
    },
    {
      "epoch": 2.543527163125781,
      "grad_norm": 0.3917123079299927,
      "learning_rate": 4.5861486486486485e-05,
      "loss": 1.0041,
      "step": 43250
    },
    {
      "epoch": 2.54411526869073,
      "grad_norm": 0.37252557277679443,
      "learning_rate": 4.5802561282212446e-05,
      "loss": 0.9994,
      "step": 43260
    },
    {
      "epoch": 2.544703374255679,
      "grad_norm": 0.3335692882537842,
      "learning_rate": 4.57436360779384e-05,
      "loss": 0.9584,
      "step": 43270
    },
    {
      "epoch": 2.545291479820628,
      "grad_norm": 0.42750751972198486,
      "learning_rate": 4.5684710873664355e-05,
      "loss": 1.0202,
      "step": 43280
    },
    {
      "epoch": 2.5458795853855767,
      "grad_norm": 0.37598952651023865,
      "learning_rate": 4.5625785669390316e-05,
      "loss": 1.0171,
      "step": 43290
    },
    {
      "epoch": 2.5464676909505255,
      "grad_norm": 0.4538273215293884,
      "learning_rate": 4.556686046511627e-05,
      "loss": 0.9681,
      "step": 43300
    },
    {
      "epoch": 2.5470557965154743,
      "grad_norm": 0.3726528286933899,
      "learning_rate": 4.550793526084223e-05,
      "loss": 0.9798,
      "step": 43310
    },
    {
      "epoch": 2.5476439020804236,
      "grad_norm": 0.42229610681533813,
      "learning_rate": 4.544901005656819e-05,
      "loss": 1.0443,
      "step": 43320
    },
    {
      "epoch": 2.5482320076453724,
      "grad_norm": 0.3732821047306061,
      "learning_rate": 4.5390084852294154e-05,
      "loss": 1.0499,
      "step": 43330
    },
    {
      "epoch": 2.548820113210321,
      "grad_norm": 0.3887008726596832,
      "learning_rate": 4.533115964802011e-05,
      "loss": 0.9828,
      "step": 43340
    },
    {
      "epoch": 2.54940821877527,
      "grad_norm": 0.4039905369281769,
      "learning_rate": 4.527223444374607e-05,
      "loss": 0.9962,
      "step": 43350
    },
    {
      "epoch": 2.549996324340219,
      "grad_norm": 0.3719521760940552,
      "learning_rate": 4.521330923947203e-05,
      "loss": 1.0351,
      "step": 43360
    },
    {
      "epoch": 2.550584429905168,
      "grad_norm": 0.38353875279426575,
      "learning_rate": 4.5154384035197985e-05,
      "loss": 1.0364,
      "step": 43370
    },
    {
      "epoch": 2.551172535470117,
      "grad_norm": 0.39334970712661743,
      "learning_rate": 4.5095458830923947e-05,
      "loss": 1.0524,
      "step": 43380
    },
    {
      "epoch": 2.5517606410350657,
      "grad_norm": 0.36097392439842224,
      "learning_rate": 4.50365336266499e-05,
      "loss": 1.0571,
      "step": 43390
    },
    {
      "epoch": 2.5523487466000145,
      "grad_norm": 0.3992609977722168,
      "learning_rate": 4.4977608422375855e-05,
      "loss": 1.1081,
      "step": 43400
    },
    {
      "epoch": 2.5529368521649634,
      "grad_norm": 0.41714993119239807,
      "learning_rate": 4.4918683218101817e-05,
      "loss": 1.0152,
      "step": 43410
    },
    {
      "epoch": 2.5535249577299126,
      "grad_norm": 0.37498244643211365,
      "learning_rate": 4.485975801382778e-05,
      "loss": 1.0193,
      "step": 43420
    },
    {
      "epoch": 2.5541130632948614,
      "grad_norm": 0.3804394602775574,
      "learning_rate": 4.480083280955373e-05,
      "loss": 1.0222,
      "step": 43430
    },
    {
      "epoch": 2.5547011688598102,
      "grad_norm": 0.4421224296092987,
      "learning_rate": 4.474190760527969e-05,
      "loss": 1.0239,
      "step": 43440
    },
    {
      "epoch": 2.5552892744247595,
      "grad_norm": 0.3782590329647064,
      "learning_rate": 4.4682982401005655e-05,
      "loss": 0.9996,
      "step": 43450
    },
    {
      "epoch": 2.555877379989708,
      "grad_norm": 0.3109440207481384,
      "learning_rate": 4.4624057196731616e-05,
      "loss": 1.0522,
      "step": 43460
    },
    {
      "epoch": 2.556465485554657,
      "grad_norm": 0.4107497036457062,
      "learning_rate": 4.456513199245757e-05,
      "loss": 0.9541,
      "step": 43470
    },
    {
      "epoch": 2.557053591119606,
      "grad_norm": 0.34829849004745483,
      "learning_rate": 4.450620678818353e-05,
      "loss": 1.0232,
      "step": 43480
    },
    {
      "epoch": 2.5576416966845548,
      "grad_norm": 0.3597494661808014,
      "learning_rate": 4.4447281583909486e-05,
      "loss": 0.9348,
      "step": 43490
    },
    {
      "epoch": 2.558229802249504,
      "grad_norm": 0.40619245171546936,
      "learning_rate": 4.438835637963544e-05,
      "loss": 1.0381,
      "step": 43500
    },
    {
      "epoch": 2.558817907814453,
      "grad_norm": 0.3887011408805847,
      "learning_rate": 4.43294311753614e-05,
      "loss": 0.9928,
      "step": 43510
    },
    {
      "epoch": 2.5594060133794017,
      "grad_norm": 0.426300585269928,
      "learning_rate": 4.427050597108736e-05,
      "loss": 1.0147,
      "step": 43520
    },
    {
      "epoch": 2.5599941189443505,
      "grad_norm": 0.3878013789653778,
      "learning_rate": 4.421158076681332e-05,
      "loss": 0.9218,
      "step": 43530
    },
    {
      "epoch": 2.5605822245092993,
      "grad_norm": 0.4177829325199127,
      "learning_rate": 4.415265556253928e-05,
      "loss": 0.9128,
      "step": 43540
    },
    {
      "epoch": 2.5611703300742485,
      "grad_norm": 0.3994106948375702,
      "learning_rate": 4.409373035826524e-05,
      "loss": 0.997,
      "step": 43550
    },
    {
      "epoch": 2.5617584356391974,
      "grad_norm": 0.403331458568573,
      "learning_rate": 4.40348051539912e-05,
      "loss": 0.9998,
      "step": 43560
    },
    {
      "epoch": 2.562346541204146,
      "grad_norm": 0.5468868613243103,
      "learning_rate": 4.3975879949717155e-05,
      "loss": 0.956,
      "step": 43570
    },
    {
      "epoch": 2.562934646769095,
      "grad_norm": 0.3772430419921875,
      "learning_rate": 4.3916954745443116e-05,
      "loss": 1.0324,
      "step": 43580
    },
    {
      "epoch": 2.563522752334044,
      "grad_norm": 0.40748000144958496,
      "learning_rate": 4.385802954116908e-05,
      "loss": 0.9121,
      "step": 43590
    },
    {
      "epoch": 2.564110857898993,
      "grad_norm": 0.4094219207763672,
      "learning_rate": 4.3799104336895025e-05,
      "loss": 0.984,
      "step": 43600
    },
    {
      "epoch": 2.564698963463942,
      "grad_norm": 0.3630681335926056,
      "learning_rate": 4.3740179132620986e-05,
      "loss": 0.9902,
      "step": 43610
    },
    {
      "epoch": 2.5652870690288907,
      "grad_norm": 0.43738406896591187,
      "learning_rate": 4.368125392834695e-05,
      "loss": 1.0079,
      "step": 43620
    },
    {
      "epoch": 2.5658751745938395,
      "grad_norm": 0.45969560742378235,
      "learning_rate": 4.36223287240729e-05,
      "loss": 1.0878,
      "step": 43630
    },
    {
      "epoch": 2.5664632801587883,
      "grad_norm": 0.3241347074508667,
      "learning_rate": 4.356340351979886e-05,
      "loss": 1.0021,
      "step": 43640
    },
    {
      "epoch": 2.5670513857237376,
      "grad_norm": 0.3205718696117401,
      "learning_rate": 4.3504478315524824e-05,
      "loss": 1.0326,
      "step": 43650
    },
    {
      "epoch": 2.5676394912886864,
      "grad_norm": 0.41522374749183655,
      "learning_rate": 4.3445553111250786e-05,
      "loss": 1.01,
      "step": 43660
    },
    {
      "epoch": 2.568227596853635,
      "grad_norm": 0.3486044406890869,
      "learning_rate": 4.338662790697674e-05,
      "loss": 0.958,
      "step": 43670
    },
    {
      "epoch": 2.568815702418584,
      "grad_norm": 0.4142588973045349,
      "learning_rate": 4.33277027027027e-05,
      "loss": 1.0146,
      "step": 43680
    },
    {
      "epoch": 2.569403807983533,
      "grad_norm": 0.38195720314979553,
      "learning_rate": 4.326877749842866e-05,
      "loss": 1.0381,
      "step": 43690
    },
    {
      "epoch": 2.569991913548482,
      "grad_norm": 0.4153680205345154,
      "learning_rate": 4.320985229415461e-05,
      "loss": 0.9703,
      "step": 43700
    },
    {
      "epoch": 2.570580019113431,
      "grad_norm": 0.4330212473869324,
      "learning_rate": 4.315092708988057e-05,
      "loss": 1.0127,
      "step": 43710
    },
    {
      "epoch": 2.5711681246783797,
      "grad_norm": 0.3494481146335602,
      "learning_rate": 4.309200188560653e-05,
      "loss": 0.9992,
      "step": 43720
    },
    {
      "epoch": 2.5717562302433286,
      "grad_norm": 0.40887451171875,
      "learning_rate": 4.303307668133249e-05,
      "loss": 0.9209,
      "step": 43730
    },
    {
      "epoch": 2.5723443358082774,
      "grad_norm": 0.3765767514705658,
      "learning_rate": 4.297415147705845e-05,
      "loss": 1.1431,
      "step": 43740
    },
    {
      "epoch": 2.5729324413732266,
      "grad_norm": 0.36498352885246277,
      "learning_rate": 4.291522627278441e-05,
      "loss": 1.0013,
      "step": 43750
    },
    {
      "epoch": 2.5735205469381754,
      "grad_norm": 0.36185139417648315,
      "learning_rate": 4.285630106851037e-05,
      "loss": 0.9605,
      "step": 43760
    },
    {
      "epoch": 2.5741086525031243,
      "grad_norm": 0.37865033745765686,
      "learning_rate": 4.2797375864236325e-05,
      "loss": 0.9549,
      "step": 43770
    },
    {
      "epoch": 2.574696758068073,
      "grad_norm": 0.33343902230262756,
      "learning_rate": 4.2738450659962286e-05,
      "loss": 0.9814,
      "step": 43780
    },
    {
      "epoch": 2.575284863633022,
      "grad_norm": 0.3659806549549103,
      "learning_rate": 4.267952545568825e-05,
      "loss": 1.0743,
      "step": 43790
    },
    {
      "epoch": 2.575872969197971,
      "grad_norm": 0.3479478359222412,
      "learning_rate": 4.262060025141421e-05,
      "loss": 1.1351,
      "step": 43800
    },
    {
      "epoch": 2.57646107476292,
      "grad_norm": 0.3884200155735016,
      "learning_rate": 4.2561675047140156e-05,
      "loss": 0.9728,
      "step": 43810
    },
    {
      "epoch": 2.577049180327869,
      "grad_norm": 0.3591468334197998,
      "learning_rate": 4.250274984286612e-05,
      "loss": 1.0035,
      "step": 43820
    },
    {
      "epoch": 2.5776372858928176,
      "grad_norm": 0.36757028102874756,
      "learning_rate": 4.244382463859207e-05,
      "loss": 1.0095,
      "step": 43830
    },
    {
      "epoch": 2.5782253914577664,
      "grad_norm": 0.36496642231941223,
      "learning_rate": 4.238489943431803e-05,
      "loss": 1.1008,
      "step": 43840
    },
    {
      "epoch": 2.5788134970227157,
      "grad_norm": 0.4175361096858978,
      "learning_rate": 4.2325974230043994e-05,
      "loss": 1.0023,
      "step": 43850
    },
    {
      "epoch": 2.5794016025876645,
      "grad_norm": 0.3556843101978302,
      "learning_rate": 4.2267049025769955e-05,
      "loss": 1.0158,
      "step": 43860
    },
    {
      "epoch": 2.5799897081526133,
      "grad_norm": 0.36910051107406616,
      "learning_rate": 4.220812382149591e-05,
      "loss": 1.046,
      "step": 43870
    },
    {
      "epoch": 2.5805778137175626,
      "grad_norm": 0.32496562600135803,
      "learning_rate": 4.214919861722187e-05,
      "loss": 0.9928,
      "step": 43880
    },
    {
      "epoch": 2.581165919282511,
      "grad_norm": 0.42211028933525085,
      "learning_rate": 4.209027341294783e-05,
      "loss": 1.0638,
      "step": 43890
    },
    {
      "epoch": 2.58175402484746,
      "grad_norm": 0.38367700576782227,
      "learning_rate": 4.203134820867379e-05,
      "loss": 1.0141,
      "step": 43900
    },
    {
      "epoch": 2.582342130412409,
      "grad_norm": 0.42024606466293335,
      "learning_rate": 4.197242300439974e-05,
      "loss": 1.0781,
      "step": 43910
    },
    {
      "epoch": 2.582930235977358,
      "grad_norm": 0.38812360167503357,
      "learning_rate": 4.19134978001257e-05,
      "loss": 1.0181,
      "step": 43920
    },
    {
      "epoch": 2.583518341542307,
      "grad_norm": 0.35953429341316223,
      "learning_rate": 4.1854572595851657e-05,
      "loss": 1.0178,
      "step": 43930
    },
    {
      "epoch": 2.584106447107256,
      "grad_norm": 0.3490743339061737,
      "learning_rate": 4.179564739157762e-05,
      "loss": 1.0574,
      "step": 43940
    },
    {
      "epoch": 2.5846945526722047,
      "grad_norm": 0.3426211476325989,
      "learning_rate": 4.173672218730358e-05,
      "loss": 0.965,
      "step": 43950
    },
    {
      "epoch": 2.5852826582371535,
      "grad_norm": 0.38628101348876953,
      "learning_rate": 4.167779698302954e-05,
      "loss": 1.0733,
      "step": 43960
    },
    {
      "epoch": 2.5858707638021023,
      "grad_norm": 0.3819305896759033,
      "learning_rate": 4.1618871778755495e-05,
      "loss": 0.9754,
      "step": 43970
    },
    {
      "epoch": 2.5864588693670516,
      "grad_norm": 0.3427959680557251,
      "learning_rate": 4.1559946574481456e-05,
      "loss": 0.9498,
      "step": 43980
    },
    {
      "epoch": 2.5870469749320004,
      "grad_norm": 0.4016196131706238,
      "learning_rate": 4.150102137020742e-05,
      "loss": 0.9585,
      "step": 43990
    },
    {
      "epoch": 2.5876350804969492,
      "grad_norm": 0.4030321538448334,
      "learning_rate": 4.144209616593338e-05,
      "loss": 0.8855,
      "step": 44000
    },
    {
      "epoch": 2.588223186061898,
      "grad_norm": 0.36686477065086365,
      "learning_rate": 4.1383170961659326e-05,
      "loss": 1.0172,
      "step": 44010
    },
    {
      "epoch": 2.588811291626847,
      "grad_norm": 0.37408414483070374,
      "learning_rate": 4.132424575738529e-05,
      "loss": 0.9932,
      "step": 44020
    },
    {
      "epoch": 2.589399397191796,
      "grad_norm": 0.3549307882785797,
      "learning_rate": 4.126532055311124e-05,
      "loss": 1.0192,
      "step": 44030
    },
    {
      "epoch": 2.589987502756745,
      "grad_norm": 0.38692665100097656,
      "learning_rate": 4.12063953488372e-05,
      "loss": 0.9732,
      "step": 44040
    },
    {
      "epoch": 2.5905756083216938,
      "grad_norm": 0.39138948917388916,
      "learning_rate": 4.1147470144563164e-05,
      "loss": 0.9205,
      "step": 44050
    },
    {
      "epoch": 2.5911637138866426,
      "grad_norm": 0.3435354232788086,
      "learning_rate": 4.1088544940289125e-05,
      "loss": 1.0496,
      "step": 44060
    },
    {
      "epoch": 2.5917518194515914,
      "grad_norm": 0.3520791828632355,
      "learning_rate": 4.102961973601508e-05,
      "loss": 0.9871,
      "step": 44070
    },
    {
      "epoch": 2.5923399250165406,
      "grad_norm": 0.4427452087402344,
      "learning_rate": 4.097658705216845e-05,
      "loss": 0.9671,
      "step": 44080
    },
    {
      "epoch": 2.5929280305814895,
      "grad_norm": 0.36414438486099243,
      "learning_rate": 4.09176618478944e-05,
      "loss": 1.026,
      "step": 44090
    },
    {
      "epoch": 2.5935161361464383,
      "grad_norm": 0.39539024233818054,
      "learning_rate": 4.085873664362036e-05,
      "loss": 0.9999,
      "step": 44100
    },
    {
      "epoch": 2.594104241711387,
      "grad_norm": 0.3792625069618225,
      "learning_rate": 4.079981143934632e-05,
      "loss": 1.0029,
      "step": 44110
    },
    {
      "epoch": 2.594692347276336,
      "grad_norm": 0.41211631894111633,
      "learning_rate": 4.074088623507227e-05,
      "loss": 0.9874,
      "step": 44120
    },
    {
      "epoch": 2.595280452841285,
      "grad_norm": 0.39995986223220825,
      "learning_rate": 4.0681961030798234e-05,
      "loss": 1.0562,
      "step": 44130
    },
    {
      "epoch": 2.595868558406234,
      "grad_norm": 0.3278239369392395,
      "learning_rate": 4.0623035826524195e-05,
      "loss": 0.914,
      "step": 44140
    },
    {
      "epoch": 2.596456663971183,
      "grad_norm": 0.38876771926879883,
      "learning_rate": 4.056411062225016e-05,
      "loss": 0.9597,
      "step": 44150
    },
    {
      "epoch": 2.5970447695361316,
      "grad_norm": 0.39885228872299194,
      "learning_rate": 4.050518541797611e-05,
      "loss": 0.9823,
      "step": 44160
    },
    {
      "epoch": 2.5976328751010804,
      "grad_norm": 0.38611117005348206,
      "learning_rate": 4.044626021370207e-05,
      "loss": 1.0734,
      "step": 44170
    },
    {
      "epoch": 2.5982209806660297,
      "grad_norm": 0.36385467648506165,
      "learning_rate": 4.0387335009428034e-05,
      "loss": 1.0157,
      "step": 44180
    },
    {
      "epoch": 2.5988090862309785,
      "grad_norm": 0.35321733355522156,
      "learning_rate": 4.032840980515399e-05,
      "loss": 1.0324,
      "step": 44190
    },
    {
      "epoch": 2.5993971917959273,
      "grad_norm": 0.3812319338321686,
      "learning_rate": 4.026948460087994e-05,
      "loss": 1.0065,
      "step": 44200
    },
    {
      "epoch": 2.599985297360876,
      "grad_norm": 0.37336209416389465,
      "learning_rate": 4.0210559396605904e-05,
      "loss": 0.9672,
      "step": 44210
    },
    {
      "epoch": 2.600573402925825,
      "grad_norm": 0.4164959490299225,
      "learning_rate": 4.015163419233186e-05,
      "loss": 1.0176,
      "step": 44220
    },
    {
      "epoch": 2.601161508490774,
      "grad_norm": 0.372577041387558,
      "learning_rate": 4.009270898805782e-05,
      "loss": 1.0261,
      "step": 44230
    },
    {
      "epoch": 2.601749614055723,
      "grad_norm": 0.49221140146255493,
      "learning_rate": 4.003378378378378e-05,
      "loss": 1.0428,
      "step": 44240
    },
    {
      "epoch": 2.602337719620672,
      "grad_norm": 0.3420987129211426,
      "learning_rate": 3.997485857950974e-05,
      "loss": 1.0724,
      "step": 44250
    },
    {
      "epoch": 2.6029258251856207,
      "grad_norm": 0.45720958709716797,
      "learning_rate": 3.9915933375235696e-05,
      "loss": 1.0274,
      "step": 44260
    },
    {
      "epoch": 2.6035139307505695,
      "grad_norm": 0.3645953834056854,
      "learning_rate": 3.985700817096166e-05,
      "loss": 0.9698,
      "step": 44270
    },
    {
      "epoch": 2.6041020363155187,
      "grad_norm": 0.40324971079826355,
      "learning_rate": 3.979808296668762e-05,
      "loss": 0.9899,
      "step": 44280
    },
    {
      "epoch": 2.6046901418804675,
      "grad_norm": 0.3899513781070709,
      "learning_rate": 3.973915776241357e-05,
      "loss": 1.0585,
      "step": 44290
    },
    {
      "epoch": 2.6052782474454164,
      "grad_norm": 0.34926900267601013,
      "learning_rate": 3.9680232558139534e-05,
      "loss": 0.9895,
      "step": 44300
    },
    {
      "epoch": 2.6058663530103656,
      "grad_norm": 0.42699989676475525,
      "learning_rate": 3.962130735386549e-05,
      "loss": 0.9864,
      "step": 44310
    },
    {
      "epoch": 2.606454458575314,
      "grad_norm": 0.3622174859046936,
      "learning_rate": 3.956238214959144e-05,
      "loss": 0.9997,
      "step": 44320
    },
    {
      "epoch": 2.6070425641402633,
      "grad_norm": 0.4461694657802582,
      "learning_rate": 3.9503456945317404e-05,
      "loss": 0.8689,
      "step": 44330
    },
    {
      "epoch": 2.607630669705212,
      "grad_norm": 0.3602733612060547,
      "learning_rate": 3.9444531741043365e-05,
      "loss": 0.8953,
      "step": 44340
    },
    {
      "epoch": 2.608218775270161,
      "grad_norm": 0.38429075479507446,
      "learning_rate": 3.9385606536769326e-05,
      "loss": 0.9607,
      "step": 44350
    },
    {
      "epoch": 2.60880688083511,
      "grad_norm": 0.45191627740859985,
      "learning_rate": 3.932668133249528e-05,
      "loss": 0.9066,
      "step": 44360
    },
    {
      "epoch": 2.609394986400059,
      "grad_norm": 0.3664458990097046,
      "learning_rate": 3.926775612822124e-05,
      "loss": 1.0103,
      "step": 44370
    },
    {
      "epoch": 2.6099830919650078,
      "grad_norm": 0.40502333641052246,
      "learning_rate": 3.92088309239472e-05,
      "loss": 0.9516,
      "step": 44380
    },
    {
      "epoch": 2.6105711975299566,
      "grad_norm": 0.34373846650123596,
      "learning_rate": 3.914990571967316e-05,
      "loss": 0.9944,
      "step": 44390
    },
    {
      "epoch": 2.6111593030949054,
      "grad_norm": 0.343313068151474,
      "learning_rate": 3.909098051539912e-05,
      "loss": 1.0308,
      "step": 44400
    },
    {
      "epoch": 2.6117474086598547,
      "grad_norm": 0.38497066497802734,
      "learning_rate": 3.903205531112507e-05,
      "loss": 0.9328,
      "step": 44410
    },
    {
      "epoch": 2.6123355142248035,
      "grad_norm": 0.37263041734695435,
      "learning_rate": 3.897313010685103e-05,
      "loss": 1.0276,
      "step": 44420
    },
    {
      "epoch": 2.6129236197897523,
      "grad_norm": 0.33416879177093506,
      "learning_rate": 3.891420490257699e-05,
      "loss": 0.9853,
      "step": 44430
    },
    {
      "epoch": 2.613511725354701,
      "grad_norm": 0.3831268548965454,
      "learning_rate": 3.885527969830295e-05,
      "loss": 1.001,
      "step": 44440
    },
    {
      "epoch": 2.61409983091965,
      "grad_norm": 0.3418971300125122,
      "learning_rate": 3.879635449402891e-05,
      "loss": 1.0115,
      "step": 44450
    },
    {
      "epoch": 2.614687936484599,
      "grad_norm": 0.34424591064453125,
      "learning_rate": 3.8737429289754866e-05,
      "loss": 0.9693,
      "step": 44460
    },
    {
      "epoch": 2.615276042049548,
      "grad_norm": 0.4337318539619446,
      "learning_rate": 3.867850408548083e-05,
      "loss": 0.9386,
      "step": 44470
    },
    {
      "epoch": 2.615864147614497,
      "grad_norm": 0.3801473379135132,
      "learning_rate": 3.861957888120679e-05,
      "loss": 0.9847,
      "step": 44480
    },
    {
      "epoch": 2.6164522531794456,
      "grad_norm": 0.36592987179756165,
      "learning_rate": 3.856065367693274e-05,
      "loss": 0.9891,
      "step": 44490
    },
    {
      "epoch": 2.6170403587443944,
      "grad_norm": 0.3845517039299011,
      "learning_rate": 3.8501728472658704e-05,
      "loss": 1.0324,
      "step": 44500
    },
    {
      "epoch": 2.6176284643093437,
      "grad_norm": 0.4168689250946045,
      "learning_rate": 3.8442803268384665e-05,
      "loss": 1.0896,
      "step": 44510
    },
    {
      "epoch": 2.6182165698742925,
      "grad_norm": 0.38166841864585876,
      "learning_rate": 3.838387806411061e-05,
      "loss": 1.0912,
      "step": 44520
    },
    {
      "epoch": 2.6188046754392413,
      "grad_norm": 0.33935263752937317,
      "learning_rate": 3.8324952859836574e-05,
      "loss": 0.942,
      "step": 44530
    },
    {
      "epoch": 2.61939278100419,
      "grad_norm": 0.37378185987472534,
      "learning_rate": 3.8266027655562535e-05,
      "loss": 1.0304,
      "step": 44540
    },
    {
      "epoch": 2.619980886569139,
      "grad_norm": 0.38695308566093445,
      "learning_rate": 3.8207102451288496e-05,
      "loss": 1.013,
      "step": 44550
    },
    {
      "epoch": 2.6205689921340882,
      "grad_norm": 0.3604237139225006,
      "learning_rate": 3.814817724701445e-05,
      "loss": 1.0679,
      "step": 44560
    },
    {
      "epoch": 2.621157097699037,
      "grad_norm": 0.41863271594047546,
      "learning_rate": 3.808925204274041e-05,
      "loss": 1.0739,
      "step": 44570
    },
    {
      "epoch": 2.621745203263986,
      "grad_norm": 0.47673097252845764,
      "learning_rate": 3.803032683846637e-05,
      "loss": 1.0656,
      "step": 44580
    },
    {
      "epoch": 2.6223333088289347,
      "grad_norm": 0.35618412494659424,
      "learning_rate": 3.797140163419233e-05,
      "loss": 1.0383,
      "step": 44590
    },
    {
      "epoch": 2.6229214143938835,
      "grad_norm": 0.39639586210250854,
      "learning_rate": 3.791247642991829e-05,
      "loss": 1.024,
      "step": 44600
    },
    {
      "epoch": 2.6235095199588327,
      "grad_norm": 0.3584446609020233,
      "learning_rate": 3.785355122564425e-05,
      "loss": 1.0539,
      "step": 44610
    },
    {
      "epoch": 2.6240976255237816,
      "grad_norm": 0.4072544574737549,
      "learning_rate": 3.77946260213702e-05,
      "loss": 0.9823,
      "step": 44620
    },
    {
      "epoch": 2.6246857310887304,
      "grad_norm": 0.3611001670360565,
      "learning_rate": 3.773570081709616e-05,
      "loss": 0.9344,
      "step": 44630
    },
    {
      "epoch": 2.625273836653679,
      "grad_norm": 0.4264029860496521,
      "learning_rate": 3.767677561282212e-05,
      "loss": 0.9645,
      "step": 44640
    },
    {
      "epoch": 2.625861942218628,
      "grad_norm": 0.3994104862213135,
      "learning_rate": 3.761785040854808e-05,
      "loss": 1.0214,
      "step": 44650
    },
    {
      "epoch": 2.6264500477835773,
      "grad_norm": 0.3910147249698639,
      "learning_rate": 3.7558925204274036e-05,
      "loss": 1.1518,
      "step": 44660
    },
    {
      "epoch": 2.627038153348526,
      "grad_norm": 0.3806348145008087,
      "learning_rate": 3.75e-05,
      "loss": 1.022,
      "step": 44670
    },
    {
      "epoch": 2.627626258913475,
      "grad_norm": 0.4079548120498657,
      "learning_rate": 3.744107479572596e-05,
      "loss": 0.9964,
      "step": 44680
    },
    {
      "epoch": 2.6282143644784237,
      "grad_norm": 0.3293341100215912,
      "learning_rate": 3.738214959145191e-05,
      "loss": 0.9547,
      "step": 44690
    },
    {
      "epoch": 2.6288024700433725,
      "grad_norm": 0.34292498230934143,
      "learning_rate": 3.7323224387177874e-05,
      "loss": 1.0376,
      "step": 44700
    },
    {
      "epoch": 2.629390575608322,
      "grad_norm": 0.384204238653183,
      "learning_rate": 3.726429918290383e-05,
      "loss": 1.0336,
      "step": 44710
    },
    {
      "epoch": 2.6299786811732706,
      "grad_norm": 0.4109266698360443,
      "learning_rate": 3.720537397862979e-05,
      "loss": 0.9528,
      "step": 44720
    },
    {
      "epoch": 2.6305667867382194,
      "grad_norm": 0.434551477432251,
      "learning_rate": 3.714644877435575e-05,
      "loss": 0.9868,
      "step": 44730
    },
    {
      "epoch": 2.6311548923031687,
      "grad_norm": 0.31382808089256287,
      "learning_rate": 3.7087523570081705e-05,
      "loss": 1.0194,
      "step": 44740
    },
    {
      "epoch": 2.631742997868117,
      "grad_norm": 0.42246609926223755,
      "learning_rate": 3.7028598365807666e-05,
      "loss": 0.9519,
      "step": 44750
    },
    {
      "epoch": 2.6323311034330663,
      "grad_norm": 0.3767613470554352,
      "learning_rate": 3.696967316153362e-05,
      "loss": 1.0785,
      "step": 44760
    },
    {
      "epoch": 2.632919208998015,
      "grad_norm": 0.4240458011627197,
      "learning_rate": 3.691074795725958e-05,
      "loss": 1.0126,
      "step": 44770
    },
    {
      "epoch": 2.633507314562964,
      "grad_norm": 0.33789151906967163,
      "learning_rate": 3.685182275298554e-05,
      "loss": 1.0767,
      "step": 44780
    },
    {
      "epoch": 2.634095420127913,
      "grad_norm": 0.4042731523513794,
      "learning_rate": 3.67928975487115e-05,
      "loss": 0.9401,
      "step": 44790
    },
    {
      "epoch": 2.634683525692862,
      "grad_norm": 0.36966729164123535,
      "learning_rate": 3.673397234443746e-05,
      "loss": 0.9752,
      "step": 44800
    },
    {
      "epoch": 2.635271631257811,
      "grad_norm": 0.33988502621650696,
      "learning_rate": 3.667504714016341e-05,
      "loss": 0.9244,
      "step": 44810
    },
    {
      "epoch": 2.6358597368227596,
      "grad_norm": 0.4301263689994812,
      "learning_rate": 3.6616121935889374e-05,
      "loss": 0.9716,
      "step": 44820
    },
    {
      "epoch": 2.6364478423877085,
      "grad_norm": 0.41254597902297974,
      "learning_rate": 3.6557196731615335e-05,
      "loss": 0.95,
      "step": 44830
    },
    {
      "epoch": 2.6370359479526577,
      "grad_norm": 0.3628440499305725,
      "learning_rate": 3.649827152734129e-05,
      "loss": 0.9066,
      "step": 44840
    },
    {
      "epoch": 2.6376240535176065,
      "grad_norm": 0.3797035217285156,
      "learning_rate": 3.643934632306725e-05,
      "loss": 1.0109,
      "step": 44850
    },
    {
      "epoch": 2.6382121590825554,
      "grad_norm": 0.374162882566452,
      "learning_rate": 3.6380421118793205e-05,
      "loss": 1.0344,
      "step": 44860
    },
    {
      "epoch": 2.638800264647504,
      "grad_norm": 0.4756115674972534,
      "learning_rate": 3.6321495914519166e-05,
      "loss": 1.0061,
      "step": 44870
    },
    {
      "epoch": 2.639388370212453,
      "grad_norm": 0.33441126346588135,
      "learning_rate": 3.626257071024513e-05,
      "loss": 1.0028,
      "step": 44880
    },
    {
      "epoch": 2.6399764757774022,
      "grad_norm": 0.39884644746780396,
      "learning_rate": 3.620364550597108e-05,
      "loss": 0.9455,
      "step": 44890
    },
    {
      "epoch": 2.640564581342351,
      "grad_norm": 0.39131900668144226,
      "learning_rate": 3.614472030169704e-05,
      "loss": 0.9964,
      "step": 44900
    },
    {
      "epoch": 2.6411526869073,
      "grad_norm": 0.36673465371131897,
      "learning_rate": 3.6085795097423e-05,
      "loss": 1.0576,
      "step": 44910
    },
    {
      "epoch": 2.6417407924722487,
      "grad_norm": 0.43408384919166565,
      "learning_rate": 3.602686989314896e-05,
      "loss": 1.048,
      "step": 44920
    },
    {
      "epoch": 2.6423288980371975,
      "grad_norm": 0.4009462893009186,
      "learning_rate": 3.596794468887492e-05,
      "loss": 0.9888,
      "step": 44930
    },
    {
      "epoch": 2.6429170036021468,
      "grad_norm": 0.43583711981773376,
      "learning_rate": 3.5909019484600875e-05,
      "loss": 1.0528,
      "step": 44940
    },
    {
      "epoch": 2.6435051091670956,
      "grad_norm": 0.36608630418777466,
      "learning_rate": 3.5850094280326836e-05,
      "loss": 0.949,
      "step": 44950
    },
    {
      "epoch": 2.6440932147320444,
      "grad_norm": 0.34927719831466675,
      "learning_rate": 3.579116907605279e-05,
      "loss": 0.9781,
      "step": 44960
    },
    {
      "epoch": 2.644681320296993,
      "grad_norm": 0.362086683511734,
      "learning_rate": 3.573224387177875e-05,
      "loss": 1.0526,
      "step": 44970
    },
    {
      "epoch": 2.645269425861942,
      "grad_norm": 0.4606161415576935,
      "learning_rate": 3.567331866750471e-05,
      "loss": 0.9571,
      "step": 44980
    },
    {
      "epoch": 2.6458575314268913,
      "grad_norm": 0.3693309426307678,
      "learning_rate": 3.561439346323067e-05,
      "loss": 0.9322,
      "step": 44990
    },
    {
      "epoch": 2.64644563699184,
      "grad_norm": 0.377288818359375,
      "learning_rate": 3.555546825895663e-05,
      "loss": 1.0318,
      "step": 45000
    },
    {
      "epoch": 2.647033742556789,
      "grad_norm": 0.41367948055267334,
      "learning_rate": 3.549654305468259e-05,
      "loss": 1.0216,
      "step": 45010
    },
    {
      "epoch": 2.6476218481217377,
      "grad_norm": 0.33556652069091797,
      "learning_rate": 3.5437617850408544e-05,
      "loss": 0.9438,
      "step": 45020
    },
    {
      "epoch": 2.6482099536866865,
      "grad_norm": 0.3556936979293823,
      "learning_rate": 3.5378692646134505e-05,
      "loss": 1.0165,
      "step": 45030
    },
    {
      "epoch": 2.648798059251636,
      "grad_norm": 0.4145248532295227,
      "learning_rate": 3.531976744186046e-05,
      "loss": 1.0579,
      "step": 45040
    },
    {
      "epoch": 2.6493861648165846,
      "grad_norm": 0.39472901821136475,
      "learning_rate": 3.526084223758642e-05,
      "loss": 1.05,
      "step": 45050
    },
    {
      "epoch": 2.6499742703815334,
      "grad_norm": 0.38589635491371155,
      "learning_rate": 3.520191703331238e-05,
      "loss": 1.0269,
      "step": 45060
    },
    {
      "epoch": 2.6505623759464823,
      "grad_norm": 0.3824576735496521,
      "learning_rate": 3.5142991829038336e-05,
      "loss": 1.057,
      "step": 45070
    },
    {
      "epoch": 2.651150481511431,
      "grad_norm": 0.4187993109226227,
      "learning_rate": 3.50840666247643e-05,
      "loss": 1.0229,
      "step": 45080
    },
    {
      "epoch": 2.6517385870763803,
      "grad_norm": 0.38524606823921204,
      "learning_rate": 3.502514142049025e-05,
      "loss": 1.0182,
      "step": 45090
    },
    {
      "epoch": 2.652326692641329,
      "grad_norm": 0.435276597738266,
      "learning_rate": 3.496621621621621e-05,
      "loss": 1.0802,
      "step": 45100
    },
    {
      "epoch": 2.652914798206278,
      "grad_norm": 0.473296582698822,
      "learning_rate": 3.4907291011942174e-05,
      "loss": 0.9298,
      "step": 45110
    },
    {
      "epoch": 2.6535029037712268,
      "grad_norm": 0.3488928973674774,
      "learning_rate": 3.484836580766813e-05,
      "loss": 0.9976,
      "step": 45120
    },
    {
      "epoch": 2.6540910093361756,
      "grad_norm": 0.37575894594192505,
      "learning_rate": 3.478944060339409e-05,
      "loss": 0.8943,
      "step": 45130
    },
    {
      "epoch": 2.654679114901125,
      "grad_norm": 0.38273417949676514,
      "learning_rate": 3.4730515399120044e-05,
      "loss": 0.9215,
      "step": 45140
    },
    {
      "epoch": 2.6552672204660737,
      "grad_norm": 0.4099677503108978,
      "learning_rate": 3.4671590194846006e-05,
      "loss": 1.0725,
      "step": 45150
    },
    {
      "epoch": 2.6558553260310225,
      "grad_norm": 0.35082897543907166,
      "learning_rate": 3.461266499057197e-05,
      "loss": 1.0158,
      "step": 45160
    },
    {
      "epoch": 2.6564434315959717,
      "grad_norm": 0.428361713886261,
      "learning_rate": 3.455373978629792e-05,
      "loss": 0.987,
      "step": 45170
    },
    {
      "epoch": 2.65703153716092,
      "grad_norm": 0.3698124587535858,
      "learning_rate": 3.449481458202388e-05,
      "loss": 1.0602,
      "step": 45180
    },
    {
      "epoch": 2.6576196427258694,
      "grad_norm": 0.3977196514606476,
      "learning_rate": 3.443588937774984e-05,
      "loss": 1.0307,
      "step": 45190
    },
    {
      "epoch": 2.658207748290818,
      "grad_norm": 0.4146955907344818,
      "learning_rate": 3.43769641734758e-05,
      "loss": 0.9536,
      "step": 45200
    },
    {
      "epoch": 2.658795853855767,
      "grad_norm": 0.40453583002090454,
      "learning_rate": 3.431803896920176e-05,
      "loss": 0.9678,
      "step": 45210
    },
    {
      "epoch": 2.6593839594207163,
      "grad_norm": 0.40971285104751587,
      "learning_rate": 3.4259113764927714e-05,
      "loss": 1.0492,
      "step": 45220
    },
    {
      "epoch": 2.659972064985665,
      "grad_norm": 0.3737979531288147,
      "learning_rate": 3.4200188560653675e-05,
      "loss": 0.9805,
      "step": 45230
    },
    {
      "epoch": 2.660560170550614,
      "grad_norm": 0.47062066197395325,
      "learning_rate": 3.414126335637963e-05,
      "loss": 1.0412,
      "step": 45240
    },
    {
      "epoch": 2.6611482761155627,
      "grad_norm": 0.360869437456131,
      "learning_rate": 3.408233815210559e-05,
      "loss": 0.967,
      "step": 45250
    },
    {
      "epoch": 2.6617363816805115,
      "grad_norm": 0.42243170738220215,
      "learning_rate": 3.402341294783155e-05,
      "loss": 1.0185,
      "step": 45260
    },
    {
      "epoch": 2.662324487245461,
      "grad_norm": 0.3770413100719452,
      "learning_rate": 3.396448774355751e-05,
      "loss": 1.027,
      "step": 45270
    },
    {
      "epoch": 2.6629125928104096,
      "grad_norm": 0.39444103837013245,
      "learning_rate": 3.390556253928347e-05,
      "loss": 1.0659,
      "step": 45280
    },
    {
      "epoch": 2.6635006983753584,
      "grad_norm": 0.4360410273075104,
      "learning_rate": 3.384663733500942e-05,
      "loss": 1.0383,
      "step": 45290
    },
    {
      "epoch": 2.6640888039403072,
      "grad_norm": 0.3776867389678955,
      "learning_rate": 3.378771213073538e-05,
      "loss": 1.0148,
      "step": 45300
    },
    {
      "epoch": 2.664676909505256,
      "grad_norm": 0.36483341455459595,
      "learning_rate": 3.3728786926461344e-05,
      "loss": 0.9053,
      "step": 45310
    },
    {
      "epoch": 2.6652650150702053,
      "grad_norm": 0.43672728538513184,
      "learning_rate": 3.3669861722187305e-05,
      "loss": 1.1436,
      "step": 45320
    },
    {
      "epoch": 2.665853120635154,
      "grad_norm": 0.3921224772930145,
      "learning_rate": 3.361093651791326e-05,
      "loss": 0.9913,
      "step": 45330
    },
    {
      "epoch": 2.666441226200103,
      "grad_norm": 0.42252641916275024,
      "learning_rate": 3.3552011313639214e-05,
      "loss": 1.0785,
      "step": 45340
    },
    {
      "epoch": 2.6670293317650517,
      "grad_norm": 0.3583669066429138,
      "learning_rate": 3.3493086109365175e-05,
      "loss": 0.9789,
      "step": 45350
    },
    {
      "epoch": 2.6676174373300006,
      "grad_norm": 0.4058813154697418,
      "learning_rate": 3.3434160905091136e-05,
      "loss": 1.0374,
      "step": 45360
    },
    {
      "epoch": 2.66820554289495,
      "grad_norm": 0.3646167814731598,
      "learning_rate": 3.33752357008171e-05,
      "loss": 0.9437,
      "step": 45370
    },
    {
      "epoch": 2.6687936484598986,
      "grad_norm": 0.32032880187034607,
      "learning_rate": 3.331631049654305e-05,
      "loss": 1.0152,
      "step": 45380
    },
    {
      "epoch": 2.6693817540248475,
      "grad_norm": 0.36917856335639954,
      "learning_rate": 3.3257385292269007e-05,
      "loss": 1.0127,
      "step": 45390
    },
    {
      "epoch": 2.6699698595897963,
      "grad_norm": 0.38867565989494324,
      "learning_rate": 3.319846008799497e-05,
      "loss": 1.0217,
      "step": 45400
    },
    {
      "epoch": 2.670557965154745,
      "grad_norm": 0.41649797558784485,
      "learning_rate": 3.313953488372093e-05,
      "loss": 1.0448,
      "step": 45410
    },
    {
      "epoch": 2.6711460707196943,
      "grad_norm": 0.39502379298210144,
      "learning_rate": 3.308060967944689e-05,
      "loss": 0.9933,
      "step": 45420
    },
    {
      "epoch": 2.671734176284643,
      "grad_norm": 0.38322699069976807,
      "learning_rate": 3.3021684475172845e-05,
      "loss": 0.9675,
      "step": 45430
    },
    {
      "epoch": 2.672322281849592,
      "grad_norm": 0.41302692890167236,
      "learning_rate": 3.29627592708988e-05,
      "loss": 0.9588,
      "step": 45440
    },
    {
      "epoch": 2.672910387414541,
      "grad_norm": 0.41143250465393066,
      "learning_rate": 3.290383406662476e-05,
      "loss": 0.9813,
      "step": 45450
    },
    {
      "epoch": 2.6734984929794896,
      "grad_norm": 0.43191495537757874,
      "learning_rate": 3.284490886235072e-05,
      "loss": 1.0097,
      "step": 45460
    },
    {
      "epoch": 2.674086598544439,
      "grad_norm": 0.3869510591030121,
      "learning_rate": 3.278598365807668e-05,
      "loss": 1.0095,
      "step": 45470
    },
    {
      "epoch": 2.6746747041093877,
      "grad_norm": 0.36794736981391907,
      "learning_rate": 3.272705845380264e-05,
      "loss": 0.9645,
      "step": 45480
    },
    {
      "epoch": 2.6752628096743365,
      "grad_norm": 0.35787448287010193,
      "learning_rate": 3.266813324952859e-05,
      "loss": 0.9888,
      "step": 45490
    },
    {
      "epoch": 2.6758509152392853,
      "grad_norm": 0.4049752950668335,
      "learning_rate": 3.260920804525455e-05,
      "loss": 1.0258,
      "step": 45500
    },
    {
      "epoch": 2.676439020804234,
      "grad_norm": 0.4188894033432007,
      "learning_rate": 3.2550282840980514e-05,
      "loss": 0.994,
      "step": 45510
    },
    {
      "epoch": 2.6770271263691834,
      "grad_norm": 0.3281419277191162,
      "learning_rate": 3.2491357636706475e-05,
      "loss": 0.9593,
      "step": 45520
    },
    {
      "epoch": 2.677615231934132,
      "grad_norm": 0.35566118359565735,
      "learning_rate": 3.243243243243243e-05,
      "loss": 1.0027,
      "step": 45530
    },
    {
      "epoch": 2.678203337499081,
      "grad_norm": 0.45840975642204285,
      "learning_rate": 3.2373507228158384e-05,
      "loss": 0.9377,
      "step": 45540
    },
    {
      "epoch": 2.67879144306403,
      "grad_norm": 0.38455113768577576,
      "learning_rate": 3.2314582023884345e-05,
      "loss": 0.9125,
      "step": 45550
    },
    {
      "epoch": 2.6793795486289786,
      "grad_norm": 0.4412943124771118,
      "learning_rate": 3.2255656819610306e-05,
      "loss": 1.0748,
      "step": 45560
    },
    {
      "epoch": 2.679967654193928,
      "grad_norm": 0.3690968155860901,
      "learning_rate": 3.219673161533627e-05,
      "loss": 0.9326,
      "step": 45570
    },
    {
      "epoch": 2.6805557597588767,
      "grad_norm": 0.39970824122428894,
      "learning_rate": 3.213780641106222e-05,
      "loss": 0.8413,
      "step": 45580
    },
    {
      "epoch": 2.6811438653238255,
      "grad_norm": 0.4147980809211731,
      "learning_rate": 3.2078881206788176e-05,
      "loss": 0.9435,
      "step": 45590
    },
    {
      "epoch": 2.681731970888775,
      "grad_norm": 0.3947925567626953,
      "learning_rate": 3.201995600251414e-05,
      "loss": 1.0067,
      "step": 45600
    },
    {
      "epoch": 2.682320076453723,
      "grad_norm": 0.40911754965782166,
      "learning_rate": 3.19610307982401e-05,
      "loss": 1.0413,
      "step": 45610
    },
    {
      "epoch": 2.6829081820186724,
      "grad_norm": 0.37421345710754395,
      "learning_rate": 3.190210559396606e-05,
      "loss": 1.0816,
      "step": 45620
    },
    {
      "epoch": 2.6834962875836212,
      "grad_norm": 0.3747112452983856,
      "learning_rate": 3.1843180389692014e-05,
      "loss": 0.9607,
      "step": 45630
    },
    {
      "epoch": 2.68408439314857,
      "grad_norm": 0.3679884970188141,
      "learning_rate": 3.178425518541797e-05,
      "loss": 0.9672,
      "step": 45640
    },
    {
      "epoch": 2.6846724987135193,
      "grad_norm": 0.36001330614089966,
      "learning_rate": 3.172532998114393e-05,
      "loss": 1.0166,
      "step": 45650
    },
    {
      "epoch": 2.685260604278468,
      "grad_norm": 0.3676117956638336,
      "learning_rate": 3.166640477686989e-05,
      "loss": 1.0478,
      "step": 45660
    },
    {
      "epoch": 2.685848709843417,
      "grad_norm": 0.3679780960083008,
      "learning_rate": 3.160747957259585e-05,
      "loss": 0.9677,
      "step": 45670
    },
    {
      "epoch": 2.6864368154083658,
      "grad_norm": 0.39136165380477905,
      "learning_rate": 3.154855436832181e-05,
      "loss": 1.045,
      "step": 45680
    },
    {
      "epoch": 2.6870249209733146,
      "grad_norm": 0.35413235425949097,
      "learning_rate": 3.148962916404777e-05,
      "loss": 1.0703,
      "step": 45690
    },
    {
      "epoch": 2.687613026538264,
      "grad_norm": 0.3955681622028351,
      "learning_rate": 3.143070395977372e-05,
      "loss": 1.0527,
      "step": 45700
    },
    {
      "epoch": 2.6882011321032127,
      "grad_norm": 0.3747521638870239,
      "learning_rate": 3.1371778755499684e-05,
      "loss": 1.0462,
      "step": 45710
    },
    {
      "epoch": 2.6887892376681615,
      "grad_norm": 0.3809453547000885,
      "learning_rate": 3.1312853551225645e-05,
      "loss": 0.9428,
      "step": 45720
    },
    {
      "epoch": 2.6893773432331103,
      "grad_norm": 0.34014642238616943,
      "learning_rate": 3.12539283469516e-05,
      "loss": 1.0013,
      "step": 45730
    },
    {
      "epoch": 2.689965448798059,
      "grad_norm": 0.3873802125453949,
      "learning_rate": 3.119500314267756e-05,
      "loss": 1.0068,
      "step": 45740
    },
    {
      "epoch": 2.6905535543630084,
      "grad_norm": 0.46591487526893616,
      "learning_rate": 3.1136077938403515e-05,
      "loss": 0.9825,
      "step": 45750
    },
    {
      "epoch": 2.691141659927957,
      "grad_norm": 0.33786872029304504,
      "learning_rate": 3.1077152734129476e-05,
      "loss": 1.1075,
      "step": 45760
    },
    {
      "epoch": 2.691729765492906,
      "grad_norm": 0.4165431559085846,
      "learning_rate": 3.101822752985544e-05,
      "loss": 1.021,
      "step": 45770
    },
    {
      "epoch": 2.692317871057855,
      "grad_norm": 0.37879687547683716,
      "learning_rate": 3.095930232558139e-05,
      "loss": 0.9524,
      "step": 45780
    },
    {
      "epoch": 2.6929059766228036,
      "grad_norm": 0.3688901364803314,
      "learning_rate": 3.090037712130735e-05,
      "loss": 0.9958,
      "step": 45790
    },
    {
      "epoch": 2.693494082187753,
      "grad_norm": 0.39599403738975525,
      "learning_rate": 3.084145191703331e-05,
      "loss": 0.9425,
      "step": 45800
    },
    {
      "epoch": 2.6940821877527017,
      "grad_norm": 0.36982882022857666,
      "learning_rate": 3.078252671275927e-05,
      "loss": 0.9176,
      "step": 45810
    },
    {
      "epoch": 2.6946702933176505,
      "grad_norm": 0.40773552656173706,
      "learning_rate": 3.072360150848523e-05,
      "loss": 0.9333,
      "step": 45820
    },
    {
      "epoch": 2.6952583988825993,
      "grad_norm": 0.3571947515010834,
      "learning_rate": 3.0664676304211184e-05,
      "loss": 0.9825,
      "step": 45830
    },
    {
      "epoch": 2.695846504447548,
      "grad_norm": 0.421477347612381,
      "learning_rate": 3.0605751099937145e-05,
      "loss": 0.9501,
      "step": 45840
    },
    {
      "epoch": 2.6964346100124974,
      "grad_norm": 0.40964704751968384,
      "learning_rate": 3.05468258956631e-05,
      "loss": 0.9631,
      "step": 45850
    },
    {
      "epoch": 2.697022715577446,
      "grad_norm": 0.37776467204093933,
      "learning_rate": 3.048790069138906e-05,
      "loss": 0.9503,
      "step": 45860
    },
    {
      "epoch": 2.697610821142395,
      "grad_norm": 0.40770870447158813,
      "learning_rate": 3.042897548711502e-05,
      "loss": 1.0107,
      "step": 45870
    },
    {
      "epoch": 2.698198926707344,
      "grad_norm": 0.41314858198165894,
      "learning_rate": 3.037005028284098e-05,
      "loss": 1.0919,
      "step": 45880
    },
    {
      "epoch": 2.6987870322722927,
      "grad_norm": 0.3416231870651245,
      "learning_rate": 3.0311125078566938e-05,
      "loss": 1.0115,
      "step": 45890
    },
    {
      "epoch": 2.699375137837242,
      "grad_norm": 0.3516712486743927,
      "learning_rate": 3.0252199874292892e-05,
      "loss": 1.0026,
      "step": 45900
    },
    {
      "epoch": 2.6999632434021907,
      "grad_norm": 0.3778317868709564,
      "learning_rate": 3.0193274670018853e-05,
      "loss": 0.9737,
      "step": 45910
    },
    {
      "epoch": 2.7005513489671396,
      "grad_norm": 0.43200284242630005,
      "learning_rate": 3.013434946574481e-05,
      "loss": 1.0806,
      "step": 45920
    },
    {
      "epoch": 2.7011394545320884,
      "grad_norm": 0.3702768385410309,
      "learning_rate": 3.0075424261470772e-05,
      "loss": 0.9751,
      "step": 45930
    },
    {
      "epoch": 2.701727560097037,
      "grad_norm": 0.3443833589553833,
      "learning_rate": 3.001649905719673e-05,
      "loss": 0.9102,
      "step": 45940
    },
    {
      "epoch": 2.7023156656619864,
      "grad_norm": 0.3895343840122223,
      "learning_rate": 2.9957573852922688e-05,
      "loss": 0.996,
      "step": 45950
    },
    {
      "epoch": 2.7029037712269353,
      "grad_norm": 0.40794986486434937,
      "learning_rate": 2.9898648648648646e-05,
      "loss": 0.9156,
      "step": 45960
    },
    {
      "epoch": 2.703491876791884,
      "grad_norm": 0.36416563391685486,
      "learning_rate": 2.9839723444374604e-05,
      "loss": 0.9573,
      "step": 45970
    },
    {
      "epoch": 2.704079982356833,
      "grad_norm": 0.4107970595359802,
      "learning_rate": 2.9780798240100565e-05,
      "loss": 0.9811,
      "step": 45980
    },
    {
      "epoch": 2.7046680879217817,
      "grad_norm": 0.3386539816856384,
      "learning_rate": 2.9721873035826523e-05,
      "loss": 0.8538,
      "step": 45990
    },
    {
      "epoch": 2.705256193486731,
      "grad_norm": 0.3943059742450714,
      "learning_rate": 2.966294783155248e-05,
      "loss": 0.9836,
      "step": 46000
    },
    {
      "epoch": 2.70584429905168,
      "grad_norm": 0.35084807872772217,
      "learning_rate": 2.9604022627278438e-05,
      "loss": 1.0007,
      "step": 46010
    },
    {
      "epoch": 2.7064324046166286,
      "grad_norm": 0.36959075927734375,
      "learning_rate": 2.9545097423004396e-05,
      "loss": 0.9337,
      "step": 46020
    },
    {
      "epoch": 2.707020510181578,
      "grad_norm": 0.3652416467666626,
      "learning_rate": 2.9486172218730357e-05,
      "loss": 0.9759,
      "step": 46030
    },
    {
      "epoch": 2.7076086157465262,
      "grad_norm": 0.35423544049263,
      "learning_rate": 2.9427247014456315e-05,
      "loss": 1.0032,
      "step": 46040
    },
    {
      "epoch": 2.7081967213114755,
      "grad_norm": 0.36664557456970215,
      "learning_rate": 2.9368321810182273e-05,
      "loss": 0.9672,
      "step": 46050
    },
    {
      "epoch": 2.7087848268764243,
      "grad_norm": 0.4362778663635254,
      "learning_rate": 2.930939660590823e-05,
      "loss": 0.9264,
      "step": 46060
    },
    {
      "epoch": 2.709372932441373,
      "grad_norm": 0.3796692490577698,
      "learning_rate": 2.925047140163419e-05,
      "loss": 1.0083,
      "step": 46070
    },
    {
      "epoch": 2.7099610380063224,
      "grad_norm": NaN,
      "learning_rate": 2.919154619736015e-05,
      "loss": 0.9799,
      "step": 46080
    },
    {
      "epoch": 2.710549143571271,
      "grad_norm": 0.36561548709869385,
      "learning_rate": 2.913851351351351e-05,
      "loss": 1.0655,
      "step": 46090
    },
    {
      "epoch": 2.71113724913622,
      "grad_norm": 0.35714828968048096,
      "learning_rate": 2.907958830923947e-05,
      "loss": 1.013,
      "step": 46100
    },
    {
      "epoch": 2.711725354701169,
      "grad_norm": 0.3952849507331848,
      "learning_rate": 2.9020663104965428e-05,
      "loss": 0.9385,
      "step": 46110
    },
    {
      "epoch": 2.7123134602661176,
      "grad_norm": 0.3979019522666931,
      "learning_rate": 2.8961737900691385e-05,
      "loss": 0.9531,
      "step": 46120
    },
    {
      "epoch": 2.712901565831067,
      "grad_norm": 0.3555065095424652,
      "learning_rate": 2.8902812696417347e-05,
      "loss": 0.9344,
      "step": 46130
    },
    {
      "epoch": 2.7134896713960157,
      "grad_norm": 0.4196799695491791,
      "learning_rate": 2.8843887492143304e-05,
      "loss": 1.0132,
      "step": 46140
    },
    {
      "epoch": 2.7140777769609645,
      "grad_norm": 0.39087018370628357,
      "learning_rate": 2.8784962287869262e-05,
      "loss": 0.9786,
      "step": 46150
    },
    {
      "epoch": 2.7146658825259133,
      "grad_norm": 0.43755483627319336,
      "learning_rate": 2.872603708359522e-05,
      "loss": 0.9808,
      "step": 46160
    },
    {
      "epoch": 2.715253988090862,
      "grad_norm": 0.37359416484832764,
      "learning_rate": 2.8667111879321178e-05,
      "loss": 1.0311,
      "step": 46170
    },
    {
      "epoch": 2.7158420936558114,
      "grad_norm": 0.4268099367618561,
      "learning_rate": 2.860818667504714e-05,
      "loss": 0.8783,
      "step": 46180
    },
    {
      "epoch": 2.7164301992207602,
      "grad_norm": 0.4048248827457428,
      "learning_rate": 2.8549261470773097e-05,
      "loss": 1.0195,
      "step": 46190
    },
    {
      "epoch": 2.717018304785709,
      "grad_norm": 0.3764890730381012,
      "learning_rate": 2.8490336266499055e-05,
      "loss": 0.9127,
      "step": 46200
    },
    {
      "epoch": 2.717606410350658,
      "grad_norm": 0.38276800513267517,
      "learning_rate": 2.8431411062225013e-05,
      "loss": 0.9027,
      "step": 46210
    },
    {
      "epoch": 2.7181945159156067,
      "grad_norm": 0.3601148724555969,
      "learning_rate": 2.837248585795097e-05,
      "loss": 1.0365,
      "step": 46220
    },
    {
      "epoch": 2.718782621480556,
      "grad_norm": 0.43203437328338623,
      "learning_rate": 2.831356065367693e-05,
      "loss": 1.041,
      "step": 46230
    },
    {
      "epoch": 2.7193707270455048,
      "grad_norm": 0.3530414402484894,
      "learning_rate": 2.825463544940289e-05,
      "loss": 1.0624,
      "step": 46240
    },
    {
      "epoch": 2.7199588326104536,
      "grad_norm": 0.44036927819252014,
      "learning_rate": 2.8195710245128847e-05,
      "loss": 1.0167,
      "step": 46250
    },
    {
      "epoch": 2.7205469381754024,
      "grad_norm": 0.4925484359264374,
      "learning_rate": 2.8136785040854805e-05,
      "loss": 0.9763,
      "step": 46260
    },
    {
      "epoch": 2.721135043740351,
      "grad_norm": 0.38115525245666504,
      "learning_rate": 2.8077859836580763e-05,
      "loss": 0.9824,
      "step": 46270
    },
    {
      "epoch": 2.7217231493053005,
      "grad_norm": 0.42994436621665955,
      "learning_rate": 2.8018934632306724e-05,
      "loss": 0.9946,
      "step": 46280
    },
    {
      "epoch": 2.7223112548702493,
      "grad_norm": 0.396126389503479,
      "learning_rate": 2.7960009428032682e-05,
      "loss": 1.0176,
      "step": 46290
    },
    {
      "epoch": 2.722899360435198,
      "grad_norm": 0.38707777857780457,
      "learning_rate": 2.790108422375864e-05,
      "loss": 0.9534,
      "step": 46300
    },
    {
      "epoch": 2.723487466000147,
      "grad_norm": 0.38278767466545105,
      "learning_rate": 2.7842159019484597e-05,
      "loss": 1.0642,
      "step": 46310
    },
    {
      "epoch": 2.7240755715650957,
      "grad_norm": 0.3635057508945465,
      "learning_rate": 2.7783233815210555e-05,
      "loss": 1.0329,
      "step": 46320
    },
    {
      "epoch": 2.724663677130045,
      "grad_norm": 0.3478449583053589,
      "learning_rate": 2.7724308610936516e-05,
      "loss": 0.9508,
      "step": 46330
    },
    {
      "epoch": 2.725251782694994,
      "grad_norm": 0.38524457812309265,
      "learning_rate": 2.7665383406662474e-05,
      "loss": 1.0117,
      "step": 46340
    },
    {
      "epoch": 2.7258398882599426,
      "grad_norm": 0.36373329162597656,
      "learning_rate": 2.7606458202388435e-05,
      "loss": 1.0841,
      "step": 46350
    },
    {
      "epoch": 2.7264279938248914,
      "grad_norm": 0.36432376503944397,
      "learning_rate": 2.754753299811439e-05,
      "loss": 0.968,
      "step": 46360
    },
    {
      "epoch": 2.7270160993898402,
      "grad_norm": 0.4396683871746063,
      "learning_rate": 2.7488607793840348e-05,
      "loss": 0.9827,
      "step": 46370
    },
    {
      "epoch": 2.7276042049547895,
      "grad_norm": 0.40991824865341187,
      "learning_rate": 2.742968258956631e-05,
      "loss": 1.0083,
      "step": 46380
    },
    {
      "epoch": 2.7281923105197383,
      "grad_norm": 0.4726414978504181,
      "learning_rate": 2.7370757385292267e-05,
      "loss": 0.9472,
      "step": 46390
    },
    {
      "epoch": 2.728780416084687,
      "grad_norm": 0.35389578342437744,
      "learning_rate": 2.7311832181018228e-05,
      "loss": 1.0267,
      "step": 46400
    },
    {
      "epoch": 2.729368521649636,
      "grad_norm": 0.3980869948863983,
      "learning_rate": 2.7252906976744182e-05,
      "loss": 0.9774,
      "step": 46410
    },
    {
      "epoch": 2.7299566272145848,
      "grad_norm": 0.41142767667770386,
      "learning_rate": 2.719398177247014e-05,
      "loss": 1.0379,
      "step": 46420
    },
    {
      "epoch": 2.730544732779534,
      "grad_norm": 0.447050541639328,
      "learning_rate": 2.71350565681961e-05,
      "loss": 0.9865,
      "step": 46430
    },
    {
      "epoch": 2.731132838344483,
      "grad_norm": 0.39716097712516785,
      "learning_rate": 2.707613136392206e-05,
      "loss": 1.0216,
      "step": 46440
    },
    {
      "epoch": 2.7317209439094317,
      "grad_norm": 0.3812381625175476,
      "learning_rate": 2.701720615964802e-05,
      "loss": 1.0609,
      "step": 46450
    },
    {
      "epoch": 2.732309049474381,
      "grad_norm": 0.33205148577690125,
      "learning_rate": 2.6958280955373975e-05,
      "loss": 0.9949,
      "step": 46460
    },
    {
      "epoch": 2.7328971550393293,
      "grad_norm": 0.3709711730480194,
      "learning_rate": 2.6899355751099933e-05,
      "loss": 0.9478,
      "step": 46470
    },
    {
      "epoch": 2.7334852606042785,
      "grad_norm": 0.41877982020378113,
      "learning_rate": 2.6840430546825894e-05,
      "loss": 1.0729,
      "step": 46480
    },
    {
      "epoch": 2.7340733661692274,
      "grad_norm": 0.3658739924430847,
      "learning_rate": 2.678150534255185e-05,
      "loss": 1.0581,
      "step": 46490
    },
    {
      "epoch": 2.734661471734176,
      "grad_norm": 0.3880887031555176,
      "learning_rate": 2.6722580138277813e-05,
      "loss": 0.9908,
      "step": 46500
    },
    {
      "epoch": 2.7352495772991254,
      "grad_norm": 0.3967512249946594,
      "learning_rate": 2.6663654934003767e-05,
      "loss": 0.9244,
      "step": 46510
    },
    {
      "epoch": 2.7358376828640742,
      "grad_norm": 0.3492740988731384,
      "learning_rate": 2.6604729729729725e-05,
      "loss": 1.0428,
      "step": 46520
    },
    {
      "epoch": 2.736425788429023,
      "grad_norm": 0.3760184049606323,
      "learning_rate": 2.6545804525455686e-05,
      "loss": 1.0338,
      "step": 46530
    },
    {
      "epoch": 2.737013893993972,
      "grad_norm": 0.35498157143592834,
      "learning_rate": 2.6486879321181644e-05,
      "loss": 0.9633,
      "step": 46540
    },
    {
      "epoch": 2.7376019995589207,
      "grad_norm": 0.3899804353713989,
      "learning_rate": 2.6427954116907605e-05,
      "loss": 1.08,
      "step": 46550
    },
    {
      "epoch": 2.73819010512387,
      "grad_norm": 0.416370153427124,
      "learning_rate": 2.6369028912633563e-05,
      "loss": 0.9911,
      "step": 46560
    },
    {
      "epoch": 2.7387782106888188,
      "grad_norm": 0.36171314120292664,
      "learning_rate": 2.6310103708359517e-05,
      "loss": 1.0778,
      "step": 46570
    },
    {
      "epoch": 2.7393663162537676,
      "grad_norm": 0.3965797424316406,
      "learning_rate": 2.625117850408548e-05,
      "loss": 1.0465,
      "step": 46580
    },
    {
      "epoch": 2.7399544218187164,
      "grad_norm": 0.34470489621162415,
      "learning_rate": 2.6192253299811436e-05,
      "loss": 1.0283,
      "step": 46590
    },
    {
      "epoch": 2.740542527383665,
      "grad_norm": 0.43815377354621887,
      "learning_rate": 2.6133328095537398e-05,
      "loss": 1.0034,
      "step": 46600
    },
    {
      "epoch": 2.7411306329486145,
      "grad_norm": 0.38758617639541626,
      "learning_rate": 2.6074402891263355e-05,
      "loss": 1.0602,
      "step": 46610
    },
    {
      "epoch": 2.7417187385135633,
      "grad_norm": 0.3636588752269745,
      "learning_rate": 2.601547768698931e-05,
      "loss": 0.9792,
      "step": 46620
    },
    {
      "epoch": 2.742306844078512,
      "grad_norm": 0.368527352809906,
      "learning_rate": 2.595655248271527e-05,
      "loss": 0.9224,
      "step": 46630
    },
    {
      "epoch": 2.742894949643461,
      "grad_norm": 0.35805654525756836,
      "learning_rate": 2.589762727844123e-05,
      "loss": 1.0022,
      "step": 46640
    },
    {
      "epoch": 2.7434830552084097,
      "grad_norm": 0.3586598336696625,
      "learning_rate": 2.583870207416719e-05,
      "loss": 1.0153,
      "step": 46650
    },
    {
      "epoch": 2.744071160773359,
      "grad_norm": 0.4390771985054016,
      "learning_rate": 2.5779776869893148e-05,
      "loss": 0.9346,
      "step": 46660
    },
    {
      "epoch": 2.744659266338308,
      "grad_norm": 0.37406739592552185,
      "learning_rate": 2.5720851665619102e-05,
      "loss": 0.9373,
      "step": 46670
    },
    {
      "epoch": 2.7452473719032566,
      "grad_norm": 0.42478448152542114,
      "learning_rate": 2.5661926461345063e-05,
      "loss": 1.1069,
      "step": 46680
    },
    {
      "epoch": 2.7458354774682054,
      "grad_norm": 0.3657771348953247,
      "learning_rate": 2.560300125707102e-05,
      "loss": 0.9709,
      "step": 46690
    },
    {
      "epoch": 2.7464235830331543,
      "grad_norm": 0.32834213972091675,
      "learning_rate": 2.5544076052796983e-05,
      "loss": 1.066,
      "step": 46700
    },
    {
      "epoch": 2.7470116885981035,
      "grad_norm": 0.38947901129722595,
      "learning_rate": 2.548515084852294e-05,
      "loss": 0.9566,
      "step": 46710
    },
    {
      "epoch": 2.7475997941630523,
      "grad_norm": 0.39115437865257263,
      "learning_rate": 2.5426225644248895e-05,
      "loss": 0.9711,
      "step": 46720
    },
    {
      "epoch": 2.748187899728001,
      "grad_norm": 0.40202075242996216,
      "learning_rate": 2.5367300439974856e-05,
      "loss": 1.0704,
      "step": 46730
    },
    {
      "epoch": 2.74877600529295,
      "grad_norm": 0.33843955397605896,
      "learning_rate": 2.5308375235700814e-05,
      "loss": 0.9436,
      "step": 46740
    },
    {
      "epoch": 2.749364110857899,
      "grad_norm": 0.37644538283348083,
      "learning_rate": 2.5249450031426775e-05,
      "loss": 0.8599,
      "step": 46750
    },
    {
      "epoch": 2.749952216422848,
      "grad_norm": 0.3856477737426758,
      "learning_rate": 2.5190524827152733e-05,
      "loss": 0.9952,
      "step": 46760
    },
    {
      "epoch": 2.750540321987797,
      "grad_norm": 0.37682974338531494,
      "learning_rate": 2.5131599622878687e-05,
      "loss": 1.0232,
      "step": 46770
    },
    {
      "epoch": 2.7511284275527457,
      "grad_norm": 0.3918776214122772,
      "learning_rate": 2.507267441860465e-05,
      "loss": 1.0804,
      "step": 46780
    },
    {
      "epoch": 2.7517165331176945,
      "grad_norm": 0.401439905166626,
      "learning_rate": 2.5013749214330606e-05,
      "loss": 0.9862,
      "step": 46790
    },
    {
      "epoch": 2.7523046386826433,
      "grad_norm": 0.4448119103908539,
      "learning_rate": 2.4954824010056567e-05,
      "loss": 0.9638,
      "step": 46800
    },
    {
      "epoch": 2.7528927442475926,
      "grad_norm": 0.38423359394073486,
      "learning_rate": 2.4895898805782525e-05,
      "loss": 0.9669,
      "step": 46810
    },
    {
      "epoch": 2.7534808498125414,
      "grad_norm": 0.3695570230484009,
      "learning_rate": 2.4836973601508486e-05,
      "loss": 0.9891,
      "step": 46820
    },
    {
      "epoch": 2.75406895537749,
      "grad_norm": 0.3791056275367737,
      "learning_rate": 2.477804839723444e-05,
      "loss": 1.0172,
      "step": 46830
    },
    {
      "epoch": 2.754657060942439,
      "grad_norm": 0.42112845182418823,
      "learning_rate": 2.47191231929604e-05,
      "loss": 1.046,
      "step": 46840
    },
    {
      "epoch": 2.755245166507388,
      "grad_norm": 0.40183010697364807,
      "learning_rate": 2.466019798868636e-05,
      "loss": 0.9359,
      "step": 46850
    },
    {
      "epoch": 2.755833272072337,
      "grad_norm": 0.3468906283378601,
      "learning_rate": 2.4601272784412318e-05,
      "loss": 0.9577,
      "step": 46860
    },
    {
      "epoch": 2.756421377637286,
      "grad_norm": 0.40437066555023193,
      "learning_rate": 2.454234758013828e-05,
      "loss": 1.0655,
      "step": 46870
    },
    {
      "epoch": 2.7570094832022347,
      "grad_norm": 0.370139479637146,
      "learning_rate": 2.4483422375864233e-05,
      "loss": 0.9554,
      "step": 46880
    },
    {
      "epoch": 2.757597588767184,
      "grad_norm": 0.38845381140708923,
      "learning_rate": 2.442449717159019e-05,
      "loss": 1.0666,
      "step": 46890
    },
    {
      "epoch": 2.7581856943321323,
      "grad_norm": 0.384501576423645,
      "learning_rate": 2.4365571967316152e-05,
      "loss": 1.1068,
      "step": 46900
    },
    {
      "epoch": 2.7587737998970816,
      "grad_norm": 0.37865254282951355,
      "learning_rate": 2.430664676304211e-05,
      "loss": 0.9673,
      "step": 46910
    },
    {
      "epoch": 2.7593619054620304,
      "grad_norm": 0.4047122001647949,
      "learning_rate": 2.424772155876807e-05,
      "loss": 1.0195,
      "step": 46920
    },
    {
      "epoch": 2.7599500110269792,
      "grad_norm": 0.3819757103919983,
      "learning_rate": 2.4188796354494026e-05,
      "loss": 1.0146,
      "step": 46930
    },
    {
      "epoch": 2.7605381165919285,
      "grad_norm": 0.3566201627254486,
      "learning_rate": 2.4129871150219984e-05,
      "loss": 0.8867,
      "step": 46940
    },
    {
      "epoch": 2.7611262221568773,
      "grad_norm": 0.3474029004573822,
      "learning_rate": 2.4070945945945945e-05,
      "loss": 1.0555,
      "step": 46950
    },
    {
      "epoch": 2.761714327721826,
      "grad_norm": 0.37333303689956665,
      "learning_rate": 2.4012020741671903e-05,
      "loss": 0.9044,
      "step": 46960
    },
    {
      "epoch": 2.762302433286775,
      "grad_norm": 0.4061570167541504,
      "learning_rate": 2.3953095537397864e-05,
      "loss": 0.9018,
      "step": 46970
    },
    {
      "epoch": 2.7628905388517238,
      "grad_norm": 0.39104095101356506,
      "learning_rate": 2.3894170333123818e-05,
      "loss": 0.9854,
      "step": 46980
    },
    {
      "epoch": 2.763478644416673,
      "grad_norm": 0.3805422782897949,
      "learning_rate": 2.3835245128849776e-05,
      "loss": 1.0413,
      "step": 46990
    },
    {
      "epoch": 2.764066749981622,
      "grad_norm": 0.36428603529930115,
      "learning_rate": 2.3776319924575737e-05,
      "loss": 1.0237,
      "step": 47000
    },
    {
      "epoch": 2.7646548555465706,
      "grad_norm": 0.3553643524646759,
      "learning_rate": 2.3717394720301695e-05,
      "loss": 1.0851,
      "step": 47010
    },
    {
      "epoch": 2.7652429611115195,
      "grad_norm": 0.4073660671710968,
      "learning_rate": 2.3658469516027656e-05,
      "loss": 0.9541,
      "step": 47020
    },
    {
      "epoch": 2.7658310666764683,
      "grad_norm": 0.33073049783706665,
      "learning_rate": 2.3599544311753614e-05,
      "loss": 0.921,
      "step": 47030
    },
    {
      "epoch": 2.7664191722414175,
      "grad_norm": 0.3598930835723877,
      "learning_rate": 2.354061910747957e-05,
      "loss": 0.9462,
      "step": 47040
    },
    {
      "epoch": 2.7670072778063663,
      "grad_norm": 0.4493519365787506,
      "learning_rate": 2.348169390320553e-05,
      "loss": 1.0352,
      "step": 47050
    },
    {
      "epoch": 2.767595383371315,
      "grad_norm": 0.41565707325935364,
      "learning_rate": 2.3422768698931487e-05,
      "loss": 1.0882,
      "step": 47060
    },
    {
      "epoch": 2.768183488936264,
      "grad_norm": 0.38233664631843567,
      "learning_rate": 2.336384349465745e-05,
      "loss": 0.9432,
      "step": 47070
    },
    {
      "epoch": 2.768771594501213,
      "grad_norm": 0.36490383744239807,
      "learning_rate": 2.3304918290383406e-05,
      "loss": 0.8921,
      "step": 47080
    },
    {
      "epoch": 2.769359700066162,
      "grad_norm": 0.4003854990005493,
      "learning_rate": 2.324599308610936e-05,
      "loss": 0.9238,
      "step": 47090
    },
    {
      "epoch": 2.769947805631111,
      "grad_norm": 0.4240788221359253,
      "learning_rate": 2.3187067881835322e-05,
      "loss": 0.9093,
      "step": 47100
    },
    {
      "epoch": 2.7705359111960597,
      "grad_norm": 0.36396703124046326,
      "learning_rate": 2.312814267756128e-05,
      "loss": 0.9991,
      "step": 47110
    },
    {
      "epoch": 2.7711240167610085,
      "grad_norm": 0.38805297017097473,
      "learning_rate": 2.306921747328724e-05,
      "loss": 1.1262,
      "step": 47120
    },
    {
      "epoch": 2.7717121223259573,
      "grad_norm": 0.3689827024936676,
      "learning_rate": 2.30102922690132e-05,
      "loss": 0.98,
      "step": 47130
    },
    {
      "epoch": 2.7723002278909066,
      "grad_norm": 0.38265031576156616,
      "learning_rate": 2.2951367064739153e-05,
      "loss": 0.9721,
      "step": 47140
    },
    {
      "epoch": 2.7728883334558554,
      "grad_norm": 0.370086133480072,
      "learning_rate": 2.2892441860465114e-05,
      "loss": 0.9718,
      "step": 47150
    },
    {
      "epoch": 2.773476439020804,
      "grad_norm": 0.4791192412376404,
      "learning_rate": 2.2833516656191072e-05,
      "loss": 1.0799,
      "step": 47160
    },
    {
      "epoch": 2.774064544585753,
      "grad_norm": 0.4009818732738495,
      "learning_rate": 2.2774591451917033e-05,
      "loss": 0.9508,
      "step": 47170
    },
    {
      "epoch": 2.774652650150702,
      "grad_norm": 0.4236295223236084,
      "learning_rate": 2.271566624764299e-05,
      "loss": 0.9927,
      "step": 47180
    },
    {
      "epoch": 2.775240755715651,
      "grad_norm": 0.36212265491485596,
      "learning_rate": 2.2656741043368946e-05,
      "loss": 1.0183,
      "step": 47190
    },
    {
      "epoch": 2.7758288612806,
      "grad_norm": 0.3530832827091217,
      "learning_rate": 2.2597815839094907e-05,
      "loss": 1.0224,
      "step": 47200
    },
    {
      "epoch": 2.7764169668455487,
      "grad_norm": 0.3617091178894043,
      "learning_rate": 2.2538890634820865e-05,
      "loss": 0.9471,
      "step": 47210
    },
    {
      "epoch": 2.7770050724104975,
      "grad_norm": 0.4116293489933014,
      "learning_rate": 2.2479965430546826e-05,
      "loss": 1.0564,
      "step": 47220
    },
    {
      "epoch": 2.7775931779754464,
      "grad_norm": 0.3948448896408081,
      "learning_rate": 2.2421040226272784e-05,
      "loss": 1.0304,
      "step": 47230
    },
    {
      "epoch": 2.7781812835403956,
      "grad_norm": 0.3764837384223938,
      "learning_rate": 2.236211502199874e-05,
      "loss": 1.0925,
      "step": 47240
    },
    {
      "epoch": 2.7787693891053444,
      "grad_norm": 0.383099228143692,
      "learning_rate": 2.23031898177247e-05,
      "loss": 1.1056,
      "step": 47250
    },
    {
      "epoch": 2.7793574946702932,
      "grad_norm": 0.3817034661769867,
      "learning_rate": 2.2244264613450657e-05,
      "loss": 0.9011,
      "step": 47260
    },
    {
      "epoch": 2.779945600235242,
      "grad_norm": 0.4284568727016449,
      "learning_rate": 2.218533940917662e-05,
      "loss": 0.9711,
      "step": 47270
    },
    {
      "epoch": 2.780533705800191,
      "grad_norm": 0.33836284279823303,
      "learning_rate": 2.2126414204902576e-05,
      "loss": 0.9649,
      "step": 47280
    },
    {
      "epoch": 2.78112181136514,
      "grad_norm": 0.4527428150177002,
      "learning_rate": 2.2067489000628534e-05,
      "loss": 1.0254,
      "step": 47290
    },
    {
      "epoch": 2.781709916930089,
      "grad_norm": 0.40713638067245483,
      "learning_rate": 2.2008563796354492e-05,
      "loss": 0.9874,
      "step": 47300
    },
    {
      "epoch": 2.7822980224950378,
      "grad_norm": 0.36660581827163696,
      "learning_rate": 2.194963859208045e-05,
      "loss": 0.926,
      "step": 47310
    },
    {
      "epoch": 2.782886128059987,
      "grad_norm": 0.3922480344772339,
      "learning_rate": 2.189071338780641e-05,
      "loss": 1.0158,
      "step": 47320
    },
    {
      "epoch": 2.7834742336249354,
      "grad_norm": 0.359531432390213,
      "learning_rate": 2.183178818353237e-05,
      "loss": 1.0053,
      "step": 47330
    },
    {
      "epoch": 2.7840623391898847,
      "grad_norm": 0.35323530435562134,
      "learning_rate": 2.1772862979258326e-05,
      "loss": 1.0071,
      "step": 47340
    },
    {
      "epoch": 2.7846504447548335,
      "grad_norm": 0.3345901072025299,
      "learning_rate": 2.1713937774984284e-05,
      "loss": 0.9993,
      "step": 47350
    },
    {
      "epoch": 2.7852385503197823,
      "grad_norm": 0.3809867799282074,
      "learning_rate": 2.1655012570710242e-05,
      "loss": 1.0133,
      "step": 47360
    },
    {
      "epoch": 2.7858266558847316,
      "grad_norm": 0.4129105508327484,
      "learning_rate": 2.1596087366436203e-05,
      "loss": 1.0176,
      "step": 47370
    },
    {
      "epoch": 2.7864147614496804,
      "grad_norm": 0.42817050218582153,
      "learning_rate": 2.153716216216216e-05,
      "loss": 0.9948,
      "step": 47380
    },
    {
      "epoch": 2.787002867014629,
      "grad_norm": 0.3323780298233032,
      "learning_rate": 2.147823695788812e-05,
      "loss": 1.1177,
      "step": 47390
    },
    {
      "epoch": 2.787590972579578,
      "grad_norm": 0.36601969599723816,
      "learning_rate": 2.1419311753614077e-05,
      "loss": 1.0365,
      "step": 47400
    },
    {
      "epoch": 2.788179078144527,
      "grad_norm": 0.39665284752845764,
      "learning_rate": 2.1360386549340034e-05,
      "loss": 1.0457,
      "step": 47410
    },
    {
      "epoch": 2.788767183709476,
      "grad_norm": 0.3607757091522217,
      "learning_rate": 2.1301461345065996e-05,
      "loss": 0.9595,
      "step": 47420
    },
    {
      "epoch": 2.789355289274425,
      "grad_norm": 0.35125741362571716,
      "learning_rate": 2.1242536140791953e-05,
      "loss": 0.9409,
      "step": 47430
    },
    {
      "epoch": 2.7899433948393737,
      "grad_norm": 0.3888219892978668,
      "learning_rate": 2.118361093651791e-05,
      "loss": 0.9525,
      "step": 47440
    },
    {
      "epoch": 2.7905315004043225,
      "grad_norm": 0.3803194463253021,
      "learning_rate": 2.1124685732243873e-05,
      "loss": 0.9626,
      "step": 47450
    },
    {
      "epoch": 2.7911196059692713,
      "grad_norm": 0.39622315764427185,
      "learning_rate": 2.1065760527969827e-05,
      "loss": 0.8635,
      "step": 47460
    },
    {
      "epoch": 2.7917077115342206,
      "grad_norm": 0.37016376852989197,
      "learning_rate": 2.1006835323695788e-05,
      "loss": 1.007,
      "step": 47470
    },
    {
      "epoch": 2.7922958170991694,
      "grad_norm": 0.34599748253822327,
      "learning_rate": 2.0947910119421746e-05,
      "loss": 0.993,
      "step": 47480
    },
    {
      "epoch": 2.7928839226641182,
      "grad_norm": 0.39624759554862976,
      "learning_rate": 2.0888984915147704e-05,
      "loss": 1.046,
      "step": 47490
    },
    {
      "epoch": 2.793472028229067,
      "grad_norm": 0.3863421380519867,
      "learning_rate": 2.0830059710873665e-05,
      "loss": 1.0836,
      "step": 47500
    },
    {
      "epoch": 2.794060133794016,
      "grad_norm": 0.3762011229991913,
      "learning_rate": 2.077113450659962e-05,
      "loss": 1.1191,
      "step": 47510
    },
    {
      "epoch": 2.794648239358965,
      "grad_norm": 0.4595823585987091,
      "learning_rate": 2.071220930232558e-05,
      "loss": 1.0402,
      "step": 47520
    },
    {
      "epoch": 2.795236344923914,
      "grad_norm": 0.4246983528137207,
      "learning_rate": 2.065328409805154e-05,
      "loss": 0.9402,
      "step": 47530
    },
    {
      "epoch": 2.7958244504888627,
      "grad_norm": 0.33846515417099,
      "learning_rate": 2.0594358893777496e-05,
      "loss": 1.0266,
      "step": 47540
    },
    {
      "epoch": 2.7964125560538116,
      "grad_norm": 0.3768213391304016,
      "learning_rate": 2.0535433689503457e-05,
      "loss": 0.9645,
      "step": 47550
    },
    {
      "epoch": 2.7970006616187604,
      "grad_norm": 0.4131273329257965,
      "learning_rate": 2.0476508485229412e-05,
      "loss": 0.9274,
      "step": 47560
    },
    {
      "epoch": 2.7975887671837096,
      "grad_norm": 0.3392484784126282,
      "learning_rate": 2.0417583280955373e-05,
      "loss": 0.9949,
      "step": 47570
    },
    {
      "epoch": 2.7981768727486585,
      "grad_norm": 0.4637696444988251,
      "learning_rate": 2.035865807668133e-05,
      "loss": 0.9571,
      "step": 47580
    },
    {
      "epoch": 2.7987649783136073,
      "grad_norm": 0.39033713936805725,
      "learning_rate": 2.029973287240729e-05,
      "loss": 1.0126,
      "step": 47590
    },
    {
      "epoch": 2.799353083878556,
      "grad_norm": 0.3210224509239197,
      "learning_rate": 2.024080766813325e-05,
      "loss": 1.064,
      "step": 47600
    },
    {
      "epoch": 2.799941189443505,
      "grad_norm": 0.36947834491729736,
      "learning_rate": 2.0181882463859204e-05,
      "loss": 0.9956,
      "step": 47610
    },
    {
      "epoch": 2.800529295008454,
      "grad_norm": 0.3724961280822754,
      "learning_rate": 2.0122957259585162e-05,
      "loss": 0.9894,
      "step": 47620
    },
    {
      "epoch": 2.801117400573403,
      "grad_norm": 0.3904975354671478,
      "learning_rate": 2.0064032055311123e-05,
      "loss": 1.0246,
      "step": 47630
    },
    {
      "epoch": 2.801705506138352,
      "grad_norm": 0.4183513820171356,
      "learning_rate": 2.000510685103708e-05,
      "loss": 1.0309,
      "step": 47640
    },
    {
      "epoch": 2.8022936117033006,
      "grad_norm": 0.39085516333580017,
      "learning_rate": 1.9946181646763042e-05,
      "loss": 1.025,
      "step": 47650
    },
    {
      "epoch": 2.8028817172682494,
      "grad_norm": 0.37647801637649536,
      "learning_rate": 1.9887256442488997e-05,
      "loss": 0.9107,
      "step": 47660
    },
    {
      "epoch": 2.8034698228331987,
      "grad_norm": 0.3711164891719818,
      "learning_rate": 1.9828331238214955e-05,
      "loss": 1.0052,
      "step": 47670
    },
    {
      "epoch": 2.8040579283981475,
      "grad_norm": 0.44872793555259705,
      "learning_rate": 1.9769406033940916e-05,
      "loss": 0.9106,
      "step": 47680
    },
    {
      "epoch": 2.8046460339630963,
      "grad_norm": 0.3838959336280823,
      "learning_rate": 1.9710480829666874e-05,
      "loss": 0.9867,
      "step": 47690
    },
    {
      "epoch": 2.805234139528045,
      "grad_norm": 0.4786570072174072,
      "learning_rate": 1.9651555625392835e-05,
      "loss": 1.0472,
      "step": 47700
    },
    {
      "epoch": 2.805822245092994,
      "grad_norm": 0.3762829303741455,
      "learning_rate": 1.9592630421118793e-05,
      "loss": 1.0076,
      "step": 47710
    },
    {
      "epoch": 2.806410350657943,
      "grad_norm": 0.3231843411922455,
      "learning_rate": 1.9533705216844747e-05,
      "loss": 0.9378,
      "step": 47720
    },
    {
      "epoch": 2.806998456222892,
      "grad_norm": 0.3258853256702423,
      "learning_rate": 1.9474780012570708e-05,
      "loss": 0.9824,
      "step": 47730
    },
    {
      "epoch": 2.807586561787841,
      "grad_norm": 0.4072295129299164,
      "learning_rate": 1.9415854808296666e-05,
      "loss": 1.0203,
      "step": 47740
    },
    {
      "epoch": 2.80817466735279,
      "grad_norm": 0.3764721751213074,
      "learning_rate": 1.9356929604022627e-05,
      "loss": 1.0687,
      "step": 47750
    },
    {
      "epoch": 2.8087627729177385,
      "grad_norm": 0.4336734414100647,
      "learning_rate": 1.9298004399748585e-05,
      "loss": 1.0256,
      "step": 47760
    },
    {
      "epoch": 2.8093508784826877,
      "grad_norm": 0.4423017203807831,
      "learning_rate": 1.923907919547454e-05,
      "loss": 0.9333,
      "step": 47770
    },
    {
      "epoch": 2.8099389840476365,
      "grad_norm": 0.4047200381755829,
      "learning_rate": 1.91801539912005e-05,
      "loss": 0.9788,
      "step": 47780
    },
    {
      "epoch": 2.8105270896125853,
      "grad_norm": 0.3784850537776947,
      "learning_rate": 1.912122878692646e-05,
      "loss": 0.9544,
      "step": 47790
    },
    {
      "epoch": 2.8111151951775346,
      "grad_norm": 0.3737334609031677,
      "learning_rate": 1.906230358265242e-05,
      "loss": 0.9461,
      "step": 47800
    },
    {
      "epoch": 2.8117033007424834,
      "grad_norm": 0.42561984062194824,
      "learning_rate": 1.9003378378378377e-05,
      "loss": 1.0319,
      "step": 47810
    },
    {
      "epoch": 2.8122914063074322,
      "grad_norm": 0.33565792441368103,
      "learning_rate": 1.8944453174104332e-05,
      "loss": 1.0201,
      "step": 47820
    },
    {
      "epoch": 2.812879511872381,
      "grad_norm": 0.41254037618637085,
      "learning_rate": 1.8885527969830293e-05,
      "loss": 1.0045,
      "step": 47830
    },
    {
      "epoch": 2.81346761743733,
      "grad_norm": 0.39742016792297363,
      "learning_rate": 1.882660276555625e-05,
      "loss": 0.9621,
      "step": 47840
    },
    {
      "epoch": 2.814055723002279,
      "grad_norm": 0.49110445380210876,
      "learning_rate": 1.8767677561282212e-05,
      "loss": 1.0609,
      "step": 47850
    },
    {
      "epoch": 2.814643828567228,
      "grad_norm": 0.4094493091106415,
      "learning_rate": 1.870875235700817e-05,
      "loss": 1.0742,
      "step": 47860
    },
    {
      "epoch": 2.8152319341321768,
      "grad_norm": 0.3887023627758026,
      "learning_rate": 1.8649827152734128e-05,
      "loss": 0.9712,
      "step": 47870
    },
    {
      "epoch": 2.8158200396971256,
      "grad_norm": 0.38521477580070496,
      "learning_rate": 1.859090194846009e-05,
      "loss": 0.9485,
      "step": 47880
    },
    {
      "epoch": 2.8164081452620744,
      "grad_norm": 0.3722473382949829,
      "learning_rate": 1.8531976744186043e-05,
      "loss": 1.0107,
      "step": 47890
    },
    {
      "epoch": 2.8169962508270237,
      "grad_norm": 0.36746764183044434,
      "learning_rate": 1.8473051539912004e-05,
      "loss": 0.9999,
      "step": 47900
    },
    {
      "epoch": 2.8175843563919725,
      "grad_norm": 0.33936992287635803,
      "learning_rate": 1.8414126335637962e-05,
      "loss": 1.05,
      "step": 47910
    },
    {
      "epoch": 2.8181724619569213,
      "grad_norm": 0.37782061100006104,
      "learning_rate": 1.835520113136392e-05,
      "loss": 0.9451,
      "step": 47920
    },
    {
      "epoch": 2.81876056752187,
      "grad_norm": 0.45065781474113464,
      "learning_rate": 1.829627592708988e-05,
      "loss": 1.0851,
      "step": 47930
    },
    {
      "epoch": 2.819348673086819,
      "grad_norm": 0.41270533204078674,
      "learning_rate": 1.8237350722815836e-05,
      "loss": 0.9972,
      "step": 47940
    },
    {
      "epoch": 2.819936778651768,
      "grad_norm": 0.36556729674339294,
      "learning_rate": 1.8178425518541797e-05,
      "loss": 0.9976,
      "step": 47950
    },
    {
      "epoch": 2.820524884216717,
      "grad_norm": 0.36596524715423584,
      "learning_rate": 1.8119500314267755e-05,
      "loss": 0.9865,
      "step": 47960
    },
    {
      "epoch": 2.821112989781666,
      "grad_norm": 0.41983112692832947,
      "learning_rate": 1.8060575109993713e-05,
      "loss": 1.0851,
      "step": 47970
    },
    {
      "epoch": 2.8217010953466146,
      "grad_norm": 0.3848251700401306,
      "learning_rate": 1.8001649905719674e-05,
      "loss": 0.942,
      "step": 47980
    },
    {
      "epoch": 2.8222892009115634,
      "grad_norm": 0.3649612367153168,
      "learning_rate": 1.7942724701445628e-05,
      "loss": 1.0162,
      "step": 47990
    },
    {
      "epoch": 2.8228773064765127,
      "grad_norm": 0.4150194525718689,
      "learning_rate": 1.788379949717159e-05,
      "loss": 1.1161,
      "step": 48000
    },
    {
      "epoch": 2.8234654120414615,
      "grad_norm": 0.3697972893714905,
      "learning_rate": 1.7824874292897547e-05,
      "loss": 1.088,
      "step": 48010
    },
    {
      "epoch": 2.8240535176064103,
      "grad_norm": 0.36896437406539917,
      "learning_rate": 1.7765949088623505e-05,
      "loss": 0.9272,
      "step": 48020
    },
    {
      "epoch": 2.824641623171359,
      "grad_norm": 0.37230291962623596,
      "learning_rate": 1.7707023884349466e-05,
      "loss": 0.9863,
      "step": 48030
    },
    {
      "epoch": 2.825229728736308,
      "grad_norm": 0.3709312975406647,
      "learning_rate": 1.764809868007542e-05,
      "loss": 0.9666,
      "step": 48040
    },
    {
      "epoch": 2.825817834301257,
      "grad_norm": 0.39160484075546265,
      "learning_rate": 1.7589173475801382e-05,
      "loss": 1.0327,
      "step": 48050
    },
    {
      "epoch": 2.826405939866206,
      "grad_norm": 0.38254573941230774,
      "learning_rate": 1.753024827152734e-05,
      "loss": 1.0169,
      "step": 48060
    },
    {
      "epoch": 2.826994045431155,
      "grad_norm": 0.41725385189056396,
      "learning_rate": 1.7471323067253297e-05,
      "loss": 1.001,
      "step": 48070
    },
    {
      "epoch": 2.8275821509961037,
      "grad_norm": 0.4735445976257324,
      "learning_rate": 1.741239786297926e-05,
      "loss": 1.0515,
      "step": 48080
    },
    {
      "epoch": 2.8281702565610525,
      "grad_norm": 0.34123724699020386,
      "learning_rate": 1.735936517913262e-05,
      "loss": 0.9914,
      "step": 48090
    },
    {
      "epoch": 2.8287583621260017,
      "grad_norm": 0.3726741075515747,
      "learning_rate": 1.730043997485858e-05,
      "loss": 1.0442,
      "step": 48100
    },
    {
      "epoch": 2.8293464676909506,
      "grad_norm": 0.40046051144599915,
      "learning_rate": 1.7241514770584537e-05,
      "loss": 1.067,
      "step": 48110
    },
    {
      "epoch": 2.8299345732558994,
      "grad_norm": 0.45077112317085266,
      "learning_rate": 1.7182589566310494e-05,
      "loss": 1.0749,
      "step": 48120
    },
    {
      "epoch": 2.830522678820848,
      "grad_norm": 0.37117037177085876,
      "learning_rate": 1.7123664362036452e-05,
      "loss": 1.0616,
      "step": 48130
    },
    {
      "epoch": 2.831110784385797,
      "grad_norm": 0.3170959949493408,
      "learning_rate": 1.7064739157762413e-05,
      "loss": 0.9754,
      "step": 48140
    },
    {
      "epoch": 2.8316988899507463,
      "grad_norm": 0.3483620285987854,
      "learning_rate": 1.700581395348837e-05,
      "loss": 1.0458,
      "step": 48150
    },
    {
      "epoch": 2.832286995515695,
      "grad_norm": 0.3952908217906952,
      "learning_rate": 1.694688874921433e-05,
      "loss": 1.0355,
      "step": 48160
    },
    {
      "epoch": 2.832875101080644,
      "grad_norm": 0.36802899837493896,
      "learning_rate": 1.6887963544940287e-05,
      "loss": 0.9978,
      "step": 48170
    },
    {
      "epoch": 2.833463206645593,
      "grad_norm": 0.4111422002315521,
      "learning_rate": 1.6829038340666245e-05,
      "loss": 1.0027,
      "step": 48180
    },
    {
      "epoch": 2.8340513122105415,
      "grad_norm": 0.36343568563461304,
      "learning_rate": 1.6770113136392206e-05,
      "loss": 1.0235,
      "step": 48190
    },
    {
      "epoch": 2.8346394177754908,
      "grad_norm": 0.36514467000961304,
      "learning_rate": 1.6711187932118164e-05,
      "loss": 0.9369,
      "step": 48200
    },
    {
      "epoch": 2.8352275233404396,
      "grad_norm": 0.39011240005493164,
      "learning_rate": 1.665226272784412e-05,
      "loss": 1.0044,
      "step": 48210
    },
    {
      "epoch": 2.8358156289053884,
      "grad_norm": 0.41705578565597534,
      "learning_rate": 1.659333752357008e-05,
      "loss": 0.9716,
      "step": 48220
    },
    {
      "epoch": 2.8364037344703377,
      "grad_norm": 0.36840933561325073,
      "learning_rate": 1.6534412319296037e-05,
      "loss": 0.9714,
      "step": 48230
    },
    {
      "epoch": 2.8369918400352865,
      "grad_norm": 0.4071277379989624,
      "learning_rate": 1.6475487115021998e-05,
      "loss": 1.0631,
      "step": 48240
    },
    {
      "epoch": 2.8375799456002353,
      "grad_norm": 0.4277479648590088,
      "learning_rate": 1.6416561910747956e-05,
      "loss": 0.961,
      "step": 48250
    },
    {
      "epoch": 2.838168051165184,
      "grad_norm": 0.3589763343334198,
      "learning_rate": 1.6357636706473914e-05,
      "loss": 1.0025,
      "step": 48260
    },
    {
      "epoch": 2.838756156730133,
      "grad_norm": 0.4453011155128479,
      "learning_rate": 1.6298711502199872e-05,
      "loss": 0.9889,
      "step": 48270
    },
    {
      "epoch": 2.839344262295082,
      "grad_norm": 0.3464774489402771,
      "learning_rate": 1.6239786297925833e-05,
      "loss": 0.937,
      "step": 48280
    },
    {
      "epoch": 2.839932367860031,
      "grad_norm": 0.36505886912345886,
      "learning_rate": 1.618086109365179e-05,
      "loss": 1.0841,
      "step": 48290
    },
    {
      "epoch": 2.84052047342498,
      "grad_norm": 0.44509342312812805,
      "learning_rate": 1.612193588937775e-05,
      "loss": 0.9695,
      "step": 48300
    },
    {
      "epoch": 2.8411085789899286,
      "grad_norm": 0.4255252480506897,
      "learning_rate": 1.6063010685103706e-05,
      "loss": 0.918,
      "step": 48310
    },
    {
      "epoch": 2.8416966845548775,
      "grad_norm": 0.41076329350471497,
      "learning_rate": 1.6004085480829664e-05,
      "loss": 0.9667,
      "step": 48320
    },
    {
      "epoch": 2.8422847901198267,
      "grad_norm": 0.3477247655391693,
      "learning_rate": 1.5945160276555625e-05,
      "loss": 0.9748,
      "step": 48330
    },
    {
      "epoch": 2.8428728956847755,
      "grad_norm": 0.34157946705818176,
      "learning_rate": 1.5886235072281583e-05,
      "loss": 0.9556,
      "step": 48340
    },
    {
      "epoch": 2.8434610012497243,
      "grad_norm": 0.36091238260269165,
      "learning_rate": 1.582730986800754e-05,
      "loss": 0.9917,
      "step": 48350
    },
    {
      "epoch": 2.844049106814673,
      "grad_norm": 0.40925779938697815,
      "learning_rate": 1.57683846637335e-05,
      "loss": 0.9973,
      "step": 48360
    },
    {
      "epoch": 2.844637212379622,
      "grad_norm": 0.31596317887306213,
      "learning_rate": 1.5709459459459457e-05,
      "loss": 1.0019,
      "step": 48370
    },
    {
      "epoch": 2.8452253179445712,
      "grad_norm": 0.35854339599609375,
      "learning_rate": 1.5650534255185418e-05,
      "loss": 1.0118,
      "step": 48380
    },
    {
      "epoch": 2.84581342350952,
      "grad_norm": 0.3325769305229187,
      "learning_rate": 1.5591609050911376e-05,
      "loss": 1.0288,
      "step": 48390
    },
    {
      "epoch": 2.846401529074469,
      "grad_norm": 0.3932381868362427,
      "learning_rate": 1.5532683846637333e-05,
      "loss": 0.9105,
      "step": 48400
    },
    {
      "epoch": 2.8469896346394177,
      "grad_norm": 0.3085331618785858,
      "learning_rate": 1.5473758642363295e-05,
      "loss": 1.0715,
      "step": 48410
    },
    {
      "epoch": 2.8475777402043665,
      "grad_norm": 0.3594013750553131,
      "learning_rate": 1.541483343808925e-05,
      "loss": 1.0206,
      "step": 48420
    },
    {
      "epoch": 2.8481658457693158,
      "grad_norm": 0.4073498249053955,
      "learning_rate": 1.535590823381521e-05,
      "loss": 0.986,
      "step": 48430
    },
    {
      "epoch": 2.8487539513342646,
      "grad_norm": 0.3388177454471588,
      "learning_rate": 1.5296983029541168e-05,
      "loss": 0.9538,
      "step": 48440
    },
    {
      "epoch": 2.8493420568992134,
      "grad_norm": 0.3855985701084137,
      "learning_rate": 1.5238057825267126e-05,
      "loss": 1.0279,
      "step": 48450
    },
    {
      "epoch": 2.849930162464162,
      "grad_norm": 0.43547797203063965,
      "learning_rate": 1.5179132620993085e-05,
      "loss": 0.9882,
      "step": 48460
    },
    {
      "epoch": 2.850518268029111,
      "grad_norm": 0.38650837540626526,
      "learning_rate": 1.5120207416719043e-05,
      "loss": 0.9635,
      "step": 48470
    },
    {
      "epoch": 2.8511063735940603,
      "grad_norm": 0.39963167905807495,
      "learning_rate": 1.5061282212445003e-05,
      "loss": 1.0775,
      "step": 48480
    },
    {
      "epoch": 2.851694479159009,
      "grad_norm": 0.41343358159065247,
      "learning_rate": 1.5002357008170959e-05,
      "loss": 0.9466,
      "step": 48490
    },
    {
      "epoch": 2.852282584723958,
      "grad_norm": 0.49563658237457275,
      "learning_rate": 1.4943431803896918e-05,
      "loss": 1.0978,
      "step": 48500
    },
    {
      "epoch": 2.8528706902889067,
      "grad_norm": 0.3647250235080719,
      "learning_rate": 1.4884506599622878e-05,
      "loss": 1.02,
      "step": 48510
    },
    {
      "epoch": 2.8534587958538555,
      "grad_norm": 0.373521625995636,
      "learning_rate": 1.4825581395348836e-05,
      "loss": 1.0902,
      "step": 48520
    },
    {
      "epoch": 2.854046901418805,
      "grad_norm": 0.3782047927379608,
      "learning_rate": 1.4766656191074795e-05,
      "loss": 0.9229,
      "step": 48530
    },
    {
      "epoch": 2.8546350069837536,
      "grad_norm": 0.39024102687835693,
      "learning_rate": 1.4707730986800755e-05,
      "loss": 0.9716,
      "step": 48540
    },
    {
      "epoch": 2.8552231125487024,
      "grad_norm": 0.35803279280662537,
      "learning_rate": 1.464880578252671e-05,
      "loss": 0.9835,
      "step": 48550
    },
    {
      "epoch": 2.8558112181136512,
      "grad_norm": 0.4340921938419342,
      "learning_rate": 1.458988057825267e-05,
      "loss": 0.9292,
      "step": 48560
    },
    {
      "epoch": 2.8563993236786,
      "grad_norm": 0.4431258738040924,
      "learning_rate": 1.4530955373978628e-05,
      "loss": 1.0363,
      "step": 48570
    },
    {
      "epoch": 2.8569874292435493,
      "grad_norm": 0.3916919529438019,
      "learning_rate": 1.4472030169704588e-05,
      "loss": 0.9772,
      "step": 48580
    },
    {
      "epoch": 2.857575534808498,
      "grad_norm": 0.34265071153640747,
      "learning_rate": 1.4413104965430547e-05,
      "loss": 1.1077,
      "step": 48590
    },
    {
      "epoch": 2.858163640373447,
      "grad_norm": 0.3811912536621094,
      "learning_rate": 1.4354179761156503e-05,
      "loss": 1.1565,
      "step": 48600
    },
    {
      "epoch": 2.858751745938396,
      "grad_norm": 0.36245688796043396,
      "learning_rate": 1.4295254556882463e-05,
      "loss": 1.0841,
      "step": 48610
    },
    {
      "epoch": 2.8593398515033446,
      "grad_norm": 0.33364325761795044,
      "learning_rate": 1.4236329352608422e-05,
      "loss": 0.9983,
      "step": 48620
    },
    {
      "epoch": 2.859927957068294,
      "grad_norm": 0.4244768023490906,
      "learning_rate": 1.417740414833438e-05,
      "loss": 1.0033,
      "step": 48630
    },
    {
      "epoch": 2.8605160626332427,
      "grad_norm": 0.37689992785453796,
      "learning_rate": 1.4118478944060338e-05,
      "loss": 0.9453,
      "step": 48640
    },
    {
      "epoch": 2.8611041681981915,
      "grad_norm": 0.4046538472175598,
      "learning_rate": 1.4059553739786296e-05,
      "loss": 1.069,
      "step": 48650
    },
    {
      "epoch": 2.8616922737631407,
      "grad_norm": 0.45325398445129395,
      "learning_rate": 1.4000628535512255e-05,
      "loss": 1.0009,
      "step": 48660
    },
    {
      "epoch": 2.8622803793280895,
      "grad_norm": 0.42108121514320374,
      "learning_rate": 1.3941703331238215e-05,
      "loss": 0.9685,
      "step": 48670
    },
    {
      "epoch": 2.8628684848930384,
      "grad_norm": 0.382062166929245,
      "learning_rate": 1.3882778126964172e-05,
      "loss": 0.921,
      "step": 48680
    },
    {
      "epoch": 2.863456590457987,
      "grad_norm": 0.37965941429138184,
      "learning_rate": 1.382385292269013e-05,
      "loss": 0.9192,
      "step": 48690
    },
    {
      "epoch": 2.864044696022936,
      "grad_norm": 0.37101882696151733,
      "learning_rate": 1.3764927718416088e-05,
      "loss": 0.9399,
      "step": 48700
    },
    {
      "epoch": 2.8646328015878852,
      "grad_norm": 0.42090293765068054,
      "learning_rate": 1.3706002514142048e-05,
      "loss": 1.031,
      "step": 48710
    },
    {
      "epoch": 2.865220907152834,
      "grad_norm": 0.3643009662628174,
      "learning_rate": 1.3647077309868007e-05,
      "loss": 0.9482,
      "step": 48720
    },
    {
      "epoch": 2.865809012717783,
      "grad_norm": 0.4173991084098816,
      "learning_rate": 1.3588152105593965e-05,
      "loss": 1.0179,
      "step": 48730
    },
    {
      "epoch": 2.8663971182827317,
      "grad_norm": 0.35890036821365356,
      "learning_rate": 1.3529226901319923e-05,
      "loss": 1.045,
      "step": 48740
    },
    {
      "epoch": 2.8669852238476805,
      "grad_norm": 0.41052332520484924,
      "learning_rate": 1.3470301697045882e-05,
      "loss": 1.1309,
      "step": 48750
    },
    {
      "epoch": 2.8675733294126298,
      "grad_norm": 0.41416069865226746,
      "learning_rate": 1.341137649277184e-05,
      "loss": 0.9939,
      "step": 48760
    },
    {
      "epoch": 2.8681614349775786,
      "grad_norm": 0.37588074803352356,
      "learning_rate": 1.33524512884978e-05,
      "loss": 1.0541,
      "step": 48770
    },
    {
      "epoch": 2.8687495405425274,
      "grad_norm": 0.42306002974510193,
      "learning_rate": 1.3293526084223757e-05,
      "loss": 0.8904,
      "step": 48780
    },
    {
      "epoch": 2.869337646107476,
      "grad_norm": 0.4473657011985779,
      "learning_rate": 1.3234600879949715e-05,
      "loss": 0.8981,
      "step": 48790
    },
    {
      "epoch": 2.869925751672425,
      "grad_norm": 0.4135979115962982,
      "learning_rate": 1.3175675675675675e-05,
      "loss": 1.0773,
      "step": 48800
    },
    {
      "epoch": 2.8705138572373743,
      "grad_norm": 0.3539586663246155,
      "learning_rate": 1.3116750471401632e-05,
      "loss": 0.9899,
      "step": 48810
    },
    {
      "epoch": 2.871101962802323,
      "grad_norm": 0.36063674092292786,
      "learning_rate": 1.3057825267127592e-05,
      "loss": 1.0285,
      "step": 48820
    },
    {
      "epoch": 2.871690068367272,
      "grad_norm": 0.4451436400413513,
      "learning_rate": 1.2998900062853551e-05,
      "loss": 1.0146,
      "step": 48830
    },
    {
      "epoch": 2.8722781739322207,
      "grad_norm": 0.3333576023578644,
      "learning_rate": 1.2939974858579508e-05,
      "loss": 0.9844,
      "step": 48840
    },
    {
      "epoch": 2.8728662794971696,
      "grad_norm": 0.41474369168281555,
      "learning_rate": 1.2881049654305467e-05,
      "loss": 0.9426,
      "step": 48850
    },
    {
      "epoch": 2.873454385062119,
      "grad_norm": 0.39653703570365906,
      "learning_rate": 1.2822124450031425e-05,
      "loss": 0.9806,
      "step": 48860
    },
    {
      "epoch": 2.8740424906270676,
      "grad_norm": 0.38509005308151245,
      "learning_rate": 1.2763199245757384e-05,
      "loss": 0.9739,
      "step": 48870
    },
    {
      "epoch": 2.8746305961920164,
      "grad_norm": 0.33124229311943054,
      "learning_rate": 1.2704274041483344e-05,
      "loss": 1.041,
      "step": 48880
    },
    {
      "epoch": 2.8752187017569653,
      "grad_norm": 0.41164788603782654,
      "learning_rate": 1.26453488372093e-05,
      "loss": 0.9677,
      "step": 48890
    },
    {
      "epoch": 2.875806807321914,
      "grad_norm": 0.37019073963165283,
      "learning_rate": 1.258642363293526e-05,
      "loss": 1.0013,
      "step": 48900
    },
    {
      "epoch": 2.8763949128868633,
      "grad_norm": 0.3922199606895447,
      "learning_rate": 1.2527498428661217e-05,
      "loss": 1.013,
      "step": 48910
    },
    {
      "epoch": 2.876983018451812,
      "grad_norm": 0.3634989857673645,
      "learning_rate": 1.2468573224387177e-05,
      "loss": 0.8876,
      "step": 48920
    },
    {
      "epoch": 2.877571124016761,
      "grad_norm": 0.371548056602478,
      "learning_rate": 1.2409648020113136e-05,
      "loss": 0.9888,
      "step": 48930
    },
    {
      "epoch": 2.8781592295817098,
      "grad_norm": 0.443258136510849,
      "learning_rate": 1.2350722815839092e-05,
      "loss": 1.0739,
      "step": 48940
    },
    {
      "epoch": 2.8787473351466586,
      "grad_norm": 0.4022568464279175,
      "learning_rate": 1.2291797611565052e-05,
      "loss": 0.946,
      "step": 48950
    },
    {
      "epoch": 2.879335440711608,
      "grad_norm": 0.3550873100757599,
      "learning_rate": 1.2232872407291011e-05,
      "loss": 0.9879,
      "step": 48960
    },
    {
      "epoch": 2.8799235462765567,
      "grad_norm": 0.43624842166900635,
      "learning_rate": 1.217394720301697e-05,
      "loss": 1.0298,
      "step": 48970
    },
    {
      "epoch": 2.8805116518415055,
      "grad_norm": 0.3979359567165375,
      "learning_rate": 1.2115021998742929e-05,
      "loss": 0.912,
      "step": 48980
    },
    {
      "epoch": 2.8810997574064543,
      "grad_norm": 0.38983577489852905,
      "learning_rate": 1.2056096794468885e-05,
      "loss": 0.9451,
      "step": 48990
    },
    {
      "epoch": 2.881687862971403,
      "grad_norm": 0.38725289702415466,
      "learning_rate": 1.1997171590194844e-05,
      "loss": 1.0205,
      "step": 49000
    },
    {
      "epoch": 2.8822759685363524,
      "grad_norm": 0.3521783649921417,
      "learning_rate": 1.1938246385920804e-05,
      "loss": 0.9602,
      "step": 49010
    },
    {
      "epoch": 2.882864074101301,
      "grad_norm": 0.363095223903656,
      "learning_rate": 1.1879321181646762e-05,
      "loss": 0.9806,
      "step": 49020
    },
    {
      "epoch": 2.88345217966625,
      "grad_norm": 0.3953799903392792,
      "learning_rate": 1.1820395977372721e-05,
      "loss": 1.0266,
      "step": 49030
    },
    {
      "epoch": 2.8840402852311993,
      "grad_norm": 0.39337068796157837,
      "learning_rate": 1.1761470773098677e-05,
      "loss": 0.974,
      "step": 49040
    },
    {
      "epoch": 2.8846283907961476,
      "grad_norm": 0.3806857466697693,
      "learning_rate": 1.1702545568824637e-05,
      "loss": 0.9843,
      "step": 49050
    },
    {
      "epoch": 2.885216496361097,
      "grad_norm": 0.39025557041168213,
      "learning_rate": 1.1643620364550596e-05,
      "loss": 1.0852,
      "step": 49060
    },
    {
      "epoch": 2.8858046019260457,
      "grad_norm": 0.37616589665412903,
      "learning_rate": 1.1584695160276554e-05,
      "loss": 0.9946,
      "step": 49070
    },
    {
      "epoch": 2.8863927074909945,
      "grad_norm": 0.37422746419906616,
      "learning_rate": 1.1525769956002514e-05,
      "loss": 0.9887,
      "step": 49080
    },
    {
      "epoch": 2.886980813055944,
      "grad_norm": 0.4180976152420044,
      "learning_rate": 1.1466844751728473e-05,
      "loss": 0.9706,
      "step": 49090
    },
    {
      "epoch": 2.8875689186208926,
      "grad_norm": 0.3713255226612091,
      "learning_rate": 1.140791954745443e-05,
      "loss": 1.0551,
      "step": 49100
    },
    {
      "epoch": 2.8881570241858414,
      "grad_norm": 0.3673402965068817,
      "learning_rate": 1.1348994343180389e-05,
      "loss": 1.0141,
      "step": 49110
    },
    {
      "epoch": 2.8887451297507902,
      "grad_norm": 0.4522891938686371,
      "learning_rate": 1.1290069138906347e-05,
      "loss": 0.9691,
      "step": 49120
    },
    {
      "epoch": 2.889333235315739,
      "grad_norm": 0.4013531506061554,
      "learning_rate": 1.1231143934632306e-05,
      "loss": 1.1057,
      "step": 49130
    },
    {
      "epoch": 2.8899213408806883,
      "grad_norm": 0.342515230178833,
      "learning_rate": 1.1172218730358266e-05,
      "loss": 1.0328,
      "step": 49140
    },
    {
      "epoch": 2.890509446445637,
      "grad_norm": 0.3989367187023163,
      "learning_rate": 1.1113293526084222e-05,
      "loss": 1.0599,
      "step": 49150
    },
    {
      "epoch": 2.891097552010586,
      "grad_norm": 0.38557377457618713,
      "learning_rate": 1.1054368321810181e-05,
      "loss": 0.9566,
      "step": 49160
    },
    {
      "epoch": 2.8916856575755348,
      "grad_norm": 0.37526410818099976,
      "learning_rate": 1.099544311753614e-05,
      "loss": 0.9737,
      "step": 49170
    },
    {
      "epoch": 2.8922737631404836,
      "grad_norm": 0.4345608353614807,
      "learning_rate": 1.0936517913262099e-05,
      "loss": 1.0468,
      "step": 49180
    },
    {
      "epoch": 2.892861868705433,
      "grad_norm": 0.37889939546585083,
      "learning_rate": 1.0877592708988058e-05,
      "loss": 1.0326,
      "step": 49190
    },
    {
      "epoch": 2.8934499742703816,
      "grad_norm": 0.3840213418006897,
      "learning_rate": 1.0818667504714014e-05,
      "loss": 1.0222,
      "step": 49200
    },
    {
      "epoch": 2.8940380798353305,
      "grad_norm": 0.3677685260772705,
      "learning_rate": 1.0759742300439974e-05,
      "loss": 1.1094,
      "step": 49210
    },
    {
      "epoch": 2.8946261854002793,
      "grad_norm": 0.35397985577583313,
      "learning_rate": 1.0700817096165933e-05,
      "loss": 0.9911,
      "step": 49220
    },
    {
      "epoch": 2.895214290965228,
      "grad_norm": 0.3721916675567627,
      "learning_rate": 1.0641891891891891e-05,
      "loss": 0.9699,
      "step": 49230
    },
    {
      "epoch": 2.8958023965301773,
      "grad_norm": 0.4030137062072754,
      "learning_rate": 1.058296668761785e-05,
      "loss": 0.9112,
      "step": 49240
    },
    {
      "epoch": 2.896390502095126,
      "grad_norm": 0.355416864156723,
      "learning_rate": 1.0524041483343807e-05,
      "loss": 0.935,
      "step": 49250
    },
    {
      "epoch": 2.896978607660075,
      "grad_norm": 0.3738442361354828,
      "learning_rate": 1.0465116279069766e-05,
      "loss": 0.9794,
      "step": 49260
    },
    {
      "epoch": 2.897566713225024,
      "grad_norm": 0.3751070201396942,
      "learning_rate": 1.0406191074795726e-05,
      "loss": 0.9558,
      "step": 49270
    },
    {
      "epoch": 2.8981548187899726,
      "grad_norm": 0.3384539783000946,
      "learning_rate": 1.0347265870521683e-05,
      "loss": 0.9022,
      "step": 49280
    },
    {
      "epoch": 2.898742924354922,
      "grad_norm": 0.37618494033813477,
      "learning_rate": 1.0288340666247643e-05,
      "loss": 0.9282,
      "step": 49290
    },
    {
      "epoch": 2.8993310299198707,
      "grad_norm": 0.39640384912490845,
      "learning_rate": 1.02294154619736e-05,
      "loss": 1.0496,
      "step": 49300
    },
    {
      "epoch": 2.8999191354848195,
      "grad_norm": 0.3702871799468994,
      "learning_rate": 1.0170490257699559e-05,
      "loss": 0.9562,
      "step": 49310
    },
    {
      "epoch": 2.9005072410497683,
      "grad_norm": 0.3819595277309418,
      "learning_rate": 1.0111565053425518e-05,
      "loss": 1.0421,
      "step": 49320
    },
    {
      "epoch": 2.901095346614717,
      "grad_norm": 0.4373311698436737,
      "learning_rate": 1.0052639849151476e-05,
      "loss": 1.0481,
      "step": 49330
    },
    {
      "epoch": 2.9016834521796664,
      "grad_norm": 0.329929918050766,
      "learning_rate": 9.993714644877435e-06,
      "loss": 1.078,
      "step": 49340
    },
    {
      "epoch": 2.902271557744615,
      "grad_norm": 0.40806761384010315,
      "learning_rate": 9.934789440603393e-06,
      "loss": 0.9679,
      "step": 49350
    },
    {
      "epoch": 2.902859663309564,
      "grad_norm": 0.40088069438934326,
      "learning_rate": 9.875864236329351e-06,
      "loss": 1.0276,
      "step": 49360
    },
    {
      "epoch": 2.903447768874513,
      "grad_norm": 0.4007278084754944,
      "learning_rate": 9.81693903205531e-06,
      "loss": 0.9737,
      "step": 49370
    },
    {
      "epoch": 2.9040358744394617,
      "grad_norm": 0.38848114013671875,
      "learning_rate": 9.758013827781268e-06,
      "loss": 1.0473,
      "step": 49380
    },
    {
      "epoch": 2.904623980004411,
      "grad_norm": 0.3720240890979767,
      "learning_rate": 9.699088623507228e-06,
      "loss": 1.0472,
      "step": 49390
    },
    {
      "epoch": 2.9052120855693597,
      "grad_norm": 0.4455593526363373,
      "learning_rate": 9.640163419233186e-06,
      "loss": 1.0294,
      "step": 49400
    },
    {
      "epoch": 2.9058001911343085,
      "grad_norm": 0.3762498199939728,
      "learning_rate": 9.581238214959143e-06,
      "loss": 1.0044,
      "step": 49410
    },
    {
      "epoch": 2.9063882966992574,
      "grad_norm": 0.4568517208099365,
      "learning_rate": 9.522313010685103e-06,
      "loss": 1.0165,
      "step": 49420
    },
    {
      "epoch": 2.906976402264206,
      "grad_norm": 0.49189767241477966,
      "learning_rate": 9.463387806411062e-06,
      "loss": 1.0085,
      "step": 49430
    },
    {
      "epoch": 2.9075645078291554,
      "grad_norm": 0.39437344670295715,
      "learning_rate": 9.40446260213702e-06,
      "loss": 0.945,
      "step": 49440
    },
    {
      "epoch": 2.9081526133941042,
      "grad_norm": 0.3962743878364563,
      "learning_rate": 9.345537397862978e-06,
      "loss": 1.0986,
      "step": 49450
    },
    {
      "epoch": 2.908740718959053,
      "grad_norm": 0.39813047647476196,
      "learning_rate": 9.286612193588938e-06,
      "loss": 0.9338,
      "step": 49460
    },
    {
      "epoch": 2.9093288245240023,
      "grad_norm": 0.5050851106643677,
      "learning_rate": 9.227686989314895e-06,
      "loss": 0.9478,
      "step": 49470
    },
    {
      "epoch": 2.9099169300889507,
      "grad_norm": 0.44293344020843506,
      "learning_rate": 9.168761785040853e-06,
      "loss": 0.9838,
      "step": 49480
    },
    {
      "epoch": 2.9105050356539,
      "grad_norm": 0.42332565784454346,
      "learning_rate": 9.109836580766813e-06,
      "loss": 0.9074,
      "step": 49490
    },
    {
      "epoch": 2.9110931412188488,
      "grad_norm": 0.3634922206401825,
      "learning_rate": 9.05091137649277e-06,
      "loss": 0.9679,
      "step": 49500
    },
    {
      "epoch": 2.9116812467837976,
      "grad_norm": 0.33243128657341003,
      "learning_rate": 8.99198617221873e-06,
      "loss": 1.0785,
      "step": 49510
    },
    {
      "epoch": 2.912269352348747,
      "grad_norm": 0.38046813011169434,
      "learning_rate": 8.933060967944688e-06,
      "loss": 1.0089,
      "step": 49520
    },
    {
      "epoch": 2.9128574579136957,
      "grad_norm": 0.4275451898574829,
      "learning_rate": 8.874135763670647e-06,
      "loss": 0.9866,
      "step": 49530
    },
    {
      "epoch": 2.9134455634786445,
      "grad_norm": 0.34739717841148376,
      "learning_rate": 8.815210559396605e-06,
      "loss": 1.0268,
      "step": 49540
    },
    {
      "epoch": 2.9140336690435933,
      "grad_norm": 0.3681030571460724,
      "learning_rate": 8.756285355122563e-06,
      "loss": 0.972,
      "step": 49550
    },
    {
      "epoch": 2.914621774608542,
      "grad_norm": 0.4603990614414215,
      "learning_rate": 8.697360150848522e-06,
      "loss": 1.0206,
      "step": 49560
    },
    {
      "epoch": 2.9152098801734914,
      "grad_norm": 0.374126136302948,
      "learning_rate": 8.63843494657448e-06,
      "loss": 1.0069,
      "step": 49570
    },
    {
      "epoch": 2.91579798573844,
      "grad_norm": 0.4242999255657196,
      "learning_rate": 8.57950974230044e-06,
      "loss": 1.1259,
      "step": 49580
    },
    {
      "epoch": 2.916386091303389,
      "grad_norm": 0.41324952244758606,
      "learning_rate": 8.520584538026398e-06,
      "loss": 0.9111,
      "step": 49590
    },
    {
      "epoch": 2.916974196868338,
      "grad_norm": 0.34333908557891846,
      "learning_rate": 8.461659333752355e-06,
      "loss": 0.9842,
      "step": 49600
    },
    {
      "epoch": 2.9175623024332866,
      "grad_norm": 0.36990776658058167,
      "learning_rate": 8.402734129478315e-06,
      "loss": 1.0419,
      "step": 49610
    },
    {
      "epoch": 2.918150407998236,
      "grad_norm": 0.3857669532299042,
      "learning_rate": 8.343808925204274e-06,
      "loss": 0.9413,
      "step": 49620
    },
    {
      "epoch": 2.9187385135631847,
      "grad_norm": 0.37732943892478943,
      "learning_rate": 8.284883720930232e-06,
      "loss": 0.9618,
      "step": 49630
    },
    {
      "epoch": 2.9193266191281335,
      "grad_norm": 0.3769277036190033,
      "learning_rate": 8.22595851665619e-06,
      "loss": 0.982,
      "step": 49640
    },
    {
      "epoch": 2.9199147246930823,
      "grad_norm": 0.3711813688278198,
      "learning_rate": 8.167033312382148e-06,
      "loss": 1.045,
      "step": 49650
    },
    {
      "epoch": 2.920502830258031,
      "grad_norm": 0.36805829405784607,
      "learning_rate": 8.108108108108107e-06,
      "loss": 1.007,
      "step": 49660
    },
    {
      "epoch": 2.9210909358229804,
      "grad_norm": 0.3968450427055359,
      "learning_rate": 8.049182903834067e-06,
      "loss": 1.0668,
      "step": 49670
    },
    {
      "epoch": 2.921679041387929,
      "grad_norm": 0.35923701524734497,
      "learning_rate": 7.990257699560025e-06,
      "loss": 1.0919,
      "step": 49680
    },
    {
      "epoch": 2.922267146952878,
      "grad_norm": 0.40728795528411865,
      "learning_rate": 7.931332495285982e-06,
      "loss": 1.0908,
      "step": 49690
    },
    {
      "epoch": 2.922855252517827,
      "grad_norm": 0.40743961930274963,
      "learning_rate": 7.872407291011942e-06,
      "loss": 1.0886,
      "step": 49700
    },
    {
      "epoch": 2.9234433580827757,
      "grad_norm": 0.36389386653900146,
      "learning_rate": 7.8134820867379e-06,
      "loss": 1.0365,
      "step": 49710
    },
    {
      "epoch": 2.924031463647725,
      "grad_norm": 0.39040088653564453,
      "learning_rate": 7.75455688246386e-06,
      "loss": 1.112,
      "step": 49720
    },
    {
      "epoch": 2.9246195692126737,
      "grad_norm": 0.3679793179035187,
      "learning_rate": 7.695631678189817e-06,
      "loss": 0.9662,
      "step": 49730
    },
    {
      "epoch": 2.9252076747776226,
      "grad_norm": 0.3541778028011322,
      "learning_rate": 7.636706473915775e-06,
      "loss": 0.978,
      "step": 49740
    },
    {
      "epoch": 2.9257957803425714,
      "grad_norm": 0.38286399841308594,
      "learning_rate": 7.577781269641734e-06,
      "loss": 0.9453,
      "step": 49750
    },
    {
      "epoch": 2.92638388590752,
      "grad_norm": 0.42595335841178894,
      "learning_rate": 7.518856065367693e-06,
      "loss": 0.984,
      "step": 49760
    },
    {
      "epoch": 2.9269719914724694,
      "grad_norm": 0.39675813913345337,
      "learning_rate": 7.459930861093651e-06,
      "loss": 1.0078,
      "step": 49770
    },
    {
      "epoch": 2.9275600970374183,
      "grad_norm": 0.3614758849143982,
      "learning_rate": 7.4010056568196095e-06,
      "loss": 1.0389,
      "step": 49780
    },
    {
      "epoch": 2.928148202602367,
      "grad_norm": 0.3574434220790863,
      "learning_rate": 7.342080452545568e-06,
      "loss": 1.0682,
      "step": 49790
    },
    {
      "epoch": 2.928736308167316,
      "grad_norm": 0.3672231435775757,
      "learning_rate": 7.283155248271527e-06,
      "loss": 1.0376,
      "step": 49800
    },
    {
      "epoch": 2.9293244137322647,
      "grad_norm": 0.3256690204143524,
      "learning_rate": 7.2242300439974855e-06,
      "loss": 0.9761,
      "step": 49810
    },
    {
      "epoch": 2.929912519297214,
      "grad_norm": 0.4077570140361786,
      "learning_rate": 7.165304839723443e-06,
      "loss": 1.006,
      "step": 49820
    },
    {
      "epoch": 2.930500624862163,
      "grad_norm": 0.3560282289981842,
      "learning_rate": 7.106379635449403e-06,
      "loss": 1.0415,
      "step": 49830
    },
    {
      "epoch": 2.9310887304271116,
      "grad_norm": 0.38265594840049744,
      "learning_rate": 7.047454431175361e-06,
      "loss": 0.9581,
      "step": 49840
    },
    {
      "epoch": 2.9316768359920604,
      "grad_norm": 0.37336042523384094,
      "learning_rate": 6.988529226901319e-06,
      "loss": 1.0996,
      "step": 49850
    },
    {
      "epoch": 2.9322649415570092,
      "grad_norm": 0.3690848648548126,
      "learning_rate": 6.929604022627278e-06,
      "loss": 1.037,
      "step": 49860
    },
    {
      "epoch": 2.9328530471219585,
      "grad_norm": 0.48350510001182556,
      "learning_rate": 6.870678818353237e-06,
      "loss": 1.0111,
      "step": 49870
    },
    {
      "epoch": 2.9334411526869073,
      "grad_norm": 0.4028020203113556,
      "learning_rate": 6.811753614079195e-06,
      "loss": 1.0023,
      "step": 49880
    },
    {
      "epoch": 2.934029258251856,
      "grad_norm": 0.42714428901672363,
      "learning_rate": 6.752828409805153e-06,
      "loss": 0.985,
      "step": 49890
    },
    {
      "epoch": 2.9346173638168054,
      "grad_norm": 0.3649967312812805,
      "learning_rate": 6.693903205531112e-06,
      "loss": 1.0369,
      "step": 49900
    },
    {
      "epoch": 2.9352054693817538,
      "grad_norm": 0.36267295479774475,
      "learning_rate": 6.6349780012570696e-06,
      "loss": 0.9933,
      "step": 49910
    },
    {
      "epoch": 2.935793574946703,
      "grad_norm": 0.38023263216018677,
      "learning_rate": 6.576052796983029e-06,
      "loss": 1.0272,
      "step": 49920
    },
    {
      "epoch": 2.936381680511652,
      "grad_norm": 0.38368093967437744,
      "learning_rate": 6.517127592708988e-06,
      "loss": 1.0782,
      "step": 49930
    },
    {
      "epoch": 2.9369697860766006,
      "grad_norm": 0.3998595178127289,
      "learning_rate": 6.4582023884349455e-06,
      "loss": 0.9849,
      "step": 49940
    },
    {
      "epoch": 2.93755789164155,
      "grad_norm": 0.44030749797821045,
      "learning_rate": 6.399277184160904e-06,
      "loss": 0.9266,
      "step": 49950
    },
    {
      "epoch": 2.9381459972064987,
      "grad_norm": 0.3641578257083893,
      "learning_rate": 6.340351979886864e-06,
      "loss": 1.0013,
      "step": 49960
    },
    {
      "epoch": 2.9387341027714475,
      "grad_norm": 0.4016685485839844,
      "learning_rate": 6.2814267756128215e-06,
      "loss": 0.9984,
      "step": 49970
    },
    {
      "epoch": 2.9393222083363963,
      "grad_norm": 0.4273223578929901,
      "learning_rate": 6.22250157133878e-06,
      "loss": 0.9363,
      "step": 49980
    },
    {
      "epoch": 2.939910313901345,
      "grad_norm": 0.36726224422454834,
      "learning_rate": 6.163576367064738e-06,
      "loss": 0.9965,
      "step": 49990
    },
    {
      "epoch": 2.9404984194662944,
      "grad_norm": 0.3710884749889374,
      "learning_rate": 6.1046511627906975e-06,
      "loss": 0.9683,
      "step": 50000
    },
    {
      "epoch": 2.9410865250312432,
      "grad_norm": 0.37557023763656616,
      "learning_rate": 6.045725958516656e-06,
      "loss": 1.0238,
      "step": 50010
    },
    {
      "epoch": 2.941674630596192,
      "grad_norm": 0.42448800802230835,
      "learning_rate": 5.986800754242614e-06,
      "loss": 1.0208,
      "step": 50020
    },
    {
      "epoch": 2.942262736161141,
      "grad_norm": 0.42958760261535645,
      "learning_rate": 5.927875549968573e-06,
      "loss": 1.0263,
      "step": 50030
    },
    {
      "epoch": 2.9428508417260897,
      "grad_norm": 0.39658835530281067,
      "learning_rate": 5.868950345694532e-06,
      "loss": 0.9973,
      "step": 50040
    },
    {
      "epoch": 2.943438947291039,
      "grad_norm": 0.3706061840057373,
      "learning_rate": 5.81002514142049e-06,
      "loss": 0.9677,
      "step": 50050
    },
    {
      "epoch": 2.9440270528559878,
      "grad_norm": 0.39515984058380127,
      "learning_rate": 5.7510999371464486e-06,
      "loss": 1.0159,
      "step": 50060
    },
    {
      "epoch": 2.9446151584209366,
      "grad_norm": 0.38996633887290955,
      "learning_rate": 5.692174732872406e-06,
      "loss": 0.9633,
      "step": 50070
    },
    {
      "epoch": 2.9452032639858854,
      "grad_norm": 0.3750986158847809,
      "learning_rate": 5.633249528598365e-06,
      "loss": 1.0148,
      "step": 50080
    },
    {
      "epoch": 2.945791369550834,
      "grad_norm": 0.3668298125267029,
      "learning_rate": 5.5743243243243245e-06,
      "loss": 1.0409,
      "step": 50090
    },
    {
      "epoch": 2.9463794751157835,
      "grad_norm": 0.35951289534568787,
      "learning_rate": 5.521291640477686e-06,
      "loss": 1.0008,
      "step": 50100
    },
    {
      "epoch": 2.9469675806807323,
      "grad_norm": 0.41757526993751526,
      "learning_rate": 5.4623664362036456e-06,
      "loss": 1.0406,
      "step": 50110
    },
    {
      "epoch": 2.947555686245681,
      "grad_norm": 0.4612702429294586,
      "learning_rate": 5.403441231929603e-06,
      "loss": 0.9435,
      "step": 50120
    },
    {
      "epoch": 2.94814379181063,
      "grad_norm": 0.3615640103816986,
      "learning_rate": 5.344516027655562e-06,
      "loss": 0.9731,
      "step": 50130
    },
    {
      "epoch": 2.9487318973755787,
      "grad_norm": 0.37771937251091003,
      "learning_rate": 5.28559082338152e-06,
      "loss": 0.9542,
      "step": 50140
    },
    {
      "epoch": 2.949320002940528,
      "grad_norm": 0.37959712743759155,
      "learning_rate": 5.226665619107479e-06,
      "loss": 1.0334,
      "step": 50150
    },
    {
      "epoch": 2.949908108505477,
      "grad_norm": 0.3471687436103821,
      "learning_rate": 5.167740414833438e-06,
      "loss": 1.0049,
      "step": 50160
    },
    {
      "epoch": 2.9504962140704256,
      "grad_norm": 0.41934308409690857,
      "learning_rate": 5.108815210559396e-06,
      "loss": 1.0006,
      "step": 50170
    },
    {
      "epoch": 2.9510843196353744,
      "grad_norm": 0.399298757314682,
      "learning_rate": 5.0498900062853545e-06,
      "loss": 0.9557,
      "step": 50180
    },
    {
      "epoch": 2.9516724252003232,
      "grad_norm": 0.355349600315094,
      "learning_rate": 4.990964802011314e-06,
      "loss": 1.0384,
      "step": 50190
    },
    {
      "epoch": 2.9522605307652725,
      "grad_norm": 0.40909335017204285,
      "learning_rate": 4.932039597737272e-06,
      "loss": 1.064,
      "step": 50200
    },
    {
      "epoch": 2.9528486363302213,
      "grad_norm": 0.36080875992774963,
      "learning_rate": 4.8731143934632305e-06,
      "loss": 1.0516,
      "step": 50210
    },
    {
      "epoch": 2.95343674189517,
      "grad_norm": 0.38989633321762085,
      "learning_rate": 4.814189189189188e-06,
      "loss": 1.0125,
      "step": 50220
    },
    {
      "epoch": 2.954024847460119,
      "grad_norm": 0.3956323266029358,
      "learning_rate": 4.755263984915147e-06,
      "loss": 1.0165,
      "step": 50230
    },
    {
      "epoch": 2.9546129530250678,
      "grad_norm": 0.3471842408180237,
      "learning_rate": 4.6963387806411064e-06,
      "loss": 0.9679,
      "step": 50240
    },
    {
      "epoch": 2.955201058590017,
      "grad_norm": 0.3977171778678894,
      "learning_rate": 4.637413576367064e-06,
      "loss": 1.0044,
      "step": 50250
    },
    {
      "epoch": 2.955789164154966,
      "grad_norm": 0.41389235854148865,
      "learning_rate": 4.578488372093023e-06,
      "loss": 1.0933,
      "step": 50260
    },
    {
      "epoch": 2.9563772697199147,
      "grad_norm": 0.4305642545223236,
      "learning_rate": 4.5195631678189816e-06,
      "loss": 1.0309,
      "step": 50270
    },
    {
      "epoch": 2.9569653752848635,
      "grad_norm": 0.4326542615890503,
      "learning_rate": 4.460637963544939e-06,
      "loss": 1.0192,
      "step": 50280
    },
    {
      "epoch": 2.9575534808498123,
      "grad_norm": 0.3862781524658203,
      "learning_rate": 4.401712759270899e-06,
      "loss": 1.0258,
      "step": 50290
    },
    {
      "epoch": 2.9581415864147615,
      "grad_norm": 0.4385744035243988,
      "learning_rate": 4.342787554996857e-06,
      "loss": 0.9444,
      "step": 50300
    },
    {
      "epoch": 2.9587296919797104,
      "grad_norm": 0.35541146993637085,
      "learning_rate": 4.283862350722815e-06,
      "loss": 0.927,
      "step": 50310
    },
    {
      "epoch": 2.959317797544659,
      "grad_norm": 0.3897078037261963,
      "learning_rate": 4.224937146448774e-06,
      "loss": 0.954,
      "step": 50320
    },
    {
      "epoch": 2.9599059031096084,
      "grad_norm": 0.3860766291618347,
      "learning_rate": 4.166011942174733e-06,
      "loss": 1.0381,
      "step": 50330
    },
    {
      "epoch": 2.960494008674557,
      "grad_norm": 0.3713478744029999,
      "learning_rate": 4.107086737900691e-06,
      "loss": 0.9565,
      "step": 50340
    },
    {
      "epoch": 2.961082114239506,
      "grad_norm": 0.36912646889686584,
      "learning_rate": 4.04816153362665e-06,
      "loss": 1.0829,
      "step": 50350
    },
    {
      "epoch": 2.961670219804455,
      "grad_norm": 0.37388232350349426,
      "learning_rate": 3.989236329352608e-06,
      "loss": 0.9585,
      "step": 50360
    },
    {
      "epoch": 2.9622583253694037,
      "grad_norm": 0.4049576222896576,
      "learning_rate": 3.9303111250785664e-06,
      "loss": 1.0357,
      "step": 50370
    },
    {
      "epoch": 2.962846430934353,
      "grad_norm": 0.3392370641231537,
      "learning_rate": 3.871385920804525e-06,
      "loss": 1.037,
      "step": 50380
    },
    {
      "epoch": 2.9634345364993018,
      "grad_norm": 0.45332497358322144,
      "learning_rate": 3.8124607165304837e-06,
      "loss": 0.9784,
      "step": 50390
    },
    {
      "epoch": 2.9640226420642506,
      "grad_norm": 0.4289717972278595,
      "learning_rate": 3.753535512256442e-06,
      "loss": 1.0049,
      "step": 50400
    },
    {
      "epoch": 2.9646107476291994,
      "grad_norm": 0.36792436242103577,
      "learning_rate": 3.6946103079824006e-06,
      "loss": 1.0952,
      "step": 50410
    },
    {
      "epoch": 2.965198853194148,
      "grad_norm": 0.3705061972141266,
      "learning_rate": 3.6356851037083593e-06,
      "loss": 1.007,
      "step": 50420
    },
    {
      "epoch": 2.9657869587590975,
      "grad_norm": 0.3939664959907532,
      "learning_rate": 3.5767598994343175e-06,
      "loss": 0.891,
      "step": 50430
    },
    {
      "epoch": 2.9663750643240463,
      "grad_norm": 0.3746770918369293,
      "learning_rate": 3.517834695160276e-06,
      "loss": 1.0615,
      "step": 50440
    },
    {
      "epoch": 2.966963169888995,
      "grad_norm": 0.38579288125038147,
      "learning_rate": 3.4589094908862344e-06,
      "loss": 1.0336,
      "step": 50450
    },
    {
      "epoch": 2.967551275453944,
      "grad_norm": 0.3409557044506073,
      "learning_rate": 3.3999842866121935e-06,
      "loss": 0.9604,
      "step": 50460
    },
    {
      "epoch": 2.9681393810188927,
      "grad_norm": 0.4002021849155426,
      "learning_rate": 3.3410590823381517e-06,
      "loss": 1.0836,
      "step": 50470
    },
    {
      "epoch": 2.968727486583842,
      "grad_norm": 0.3569891154766083,
      "learning_rate": 3.2821338780641104e-06,
      "loss": 0.9684,
      "step": 50480
    },
    {
      "epoch": 2.969315592148791,
      "grad_norm": 0.40275076031684875,
      "learning_rate": 3.2232086737900686e-06,
      "loss": 1.0134,
      "step": 50490
    },
    {
      "epoch": 2.9699036977137396,
      "grad_norm": 0.35270825028419495,
      "learning_rate": 3.1642834695160277e-06,
      "loss": 0.9821,
      "step": 50500
    },
    {
      "epoch": 2.9704918032786884,
      "grad_norm": 0.41343680024147034,
      "learning_rate": 3.105358265241986e-06,
      "loss": 1.0007,
      "step": 50510
    },
    {
      "epoch": 2.9710799088436373,
      "grad_norm": 0.3764214813709259,
      "learning_rate": 3.0464330609679446e-06,
      "loss": 0.9872,
      "step": 50520
    },
    {
      "epoch": 2.9716680144085865,
      "grad_norm": 0.3314213752746582,
      "learning_rate": 2.987507856693903e-06,
      "loss": 1.0108,
      "step": 50530
    },
    {
      "epoch": 2.9722561199735353,
      "grad_norm": 0.42992377281188965,
      "learning_rate": 2.9285826524198615e-06,
      "loss": 0.9663,
      "step": 50540
    },
    {
      "epoch": 2.972844225538484,
      "grad_norm": 0.3712724447250366,
      "learning_rate": 2.86965744814582e-06,
      "loss": 0.9809,
      "step": 50550
    },
    {
      "epoch": 2.973432331103433,
      "grad_norm": 0.41593316197395325,
      "learning_rate": 2.810732243871779e-06,
      "loss": 0.9726,
      "step": 50560
    },
    {
      "epoch": 2.974020436668382,
      "grad_norm": 0.3917485773563385,
      "learning_rate": 2.751807039597737e-06,
      "loss": 1.0093,
      "step": 50570
    },
    {
      "epoch": 2.974608542233331,
      "grad_norm": 0.3972998857498169,
      "learning_rate": 2.6928818353236953e-06,
      "loss": 0.9949,
      "step": 50580
    },
    {
      "epoch": 2.97519664779828,
      "grad_norm": 0.400642454624176,
      "learning_rate": 2.633956631049654e-06,
      "loss": 1.022,
      "step": 50590
    },
    {
      "epoch": 2.9757847533632287,
      "grad_norm": 0.3844200074672699,
      "learning_rate": 2.5750314267756126e-06,
      "loss": 1.2177,
      "step": 50600
    },
    {
      "epoch": 2.9763728589281775,
      "grad_norm": 0.42881426215171814,
      "learning_rate": 2.5161062225015713e-06,
      "loss": 0.9832,
      "step": 50610
    },
    {
      "epoch": 2.9769609644931263,
      "grad_norm": 0.351441353559494,
      "learning_rate": 2.4571810182275295e-06,
      "loss": 1.1235,
      "step": 50620
    },
    {
      "epoch": 2.9775490700580756,
      "grad_norm": 0.404025137424469,
      "learning_rate": 2.398255813953488e-06,
      "loss": 1.0477,
      "step": 50630
    },
    {
      "epoch": 2.9781371756230244,
      "grad_norm": 0.3869589865207672,
      "learning_rate": 2.339330609679447e-06,
      "loss": 1.0091,
      "step": 50640
    },
    {
      "epoch": 2.978725281187973,
      "grad_norm": 0.3813953697681427,
      "learning_rate": 2.2804054054054055e-06,
      "loss": 1.0709,
      "step": 50650
    },
    {
      "epoch": 2.979313386752922,
      "grad_norm": 0.45145368576049805,
      "learning_rate": 2.2214802011313637e-06,
      "loss": 1.0896,
      "step": 50660
    },
    {
      "epoch": 2.979901492317871,
      "grad_norm": 0.3677501082420349,
      "learning_rate": 2.1625549968573223e-06,
      "loss": 1.0389,
      "step": 50670
    },
    {
      "epoch": 2.98048959788282,
      "grad_norm": 0.4022984206676483,
      "learning_rate": 2.1036297925832806e-06,
      "loss": 1.0067,
      "step": 50680
    },
    {
      "epoch": 2.981077703447769,
      "grad_norm": 0.43501541018486023,
      "learning_rate": 2.0447045883092392e-06,
      "loss": 0.9627,
      "step": 50690
    },
    {
      "epoch": 2.9816658090127177,
      "grad_norm": 0.3730117678642273,
      "learning_rate": 1.985779384035198e-06,
      "loss": 1.0565,
      "step": 50700
    },
    {
      "epoch": 2.9822539145776665,
      "grad_norm": 0.41720083355903625,
      "learning_rate": 1.926854179761156e-06,
      "loss": 1.0338,
      "step": 50710
    },
    {
      "epoch": 2.9828420201426153,
      "grad_norm": 0.4209699034690857,
      "learning_rate": 1.8679289754871148e-06,
      "loss": 0.8297,
      "step": 50720
    },
    {
      "epoch": 2.9834301257075646,
      "grad_norm": 0.43156084418296814,
      "learning_rate": 1.8090037712130734e-06,
      "loss": 1.022,
      "step": 50730
    },
    {
      "epoch": 2.9840182312725134,
      "grad_norm": 0.4271991550922394,
      "learning_rate": 1.7500785669390319e-06,
      "loss": 0.9002,
      "step": 50740
    },
    {
      "epoch": 2.9846063368374622,
      "grad_norm": 0.388077050447464,
      "learning_rate": 1.6911533626649903e-06,
      "loss": 0.9121,
      "step": 50750
    },
    {
      "epoch": 2.9851944424024115,
      "grad_norm": 0.4193415939807892,
      "learning_rate": 1.632228158390949e-06,
      "loss": 0.937,
      "step": 50760
    },
    {
      "epoch": 2.98578254796736,
      "grad_norm": 0.36437782645225525,
      "learning_rate": 1.5733029541169074e-06,
      "loss": 0.9469,
      "step": 50770
    },
    {
      "epoch": 2.986370653532309,
      "grad_norm": 0.42073994874954224,
      "learning_rate": 1.514377749842866e-06,
      "loss": 1.0442,
      "step": 50780
    },
    {
      "epoch": 2.986958759097258,
      "grad_norm": 0.4069031774997711,
      "learning_rate": 1.4554525455688245e-06,
      "loss": 0.9112,
      "step": 50790
    },
    {
      "epoch": 2.9875468646622068,
      "grad_norm": 0.38710689544677734,
      "learning_rate": 1.3965273412947832e-06,
      "loss": 1.0392,
      "step": 50800
    },
    {
      "epoch": 2.988134970227156,
      "grad_norm": 0.39422866702079773,
      "learning_rate": 1.3376021370207416e-06,
      "loss": 0.9457,
      "step": 50810
    },
    {
      "epoch": 2.988723075792105,
      "grad_norm": 0.36875054240226746,
      "learning_rate": 1.2786769327467e-06,
      "loss": 0.9706,
      "step": 50820
    },
    {
      "epoch": 2.9893111813570536,
      "grad_norm": 0.36390170454978943,
      "learning_rate": 1.2197517284726585e-06,
      "loss": 1.0951,
      "step": 50830
    },
    {
      "epoch": 2.9898992869220025,
      "grad_norm": 0.32700371742248535,
      "learning_rate": 1.1608265241986172e-06,
      "loss": 0.9701,
      "step": 50840
    },
    {
      "epoch": 2.9904873924869513,
      "grad_norm": 0.3804301619529724,
      "learning_rate": 1.1019013199245756e-06,
      "loss": 0.9608,
      "step": 50850
    },
    {
      "epoch": 2.9910754980519005,
      "grad_norm": 0.44720637798309326,
      "learning_rate": 1.0429761156505343e-06,
      "loss": 1.0413,
      "step": 50860
    },
    {
      "epoch": 2.9916636036168494,
      "grad_norm": 0.40034258365631104,
      "learning_rate": 9.840509113764927e-07,
      "loss": 1.0026,
      "step": 50870
    },
    {
      "epoch": 2.992251709181798,
      "grad_norm": 0.3609641492366791,
      "learning_rate": 9.251257071024512e-07,
      "loss": 1.0624,
      "step": 50880
    },
    {
      "epoch": 2.992839814746747,
      "grad_norm": 0.369622141122818,
      "learning_rate": 8.662005028284097e-07,
      "loss": 1.1003,
      "step": 50890
    },
    {
      "epoch": 2.993427920311696,
      "grad_norm": 0.3958040475845337,
      "learning_rate": 8.072752985543682e-07,
      "loss": 1.0201,
      "step": 50900
    },
    {
      "epoch": 2.994016025876645,
      "grad_norm": 0.35858091711997986,
      "learning_rate": 7.483500942803267e-07,
      "loss": 1.0161,
      "step": 50910
    },
    {
      "epoch": 2.994604131441594,
      "grad_norm": 0.37441685795783997,
      "learning_rate": 6.894248900062853e-07,
      "loss": 0.9809,
      "step": 50920
    },
    {
      "epoch": 2.9951922370065427,
      "grad_norm": 0.40348657965660095,
      "learning_rate": 6.304996857322438e-07,
      "loss": 0.9436,
      "step": 50930
    },
    {
      "epoch": 2.9957803425714915,
      "grad_norm": 0.3746040463447571,
      "learning_rate": 5.715744814582023e-07,
      "loss": 0.9982,
      "step": 50940
    },
    {
      "epoch": 2.9963684481364403,
      "grad_norm": 0.3888121545314789,
      "learning_rate": 5.126492771841608e-07,
      "loss": 0.9845,
      "step": 50950
    },
    {
      "epoch": 2.9969565537013896,
      "grad_norm": 0.344278484582901,
      "learning_rate": 4.537240729101194e-07,
      "loss": 1.0217,
      "step": 50960
    },
    {
      "epoch": 2.9975446592663384,
      "grad_norm": 0.39209288358688354,
      "learning_rate": 3.9479886863607795e-07,
      "loss": 0.9701,
      "step": 50970
    },
    {
      "epoch": 2.998132764831287,
      "grad_norm": 0.35513144731521606,
      "learning_rate": 3.3587366436203645e-07,
      "loss": 0.9476,
      "step": 50980
    },
    {
      "epoch": 2.998720870396236,
      "grad_norm": 0.4134700894355774,
      "learning_rate": 2.7694846008799494e-07,
      "loss": 1.0972,
      "step": 50990
    },
    {
      "epoch": 2.999308975961185,
      "grad_norm": 0.389211505651474,
      "learning_rate": 2.1802325581395347e-07,
      "loss": 1.0091,
      "step": 51000
    },
    {
      "epoch": 2.999897081526134,
      "grad_norm": 0.37005120515823364,
      "learning_rate": 1.59098051539912e-07,
      "loss": 1.0161,
      "step": 51010
    }
  ],
  "logging_steps": 10,
  "max_steps": 51012,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.033437068786139e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
