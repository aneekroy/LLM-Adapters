{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 24.0,
  "eval_steps": 500,
  "global_step": 6000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04,
      "grad_norm": 1.1688700914382935,
      "learning_rate": 2.7e-06,
      "loss": 2.6929,
      "step": 10
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.3584473133087158,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 2.8147,
      "step": 20
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.3681257963180542,
      "learning_rate": 8.7e-06,
      "loss": 2.771,
      "step": 30
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.150307536125183,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 2.6469,
      "step": 40
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.2555053234100342,
      "learning_rate": 1.47e-05,
      "loss": 2.6937,
      "step": 50
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.318642258644104,
      "learning_rate": 1.77e-05,
      "loss": 2.6426,
      "step": 60
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.7308584451675415,
      "learning_rate": 2.07e-05,
      "loss": 2.4392,
      "step": 70
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.6401971578598022,
      "learning_rate": 2.37e-05,
      "loss": 2.3004,
      "step": 80
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.764438509941101,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.249,
      "step": 90
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.1082239151000977,
      "learning_rate": 2.97e-05,
      "loss": 2.1318,
      "step": 100
    },
    {
      "epoch": 0.44,
      "grad_norm": 3.300028085708618,
      "learning_rate": 2.995609756097561e-05,
      "loss": 1.8913,
      "step": 110
    },
    {
      "epoch": 0.48,
      "grad_norm": 3.3156168460845947,
      "learning_rate": 2.9907317073170735e-05,
      "loss": 1.6138,
      "step": 120
    },
    {
      "epoch": 0.52,
      "grad_norm": 2.2049193382263184,
      "learning_rate": 2.9858536585365853e-05,
      "loss": 1.4087,
      "step": 130
    },
    {
      "epoch": 0.56,
      "grad_norm": 2.075887441635132,
      "learning_rate": 2.9809756097560977e-05,
      "loss": 1.3659,
      "step": 140
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.7324204444885254,
      "learning_rate": 2.9760975609756098e-05,
      "loss": 1.2228,
      "step": 150
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.4667006731033325,
      "learning_rate": 2.971219512195122e-05,
      "loss": 1.1763,
      "step": 160
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.6689746379852295,
      "learning_rate": 2.9663414634146344e-05,
      "loss": 1.1444,
      "step": 170
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.2287009954452515,
      "learning_rate": 2.9614634146341465e-05,
      "loss": 1.1625,
      "step": 180
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.365359902381897,
      "learning_rate": 2.9565853658536586e-05,
      "loss": 1.1283,
      "step": 190
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.3421701192855835,
      "learning_rate": 2.9517073170731707e-05,
      "loss": 1.1934,
      "step": 200
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.2221064567565918,
      "learning_rate": 2.946829268292683e-05,
      "loss": 1.1187,
      "step": 210
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.416458010673523,
      "learning_rate": 2.9419512195121952e-05,
      "loss": 1.0591,
      "step": 220
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.2945457696914673,
      "learning_rate": 2.9370731707317073e-05,
      "loss": 1.1769,
      "step": 230
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.5920535326004028,
      "learning_rate": 2.9321951219512197e-05,
      "loss": 1.1239,
      "step": 240
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.2558999061584473,
      "learning_rate": 2.927317073170732e-05,
      "loss": 1.1169,
      "step": 250
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.204984188079834,
      "learning_rate": 2.922439024390244e-05,
      "loss": 1.1372,
      "step": 260
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.1929585933685303,
      "learning_rate": 2.917560975609756e-05,
      "loss": 1.1695,
      "step": 270
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.085050344467163,
      "learning_rate": 2.9126829268292685e-05,
      "loss": 1.1368,
      "step": 280
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.3612853288650513,
      "learning_rate": 2.9078048780487806e-05,
      "loss": 1.1653,
      "step": 290
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.5942784547805786,
      "learning_rate": 2.9029268292682927e-05,
      "loss": 1.1196,
      "step": 300
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.5318529605865479,
      "learning_rate": 2.898048780487805e-05,
      "loss": 1.0881,
      "step": 310
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.0569325685501099,
      "learning_rate": 2.893170731707317e-05,
      "loss": 1.0419,
      "step": 320
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.267160177230835,
      "learning_rate": 2.8882926829268293e-05,
      "loss": 1.0811,
      "step": 330
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.604270339012146,
      "learning_rate": 2.8834146341463418e-05,
      "loss": 1.058,
      "step": 340
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.3941470384597778,
      "learning_rate": 2.8785365853658535e-05,
      "loss": 1.0481,
      "step": 350
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.546186923980713,
      "learning_rate": 2.873658536585366e-05,
      "loss": 1.1541,
      "step": 360
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.3231552839279175,
      "learning_rate": 2.868780487804878e-05,
      "loss": 1.1798,
      "step": 370
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.2655686140060425,
      "learning_rate": 2.8639024390243902e-05,
      "loss": 1.0881,
      "step": 380
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.2145224809646606,
      "learning_rate": 2.8590243902439026e-05,
      "loss": 1.0932,
      "step": 390
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.5455907583236694,
      "learning_rate": 2.8541463414634147e-05,
      "loss": 1.082,
      "step": 400
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.3565700054168701,
      "learning_rate": 2.8492682926829268e-05,
      "loss": 1.0926,
      "step": 410
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.3601484298706055,
      "learning_rate": 2.844390243902439e-05,
      "loss": 1.0991,
      "step": 420
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.411371111869812,
      "learning_rate": 2.8395121951219514e-05,
      "loss": 1.0546,
      "step": 430
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.3540147542953491,
      "learning_rate": 2.8346341463414635e-05,
      "loss": 1.177,
      "step": 440
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.4171415567398071,
      "learning_rate": 2.8297560975609756e-05,
      "loss": 1.1056,
      "step": 450
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.1550066471099854,
      "learning_rate": 2.824878048780488e-05,
      "loss": 1.1107,
      "step": 460
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.2614080905914307,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 1.1073,
      "step": 470
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.2221201658248901,
      "learning_rate": 2.8151219512195122e-05,
      "loss": 1.0148,
      "step": 480
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.5668623447418213,
      "learning_rate": 2.8102439024390246e-05,
      "loss": 1.03,
      "step": 490
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.440462350845337,
      "learning_rate": 2.8053658536585367e-05,
      "loss": 1.0954,
      "step": 500
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.1678764820098877,
      "learning_rate": 2.800487804878049e-05,
      "loss": 1.1277,
      "step": 510
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.154958724975586,
      "learning_rate": 2.795609756097561e-05,
      "loss": 1.0191,
      "step": 520
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.5997684001922607,
      "learning_rate": 2.7907317073170734e-05,
      "loss": 1.071,
      "step": 530
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.381569266319275,
      "learning_rate": 2.7858536585365855e-05,
      "loss": 1.1187,
      "step": 540
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.2863349914550781,
      "learning_rate": 2.7809756097560976e-05,
      "loss": 1.0187,
      "step": 550
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.2421870231628418,
      "learning_rate": 2.77609756097561e-05,
      "loss": 1.0914,
      "step": 560
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 1.1838794946670532,
      "learning_rate": 2.7712195121951218e-05,
      "loss": 1.0906,
      "step": 570
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.1942822933197021,
      "learning_rate": 2.7663414634146342e-05,
      "loss": 1.1334,
      "step": 580
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.2551262378692627,
      "learning_rate": 2.7614634146341467e-05,
      "loss": 1.0111,
      "step": 590
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.4020637273788452,
      "learning_rate": 2.7565853658536584e-05,
      "loss": 1.0707,
      "step": 600
    },
    {
      "epoch": 2.44,
      "grad_norm": 1.3964531421661377,
      "learning_rate": 2.751707317073171e-05,
      "loss": 1.0621,
      "step": 610
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.236642599105835,
      "learning_rate": 2.746829268292683e-05,
      "loss": 1.0552,
      "step": 620
    },
    {
      "epoch": 2.52,
      "grad_norm": 1.299494743347168,
      "learning_rate": 2.741951219512195e-05,
      "loss": 1.0772,
      "step": 630
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.3094749450683594,
      "learning_rate": 2.7370731707317075e-05,
      "loss": 1.1162,
      "step": 640
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.3155055046081543,
      "learning_rate": 2.7321951219512196e-05,
      "loss": 1.0906,
      "step": 650
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.3959325551986694,
      "learning_rate": 2.7273170731707317e-05,
      "loss": 1.0682,
      "step": 660
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.3226523399353027,
      "learning_rate": 2.7224390243902438e-05,
      "loss": 1.0543,
      "step": 670
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.4103403091430664,
      "learning_rate": 2.7175609756097563e-05,
      "loss": 1.0559,
      "step": 680
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.2313319444656372,
      "learning_rate": 2.7126829268292684e-05,
      "loss": 1.0382,
      "step": 690
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.1227056980133057,
      "learning_rate": 2.7078048780487805e-05,
      "loss": 1.1001,
      "step": 700
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.6116249561309814,
      "learning_rate": 2.702926829268293e-05,
      "loss": 1.1197,
      "step": 710
    },
    {
      "epoch": 2.88,
      "grad_norm": 1.42411208152771,
      "learning_rate": 2.6980487804878047e-05,
      "loss": 1.0965,
      "step": 720
    },
    {
      "epoch": 2.92,
      "grad_norm": 1.7816351652145386,
      "learning_rate": 2.693170731707317e-05,
      "loss": 1.0694,
      "step": 730
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.4240866899490356,
      "learning_rate": 2.6882926829268296e-05,
      "loss": 1.0821,
      "step": 740
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.570249080657959,
      "learning_rate": 2.6834146341463417e-05,
      "loss": 1.1242,
      "step": 750
    },
    {
      "epoch": 3.04,
      "grad_norm": 1.4615281820297241,
      "learning_rate": 2.6785365853658538e-05,
      "loss": 1.035,
      "step": 760
    },
    {
      "epoch": 3.08,
      "grad_norm": 1.262595772743225,
      "learning_rate": 2.673658536585366e-05,
      "loss": 1.1023,
      "step": 770
    },
    {
      "epoch": 3.12,
      "grad_norm": 1.2699280977249146,
      "learning_rate": 2.6687804878048783e-05,
      "loss": 1.0706,
      "step": 780
    },
    {
      "epoch": 3.16,
      "grad_norm": 1.5177870988845825,
      "learning_rate": 2.6639024390243904e-05,
      "loss": 1.0707,
      "step": 790
    },
    {
      "epoch": 3.2,
      "grad_norm": 1.1166919469833374,
      "learning_rate": 2.6590243902439025e-05,
      "loss": 1.0134,
      "step": 800
    },
    {
      "epoch": 3.24,
      "grad_norm": 1.5084338188171387,
      "learning_rate": 2.654146341463415e-05,
      "loss": 1.0778,
      "step": 810
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 1.3219053745269775,
      "learning_rate": 2.6492682926829267e-05,
      "loss": 1.0776,
      "step": 820
    },
    {
      "epoch": 3.32,
      "grad_norm": 1.6122784614562988,
      "learning_rate": 2.644390243902439e-05,
      "loss": 1.1387,
      "step": 830
    },
    {
      "epoch": 3.36,
      "grad_norm": 1.430620551109314,
      "learning_rate": 2.6395121951219512e-05,
      "loss": 1.0216,
      "step": 840
    },
    {
      "epoch": 3.4,
      "grad_norm": 1.2430897951126099,
      "learning_rate": 2.6346341463414633e-05,
      "loss": 1.0909,
      "step": 850
    },
    {
      "epoch": 3.44,
      "grad_norm": 1.5168577432632446,
      "learning_rate": 2.6297560975609758e-05,
      "loss": 1.1328,
      "step": 860
    },
    {
      "epoch": 3.48,
      "grad_norm": 1.2873090505599976,
      "learning_rate": 2.624878048780488e-05,
      "loss": 1.0312,
      "step": 870
    },
    {
      "epoch": 3.52,
      "grad_norm": 1.5723990201950073,
      "learning_rate": 2.62e-05,
      "loss": 1.1433,
      "step": 880
    },
    {
      "epoch": 3.56,
      "grad_norm": 1.3273590803146362,
      "learning_rate": 2.615121951219512e-05,
      "loss": 1.0638,
      "step": 890
    },
    {
      "epoch": 3.6,
      "grad_norm": 1.309921383857727,
      "learning_rate": 2.6102439024390245e-05,
      "loss": 1.033,
      "step": 900
    },
    {
      "epoch": 3.64,
      "grad_norm": 1.521492838859558,
      "learning_rate": 2.6053658536585366e-05,
      "loss": 1.0294,
      "step": 910
    },
    {
      "epoch": 3.68,
      "grad_norm": 1.5821024179458618,
      "learning_rate": 2.6004878048780487e-05,
      "loss": 1.0823,
      "step": 920
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 1.519040584564209,
      "learning_rate": 2.5956097560975612e-05,
      "loss": 1.0265,
      "step": 930
    },
    {
      "epoch": 3.76,
      "grad_norm": 1.8041974306106567,
      "learning_rate": 2.590731707317073e-05,
      "loss": 1.0747,
      "step": 940
    },
    {
      "epoch": 3.8,
      "grad_norm": 1.3858944177627563,
      "learning_rate": 2.5858536585365854e-05,
      "loss": 1.0879,
      "step": 950
    },
    {
      "epoch": 3.84,
      "grad_norm": 1.8065584897994995,
      "learning_rate": 2.5809756097560978e-05,
      "loss": 1.0257,
      "step": 960
    },
    {
      "epoch": 3.88,
      "grad_norm": 1.3677866458892822,
      "learning_rate": 2.5760975609756096e-05,
      "loss": 1.094,
      "step": 970
    },
    {
      "epoch": 3.92,
      "grad_norm": 1.6113485097885132,
      "learning_rate": 2.571219512195122e-05,
      "loss": 1.0643,
      "step": 980
    },
    {
      "epoch": 3.96,
      "grad_norm": 1.578437089920044,
      "learning_rate": 2.566341463414634e-05,
      "loss": 1.0741,
      "step": 990
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.6794363260269165,
      "learning_rate": 2.5614634146341466e-05,
      "loss": 1.0431,
      "step": 1000
    },
    {
      "epoch": 4.04,
      "grad_norm": 2.0065081119537354,
      "learning_rate": 2.5565853658536587e-05,
      "loss": 0.9986,
      "step": 1010
    },
    {
      "epoch": 4.08,
      "grad_norm": 1.3309295177459717,
      "learning_rate": 2.5517073170731708e-05,
      "loss": 1.0487,
      "step": 1020
    },
    {
      "epoch": 4.12,
      "grad_norm": 1.3129907846450806,
      "learning_rate": 2.5468292682926832e-05,
      "loss": 1.0863,
      "step": 1030
    },
    {
      "epoch": 4.16,
      "grad_norm": 1.580863356590271,
      "learning_rate": 2.541951219512195e-05,
      "loss": 1.0364,
      "step": 1040
    },
    {
      "epoch": 4.2,
      "grad_norm": 1.5472080707550049,
      "learning_rate": 2.5370731707317074e-05,
      "loss": 1.0784,
      "step": 1050
    },
    {
      "epoch": 4.24,
      "grad_norm": 1.4265741109848022,
      "learning_rate": 2.53219512195122e-05,
      "loss": 1.0218,
      "step": 1060
    },
    {
      "epoch": 4.28,
      "grad_norm": 1.5412578582763672,
      "learning_rate": 2.5273170731707316e-05,
      "loss": 1.0074,
      "step": 1070
    },
    {
      "epoch": 4.32,
      "grad_norm": 1.8173787593841553,
      "learning_rate": 2.522439024390244e-05,
      "loss": 1.0517,
      "step": 1080
    },
    {
      "epoch": 4.36,
      "grad_norm": 1.4347915649414062,
      "learning_rate": 2.517560975609756e-05,
      "loss": 1.088,
      "step": 1090
    },
    {
      "epoch": 4.4,
      "grad_norm": 1.5521100759506226,
      "learning_rate": 2.5126829268292682e-05,
      "loss": 1.0428,
      "step": 1100
    },
    {
      "epoch": 4.44,
      "grad_norm": 1.6265665292739868,
      "learning_rate": 2.5078048780487807e-05,
      "loss": 0.994,
      "step": 1110
    },
    {
      "epoch": 4.48,
      "grad_norm": 1.8296360969543457,
      "learning_rate": 2.5029268292682928e-05,
      "loss": 1.0725,
      "step": 1120
    },
    {
      "epoch": 4.52,
      "grad_norm": 1.6674081087112427,
      "learning_rate": 2.498048780487805e-05,
      "loss": 1.0628,
      "step": 1130
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 1.8785977363586426,
      "learning_rate": 2.493170731707317e-05,
      "loss": 1.0632,
      "step": 1140
    },
    {
      "epoch": 4.6,
      "grad_norm": 1.6019994020462036,
      "learning_rate": 2.4882926829268294e-05,
      "loss": 1.0094,
      "step": 1150
    },
    {
      "epoch": 4.64,
      "grad_norm": 1.711684226989746,
      "learning_rate": 2.4834146341463415e-05,
      "loss": 1.0361,
      "step": 1160
    },
    {
      "epoch": 4.68,
      "grad_norm": 1.5820338726043701,
      "learning_rate": 2.4785365853658536e-05,
      "loss": 1.07,
      "step": 1170
    },
    {
      "epoch": 4.72,
      "grad_norm": 1.5999096632003784,
      "learning_rate": 2.473658536585366e-05,
      "loss": 1.0347,
      "step": 1180
    },
    {
      "epoch": 4.76,
      "grad_norm": 1.4569591283798218,
      "learning_rate": 2.468780487804878e-05,
      "loss": 1.0368,
      "step": 1190
    },
    {
      "epoch": 4.8,
      "grad_norm": 1.5206530094146729,
      "learning_rate": 2.4639024390243903e-05,
      "loss": 1.0444,
      "step": 1200
    },
    {
      "epoch": 4.84,
      "grad_norm": 1.671362280845642,
      "learning_rate": 2.4590243902439027e-05,
      "loss": 1.0296,
      "step": 1210
    },
    {
      "epoch": 4.88,
      "grad_norm": 1.38772451877594,
      "learning_rate": 2.4541463414634145e-05,
      "loss": 1.092,
      "step": 1220
    },
    {
      "epoch": 4.92,
      "grad_norm": 1.632104754447937,
      "learning_rate": 2.449268292682927e-05,
      "loss": 1.0479,
      "step": 1230
    },
    {
      "epoch": 4.96,
      "grad_norm": 1.7231851816177368,
      "learning_rate": 2.444390243902439e-05,
      "loss": 1.1118,
      "step": 1240
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.8351958990097046,
      "learning_rate": 2.439512195121951e-05,
      "loss": 1.064,
      "step": 1250
    },
    {
      "epoch": 5.04,
      "grad_norm": 1.4819045066833496,
      "learning_rate": 2.4346341463414636e-05,
      "loss": 1.0812,
      "step": 1260
    },
    {
      "epoch": 5.08,
      "grad_norm": 1.6733269691467285,
      "learning_rate": 2.4297560975609757e-05,
      "loss": 1.0113,
      "step": 1270
    },
    {
      "epoch": 5.12,
      "grad_norm": 1.591524600982666,
      "learning_rate": 2.424878048780488e-05,
      "loss": 1.0481,
      "step": 1280
    },
    {
      "epoch": 5.16,
      "grad_norm": 1.4384403228759766,
      "learning_rate": 2.42e-05,
      "loss": 1.0701,
      "step": 1290
    },
    {
      "epoch": 5.2,
      "grad_norm": 1.384683609008789,
      "learning_rate": 2.4151219512195123e-05,
      "loss": 1.0494,
      "step": 1300
    },
    {
      "epoch": 5.24,
      "grad_norm": 2.289689064025879,
      "learning_rate": 2.4102439024390247e-05,
      "loss": 1.0609,
      "step": 1310
    },
    {
      "epoch": 5.28,
      "grad_norm": 1.8640292882919312,
      "learning_rate": 2.4053658536585365e-05,
      "loss": 1.0542,
      "step": 1320
    },
    {
      "epoch": 5.32,
      "grad_norm": 1.5867329835891724,
      "learning_rate": 2.400487804878049e-05,
      "loss": 1.0702,
      "step": 1330
    },
    {
      "epoch": 5.36,
      "grad_norm": 2.090433120727539,
      "learning_rate": 2.395609756097561e-05,
      "loss": 1.0793,
      "step": 1340
    },
    {
      "epoch": 5.4,
      "grad_norm": 1.617594838142395,
      "learning_rate": 2.390731707317073e-05,
      "loss": 1.0404,
      "step": 1350
    },
    {
      "epoch": 5.44,
      "grad_norm": 1.6717110872268677,
      "learning_rate": 2.3858536585365856e-05,
      "loss": 1.058,
      "step": 1360
    },
    {
      "epoch": 5.48,
      "grad_norm": 1.9520996809005737,
      "learning_rate": 2.3809756097560977e-05,
      "loss": 1.0824,
      "step": 1370
    },
    {
      "epoch": 5.52,
      "grad_norm": 1.51129150390625,
      "learning_rate": 2.3760975609756098e-05,
      "loss": 1.0602,
      "step": 1380
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 1.5712217092514038,
      "learning_rate": 2.371219512195122e-05,
      "loss": 1.0195,
      "step": 1390
    },
    {
      "epoch": 5.6,
      "grad_norm": 1.5067991018295288,
      "learning_rate": 2.3663414634146343e-05,
      "loss": 1.0339,
      "step": 1400
    },
    {
      "epoch": 5.64,
      "grad_norm": 1.7404403686523438,
      "learning_rate": 2.3614634146341464e-05,
      "loss": 1.0287,
      "step": 1410
    },
    {
      "epoch": 5.68,
      "grad_norm": 1.8319827318191528,
      "learning_rate": 2.3565853658536585e-05,
      "loss": 1.0664,
      "step": 1420
    },
    {
      "epoch": 5.72,
      "grad_norm": 1.6339563131332397,
      "learning_rate": 2.351707317073171e-05,
      "loss": 1.054,
      "step": 1430
    },
    {
      "epoch": 5.76,
      "grad_norm": 1.698641300201416,
      "learning_rate": 2.3468292682926827e-05,
      "loss": 1.1356,
      "step": 1440
    },
    {
      "epoch": 5.8,
      "grad_norm": 1.7430305480957031,
      "learning_rate": 2.3419512195121952e-05,
      "loss": 1.037,
      "step": 1450
    },
    {
      "epoch": 5.84,
      "grad_norm": 1.7063935995101929,
      "learning_rate": 2.3370731707317073e-05,
      "loss": 0.9825,
      "step": 1460
    },
    {
      "epoch": 5.88,
      "grad_norm": 1.7269426584243774,
      "learning_rate": 2.3321951219512194e-05,
      "loss": 1.0746,
      "step": 1470
    },
    {
      "epoch": 5.92,
      "grad_norm": 1.929229974746704,
      "learning_rate": 2.3273170731707318e-05,
      "loss": 0.9696,
      "step": 1480
    },
    {
      "epoch": 5.96,
      "grad_norm": 2.5966711044311523,
      "learning_rate": 2.322439024390244e-05,
      "loss": 1.0112,
      "step": 1490
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.7272107601165771,
      "learning_rate": 2.317560975609756e-05,
      "loss": 1.0548,
      "step": 1500
    },
    {
      "epoch": 6.04,
      "grad_norm": 1.9447827339172363,
      "learning_rate": 2.312682926829268e-05,
      "loss": 1.0266,
      "step": 1510
    },
    {
      "epoch": 6.08,
      "grad_norm": 1.6270776987075806,
      "learning_rate": 2.3078048780487806e-05,
      "loss": 1.1003,
      "step": 1520
    },
    {
      "epoch": 6.12,
      "grad_norm": 1.8036826848983765,
      "learning_rate": 2.302926829268293e-05,
      "loss": 0.9857,
      "step": 1530
    },
    {
      "epoch": 6.16,
      "grad_norm": 1.8650966882705688,
      "learning_rate": 2.2980487804878048e-05,
      "loss": 1.0702,
      "step": 1540
    },
    {
      "epoch": 6.2,
      "grad_norm": 1.6593084335327148,
      "learning_rate": 2.2931707317073172e-05,
      "loss": 1.035,
      "step": 1550
    },
    {
      "epoch": 6.24,
      "grad_norm": 1.5422409772872925,
      "learning_rate": 2.2882926829268293e-05,
      "loss": 1.0787,
      "step": 1560
    },
    {
      "epoch": 6.28,
      "grad_norm": 1.872649073600769,
      "learning_rate": 2.2834146341463414e-05,
      "loss": 1.1121,
      "step": 1570
    },
    {
      "epoch": 6.32,
      "grad_norm": 1.504109263420105,
      "learning_rate": 2.278536585365854e-05,
      "loss": 0.9949,
      "step": 1580
    },
    {
      "epoch": 6.36,
      "grad_norm": 1.6236985921859741,
      "learning_rate": 2.273658536585366e-05,
      "loss": 1.0704,
      "step": 1590
    },
    {
      "epoch": 6.4,
      "grad_norm": 1.9232980012893677,
      "learning_rate": 2.268780487804878e-05,
      "loss": 1.0324,
      "step": 1600
    },
    {
      "epoch": 6.44,
      "grad_norm": 1.7844196557998657,
      "learning_rate": 2.26390243902439e-05,
      "loss": 1.015,
      "step": 1610
    },
    {
      "epoch": 6.48,
      "grad_norm": 1.4567607641220093,
      "learning_rate": 2.2590243902439026e-05,
      "loss": 1.0412,
      "step": 1620
    },
    {
      "epoch": 6.52,
      "grad_norm": 1.7331368923187256,
      "learning_rate": 2.2541463414634147e-05,
      "loss": 0.9908,
      "step": 1630
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 1.6409037113189697,
      "learning_rate": 2.2492682926829268e-05,
      "loss": 1.0734,
      "step": 1640
    },
    {
      "epoch": 6.6,
      "grad_norm": 1.8899061679840088,
      "learning_rate": 2.2443902439024392e-05,
      "loss": 1.0759,
      "step": 1650
    },
    {
      "epoch": 6.64,
      "grad_norm": 2.100067377090454,
      "learning_rate": 2.239512195121951e-05,
      "loss": 1.0322,
      "step": 1660
    },
    {
      "epoch": 6.68,
      "grad_norm": 1.7501314878463745,
      "learning_rate": 2.2346341463414634e-05,
      "loss": 1.0085,
      "step": 1670
    },
    {
      "epoch": 6.72,
      "grad_norm": 1.6868911981582642,
      "learning_rate": 2.229756097560976e-05,
      "loss": 1.0219,
      "step": 1680
    },
    {
      "epoch": 6.76,
      "grad_norm": 1.4517631530761719,
      "learning_rate": 2.2248780487804876e-05,
      "loss": 1.0794,
      "step": 1690
    },
    {
      "epoch": 6.8,
      "grad_norm": 1.707841157913208,
      "learning_rate": 2.22e-05,
      "loss": 1.0011,
      "step": 1700
    },
    {
      "epoch": 6.84,
      "grad_norm": 2.099228620529175,
      "learning_rate": 2.2151219512195122e-05,
      "loss": 1.0351,
      "step": 1710
    },
    {
      "epoch": 6.88,
      "grad_norm": 1.8449513912200928,
      "learning_rate": 2.2102439024390243e-05,
      "loss": 1.0606,
      "step": 1720
    },
    {
      "epoch": 6.92,
      "grad_norm": 1.6054192781448364,
      "learning_rate": 2.2053658536585367e-05,
      "loss": 1.0165,
      "step": 1730
    },
    {
      "epoch": 6.96,
      "grad_norm": 1.811502456665039,
      "learning_rate": 2.200487804878049e-05,
      "loss": 1.0125,
      "step": 1740
    },
    {
      "epoch": 7.0,
      "grad_norm": 1.6820034980773926,
      "learning_rate": 2.195609756097561e-05,
      "loss": 1.0655,
      "step": 1750
    },
    {
      "epoch": 7.04,
      "grad_norm": 1.5309358835220337,
      "learning_rate": 2.190731707317073e-05,
      "loss": 1.0018,
      "step": 1760
    },
    {
      "epoch": 7.08,
      "grad_norm": 1.7954095602035522,
      "learning_rate": 2.1858536585365855e-05,
      "loss": 1.0177,
      "step": 1770
    },
    {
      "epoch": 7.12,
      "grad_norm": 1.776421308517456,
      "learning_rate": 2.180975609756098e-05,
      "loss": 1.0899,
      "step": 1780
    },
    {
      "epoch": 7.16,
      "grad_norm": 1.6592928171157837,
      "learning_rate": 2.1760975609756097e-05,
      "loss": 1.0467,
      "step": 1790
    },
    {
      "epoch": 7.2,
      "grad_norm": 1.5870907306671143,
      "learning_rate": 2.171219512195122e-05,
      "loss": 1.0353,
      "step": 1800
    },
    {
      "epoch": 7.24,
      "grad_norm": 1.5869622230529785,
      "learning_rate": 2.1663414634146342e-05,
      "loss": 1.0386,
      "step": 1810
    },
    {
      "epoch": 7.28,
      "grad_norm": 1.6503806114196777,
      "learning_rate": 2.1614634146341463e-05,
      "loss": 1.0299,
      "step": 1820
    },
    {
      "epoch": 7.32,
      "grad_norm": 1.6012881994247437,
      "learning_rate": 2.1565853658536588e-05,
      "loss": 1.0354,
      "step": 1830
    },
    {
      "epoch": 7.36,
      "grad_norm": 1.9467365741729736,
      "learning_rate": 2.151707317073171e-05,
      "loss": 1.0547,
      "step": 1840
    },
    {
      "epoch": 7.4,
      "grad_norm": 1.8465933799743652,
      "learning_rate": 2.146829268292683e-05,
      "loss": 1.0319,
      "step": 1850
    },
    {
      "epoch": 7.44,
      "grad_norm": 1.5900262594223022,
      "learning_rate": 2.141951219512195e-05,
      "loss": 1.019,
      "step": 1860
    },
    {
      "epoch": 7.48,
      "grad_norm": 1.7980040311813354,
      "learning_rate": 2.1370731707317075e-05,
      "loss": 1.0498,
      "step": 1870
    },
    {
      "epoch": 7.52,
      "grad_norm": 1.891050100326538,
      "learning_rate": 2.1321951219512196e-05,
      "loss": 1.0188,
      "step": 1880
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 1.706146240234375,
      "learning_rate": 2.1273170731707317e-05,
      "loss": 1.0127,
      "step": 1890
    },
    {
      "epoch": 7.6,
      "grad_norm": 1.6965821981430054,
      "learning_rate": 2.122439024390244e-05,
      "loss": 1.0988,
      "step": 1900
    },
    {
      "epoch": 7.64,
      "grad_norm": 1.8544410467147827,
      "learning_rate": 2.117560975609756e-05,
      "loss": 1.0861,
      "step": 1910
    },
    {
      "epoch": 7.68,
      "grad_norm": 1.6629602909088135,
      "learning_rate": 2.1126829268292684e-05,
      "loss": 1.081,
      "step": 1920
    },
    {
      "epoch": 7.72,
      "grad_norm": 2.1354730129241943,
      "learning_rate": 2.1078048780487808e-05,
      "loss": 1.0084,
      "step": 1930
    },
    {
      "epoch": 7.76,
      "grad_norm": 1.9221699237823486,
      "learning_rate": 2.1029268292682926e-05,
      "loss": 1.0228,
      "step": 1940
    },
    {
      "epoch": 7.8,
      "grad_norm": 2.0067718029022217,
      "learning_rate": 2.098048780487805e-05,
      "loss": 1.0124,
      "step": 1950
    },
    {
      "epoch": 7.84,
      "grad_norm": 1.9694463014602661,
      "learning_rate": 2.093170731707317e-05,
      "loss": 1.0536,
      "step": 1960
    },
    {
      "epoch": 7.88,
      "grad_norm": 1.8480260372161865,
      "learning_rate": 2.0882926829268292e-05,
      "loss": 1.0297,
      "step": 1970
    },
    {
      "epoch": 7.92,
      "grad_norm": 1.8088165521621704,
      "learning_rate": 2.0834146341463416e-05,
      "loss": 0.9715,
      "step": 1980
    },
    {
      "epoch": 7.96,
      "grad_norm": 1.636018991470337,
      "learning_rate": 2.0785365853658537e-05,
      "loss": 1.0029,
      "step": 1990
    },
    {
      "epoch": 8.0,
      "grad_norm": 2.024467706680298,
      "learning_rate": 2.073658536585366e-05,
      "loss": 1.062,
      "step": 2000
    },
    {
      "epoch": 8.04,
      "grad_norm": 1.9122339487075806,
      "learning_rate": 2.068780487804878e-05,
      "loss": 1.0416,
      "step": 2010
    },
    {
      "epoch": 8.08,
      "grad_norm": 2.1096765995025635,
      "learning_rate": 2.0639024390243904e-05,
      "loss": 1.0605,
      "step": 2020
    },
    {
      "epoch": 8.12,
      "grad_norm": 2.237820625305176,
      "learning_rate": 2.0590243902439028e-05,
      "loss": 1.0396,
      "step": 2030
    },
    {
      "epoch": 8.16,
      "grad_norm": 1.9799563884735107,
      "learning_rate": 2.0541463414634146e-05,
      "loss": 1.0281,
      "step": 2040
    },
    {
      "epoch": 8.2,
      "grad_norm": 1.7060198783874512,
      "learning_rate": 2.049268292682927e-05,
      "loss": 1.0168,
      "step": 2050
    },
    {
      "epoch": 8.24,
      "grad_norm": 1.733251690864563,
      "learning_rate": 2.044390243902439e-05,
      "loss": 1.0714,
      "step": 2060
    },
    {
      "epoch": 8.28,
      "grad_norm": 1.9500455856323242,
      "learning_rate": 2.0395121951219512e-05,
      "loss": 1.089,
      "step": 2070
    },
    {
      "epoch": 8.32,
      "grad_norm": 2.1139719486236572,
      "learning_rate": 2.0346341463414633e-05,
      "loss": 1.0653,
      "step": 2080
    },
    {
      "epoch": 8.36,
      "grad_norm": 2.125704765319824,
      "learning_rate": 2.0297560975609758e-05,
      "loss": 0.9968,
      "step": 2090
    },
    {
      "epoch": 8.4,
      "grad_norm": 1.970096230506897,
      "learning_rate": 2.024878048780488e-05,
      "loss": 0.9851,
      "step": 2100
    },
    {
      "epoch": 8.44,
      "grad_norm": 1.9981809854507446,
      "learning_rate": 2.02e-05,
      "loss": 1.0694,
      "step": 2110
    },
    {
      "epoch": 8.48,
      "grad_norm": 2.045903444290161,
      "learning_rate": 2.0151219512195124e-05,
      "loss": 0.9912,
      "step": 2120
    },
    {
      "epoch": 8.52,
      "grad_norm": 2.6622252464294434,
      "learning_rate": 2.0102439024390242e-05,
      "loss": 1.0447,
      "step": 2130
    },
    {
      "epoch": 8.56,
      "grad_norm": 1.9588640928268433,
      "learning_rate": 2.0053658536585366e-05,
      "loss": 0.96,
      "step": 2140
    },
    {
      "epoch": 8.6,
      "grad_norm": 1.9865810871124268,
      "learning_rate": 2.000487804878049e-05,
      "loss": 1.0072,
      "step": 2150
    },
    {
      "epoch": 8.64,
      "grad_norm": 1.789744257926941,
      "learning_rate": 1.9956097560975608e-05,
      "loss": 1.0317,
      "step": 2160
    },
    {
      "epoch": 8.68,
      "grad_norm": 1.8638224601745605,
      "learning_rate": 1.9907317073170733e-05,
      "loss": 1.0597,
      "step": 2170
    },
    {
      "epoch": 8.72,
      "grad_norm": 2.0535635948181152,
      "learning_rate": 1.9858536585365854e-05,
      "loss": 1.0279,
      "step": 2180
    },
    {
      "epoch": 8.76,
      "grad_norm": 1.716568112373352,
      "learning_rate": 1.9809756097560975e-05,
      "loss": 1.0591,
      "step": 2190
    },
    {
      "epoch": 8.8,
      "grad_norm": 1.980300784111023,
      "learning_rate": 1.97609756097561e-05,
      "loss": 1.0491,
      "step": 2200
    },
    {
      "epoch": 8.84,
      "grad_norm": 2.335603713989258,
      "learning_rate": 1.971219512195122e-05,
      "loss": 1.0417,
      "step": 2210
    },
    {
      "epoch": 8.88,
      "grad_norm": 2.084998607635498,
      "learning_rate": 1.966341463414634e-05,
      "loss": 0.9836,
      "step": 2220
    },
    {
      "epoch": 8.92,
      "grad_norm": 1.6973423957824707,
      "learning_rate": 1.9614634146341462e-05,
      "loss": 0.9901,
      "step": 2230
    },
    {
      "epoch": 8.96,
      "grad_norm": 1.8587234020233154,
      "learning_rate": 1.9565853658536586e-05,
      "loss": 0.9609,
      "step": 2240
    },
    {
      "epoch": 9.0,
      "grad_norm": 1.9595948457717896,
      "learning_rate": 1.9517073170731707e-05,
      "loss": 0.9727,
      "step": 2250
    },
    {
      "epoch": 9.04,
      "grad_norm": 2.2476701736450195,
      "learning_rate": 1.946829268292683e-05,
      "loss": 1.0293,
      "step": 2260
    },
    {
      "epoch": 9.08,
      "grad_norm": 2.0755112171173096,
      "learning_rate": 1.9419512195121953e-05,
      "loss": 1.0443,
      "step": 2270
    },
    {
      "epoch": 9.12,
      "grad_norm": 2.207231283187866,
      "learning_rate": 1.9370731707317074e-05,
      "loss": 1.0048,
      "step": 2280
    },
    {
      "epoch": 9.16,
      "grad_norm": 2.045597553253174,
      "learning_rate": 1.9321951219512195e-05,
      "loss": 1.0575,
      "step": 2290
    },
    {
      "epoch": 9.2,
      "grad_norm": 1.8585621118545532,
      "learning_rate": 1.927317073170732e-05,
      "loss": 1.0326,
      "step": 2300
    },
    {
      "epoch": 9.24,
      "grad_norm": 1.9283804893493652,
      "learning_rate": 1.922439024390244e-05,
      "loss": 0.9541,
      "step": 2310
    },
    {
      "epoch": 9.28,
      "grad_norm": 1.965278148651123,
      "learning_rate": 1.917560975609756e-05,
      "loss": 1.0061,
      "step": 2320
    },
    {
      "epoch": 9.32,
      "grad_norm": 2.3348753452301025,
      "learning_rate": 1.9126829268292682e-05,
      "loss": 1.0475,
      "step": 2330
    },
    {
      "epoch": 9.36,
      "grad_norm": 1.9119704961776733,
      "learning_rate": 1.9078048780487807e-05,
      "loss": 1.0015,
      "step": 2340
    },
    {
      "epoch": 9.4,
      "grad_norm": 2.3264362812042236,
      "learning_rate": 1.9029268292682928e-05,
      "loss": 1.0427,
      "step": 2350
    },
    {
      "epoch": 9.44,
      "grad_norm": 1.963442087173462,
      "learning_rate": 1.898048780487805e-05,
      "loss": 0.979,
      "step": 2360
    },
    {
      "epoch": 9.48,
      "grad_norm": 1.9350759983062744,
      "learning_rate": 1.8931707317073173e-05,
      "loss": 1.0455,
      "step": 2370
    },
    {
      "epoch": 9.52,
      "grad_norm": 2.1388862133026123,
      "learning_rate": 1.888292682926829e-05,
      "loss": 0.9936,
      "step": 2380
    },
    {
      "epoch": 9.56,
      "grad_norm": 2.0778305530548096,
      "learning_rate": 1.8834146341463415e-05,
      "loss": 1.0565,
      "step": 2390
    },
    {
      "epoch": 9.6,
      "grad_norm": 2.035095453262329,
      "learning_rate": 1.878536585365854e-05,
      "loss": 1.0687,
      "step": 2400
    },
    {
      "epoch": 9.64,
      "grad_norm": 1.891983985900879,
      "learning_rate": 1.8736585365853657e-05,
      "loss": 1.0142,
      "step": 2410
    },
    {
      "epoch": 9.68,
      "grad_norm": 1.9184638261795044,
      "learning_rate": 1.868780487804878e-05,
      "loss": 1.0138,
      "step": 2420
    },
    {
      "epoch": 9.72,
      "grad_norm": 1.8592292070388794,
      "learning_rate": 1.8639024390243903e-05,
      "loss": 1.0421,
      "step": 2430
    },
    {
      "epoch": 9.76,
      "grad_norm": 2.013784885406494,
      "learning_rate": 1.8590243902439024e-05,
      "loss": 1.0449,
      "step": 2440
    },
    {
      "epoch": 9.8,
      "grad_norm": 1.9420833587646484,
      "learning_rate": 1.8541463414634148e-05,
      "loss": 1.033,
      "step": 2450
    },
    {
      "epoch": 9.84,
      "grad_norm": 2.1033666133880615,
      "learning_rate": 1.849268292682927e-05,
      "loss": 1.0371,
      "step": 2460
    },
    {
      "epoch": 9.88,
      "grad_norm": 1.916162371635437,
      "learning_rate": 1.844390243902439e-05,
      "loss": 1.0012,
      "step": 2470
    },
    {
      "epoch": 9.92,
      "grad_norm": 1.8388314247131348,
      "learning_rate": 1.839512195121951e-05,
      "loss": 1.0146,
      "step": 2480
    },
    {
      "epoch": 9.96,
      "grad_norm": 1.9802045822143555,
      "learning_rate": 1.8346341463414636e-05,
      "loss": 1.0285,
      "step": 2490
    },
    {
      "epoch": 10.0,
      "grad_norm": 1.8154810667037964,
      "learning_rate": 1.8297560975609757e-05,
      "loss": 1.0086,
      "step": 2500
    },
    {
      "epoch": 10.04,
      "grad_norm": 1.9991716146469116,
      "learning_rate": 1.8248780487804878e-05,
      "loss": 1.0408,
      "step": 2510
    },
    {
      "epoch": 10.08,
      "grad_norm": 2.187291383743286,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.9297,
      "step": 2520
    },
    {
      "epoch": 10.12,
      "grad_norm": 2.035094976425171,
      "learning_rate": 1.815121951219512e-05,
      "loss": 0.9463,
      "step": 2530
    },
    {
      "epoch": 10.16,
      "grad_norm": 1.8754650354385376,
      "learning_rate": 1.8102439024390244e-05,
      "loss": 1.0082,
      "step": 2540
    },
    {
      "epoch": 10.2,
      "grad_norm": 2.1770689487457275,
      "learning_rate": 1.805365853658537e-05,
      "loss": 0.9866,
      "step": 2550
    },
    {
      "epoch": 10.24,
      "grad_norm": 4.56948709487915,
      "learning_rate": 1.800487804878049e-05,
      "loss": 1.0197,
      "step": 2560
    },
    {
      "epoch": 10.28,
      "grad_norm": 2.2404191493988037,
      "learning_rate": 1.795609756097561e-05,
      "loss": 0.9244,
      "step": 2570
    },
    {
      "epoch": 10.32,
      "grad_norm": 2.1756229400634766,
      "learning_rate": 1.790731707317073e-05,
      "loss": 1.0576,
      "step": 2580
    },
    {
      "epoch": 10.36,
      "grad_norm": 2.290297269821167,
      "learning_rate": 1.7858536585365856e-05,
      "loss": 0.9998,
      "step": 2590
    },
    {
      "epoch": 10.4,
      "grad_norm": 1.7985050678253174,
      "learning_rate": 1.7809756097560977e-05,
      "loss": 1.0271,
      "step": 2600
    },
    {
      "epoch": 10.44,
      "grad_norm": 2.4316935539245605,
      "learning_rate": 1.7760975609756098e-05,
      "loss": 0.9852,
      "step": 2610
    },
    {
      "epoch": 10.48,
      "grad_norm": 2.140961170196533,
      "learning_rate": 1.7712195121951222e-05,
      "loss": 1.0283,
      "step": 2620
    },
    {
      "epoch": 10.52,
      "grad_norm": 2.147447109222412,
      "learning_rate": 1.766341463414634e-05,
      "loss": 1.0667,
      "step": 2630
    },
    {
      "epoch": 10.56,
      "grad_norm": 2.031766891479492,
      "learning_rate": 1.7614634146341464e-05,
      "loss": 1.0146,
      "step": 2640
    },
    {
      "epoch": 10.6,
      "grad_norm": 1.7360292673110962,
      "learning_rate": 1.756585365853659e-05,
      "loss": 0.9492,
      "step": 2650
    },
    {
      "epoch": 10.64,
      "grad_norm": 2.097363233566284,
      "learning_rate": 1.7517073170731706e-05,
      "loss": 1.0082,
      "step": 2660
    },
    {
      "epoch": 10.68,
      "grad_norm": 2.295578718185425,
      "learning_rate": 1.746829268292683e-05,
      "loss": 1.0134,
      "step": 2670
    },
    {
      "epoch": 10.72,
      "grad_norm": 2.5976569652557373,
      "learning_rate": 1.7419512195121952e-05,
      "loss": 1.0505,
      "step": 2680
    },
    {
      "epoch": 10.76,
      "grad_norm": 2.040323257446289,
      "learning_rate": 1.7370731707317073e-05,
      "loss": 1.0576,
      "step": 2690
    },
    {
      "epoch": 10.8,
      "grad_norm": 2.0977723598480225,
      "learning_rate": 1.7321951219512194e-05,
      "loss": 1.1065,
      "step": 2700
    },
    {
      "epoch": 10.84,
      "grad_norm": 2.093059539794922,
      "learning_rate": 1.7273170731707318e-05,
      "loss": 0.9703,
      "step": 2710
    },
    {
      "epoch": 10.88,
      "grad_norm": 2.1296470165252686,
      "learning_rate": 1.722439024390244e-05,
      "loss": 1.0482,
      "step": 2720
    },
    {
      "epoch": 10.92,
      "grad_norm": 2.3367292881011963,
      "learning_rate": 1.717560975609756e-05,
      "loss": 1.0418,
      "step": 2730
    },
    {
      "epoch": 10.96,
      "grad_norm": 2.3634767532348633,
      "learning_rate": 1.7126829268292685e-05,
      "loss": 1.053,
      "step": 2740
    },
    {
      "epoch": 11.0,
      "grad_norm": 1.9830304384231567,
      "learning_rate": 1.7078048780487802e-05,
      "loss": 1.0286,
      "step": 2750
    },
    {
      "epoch": 11.04,
      "grad_norm": 1.8621641397476196,
      "learning_rate": 1.7029268292682927e-05,
      "loss": 1.0042,
      "step": 2760
    },
    {
      "epoch": 11.08,
      "grad_norm": 2.390082836151123,
      "learning_rate": 1.698048780487805e-05,
      "loss": 0.9772,
      "step": 2770
    },
    {
      "epoch": 11.12,
      "grad_norm": 2.0652809143066406,
      "learning_rate": 1.693170731707317e-05,
      "loss": 0.9958,
      "step": 2780
    },
    {
      "epoch": 11.16,
      "grad_norm": 2.059678077697754,
      "learning_rate": 1.6882926829268293e-05,
      "loss": 1.0173,
      "step": 2790
    },
    {
      "epoch": 11.2,
      "grad_norm": 2.2903616428375244,
      "learning_rate": 1.6834146341463414e-05,
      "loss": 1.0174,
      "step": 2800
    },
    {
      "epoch": 11.24,
      "grad_norm": 2.3888895511627197,
      "learning_rate": 1.678536585365854e-05,
      "loss": 1.0355,
      "step": 2810
    },
    {
      "epoch": 11.28,
      "grad_norm": 2.363591194152832,
      "learning_rate": 1.673658536585366e-05,
      "loss": 1.0364,
      "step": 2820
    },
    {
      "epoch": 11.32,
      "grad_norm": 2.6586883068084717,
      "learning_rate": 1.669268292682927e-05,
      "loss": 1.0478,
      "step": 2830
    },
    {
      "epoch": 11.36,
      "grad_norm": 2.3452579975128174,
      "learning_rate": 1.664390243902439e-05,
      "loss": 0.9713,
      "step": 2840
    },
    {
      "epoch": 11.4,
      "grad_norm": 2.2790417671203613,
      "learning_rate": 1.6595121951219515e-05,
      "loss": 1.0259,
      "step": 2850
    },
    {
      "epoch": 11.44,
      "grad_norm": 2.3896939754486084,
      "learning_rate": 1.6546341463414632e-05,
      "loss": 0.9588,
      "step": 2860
    },
    {
      "epoch": 11.48,
      "grad_norm": 1.897997260093689,
      "learning_rate": 1.6497560975609757e-05,
      "loss": 1.0223,
      "step": 2870
    },
    {
      "epoch": 11.52,
      "grad_norm": 1.7971802949905396,
      "learning_rate": 1.644878048780488e-05,
      "loss": 0.9383,
      "step": 2880
    },
    {
      "epoch": 11.56,
      "grad_norm": 2.3241066932678223,
      "learning_rate": 1.64e-05,
      "loss": 1.0459,
      "step": 2890
    },
    {
      "epoch": 11.6,
      "grad_norm": 2.2230160236358643,
      "learning_rate": 1.6351219512195123e-05,
      "loss": 0.9942,
      "step": 2900
    },
    {
      "epoch": 11.64,
      "grad_norm": 2.1901140213012695,
      "learning_rate": 1.6302439024390244e-05,
      "loss": 1.0546,
      "step": 2910
    },
    {
      "epoch": 11.68,
      "grad_norm": 1.9833277463912964,
      "learning_rate": 1.6253658536585365e-05,
      "loss": 1.0387,
      "step": 2920
    },
    {
      "epoch": 11.72,
      "grad_norm": 2.378915786743164,
      "learning_rate": 1.620487804878049e-05,
      "loss": 1.0187,
      "step": 2930
    },
    {
      "epoch": 11.76,
      "grad_norm": 3.2885100841522217,
      "learning_rate": 1.615609756097561e-05,
      "loss": 1.0521,
      "step": 2940
    },
    {
      "epoch": 11.8,
      "grad_norm": 2.1259500980377197,
      "learning_rate": 1.610731707317073e-05,
      "loss": 1.006,
      "step": 2950
    },
    {
      "epoch": 11.84,
      "grad_norm": 2.1479287147521973,
      "learning_rate": 1.6058536585365853e-05,
      "loss": 1.0267,
      "step": 2960
    },
    {
      "epoch": 11.88,
      "grad_norm": 2.203242063522339,
      "learning_rate": 1.6009756097560977e-05,
      "loss": 1.0809,
      "step": 2970
    },
    {
      "epoch": 11.92,
      "grad_norm": 2.159111499786377,
      "learning_rate": 1.5960975609756098e-05,
      "loss": 1.0118,
      "step": 2980
    },
    {
      "epoch": 11.96,
      "grad_norm": 2.211432933807373,
      "learning_rate": 1.591219512195122e-05,
      "loss": 0.9921,
      "step": 2990
    },
    {
      "epoch": 12.0,
      "grad_norm": 1.98696768283844,
      "learning_rate": 1.5863414634146344e-05,
      "loss": 0.9494,
      "step": 3000
    },
    {
      "epoch": 12.04,
      "grad_norm": 2.2587056159973145,
      "learning_rate": 1.581463414634146e-05,
      "loss": 1.0034,
      "step": 3010
    },
    {
      "epoch": 12.08,
      "grad_norm": 2.037301540374756,
      "learning_rate": 1.5765853658536586e-05,
      "loss": 0.9527,
      "step": 3020
    },
    {
      "epoch": 12.12,
      "grad_norm": 2.0161495208740234,
      "learning_rate": 1.571707317073171e-05,
      "loss": 1.0018,
      "step": 3030
    },
    {
      "epoch": 12.16,
      "grad_norm": 2.4395453929901123,
      "learning_rate": 1.5668292682926828e-05,
      "loss": 0.9924,
      "step": 3040
    },
    {
      "epoch": 12.2,
      "grad_norm": 2.1200459003448486,
      "learning_rate": 1.5619512195121952e-05,
      "loss": 1.0389,
      "step": 3050
    },
    {
      "epoch": 12.24,
      "grad_norm": 2.0846807956695557,
      "learning_rate": 1.5570731707317073e-05,
      "loss": 1.0499,
      "step": 3060
    },
    {
      "epoch": 12.28,
      "grad_norm": 2.5077004432678223,
      "learning_rate": 1.5521951219512197e-05,
      "loss": 1.0332,
      "step": 3070
    },
    {
      "epoch": 12.32,
      "grad_norm": 2.3340578079223633,
      "learning_rate": 1.547317073170732e-05,
      "loss": 0.9899,
      "step": 3080
    },
    {
      "epoch": 12.36,
      "grad_norm": 2.008941173553467,
      "learning_rate": 1.542439024390244e-05,
      "loss": 0.9605,
      "step": 3090
    },
    {
      "epoch": 12.4,
      "grad_norm": 2.429421901702881,
      "learning_rate": 1.5375609756097564e-05,
      "loss": 1.0131,
      "step": 3100
    },
    {
      "epoch": 12.44,
      "grad_norm": 2.1523115634918213,
      "learning_rate": 1.532682926829268e-05,
      "loss": 0.9469,
      "step": 3110
    },
    {
      "epoch": 12.48,
      "grad_norm": 2.255718231201172,
      "learning_rate": 1.5278048780487806e-05,
      "loss": 0.9465,
      "step": 3120
    },
    {
      "epoch": 12.52,
      "grad_norm": 2.30104923248291,
      "learning_rate": 1.5229268292682929e-05,
      "loss": 1.0071,
      "step": 3130
    },
    {
      "epoch": 12.56,
      "grad_norm": 2.0158286094665527,
      "learning_rate": 1.5180487804878048e-05,
      "loss": 0.9875,
      "step": 3140
    },
    {
      "epoch": 12.6,
      "grad_norm": 2.3664746284484863,
      "learning_rate": 1.513170731707317e-05,
      "loss": 1.0534,
      "step": 3150
    },
    {
      "epoch": 12.64,
      "grad_norm": 2.5249736309051514,
      "learning_rate": 1.5082926829268295e-05,
      "loss": 1.0314,
      "step": 3160
    },
    {
      "epoch": 12.68,
      "grad_norm": 2.2143735885620117,
      "learning_rate": 1.5034146341463414e-05,
      "loss": 0.9491,
      "step": 3170
    },
    {
      "epoch": 12.72,
      "grad_norm": 2.1599225997924805,
      "learning_rate": 1.4985365853658537e-05,
      "loss": 0.9969,
      "step": 3180
    },
    {
      "epoch": 12.76,
      "grad_norm": 2.1934854984283447,
      "learning_rate": 1.4936585365853658e-05,
      "loss": 1.0453,
      "step": 3190
    },
    {
      "epoch": 12.8,
      "grad_norm": 2.362182140350342,
      "learning_rate": 1.488780487804878e-05,
      "loss": 1.0264,
      "step": 3200
    },
    {
      "epoch": 12.84,
      "grad_norm": 2.6189725399017334,
      "learning_rate": 1.4839024390243903e-05,
      "loss": 0.9789,
      "step": 3210
    },
    {
      "epoch": 12.88,
      "grad_norm": 2.3685193061828613,
      "learning_rate": 1.4790243902439024e-05,
      "loss": 1.0701,
      "step": 3220
    },
    {
      "epoch": 12.92,
      "grad_norm": 2.4354705810546875,
      "learning_rate": 1.4741463414634147e-05,
      "loss": 1.0684,
      "step": 3230
    },
    {
      "epoch": 12.96,
      "grad_norm": 2.2568283081054688,
      "learning_rate": 1.4692682926829268e-05,
      "loss": 1.0382,
      "step": 3240
    },
    {
      "epoch": 13.0,
      "grad_norm": 2.626807451248169,
      "learning_rate": 1.4643902439024391e-05,
      "loss": 0.9867,
      "step": 3250
    },
    {
      "epoch": 13.04,
      "grad_norm": 2.0714664459228516,
      "learning_rate": 1.4595121951219514e-05,
      "loss": 1.0093,
      "step": 3260
    },
    {
      "epoch": 13.08,
      "grad_norm": 1.9966323375701904,
      "learning_rate": 1.4546341463414635e-05,
      "loss": 1.0533,
      "step": 3270
    },
    {
      "epoch": 13.12,
      "grad_norm": 2.1949543952941895,
      "learning_rate": 1.4502439024390245e-05,
      "loss": 0.9892,
      "step": 3280
    },
    {
      "epoch": 13.16,
      "grad_norm": 2.586289644241333,
      "learning_rate": 1.4453658536585367e-05,
      "loss": 1.0104,
      "step": 3290
    },
    {
      "epoch": 13.2,
      "grad_norm": 2.4086098670959473,
      "learning_rate": 1.4404878048780488e-05,
      "loss": 0.9875,
      "step": 3300
    },
    {
      "epoch": 13.24,
      "grad_norm": 2.518091917037964,
      "learning_rate": 1.435609756097561e-05,
      "loss": 1.0214,
      "step": 3310
    },
    {
      "epoch": 13.28,
      "grad_norm": 2.234133720397949,
      "learning_rate": 1.4307317073170732e-05,
      "loss": 0.9866,
      "step": 3320
    },
    {
      "epoch": 13.32,
      "grad_norm": 2.3698740005493164,
      "learning_rate": 1.4258536585365855e-05,
      "loss": 1.0271,
      "step": 3330
    },
    {
      "epoch": 13.36,
      "grad_norm": 2.202946662902832,
      "learning_rate": 1.4209756097560976e-05,
      "loss": 0.9774,
      "step": 3340
    },
    {
      "epoch": 13.4,
      "grad_norm": 2.6343603134155273,
      "learning_rate": 1.4160975609756098e-05,
      "loss": 0.9926,
      "step": 3350
    },
    {
      "epoch": 13.44,
      "grad_norm": 2.4943814277648926,
      "learning_rate": 1.411219512195122e-05,
      "loss": 1.0307,
      "step": 3360
    },
    {
      "epoch": 13.48,
      "grad_norm": 2.468871831893921,
      "learning_rate": 1.406341463414634e-05,
      "loss": 1.0074,
      "step": 3370
    },
    {
      "epoch": 13.52,
      "grad_norm": 2.3487439155578613,
      "learning_rate": 1.4014634146341465e-05,
      "loss": 1.0341,
      "step": 3380
    },
    {
      "epoch": 13.56,
      "grad_norm": 2.3233580589294434,
      "learning_rate": 1.3965853658536586e-05,
      "loss": 1.0342,
      "step": 3390
    },
    {
      "epoch": 13.6,
      "grad_norm": 3.0250802040100098,
      "learning_rate": 1.3917073170731709e-05,
      "loss": 1.0403,
      "step": 3400
    },
    {
      "epoch": 13.64,
      "grad_norm": 2.510436773300171,
      "learning_rate": 1.386829268292683e-05,
      "loss": 0.9726,
      "step": 3410
    },
    {
      "epoch": 13.68,
      "grad_norm": 2.17079758644104,
      "learning_rate": 1.381951219512195e-05,
      "loss": 0.9732,
      "step": 3420
    },
    {
      "epoch": 13.72,
      "grad_norm": 2.5039918422698975,
      "learning_rate": 1.3770731707317073e-05,
      "loss": 0.961,
      "step": 3430
    },
    {
      "epoch": 13.76,
      "grad_norm": 2.2259442806243896,
      "learning_rate": 1.3721951219512196e-05,
      "loss": 0.9551,
      "step": 3440
    },
    {
      "epoch": 13.8,
      "grad_norm": 2.704951524734497,
      "learning_rate": 1.3673170731707317e-05,
      "loss": 0.9851,
      "step": 3450
    },
    {
      "epoch": 13.84,
      "grad_norm": 2.5544931888580322,
      "learning_rate": 1.362439024390244e-05,
      "loss": 0.9938,
      "step": 3460
    },
    {
      "epoch": 13.88,
      "grad_norm": 2.4340226650238037,
      "learning_rate": 1.357560975609756e-05,
      "loss": 1.0168,
      "step": 3470
    },
    {
      "epoch": 13.92,
      "grad_norm": 2.085047483444214,
      "learning_rate": 1.3526829268292682e-05,
      "loss": 0.9999,
      "step": 3480
    },
    {
      "epoch": 13.96,
      "grad_norm": 2.5522940158843994,
      "learning_rate": 1.3478048780487806e-05,
      "loss": 1.0527,
      "step": 3490
    },
    {
      "epoch": 14.0,
      "grad_norm": 2.597377300262451,
      "learning_rate": 1.3429268292682927e-05,
      "loss": 0.9873,
      "step": 3500
    },
    {
      "epoch": 14.04,
      "grad_norm": 2.7011191844940186,
      "learning_rate": 1.338048780487805e-05,
      "loss": 1.0024,
      "step": 3510
    },
    {
      "epoch": 14.08,
      "grad_norm": 2.447695732116699,
      "learning_rate": 1.3331707317073171e-05,
      "loss": 0.9907,
      "step": 3520
    },
    {
      "epoch": 14.12,
      "grad_norm": 2.0338144302368164,
      "learning_rate": 1.3282926829268292e-05,
      "loss": 1.0113,
      "step": 3530
    },
    {
      "epoch": 14.16,
      "grad_norm": 2.644388437271118,
      "learning_rate": 1.3234146341463416e-05,
      "loss": 1.0185,
      "step": 3540
    },
    {
      "epoch": 14.2,
      "grad_norm": 2.9002864360809326,
      "learning_rate": 1.3185365853658537e-05,
      "loss": 0.957,
      "step": 3550
    },
    {
      "epoch": 14.24,
      "grad_norm": 2.2015187740325928,
      "learning_rate": 1.3136585365853658e-05,
      "loss": 1.0001,
      "step": 3560
    },
    {
      "epoch": 14.28,
      "grad_norm": 2.6652519702911377,
      "learning_rate": 1.3087804878048781e-05,
      "loss": 1.012,
      "step": 3570
    },
    {
      "epoch": 14.32,
      "grad_norm": 2.61084246635437,
      "learning_rate": 1.3039024390243902e-05,
      "loss": 0.9877,
      "step": 3580
    },
    {
      "epoch": 14.36,
      "grad_norm": 2.3726935386657715,
      "learning_rate": 1.2990243902439025e-05,
      "loss": 0.9454,
      "step": 3590
    },
    {
      "epoch": 14.4,
      "grad_norm": 2.5292301177978516,
      "learning_rate": 1.2941463414634147e-05,
      "loss": 1.0485,
      "step": 3600
    },
    {
      "epoch": 14.44,
      "grad_norm": 2.3954660892486572,
      "learning_rate": 1.2892682926829268e-05,
      "loss": 0.9697,
      "step": 3610
    },
    {
      "epoch": 14.48,
      "grad_norm": 2.3467342853546143,
      "learning_rate": 1.284390243902439e-05,
      "loss": 0.9634,
      "step": 3620
    },
    {
      "epoch": 14.52,
      "grad_norm": 2.4215447902679443,
      "learning_rate": 1.2795121951219512e-05,
      "loss": 1.0341,
      "step": 3630
    },
    {
      "epoch": 14.56,
      "grad_norm": 2.1047074794769287,
      "learning_rate": 1.2746341463414635e-05,
      "loss": 0.936,
      "step": 3640
    },
    {
      "epoch": 14.6,
      "grad_norm": 2.4379405975341797,
      "learning_rate": 1.2697560975609758e-05,
      "loss": 0.9361,
      "step": 3650
    },
    {
      "epoch": 14.64,
      "grad_norm": 2.5262813568115234,
      "learning_rate": 1.2648780487804879e-05,
      "loss": 0.9487,
      "step": 3660
    },
    {
      "epoch": 14.68,
      "grad_norm": 2.8513333797454834,
      "learning_rate": 1.26e-05,
      "loss": 1.0325,
      "step": 3670
    },
    {
      "epoch": 14.72,
      "grad_norm": 2.5564610958099365,
      "learning_rate": 1.2551219512195122e-05,
      "loss": 0.969,
      "step": 3680
    },
    {
      "epoch": 14.76,
      "grad_norm": 2.541638135910034,
      "learning_rate": 1.2502439024390245e-05,
      "loss": 0.9858,
      "step": 3690
    },
    {
      "epoch": 14.8,
      "grad_norm": 2.5552287101745605,
      "learning_rate": 1.2453658536585366e-05,
      "loss": 0.9985,
      "step": 3700
    },
    {
      "epoch": 14.84,
      "grad_norm": 2.368230104446411,
      "learning_rate": 1.2404878048780489e-05,
      "loss": 1.0128,
      "step": 3710
    },
    {
      "epoch": 14.88,
      "grad_norm": 2.246131420135498,
      "learning_rate": 1.235609756097561e-05,
      "loss": 1.0092,
      "step": 3720
    },
    {
      "epoch": 14.92,
      "grad_norm": 2.5347204208374023,
      "learning_rate": 1.230731707317073e-05,
      "loss": 1.0182,
      "step": 3730
    },
    {
      "epoch": 14.96,
      "grad_norm": 2.457320213317871,
      "learning_rate": 1.2258536585365854e-05,
      "loss": 0.9882,
      "step": 3740
    },
    {
      "epoch": 15.0,
      "grad_norm": 2.1251938343048096,
      "learning_rate": 1.2209756097560976e-05,
      "loss": 0.9778,
      "step": 3750
    },
    {
      "epoch": 15.04,
      "grad_norm": 2.539471387863159,
      "learning_rate": 1.2160975609756099e-05,
      "loss": 0.9919,
      "step": 3760
    },
    {
      "epoch": 15.08,
      "grad_norm": 2.4332668781280518,
      "learning_rate": 1.211219512195122e-05,
      "loss": 0.9373,
      "step": 3770
    },
    {
      "epoch": 15.12,
      "grad_norm": 2.2232213020324707,
      "learning_rate": 1.2063414634146341e-05,
      "loss": 1.0712,
      "step": 3780
    },
    {
      "epoch": 15.16,
      "grad_norm": 2.5294477939605713,
      "learning_rate": 1.2014634146341464e-05,
      "loss": 1.027,
      "step": 3790
    },
    {
      "epoch": 15.2,
      "grad_norm": 2.6656529903411865,
      "learning_rate": 1.1965853658536586e-05,
      "loss": 1.0337,
      "step": 3800
    },
    {
      "epoch": 15.24,
      "grad_norm": 2.6257846355438232,
      "learning_rate": 1.1917073170731707e-05,
      "loss": 1.0471,
      "step": 3810
    },
    {
      "epoch": 15.28,
      "grad_norm": 2.655789375305176,
      "learning_rate": 1.186829268292683e-05,
      "loss": 0.976,
      "step": 3820
    },
    {
      "epoch": 15.32,
      "grad_norm": 2.3866779804229736,
      "learning_rate": 1.1819512195121951e-05,
      "loss": 0.9031,
      "step": 3830
    },
    {
      "epoch": 15.36,
      "grad_norm": 2.5283288955688477,
      "learning_rate": 1.1770731707317072e-05,
      "loss": 0.9297,
      "step": 3840
    },
    {
      "epoch": 15.4,
      "grad_norm": 2.322819709777832,
      "learning_rate": 1.1721951219512197e-05,
      "loss": 0.9969,
      "step": 3850
    },
    {
      "epoch": 15.44,
      "grad_norm": 2.7201180458068848,
      "learning_rate": 1.1673170731707318e-05,
      "loss": 1.054,
      "step": 3860
    },
    {
      "epoch": 15.48,
      "grad_norm": 2.47334623336792,
      "learning_rate": 1.1624390243902439e-05,
      "loss": 0.9655,
      "step": 3870
    },
    {
      "epoch": 15.52,
      "grad_norm": 2.1869125366210938,
      "learning_rate": 1.1575609756097561e-05,
      "loss": 0.9978,
      "step": 3880
    },
    {
      "epoch": 15.56,
      "grad_norm": 3.095982074737549,
      "learning_rate": 1.1526829268292682e-05,
      "loss": 1.1117,
      "step": 3890
    },
    {
      "epoch": 15.6,
      "grad_norm": 2.235604763031006,
      "learning_rate": 1.1478048780487807e-05,
      "loss": 0.9893,
      "step": 3900
    },
    {
      "epoch": 15.64,
      "grad_norm": 2.462442398071289,
      "learning_rate": 1.1429268292682928e-05,
      "loss": 0.977,
      "step": 3910
    },
    {
      "epoch": 15.68,
      "grad_norm": 2.8366522789001465,
      "learning_rate": 1.1380487804878049e-05,
      "loss": 1.0161,
      "step": 3920
    },
    {
      "epoch": 15.72,
      "grad_norm": 3.488097667694092,
      "learning_rate": 1.1331707317073171e-05,
      "loss": 1.0065,
      "step": 3930
    },
    {
      "epoch": 15.76,
      "grad_norm": 2.337745189666748,
      "learning_rate": 1.1282926829268292e-05,
      "loss": 0.9171,
      "step": 3940
    },
    {
      "epoch": 15.8,
      "grad_norm": 2.416362762451172,
      "learning_rate": 1.1234146341463415e-05,
      "loss": 1.0009,
      "step": 3950
    },
    {
      "epoch": 15.84,
      "grad_norm": 2.1911323070526123,
      "learning_rate": 1.1185365853658538e-05,
      "loss": 0.9785,
      "step": 3960
    },
    {
      "epoch": 15.88,
      "grad_norm": 2.6569905281066895,
      "learning_rate": 1.1136585365853659e-05,
      "loss": 0.9843,
      "step": 3970
    },
    {
      "epoch": 15.92,
      "grad_norm": 2.102984666824341,
      "learning_rate": 1.108780487804878e-05,
      "loss": 1.0246,
      "step": 3980
    },
    {
      "epoch": 15.96,
      "grad_norm": 3.121445417404175,
      "learning_rate": 1.1039024390243903e-05,
      "loss": 0.9845,
      "step": 3990
    },
    {
      "epoch": 16.0,
      "grad_norm": 2.656400680541992,
      "learning_rate": 1.0990243902439025e-05,
      "loss": 1.0156,
      "step": 4000
    },
    {
      "epoch": 16.04,
      "grad_norm": 2.421942710876465,
      "learning_rate": 1.0941463414634146e-05,
      "loss": 0.9484,
      "step": 4010
    },
    {
      "epoch": 16.08,
      "grad_norm": 2.080932378768921,
      "learning_rate": 1.0892682926829269e-05,
      "loss": 0.9834,
      "step": 4020
    },
    {
      "epoch": 16.12,
      "grad_norm": 2.583841323852539,
      "learning_rate": 1.084390243902439e-05,
      "loss": 0.9866,
      "step": 4030
    },
    {
      "epoch": 16.16,
      "grad_norm": 3.0237669944763184,
      "learning_rate": 1.0795121951219513e-05,
      "loss": 0.9989,
      "step": 4040
    },
    {
      "epoch": 16.2,
      "grad_norm": 2.3977341651916504,
      "learning_rate": 1.0746341463414634e-05,
      "loss": 0.9613,
      "step": 4050
    },
    {
      "epoch": 16.24,
      "grad_norm": 2.590914011001587,
      "learning_rate": 1.0697560975609756e-05,
      "loss": 1.0275,
      "step": 4060
    },
    {
      "epoch": 16.28,
      "grad_norm": 2.424954891204834,
      "learning_rate": 1.0648780487804879e-05,
      "loss": 1.0194,
      "step": 4070
    },
    {
      "epoch": 16.32,
      "grad_norm": 2.447047233581543,
      "learning_rate": 1.06e-05,
      "loss": 0.9351,
      "step": 4080
    },
    {
      "epoch": 16.36,
      "grad_norm": 2.915742874145508,
      "learning_rate": 1.0551219512195121e-05,
      "loss": 1.0152,
      "step": 4090
    },
    {
      "epoch": 16.4,
      "grad_norm": 2.8305013179779053,
      "learning_rate": 1.0502439024390244e-05,
      "loss": 0.9323,
      "step": 4100
    },
    {
      "epoch": 16.44,
      "grad_norm": 2.3786673545837402,
      "learning_rate": 1.0453658536585367e-05,
      "loss": 0.9817,
      "step": 4110
    },
    {
      "epoch": 16.48,
      "grad_norm": 2.788111686706543,
      "learning_rate": 1.0404878048780488e-05,
      "loss": 1.0044,
      "step": 4120
    },
    {
      "epoch": 16.52,
      "grad_norm": 2.473033905029297,
      "learning_rate": 1.035609756097561e-05,
      "loss": 1.0352,
      "step": 4130
    },
    {
      "epoch": 16.56,
      "grad_norm": 2.5902957916259766,
      "learning_rate": 1.0307317073170731e-05,
      "loss": 0.9478,
      "step": 4140
    },
    {
      "epoch": 16.6,
      "grad_norm": 2.635887861251831,
      "learning_rate": 1.0258536585365854e-05,
      "loss": 0.9519,
      "step": 4150
    },
    {
      "epoch": 16.64,
      "grad_norm": 2.4107956886291504,
      "learning_rate": 1.0209756097560977e-05,
      "loss": 1.0744,
      "step": 4160
    },
    {
      "epoch": 16.68,
      "grad_norm": 2.7333896160125732,
      "learning_rate": 1.0160975609756098e-05,
      "loss": 1.0047,
      "step": 4170
    },
    {
      "epoch": 16.72,
      "grad_norm": 2.67779803276062,
      "learning_rate": 1.011219512195122e-05,
      "loss": 0.9836,
      "step": 4180
    },
    {
      "epoch": 16.76,
      "grad_norm": 3.0586795806884766,
      "learning_rate": 1.0063414634146341e-05,
      "loss": 1.0181,
      "step": 4190
    },
    {
      "epoch": 16.8,
      "grad_norm": 3.1866629123687744,
      "learning_rate": 1.0014634146341462e-05,
      "loss": 0.9935,
      "step": 4200
    },
    {
      "epoch": 16.84,
      "grad_norm": 2.360161781311035,
      "learning_rate": 9.965853658536587e-06,
      "loss": 0.9838,
      "step": 4210
    },
    {
      "epoch": 16.88,
      "grad_norm": 2.5942111015319824,
      "learning_rate": 9.917073170731708e-06,
      "loss": 1.0689,
      "step": 4220
    },
    {
      "epoch": 16.92,
      "grad_norm": 2.8211073875427246,
      "learning_rate": 9.868292682926829e-06,
      "loss": 0.9951,
      "step": 4230
    },
    {
      "epoch": 16.96,
      "grad_norm": 2.494117021560669,
      "learning_rate": 9.819512195121952e-06,
      "loss": 0.9764,
      "step": 4240
    },
    {
      "epoch": 17.0,
      "grad_norm": 2.2999815940856934,
      "learning_rate": 9.770731707317073e-06,
      "loss": 1.0324,
      "step": 4250
    },
    {
      "epoch": 17.04,
      "grad_norm": 2.793164014816284,
      "learning_rate": 9.721951219512195e-06,
      "loss": 0.9999,
      "step": 4260
    },
    {
      "epoch": 17.08,
      "grad_norm": 2.6660690307617188,
      "learning_rate": 9.673170731707318e-06,
      "loss": 1.0007,
      "step": 4270
    },
    {
      "epoch": 17.12,
      "grad_norm": 2.652433395385742,
      "learning_rate": 9.624390243902439e-06,
      "loss": 1.0201,
      "step": 4280
    },
    {
      "epoch": 17.16,
      "grad_norm": 2.698636531829834,
      "learning_rate": 9.575609756097562e-06,
      "loss": 0.966,
      "step": 4290
    },
    {
      "epoch": 17.2,
      "grad_norm": 2.4018893241882324,
      "learning_rate": 9.526829268292683e-06,
      "loss": 0.9669,
      "step": 4300
    },
    {
      "epoch": 17.24,
      "grad_norm": 2.7511355876922607,
      "learning_rate": 9.478048780487805e-06,
      "loss": 1.0801,
      "step": 4310
    },
    {
      "epoch": 17.28,
      "grad_norm": 2.445847272872925,
      "learning_rate": 9.429268292682928e-06,
      "loss": 1.0467,
      "step": 4320
    },
    {
      "epoch": 17.32,
      "grad_norm": 2.4481146335601807,
      "learning_rate": 9.38048780487805e-06,
      "loss": 0.9866,
      "step": 4330
    },
    {
      "epoch": 17.36,
      "grad_norm": 2.964562177658081,
      "learning_rate": 9.33170731707317e-06,
      "loss": 1.0136,
      "step": 4340
    },
    {
      "epoch": 17.4,
      "grad_norm": 2.8022921085357666,
      "learning_rate": 9.282926829268293e-06,
      "loss": 0.9537,
      "step": 4350
    },
    {
      "epoch": 17.44,
      "grad_norm": 2.674891710281372,
      "learning_rate": 9.234146341463414e-06,
      "loss": 0.9598,
      "step": 4360
    },
    {
      "epoch": 17.48,
      "grad_norm": 2.580176830291748,
      "learning_rate": 9.185365853658537e-06,
      "loss": 1.0079,
      "step": 4370
    },
    {
      "epoch": 17.52,
      "grad_norm": 2.741872787475586,
      "learning_rate": 9.13658536585366e-06,
      "loss": 0.9752,
      "step": 4380
    },
    {
      "epoch": 17.56,
      "grad_norm": 2.7019944190979004,
      "learning_rate": 9.08780487804878e-06,
      "loss": 1.005,
      "step": 4390
    },
    {
      "epoch": 17.6,
      "grad_norm": 3.116828203201294,
      "learning_rate": 9.039024390243903e-06,
      "loss": 0.9453,
      "step": 4400
    },
    {
      "epoch": 17.64,
      "grad_norm": 2.720029830932617,
      "learning_rate": 8.990243902439024e-06,
      "loss": 0.9953,
      "step": 4410
    },
    {
      "epoch": 17.68,
      "grad_norm": 2.3072240352630615,
      "learning_rate": 8.941463414634147e-06,
      "loss": 1.0055,
      "step": 4420
    },
    {
      "epoch": 17.72,
      "grad_norm": 2.879120349884033,
      "learning_rate": 8.89268292682927e-06,
      "loss": 0.9346,
      "step": 4430
    },
    {
      "epoch": 17.76,
      "grad_norm": 2.891040086746216,
      "learning_rate": 8.84390243902439e-06,
      "loss": 0.9973,
      "step": 4440
    },
    {
      "epoch": 17.8,
      "grad_norm": 2.5404269695281982,
      "learning_rate": 8.795121951219512e-06,
      "loss": 0.9728,
      "step": 4450
    },
    {
      "epoch": 17.84,
      "grad_norm": 2.4759769439697266,
      "learning_rate": 8.746341463414634e-06,
      "loss": 0.9985,
      "step": 4460
    },
    {
      "epoch": 17.88,
      "grad_norm": 2.807933807373047,
      "learning_rate": 8.697560975609757e-06,
      "loss": 1.0121,
      "step": 4470
    },
    {
      "epoch": 17.92,
      "grad_norm": 3.062622308731079,
      "learning_rate": 8.648780487804878e-06,
      "loss": 1.005,
      "step": 4480
    },
    {
      "epoch": 17.96,
      "grad_norm": 2.6651484966278076,
      "learning_rate": 8.6e-06,
      "loss": 0.962,
      "step": 4490
    },
    {
      "epoch": 18.0,
      "grad_norm": 2.561845302581787,
      "learning_rate": 8.551219512195122e-06,
      "loss": 1.0756,
      "step": 4500
    },
    {
      "epoch": 18.04,
      "grad_norm": 2.0714704990386963,
      "learning_rate": 8.502439024390243e-06,
      "loss": 0.9452,
      "step": 4510
    },
    {
      "epoch": 18.08,
      "grad_norm": 2.553283214569092,
      "learning_rate": 8.453658536585367e-06,
      "loss": 1.0724,
      "step": 4520
    },
    {
      "epoch": 18.12,
      "grad_norm": 2.785649061203003,
      "learning_rate": 8.404878048780488e-06,
      "loss": 1.0321,
      "step": 4530
    },
    {
      "epoch": 18.16,
      "grad_norm": 2.4679229259490967,
      "learning_rate": 8.35609756097561e-06,
      "loss": 0.9585,
      "step": 4540
    },
    {
      "epoch": 18.2,
      "grad_norm": 2.4974682331085205,
      "learning_rate": 8.307317073170732e-06,
      "loss": 0.9845,
      "step": 4550
    },
    {
      "epoch": 18.24,
      "grad_norm": 2.9310879707336426,
      "learning_rate": 8.258536585365853e-06,
      "loss": 1.0161,
      "step": 4560
    },
    {
      "epoch": 18.28,
      "grad_norm": 2.2200334072113037,
      "learning_rate": 8.209756097560977e-06,
      "loss": 0.9824,
      "step": 4570
    },
    {
      "epoch": 18.32,
      "grad_norm": 2.845980405807495,
      "learning_rate": 8.160975609756098e-06,
      "loss": 0.9489,
      "step": 4580
    },
    {
      "epoch": 18.36,
      "grad_norm": 2.4866130352020264,
      "learning_rate": 8.11219512195122e-06,
      "loss": 0.9879,
      "step": 4590
    },
    {
      "epoch": 18.4,
      "grad_norm": 2.315067768096924,
      "learning_rate": 8.063414634146342e-06,
      "loss": 0.9431,
      "step": 4600
    },
    {
      "epoch": 18.44,
      "grad_norm": 3.2981793880462646,
      "learning_rate": 8.014634146341463e-06,
      "loss": 0.986,
      "step": 4610
    },
    {
      "epoch": 18.48,
      "grad_norm": 2.6831324100494385,
      "learning_rate": 7.965853658536586e-06,
      "loss": 0.9574,
      "step": 4620
    },
    {
      "epoch": 18.52,
      "grad_norm": 3.011359214782715,
      "learning_rate": 7.917073170731708e-06,
      "loss": 0.945,
      "step": 4630
    },
    {
      "epoch": 18.56,
      "grad_norm": 2.707200527191162,
      "learning_rate": 7.86829268292683e-06,
      "loss": 1.0305,
      "step": 4640
    },
    {
      "epoch": 18.6,
      "grad_norm": 2.592726230621338,
      "learning_rate": 7.81951219512195e-06,
      "loss": 0.9769,
      "step": 4650
    },
    {
      "epoch": 18.64,
      "grad_norm": 2.8364133834838867,
      "learning_rate": 7.770731707317073e-06,
      "loss": 1.021,
      "step": 4660
    },
    {
      "epoch": 18.68,
      "grad_norm": 2.845388650894165,
      "learning_rate": 7.721951219512194e-06,
      "loss": 0.9234,
      "step": 4670
    },
    {
      "epoch": 18.72,
      "grad_norm": 3.235386610031128,
      "learning_rate": 7.673170731707319e-06,
      "loss": 1.0387,
      "step": 4680
    },
    {
      "epoch": 18.76,
      "grad_norm": 2.995105028152466,
      "learning_rate": 7.6243902439024396e-06,
      "loss": 1.0174,
      "step": 4690
    },
    {
      "epoch": 18.8,
      "grad_norm": 2.5576987266540527,
      "learning_rate": 7.575609756097561e-06,
      "loss": 0.9752,
      "step": 4700
    },
    {
      "epoch": 18.84,
      "grad_norm": 2.5619845390319824,
      "learning_rate": 7.526829268292683e-06,
      "loss": 1.011,
      "step": 4710
    },
    {
      "epoch": 18.88,
      "grad_norm": 2.414640188217163,
      "learning_rate": 7.478048780487805e-06,
      "loss": 0.944,
      "step": 4720
    },
    {
      "epoch": 18.92,
      "grad_norm": 2.908992052078247,
      "learning_rate": 7.429268292682927e-06,
      "loss": 0.9665,
      "step": 4730
    },
    {
      "epoch": 18.96,
      "grad_norm": 2.6340315341949463,
      "learning_rate": 7.380487804878049e-06,
      "loss": 1.0009,
      "step": 4740
    },
    {
      "epoch": 19.0,
      "grad_norm": 3.0599570274353027,
      "learning_rate": 7.331707317073171e-06,
      "loss": 0.9899,
      "step": 4750
    },
    {
      "epoch": 19.04,
      "grad_norm": 2.378236770629883,
      "learning_rate": 7.282926829268293e-06,
      "loss": 0.9979,
      "step": 4760
    },
    {
      "epoch": 19.08,
      "grad_norm": 2.665651798248291,
      "learning_rate": 7.234146341463415e-06,
      "loss": 1.0155,
      "step": 4770
    },
    {
      "epoch": 19.12,
      "grad_norm": 2.8223607540130615,
      "learning_rate": 7.185365853658536e-06,
      "loss": 1.0148,
      "step": 4780
    },
    {
      "epoch": 19.16,
      "grad_norm": 3.0092527866363525,
      "learning_rate": 7.136585365853659e-06,
      "loss": 1.004,
      "step": 4790
    },
    {
      "epoch": 19.2,
      "grad_norm": 2.592388868331909,
      "learning_rate": 7.087804878048781e-06,
      "loss": 0.9952,
      "step": 4800
    },
    {
      "epoch": 19.24,
      "grad_norm": 2.6408743858337402,
      "learning_rate": 7.039024390243902e-06,
      "loss": 0.9433,
      "step": 4810
    },
    {
      "epoch": 19.28,
      "grad_norm": 2.6140034198760986,
      "learning_rate": 6.990243902439025e-06,
      "loss": 1.0188,
      "step": 4820
    },
    {
      "epoch": 19.32,
      "grad_norm": 2.804612636566162,
      "learning_rate": 6.9414634146341465e-06,
      "loss": 0.9922,
      "step": 4830
    },
    {
      "epoch": 19.36,
      "grad_norm": 3.0202796459198,
      "learning_rate": 6.892682926829269e-06,
      "loss": 0.9681,
      "step": 4840
    },
    {
      "epoch": 19.4,
      "grad_norm": 3.288783073425293,
      "learning_rate": 6.84390243902439e-06,
      "loss": 1.021,
      "step": 4850
    },
    {
      "epoch": 19.44,
      "grad_norm": 2.8753974437713623,
      "learning_rate": 6.795121951219512e-06,
      "loss": 1.0355,
      "step": 4860
    },
    {
      "epoch": 19.48,
      "grad_norm": 3.645750045776367,
      "learning_rate": 6.746341463414635e-06,
      "loss": 0.9783,
      "step": 4870
    },
    {
      "epoch": 19.52,
      "grad_norm": 2.8281431198120117,
      "learning_rate": 6.697560975609757e-06,
      "loss": 0.9794,
      "step": 4880
    },
    {
      "epoch": 19.56,
      "grad_norm": 2.584843158721924,
      "learning_rate": 6.648780487804878e-06,
      "loss": 1.0173,
      "step": 4890
    },
    {
      "epoch": 19.6,
      "grad_norm": 3.3921234607696533,
      "learning_rate": 6.6e-06,
      "loss": 1.0598,
      "step": 4900
    },
    {
      "epoch": 19.64,
      "grad_norm": 2.8415894508361816,
      "learning_rate": 6.551219512195122e-06,
      "loss": 1.0091,
      "step": 4910
    },
    {
      "epoch": 19.68,
      "grad_norm": 2.679471731185913,
      "learning_rate": 6.502439024390244e-06,
      "loss": 0.9605,
      "step": 4920
    },
    {
      "epoch": 19.72,
      "grad_norm": 2.862009286880493,
      "learning_rate": 6.453658536585366e-06,
      "loss": 0.9353,
      "step": 4930
    },
    {
      "epoch": 19.76,
      "grad_norm": 3.1569557189941406,
      "learning_rate": 6.404878048780488e-06,
      "loss": 0.9202,
      "step": 4940
    },
    {
      "epoch": 19.8,
      "grad_norm": 2.7082252502441406,
      "learning_rate": 6.3560975609756105e-06,
      "loss": 0.9651,
      "step": 4950
    },
    {
      "epoch": 19.84,
      "grad_norm": 2.4843215942382812,
      "learning_rate": 6.3073170731707315e-06,
      "loss": 1.0079,
      "step": 4960
    },
    {
      "epoch": 19.88,
      "grad_norm": 2.9538419246673584,
      "learning_rate": 6.258536585365854e-06,
      "loss": 0.9785,
      "step": 4970
    },
    {
      "epoch": 19.92,
      "grad_norm": 2.726752758026123,
      "learning_rate": 6.209756097560976e-06,
      "loss": 1.0226,
      "step": 4980
    },
    {
      "epoch": 19.96,
      "grad_norm": 3.0000712871551514,
      "learning_rate": 6.160975609756097e-06,
      "loss": 1.0074,
      "step": 4990
    },
    {
      "epoch": 20.0,
      "grad_norm": 2.9530327320098877,
      "learning_rate": 6.11219512195122e-06,
      "loss": 0.9775,
      "step": 5000
    },
    {
      "epoch": 20.04,
      "grad_norm": 2.8679494857788086,
      "learning_rate": 6.063414634146342e-06,
      "loss": 1.0001,
      "step": 5010
    },
    {
      "epoch": 20.08,
      "grad_norm": 2.752312421798706,
      "learning_rate": 6.0146341463414635e-06,
      "loss": 0.9389,
      "step": 5020
    },
    {
      "epoch": 20.12,
      "grad_norm": 2.7400615215301514,
      "learning_rate": 5.965853658536585e-06,
      "loss": 0.9724,
      "step": 5030
    },
    {
      "epoch": 20.16,
      "grad_norm": 2.5148158073425293,
      "learning_rate": 5.917073170731707e-06,
      "loss": 0.9506,
      "step": 5040
    },
    {
      "epoch": 20.2,
      "grad_norm": 2.8293488025665283,
      "learning_rate": 5.86829268292683e-06,
      "loss": 0.9473,
      "step": 5050
    },
    {
      "epoch": 20.24,
      "grad_norm": 2.871351718902588,
      "learning_rate": 5.819512195121951e-06,
      "loss": 0.9427,
      "step": 5060
    },
    {
      "epoch": 20.28,
      "grad_norm": 2.4867613315582275,
      "learning_rate": 5.770731707317073e-06,
      "loss": 0.9678,
      "step": 5070
    },
    {
      "epoch": 20.32,
      "grad_norm": 3.0508878231048584,
      "learning_rate": 5.7219512195121955e-06,
      "loss": 0.9851,
      "step": 5080
    },
    {
      "epoch": 20.36,
      "grad_norm": 3.058626890182495,
      "learning_rate": 5.673170731707317e-06,
      "loss": 0.96,
      "step": 5090
    },
    {
      "epoch": 20.4,
      "grad_norm": 3.0625557899475098,
      "learning_rate": 5.624390243902439e-06,
      "loss": 0.9455,
      "step": 5100
    },
    {
      "epoch": 20.44,
      "grad_norm": 2.4720773696899414,
      "learning_rate": 5.575609756097561e-06,
      "loss": 0.9836,
      "step": 5110
    },
    {
      "epoch": 20.48,
      "grad_norm": 2.6509063243865967,
      "learning_rate": 5.526829268292683e-06,
      "loss": 0.9322,
      "step": 5120
    },
    {
      "epoch": 20.52,
      "grad_norm": 2.8238368034362793,
      "learning_rate": 5.478048780487805e-06,
      "loss": 0.9876,
      "step": 5130
    },
    {
      "epoch": 20.56,
      "grad_norm": 2.88873028755188,
      "learning_rate": 5.429268292682927e-06,
      "loss": 0.9682,
      "step": 5140
    },
    {
      "epoch": 20.6,
      "grad_norm": 2.780001640319824,
      "learning_rate": 5.380487804878049e-06,
      "loss": 0.9931,
      "step": 5150
    },
    {
      "epoch": 20.64,
      "grad_norm": 3.290083885192871,
      "learning_rate": 5.331707317073171e-06,
      "loss": 1.0372,
      "step": 5160
    },
    {
      "epoch": 20.68,
      "grad_norm": 2.999349355697632,
      "learning_rate": 5.282926829268292e-06,
      "loss": 1.0088,
      "step": 5170
    },
    {
      "epoch": 20.72,
      "grad_norm": 2.736302614212036,
      "learning_rate": 5.234146341463415e-06,
      "loss": 0.9423,
      "step": 5180
    },
    {
      "epoch": 20.76,
      "grad_norm": 2.940755844116211,
      "learning_rate": 5.185365853658537e-06,
      "loss": 1.0535,
      "step": 5190
    },
    {
      "epoch": 20.8,
      "grad_norm": 3.0859575271606445,
      "learning_rate": 5.136585365853659e-06,
      "loss": 0.9473,
      "step": 5200
    },
    {
      "epoch": 20.84,
      "grad_norm": 2.5998244285583496,
      "learning_rate": 5.0878048780487806e-06,
      "loss": 0.9513,
      "step": 5210
    },
    {
      "epoch": 20.88,
      "grad_norm": 2.8460776805877686,
      "learning_rate": 5.039024390243902e-06,
      "loss": 1.0111,
      "step": 5220
    },
    {
      "epoch": 20.92,
      "grad_norm": 2.6774754524230957,
      "learning_rate": 4.990243902439025e-06,
      "loss": 1.0058,
      "step": 5230
    },
    {
      "epoch": 20.96,
      "grad_norm": 2.843390703201294,
      "learning_rate": 4.941463414634146e-06,
      "loss": 1.0142,
      "step": 5240
    },
    {
      "epoch": 21.0,
      "grad_norm": 2.60766863822937,
      "learning_rate": 4.892682926829268e-06,
      "loss": 0.9563,
      "step": 5250
    },
    {
      "epoch": 21.04,
      "grad_norm": 2.8585257530212402,
      "learning_rate": 4.843902439024391e-06,
      "loss": 1.0248,
      "step": 5260
    },
    {
      "epoch": 21.08,
      "grad_norm": 2.859933853149414,
      "learning_rate": 4.7951219512195126e-06,
      "loss": 0.9924,
      "step": 5270
    },
    {
      "epoch": 21.12,
      "grad_norm": 2.7086117267608643,
      "learning_rate": 4.746341463414634e-06,
      "loss": 1.0211,
      "step": 5280
    },
    {
      "epoch": 21.16,
      "grad_norm": 3.3288731575012207,
      "learning_rate": 4.697560975609756e-06,
      "loss": 0.9097,
      "step": 5290
    },
    {
      "epoch": 21.2,
      "grad_norm": 3.068776845932007,
      "learning_rate": 4.648780487804878e-06,
      "loss": 0.9766,
      "step": 5300
    },
    {
      "epoch": 21.24,
      "grad_norm": 2.4978079795837402,
      "learning_rate": 4.6e-06,
      "loss": 0.9516,
      "step": 5310
    },
    {
      "epoch": 21.28,
      "grad_norm": 4.035874843597412,
      "learning_rate": 4.551219512195122e-06,
      "loss": 0.9465,
      "step": 5320
    },
    {
      "epoch": 21.32,
      "grad_norm": 2.8526785373687744,
      "learning_rate": 4.502439024390244e-06,
      "loss": 1.0343,
      "step": 5330
    },
    {
      "epoch": 21.36,
      "grad_norm": 2.8097362518310547,
      "learning_rate": 4.4536585365853664e-06,
      "loss": 0.9491,
      "step": 5340
    },
    {
      "epoch": 21.4,
      "grad_norm": 2.8753411769866943,
      "learning_rate": 4.4048780487804874e-06,
      "loss": 1.0318,
      "step": 5350
    },
    {
      "epoch": 21.44,
      "grad_norm": 2.8142404556274414,
      "learning_rate": 4.35609756097561e-06,
      "loss": 1.0043,
      "step": 5360
    },
    {
      "epoch": 21.48,
      "grad_norm": 2.9459915161132812,
      "learning_rate": 4.307317073170732e-06,
      "loss": 1.0063,
      "step": 5370
    },
    {
      "epoch": 21.52,
      "grad_norm": 2.872401475906372,
      "learning_rate": 4.258536585365853e-06,
      "loss": 0.9641,
      "step": 5380
    },
    {
      "epoch": 21.56,
      "grad_norm": 2.872567653656006,
      "learning_rate": 4.209756097560976e-06,
      "loss": 0.9743,
      "step": 5390
    },
    {
      "epoch": 21.6,
      "grad_norm": 3.132110357284546,
      "learning_rate": 4.160975609756098e-06,
      "loss": 0.9948,
      "step": 5400
    },
    {
      "epoch": 21.64,
      "grad_norm": 3.0231785774230957,
      "learning_rate": 4.11219512195122e-06,
      "loss": 0.9312,
      "step": 5410
    },
    {
      "epoch": 21.68,
      "grad_norm": 2.58503794670105,
      "learning_rate": 4.063414634146341e-06,
      "loss": 1.0076,
      "step": 5420
    },
    {
      "epoch": 21.72,
      "grad_norm": 2.540079116821289,
      "learning_rate": 4.014634146341463e-06,
      "loss": 0.8865,
      "step": 5430
    },
    {
      "epoch": 21.76,
      "grad_norm": 2.95508074760437,
      "learning_rate": 3.965853658536586e-06,
      "loss": 1.0064,
      "step": 5440
    },
    {
      "epoch": 21.8,
      "grad_norm": 3.4700534343719482,
      "learning_rate": 3.917073170731707e-06,
      "loss": 1.0499,
      "step": 5450
    },
    {
      "epoch": 21.84,
      "grad_norm": 2.9068925380706787,
      "learning_rate": 3.86829268292683e-06,
      "loss": 0.9423,
      "step": 5460
    },
    {
      "epoch": 21.88,
      "grad_norm": 2.8632760047912598,
      "learning_rate": 3.8195121951219515e-06,
      "loss": 1.0492,
      "step": 5470
    },
    {
      "epoch": 21.92,
      "grad_norm": 2.8536925315856934,
      "learning_rate": 3.7707317073170737e-06,
      "loss": 0.9235,
      "step": 5480
    },
    {
      "epoch": 21.96,
      "grad_norm": 2.8463988304138184,
      "learning_rate": 3.721951219512195e-06,
      "loss": 0.9686,
      "step": 5490
    },
    {
      "epoch": 22.0,
      "grad_norm": 2.876467704772949,
      "learning_rate": 3.673170731707317e-06,
      "loss": 1.0023,
      "step": 5500
    },
    {
      "epoch": 22.04,
      "grad_norm": 2.3290889263153076,
      "learning_rate": 3.6243902439024393e-06,
      "loss": 0.8858,
      "step": 5510
    },
    {
      "epoch": 22.08,
      "grad_norm": 2.7013819217681885,
      "learning_rate": 3.575609756097561e-06,
      "loss": 0.9957,
      "step": 5520
    },
    {
      "epoch": 22.12,
      "grad_norm": 2.75156307220459,
      "learning_rate": 3.5268292682926826e-06,
      "loss": 0.997,
      "step": 5530
    },
    {
      "epoch": 22.16,
      "grad_norm": 3.3247311115264893,
      "learning_rate": 3.478048780487805e-06,
      "loss": 1.0381,
      "step": 5540
    },
    {
      "epoch": 22.2,
      "grad_norm": 2.755573034286499,
      "learning_rate": 3.4292682926829268e-06,
      "loss": 0.9794,
      "step": 5550
    },
    {
      "epoch": 22.24,
      "grad_norm": 2.8586537837982178,
      "learning_rate": 3.380487804878049e-06,
      "loss": 0.9442,
      "step": 5560
    },
    {
      "epoch": 22.28,
      "grad_norm": 3.0843393802642822,
      "learning_rate": 3.331707317073171e-06,
      "loss": 0.9586,
      "step": 5570
    },
    {
      "epoch": 22.32,
      "grad_norm": 2.9592983722686768,
      "learning_rate": 3.2829268292682928e-06,
      "loss": 0.9682,
      "step": 5580
    },
    {
      "epoch": 22.36,
      "grad_norm": 2.8828601837158203,
      "learning_rate": 3.2341463414634146e-06,
      "loss": 0.9427,
      "step": 5590
    },
    {
      "epoch": 22.4,
      "grad_norm": 2.685886859893799,
      "learning_rate": 3.185365853658537e-06,
      "loss": 0.9414,
      "step": 5600
    },
    {
      "epoch": 22.44,
      "grad_norm": 3.1030478477478027,
      "learning_rate": 3.1365853658536588e-06,
      "loss": 0.9904,
      "step": 5610
    },
    {
      "epoch": 22.48,
      "grad_norm": 3.0550549030303955,
      "learning_rate": 3.0878048780487802e-06,
      "loss": 1.0055,
      "step": 5620
    },
    {
      "epoch": 22.52,
      "grad_norm": 2.5462207794189453,
      "learning_rate": 3.0390243902439025e-06,
      "loss": 0.9371,
      "step": 5630
    },
    {
      "epoch": 22.56,
      "grad_norm": 3.30867862701416,
      "learning_rate": 2.9902439024390244e-06,
      "loss": 1.0,
      "step": 5640
    },
    {
      "epoch": 22.6,
      "grad_norm": 3.3345224857330322,
      "learning_rate": 2.9414634146341466e-06,
      "loss": 0.9971,
      "step": 5650
    },
    {
      "epoch": 22.64,
      "grad_norm": 2.722686290740967,
      "learning_rate": 2.8926829268292685e-06,
      "loss": 0.9919,
      "step": 5660
    },
    {
      "epoch": 22.68,
      "grad_norm": 3.173022508621216,
      "learning_rate": 2.8439024390243904e-06,
      "loss": 0.9683,
      "step": 5670
    },
    {
      "epoch": 22.72,
      "grad_norm": 3.0130693912506104,
      "learning_rate": 2.7951219512195122e-06,
      "loss": 0.987,
      "step": 5680
    },
    {
      "epoch": 22.76,
      "grad_norm": 3.286449909210205,
      "learning_rate": 2.746341463414634e-06,
      "loss": 0.9646,
      "step": 5690
    },
    {
      "epoch": 22.8,
      "grad_norm": 3.1293604373931885,
      "learning_rate": 2.6975609756097564e-06,
      "loss": 1.0787,
      "step": 5700
    },
    {
      "epoch": 22.84,
      "grad_norm": 3.054807424545288,
      "learning_rate": 2.648780487804878e-06,
      "loss": 1.0119,
      "step": 5710
    },
    {
      "epoch": 22.88,
      "grad_norm": 2.8736655712127686,
      "learning_rate": 2.6e-06,
      "loss": 1.0345,
      "step": 5720
    },
    {
      "epoch": 22.92,
      "grad_norm": 3.1265709400177,
      "learning_rate": 2.551219512195122e-06,
      "loss": 0.9988,
      "step": 5730
    },
    {
      "epoch": 22.96,
      "grad_norm": 3.124298334121704,
      "learning_rate": 2.5024390243902442e-06,
      "loss": 1.0218,
      "step": 5740
    },
    {
      "epoch": 23.0,
      "grad_norm": 2.8386833667755127,
      "learning_rate": 2.4536585365853657e-06,
      "loss": 0.9929,
      "step": 5750
    },
    {
      "epoch": 23.04,
      "grad_norm": 3.1756722927093506,
      "learning_rate": 2.404878048780488e-06,
      "loss": 0.9844,
      "step": 5760
    },
    {
      "epoch": 23.08,
      "grad_norm": 3.0452067852020264,
      "learning_rate": 2.35609756097561e-06,
      "loss": 0.9358,
      "step": 5770
    },
    {
      "epoch": 23.12,
      "grad_norm": 3.0898048877716064,
      "learning_rate": 2.3073170731707317e-06,
      "loss": 0.9539,
      "step": 5780
    },
    {
      "epoch": 23.16,
      "grad_norm": 2.6542203426361084,
      "learning_rate": 2.258536585365854e-06,
      "loss": 0.9695,
      "step": 5790
    },
    {
      "epoch": 23.2,
      "grad_norm": 2.9270401000976562,
      "learning_rate": 2.2097560975609754e-06,
      "loss": 0.9981,
      "step": 5800
    },
    {
      "epoch": 23.24,
      "grad_norm": 2.7642383575439453,
      "learning_rate": 2.1609756097560977e-06,
      "loss": 1.025,
      "step": 5810
    },
    {
      "epoch": 23.28,
      "grad_norm": 2.647831916809082,
      "learning_rate": 2.1121951219512195e-06,
      "loss": 0.9471,
      "step": 5820
    },
    {
      "epoch": 23.32,
      "grad_norm": 2.756711959838867,
      "learning_rate": 2.063414634146342e-06,
      "loss": 0.9135,
      "step": 5830
    },
    {
      "epoch": 23.36,
      "grad_norm": 2.853339910507202,
      "learning_rate": 2.0146341463414633e-06,
      "loss": 0.9729,
      "step": 5840
    },
    {
      "epoch": 23.4,
      "grad_norm": 2.8802545070648193,
      "learning_rate": 1.965853658536585e-06,
      "loss": 0.9932,
      "step": 5850
    },
    {
      "epoch": 23.44,
      "grad_norm": 2.629103899002075,
      "learning_rate": 1.9170731707317074e-06,
      "loss": 0.9446,
      "step": 5860
    },
    {
      "epoch": 23.48,
      "grad_norm": 2.8734283447265625,
      "learning_rate": 1.8682926829268295e-06,
      "loss": 0.9996,
      "step": 5870
    },
    {
      "epoch": 23.52,
      "grad_norm": 3.4117720127105713,
      "learning_rate": 1.8195121951219513e-06,
      "loss": 0.966,
      "step": 5880
    },
    {
      "epoch": 23.56,
      "grad_norm": 2.8636443614959717,
      "learning_rate": 1.7707317073170732e-06,
      "loss": 1.0002,
      "step": 5890
    },
    {
      "epoch": 23.6,
      "grad_norm": 2.7637314796447754,
      "learning_rate": 1.721951219512195e-06,
      "loss": 1.0386,
      "step": 5900
    },
    {
      "epoch": 23.64,
      "grad_norm": 3.092881202697754,
      "learning_rate": 1.6731707317073171e-06,
      "loss": 0.9825,
      "step": 5910
    },
    {
      "epoch": 23.68,
      "grad_norm": 2.9407460689544678,
      "learning_rate": 1.624390243902439e-06,
      "loss": 0.9493,
      "step": 5920
    },
    {
      "epoch": 23.72,
      "grad_norm": 2.8235557079315186,
      "learning_rate": 1.575609756097561e-06,
      "loss": 0.9122,
      "step": 5930
    },
    {
      "epoch": 23.76,
      "grad_norm": 2.9358999729156494,
      "learning_rate": 1.526829268292683e-06,
      "loss": 0.9256,
      "step": 5940
    },
    {
      "epoch": 23.8,
      "grad_norm": 2.7665371894836426,
      "learning_rate": 1.478048780487805e-06,
      "loss": 0.9723,
      "step": 5950
    },
    {
      "epoch": 23.84,
      "grad_norm": 2.9820220470428467,
      "learning_rate": 1.4292682926829269e-06,
      "loss": 1.0234,
      "step": 5960
    },
    {
      "epoch": 23.88,
      "grad_norm": 3.7958736419677734,
      "learning_rate": 1.3804878048780487e-06,
      "loss": 1.0473,
      "step": 5970
    },
    {
      "epoch": 23.92,
      "grad_norm": 2.874720573425293,
      "learning_rate": 1.3317073170731708e-06,
      "loss": 0.9279,
      "step": 5980
    },
    {
      "epoch": 23.96,
      "grad_norm": 3.249727725982666,
      "learning_rate": 1.2829268292682927e-06,
      "loss": 0.9728,
      "step": 5990
    },
    {
      "epoch": 24.0,
      "grad_norm": 2.7603352069854736,
      "learning_rate": 1.2341463414634147e-06,
      "loss": 1.022,
      "step": 6000
    }
  ],
  "logging_steps": 10,
  "max_steps": 6250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 25,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6952615768162304e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
