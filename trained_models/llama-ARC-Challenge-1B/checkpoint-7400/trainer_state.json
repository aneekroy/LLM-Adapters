{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 29.6,
  "eval_steps": 500,
  "global_step": 7400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04,
      "grad_norm": 1.0931264162063599,
      "learning_rate": 2.7e-06,
      "loss": 2.6921,
      "step": 10
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.3348956108093262,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 2.6629,
      "step": 20
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.0814627408981323,
      "learning_rate": 8.7e-06,
      "loss": 2.5683,
      "step": 30
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.2613462209701538,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 2.7488,
      "step": 40
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.0108643770217896,
      "learning_rate": 1.47e-05,
      "loss": 2.6478,
      "step": 50
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.2002874612808228,
      "learning_rate": 1.77e-05,
      "loss": 2.6847,
      "step": 60
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.673490047454834,
      "learning_rate": 2.07e-05,
      "loss": 2.5584,
      "step": 70
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.5276501178741455,
      "learning_rate": 2.37e-05,
      "loss": 2.3864,
      "step": 80
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.979907751083374,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.2457,
      "step": 90
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.0227901935577393,
      "learning_rate": 2.97e-05,
      "loss": 2.0587,
      "step": 100
    },
    {
      "epoch": 0.44,
      "grad_norm": 3.1365504264831543,
      "learning_rate": 2.9963513513513512e-05,
      "loss": 1.7926,
      "step": 110
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.5320205688476562,
      "learning_rate": 2.9922972972972972e-05,
      "loss": 1.6702,
      "step": 120
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.8896889686584473,
      "learning_rate": 2.9882432432432432e-05,
      "loss": 1.5061,
      "step": 130
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.9034868478775024,
      "learning_rate": 2.9841891891891892e-05,
      "loss": 1.3745,
      "step": 140
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.339062213897705,
      "learning_rate": 2.9801351351351352e-05,
      "loss": 1.2855,
      "step": 150
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.4030025005340576,
      "learning_rate": 2.9760810810810812e-05,
      "loss": 1.1779,
      "step": 160
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.7067049741744995,
      "learning_rate": 2.9720270270270272e-05,
      "loss": 1.1596,
      "step": 170
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.4823803901672363,
      "learning_rate": 2.9679729729729732e-05,
      "loss": 1.0882,
      "step": 180
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.1395207643508911,
      "learning_rate": 2.963918918918919e-05,
      "loss": 1.1182,
      "step": 190
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.1084072589874268,
      "learning_rate": 2.959864864864865e-05,
      "loss": 1.1754,
      "step": 200
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.7225207090377808,
      "learning_rate": 2.955810810810811e-05,
      "loss": 1.2067,
      "step": 210
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.5656486749649048,
      "learning_rate": 2.951756756756757e-05,
      "loss": 1.1197,
      "step": 220
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.2183152437210083,
      "learning_rate": 2.947702702702703e-05,
      "loss": 1.1773,
      "step": 230
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.2970390319824219,
      "learning_rate": 2.943648648648649e-05,
      "loss": 1.0476,
      "step": 240
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.3632463216781616,
      "learning_rate": 2.939594594594595e-05,
      "loss": 1.0378,
      "step": 250
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.4168075323104858,
      "learning_rate": 2.935540540540541e-05,
      "loss": 1.1304,
      "step": 260
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.2085808515548706,
      "learning_rate": 2.9314864864864865e-05,
      "loss": 1.0353,
      "step": 270
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.341692566871643,
      "learning_rate": 2.9274324324324325e-05,
      "loss": 1.088,
      "step": 280
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.0623589754104614,
      "learning_rate": 2.9233783783783786e-05,
      "loss": 1.0775,
      "step": 290
    },
    {
      "epoch": 1.2,
      "grad_norm": 2.7281851768493652,
      "learning_rate": 2.9193243243243246e-05,
      "loss": 1.0578,
      "step": 300
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.4263553619384766,
      "learning_rate": 2.9152702702702702e-05,
      "loss": 1.0761,
      "step": 310
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.3647888898849487,
      "learning_rate": 2.9112162162162162e-05,
      "loss": 1.0916,
      "step": 320
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.4959251880645752,
      "learning_rate": 2.9071621621621622e-05,
      "loss": 1.1071,
      "step": 330
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.3469465970993042,
      "learning_rate": 2.9031081081081082e-05,
      "loss": 1.1517,
      "step": 340
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.1237493753433228,
      "learning_rate": 2.899054054054054e-05,
      "loss": 1.0816,
      "step": 350
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.213716745376587,
      "learning_rate": 2.895e-05,
      "loss": 1.1131,
      "step": 360
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.8495614528656006,
      "learning_rate": 2.890945945945946e-05,
      "loss": 1.0302,
      "step": 370
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.1626802682876587,
      "learning_rate": 2.886891891891892e-05,
      "loss": 1.0287,
      "step": 380
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.5138953924179077,
      "learning_rate": 2.882837837837838e-05,
      "loss": 1.0434,
      "step": 390
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.3588043451309204,
      "learning_rate": 2.878783783783784e-05,
      "loss": 1.098,
      "step": 400
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.7499724626541138,
      "learning_rate": 2.87472972972973e-05,
      "loss": 1.1056,
      "step": 410
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.0739154815673828,
      "learning_rate": 2.870675675675676e-05,
      "loss": 1.1374,
      "step": 420
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.2179515361785889,
      "learning_rate": 2.8666216216216216e-05,
      "loss": 1.0918,
      "step": 430
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.172440767288208,
      "learning_rate": 2.8625675675675676e-05,
      "loss": 1.0305,
      "step": 440
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.5629267692565918,
      "learning_rate": 2.8585135135135136e-05,
      "loss": 1.1036,
      "step": 450
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.5615150928497314,
      "learning_rate": 2.8544594594594596e-05,
      "loss": 1.1086,
      "step": 460
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.2012321949005127,
      "learning_rate": 2.8504054054054056e-05,
      "loss": 1.0931,
      "step": 470
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.2184772491455078,
      "learning_rate": 2.8463513513513516e-05,
      "loss": 1.0813,
      "step": 480
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.3534032106399536,
      "learning_rate": 2.8422972972972976e-05,
      "loss": 1.0707,
      "step": 490
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.2654935121536255,
      "learning_rate": 2.8382432432432436e-05,
      "loss": 1.133,
      "step": 500
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.0881060361862183,
      "learning_rate": 2.8341891891891893e-05,
      "loss": 1.1106,
      "step": 510
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.819575309753418,
      "learning_rate": 2.8301351351351353e-05,
      "loss": 1.1274,
      "step": 520
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.4147822856903076,
      "learning_rate": 2.826081081081081e-05,
      "loss": 1.1297,
      "step": 530
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.4534454345703125,
      "learning_rate": 2.822027027027027e-05,
      "loss": 1.139,
      "step": 540
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.503135323524475,
      "learning_rate": 2.817972972972973e-05,
      "loss": 1.1055,
      "step": 550
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.3483574390411377,
      "learning_rate": 2.813918918918919e-05,
      "loss": 1.037,
      "step": 560
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 1.3311423063278198,
      "learning_rate": 2.809864864864865e-05,
      "loss": 1.1294,
      "step": 570
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.5875862836837769,
      "learning_rate": 2.805810810810811e-05,
      "loss": 1.0885,
      "step": 580
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.6226550340652466,
      "learning_rate": 2.8017567567567566e-05,
      "loss": 1.1286,
      "step": 590
    },
    {
      "epoch": 2.4,
      "grad_norm": 2.2838332653045654,
      "learning_rate": 2.7977027027027026e-05,
      "loss": 1.1417,
      "step": 600
    },
    {
      "epoch": 2.44,
      "grad_norm": 1.4591162204742432,
      "learning_rate": 2.7936486486486486e-05,
      "loss": 1.039,
      "step": 610
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.4442206621170044,
      "learning_rate": 2.7895945945945946e-05,
      "loss": 1.072,
      "step": 620
    },
    {
      "epoch": 2.52,
      "grad_norm": 1.4048948287963867,
      "learning_rate": 2.7855405405405406e-05,
      "loss": 1.044,
      "step": 630
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.416338324546814,
      "learning_rate": 2.7814864864864866e-05,
      "loss": 1.1133,
      "step": 640
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.3247343301773071,
      "learning_rate": 2.7774324324324326e-05,
      "loss": 1.0773,
      "step": 650
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.5799968242645264,
      "learning_rate": 2.7733783783783786e-05,
      "loss": 1.0505,
      "step": 660
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.2009943723678589,
      "learning_rate": 2.7693243243243243e-05,
      "loss": 1.0298,
      "step": 670
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.7508103847503662,
      "learning_rate": 2.7652702702702703e-05,
      "loss": 1.0526,
      "step": 680
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.464398741722107,
      "learning_rate": 2.7612162162162163e-05,
      "loss": 1.051,
      "step": 690
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.278113842010498,
      "learning_rate": 2.7571621621621623e-05,
      "loss": 1.1257,
      "step": 700
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.3153778314590454,
      "learning_rate": 2.7531081081081083e-05,
      "loss": 1.0741,
      "step": 710
    },
    {
      "epoch": 2.88,
      "grad_norm": 1.4198377132415771,
      "learning_rate": 2.7490540540540543e-05,
      "loss": 1.1149,
      "step": 720
    },
    {
      "epoch": 2.92,
      "grad_norm": 1.364377737045288,
      "learning_rate": 2.7450000000000003e-05,
      "loss": 1.0338,
      "step": 730
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.2083368301391602,
      "learning_rate": 2.7409459459459463e-05,
      "loss": 1.003,
      "step": 740
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.2852270603179932,
      "learning_rate": 2.736891891891892e-05,
      "loss": 1.0142,
      "step": 750
    },
    {
      "epoch": 3.04,
      "grad_norm": 1.2367137670516968,
      "learning_rate": 2.7328378378378376e-05,
      "loss": 1.0385,
      "step": 760
    },
    {
      "epoch": 3.08,
      "grad_norm": 1.394768476486206,
      "learning_rate": 2.7287837837837836e-05,
      "loss": 1.0721,
      "step": 770
    },
    {
      "epoch": 3.12,
      "grad_norm": 1.4034327268600464,
      "learning_rate": 2.7247297297297297e-05,
      "loss": 1.049,
      "step": 780
    },
    {
      "epoch": 3.16,
      "grad_norm": 1.4501032829284668,
      "learning_rate": 2.7206756756756757e-05,
      "loss": 1.0378,
      "step": 790
    },
    {
      "epoch": 3.2,
      "grad_norm": 1.7568070888519287,
      "learning_rate": 2.7166216216216217e-05,
      "loss": 1.1087,
      "step": 800
    },
    {
      "epoch": 3.24,
      "grad_norm": 1.3652344942092896,
      "learning_rate": 2.7125675675675677e-05,
      "loss": 1.0729,
      "step": 810
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 1.2520252466201782,
      "learning_rate": 2.7085135135135137e-05,
      "loss": 1.0208,
      "step": 820
    },
    {
      "epoch": 3.32,
      "grad_norm": 1.3689733743667603,
      "learning_rate": 2.7044594594594593e-05,
      "loss": 1.0245,
      "step": 830
    },
    {
      "epoch": 3.36,
      "grad_norm": 1.4480879306793213,
      "learning_rate": 2.7004054054054053e-05,
      "loss": 1.0983,
      "step": 840
    },
    {
      "epoch": 3.4,
      "grad_norm": 1.7063995599746704,
      "learning_rate": 2.6963513513513513e-05,
      "loss": 1.0957,
      "step": 850
    },
    {
      "epoch": 3.44,
      "grad_norm": 1.4597467184066772,
      "learning_rate": 2.6922972972972973e-05,
      "loss": 1.0546,
      "step": 860
    },
    {
      "epoch": 3.48,
      "grad_norm": 1.699051856994629,
      "learning_rate": 2.6882432432432433e-05,
      "loss": 1.109,
      "step": 870
    },
    {
      "epoch": 3.52,
      "grad_norm": 1.3525722026824951,
      "learning_rate": 2.6841891891891893e-05,
      "loss": 1.0986,
      "step": 880
    },
    {
      "epoch": 3.56,
      "grad_norm": 1.5673205852508545,
      "learning_rate": 2.6801351351351353e-05,
      "loss": 1.055,
      "step": 890
    },
    {
      "epoch": 3.6,
      "grad_norm": 1.6411473751068115,
      "learning_rate": 2.6760810810810814e-05,
      "loss": 1.1316,
      "step": 900
    },
    {
      "epoch": 3.64,
      "grad_norm": 1.4709174633026123,
      "learning_rate": 2.672027027027027e-05,
      "loss": 1.0118,
      "step": 910
    },
    {
      "epoch": 3.68,
      "grad_norm": 1.3273779153823853,
      "learning_rate": 2.667972972972973e-05,
      "loss": 1.029,
      "step": 920
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 1.5244859457015991,
      "learning_rate": 2.663918918918919e-05,
      "loss": 1.0646,
      "step": 930
    },
    {
      "epoch": 3.76,
      "grad_norm": 1.5658011436462402,
      "learning_rate": 2.659864864864865e-05,
      "loss": 0.9591,
      "step": 940
    },
    {
      "epoch": 3.8,
      "grad_norm": 1.7117252349853516,
      "learning_rate": 2.655810810810811e-05,
      "loss": 1.053,
      "step": 950
    },
    {
      "epoch": 3.84,
      "grad_norm": 1.4905455112457275,
      "learning_rate": 2.651756756756757e-05,
      "loss": 1.0042,
      "step": 960
    },
    {
      "epoch": 3.88,
      "grad_norm": 1.4099926948547363,
      "learning_rate": 2.647702702702703e-05,
      "loss": 1.1304,
      "step": 970
    },
    {
      "epoch": 3.92,
      "grad_norm": 1.301185131072998,
      "learning_rate": 2.6436486486486487e-05,
      "loss": 1.0838,
      "step": 980
    },
    {
      "epoch": 3.96,
      "grad_norm": 1.3458406925201416,
      "learning_rate": 2.6395945945945944e-05,
      "loss": 1.0992,
      "step": 990
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.5448764562606812,
      "learning_rate": 2.6355405405405404e-05,
      "loss": 1.0884,
      "step": 1000
    },
    {
      "epoch": 4.04,
      "grad_norm": 1.5792510509490967,
      "learning_rate": 2.6314864864864864e-05,
      "loss": 1.0139,
      "step": 1010
    },
    {
      "epoch": 4.08,
      "grad_norm": 1.3154048919677734,
      "learning_rate": 2.6274324324324324e-05,
      "loss": 1.1201,
      "step": 1020
    },
    {
      "epoch": 4.12,
      "grad_norm": 1.7549630403518677,
      "learning_rate": 2.6233783783783784e-05,
      "loss": 1.0358,
      "step": 1030
    },
    {
      "epoch": 4.16,
      "grad_norm": 1.9336252212524414,
      "learning_rate": 2.6193243243243244e-05,
      "loss": 1.0302,
      "step": 1040
    },
    {
      "epoch": 4.2,
      "grad_norm": 2.122990131378174,
      "learning_rate": 2.6152702702702704e-05,
      "loss": 1.0703,
      "step": 1050
    },
    {
      "epoch": 4.24,
      "grad_norm": 1.6940516233444214,
      "learning_rate": 2.6112162162162164e-05,
      "loss": 1.0743,
      "step": 1060
    },
    {
      "epoch": 4.28,
      "grad_norm": 1.7816587686538696,
      "learning_rate": 2.607162162162162e-05,
      "loss": 1.0603,
      "step": 1070
    },
    {
      "epoch": 4.32,
      "grad_norm": 1.4713245630264282,
      "learning_rate": 2.603108108108108e-05,
      "loss": 1.0711,
      "step": 1080
    },
    {
      "epoch": 4.36,
      "grad_norm": 2.208613395690918,
      "learning_rate": 2.599054054054054e-05,
      "loss": 1.0515,
      "step": 1090
    },
    {
      "epoch": 4.4,
      "grad_norm": 1.5937739610671997,
      "learning_rate": 2.595e-05,
      "loss": 1.133,
      "step": 1100
    },
    {
      "epoch": 4.44,
      "grad_norm": 1.5894999504089355,
      "learning_rate": 2.590945945945946e-05,
      "loss": 1.0325,
      "step": 1110
    },
    {
      "epoch": 4.48,
      "grad_norm": 1.533430814743042,
      "learning_rate": 2.586891891891892e-05,
      "loss": 1.0094,
      "step": 1120
    },
    {
      "epoch": 4.52,
      "grad_norm": 1.6294423341751099,
      "learning_rate": 2.582837837837838e-05,
      "loss": 1.0858,
      "step": 1130
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 1.8665839433670044,
      "learning_rate": 2.578783783783784e-05,
      "loss": 1.0868,
      "step": 1140
    },
    {
      "epoch": 4.6,
      "grad_norm": 1.7340439558029175,
      "learning_rate": 2.5747297297297297e-05,
      "loss": 1.0222,
      "step": 1150
    },
    {
      "epoch": 4.64,
      "grad_norm": 1.7609463930130005,
      "learning_rate": 2.5706756756756757e-05,
      "loss": 1.0325,
      "step": 1160
    },
    {
      "epoch": 4.68,
      "grad_norm": 1.7856075763702393,
      "learning_rate": 2.5666216216216217e-05,
      "loss": 1.0789,
      "step": 1170
    },
    {
      "epoch": 4.72,
      "grad_norm": 1.3927141427993774,
      "learning_rate": 2.5625675675675677e-05,
      "loss": 1.0483,
      "step": 1180
    },
    {
      "epoch": 4.76,
      "grad_norm": 1.5572612285614014,
      "learning_rate": 2.5585135135135137e-05,
      "loss": 1.1777,
      "step": 1190
    },
    {
      "epoch": 4.8,
      "grad_norm": 1.7420926094055176,
      "learning_rate": 2.5544594594594594e-05,
      "loss": 1.0777,
      "step": 1200
    },
    {
      "epoch": 4.84,
      "grad_norm": 1.4285906553268433,
      "learning_rate": 2.5504054054054054e-05,
      "loss": 0.9889,
      "step": 1210
    },
    {
      "epoch": 4.88,
      "grad_norm": 1.888342022895813,
      "learning_rate": 2.5463513513513514e-05,
      "loss": 1.0444,
      "step": 1220
    },
    {
      "epoch": 4.92,
      "grad_norm": 2.088263511657715,
      "learning_rate": 2.542297297297297e-05,
      "loss": 1.0842,
      "step": 1230
    },
    {
      "epoch": 4.96,
      "grad_norm": 2.167992353439331,
      "learning_rate": 2.538243243243243e-05,
      "loss": 1.0763,
      "step": 1240
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.5878697633743286,
      "learning_rate": 2.534189189189189e-05,
      "loss": 1.078,
      "step": 1250
    },
    {
      "epoch": 5.04,
      "grad_norm": 1.4114980697631836,
      "learning_rate": 2.530135135135135e-05,
      "loss": 0.9968,
      "step": 1260
    },
    {
      "epoch": 5.08,
      "grad_norm": 1.582615613937378,
      "learning_rate": 2.526081081081081e-05,
      "loss": 1.0537,
      "step": 1270
    },
    {
      "epoch": 5.12,
      "grad_norm": 1.7873281240463257,
      "learning_rate": 2.522027027027027e-05,
      "loss": 1.0937,
      "step": 1280
    },
    {
      "epoch": 5.16,
      "grad_norm": 1.4955072402954102,
      "learning_rate": 2.517972972972973e-05,
      "loss": 1.0294,
      "step": 1290
    },
    {
      "epoch": 5.2,
      "grad_norm": 1.6306354999542236,
      "learning_rate": 2.513918918918919e-05,
      "loss": 1.0725,
      "step": 1300
    },
    {
      "epoch": 5.24,
      "grad_norm": 1.6560006141662598,
      "learning_rate": 2.5098648648648648e-05,
      "loss": 1.001,
      "step": 1310
    },
    {
      "epoch": 5.28,
      "grad_norm": 1.5805485248565674,
      "learning_rate": 2.5058108108108108e-05,
      "loss": 1.0517,
      "step": 1320
    },
    {
      "epoch": 5.32,
      "grad_norm": 1.4654488563537598,
      "learning_rate": 2.5017567567567568e-05,
      "loss": 1.0965,
      "step": 1330
    },
    {
      "epoch": 5.36,
      "grad_norm": 1.6344740390777588,
      "learning_rate": 2.4977027027027028e-05,
      "loss": 1.0432,
      "step": 1340
    },
    {
      "epoch": 5.4,
      "grad_norm": 1.8297268152236938,
      "learning_rate": 2.4936486486486488e-05,
      "loss": 1.0285,
      "step": 1350
    },
    {
      "epoch": 5.44,
      "grad_norm": 1.555526852607727,
      "learning_rate": 2.4895945945945948e-05,
      "loss": 1.0747,
      "step": 1360
    },
    {
      "epoch": 5.48,
      "grad_norm": 1.615883708000183,
      "learning_rate": 2.4855405405405408e-05,
      "loss": 1.0548,
      "step": 1370
    },
    {
      "epoch": 5.52,
      "grad_norm": 1.8567390441894531,
      "learning_rate": 2.4814864864864868e-05,
      "loss": 1.0381,
      "step": 1380
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 1.875417709350586,
      "learning_rate": 2.4774324324324325e-05,
      "loss": 1.0447,
      "step": 1390
    },
    {
      "epoch": 5.6,
      "grad_norm": 1.668332815170288,
      "learning_rate": 2.4733783783783785e-05,
      "loss": 1.0576,
      "step": 1400
    },
    {
      "epoch": 5.64,
      "grad_norm": 2.4781405925750732,
      "learning_rate": 2.4693243243243245e-05,
      "loss": 1.0374,
      "step": 1410
    },
    {
      "epoch": 5.68,
      "grad_norm": 1.3274352550506592,
      "learning_rate": 2.4652702702702705e-05,
      "loss": 1.0265,
      "step": 1420
    },
    {
      "epoch": 5.72,
      "grad_norm": 1.5801197290420532,
      "learning_rate": 2.461216216216216e-05,
      "loss": 1.1375,
      "step": 1430
    },
    {
      "epoch": 5.76,
      "grad_norm": 1.816058874130249,
      "learning_rate": 2.457162162162162e-05,
      "loss": 1.0904,
      "step": 1440
    },
    {
      "epoch": 5.8,
      "grad_norm": 1.4916446208953857,
      "learning_rate": 2.453108108108108e-05,
      "loss": 1.0463,
      "step": 1450
    },
    {
      "epoch": 5.84,
      "grad_norm": 1.855538249015808,
      "learning_rate": 2.449054054054054e-05,
      "loss": 1.0468,
      "step": 1460
    },
    {
      "epoch": 5.88,
      "grad_norm": 1.6186453104019165,
      "learning_rate": 2.4449999999999998e-05,
      "loss": 1.0169,
      "step": 1470
    },
    {
      "epoch": 5.92,
      "grad_norm": 1.7818845510482788,
      "learning_rate": 2.4409459459459458e-05,
      "loss": 1.0995,
      "step": 1480
    },
    {
      "epoch": 5.96,
      "grad_norm": 1.8138619661331177,
      "learning_rate": 2.4368918918918918e-05,
      "loss": 1.0326,
      "step": 1490
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.7857749462127686,
      "learning_rate": 2.4328378378378378e-05,
      "loss": 1.0211,
      "step": 1500
    },
    {
      "epoch": 6.04,
      "grad_norm": 1.7830160856246948,
      "learning_rate": 2.4287837837837838e-05,
      "loss": 1.0259,
      "step": 1510
    },
    {
      "epoch": 6.08,
      "grad_norm": 1.7287027835845947,
      "learning_rate": 2.4247297297297298e-05,
      "loss": 1.058,
      "step": 1520
    },
    {
      "epoch": 6.12,
      "grad_norm": 2.7066564559936523,
      "learning_rate": 2.4206756756756758e-05,
      "loss": 1.0519,
      "step": 1530
    },
    {
      "epoch": 6.16,
      "grad_norm": 1.6077512502670288,
      "learning_rate": 2.4166216216216218e-05,
      "loss": 1.0368,
      "step": 1540
    },
    {
      "epoch": 6.2,
      "grad_norm": 1.7030162811279297,
      "learning_rate": 2.4125675675675675e-05,
      "loss": 1.0811,
      "step": 1550
    },
    {
      "epoch": 6.24,
      "grad_norm": 2.0065531730651855,
      "learning_rate": 2.4085135135135135e-05,
      "loss": 1.0612,
      "step": 1560
    },
    {
      "epoch": 6.28,
      "grad_norm": 1.7442837953567505,
      "learning_rate": 2.4044594594594595e-05,
      "loss": 1.0091,
      "step": 1570
    },
    {
      "epoch": 6.32,
      "grad_norm": 1.8645803928375244,
      "learning_rate": 2.4004054054054055e-05,
      "loss": 1.0221,
      "step": 1580
    },
    {
      "epoch": 6.36,
      "grad_norm": 1.8437674045562744,
      "learning_rate": 2.3963513513513515e-05,
      "loss": 1.0342,
      "step": 1590
    },
    {
      "epoch": 6.4,
      "grad_norm": 1.8754721879959106,
      "learning_rate": 2.3922972972972975e-05,
      "loss": 1.0016,
      "step": 1600
    },
    {
      "epoch": 6.44,
      "grad_norm": 1.8424700498580933,
      "learning_rate": 2.3882432432432435e-05,
      "loss": 0.9862,
      "step": 1610
    },
    {
      "epoch": 6.48,
      "grad_norm": 1.799903392791748,
      "learning_rate": 2.3841891891891895e-05,
      "loss": 1.0758,
      "step": 1620
    },
    {
      "epoch": 6.52,
      "grad_norm": 1.6947696208953857,
      "learning_rate": 2.3801351351351352e-05,
      "loss": 1.0712,
      "step": 1630
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 1.960034728050232,
      "learning_rate": 2.3760810810810812e-05,
      "loss": 1.0396,
      "step": 1640
    },
    {
      "epoch": 6.6,
      "grad_norm": 1.7000423669815063,
      "learning_rate": 2.372027027027027e-05,
      "loss": 1.066,
      "step": 1650
    },
    {
      "epoch": 6.64,
      "grad_norm": 1.6851661205291748,
      "learning_rate": 2.367972972972973e-05,
      "loss": 1.0152,
      "step": 1660
    },
    {
      "epoch": 6.68,
      "grad_norm": 1.6717034578323364,
      "learning_rate": 2.363918918918919e-05,
      "loss": 1.0153,
      "step": 1670
    },
    {
      "epoch": 6.72,
      "grad_norm": 2.0583035945892334,
      "learning_rate": 2.359864864864865e-05,
      "loss": 1.0627,
      "step": 1680
    },
    {
      "epoch": 6.76,
      "grad_norm": 1.5848143100738525,
      "learning_rate": 2.355810810810811e-05,
      "loss": 1.0483,
      "step": 1690
    },
    {
      "epoch": 6.8,
      "grad_norm": 1.8061250448226929,
      "learning_rate": 2.351756756756757e-05,
      "loss": 1.0189,
      "step": 1700
    },
    {
      "epoch": 6.84,
      "grad_norm": 1.6170462369918823,
      "learning_rate": 2.3477027027027025e-05,
      "loss": 1.0214,
      "step": 1710
    },
    {
      "epoch": 6.88,
      "grad_norm": 1.8538154363632202,
      "learning_rate": 2.3436486486486485e-05,
      "loss": 0.9902,
      "step": 1720
    },
    {
      "epoch": 6.92,
      "grad_norm": 1.9102532863616943,
      "learning_rate": 2.3395945945945945e-05,
      "loss": 1.021,
      "step": 1730
    },
    {
      "epoch": 6.96,
      "grad_norm": 1.99350106716156,
      "learning_rate": 2.3355405405405405e-05,
      "loss": 1.0276,
      "step": 1740
    },
    {
      "epoch": 7.0,
      "grad_norm": 1.7882616519927979,
      "learning_rate": 2.3314864864864865e-05,
      "loss": 0.9706,
      "step": 1750
    },
    {
      "epoch": 7.04,
      "grad_norm": 2.0380539894104004,
      "learning_rate": 2.3274324324324325e-05,
      "loss": 1.1168,
      "step": 1760
    },
    {
      "epoch": 7.08,
      "grad_norm": 1.743651270866394,
      "learning_rate": 2.3233783783783785e-05,
      "loss": 0.9881,
      "step": 1770
    },
    {
      "epoch": 7.12,
      "grad_norm": 1.9299659729003906,
      "learning_rate": 2.3193243243243245e-05,
      "loss": 1.037,
      "step": 1780
    },
    {
      "epoch": 7.16,
      "grad_norm": 1.5667310953140259,
      "learning_rate": 2.3152702702702705e-05,
      "loss": 1.0524,
      "step": 1790
    },
    {
      "epoch": 7.2,
      "grad_norm": 1.7346580028533936,
      "learning_rate": 2.3112162162162162e-05,
      "loss": 0.9847,
      "step": 1800
    },
    {
      "epoch": 7.24,
      "grad_norm": 1.6750229597091675,
      "learning_rate": 2.3071621621621622e-05,
      "loss": 1.0202,
      "step": 1810
    },
    {
      "epoch": 7.28,
      "grad_norm": 1.6282891035079956,
      "learning_rate": 2.3031081081081082e-05,
      "loss": 1.0413,
      "step": 1820
    },
    {
      "epoch": 7.32,
      "grad_norm": 2.1101508140563965,
      "learning_rate": 2.2990540540540542e-05,
      "loss": 1.0127,
      "step": 1830
    },
    {
      "epoch": 7.36,
      "grad_norm": 1.924953818321228,
      "learning_rate": 2.2950000000000002e-05,
      "loss": 1.0718,
      "step": 1840
    },
    {
      "epoch": 7.4,
      "grad_norm": 2.1216039657592773,
      "learning_rate": 2.2909459459459462e-05,
      "loss": 0.9874,
      "step": 1850
    },
    {
      "epoch": 7.44,
      "grad_norm": 1.9360507726669312,
      "learning_rate": 2.2868918918918922e-05,
      "loss": 1.0122,
      "step": 1860
    },
    {
      "epoch": 7.48,
      "grad_norm": 1.8250579833984375,
      "learning_rate": 2.2828378378378382e-05,
      "loss": 0.9808,
      "step": 1870
    },
    {
      "epoch": 7.52,
      "grad_norm": 1.9351491928100586,
      "learning_rate": 2.2787837837837836e-05,
      "loss": 1.1056,
      "step": 1880
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 1.8633705377578735,
      "learning_rate": 2.2747297297297296e-05,
      "loss": 1.064,
      "step": 1890
    },
    {
      "epoch": 7.6,
      "grad_norm": 1.8064600229263306,
      "learning_rate": 2.2706756756756756e-05,
      "loss": 1.0342,
      "step": 1900
    },
    {
      "epoch": 7.64,
      "grad_norm": 2.012455701828003,
      "learning_rate": 2.2666216216216216e-05,
      "loss": 1.0041,
      "step": 1910
    },
    {
      "epoch": 7.68,
      "grad_norm": 1.8332682847976685,
      "learning_rate": 2.2625675675675676e-05,
      "loss": 1.0513,
      "step": 1920
    },
    {
      "epoch": 7.72,
      "grad_norm": 1.9793258905410767,
      "learning_rate": 2.2585135135135136e-05,
      "loss": 1.0523,
      "step": 1930
    },
    {
      "epoch": 7.76,
      "grad_norm": 2.054452419281006,
      "learning_rate": 2.2544594594594596e-05,
      "loss": 1.0714,
      "step": 1940
    },
    {
      "epoch": 7.8,
      "grad_norm": 1.8992127180099487,
      "learning_rate": 2.2504054054054056e-05,
      "loss": 1.0035,
      "step": 1950
    },
    {
      "epoch": 7.84,
      "grad_norm": 1.9324427843093872,
      "learning_rate": 2.2463513513513512e-05,
      "loss": 0.9692,
      "step": 1960
    },
    {
      "epoch": 7.88,
      "grad_norm": 1.9098361730575562,
      "learning_rate": 2.2422972972972972e-05,
      "loss": 1.0364,
      "step": 1970
    },
    {
      "epoch": 7.92,
      "grad_norm": 1.919043779373169,
      "learning_rate": 2.2382432432432432e-05,
      "loss": 1.0585,
      "step": 1980
    },
    {
      "epoch": 7.96,
      "grad_norm": 1.7208080291748047,
      "learning_rate": 2.2341891891891892e-05,
      "loss": 1.0455,
      "step": 1990
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.8988206386566162,
      "learning_rate": 2.2301351351351353e-05,
      "loss": 1.1019,
      "step": 2000
    },
    {
      "epoch": 8.04,
      "grad_norm": 1.9313462972640991,
      "learning_rate": 2.2260810810810813e-05,
      "loss": 1.011,
      "step": 2010
    },
    {
      "epoch": 8.08,
      "grad_norm": 1.7356077432632446,
      "learning_rate": 2.2220270270270273e-05,
      "loss": 0.9856,
      "step": 2020
    },
    {
      "epoch": 8.12,
      "grad_norm": 1.8913874626159668,
      "learning_rate": 2.2179729729729733e-05,
      "loss": 0.9846,
      "step": 2030
    },
    {
      "epoch": 8.16,
      "grad_norm": 1.7344635725021362,
      "learning_rate": 2.213918918918919e-05,
      "loss": 1.0651,
      "step": 2040
    },
    {
      "epoch": 8.2,
      "grad_norm": 1.9047659635543823,
      "learning_rate": 2.209864864864865e-05,
      "loss": 1.022,
      "step": 2050
    },
    {
      "epoch": 8.24,
      "grad_norm": 2.0264487266540527,
      "learning_rate": 2.205810810810811e-05,
      "loss": 1.0038,
      "step": 2060
    },
    {
      "epoch": 8.28,
      "grad_norm": 1.8445197343826294,
      "learning_rate": 2.201756756756757e-05,
      "loss": 1.0915,
      "step": 2070
    },
    {
      "epoch": 8.32,
      "grad_norm": 1.7569189071655273,
      "learning_rate": 2.197702702702703e-05,
      "loss": 1.0465,
      "step": 2080
    },
    {
      "epoch": 8.36,
      "grad_norm": 2.279928207397461,
      "learning_rate": 2.193648648648649e-05,
      "loss": 1.0371,
      "step": 2090
    },
    {
      "epoch": 8.4,
      "grad_norm": 1.7871506214141846,
      "learning_rate": 2.1895945945945946e-05,
      "loss": 1.035,
      "step": 2100
    },
    {
      "epoch": 8.44,
      "grad_norm": 2.0707955360412598,
      "learning_rate": 2.1855405405405406e-05,
      "loss": 1.0143,
      "step": 2110
    },
    {
      "epoch": 8.48,
      "grad_norm": 2.018686294555664,
      "learning_rate": 2.1814864864864863e-05,
      "loss": 1.0035,
      "step": 2120
    },
    {
      "epoch": 8.52,
      "grad_norm": 1.7836072444915771,
      "learning_rate": 2.1774324324324323e-05,
      "loss": 1.0422,
      "step": 2130
    },
    {
      "epoch": 8.56,
      "grad_norm": 1.7841869592666626,
      "learning_rate": 2.1733783783783783e-05,
      "loss": 1.0064,
      "step": 2140
    },
    {
      "epoch": 8.6,
      "grad_norm": 2.011796474456787,
      "learning_rate": 2.1693243243243243e-05,
      "loss": 1.0531,
      "step": 2150
    },
    {
      "epoch": 8.64,
      "grad_norm": 1.9581096172332764,
      "learning_rate": 2.1652702702702703e-05,
      "loss": 1.0559,
      "step": 2160
    },
    {
      "epoch": 8.68,
      "grad_norm": 2.531367301940918,
      "learning_rate": 2.1612162162162163e-05,
      "loss": 1.041,
      "step": 2170
    },
    {
      "epoch": 8.72,
      "grad_norm": 1.9774680137634277,
      "learning_rate": 2.1571621621621623e-05,
      "loss": 1.021,
      "step": 2180
    },
    {
      "epoch": 8.76,
      "grad_norm": 2.268975019454956,
      "learning_rate": 2.1531081081081083e-05,
      "loss": 0.9585,
      "step": 2190
    },
    {
      "epoch": 8.8,
      "grad_norm": 1.7959719896316528,
      "learning_rate": 2.149054054054054e-05,
      "loss": 0.9899,
      "step": 2200
    },
    {
      "epoch": 8.84,
      "grad_norm": 1.8109136819839478,
      "learning_rate": 2.145e-05,
      "loss": 1.0296,
      "step": 2210
    },
    {
      "epoch": 8.88,
      "grad_norm": 1.8091000318527222,
      "learning_rate": 2.140945945945946e-05,
      "loss": 1.0334,
      "step": 2220
    },
    {
      "epoch": 8.92,
      "grad_norm": 2.045886754989624,
      "learning_rate": 2.136891891891892e-05,
      "loss": 1.0764,
      "step": 2230
    },
    {
      "epoch": 8.96,
      "grad_norm": 2.2018039226531982,
      "learning_rate": 2.132837837837838e-05,
      "loss": 1.0648,
      "step": 2240
    },
    {
      "epoch": 9.0,
      "grad_norm": 2.100388765335083,
      "learning_rate": 2.128783783783784e-05,
      "loss": 1.0785,
      "step": 2250
    },
    {
      "epoch": 9.04,
      "grad_norm": 1.87264883518219,
      "learning_rate": 2.12472972972973e-05,
      "loss": 1.0288,
      "step": 2260
    },
    {
      "epoch": 9.08,
      "grad_norm": 1.9530293941497803,
      "learning_rate": 2.120675675675676e-05,
      "loss": 1.1238,
      "step": 2270
    },
    {
      "epoch": 9.12,
      "grad_norm": 1.7120563983917236,
      "learning_rate": 2.1166216216216216e-05,
      "loss": 1.0212,
      "step": 2280
    },
    {
      "epoch": 9.16,
      "grad_norm": 2.4282665252685547,
      "learning_rate": 2.1125675675675676e-05,
      "loss": 0.9988,
      "step": 2290
    },
    {
      "epoch": 9.2,
      "grad_norm": 1.9810352325439453,
      "learning_rate": 2.1085135135135137e-05,
      "loss": 1.0418,
      "step": 2300
    },
    {
      "epoch": 9.24,
      "grad_norm": 2.1387276649475098,
      "learning_rate": 2.1044594594594597e-05,
      "loss": 1.0373,
      "step": 2310
    },
    {
      "epoch": 9.28,
      "grad_norm": 2.440870761871338,
      "learning_rate": 2.1004054054054057e-05,
      "loss": 1.0447,
      "step": 2320
    },
    {
      "epoch": 9.32,
      "grad_norm": 2.198458671569824,
      "learning_rate": 2.0963513513513513e-05,
      "loss": 1.0459,
      "step": 2330
    },
    {
      "epoch": 9.36,
      "grad_norm": 2.0030124187469482,
      "learning_rate": 2.0922972972972973e-05,
      "loss": 0.9938,
      "step": 2340
    },
    {
      "epoch": 9.4,
      "grad_norm": 1.878410816192627,
      "learning_rate": 2.0882432432432433e-05,
      "loss": 1.0788,
      "step": 2350
    },
    {
      "epoch": 9.44,
      "grad_norm": 1.8568061590194702,
      "learning_rate": 2.084189189189189e-05,
      "loss": 1.0172,
      "step": 2360
    },
    {
      "epoch": 9.48,
      "grad_norm": 2.305119514465332,
      "learning_rate": 2.080135135135135e-05,
      "loss": 1.0161,
      "step": 2370
    },
    {
      "epoch": 9.52,
      "grad_norm": 1.9548336267471313,
      "learning_rate": 2.076081081081081e-05,
      "loss": 1.0134,
      "step": 2380
    },
    {
      "epoch": 9.56,
      "grad_norm": 1.980448842048645,
      "learning_rate": 2.072027027027027e-05,
      "loss": 1.0267,
      "step": 2390
    },
    {
      "epoch": 9.6,
      "grad_norm": 2.085681915283203,
      "learning_rate": 2.067972972972973e-05,
      "loss": 1.0501,
      "step": 2400
    },
    {
      "epoch": 9.64,
      "grad_norm": 1.9577628374099731,
      "learning_rate": 2.063918918918919e-05,
      "loss": 1.0399,
      "step": 2410
    },
    {
      "epoch": 9.68,
      "grad_norm": 1.8744648694992065,
      "learning_rate": 2.059864864864865e-05,
      "loss": 1.0447,
      "step": 2420
    },
    {
      "epoch": 9.72,
      "grad_norm": 2.4375898838043213,
      "learning_rate": 2.055810810810811e-05,
      "loss": 0.9752,
      "step": 2430
    },
    {
      "epoch": 9.76,
      "grad_norm": 2.2030551433563232,
      "learning_rate": 2.0517567567567567e-05,
      "loss": 0.9968,
      "step": 2440
    },
    {
      "epoch": 9.8,
      "grad_norm": 2.1148412227630615,
      "learning_rate": 2.0477027027027027e-05,
      "loss": 1.0605,
      "step": 2450
    },
    {
      "epoch": 9.84,
      "grad_norm": 2.012613296508789,
      "learning_rate": 2.0436486486486487e-05,
      "loss": 0.9841,
      "step": 2460
    },
    {
      "epoch": 9.88,
      "grad_norm": 2.387558937072754,
      "learning_rate": 2.0395945945945947e-05,
      "loss": 1.044,
      "step": 2470
    },
    {
      "epoch": 9.92,
      "grad_norm": 1.9929887056350708,
      "learning_rate": 2.0355405405405407e-05,
      "loss": 0.9672,
      "step": 2480
    },
    {
      "epoch": 9.96,
      "grad_norm": 2.1651699542999268,
      "learning_rate": 2.0314864864864867e-05,
      "loss": 0.9779,
      "step": 2490
    },
    {
      "epoch": 10.0,
      "grad_norm": 2.1593234539031982,
      "learning_rate": 2.0274324324324327e-05,
      "loss": 1.0571,
      "step": 2500
    },
    {
      "epoch": 10.04,
      "grad_norm": 2.2856342792510986,
      "learning_rate": 2.0233783783783787e-05,
      "loss": 1.0074,
      "step": 2510
    },
    {
      "epoch": 10.08,
      "grad_norm": 2.1467702388763428,
      "learning_rate": 2.0193243243243244e-05,
      "loss": 1.0012,
      "step": 2520
    },
    {
      "epoch": 10.12,
      "grad_norm": 1.9798389673233032,
      "learning_rate": 2.0152702702702704e-05,
      "loss": 1.0574,
      "step": 2530
    },
    {
      "epoch": 10.16,
      "grad_norm": 1.9316022396087646,
      "learning_rate": 2.0112162162162164e-05,
      "loss": 1.0946,
      "step": 2540
    },
    {
      "epoch": 10.2,
      "grad_norm": 2.2211296558380127,
      "learning_rate": 2.007162162162162e-05,
      "loss": 1.0848,
      "step": 2550
    },
    {
      "epoch": 10.24,
      "grad_norm": 2.3148388862609863,
      "learning_rate": 2.003108108108108e-05,
      "loss": 0.9796,
      "step": 2560
    },
    {
      "epoch": 10.28,
      "grad_norm": 1.6946560144424438,
      "learning_rate": 1.999054054054054e-05,
      "loss": 0.9925,
      "step": 2570
    },
    {
      "epoch": 10.32,
      "grad_norm": 2.9052255153656006,
      "learning_rate": 1.995e-05,
      "loss": 1.0762,
      "step": 2580
    },
    {
      "epoch": 10.36,
      "grad_norm": 2.1500258445739746,
      "learning_rate": 1.990945945945946e-05,
      "loss": 0.9991,
      "step": 2590
    },
    {
      "epoch": 10.4,
      "grad_norm": 2.7703566551208496,
      "learning_rate": 1.9868918918918917e-05,
      "loss": 1.0249,
      "step": 2600
    },
    {
      "epoch": 10.44,
      "grad_norm": 2.2869999408721924,
      "learning_rate": 1.9828378378378377e-05,
      "loss": 0.9906,
      "step": 2610
    },
    {
      "epoch": 10.48,
      "grad_norm": 2.2369866371154785,
      "learning_rate": 1.9787837837837837e-05,
      "loss": 0.971,
      "step": 2620
    },
    {
      "epoch": 10.52,
      "grad_norm": 2.2527401447296143,
      "learning_rate": 1.9747297297297297e-05,
      "loss": 1.0296,
      "step": 2630
    },
    {
      "epoch": 10.56,
      "grad_norm": 2.2227747440338135,
      "learning_rate": 1.9706756756756757e-05,
      "loss": 0.981,
      "step": 2640
    },
    {
      "epoch": 10.6,
      "grad_norm": 2.026672840118408,
      "learning_rate": 1.9666216216216217e-05,
      "loss": 0.9697,
      "step": 2650
    },
    {
      "epoch": 10.64,
      "grad_norm": 1.965103030204773,
      "learning_rate": 1.9625675675675677e-05,
      "loss": 0.9701,
      "step": 2660
    },
    {
      "epoch": 10.68,
      "grad_norm": 2.3882808685302734,
      "learning_rate": 1.9585135135135137e-05,
      "loss": 1.0424,
      "step": 2670
    },
    {
      "epoch": 10.72,
      "grad_norm": 1.8505041599273682,
      "learning_rate": 1.9544594594594594e-05,
      "loss": 1.0351,
      "step": 2680
    },
    {
      "epoch": 10.76,
      "grad_norm": 2.0682992935180664,
      "learning_rate": 1.950810810810811e-05,
      "loss": 0.9889,
      "step": 2690
    },
    {
      "epoch": 10.8,
      "grad_norm": 2.3117873668670654,
      "learning_rate": 1.946756756756757e-05,
      "loss": 1.0404,
      "step": 2700
    },
    {
      "epoch": 10.84,
      "grad_norm": 2.3565313816070557,
      "learning_rate": 1.942702702702703e-05,
      "loss": 1.03,
      "step": 2710
    },
    {
      "epoch": 10.88,
      "grad_norm": 2.1813721656799316,
      "learning_rate": 1.9386486486486485e-05,
      "loss": 1.0386,
      "step": 2720
    },
    {
      "epoch": 10.92,
      "grad_norm": 1.8320778608322144,
      "learning_rate": 1.9345945945945945e-05,
      "loss": 1.0573,
      "step": 2730
    },
    {
      "epoch": 10.96,
      "grad_norm": 2.8482978343963623,
      "learning_rate": 1.9305405405405405e-05,
      "loss": 1.0806,
      "step": 2740
    },
    {
      "epoch": 11.0,
      "grad_norm": 2.661045551300049,
      "learning_rate": 1.9264864864864865e-05,
      "loss": 0.969,
      "step": 2750
    },
    {
      "epoch": 11.04,
      "grad_norm": 2.3020777702331543,
      "learning_rate": 1.9224324324324325e-05,
      "loss": 1.0615,
      "step": 2760
    },
    {
      "epoch": 11.08,
      "grad_norm": 2.2580041885375977,
      "learning_rate": 1.9183783783783785e-05,
      "loss": 0.9974,
      "step": 2770
    },
    {
      "epoch": 11.12,
      "grad_norm": 2.0918288230895996,
      "learning_rate": 1.9143243243243245e-05,
      "loss": 1.0292,
      "step": 2780
    },
    {
      "epoch": 11.16,
      "grad_norm": 2.054227352142334,
      "learning_rate": 1.9102702702702705e-05,
      "loss": 1.0097,
      "step": 2790
    },
    {
      "epoch": 11.2,
      "grad_norm": 2.354130506515503,
      "learning_rate": 1.9062162162162162e-05,
      "loss": 1.0145,
      "step": 2800
    },
    {
      "epoch": 11.24,
      "grad_norm": 2.143738269805908,
      "learning_rate": 1.9021621621621622e-05,
      "loss": 0.9774,
      "step": 2810
    },
    {
      "epoch": 11.28,
      "grad_norm": 2.2686591148376465,
      "learning_rate": 1.8981081081081082e-05,
      "loss": 1.0442,
      "step": 2820
    },
    {
      "epoch": 11.32,
      "grad_norm": 2.4211854934692383,
      "learning_rate": 1.8940540540540542e-05,
      "loss": 1.0303,
      "step": 2830
    },
    {
      "epoch": 11.36,
      "grad_norm": 2.343273162841797,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 1.0389,
      "step": 2840
    },
    {
      "epoch": 11.4,
      "grad_norm": 2.0032291412353516,
      "learning_rate": 1.8859459459459462e-05,
      "loss": 1.0243,
      "step": 2850
    },
    {
      "epoch": 11.44,
      "grad_norm": 2.9356281757354736,
      "learning_rate": 1.881891891891892e-05,
      "loss": 0.9726,
      "step": 2860
    },
    {
      "epoch": 11.48,
      "grad_norm": 2.3072969913482666,
      "learning_rate": 1.877837837837838e-05,
      "loss": 1.0374,
      "step": 2870
    },
    {
      "epoch": 11.52,
      "grad_norm": 2.9386579990386963,
      "learning_rate": 1.8737837837837835e-05,
      "loss": 0.9965,
      "step": 2880
    },
    {
      "epoch": 11.56,
      "grad_norm": 2.1722819805145264,
      "learning_rate": 1.8697297297297295e-05,
      "loss": 0.9983,
      "step": 2890
    },
    {
      "epoch": 11.6,
      "grad_norm": 2.068202257156372,
      "learning_rate": 1.8656756756756755e-05,
      "loss": 1.0297,
      "step": 2900
    },
    {
      "epoch": 11.64,
      "grad_norm": 2.0747873783111572,
      "learning_rate": 1.8616216216216215e-05,
      "loss": 1.0203,
      "step": 2910
    },
    {
      "epoch": 11.68,
      "grad_norm": 2.04205322265625,
      "learning_rate": 1.8575675675675675e-05,
      "loss": 0.966,
      "step": 2920
    },
    {
      "epoch": 11.72,
      "grad_norm": 2.5886118412017822,
      "learning_rate": 1.8535135135135136e-05,
      "loss": 0.9872,
      "step": 2930
    },
    {
      "epoch": 11.76,
      "grad_norm": 2.172919750213623,
      "learning_rate": 1.8494594594594596e-05,
      "loss": 0.9892,
      "step": 2940
    },
    {
      "epoch": 11.8,
      "grad_norm": 2.08783221244812,
      "learning_rate": 1.8454054054054056e-05,
      "loss": 1.0341,
      "step": 2950
    },
    {
      "epoch": 11.84,
      "grad_norm": 2.15533709526062,
      "learning_rate": 1.8413513513513512e-05,
      "loss": 1.0057,
      "step": 2960
    },
    {
      "epoch": 11.88,
      "grad_norm": 2.483426809310913,
      "learning_rate": 1.8372972972972972e-05,
      "loss": 1.0351,
      "step": 2970
    },
    {
      "epoch": 11.92,
      "grad_norm": 2.339688301086426,
      "learning_rate": 1.8332432432432432e-05,
      "loss": 1.0306,
      "step": 2980
    },
    {
      "epoch": 11.96,
      "grad_norm": 2.5312225818634033,
      "learning_rate": 1.8291891891891892e-05,
      "loss": 0.9793,
      "step": 2990
    },
    {
      "epoch": 12.0,
      "grad_norm": 2.681302309036255,
      "learning_rate": 1.8251351351351352e-05,
      "loss": 1.0197,
      "step": 3000
    },
    {
      "epoch": 12.04,
      "grad_norm": 2.0051515102386475,
      "learning_rate": 1.8210810810810812e-05,
      "loss": 0.9488,
      "step": 3010
    },
    {
      "epoch": 12.08,
      "grad_norm": 2.0902273654937744,
      "learning_rate": 1.8170270270270272e-05,
      "loss": 0.9673,
      "step": 3020
    },
    {
      "epoch": 12.12,
      "grad_norm": 2.7685554027557373,
      "learning_rate": 1.8129729729729732e-05,
      "loss": 1.0169,
      "step": 3030
    },
    {
      "epoch": 12.16,
      "grad_norm": 2.44528865814209,
      "learning_rate": 1.808918918918919e-05,
      "loss": 1.0832,
      "step": 3040
    },
    {
      "epoch": 12.2,
      "grad_norm": 2.1614389419555664,
      "learning_rate": 1.804864864864865e-05,
      "loss": 1.0073,
      "step": 3050
    },
    {
      "epoch": 12.24,
      "grad_norm": 2.4301750659942627,
      "learning_rate": 1.800810810810811e-05,
      "loss": 1.1095,
      "step": 3060
    },
    {
      "epoch": 12.28,
      "grad_norm": 2.231187105178833,
      "learning_rate": 1.796756756756757e-05,
      "loss": 1.0246,
      "step": 3070
    },
    {
      "epoch": 12.32,
      "grad_norm": 2.245069980621338,
      "learning_rate": 1.792702702702703e-05,
      "loss": 1.0273,
      "step": 3080
    },
    {
      "epoch": 12.36,
      "grad_norm": 2.104050397872925,
      "learning_rate": 1.7886486486486486e-05,
      "loss": 1.0144,
      "step": 3090
    },
    {
      "epoch": 12.4,
      "grad_norm": 2.2905373573303223,
      "learning_rate": 1.7845945945945946e-05,
      "loss": 0.9451,
      "step": 3100
    },
    {
      "epoch": 12.44,
      "grad_norm": 2.3188490867614746,
      "learning_rate": 1.7805405405405406e-05,
      "loss": 1.0068,
      "step": 3110
    },
    {
      "epoch": 12.48,
      "grad_norm": 2.4195377826690674,
      "learning_rate": 1.7764864864864863e-05,
      "loss": 1.002,
      "step": 3120
    },
    {
      "epoch": 12.52,
      "grad_norm": 2.3720338344573975,
      "learning_rate": 1.7724324324324323e-05,
      "loss": 0.993,
      "step": 3130
    },
    {
      "epoch": 12.56,
      "grad_norm": 2.277360200881958,
      "learning_rate": 1.7683783783783783e-05,
      "loss": 1.0357,
      "step": 3140
    },
    {
      "epoch": 12.6,
      "grad_norm": 2.191610813140869,
      "learning_rate": 1.7643243243243243e-05,
      "loss": 0.959,
      "step": 3150
    },
    {
      "epoch": 12.64,
      "grad_norm": 2.1988353729248047,
      "learning_rate": 1.7602702702702703e-05,
      "loss": 0.9735,
      "step": 3160
    },
    {
      "epoch": 12.68,
      "grad_norm": 2.1056694984436035,
      "learning_rate": 1.7562162162162163e-05,
      "loss": 0.9767,
      "step": 3170
    },
    {
      "epoch": 12.72,
      "grad_norm": 2.0855188369750977,
      "learning_rate": 1.7521621621621623e-05,
      "loss": 1.0036,
      "step": 3180
    },
    {
      "epoch": 12.76,
      "grad_norm": 2.2737514972686768,
      "learning_rate": 1.7481081081081083e-05,
      "loss": 0.9382,
      "step": 3190
    },
    {
      "epoch": 12.8,
      "grad_norm": 2.4236650466918945,
      "learning_rate": 1.744054054054054e-05,
      "loss": 0.9653,
      "step": 3200
    },
    {
      "epoch": 12.84,
      "grad_norm": 2.6990878582000732,
      "learning_rate": 1.74e-05,
      "loss": 1.0372,
      "step": 3210
    },
    {
      "epoch": 12.88,
      "grad_norm": 2.112499237060547,
      "learning_rate": 1.735945945945946e-05,
      "loss": 0.9262,
      "step": 3220
    },
    {
      "epoch": 12.92,
      "grad_norm": 2.199985980987549,
      "learning_rate": 1.731891891891892e-05,
      "loss": 0.9615,
      "step": 3230
    },
    {
      "epoch": 12.96,
      "grad_norm": 2.445866346359253,
      "learning_rate": 1.727837837837838e-05,
      "loss": 1.0166,
      "step": 3240
    },
    {
      "epoch": 13.0,
      "grad_norm": 2.563457489013672,
      "learning_rate": 1.723783783783784e-05,
      "loss": 0.9838,
      "step": 3250
    },
    {
      "epoch": 13.04,
      "grad_norm": 3.696180820465088,
      "learning_rate": 1.71972972972973e-05,
      "loss": 1.0943,
      "step": 3260
    },
    {
      "epoch": 13.08,
      "grad_norm": 3.081249952316284,
      "learning_rate": 1.715675675675676e-05,
      "loss": 0.9869,
      "step": 3270
    },
    {
      "epoch": 13.12,
      "grad_norm": 2.571688413619995,
      "learning_rate": 1.7116216216216216e-05,
      "loss": 1.0265,
      "step": 3280
    },
    {
      "epoch": 13.16,
      "grad_norm": 2.406726121902466,
      "learning_rate": 1.7075675675675676e-05,
      "loss": 1.0084,
      "step": 3290
    },
    {
      "epoch": 13.2,
      "grad_norm": 2.126612663269043,
      "learning_rate": 1.7035135135135136e-05,
      "loss": 1.0912,
      "step": 3300
    },
    {
      "epoch": 13.24,
      "grad_norm": 2.327993392944336,
      "learning_rate": 1.6994594594594593e-05,
      "loss": 0.9888,
      "step": 3310
    },
    {
      "epoch": 13.28,
      "grad_norm": 2.2401673793792725,
      "learning_rate": 1.6954054054054053e-05,
      "loss": 1.0194,
      "step": 3320
    },
    {
      "epoch": 13.32,
      "grad_norm": 2.3891515731811523,
      "learning_rate": 1.6913513513513513e-05,
      "loss": 1.0298,
      "step": 3330
    },
    {
      "epoch": 13.36,
      "grad_norm": 2.4946134090423584,
      "learning_rate": 1.6872972972972973e-05,
      "loss": 0.9513,
      "step": 3340
    },
    {
      "epoch": 13.4,
      "grad_norm": 2.456974744796753,
      "learning_rate": 1.6832432432432433e-05,
      "loss": 1.0094,
      "step": 3350
    },
    {
      "epoch": 13.44,
      "grad_norm": 2.962124824523926,
      "learning_rate": 1.679189189189189e-05,
      "loss": 0.9873,
      "step": 3360
    },
    {
      "epoch": 13.48,
      "grad_norm": 2.074873447418213,
      "learning_rate": 1.675135135135135e-05,
      "loss": 0.9812,
      "step": 3370
    },
    {
      "epoch": 13.52,
      "grad_norm": 2.564842939376831,
      "learning_rate": 1.671081081081081e-05,
      "loss": 1.0085,
      "step": 3380
    },
    {
      "epoch": 13.56,
      "grad_norm": 2.5129454135894775,
      "learning_rate": 1.667027027027027e-05,
      "loss": 1.0311,
      "step": 3390
    },
    {
      "epoch": 13.6,
      "grad_norm": 2.570812702178955,
      "learning_rate": 1.662972972972973e-05,
      "loss": 0.9828,
      "step": 3400
    },
    {
      "epoch": 13.64,
      "grad_norm": 2.4190831184387207,
      "learning_rate": 1.658918918918919e-05,
      "loss": 1.0029,
      "step": 3410
    },
    {
      "epoch": 13.68,
      "grad_norm": 2.6787664890289307,
      "learning_rate": 1.654864864864865e-05,
      "loss": 1.0483,
      "step": 3420
    },
    {
      "epoch": 13.72,
      "grad_norm": 2.2204504013061523,
      "learning_rate": 1.650810810810811e-05,
      "loss": 1.0494,
      "step": 3430
    },
    {
      "epoch": 13.76,
      "grad_norm": 2.147315740585327,
      "learning_rate": 1.646756756756757e-05,
      "loss": 1.0018,
      "step": 3440
    },
    {
      "epoch": 13.8,
      "grad_norm": 2.1053850650787354,
      "learning_rate": 1.6427027027027027e-05,
      "loss": 0.9631,
      "step": 3450
    },
    {
      "epoch": 13.84,
      "grad_norm": 2.504837989807129,
      "learning_rate": 1.6386486486486487e-05,
      "loss": 0.9843,
      "step": 3460
    },
    {
      "epoch": 13.88,
      "grad_norm": 2.7635085582733154,
      "learning_rate": 1.6345945945945947e-05,
      "loss": 0.9783,
      "step": 3470
    },
    {
      "epoch": 13.92,
      "grad_norm": 2.347792625427246,
      "learning_rate": 1.6305405405405407e-05,
      "loss": 0.9933,
      "step": 3480
    },
    {
      "epoch": 13.96,
      "grad_norm": 2.4662375450134277,
      "learning_rate": 1.6264864864864867e-05,
      "loss": 1.0356,
      "step": 3490
    },
    {
      "epoch": 14.0,
      "grad_norm": 2.46684193611145,
      "learning_rate": 1.6224324324324327e-05,
      "loss": 0.9641,
      "step": 3500
    },
    {
      "epoch": 14.04,
      "grad_norm": 2.5315611362457275,
      "learning_rate": 1.6183783783783787e-05,
      "loss": 0.9975,
      "step": 3510
    },
    {
      "epoch": 14.08,
      "grad_norm": 2.542607069015503,
      "learning_rate": 1.6143243243243247e-05,
      "loss": 1.0522,
      "step": 3520
    },
    {
      "epoch": 14.12,
      "grad_norm": 2.699225902557373,
      "learning_rate": 1.6102702702702703e-05,
      "loss": 1.0075,
      "step": 3530
    },
    {
      "epoch": 14.16,
      "grad_norm": 2.5072803497314453,
      "learning_rate": 1.606216216216216e-05,
      "loss": 1.0671,
      "step": 3540
    },
    {
      "epoch": 14.2,
      "grad_norm": 2.175577402114868,
      "learning_rate": 1.602162162162162e-05,
      "loss": 0.9617,
      "step": 3550
    },
    {
      "epoch": 14.24,
      "grad_norm": 2.737016201019287,
      "learning_rate": 1.598108108108108e-05,
      "loss": 1.0113,
      "step": 3560
    },
    {
      "epoch": 14.28,
      "grad_norm": 2.4048211574554443,
      "learning_rate": 1.594054054054054e-05,
      "loss": 0.936,
      "step": 3570
    },
    {
      "epoch": 14.32,
      "grad_norm": 2.323291301727295,
      "learning_rate": 1.59e-05,
      "loss": 0.9742,
      "step": 3580
    },
    {
      "epoch": 14.36,
      "grad_norm": 2.5037364959716797,
      "learning_rate": 1.585945945945946e-05,
      "loss": 1.031,
      "step": 3590
    },
    {
      "epoch": 14.4,
      "grad_norm": 2.224902629852295,
      "learning_rate": 1.581891891891892e-05,
      "loss": 1.0271,
      "step": 3600
    },
    {
      "epoch": 14.44,
      "grad_norm": 2.4103167057037354,
      "learning_rate": 1.5778378378378377e-05,
      "loss": 1.1003,
      "step": 3610
    },
    {
      "epoch": 14.48,
      "grad_norm": 2.460414171218872,
      "learning_rate": 1.5737837837837837e-05,
      "loss": 1.0449,
      "step": 3620
    },
    {
      "epoch": 14.52,
      "grad_norm": 2.3952324390411377,
      "learning_rate": 1.5697297297297297e-05,
      "loss": 0.9595,
      "step": 3630
    },
    {
      "epoch": 14.56,
      "grad_norm": 2.479400157928467,
      "learning_rate": 1.5656756756756757e-05,
      "loss": 1.0776,
      "step": 3640
    },
    {
      "epoch": 14.6,
      "grad_norm": 2.227672815322876,
      "learning_rate": 1.5616216216216217e-05,
      "loss": 0.9565,
      "step": 3650
    },
    {
      "epoch": 14.64,
      "grad_norm": 2.523547410964966,
      "learning_rate": 1.5575675675675677e-05,
      "loss": 1.0035,
      "step": 3660
    },
    {
      "epoch": 14.68,
      "grad_norm": 2.832606315612793,
      "learning_rate": 1.5535135135135137e-05,
      "loss": 0.9724,
      "step": 3670
    },
    {
      "epoch": 14.72,
      "grad_norm": 2.2312662601470947,
      "learning_rate": 1.5494594594594597e-05,
      "loss": 0.972,
      "step": 3680
    },
    {
      "epoch": 14.76,
      "grad_norm": 2.3566133975982666,
      "learning_rate": 1.5454054054054054e-05,
      "loss": 1.0103,
      "step": 3690
    },
    {
      "epoch": 14.8,
      "grad_norm": 2.2126803398132324,
      "learning_rate": 1.5413513513513514e-05,
      "loss": 0.953,
      "step": 3700
    },
    {
      "epoch": 14.84,
      "grad_norm": 2.3654682636260986,
      "learning_rate": 1.5372972972972974e-05,
      "loss": 0.9926,
      "step": 3710
    },
    {
      "epoch": 14.88,
      "grad_norm": 2.452854871749878,
      "learning_rate": 1.5332432432432434e-05,
      "loss": 0.9434,
      "step": 3720
    },
    {
      "epoch": 14.92,
      "grad_norm": 2.6353719234466553,
      "learning_rate": 1.5291891891891894e-05,
      "loss": 1.0105,
      "step": 3730
    },
    {
      "epoch": 14.96,
      "grad_norm": 2.5768425464630127,
      "learning_rate": 1.5251351351351352e-05,
      "loss": 1.0699,
      "step": 3740
    },
    {
      "epoch": 15.0,
      "grad_norm": 2.353853464126587,
      "learning_rate": 1.5210810810810812e-05,
      "loss": 0.9974,
      "step": 3750
    },
    {
      "epoch": 15.04,
      "grad_norm": 2.6571507453918457,
      "learning_rate": 1.5170270270270272e-05,
      "loss": 1.0194,
      "step": 3760
    },
    {
      "epoch": 15.08,
      "grad_norm": 2.5449907779693604,
      "learning_rate": 1.5129729729729729e-05,
      "loss": 0.9914,
      "step": 3770
    },
    {
      "epoch": 15.12,
      "grad_norm": 2.359847068786621,
      "learning_rate": 1.5089189189189189e-05,
      "loss": 1.0161,
      "step": 3780
    },
    {
      "epoch": 15.16,
      "grad_norm": 2.468275547027588,
      "learning_rate": 1.5048648648648649e-05,
      "loss": 0.9812,
      "step": 3790
    },
    {
      "epoch": 15.2,
      "grad_norm": 2.638338565826416,
      "learning_rate": 1.5008108108108109e-05,
      "loss": 0.9699,
      "step": 3800
    },
    {
      "epoch": 15.24,
      "grad_norm": 3.132894277572632,
      "learning_rate": 1.4967567567567569e-05,
      "loss": 0.995,
      "step": 3810
    },
    {
      "epoch": 15.28,
      "grad_norm": 3.0899672508239746,
      "learning_rate": 1.4927027027027027e-05,
      "loss": 0.9762,
      "step": 3820
    },
    {
      "epoch": 15.32,
      "grad_norm": 3.1542251110076904,
      "learning_rate": 1.4886486486486486e-05,
      "loss": 0.976,
      "step": 3830
    },
    {
      "epoch": 15.36,
      "grad_norm": 2.789340019226074,
      "learning_rate": 1.4845945945945946e-05,
      "loss": 0.9441,
      "step": 3840
    },
    {
      "epoch": 15.4,
      "grad_norm": 2.845726490020752,
      "learning_rate": 1.4805405405405406e-05,
      "loss": 0.951,
      "step": 3850
    },
    {
      "epoch": 15.44,
      "grad_norm": 2.7843782901763916,
      "learning_rate": 1.4764864864864866e-05,
      "loss": 0.9235,
      "step": 3860
    },
    {
      "epoch": 15.48,
      "grad_norm": 2.861250400543213,
      "learning_rate": 1.4724324324324324e-05,
      "loss": 0.9384,
      "step": 3870
    },
    {
      "epoch": 15.52,
      "grad_norm": 2.725334405899048,
      "learning_rate": 1.4683783783783784e-05,
      "loss": 0.9469,
      "step": 3880
    },
    {
      "epoch": 15.56,
      "grad_norm": 2.6078593730926514,
      "learning_rate": 1.4643243243243244e-05,
      "loss": 0.9988,
      "step": 3890
    },
    {
      "epoch": 15.6,
      "grad_norm": 2.6812939643859863,
      "learning_rate": 1.4602702702702704e-05,
      "loss": 1.0281,
      "step": 3900
    },
    {
      "epoch": 15.64,
      "grad_norm": 2.7669198513031006,
      "learning_rate": 1.4562162162162163e-05,
      "loss": 1.0447,
      "step": 3910
    },
    {
      "epoch": 15.68,
      "grad_norm": 3.3740859031677246,
      "learning_rate": 1.4521621621621623e-05,
      "loss": 1.0252,
      "step": 3920
    },
    {
      "epoch": 15.72,
      "grad_norm": 2.6406548023223877,
      "learning_rate": 1.4481081081081081e-05,
      "loss": 1.01,
      "step": 3930
    },
    {
      "epoch": 15.76,
      "grad_norm": 3.5808215141296387,
      "learning_rate": 1.4440540540540541e-05,
      "loss": 1.0383,
      "step": 3940
    },
    {
      "epoch": 15.8,
      "grad_norm": 2.718170404434204,
      "learning_rate": 1.44e-05,
      "loss": 0.9781,
      "step": 3950
    },
    {
      "epoch": 15.84,
      "grad_norm": 2.6863250732421875,
      "learning_rate": 1.435945945945946e-05,
      "loss": 0.9511,
      "step": 3960
    },
    {
      "epoch": 15.88,
      "grad_norm": 2.725783348083496,
      "learning_rate": 1.431891891891892e-05,
      "loss": 0.9902,
      "step": 3970
    },
    {
      "epoch": 15.92,
      "grad_norm": 2.494182586669922,
      "learning_rate": 1.427837837837838e-05,
      "loss": 0.9829,
      "step": 3980
    },
    {
      "epoch": 15.96,
      "grad_norm": 3.13614559173584,
      "learning_rate": 1.4237837837837838e-05,
      "loss": 0.9862,
      "step": 3990
    },
    {
      "epoch": 16.0,
      "grad_norm": 2.5403404235839844,
      "learning_rate": 1.4197297297297298e-05,
      "loss": 0.94,
      "step": 4000
    },
    {
      "epoch": 16.04,
      "grad_norm": 2.4825198650360107,
      "learning_rate": 1.4156756756756758e-05,
      "loss": 0.9745,
      "step": 4010
    },
    {
      "epoch": 16.08,
      "grad_norm": 2.9094290733337402,
      "learning_rate": 1.4116216216216218e-05,
      "loss": 1.0427,
      "step": 4020
    },
    {
      "epoch": 16.12,
      "grad_norm": 3.7421436309814453,
      "learning_rate": 1.4075675675675676e-05,
      "loss": 1.0767,
      "step": 4030
    },
    {
      "epoch": 16.16,
      "grad_norm": 2.6979897022247314,
      "learning_rate": 1.4035135135135135e-05,
      "loss": 0.9442,
      "step": 4040
    },
    {
      "epoch": 16.2,
      "grad_norm": 2.4070942401885986,
      "learning_rate": 1.3994594594594595e-05,
      "loss": 0.956,
      "step": 4050
    },
    {
      "epoch": 16.24,
      "grad_norm": 2.8553168773651123,
      "learning_rate": 1.3954054054054055e-05,
      "loss": 0.9588,
      "step": 4060
    },
    {
      "epoch": 16.28,
      "grad_norm": 2.6743345260620117,
      "learning_rate": 1.3913513513513513e-05,
      "loss": 0.9565,
      "step": 4070
    },
    {
      "epoch": 16.32,
      "grad_norm": 2.902702569961548,
      "learning_rate": 1.3872972972972973e-05,
      "loss": 0.9426,
      "step": 4080
    },
    {
      "epoch": 16.36,
      "grad_norm": 2.5935251712799072,
      "learning_rate": 1.3832432432432433e-05,
      "loss": 0.9492,
      "step": 4090
    },
    {
      "epoch": 16.4,
      "grad_norm": 2.8585522174835205,
      "learning_rate": 1.3791891891891893e-05,
      "loss": 1.0081,
      "step": 4100
    },
    {
      "epoch": 16.44,
      "grad_norm": 2.7349021434783936,
      "learning_rate": 1.3751351351351351e-05,
      "loss": 1.0023,
      "step": 4110
    },
    {
      "epoch": 16.48,
      "grad_norm": 2.8847973346710205,
      "learning_rate": 1.3710810810810811e-05,
      "loss": 0.9967,
      "step": 4120
    },
    {
      "epoch": 16.52,
      "grad_norm": 2.971470832824707,
      "learning_rate": 1.3670270270270271e-05,
      "loss": 1.0506,
      "step": 4130
    },
    {
      "epoch": 16.56,
      "grad_norm": 3.0734567642211914,
      "learning_rate": 1.3629729729729731e-05,
      "loss": 1.0526,
      "step": 4140
    },
    {
      "epoch": 16.6,
      "grad_norm": 2.559835910797119,
      "learning_rate": 1.358918918918919e-05,
      "loss": 0.9933,
      "step": 4150
    },
    {
      "epoch": 16.64,
      "grad_norm": 2.6839005947113037,
      "learning_rate": 1.3548648648648648e-05,
      "loss": 0.9868,
      "step": 4160
    },
    {
      "epoch": 16.68,
      "grad_norm": 2.5105702877044678,
      "learning_rate": 1.3508108108108108e-05,
      "loss": 0.978,
      "step": 4170
    },
    {
      "epoch": 16.72,
      "grad_norm": 3.0678954124450684,
      "learning_rate": 1.3467567567567568e-05,
      "loss": 1.0327,
      "step": 4180
    },
    {
      "epoch": 16.76,
      "grad_norm": 2.8636763095855713,
      "learning_rate": 1.3427027027027027e-05,
      "loss": 1.0164,
      "step": 4190
    },
    {
      "epoch": 16.8,
      "grad_norm": 2.734701156616211,
      "learning_rate": 1.3386486486486487e-05,
      "loss": 1.0176,
      "step": 4200
    },
    {
      "epoch": 16.84,
      "grad_norm": 2.3941709995269775,
      "learning_rate": 1.3345945945945947e-05,
      "loss": 0.9411,
      "step": 4210
    },
    {
      "epoch": 16.88,
      "grad_norm": 2.6494901180267334,
      "learning_rate": 1.3305405405405407e-05,
      "loss": 0.9802,
      "step": 4220
    },
    {
      "epoch": 16.92,
      "grad_norm": 2.3265280723571777,
      "learning_rate": 1.3264864864864865e-05,
      "loss": 0.9372,
      "step": 4230
    },
    {
      "epoch": 16.96,
      "grad_norm": 2.7148125171661377,
      "learning_rate": 1.3224324324324325e-05,
      "loss": 0.9845,
      "step": 4240
    },
    {
      "epoch": 17.0,
      "grad_norm": 2.833925724029541,
      "learning_rate": 1.3183783783783785e-05,
      "loss": 1.0089,
      "step": 4250
    },
    {
      "epoch": 17.04,
      "grad_norm": 2.973768949508667,
      "learning_rate": 1.3143243243243245e-05,
      "loss": 0.9631,
      "step": 4260
    },
    {
      "epoch": 17.08,
      "grad_norm": 3.038902521133423,
      "learning_rate": 1.3102702702702702e-05,
      "loss": 0.9723,
      "step": 4270
    },
    {
      "epoch": 17.12,
      "grad_norm": 2.5647177696228027,
      "learning_rate": 1.3062162162162162e-05,
      "loss": 1.0068,
      "step": 4280
    },
    {
      "epoch": 17.16,
      "grad_norm": 2.6907339096069336,
      "learning_rate": 1.3021621621621622e-05,
      "loss": 0.9972,
      "step": 4290
    },
    {
      "epoch": 17.2,
      "grad_norm": 2.8139727115631104,
      "learning_rate": 1.2981081081081082e-05,
      "loss": 0.9936,
      "step": 4300
    },
    {
      "epoch": 17.24,
      "grad_norm": 2.353076457977295,
      "learning_rate": 1.294054054054054e-05,
      "loss": 1.0008,
      "step": 4310
    },
    {
      "epoch": 17.28,
      "grad_norm": 2.614696741104126,
      "learning_rate": 1.29e-05,
      "loss": 1.0028,
      "step": 4320
    },
    {
      "epoch": 17.32,
      "grad_norm": 2.853217601776123,
      "learning_rate": 1.285945945945946e-05,
      "loss": 1.0003,
      "step": 4330
    },
    {
      "epoch": 17.36,
      "grad_norm": 2.8536477088928223,
      "learning_rate": 1.281891891891892e-05,
      "loss": 1.0149,
      "step": 4340
    },
    {
      "epoch": 17.4,
      "grad_norm": 2.821047067642212,
      "learning_rate": 1.2778378378378379e-05,
      "loss": 1.0185,
      "step": 4350
    },
    {
      "epoch": 17.44,
      "grad_norm": 2.5795044898986816,
      "learning_rate": 1.2737837837837839e-05,
      "loss": 0.927,
      "step": 4360
    },
    {
      "epoch": 17.48,
      "grad_norm": 2.528817653656006,
      "learning_rate": 1.2697297297297299e-05,
      "loss": 0.9191,
      "step": 4370
    },
    {
      "epoch": 17.52,
      "grad_norm": 3.220043182373047,
      "learning_rate": 1.2656756756756757e-05,
      "loss": 0.9602,
      "step": 4380
    },
    {
      "epoch": 17.56,
      "grad_norm": 2.5007483959198,
      "learning_rate": 1.2616216216216215e-05,
      "loss": 0.9742,
      "step": 4390
    },
    {
      "epoch": 17.6,
      "grad_norm": 2.507725954055786,
      "learning_rate": 1.2575675675675675e-05,
      "loss": 0.9825,
      "step": 4400
    },
    {
      "epoch": 17.64,
      "grad_norm": 2.54111385345459,
      "learning_rate": 1.2535135135135135e-05,
      "loss": 1.0341,
      "step": 4410
    },
    {
      "epoch": 17.68,
      "grad_norm": 2.4261715412139893,
      "learning_rate": 1.2494594594594595e-05,
      "loss": 0.9592,
      "step": 4420
    },
    {
      "epoch": 17.72,
      "grad_norm": 2.568804979324341,
      "learning_rate": 1.2454054054054054e-05,
      "loss": 0.9791,
      "step": 4430
    },
    {
      "epoch": 17.76,
      "grad_norm": 2.8320629596710205,
      "learning_rate": 1.2413513513513514e-05,
      "loss": 1.031,
      "step": 4440
    },
    {
      "epoch": 17.8,
      "grad_norm": 3.328793525695801,
      "learning_rate": 1.2372972972972974e-05,
      "loss": 0.9452,
      "step": 4450
    },
    {
      "epoch": 17.84,
      "grad_norm": 2.7524688243865967,
      "learning_rate": 1.2332432432432434e-05,
      "loss": 0.9234,
      "step": 4460
    },
    {
      "epoch": 17.88,
      "grad_norm": 2.6647872924804688,
      "learning_rate": 1.2291891891891892e-05,
      "loss": 0.9815,
      "step": 4470
    },
    {
      "epoch": 17.92,
      "grad_norm": 2.5942327976226807,
      "learning_rate": 1.2251351351351352e-05,
      "loss": 0.9471,
      "step": 4480
    },
    {
      "epoch": 17.96,
      "grad_norm": 2.561006784439087,
      "learning_rate": 1.221081081081081e-05,
      "loss": 0.9518,
      "step": 4490
    },
    {
      "epoch": 18.0,
      "grad_norm": 2.724867582321167,
      "learning_rate": 1.217027027027027e-05,
      "loss": 1.0433,
      "step": 4500
    },
    {
      "epoch": 18.04,
      "grad_norm": 2.699409008026123,
      "learning_rate": 1.2129729729729729e-05,
      "loss": 0.9637,
      "step": 4510
    },
    {
      "epoch": 18.08,
      "grad_norm": 2.887531280517578,
      "learning_rate": 1.2089189189189189e-05,
      "loss": 0.9987,
      "step": 4520
    },
    {
      "epoch": 18.12,
      "grad_norm": 2.588595390319824,
      "learning_rate": 1.2048648648648649e-05,
      "loss": 0.9439,
      "step": 4530
    },
    {
      "epoch": 18.16,
      "grad_norm": 2.7272965908050537,
      "learning_rate": 1.2008108108108109e-05,
      "loss": 1.0153,
      "step": 4540
    },
    {
      "epoch": 18.2,
      "grad_norm": 2.9515838623046875,
      "learning_rate": 1.1967567567567567e-05,
      "loss": 1.04,
      "step": 4550
    },
    {
      "epoch": 18.24,
      "grad_norm": 2.7623867988586426,
      "learning_rate": 1.1927027027027027e-05,
      "loss": 0.9952,
      "step": 4560
    },
    {
      "epoch": 18.28,
      "grad_norm": 2.878279685974121,
      "learning_rate": 1.1886486486486487e-05,
      "loss": 0.9575,
      "step": 4570
    },
    {
      "epoch": 18.32,
      "grad_norm": 2.830656051635742,
      "learning_rate": 1.1845945945945947e-05,
      "loss": 0.932,
      "step": 4580
    },
    {
      "epoch": 18.36,
      "grad_norm": 2.505038261413574,
      "learning_rate": 1.1805405405405406e-05,
      "loss": 0.9862,
      "step": 4590
    },
    {
      "epoch": 18.4,
      "grad_norm": 2.943530797958374,
      "learning_rate": 1.1764864864864864e-05,
      "loss": 0.9596,
      "step": 4600
    },
    {
      "epoch": 18.44,
      "grad_norm": 2.554337978363037,
      "learning_rate": 1.1724324324324324e-05,
      "loss": 1.0045,
      "step": 4610
    },
    {
      "epoch": 18.48,
      "grad_norm": 2.8107335567474365,
      "learning_rate": 1.1683783783783784e-05,
      "loss": 0.9802,
      "step": 4620
    },
    {
      "epoch": 18.52,
      "grad_norm": 2.7674968242645264,
      "learning_rate": 1.1643243243243242e-05,
      "loss": 1.0518,
      "step": 4630
    },
    {
      "epoch": 18.56,
      "grad_norm": 3.1446869373321533,
      "learning_rate": 1.1602702702702703e-05,
      "loss": 0.9302,
      "step": 4640
    },
    {
      "epoch": 18.6,
      "grad_norm": 2.9994258880615234,
      "learning_rate": 1.1562162162162163e-05,
      "loss": 0.9675,
      "step": 4650
    },
    {
      "epoch": 18.64,
      "grad_norm": 3.0291903018951416,
      "learning_rate": 1.1521621621621623e-05,
      "loss": 1.0042,
      "step": 4660
    },
    {
      "epoch": 18.68,
      "grad_norm": 2.7169313430786133,
      "learning_rate": 1.1481081081081081e-05,
      "loss": 0.9151,
      "step": 4670
    },
    {
      "epoch": 18.72,
      "grad_norm": 2.8211236000061035,
      "learning_rate": 1.1440540540540541e-05,
      "loss": 0.9808,
      "step": 4680
    },
    {
      "epoch": 18.76,
      "grad_norm": 3.1136276721954346,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.9693,
      "step": 4690
    },
    {
      "epoch": 18.8,
      "grad_norm": 2.6505775451660156,
      "learning_rate": 1.1359459459459461e-05,
      "loss": 0.9426,
      "step": 4700
    },
    {
      "epoch": 18.84,
      "grad_norm": 3.1129448413848877,
      "learning_rate": 1.131891891891892e-05,
      "loss": 0.9912,
      "step": 4710
    },
    {
      "epoch": 18.88,
      "grad_norm": 2.837641954421997,
      "learning_rate": 1.1278378378378378e-05,
      "loss": 1.0252,
      "step": 4720
    },
    {
      "epoch": 18.92,
      "grad_norm": 2.4885482788085938,
      "learning_rate": 1.1237837837837838e-05,
      "loss": 0.9303,
      "step": 4730
    },
    {
      "epoch": 18.96,
      "grad_norm": 2.6302285194396973,
      "learning_rate": 1.1197297297297298e-05,
      "loss": 0.9686,
      "step": 4740
    },
    {
      "epoch": 19.0,
      "grad_norm": 2.575929641723633,
      "learning_rate": 1.1156756756756756e-05,
      "loss": 0.9657,
      "step": 4750
    },
    {
      "epoch": 19.04,
      "grad_norm": 2.8436739444732666,
      "learning_rate": 1.1116216216216216e-05,
      "loss": 1.0409,
      "step": 4760
    },
    {
      "epoch": 19.08,
      "grad_norm": 2.497572898864746,
      "learning_rate": 1.1075675675675676e-05,
      "loss": 0.9926,
      "step": 4770
    },
    {
      "epoch": 19.12,
      "grad_norm": 2.7853972911834717,
      "learning_rate": 1.1035135135135136e-05,
      "loss": 0.964,
      "step": 4780
    },
    {
      "epoch": 19.16,
      "grad_norm": 2.8694753646850586,
      "learning_rate": 1.0994594594594594e-05,
      "loss": 0.9802,
      "step": 4790
    },
    {
      "epoch": 19.2,
      "grad_norm": 2.873290538787842,
      "learning_rate": 1.0954054054054055e-05,
      "loss": 0.9685,
      "step": 4800
    },
    {
      "epoch": 19.24,
      "grad_norm": 3.271519899368286,
      "learning_rate": 1.0913513513513515e-05,
      "loss": 0.9365,
      "step": 4810
    },
    {
      "epoch": 19.28,
      "grad_norm": 2.5435879230499268,
      "learning_rate": 1.0872972972972975e-05,
      "loss": 0.9631,
      "step": 4820
    },
    {
      "epoch": 19.32,
      "grad_norm": 2.9564437866210938,
      "learning_rate": 1.0832432432432431e-05,
      "loss": 1.0237,
      "step": 4830
    },
    {
      "epoch": 19.36,
      "grad_norm": 2.66162371635437,
      "learning_rate": 1.0791891891891891e-05,
      "loss": 0.9838,
      "step": 4840
    },
    {
      "epoch": 19.4,
      "grad_norm": 3.007584810256958,
      "learning_rate": 1.0751351351351351e-05,
      "loss": 0.9388,
      "step": 4850
    },
    {
      "epoch": 19.44,
      "grad_norm": 3.37147855758667,
      "learning_rate": 1.0710810810810811e-05,
      "loss": 0.9637,
      "step": 4860
    },
    {
      "epoch": 19.48,
      "grad_norm": 2.7388193607330322,
      "learning_rate": 1.067027027027027e-05,
      "loss": 1.0758,
      "step": 4870
    },
    {
      "epoch": 19.52,
      "grad_norm": 2.903205394744873,
      "learning_rate": 1.062972972972973e-05,
      "loss": 1.0489,
      "step": 4880
    },
    {
      "epoch": 19.56,
      "grad_norm": 2.8761048316955566,
      "learning_rate": 1.058918918918919e-05,
      "loss": 0.9412,
      "step": 4890
    },
    {
      "epoch": 19.6,
      "grad_norm": 2.783726930618286,
      "learning_rate": 1.054864864864865e-05,
      "loss": 0.9486,
      "step": 4900
    },
    {
      "epoch": 19.64,
      "grad_norm": 2.7788074016571045,
      "learning_rate": 1.0508108108108108e-05,
      "loss": 0.9404,
      "step": 4910
    },
    {
      "epoch": 19.68,
      "grad_norm": 2.7755320072174072,
      "learning_rate": 1.047162162162162e-05,
      "loss": 0.9828,
      "step": 4920
    },
    {
      "epoch": 19.72,
      "grad_norm": 3.3366363048553467,
      "learning_rate": 1.043108108108108e-05,
      "loss": 0.9674,
      "step": 4930
    },
    {
      "epoch": 19.76,
      "grad_norm": 3.0046346187591553,
      "learning_rate": 1.039054054054054e-05,
      "loss": 1.0645,
      "step": 4940
    },
    {
      "epoch": 19.8,
      "grad_norm": 2.478111982345581,
      "learning_rate": 1.035e-05,
      "loss": 0.8781,
      "step": 4950
    },
    {
      "epoch": 19.84,
      "grad_norm": 2.701723337173462,
      "learning_rate": 1.030945945945946e-05,
      "loss": 0.9626,
      "step": 4960
    },
    {
      "epoch": 19.88,
      "grad_norm": 2.897798776626587,
      "learning_rate": 1.026891891891892e-05,
      "loss": 1.022,
      "step": 4970
    },
    {
      "epoch": 19.92,
      "grad_norm": 2.890669822692871,
      "learning_rate": 1.022837837837838e-05,
      "loss": 0.9447,
      "step": 4980
    },
    {
      "epoch": 19.96,
      "grad_norm": 3.0988903045654297,
      "learning_rate": 1.0187837837837838e-05,
      "loss": 0.9602,
      "step": 4990
    },
    {
      "epoch": 20.0,
      "grad_norm": 2.7061400413513184,
      "learning_rate": 1.0147297297297298e-05,
      "loss": 1.0267,
      "step": 5000
    },
    {
      "epoch": 20.04,
      "grad_norm": 3.3966965675354004,
      "learning_rate": 1.0106756756756758e-05,
      "loss": 1.0007,
      "step": 5010
    },
    {
      "epoch": 20.08,
      "grad_norm": 3.406080961227417,
      "learning_rate": 1.0066216216216218e-05,
      "loss": 1.0231,
      "step": 5020
    },
    {
      "epoch": 20.12,
      "grad_norm": 2.690009117126465,
      "learning_rate": 1.0025675675675674e-05,
      "loss": 1.0187,
      "step": 5030
    },
    {
      "epoch": 20.16,
      "grad_norm": 3.2239134311676025,
      "learning_rate": 9.985135135135134e-06,
      "loss": 0.9342,
      "step": 5040
    },
    {
      "epoch": 20.2,
      "grad_norm": 2.826200008392334,
      "learning_rate": 9.948648648648649e-06,
      "loss": 0.9953,
      "step": 5050
    },
    {
      "epoch": 20.24,
      "grad_norm": 3.026378631591797,
      "learning_rate": 9.908108108108109e-06,
      "loss": 0.9445,
      "step": 5060
    },
    {
      "epoch": 20.28,
      "grad_norm": 3.042712926864624,
      "learning_rate": 9.867567567567569e-06,
      "loss": 1.0034,
      "step": 5070
    },
    {
      "epoch": 20.32,
      "grad_norm": 2.9613196849823,
      "learning_rate": 9.827027027027027e-06,
      "loss": 0.9817,
      "step": 5080
    },
    {
      "epoch": 20.36,
      "grad_norm": 2.8165974617004395,
      "learning_rate": 9.786486486486487e-06,
      "loss": 0.96,
      "step": 5090
    },
    {
      "epoch": 20.4,
      "grad_norm": 3.36576771736145,
      "learning_rate": 9.745945945945947e-06,
      "loss": 1.0117,
      "step": 5100
    },
    {
      "epoch": 20.44,
      "grad_norm": 2.715649127960205,
      "learning_rate": 9.705405405405407e-06,
      "loss": 0.9437,
      "step": 5110
    },
    {
      "epoch": 20.48,
      "grad_norm": 2.9221792221069336,
      "learning_rate": 9.664864864864864e-06,
      "loss": 0.9299,
      "step": 5120
    },
    {
      "epoch": 20.52,
      "grad_norm": 3.0069892406463623,
      "learning_rate": 9.624324324324324e-06,
      "loss": 0.9337,
      "step": 5130
    },
    {
      "epoch": 20.56,
      "grad_norm": 3.224621534347534,
      "learning_rate": 9.583783783783784e-06,
      "loss": 1.02,
      "step": 5140
    },
    {
      "epoch": 20.6,
      "grad_norm": 3.1807796955108643,
      "learning_rate": 9.543243243243244e-06,
      "loss": 0.9486,
      "step": 5150
    },
    {
      "epoch": 20.64,
      "grad_norm": 3.3245315551757812,
      "learning_rate": 9.502702702702702e-06,
      "loss": 0.9808,
      "step": 5160
    },
    {
      "epoch": 20.68,
      "grad_norm": 2.8007943630218506,
      "learning_rate": 9.462162162162162e-06,
      "loss": 0.9457,
      "step": 5170
    },
    {
      "epoch": 20.72,
      "grad_norm": 3.098586082458496,
      "learning_rate": 9.421621621621622e-06,
      "loss": 0.9166,
      "step": 5180
    },
    {
      "epoch": 20.76,
      "grad_norm": 3.651254177093506,
      "learning_rate": 9.381081081081082e-06,
      "loss": 0.9101,
      "step": 5190
    },
    {
      "epoch": 20.8,
      "grad_norm": 2.7856059074401855,
      "learning_rate": 9.34054054054054e-06,
      "loss": 0.9721,
      "step": 5200
    },
    {
      "epoch": 20.84,
      "grad_norm": 3.4855916500091553,
      "learning_rate": 9.3e-06,
      "loss": 0.9996,
      "step": 5210
    },
    {
      "epoch": 20.88,
      "grad_norm": 2.9728574752807617,
      "learning_rate": 9.25945945945946e-06,
      "loss": 0.9826,
      "step": 5220
    },
    {
      "epoch": 20.92,
      "grad_norm": 3.00925350189209,
      "learning_rate": 9.218918918918919e-06,
      "loss": 0.9942,
      "step": 5230
    },
    {
      "epoch": 20.96,
      "grad_norm": 2.950573205947876,
      "learning_rate": 9.178378378378377e-06,
      "loss": 1.0404,
      "step": 5240
    },
    {
      "epoch": 21.0,
      "grad_norm": 2.5406429767608643,
      "learning_rate": 9.137837837837837e-06,
      "loss": 0.9904,
      "step": 5250
    },
    {
      "epoch": 21.04,
      "grad_norm": 2.705806016921997,
      "learning_rate": 9.097297297297298e-06,
      "loss": 0.9651,
      "step": 5260
    },
    {
      "epoch": 21.08,
      "grad_norm": 2.574540138244629,
      "learning_rate": 9.056756756756758e-06,
      "loss": 0.9548,
      "step": 5270
    },
    {
      "epoch": 21.12,
      "grad_norm": 2.7781901359558105,
      "learning_rate": 9.016216216216216e-06,
      "loss": 0.9247,
      "step": 5280
    },
    {
      "epoch": 21.16,
      "grad_norm": 3.2010116577148438,
      "learning_rate": 8.975675675675676e-06,
      "loss": 1.0493,
      "step": 5290
    },
    {
      "epoch": 21.2,
      "grad_norm": 3.3932442665100098,
      "learning_rate": 8.935135135135136e-06,
      "loss": 1.0163,
      "step": 5300
    },
    {
      "epoch": 21.24,
      "grad_norm": 2.972703456878662,
      "learning_rate": 8.894594594594596e-06,
      "loss": 0.9895,
      "step": 5310
    },
    {
      "epoch": 21.28,
      "grad_norm": 3.116302967071533,
      "learning_rate": 8.854054054054054e-06,
      "loss": 0.9832,
      "step": 5320
    },
    {
      "epoch": 21.32,
      "grad_norm": 2.8515660762786865,
      "learning_rate": 8.813513513513514e-06,
      "loss": 0.9726,
      "step": 5330
    },
    {
      "epoch": 21.36,
      "grad_norm": 3.1141955852508545,
      "learning_rate": 8.772972972972973e-06,
      "loss": 1.0145,
      "step": 5340
    },
    {
      "epoch": 21.4,
      "grad_norm": 3.0943539142608643,
      "learning_rate": 8.732432432432433e-06,
      "loss": 0.9747,
      "step": 5350
    },
    {
      "epoch": 21.44,
      "grad_norm": 3.189718008041382,
      "learning_rate": 8.691891891891891e-06,
      "loss": 0.971,
      "step": 5360
    },
    {
      "epoch": 21.48,
      "grad_norm": 3.386143445968628,
      "learning_rate": 8.651351351351351e-06,
      "loss": 0.9809,
      "step": 5370
    },
    {
      "epoch": 21.52,
      "grad_norm": 2.8621933460235596,
      "learning_rate": 8.610810810810811e-06,
      "loss": 0.9457,
      "step": 5380
    },
    {
      "epoch": 21.56,
      "grad_norm": 2.579432487487793,
      "learning_rate": 8.570270270270271e-06,
      "loss": 0.9515,
      "step": 5390
    },
    {
      "epoch": 21.6,
      "grad_norm": 3.0825886726379395,
      "learning_rate": 8.52972972972973e-06,
      "loss": 0.9836,
      "step": 5400
    },
    {
      "epoch": 21.64,
      "grad_norm": 2.918325185775757,
      "learning_rate": 8.48918918918919e-06,
      "loss": 1.026,
      "step": 5410
    },
    {
      "epoch": 21.68,
      "grad_norm": 2.9823317527770996,
      "learning_rate": 8.44864864864865e-06,
      "loss": 1.0175,
      "step": 5420
    },
    {
      "epoch": 21.72,
      "grad_norm": 2.8878540992736816,
      "learning_rate": 8.40810810810811e-06,
      "loss": 0.9354,
      "step": 5430
    },
    {
      "epoch": 21.76,
      "grad_norm": 3.554452896118164,
      "learning_rate": 8.367567567567568e-06,
      "loss": 0.9795,
      "step": 5440
    },
    {
      "epoch": 21.8,
      "grad_norm": 3.0766332149505615,
      "learning_rate": 8.327027027027028e-06,
      "loss": 0.9895,
      "step": 5450
    },
    {
      "epoch": 21.84,
      "grad_norm": 2.961117744445801,
      "learning_rate": 8.286486486486486e-06,
      "loss": 0.9091,
      "step": 5460
    },
    {
      "epoch": 21.88,
      "grad_norm": 3.0251247882843018,
      "learning_rate": 8.245945945945946e-06,
      "loss": 1.0515,
      "step": 5470
    },
    {
      "epoch": 21.92,
      "grad_norm": 3.3612849712371826,
      "learning_rate": 8.205405405405405e-06,
      "loss": 0.9839,
      "step": 5480
    },
    {
      "epoch": 21.96,
      "grad_norm": 3.1837191581726074,
      "learning_rate": 8.164864864864865e-06,
      "loss": 0.9657,
      "step": 5490
    },
    {
      "epoch": 22.0,
      "grad_norm": 2.763920783996582,
      "learning_rate": 8.124324324324325e-06,
      "loss": 0.9554,
      "step": 5500
    },
    {
      "epoch": 22.04,
      "grad_norm": 2.5297794342041016,
      "learning_rate": 8.083783783783785e-06,
      "loss": 0.9587,
      "step": 5510
    },
    {
      "epoch": 22.08,
      "grad_norm": 2.9266252517700195,
      "learning_rate": 8.043243243243243e-06,
      "loss": 0.9817,
      "step": 5520
    },
    {
      "epoch": 22.12,
      "grad_norm": 3.0272533893585205,
      "learning_rate": 8.002702702702703e-06,
      "loss": 0.9809,
      "step": 5530
    },
    {
      "epoch": 22.16,
      "grad_norm": 3.3072574138641357,
      "learning_rate": 7.962162162162163e-06,
      "loss": 0.9879,
      "step": 5540
    },
    {
      "epoch": 22.2,
      "grad_norm": 2.787026882171631,
      "learning_rate": 7.921621621621623e-06,
      "loss": 0.9804,
      "step": 5550
    },
    {
      "epoch": 22.24,
      "grad_norm": 3.3421404361724854,
      "learning_rate": 7.881081081081081e-06,
      "loss": 1.0139,
      "step": 5560
    },
    {
      "epoch": 22.28,
      "grad_norm": 3.169401168823242,
      "learning_rate": 7.84054054054054e-06,
      "loss": 0.9593,
      "step": 5570
    },
    {
      "epoch": 22.32,
      "grad_norm": 2.827693462371826,
      "learning_rate": 7.8e-06,
      "loss": 0.9245,
      "step": 5580
    },
    {
      "epoch": 22.36,
      "grad_norm": 3.8918211460113525,
      "learning_rate": 7.75945945945946e-06,
      "loss": 0.9802,
      "step": 5590
    },
    {
      "epoch": 22.4,
      "grad_norm": 3.3910388946533203,
      "learning_rate": 7.718918918918918e-06,
      "loss": 0.9722,
      "step": 5600
    },
    {
      "epoch": 22.44,
      "grad_norm": 2.9861645698547363,
      "learning_rate": 7.678378378378378e-06,
      "loss": 0.98,
      "step": 5610
    },
    {
      "epoch": 22.48,
      "grad_norm": 3.478778600692749,
      "learning_rate": 7.637837837837838e-06,
      "loss": 0.9722,
      "step": 5620
    },
    {
      "epoch": 22.52,
      "grad_norm": 3.4198269844055176,
      "learning_rate": 7.597297297297298e-06,
      "loss": 0.9249,
      "step": 5630
    },
    {
      "epoch": 22.56,
      "grad_norm": 3.5688154697418213,
      "learning_rate": 7.556756756756757e-06,
      "loss": 0.9888,
      "step": 5640
    },
    {
      "epoch": 22.6,
      "grad_norm": 2.7014477252960205,
      "learning_rate": 7.516216216216216e-06,
      "loss": 0.8973,
      "step": 5650
    },
    {
      "epoch": 22.64,
      "grad_norm": 3.6990773677825928,
      "learning_rate": 7.475675675675676e-06,
      "loss": 0.9509,
      "step": 5660
    },
    {
      "epoch": 22.68,
      "grad_norm": 3.2474777698516846,
      "learning_rate": 7.435135135135135e-06,
      "loss": 0.9234,
      "step": 5670
    },
    {
      "epoch": 22.72,
      "grad_norm": 3.386773109436035,
      "learning_rate": 7.394594594594595e-06,
      "loss": 0.9251,
      "step": 5680
    },
    {
      "epoch": 22.76,
      "grad_norm": 2.904475450515747,
      "learning_rate": 7.354054054054054e-06,
      "loss": 0.9643,
      "step": 5690
    },
    {
      "epoch": 22.8,
      "grad_norm": 2.659912347793579,
      "learning_rate": 7.313513513513514e-06,
      "loss": 0.9892,
      "step": 5700
    },
    {
      "epoch": 22.84,
      "grad_norm": 3.101536273956299,
      "learning_rate": 7.272972972972973e-06,
      "loss": 0.9706,
      "step": 5710
    },
    {
      "epoch": 22.88,
      "grad_norm": 3.3238580226898193,
      "learning_rate": 7.232432432432433e-06,
      "loss": 0.9523,
      "step": 5720
    },
    {
      "epoch": 22.92,
      "grad_norm": 3.180690288543701,
      "learning_rate": 7.191891891891892e-06,
      "loss": 1.0006,
      "step": 5730
    },
    {
      "epoch": 22.96,
      "grad_norm": 2.997227907180786,
      "learning_rate": 7.151351351351352e-06,
      "loss": 0.9958,
      "step": 5740
    },
    {
      "epoch": 23.0,
      "grad_norm": 2.9159905910491943,
      "learning_rate": 7.110810810810811e-06,
      "loss": 0.9809,
      "step": 5750
    },
    {
      "epoch": 23.04,
      "grad_norm": 3.395756959915161,
      "learning_rate": 7.070270270270271e-06,
      "loss": 1.0351,
      "step": 5760
    },
    {
      "epoch": 23.08,
      "grad_norm": 2.6385440826416016,
      "learning_rate": 7.0297297297297294e-06,
      "loss": 1.0515,
      "step": 5770
    },
    {
      "epoch": 23.12,
      "grad_norm": 2.945957660675049,
      "learning_rate": 6.9891891891891895e-06,
      "loss": 0.9233,
      "step": 5780
    },
    {
      "epoch": 23.16,
      "grad_norm": 3.1579246520996094,
      "learning_rate": 6.948648648648649e-06,
      "loss": 1.0276,
      "step": 5790
    },
    {
      "epoch": 23.2,
      "grad_norm": 3.099695920944214,
      "learning_rate": 6.908108108108109e-06,
      "loss": 0.9688,
      "step": 5800
    },
    {
      "epoch": 23.24,
      "grad_norm": 2.9133057594299316,
      "learning_rate": 6.867567567567568e-06,
      "loss": 0.9485,
      "step": 5810
    },
    {
      "epoch": 23.28,
      "grad_norm": 3.205348253250122,
      "learning_rate": 6.827027027027027e-06,
      "loss": 0.9325,
      "step": 5820
    },
    {
      "epoch": 23.32,
      "grad_norm": 3.415388345718384,
      "learning_rate": 6.786486486486486e-06,
      "loss": 0.9044,
      "step": 5830
    },
    {
      "epoch": 23.36,
      "grad_norm": 3.6607279777526855,
      "learning_rate": 6.745945945945946e-06,
      "loss": 1.0098,
      "step": 5840
    },
    {
      "epoch": 23.4,
      "grad_norm": 2.8956003189086914,
      "learning_rate": 6.7054054054054054e-06,
      "loss": 0.9109,
      "step": 5850
    },
    {
      "epoch": 23.44,
      "grad_norm": 3.4380362033843994,
      "learning_rate": 6.6648648648648655e-06,
      "loss": 0.9865,
      "step": 5860
    },
    {
      "epoch": 23.48,
      "grad_norm": 3.391061782836914,
      "learning_rate": 6.624324324324325e-06,
      "loss": 0.9642,
      "step": 5870
    },
    {
      "epoch": 23.52,
      "grad_norm": 3.195321559906006,
      "learning_rate": 6.583783783783784e-06,
      "loss": 0.9131,
      "step": 5880
    },
    {
      "epoch": 23.56,
      "grad_norm": 3.114140272140503,
      "learning_rate": 6.543243243243243e-06,
      "loss": 0.9338,
      "step": 5890
    },
    {
      "epoch": 23.6,
      "grad_norm": 3.4138224124908447,
      "learning_rate": 6.502702702702703e-06,
      "loss": 1.0045,
      "step": 5900
    },
    {
      "epoch": 23.64,
      "grad_norm": 3.445298671722412,
      "learning_rate": 6.462162162162162e-06,
      "loss": 0.9567,
      "step": 5910
    },
    {
      "epoch": 23.68,
      "grad_norm": 3.4252769947052,
      "learning_rate": 6.421621621621622e-06,
      "loss": 0.986,
      "step": 5920
    },
    {
      "epoch": 23.72,
      "grad_norm": 3.381051778793335,
      "learning_rate": 6.381081081081081e-06,
      "loss": 0.9213,
      "step": 5930
    },
    {
      "epoch": 23.76,
      "grad_norm": 2.9266293048858643,
      "learning_rate": 6.340540540540541e-06,
      "loss": 0.9123,
      "step": 5940
    },
    {
      "epoch": 23.8,
      "grad_norm": 3.3497893810272217,
      "learning_rate": 6.3e-06,
      "loss": 1.0239,
      "step": 5950
    },
    {
      "epoch": 23.84,
      "grad_norm": 3.5225305557250977,
      "learning_rate": 6.25945945945946e-06,
      "loss": 0.97,
      "step": 5960
    },
    {
      "epoch": 23.88,
      "grad_norm": 3.113309383392334,
      "learning_rate": 6.218918918918919e-06,
      "loss": 0.8696,
      "step": 5970
    },
    {
      "epoch": 23.92,
      "grad_norm": 3.1547505855560303,
      "learning_rate": 6.178378378378379e-06,
      "loss": 0.9407,
      "step": 5980
    },
    {
      "epoch": 23.96,
      "grad_norm": 3.2741966247558594,
      "learning_rate": 6.137837837837837e-06,
      "loss": 0.923,
      "step": 5990
    },
    {
      "epoch": 24.0,
      "grad_norm": 3.031144380569458,
      "learning_rate": 6.0972972972972974e-06,
      "loss": 0.9798,
      "step": 6000
    },
    {
      "epoch": 24.04,
      "grad_norm": 2.685048818588257,
      "learning_rate": 6.056756756756757e-06,
      "loss": 0.946,
      "step": 6010
    },
    {
      "epoch": 24.08,
      "grad_norm": 4.504993915557861,
      "learning_rate": 6.016216216216217e-06,
      "loss": 1.0032,
      "step": 6020
    },
    {
      "epoch": 24.12,
      "grad_norm": 3.057793617248535,
      "learning_rate": 5.975675675675676e-06,
      "loss": 1.0497,
      "step": 6030
    },
    {
      "epoch": 24.16,
      "grad_norm": 2.8657631874084473,
      "learning_rate": 5.935135135135136e-06,
      "loss": 0.9916,
      "step": 6040
    },
    {
      "epoch": 24.2,
      "grad_norm": 3.083625078201294,
      "learning_rate": 5.894594594594594e-06,
      "loss": 0.9782,
      "step": 6050
    },
    {
      "epoch": 24.24,
      "grad_norm": 3.3981151580810547,
      "learning_rate": 5.854054054054054e-06,
      "loss": 0.9417,
      "step": 6060
    },
    {
      "epoch": 24.28,
      "grad_norm": 3.137484550476074,
      "learning_rate": 5.813513513513513e-06,
      "loss": 0.9462,
      "step": 6070
    },
    {
      "epoch": 24.32,
      "grad_norm": 4.01881742477417,
      "learning_rate": 5.7729729729729734e-06,
      "loss": 0.9734,
      "step": 6080
    },
    {
      "epoch": 24.36,
      "grad_norm": 3.029776096343994,
      "learning_rate": 5.732432432432433e-06,
      "loss": 0.938,
      "step": 6090
    },
    {
      "epoch": 24.4,
      "grad_norm": 3.2238786220550537,
      "learning_rate": 5.691891891891892e-06,
      "loss": 0.9708,
      "step": 6100
    },
    {
      "epoch": 24.44,
      "grad_norm": 2.8986001014709473,
      "learning_rate": 5.651351351351351e-06,
      "loss": 0.9904,
      "step": 6110
    },
    {
      "epoch": 24.48,
      "grad_norm": 4.996669292449951,
      "learning_rate": 5.610810810810811e-06,
      "loss": 0.9882,
      "step": 6120
    },
    {
      "epoch": 24.52,
      "grad_norm": 3.2492313385009766,
      "learning_rate": 5.57027027027027e-06,
      "loss": 0.8758,
      "step": 6130
    },
    {
      "epoch": 24.56,
      "grad_norm": 2.8425331115722656,
      "learning_rate": 5.52972972972973e-06,
      "loss": 1.0055,
      "step": 6140
    },
    {
      "epoch": 24.6,
      "grad_norm": 3.064181327819824,
      "learning_rate": 5.4891891891891894e-06,
      "loss": 0.9476,
      "step": 6150
    },
    {
      "epoch": 24.64,
      "grad_norm": 3.2355122566223145,
      "learning_rate": 5.448648648648649e-06,
      "loss": 0.9471,
      "step": 6160
    },
    {
      "epoch": 24.68,
      "grad_norm": 3.1903765201568604,
      "learning_rate": 5.408108108108108e-06,
      "loss": 1.0002,
      "step": 6170
    },
    {
      "epoch": 24.72,
      "grad_norm": 3.2591848373413086,
      "learning_rate": 5.367567567567568e-06,
      "loss": 0.9339,
      "step": 6180
    },
    {
      "epoch": 24.76,
      "grad_norm": 3.41473126411438,
      "learning_rate": 5.327027027027027e-06,
      "loss": 0.8935,
      "step": 6190
    },
    {
      "epoch": 24.8,
      "grad_norm": 3.2382092475891113,
      "learning_rate": 5.286486486486487e-06,
      "loss": 0.9388,
      "step": 6200
    },
    {
      "epoch": 24.84,
      "grad_norm": 3.705148696899414,
      "learning_rate": 5.245945945945945e-06,
      "loss": 0.9079,
      "step": 6210
    },
    {
      "epoch": 24.88,
      "grad_norm": 2.9160845279693604,
      "learning_rate": 5.205405405405405e-06,
      "loss": 0.9773,
      "step": 6220
    },
    {
      "epoch": 24.92,
      "grad_norm": 2.589449882507324,
      "learning_rate": 5.164864864864865e-06,
      "loss": 0.9514,
      "step": 6230
    },
    {
      "epoch": 24.96,
      "grad_norm": 4.2641143798828125,
      "learning_rate": 5.124324324324325e-06,
      "loss": 0.9778,
      "step": 6240
    },
    {
      "epoch": 25.0,
      "grad_norm": 3.7900233268737793,
      "learning_rate": 5.083783783783784e-06,
      "loss": 1.0149,
      "step": 6250
    },
    {
      "epoch": 25.04,
      "grad_norm": 2.902169942855835,
      "learning_rate": 5.043243243243244e-06,
      "loss": 0.9471,
      "step": 6260
    },
    {
      "epoch": 25.08,
      "grad_norm": 3.275390863418579,
      "learning_rate": 5.002702702702702e-06,
      "loss": 0.9612,
      "step": 6270
    },
    {
      "epoch": 25.12,
      "grad_norm": 3.406909942626953,
      "learning_rate": 4.962162162162162e-06,
      "loss": 0.9957,
      "step": 6280
    },
    {
      "epoch": 25.16,
      "grad_norm": 3.1829192638397217,
      "learning_rate": 4.921621621621621e-06,
      "loss": 0.9191,
      "step": 6290
    },
    {
      "epoch": 25.2,
      "grad_norm": 3.4307124614715576,
      "learning_rate": 4.881081081081081e-06,
      "loss": 1.0059,
      "step": 6300
    },
    {
      "epoch": 25.24,
      "grad_norm": 3.0146782398223877,
      "learning_rate": 4.840540540540541e-06,
      "loss": 0.985,
      "step": 6310
    },
    {
      "epoch": 25.28,
      "grad_norm": 3.972818613052368,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.9278,
      "step": 6320
    },
    {
      "epoch": 25.32,
      "grad_norm": 3.5587069988250732,
      "learning_rate": 4.759459459459459e-06,
      "loss": 0.9775,
      "step": 6330
    },
    {
      "epoch": 25.36,
      "grad_norm": 3.1705052852630615,
      "learning_rate": 4.718918918918919e-06,
      "loss": 0.9359,
      "step": 6340
    },
    {
      "epoch": 25.4,
      "grad_norm": 3.4802680015563965,
      "learning_rate": 4.678378378378378e-06,
      "loss": 0.877,
      "step": 6350
    },
    {
      "epoch": 25.44,
      "grad_norm": 3.9646785259246826,
      "learning_rate": 4.637837837837838e-06,
      "loss": 0.9405,
      "step": 6360
    },
    {
      "epoch": 25.48,
      "grad_norm": 3.122668981552124,
      "learning_rate": 4.597297297297297e-06,
      "loss": 0.9168,
      "step": 6370
    },
    {
      "epoch": 25.52,
      "grad_norm": 3.326180934906006,
      "learning_rate": 4.556756756756757e-06,
      "loss": 0.9305,
      "step": 6380
    },
    {
      "epoch": 25.56,
      "grad_norm": 2.9143025875091553,
      "learning_rate": 4.516216216216216e-06,
      "loss": 0.935,
      "step": 6390
    },
    {
      "epoch": 25.6,
      "grad_norm": 3.8411097526550293,
      "learning_rate": 4.475675675675676e-06,
      "loss": 0.9538,
      "step": 6400
    },
    {
      "epoch": 25.64,
      "grad_norm": 3.4402451515197754,
      "learning_rate": 4.435135135135135e-06,
      "loss": 1.0003,
      "step": 6410
    },
    {
      "epoch": 25.68,
      "grad_norm": 3.2575454711914062,
      "learning_rate": 4.394594594594595e-06,
      "loss": 0.9788,
      "step": 6420
    },
    {
      "epoch": 25.72,
      "grad_norm": 3.212146759033203,
      "learning_rate": 4.354054054054054e-06,
      "loss": 1.008,
      "step": 6430
    },
    {
      "epoch": 25.76,
      "grad_norm": 3.753993272781372,
      "learning_rate": 4.313513513513513e-06,
      "loss": 1.0034,
      "step": 6440
    },
    {
      "epoch": 25.8,
      "grad_norm": 2.857346534729004,
      "learning_rate": 4.2729729729729726e-06,
      "loss": 0.9662,
      "step": 6450
    },
    {
      "epoch": 25.84,
      "grad_norm": 3.814976215362549,
      "learning_rate": 4.232432432432433e-06,
      "loss": 0.9566,
      "step": 6460
    },
    {
      "epoch": 25.88,
      "grad_norm": 3.176661968231201,
      "learning_rate": 4.191891891891892e-06,
      "loss": 0.9264,
      "step": 6470
    },
    {
      "epoch": 25.92,
      "grad_norm": 3.412036657333374,
      "learning_rate": 4.151351351351352e-06,
      "loss": 0.9692,
      "step": 6480
    },
    {
      "epoch": 25.96,
      "grad_norm": 3.703213691711426,
      "learning_rate": 4.11081081081081e-06,
      "loss": 0.985,
      "step": 6490
    },
    {
      "epoch": 26.0,
      "grad_norm": 4.044564247131348,
      "learning_rate": 4.07027027027027e-06,
      "loss": 0.9493,
      "step": 6500
    },
    {
      "epoch": 26.04,
      "grad_norm": 3.2918429374694824,
      "learning_rate": 4.029729729729729e-06,
      "loss": 0.9576,
      "step": 6510
    },
    {
      "epoch": 26.08,
      "grad_norm": 3.4067983627319336,
      "learning_rate": 3.989189189189189e-06,
      "loss": 0.9757,
      "step": 6520
    },
    {
      "epoch": 26.12,
      "grad_norm": 3.229865312576294,
      "learning_rate": 3.9486486486486486e-06,
      "loss": 0.908,
      "step": 6530
    },
    {
      "epoch": 26.16,
      "grad_norm": 3.357055425643921,
      "learning_rate": 3.908108108108109e-06,
      "loss": 0.968,
      "step": 6540
    },
    {
      "epoch": 26.2,
      "grad_norm": 3.1051759719848633,
      "learning_rate": 3.867567567567567e-06,
      "loss": 0.8864,
      "step": 6550
    },
    {
      "epoch": 26.24,
      "grad_norm": 3.110442876815796,
      "learning_rate": 3.827027027027027e-06,
      "loss": 0.9496,
      "step": 6560
    },
    {
      "epoch": 26.28,
      "grad_norm": 3.0586130619049072,
      "learning_rate": 3.786486486486486e-06,
      "loss": 0.9655,
      "step": 6570
    },
    {
      "epoch": 26.32,
      "grad_norm": 3.479560136795044,
      "learning_rate": 3.745945945945946e-06,
      "loss": 0.9694,
      "step": 6580
    },
    {
      "epoch": 26.36,
      "grad_norm": 3.784788131713867,
      "learning_rate": 3.7054054054054054e-06,
      "loss": 0.9517,
      "step": 6590
    },
    {
      "epoch": 26.4,
      "grad_norm": 3.457336187362671,
      "learning_rate": 3.664864864864865e-06,
      "loss": 0.9744,
      "step": 6600
    },
    {
      "epoch": 26.44,
      "grad_norm": 3.095667600631714,
      "learning_rate": 3.6243243243243246e-06,
      "loss": 0.9583,
      "step": 6610
    },
    {
      "epoch": 26.48,
      "grad_norm": 3.7599644660949707,
      "learning_rate": 3.5837837837837838e-06,
      "loss": 0.9997,
      "step": 6620
    },
    {
      "epoch": 26.52,
      "grad_norm": 3.371051549911499,
      "learning_rate": 3.5432432432432434e-06,
      "loss": 0.9615,
      "step": 6630
    },
    {
      "epoch": 26.56,
      "grad_norm": 3.4415833950042725,
      "learning_rate": 3.502702702702703e-06,
      "loss": 0.9772,
      "step": 6640
    },
    {
      "epoch": 26.6,
      "grad_norm": 3.1058363914489746,
      "learning_rate": 3.462162162162162e-06,
      "loss": 0.9355,
      "step": 6650
    },
    {
      "epoch": 26.64,
      "grad_norm": 3.0031862258911133,
      "learning_rate": 3.4216216216216218e-06,
      "loss": 0.9854,
      "step": 6660
    },
    {
      "epoch": 26.68,
      "grad_norm": 3.311701774597168,
      "learning_rate": 3.3810810810810814e-06,
      "loss": 1.013,
      "step": 6670
    },
    {
      "epoch": 26.72,
      "grad_norm": 3.2490618228912354,
      "learning_rate": 3.3405405405405406e-06,
      "loss": 1.0041,
      "step": 6680
    },
    {
      "epoch": 26.76,
      "grad_norm": 2.956209421157837,
      "learning_rate": 3.3e-06,
      "loss": 0.9581,
      "step": 6690
    },
    {
      "epoch": 26.8,
      "grad_norm": 2.8339927196502686,
      "learning_rate": 3.2594594594594594e-06,
      "loss": 0.9663,
      "step": 6700
    },
    {
      "epoch": 26.84,
      "grad_norm": 3.079805374145508,
      "learning_rate": 3.218918918918919e-06,
      "loss": 0.9927,
      "step": 6710
    },
    {
      "epoch": 26.88,
      "grad_norm": 3.4998013973236084,
      "learning_rate": 3.1783783783783786e-06,
      "loss": 0.946,
      "step": 6720
    },
    {
      "epoch": 26.92,
      "grad_norm": 3.1387603282928467,
      "learning_rate": 3.1378378378378378e-06,
      "loss": 1.0302,
      "step": 6730
    },
    {
      "epoch": 26.96,
      "grad_norm": 3.160545825958252,
      "learning_rate": 3.0972972972972974e-06,
      "loss": 0.9893,
      "step": 6740
    },
    {
      "epoch": 27.0,
      "grad_norm": 3.666139602661133,
      "learning_rate": 3.056756756756757e-06,
      "loss": 0.9171,
      "step": 6750
    },
    {
      "epoch": 27.04,
      "grad_norm": 3.6139326095581055,
      "learning_rate": 3.016216216216216e-06,
      "loss": 0.9479,
      "step": 6760
    },
    {
      "epoch": 27.08,
      "grad_norm": 2.7635936737060547,
      "learning_rate": 2.9756756756756758e-06,
      "loss": 0.9296,
      "step": 6770
    },
    {
      "epoch": 27.12,
      "grad_norm": 3.207040786743164,
      "learning_rate": 2.9351351351351354e-06,
      "loss": 0.9972,
      "step": 6780
    },
    {
      "epoch": 27.16,
      "grad_norm": 2.9000661373138428,
      "learning_rate": 2.8945945945945945e-06,
      "loss": 0.9361,
      "step": 6790
    },
    {
      "epoch": 27.2,
      "grad_norm": 3.683936357498169,
      "learning_rate": 2.854054054054054e-06,
      "loss": 1.0062,
      "step": 6800
    },
    {
      "epoch": 27.24,
      "grad_norm": 3.1213860511779785,
      "learning_rate": 2.8135135135135138e-06,
      "loss": 0.9724,
      "step": 6810
    },
    {
      "epoch": 27.28,
      "grad_norm": 3.2061920166015625,
      "learning_rate": 2.772972972972973e-06,
      "loss": 0.9127,
      "step": 6820
    },
    {
      "epoch": 27.32,
      "grad_norm": 3.2843854427337646,
      "learning_rate": 2.7324324324324326e-06,
      "loss": 0.9179,
      "step": 6830
    },
    {
      "epoch": 27.36,
      "grad_norm": 2.9420459270477295,
      "learning_rate": 2.6918918918918917e-06,
      "loss": 0.9171,
      "step": 6840
    },
    {
      "epoch": 27.4,
      "grad_norm": 3.2216475009918213,
      "learning_rate": 2.6513513513513513e-06,
      "loss": 0.9978,
      "step": 6850
    },
    {
      "epoch": 27.44,
      "grad_norm": 3.0847039222717285,
      "learning_rate": 2.610810810810811e-06,
      "loss": 0.9741,
      "step": 6860
    },
    {
      "epoch": 27.48,
      "grad_norm": 3.215877056121826,
      "learning_rate": 2.57027027027027e-06,
      "loss": 0.9698,
      "step": 6870
    },
    {
      "epoch": 27.52,
      "grad_norm": 3.447396993637085,
      "learning_rate": 2.5297297297297297e-06,
      "loss": 0.9839,
      "step": 6880
    },
    {
      "epoch": 27.56,
      "grad_norm": 3.2726011276245117,
      "learning_rate": 2.4891891891891893e-06,
      "loss": 1.0021,
      "step": 6890
    },
    {
      "epoch": 27.6,
      "grad_norm": 3.606888771057129,
      "learning_rate": 2.4486486486486485e-06,
      "loss": 0.9656,
      "step": 6900
    },
    {
      "epoch": 27.64,
      "grad_norm": 3.431452751159668,
      "learning_rate": 2.408108108108108e-06,
      "loss": 0.9469,
      "step": 6910
    },
    {
      "epoch": 27.68,
      "grad_norm": 3.8170886039733887,
      "learning_rate": 2.3675675675675677e-06,
      "loss": 1.0061,
      "step": 6920
    },
    {
      "epoch": 27.72,
      "grad_norm": 3.1724414825439453,
      "learning_rate": 2.327027027027027e-06,
      "loss": 0.9341,
      "step": 6930
    },
    {
      "epoch": 27.76,
      "grad_norm": 3.534543991088867,
      "learning_rate": 2.2864864864864865e-06,
      "loss": 0.9591,
      "step": 6940
    },
    {
      "epoch": 27.8,
      "grad_norm": 3.3999452590942383,
      "learning_rate": 2.245945945945946e-06,
      "loss": 0.9517,
      "step": 6950
    },
    {
      "epoch": 27.84,
      "grad_norm": 3.3074090480804443,
      "learning_rate": 2.2054054054054053e-06,
      "loss": 0.9762,
      "step": 6960
    },
    {
      "epoch": 27.88,
      "grad_norm": 3.209118604660034,
      "learning_rate": 2.164864864864865e-06,
      "loss": 0.9686,
      "step": 6970
    },
    {
      "epoch": 27.92,
      "grad_norm": 3.2969624996185303,
      "learning_rate": 2.124324324324324e-06,
      "loss": 0.9676,
      "step": 6980
    },
    {
      "epoch": 27.96,
      "grad_norm": 3.327712297439575,
      "learning_rate": 2.0837837837837837e-06,
      "loss": 0.9792,
      "step": 6990
    },
    {
      "epoch": 28.0,
      "grad_norm": 3.4448280334472656,
      "learning_rate": 2.0432432432432433e-06,
      "loss": 1.0084,
      "step": 7000
    },
    {
      "epoch": 28.04,
      "grad_norm": 4.075705528259277,
      "learning_rate": 2.0027027027027025e-06,
      "loss": 0.9428,
      "step": 7010
    },
    {
      "epoch": 28.08,
      "grad_norm": 3.612226724624634,
      "learning_rate": 1.962162162162162e-06,
      "loss": 0.9377,
      "step": 7020
    },
    {
      "epoch": 28.12,
      "grad_norm": 3.1055665016174316,
      "learning_rate": 1.9216216216216217e-06,
      "loss": 0.9649,
      "step": 7030
    },
    {
      "epoch": 28.16,
      "grad_norm": 3.5644311904907227,
      "learning_rate": 1.8810810810810811e-06,
      "loss": 0.8899,
      "step": 7040
    },
    {
      "epoch": 28.2,
      "grad_norm": 3.483027696609497,
      "learning_rate": 1.8405405405405405e-06,
      "loss": 0.963,
      "step": 7050
    },
    {
      "epoch": 28.24,
      "grad_norm": 3.7555525302886963,
      "learning_rate": 1.8e-06,
      "loss": 0.9392,
      "step": 7060
    },
    {
      "epoch": 28.28,
      "grad_norm": 3.8520748615264893,
      "learning_rate": 1.7594594594594595e-06,
      "loss": 0.9863,
      "step": 7070
    },
    {
      "epoch": 28.32,
      "grad_norm": 3.772256374359131,
      "learning_rate": 1.718918918918919e-06,
      "loss": 0.9757,
      "step": 7080
    },
    {
      "epoch": 28.36,
      "grad_norm": 3.0686421394348145,
      "learning_rate": 1.6783783783783783e-06,
      "loss": 0.9727,
      "step": 7090
    },
    {
      "epoch": 28.4,
      "grad_norm": 3.90561580657959,
      "learning_rate": 1.6378378378378377e-06,
      "loss": 0.9128,
      "step": 7100
    },
    {
      "epoch": 28.44,
      "grad_norm": 3.0084195137023926,
      "learning_rate": 1.5972972972972973e-06,
      "loss": 0.9852,
      "step": 7110
    },
    {
      "epoch": 28.48,
      "grad_norm": 3.2840442657470703,
      "learning_rate": 1.5567567567567567e-06,
      "loss": 0.9155,
      "step": 7120
    },
    {
      "epoch": 28.52,
      "grad_norm": 3.183075428009033,
      "learning_rate": 1.5162162162162161e-06,
      "loss": 0.9558,
      "step": 7130
    },
    {
      "epoch": 28.56,
      "grad_norm": 3.1489341259002686,
      "learning_rate": 1.4756756756756757e-06,
      "loss": 0.9071,
      "step": 7140
    },
    {
      "epoch": 28.6,
      "grad_norm": 3.192004680633545,
      "learning_rate": 1.4351351351351351e-06,
      "loss": 0.9283,
      "step": 7150
    },
    {
      "epoch": 28.64,
      "grad_norm": 3.3101115226745605,
      "learning_rate": 1.3945945945945945e-06,
      "loss": 0.9279,
      "step": 7160
    },
    {
      "epoch": 28.68,
      "grad_norm": 3.244718551635742,
      "learning_rate": 1.354054054054054e-06,
      "loss": 1.013,
      "step": 7170
    },
    {
      "epoch": 28.72,
      "grad_norm": 3.2561609745025635,
      "learning_rate": 1.3135135135135135e-06,
      "loss": 0.9188,
      "step": 7180
    },
    {
      "epoch": 28.76,
      "grad_norm": 3.6618030071258545,
      "learning_rate": 1.272972972972973e-06,
      "loss": 0.9668,
      "step": 7190
    },
    {
      "epoch": 28.8,
      "grad_norm": 3.5634162425994873,
      "learning_rate": 1.2324324324324323e-06,
      "loss": 1.0075,
      "step": 7200
    },
    {
      "epoch": 28.84,
      "grad_norm": 3.614713430404663,
      "learning_rate": 1.191891891891892e-06,
      "loss": 0.9085,
      "step": 7210
    },
    {
      "epoch": 28.88,
      "grad_norm": 3.561319589614868,
      "learning_rate": 1.1513513513513513e-06,
      "loss": 0.9701,
      "step": 7220
    },
    {
      "epoch": 28.92,
      "grad_norm": 3.449606418609619,
      "learning_rate": 1.110810810810811e-06,
      "loss": 0.9376,
      "step": 7230
    },
    {
      "epoch": 28.96,
      "grad_norm": 3.2527658939361572,
      "learning_rate": 1.0702702702702703e-06,
      "loss": 0.999,
      "step": 7240
    },
    {
      "epoch": 29.0,
      "grad_norm": 3.0054728984832764,
      "learning_rate": 1.02972972972973e-06,
      "loss": 1.0183,
      "step": 7250
    },
    {
      "epoch": 29.04,
      "grad_norm": 3.5283288955688477,
      "learning_rate": 9.891891891891893e-07,
      "loss": 0.929,
      "step": 7260
    },
    {
      "epoch": 29.08,
      "grad_norm": 3.2983362674713135,
      "learning_rate": 9.486486486486488e-07,
      "loss": 0.9344,
      "step": 7270
    },
    {
      "epoch": 29.12,
      "grad_norm": 3.460405111312866,
      "learning_rate": 9.081081081081081e-07,
      "loss": 0.9557,
      "step": 7280
    },
    {
      "epoch": 29.16,
      "grad_norm": 2.9659223556518555,
      "learning_rate": 8.675675675675676e-07,
      "loss": 0.9943,
      "step": 7290
    },
    {
      "epoch": 29.2,
      "grad_norm": 3.5510547161102295,
      "learning_rate": 8.27027027027027e-07,
      "loss": 0.9996,
      "step": 7300
    },
    {
      "epoch": 29.24,
      "grad_norm": 3.4926064014434814,
      "learning_rate": 7.864864864864865e-07,
      "loss": 0.9576,
      "step": 7310
    },
    {
      "epoch": 29.28,
      "grad_norm": 2.9598629474639893,
      "learning_rate": 7.459459459459459e-07,
      "loss": 0.9324,
      "step": 7320
    },
    {
      "epoch": 29.32,
      "grad_norm": 3.5130834579467773,
      "learning_rate": 7.054054054054054e-07,
      "loss": 0.9426,
      "step": 7330
    },
    {
      "epoch": 29.36,
      "grad_norm": 3.591768503189087,
      "learning_rate": 6.648648648648649e-07,
      "loss": 0.9142,
      "step": 7340
    },
    {
      "epoch": 29.4,
      "grad_norm": 2.946486473083496,
      "learning_rate": 6.243243243243243e-07,
      "loss": 0.8988,
      "step": 7350
    },
    {
      "epoch": 29.44,
      "grad_norm": 2.9660134315490723,
      "learning_rate": 5.837837837837838e-07,
      "loss": 1.0173,
      "step": 7360
    },
    {
      "epoch": 29.48,
      "grad_norm": 3.175981044769287,
      "learning_rate": 5.432432432432432e-07,
      "loss": 0.9865,
      "step": 7370
    },
    {
      "epoch": 29.52,
      "grad_norm": 3.2397520542144775,
      "learning_rate": 5.027027027027027e-07,
      "loss": 0.9787,
      "step": 7380
    },
    {
      "epoch": 29.56,
      "grad_norm": 3.557844400405884,
      "learning_rate": 4.621621621621622e-07,
      "loss": 1.0008,
      "step": 7390
    },
    {
      "epoch": 29.6,
      "grad_norm": 3.111210584640503,
      "learning_rate": 4.2162162162162164e-07,
      "loss": 0.935,
      "step": 7400
    }
  ],
  "logging_steps": 10,
  "max_steps": 7500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 30,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.0895671680761856e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
