{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.878460816518067,
  "eval_steps": 500,
  "global_step": 2600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018770530267480056,
      "grad_norm": 1.1202815771102905,
      "learning_rate": 2.7e-06,
      "loss": 2.8165,
      "step": 10
    },
    {
      "epoch": 0.03754106053496011,
      "grad_norm": 1.282605528831482,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 2.8241,
      "step": 20
    },
    {
      "epoch": 0.05631159080244017,
      "grad_norm": 1.1786776781082153,
      "learning_rate": 8.7e-06,
      "loss": 2.795,
      "step": 30
    },
    {
      "epoch": 0.07508212106992022,
      "grad_norm": 1.3664096593856812,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 2.8472,
      "step": 40
    },
    {
      "epoch": 0.09385265133740028,
      "grad_norm": 1.1133095026016235,
      "learning_rate": 1.47e-05,
      "loss": 2.7466,
      "step": 50
    },
    {
      "epoch": 0.11262318160488034,
      "grad_norm": 1.4065557718276978,
      "learning_rate": 1.77e-05,
      "loss": 2.6504,
      "step": 60
    },
    {
      "epoch": 0.1313937118723604,
      "grad_norm": 1.7345391511917114,
      "learning_rate": 2.07e-05,
      "loss": 2.6131,
      "step": 70
    },
    {
      "epoch": 0.15016424213984045,
      "grad_norm": 1.5699130296707153,
      "learning_rate": 2.37e-05,
      "loss": 2.5351,
      "step": 80
    },
    {
      "epoch": 0.1689347724073205,
      "grad_norm": 1.9949438571929932,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.3406,
      "step": 90
    },
    {
      "epoch": 0.18770530267480057,
      "grad_norm": 2.472611427307129,
      "learning_rate": 2.97e-05,
      "loss": 2.1522,
      "step": 100
    },
    {
      "epoch": 0.2064758329422806,
      "grad_norm": 2.6940503120422363,
      "learning_rate": 2.9894736842105264e-05,
      "loss": 1.8667,
      "step": 110
    },
    {
      "epoch": 0.22524636320976069,
      "grad_norm": 2.631016254425049,
      "learning_rate": 2.9777777777777777e-05,
      "loss": 1.6395,
      "step": 120
    },
    {
      "epoch": 0.24401689347724073,
      "grad_norm": 2.3752858638763428,
      "learning_rate": 2.9660818713450293e-05,
      "loss": 1.4721,
      "step": 130
    },
    {
      "epoch": 0.2627874237447208,
      "grad_norm": 1.7891391515731812,
      "learning_rate": 2.9543859649122806e-05,
      "loss": 1.2358,
      "step": 140
    },
    {
      "epoch": 0.28155795401220085,
      "grad_norm": 2.395554542541504,
      "learning_rate": 2.9426900584795322e-05,
      "loss": 1.2231,
      "step": 150
    },
    {
      "epoch": 0.3003284842796809,
      "grad_norm": 1.5866427421569824,
      "learning_rate": 2.9309941520467835e-05,
      "loss": 1.1338,
      "step": 160
    },
    {
      "epoch": 0.31909901454716094,
      "grad_norm": 1.8508955240249634,
      "learning_rate": 2.919298245614035e-05,
      "loss": 1.1657,
      "step": 170
    },
    {
      "epoch": 0.337869544814641,
      "grad_norm": 3.399611234664917,
      "learning_rate": 2.9076023391812864e-05,
      "loss": 1.1155,
      "step": 180
    },
    {
      "epoch": 0.3566400750821211,
      "grad_norm": 1.3900465965270996,
      "learning_rate": 2.895906432748538e-05,
      "loss": 1.0131,
      "step": 190
    },
    {
      "epoch": 0.37541060534960113,
      "grad_norm": 1.222495436668396,
      "learning_rate": 2.8842105263157897e-05,
      "loss": 1.1262,
      "step": 200
    },
    {
      "epoch": 0.3941811356170812,
      "grad_norm": 1.7063308954238892,
      "learning_rate": 2.872514619883041e-05,
      "loss": 1.1066,
      "step": 210
    },
    {
      "epoch": 0.4129516658845612,
      "grad_norm": 1.3656322956085205,
      "learning_rate": 2.8608187134502926e-05,
      "loss": 1.0642,
      "step": 220
    },
    {
      "epoch": 0.43172219615204127,
      "grad_norm": 1.4196778535842896,
      "learning_rate": 2.849122807017544e-05,
      "loss": 1.0222,
      "step": 230
    },
    {
      "epoch": 0.45049272641952137,
      "grad_norm": 1.4950852394104004,
      "learning_rate": 2.8374269005847955e-05,
      "loss": 1.1129,
      "step": 240
    },
    {
      "epoch": 0.4692632566870014,
      "grad_norm": 1.4057639837265015,
      "learning_rate": 2.8257309941520468e-05,
      "loss": 1.0533,
      "step": 250
    },
    {
      "epoch": 0.48803378695448146,
      "grad_norm": 1.5042389631271362,
      "learning_rate": 2.8140350877192984e-05,
      "loss": 1.0232,
      "step": 260
    },
    {
      "epoch": 0.5068043172219615,
      "grad_norm": 1.5826506614685059,
      "learning_rate": 2.8023391812865497e-05,
      "loss": 1.0451,
      "step": 270
    },
    {
      "epoch": 0.5255748474894416,
      "grad_norm": 1.4087214469909668,
      "learning_rate": 2.7906432748538013e-05,
      "loss": 1.0618,
      "step": 280
    },
    {
      "epoch": 0.5443453777569216,
      "grad_norm": 1.649901270866394,
      "learning_rate": 2.7789473684210526e-05,
      "loss": 1.0691,
      "step": 290
    },
    {
      "epoch": 0.5631159080244017,
      "grad_norm": 1.2997617721557617,
      "learning_rate": 2.7672514619883042e-05,
      "loss": 1.1502,
      "step": 300
    },
    {
      "epoch": 0.5818864382918817,
      "grad_norm": 1.2227699756622314,
      "learning_rate": 2.7555555555555555e-05,
      "loss": 1.0637,
      "step": 310
    },
    {
      "epoch": 0.6006569685593618,
      "grad_norm": 1.534529209136963,
      "learning_rate": 2.743859649122807e-05,
      "loss": 1.0155,
      "step": 320
    },
    {
      "epoch": 0.6194274988268419,
      "grad_norm": 1.3677250146865845,
      "learning_rate": 2.7321637426900584e-05,
      "loss": 1.0909,
      "step": 330
    },
    {
      "epoch": 0.6381980290943219,
      "grad_norm": 1.306428074836731,
      "learning_rate": 2.72046783625731e-05,
      "loss": 1.0056,
      "step": 340
    },
    {
      "epoch": 0.656968559361802,
      "grad_norm": 1.3206686973571777,
      "learning_rate": 2.7087719298245613e-05,
      "loss": 1.0213,
      "step": 350
    },
    {
      "epoch": 0.675739089629282,
      "grad_norm": 1.320477843284607,
      "learning_rate": 2.697076023391813e-05,
      "loss": 1.0881,
      "step": 360
    },
    {
      "epoch": 0.6945096198967621,
      "grad_norm": 1.7826400995254517,
      "learning_rate": 2.6853801169590643e-05,
      "loss": 1.0384,
      "step": 370
    },
    {
      "epoch": 0.7132801501642422,
      "grad_norm": 1.3410555124282837,
      "learning_rate": 2.673684210526316e-05,
      "loss": 1.0948,
      "step": 380
    },
    {
      "epoch": 0.7320506804317222,
      "grad_norm": 0.943024754524231,
      "learning_rate": 2.6619883040935672e-05,
      "loss": 1.0217,
      "step": 390
    },
    {
      "epoch": 0.7508212106992023,
      "grad_norm": 1.2511401176452637,
      "learning_rate": 2.6502923976608188e-05,
      "loss": 1.024,
      "step": 400
    },
    {
      "epoch": 0.7695917409666823,
      "grad_norm": 1.409589409828186,
      "learning_rate": 2.63859649122807e-05,
      "loss": 1.088,
      "step": 410
    },
    {
      "epoch": 0.7883622712341624,
      "grad_norm": 1.7547527551651,
      "learning_rate": 2.6269005847953217e-05,
      "loss": 1.1214,
      "step": 420
    },
    {
      "epoch": 0.8071328015016425,
      "grad_norm": 1.335442066192627,
      "learning_rate": 2.615204678362573e-05,
      "loss": 1.0792,
      "step": 430
    },
    {
      "epoch": 0.8259033317691225,
      "grad_norm": 1.1787079572677612,
      "learning_rate": 2.6035087719298246e-05,
      "loss": 1.0765,
      "step": 440
    },
    {
      "epoch": 0.8446738620366026,
      "grad_norm": 1.3021979331970215,
      "learning_rate": 2.591812865497076e-05,
      "loss": 1.0423,
      "step": 450
    },
    {
      "epoch": 0.8634443923040825,
      "grad_norm": 1.3173191547393799,
      "learning_rate": 2.5801169590643275e-05,
      "loss": 1.0822,
      "step": 460
    },
    {
      "epoch": 0.8822149225715626,
      "grad_norm": 1.2684398889541626,
      "learning_rate": 2.568421052631579e-05,
      "loss": 1.0734,
      "step": 470
    },
    {
      "epoch": 0.9009854528390427,
      "grad_norm": 1.5713227987289429,
      "learning_rate": 2.5567251461988304e-05,
      "loss": 1.0648,
      "step": 480
    },
    {
      "epoch": 0.9197559831065227,
      "grad_norm": 1.6503633260726929,
      "learning_rate": 2.545029239766082e-05,
      "loss": 0.9946,
      "step": 490
    },
    {
      "epoch": 0.9385265133740028,
      "grad_norm": 1.2162041664123535,
      "learning_rate": 2.5333333333333334e-05,
      "loss": 1.0706,
      "step": 500
    },
    {
      "epoch": 0.9572970436414828,
      "grad_norm": 1.2371727228164673,
      "learning_rate": 2.521637426900585e-05,
      "loss": 1.0901,
      "step": 510
    },
    {
      "epoch": 0.9760675739089629,
      "grad_norm": 1.219107747077942,
      "learning_rate": 2.5099415204678363e-05,
      "loss": 1.0431,
      "step": 520
    },
    {
      "epoch": 0.994838104176443,
      "grad_norm": 1.7174878120422363,
      "learning_rate": 2.498245614035088e-05,
      "loss": 1.0582,
      "step": 530
    },
    {
      "epoch": 1.013139371187236,
      "grad_norm": 2.534735918045044,
      "learning_rate": 2.4865497076023392e-05,
      "loss": 1.0967,
      "step": 540
    },
    {
      "epoch": 1.0319099014547162,
      "grad_norm": 1.1446977853775024,
      "learning_rate": 2.4748538011695908e-05,
      "loss": 1.0246,
      "step": 550
    },
    {
      "epoch": 1.050680431722196,
      "grad_norm": 1.334630012512207,
      "learning_rate": 2.463157894736842e-05,
      "loss": 1.0398,
      "step": 560
    },
    {
      "epoch": 1.0694509619896762,
      "grad_norm": 1.353432297706604,
      "learning_rate": 2.4514619883040937e-05,
      "loss": 1.0354,
      "step": 570
    },
    {
      "epoch": 1.0882214922571563,
      "grad_norm": 1.321823239326477,
      "learning_rate": 2.439766081871345e-05,
      "loss": 1.0681,
      "step": 580
    },
    {
      "epoch": 1.1069920225246364,
      "grad_norm": 1.355433702468872,
      "learning_rate": 2.4280701754385966e-05,
      "loss": 1.0449,
      "step": 590
    },
    {
      "epoch": 1.1257625527921165,
      "grad_norm": 1.6193524599075317,
      "learning_rate": 2.416374269005848e-05,
      "loss": 1.0227,
      "step": 600
    },
    {
      "epoch": 1.1445330830595966,
      "grad_norm": 1.494401454925537,
      "learning_rate": 2.4046783625730995e-05,
      "loss": 1.0463,
      "step": 610
    },
    {
      "epoch": 1.1633036133270764,
      "grad_norm": 1.2029706239700317,
      "learning_rate": 2.392982456140351e-05,
      "loss": 1.1024,
      "step": 620
    },
    {
      "epoch": 1.1820741435945565,
      "grad_norm": 1.4653022289276123,
      "learning_rate": 2.3812865497076025e-05,
      "loss": 1.0378,
      "step": 630
    },
    {
      "epoch": 1.2008446738620366,
      "grad_norm": 1.2546745538711548,
      "learning_rate": 2.3695906432748537e-05,
      "loss": 1.0305,
      "step": 640
    },
    {
      "epoch": 1.2196152041295167,
      "grad_norm": 1.4774360656738281,
      "learning_rate": 2.3578947368421054e-05,
      "loss": 1.092,
      "step": 650
    },
    {
      "epoch": 1.2383857343969966,
      "grad_norm": 1.3418759107589722,
      "learning_rate": 2.3461988304093567e-05,
      "loss": 1.0652,
      "step": 660
    },
    {
      "epoch": 1.2571562646644767,
      "grad_norm": 1.307619571685791,
      "learning_rate": 2.3345029239766083e-05,
      "loss": 1.0644,
      "step": 670
    },
    {
      "epoch": 1.2759267949319568,
      "grad_norm": 1.4194145202636719,
      "learning_rate": 2.3228070175438596e-05,
      "loss": 1.1005,
      "step": 680
    },
    {
      "epoch": 1.294697325199437,
      "grad_norm": 1.375868320465088,
      "learning_rate": 2.3111111111111112e-05,
      "loss": 1.0844,
      "step": 690
    },
    {
      "epoch": 1.313467855466917,
      "grad_norm": 1.4797358512878418,
      "learning_rate": 2.2994152046783625e-05,
      "loss": 1.0884,
      "step": 700
    },
    {
      "epoch": 1.3322383857343971,
      "grad_norm": 1.2240514755249023,
      "learning_rate": 2.287719298245614e-05,
      "loss": 1.0435,
      "step": 710
    },
    {
      "epoch": 1.351008916001877,
      "grad_norm": 1.3153331279754639,
      "learning_rate": 2.2760233918128654e-05,
      "loss": 0.965,
      "step": 720
    },
    {
      "epoch": 1.369779446269357,
      "grad_norm": 1.3371769189834595,
      "learning_rate": 2.264327485380117e-05,
      "loss": 0.9801,
      "step": 730
    },
    {
      "epoch": 1.3885499765368372,
      "grad_norm": 1.6925606727600098,
      "learning_rate": 2.2526315789473686e-05,
      "loss": 1.0811,
      "step": 740
    },
    {
      "epoch": 1.407320506804317,
      "grad_norm": 1.5866782665252686,
      "learning_rate": 2.24093567251462e-05,
      "loss": 1.064,
      "step": 750
    },
    {
      "epoch": 1.4260910370717972,
      "grad_norm": 1.736189603805542,
      "learning_rate": 2.2292397660818716e-05,
      "loss": 1.0982,
      "step": 760
    },
    {
      "epoch": 1.4448615673392773,
      "grad_norm": 1.33293616771698,
      "learning_rate": 2.217543859649123e-05,
      "loss": 1.0992,
      "step": 770
    },
    {
      "epoch": 1.4636320976067574,
      "grad_norm": 1.2446380853652954,
      "learning_rate": 2.2058479532163745e-05,
      "loss": 1.0007,
      "step": 780
    },
    {
      "epoch": 1.4824026278742375,
      "grad_norm": 1.2714040279388428,
      "learning_rate": 2.1941520467836258e-05,
      "loss": 1.0605,
      "step": 790
    },
    {
      "epoch": 1.5011731581417176,
      "grad_norm": 1.2200591564178467,
      "learning_rate": 2.1824561403508774e-05,
      "loss": 1.0492,
      "step": 800
    },
    {
      "epoch": 1.5199436884091977,
      "grad_norm": 1.2473233938217163,
      "learning_rate": 2.1707602339181287e-05,
      "loss": 1.0685,
      "step": 810
    },
    {
      "epoch": 1.5387142186766776,
      "grad_norm": 1.3729077577590942,
      "learning_rate": 2.1590643274853803e-05,
      "loss": 1.0062,
      "step": 820
    },
    {
      "epoch": 1.5574847489441577,
      "grad_norm": 1.3853470087051392,
      "learning_rate": 2.1473684210526316e-05,
      "loss": 1.0604,
      "step": 830
    },
    {
      "epoch": 1.5762552792116378,
      "grad_norm": 1.2622567415237427,
      "learning_rate": 2.1356725146198832e-05,
      "loss": 0.9834,
      "step": 840
    },
    {
      "epoch": 1.5950258094791177,
      "grad_norm": 1.347294807434082,
      "learning_rate": 2.1239766081871345e-05,
      "loss": 0.9519,
      "step": 850
    },
    {
      "epoch": 1.6137963397465978,
      "grad_norm": 1.4210203886032104,
      "learning_rate": 2.112280701754386e-05,
      "loss": 1.0699,
      "step": 860
    },
    {
      "epoch": 1.6325668700140779,
      "grad_norm": 1.3828288316726685,
      "learning_rate": 2.1005847953216374e-05,
      "loss": 1.001,
      "step": 870
    },
    {
      "epoch": 1.651337400281558,
      "grad_norm": 1.3890905380249023,
      "learning_rate": 2.088888888888889e-05,
      "loss": 1.061,
      "step": 880
    },
    {
      "epoch": 1.670107930549038,
      "grad_norm": 1.5771127939224243,
      "learning_rate": 2.0771929824561403e-05,
      "loss": 1.0248,
      "step": 890
    },
    {
      "epoch": 1.6888784608165182,
      "grad_norm": 1.4189597368240356,
      "learning_rate": 2.065497076023392e-05,
      "loss": 1.0537,
      "step": 900
    },
    {
      "epoch": 1.7076489910839983,
      "grad_norm": 1.462896466255188,
      "learning_rate": 2.0538011695906432e-05,
      "loss": 1.0017,
      "step": 910
    },
    {
      "epoch": 1.7264195213514781,
      "grad_norm": 1.6053555011749268,
      "learning_rate": 2.042105263157895e-05,
      "loss": 1.0803,
      "step": 920
    },
    {
      "epoch": 1.7451900516189582,
      "grad_norm": 1.34902822971344,
      "learning_rate": 2.030409356725146e-05,
      "loss": 0.998,
      "step": 930
    },
    {
      "epoch": 1.7639605818864383,
      "grad_norm": 1.7094680070877075,
      "learning_rate": 2.0187134502923978e-05,
      "loss": 1.011,
      "step": 940
    },
    {
      "epoch": 1.7827311121539182,
      "grad_norm": 1.4561567306518555,
      "learning_rate": 2.007017543859649e-05,
      "loss": 1.1072,
      "step": 950
    },
    {
      "epoch": 1.8015016424213983,
      "grad_norm": 1.8425699472427368,
      "learning_rate": 1.9953216374269007e-05,
      "loss": 1.1422,
      "step": 960
    },
    {
      "epoch": 1.8202721726888784,
      "grad_norm": 1.5202440023422241,
      "learning_rate": 1.983625730994152e-05,
      "loss": 1.0513,
      "step": 970
    },
    {
      "epoch": 1.8390427029563585,
      "grad_norm": 1.406469464302063,
      "learning_rate": 1.9719298245614036e-05,
      "loss": 1.0324,
      "step": 980
    },
    {
      "epoch": 1.8578132332238386,
      "grad_norm": 1.4065542221069336,
      "learning_rate": 1.960233918128655e-05,
      "loss": 1.0837,
      "step": 990
    },
    {
      "epoch": 1.8765837634913187,
      "grad_norm": 1.5772106647491455,
      "learning_rate": 1.9485380116959065e-05,
      "loss": 1.0792,
      "step": 1000
    },
    {
      "epoch": 1.8953542937587988,
      "grad_norm": 1.605684757232666,
      "learning_rate": 1.936842105263158e-05,
      "loss": 1.0245,
      "step": 1010
    },
    {
      "epoch": 1.9141248240262787,
      "grad_norm": 1.245679497718811,
      "learning_rate": 1.9251461988304094e-05,
      "loss": 1.0145,
      "step": 1020
    },
    {
      "epoch": 1.9328953542937588,
      "grad_norm": 1.3203325271606445,
      "learning_rate": 1.913450292397661e-05,
      "loss": 1.0278,
      "step": 1030
    },
    {
      "epoch": 1.951665884561239,
      "grad_norm": 1.769954800605774,
      "learning_rate": 1.9017543859649123e-05,
      "loss": 1.0436,
      "step": 1040
    },
    {
      "epoch": 1.9704364148287188,
      "grad_norm": 1.9347093105316162,
      "learning_rate": 1.890058479532164e-05,
      "loss": 0.9938,
      "step": 1050
    },
    {
      "epoch": 1.9892069450961989,
      "grad_norm": 1.3198094367980957,
      "learning_rate": 1.8783625730994152e-05,
      "loss": 1.0381,
      "step": 1060
    },
    {
      "epoch": 2.007508212106992,
      "grad_norm": 1.2588292360305786,
      "learning_rate": 1.866666666666667e-05,
      "loss": 1.0522,
      "step": 1070
    },
    {
      "epoch": 2.026278742374472,
      "grad_norm": 1.579002857208252,
      "learning_rate": 1.854970760233918e-05,
      "loss": 1.0693,
      "step": 1080
    },
    {
      "epoch": 2.0450492726419522,
      "grad_norm": 1.3624385595321655,
      "learning_rate": 1.8432748538011698e-05,
      "loss": 0.9645,
      "step": 1090
    },
    {
      "epoch": 2.0638198029094323,
      "grad_norm": 1.4081312417984009,
      "learning_rate": 1.831578947368421e-05,
      "loss": 1.0955,
      "step": 1100
    },
    {
      "epoch": 2.0825903331769124,
      "grad_norm": 1.373418927192688,
      "learning_rate": 1.8198830409356727e-05,
      "loss": 1.0728,
      "step": 1110
    },
    {
      "epoch": 2.101360863444392,
      "grad_norm": 1.7192984819412231,
      "learning_rate": 1.808187134502924e-05,
      "loss": 1.0656,
      "step": 1120
    },
    {
      "epoch": 2.120131393711872,
      "grad_norm": 1.4918919801712036,
      "learning_rate": 1.7964912280701756e-05,
      "loss": 1.0517,
      "step": 1130
    },
    {
      "epoch": 2.1389019239793523,
      "grad_norm": 1.4917773008346558,
      "learning_rate": 1.784795321637427e-05,
      "loss": 1.0413,
      "step": 1140
    },
    {
      "epoch": 2.1576724542468324,
      "grad_norm": 1.4494433403015137,
      "learning_rate": 1.7730994152046785e-05,
      "loss": 0.9604,
      "step": 1150
    },
    {
      "epoch": 2.1764429845143125,
      "grad_norm": 1.420532464981079,
      "learning_rate": 1.7614035087719298e-05,
      "loss": 1.0082,
      "step": 1160
    },
    {
      "epoch": 2.1952135147817926,
      "grad_norm": 1.5197529792785645,
      "learning_rate": 1.7497076023391814e-05,
      "loss": 1.0278,
      "step": 1170
    },
    {
      "epoch": 2.2139840450492727,
      "grad_norm": 1.5958317518234253,
      "learning_rate": 1.7380116959064327e-05,
      "loss": 1.0505,
      "step": 1180
    },
    {
      "epoch": 2.232754575316753,
      "grad_norm": 1.3921489715576172,
      "learning_rate": 1.7263157894736843e-05,
      "loss": 0.9998,
      "step": 1190
    },
    {
      "epoch": 2.251525105584233,
      "grad_norm": 1.9152650833129883,
      "learning_rate": 1.7146198830409356e-05,
      "loss": 1.017,
      "step": 1200
    },
    {
      "epoch": 2.2702956358517126,
      "grad_norm": 1.8480682373046875,
      "learning_rate": 1.7029239766081872e-05,
      "loss": 1.0613,
      "step": 1210
    },
    {
      "epoch": 2.289066166119193,
      "grad_norm": 1.4356220960617065,
      "learning_rate": 1.6912280701754385e-05,
      "loss": 1.1073,
      "step": 1220
    },
    {
      "epoch": 2.3078366963866728,
      "grad_norm": 1.3845895528793335,
      "learning_rate": 1.67953216374269e-05,
      "loss": 1.0196,
      "step": 1230
    },
    {
      "epoch": 2.326607226654153,
      "grad_norm": 1.886409044265747,
      "learning_rate": 1.6678362573099414e-05,
      "loss": 1.045,
      "step": 1240
    },
    {
      "epoch": 2.345377756921633,
      "grad_norm": 1.6996678113937378,
      "learning_rate": 1.656140350877193e-05,
      "loss": 1.0034,
      "step": 1250
    },
    {
      "epoch": 2.364148287189113,
      "grad_norm": 1.7426698207855225,
      "learning_rate": 1.6444444444444444e-05,
      "loss": 1.1046,
      "step": 1260
    },
    {
      "epoch": 2.382918817456593,
      "grad_norm": 1.5431551933288574,
      "learning_rate": 1.632748538011696e-05,
      "loss": 1.0457,
      "step": 1270
    },
    {
      "epoch": 2.4016893477240733,
      "grad_norm": 1.847312569618225,
      "learning_rate": 1.6210526315789476e-05,
      "loss": 1.0826,
      "step": 1280
    },
    {
      "epoch": 2.4204598779915534,
      "grad_norm": 1.2768388986587524,
      "learning_rate": 1.609356725146199e-05,
      "loss": 0.9627,
      "step": 1290
    },
    {
      "epoch": 2.4392304082590335,
      "grad_norm": 1.666455626487732,
      "learning_rate": 1.5976608187134505e-05,
      "loss": 1.012,
      "step": 1300
    },
    {
      "epoch": 2.4580009385265136,
      "grad_norm": 1.4230018854141235,
      "learning_rate": 1.5859649122807018e-05,
      "loss": 1.0324,
      "step": 1310
    },
    {
      "epoch": 2.4767714687939932,
      "grad_norm": 1.637660264968872,
      "learning_rate": 1.5742690058479534e-05,
      "loss": 0.9725,
      "step": 1320
    },
    {
      "epoch": 2.4955419990614733,
      "grad_norm": 1.5576722621917725,
      "learning_rate": 1.5625730994152047e-05,
      "loss": 0.9815,
      "step": 1330
    },
    {
      "epoch": 2.5143125293289534,
      "grad_norm": 1.253407597541809,
      "learning_rate": 1.5508771929824563e-05,
      "loss": 1.0104,
      "step": 1340
    },
    {
      "epoch": 2.5330830595964335,
      "grad_norm": 1.6710493564605713,
      "learning_rate": 1.5391812865497076e-05,
      "loss": 1.0282,
      "step": 1350
    },
    {
      "epoch": 2.5518535898639136,
      "grad_norm": 1.32734215259552,
      "learning_rate": 1.5274853801169593e-05,
      "loss": 1.0024,
      "step": 1360
    },
    {
      "epoch": 2.5706241201313937,
      "grad_norm": 2.172443151473999,
      "learning_rate": 1.5157894736842105e-05,
      "loss": 1.0852,
      "step": 1370
    },
    {
      "epoch": 2.589394650398874,
      "grad_norm": 1.6400146484375,
      "learning_rate": 1.504093567251462e-05,
      "loss": 1.0048,
      "step": 1380
    },
    {
      "epoch": 2.608165180666354,
      "grad_norm": 1.722831130027771,
      "learning_rate": 1.4923976608187135e-05,
      "loss": 1.0722,
      "step": 1390
    },
    {
      "epoch": 2.626935710933834,
      "grad_norm": 1.612693428993225,
      "learning_rate": 1.480701754385965e-05,
      "loss": 0.9903,
      "step": 1400
    },
    {
      "epoch": 2.6457062412013137,
      "grad_norm": 1.493208646774292,
      "learning_rate": 1.4690058479532165e-05,
      "loss": 1.0556,
      "step": 1410
    },
    {
      "epoch": 2.6644767714687942,
      "grad_norm": 1.5064327716827393,
      "learning_rate": 1.457309941520468e-05,
      "loss": 1.0391,
      "step": 1420
    },
    {
      "epoch": 2.683247301736274,
      "grad_norm": 1.4316850900650024,
      "learning_rate": 1.4456140350877195e-05,
      "loss": 1.0647,
      "step": 1430
    },
    {
      "epoch": 2.702017832003754,
      "grad_norm": 1.4162139892578125,
      "learning_rate": 1.4339181286549709e-05,
      "loss": 1.0028,
      "step": 1440
    },
    {
      "epoch": 2.720788362271234,
      "grad_norm": 1.4964103698730469,
      "learning_rate": 1.4222222222222224e-05,
      "loss": 1.0125,
      "step": 1450
    },
    {
      "epoch": 2.739558892538714,
      "grad_norm": 1.5890921354293823,
      "learning_rate": 1.4105263157894738e-05,
      "loss": 1.0441,
      "step": 1460
    },
    {
      "epoch": 2.7583294228061943,
      "grad_norm": 1.5796644687652588,
      "learning_rate": 1.3988304093567253e-05,
      "loss": 1.0164,
      "step": 1470
    },
    {
      "epoch": 2.7770999530736744,
      "grad_norm": 1.2642537355422974,
      "learning_rate": 1.3871345029239767e-05,
      "loss": 1.0152,
      "step": 1480
    },
    {
      "epoch": 2.7958704833411545,
      "grad_norm": 1.5553851127624512,
      "learning_rate": 1.3754385964912282e-05,
      "loss": 1.0081,
      "step": 1490
    },
    {
      "epoch": 2.814641013608634,
      "grad_norm": 1.7099089622497559,
      "learning_rate": 1.3637426900584796e-05,
      "loss": 1.0918,
      "step": 1500
    },
    {
      "epoch": 2.8334115438761147,
      "grad_norm": 1.3786115646362305,
      "learning_rate": 1.3520467836257311e-05,
      "loss": 1.0436,
      "step": 1510
    },
    {
      "epoch": 2.8521820741435944,
      "grad_norm": 1.4524502754211426,
      "learning_rate": 1.3403508771929826e-05,
      "loss": 0.9892,
      "step": 1520
    },
    {
      "epoch": 2.8709526044110745,
      "grad_norm": 1.5974878072738647,
      "learning_rate": 1.328654970760234e-05,
      "loss": 1.0841,
      "step": 1530
    },
    {
      "epoch": 2.8897231346785546,
      "grad_norm": 1.3708829879760742,
      "learning_rate": 1.3169590643274855e-05,
      "loss": 1.0467,
      "step": 1540
    },
    {
      "epoch": 2.9084936649460347,
      "grad_norm": 1.5707378387451172,
      "learning_rate": 1.305263157894737e-05,
      "loss": 1.0191,
      "step": 1550
    },
    {
      "epoch": 2.9272641952135148,
      "grad_norm": 1.7329330444335938,
      "learning_rate": 1.2935672514619884e-05,
      "loss": 1.131,
      "step": 1560
    },
    {
      "epoch": 2.946034725480995,
      "grad_norm": 1.7056057453155518,
      "learning_rate": 1.2818713450292398e-05,
      "loss": 1.0417,
      "step": 1570
    },
    {
      "epoch": 2.964805255748475,
      "grad_norm": 1.6696583032608032,
      "learning_rate": 1.2701754385964913e-05,
      "loss": 0.9904,
      "step": 1580
    },
    {
      "epoch": 2.983575786015955,
      "grad_norm": 1.6169079542160034,
      "learning_rate": 1.2584795321637428e-05,
      "loss": 1.041,
      "step": 1590
    },
    {
      "epoch": 3.001877053026748,
      "grad_norm": 1.494532823562622,
      "learning_rate": 1.2467836257309942e-05,
      "loss": 0.9948,
      "step": 1600
    },
    {
      "epoch": 3.020647583294228,
      "grad_norm": 1.8781267404556274,
      "learning_rate": 1.2350877192982457e-05,
      "loss": 1.099,
      "step": 1610
    },
    {
      "epoch": 3.039418113561708,
      "grad_norm": 1.5553170442581177,
      "learning_rate": 1.2233918128654971e-05,
      "loss": 0.994,
      "step": 1620
    },
    {
      "epoch": 3.058188643829188,
      "grad_norm": 1.7817448377609253,
      "learning_rate": 1.2116959064327486e-05,
      "loss": 1.01,
      "step": 1630
    },
    {
      "epoch": 3.076959174096668,
      "grad_norm": 1.5768227577209473,
      "learning_rate": 1.2e-05,
      "loss": 1.0078,
      "step": 1640
    },
    {
      "epoch": 3.0957297043641483,
      "grad_norm": 1.6041589975357056,
      "learning_rate": 1.1883040935672515e-05,
      "loss": 1.0403,
      "step": 1650
    },
    {
      "epoch": 3.1145002346316284,
      "grad_norm": 1.4780499935150146,
      "learning_rate": 1.176608187134503e-05,
      "loss": 1.0155,
      "step": 1660
    },
    {
      "epoch": 3.1332707648991085,
      "grad_norm": 1.9231467247009277,
      "learning_rate": 1.1649122807017546e-05,
      "loss": 1.0379,
      "step": 1670
    },
    {
      "epoch": 3.1520412951665886,
      "grad_norm": 1.9592289924621582,
      "learning_rate": 1.153216374269006e-05,
      "loss": 0.9973,
      "step": 1680
    },
    {
      "epoch": 3.1708118254340687,
      "grad_norm": 1.3479511737823486,
      "learning_rate": 1.1415204678362575e-05,
      "loss": 0.9848,
      "step": 1690
    },
    {
      "epoch": 3.189582355701549,
      "grad_norm": 1.5661611557006836,
      "learning_rate": 1.129824561403509e-05,
      "loss": 0.9858,
      "step": 1700
    },
    {
      "epoch": 3.2083528859690285,
      "grad_norm": 1.7842655181884766,
      "learning_rate": 1.1181286549707604e-05,
      "loss": 1.0029,
      "step": 1710
    },
    {
      "epoch": 3.2271234162365086,
      "grad_norm": 1.5010117292404175,
      "learning_rate": 1.1064327485380118e-05,
      "loss": 1.0431,
      "step": 1720
    },
    {
      "epoch": 3.2458939465039887,
      "grad_norm": 1.644823670387268,
      "learning_rate": 1.0947368421052633e-05,
      "loss": 1.0319,
      "step": 1730
    },
    {
      "epoch": 3.2646644767714688,
      "grad_norm": 1.8209483623504639,
      "learning_rate": 1.0830409356725148e-05,
      "loss": 1.0204,
      "step": 1740
    },
    {
      "epoch": 3.283435007038949,
      "grad_norm": 1.3643527030944824,
      "learning_rate": 1.0713450292397662e-05,
      "loss": 1.0796,
      "step": 1750
    },
    {
      "epoch": 3.302205537306429,
      "grad_norm": 1.4273388385772705,
      "learning_rate": 1.0596491228070177e-05,
      "loss": 1.059,
      "step": 1760
    },
    {
      "epoch": 3.320976067573909,
      "grad_norm": 1.579237937927246,
      "learning_rate": 1.0479532163742691e-05,
      "loss": 1.0217,
      "step": 1770
    },
    {
      "epoch": 3.339746597841389,
      "grad_norm": 1.5533541440963745,
      "learning_rate": 1.0362573099415206e-05,
      "loss": 1.0337,
      "step": 1780
    },
    {
      "epoch": 3.3585171281088693,
      "grad_norm": 1.709679365158081,
      "learning_rate": 1.024561403508772e-05,
      "loss": 1.0106,
      "step": 1790
    },
    {
      "epoch": 3.377287658376349,
      "grad_norm": 1.5472240447998047,
      "learning_rate": 1.0128654970760235e-05,
      "loss": 1.0088,
      "step": 1800
    },
    {
      "epoch": 3.396058188643829,
      "grad_norm": 1.4067801237106323,
      "learning_rate": 1.001169590643275e-05,
      "loss": 1.0219,
      "step": 1810
    },
    {
      "epoch": 3.414828718911309,
      "grad_norm": 1.7091771364212036,
      "learning_rate": 9.894736842105264e-06,
      "loss": 1.045,
      "step": 1820
    },
    {
      "epoch": 3.4335992491787892,
      "grad_norm": 1.4322885274887085,
      "learning_rate": 9.777777777777779e-06,
      "loss": 1.027,
      "step": 1830
    },
    {
      "epoch": 3.4523697794462693,
      "grad_norm": 1.5077892541885376,
      "learning_rate": 9.660818713450293e-06,
      "loss": 1.0082,
      "step": 1840
    },
    {
      "epoch": 3.4711403097137494,
      "grad_norm": 1.268489122390747,
      "learning_rate": 9.543859649122808e-06,
      "loss": 1.0863,
      "step": 1850
    },
    {
      "epoch": 3.4899108399812295,
      "grad_norm": 1.5257548093795776,
      "learning_rate": 9.426900584795322e-06,
      "loss": 1.031,
      "step": 1860
    },
    {
      "epoch": 3.5086813702487096,
      "grad_norm": 1.824489712715149,
      "learning_rate": 9.309941520467837e-06,
      "loss": 1.0371,
      "step": 1870
    },
    {
      "epoch": 3.5274519005161897,
      "grad_norm": 1.5076593160629272,
      "learning_rate": 9.192982456140351e-06,
      "loss": 1.0074,
      "step": 1880
    },
    {
      "epoch": 3.5462224307836694,
      "grad_norm": 1.836136817932129,
      "learning_rate": 9.076023391812866e-06,
      "loss": 0.9704,
      "step": 1890
    },
    {
      "epoch": 3.56499296105115,
      "grad_norm": 1.548671841621399,
      "learning_rate": 8.959064327485379e-06,
      "loss": 1.0629,
      "step": 1900
    },
    {
      "epoch": 3.5837634913186296,
      "grad_norm": 1.6478148698806763,
      "learning_rate": 8.842105263157893e-06,
      "loss": 1.0487,
      "step": 1910
    },
    {
      "epoch": 3.6025340215861097,
      "grad_norm": 1.6225496530532837,
      "learning_rate": 8.725146198830408e-06,
      "loss": 1.103,
      "step": 1920
    },
    {
      "epoch": 3.62130455185359,
      "grad_norm": 1.6466907262802124,
      "learning_rate": 8.608187134502923e-06,
      "loss": 1.1095,
      "step": 1930
    },
    {
      "epoch": 3.64007508212107,
      "grad_norm": 1.7107810974121094,
      "learning_rate": 8.491228070175439e-06,
      "loss": 1.0009,
      "step": 1940
    },
    {
      "epoch": 3.65884561238855,
      "grad_norm": 2.519850969314575,
      "learning_rate": 8.374269005847953e-06,
      "loss": 1.1009,
      "step": 1950
    },
    {
      "epoch": 3.67761614265603,
      "grad_norm": 1.6257126331329346,
      "learning_rate": 8.257309941520468e-06,
      "loss": 1.0399,
      "step": 1960
    },
    {
      "epoch": 3.69638667292351,
      "grad_norm": 1.6370232105255127,
      "learning_rate": 8.140350877192983e-06,
      "loss": 1.0317,
      "step": 1970
    },
    {
      "epoch": 3.7151572031909903,
      "grad_norm": 1.7615156173706055,
      "learning_rate": 8.023391812865497e-06,
      "loss": 0.9933,
      "step": 1980
    },
    {
      "epoch": 3.7339277334584704,
      "grad_norm": 1.613864779472351,
      "learning_rate": 7.906432748538012e-06,
      "loss": 1.0115,
      "step": 1990
    },
    {
      "epoch": 3.75269826372595,
      "grad_norm": 1.840420126914978,
      "learning_rate": 7.789473684210526e-06,
      "loss": 1.04,
      "step": 2000
    },
    {
      "epoch": 3.77146879399343,
      "grad_norm": 1.6566011905670166,
      "learning_rate": 7.67251461988304e-06,
      "loss": 1.046,
      "step": 2010
    },
    {
      "epoch": 3.7902393242609103,
      "grad_norm": 2.0227200984954834,
      "learning_rate": 7.555555555555555e-06,
      "loss": 0.9864,
      "step": 2020
    },
    {
      "epoch": 3.8090098545283904,
      "grad_norm": 1.5745866298675537,
      "learning_rate": 7.438596491228071e-06,
      "loss": 0.9653,
      "step": 2030
    },
    {
      "epoch": 3.8277803847958705,
      "grad_norm": 1.433463215827942,
      "learning_rate": 7.321637426900585e-06,
      "loss": 1.061,
      "step": 2040
    },
    {
      "epoch": 3.8465509150633506,
      "grad_norm": 1.946406364440918,
      "learning_rate": 7.2046783625731e-06,
      "loss": 1.0047,
      "step": 2050
    },
    {
      "epoch": 3.8653214453308307,
      "grad_norm": 1.5832023620605469,
      "learning_rate": 7.087719298245614e-06,
      "loss": 0.9667,
      "step": 2060
    },
    {
      "epoch": 3.8840919755983108,
      "grad_norm": 1.608316421508789,
      "learning_rate": 6.970760233918129e-06,
      "loss": 1.0421,
      "step": 2070
    },
    {
      "epoch": 3.902862505865791,
      "grad_norm": 1.554136037826538,
      "learning_rate": 6.8538011695906435e-06,
      "loss": 1.0235,
      "step": 2080
    },
    {
      "epoch": 3.9216330361332705,
      "grad_norm": 1.7466925382614136,
      "learning_rate": 6.736842105263158e-06,
      "loss": 1.0247,
      "step": 2090
    },
    {
      "epoch": 3.940403566400751,
      "grad_norm": 1.4513527154922485,
      "learning_rate": 6.619883040935673e-06,
      "loss": 1.0203,
      "step": 2100
    },
    {
      "epoch": 3.9591740966682307,
      "grad_norm": 1.3342688083648682,
      "learning_rate": 6.502923976608187e-06,
      "loss": 1.0286,
      "step": 2110
    },
    {
      "epoch": 3.977944626935711,
      "grad_norm": 2.003624677658081,
      "learning_rate": 6.385964912280702e-06,
      "loss": 0.9924,
      "step": 2120
    },
    {
      "epoch": 3.996715157203191,
      "grad_norm": 1.51568603515625,
      "learning_rate": 6.269005847953216e-06,
      "loss": 1.0405,
      "step": 2130
    },
    {
      "epoch": 4.015016424213984,
      "grad_norm": 1.6357861757278442,
      "learning_rate": 6.152046783625732e-06,
      "loss": 1.0284,
      "step": 2140
    },
    {
      "epoch": 4.033786954481464,
      "grad_norm": 1.5280897617340088,
      "learning_rate": 6.035087719298246e-06,
      "loss": 1.0758,
      "step": 2150
    },
    {
      "epoch": 4.052557484748944,
      "grad_norm": 1.4867459535598755,
      "learning_rate": 5.918128654970761e-06,
      "loss": 1.0122,
      "step": 2160
    },
    {
      "epoch": 4.071328015016424,
      "grad_norm": 1.8861750364303589,
      "learning_rate": 5.801169590643275e-06,
      "loss": 1.0579,
      "step": 2170
    },
    {
      "epoch": 4.0900985452839045,
      "grad_norm": 1.4529989957809448,
      "learning_rate": 5.68421052631579e-06,
      "loss": 0.9826,
      "step": 2180
    },
    {
      "epoch": 4.108869075551384,
      "grad_norm": 1.6541259288787842,
      "learning_rate": 5.5672514619883045e-06,
      "loss": 0.9695,
      "step": 2190
    },
    {
      "epoch": 4.127639605818865,
      "grad_norm": 1.8165355920791626,
      "learning_rate": 5.450292397660819e-06,
      "loss": 0.9889,
      "step": 2200
    },
    {
      "epoch": 4.146410136086344,
      "grad_norm": 1.4899959564208984,
      "learning_rate": 5.333333333333334e-06,
      "loss": 1.0551,
      "step": 2210
    },
    {
      "epoch": 4.165180666353825,
      "grad_norm": 1.5350470542907715,
      "learning_rate": 5.216374269005848e-06,
      "loss": 0.9773,
      "step": 2220
    },
    {
      "epoch": 4.1839511966213045,
      "grad_norm": 1.768562912940979,
      "learning_rate": 5.099415204678363e-06,
      "loss": 0.9867,
      "step": 2230
    },
    {
      "epoch": 4.202721726888784,
      "grad_norm": 2.8757827281951904,
      "learning_rate": 4.982456140350877e-06,
      "loss": 1.0099,
      "step": 2240
    },
    {
      "epoch": 4.221492257156265,
      "grad_norm": 1.6470146179199219,
      "learning_rate": 4.865497076023392e-06,
      "loss": 1.0467,
      "step": 2250
    },
    {
      "epoch": 4.240262787423744,
      "grad_norm": 1.875237226486206,
      "learning_rate": 4.7485380116959065e-06,
      "loss": 0.9504,
      "step": 2260
    },
    {
      "epoch": 4.259033317691225,
      "grad_norm": 1.766755223274231,
      "learning_rate": 4.631578947368421e-06,
      "loss": 0.9833,
      "step": 2270
    },
    {
      "epoch": 4.277803847958705,
      "grad_norm": 1.6684540510177612,
      "learning_rate": 4.5146198830409364e-06,
      "loss": 1.0441,
      "step": 2280
    },
    {
      "epoch": 4.296574378226185,
      "grad_norm": 1.7133615016937256,
      "learning_rate": 4.397660818713451e-06,
      "loss": 0.9943,
      "step": 2290
    },
    {
      "epoch": 4.315344908493665,
      "grad_norm": 1.83482825756073,
      "learning_rate": 4.2807017543859656e-06,
      "loss": 1.0687,
      "step": 2300
    },
    {
      "epoch": 4.334115438761145,
      "grad_norm": 1.58540678024292,
      "learning_rate": 4.16374269005848e-06,
      "loss": 1.0953,
      "step": 2310
    },
    {
      "epoch": 4.352885969028625,
      "grad_norm": 1.5536493062973022,
      "learning_rate": 4.046783625730995e-06,
      "loss": 1.0324,
      "step": 2320
    },
    {
      "epoch": 4.371656499296105,
      "grad_norm": 1.6974821090698242,
      "learning_rate": 3.929824561403509e-06,
      "loss": 0.9598,
      "step": 2330
    },
    {
      "epoch": 4.390427029563585,
      "grad_norm": 1.6756399869918823,
      "learning_rate": 3.812865497076024e-06,
      "loss": 0.9833,
      "step": 2340
    },
    {
      "epoch": 4.409197559831065,
      "grad_norm": 1.8428401947021484,
      "learning_rate": 3.695906432748538e-06,
      "loss": 1.0254,
      "step": 2350
    },
    {
      "epoch": 4.427968090098545,
      "grad_norm": 2.066547155380249,
      "learning_rate": 3.5789473684210525e-06,
      "loss": 1.0258,
      "step": 2360
    },
    {
      "epoch": 4.446738620366025,
      "grad_norm": 1.7340404987335205,
      "learning_rate": 3.461988304093567e-06,
      "loss": 0.9918,
      "step": 2370
    },
    {
      "epoch": 4.465509150633506,
      "grad_norm": 1.9203734397888184,
      "learning_rate": 3.345029239766082e-06,
      "loss": 1.0377,
      "step": 2380
    },
    {
      "epoch": 4.484279680900985,
      "grad_norm": 1.788905382156372,
      "learning_rate": 3.2280701754385966e-06,
      "loss": 1.0955,
      "step": 2390
    },
    {
      "epoch": 4.503050211168466,
      "grad_norm": 1.902572751045227,
      "learning_rate": 3.111111111111111e-06,
      "loss": 1.0227,
      "step": 2400
    },
    {
      "epoch": 4.5218207414359455,
      "grad_norm": 1.7204432487487793,
      "learning_rate": 2.9941520467836257e-06,
      "loss": 1.0211,
      "step": 2410
    },
    {
      "epoch": 4.540591271703425,
      "grad_norm": 1.6670310497283936,
      "learning_rate": 2.8771929824561403e-06,
      "loss": 0.9783,
      "step": 2420
    },
    {
      "epoch": 4.559361801970906,
      "grad_norm": 1.6039329767227173,
      "learning_rate": 2.760233918128655e-06,
      "loss": 1.0344,
      "step": 2430
    },
    {
      "epoch": 4.578132332238386,
      "grad_norm": 1.648133397102356,
      "learning_rate": 2.6432748538011694e-06,
      "loss": 0.9734,
      "step": 2440
    },
    {
      "epoch": 4.596902862505866,
      "grad_norm": 1.5793851613998413,
      "learning_rate": 2.5263157894736844e-06,
      "loss": 1.0909,
      "step": 2450
    },
    {
      "epoch": 4.6156733927733455,
      "grad_norm": 1.8303662538528442,
      "learning_rate": 2.409356725146199e-06,
      "loss": 1.0286,
      "step": 2460
    },
    {
      "epoch": 4.634443923040826,
      "grad_norm": 1.7100006341934204,
      "learning_rate": 2.2923976608187135e-06,
      "loss": 1.0393,
      "step": 2470
    },
    {
      "epoch": 4.653214453308306,
      "grad_norm": 1.6568533182144165,
      "learning_rate": 2.175438596491228e-06,
      "loss": 1.0805,
      "step": 2480
    },
    {
      "epoch": 4.671984983575786,
      "grad_norm": 1.908833384513855,
      "learning_rate": 2.0584795321637426e-06,
      "loss": 1.0785,
      "step": 2490
    },
    {
      "epoch": 4.690755513843266,
      "grad_norm": 1.609030842781067,
      "learning_rate": 1.941520467836257e-06,
      "loss": 1.0379,
      "step": 2500
    },
    {
      "epoch": 4.7095260441107465,
      "grad_norm": 1.6082990169525146,
      "learning_rate": 1.824561403508772e-06,
      "loss": 1.0656,
      "step": 2510
    },
    {
      "epoch": 4.728296574378226,
      "grad_norm": 1.6080412864685059,
      "learning_rate": 1.7076023391812865e-06,
      "loss": 1.0632,
      "step": 2520
    },
    {
      "epoch": 4.747067104645707,
      "grad_norm": 1.65628182888031,
      "learning_rate": 1.5906432748538013e-06,
      "loss": 1.0253,
      "step": 2530
    },
    {
      "epoch": 4.765837634913186,
      "grad_norm": 2.0083086490631104,
      "learning_rate": 1.4736842105263159e-06,
      "loss": 1.0293,
      "step": 2540
    },
    {
      "epoch": 4.784608165180666,
      "grad_norm": 1.6712477207183838,
      "learning_rate": 1.3567251461988304e-06,
      "loss": 1.0087,
      "step": 2550
    },
    {
      "epoch": 4.8033786954481466,
      "grad_norm": 1.9123042821884155,
      "learning_rate": 1.239766081871345e-06,
      "loss": 0.9714,
      "step": 2560
    },
    {
      "epoch": 4.822149225715626,
      "grad_norm": 1.797647476196289,
      "learning_rate": 1.1228070175438598e-06,
      "loss": 1.0915,
      "step": 2570
    },
    {
      "epoch": 4.840919755983107,
      "grad_norm": 1.9668816328048706,
      "learning_rate": 1.0058479532163743e-06,
      "loss": 1.0259,
      "step": 2580
    },
    {
      "epoch": 4.859690286250586,
      "grad_norm": 1.862834095954895,
      "learning_rate": 8.88888888888889e-07,
      "loss": 1.0244,
      "step": 2590
    },
    {
      "epoch": 4.878460816518067,
      "grad_norm": 1.869499683380127,
      "learning_rate": 7.719298245614035e-07,
      "loss": 0.9661,
      "step": 2600
    }
  ],
  "logging_steps": 10,
  "max_steps": 2665,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6822505931079680.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
