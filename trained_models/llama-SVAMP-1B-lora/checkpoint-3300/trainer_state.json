{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 15.0,
  "eval_steps": 500,
  "global_step": 3300,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 2.9941370487213135,
      "learning_rate": 2.7e-06,
      "loss": 4.2707,
      "step": 10
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 3.0065464973449707,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 4.2581,
      "step": 20
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 3.091063976287842,
      "learning_rate": 8.7e-06,
      "loss": 4.2191,
      "step": 30
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 3.1578032970428467,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 4.1458,
      "step": 40
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 3.5113072395324707,
      "learning_rate": 1.47e-05,
      "loss": 4.0264,
      "step": 50
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 4.4908928871154785,
      "learning_rate": 1.77e-05,
      "loss": 3.8414,
      "step": 60
    },
    {
      "epoch": 0.3181818181818182,
      "grad_norm": 4.926671028137207,
      "learning_rate": 2.07e-05,
      "loss": 3.5537,
      "step": 70
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 4.851468086242676,
      "learning_rate": 2.37e-05,
      "loss": 3.1886,
      "step": 80
    },
    {
      "epoch": 0.4090909090909091,
      "grad_norm": 5.560506343841553,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.7378,
      "step": 90
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 5.13209342956543,
      "learning_rate": 2.97e-05,
      "loss": 2.2012,
      "step": 100
    },
    {
      "epoch": 0.5,
      "grad_norm": 5.846700668334961,
      "learning_rate": 2.9915625e-05,
      "loss": 1.6829,
      "step": 110
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 3.469672918319702,
      "learning_rate": 2.9821874999999998e-05,
      "loss": 1.2848,
      "step": 120
    },
    {
      "epoch": 0.5909090909090909,
      "grad_norm": 3.6652908325195312,
      "learning_rate": 2.9728125000000003e-05,
      "loss": 1.0309,
      "step": 130
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 2.1457035541534424,
      "learning_rate": 2.9634375e-05,
      "loss": 0.8117,
      "step": 140
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 0.24729572236537933,
      "learning_rate": 2.9540625e-05,
      "loss": 0.7356,
      "step": 150
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.17855264246463776,
      "learning_rate": 2.9446875000000003e-05,
      "loss": 0.7269,
      "step": 160
    },
    {
      "epoch": 0.7727272727272727,
      "grad_norm": 0.1528666466474533,
      "learning_rate": 2.9353125e-05,
      "loss": 0.7219,
      "step": 170
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.14520937204360962,
      "learning_rate": 2.9259375e-05,
      "loss": 0.7182,
      "step": 180
    },
    {
      "epoch": 0.8636363636363636,
      "grad_norm": 0.1248415857553482,
      "learning_rate": 2.9165625e-05,
      "loss": 0.7152,
      "step": 190
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.06815403699874878,
      "learning_rate": 2.9071875000000004e-05,
      "loss": 0.713,
      "step": 200
    },
    {
      "epoch": 0.9545454545454546,
      "grad_norm": 0.04519055783748627,
      "learning_rate": 2.8978125e-05,
      "loss": 0.7121,
      "step": 210
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.04454006254673004,
      "learning_rate": 2.8884375e-05,
      "loss": 0.7117,
      "step": 220
    },
    {
      "epoch": 1.0454545454545454,
      "grad_norm": 0.044474516063928604,
      "learning_rate": 2.8790625e-05,
      "loss": 0.7115,
      "step": 230
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 0.042177893221378326,
      "learning_rate": 2.8696875e-05,
      "loss": 0.7114,
      "step": 240
    },
    {
      "epoch": 1.1363636363636362,
      "grad_norm": 0.04381674900650978,
      "learning_rate": 2.8603125000000002e-05,
      "loss": 0.7111,
      "step": 250
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 0.043367501348257065,
      "learning_rate": 2.8509375e-05,
      "loss": 0.7109,
      "step": 260
    },
    {
      "epoch": 1.2272727272727273,
      "grad_norm": 0.044297099113464355,
      "learning_rate": 2.8415625e-05,
      "loss": 0.7107,
      "step": 270
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 0.044167593121528625,
      "learning_rate": 2.8321875000000002e-05,
      "loss": 0.7104,
      "step": 280
    },
    {
      "epoch": 1.3181818181818181,
      "grad_norm": 0.04571820795536041,
      "learning_rate": 2.8228125e-05,
      "loss": 0.7101,
      "step": 290
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 0.046962104737758636,
      "learning_rate": 2.8134375e-05,
      "loss": 0.7099,
      "step": 300
    },
    {
      "epoch": 1.4090909090909092,
      "grad_norm": 0.0479063056409359,
      "learning_rate": 2.8040625000000002e-05,
      "loss": 0.7096,
      "step": 310
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 0.04693591222167015,
      "learning_rate": 2.7946875e-05,
      "loss": 0.7093,
      "step": 320
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.05329063534736633,
      "learning_rate": 2.7853125e-05,
      "loss": 0.7089,
      "step": 330
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 0.055170297622680664,
      "learning_rate": 2.7759375e-05,
      "loss": 0.7085,
      "step": 340
    },
    {
      "epoch": 1.5909090909090908,
      "grad_norm": 0.05985119193792343,
      "learning_rate": 2.7665625000000003e-05,
      "loss": 0.7081,
      "step": 350
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 0.0661674216389656,
      "learning_rate": 2.7571875e-05,
      "loss": 0.7075,
      "step": 360
    },
    {
      "epoch": 1.6818181818181817,
      "grad_norm": 0.07189767807722092,
      "learning_rate": 2.7478125e-05,
      "loss": 0.7069,
      "step": 370
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.0793246328830719,
      "learning_rate": 2.7384375e-05,
      "loss": 0.7063,
      "step": 380
    },
    {
      "epoch": 1.7727272727272727,
      "grad_norm": 0.08802509307861328,
      "learning_rate": 2.7290625e-05,
      "loss": 0.7054,
      "step": 390
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.09843707084655762,
      "learning_rate": 2.7196875000000002e-05,
      "loss": 0.7045,
      "step": 400
    },
    {
      "epoch": 1.8636363636363638,
      "grad_norm": 0.10588536411523819,
      "learning_rate": 2.7103125e-05,
      "loss": 0.7032,
      "step": 410
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 0.10702824592590332,
      "learning_rate": 2.7009375e-05,
      "loss": 0.7018,
      "step": 420
    },
    {
      "epoch": 1.9545454545454546,
      "grad_norm": 0.10463116317987442,
      "learning_rate": 2.6915625000000002e-05,
      "loss": 0.7001,
      "step": 430
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.10025162249803543,
      "learning_rate": 2.6821875e-05,
      "loss": 0.6986,
      "step": 440
    },
    {
      "epoch": 2.0454545454545454,
      "grad_norm": 0.09621725976467133,
      "learning_rate": 2.6728125e-05,
      "loss": 0.6969,
      "step": 450
    },
    {
      "epoch": 2.090909090909091,
      "grad_norm": 0.08783504366874695,
      "learning_rate": 2.6634375000000002e-05,
      "loss": 0.6952,
      "step": 460
    },
    {
      "epoch": 2.1363636363636362,
      "grad_norm": 0.07809078693389893,
      "learning_rate": 2.6540625e-05,
      "loss": 0.6936,
      "step": 470
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 0.07007727771997452,
      "learning_rate": 2.6446875e-05,
      "loss": 0.6922,
      "step": 480
    },
    {
      "epoch": 2.227272727272727,
      "grad_norm": 0.062888503074646,
      "learning_rate": 2.6353125e-05,
      "loss": 0.6909,
      "step": 490
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 0.05852225050330162,
      "learning_rate": 2.6259375000000003e-05,
      "loss": 0.6896,
      "step": 500
    },
    {
      "epoch": 2.3181818181818183,
      "grad_norm": 0.05637324973940849,
      "learning_rate": 2.6165625e-05,
      "loss": 0.6883,
      "step": 510
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 0.05686906352639198,
      "learning_rate": 2.6071875e-05,
      "loss": 0.6869,
      "step": 520
    },
    {
      "epoch": 2.409090909090909,
      "grad_norm": 0.055868618190288544,
      "learning_rate": 2.5978125000000003e-05,
      "loss": 0.6856,
      "step": 530
    },
    {
      "epoch": 2.4545454545454546,
      "grad_norm": 0.05784834176301956,
      "learning_rate": 2.5884375e-05,
      "loss": 0.6841,
      "step": 540
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.05969943106174469,
      "learning_rate": 2.5790625000000002e-05,
      "loss": 0.6826,
      "step": 550
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 0.0626450777053833,
      "learning_rate": 2.5696875e-05,
      "loss": 0.681,
      "step": 560
    },
    {
      "epoch": 2.590909090909091,
      "grad_norm": 0.06315464526414871,
      "learning_rate": 2.5603125e-05,
      "loss": 0.6794,
      "step": 570
    },
    {
      "epoch": 2.6363636363636362,
      "grad_norm": 0.0627870038151741,
      "learning_rate": 2.5509375000000002e-05,
      "loss": 0.6777,
      "step": 580
    },
    {
      "epoch": 2.6818181818181817,
      "grad_norm": 0.06529048830270767,
      "learning_rate": 2.5415625e-05,
      "loss": 0.676,
      "step": 590
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 0.06774353235960007,
      "learning_rate": 2.5321875e-05,
      "loss": 0.6741,
      "step": 600
    },
    {
      "epoch": 2.7727272727272725,
      "grad_norm": 0.07164552062749863,
      "learning_rate": 2.5228125e-05,
      "loss": 0.6723,
      "step": 610
    },
    {
      "epoch": 2.8181818181818183,
      "grad_norm": 0.07379096001386642,
      "learning_rate": 2.5134375e-05,
      "loss": 0.6703,
      "step": 620
    },
    {
      "epoch": 2.8636363636363638,
      "grad_norm": 0.07557099312543869,
      "learning_rate": 2.5040625e-05,
      "loss": 0.6682,
      "step": 630
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 0.08175420016050339,
      "learning_rate": 2.4946875e-05,
      "loss": 0.666,
      "step": 640
    },
    {
      "epoch": 2.9545454545454546,
      "grad_norm": 0.08477544039487839,
      "learning_rate": 2.4853125000000003e-05,
      "loss": 0.6635,
      "step": 650
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.09084703028202057,
      "learning_rate": 2.4759375e-05,
      "loss": 0.6609,
      "step": 660
    },
    {
      "epoch": 3.0454545454545454,
      "grad_norm": 0.09932000935077667,
      "learning_rate": 2.4665624999999998e-05,
      "loss": 0.6576,
      "step": 670
    },
    {
      "epoch": 3.090909090909091,
      "grad_norm": 0.11293788999319077,
      "learning_rate": 2.4571875000000003e-05,
      "loss": 0.6544,
      "step": 680
    },
    {
      "epoch": 3.1363636363636362,
      "grad_norm": 0.13143441081047058,
      "learning_rate": 2.4478125e-05,
      "loss": 0.6505,
      "step": 690
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 0.16573761403560638,
      "learning_rate": 2.4384375e-05,
      "loss": 0.6458,
      "step": 700
    },
    {
      "epoch": 3.227272727272727,
      "grad_norm": 0.2472403645515442,
      "learning_rate": 2.4290625e-05,
      "loss": 0.6397,
      "step": 710
    },
    {
      "epoch": 3.2727272727272725,
      "grad_norm": 0.5252473950386047,
      "learning_rate": 2.4196875e-05,
      "loss": 0.6299,
      "step": 720
    },
    {
      "epoch": 3.3181818181818183,
      "grad_norm": 0.8761564493179321,
      "learning_rate": 2.4103125e-05,
      "loss": 0.6074,
      "step": 730
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 0.5516794323921204,
      "learning_rate": 2.4009375e-05,
      "loss": 0.5839,
      "step": 740
    },
    {
      "epoch": 3.409090909090909,
      "grad_norm": 0.2825559377670288,
      "learning_rate": 2.3915625000000004e-05,
      "loss": 0.563,
      "step": 750
    },
    {
      "epoch": 3.4545454545454546,
      "grad_norm": 0.2876751720905304,
      "learning_rate": 2.3821875e-05,
      "loss": 0.5405,
      "step": 760
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.33401790261268616,
      "learning_rate": 2.3728125e-05,
      "loss": 0.5182,
      "step": 770
    },
    {
      "epoch": 3.5454545454545454,
      "grad_norm": 0.32221752405166626,
      "learning_rate": 2.3634375e-05,
      "loss": 0.4981,
      "step": 780
    },
    {
      "epoch": 3.590909090909091,
      "grad_norm": 0.30999478697776794,
      "learning_rate": 2.3540625e-05,
      "loss": 0.482,
      "step": 790
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 0.15514260530471802,
      "learning_rate": 2.3446875000000002e-05,
      "loss": 0.472,
      "step": 800
    },
    {
      "epoch": 3.6818181818181817,
      "grad_norm": 0.1291952282190323,
      "learning_rate": 2.3353125e-05,
      "loss": 0.4675,
      "step": 810
    },
    {
      "epoch": 3.7272727272727275,
      "grad_norm": 0.057111866772174835,
      "learning_rate": 2.3259374999999998e-05,
      "loss": 0.4659,
      "step": 820
    },
    {
      "epoch": 3.7727272727272725,
      "grad_norm": 0.0432114377617836,
      "learning_rate": 2.3165625000000002e-05,
      "loss": 0.4653,
      "step": 830
    },
    {
      "epoch": 3.8181818181818183,
      "grad_norm": 0.02134585939347744,
      "learning_rate": 2.3071875e-05,
      "loss": 0.465,
      "step": 840
    },
    {
      "epoch": 3.8636363636363638,
      "grad_norm": 0.020541194826364517,
      "learning_rate": 2.2978125e-05,
      "loss": 0.4649,
      "step": 850
    },
    {
      "epoch": 3.909090909090909,
      "grad_norm": 0.022006971761584282,
      "learning_rate": 2.2884375000000002e-05,
      "loss": 0.4648,
      "step": 860
    },
    {
      "epoch": 3.9545454545454546,
      "grad_norm": 0.01768113300204277,
      "learning_rate": 2.2790625e-05,
      "loss": 0.4647,
      "step": 870
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.016669882461428642,
      "learning_rate": 2.2696875e-05,
      "loss": 0.4646,
      "step": 880
    },
    {
      "epoch": 4.045454545454546,
      "grad_norm": 0.010789000429213047,
      "learning_rate": 2.2603125e-05,
      "loss": 0.4646,
      "step": 890
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 0.010667643509805202,
      "learning_rate": 2.2509375000000003e-05,
      "loss": 0.4646,
      "step": 900
    },
    {
      "epoch": 4.136363636363637,
      "grad_norm": 0.011053169146180153,
      "learning_rate": 2.2415625e-05,
      "loss": 0.4645,
      "step": 910
    },
    {
      "epoch": 4.181818181818182,
      "grad_norm": 0.009935297071933746,
      "learning_rate": 2.2321875e-05,
      "loss": 0.4645,
      "step": 920
    },
    {
      "epoch": 4.2272727272727275,
      "grad_norm": 0.012310110963881016,
      "learning_rate": 2.2228125e-05,
      "loss": 0.4645,
      "step": 930
    },
    {
      "epoch": 4.2727272727272725,
      "grad_norm": 0.007055090740323067,
      "learning_rate": 2.2134375e-05,
      "loss": 0.4644,
      "step": 940
    },
    {
      "epoch": 4.318181818181818,
      "grad_norm": 0.006967074237763882,
      "learning_rate": 2.2040625000000002e-05,
      "loss": 0.4644,
      "step": 950
    },
    {
      "epoch": 4.363636363636363,
      "grad_norm": 0.00872139260172844,
      "learning_rate": 2.1946875e-05,
      "loss": 0.4644,
      "step": 960
    },
    {
      "epoch": 4.409090909090909,
      "grad_norm": 0.00822234246879816,
      "learning_rate": 2.1853125e-05,
      "loss": 0.4644,
      "step": 970
    },
    {
      "epoch": 4.454545454545454,
      "grad_norm": 0.006137849763035774,
      "learning_rate": 2.1759375000000002e-05,
      "loss": 0.4644,
      "step": 980
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.005923737771809101,
      "learning_rate": 2.1665625e-05,
      "loss": 0.4644,
      "step": 990
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 0.005043579265475273,
      "learning_rate": 2.1571875e-05,
      "loss": 0.4644,
      "step": 1000
    },
    {
      "epoch": 4.590909090909091,
      "grad_norm": 0.02319347858428955,
      "learning_rate": 2.1478125000000002e-05,
      "loss": 0.4644,
      "step": 1010
    },
    {
      "epoch": 4.636363636363637,
      "grad_norm": 0.004971912130713463,
      "learning_rate": 2.1384375e-05,
      "loss": 0.4643,
      "step": 1020
    },
    {
      "epoch": 4.681818181818182,
      "grad_norm": 0.009052928537130356,
      "learning_rate": 2.1290625e-05,
      "loss": 0.4643,
      "step": 1030
    },
    {
      "epoch": 4.7272727272727275,
      "grad_norm": 0.006104897707700729,
      "learning_rate": 2.1196875e-05,
      "loss": 0.4643,
      "step": 1040
    },
    {
      "epoch": 4.7727272727272725,
      "grad_norm": 0.0062718172557652,
      "learning_rate": 2.1103125000000003e-05,
      "loss": 0.4643,
      "step": 1050
    },
    {
      "epoch": 4.818181818181818,
      "grad_norm": 0.005670508835464716,
      "learning_rate": 2.1009375e-05,
      "loss": 0.4643,
      "step": 1060
    },
    {
      "epoch": 4.863636363636363,
      "grad_norm": 0.005683473311364651,
      "learning_rate": 2.0915625e-05,
      "loss": 0.4643,
      "step": 1070
    },
    {
      "epoch": 4.909090909090909,
      "grad_norm": 0.007756777573376894,
      "learning_rate": 2.0821875000000003e-05,
      "loss": 0.4643,
      "step": 1080
    },
    {
      "epoch": 4.954545454545455,
      "grad_norm": 0.004445297177881002,
      "learning_rate": 2.0728125e-05,
      "loss": 0.4643,
      "step": 1090
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.004893554374575615,
      "learning_rate": 2.0634375000000002e-05,
      "loss": 0.4643,
      "step": 1100
    },
    {
      "epoch": 5.045454545454546,
      "grad_norm": 0.003772519063204527,
      "learning_rate": 2.0540625e-05,
      "loss": 0.4643,
      "step": 1110
    },
    {
      "epoch": 5.090909090909091,
      "grad_norm": 0.010437372140586376,
      "learning_rate": 2.0446875e-05,
      "loss": 0.4643,
      "step": 1120
    },
    {
      "epoch": 5.136363636363637,
      "grad_norm": 0.0051013329066336155,
      "learning_rate": 2.0353125000000002e-05,
      "loss": 0.4643,
      "step": 1130
    },
    {
      "epoch": 5.181818181818182,
      "grad_norm": 0.0035280990414321423,
      "learning_rate": 2.0259375e-05,
      "loss": 0.4643,
      "step": 1140
    },
    {
      "epoch": 5.2272727272727275,
      "grad_norm": 0.007602575700730085,
      "learning_rate": 2.0165625e-05,
      "loss": 0.4643,
      "step": 1150
    },
    {
      "epoch": 5.2727272727272725,
      "grad_norm": 0.0064558410085737705,
      "learning_rate": 2.0071875e-05,
      "loss": 0.4643,
      "step": 1160
    },
    {
      "epoch": 5.318181818181818,
      "grad_norm": 0.004917261190712452,
      "learning_rate": 1.9978125e-05,
      "loss": 0.4643,
      "step": 1170
    },
    {
      "epoch": 5.363636363636363,
      "grad_norm": 0.0033094575628638268,
      "learning_rate": 1.9884375e-05,
      "loss": 0.4643,
      "step": 1180
    },
    {
      "epoch": 5.409090909090909,
      "grad_norm": 0.006339447107166052,
      "learning_rate": 1.9790625e-05,
      "loss": 0.4643,
      "step": 1190
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 0.002674342365935445,
      "learning_rate": 1.9696875000000003e-05,
      "loss": 0.4643,
      "step": 1200
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.0026001334190368652,
      "learning_rate": 1.9603125e-05,
      "loss": 0.4643,
      "step": 1210
    },
    {
      "epoch": 5.545454545454545,
      "grad_norm": 0.002630590694025159,
      "learning_rate": 1.9509374999999998e-05,
      "loss": 0.4643,
      "step": 1220
    },
    {
      "epoch": 5.590909090909091,
      "grad_norm": 0.006757140159606934,
      "learning_rate": 1.9415625000000003e-05,
      "loss": 0.4643,
      "step": 1230
    },
    {
      "epoch": 5.636363636363637,
      "grad_norm": 0.0024248932022601366,
      "learning_rate": 1.9321875e-05,
      "loss": 0.4643,
      "step": 1240
    },
    {
      "epoch": 5.681818181818182,
      "grad_norm": 0.0028731978964060545,
      "learning_rate": 1.9228125e-05,
      "loss": 0.4643,
      "step": 1250
    },
    {
      "epoch": 5.7272727272727275,
      "grad_norm": 0.00280591007322073,
      "learning_rate": 1.9134375e-05,
      "loss": 0.4643,
      "step": 1260
    },
    {
      "epoch": 5.7727272727272725,
      "grad_norm": 0.00259724841453135,
      "learning_rate": 1.9040625e-05,
      "loss": 0.4642,
      "step": 1270
    },
    {
      "epoch": 5.818181818181818,
      "grad_norm": 0.0022445544600486755,
      "learning_rate": 1.8946875e-05,
      "loss": 0.4642,
      "step": 1280
    },
    {
      "epoch": 5.863636363636363,
      "grad_norm": 0.004605736583471298,
      "learning_rate": 1.8853125e-05,
      "loss": 0.4642,
      "step": 1290
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 0.005605182144790888,
      "learning_rate": 1.8759375e-05,
      "loss": 0.4642,
      "step": 1300
    },
    {
      "epoch": 5.954545454545455,
      "grad_norm": 0.0024317526258528233,
      "learning_rate": 1.8665625e-05,
      "loss": 0.4642,
      "step": 1310
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.004721471574157476,
      "learning_rate": 1.8571875e-05,
      "loss": 0.4642,
      "step": 1320
    },
    {
      "epoch": 6.045454545454546,
      "grad_norm": 0.002774900058284402,
      "learning_rate": 1.8478125e-05,
      "loss": 0.4642,
      "step": 1330
    },
    {
      "epoch": 6.090909090909091,
      "grad_norm": 0.001989364391192794,
      "learning_rate": 1.8384375e-05,
      "loss": 0.4642,
      "step": 1340
    },
    {
      "epoch": 6.136363636363637,
      "grad_norm": 0.002178961643949151,
      "learning_rate": 1.8290625000000002e-05,
      "loss": 0.4642,
      "step": 1350
    },
    {
      "epoch": 6.181818181818182,
      "grad_norm": 0.003167480695992708,
      "learning_rate": 1.8196875e-05,
      "loss": 0.4642,
      "step": 1360
    },
    {
      "epoch": 6.2272727272727275,
      "grad_norm": 0.0021172002889215946,
      "learning_rate": 1.8103124999999998e-05,
      "loss": 0.4642,
      "step": 1370
    },
    {
      "epoch": 6.2727272727272725,
      "grad_norm": 0.002062044572085142,
      "learning_rate": 1.8009375000000002e-05,
      "loss": 0.4642,
      "step": 1380
    },
    {
      "epoch": 6.318181818181818,
      "grad_norm": 0.0021860655397176743,
      "learning_rate": 1.7915625e-05,
      "loss": 0.4642,
      "step": 1390
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 0.002422742545604706,
      "learning_rate": 1.7821875e-05,
      "loss": 0.4642,
      "step": 1400
    },
    {
      "epoch": 6.409090909090909,
      "grad_norm": 0.0018190575065091252,
      "learning_rate": 1.7728125e-05,
      "loss": 0.4642,
      "step": 1410
    },
    {
      "epoch": 6.454545454545454,
      "grad_norm": 0.0017817389452829957,
      "learning_rate": 1.7634375e-05,
      "loss": 0.4642,
      "step": 1420
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.0018191093113273382,
      "learning_rate": 1.7540625e-05,
      "loss": 0.4642,
      "step": 1430
    },
    {
      "epoch": 6.545454545454545,
      "grad_norm": 0.001680269604548812,
      "learning_rate": 1.7446875e-05,
      "loss": 0.4642,
      "step": 1440
    },
    {
      "epoch": 6.590909090909091,
      "grad_norm": 0.0018031448125839233,
      "learning_rate": 1.7353125000000003e-05,
      "loss": 0.4642,
      "step": 1450
    },
    {
      "epoch": 6.636363636363637,
      "grad_norm": 0.0018648982513695955,
      "learning_rate": 1.7259375e-05,
      "loss": 0.4642,
      "step": 1460
    },
    {
      "epoch": 6.681818181818182,
      "grad_norm": 0.002138452837243676,
      "learning_rate": 1.7165625e-05,
      "loss": 0.4642,
      "step": 1470
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 0.0029255924746394157,
      "learning_rate": 1.7071875e-05,
      "loss": 0.4642,
      "step": 1480
    },
    {
      "epoch": 6.7727272727272725,
      "grad_norm": 0.00182163598947227,
      "learning_rate": 1.6978125e-05,
      "loss": 0.4642,
      "step": 1490
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 0.001778797130100429,
      "learning_rate": 1.6884375000000002e-05,
      "loss": 0.4642,
      "step": 1500
    },
    {
      "epoch": 6.863636363636363,
      "grad_norm": 0.001735688536427915,
      "learning_rate": 1.6790625e-05,
      "loss": 0.4642,
      "step": 1510
    },
    {
      "epoch": 6.909090909090909,
      "grad_norm": 0.0019232445629313588,
      "learning_rate": 1.6696874999999998e-05,
      "loss": 0.4642,
      "step": 1520
    },
    {
      "epoch": 6.954545454545455,
      "grad_norm": 0.0038792872801423073,
      "learning_rate": 1.6603125000000002e-05,
      "loss": 0.4642,
      "step": 1530
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.0016914294101297855,
      "learning_rate": 1.6509375e-05,
      "loss": 0.4642,
      "step": 1540
    },
    {
      "epoch": 7.045454545454546,
      "grad_norm": 0.0017492023762315512,
      "learning_rate": 1.6415625e-05,
      "loss": 0.4642,
      "step": 1550
    },
    {
      "epoch": 7.090909090909091,
      "grad_norm": 0.0014103372814133763,
      "learning_rate": 1.6321875000000002e-05,
      "loss": 0.4642,
      "step": 1560
    },
    {
      "epoch": 7.136363636363637,
      "grad_norm": 0.0021702798549085855,
      "learning_rate": 1.6228125e-05,
      "loss": 0.4642,
      "step": 1570
    },
    {
      "epoch": 7.181818181818182,
      "grad_norm": 0.0021145776845514774,
      "learning_rate": 1.6134375e-05,
      "loss": 0.4642,
      "step": 1580
    },
    {
      "epoch": 7.2272727272727275,
      "grad_norm": 0.0015718307113274932,
      "learning_rate": 1.6040625e-05,
      "loss": 0.4642,
      "step": 1590
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 0.0016412623226642609,
      "learning_rate": 1.5946875000000003e-05,
      "loss": 0.4642,
      "step": 1600
    },
    {
      "epoch": 7.318181818181818,
      "grad_norm": 0.0013514406746253371,
      "learning_rate": 1.5853125e-05,
      "loss": 0.4642,
      "step": 1610
    },
    {
      "epoch": 7.363636363636363,
      "grad_norm": 0.0023757934104651213,
      "learning_rate": 1.5759375e-05,
      "loss": 0.4642,
      "step": 1620
    },
    {
      "epoch": 7.409090909090909,
      "grad_norm": 0.001517987810075283,
      "learning_rate": 1.5665625e-05,
      "loss": 0.4642,
      "step": 1630
    },
    {
      "epoch": 7.454545454545454,
      "grad_norm": 0.0013865113724023104,
      "learning_rate": 1.5571875e-05,
      "loss": 0.4642,
      "step": 1640
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.0014916760846972466,
      "learning_rate": 1.5478125000000002e-05,
      "loss": 0.4642,
      "step": 1650
    },
    {
      "epoch": 7.545454545454545,
      "grad_norm": 0.0025615289341658354,
      "learning_rate": 1.5384375e-05,
      "loss": 0.4642,
      "step": 1660
    },
    {
      "epoch": 7.590909090909091,
      "grad_norm": 0.0016796075506135821,
      "learning_rate": 1.5290625e-05,
      "loss": 0.4642,
      "step": 1670
    },
    {
      "epoch": 7.636363636363637,
      "grad_norm": 0.0014106259914115071,
      "learning_rate": 1.5196875000000002e-05,
      "loss": 0.4642,
      "step": 1680
    },
    {
      "epoch": 7.681818181818182,
      "grad_norm": 0.001353563740849495,
      "learning_rate": 1.5103125e-05,
      "loss": 0.4642,
      "step": 1690
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 0.00117214722558856,
      "learning_rate": 1.5009375000000002e-05,
      "loss": 0.4642,
      "step": 1700
    },
    {
      "epoch": 7.7727272727272725,
      "grad_norm": 0.001563602709211409,
      "learning_rate": 1.4915625e-05,
      "loss": 0.4642,
      "step": 1710
    },
    {
      "epoch": 7.818181818181818,
      "grad_norm": 0.0023706010542809963,
      "learning_rate": 1.4821875000000001e-05,
      "loss": 0.4642,
      "step": 1720
    },
    {
      "epoch": 7.863636363636363,
      "grad_norm": 0.0012715532211586833,
      "learning_rate": 1.4728125e-05,
      "loss": 0.4642,
      "step": 1730
    },
    {
      "epoch": 7.909090909090909,
      "grad_norm": 0.00167429493740201,
      "learning_rate": 1.4634375e-05,
      "loss": 0.4642,
      "step": 1740
    },
    {
      "epoch": 7.954545454545455,
      "grad_norm": 0.0012091249227523804,
      "learning_rate": 1.4540625e-05,
      "loss": 0.4642,
      "step": 1750
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.0018969918601214886,
      "learning_rate": 1.4446875e-05,
      "loss": 0.4642,
      "step": 1760
    },
    {
      "epoch": 8.045454545454545,
      "grad_norm": 0.0023389842826873064,
      "learning_rate": 1.4353125000000002e-05,
      "loss": 0.4642,
      "step": 1770
    },
    {
      "epoch": 8.090909090909092,
      "grad_norm": 0.0014696879079565406,
      "learning_rate": 1.4259375000000001e-05,
      "loss": 0.4642,
      "step": 1780
    },
    {
      "epoch": 8.136363636363637,
      "grad_norm": 0.0014584357850253582,
      "learning_rate": 1.4165625e-05,
      "loss": 0.4642,
      "step": 1790
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 0.0010791087988764048,
      "learning_rate": 1.4071875e-05,
      "loss": 0.4642,
      "step": 1800
    },
    {
      "epoch": 8.227272727272727,
      "grad_norm": 0.0012793663190677762,
      "learning_rate": 1.3978125000000001e-05,
      "loss": 0.4642,
      "step": 1810
    },
    {
      "epoch": 8.272727272727273,
      "grad_norm": 0.0010855706641450524,
      "learning_rate": 1.3884375e-05,
      "loss": 0.4642,
      "step": 1820
    },
    {
      "epoch": 8.318181818181818,
      "grad_norm": 0.00282233115285635,
      "learning_rate": 1.3790625000000001e-05,
      "loss": 0.4642,
      "step": 1830
    },
    {
      "epoch": 8.363636363636363,
      "grad_norm": 0.0014372393488883972,
      "learning_rate": 1.3696875e-05,
      "loss": 0.4642,
      "step": 1840
    },
    {
      "epoch": 8.409090909090908,
      "grad_norm": 0.001144816866144538,
      "learning_rate": 1.3603125e-05,
      "loss": 0.4642,
      "step": 1850
    },
    {
      "epoch": 8.454545454545455,
      "grad_norm": 0.0010840286267921329,
      "learning_rate": 1.3509375e-05,
      "loss": 0.4642,
      "step": 1860
    },
    {
      "epoch": 8.5,
      "grad_norm": 0.0020470470190048218,
      "learning_rate": 1.3415625e-05,
      "loss": 0.4642,
      "step": 1870
    },
    {
      "epoch": 8.545454545454545,
      "grad_norm": 0.0013273850781843066,
      "learning_rate": 1.3321875000000002e-05,
      "loss": 0.4642,
      "step": 1880
    },
    {
      "epoch": 8.590909090909092,
      "grad_norm": 0.001084350747987628,
      "learning_rate": 1.3228125e-05,
      "loss": 0.4642,
      "step": 1890
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 0.0017502809641882777,
      "learning_rate": 1.3134375e-05,
      "loss": 0.4642,
      "step": 1900
    },
    {
      "epoch": 8.681818181818182,
      "grad_norm": 0.0011386416153982282,
      "learning_rate": 1.3040625e-05,
      "loss": 0.4642,
      "step": 1910
    },
    {
      "epoch": 8.727272727272727,
      "grad_norm": 0.0014086439041420817,
      "learning_rate": 1.2946875000000001e-05,
      "loss": 0.4642,
      "step": 1920
    },
    {
      "epoch": 8.772727272727273,
      "grad_norm": 0.002650265349075198,
      "learning_rate": 1.2853125e-05,
      "loss": 0.4642,
      "step": 1930
    },
    {
      "epoch": 8.818181818181818,
      "grad_norm": 0.0010770374210551381,
      "learning_rate": 1.2759375e-05,
      "loss": 0.4642,
      "step": 1940
    },
    {
      "epoch": 8.863636363636363,
      "grad_norm": 0.0009780582040548325,
      "learning_rate": 1.2665625e-05,
      "loss": 0.4642,
      "step": 1950
    },
    {
      "epoch": 8.909090909090908,
      "grad_norm": 0.0014324585208669305,
      "learning_rate": 1.2571875e-05,
      "loss": 0.4642,
      "step": 1960
    },
    {
      "epoch": 8.954545454545455,
      "grad_norm": 0.0027123098261654377,
      "learning_rate": 1.2478125e-05,
      "loss": 0.4642,
      "step": 1970
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.0009299330413341522,
      "learning_rate": 1.2384375000000001e-05,
      "loss": 0.4642,
      "step": 1980
    },
    {
      "epoch": 9.045454545454545,
      "grad_norm": 0.0014455666532739997,
      "learning_rate": 1.2290625e-05,
      "loss": 0.4642,
      "step": 1990
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 0.000972858804743737,
      "learning_rate": 1.2196875e-05,
      "loss": 0.4642,
      "step": 2000
    },
    {
      "epoch": 9.136363636363637,
      "grad_norm": 0.0009310562163591385,
      "learning_rate": 1.2103125000000001e-05,
      "loss": 0.4642,
      "step": 2010
    },
    {
      "epoch": 9.181818181818182,
      "grad_norm": 0.003530373563989997,
      "learning_rate": 1.2009375e-05,
      "loss": 0.4642,
      "step": 2020
    },
    {
      "epoch": 9.227272727272727,
      "grad_norm": 0.0010482417419552803,
      "learning_rate": 1.1915625000000002e-05,
      "loss": 0.4642,
      "step": 2030
    },
    {
      "epoch": 9.272727272727273,
      "grad_norm": 0.0011094595538452268,
      "learning_rate": 1.1821875e-05,
      "loss": 0.4642,
      "step": 2040
    },
    {
      "epoch": 9.318181818181818,
      "grad_norm": 0.0009056992130354047,
      "learning_rate": 1.1728125e-05,
      "loss": 0.4642,
      "step": 2050
    },
    {
      "epoch": 9.363636363636363,
      "grad_norm": 0.0010499295312911272,
      "learning_rate": 1.1634375e-05,
      "loss": 0.4642,
      "step": 2060
    },
    {
      "epoch": 9.409090909090908,
      "grad_norm": 0.0010671225609257817,
      "learning_rate": 1.1540625000000001e-05,
      "loss": 0.4642,
      "step": 2070
    },
    {
      "epoch": 9.454545454545455,
      "grad_norm": 0.0008629102376289666,
      "learning_rate": 1.1446875e-05,
      "loss": 0.4642,
      "step": 2080
    },
    {
      "epoch": 9.5,
      "grad_norm": 0.0012572070118039846,
      "learning_rate": 1.1353125e-05,
      "loss": 0.4642,
      "step": 2090
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 0.0009391648345626891,
      "learning_rate": 1.1259375e-05,
      "loss": 0.4642,
      "step": 2100
    },
    {
      "epoch": 9.590909090909092,
      "grad_norm": 0.0017383188242092729,
      "learning_rate": 1.1165625e-05,
      "loss": 0.4642,
      "step": 2110
    },
    {
      "epoch": 9.636363636363637,
      "grad_norm": 0.0010961182415485382,
      "learning_rate": 1.1071875000000001e-05,
      "loss": 0.4642,
      "step": 2120
    },
    {
      "epoch": 9.681818181818182,
      "grad_norm": 0.0018756990320980549,
      "learning_rate": 1.0978125000000001e-05,
      "loss": 0.4642,
      "step": 2130
    },
    {
      "epoch": 9.727272727272727,
      "grad_norm": 0.000928345660213381,
      "learning_rate": 1.0884375e-05,
      "loss": 0.4642,
      "step": 2140
    },
    {
      "epoch": 9.772727272727273,
      "grad_norm": 0.0008286210359074175,
      "learning_rate": 1.0790625e-05,
      "loss": 0.4642,
      "step": 2150
    },
    {
      "epoch": 9.818181818181818,
      "grad_norm": 0.000918805890250951,
      "learning_rate": 1.0696875e-05,
      "loss": 0.4642,
      "step": 2160
    },
    {
      "epoch": 9.863636363636363,
      "grad_norm": 0.0011582747101783752,
      "learning_rate": 1.0603125e-05,
      "loss": 0.4642,
      "step": 2170
    },
    {
      "epoch": 9.909090909090908,
      "grad_norm": 0.0013348963111639023,
      "learning_rate": 1.0509375000000001e-05,
      "loss": 0.4642,
      "step": 2180
    },
    {
      "epoch": 9.954545454545455,
      "grad_norm": 0.000796896347310394,
      "learning_rate": 1.0415624999999999e-05,
      "loss": 0.4642,
      "step": 2190
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.0012075722916051745,
      "learning_rate": 1.0321875e-05,
      "loss": 0.4642,
      "step": 2200
    },
    {
      "epoch": 10.045454545454545,
      "grad_norm": 0.0009246192639693618,
      "learning_rate": 1.0228125e-05,
      "loss": 0.4642,
      "step": 2210
    },
    {
      "epoch": 10.090909090909092,
      "grad_norm": 0.0009462969028390944,
      "learning_rate": 1.0134375e-05,
      "loss": 0.4642,
      "step": 2220
    },
    {
      "epoch": 10.136363636363637,
      "grad_norm": 0.0008688719244673848,
      "learning_rate": 1.0040625000000002e-05,
      "loss": 0.4642,
      "step": 2230
    },
    {
      "epoch": 10.181818181818182,
      "grad_norm": 0.001033050357364118,
      "learning_rate": 9.946875e-06,
      "loss": 0.4642,
      "step": 2240
    },
    {
      "epoch": 10.227272727272727,
      "grad_norm": 0.0011896229116246104,
      "learning_rate": 9.853125e-06,
      "loss": 0.4642,
      "step": 2250
    },
    {
      "epoch": 10.272727272727273,
      "grad_norm": 0.001393748796544969,
      "learning_rate": 9.759375e-06,
      "loss": 0.4642,
      "step": 2260
    },
    {
      "epoch": 10.318181818181818,
      "grad_norm": 0.0008585734758526087,
      "learning_rate": 9.665625000000001e-06,
      "loss": 0.4642,
      "step": 2270
    },
    {
      "epoch": 10.363636363636363,
      "grad_norm": 0.001166393281891942,
      "learning_rate": 9.571875e-06,
      "loss": 0.4642,
      "step": 2280
    },
    {
      "epoch": 10.409090909090908,
      "grad_norm": 0.0009348163148388267,
      "learning_rate": 9.478125e-06,
      "loss": 0.4642,
      "step": 2290
    },
    {
      "epoch": 10.454545454545455,
      "grad_norm": 0.0014737430028617382,
      "learning_rate": 9.384375e-06,
      "loss": 0.4642,
      "step": 2300
    },
    {
      "epoch": 10.5,
      "grad_norm": 0.0010495653841644526,
      "learning_rate": 9.290625e-06,
      "loss": 0.4642,
      "step": 2310
    },
    {
      "epoch": 10.545454545454545,
      "grad_norm": 0.001128247706219554,
      "learning_rate": 9.196875e-06,
      "loss": 0.4642,
      "step": 2320
    },
    {
      "epoch": 10.590909090909092,
      "grad_norm": 0.0011476966319605708,
      "learning_rate": 9.103125000000001e-06,
      "loss": 0.4642,
      "step": 2330
    },
    {
      "epoch": 10.636363636363637,
      "grad_norm": 0.001171792158856988,
      "learning_rate": 9.009375e-06,
      "loss": 0.4642,
      "step": 2340
    },
    {
      "epoch": 10.681818181818182,
      "grad_norm": 0.0009160977206192911,
      "learning_rate": 8.915625e-06,
      "loss": 0.4642,
      "step": 2350
    },
    {
      "epoch": 10.727272727272727,
      "grad_norm": 0.0008448445587418973,
      "learning_rate": 8.821875000000001e-06,
      "loss": 0.4642,
      "step": 2360
    },
    {
      "epoch": 10.772727272727273,
      "grad_norm": 0.0007758695282973349,
      "learning_rate": 8.728125e-06,
      "loss": 0.4642,
      "step": 2370
    },
    {
      "epoch": 10.818181818181818,
      "grad_norm": 0.0007463025394827127,
      "learning_rate": 8.634375000000002e-06,
      "loss": 0.4642,
      "step": 2380
    },
    {
      "epoch": 10.863636363636363,
      "grad_norm": 0.0009579039178788662,
      "learning_rate": 8.540625e-06,
      "loss": 0.4642,
      "step": 2390
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 0.0007374187698587775,
      "learning_rate": 8.446875e-06,
      "loss": 0.4642,
      "step": 2400
    },
    {
      "epoch": 10.954545454545455,
      "grad_norm": 0.0013902366627007723,
      "learning_rate": 8.353125e-06,
      "loss": 0.4642,
      "step": 2410
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.0006980224861763418,
      "learning_rate": 8.259375000000001e-06,
      "loss": 0.4642,
      "step": 2420
    },
    {
      "epoch": 11.045454545454545,
      "grad_norm": 0.0007678446709178388,
      "learning_rate": 8.165625e-06,
      "loss": 0.4642,
      "step": 2430
    },
    {
      "epoch": 11.090909090909092,
      "grad_norm": 0.0007284774910658598,
      "learning_rate": 8.071875e-06,
      "loss": 0.4642,
      "step": 2440
    },
    {
      "epoch": 11.136363636363637,
      "grad_norm": 0.0007639494724571705,
      "learning_rate": 7.978125e-06,
      "loss": 0.4642,
      "step": 2450
    },
    {
      "epoch": 11.181818181818182,
      "grad_norm": 0.0007081347866915166,
      "learning_rate": 7.884375e-06,
      "loss": 0.4642,
      "step": 2460
    },
    {
      "epoch": 11.227272727272727,
      "grad_norm": 0.0007112338207662106,
      "learning_rate": 7.790625000000001e-06,
      "loss": 0.4642,
      "step": 2470
    },
    {
      "epoch": 11.272727272727273,
      "grad_norm": 0.0010447542881593108,
      "learning_rate": 7.696875e-06,
      "loss": 0.4642,
      "step": 2480
    },
    {
      "epoch": 11.318181818181818,
      "grad_norm": 0.0006701513775624335,
      "learning_rate": 7.603124999999999e-06,
      "loss": 0.4642,
      "step": 2490
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 0.001142153749242425,
      "learning_rate": 7.5093749999999996e-06,
      "loss": 0.4642,
      "step": 2500
    },
    {
      "epoch": 11.409090909090908,
      "grad_norm": 0.0010766165796667337,
      "learning_rate": 7.415625000000001e-06,
      "loss": 0.4642,
      "step": 2510
    },
    {
      "epoch": 11.454545454545455,
      "grad_norm": 0.0012449577916413546,
      "learning_rate": 7.321875e-06,
      "loss": 0.4642,
      "step": 2520
    },
    {
      "epoch": 11.5,
      "grad_norm": 0.0006771717453375459,
      "learning_rate": 7.228125e-06,
      "loss": 0.4642,
      "step": 2530
    },
    {
      "epoch": 11.545454545454545,
      "grad_norm": 0.0007649657782167196,
      "learning_rate": 7.134375000000001e-06,
      "loss": 0.4642,
      "step": 2540
    },
    {
      "epoch": 11.590909090909092,
      "grad_norm": 0.0007896947208791971,
      "learning_rate": 7.040625e-06,
      "loss": 0.4642,
      "step": 2550
    },
    {
      "epoch": 11.636363636363637,
      "grad_norm": 0.0009262819075956941,
      "learning_rate": 6.946875e-06,
      "loss": 0.4642,
      "step": 2560
    },
    {
      "epoch": 11.681818181818182,
      "grad_norm": 0.0008711301488801837,
      "learning_rate": 6.853125e-06,
      "loss": 0.4642,
      "step": 2570
    },
    {
      "epoch": 11.727272727272727,
      "grad_norm": 0.000668432330712676,
      "learning_rate": 6.759375e-06,
      "loss": 0.4642,
      "step": 2580
    },
    {
      "epoch": 11.772727272727273,
      "grad_norm": 0.0006604869849979877,
      "learning_rate": 6.665625e-06,
      "loss": 0.4642,
      "step": 2590
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 0.000831431127153337,
      "learning_rate": 6.571875e-06,
      "loss": 0.4642,
      "step": 2600
    },
    {
      "epoch": 11.863636363636363,
      "grad_norm": 0.0006909522926434875,
      "learning_rate": 6.478125e-06,
      "loss": 0.4642,
      "step": 2610
    },
    {
      "epoch": 11.909090909090908,
      "grad_norm": 0.0006539658643305302,
      "learning_rate": 6.384375e-06,
      "loss": 0.4642,
      "step": 2620
    },
    {
      "epoch": 11.954545454545455,
      "grad_norm": 0.0007169246091507375,
      "learning_rate": 6.2906250000000004e-06,
      "loss": 0.4642,
      "step": 2630
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.0034253369085490704,
      "learning_rate": 6.196875000000001e-06,
      "loss": 0.4642,
      "step": 2640
    },
    {
      "epoch": 12.045454545454545,
      "grad_norm": 0.0008129489724524319,
      "learning_rate": 6.103125e-06,
      "loss": 0.4642,
      "step": 2650
    },
    {
      "epoch": 12.090909090909092,
      "grad_norm": 0.0006479012663476169,
      "learning_rate": 6.009375e-06,
      "loss": 0.4642,
      "step": 2660
    },
    {
      "epoch": 12.136363636363637,
      "grad_norm": 0.0006334343343041837,
      "learning_rate": 5.915625e-06,
      "loss": 0.4642,
      "step": 2670
    },
    {
      "epoch": 12.181818181818182,
      "grad_norm": 0.0025649280287325382,
      "learning_rate": 5.821875e-06,
      "loss": 0.4642,
      "step": 2680
    },
    {
      "epoch": 12.227272727272727,
      "grad_norm": 0.0009866678155958652,
      "learning_rate": 5.728125e-06,
      "loss": 0.4642,
      "step": 2690
    },
    {
      "epoch": 12.272727272727273,
      "grad_norm": 0.0009258329519070685,
      "learning_rate": 5.634375e-06,
      "loss": 0.4642,
      "step": 2700
    },
    {
      "epoch": 12.318181818181818,
      "grad_norm": 0.000833692669402808,
      "learning_rate": 5.540625e-06,
      "loss": 0.4642,
      "step": 2710
    },
    {
      "epoch": 12.363636363636363,
      "grad_norm": 0.0006890598451718688,
      "learning_rate": 5.446874999999999e-06,
      "loss": 0.4642,
      "step": 2720
    },
    {
      "epoch": 12.409090909090908,
      "grad_norm": 0.0006404007435776293,
      "learning_rate": 5.3531250000000005e-06,
      "loss": 0.4642,
      "step": 2730
    },
    {
      "epoch": 12.454545454545455,
      "grad_norm": 0.0006160622579045594,
      "learning_rate": 5.259375000000001e-06,
      "loss": 0.4642,
      "step": 2740
    },
    {
      "epoch": 12.5,
      "grad_norm": 0.0007782665197737515,
      "learning_rate": 5.165625e-06,
      "loss": 0.4642,
      "step": 2750
    },
    {
      "epoch": 12.545454545454545,
      "grad_norm": 0.0007835779106244445,
      "learning_rate": 5.0718750000000005e-06,
      "loss": 0.4642,
      "step": 2760
    },
    {
      "epoch": 12.590909090909092,
      "grad_norm": 0.0006299831438809633,
      "learning_rate": 4.978125e-06,
      "loss": 0.4642,
      "step": 2770
    },
    {
      "epoch": 12.636363636363637,
      "grad_norm": 0.001342492294497788,
      "learning_rate": 4.884375e-06,
      "loss": 0.4642,
      "step": 2780
    },
    {
      "epoch": 12.681818181818182,
      "grad_norm": 0.0006987859378568828,
      "learning_rate": 4.790625e-06,
      "loss": 0.4642,
      "step": 2790
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 0.0009468944626860321,
      "learning_rate": 4.696875e-06,
      "loss": 0.4642,
      "step": 2800
    },
    {
      "epoch": 12.772727272727273,
      "grad_norm": 0.0006209868006408215,
      "learning_rate": 4.603125e-06,
      "loss": 0.4642,
      "step": 2810
    },
    {
      "epoch": 12.818181818181818,
      "grad_norm": 0.0011709658429026604,
      "learning_rate": 4.5093749999999995e-06,
      "loss": 0.4642,
      "step": 2820
    },
    {
      "epoch": 12.863636363636363,
      "grad_norm": 0.0008255922584794462,
      "learning_rate": 4.415625e-06,
      "loss": 0.4642,
      "step": 2830
    },
    {
      "epoch": 12.909090909090908,
      "grad_norm": 0.0011549459304660559,
      "learning_rate": 4.321875e-06,
      "loss": 0.4642,
      "step": 2840
    },
    {
      "epoch": 12.954545454545455,
      "grad_norm": 0.0006367340683937073,
      "learning_rate": 4.228125e-06,
      "loss": 0.4642,
      "step": 2850
    },
    {
      "epoch": 13.0,
      "grad_norm": 0.0013248455943539739,
      "learning_rate": 4.1343750000000005e-06,
      "loss": 0.4642,
      "step": 2860
    },
    {
      "epoch": 13.045454545454545,
      "grad_norm": 0.0006464097532443702,
      "learning_rate": 4.040625e-06,
      "loss": 0.4642,
      "step": 2870
    },
    {
      "epoch": 13.090909090909092,
      "grad_norm": 0.000654740899335593,
      "learning_rate": 3.946875e-06,
      "loss": 0.4642,
      "step": 2880
    },
    {
      "epoch": 13.136363636363637,
      "grad_norm": 0.0005906338337808847,
      "learning_rate": 3.8531250000000005e-06,
      "loss": 0.4642,
      "step": 2890
    },
    {
      "epoch": 13.181818181818182,
      "grad_norm": 0.0007064431556500494,
      "learning_rate": 3.759375e-06,
      "loss": 0.4642,
      "step": 2900
    },
    {
      "epoch": 13.227272727272727,
      "grad_norm": 0.0011158403940498829,
      "learning_rate": 3.665625e-06,
      "loss": 0.4642,
      "step": 2910
    },
    {
      "epoch": 13.272727272727273,
      "grad_norm": 0.0008564001182094216,
      "learning_rate": 3.571875e-06,
      "loss": 0.4642,
      "step": 2920
    },
    {
      "epoch": 13.318181818181818,
      "grad_norm": 0.0009798567043617368,
      "learning_rate": 3.4781250000000003e-06,
      "loss": 0.4642,
      "step": 2930
    },
    {
      "epoch": 13.363636363636363,
      "grad_norm": 0.0010887891985476017,
      "learning_rate": 3.384375e-06,
      "loss": 0.4642,
      "step": 2940
    },
    {
      "epoch": 13.409090909090908,
      "grad_norm": 0.0019831901881843805,
      "learning_rate": 3.290625e-06,
      "loss": 0.4642,
      "step": 2950
    },
    {
      "epoch": 13.454545454545455,
      "grad_norm": 0.0021282886154949665,
      "learning_rate": 3.196875e-06,
      "loss": 0.4642,
      "step": 2960
    },
    {
      "epoch": 13.5,
      "grad_norm": 0.000765003904234618,
      "learning_rate": 3.103125e-06,
      "loss": 0.4642,
      "step": 2970
    },
    {
      "epoch": 13.545454545454545,
      "grad_norm": 0.0005939335678704083,
      "learning_rate": 3.009375e-06,
      "loss": 0.4642,
      "step": 2980
    },
    {
      "epoch": 13.590909090909092,
      "grad_norm": 0.0006015360122546554,
      "learning_rate": 2.915625e-06,
      "loss": 0.4642,
      "step": 2990
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 0.0006400559796020389,
      "learning_rate": 2.821875e-06,
      "loss": 0.4642,
      "step": 3000
    },
    {
      "epoch": 13.681818181818182,
      "grad_norm": 0.0005637428839690983,
      "learning_rate": 2.7281250000000002e-06,
      "loss": 0.4642,
      "step": 3010
    },
    {
      "epoch": 13.727272727272727,
      "grad_norm": 0.0013766190968453884,
      "learning_rate": 2.634375e-06,
      "loss": 0.4642,
      "step": 3020
    },
    {
      "epoch": 13.772727272727273,
      "grad_norm": 0.0006614591111429036,
      "learning_rate": 2.540625e-06,
      "loss": 0.4642,
      "step": 3030
    },
    {
      "epoch": 13.818181818181818,
      "grad_norm": 0.0005750561831519008,
      "learning_rate": 2.4468749999999998e-06,
      "loss": 0.4642,
      "step": 3040
    },
    {
      "epoch": 13.863636363636363,
      "grad_norm": 0.0006187203689478338,
      "learning_rate": 2.353125e-06,
      "loss": 0.4642,
      "step": 3050
    },
    {
      "epoch": 13.909090909090908,
      "grad_norm": 0.0005804410902783275,
      "learning_rate": 2.2593750000000003e-06,
      "loss": 0.4642,
      "step": 3060
    },
    {
      "epoch": 13.954545454545455,
      "grad_norm": 0.0005752899451181293,
      "learning_rate": 2.165625e-06,
      "loss": 0.4642,
      "step": 3070
    },
    {
      "epoch": 14.0,
      "grad_norm": 0.0005944357835687697,
      "learning_rate": 2.071875e-06,
      "loss": 0.4642,
      "step": 3080
    },
    {
      "epoch": 14.045454545454545,
      "grad_norm": 0.0009219255880452693,
      "learning_rate": 1.978125e-06,
      "loss": 0.4642,
      "step": 3090
    },
    {
      "epoch": 14.090909090909092,
      "grad_norm": 0.0006777294329367578,
      "learning_rate": 1.8843749999999999e-06,
      "loss": 0.4642,
      "step": 3100
    },
    {
      "epoch": 14.136363636363637,
      "grad_norm": 0.0006409533671103418,
      "learning_rate": 1.790625e-06,
      "loss": 0.4642,
      "step": 3110
    },
    {
      "epoch": 14.181818181818182,
      "grad_norm": 0.0009574271971359849,
      "learning_rate": 1.6968750000000002e-06,
      "loss": 0.4642,
      "step": 3120
    },
    {
      "epoch": 14.227272727272727,
      "grad_norm": 0.0005896387156099081,
      "learning_rate": 1.603125e-06,
      "loss": 0.4642,
      "step": 3130
    },
    {
      "epoch": 14.272727272727273,
      "grad_norm": 0.0007913033477962017,
      "learning_rate": 1.509375e-06,
      "loss": 0.4642,
      "step": 3140
    },
    {
      "epoch": 14.318181818181818,
      "grad_norm": 0.0006301162648014724,
      "learning_rate": 1.415625e-06,
      "loss": 0.4642,
      "step": 3150
    },
    {
      "epoch": 14.363636363636363,
      "grad_norm": 0.001242926111444831,
      "learning_rate": 1.321875e-06,
      "loss": 0.4642,
      "step": 3160
    },
    {
      "epoch": 14.409090909090908,
      "grad_norm": 0.0006306195282377303,
      "learning_rate": 1.228125e-06,
      "loss": 0.4642,
      "step": 3170
    },
    {
      "epoch": 14.454545454545455,
      "grad_norm": 0.0011001962702721357,
      "learning_rate": 1.134375e-06,
      "loss": 0.4642,
      "step": 3180
    },
    {
      "epoch": 14.5,
      "grad_norm": 0.0005850127781741321,
      "learning_rate": 1.040625e-06,
      "loss": 0.4642,
      "step": 3190
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 0.000591814168728888,
      "learning_rate": 9.46875e-07,
      "loss": 0.4642,
      "step": 3200
    },
    {
      "epoch": 14.590909090909092,
      "grad_norm": 0.0009115299908444285,
      "learning_rate": 8.531250000000001e-07,
      "loss": 0.4642,
      "step": 3210
    },
    {
      "epoch": 14.636363636363637,
      "grad_norm": 0.0013371146051213145,
      "learning_rate": 7.59375e-07,
      "loss": 0.4642,
      "step": 3220
    },
    {
      "epoch": 14.681818181818182,
      "grad_norm": 0.0005926316953264177,
      "learning_rate": 6.65625e-07,
      "loss": 0.4642,
      "step": 3230
    },
    {
      "epoch": 14.727272727272727,
      "grad_norm": 0.0006567013333551586,
      "learning_rate": 5.71875e-07,
      "loss": 0.4642,
      "step": 3240
    },
    {
      "epoch": 14.772727272727273,
      "grad_norm": 0.0005570892244577408,
      "learning_rate": 4.78125e-07,
      "loss": 0.4642,
      "step": 3250
    },
    {
      "epoch": 14.818181818181818,
      "grad_norm": 0.0006097027217037976,
      "learning_rate": 3.8437499999999997e-07,
      "loss": 0.4642,
      "step": 3260
    },
    {
      "epoch": 14.863636363636363,
      "grad_norm": 0.0005551028298214078,
      "learning_rate": 2.90625e-07,
      "loss": 0.4642,
      "step": 3270
    },
    {
      "epoch": 14.909090909090908,
      "grad_norm": 0.0005862545222043991,
      "learning_rate": 1.96875e-07,
      "loss": 0.4642,
      "step": 3280
    },
    {
      "epoch": 14.954545454545455,
      "grad_norm": 0.0006166801322251558,
      "learning_rate": 1.03125e-07,
      "loss": 0.4642,
      "step": 3290
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.0006039313157089055,
      "learning_rate": 9.375000000000001e-09,
      "loss": 0.4642,
      "step": 3300
    }
  ],
  "logging_steps": 10,
  "max_steps": 3300,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3085626003947520.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
