{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 15.0,
  "eval_steps": 500,
  "global_step": 1455,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.10309278350515463,
      "grad_norm": 2.8852572441101074,
      "learning_rate": 2.7e-06,
      "loss": 4.2711,
      "step": 10
    },
    {
      "epoch": 0.20618556701030927,
      "grad_norm": 2.9242453575134277,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 4.2592,
      "step": 20
    },
    {
      "epoch": 0.30927835051546393,
      "grad_norm": 3.0398521423339844,
      "learning_rate": 8.7e-06,
      "loss": 4.2209,
      "step": 30
    },
    {
      "epoch": 0.41237113402061853,
      "grad_norm": 3.177349090576172,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 4.148,
      "step": 40
    },
    {
      "epoch": 0.5154639175257731,
      "grad_norm": 3.504631280899048,
      "learning_rate": 1.47e-05,
      "loss": 4.03,
      "step": 50
    },
    {
      "epoch": 0.6185567010309279,
      "grad_norm": 4.516543865203857,
      "learning_rate": 1.77e-05,
      "loss": 3.8428,
      "step": 60
    },
    {
      "epoch": 0.7216494845360825,
      "grad_norm": 4.829545497894287,
      "learning_rate": 2.07e-05,
      "loss": 3.5609,
      "step": 70
    },
    {
      "epoch": 0.8247422680412371,
      "grad_norm": 4.9151082038879395,
      "learning_rate": 2.37e-05,
      "loss": 3.1955,
      "step": 80
    },
    {
      "epoch": 0.9278350515463918,
      "grad_norm": 5.433284759521484,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.7385,
      "step": 90
    },
    {
      "epoch": 1.0309278350515463,
      "grad_norm": 5.078189849853516,
      "learning_rate": 2.97e-05,
      "loss": 2.1999,
      "step": 100
    },
    {
      "epoch": 1.134020618556701,
      "grad_norm": 6.039065837860107,
      "learning_rate": 2.9800738007380074e-05,
      "loss": 1.6834,
      "step": 110
    },
    {
      "epoch": 1.2371134020618557,
      "grad_norm": 3.9825079441070557,
      "learning_rate": 2.9579335793357932e-05,
      "loss": 1.2803,
      "step": 120
    },
    {
      "epoch": 1.3402061855670104,
      "grad_norm": 3.4676005840301514,
      "learning_rate": 2.9357933579335793e-05,
      "loss": 1.0353,
      "step": 130
    },
    {
      "epoch": 1.443298969072165,
      "grad_norm": 1.3226354122161865,
      "learning_rate": 2.913653136531365e-05,
      "loss": 0.8158,
      "step": 140
    },
    {
      "epoch": 1.5463917525773194,
      "grad_norm": 0.21673135459423065,
      "learning_rate": 2.8915129151291516e-05,
      "loss": 0.7346,
      "step": 150
    },
    {
      "epoch": 1.6494845360824741,
      "grad_norm": 0.16362540423870087,
      "learning_rate": 2.8693726937269373e-05,
      "loss": 0.727,
      "step": 160
    },
    {
      "epoch": 1.7525773195876289,
      "grad_norm": 0.14210520684719086,
      "learning_rate": 2.8472324723247235e-05,
      "loss": 0.7226,
      "step": 170
    },
    {
      "epoch": 1.8556701030927836,
      "grad_norm": 0.13436594605445862,
      "learning_rate": 2.8250922509225092e-05,
      "loss": 0.7194,
      "step": 180
    },
    {
      "epoch": 1.9587628865979383,
      "grad_norm": 0.12905757129192352,
      "learning_rate": 2.8029520295202954e-05,
      "loss": 0.7168,
      "step": 190
    },
    {
      "epoch": 2.0618556701030926,
      "grad_norm": 0.1181640550494194,
      "learning_rate": 2.780811808118081e-05,
      "loss": 0.7146,
      "step": 200
    },
    {
      "epoch": 2.1649484536082473,
      "grad_norm": 0.07928981631994247,
      "learning_rate": 2.7586715867158673e-05,
      "loss": 0.7128,
      "step": 210
    },
    {
      "epoch": 2.268041237113402,
      "grad_norm": 0.04510723799467087,
      "learning_rate": 2.736531365313653e-05,
      "loss": 0.7119,
      "step": 220
    },
    {
      "epoch": 2.3711340206185567,
      "grad_norm": 0.04585675895214081,
      "learning_rate": 2.7143911439114392e-05,
      "loss": 0.7116,
      "step": 230
    },
    {
      "epoch": 2.4742268041237114,
      "grad_norm": 0.04466419667005539,
      "learning_rate": 2.692250922509225e-05,
      "loss": 0.7113,
      "step": 240
    },
    {
      "epoch": 2.5773195876288657,
      "grad_norm": 0.042852044105529785,
      "learning_rate": 2.6701107011070114e-05,
      "loss": 0.7111,
      "step": 250
    },
    {
      "epoch": 2.680412371134021,
      "grad_norm": 0.04413126781582832,
      "learning_rate": 2.6479704797047972e-05,
      "loss": 0.7109,
      "step": 260
    },
    {
      "epoch": 2.783505154639175,
      "grad_norm": 0.04329627752304077,
      "learning_rate": 2.6258302583025833e-05,
      "loss": 0.7106,
      "step": 270
    },
    {
      "epoch": 2.88659793814433,
      "grad_norm": 0.04280366376042366,
      "learning_rate": 2.603690036900369e-05,
      "loss": 0.7104,
      "step": 280
    },
    {
      "epoch": 2.9896907216494846,
      "grad_norm": 0.043947525322437286,
      "learning_rate": 2.581549815498155e-05,
      "loss": 0.7101,
      "step": 290
    },
    {
      "epoch": 3.0927835051546393,
      "grad_norm": 0.046004991978406906,
      "learning_rate": 2.559409594095941e-05,
      "loss": 0.7098,
      "step": 300
    },
    {
      "epoch": 3.195876288659794,
      "grad_norm": 0.04761834070086479,
      "learning_rate": 2.5372693726937268e-05,
      "loss": 0.7095,
      "step": 310
    },
    {
      "epoch": 3.2989690721649483,
      "grad_norm": 0.04977985471487045,
      "learning_rate": 2.515129151291513e-05,
      "loss": 0.7092,
      "step": 320
    },
    {
      "epoch": 3.402061855670103,
      "grad_norm": 0.0516384020447731,
      "learning_rate": 2.4929889298892987e-05,
      "loss": 0.7089,
      "step": 330
    },
    {
      "epoch": 3.5051546391752577,
      "grad_norm": 0.052898991852998734,
      "learning_rate": 2.4708487084870848e-05,
      "loss": 0.7085,
      "step": 340
    },
    {
      "epoch": 3.6082474226804124,
      "grad_norm": 0.05773530900478363,
      "learning_rate": 2.448708487084871e-05,
      "loss": 0.7081,
      "step": 350
    },
    {
      "epoch": 3.711340206185567,
      "grad_norm": 0.061407286673784256,
      "learning_rate": 2.426568265682657e-05,
      "loss": 0.7076,
      "step": 360
    },
    {
      "epoch": 3.8144329896907214,
      "grad_norm": 0.06545349955558777,
      "learning_rate": 2.404428044280443e-05,
      "loss": 0.707,
      "step": 370
    },
    {
      "epoch": 3.917525773195876,
      "grad_norm": 0.06916937232017517,
      "learning_rate": 2.382287822878229e-05,
      "loss": 0.7064,
      "step": 380
    },
    {
      "epoch": 4.020618556701031,
      "grad_norm": 0.07832988351583481,
      "learning_rate": 2.3601476014760147e-05,
      "loss": 0.7057,
      "step": 390
    },
    {
      "epoch": 4.123711340206185,
      "grad_norm": 0.07916425913572311,
      "learning_rate": 2.338007380073801e-05,
      "loss": 0.705,
      "step": 400
    },
    {
      "epoch": 4.22680412371134,
      "grad_norm": 0.09260431677103043,
      "learning_rate": 2.3158671586715866e-05,
      "loss": 0.7039,
      "step": 410
    },
    {
      "epoch": 4.329896907216495,
      "grad_norm": 0.10574176907539368,
      "learning_rate": 2.2937269372693728e-05,
      "loss": 0.7028,
      "step": 420
    },
    {
      "epoch": 4.43298969072165,
      "grad_norm": 0.11403672397136688,
      "learning_rate": 2.2715867158671585e-05,
      "loss": 0.7014,
      "step": 430
    },
    {
      "epoch": 4.536082474226804,
      "grad_norm": 0.11515679210424423,
      "learning_rate": 2.2494464944649447e-05,
      "loss": 0.6998,
      "step": 440
    },
    {
      "epoch": 4.639175257731958,
      "grad_norm": 0.11179666221141815,
      "learning_rate": 2.2273062730627308e-05,
      "loss": 0.6981,
      "step": 450
    },
    {
      "epoch": 4.742268041237113,
      "grad_norm": 0.10999076068401337,
      "learning_rate": 2.205166051660517e-05,
      "loss": 0.6963,
      "step": 460
    },
    {
      "epoch": 4.845360824742268,
      "grad_norm": 0.10533085465431213,
      "learning_rate": 2.1830258302583027e-05,
      "loss": 0.6947,
      "step": 470
    },
    {
      "epoch": 4.948453608247423,
      "grad_norm": 0.10068085044622421,
      "learning_rate": 2.1608856088560888e-05,
      "loss": 0.6929,
      "step": 480
    },
    {
      "epoch": 5.051546391752577,
      "grad_norm": 0.08974632620811462,
      "learning_rate": 2.1387453874538746e-05,
      "loss": 0.6912,
      "step": 490
    },
    {
      "epoch": 5.154639175257732,
      "grad_norm": 0.08097260445356369,
      "learning_rate": 2.1166051660516604e-05,
      "loss": 0.6896,
      "step": 500
    },
    {
      "epoch": 5.257731958762887,
      "grad_norm": 0.07323428988456726,
      "learning_rate": 2.0944649446494465e-05,
      "loss": 0.6882,
      "step": 510
    },
    {
      "epoch": 5.360824742268041,
      "grad_norm": 0.06945792585611343,
      "learning_rate": 2.0723247232472323e-05,
      "loss": 0.6868,
      "step": 520
    },
    {
      "epoch": 5.463917525773196,
      "grad_norm": 0.06564440578222275,
      "learning_rate": 2.0501845018450184e-05,
      "loss": 0.6855,
      "step": 530
    },
    {
      "epoch": 5.56701030927835,
      "grad_norm": 0.06446046382188797,
      "learning_rate": 2.0280442804428042e-05,
      "loss": 0.6841,
      "step": 540
    },
    {
      "epoch": 5.670103092783505,
      "grad_norm": 0.06506499648094177,
      "learning_rate": 2.0059040590405906e-05,
      "loss": 0.6828,
      "step": 550
    },
    {
      "epoch": 5.77319587628866,
      "grad_norm": 0.06512486934661865,
      "learning_rate": 1.9837638376383764e-05,
      "loss": 0.6814,
      "step": 560
    },
    {
      "epoch": 5.876288659793815,
      "grad_norm": 0.0645662173628807,
      "learning_rate": 1.9616236162361625e-05,
      "loss": 0.68,
      "step": 570
    },
    {
      "epoch": 5.979381443298969,
      "grad_norm": 0.06583859026432037,
      "learning_rate": 1.9394833948339483e-05,
      "loss": 0.6787,
      "step": 580
    },
    {
      "epoch": 6.082474226804123,
      "grad_norm": 0.06712577491998672,
      "learning_rate": 1.9173431734317345e-05,
      "loss": 0.6774,
      "step": 590
    },
    {
      "epoch": 6.185567010309279,
      "grad_norm": 0.067998506128788,
      "learning_rate": 1.8952029520295202e-05,
      "loss": 0.676,
      "step": 600
    },
    {
      "epoch": 6.288659793814433,
      "grad_norm": 0.06775397807359695,
      "learning_rate": 1.8730627306273064e-05,
      "loss": 0.6747,
      "step": 610
    },
    {
      "epoch": 6.391752577319588,
      "grad_norm": 0.07035321742296219,
      "learning_rate": 1.850922509225092e-05,
      "loss": 0.6733,
      "step": 620
    },
    {
      "epoch": 6.494845360824742,
      "grad_norm": 0.07195650786161423,
      "learning_rate": 1.8287822878228783e-05,
      "loss": 0.6719,
      "step": 630
    },
    {
      "epoch": 6.597938144329897,
      "grad_norm": 0.07609745115041733,
      "learning_rate": 1.806642066420664e-05,
      "loss": 0.6704,
      "step": 640
    },
    {
      "epoch": 6.701030927835052,
      "grad_norm": 0.0813489630818367,
      "learning_rate": 1.7845018450184505e-05,
      "loss": 0.669,
      "step": 650
    },
    {
      "epoch": 6.804123711340206,
      "grad_norm": 0.07999131828546524,
      "learning_rate": 1.7623616236162363e-05,
      "loss": 0.6672,
      "step": 660
    },
    {
      "epoch": 6.907216494845361,
      "grad_norm": 0.08563375473022461,
      "learning_rate": 1.7402214022140224e-05,
      "loss": 0.6655,
      "step": 670
    },
    {
      "epoch": 7.010309278350515,
      "grad_norm": 0.0884375125169754,
      "learning_rate": 1.7180811808118082e-05,
      "loss": 0.6639,
      "step": 680
    },
    {
      "epoch": 7.11340206185567,
      "grad_norm": 0.09349290281534195,
      "learning_rate": 1.6959409594095943e-05,
      "loss": 0.662,
      "step": 690
    },
    {
      "epoch": 7.216494845360825,
      "grad_norm": 0.09915819764137268,
      "learning_rate": 1.67380073800738e-05,
      "loss": 0.6599,
      "step": 700
    },
    {
      "epoch": 7.319587628865979,
      "grad_norm": 0.10668974369764328,
      "learning_rate": 1.651660516605166e-05,
      "loss": 0.6578,
      "step": 710
    },
    {
      "epoch": 7.422680412371134,
      "grad_norm": 0.11356887966394424,
      "learning_rate": 1.629520295202952e-05,
      "loss": 0.6556,
      "step": 720
    },
    {
      "epoch": 7.525773195876289,
      "grad_norm": 0.12782879173755646,
      "learning_rate": 1.6073800738007378e-05,
      "loss": 0.653,
      "step": 730
    },
    {
      "epoch": 7.628865979381443,
      "grad_norm": 0.14645376801490784,
      "learning_rate": 1.585239852398524e-05,
      "loss": 0.6501,
      "step": 740
    },
    {
      "epoch": 7.731958762886598,
      "grad_norm": 0.1767863929271698,
      "learning_rate": 1.56309963099631e-05,
      "loss": 0.6469,
      "step": 750
    },
    {
      "epoch": 7.835051546391752,
      "grad_norm": 0.23285363614559174,
      "learning_rate": 1.540959409594096e-05,
      "loss": 0.6429,
      "step": 760
    },
    {
      "epoch": 7.938144329896907,
      "grad_norm": 0.38007020950317383,
      "learning_rate": 1.5188191881918821e-05,
      "loss": 0.6375,
      "step": 770
    },
    {
      "epoch": 8.041237113402062,
      "grad_norm": 0.6360693573951721,
      "learning_rate": 1.496678966789668e-05,
      "loss": 0.6273,
      "step": 780
    },
    {
      "epoch": 8.144329896907216,
      "grad_norm": 0.7090991139411926,
      "learning_rate": 1.474538745387454e-05,
      "loss": 0.6106,
      "step": 790
    },
    {
      "epoch": 8.24742268041237,
      "grad_norm": 0.2671281695365906,
      "learning_rate": 1.45239852398524e-05,
      "loss": 0.5993,
      "step": 800
    },
    {
      "epoch": 8.350515463917526,
      "grad_norm": 0.27472564578056335,
      "learning_rate": 1.4302583025830259e-05,
      "loss": 0.5884,
      "step": 810
    },
    {
      "epoch": 8.45360824742268,
      "grad_norm": 0.34358614683151245,
      "learning_rate": 1.4081180811808118e-05,
      "loss": 0.5774,
      "step": 820
    },
    {
      "epoch": 8.556701030927835,
      "grad_norm": 0.36636045575141907,
      "learning_rate": 1.3859778597785978e-05,
      "loss": 0.5667,
      "step": 830
    },
    {
      "epoch": 8.65979381443299,
      "grad_norm": 0.3229757249355316,
      "learning_rate": 1.3638376383763838e-05,
      "loss": 0.5563,
      "step": 840
    },
    {
      "epoch": 8.762886597938145,
      "grad_norm": 0.7651602625846863,
      "learning_rate": 1.3416974169741697e-05,
      "loss": 0.5462,
      "step": 850
    },
    {
      "epoch": 8.8659793814433,
      "grad_norm": 0.44008520245552063,
      "learning_rate": 1.3195571955719557e-05,
      "loss": 0.5359,
      "step": 860
    },
    {
      "epoch": 8.969072164948454,
      "grad_norm": 0.2920249104499817,
      "learning_rate": 1.2974169741697418e-05,
      "loss": 0.5263,
      "step": 870
    },
    {
      "epoch": 9.072164948453608,
      "grad_norm": 0.3215532898902893,
      "learning_rate": 1.2752767527675277e-05,
      "loss": 0.5163,
      "step": 880
    },
    {
      "epoch": 9.175257731958762,
      "grad_norm": 0.29618895053863525,
      "learning_rate": 1.2531365313653137e-05,
      "loss": 0.5071,
      "step": 890
    },
    {
      "epoch": 9.278350515463918,
      "grad_norm": 0.4271494150161743,
      "learning_rate": 1.2309963099630996e-05,
      "loss": 0.4994,
      "step": 900
    },
    {
      "epoch": 9.381443298969073,
      "grad_norm": 0.28482940793037415,
      "learning_rate": 1.2088560885608856e-05,
      "loss": 0.4908,
      "step": 910
    },
    {
      "epoch": 9.484536082474227,
      "grad_norm": 0.3181530237197876,
      "learning_rate": 1.1867158671586717e-05,
      "loss": 0.4844,
      "step": 920
    },
    {
      "epoch": 9.587628865979381,
      "grad_norm": 0.33603110909461975,
      "learning_rate": 1.1645756457564577e-05,
      "loss": 0.4784,
      "step": 930
    },
    {
      "epoch": 9.690721649484535,
      "grad_norm": 0.23541906476020813,
      "learning_rate": 1.1424354243542436e-05,
      "loss": 0.4735,
      "step": 940
    },
    {
      "epoch": 9.793814432989691,
      "grad_norm": 0.2029321938753128,
      "learning_rate": 1.1202952029520296e-05,
      "loss": 0.4701,
      "step": 950
    },
    {
      "epoch": 9.896907216494846,
      "grad_norm": 0.15668128430843353,
      "learning_rate": 1.0981549815498155e-05,
      "loss": 0.4681,
      "step": 960
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.09181110560894012,
      "learning_rate": 1.0760147601476016e-05,
      "loss": 0.4668,
      "step": 970
    },
    {
      "epoch": 10.103092783505154,
      "grad_norm": 0.06227580085396767,
      "learning_rate": 1.0538745387453876e-05,
      "loss": 0.4662,
      "step": 980
    },
    {
      "epoch": 10.206185567010309,
      "grad_norm": 0.08010394871234894,
      "learning_rate": 1.0317343173431735e-05,
      "loss": 0.4658,
      "step": 990
    },
    {
      "epoch": 10.309278350515465,
      "grad_norm": 0.06539181619882584,
      "learning_rate": 1.0095940959409595e-05,
      "loss": 0.4655,
      "step": 1000
    },
    {
      "epoch": 10.412371134020619,
      "grad_norm": 0.032139524817466736,
      "learning_rate": 9.874538745387454e-06,
      "loss": 0.4654,
      "step": 1010
    },
    {
      "epoch": 10.515463917525773,
      "grad_norm": 0.04371219873428345,
      "learning_rate": 9.653136531365314e-06,
      "loss": 0.4652,
      "step": 1020
    },
    {
      "epoch": 10.618556701030927,
      "grad_norm": 0.05580022931098938,
      "learning_rate": 9.431734317343173e-06,
      "loss": 0.4651,
      "step": 1030
    },
    {
      "epoch": 10.721649484536082,
      "grad_norm": 0.026278739795088768,
      "learning_rate": 9.210332103321033e-06,
      "loss": 0.465,
      "step": 1040
    },
    {
      "epoch": 10.824742268041238,
      "grad_norm": 0.0304583627730608,
      "learning_rate": 8.988929889298892e-06,
      "loss": 0.465,
      "step": 1050
    },
    {
      "epoch": 10.927835051546392,
      "grad_norm": 0.0342554897069931,
      "learning_rate": 8.767527675276752e-06,
      "loss": 0.4649,
      "step": 1060
    },
    {
      "epoch": 11.030927835051546,
      "grad_norm": 0.027393069118261337,
      "learning_rate": 8.546125461254613e-06,
      "loss": 0.4648,
      "step": 1070
    },
    {
      "epoch": 11.1340206185567,
      "grad_norm": 0.01861928403377533,
      "learning_rate": 8.324723247232473e-06,
      "loss": 0.4648,
      "step": 1080
    },
    {
      "epoch": 11.237113402061855,
      "grad_norm": 0.015897737815976143,
      "learning_rate": 8.103321033210332e-06,
      "loss": 0.4648,
      "step": 1090
    },
    {
      "epoch": 11.34020618556701,
      "grad_norm": 0.022250067442655563,
      "learning_rate": 7.881918819188192e-06,
      "loss": 0.4648,
      "step": 1100
    },
    {
      "epoch": 11.443298969072165,
      "grad_norm": 0.01476998534053564,
      "learning_rate": 7.660516605166051e-06,
      "loss": 0.4647,
      "step": 1110
    },
    {
      "epoch": 11.54639175257732,
      "grad_norm": 0.014889853075146675,
      "learning_rate": 7.439114391143912e-06,
      "loss": 0.4647,
      "step": 1120
    },
    {
      "epoch": 11.649484536082474,
      "grad_norm": 0.054178863763809204,
      "learning_rate": 7.217712177121771e-06,
      "loss": 0.4647,
      "step": 1130
    },
    {
      "epoch": 11.75257731958763,
      "grad_norm": 0.014278178103268147,
      "learning_rate": 6.9963099630996315e-06,
      "loss": 0.4646,
      "step": 1140
    },
    {
      "epoch": 11.855670103092784,
      "grad_norm": 0.014502245932817459,
      "learning_rate": 6.774907749077491e-06,
      "loss": 0.4646,
      "step": 1150
    },
    {
      "epoch": 11.958762886597938,
      "grad_norm": 0.013849555514752865,
      "learning_rate": 6.553505535055351e-06,
      "loss": 0.4646,
      "step": 1160
    },
    {
      "epoch": 12.061855670103093,
      "grad_norm": 0.016669781878590584,
      "learning_rate": 6.332103321033211e-06,
      "loss": 0.4646,
      "step": 1170
    },
    {
      "epoch": 12.164948453608247,
      "grad_norm": 0.024310989305377007,
      "learning_rate": 6.1107011070110695e-06,
      "loss": 0.4646,
      "step": 1180
    },
    {
      "epoch": 12.268041237113403,
      "grad_norm": 0.015370896086096764,
      "learning_rate": 5.88929889298893e-06,
      "loss": 0.4646,
      "step": 1190
    },
    {
      "epoch": 12.371134020618557,
      "grad_norm": 0.018044553697109222,
      "learning_rate": 5.6678966789667894e-06,
      "loss": 0.4646,
      "step": 1200
    },
    {
      "epoch": 12.474226804123711,
      "grad_norm": 0.015504284761846066,
      "learning_rate": 5.44649446494465e-06,
      "loss": 0.4645,
      "step": 1210
    },
    {
      "epoch": 12.577319587628866,
      "grad_norm": 0.01716674491763115,
      "learning_rate": 5.225092250922509e-06,
      "loss": 0.4645,
      "step": 1220
    },
    {
      "epoch": 12.68041237113402,
      "grad_norm": 0.018515387549996376,
      "learning_rate": 5.003690036900369e-06,
      "loss": 0.4645,
      "step": 1230
    },
    {
      "epoch": 12.783505154639176,
      "grad_norm": 0.01724458672106266,
      "learning_rate": 4.782287822878229e-06,
      "loss": 0.4645,
      "step": 1240
    },
    {
      "epoch": 12.88659793814433,
      "grad_norm": 0.020749958232045174,
      "learning_rate": 4.560885608856089e-06,
      "loss": 0.4645,
      "step": 1250
    },
    {
      "epoch": 12.989690721649485,
      "grad_norm": 0.02295425534248352,
      "learning_rate": 4.339483394833949e-06,
      "loss": 0.4645,
      "step": 1260
    },
    {
      "epoch": 13.092783505154639,
      "grad_norm": 0.01030000764876604,
      "learning_rate": 4.118081180811809e-06,
      "loss": 0.4645,
      "step": 1270
    },
    {
      "epoch": 13.195876288659793,
      "grad_norm": 0.0236645694822073,
      "learning_rate": 3.896678966789667e-06,
      "loss": 0.4645,
      "step": 1280
    },
    {
      "epoch": 13.29896907216495,
      "grad_norm": 0.00933060236275196,
      "learning_rate": 3.675276752767528e-06,
      "loss": 0.4645,
      "step": 1290
    },
    {
      "epoch": 13.402061855670103,
      "grad_norm": 0.008531731553375721,
      "learning_rate": 3.4538745387453876e-06,
      "loss": 0.4645,
      "step": 1300
    },
    {
      "epoch": 13.505154639175258,
      "grad_norm": 0.011960811913013458,
      "learning_rate": 3.232472324723247e-06,
      "loss": 0.4645,
      "step": 1310
    },
    {
      "epoch": 13.608247422680412,
      "grad_norm": 0.012317224405705929,
      "learning_rate": 3.011070110701107e-06,
      "loss": 0.4645,
      "step": 1320
    },
    {
      "epoch": 13.711340206185566,
      "grad_norm": 0.010905150324106216,
      "learning_rate": 2.789667896678967e-06,
      "loss": 0.4645,
      "step": 1330
    },
    {
      "epoch": 13.814432989690722,
      "grad_norm": 0.008025813847780228,
      "learning_rate": 2.568265682656827e-06,
      "loss": 0.4645,
      "step": 1340
    },
    {
      "epoch": 13.917525773195877,
      "grad_norm": 0.0237551499158144,
      "learning_rate": 2.3468634686346864e-06,
      "loss": 0.4645,
      "step": 1350
    },
    {
      "epoch": 14.02061855670103,
      "grad_norm": 0.008257406763732433,
      "learning_rate": 2.125461254612546e-06,
      "loss": 0.4645,
      "step": 1360
    },
    {
      "epoch": 14.123711340206185,
      "grad_norm": 0.007931391708552837,
      "learning_rate": 1.9040590405904059e-06,
      "loss": 0.4645,
      "step": 1370
    },
    {
      "epoch": 14.22680412371134,
      "grad_norm": 0.012501969002187252,
      "learning_rate": 1.6826568265682656e-06,
      "loss": 0.4645,
      "step": 1380
    },
    {
      "epoch": 14.329896907216495,
      "grad_norm": 0.008166053332388401,
      "learning_rate": 1.4612546125461255e-06,
      "loss": 0.4645,
      "step": 1390
    },
    {
      "epoch": 14.43298969072165,
      "grad_norm": 0.013154321350157261,
      "learning_rate": 1.2398523985239853e-06,
      "loss": 0.4645,
      "step": 1400
    },
    {
      "epoch": 14.536082474226804,
      "grad_norm": 0.009114671498537064,
      "learning_rate": 1.018450184501845e-06,
      "loss": 0.4645,
      "step": 1410
    },
    {
      "epoch": 14.639175257731958,
      "grad_norm": 0.009966427460312843,
      "learning_rate": 7.970479704797048e-07,
      "loss": 0.4645,
      "step": 1420
    },
    {
      "epoch": 14.742268041237114,
      "grad_norm": 0.01172326784580946,
      "learning_rate": 5.756457564575646e-07,
      "loss": 0.4645,
      "step": 1430
    },
    {
      "epoch": 14.845360824742269,
      "grad_norm": 0.009333634749054909,
      "learning_rate": 3.5424354243542434e-07,
      "loss": 0.4645,
      "step": 1440
    },
    {
      "epoch": 14.948453608247423,
      "grad_norm": 0.020853698253631592,
      "learning_rate": 1.3284132841328412e-07,
      "loss": 0.4645,
      "step": 1450
    }
  ],
  "logging_steps": 10,
  "max_steps": 1455,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1360480556285952.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
