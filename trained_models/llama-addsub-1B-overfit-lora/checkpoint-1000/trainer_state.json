{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 14.492753623188406,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.14492753623188406,
      "grad_norm": 2.742799758911133,
      "learning_rate": 2.7e-06,
      "loss": 4.2714,
      "step": 10
    },
    {
      "epoch": 0.2898550724637681,
      "grad_norm": 2.7862534523010254,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 4.2595,
      "step": 20
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 2.8392655849456787,
      "learning_rate": 8.7e-06,
      "loss": 4.2212,
      "step": 30
    },
    {
      "epoch": 0.5797101449275363,
      "grad_norm": 2.9891510009765625,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 4.1487,
      "step": 40
    },
    {
      "epoch": 0.7246376811594203,
      "grad_norm": 3.3146071434020996,
      "learning_rate": 1.47e-05,
      "loss": 4.0342,
      "step": 50
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 4.267806053161621,
      "learning_rate": 1.77e-05,
      "loss": 3.8534,
      "step": 60
    },
    {
      "epoch": 1.0144927536231885,
      "grad_norm": 4.668735504150391,
      "learning_rate": 2.07e-05,
      "loss": 3.5776,
      "step": 70
    },
    {
      "epoch": 1.1594202898550725,
      "grad_norm": 4.862610340118408,
      "learning_rate": 2.37e-05,
      "loss": 3.2235,
      "step": 80
    },
    {
      "epoch": 1.3043478260869565,
      "grad_norm": 5.605011940002441,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.7789,
      "step": 90
    },
    {
      "epoch": 1.4492753623188406,
      "grad_norm": 5.159924507141113,
      "learning_rate": 2.97e-05,
      "loss": 2.2566,
      "step": 100
    },
    {
      "epoch": 1.5942028985507246,
      "grad_norm": 4.297358512878418,
      "learning_rate": 2.9711229946524065e-05,
      "loss": 1.7548,
      "step": 110
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 3.453411340713501,
      "learning_rate": 2.9390374331550804e-05,
      "loss": 1.3496,
      "step": 120
    },
    {
      "epoch": 1.8840579710144927,
      "grad_norm": 3.1186459064483643,
      "learning_rate": 2.906951871657754e-05,
      "loss": 1.0473,
      "step": 130
    },
    {
      "epoch": 2.028985507246377,
      "grad_norm": 1.4859881401062012,
      "learning_rate": 2.874866310160428e-05,
      "loss": 0.8234,
      "step": 140
    },
    {
      "epoch": 2.1739130434782608,
      "grad_norm": 0.2185765653848648,
      "learning_rate": 2.8427807486631018e-05,
      "loss": 0.735,
      "step": 150
    },
    {
      "epoch": 2.318840579710145,
      "grad_norm": 0.17156125605106354,
      "learning_rate": 2.8106951871657753e-05,
      "loss": 0.7275,
      "step": 160
    },
    {
      "epoch": 2.463768115942029,
      "grad_norm": 0.14988937973976135,
      "learning_rate": 2.7786096256684492e-05,
      "loss": 0.723,
      "step": 170
    },
    {
      "epoch": 2.608695652173913,
      "grad_norm": 0.13643045723438263,
      "learning_rate": 2.746524064171123e-05,
      "loss": 0.7196,
      "step": 180
    },
    {
      "epoch": 2.753623188405797,
      "grad_norm": 0.12284619361162186,
      "learning_rate": 2.7144385026737967e-05,
      "loss": 0.7171,
      "step": 190
    },
    {
      "epoch": 2.898550724637681,
      "grad_norm": 0.10673610866069794,
      "learning_rate": 2.682352941176471e-05,
      "loss": 0.715,
      "step": 200
    },
    {
      "epoch": 3.0434782608695654,
      "grad_norm": 0.08337780088186264,
      "learning_rate": 2.6502673796791444e-05,
      "loss": 0.7135,
      "step": 210
    },
    {
      "epoch": 3.1884057971014492,
      "grad_norm": 0.05599864572286606,
      "learning_rate": 2.618181818181818e-05,
      "loss": 0.7125,
      "step": 220
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.041690926998853683,
      "learning_rate": 2.5860962566844922e-05,
      "loss": 0.712,
      "step": 230
    },
    {
      "epoch": 3.4782608695652173,
      "grad_norm": 0.038220636546611786,
      "learning_rate": 2.5540106951871658e-05,
      "loss": 0.7117,
      "step": 240
    },
    {
      "epoch": 3.6231884057971016,
      "grad_norm": 0.0372014120221138,
      "learning_rate": 2.5219251336898397e-05,
      "loss": 0.7115,
      "step": 250
    },
    {
      "epoch": 3.7681159420289854,
      "grad_norm": 0.03787326067686081,
      "learning_rate": 2.4898395721925136e-05,
      "loss": 0.7113,
      "step": 260
    },
    {
      "epoch": 3.9130434782608696,
      "grad_norm": 0.0378224216401577,
      "learning_rate": 2.457754010695187e-05,
      "loss": 0.7111,
      "step": 270
    },
    {
      "epoch": 4.057971014492754,
      "grad_norm": 0.037897441536188126,
      "learning_rate": 2.425668449197861e-05,
      "loss": 0.7109,
      "step": 280
    },
    {
      "epoch": 4.202898550724638,
      "grad_norm": 0.03725174069404602,
      "learning_rate": 2.393582887700535e-05,
      "loss": 0.7107,
      "step": 290
    },
    {
      "epoch": 4.3478260869565215,
      "grad_norm": 0.038119226694107056,
      "learning_rate": 2.3614973262032085e-05,
      "loss": 0.7105,
      "step": 300
    },
    {
      "epoch": 4.492753623188406,
      "grad_norm": 0.03850226104259491,
      "learning_rate": 2.3294117647058824e-05,
      "loss": 0.7103,
      "step": 310
    },
    {
      "epoch": 4.63768115942029,
      "grad_norm": 0.04084942862391472,
      "learning_rate": 2.2973262032085562e-05,
      "loss": 0.7101,
      "step": 320
    },
    {
      "epoch": 4.782608695652174,
      "grad_norm": 0.04069431498646736,
      "learning_rate": 2.26524064171123e-05,
      "loss": 0.7099,
      "step": 330
    },
    {
      "epoch": 4.927536231884058,
      "grad_norm": 0.04193050041794777,
      "learning_rate": 2.2331550802139037e-05,
      "loss": 0.7096,
      "step": 340
    },
    {
      "epoch": 5.072463768115942,
      "grad_norm": 0.04391707479953766,
      "learning_rate": 2.2010695187165776e-05,
      "loss": 0.7094,
      "step": 350
    },
    {
      "epoch": 5.217391304347826,
      "grad_norm": 0.045778293162584305,
      "learning_rate": 2.1689839572192515e-05,
      "loss": 0.7091,
      "step": 360
    },
    {
      "epoch": 5.36231884057971,
      "grad_norm": 0.0469028577208519,
      "learning_rate": 2.136898395721925e-05,
      "loss": 0.7088,
      "step": 370
    },
    {
      "epoch": 5.507246376811594,
      "grad_norm": 0.04777994751930237,
      "learning_rate": 2.1048128342245993e-05,
      "loss": 0.7085,
      "step": 380
    },
    {
      "epoch": 5.6521739130434785,
      "grad_norm": 0.04979841411113739,
      "learning_rate": 2.0727272727272728e-05,
      "loss": 0.7082,
      "step": 390
    },
    {
      "epoch": 5.797101449275362,
      "grad_norm": 0.051418550312519073,
      "learning_rate": 2.0406417112299464e-05,
      "loss": 0.7078,
      "step": 400
    },
    {
      "epoch": 5.942028985507246,
      "grad_norm": 0.054450444877147675,
      "learning_rate": 2.0085561497326206e-05,
      "loss": 0.7074,
      "step": 410
    },
    {
      "epoch": 6.086956521739131,
      "grad_norm": 0.06055305525660515,
      "learning_rate": 1.976470588235294e-05,
      "loss": 0.707,
      "step": 420
    },
    {
      "epoch": 6.231884057971015,
      "grad_norm": 0.06462454795837402,
      "learning_rate": 1.9443850267379677e-05,
      "loss": 0.7065,
      "step": 430
    },
    {
      "epoch": 6.3768115942028984,
      "grad_norm": 0.06829732656478882,
      "learning_rate": 1.912299465240642e-05,
      "loss": 0.7059,
      "step": 440
    },
    {
      "epoch": 6.521739130434782,
      "grad_norm": 0.07561581581830978,
      "learning_rate": 1.8802139037433155e-05,
      "loss": 0.7054,
      "step": 450
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.08198453485965729,
      "learning_rate": 1.8481283422459894e-05,
      "loss": 0.7046,
      "step": 460
    },
    {
      "epoch": 6.811594202898551,
      "grad_norm": 0.0860808789730072,
      "learning_rate": 1.8160427807486633e-05,
      "loss": 0.7039,
      "step": 470
    },
    {
      "epoch": 6.956521739130435,
      "grad_norm": 0.09343503415584564,
      "learning_rate": 1.7839572192513368e-05,
      "loss": 0.703,
      "step": 480
    },
    {
      "epoch": 7.101449275362318,
      "grad_norm": 0.10525474697351456,
      "learning_rate": 1.7518716577540107e-05,
      "loss": 0.7019,
      "step": 490
    },
    {
      "epoch": 7.246376811594203,
      "grad_norm": 0.11165975034236908,
      "learning_rate": 1.7197860962566846e-05,
      "loss": 0.7007,
      "step": 500
    },
    {
      "epoch": 7.391304347826087,
      "grad_norm": 0.11193834990262985,
      "learning_rate": 1.6877005347593585e-05,
      "loss": 0.6994,
      "step": 510
    },
    {
      "epoch": 7.536231884057971,
      "grad_norm": 0.10924576967954636,
      "learning_rate": 1.655614973262032e-05,
      "loss": 0.6981,
      "step": 520
    },
    {
      "epoch": 7.681159420289855,
      "grad_norm": 0.10660191625356674,
      "learning_rate": 1.623529411764706e-05,
      "loss": 0.6968,
      "step": 530
    },
    {
      "epoch": 7.826086956521739,
      "grad_norm": 0.10478820651769638,
      "learning_rate": 1.59144385026738e-05,
      "loss": 0.6955,
      "step": 540
    },
    {
      "epoch": 7.971014492753623,
      "grad_norm": 0.10118599236011505,
      "learning_rate": 1.5593582887700534e-05,
      "loss": 0.6942,
      "step": 550
    },
    {
      "epoch": 8.115942028985508,
      "grad_norm": 0.09696729481220245,
      "learning_rate": 1.5272727272727273e-05,
      "loss": 0.693,
      "step": 560
    },
    {
      "epoch": 8.26086956521739,
      "grad_norm": 0.09159260988235474,
      "learning_rate": 1.4951871657754012e-05,
      "loss": 0.6918,
      "step": 570
    },
    {
      "epoch": 8.405797101449275,
      "grad_norm": 0.08393704146146774,
      "learning_rate": 1.4631016042780749e-05,
      "loss": 0.6907,
      "step": 580
    },
    {
      "epoch": 8.55072463768116,
      "grad_norm": 0.07923193275928497,
      "learning_rate": 1.4310160427807486e-05,
      "loss": 0.6897,
      "step": 590
    },
    {
      "epoch": 8.695652173913043,
      "grad_norm": 0.07388373464345932,
      "learning_rate": 1.3989304812834225e-05,
      "loss": 0.6887,
      "step": 600
    },
    {
      "epoch": 8.840579710144928,
      "grad_norm": 0.07079710811376572,
      "learning_rate": 1.3668449197860964e-05,
      "loss": 0.6879,
      "step": 610
    },
    {
      "epoch": 8.985507246376812,
      "grad_norm": 0.06687342375516891,
      "learning_rate": 1.3347593582887701e-05,
      "loss": 0.6869,
      "step": 620
    },
    {
      "epoch": 9.130434782608695,
      "grad_norm": 0.06650297343730927,
      "learning_rate": 1.3026737967914438e-05,
      "loss": 0.6862,
      "step": 630
    },
    {
      "epoch": 9.27536231884058,
      "grad_norm": 0.06446882337331772,
      "learning_rate": 1.2705882352941177e-05,
      "loss": 0.6854,
      "step": 640
    },
    {
      "epoch": 9.420289855072463,
      "grad_norm": 0.06413043290376663,
      "learning_rate": 1.2385026737967915e-05,
      "loss": 0.6846,
      "step": 650
    },
    {
      "epoch": 9.565217391304348,
      "grad_norm": 0.062498513609170914,
      "learning_rate": 1.2064171122994654e-05,
      "loss": 0.6837,
      "step": 660
    },
    {
      "epoch": 9.710144927536232,
      "grad_norm": 0.06268610060214996,
      "learning_rate": 1.174331550802139e-05,
      "loss": 0.6829,
      "step": 670
    },
    {
      "epoch": 9.855072463768115,
      "grad_norm": 0.06348424404859543,
      "learning_rate": 1.1422459893048128e-05,
      "loss": 0.6823,
      "step": 680
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.06245017424225807,
      "learning_rate": 1.1101604278074867e-05,
      "loss": 0.6815,
      "step": 690
    },
    {
      "epoch": 10.144927536231885,
      "grad_norm": 0.06325192749500275,
      "learning_rate": 1.0780748663101606e-05,
      "loss": 0.6807,
      "step": 700
    },
    {
      "epoch": 10.289855072463768,
      "grad_norm": 0.06267234683036804,
      "learning_rate": 1.0459893048128341e-05,
      "loss": 0.6799,
      "step": 710
    },
    {
      "epoch": 10.434782608695652,
      "grad_norm": 0.0626390352845192,
      "learning_rate": 1.013903743315508e-05,
      "loss": 0.6793,
      "step": 720
    },
    {
      "epoch": 10.579710144927537,
      "grad_norm": 0.0642523318529129,
      "learning_rate": 9.81818181818182e-06,
      "loss": 0.6786,
      "step": 730
    },
    {
      "epoch": 10.72463768115942,
      "grad_norm": 0.06406156718730927,
      "learning_rate": 9.497326203208556e-06,
      "loss": 0.6778,
      "step": 740
    },
    {
      "epoch": 10.869565217391305,
      "grad_norm": 0.06429386138916016,
      "learning_rate": 9.176470588235295e-06,
      "loss": 0.6772,
      "step": 750
    },
    {
      "epoch": 11.014492753623188,
      "grad_norm": 0.0646902322769165,
      "learning_rate": 8.855614973262033e-06,
      "loss": 0.6766,
      "step": 760
    },
    {
      "epoch": 11.159420289855072,
      "grad_norm": 0.06738309562206268,
      "learning_rate": 8.53475935828877e-06,
      "loss": 0.676,
      "step": 770
    },
    {
      "epoch": 11.304347826086957,
      "grad_norm": 0.0651119127869606,
      "learning_rate": 8.213903743315509e-06,
      "loss": 0.6754,
      "step": 780
    },
    {
      "epoch": 11.44927536231884,
      "grad_norm": 0.06535934656858444,
      "learning_rate": 7.893048128342246e-06,
      "loss": 0.6747,
      "step": 790
    },
    {
      "epoch": 11.594202898550725,
      "grad_norm": 0.06516967713832855,
      "learning_rate": 7.572192513368983e-06,
      "loss": 0.6742,
      "step": 800
    },
    {
      "epoch": 11.73913043478261,
      "grad_norm": 0.06702619791030884,
      "learning_rate": 7.251336898395722e-06,
      "loss": 0.6737,
      "step": 810
    },
    {
      "epoch": 11.884057971014492,
      "grad_norm": 0.0671999379992485,
      "learning_rate": 6.93048128342246e-06,
      "loss": 0.6732,
      "step": 820
    },
    {
      "epoch": 12.028985507246377,
      "grad_norm": 0.06734058260917664,
      "learning_rate": 6.609625668449197e-06,
      "loss": 0.6726,
      "step": 830
    },
    {
      "epoch": 12.173913043478262,
      "grad_norm": 0.06724248826503754,
      "learning_rate": 6.288770053475936e-06,
      "loss": 0.6721,
      "step": 840
    },
    {
      "epoch": 12.318840579710145,
      "grad_norm": 0.06880488991737366,
      "learning_rate": 5.967914438502674e-06,
      "loss": 0.6716,
      "step": 850
    },
    {
      "epoch": 12.46376811594203,
      "grad_norm": 0.06892091780900955,
      "learning_rate": 5.647058823529412e-06,
      "loss": 0.6711,
      "step": 860
    },
    {
      "epoch": 12.608695652173914,
      "grad_norm": 0.07062418758869171,
      "learning_rate": 5.3262032085561505e-06,
      "loss": 0.6707,
      "step": 870
    },
    {
      "epoch": 12.753623188405797,
      "grad_norm": 0.06992269307374954,
      "learning_rate": 5.005347593582888e-06,
      "loss": 0.6701,
      "step": 880
    },
    {
      "epoch": 12.898550724637682,
      "grad_norm": 0.07018856704235077,
      "learning_rate": 4.684491978609626e-06,
      "loss": 0.6699,
      "step": 890
    },
    {
      "epoch": 13.043478260869565,
      "grad_norm": 0.07437272369861603,
      "learning_rate": 4.363636363636364e-06,
      "loss": 0.6695,
      "step": 900
    },
    {
      "epoch": 13.18840579710145,
      "grad_norm": 0.07131844758987427,
      "learning_rate": 4.042780748663102e-06,
      "loss": 0.6692,
      "step": 910
    },
    {
      "epoch": 13.333333333333334,
      "grad_norm": 0.07174045592546463,
      "learning_rate": 3.7219251336898396e-06,
      "loss": 0.6688,
      "step": 920
    },
    {
      "epoch": 13.478260869565217,
      "grad_norm": 0.07637178152799606,
      "learning_rate": 3.4010695187165777e-06,
      "loss": 0.6684,
      "step": 930
    },
    {
      "epoch": 13.623188405797102,
      "grad_norm": 0.07297246903181076,
      "learning_rate": 3.0802139037433158e-06,
      "loss": 0.6681,
      "step": 940
    },
    {
      "epoch": 13.768115942028986,
      "grad_norm": 0.07444831728935242,
      "learning_rate": 2.7593582887700534e-06,
      "loss": 0.6679,
      "step": 950
    },
    {
      "epoch": 13.91304347826087,
      "grad_norm": 0.07479577511548996,
      "learning_rate": 2.4385026737967915e-06,
      "loss": 0.6678,
      "step": 960
    },
    {
      "epoch": 14.057971014492754,
      "grad_norm": 0.07367344200611115,
      "learning_rate": 2.1176470588235296e-06,
      "loss": 0.6675,
      "step": 970
    },
    {
      "epoch": 14.202898550724637,
      "grad_norm": 0.0738438218832016,
      "learning_rate": 1.7967914438502674e-06,
      "loss": 0.6673,
      "step": 980
    },
    {
      "epoch": 14.347826086956522,
      "grad_norm": 0.07444100081920624,
      "learning_rate": 1.4759358288770053e-06,
      "loss": 0.6671,
      "step": 990
    },
    {
      "epoch": 14.492753623188406,
      "grad_norm": 0.07438851147890091,
      "learning_rate": 1.1550802139037434e-06,
      "loss": 0.6671,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1035,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 935038183014400.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
