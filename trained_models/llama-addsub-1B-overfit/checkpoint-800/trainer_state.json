{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 11.594202898550725,
  "eval_steps": 500,
  "global_step": 800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.14492753623188406,
      "grad_norm": 3.2496728897094727,
      "learning_rate": 2.7e-06,
      "loss": 4.271,
      "step": 10
    },
    {
      "epoch": 0.2898550724637681,
      "grad_norm": 3.2948763370513916,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 4.2567,
      "step": 20
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 3.3041555881500244,
      "learning_rate": 8.7e-06,
      "loss": 4.2151,
      "step": 30
    },
    {
      "epoch": 0.5797101449275363,
      "grad_norm": 3.4146151542663574,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 4.1361,
      "step": 40
    },
    {
      "epoch": 0.7246376811594203,
      "grad_norm": 3.8971798419952393,
      "learning_rate": 1.47e-05,
      "loss": 4.0107,
      "step": 50
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 4.798886299133301,
      "learning_rate": 1.77e-05,
      "loss": 3.8095,
      "step": 60
    },
    {
      "epoch": 1.0144927536231885,
      "grad_norm": 5.07028865814209,
      "learning_rate": 2.07e-05,
      "loss": 3.5178,
      "step": 70
    },
    {
      "epoch": 1.1594202898550725,
      "grad_norm": 5.126619338989258,
      "learning_rate": 2.37e-05,
      "loss": 3.1394,
      "step": 80
    },
    {
      "epoch": 1.3043478260869565,
      "grad_norm": 5.74363374710083,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.6685,
      "step": 90
    },
    {
      "epoch": 1.4492753623188406,
      "grad_norm": 5.14594841003418,
      "learning_rate": 2.97e-05,
      "loss": 2.1168,
      "step": 100
    },
    {
      "epoch": 1.5942028985507246,
      "grad_norm": 7.650655269622803,
      "learning_rate": 2.9711229946524065e-05,
      "loss": 1.6115,
      "step": 110
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 2.727729558944702,
      "learning_rate": 2.9390374331550804e-05,
      "loss": 1.2268,
      "step": 120
    },
    {
      "epoch": 1.8840579710144927,
      "grad_norm": 3.7348358631134033,
      "learning_rate": 2.906951871657754e-05,
      "loss": 0.9986,
      "step": 130
    },
    {
      "epoch": 2.028985507246377,
      "grad_norm": 0.9955636858940125,
      "learning_rate": 2.874866310160428e-05,
      "loss": 0.7912,
      "step": 140
    },
    {
      "epoch": 2.1739130434782608,
      "grad_norm": 0.20955480635166168,
      "learning_rate": 2.8427807486631018e-05,
      "loss": 0.7324,
      "step": 150
    },
    {
      "epoch": 2.318840579710145,
      "grad_norm": 0.16305001080036163,
      "learning_rate": 2.8106951871657753e-05,
      "loss": 0.7259,
      "step": 160
    },
    {
      "epoch": 2.463768115942029,
      "grad_norm": 0.14691762626171112,
      "learning_rate": 2.7786096256684492e-05,
      "loss": 0.7217,
      "step": 170
    },
    {
      "epoch": 2.608695652173913,
      "grad_norm": 0.13970685005187988,
      "learning_rate": 2.746524064171123e-05,
      "loss": 0.7185,
      "step": 180
    },
    {
      "epoch": 2.753623188405797,
      "grad_norm": 0.12930698692798615,
      "learning_rate": 2.7144385026737967e-05,
      "loss": 0.7159,
      "step": 190
    },
    {
      "epoch": 2.898550724637681,
      "grad_norm": 0.09765709936618805,
      "learning_rate": 2.682352941176471e-05,
      "loss": 0.7137,
      "step": 200
    },
    {
      "epoch": 3.0434782608695654,
      "grad_norm": 0.052028968930244446,
      "learning_rate": 2.6502673796791444e-05,
      "loss": 0.7123,
      "step": 210
    },
    {
      "epoch": 3.1884057971014492,
      "grad_norm": 0.045604512095451355,
      "learning_rate": 2.618181818181818e-05,
      "loss": 0.7118,
      "step": 220
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.04553331434726715,
      "learning_rate": 2.5860962566844922e-05,
      "loss": 0.7116,
      "step": 230
    },
    {
      "epoch": 3.4782608695652173,
      "grad_norm": 0.045249756425619125,
      "learning_rate": 2.5540106951871658e-05,
      "loss": 0.7114,
      "step": 240
    },
    {
      "epoch": 3.6231884057971016,
      "grad_norm": 0.042867209762334824,
      "learning_rate": 2.5219251336898397e-05,
      "loss": 0.7112,
      "step": 250
    },
    {
      "epoch": 3.7681159420289854,
      "grad_norm": 0.04517786577343941,
      "learning_rate": 2.4898395721925136e-05,
      "loss": 0.7109,
      "step": 260
    },
    {
      "epoch": 3.9130434782608696,
      "grad_norm": 0.044250886887311935,
      "learning_rate": 2.457754010695187e-05,
      "loss": 0.7108,
      "step": 270
    },
    {
      "epoch": 4.057971014492754,
      "grad_norm": 0.04324273020029068,
      "learning_rate": 2.425668449197861e-05,
      "loss": 0.7105,
      "step": 280
    },
    {
      "epoch": 4.202898550724638,
      "grad_norm": 0.045348647981882095,
      "learning_rate": 2.393582887700535e-05,
      "loss": 0.7103,
      "step": 290
    },
    {
      "epoch": 4.3478260869565215,
      "grad_norm": 0.045504309237003326,
      "learning_rate": 2.3614973262032085e-05,
      "loss": 0.71,
      "step": 300
    },
    {
      "epoch": 4.492753623188406,
      "grad_norm": 0.0482073649764061,
      "learning_rate": 2.3294117647058824e-05,
      "loss": 0.7098,
      "step": 310
    },
    {
      "epoch": 4.63768115942029,
      "grad_norm": 0.05130203813314438,
      "learning_rate": 2.2973262032085562e-05,
      "loss": 0.7096,
      "step": 320
    },
    {
      "epoch": 4.782608695652174,
      "grad_norm": 0.05217429995536804,
      "learning_rate": 2.26524064171123e-05,
      "loss": 0.7093,
      "step": 330
    },
    {
      "epoch": 4.927536231884058,
      "grad_norm": 0.052698902785778046,
      "learning_rate": 2.2331550802139037e-05,
      "loss": 0.7089,
      "step": 340
    },
    {
      "epoch": 5.072463768115942,
      "grad_norm": 0.05655134841799736,
      "learning_rate": 2.2010695187165776e-05,
      "loss": 0.7086,
      "step": 350
    },
    {
      "epoch": 5.217391304347826,
      "grad_norm": 0.05960889160633087,
      "learning_rate": 2.1689839572192515e-05,
      "loss": 0.7082,
      "step": 360
    },
    {
      "epoch": 5.36231884057971,
      "grad_norm": 0.06315913796424866,
      "learning_rate": 2.136898395721925e-05,
      "loss": 0.7078,
      "step": 370
    },
    {
      "epoch": 5.507246376811594,
      "grad_norm": 0.0659918263554573,
      "learning_rate": 2.1048128342245993e-05,
      "loss": 0.7074,
      "step": 380
    },
    {
      "epoch": 5.6521739130434785,
      "grad_norm": 0.07256153970956802,
      "learning_rate": 2.0727272727272728e-05,
      "loss": 0.7068,
      "step": 390
    },
    {
      "epoch": 5.797101449275362,
      "grad_norm": 0.07241956144571304,
      "learning_rate": 2.0406417112299464e-05,
      "loss": 0.7063,
      "step": 400
    },
    {
      "epoch": 5.942028985507246,
      "grad_norm": 0.08163947612047195,
      "learning_rate": 2.0085561497326206e-05,
      "loss": 0.7057,
      "step": 410
    },
    {
      "epoch": 6.086956521739131,
      "grad_norm": 0.09162082523107529,
      "learning_rate": 1.976470588235294e-05,
      "loss": 0.7049,
      "step": 420
    },
    {
      "epoch": 6.231884057971015,
      "grad_norm": 0.09964711964130402,
      "learning_rate": 1.9443850267379677e-05,
      "loss": 0.7041,
      "step": 430
    },
    {
      "epoch": 6.3768115942028984,
      "grad_norm": 0.10776206105947495,
      "learning_rate": 1.912299465240642e-05,
      "loss": 0.7031,
      "step": 440
    },
    {
      "epoch": 6.521739130434782,
      "grad_norm": 0.1139865592122078,
      "learning_rate": 1.8802139037433155e-05,
      "loss": 0.7021,
      "step": 450
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.11536630988121033,
      "learning_rate": 1.8481283422459894e-05,
      "loss": 0.7008,
      "step": 460
    },
    {
      "epoch": 6.811594202898551,
      "grad_norm": 0.11156666278839111,
      "learning_rate": 1.8160427807486633e-05,
      "loss": 0.6995,
      "step": 470
    },
    {
      "epoch": 6.956521739130435,
      "grad_norm": 0.10831759124994278,
      "learning_rate": 1.7839572192513368e-05,
      "loss": 0.6982,
      "step": 480
    },
    {
      "epoch": 7.101449275362318,
      "grad_norm": 0.10719408839941025,
      "learning_rate": 1.7518716577540107e-05,
      "loss": 0.6968,
      "step": 490
    },
    {
      "epoch": 7.246376811594203,
      "grad_norm": 0.10484587401151657,
      "learning_rate": 1.7197860962566846e-05,
      "loss": 0.6955,
      "step": 500
    },
    {
      "epoch": 7.391304347826087,
      "grad_norm": 0.09997083991765976,
      "learning_rate": 1.6877005347593585e-05,
      "loss": 0.6943,
      "step": 510
    },
    {
      "epoch": 7.536231884057971,
      "grad_norm": 0.09284677356481552,
      "learning_rate": 1.655614973262032e-05,
      "loss": 0.6931,
      "step": 520
    },
    {
      "epoch": 7.681159420289855,
      "grad_norm": 0.08642328530550003,
      "learning_rate": 1.623529411764706e-05,
      "loss": 0.692,
      "step": 530
    },
    {
      "epoch": 7.826086956521739,
      "grad_norm": 0.07903017103672028,
      "learning_rate": 1.59144385026738e-05,
      "loss": 0.6908,
      "step": 540
    },
    {
      "epoch": 7.971014492753623,
      "grad_norm": 0.07346697151660919,
      "learning_rate": 1.5593582887700534e-05,
      "loss": 0.6898,
      "step": 550
    },
    {
      "epoch": 8.115942028985508,
      "grad_norm": 0.07096702605485916,
      "learning_rate": 1.5272727272727273e-05,
      "loss": 0.6888,
      "step": 560
    },
    {
      "epoch": 8.26086956521739,
      "grad_norm": 0.06700929254293442,
      "learning_rate": 1.4951871657754012e-05,
      "loss": 0.6878,
      "step": 570
    },
    {
      "epoch": 8.405797101449275,
      "grad_norm": 0.06416608393192291,
      "learning_rate": 1.4631016042780749e-05,
      "loss": 0.6869,
      "step": 580
    },
    {
      "epoch": 8.55072463768116,
      "grad_norm": 0.06345582008361816,
      "learning_rate": 1.4310160427807486e-05,
      "loss": 0.686,
      "step": 590
    },
    {
      "epoch": 8.695652173913043,
      "grad_norm": 0.06257657706737518,
      "learning_rate": 1.3989304812834225e-05,
      "loss": 0.685,
      "step": 600
    },
    {
      "epoch": 8.840579710144928,
      "grad_norm": 0.062044501304626465,
      "learning_rate": 1.3668449197860964e-05,
      "loss": 0.6842,
      "step": 610
    },
    {
      "epoch": 8.985507246376812,
      "grad_norm": 0.061964333057403564,
      "learning_rate": 1.3347593582887701e-05,
      "loss": 0.6834,
      "step": 620
    },
    {
      "epoch": 9.130434782608695,
      "grad_norm": 0.06367821246385574,
      "learning_rate": 1.3026737967914438e-05,
      "loss": 0.6824,
      "step": 630
    },
    {
      "epoch": 9.27536231884058,
      "grad_norm": 0.06523928791284561,
      "learning_rate": 1.2705882352941177e-05,
      "loss": 0.6816,
      "step": 640
    },
    {
      "epoch": 9.420289855072463,
      "grad_norm": 0.06522349268198013,
      "learning_rate": 1.2385026737967915e-05,
      "loss": 0.6807,
      "step": 650
    },
    {
      "epoch": 9.565217391304348,
      "grad_norm": 0.06323010474443436,
      "learning_rate": 1.2064171122994654e-05,
      "loss": 0.6797,
      "step": 660
    },
    {
      "epoch": 9.710144927536232,
      "grad_norm": 0.06447306275367737,
      "learning_rate": 1.174331550802139e-05,
      "loss": 0.6789,
      "step": 670
    },
    {
      "epoch": 9.855072463768115,
      "grad_norm": 0.06390964984893799,
      "learning_rate": 1.1422459893048128e-05,
      "loss": 0.6782,
      "step": 680
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.06465063244104385,
      "learning_rate": 1.1101604278074867e-05,
      "loss": 0.6774,
      "step": 690
    },
    {
      "epoch": 10.144927536231885,
      "grad_norm": 0.06524018198251724,
      "learning_rate": 1.0780748663101606e-05,
      "loss": 0.6765,
      "step": 700
    },
    {
      "epoch": 10.289855072463768,
      "grad_norm": 0.06549602746963501,
      "learning_rate": 1.0459893048128341e-05,
      "loss": 0.6757,
      "step": 710
    },
    {
      "epoch": 10.434782608695652,
      "grad_norm": 0.06547778099775314,
      "learning_rate": 1.013903743315508e-05,
      "loss": 0.675,
      "step": 720
    },
    {
      "epoch": 10.579710144927537,
      "grad_norm": 0.06766993552446365,
      "learning_rate": 9.81818181818182e-06,
      "loss": 0.6742,
      "step": 730
    },
    {
      "epoch": 10.72463768115942,
      "grad_norm": 0.06794000416994095,
      "learning_rate": 9.497326203208556e-06,
      "loss": 0.6734,
      "step": 740
    },
    {
      "epoch": 10.869565217391305,
      "grad_norm": 0.0683973878622055,
      "learning_rate": 9.176470588235295e-06,
      "loss": 0.6727,
      "step": 750
    },
    {
      "epoch": 11.014492753623188,
      "grad_norm": 0.06897702813148499,
      "learning_rate": 8.855614973262033e-06,
      "loss": 0.6719,
      "step": 760
    },
    {
      "epoch": 11.159420289855072,
      "grad_norm": 0.07128150016069412,
      "learning_rate": 8.53475935828877e-06,
      "loss": 0.6713,
      "step": 770
    },
    {
      "epoch": 11.304347826086957,
      "grad_norm": 0.07111681997776031,
      "learning_rate": 8.213903743315509e-06,
      "loss": 0.6705,
      "step": 780
    },
    {
      "epoch": 11.44927536231884,
      "grad_norm": 0.07102375477552414,
      "learning_rate": 7.893048128342246e-06,
      "loss": 0.6698,
      "step": 790
    },
    {
      "epoch": 11.594202898550725,
      "grad_norm": 0.07117579132318497,
      "learning_rate": 7.572192513368983e-06,
      "loss": 0.6692,
      "step": 800
    }
  ],
  "logging_steps": 10,
  "max_steps": 1035,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 748030546411520.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
