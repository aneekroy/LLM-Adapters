{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9744572900327273,
  "eval_steps": 500,
  "global_step": 38400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007746083386587656,
      "grad_norm": 1.040889024734497,
      "learning_rate": 2.7e-06,
      "loss": 2.6667,
      "step": 10
    },
    {
      "epoch": 0.0015492166773175312,
      "grad_norm": 1.3457756042480469,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 2.5455,
      "step": 20
    },
    {
      "epoch": 0.002323825015976297,
      "grad_norm": 1.0335192680358887,
      "learning_rate": 8.7e-06,
      "loss": 2.502,
      "step": 30
    },
    {
      "epoch": 0.0030984333546350625,
      "grad_norm": 1.1365264654159546,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 2.7171,
      "step": 40
    },
    {
      "epoch": 0.0038730416932938284,
      "grad_norm": 1.3445944786071777,
      "learning_rate": 1.47e-05,
      "loss": 2.4072,
      "step": 50
    },
    {
      "epoch": 0.004647650031952594,
      "grad_norm": 1.5453259944915771,
      "learning_rate": 1.77e-05,
      "loss": 2.4153,
      "step": 60
    },
    {
      "epoch": 0.005422258370611359,
      "grad_norm": 1.520897388458252,
      "learning_rate": 2.07e-05,
      "loss": 2.2198,
      "step": 70
    },
    {
      "epoch": 0.006196866709270125,
      "grad_norm": 1.6597900390625,
      "learning_rate": 2.37e-05,
      "loss": 2.253,
      "step": 80
    },
    {
      "epoch": 0.006971475047928891,
      "grad_norm": 1.2860221862792969,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.0472,
      "step": 90
    },
    {
      "epoch": 0.007746083386587657,
      "grad_norm": 2.285658836364746,
      "learning_rate": 2.97e-05,
      "loss": 2.0369,
      "step": 100
    },
    {
      "epoch": 0.008520691725246422,
      "grad_norm": 2.211620330810547,
      "learning_rate": 2.9993010613512814e-05,
      "loss": 1.8682,
      "step": 110
    },
    {
      "epoch": 0.009295300063905188,
      "grad_norm": 2.611241102218628,
      "learning_rate": 2.9985244628527052e-05,
      "loss": 1.8081,
      "step": 120
    },
    {
      "epoch": 0.010069908402563953,
      "grad_norm": 1.4869638681411743,
      "learning_rate": 2.997747864354129e-05,
      "loss": 1.6998,
      "step": 130
    },
    {
      "epoch": 0.010844516741222719,
      "grad_norm": 2.8939950466156006,
      "learning_rate": 2.9969712658555524e-05,
      "loss": 1.5748,
      "step": 140
    },
    {
      "epoch": 0.011619125079881484,
      "grad_norm": 1.58890962600708,
      "learning_rate": 2.9961946673569766e-05,
      "loss": 1.5762,
      "step": 150
    },
    {
      "epoch": 0.01239373341854025,
      "grad_norm": 1.405517816543579,
      "learning_rate": 2.9954180688584003e-05,
      "loss": 1.4955,
      "step": 160
    },
    {
      "epoch": 0.013168341757199017,
      "grad_norm": 1.9227255582809448,
      "learning_rate": 2.994641470359824e-05,
      "loss": 1.5126,
      "step": 170
    },
    {
      "epoch": 0.013942950095857783,
      "grad_norm": 1.6661046743392944,
      "learning_rate": 2.993864871861248e-05,
      "loss": 1.4588,
      "step": 180
    },
    {
      "epoch": 0.014717558434516548,
      "grad_norm": 1.6522080898284912,
      "learning_rate": 2.9930882733626717e-05,
      "loss": 1.4688,
      "step": 190
    },
    {
      "epoch": 0.015492166773175314,
      "grad_norm": 1.7435343265533447,
      "learning_rate": 2.9923116748640955e-05,
      "loss": 1.3969,
      "step": 200
    },
    {
      "epoch": 0.016266775111834077,
      "grad_norm": 1.6582820415496826,
      "learning_rate": 2.9915350763655193e-05,
      "loss": 1.4487,
      "step": 210
    },
    {
      "epoch": 0.017041383450492845,
      "grad_norm": 1.2961540222167969,
      "learning_rate": 2.9907584778669427e-05,
      "loss": 1.5144,
      "step": 220
    },
    {
      "epoch": 0.017815991789151612,
      "grad_norm": 3.294999122619629,
      "learning_rate": 2.9899818793683665e-05,
      "loss": 1.483,
      "step": 230
    },
    {
      "epoch": 0.018590600127810376,
      "grad_norm": 1.472785472869873,
      "learning_rate": 2.9892052808697903e-05,
      "loss": 1.4584,
      "step": 240
    },
    {
      "epoch": 0.019365208466469143,
      "grad_norm": 1.5442689657211304,
      "learning_rate": 2.988428682371214e-05,
      "loss": 1.4249,
      "step": 250
    },
    {
      "epoch": 0.020139816805127907,
      "grad_norm": 1.9941391944885254,
      "learning_rate": 2.987652083872638e-05,
      "loss": 1.3978,
      "step": 260
    },
    {
      "epoch": 0.020914425143786674,
      "grad_norm": 1.9761314392089844,
      "learning_rate": 2.9868754853740617e-05,
      "loss": 1.5495,
      "step": 270
    },
    {
      "epoch": 0.021689033482445438,
      "grad_norm": 1.918114423751831,
      "learning_rate": 2.9860988868754854e-05,
      "loss": 1.4705,
      "step": 280
    },
    {
      "epoch": 0.022463641821104205,
      "grad_norm": 1.068616509437561,
      "learning_rate": 2.9853222883769092e-05,
      "loss": 1.4713,
      "step": 290
    },
    {
      "epoch": 0.02323825015976297,
      "grad_norm": 1.5563123226165771,
      "learning_rate": 2.984545689878333e-05,
      "loss": 1.422,
      "step": 300
    },
    {
      "epoch": 0.024012858498421736,
      "grad_norm": 1.1541975736618042,
      "learning_rate": 2.9837690913797565e-05,
      "loss": 1.3497,
      "step": 310
    },
    {
      "epoch": 0.0247874668370805,
      "grad_norm": 1.336923360824585,
      "learning_rate": 2.9829924928811803e-05,
      "loss": 1.4376,
      "step": 320
    },
    {
      "epoch": 0.025562075175739267,
      "grad_norm": 1.6735671758651733,
      "learning_rate": 2.982215894382604e-05,
      "loss": 1.5412,
      "step": 330
    },
    {
      "epoch": 0.026336683514398034,
      "grad_norm": 1.4838542938232422,
      "learning_rate": 2.9814392958840282e-05,
      "loss": 1.317,
      "step": 340
    },
    {
      "epoch": 0.027111291853056798,
      "grad_norm": 1.5209298133850098,
      "learning_rate": 2.980662697385452e-05,
      "loss": 1.4669,
      "step": 350
    },
    {
      "epoch": 0.027885900191715565,
      "grad_norm": 1.4961273670196533,
      "learning_rate": 2.9798860988868757e-05,
      "loss": 1.409,
      "step": 360
    },
    {
      "epoch": 0.02866050853037433,
      "grad_norm": 2.889659881591797,
      "learning_rate": 2.9791095003882995e-05,
      "loss": 1.4719,
      "step": 370
    },
    {
      "epoch": 0.029435116869033096,
      "grad_norm": 2.6725523471832275,
      "learning_rate": 2.9783329018897233e-05,
      "loss": 1.4942,
      "step": 380
    },
    {
      "epoch": 0.03020972520769186,
      "grad_norm": 1.3964866399765015,
      "learning_rate": 2.9775563033911468e-05,
      "loss": 1.4728,
      "step": 390
    },
    {
      "epoch": 0.030984333546350627,
      "grad_norm": 1.543731927871704,
      "learning_rate": 2.9767797048925706e-05,
      "loss": 1.4436,
      "step": 400
    },
    {
      "epoch": 0.03175894188500939,
      "grad_norm": 1.4189386367797852,
      "learning_rate": 2.9760031063939943e-05,
      "loss": 1.3895,
      "step": 410
    },
    {
      "epoch": 0.032533550223668155,
      "grad_norm": 1.634498953819275,
      "learning_rate": 2.975226507895418e-05,
      "loss": 1.3431,
      "step": 420
    },
    {
      "epoch": 0.033308158562326926,
      "grad_norm": 2.119978427886963,
      "learning_rate": 2.974449909396842e-05,
      "loss": 1.4257,
      "step": 430
    },
    {
      "epoch": 0.03408276690098569,
      "grad_norm": 1.6885108947753906,
      "learning_rate": 2.9736733108982657e-05,
      "loss": 1.4397,
      "step": 440
    },
    {
      "epoch": 0.03485737523964445,
      "grad_norm": 1.014920711517334,
      "learning_rate": 2.9728967123996895e-05,
      "loss": 1.4674,
      "step": 450
    },
    {
      "epoch": 0.035631983578303224,
      "grad_norm": 1.3299297094345093,
      "learning_rate": 2.9721201139011133e-05,
      "loss": 1.4421,
      "step": 460
    },
    {
      "epoch": 0.03640659191696199,
      "grad_norm": 1.6062976121902466,
      "learning_rate": 2.971343515402537e-05,
      "loss": 1.3813,
      "step": 470
    },
    {
      "epoch": 0.03718120025562075,
      "grad_norm": 1.2981724739074707,
      "learning_rate": 2.9705669169039605e-05,
      "loss": 1.5102,
      "step": 480
    },
    {
      "epoch": 0.037955808594279515,
      "grad_norm": 1.2808231115341187,
      "learning_rate": 2.9697903184053843e-05,
      "loss": 1.3455,
      "step": 490
    },
    {
      "epoch": 0.038730416932938286,
      "grad_norm": 2.190398931503296,
      "learning_rate": 2.969013719906808e-05,
      "loss": 1.4525,
      "step": 500
    },
    {
      "epoch": 0.03950502527159705,
      "grad_norm": 1.4548110961914062,
      "learning_rate": 2.968237121408232e-05,
      "loss": 1.3345,
      "step": 510
    },
    {
      "epoch": 0.04027963361025581,
      "grad_norm": 1.4209299087524414,
      "learning_rate": 2.9674605229096557e-05,
      "loss": 1.3377,
      "step": 520
    },
    {
      "epoch": 0.04105424194891458,
      "grad_norm": 1.7820106744766235,
      "learning_rate": 2.9666839244110794e-05,
      "loss": 1.4824,
      "step": 530
    },
    {
      "epoch": 0.04182885028757335,
      "grad_norm": 1.17951500415802,
      "learning_rate": 2.9659073259125036e-05,
      "loss": 1.4148,
      "step": 540
    },
    {
      "epoch": 0.04260345862623211,
      "grad_norm": 1.6635894775390625,
      "learning_rate": 2.9651307274139273e-05,
      "loss": 1.3373,
      "step": 550
    },
    {
      "epoch": 0.043378066964890875,
      "grad_norm": 2.246767282485962,
      "learning_rate": 2.9643541289153508e-05,
      "loss": 1.4454,
      "step": 560
    },
    {
      "epoch": 0.044152675303549646,
      "grad_norm": 1.6649283170700073,
      "learning_rate": 2.9635775304167746e-05,
      "loss": 1.4164,
      "step": 570
    },
    {
      "epoch": 0.04492728364220841,
      "grad_norm": 1.5304629802703857,
      "learning_rate": 2.9628009319181984e-05,
      "loss": 1.3898,
      "step": 580
    },
    {
      "epoch": 0.045701891980867174,
      "grad_norm": 1.1954220533370972,
      "learning_rate": 2.962024333419622e-05,
      "loss": 1.4108,
      "step": 590
    },
    {
      "epoch": 0.04647650031952594,
      "grad_norm": 1.7157237529754639,
      "learning_rate": 2.961247734921046e-05,
      "loss": 1.3807,
      "step": 600
    },
    {
      "epoch": 0.04725110865818471,
      "grad_norm": 1.4570473432540894,
      "learning_rate": 2.9604711364224697e-05,
      "loss": 1.3808,
      "step": 610
    },
    {
      "epoch": 0.04802571699684347,
      "grad_norm": 1.7467273473739624,
      "learning_rate": 2.9596945379238935e-05,
      "loss": 1.3535,
      "step": 620
    },
    {
      "epoch": 0.048800325335502236,
      "grad_norm": 1.5915828943252563,
      "learning_rate": 2.9589179394253173e-05,
      "loss": 1.497,
      "step": 630
    },
    {
      "epoch": 0.049574933674161,
      "grad_norm": 2.6713080406188965,
      "learning_rate": 2.958141340926741e-05,
      "loss": 1.299,
      "step": 640
    },
    {
      "epoch": 0.05034954201281977,
      "grad_norm": 1.8472063541412354,
      "learning_rate": 2.9573647424281645e-05,
      "loss": 1.4531,
      "step": 650
    },
    {
      "epoch": 0.051124150351478534,
      "grad_norm": 1.4954941272735596,
      "learning_rate": 2.9565881439295883e-05,
      "loss": 1.516,
      "step": 660
    },
    {
      "epoch": 0.0518987586901373,
      "grad_norm": 2.2380809783935547,
      "learning_rate": 2.955811545431012e-05,
      "loss": 1.4239,
      "step": 670
    },
    {
      "epoch": 0.05267336702879607,
      "grad_norm": 1.575984239578247,
      "learning_rate": 2.955034946932436e-05,
      "loss": 1.3862,
      "step": 680
    },
    {
      "epoch": 0.05344797536745483,
      "grad_norm": 1.8305975198745728,
      "learning_rate": 2.9542583484338597e-05,
      "loss": 1.4012,
      "step": 690
    },
    {
      "epoch": 0.054222583706113596,
      "grad_norm": 2.019002914428711,
      "learning_rate": 2.9534817499352835e-05,
      "loss": 1.4539,
      "step": 700
    },
    {
      "epoch": 0.05499719204477236,
      "grad_norm": 2.388767957687378,
      "learning_rate": 2.9527051514367073e-05,
      "loss": 1.2865,
      "step": 710
    },
    {
      "epoch": 0.05577180038343113,
      "grad_norm": 1.6992499828338623,
      "learning_rate": 2.951928552938131e-05,
      "loss": 1.4762,
      "step": 720
    },
    {
      "epoch": 0.056546408722089894,
      "grad_norm": 1.849805474281311,
      "learning_rate": 2.951151954439555e-05,
      "loss": 1.3595,
      "step": 730
    },
    {
      "epoch": 0.05732101706074866,
      "grad_norm": 1.4159356355667114,
      "learning_rate": 2.9503753559409786e-05,
      "loss": 1.432,
      "step": 740
    },
    {
      "epoch": 0.05809562539940742,
      "grad_norm": 1.8417673110961914,
      "learning_rate": 2.9495987574424024e-05,
      "loss": 1.4212,
      "step": 750
    },
    {
      "epoch": 0.05887023373806619,
      "grad_norm": 1.3747239112854004,
      "learning_rate": 2.9488221589438262e-05,
      "loss": 1.534,
      "step": 760
    },
    {
      "epoch": 0.059644842076724956,
      "grad_norm": 1.6210901737213135,
      "learning_rate": 2.94804556044525e-05,
      "loss": 1.4966,
      "step": 770
    },
    {
      "epoch": 0.06041945041538372,
      "grad_norm": 1.186773419380188,
      "learning_rate": 2.9472689619466738e-05,
      "loss": 1.5193,
      "step": 780
    },
    {
      "epoch": 0.06119405875404249,
      "grad_norm": 1.2022613286972046,
      "learning_rate": 2.9464923634480976e-05,
      "loss": 1.4461,
      "step": 790
    },
    {
      "epoch": 0.061968667092701255,
      "grad_norm": 1.4385637044906616,
      "learning_rate": 2.9457157649495213e-05,
      "loss": 1.426,
      "step": 800
    },
    {
      "epoch": 0.06274327543136002,
      "grad_norm": 2.7766804695129395,
      "learning_rate": 2.944939166450945e-05,
      "loss": 1.4366,
      "step": 810
    },
    {
      "epoch": 0.06351788377001878,
      "grad_norm": 1.4219303131103516,
      "learning_rate": 2.9441625679523686e-05,
      "loss": 1.3461,
      "step": 820
    },
    {
      "epoch": 0.06429249210867755,
      "grad_norm": 1.5983226299285889,
      "learning_rate": 2.9433859694537924e-05,
      "loss": 1.4164,
      "step": 830
    },
    {
      "epoch": 0.06506710044733631,
      "grad_norm": 2.428565740585327,
      "learning_rate": 2.942609370955216e-05,
      "loss": 1.4108,
      "step": 840
    },
    {
      "epoch": 0.06584170878599509,
      "grad_norm": 1.2099117040634155,
      "learning_rate": 2.94183277245664e-05,
      "loss": 1.3947,
      "step": 850
    },
    {
      "epoch": 0.06661631712465385,
      "grad_norm": 1.9666874408721924,
      "learning_rate": 2.9410561739580637e-05,
      "loss": 1.4623,
      "step": 860
    },
    {
      "epoch": 0.06739092546331261,
      "grad_norm": 1.3467434644699097,
      "learning_rate": 2.9402795754594875e-05,
      "loss": 1.3474,
      "step": 870
    },
    {
      "epoch": 0.06816553380197138,
      "grad_norm": 1.7946162223815918,
      "learning_rate": 2.9395029769609113e-05,
      "loss": 1.4118,
      "step": 880
    },
    {
      "epoch": 0.06894014214063014,
      "grad_norm": 1.7518665790557861,
      "learning_rate": 2.938726378462335e-05,
      "loss": 1.4186,
      "step": 890
    },
    {
      "epoch": 0.0697147504792889,
      "grad_norm": 1.4787054061889648,
      "learning_rate": 2.9379497799637585e-05,
      "loss": 1.5251,
      "step": 900
    },
    {
      "epoch": 0.07048935881794767,
      "grad_norm": 2.268592357635498,
      "learning_rate": 2.9371731814651823e-05,
      "loss": 1.4081,
      "step": 910
    },
    {
      "epoch": 0.07126396715660645,
      "grad_norm": 1.8688170909881592,
      "learning_rate": 2.936396582966606e-05,
      "loss": 1.4017,
      "step": 920
    },
    {
      "epoch": 0.07203857549526521,
      "grad_norm": 1.5364253520965576,
      "learning_rate": 2.9356199844680302e-05,
      "loss": 1.4206,
      "step": 930
    },
    {
      "epoch": 0.07281318383392398,
      "grad_norm": 1.9903485774993896,
      "learning_rate": 2.934843385969454e-05,
      "loss": 1.4162,
      "step": 940
    },
    {
      "epoch": 0.07358779217258274,
      "grad_norm": 1.7321147918701172,
      "learning_rate": 2.9340667874708778e-05,
      "loss": 1.4833,
      "step": 950
    },
    {
      "epoch": 0.0743624005112415,
      "grad_norm": 1.999578595161438,
      "learning_rate": 2.9332901889723016e-05,
      "loss": 1.3242,
      "step": 960
    },
    {
      "epoch": 0.07513700884990027,
      "grad_norm": 2.066999673843384,
      "learning_rate": 2.9325135904737254e-05,
      "loss": 1.4109,
      "step": 970
    },
    {
      "epoch": 0.07591161718855903,
      "grad_norm": 2.2203314304351807,
      "learning_rate": 2.931736991975149e-05,
      "loss": 1.3997,
      "step": 980
    },
    {
      "epoch": 0.0766862255272178,
      "grad_norm": 1.5480259656906128,
      "learning_rate": 2.9309603934765726e-05,
      "loss": 1.3945,
      "step": 990
    },
    {
      "epoch": 0.07746083386587657,
      "grad_norm": 1.8211700916290283,
      "learning_rate": 2.9301837949779964e-05,
      "loss": 1.3833,
      "step": 1000
    },
    {
      "epoch": 0.07823544220453534,
      "grad_norm": 1.447389841079712,
      "learning_rate": 2.9294071964794202e-05,
      "loss": 1.4028,
      "step": 1010
    },
    {
      "epoch": 0.0790100505431941,
      "grad_norm": 2.220945119857788,
      "learning_rate": 2.928630597980844e-05,
      "loss": 1.3197,
      "step": 1020
    },
    {
      "epoch": 0.07978465888185286,
      "grad_norm": 2.01949143409729,
      "learning_rate": 2.9278539994822678e-05,
      "loss": 1.3473,
      "step": 1030
    },
    {
      "epoch": 0.08055926722051163,
      "grad_norm": 1.3135337829589844,
      "learning_rate": 2.9270774009836915e-05,
      "loss": 1.4561,
      "step": 1040
    },
    {
      "epoch": 0.08133387555917039,
      "grad_norm": 2.265777111053467,
      "learning_rate": 2.9263008024851153e-05,
      "loss": 1.3598,
      "step": 1050
    },
    {
      "epoch": 0.08210848389782915,
      "grad_norm": 1.6634564399719238,
      "learning_rate": 2.925524203986539e-05,
      "loss": 1.3067,
      "step": 1060
    },
    {
      "epoch": 0.08288309223648793,
      "grad_norm": 1.5596888065338135,
      "learning_rate": 2.9247476054879626e-05,
      "loss": 1.3915,
      "step": 1070
    },
    {
      "epoch": 0.0836577005751467,
      "grad_norm": 1.9059189558029175,
      "learning_rate": 2.9239710069893863e-05,
      "loss": 1.3005,
      "step": 1080
    },
    {
      "epoch": 0.08443230891380546,
      "grad_norm": 2.3058176040649414,
      "learning_rate": 2.92319440849081e-05,
      "loss": 1.3812,
      "step": 1090
    },
    {
      "epoch": 0.08520691725246422,
      "grad_norm": 1.3925081491470337,
      "learning_rate": 2.922417809992234e-05,
      "loss": 1.3136,
      "step": 1100
    },
    {
      "epoch": 0.08598152559112299,
      "grad_norm": 2.7321064472198486,
      "learning_rate": 2.9216412114936577e-05,
      "loss": 1.3314,
      "step": 1110
    },
    {
      "epoch": 0.08675613392978175,
      "grad_norm": 1.3491090536117554,
      "learning_rate": 2.9208646129950815e-05,
      "loss": 1.4245,
      "step": 1120
    },
    {
      "epoch": 0.08753074226844051,
      "grad_norm": 1.3061792850494385,
      "learning_rate": 2.9200880144965056e-05,
      "loss": 1.4473,
      "step": 1130
    },
    {
      "epoch": 0.08830535060709929,
      "grad_norm": 2.40509033203125,
      "learning_rate": 2.9193114159979294e-05,
      "loss": 1.2843,
      "step": 1140
    },
    {
      "epoch": 0.08907995894575806,
      "grad_norm": 2.290400505065918,
      "learning_rate": 2.9185348174993532e-05,
      "loss": 1.4745,
      "step": 1150
    },
    {
      "epoch": 0.08985456728441682,
      "grad_norm": 1.402477741241455,
      "learning_rate": 2.9177582190007766e-05,
      "loss": 1.3274,
      "step": 1160
    },
    {
      "epoch": 0.09062917562307558,
      "grad_norm": 1.415452003479004,
      "learning_rate": 2.9169816205022004e-05,
      "loss": 1.5333,
      "step": 1170
    },
    {
      "epoch": 0.09140378396173435,
      "grad_norm": 1.3495697975158691,
      "learning_rate": 2.9162050220036242e-05,
      "loss": 1.3531,
      "step": 1180
    },
    {
      "epoch": 0.09217839230039311,
      "grad_norm": 1.8287314176559448,
      "learning_rate": 2.915428423505048e-05,
      "loss": 1.3996,
      "step": 1190
    },
    {
      "epoch": 0.09295300063905187,
      "grad_norm": 3.054572582244873,
      "learning_rate": 2.9146518250064718e-05,
      "loss": 1.4327,
      "step": 1200
    },
    {
      "epoch": 0.09372760897771064,
      "grad_norm": 1.5178815126419067,
      "learning_rate": 2.9138752265078956e-05,
      "loss": 1.4289,
      "step": 1210
    },
    {
      "epoch": 0.09450221731636942,
      "grad_norm": 1.629580020904541,
      "learning_rate": 2.9130986280093194e-05,
      "loss": 1.3278,
      "step": 1220
    },
    {
      "epoch": 0.09527682565502818,
      "grad_norm": 1.6124743223190308,
      "learning_rate": 2.912322029510743e-05,
      "loss": 1.3957,
      "step": 1230
    },
    {
      "epoch": 0.09605143399368694,
      "grad_norm": 2.0729739665985107,
      "learning_rate": 2.9115454310121666e-05,
      "loss": 1.4068,
      "step": 1240
    },
    {
      "epoch": 0.09682604233234571,
      "grad_norm": 2.0443761348724365,
      "learning_rate": 2.9107688325135904e-05,
      "loss": 1.3895,
      "step": 1250
    },
    {
      "epoch": 0.09760065067100447,
      "grad_norm": 1.84819495677948,
      "learning_rate": 2.909992234015014e-05,
      "loss": 1.421,
      "step": 1260
    },
    {
      "epoch": 0.09837525900966324,
      "grad_norm": 1.833727478981018,
      "learning_rate": 2.909215635516438e-05,
      "loss": 1.4643,
      "step": 1270
    },
    {
      "epoch": 0.099149867348322,
      "grad_norm": 3.267225503921509,
      "learning_rate": 2.9084390370178617e-05,
      "loss": 1.3607,
      "step": 1280
    },
    {
      "epoch": 0.09992447568698078,
      "grad_norm": 1.9213578701019287,
      "learning_rate": 2.9076624385192855e-05,
      "loss": 1.3018,
      "step": 1290
    },
    {
      "epoch": 0.10069908402563954,
      "grad_norm": 2.07706880569458,
      "learning_rate": 2.9068858400207093e-05,
      "loss": 1.4464,
      "step": 1300
    },
    {
      "epoch": 0.1014736923642983,
      "grad_norm": 1.6257599592208862,
      "learning_rate": 2.906109241522133e-05,
      "loss": 1.4819,
      "step": 1310
    },
    {
      "epoch": 0.10224830070295707,
      "grad_norm": 2.4557833671569824,
      "learning_rate": 2.9053326430235572e-05,
      "loss": 1.3929,
      "step": 1320
    },
    {
      "epoch": 0.10302290904161583,
      "grad_norm": 2.326899766921997,
      "learning_rate": 2.9045560445249807e-05,
      "loss": 1.4801,
      "step": 1330
    },
    {
      "epoch": 0.1037975173802746,
      "grad_norm": 2.1127912998199463,
      "learning_rate": 2.9037794460264045e-05,
      "loss": 1.3941,
      "step": 1340
    },
    {
      "epoch": 0.10457212571893336,
      "grad_norm": 2.0080463886260986,
      "learning_rate": 2.9030028475278282e-05,
      "loss": 1.3614,
      "step": 1350
    },
    {
      "epoch": 0.10534673405759214,
      "grad_norm": 1.6381359100341797,
      "learning_rate": 2.902226249029252e-05,
      "loss": 1.3471,
      "step": 1360
    },
    {
      "epoch": 0.1061213423962509,
      "grad_norm": 1.8331385850906372,
      "learning_rate": 2.9014496505306758e-05,
      "loss": 1.4234,
      "step": 1370
    },
    {
      "epoch": 0.10689595073490966,
      "grad_norm": 2.522474527359009,
      "learning_rate": 2.9006730520320996e-05,
      "loss": 1.3818,
      "step": 1380
    },
    {
      "epoch": 0.10767055907356843,
      "grad_norm": 2.0433173179626465,
      "learning_rate": 2.8998964535335234e-05,
      "loss": 1.325,
      "step": 1390
    },
    {
      "epoch": 0.10844516741222719,
      "grad_norm": 2.1385116577148438,
      "learning_rate": 2.8991198550349472e-05,
      "loss": 1.4506,
      "step": 1400
    },
    {
      "epoch": 0.10921977575088596,
      "grad_norm": 1.6381635665893555,
      "learning_rate": 2.8983432565363706e-05,
      "loss": 1.4214,
      "step": 1410
    },
    {
      "epoch": 0.10999438408954472,
      "grad_norm": 1.9883382320404053,
      "learning_rate": 2.8975666580377944e-05,
      "loss": 1.3681,
      "step": 1420
    },
    {
      "epoch": 0.11076899242820348,
      "grad_norm": 1.65022611618042,
      "learning_rate": 2.8967900595392182e-05,
      "loss": 1.3896,
      "step": 1430
    },
    {
      "epoch": 0.11154360076686226,
      "grad_norm": 2.017504930496216,
      "learning_rate": 2.896013461040642e-05,
      "loss": 1.3527,
      "step": 1440
    },
    {
      "epoch": 0.11231820910552102,
      "grad_norm": 2.015378713607788,
      "learning_rate": 2.8952368625420658e-05,
      "loss": 1.4276,
      "step": 1450
    },
    {
      "epoch": 0.11309281744417979,
      "grad_norm": 2.4549026489257812,
      "learning_rate": 2.8944602640434896e-05,
      "loss": 1.3809,
      "step": 1460
    },
    {
      "epoch": 0.11386742578283855,
      "grad_norm": 1.8473886251449585,
      "learning_rate": 2.8936836655449133e-05,
      "loss": 1.4164,
      "step": 1470
    },
    {
      "epoch": 0.11464203412149732,
      "grad_norm": 1.5916579961776733,
      "learning_rate": 2.892907067046337e-05,
      "loss": 1.4264,
      "step": 1480
    },
    {
      "epoch": 0.11541664246015608,
      "grad_norm": 1.675386667251587,
      "learning_rate": 2.892130468547761e-05,
      "loss": 1.3713,
      "step": 1490
    },
    {
      "epoch": 0.11619125079881484,
      "grad_norm": 1.6789922714233398,
      "learning_rate": 2.8913538700491844e-05,
      "loss": 1.3606,
      "step": 1500
    },
    {
      "epoch": 0.11696585913747362,
      "grad_norm": 1.9848545789718628,
      "learning_rate": 2.890577271550608e-05,
      "loss": 1.3231,
      "step": 1510
    },
    {
      "epoch": 0.11774046747613238,
      "grad_norm": 3.1034069061279297,
      "learning_rate": 2.8898006730520323e-05,
      "loss": 1.4118,
      "step": 1520
    },
    {
      "epoch": 0.11851507581479115,
      "grad_norm": 2.3465304374694824,
      "learning_rate": 2.889024074553456e-05,
      "loss": 1.4743,
      "step": 1530
    },
    {
      "epoch": 0.11928968415344991,
      "grad_norm": 2.0259060859680176,
      "learning_rate": 2.88824747605488e-05,
      "loss": 1.2469,
      "step": 1540
    },
    {
      "epoch": 0.12006429249210868,
      "grad_norm": 2.467542886734009,
      "learning_rate": 2.8874708775563036e-05,
      "loss": 1.4311,
      "step": 1550
    },
    {
      "epoch": 0.12083890083076744,
      "grad_norm": 2.737661361694336,
      "learning_rate": 2.8866942790577274e-05,
      "loss": 1.3567,
      "step": 1560
    },
    {
      "epoch": 0.1216135091694262,
      "grad_norm": 2.3866145610809326,
      "learning_rate": 2.8859176805591512e-05,
      "loss": 1.3777,
      "step": 1570
    },
    {
      "epoch": 0.12238811750808498,
      "grad_norm": 2.0391862392425537,
      "learning_rate": 2.8851410820605747e-05,
      "loss": 1.4475,
      "step": 1580
    },
    {
      "epoch": 0.12316272584674375,
      "grad_norm": 2.087245464324951,
      "learning_rate": 2.8843644835619984e-05,
      "loss": 1.4478,
      "step": 1590
    },
    {
      "epoch": 0.12393733418540251,
      "grad_norm": 1.5587608814239502,
      "learning_rate": 2.8835878850634222e-05,
      "loss": 1.4287,
      "step": 1600
    },
    {
      "epoch": 0.12471194252406127,
      "grad_norm": 2.4665958881378174,
      "learning_rate": 2.882811286564846e-05,
      "loss": 1.4444,
      "step": 1610
    },
    {
      "epoch": 0.12548655086272004,
      "grad_norm": 1.7200076580047607,
      "learning_rate": 2.8820346880662698e-05,
      "loss": 1.3344,
      "step": 1620
    },
    {
      "epoch": 0.12626115920137881,
      "grad_norm": 1.8239622116088867,
      "learning_rate": 2.8812580895676936e-05,
      "loss": 1.4613,
      "step": 1630
    },
    {
      "epoch": 0.12703576754003756,
      "grad_norm": 2.0373876094818115,
      "learning_rate": 2.8804814910691174e-05,
      "loss": 1.3721,
      "step": 1640
    },
    {
      "epoch": 0.12781037587869634,
      "grad_norm": 1.7194746732711792,
      "learning_rate": 2.879704892570541e-05,
      "loss": 1.4523,
      "step": 1650
    },
    {
      "epoch": 0.1285849842173551,
      "grad_norm": 1.731570839881897,
      "learning_rate": 2.878928294071965e-05,
      "loss": 1.378,
      "step": 1660
    },
    {
      "epoch": 0.12935959255601387,
      "grad_norm": 2.2058751583099365,
      "learning_rate": 2.8781516955733884e-05,
      "loss": 1.3972,
      "step": 1670
    },
    {
      "epoch": 0.13013420089467262,
      "grad_norm": 2.1048927307128906,
      "learning_rate": 2.8773750970748122e-05,
      "loss": 1.4136,
      "step": 1680
    },
    {
      "epoch": 0.1309088092333314,
      "grad_norm": 2.3421926498413086,
      "learning_rate": 2.876598498576236e-05,
      "loss": 1.3793,
      "step": 1690
    },
    {
      "epoch": 0.13168341757199017,
      "grad_norm": 1.8206202983856201,
      "learning_rate": 2.8758219000776598e-05,
      "loss": 1.3891,
      "step": 1700
    },
    {
      "epoch": 0.13245802591064892,
      "grad_norm": 1.606435775756836,
      "learning_rate": 2.875045301579084e-05,
      "loss": 1.417,
      "step": 1710
    },
    {
      "epoch": 0.1332326342493077,
      "grad_norm": 2.0943727493286133,
      "learning_rate": 2.8742687030805077e-05,
      "loss": 1.3375,
      "step": 1720
    },
    {
      "epoch": 0.13400724258796645,
      "grad_norm": 1.888784646987915,
      "learning_rate": 2.8734921045819315e-05,
      "loss": 1.3632,
      "step": 1730
    },
    {
      "epoch": 0.13478185092662523,
      "grad_norm": 2.193333387374878,
      "learning_rate": 2.8727155060833552e-05,
      "loss": 1.4268,
      "step": 1740
    },
    {
      "epoch": 0.13555645926528398,
      "grad_norm": 2.2371792793273926,
      "learning_rate": 2.8719389075847787e-05,
      "loss": 1.3912,
      "step": 1750
    },
    {
      "epoch": 0.13633106760394276,
      "grad_norm": 2.483868360519409,
      "learning_rate": 2.8711623090862025e-05,
      "loss": 1.3721,
      "step": 1760
    },
    {
      "epoch": 0.13710567594260153,
      "grad_norm": 2.5617198944091797,
      "learning_rate": 2.8703857105876263e-05,
      "loss": 1.3082,
      "step": 1770
    },
    {
      "epoch": 0.13788028428126028,
      "grad_norm": 2.5413262844085693,
      "learning_rate": 2.86960911208905e-05,
      "loss": 1.2778,
      "step": 1780
    },
    {
      "epoch": 0.13865489261991906,
      "grad_norm": 2.409559726715088,
      "learning_rate": 2.868832513590474e-05,
      "loss": 1.339,
      "step": 1790
    },
    {
      "epoch": 0.1394295009585778,
      "grad_norm": 1.7785494327545166,
      "learning_rate": 2.8680559150918976e-05,
      "loss": 1.3598,
      "step": 1800
    },
    {
      "epoch": 0.1402041092972366,
      "grad_norm": 2.294790744781494,
      "learning_rate": 2.8672793165933214e-05,
      "loss": 1.3525,
      "step": 1810
    },
    {
      "epoch": 0.14097871763589534,
      "grad_norm": 1.601219892501831,
      "learning_rate": 2.8665027180947452e-05,
      "loss": 1.3848,
      "step": 1820
    },
    {
      "epoch": 0.14175332597455412,
      "grad_norm": 1.3207563161849976,
      "learning_rate": 2.8657261195961686e-05,
      "loss": 1.3491,
      "step": 1830
    },
    {
      "epoch": 0.1425279343132129,
      "grad_norm": 1.2960774898529053,
      "learning_rate": 2.8649495210975924e-05,
      "loss": 1.3701,
      "step": 1840
    },
    {
      "epoch": 0.14330254265187164,
      "grad_norm": 2.1977338790893555,
      "learning_rate": 2.8641729225990162e-05,
      "loss": 1.3629,
      "step": 1850
    },
    {
      "epoch": 0.14407715099053042,
      "grad_norm": 1.6956312656402588,
      "learning_rate": 2.86339632410044e-05,
      "loss": 1.3484,
      "step": 1860
    },
    {
      "epoch": 0.14485175932918917,
      "grad_norm": 1.8790004253387451,
      "learning_rate": 2.8626197256018638e-05,
      "loss": 1.3377,
      "step": 1870
    },
    {
      "epoch": 0.14562636766784795,
      "grad_norm": 1.783467411994934,
      "learning_rate": 2.8618431271032876e-05,
      "loss": 1.4403,
      "step": 1880
    },
    {
      "epoch": 0.1464009760065067,
      "grad_norm": 1.6170432567596436,
      "learning_rate": 2.8610665286047114e-05,
      "loss": 1.4759,
      "step": 1890
    },
    {
      "epoch": 0.14717558434516548,
      "grad_norm": 1.3606630563735962,
      "learning_rate": 2.860289930106135e-05,
      "loss": 1.4669,
      "step": 1900
    },
    {
      "epoch": 0.14795019268382423,
      "grad_norm": 1.6198736429214478,
      "learning_rate": 2.8595133316075593e-05,
      "loss": 1.3975,
      "step": 1910
    },
    {
      "epoch": 0.148724801022483,
      "grad_norm": 1.840481162071228,
      "learning_rate": 2.8587367331089827e-05,
      "loss": 1.4071,
      "step": 1920
    },
    {
      "epoch": 0.14949940936114178,
      "grad_norm": 2.81181001663208,
      "learning_rate": 2.8579601346104065e-05,
      "loss": 1.4079,
      "step": 1930
    },
    {
      "epoch": 0.15027401769980053,
      "grad_norm": 1.7882577180862427,
      "learning_rate": 2.8571835361118303e-05,
      "loss": 1.4434,
      "step": 1940
    },
    {
      "epoch": 0.1510486260384593,
      "grad_norm": 2.448936939239502,
      "learning_rate": 2.856406937613254e-05,
      "loss": 1.3341,
      "step": 1950
    },
    {
      "epoch": 0.15182323437711806,
      "grad_norm": 1.6352202892303467,
      "learning_rate": 2.855630339114678e-05,
      "loss": 1.4225,
      "step": 1960
    },
    {
      "epoch": 0.15259784271577684,
      "grad_norm": 2.660518169403076,
      "learning_rate": 2.8548537406161017e-05,
      "loss": 1.4181,
      "step": 1970
    },
    {
      "epoch": 0.1533724510544356,
      "grad_norm": 1.9798431396484375,
      "learning_rate": 2.8540771421175254e-05,
      "loss": 1.3815,
      "step": 1980
    },
    {
      "epoch": 0.15414705939309437,
      "grad_norm": 2.102370262145996,
      "learning_rate": 2.8533005436189492e-05,
      "loss": 1.3759,
      "step": 1990
    },
    {
      "epoch": 0.15492166773175314,
      "grad_norm": 1.6773416996002197,
      "learning_rate": 2.8525239451203727e-05,
      "loss": 1.3163,
      "step": 2000
    },
    {
      "epoch": 0.1556962760704119,
      "grad_norm": 2.1935882568359375,
      "learning_rate": 2.8517473466217965e-05,
      "loss": 1.4063,
      "step": 2010
    },
    {
      "epoch": 0.15647088440907067,
      "grad_norm": 2.3599579334259033,
      "learning_rate": 2.8509707481232203e-05,
      "loss": 1.4626,
      "step": 2020
    },
    {
      "epoch": 0.15724549274772942,
      "grad_norm": 2.3229963779449463,
      "learning_rate": 2.850194149624644e-05,
      "loss": 1.3447,
      "step": 2030
    },
    {
      "epoch": 0.1580201010863882,
      "grad_norm": 1.8202418088912964,
      "learning_rate": 2.8494175511260678e-05,
      "loss": 1.3428,
      "step": 2040
    },
    {
      "epoch": 0.15879470942504695,
      "grad_norm": 2.4268693923950195,
      "learning_rate": 2.8486409526274916e-05,
      "loss": 1.5027,
      "step": 2050
    },
    {
      "epoch": 0.15956931776370573,
      "grad_norm": 2.4634103775024414,
      "learning_rate": 2.8478643541289154e-05,
      "loss": 1.4189,
      "step": 2060
    },
    {
      "epoch": 0.1603439261023645,
      "grad_norm": 2.062577962875366,
      "learning_rate": 2.8470877556303392e-05,
      "loss": 1.3562,
      "step": 2070
    },
    {
      "epoch": 0.16111853444102325,
      "grad_norm": 1.4138545989990234,
      "learning_rate": 2.846311157131763e-05,
      "loss": 1.4421,
      "step": 2080
    },
    {
      "epoch": 0.16189314277968203,
      "grad_norm": 1.7305285930633545,
      "learning_rate": 2.8455345586331864e-05,
      "loss": 1.3699,
      "step": 2090
    },
    {
      "epoch": 0.16266775111834078,
      "grad_norm": 2.0937142372131348,
      "learning_rate": 2.8447579601346102e-05,
      "loss": 1.4256,
      "step": 2100
    },
    {
      "epoch": 0.16344235945699956,
      "grad_norm": 1.7880890369415283,
      "learning_rate": 2.8439813616360343e-05,
      "loss": 1.5296,
      "step": 2110
    },
    {
      "epoch": 0.1642169677956583,
      "grad_norm": 1.6174981594085693,
      "learning_rate": 2.843204763137458e-05,
      "loss": 1.4965,
      "step": 2120
    },
    {
      "epoch": 0.1649915761343171,
      "grad_norm": 2.8106532096862793,
      "learning_rate": 2.842428164638882e-05,
      "loss": 1.3727,
      "step": 2130
    },
    {
      "epoch": 0.16576618447297586,
      "grad_norm": 1.7096253633499146,
      "learning_rate": 2.8416515661403057e-05,
      "loss": 1.4405,
      "step": 2140
    },
    {
      "epoch": 0.1665407928116346,
      "grad_norm": 2.136658191680908,
      "learning_rate": 2.8408749676417295e-05,
      "loss": 1.4605,
      "step": 2150
    },
    {
      "epoch": 0.1673154011502934,
      "grad_norm": 1.9447309970855713,
      "learning_rate": 2.8400983691431533e-05,
      "loss": 1.3291,
      "step": 2160
    },
    {
      "epoch": 0.16809000948895214,
      "grad_norm": 2.5538933277130127,
      "learning_rate": 2.8393217706445767e-05,
      "loss": 1.3209,
      "step": 2170
    },
    {
      "epoch": 0.16886461782761092,
      "grad_norm": 1.5324676036834717,
      "learning_rate": 2.8385451721460005e-05,
      "loss": 1.3894,
      "step": 2180
    },
    {
      "epoch": 0.16963922616626967,
      "grad_norm": 1.8398752212524414,
      "learning_rate": 2.8377685736474243e-05,
      "loss": 1.4301,
      "step": 2190
    },
    {
      "epoch": 0.17041383450492845,
      "grad_norm": 2.2367193698883057,
      "learning_rate": 2.836991975148848e-05,
      "loss": 1.3614,
      "step": 2200
    },
    {
      "epoch": 0.17118844284358722,
      "grad_norm": 1.4501112699508667,
      "learning_rate": 2.836215376650272e-05,
      "loss": 1.3647,
      "step": 2210
    },
    {
      "epoch": 0.17196305118224597,
      "grad_norm": 2.181422233581543,
      "learning_rate": 2.8354387781516956e-05,
      "loss": 1.362,
      "step": 2220
    },
    {
      "epoch": 0.17273765952090475,
      "grad_norm": 1.7009400129318237,
      "learning_rate": 2.8346621796531194e-05,
      "loss": 1.3494,
      "step": 2230
    },
    {
      "epoch": 0.1735122678595635,
      "grad_norm": 2.1108932495117188,
      "learning_rate": 2.8338855811545432e-05,
      "loss": 1.2687,
      "step": 2240
    },
    {
      "epoch": 0.17428687619822228,
      "grad_norm": 1.6800947189331055,
      "learning_rate": 2.833108982655967e-05,
      "loss": 1.3929,
      "step": 2250
    },
    {
      "epoch": 0.17506148453688103,
      "grad_norm": 1.7616231441497803,
      "learning_rate": 2.8323323841573905e-05,
      "loss": 1.4115,
      "step": 2260
    },
    {
      "epoch": 0.1758360928755398,
      "grad_norm": 2.253950357437134,
      "learning_rate": 2.8315557856588142e-05,
      "loss": 1.3976,
      "step": 2270
    },
    {
      "epoch": 0.17661070121419858,
      "grad_norm": 1.460245966911316,
      "learning_rate": 2.830779187160238e-05,
      "loss": 1.4699,
      "step": 2280
    },
    {
      "epoch": 0.17738530955285733,
      "grad_norm": 2.123849868774414,
      "learning_rate": 2.8300025886616618e-05,
      "loss": 1.3799,
      "step": 2290
    },
    {
      "epoch": 0.1781599178915161,
      "grad_norm": 1.5434983968734741,
      "learning_rate": 2.829225990163086e-05,
      "loss": 1.339,
      "step": 2300
    },
    {
      "epoch": 0.17893452623017486,
      "grad_norm": 1.9510871171951294,
      "learning_rate": 2.8284493916645097e-05,
      "loss": 1.4254,
      "step": 2310
    },
    {
      "epoch": 0.17970913456883364,
      "grad_norm": 1.670914649963379,
      "learning_rate": 2.8276727931659335e-05,
      "loss": 1.3355,
      "step": 2320
    },
    {
      "epoch": 0.1804837429074924,
      "grad_norm": 1.817063570022583,
      "learning_rate": 2.8268961946673573e-05,
      "loss": 1.386,
      "step": 2330
    },
    {
      "epoch": 0.18125835124615117,
      "grad_norm": 2.181425094604492,
      "learning_rate": 2.8261195961687807e-05,
      "loss": 1.3122,
      "step": 2340
    },
    {
      "epoch": 0.18203295958480992,
      "grad_norm": 1.7498230934143066,
      "learning_rate": 2.8253429976702045e-05,
      "loss": 1.3499,
      "step": 2350
    },
    {
      "epoch": 0.1828075679234687,
      "grad_norm": 2.9229466915130615,
      "learning_rate": 2.8245663991716283e-05,
      "loss": 1.2955,
      "step": 2360
    },
    {
      "epoch": 0.18358217626212747,
      "grad_norm": 2.3527004718780518,
      "learning_rate": 2.823789800673052e-05,
      "loss": 1.375,
      "step": 2370
    },
    {
      "epoch": 0.18435678460078622,
      "grad_norm": 2.4167587757110596,
      "learning_rate": 2.823013202174476e-05,
      "loss": 1.3706,
      "step": 2380
    },
    {
      "epoch": 0.185131392939445,
      "grad_norm": 2.260951042175293,
      "learning_rate": 2.8222366036758997e-05,
      "loss": 1.4595,
      "step": 2390
    },
    {
      "epoch": 0.18590600127810375,
      "grad_norm": 2.2652461528778076,
      "learning_rate": 2.8214600051773235e-05,
      "loss": 1.4269,
      "step": 2400
    },
    {
      "epoch": 0.18668060961676253,
      "grad_norm": 1.9447325468063354,
      "learning_rate": 2.8206834066787473e-05,
      "loss": 1.4145,
      "step": 2410
    },
    {
      "epoch": 0.18745521795542128,
      "grad_norm": 1.6699305772781372,
      "learning_rate": 2.819906808180171e-05,
      "loss": 1.3227,
      "step": 2420
    },
    {
      "epoch": 0.18822982629408005,
      "grad_norm": 1.9489293098449707,
      "learning_rate": 2.8191302096815945e-05,
      "loss": 1.4336,
      "step": 2430
    },
    {
      "epoch": 0.18900443463273883,
      "grad_norm": 1.8914282321929932,
      "learning_rate": 2.8183536111830183e-05,
      "loss": 1.3133,
      "step": 2440
    },
    {
      "epoch": 0.18977904297139758,
      "grad_norm": 1.793596625328064,
      "learning_rate": 2.817577012684442e-05,
      "loss": 1.3905,
      "step": 2450
    },
    {
      "epoch": 0.19055365131005636,
      "grad_norm": 1.7409613132476807,
      "learning_rate": 2.816800414185866e-05,
      "loss": 1.4241,
      "step": 2460
    },
    {
      "epoch": 0.1913282596487151,
      "grad_norm": 2.005398988723755,
      "learning_rate": 2.8160238156872896e-05,
      "loss": 1.3205,
      "step": 2470
    },
    {
      "epoch": 0.1921028679873739,
      "grad_norm": 2.049870729446411,
      "learning_rate": 2.8152472171887134e-05,
      "loss": 1.2731,
      "step": 2480
    },
    {
      "epoch": 0.19287747632603264,
      "grad_norm": 1.3763504028320312,
      "learning_rate": 2.8144706186901372e-05,
      "loss": 1.3145,
      "step": 2490
    },
    {
      "epoch": 0.19365208466469142,
      "grad_norm": 1.6708521842956543,
      "learning_rate": 2.8136940201915613e-05,
      "loss": 1.3436,
      "step": 2500
    },
    {
      "epoch": 0.1944266930033502,
      "grad_norm": 1.6772174835205078,
      "learning_rate": 2.8129174216929848e-05,
      "loss": 1.4516,
      "step": 2510
    },
    {
      "epoch": 0.19520130134200894,
      "grad_norm": 1.9511886835098267,
      "learning_rate": 2.8121408231944086e-05,
      "loss": 1.4308,
      "step": 2520
    },
    {
      "epoch": 0.19597590968066772,
      "grad_norm": 1.5433961153030396,
      "learning_rate": 2.8113642246958324e-05,
      "loss": 1.3859,
      "step": 2530
    },
    {
      "epoch": 0.19675051801932647,
      "grad_norm": 1.5548330545425415,
      "learning_rate": 2.810587626197256e-05,
      "loss": 1.4357,
      "step": 2540
    },
    {
      "epoch": 0.19752512635798525,
      "grad_norm": 1.964456558227539,
      "learning_rate": 2.80981102769868e-05,
      "loss": 1.2751,
      "step": 2550
    },
    {
      "epoch": 0.198299734696644,
      "grad_norm": 1.7032779455184937,
      "learning_rate": 2.8090344292001037e-05,
      "loss": 1.4324,
      "step": 2560
    },
    {
      "epoch": 0.19907434303530278,
      "grad_norm": 2.020057201385498,
      "learning_rate": 2.8082578307015275e-05,
      "loss": 1.3659,
      "step": 2570
    },
    {
      "epoch": 0.19984895137396155,
      "grad_norm": 1.907408595085144,
      "learning_rate": 2.8074812322029513e-05,
      "loss": 1.4035,
      "step": 2580
    },
    {
      "epoch": 0.2006235597126203,
      "grad_norm": 2.5626890659332275,
      "learning_rate": 2.806704633704375e-05,
      "loss": 1.4279,
      "step": 2590
    },
    {
      "epoch": 0.20139816805127908,
      "grad_norm": 1.8407142162322998,
      "learning_rate": 2.8059280352057985e-05,
      "loss": 1.5245,
      "step": 2600
    },
    {
      "epoch": 0.20217277638993783,
      "grad_norm": 1.500603199005127,
      "learning_rate": 2.8051514367072223e-05,
      "loss": 1.4619,
      "step": 2610
    },
    {
      "epoch": 0.2029473847285966,
      "grad_norm": 3.0873286724090576,
      "learning_rate": 2.804374838208646e-05,
      "loss": 1.4177,
      "step": 2620
    },
    {
      "epoch": 0.20372199306725536,
      "grad_norm": 2.268784761428833,
      "learning_rate": 2.80359823971007e-05,
      "loss": 1.3874,
      "step": 2630
    },
    {
      "epoch": 0.20449660140591414,
      "grad_norm": 2.0670254230499268,
      "learning_rate": 2.8028216412114937e-05,
      "loss": 1.4409,
      "step": 2640
    },
    {
      "epoch": 0.2052712097445729,
      "grad_norm": 1.785929560661316,
      "learning_rate": 2.8020450427129175e-05,
      "loss": 1.461,
      "step": 2650
    },
    {
      "epoch": 0.20604581808323166,
      "grad_norm": 1.664514422416687,
      "learning_rate": 2.8012684442143412e-05,
      "loss": 1.3955,
      "step": 2660
    },
    {
      "epoch": 0.20682042642189044,
      "grad_norm": 2.014519214630127,
      "learning_rate": 2.800491845715765e-05,
      "loss": 1.4288,
      "step": 2670
    },
    {
      "epoch": 0.2075950347605492,
      "grad_norm": 1.8887070417404175,
      "learning_rate": 2.7997152472171885e-05,
      "loss": 1.332,
      "step": 2680
    },
    {
      "epoch": 0.20836964309920797,
      "grad_norm": 2.1709744930267334,
      "learning_rate": 2.7989386487186126e-05,
      "loss": 1.3651,
      "step": 2690
    },
    {
      "epoch": 0.20914425143786672,
      "grad_norm": 2.128208637237549,
      "learning_rate": 2.7981620502200364e-05,
      "loss": 1.3968,
      "step": 2700
    },
    {
      "epoch": 0.2099188597765255,
      "grad_norm": 1.554371953010559,
      "learning_rate": 2.7973854517214602e-05,
      "loss": 1.3695,
      "step": 2710
    },
    {
      "epoch": 0.21069346811518427,
      "grad_norm": 2.3793773651123047,
      "learning_rate": 2.796608853222884e-05,
      "loss": 1.3512,
      "step": 2720
    },
    {
      "epoch": 0.21146807645384302,
      "grad_norm": 1.810225248336792,
      "learning_rate": 2.7958322547243077e-05,
      "loss": 1.2937,
      "step": 2730
    },
    {
      "epoch": 0.2122426847925018,
      "grad_norm": 2.085806369781494,
      "learning_rate": 2.7950556562257315e-05,
      "loss": 1.3658,
      "step": 2740
    },
    {
      "epoch": 0.21301729313116055,
      "grad_norm": 1.972305178642273,
      "learning_rate": 2.7942790577271553e-05,
      "loss": 1.4366,
      "step": 2750
    },
    {
      "epoch": 0.21379190146981933,
      "grad_norm": 2.0311474800109863,
      "learning_rate": 2.793502459228579e-05,
      "loss": 1.4025,
      "step": 2760
    },
    {
      "epoch": 0.21456650980847808,
      "grad_norm": 2.338866949081421,
      "learning_rate": 2.7927258607300026e-05,
      "loss": 1.3538,
      "step": 2770
    },
    {
      "epoch": 0.21534111814713686,
      "grad_norm": 1.6445509195327759,
      "learning_rate": 2.7919492622314263e-05,
      "loss": 1.2978,
      "step": 2780
    },
    {
      "epoch": 0.2161157264857956,
      "grad_norm": 1.6779170036315918,
      "learning_rate": 2.79117266373285e-05,
      "loss": 1.3604,
      "step": 2790
    },
    {
      "epoch": 0.21689033482445438,
      "grad_norm": 1.7576665878295898,
      "learning_rate": 2.790396065234274e-05,
      "loss": 1.2828,
      "step": 2800
    },
    {
      "epoch": 0.21766494316311316,
      "grad_norm": 2.493762493133545,
      "learning_rate": 2.7896194667356977e-05,
      "loss": 1.4671,
      "step": 2810
    },
    {
      "epoch": 0.2184395515017719,
      "grad_norm": 1.8502061367034912,
      "learning_rate": 2.7888428682371215e-05,
      "loss": 1.3648,
      "step": 2820
    },
    {
      "epoch": 0.2192141598404307,
      "grad_norm": 1.707118272781372,
      "learning_rate": 2.7880662697385453e-05,
      "loss": 1.3688,
      "step": 2830
    },
    {
      "epoch": 0.21998876817908944,
      "grad_norm": 1.5942034721374512,
      "learning_rate": 2.787289671239969e-05,
      "loss": 1.4706,
      "step": 2840
    },
    {
      "epoch": 0.22076337651774822,
      "grad_norm": 1.9741336107254028,
      "learning_rate": 2.7865130727413925e-05,
      "loss": 1.4157,
      "step": 2850
    },
    {
      "epoch": 0.22153798485640697,
      "grad_norm": 2.103844404220581,
      "learning_rate": 2.7857364742428163e-05,
      "loss": 1.4562,
      "step": 2860
    },
    {
      "epoch": 0.22231259319506574,
      "grad_norm": 2.0788991451263428,
      "learning_rate": 2.78495987574424e-05,
      "loss": 1.3239,
      "step": 2870
    },
    {
      "epoch": 0.22308720153372452,
      "grad_norm": 1.8931974172592163,
      "learning_rate": 2.784183277245664e-05,
      "loss": 1.4335,
      "step": 2880
    },
    {
      "epoch": 0.22386180987238327,
      "grad_norm": 1.5913469791412354,
      "learning_rate": 2.783406678747088e-05,
      "loss": 1.4097,
      "step": 2890
    },
    {
      "epoch": 0.22463641821104205,
      "grad_norm": 2.083609104156494,
      "learning_rate": 2.7826300802485118e-05,
      "loss": 1.3469,
      "step": 2900
    },
    {
      "epoch": 0.2254110265497008,
      "grad_norm": 1.5022245645523071,
      "learning_rate": 2.7818534817499356e-05,
      "loss": 1.3971,
      "step": 2910
    },
    {
      "epoch": 0.22618563488835958,
      "grad_norm": 1.8880620002746582,
      "learning_rate": 2.7810768832513594e-05,
      "loss": 1.2335,
      "step": 2920
    },
    {
      "epoch": 0.22696024322701833,
      "grad_norm": 2.9112913608551025,
      "learning_rate": 2.780300284752783e-05,
      "loss": 1.4941,
      "step": 2930
    },
    {
      "epoch": 0.2277348515656771,
      "grad_norm": 2.046342372894287,
      "learning_rate": 2.7795236862542066e-05,
      "loss": 1.3385,
      "step": 2940
    },
    {
      "epoch": 0.22850945990433588,
      "grad_norm": 1.8709993362426758,
      "learning_rate": 2.7787470877556304e-05,
      "loss": 1.3379,
      "step": 2950
    },
    {
      "epoch": 0.22928406824299463,
      "grad_norm": 2.1010212898254395,
      "learning_rate": 2.777970489257054e-05,
      "loss": 1.3521,
      "step": 2960
    },
    {
      "epoch": 0.2300586765816534,
      "grad_norm": 2.39827036857605,
      "learning_rate": 2.777193890758478e-05,
      "loss": 1.3664,
      "step": 2970
    },
    {
      "epoch": 0.23083328492031216,
      "grad_norm": 2.086989164352417,
      "learning_rate": 2.7764172922599017e-05,
      "loss": 1.3372,
      "step": 2980
    },
    {
      "epoch": 0.23160789325897094,
      "grad_norm": 3.3026576042175293,
      "learning_rate": 2.775718353611183e-05,
      "loss": 1.4047,
      "step": 2990
    },
    {
      "epoch": 0.2323825015976297,
      "grad_norm": 1.9891080856323242,
      "learning_rate": 2.774941755112607e-05,
      "loss": 1.4006,
      "step": 3000
    },
    {
      "epoch": 0.23315710993628846,
      "grad_norm": 1.881161093711853,
      "learning_rate": 2.7741651566140303e-05,
      "loss": 1.3698,
      "step": 3010
    },
    {
      "epoch": 0.23393171827494724,
      "grad_norm": 2.09169864654541,
      "learning_rate": 2.773388558115454e-05,
      "loss": 1.4284,
      "step": 3020
    },
    {
      "epoch": 0.234706326613606,
      "grad_norm": 2.1708579063415527,
      "learning_rate": 2.7726119596168782e-05,
      "loss": 1.3948,
      "step": 3030
    },
    {
      "epoch": 0.23548093495226477,
      "grad_norm": 1.8244209289550781,
      "learning_rate": 2.771835361118302e-05,
      "loss": 1.2667,
      "step": 3040
    },
    {
      "epoch": 0.23625554329092352,
      "grad_norm": 1.8988511562347412,
      "learning_rate": 2.7710587626197258e-05,
      "loss": 1.3422,
      "step": 3050
    },
    {
      "epoch": 0.2370301516295823,
      "grad_norm": 1.6313751935958862,
      "learning_rate": 2.7702821641211496e-05,
      "loss": 1.3658,
      "step": 3060
    },
    {
      "epoch": 0.23780475996824105,
      "grad_norm": 1.7170292139053345,
      "learning_rate": 2.7695055656225734e-05,
      "loss": 1.4655,
      "step": 3070
    },
    {
      "epoch": 0.23857936830689982,
      "grad_norm": 2.6905252933502197,
      "learning_rate": 2.768728967123997e-05,
      "loss": 1.3894,
      "step": 3080
    },
    {
      "epoch": 0.2393539766455586,
      "grad_norm": 1.5166258811950684,
      "learning_rate": 2.767952368625421e-05,
      "loss": 1.3966,
      "step": 3090
    },
    {
      "epoch": 0.24012858498421735,
      "grad_norm": 1.7806110382080078,
      "learning_rate": 2.7671757701268444e-05,
      "loss": 1.304,
      "step": 3100
    },
    {
      "epoch": 0.24090319332287613,
      "grad_norm": 1.596228837966919,
      "learning_rate": 2.7663991716282682e-05,
      "loss": 1.3742,
      "step": 3110
    },
    {
      "epoch": 0.24167780166153488,
      "grad_norm": 1.7936153411865234,
      "learning_rate": 2.765622573129692e-05,
      "loss": 1.4559,
      "step": 3120
    },
    {
      "epoch": 0.24245241000019366,
      "grad_norm": 1.841230034828186,
      "learning_rate": 2.7648459746311157e-05,
      "loss": 1.4213,
      "step": 3130
    },
    {
      "epoch": 0.2432270183388524,
      "grad_norm": 1.895527958869934,
      "learning_rate": 2.7640693761325395e-05,
      "loss": 1.4654,
      "step": 3140
    },
    {
      "epoch": 0.24400162667751119,
      "grad_norm": 1.9358474016189575,
      "learning_rate": 2.7632927776339633e-05,
      "loss": 1.3305,
      "step": 3150
    },
    {
      "epoch": 0.24477623501616996,
      "grad_norm": 1.8813395500183105,
      "learning_rate": 2.762516179135387e-05,
      "loss": 1.3098,
      "step": 3160
    },
    {
      "epoch": 0.2455508433548287,
      "grad_norm": 2.1671814918518066,
      "learning_rate": 2.761739580636811e-05,
      "loss": 1.3412,
      "step": 3170
    },
    {
      "epoch": 0.2463254516934875,
      "grad_norm": 2.3781254291534424,
      "learning_rate": 2.7609629821382343e-05,
      "loss": 1.3506,
      "step": 3180
    },
    {
      "epoch": 0.24710006003214624,
      "grad_norm": 1.514906406402588,
      "learning_rate": 2.760186383639658e-05,
      "loss": 1.3743,
      "step": 3190
    },
    {
      "epoch": 0.24787466837080502,
      "grad_norm": 2.5755412578582764,
      "learning_rate": 2.759409785141082e-05,
      "loss": 1.3583,
      "step": 3200
    },
    {
      "epoch": 0.24864927670946377,
      "grad_norm": 2.15248966217041,
      "learning_rate": 2.7586331866425057e-05,
      "loss": 1.4033,
      "step": 3210
    },
    {
      "epoch": 0.24942388504812255,
      "grad_norm": 2.048130750656128,
      "learning_rate": 2.7578565881439298e-05,
      "loss": 1.4118,
      "step": 3220
    },
    {
      "epoch": 0.2501984933867813,
      "grad_norm": 2.9516727924346924,
      "learning_rate": 2.7570799896453536e-05,
      "loss": 1.4465,
      "step": 3230
    },
    {
      "epoch": 0.2509731017254401,
      "grad_norm": 2.99647855758667,
      "learning_rate": 2.7563033911467774e-05,
      "loss": 1.3724,
      "step": 3240
    },
    {
      "epoch": 0.25174771006409885,
      "grad_norm": 1.4150632619857788,
      "learning_rate": 2.7555267926482012e-05,
      "loss": 1.3721,
      "step": 3250
    },
    {
      "epoch": 0.25252231840275763,
      "grad_norm": 1.7837611436843872,
      "learning_rate": 2.754750194149625e-05,
      "loss": 1.2905,
      "step": 3260
    },
    {
      "epoch": 0.25329692674141635,
      "grad_norm": 2.027198076248169,
      "learning_rate": 2.7539735956510484e-05,
      "loss": 1.4,
      "step": 3270
    },
    {
      "epoch": 0.25407153508007513,
      "grad_norm": 2.564924955368042,
      "learning_rate": 2.7531969971524722e-05,
      "loss": 1.3183,
      "step": 3280
    },
    {
      "epoch": 0.2548461434187339,
      "grad_norm": 1.561368465423584,
      "learning_rate": 2.752420398653896e-05,
      "loss": 1.4225,
      "step": 3290
    },
    {
      "epoch": 0.2556207517573927,
      "grad_norm": 2.1191601753234863,
      "learning_rate": 2.7516438001553198e-05,
      "loss": 1.2675,
      "step": 3300
    },
    {
      "epoch": 0.25639536009605146,
      "grad_norm": 2.305990695953369,
      "learning_rate": 2.7508672016567436e-05,
      "loss": 1.4441,
      "step": 3310
    },
    {
      "epoch": 0.2571699684347102,
      "grad_norm": 1.6965970993041992,
      "learning_rate": 2.7500906031581674e-05,
      "loss": 1.458,
      "step": 3320
    },
    {
      "epoch": 0.25794457677336896,
      "grad_norm": 2.4819579124450684,
      "learning_rate": 2.749314004659591e-05,
      "loss": 1.4661,
      "step": 3330
    },
    {
      "epoch": 0.25871918511202774,
      "grad_norm": 1.8826476335525513,
      "learning_rate": 2.748537406161015e-05,
      "loss": 1.3288,
      "step": 3340
    },
    {
      "epoch": 0.2594937934506865,
      "grad_norm": 2.4627737998962402,
      "learning_rate": 2.7477608076624384e-05,
      "loss": 1.4328,
      "step": 3350
    },
    {
      "epoch": 0.26026840178934524,
      "grad_norm": 1.8994475603103638,
      "learning_rate": 2.746984209163862e-05,
      "loss": 1.3742,
      "step": 3360
    },
    {
      "epoch": 0.261043010128004,
      "grad_norm": 2.177466869354248,
      "learning_rate": 2.746207610665286e-05,
      "loss": 1.3519,
      "step": 3370
    },
    {
      "epoch": 0.2618176184666628,
      "grad_norm": 2.8101470470428467,
      "learning_rate": 2.7454310121667097e-05,
      "loss": 1.4584,
      "step": 3380
    },
    {
      "epoch": 0.26259222680532157,
      "grad_norm": 2.1847400665283203,
      "learning_rate": 2.7446544136681335e-05,
      "loss": 1.3493,
      "step": 3390
    },
    {
      "epoch": 0.26336683514398035,
      "grad_norm": 2.265021324157715,
      "learning_rate": 2.7438778151695573e-05,
      "loss": 1.3198,
      "step": 3400
    },
    {
      "epoch": 0.26414144348263907,
      "grad_norm": 2.5834813117980957,
      "learning_rate": 2.743101216670981e-05,
      "loss": 1.2944,
      "step": 3410
    },
    {
      "epoch": 0.26491605182129785,
      "grad_norm": 1.9844568967819214,
      "learning_rate": 2.7423246181724052e-05,
      "loss": 1.512,
      "step": 3420
    },
    {
      "epoch": 0.2656906601599566,
      "grad_norm": 2.256381034851074,
      "learning_rate": 2.741548019673829e-05,
      "loss": 1.2821,
      "step": 3430
    },
    {
      "epoch": 0.2664652684986154,
      "grad_norm": 1.7786041498184204,
      "learning_rate": 2.7407714211752525e-05,
      "loss": 1.3798,
      "step": 3440
    },
    {
      "epoch": 0.2672398768372741,
      "grad_norm": 2.0039472579956055,
      "learning_rate": 2.7399948226766762e-05,
      "loss": 1.3243,
      "step": 3450
    },
    {
      "epoch": 0.2680144851759329,
      "grad_norm": 1.8474605083465576,
      "learning_rate": 2.7392182241781e-05,
      "loss": 1.3167,
      "step": 3460
    },
    {
      "epoch": 0.2687890935145917,
      "grad_norm": 1.8027704954147339,
      "learning_rate": 2.7384416256795238e-05,
      "loss": 1.4351,
      "step": 3470
    },
    {
      "epoch": 0.26956370185325046,
      "grad_norm": 2.0710697174072266,
      "learning_rate": 2.7376650271809476e-05,
      "loss": 1.4645,
      "step": 3480
    },
    {
      "epoch": 0.27033831019190924,
      "grad_norm": 2.007577657699585,
      "learning_rate": 2.7368884286823714e-05,
      "loss": 1.4416,
      "step": 3490
    },
    {
      "epoch": 0.27111291853056796,
      "grad_norm": 2.1410703659057617,
      "learning_rate": 2.7361118301837952e-05,
      "loss": 1.3557,
      "step": 3500
    },
    {
      "epoch": 0.27188752686922674,
      "grad_norm": 2.027327299118042,
      "learning_rate": 2.735335231685219e-05,
      "loss": 1.3899,
      "step": 3510
    },
    {
      "epoch": 0.2726621352078855,
      "grad_norm": 1.861595869064331,
      "learning_rate": 2.7345586331866424e-05,
      "loss": 1.3846,
      "step": 3520
    },
    {
      "epoch": 0.2734367435465443,
      "grad_norm": 2.0826027393341064,
      "learning_rate": 2.7337820346880662e-05,
      "loss": 1.3044,
      "step": 3530
    },
    {
      "epoch": 0.27421135188520307,
      "grad_norm": 2.2013914585113525,
      "learning_rate": 2.73300543618949e-05,
      "loss": 1.3931,
      "step": 3540
    },
    {
      "epoch": 0.2749859602238618,
      "grad_norm": 1.6255178451538086,
      "learning_rate": 2.7322288376909138e-05,
      "loss": 1.3587,
      "step": 3550
    },
    {
      "epoch": 0.27576056856252057,
      "grad_norm": 2.6638708114624023,
      "learning_rate": 2.7314522391923376e-05,
      "loss": 1.3965,
      "step": 3560
    },
    {
      "epoch": 0.27653517690117935,
      "grad_norm": 1.9526418447494507,
      "learning_rate": 2.7306756406937613e-05,
      "loss": 1.428,
      "step": 3570
    },
    {
      "epoch": 0.2773097852398381,
      "grad_norm": 2.1983394622802734,
      "learning_rate": 2.729899042195185e-05,
      "loss": 1.3321,
      "step": 3580
    },
    {
      "epoch": 0.27808439357849685,
      "grad_norm": 2.2516543865203857,
      "learning_rate": 2.729122443696609e-05,
      "loss": 1.3429,
      "step": 3590
    },
    {
      "epoch": 0.2788590019171556,
      "grad_norm": 2.6503546237945557,
      "learning_rate": 2.7283458451980327e-05,
      "loss": 1.3857,
      "step": 3600
    },
    {
      "epoch": 0.2796336102558144,
      "grad_norm": 1.7393653392791748,
      "learning_rate": 2.727569246699456e-05,
      "loss": 1.3541,
      "step": 3610
    },
    {
      "epoch": 0.2804082185944732,
      "grad_norm": 2.67938232421875,
      "learning_rate": 2.7267926482008803e-05,
      "loss": 1.4004,
      "step": 3620
    },
    {
      "epoch": 0.28118282693313196,
      "grad_norm": 1.3702934980392456,
      "learning_rate": 2.726016049702304e-05,
      "loss": 1.3472,
      "step": 3630
    },
    {
      "epoch": 0.2819574352717907,
      "grad_norm": 1.7532790899276733,
      "learning_rate": 2.725239451203728e-05,
      "loss": 1.2954,
      "step": 3640
    },
    {
      "epoch": 0.28273204361044946,
      "grad_norm": 2.300851345062256,
      "learning_rate": 2.7244628527051516e-05,
      "loss": 1.3234,
      "step": 3650
    },
    {
      "epoch": 0.28350665194910823,
      "grad_norm": 1.4941855669021606,
      "learning_rate": 2.7236862542065754e-05,
      "loss": 1.3903,
      "step": 3660
    },
    {
      "epoch": 0.284281260287767,
      "grad_norm": 1.6798298358917236,
      "learning_rate": 2.7229096557079992e-05,
      "loss": 1.357,
      "step": 3670
    },
    {
      "epoch": 0.2850558686264258,
      "grad_norm": 2.0041141510009766,
      "learning_rate": 2.722133057209423e-05,
      "loss": 1.3541,
      "step": 3680
    },
    {
      "epoch": 0.2858304769650845,
      "grad_norm": 1.599748134613037,
      "learning_rate": 2.7213564587108464e-05,
      "loss": 1.3784,
      "step": 3690
    },
    {
      "epoch": 0.2866050853037433,
      "grad_norm": 1.808371901512146,
      "learning_rate": 2.7205798602122702e-05,
      "loss": 1.3193,
      "step": 3700
    },
    {
      "epoch": 0.28737969364240207,
      "grad_norm": 1.5548759698867798,
      "learning_rate": 2.719803261713694e-05,
      "loss": 1.3368,
      "step": 3710
    },
    {
      "epoch": 0.28815430198106085,
      "grad_norm": 2.076599359512329,
      "learning_rate": 2.7190266632151178e-05,
      "loss": 1.304,
      "step": 3720
    },
    {
      "epoch": 0.28892891031971957,
      "grad_norm": 2.140089988708496,
      "learning_rate": 2.7182500647165416e-05,
      "loss": 1.4674,
      "step": 3730
    },
    {
      "epoch": 0.28970351865837835,
      "grad_norm": 2.200303554534912,
      "learning_rate": 2.7174734662179654e-05,
      "loss": 1.3782,
      "step": 3740
    },
    {
      "epoch": 0.2904781269970371,
      "grad_norm": 2.2986483573913574,
      "learning_rate": 2.716696867719389e-05,
      "loss": 1.3707,
      "step": 3750
    },
    {
      "epoch": 0.2912527353356959,
      "grad_norm": 2.195906162261963,
      "learning_rate": 2.715920269220813e-05,
      "loss": 1.3259,
      "step": 3760
    },
    {
      "epoch": 0.2920273436743547,
      "grad_norm": 2.206803321838379,
      "learning_rate": 2.7151436707222367e-05,
      "loss": 1.3459,
      "step": 3770
    },
    {
      "epoch": 0.2928019520130134,
      "grad_norm": 1.9149448871612549,
      "learning_rate": 2.7143670722236602e-05,
      "loss": 1.347,
      "step": 3780
    },
    {
      "epoch": 0.2935765603516722,
      "grad_norm": 2.1476709842681885,
      "learning_rate": 2.713590473725084e-05,
      "loss": 1.3475,
      "step": 3790
    },
    {
      "epoch": 0.29435116869033096,
      "grad_norm": 1.8517489433288574,
      "learning_rate": 2.7128915350763657e-05,
      "loss": 1.2666,
      "step": 3800
    },
    {
      "epoch": 0.29512577702898973,
      "grad_norm": 1.6887702941894531,
      "learning_rate": 2.7121149365777894e-05,
      "loss": 1.3536,
      "step": 3810
    },
    {
      "epoch": 0.29590038536764846,
      "grad_norm": 2.1302123069763184,
      "learning_rate": 2.7113383380792132e-05,
      "loss": 1.4551,
      "step": 3820
    },
    {
      "epoch": 0.29667499370630723,
      "grad_norm": 1.6919621229171753,
      "learning_rate": 2.710561739580637e-05,
      "loss": 1.3044,
      "step": 3830
    },
    {
      "epoch": 0.297449602044966,
      "grad_norm": 1.6979820728302002,
      "learning_rate": 2.7097851410820608e-05,
      "loss": 1.3271,
      "step": 3840
    },
    {
      "epoch": 0.2982242103836248,
      "grad_norm": 1.8628753423690796,
      "learning_rate": 2.7090085425834842e-05,
      "loss": 1.3669,
      "step": 3850
    },
    {
      "epoch": 0.29899881872228357,
      "grad_norm": 1.88213312625885,
      "learning_rate": 2.708231944084908e-05,
      "loss": 1.2863,
      "step": 3860
    },
    {
      "epoch": 0.2997734270609423,
      "grad_norm": 1.8541834354400635,
      "learning_rate": 2.7074553455863318e-05,
      "loss": 1.3829,
      "step": 3870
    },
    {
      "epoch": 0.30054803539960107,
      "grad_norm": 2.1723697185516357,
      "learning_rate": 2.7066787470877556e-05,
      "loss": 1.246,
      "step": 3880
    },
    {
      "epoch": 0.30132264373825984,
      "grad_norm": 1.335213541984558,
      "learning_rate": 2.7059021485891794e-05,
      "loss": 1.474,
      "step": 3890
    },
    {
      "epoch": 0.3020972520769186,
      "grad_norm": 2.0030906200408936,
      "learning_rate": 2.7051255500906032e-05,
      "loss": 1.3532,
      "step": 3900
    },
    {
      "epoch": 0.3028718604155774,
      "grad_norm": 1.9827536344528198,
      "learning_rate": 2.704348951592027e-05,
      "loss": 1.4632,
      "step": 3910
    },
    {
      "epoch": 0.3036464687542361,
      "grad_norm": 1.5805875062942505,
      "learning_rate": 2.7035723530934508e-05,
      "loss": 1.3288,
      "step": 3920
    },
    {
      "epoch": 0.3044210770928949,
      "grad_norm": 2.334123134613037,
      "learning_rate": 2.7027957545948745e-05,
      "loss": 1.3553,
      "step": 3930
    },
    {
      "epoch": 0.3051956854315537,
      "grad_norm": 2.7618377208709717,
      "learning_rate": 2.702019156096298e-05,
      "loss": 1.3074,
      "step": 3940
    },
    {
      "epoch": 0.30597029377021245,
      "grad_norm": 3.213888168334961,
      "learning_rate": 2.701242557597722e-05,
      "loss": 1.4124,
      "step": 3950
    },
    {
      "epoch": 0.3067449021088712,
      "grad_norm": 2.3708314895629883,
      "learning_rate": 2.700465959099146e-05,
      "loss": 1.422,
      "step": 3960
    },
    {
      "epoch": 0.30751951044752995,
      "grad_norm": 1.8687870502471924,
      "learning_rate": 2.6996893606005697e-05,
      "loss": 1.4751,
      "step": 3970
    },
    {
      "epoch": 0.30829411878618873,
      "grad_norm": 2.1923913955688477,
      "learning_rate": 2.6989127621019935e-05,
      "loss": 1.3946,
      "step": 3980
    },
    {
      "epoch": 0.3090687271248475,
      "grad_norm": 2.0621774196624756,
      "learning_rate": 2.6981361636034173e-05,
      "loss": 1.3092,
      "step": 3990
    },
    {
      "epoch": 0.3098433354635063,
      "grad_norm": 2.024932384490967,
      "learning_rate": 2.697359565104841e-05,
      "loss": 1.335,
      "step": 4000
    },
    {
      "epoch": 0.310617943802165,
      "grad_norm": 1.9870883226394653,
      "learning_rate": 2.696582966606265e-05,
      "loss": 1.42,
      "step": 4010
    },
    {
      "epoch": 0.3113925521408238,
      "grad_norm": 1.818912386894226,
      "learning_rate": 2.6958063681076883e-05,
      "loss": 1.3724,
      "step": 4020
    },
    {
      "epoch": 0.31216716047948256,
      "grad_norm": 2.119112491607666,
      "learning_rate": 2.695029769609112e-05,
      "loss": 1.3265,
      "step": 4030
    },
    {
      "epoch": 0.31294176881814134,
      "grad_norm": 1.7756009101867676,
      "learning_rate": 2.694253171110536e-05,
      "loss": 1.3135,
      "step": 4040
    },
    {
      "epoch": 0.3137163771568001,
      "grad_norm": 2.281104803085327,
      "learning_rate": 2.6934765726119596e-05,
      "loss": 1.4285,
      "step": 4050
    },
    {
      "epoch": 0.31449098549545884,
      "grad_norm": 1.9557714462280273,
      "learning_rate": 2.6926999741133834e-05,
      "loss": 1.4173,
      "step": 4060
    },
    {
      "epoch": 0.3152655938341176,
      "grad_norm": 1.8659254312515259,
      "learning_rate": 2.6919233756148072e-05,
      "loss": 1.4017,
      "step": 4070
    },
    {
      "epoch": 0.3160402021727764,
      "grad_norm": 1.7908719778060913,
      "learning_rate": 2.691146777116231e-05,
      "loss": 1.406,
      "step": 4080
    },
    {
      "epoch": 0.3168148105114352,
      "grad_norm": 1.9588013887405396,
      "learning_rate": 2.6903701786176548e-05,
      "loss": 1.4754,
      "step": 4090
    },
    {
      "epoch": 0.3175894188500939,
      "grad_norm": 1.5995426177978516,
      "learning_rate": 2.6895935801190786e-05,
      "loss": 1.4045,
      "step": 4100
    },
    {
      "epoch": 0.3183640271887527,
      "grad_norm": 2.587292194366455,
      "learning_rate": 2.688816981620502e-05,
      "loss": 1.3519,
      "step": 4110
    },
    {
      "epoch": 0.31913863552741145,
      "grad_norm": 1.7680983543395996,
      "learning_rate": 2.6880403831219258e-05,
      "loss": 1.2933,
      "step": 4120
    },
    {
      "epoch": 0.31991324386607023,
      "grad_norm": 2.0504140853881836,
      "learning_rate": 2.6872637846233496e-05,
      "loss": 1.3663,
      "step": 4130
    },
    {
      "epoch": 0.320687852204729,
      "grad_norm": 1.95406174659729,
      "learning_rate": 2.6864871861247734e-05,
      "loss": 1.3241,
      "step": 4140
    },
    {
      "epoch": 0.32146246054338773,
      "grad_norm": 1.4397131204605103,
      "learning_rate": 2.6857105876261975e-05,
      "loss": 1.3143,
      "step": 4150
    },
    {
      "epoch": 0.3222370688820465,
      "grad_norm": 2.465041399002075,
      "learning_rate": 2.6849339891276213e-05,
      "loss": 1.3021,
      "step": 4160
    },
    {
      "epoch": 0.3230116772207053,
      "grad_norm": 1.613321304321289,
      "learning_rate": 2.684157390629045e-05,
      "loss": 1.2711,
      "step": 4170
    },
    {
      "epoch": 0.32378628555936406,
      "grad_norm": 1.5652897357940674,
      "learning_rate": 2.683380792130469e-05,
      "loss": 1.3946,
      "step": 4180
    },
    {
      "epoch": 0.32456089389802284,
      "grad_norm": 2.83069109916687,
      "learning_rate": 2.6826041936318923e-05,
      "loss": 1.2988,
      "step": 4190
    },
    {
      "epoch": 0.32533550223668156,
      "grad_norm": 1.9654872417449951,
      "learning_rate": 2.681827595133316e-05,
      "loss": 1.3488,
      "step": 4200
    },
    {
      "epoch": 0.32611011057534034,
      "grad_norm": 2.2770352363586426,
      "learning_rate": 2.68105099663474e-05,
      "loss": 1.4058,
      "step": 4210
    },
    {
      "epoch": 0.3268847189139991,
      "grad_norm": 1.5127121210098267,
      "learning_rate": 2.6802743981361637e-05,
      "loss": 1.3681,
      "step": 4220
    },
    {
      "epoch": 0.3276593272526579,
      "grad_norm": 2.3046159744262695,
      "learning_rate": 2.6794977996375875e-05,
      "loss": 1.3571,
      "step": 4230
    },
    {
      "epoch": 0.3284339355913166,
      "grad_norm": 2.111872434616089,
      "learning_rate": 2.6787212011390112e-05,
      "loss": 1.3852,
      "step": 4240
    },
    {
      "epoch": 0.3292085439299754,
      "grad_norm": 1.8284177780151367,
      "learning_rate": 2.677944602640435e-05,
      "loss": 1.2901,
      "step": 4250
    },
    {
      "epoch": 0.3299831522686342,
      "grad_norm": 1.994808316230774,
      "learning_rate": 2.6771680041418588e-05,
      "loss": 1.3439,
      "step": 4260
    },
    {
      "epoch": 0.33075776060729295,
      "grad_norm": 2.0119495391845703,
      "learning_rate": 2.6763914056432826e-05,
      "loss": 1.4089,
      "step": 4270
    },
    {
      "epoch": 0.3315323689459517,
      "grad_norm": 2.3536651134490967,
      "learning_rate": 2.675614807144706e-05,
      "loss": 1.3469,
      "step": 4280
    },
    {
      "epoch": 0.33230697728461045,
      "grad_norm": 2.1747794151306152,
      "learning_rate": 2.67483820864613e-05,
      "loss": 1.3769,
      "step": 4290
    },
    {
      "epoch": 0.3330815856232692,
      "grad_norm": 2.296754837036133,
      "learning_rate": 2.6740616101475536e-05,
      "loss": 1.303,
      "step": 4300
    },
    {
      "epoch": 0.333856193961928,
      "grad_norm": 2.081937313079834,
      "learning_rate": 2.6732850116489774e-05,
      "loss": 1.3886,
      "step": 4310
    },
    {
      "epoch": 0.3346308023005868,
      "grad_norm": 2.2020533084869385,
      "learning_rate": 2.6725084131504012e-05,
      "loss": 1.3832,
      "step": 4320
    },
    {
      "epoch": 0.3354054106392455,
      "grad_norm": 1.9286404848098755,
      "learning_rate": 2.671731814651825e-05,
      "loss": 1.3103,
      "step": 4330
    },
    {
      "epoch": 0.3361800189779043,
      "grad_norm": 2.6386666297912598,
      "learning_rate": 2.670955216153249e-05,
      "loss": 1.2551,
      "step": 4340
    },
    {
      "epoch": 0.33695462731656306,
      "grad_norm": 2.302783966064453,
      "learning_rate": 2.670178617654673e-05,
      "loss": 1.298,
      "step": 4350
    },
    {
      "epoch": 0.33772923565522184,
      "grad_norm": 1.6836986541748047,
      "learning_rate": 2.6694020191560963e-05,
      "loss": 1.4205,
      "step": 4360
    },
    {
      "epoch": 0.3385038439938806,
      "grad_norm": 2.0189931392669678,
      "learning_rate": 2.66862542065752e-05,
      "loss": 1.3341,
      "step": 4370
    },
    {
      "epoch": 0.33927845233253934,
      "grad_norm": 1.630266547203064,
      "learning_rate": 2.667848822158944e-05,
      "loss": 1.3941,
      "step": 4380
    },
    {
      "epoch": 0.3400530606711981,
      "grad_norm": 1.7800084352493286,
      "learning_rate": 2.6670722236603677e-05,
      "loss": 1.3102,
      "step": 4390
    },
    {
      "epoch": 0.3408276690098569,
      "grad_norm": 1.742933988571167,
      "learning_rate": 2.6662956251617915e-05,
      "loss": 1.3115,
      "step": 4400
    },
    {
      "epoch": 0.34160227734851567,
      "grad_norm": 1.7509547472000122,
      "learning_rate": 2.6655190266632153e-05,
      "loss": 1.3911,
      "step": 4410
    },
    {
      "epoch": 0.34237688568717445,
      "grad_norm": 2.9474520683288574,
      "learning_rate": 2.664742428164639e-05,
      "loss": 1.2833,
      "step": 4420
    },
    {
      "epoch": 0.34315149402583317,
      "grad_norm": 1.8555290699005127,
      "learning_rate": 2.663965829666063e-05,
      "loss": 1.2866,
      "step": 4430
    },
    {
      "epoch": 0.34392610236449195,
      "grad_norm": 1.971957802772522,
      "learning_rate": 2.6631892311674866e-05,
      "loss": 1.4384,
      "step": 4440
    },
    {
      "epoch": 0.3447007107031507,
      "grad_norm": 1.9245022535324097,
      "learning_rate": 2.66241263266891e-05,
      "loss": 1.3762,
      "step": 4450
    },
    {
      "epoch": 0.3454753190418095,
      "grad_norm": 3.507359504699707,
      "learning_rate": 2.661636034170334e-05,
      "loss": 1.3661,
      "step": 4460
    },
    {
      "epoch": 0.3462499273804682,
      "grad_norm": 1.8368874788284302,
      "learning_rate": 2.6608594356717577e-05,
      "loss": 1.4649,
      "step": 4470
    },
    {
      "epoch": 0.347024535719127,
      "grad_norm": 2.5284838676452637,
      "learning_rate": 2.6600828371731814e-05,
      "loss": 1.4777,
      "step": 4480
    },
    {
      "epoch": 0.3477991440577858,
      "grad_norm": 1.9615252017974854,
      "learning_rate": 2.6593062386746052e-05,
      "loss": 1.4464,
      "step": 4490
    },
    {
      "epoch": 0.34857375239644456,
      "grad_norm": 2.045970916748047,
      "learning_rate": 2.658529640176029e-05,
      "loss": 1.3832,
      "step": 4500
    },
    {
      "epoch": 0.34934836073510334,
      "grad_norm": 2.467158555984497,
      "learning_rate": 2.6577530416774528e-05,
      "loss": 1.3665,
      "step": 4510
    },
    {
      "epoch": 0.35012296907376206,
      "grad_norm": 2.643355369567871,
      "learning_rate": 2.6569764431788766e-05,
      "loss": 1.3771,
      "step": 4520
    },
    {
      "epoch": 0.35089757741242084,
      "grad_norm": 2.681931495666504,
      "learning_rate": 2.6561998446803e-05,
      "loss": 1.286,
      "step": 4530
    },
    {
      "epoch": 0.3516721857510796,
      "grad_norm": 1.9315794706344604,
      "learning_rate": 2.655423246181724e-05,
      "loss": 1.4157,
      "step": 4540
    },
    {
      "epoch": 0.3524467940897384,
      "grad_norm": 2.0505900382995605,
      "learning_rate": 2.654646647683148e-05,
      "loss": 1.3698,
      "step": 4550
    },
    {
      "epoch": 0.35322140242839717,
      "grad_norm": 2.591069221496582,
      "learning_rate": 2.6538700491845717e-05,
      "loss": 1.3498,
      "step": 4560
    },
    {
      "epoch": 0.3539960107670559,
      "grad_norm": 1.792336344718933,
      "learning_rate": 2.6530934506859955e-05,
      "loss": 1.3444,
      "step": 4570
    },
    {
      "epoch": 0.35477061910571467,
      "grad_norm": 1.9714194536209106,
      "learning_rate": 2.6523168521874193e-05,
      "loss": 1.3616,
      "step": 4580
    },
    {
      "epoch": 0.35554522744437345,
      "grad_norm": 1.9447650909423828,
      "learning_rate": 2.651540253688843e-05,
      "loss": 1.4286,
      "step": 4590
    },
    {
      "epoch": 0.3563198357830322,
      "grad_norm": 2.812624454498291,
      "learning_rate": 2.650763655190267e-05,
      "loss": 1.337,
      "step": 4600
    },
    {
      "epoch": 0.35709444412169095,
      "grad_norm": 2.151063919067383,
      "learning_rate": 2.6499870566916903e-05,
      "loss": 1.292,
      "step": 4610
    },
    {
      "epoch": 0.3578690524603497,
      "grad_norm": 1.7481205463409424,
      "learning_rate": 2.649210458193114e-05,
      "loss": 1.3571,
      "step": 4620
    },
    {
      "epoch": 0.3586436607990085,
      "grad_norm": 2.8498308658599854,
      "learning_rate": 2.648433859694538e-05,
      "loss": 1.3493,
      "step": 4630
    },
    {
      "epoch": 0.3594182691376673,
      "grad_norm": 2.005898952484131,
      "learning_rate": 2.6476572611959617e-05,
      "loss": 1.2736,
      "step": 4640
    },
    {
      "epoch": 0.36019287747632606,
      "grad_norm": 2.1880481243133545,
      "learning_rate": 2.6468806626973855e-05,
      "loss": 1.416,
      "step": 4650
    },
    {
      "epoch": 0.3609674858149848,
      "grad_norm": 2.2911734580993652,
      "learning_rate": 2.6461040641988093e-05,
      "loss": 1.2824,
      "step": 4660
    },
    {
      "epoch": 0.36174209415364356,
      "grad_norm": 1.9683092832565308,
      "learning_rate": 2.645327465700233e-05,
      "loss": 1.4721,
      "step": 4670
    },
    {
      "epoch": 0.36251670249230233,
      "grad_norm": 1.902295470237732,
      "learning_rate": 2.644550867201657e-05,
      "loss": 1.4128,
      "step": 4680
    },
    {
      "epoch": 0.3632913108309611,
      "grad_norm": 2.8161323070526123,
      "learning_rate": 2.6437742687030806e-05,
      "loss": 1.4054,
      "step": 4690
    },
    {
      "epoch": 0.36406591916961983,
      "grad_norm": 2.0731866359710693,
      "learning_rate": 2.642997670204504e-05,
      "loss": 1.3379,
      "step": 4700
    },
    {
      "epoch": 0.3648405275082786,
      "grad_norm": 1.9509422779083252,
      "learning_rate": 2.642221071705928e-05,
      "loss": 1.3209,
      "step": 4710
    },
    {
      "epoch": 0.3656151358469374,
      "grad_norm": 1.5624829530715942,
      "learning_rate": 2.6414444732073516e-05,
      "loss": 1.4757,
      "step": 4720
    },
    {
      "epoch": 0.36638974418559617,
      "grad_norm": 1.953439712524414,
      "learning_rate": 2.6406678747087754e-05,
      "loss": 1.4302,
      "step": 4730
    },
    {
      "epoch": 0.36716435252425494,
      "grad_norm": 1.8021926879882812,
      "learning_rate": 2.6398912762101996e-05,
      "loss": 1.3993,
      "step": 4740
    },
    {
      "epoch": 0.36793896086291367,
      "grad_norm": 2.3871006965637207,
      "learning_rate": 2.6391146777116233e-05,
      "loss": 1.5506,
      "step": 4750
    },
    {
      "epoch": 0.36871356920157244,
      "grad_norm": 3.327237367630005,
      "learning_rate": 2.638338079213047e-05,
      "loss": 1.3464,
      "step": 4760
    },
    {
      "epoch": 0.3694881775402312,
      "grad_norm": 2.2690606117248535,
      "learning_rate": 2.637561480714471e-05,
      "loss": 1.3596,
      "step": 4770
    },
    {
      "epoch": 0.37026278587889,
      "grad_norm": 1.8783061504364014,
      "learning_rate": 2.6367848822158944e-05,
      "loss": 1.3384,
      "step": 4780
    },
    {
      "epoch": 0.3710373942175488,
      "grad_norm": 1.6364574432373047,
      "learning_rate": 2.636008283717318e-05,
      "loss": 1.3738,
      "step": 4790
    },
    {
      "epoch": 0.3718120025562075,
      "grad_norm": 2.317172050476074,
      "learning_rate": 2.635231685218742e-05,
      "loss": 1.3269,
      "step": 4800
    },
    {
      "epoch": 0.3725866108948663,
      "grad_norm": 2.105506420135498,
      "learning_rate": 2.6344550867201657e-05,
      "loss": 1.4577,
      "step": 4810
    },
    {
      "epoch": 0.37336121923352505,
      "grad_norm": 2.0992796421051025,
      "learning_rate": 2.6336784882215895e-05,
      "loss": 1.2996,
      "step": 4820
    },
    {
      "epoch": 0.37413582757218383,
      "grad_norm": 1.9937677383422852,
      "learning_rate": 2.6329018897230133e-05,
      "loss": 1.3294,
      "step": 4830
    },
    {
      "epoch": 0.37491043591084255,
      "grad_norm": 2.275402307510376,
      "learning_rate": 2.632125291224437e-05,
      "loss": 1.4015,
      "step": 4840
    },
    {
      "epoch": 0.37568504424950133,
      "grad_norm": 1.9582152366638184,
      "learning_rate": 2.631348692725861e-05,
      "loss": 1.4241,
      "step": 4850
    },
    {
      "epoch": 0.3764596525881601,
      "grad_norm": 2.159592628479004,
      "learning_rate": 2.6305720942272847e-05,
      "loss": 1.4172,
      "step": 4860
    },
    {
      "epoch": 0.3772342609268189,
      "grad_norm": 2.346510410308838,
      "learning_rate": 2.629795495728708e-05,
      "loss": 1.3775,
      "step": 4870
    },
    {
      "epoch": 0.37800886926547766,
      "grad_norm": 2.5311472415924072,
      "learning_rate": 2.629018897230132e-05,
      "loss": 1.3969,
      "step": 4880
    },
    {
      "epoch": 0.3787834776041364,
      "grad_norm": 1.9197540283203125,
      "learning_rate": 2.6282422987315557e-05,
      "loss": 1.3569,
      "step": 4890
    },
    {
      "epoch": 0.37955808594279516,
      "grad_norm": 2.6910970211029053,
      "learning_rate": 2.6274657002329795e-05,
      "loss": 1.344,
      "step": 4900
    },
    {
      "epoch": 0.38033269428145394,
      "grad_norm": 2.2096593379974365,
      "learning_rate": 2.6266891017344033e-05,
      "loss": 1.2959,
      "step": 4910
    },
    {
      "epoch": 0.3811073026201127,
      "grad_norm": 2.3961105346679688,
      "learning_rate": 2.625912503235827e-05,
      "loss": 1.3917,
      "step": 4920
    },
    {
      "epoch": 0.3818819109587715,
      "grad_norm": 2.1367294788360596,
      "learning_rate": 2.625135904737251e-05,
      "loss": 1.4659,
      "step": 4930
    },
    {
      "epoch": 0.3826565192974302,
      "grad_norm": 2.147061824798584,
      "learning_rate": 2.624359306238675e-05,
      "loss": 1.3517,
      "step": 4940
    },
    {
      "epoch": 0.383431127636089,
      "grad_norm": 1.6986231803894043,
      "learning_rate": 2.6235827077400984e-05,
      "loss": 1.318,
      "step": 4950
    },
    {
      "epoch": 0.3842057359747478,
      "grad_norm": 1.8605889081954956,
      "learning_rate": 2.6228061092415222e-05,
      "loss": 1.4312,
      "step": 4960
    },
    {
      "epoch": 0.38498034431340655,
      "grad_norm": 2.0025269985198975,
      "learning_rate": 2.622029510742946e-05,
      "loss": 1.2837,
      "step": 4970
    },
    {
      "epoch": 0.3857549526520653,
      "grad_norm": 2.0191476345062256,
      "learning_rate": 2.6212529122443698e-05,
      "loss": 1.4498,
      "step": 4980
    },
    {
      "epoch": 0.38652956099072405,
      "grad_norm": 2.4077811241149902,
      "learning_rate": 2.6204763137457935e-05,
      "loss": 1.3582,
      "step": 4990
    },
    {
      "epoch": 0.38730416932938283,
      "grad_norm": 1.7235230207443237,
      "learning_rate": 2.6196997152472173e-05,
      "loss": 1.3329,
      "step": 5000
    },
    {
      "epoch": 0.3880787776680416,
      "grad_norm": 1.589080810546875,
      "learning_rate": 2.618923116748641e-05,
      "loss": 1.3936,
      "step": 5010
    },
    {
      "epoch": 0.3888533860067004,
      "grad_norm": 2.0925254821777344,
      "learning_rate": 2.618146518250065e-05,
      "loss": 1.2517,
      "step": 5020
    },
    {
      "epoch": 0.3896279943453591,
      "grad_norm": 2.0256898403167725,
      "learning_rate": 2.6173699197514887e-05,
      "loss": 1.3968,
      "step": 5030
    },
    {
      "epoch": 0.3904026026840179,
      "grad_norm": 1.4868162870407104,
      "learning_rate": 2.616593321252912e-05,
      "loss": 1.2969,
      "step": 5040
    },
    {
      "epoch": 0.39117721102267666,
      "grad_norm": 2.9685306549072266,
      "learning_rate": 2.615816722754336e-05,
      "loss": 1.3262,
      "step": 5050
    },
    {
      "epoch": 0.39195181936133544,
      "grad_norm": 1.7751504182815552,
      "learning_rate": 2.6150401242557597e-05,
      "loss": 1.362,
      "step": 5060
    },
    {
      "epoch": 0.39272642769999416,
      "grad_norm": 1.6565310955047607,
      "learning_rate": 2.6142635257571835e-05,
      "loss": 1.3262,
      "step": 5070
    },
    {
      "epoch": 0.39350103603865294,
      "grad_norm": 2.5907087326049805,
      "learning_rate": 2.6134869272586073e-05,
      "loss": 1.3527,
      "step": 5080
    },
    {
      "epoch": 0.3942756443773117,
      "grad_norm": 2.252542495727539,
      "learning_rate": 2.612710328760031e-05,
      "loss": 1.3457,
      "step": 5090
    },
    {
      "epoch": 0.3950502527159705,
      "grad_norm": 1.7380225658416748,
      "learning_rate": 2.611933730261455e-05,
      "loss": 1.3736,
      "step": 5100
    },
    {
      "epoch": 0.3958248610546293,
      "grad_norm": 1.9707502126693726,
      "learning_rate": 2.6111571317628786e-05,
      "loss": 1.3788,
      "step": 5110
    },
    {
      "epoch": 0.396599469393288,
      "grad_norm": 2.1036009788513184,
      "learning_rate": 2.610380533264302e-05,
      "loss": 1.2885,
      "step": 5120
    },
    {
      "epoch": 0.3973740777319468,
      "grad_norm": 1.8001786470413208,
      "learning_rate": 2.6096039347657262e-05,
      "loss": 1.3065,
      "step": 5130
    },
    {
      "epoch": 0.39814868607060555,
      "grad_norm": 2.3362107276916504,
      "learning_rate": 2.60882733626715e-05,
      "loss": 1.302,
      "step": 5140
    },
    {
      "epoch": 0.39892329440926433,
      "grad_norm": 1.977236032485962,
      "learning_rate": 2.6080507377685738e-05,
      "loss": 1.3948,
      "step": 5150
    },
    {
      "epoch": 0.3996979027479231,
      "grad_norm": 1.6904057264328003,
      "learning_rate": 2.6072741392699976e-05,
      "loss": 1.381,
      "step": 5160
    },
    {
      "epoch": 0.40047251108658183,
      "grad_norm": 1.9243884086608887,
      "learning_rate": 2.6064975407714214e-05,
      "loss": 1.3191,
      "step": 5170
    },
    {
      "epoch": 0.4012471194252406,
      "grad_norm": 1.8192312717437744,
      "learning_rate": 2.605720942272845e-05,
      "loss": 1.4222,
      "step": 5180
    },
    {
      "epoch": 0.4020217277638994,
      "grad_norm": 2.4065909385681152,
      "learning_rate": 2.604944343774269e-05,
      "loss": 1.3398,
      "step": 5190
    },
    {
      "epoch": 0.40279633610255816,
      "grad_norm": 1.7417118549346924,
      "learning_rate": 2.6041677452756927e-05,
      "loss": 1.3848,
      "step": 5200
    },
    {
      "epoch": 0.4035709444412169,
      "grad_norm": 2.2531909942626953,
      "learning_rate": 2.6033911467771162e-05,
      "loss": 1.325,
      "step": 5210
    },
    {
      "epoch": 0.40434555277987566,
      "grad_norm": 2.133880853652954,
      "learning_rate": 2.60261454827854e-05,
      "loss": 1.3341,
      "step": 5220
    },
    {
      "epoch": 0.40512016111853444,
      "grad_norm": 2.118203639984131,
      "learning_rate": 2.6018379497799637e-05,
      "loss": 1.3357,
      "step": 5230
    },
    {
      "epoch": 0.4058947694571932,
      "grad_norm": 1.8258588314056396,
      "learning_rate": 2.6010613512813875e-05,
      "loss": 1.3701,
      "step": 5240
    },
    {
      "epoch": 0.406669377795852,
      "grad_norm": 1.813340663909912,
      "learning_rate": 2.6002847527828113e-05,
      "loss": 1.4241,
      "step": 5250
    },
    {
      "epoch": 0.4074439861345107,
      "grad_norm": 2.2106752395629883,
      "learning_rate": 2.599508154284235e-05,
      "loss": 1.3713,
      "step": 5260
    },
    {
      "epoch": 0.4082185944731695,
      "grad_norm": 2.4660141468048096,
      "learning_rate": 2.598731555785659e-05,
      "loss": 1.3029,
      "step": 5270
    },
    {
      "epoch": 0.40899320281182827,
      "grad_norm": 1.7668174505233765,
      "learning_rate": 2.5979549572870827e-05,
      "loss": 1.3318,
      "step": 5280
    },
    {
      "epoch": 0.40976781115048705,
      "grad_norm": 1.8715672492980957,
      "learning_rate": 2.597178358788506e-05,
      "loss": 1.3649,
      "step": 5290
    },
    {
      "epoch": 0.4105424194891458,
      "grad_norm": 1.9840372800827026,
      "learning_rate": 2.59640176028993e-05,
      "loss": 1.3566,
      "step": 5300
    },
    {
      "epoch": 0.41131702782780455,
      "grad_norm": 2.0734026432037354,
      "learning_rate": 2.5956251617913537e-05,
      "loss": 1.4807,
      "step": 5310
    },
    {
      "epoch": 0.4120916361664633,
      "grad_norm": 2.2093114852905273,
      "learning_rate": 2.5948485632927778e-05,
      "loss": 1.4567,
      "step": 5320
    },
    {
      "epoch": 0.4128662445051221,
      "grad_norm": 2.619439125061035,
      "learning_rate": 2.5940719647942016e-05,
      "loss": 1.3125,
      "step": 5330
    },
    {
      "epoch": 0.4136408528437809,
      "grad_norm": 1.595595359802246,
      "learning_rate": 2.5932953662956254e-05,
      "loss": 1.2735,
      "step": 5340
    },
    {
      "epoch": 0.4144154611824396,
      "grad_norm": 2.4629132747650146,
      "learning_rate": 2.5925187677970492e-05,
      "loss": 1.3756,
      "step": 5350
    },
    {
      "epoch": 0.4151900695210984,
      "grad_norm": 1.9825339317321777,
      "learning_rate": 2.591742169298473e-05,
      "loss": 1.3748,
      "step": 5360
    },
    {
      "epoch": 0.41596467785975716,
      "grad_norm": 2.560952663421631,
      "learning_rate": 2.5909655707998968e-05,
      "loss": 1.4072,
      "step": 5370
    },
    {
      "epoch": 0.41673928619841594,
      "grad_norm": 2.4355223178863525,
      "learning_rate": 2.5901889723013202e-05,
      "loss": 1.3212,
      "step": 5380
    },
    {
      "epoch": 0.4175138945370747,
      "grad_norm": 2.0037901401519775,
      "learning_rate": 2.589412373802744e-05,
      "loss": 1.3434,
      "step": 5390
    },
    {
      "epoch": 0.41828850287573344,
      "grad_norm": 2.2283921241760254,
      "learning_rate": 2.5886357753041678e-05,
      "loss": 1.2857,
      "step": 5400
    },
    {
      "epoch": 0.4190631112143922,
      "grad_norm": 2.292635679244995,
      "learning_rate": 2.5878591768055916e-05,
      "loss": 1.3857,
      "step": 5410
    },
    {
      "epoch": 0.419837719553051,
      "grad_norm": 2.1885826587677,
      "learning_rate": 2.5870825783070154e-05,
      "loss": 1.3148,
      "step": 5420
    },
    {
      "epoch": 0.42061232789170977,
      "grad_norm": 1.9145792722702026,
      "learning_rate": 2.586305979808439e-05,
      "loss": 1.3327,
      "step": 5430
    },
    {
      "epoch": 0.42138693623036855,
      "grad_norm": 2.0888185501098633,
      "learning_rate": 2.585529381309863e-05,
      "loss": 1.3845,
      "step": 5440
    },
    {
      "epoch": 0.42216154456902727,
      "grad_norm": 1.7681875228881836,
      "learning_rate": 2.5847527828112867e-05,
      "loss": 1.3168,
      "step": 5450
    },
    {
      "epoch": 0.42293615290768605,
      "grad_norm": 2.9967589378356934,
      "learning_rate": 2.58397618431271e-05,
      "loss": 1.3162,
      "step": 5460
    },
    {
      "epoch": 0.4237107612463448,
      "grad_norm": 2.0552926063537598,
      "learning_rate": 2.583199585814134e-05,
      "loss": 1.2789,
      "step": 5470
    },
    {
      "epoch": 0.4244853695850036,
      "grad_norm": 2.0070924758911133,
      "learning_rate": 2.5824229873155577e-05,
      "loss": 1.4031,
      "step": 5480
    },
    {
      "epoch": 0.4252599779236623,
      "grad_norm": 2.026437520980835,
      "learning_rate": 2.5816463888169815e-05,
      "loss": 1.3378,
      "step": 5490
    },
    {
      "epoch": 0.4260345862623211,
      "grad_norm": 1.8102197647094727,
      "learning_rate": 2.5808697903184053e-05,
      "loss": 1.3228,
      "step": 5500
    },
    {
      "epoch": 0.4268091946009799,
      "grad_norm": 2.0346601009368896,
      "learning_rate": 2.580093191819829e-05,
      "loss": 1.4359,
      "step": 5510
    },
    {
      "epoch": 0.42758380293963866,
      "grad_norm": 2.708839178085327,
      "learning_rate": 2.5793165933212532e-05,
      "loss": 1.234,
      "step": 5520
    },
    {
      "epoch": 0.42835841127829744,
      "grad_norm": 2.2861075401306152,
      "learning_rate": 2.578539994822677e-05,
      "loss": 1.3621,
      "step": 5530
    },
    {
      "epoch": 0.42913301961695616,
      "grad_norm": 2.1977875232696533,
      "learning_rate": 2.5777633963241008e-05,
      "loss": 1.2442,
      "step": 5540
    },
    {
      "epoch": 0.42990762795561493,
      "grad_norm": 2.3993709087371826,
      "learning_rate": 2.5769867978255242e-05,
      "loss": 1.3358,
      "step": 5550
    },
    {
      "epoch": 0.4306822362942737,
      "grad_norm": 1.961517333984375,
      "learning_rate": 2.576210199326948e-05,
      "loss": 1.4731,
      "step": 5560
    },
    {
      "epoch": 0.4314568446329325,
      "grad_norm": 2.3862228393554688,
      "learning_rate": 2.5754336008283718e-05,
      "loss": 1.3559,
      "step": 5570
    },
    {
      "epoch": 0.4322314529715912,
      "grad_norm": 2.7380123138427734,
      "learning_rate": 2.5746570023297956e-05,
      "loss": 1.4079,
      "step": 5580
    },
    {
      "epoch": 0.43300606131025,
      "grad_norm": 1.7700397968292236,
      "learning_rate": 2.5738804038312194e-05,
      "loss": 1.4088,
      "step": 5590
    },
    {
      "epoch": 0.43378066964890877,
      "grad_norm": 1.7352429628372192,
      "learning_rate": 2.5731038053326432e-05,
      "loss": 1.4822,
      "step": 5600
    },
    {
      "epoch": 0.43455527798756755,
      "grad_norm": 2.1964380741119385,
      "learning_rate": 2.572327206834067e-05,
      "loss": 1.3131,
      "step": 5610
    },
    {
      "epoch": 0.4353298863262263,
      "grad_norm": 2.0487232208251953,
      "learning_rate": 2.5715506083354907e-05,
      "loss": 1.3811,
      "step": 5620
    },
    {
      "epoch": 0.43610449466488505,
      "grad_norm": 1.7966810464859009,
      "learning_rate": 2.5707740098369142e-05,
      "loss": 1.3917,
      "step": 5630
    },
    {
      "epoch": 0.4368791030035438,
      "grad_norm": 2.0208213329315186,
      "learning_rate": 2.569997411338338e-05,
      "loss": 1.3116,
      "step": 5640
    },
    {
      "epoch": 0.4376537113422026,
      "grad_norm": 2.433981418609619,
      "learning_rate": 2.5692208128397618e-05,
      "loss": 1.2472,
      "step": 5650
    },
    {
      "epoch": 0.4384283196808614,
      "grad_norm": 1.9710639715194702,
      "learning_rate": 2.5684442143411856e-05,
      "loss": 1.2811,
      "step": 5660
    },
    {
      "epoch": 0.43920292801952016,
      "grad_norm": 2.357933759689331,
      "learning_rate": 2.5676676158426093e-05,
      "loss": 1.3362,
      "step": 5670
    },
    {
      "epoch": 0.4399775363581789,
      "grad_norm": 1.740323543548584,
      "learning_rate": 2.566891017344033e-05,
      "loss": 1.3415,
      "step": 5680
    },
    {
      "epoch": 0.44075214469683766,
      "grad_norm": 1.874901294708252,
      "learning_rate": 2.566114418845457e-05,
      "loss": 1.3142,
      "step": 5690
    },
    {
      "epoch": 0.44152675303549643,
      "grad_norm": 2.4519574642181396,
      "learning_rate": 2.5653378203468807e-05,
      "loss": 1.4313,
      "step": 5700
    },
    {
      "epoch": 0.4423013613741552,
      "grad_norm": 1.9132261276245117,
      "learning_rate": 2.5645612218483045e-05,
      "loss": 1.3297,
      "step": 5710
    },
    {
      "epoch": 0.44307596971281393,
      "grad_norm": 2.3073487281799316,
      "learning_rate": 2.5637846233497283e-05,
      "loss": 1.3342,
      "step": 5720
    },
    {
      "epoch": 0.4438505780514727,
      "grad_norm": 2.036036729812622,
      "learning_rate": 2.563008024851152e-05,
      "loss": 1.2982,
      "step": 5730
    },
    {
      "epoch": 0.4446251863901315,
      "grad_norm": 3.1818857192993164,
      "learning_rate": 2.562231426352576e-05,
      "loss": 1.3518,
      "step": 5740
    },
    {
      "epoch": 0.44539979472879027,
      "grad_norm": 2.113186836242676,
      "learning_rate": 2.5614548278539996e-05,
      "loss": 1.2802,
      "step": 5750
    },
    {
      "epoch": 0.44617440306744904,
      "grad_norm": 2.7745981216430664,
      "learning_rate": 2.5606782293554234e-05,
      "loss": 1.345,
      "step": 5760
    },
    {
      "epoch": 0.44694901140610777,
      "grad_norm": 2.060807466506958,
      "learning_rate": 2.5599016308568472e-05,
      "loss": 1.3151,
      "step": 5770
    },
    {
      "epoch": 0.44772361974476654,
      "grad_norm": 1.7813583612442017,
      "learning_rate": 2.559125032358271e-05,
      "loss": 1.2949,
      "step": 5780
    },
    {
      "epoch": 0.4484982280834253,
      "grad_norm": 2.0125434398651123,
      "learning_rate": 2.5583484338596948e-05,
      "loss": 1.2958,
      "step": 5790
    },
    {
      "epoch": 0.4492728364220841,
      "grad_norm": 2.6355135440826416,
      "learning_rate": 2.5575718353611182e-05,
      "loss": 1.3409,
      "step": 5800
    },
    {
      "epoch": 0.4500474447607429,
      "grad_norm": 2.3318827152252197,
      "learning_rate": 2.556795236862542e-05,
      "loss": 1.3455,
      "step": 5810
    },
    {
      "epoch": 0.4508220530994016,
      "grad_norm": 2.160879611968994,
      "learning_rate": 2.5560186383639658e-05,
      "loss": 1.358,
      "step": 5820
    },
    {
      "epoch": 0.4515966614380604,
      "grad_norm": 2.5410823822021484,
      "learning_rate": 2.5552420398653896e-05,
      "loss": 1.3252,
      "step": 5830
    },
    {
      "epoch": 0.45237126977671915,
      "grad_norm": 1.9718068838119507,
      "learning_rate": 2.5544654413668134e-05,
      "loss": 1.4549,
      "step": 5840
    },
    {
      "epoch": 0.45314587811537793,
      "grad_norm": 2.1510403156280518,
      "learning_rate": 2.553688842868237e-05,
      "loss": 1.3207,
      "step": 5850
    },
    {
      "epoch": 0.45392048645403665,
      "grad_norm": 3.1765801906585693,
      "learning_rate": 2.552912244369661e-05,
      "loss": 1.2469,
      "step": 5860
    },
    {
      "epoch": 0.45469509479269543,
      "grad_norm": 2.2839884757995605,
      "learning_rate": 2.5521356458710847e-05,
      "loss": 1.3384,
      "step": 5870
    },
    {
      "epoch": 0.4554697031313542,
      "grad_norm": 2.2491350173950195,
      "learning_rate": 2.5513590473725085e-05,
      "loss": 1.2127,
      "step": 5880
    },
    {
      "epoch": 0.456244311470013,
      "grad_norm": 1.836147427558899,
      "learning_rate": 2.550582448873932e-05,
      "loss": 1.3536,
      "step": 5890
    },
    {
      "epoch": 0.45701891980867176,
      "grad_norm": 2.418487310409546,
      "learning_rate": 2.5498058503753558e-05,
      "loss": 1.3516,
      "step": 5900
    },
    {
      "epoch": 0.4577935281473305,
      "grad_norm": 2.035896062850952,
      "learning_rate": 2.54902925187678e-05,
      "loss": 1.3594,
      "step": 5910
    },
    {
      "epoch": 0.45856813648598926,
      "grad_norm": 2.11012601852417,
      "learning_rate": 2.5482526533782037e-05,
      "loss": 1.4908,
      "step": 5920
    },
    {
      "epoch": 0.45934274482464804,
      "grad_norm": 1.6971133947372437,
      "learning_rate": 2.5474760548796275e-05,
      "loss": 1.3498,
      "step": 5930
    },
    {
      "epoch": 0.4601173531633068,
      "grad_norm": 2.165112257003784,
      "learning_rate": 2.5466994563810512e-05,
      "loss": 1.3988,
      "step": 5940
    },
    {
      "epoch": 0.46089196150196554,
      "grad_norm": 2.068333864212036,
      "learning_rate": 2.545922857882475e-05,
      "loss": 1.3271,
      "step": 5950
    },
    {
      "epoch": 0.4616665698406243,
      "grad_norm": 2.44392728805542,
      "learning_rate": 2.5451462593838988e-05,
      "loss": 1.3739,
      "step": 5960
    },
    {
      "epoch": 0.4624411781792831,
      "grad_norm": 2.6103012561798096,
      "learning_rate": 2.5443696608853223e-05,
      "loss": 1.4386,
      "step": 5970
    },
    {
      "epoch": 0.4632157865179419,
      "grad_norm": 2.352457046508789,
      "learning_rate": 2.543593062386746e-05,
      "loss": 1.3778,
      "step": 5980
    },
    {
      "epoch": 0.46399039485660065,
      "grad_norm": 2.4222218990325928,
      "learning_rate": 2.54281646388817e-05,
      "loss": 1.3576,
      "step": 5990
    },
    {
      "epoch": 0.4647650031952594,
      "grad_norm": 2.0362579822540283,
      "learning_rate": 2.5420398653895936e-05,
      "loss": 1.2918,
      "step": 6000
    },
    {
      "epoch": 0.46553961153391815,
      "grad_norm": 2.5981860160827637,
      "learning_rate": 2.5412632668910174e-05,
      "loss": 1.3611,
      "step": 6010
    },
    {
      "epoch": 0.46631421987257693,
      "grad_norm": 1.9055839776992798,
      "learning_rate": 2.5404866683924412e-05,
      "loss": 1.3026,
      "step": 6020
    },
    {
      "epoch": 0.4670888282112357,
      "grad_norm": 2.1409220695495605,
      "learning_rate": 2.539710069893865e-05,
      "loss": 1.3563,
      "step": 6030
    },
    {
      "epoch": 0.4678634365498945,
      "grad_norm": 3.1316659450531006,
      "learning_rate": 2.5389334713952888e-05,
      "loss": 1.3904,
      "step": 6040
    },
    {
      "epoch": 0.4686380448885532,
      "grad_norm": 1.7926397323608398,
      "learning_rate": 2.5381568728967126e-05,
      "loss": 1.2789,
      "step": 6050
    },
    {
      "epoch": 0.469412653227212,
      "grad_norm": 2.601353883743286,
      "learning_rate": 2.537380274398136e-05,
      "loss": 1.3486,
      "step": 6060
    },
    {
      "epoch": 0.47018726156587076,
      "grad_norm": 1.9782849550247192,
      "learning_rate": 2.5366036758995598e-05,
      "loss": 1.3426,
      "step": 6070
    },
    {
      "epoch": 0.47096186990452954,
      "grad_norm": 2.5084378719329834,
      "learning_rate": 2.5358270774009836e-05,
      "loss": 1.344,
      "step": 6080
    },
    {
      "epoch": 0.47173647824318826,
      "grad_norm": 2.0932211875915527,
      "learning_rate": 2.5350504789024074e-05,
      "loss": 1.2999,
      "step": 6090
    },
    {
      "epoch": 0.47251108658184704,
      "grad_norm": 2.2356302738189697,
      "learning_rate": 2.534273880403831e-05,
      "loss": 1.2585,
      "step": 6100
    },
    {
      "epoch": 0.4732856949205058,
      "grad_norm": 2.008838653564453,
      "learning_rate": 2.5334972819052553e-05,
      "loss": 1.2991,
      "step": 6110
    },
    {
      "epoch": 0.4740603032591646,
      "grad_norm": 2.20740008354187,
      "learning_rate": 2.532720683406679e-05,
      "loss": 1.2682,
      "step": 6120
    },
    {
      "epoch": 0.4748349115978234,
      "grad_norm": 2.0481414794921875,
      "learning_rate": 2.531944084908103e-05,
      "loss": 1.3822,
      "step": 6130
    },
    {
      "epoch": 0.4756095199364821,
      "grad_norm": 2.772554874420166,
      "learning_rate": 2.5311674864095263e-05,
      "loss": 1.3382,
      "step": 6140
    },
    {
      "epoch": 0.4763841282751409,
      "grad_norm": 2.2620184421539307,
      "learning_rate": 2.53039088791095e-05,
      "loss": 1.3904,
      "step": 6150
    },
    {
      "epoch": 0.47715873661379965,
      "grad_norm": 2.6884872913360596,
      "learning_rate": 2.529614289412374e-05,
      "loss": 1.2505,
      "step": 6160
    },
    {
      "epoch": 0.4779333449524584,
      "grad_norm": 1.9192240238189697,
      "learning_rate": 2.5288376909137977e-05,
      "loss": 1.3802,
      "step": 6170
    },
    {
      "epoch": 0.4787079532911172,
      "grad_norm": 2.4386470317840576,
      "learning_rate": 2.5280610924152214e-05,
      "loss": 1.2464,
      "step": 6180
    },
    {
      "epoch": 0.4794825616297759,
      "grad_norm": 2.6017839908599854,
      "learning_rate": 2.5272844939166452e-05,
      "loss": 1.3543,
      "step": 6190
    },
    {
      "epoch": 0.4802571699684347,
      "grad_norm": 2.5975377559661865,
      "learning_rate": 2.526507895418069e-05,
      "loss": 1.3277,
      "step": 6200
    },
    {
      "epoch": 0.4810317783070935,
      "grad_norm": 2.2355074882507324,
      "learning_rate": 2.5257312969194928e-05,
      "loss": 1.4077,
      "step": 6210
    },
    {
      "epoch": 0.48180638664575226,
      "grad_norm": 2.0221831798553467,
      "learning_rate": 2.5249546984209163e-05,
      "loss": 1.353,
      "step": 6220
    },
    {
      "epoch": 0.482580994984411,
      "grad_norm": 1.8151130676269531,
      "learning_rate": 2.52417809992234e-05,
      "loss": 1.2647,
      "step": 6230
    },
    {
      "epoch": 0.48335560332306976,
      "grad_norm": 2.2739219665527344,
      "learning_rate": 2.5234015014237638e-05,
      "loss": 1.3346,
      "step": 6240
    },
    {
      "epoch": 0.48413021166172854,
      "grad_norm": 2.4229164123535156,
      "learning_rate": 2.5226249029251876e-05,
      "loss": 1.3057,
      "step": 6250
    },
    {
      "epoch": 0.4849048200003873,
      "grad_norm": 2.722470283508301,
      "learning_rate": 2.5218483044266114e-05,
      "loss": 1.4206,
      "step": 6260
    },
    {
      "epoch": 0.4856794283390461,
      "grad_norm": 2.573873519897461,
      "learning_rate": 2.5210717059280352e-05,
      "loss": 1.3534,
      "step": 6270
    },
    {
      "epoch": 0.4864540366777048,
      "grad_norm": 2.058461904525757,
      "learning_rate": 2.520295107429459e-05,
      "loss": 1.3035,
      "step": 6280
    },
    {
      "epoch": 0.4872286450163636,
      "grad_norm": 1.93390691280365,
      "learning_rate": 2.5195185089308828e-05,
      "loss": 1.3149,
      "step": 6290
    },
    {
      "epoch": 0.48800325335502237,
      "grad_norm": 2.26357364654541,
      "learning_rate": 2.518741910432307e-05,
      "loss": 1.3877,
      "step": 6300
    },
    {
      "epoch": 0.48877786169368115,
      "grad_norm": 2.3274879455566406,
      "learning_rate": 2.5179653119337303e-05,
      "loss": 1.3317,
      "step": 6310
    },
    {
      "epoch": 0.4895524700323399,
      "grad_norm": 1.8580491542816162,
      "learning_rate": 2.517188713435154e-05,
      "loss": 1.4001,
      "step": 6320
    },
    {
      "epoch": 0.49032707837099865,
      "grad_norm": 2.0165419578552246,
      "learning_rate": 2.516412114936578e-05,
      "loss": 1.358,
      "step": 6330
    },
    {
      "epoch": 0.4911016867096574,
      "grad_norm": 2.3426480293273926,
      "learning_rate": 2.5156355164380017e-05,
      "loss": 1.3675,
      "step": 6340
    },
    {
      "epoch": 0.4918762950483162,
      "grad_norm": 1.6950955390930176,
      "learning_rate": 2.5148589179394255e-05,
      "loss": 1.3379,
      "step": 6350
    },
    {
      "epoch": 0.492650903386975,
      "grad_norm": 2.450467824935913,
      "learning_rate": 2.5140823194408493e-05,
      "loss": 1.345,
      "step": 6360
    },
    {
      "epoch": 0.4934255117256337,
      "grad_norm": 2.249866247177124,
      "learning_rate": 2.513305720942273e-05,
      "loss": 1.2969,
      "step": 6370
    },
    {
      "epoch": 0.4942001200642925,
      "grad_norm": 1.6804324388504028,
      "learning_rate": 2.512529122443697e-05,
      "loss": 1.3227,
      "step": 6380
    },
    {
      "epoch": 0.49497472840295126,
      "grad_norm": 2.658407688140869,
      "learning_rate": 2.5117525239451203e-05,
      "loss": 1.3634,
      "step": 6390
    },
    {
      "epoch": 0.49574933674161004,
      "grad_norm": 1.8594245910644531,
      "learning_rate": 2.510975925446544e-05,
      "loss": 1.2958,
      "step": 6400
    },
    {
      "epoch": 0.4965239450802688,
      "grad_norm": 2.4144372940063477,
      "learning_rate": 2.510199326947968e-05,
      "loss": 1.2472,
      "step": 6410
    },
    {
      "epoch": 0.49729855341892754,
      "grad_norm": 2.189610004425049,
      "learning_rate": 2.5094227284493916e-05,
      "loss": 1.2614,
      "step": 6420
    },
    {
      "epoch": 0.4980731617575863,
      "grad_norm": 1.7475764751434326,
      "learning_rate": 2.5086461299508154e-05,
      "loss": 1.3376,
      "step": 6430
    },
    {
      "epoch": 0.4988477700962451,
      "grad_norm": 2.295107126235962,
      "learning_rate": 2.5078695314522392e-05,
      "loss": 1.3195,
      "step": 6440
    },
    {
      "epoch": 0.49962237843490387,
      "grad_norm": 2.4350850582122803,
      "learning_rate": 2.507092932953663e-05,
      "loss": 1.2424,
      "step": 6450
    },
    {
      "epoch": 0.5003969867735626,
      "grad_norm": 2.0347893238067627,
      "learning_rate": 2.5063163344550868e-05,
      "loss": 1.3545,
      "step": 6460
    },
    {
      "epoch": 0.5011715951122214,
      "grad_norm": 2.4333724975585938,
      "learning_rate": 2.5055397359565106e-05,
      "loss": 1.2943,
      "step": 6470
    },
    {
      "epoch": 0.5019462034508801,
      "grad_norm": 1.8944393396377563,
      "learning_rate": 2.504763137457934e-05,
      "loss": 1.3801,
      "step": 6480
    },
    {
      "epoch": 0.5027208117895389,
      "grad_norm": 2.2106220722198486,
      "learning_rate": 2.5039865389593578e-05,
      "loss": 1.336,
      "step": 6490
    },
    {
      "epoch": 0.5034954201281977,
      "grad_norm": 1.9660459756851196,
      "learning_rate": 2.503209940460782e-05,
      "loss": 1.4455,
      "step": 6500
    },
    {
      "epoch": 0.5042700284668564,
      "grad_norm": 2.1217124462127686,
      "learning_rate": 2.5024333419622057e-05,
      "loss": 1.3304,
      "step": 6510
    },
    {
      "epoch": 0.5050446368055153,
      "grad_norm": 2.1747324466705322,
      "learning_rate": 2.5016567434636295e-05,
      "loss": 1.3403,
      "step": 6520
    },
    {
      "epoch": 0.505819245144174,
      "grad_norm": 2.0808377265930176,
      "learning_rate": 2.5008801449650533e-05,
      "loss": 1.3422,
      "step": 6530
    },
    {
      "epoch": 0.5065938534828327,
      "grad_norm": 1.8674249649047852,
      "learning_rate": 2.500103546466477e-05,
      "loss": 1.3221,
      "step": 6540
    },
    {
      "epoch": 0.5073684618214915,
      "grad_norm": 2.1739068031311035,
      "learning_rate": 2.499326947967901e-05,
      "loss": 1.2976,
      "step": 6550
    },
    {
      "epoch": 0.5081430701601503,
      "grad_norm": 2.3463187217712402,
      "learning_rate": 2.4985503494693243e-05,
      "loss": 1.2025,
      "step": 6560
    },
    {
      "epoch": 0.5089176784988091,
      "grad_norm": 3.0146644115448,
      "learning_rate": 2.497773750970748e-05,
      "loss": 1.3958,
      "step": 6570
    },
    {
      "epoch": 0.5096922868374678,
      "grad_norm": 2.489541530609131,
      "learning_rate": 2.496997152472172e-05,
      "loss": 1.2749,
      "step": 6580
    },
    {
      "epoch": 0.5104668951761265,
      "grad_norm": 3.07153058052063,
      "learning_rate": 2.4962205539735957e-05,
      "loss": 1.2837,
      "step": 6590
    },
    {
      "epoch": 0.5112415035147854,
      "grad_norm": 2.6107163429260254,
      "learning_rate": 2.495521615324877e-05,
      "loss": 1.3946,
      "step": 6600
    },
    {
      "epoch": 0.5120161118534441,
      "grad_norm": 2.6386125087738037,
      "learning_rate": 2.4947450168263008e-05,
      "loss": 1.3841,
      "step": 6610
    },
    {
      "epoch": 0.5127907201921029,
      "grad_norm": 2.4759786128997803,
      "learning_rate": 2.4939684183277246e-05,
      "loss": 1.3022,
      "step": 6620
    },
    {
      "epoch": 0.5135653285307616,
      "grad_norm": 2.0523171424865723,
      "learning_rate": 2.4931918198291484e-05,
      "loss": 1.3571,
      "step": 6630
    },
    {
      "epoch": 0.5143399368694204,
      "grad_norm": 2.6654434204101562,
      "learning_rate": 2.492415221330572e-05,
      "loss": 1.3105,
      "step": 6640
    },
    {
      "epoch": 0.5151145452080792,
      "grad_norm": 2.179287910461426,
      "learning_rate": 2.491638622831996e-05,
      "loss": 1.3166,
      "step": 6650
    },
    {
      "epoch": 0.5158891535467379,
      "grad_norm": 2.257816791534424,
      "learning_rate": 2.4908620243334197e-05,
      "loss": 1.2928,
      "step": 6660
    },
    {
      "epoch": 0.5166637618853966,
      "grad_norm": 1.783775806427002,
      "learning_rate": 2.4900854258348435e-05,
      "loss": 1.3454,
      "step": 6670
    },
    {
      "epoch": 0.5174383702240555,
      "grad_norm": 1.9949030876159668,
      "learning_rate": 2.4893088273362673e-05,
      "loss": 1.4285,
      "step": 6680
    },
    {
      "epoch": 0.5182129785627142,
      "grad_norm": 2.334963798522949,
      "learning_rate": 2.488532228837691e-05,
      "loss": 1.2986,
      "step": 6690
    },
    {
      "epoch": 0.518987586901373,
      "grad_norm": 2.173985481262207,
      "learning_rate": 2.487755630339115e-05,
      "loss": 1.3695,
      "step": 6700
    },
    {
      "epoch": 0.5197621952400318,
      "grad_norm": 2.8138961791992188,
      "learning_rate": 2.4869790318405387e-05,
      "loss": 1.3133,
      "step": 6710
    },
    {
      "epoch": 0.5205368035786905,
      "grad_norm": 2.459113836288452,
      "learning_rate": 2.4862024333419625e-05,
      "loss": 1.2977,
      "step": 6720
    },
    {
      "epoch": 0.5213114119173493,
      "grad_norm": 2.075557231903076,
      "learning_rate": 2.485425834843386e-05,
      "loss": 1.4054,
      "step": 6730
    },
    {
      "epoch": 0.522086020256008,
      "grad_norm": 2.3087050914764404,
      "learning_rate": 2.4846492363448097e-05,
      "loss": 1.3507,
      "step": 6740
    },
    {
      "epoch": 0.5228606285946669,
      "grad_norm": 2.469907760620117,
      "learning_rate": 2.4838726378462335e-05,
      "loss": 1.359,
      "step": 6750
    },
    {
      "epoch": 0.5236352369333256,
      "grad_norm": 2.331515073776245,
      "learning_rate": 2.4830960393476573e-05,
      "loss": 1.2935,
      "step": 6760
    },
    {
      "epoch": 0.5244098452719843,
      "grad_norm": 2.3362293243408203,
      "learning_rate": 2.482319440849081e-05,
      "loss": 1.4352,
      "step": 6770
    },
    {
      "epoch": 0.5251844536106431,
      "grad_norm": 2.4957635402679443,
      "learning_rate": 2.481542842350505e-05,
      "loss": 1.2671,
      "step": 6780
    },
    {
      "epoch": 0.5259590619493019,
      "grad_norm": 3.1318602561950684,
      "learning_rate": 2.4807662438519286e-05,
      "loss": 1.46,
      "step": 6790
    },
    {
      "epoch": 0.5267336702879607,
      "grad_norm": 2.1587038040161133,
      "learning_rate": 2.4799896453533524e-05,
      "loss": 1.2941,
      "step": 6800
    },
    {
      "epoch": 0.5275082786266194,
      "grad_norm": 2.0939602851867676,
      "learning_rate": 2.479213046854776e-05,
      "loss": 1.3144,
      "step": 6810
    },
    {
      "epoch": 0.5282828869652781,
      "grad_norm": 2.132331371307373,
      "learning_rate": 2.4784364483561996e-05,
      "loss": 1.3334,
      "step": 6820
    },
    {
      "epoch": 0.529057495303937,
      "grad_norm": 1.7921127080917358,
      "learning_rate": 2.4776598498576238e-05,
      "loss": 1.3353,
      "step": 6830
    },
    {
      "epoch": 0.5298321036425957,
      "grad_norm": 1.8999168872833252,
      "learning_rate": 2.4768832513590476e-05,
      "loss": 1.3094,
      "step": 6840
    },
    {
      "epoch": 0.5306067119812545,
      "grad_norm": 2.2736430168151855,
      "learning_rate": 2.4761066528604713e-05,
      "loss": 1.4039,
      "step": 6850
    },
    {
      "epoch": 0.5313813203199133,
      "grad_norm": 1.9225285053253174,
      "learning_rate": 2.475330054361895e-05,
      "loss": 1.3188,
      "step": 6860
    },
    {
      "epoch": 0.532155928658572,
      "grad_norm": 2.1138527393341064,
      "learning_rate": 2.474553455863319e-05,
      "loss": 1.4014,
      "step": 6870
    },
    {
      "epoch": 0.5329305369972308,
      "grad_norm": 2.262077569961548,
      "learning_rate": 2.4737768573647427e-05,
      "loss": 1.2731,
      "step": 6880
    },
    {
      "epoch": 0.5337051453358895,
      "grad_norm": 1.7518669366836548,
      "learning_rate": 2.473000258866166e-05,
      "loss": 1.4191,
      "step": 6890
    },
    {
      "epoch": 0.5344797536745483,
      "grad_norm": 2.6178057193756104,
      "learning_rate": 2.47222366036759e-05,
      "loss": 1.2799,
      "step": 6900
    },
    {
      "epoch": 0.5352543620132071,
      "grad_norm": 2.4725911617279053,
      "learning_rate": 2.4714470618690137e-05,
      "loss": 1.3444,
      "step": 6910
    },
    {
      "epoch": 0.5360289703518658,
      "grad_norm": 2.203489303588867,
      "learning_rate": 2.4706704633704375e-05,
      "loss": 1.3506,
      "step": 6920
    },
    {
      "epoch": 0.5368035786905246,
      "grad_norm": 2.742150068283081,
      "learning_rate": 2.4698938648718613e-05,
      "loss": 1.2751,
      "step": 6930
    },
    {
      "epoch": 0.5375781870291834,
      "grad_norm": 2.3853790760040283,
      "learning_rate": 2.469117266373285e-05,
      "loss": 1.3054,
      "step": 6940
    },
    {
      "epoch": 0.5383527953678421,
      "grad_norm": 2.714136838912964,
      "learning_rate": 2.468340667874709e-05,
      "loss": 1.4395,
      "step": 6950
    },
    {
      "epoch": 0.5391274037065009,
      "grad_norm": 2.311898946762085,
      "learning_rate": 2.4675640693761327e-05,
      "loss": 1.3613,
      "step": 6960
    },
    {
      "epoch": 0.5399020120451596,
      "grad_norm": 2.501208543777466,
      "learning_rate": 2.4667874708775564e-05,
      "loss": 1.2605,
      "step": 6970
    },
    {
      "epoch": 0.5406766203838185,
      "grad_norm": 2.5929532051086426,
      "learning_rate": 2.46601087237898e-05,
      "loss": 1.2592,
      "step": 6980
    },
    {
      "epoch": 0.5414512287224772,
      "grad_norm": 4.106910705566406,
      "learning_rate": 2.4652342738804037e-05,
      "loss": 1.3141,
      "step": 6990
    },
    {
      "epoch": 0.5422258370611359,
      "grad_norm": 2.1705141067504883,
      "learning_rate": 2.4644576753818275e-05,
      "loss": 1.4018,
      "step": 7000
    },
    {
      "epoch": 0.5430004453997948,
      "grad_norm": 2.463623046875,
      "learning_rate": 2.4636810768832513e-05,
      "loss": 1.4563,
      "step": 7010
    },
    {
      "epoch": 0.5437750537384535,
      "grad_norm": 2.5263729095458984,
      "learning_rate": 2.462904478384675e-05,
      "loss": 1.3503,
      "step": 7020
    },
    {
      "epoch": 0.5445496620771123,
      "grad_norm": 3.222943067550659,
      "learning_rate": 2.462127879886099e-05,
      "loss": 1.2659,
      "step": 7030
    },
    {
      "epoch": 0.545324270415771,
      "grad_norm": 2.0167109966278076,
      "learning_rate": 2.461351281387523e-05,
      "loss": 1.4487,
      "step": 7040
    },
    {
      "epoch": 0.5460988787544298,
      "grad_norm": 2.550612449645996,
      "learning_rate": 2.4605746828889467e-05,
      "loss": 1.3391,
      "step": 7050
    },
    {
      "epoch": 0.5468734870930886,
      "grad_norm": 2.772655963897705,
      "learning_rate": 2.4597980843903702e-05,
      "loss": 1.4196,
      "step": 7060
    },
    {
      "epoch": 0.5476480954317473,
      "grad_norm": 2.244508743286133,
      "learning_rate": 2.459021485891794e-05,
      "loss": 1.3222,
      "step": 7070
    },
    {
      "epoch": 0.5484227037704061,
      "grad_norm": 2.1786158084869385,
      "learning_rate": 2.4582448873932178e-05,
      "loss": 1.2938,
      "step": 7080
    },
    {
      "epoch": 0.5491973121090649,
      "grad_norm": 1.9670354127883911,
      "learning_rate": 2.4574682888946415e-05,
      "loss": 1.3672,
      "step": 7090
    },
    {
      "epoch": 0.5499719204477236,
      "grad_norm": 4.0725507736206055,
      "learning_rate": 2.4566916903960653e-05,
      "loss": 1.155,
      "step": 7100
    },
    {
      "epoch": 0.5507465287863824,
      "grad_norm": 2.536170721054077,
      "learning_rate": 2.455915091897489e-05,
      "loss": 1.2878,
      "step": 7110
    },
    {
      "epoch": 0.5515211371250411,
      "grad_norm": 2.010587692260742,
      "learning_rate": 2.455138493398913e-05,
      "loss": 1.3287,
      "step": 7120
    },
    {
      "epoch": 0.5522957454637,
      "grad_norm": 2.489400625228882,
      "learning_rate": 2.4543618949003367e-05,
      "loss": 1.3183,
      "step": 7130
    },
    {
      "epoch": 0.5530703538023587,
      "grad_norm": 1.898979663848877,
      "learning_rate": 2.4535852964017605e-05,
      "loss": 1.2343,
      "step": 7140
    },
    {
      "epoch": 0.5538449621410174,
      "grad_norm": 1.7493165731430054,
      "learning_rate": 2.452808697903184e-05,
      "loss": 1.3759,
      "step": 7150
    },
    {
      "epoch": 0.5546195704796762,
      "grad_norm": 3.576183557510376,
      "learning_rate": 2.4520320994046077e-05,
      "loss": 1.2715,
      "step": 7160
    },
    {
      "epoch": 0.555394178818335,
      "grad_norm": 2.6007535457611084,
      "learning_rate": 2.4512555009060315e-05,
      "loss": 1.3555,
      "step": 7170
    },
    {
      "epoch": 0.5561687871569937,
      "grad_norm": 2.296112060546875,
      "learning_rate": 2.4504789024074553e-05,
      "loss": 1.2924,
      "step": 7180
    },
    {
      "epoch": 0.5569433954956525,
      "grad_norm": 1.8327884674072266,
      "learning_rate": 2.449702303908879e-05,
      "loss": 1.3156,
      "step": 7190
    },
    {
      "epoch": 0.5577180038343112,
      "grad_norm": 2.0858917236328125,
      "learning_rate": 2.448925705410303e-05,
      "loss": 1.324,
      "step": 7200
    },
    {
      "epoch": 0.5584926121729701,
      "grad_norm": 1.9049078226089478,
      "learning_rate": 2.4481491069117266e-05,
      "loss": 1.328,
      "step": 7210
    },
    {
      "epoch": 0.5592672205116288,
      "grad_norm": 2.2224936485290527,
      "learning_rate": 2.4473725084131504e-05,
      "loss": 1.3276,
      "step": 7220
    },
    {
      "epoch": 0.5600418288502875,
      "grad_norm": 2.7955801486968994,
      "learning_rate": 2.4465959099145742e-05,
      "loss": 1.3241,
      "step": 7230
    },
    {
      "epoch": 0.5608164371889464,
      "grad_norm": 2.21618914604187,
      "learning_rate": 2.445819311415998e-05,
      "loss": 1.2723,
      "step": 7240
    },
    {
      "epoch": 0.5615910455276051,
      "grad_norm": 2.629575729370117,
      "learning_rate": 2.4450427129174218e-05,
      "loss": 1.2474,
      "step": 7250
    },
    {
      "epoch": 0.5623656538662639,
      "grad_norm": 2.8455557823181152,
      "learning_rate": 2.4442661144188456e-05,
      "loss": 1.3105,
      "step": 7260
    },
    {
      "epoch": 0.5631402622049226,
      "grad_norm": 2.355790853500366,
      "learning_rate": 2.4434895159202694e-05,
      "loss": 1.2974,
      "step": 7270
    },
    {
      "epoch": 0.5639148705435814,
      "grad_norm": 2.4481735229492188,
      "learning_rate": 2.442712917421693e-05,
      "loss": 1.3758,
      "step": 7280
    },
    {
      "epoch": 0.5646894788822402,
      "grad_norm": 2.6228413581848145,
      "learning_rate": 2.441936318923117e-05,
      "loss": 1.3863,
      "step": 7290
    },
    {
      "epoch": 0.5654640872208989,
      "grad_norm": 2.3334362506866455,
      "learning_rate": 2.4411597204245407e-05,
      "loss": 1.2321,
      "step": 7300
    },
    {
      "epoch": 0.5662386955595577,
      "grad_norm": 2.353571891784668,
      "learning_rate": 2.4403831219259645e-05,
      "loss": 1.3949,
      "step": 7310
    },
    {
      "epoch": 0.5670133038982165,
      "grad_norm": 2.5879597663879395,
      "learning_rate": 2.439606523427388e-05,
      "loss": 1.3195,
      "step": 7320
    },
    {
      "epoch": 0.5677879122368752,
      "grad_norm": 1.999869704246521,
      "learning_rate": 2.4388299249288117e-05,
      "loss": 1.3006,
      "step": 7330
    },
    {
      "epoch": 0.568562520575534,
      "grad_norm": 2.9919614791870117,
      "learning_rate": 2.4380533264302355e-05,
      "loss": 1.3402,
      "step": 7340
    },
    {
      "epoch": 0.5693371289141927,
      "grad_norm": 3.23089599609375,
      "learning_rate": 2.4372767279316593e-05,
      "loss": 1.2317,
      "step": 7350
    },
    {
      "epoch": 0.5701117372528516,
      "grad_norm": 2.9640767574310303,
      "learning_rate": 2.436500129433083e-05,
      "loss": 1.3809,
      "step": 7360
    },
    {
      "epoch": 0.5708863455915103,
      "grad_norm": 2.7389893531799316,
      "learning_rate": 2.435723530934507e-05,
      "loss": 1.3329,
      "step": 7370
    },
    {
      "epoch": 0.571660953930169,
      "grad_norm": 2.239565372467041,
      "learning_rate": 2.4349469324359307e-05,
      "loss": 1.3331,
      "step": 7380
    },
    {
      "epoch": 0.5724355622688279,
      "grad_norm": 2.353203773498535,
      "learning_rate": 2.4341703339373545e-05,
      "loss": 1.3601,
      "step": 7390
    },
    {
      "epoch": 0.5732101706074866,
      "grad_norm": 2.4611406326293945,
      "learning_rate": 2.433393735438778e-05,
      "loss": 1.164,
      "step": 7400
    },
    {
      "epoch": 0.5739847789461453,
      "grad_norm": 2.219125270843506,
      "learning_rate": 2.4326171369402017e-05,
      "loss": 1.4228,
      "step": 7410
    },
    {
      "epoch": 0.5747593872848041,
      "grad_norm": 2.459637403488159,
      "learning_rate": 2.4318405384416258e-05,
      "loss": 1.3078,
      "step": 7420
    },
    {
      "epoch": 0.5755339956234629,
      "grad_norm": 2.392465353012085,
      "learning_rate": 2.4310639399430496e-05,
      "loss": 1.2869,
      "step": 7430
    },
    {
      "epoch": 0.5763086039621217,
      "grad_norm": 2.5185413360595703,
      "learning_rate": 2.4302873414444734e-05,
      "loss": 1.3043,
      "step": 7440
    },
    {
      "epoch": 0.5770832123007804,
      "grad_norm": 2.8056890964508057,
      "learning_rate": 2.4295107429458972e-05,
      "loss": 1.3624,
      "step": 7450
    },
    {
      "epoch": 0.5778578206394391,
      "grad_norm": 2.206220865249634,
      "learning_rate": 2.428734144447321e-05,
      "loss": 1.2576,
      "step": 7460
    },
    {
      "epoch": 0.578632428978098,
      "grad_norm": 2.3580057621002197,
      "learning_rate": 2.4279575459487448e-05,
      "loss": 1.2487,
      "step": 7470
    },
    {
      "epoch": 0.5794070373167567,
      "grad_norm": 2.427168130874634,
      "learning_rate": 2.4271809474501685e-05,
      "loss": 1.2415,
      "step": 7480
    },
    {
      "epoch": 0.5801816456554155,
      "grad_norm": 1.9208317995071411,
      "learning_rate": 2.426404348951592e-05,
      "loss": 1.4454,
      "step": 7490
    },
    {
      "epoch": 0.5809562539940742,
      "grad_norm": 2.3145089149475098,
      "learning_rate": 2.4256277504530158e-05,
      "loss": 1.2772,
      "step": 7500
    },
    {
      "epoch": 0.581730862332733,
      "grad_norm": 2.2798800468444824,
      "learning_rate": 2.4248511519544396e-05,
      "loss": 1.2293,
      "step": 7510
    },
    {
      "epoch": 0.5825054706713918,
      "grad_norm": 2.2500455379486084,
      "learning_rate": 2.4240745534558634e-05,
      "loss": 1.2879,
      "step": 7520
    },
    {
      "epoch": 0.5832800790100505,
      "grad_norm": 3.357776403427124,
      "learning_rate": 2.423297954957287e-05,
      "loss": 1.2436,
      "step": 7530
    },
    {
      "epoch": 0.5840546873487094,
      "grad_norm": 2.4499659538269043,
      "learning_rate": 2.422521356458711e-05,
      "loss": 1.1558,
      "step": 7540
    },
    {
      "epoch": 0.5848292956873681,
      "grad_norm": 2.2457892894744873,
      "learning_rate": 2.4217447579601347e-05,
      "loss": 1.2519,
      "step": 7550
    },
    {
      "epoch": 0.5856039040260268,
      "grad_norm": 2.721733808517456,
      "learning_rate": 2.4209681594615585e-05,
      "loss": 1.2202,
      "step": 7560
    },
    {
      "epoch": 0.5863785123646856,
      "grad_norm": 3.166006326675415,
      "learning_rate": 2.420191560962982e-05,
      "loss": 1.3628,
      "step": 7570
    },
    {
      "epoch": 0.5871531207033444,
      "grad_norm": 2.4414219856262207,
      "learning_rate": 2.4194149624644057e-05,
      "loss": 1.2331,
      "step": 7580
    },
    {
      "epoch": 0.5879277290420032,
      "grad_norm": 2.4372975826263428,
      "learning_rate": 2.4186383639658295e-05,
      "loss": 1.323,
      "step": 7590
    },
    {
      "epoch": 0.5887023373806619,
      "grad_norm": 2.6587393283843994,
      "learning_rate": 2.4178617654672533e-05,
      "loss": 1.3136,
      "step": 7600
    },
    {
      "epoch": 0.5894769457193206,
      "grad_norm": 1.6160699129104614,
      "learning_rate": 2.417085166968677e-05,
      "loss": 1.296,
      "step": 7610
    },
    {
      "epoch": 0.5902515540579795,
      "grad_norm": 3.0913965702056885,
      "learning_rate": 2.4163085684701012e-05,
      "loss": 1.2975,
      "step": 7620
    },
    {
      "epoch": 0.5910261623966382,
      "grad_norm": 2.405177354812622,
      "learning_rate": 2.415531969971525e-05,
      "loss": 1.3905,
      "step": 7630
    },
    {
      "epoch": 0.5918007707352969,
      "grad_norm": 2.6511197090148926,
      "learning_rate": 2.4147553714729488e-05,
      "loss": 1.2393,
      "step": 7640
    },
    {
      "epoch": 0.5925753790739557,
      "grad_norm": 2.1710267066955566,
      "learning_rate": 2.4139787729743726e-05,
      "loss": 1.2818,
      "step": 7650
    },
    {
      "epoch": 0.5933499874126145,
      "grad_norm": 2.4635133743286133,
      "learning_rate": 2.413202174475796e-05,
      "loss": 1.255,
      "step": 7660
    },
    {
      "epoch": 0.5941245957512733,
      "grad_norm": 3.100377321243286,
      "learning_rate": 2.4124255759772198e-05,
      "loss": 1.3022,
      "step": 7670
    },
    {
      "epoch": 0.594899204089932,
      "grad_norm": 2.072376012802124,
      "learning_rate": 2.4116489774786436e-05,
      "loss": 1.3153,
      "step": 7680
    },
    {
      "epoch": 0.5956738124285907,
      "grad_norm": 2.503443956375122,
      "learning_rate": 2.4108723789800674e-05,
      "loss": 1.2382,
      "step": 7690
    },
    {
      "epoch": 0.5964484207672496,
      "grad_norm": 2.035585403442383,
      "learning_rate": 2.4100957804814912e-05,
      "loss": 1.2913,
      "step": 7700
    },
    {
      "epoch": 0.5972230291059083,
      "grad_norm": 2.1056549549102783,
      "learning_rate": 2.409319181982915e-05,
      "loss": 1.3452,
      "step": 7710
    },
    {
      "epoch": 0.5979976374445671,
      "grad_norm": 3.001999616622925,
      "learning_rate": 2.4085425834843387e-05,
      "loss": 1.2644,
      "step": 7720
    },
    {
      "epoch": 0.5987722457832259,
      "grad_norm": 2.3085086345672607,
      "learning_rate": 2.4077659849857625e-05,
      "loss": 1.2939,
      "step": 7730
    },
    {
      "epoch": 0.5995468541218846,
      "grad_norm": 2.1445436477661133,
      "learning_rate": 2.406989386487186e-05,
      "loss": 1.2756,
      "step": 7740
    },
    {
      "epoch": 0.6003214624605434,
      "grad_norm": 2.2497947216033936,
      "learning_rate": 2.4062127879886098e-05,
      "loss": 1.2468,
      "step": 7750
    },
    {
      "epoch": 0.6010960707992021,
      "grad_norm": 3.351301908493042,
      "learning_rate": 2.4054361894900336e-05,
      "loss": 1.298,
      "step": 7760
    },
    {
      "epoch": 0.601870679137861,
      "grad_norm": 2.0941858291625977,
      "learning_rate": 2.4046595909914573e-05,
      "loss": 1.3301,
      "step": 7770
    },
    {
      "epoch": 0.6026452874765197,
      "grad_norm": 2.4272642135620117,
      "learning_rate": 2.403882992492881e-05,
      "loss": 1.2664,
      "step": 7780
    },
    {
      "epoch": 0.6034198958151784,
      "grad_norm": 2.7850139141082764,
      "learning_rate": 2.403106393994305e-05,
      "loss": 1.2896,
      "step": 7790
    },
    {
      "epoch": 0.6041945041538372,
      "grad_norm": 2.1432104110717773,
      "learning_rate": 2.4023297954957287e-05,
      "loss": 1.3915,
      "step": 7800
    },
    {
      "epoch": 0.604969112492496,
      "grad_norm": 2.4393253326416016,
      "learning_rate": 2.4015531969971525e-05,
      "loss": 1.309,
      "step": 7810
    },
    {
      "epoch": 0.6057437208311548,
      "grad_norm": 1.8159592151641846,
      "learning_rate": 2.4007765984985766e-05,
      "loss": 1.3425,
      "step": 7820
    },
    {
      "epoch": 0.6065183291698135,
      "grad_norm": 1.9236929416656494,
      "learning_rate": 2.4e-05,
      "loss": 1.3372,
      "step": 7830
    },
    {
      "epoch": 0.6072929375084722,
      "grad_norm": 2.3291139602661133,
      "learning_rate": 2.399223401501424e-05,
      "loss": 1.2555,
      "step": 7840
    },
    {
      "epoch": 0.6080675458471311,
      "grad_norm": 2.8252153396606445,
      "learning_rate": 2.3984468030028476e-05,
      "loss": 1.2704,
      "step": 7850
    },
    {
      "epoch": 0.6088421541857898,
      "grad_norm": 2.432481288909912,
      "learning_rate": 2.3976702045042714e-05,
      "loss": 1.2775,
      "step": 7860
    },
    {
      "epoch": 0.6096167625244486,
      "grad_norm": 2.4978432655334473,
      "learning_rate": 2.3968936060056952e-05,
      "loss": 1.2458,
      "step": 7870
    },
    {
      "epoch": 0.6103913708631074,
      "grad_norm": 1.8954613208770752,
      "learning_rate": 2.396117007507119e-05,
      "loss": 1.2879,
      "step": 7880
    },
    {
      "epoch": 0.6111659792017661,
      "grad_norm": 2.810861349105835,
      "learning_rate": 2.3953404090085428e-05,
      "loss": 1.283,
      "step": 7890
    },
    {
      "epoch": 0.6119405875404249,
      "grad_norm": 3.2008137702941895,
      "learning_rate": 2.3945638105099666e-05,
      "loss": 1.2932,
      "step": 7900
    },
    {
      "epoch": 0.6127151958790836,
      "grad_norm": 2.5535480976104736,
      "learning_rate": 2.39378721201139e-05,
      "loss": 1.3003,
      "step": 7910
    },
    {
      "epoch": 0.6134898042177424,
      "grad_norm": 2.8445069789886475,
      "learning_rate": 2.3930106135128138e-05,
      "loss": 1.2805,
      "step": 7920
    },
    {
      "epoch": 0.6142644125564012,
      "grad_norm": 2.056576728820801,
      "learning_rate": 2.3922340150142376e-05,
      "loss": 1.2638,
      "step": 7930
    },
    {
      "epoch": 0.6150390208950599,
      "grad_norm": 2.136296033859253,
      "learning_rate": 2.3914574165156614e-05,
      "loss": 1.3238,
      "step": 7940
    },
    {
      "epoch": 0.6158136292337187,
      "grad_norm": 2.606851100921631,
      "learning_rate": 2.390680818017085e-05,
      "loss": 1.3106,
      "step": 7950
    },
    {
      "epoch": 0.6165882375723775,
      "grad_norm": 2.3574087619781494,
      "learning_rate": 2.389904219518509e-05,
      "loss": 1.3314,
      "step": 7960
    },
    {
      "epoch": 0.6173628459110362,
      "grad_norm": 2.474916458129883,
      "learning_rate": 2.3891276210199327e-05,
      "loss": 1.3649,
      "step": 7970
    },
    {
      "epoch": 0.618137454249695,
      "grad_norm": 2.500783920288086,
      "learning_rate": 2.3883510225213565e-05,
      "loss": 1.3065,
      "step": 7980
    },
    {
      "epoch": 0.6189120625883537,
      "grad_norm": 1.9686874151229858,
      "learning_rate": 2.3875744240227803e-05,
      "loss": 1.4028,
      "step": 7990
    },
    {
      "epoch": 0.6196866709270126,
      "grad_norm": 2.0424060821533203,
      "learning_rate": 2.3867978255242038e-05,
      "loss": 1.3253,
      "step": 8000
    },
    {
      "epoch": 0.6204612792656713,
      "grad_norm": 2.272460699081421,
      "learning_rate": 2.386021227025628e-05,
      "loss": 1.2477,
      "step": 8010
    },
    {
      "epoch": 0.62123588760433,
      "grad_norm": 2.523674964904785,
      "learning_rate": 2.3852446285270517e-05,
      "loss": 1.2768,
      "step": 8020
    },
    {
      "epoch": 0.6220104959429889,
      "grad_norm": 3.1491684913635254,
      "learning_rate": 2.3844680300284755e-05,
      "loss": 1.3041,
      "step": 8030
    },
    {
      "epoch": 0.6227851042816476,
      "grad_norm": 1.8864723443984985,
      "learning_rate": 2.3836914315298992e-05,
      "loss": 1.2501,
      "step": 8040
    },
    {
      "epoch": 0.6235597126203064,
      "grad_norm": 3.6857078075408936,
      "learning_rate": 2.382914833031323e-05,
      "loss": 1.2121,
      "step": 8050
    },
    {
      "epoch": 0.6243343209589651,
      "grad_norm": 3.1318001747131348,
      "learning_rate": 2.3821382345327468e-05,
      "loss": 1.4265,
      "step": 8060
    },
    {
      "epoch": 0.6251089292976238,
      "grad_norm": 3.4608607292175293,
      "learning_rate": 2.3813616360341706e-05,
      "loss": 1.2662,
      "step": 8070
    },
    {
      "epoch": 0.6258835376362827,
      "grad_norm": 3.0567426681518555,
      "learning_rate": 2.380585037535594e-05,
      "loss": 1.2883,
      "step": 8080
    },
    {
      "epoch": 0.6266581459749414,
      "grad_norm": 3.801171064376831,
      "learning_rate": 2.379808439037018e-05,
      "loss": 1.2659,
      "step": 8090
    },
    {
      "epoch": 0.6274327543136002,
      "grad_norm": 2.0683348178863525,
      "learning_rate": 2.3790318405384416e-05,
      "loss": 1.286,
      "step": 8100
    },
    {
      "epoch": 0.628207362652259,
      "grad_norm": 2.3288538455963135,
      "learning_rate": 2.3782552420398654e-05,
      "loss": 1.2738,
      "step": 8110
    },
    {
      "epoch": 0.6289819709909177,
      "grad_norm": 1.937911033630371,
      "learning_rate": 2.3774786435412892e-05,
      "loss": 1.3539,
      "step": 8120
    },
    {
      "epoch": 0.6297565793295765,
      "grad_norm": 2.490565538406372,
      "learning_rate": 2.376702045042713e-05,
      "loss": 1.205,
      "step": 8130
    },
    {
      "epoch": 0.6305311876682352,
      "grad_norm": 2.5336930751800537,
      "learning_rate": 2.3759254465441368e-05,
      "loss": 1.2636,
      "step": 8140
    },
    {
      "epoch": 0.631305796006894,
      "grad_norm": 2.519601345062256,
      "learning_rate": 2.3751488480455606e-05,
      "loss": 1.2029,
      "step": 8150
    },
    {
      "epoch": 0.6320804043455528,
      "grad_norm": 3.2106785774230957,
      "learning_rate": 2.3743722495469843e-05,
      "loss": 1.2239,
      "step": 8160
    },
    {
      "epoch": 0.6328550126842115,
      "grad_norm": 2.6205732822418213,
      "learning_rate": 2.3735956510484078e-05,
      "loss": 1.3305,
      "step": 8170
    },
    {
      "epoch": 0.6336296210228703,
      "grad_norm": 2.8811192512512207,
      "learning_rate": 2.3728190525498316e-05,
      "loss": 1.1913,
      "step": 8180
    },
    {
      "epoch": 0.6344042293615291,
      "grad_norm": 2.490971565246582,
      "learning_rate": 2.3720424540512554e-05,
      "loss": 1.3144,
      "step": 8190
    },
    {
      "epoch": 0.6351788377001878,
      "grad_norm": 2.8547816276550293,
      "learning_rate": 2.371265855552679e-05,
      "loss": 1.3504,
      "step": 8200
    },
    {
      "epoch": 0.6359534460388466,
      "grad_norm": 2.402132749557495,
      "learning_rate": 2.3704892570541033e-05,
      "loss": 1.3497,
      "step": 8210
    },
    {
      "epoch": 0.6367280543775053,
      "grad_norm": 2.4205691814422607,
      "learning_rate": 2.369712658555527e-05,
      "loss": 1.3643,
      "step": 8220
    },
    {
      "epoch": 0.6375026627161642,
      "grad_norm": 1.938888430595398,
      "learning_rate": 2.368936060056951e-05,
      "loss": 1.3456,
      "step": 8230
    },
    {
      "epoch": 0.6382772710548229,
      "grad_norm": 2.1931445598602295,
      "learning_rate": 2.3681594615583746e-05,
      "loss": 1.3216,
      "step": 8240
    },
    {
      "epoch": 0.6390518793934816,
      "grad_norm": 2.7685251235961914,
      "learning_rate": 2.367382863059798e-05,
      "loss": 1.2905,
      "step": 8250
    },
    {
      "epoch": 0.6398264877321405,
      "grad_norm": 2.5473077297210693,
      "learning_rate": 2.366606264561222e-05,
      "loss": 1.3575,
      "step": 8260
    },
    {
      "epoch": 0.6406010960707992,
      "grad_norm": 2.5849883556365967,
      "learning_rate": 2.3658296660626457e-05,
      "loss": 1.2622,
      "step": 8270
    },
    {
      "epoch": 0.641375704409458,
      "grad_norm": 2.423401355743408,
      "learning_rate": 2.3650530675640694e-05,
      "loss": 1.3505,
      "step": 8280
    },
    {
      "epoch": 0.6421503127481167,
      "grad_norm": 2.7807979583740234,
      "learning_rate": 2.3642764690654932e-05,
      "loss": 1.2496,
      "step": 8290
    },
    {
      "epoch": 0.6429249210867755,
      "grad_norm": 2.594883680343628,
      "learning_rate": 2.363499870566917e-05,
      "loss": 1.29,
      "step": 8300
    },
    {
      "epoch": 0.6436995294254343,
      "grad_norm": 2.481123685836792,
      "learning_rate": 2.3627232720683408e-05,
      "loss": 1.3273,
      "step": 8310
    },
    {
      "epoch": 0.644474137764093,
      "grad_norm": 2.780766248703003,
      "learning_rate": 2.3619466735697646e-05,
      "loss": 1.2316,
      "step": 8320
    },
    {
      "epoch": 0.6452487461027518,
      "grad_norm": 3.379803419113159,
      "learning_rate": 2.3611700750711884e-05,
      "loss": 1.3219,
      "step": 8330
    },
    {
      "epoch": 0.6460233544414106,
      "grad_norm": 2.8670856952667236,
      "learning_rate": 2.3603934765726118e-05,
      "loss": 1.3002,
      "step": 8340
    },
    {
      "epoch": 0.6467979627800693,
      "grad_norm": 2.7012181282043457,
      "learning_rate": 2.3596168780740356e-05,
      "loss": 1.3879,
      "step": 8350
    },
    {
      "epoch": 0.6475725711187281,
      "grad_norm": 3.1145083904266357,
      "learning_rate": 2.3588402795754594e-05,
      "loss": 1.3232,
      "step": 8360
    },
    {
      "epoch": 0.6483471794573868,
      "grad_norm": 2.9477486610412598,
      "learning_rate": 2.3580636810768832e-05,
      "loss": 1.331,
      "step": 8370
    },
    {
      "epoch": 0.6491217877960457,
      "grad_norm": 2.3299992084503174,
      "learning_rate": 2.357287082578307e-05,
      "loss": 1.2494,
      "step": 8380
    },
    {
      "epoch": 0.6498963961347044,
      "grad_norm": 2.341097116470337,
      "learning_rate": 2.3565104840797308e-05,
      "loss": 1.2825,
      "step": 8390
    },
    {
      "epoch": 0.6506710044733631,
      "grad_norm": 2.070028066635132,
      "learning_rate": 2.355733885581155e-05,
      "loss": 1.2725,
      "step": 8400
    },
    {
      "epoch": 0.651445612812022,
      "grad_norm": 2.878375291824341,
      "learning_rate": 2.3549572870825787e-05,
      "loss": 1.3049,
      "step": 8410
    },
    {
      "epoch": 0.6522202211506807,
      "grad_norm": 2.6908302307128906,
      "learning_rate": 2.354180688584002e-05,
      "loss": 1.2074,
      "step": 8420
    },
    {
      "epoch": 0.6529948294893394,
      "grad_norm": 2.348602771759033,
      "learning_rate": 2.353404090085426e-05,
      "loss": 1.3379,
      "step": 8430
    },
    {
      "epoch": 0.6537694378279982,
      "grad_norm": 2.628628969192505,
      "learning_rate": 2.3526274915868497e-05,
      "loss": 1.3369,
      "step": 8440
    },
    {
      "epoch": 0.654544046166657,
      "grad_norm": 2.2942028045654297,
      "learning_rate": 2.3518508930882735e-05,
      "loss": 1.2666,
      "step": 8450
    },
    {
      "epoch": 0.6553186545053158,
      "grad_norm": 2.4761476516723633,
      "learning_rate": 2.3510742945896973e-05,
      "loss": 1.2585,
      "step": 8460
    },
    {
      "epoch": 0.6560932628439745,
      "grad_norm": 4.227903842926025,
      "learning_rate": 2.350297696091121e-05,
      "loss": 1.4074,
      "step": 8470
    },
    {
      "epoch": 0.6568678711826332,
      "grad_norm": 3.08499813079834,
      "learning_rate": 2.349521097592545e-05,
      "loss": 1.2755,
      "step": 8480
    },
    {
      "epoch": 0.6576424795212921,
      "grad_norm": 2.548185110092163,
      "learning_rate": 2.3487444990939686e-05,
      "loss": 1.245,
      "step": 8490
    },
    {
      "epoch": 0.6584170878599508,
      "grad_norm": 2.202484607696533,
      "learning_rate": 2.347967900595392e-05,
      "loss": 1.1866,
      "step": 8500
    },
    {
      "epoch": 0.6591916961986096,
      "grad_norm": 2.557821273803711,
      "learning_rate": 2.347191302096816e-05,
      "loss": 1.2364,
      "step": 8510
    },
    {
      "epoch": 0.6599663045372683,
      "grad_norm": 2.595573902130127,
      "learning_rate": 2.3464147035982396e-05,
      "loss": 1.3436,
      "step": 8520
    },
    {
      "epoch": 0.6607409128759271,
      "grad_norm": 2.825474500656128,
      "learning_rate": 2.3456381050996634e-05,
      "loss": 1.1875,
      "step": 8530
    },
    {
      "epoch": 0.6615155212145859,
      "grad_norm": 2.1778311729431152,
      "learning_rate": 2.3448615066010872e-05,
      "loss": 1.29,
      "step": 8540
    },
    {
      "epoch": 0.6622901295532446,
      "grad_norm": 2.424189567565918,
      "learning_rate": 2.344084908102511e-05,
      "loss": 1.3291,
      "step": 8550
    },
    {
      "epoch": 0.6630647378919035,
      "grad_norm": 2.0121402740478516,
      "learning_rate": 2.3433083096039348e-05,
      "loss": 1.2643,
      "step": 8560
    },
    {
      "epoch": 0.6638393462305622,
      "grad_norm": 2.4547061920166016,
      "learning_rate": 2.3425317111053586e-05,
      "loss": 1.3005,
      "step": 8570
    },
    {
      "epoch": 0.6646139545692209,
      "grad_norm": 3.0161805152893066,
      "learning_rate": 2.3417551126067824e-05,
      "loss": 1.3102,
      "step": 8580
    },
    {
      "epoch": 0.6653885629078797,
      "grad_norm": 2.515139102935791,
      "learning_rate": 2.3409785141082058e-05,
      "loss": 1.2714,
      "step": 8590
    },
    {
      "epoch": 0.6661631712465385,
      "grad_norm": 2.5061073303222656,
      "learning_rate": 2.34020191560963e-05,
      "loss": 1.3103,
      "step": 8600
    },
    {
      "epoch": 0.6669377795851973,
      "grad_norm": 2.4444003105163574,
      "learning_rate": 2.3394253171110537e-05,
      "loss": 1.2786,
      "step": 8610
    },
    {
      "epoch": 0.667712387923856,
      "grad_norm": 2.3205208778381348,
      "learning_rate": 2.3386487186124775e-05,
      "loss": 1.3551,
      "step": 8620
    },
    {
      "epoch": 0.6684869962625147,
      "grad_norm": 2.6274287700653076,
      "learning_rate": 2.3378721201139013e-05,
      "loss": 1.2155,
      "step": 8630
    },
    {
      "epoch": 0.6692616046011736,
      "grad_norm": 2.3723971843719482,
      "learning_rate": 2.337095521615325e-05,
      "loss": 1.2508,
      "step": 8640
    },
    {
      "epoch": 0.6700362129398323,
      "grad_norm": 2.222456216812134,
      "learning_rate": 2.336318923116749e-05,
      "loss": 1.2898,
      "step": 8650
    },
    {
      "epoch": 0.670810821278491,
      "grad_norm": 2.123591899871826,
      "learning_rate": 2.3355423246181727e-05,
      "loss": 1.3053,
      "step": 8660
    },
    {
      "epoch": 0.6715854296171498,
      "grad_norm": 2.907914638519287,
      "learning_rate": 2.334765726119596e-05,
      "loss": 1.3917,
      "step": 8670
    },
    {
      "epoch": 0.6723600379558086,
      "grad_norm": 2.32047176361084,
      "learning_rate": 2.33398912762102e-05,
      "loss": 1.273,
      "step": 8680
    },
    {
      "epoch": 0.6731346462944674,
      "grad_norm": 3.073145866394043,
      "learning_rate": 2.3332125291224437e-05,
      "loss": 1.2588,
      "step": 8690
    },
    {
      "epoch": 0.6739092546331261,
      "grad_norm": 2.2709500789642334,
      "learning_rate": 2.3324359306238675e-05,
      "loss": 1.2579,
      "step": 8700
    },
    {
      "epoch": 0.6746838629717848,
      "grad_norm": 2.725512981414795,
      "learning_rate": 2.3316593321252912e-05,
      "loss": 1.2641,
      "step": 8710
    },
    {
      "epoch": 0.6754584713104437,
      "grad_norm": 2.146599769592285,
      "learning_rate": 2.330882733626715e-05,
      "loss": 1.3258,
      "step": 8720
    },
    {
      "epoch": 0.6762330796491024,
      "grad_norm": 3.231240749359131,
      "learning_rate": 2.3301061351281388e-05,
      "loss": 1.3495,
      "step": 8730
    },
    {
      "epoch": 0.6770076879877612,
      "grad_norm": 2.864128351211548,
      "learning_rate": 2.3293295366295626e-05,
      "loss": 1.2622,
      "step": 8740
    },
    {
      "epoch": 0.67778229632642,
      "grad_norm": 2.994661331176758,
      "learning_rate": 2.3285529381309864e-05,
      "loss": 1.338,
      "step": 8750
    },
    {
      "epoch": 0.6785569046650787,
      "grad_norm": 2.3672335147857666,
      "learning_rate": 2.32777633963241e-05,
      "loss": 1.2927,
      "step": 8760
    },
    {
      "epoch": 0.6793315130037375,
      "grad_norm": 2.3428239822387695,
      "learning_rate": 2.3269997411338336e-05,
      "loss": 1.265,
      "step": 8770
    },
    {
      "epoch": 0.6801061213423962,
      "grad_norm": 3.3951313495635986,
      "learning_rate": 2.3262231426352574e-05,
      "loss": 1.3249,
      "step": 8780
    },
    {
      "epoch": 0.6808807296810551,
      "grad_norm": 2.512885093688965,
      "learning_rate": 2.3254465441366812e-05,
      "loss": 1.28,
      "step": 8790
    },
    {
      "epoch": 0.6816553380197138,
      "grad_norm": 2.962230920791626,
      "learning_rate": 2.3246699456381053e-05,
      "loss": 1.2621,
      "step": 8800
    },
    {
      "epoch": 0.6824299463583725,
      "grad_norm": 2.1705424785614014,
      "learning_rate": 2.323893347139529e-05,
      "loss": 1.3263,
      "step": 8810
    },
    {
      "epoch": 0.6832045546970313,
      "grad_norm": 2.336466073989868,
      "learning_rate": 2.323116748640953e-05,
      "loss": 1.2587,
      "step": 8820
    },
    {
      "epoch": 0.6839791630356901,
      "grad_norm": 2.421159267425537,
      "learning_rate": 2.3223401501423767e-05,
      "loss": 1.3472,
      "step": 8830
    },
    {
      "epoch": 0.6847537713743489,
      "grad_norm": 3.4472224712371826,
      "learning_rate": 2.3215635516438e-05,
      "loss": 1.2587,
      "step": 8840
    },
    {
      "epoch": 0.6855283797130076,
      "grad_norm": 2.8448634147644043,
      "learning_rate": 2.320786953145224e-05,
      "loss": 1.2988,
      "step": 8850
    },
    {
      "epoch": 0.6863029880516663,
      "grad_norm": 2.836644411087036,
      "learning_rate": 2.3200103546466477e-05,
      "loss": 1.3545,
      "step": 8860
    },
    {
      "epoch": 0.6870775963903252,
      "grad_norm": 2.390636682510376,
      "learning_rate": 2.3192337561480715e-05,
      "loss": 1.4234,
      "step": 8870
    },
    {
      "epoch": 0.6878522047289839,
      "grad_norm": 1.9167224168777466,
      "learning_rate": 2.3184571576494953e-05,
      "loss": 1.3014,
      "step": 8880
    },
    {
      "epoch": 0.6886268130676426,
      "grad_norm": 2.174694776535034,
      "learning_rate": 2.317680559150919e-05,
      "loss": 1.306,
      "step": 8890
    },
    {
      "epoch": 0.6894014214063015,
      "grad_norm": 2.500702381134033,
      "learning_rate": 2.316903960652343e-05,
      "loss": 1.1084,
      "step": 8900
    },
    {
      "epoch": 0.6901760297449602,
      "grad_norm": 2.3765954971313477,
      "learning_rate": 2.3161273621537666e-05,
      "loss": 1.3142,
      "step": 8910
    },
    {
      "epoch": 0.690950638083619,
      "grad_norm": 2.3192269802093506,
      "learning_rate": 2.3153507636551904e-05,
      "loss": 1.1591,
      "step": 8920
    },
    {
      "epoch": 0.6917252464222777,
      "grad_norm": 2.185208559036255,
      "learning_rate": 2.314574165156614e-05,
      "loss": 1.3389,
      "step": 8930
    },
    {
      "epoch": 0.6924998547609365,
      "grad_norm": 2.1541647911071777,
      "learning_rate": 2.3137975666580377e-05,
      "loss": 1.2676,
      "step": 8940
    },
    {
      "epoch": 0.6932744630995953,
      "grad_norm": 2.365835428237915,
      "learning_rate": 2.3130209681594615e-05,
      "loss": 1.3266,
      "step": 8950
    },
    {
      "epoch": 0.694049071438254,
      "grad_norm": 2.0289065837860107,
      "learning_rate": 2.3122443696608852e-05,
      "loss": 1.2959,
      "step": 8960
    },
    {
      "epoch": 0.6948236797769128,
      "grad_norm": 2.399242877960205,
      "learning_rate": 2.311467771162309e-05,
      "loss": 1.3257,
      "step": 8970
    },
    {
      "epoch": 0.6955982881155716,
      "grad_norm": 2.9556314945220947,
      "learning_rate": 2.3106911726637328e-05,
      "loss": 1.2349,
      "step": 8980
    },
    {
      "epoch": 0.6963728964542303,
      "grad_norm": 2.540473699569702,
      "learning_rate": 2.309914574165157e-05,
      "loss": 1.2878,
      "step": 8990
    },
    {
      "epoch": 0.6971475047928891,
      "grad_norm": 2.3633134365081787,
      "learning_rate": 2.3091379756665807e-05,
      "loss": 1.3585,
      "step": 9000
    },
    {
      "epoch": 0.6979221131315478,
      "grad_norm": 2.6956591606140137,
      "learning_rate": 2.3083613771680042e-05,
      "loss": 1.321,
      "step": 9010
    },
    {
      "epoch": 0.6986967214702067,
      "grad_norm": 2.9386518001556396,
      "learning_rate": 2.307584778669428e-05,
      "loss": 1.257,
      "step": 9020
    },
    {
      "epoch": 0.6994713298088654,
      "grad_norm": 2.381486415863037,
      "learning_rate": 2.3068081801708517e-05,
      "loss": 1.3203,
      "step": 9030
    },
    {
      "epoch": 0.7002459381475241,
      "grad_norm": 2.2185308933258057,
      "learning_rate": 2.3060315816722755e-05,
      "loss": 1.3819,
      "step": 9040
    },
    {
      "epoch": 0.701020546486183,
      "grad_norm": 2.0627784729003906,
      "learning_rate": 2.3052549831736993e-05,
      "loss": 1.3042,
      "step": 9050
    },
    {
      "epoch": 0.7017951548248417,
      "grad_norm": 2.256587266921997,
      "learning_rate": 2.304478384675123e-05,
      "loss": 1.2685,
      "step": 9060
    },
    {
      "epoch": 0.7025697631635005,
      "grad_norm": 2.725615978240967,
      "learning_rate": 2.303701786176547e-05,
      "loss": 1.2538,
      "step": 9070
    },
    {
      "epoch": 0.7033443715021592,
      "grad_norm": 2.4140045642852783,
      "learning_rate": 2.3029251876779707e-05,
      "loss": 1.1382,
      "step": 9080
    },
    {
      "epoch": 0.704118979840818,
      "grad_norm": 3.0054755210876465,
      "learning_rate": 2.3021485891793945e-05,
      "loss": 1.2991,
      "step": 9090
    },
    {
      "epoch": 0.7048935881794768,
      "grad_norm": 2.1822397708892822,
      "learning_rate": 2.301371990680818e-05,
      "loss": 1.2975,
      "step": 9100
    },
    {
      "epoch": 0.7056681965181355,
      "grad_norm": 1.997936725616455,
      "learning_rate": 2.3005953921822417e-05,
      "loss": 1.2419,
      "step": 9110
    },
    {
      "epoch": 0.7064428048567943,
      "grad_norm": 2.4174704551696777,
      "learning_rate": 2.2998187936836655e-05,
      "loss": 1.2684,
      "step": 9120
    },
    {
      "epoch": 0.7072174131954531,
      "grad_norm": 2.192700147628784,
      "learning_rate": 2.2990421951850893e-05,
      "loss": 1.301,
      "step": 9130
    },
    {
      "epoch": 0.7079920215341118,
      "grad_norm": 2.3639533519744873,
      "learning_rate": 2.298265596686513e-05,
      "loss": 1.3414,
      "step": 9140
    },
    {
      "epoch": 0.7087666298727706,
      "grad_norm": 2.5774521827697754,
      "learning_rate": 2.297488998187937e-05,
      "loss": 1.3128,
      "step": 9150
    },
    {
      "epoch": 0.7095412382114293,
      "grad_norm": 3.2461774349212646,
      "learning_rate": 2.2967123996893606e-05,
      "loss": 1.1488,
      "step": 9160
    },
    {
      "epoch": 0.7103158465500881,
      "grad_norm": 1.9696063995361328,
      "learning_rate": 2.2959358011907844e-05,
      "loss": 1.2568,
      "step": 9170
    },
    {
      "epoch": 0.7110904548887469,
      "grad_norm": 2.1048402786254883,
      "learning_rate": 2.295159202692208e-05,
      "loss": 1.2406,
      "step": 9180
    },
    {
      "epoch": 0.7118650632274056,
      "grad_norm": 2.5648581981658936,
      "learning_rate": 2.294382604193632e-05,
      "loss": 1.3192,
      "step": 9190
    },
    {
      "epoch": 0.7126396715660644,
      "grad_norm": 2.874624729156494,
      "learning_rate": 2.2936060056950558e-05,
      "loss": 1.1377,
      "step": 9200
    },
    {
      "epoch": 0.7134142799047232,
      "grad_norm": 3.078538656234741,
      "learning_rate": 2.2928294071964796e-05,
      "loss": 1.2136,
      "step": 9210
    },
    {
      "epoch": 0.7141888882433819,
      "grad_norm": 2.426241874694824,
      "learning_rate": 2.2920528086979034e-05,
      "loss": 1.1835,
      "step": 9220
    },
    {
      "epoch": 0.7149634965820407,
      "grad_norm": 2.2797884941101074,
      "learning_rate": 2.291276210199327e-05,
      "loss": 1.177,
      "step": 9230
    },
    {
      "epoch": 0.7157381049206994,
      "grad_norm": 1.984817624092102,
      "learning_rate": 2.290499611700751e-05,
      "loss": 1.2531,
      "step": 9240
    },
    {
      "epoch": 0.7165127132593583,
      "grad_norm": 2.59089732170105,
      "learning_rate": 2.2898006730520323e-05,
      "loss": 1.341,
      "step": 9250
    },
    {
      "epoch": 0.717287321598017,
      "grad_norm": 3.067678451538086,
      "learning_rate": 2.2890240745534557e-05,
      "loss": 1.2986,
      "step": 9260
    },
    {
      "epoch": 0.7180619299366757,
      "grad_norm": 2.7887210845947266,
      "learning_rate": 2.2882474760548795e-05,
      "loss": 1.2275,
      "step": 9270
    },
    {
      "epoch": 0.7188365382753346,
      "grad_norm": 2.2992260456085205,
      "learning_rate": 2.2874708775563033e-05,
      "loss": 1.183,
      "step": 9280
    },
    {
      "epoch": 0.7196111466139933,
      "grad_norm": 2.0623939037323,
      "learning_rate": 2.286694279057727e-05,
      "loss": 1.2594,
      "step": 9290
    },
    {
      "epoch": 0.7203857549526521,
      "grad_norm": 2.287417411804199,
      "learning_rate": 2.285917680559151e-05,
      "loss": 1.3855,
      "step": 9300
    },
    {
      "epoch": 0.7211603632913108,
      "grad_norm": 2.1726956367492676,
      "learning_rate": 2.2851410820605746e-05,
      "loss": 1.2826,
      "step": 9310
    },
    {
      "epoch": 0.7219349716299696,
      "grad_norm": 2.016849994659424,
      "learning_rate": 2.2843644835619984e-05,
      "loss": 1.2813,
      "step": 9320
    },
    {
      "epoch": 0.7227095799686284,
      "grad_norm": 2.7901172637939453,
      "learning_rate": 2.2835878850634226e-05,
      "loss": 1.2758,
      "step": 9330
    },
    {
      "epoch": 0.7234841883072871,
      "grad_norm": 2.6431355476379395,
      "learning_rate": 2.282811286564846e-05,
      "loss": 1.3165,
      "step": 9340
    },
    {
      "epoch": 0.724258796645946,
      "grad_norm": 1.8589402437210083,
      "learning_rate": 2.2820346880662698e-05,
      "loss": 1.3026,
      "step": 9350
    },
    {
      "epoch": 0.7250334049846047,
      "grad_norm": 2.254653215408325,
      "learning_rate": 2.2812580895676936e-05,
      "loss": 1.2704,
      "step": 9360
    },
    {
      "epoch": 0.7258080133232634,
      "grad_norm": 2.2473928928375244,
      "learning_rate": 2.2804814910691174e-05,
      "loss": 1.2357,
      "step": 9370
    },
    {
      "epoch": 0.7265826216619222,
      "grad_norm": 2.4999051094055176,
      "learning_rate": 2.279704892570541e-05,
      "loss": 1.2556,
      "step": 9380
    },
    {
      "epoch": 0.727357230000581,
      "grad_norm": 3.7250940799713135,
      "learning_rate": 2.278928294071965e-05,
      "loss": 1.2517,
      "step": 9390
    },
    {
      "epoch": 0.7281318383392397,
      "grad_norm": 3.4276111125946045,
      "learning_rate": 2.2781516955733887e-05,
      "loss": 1.3337,
      "step": 9400
    },
    {
      "epoch": 0.7289064466778985,
      "grad_norm": 2.3205177783966064,
      "learning_rate": 2.2773750970748125e-05,
      "loss": 1.272,
      "step": 9410
    },
    {
      "epoch": 0.7296810550165572,
      "grad_norm": 2.0711114406585693,
      "learning_rate": 2.2765984985762363e-05,
      "loss": 1.2621,
      "step": 9420
    },
    {
      "epoch": 0.7304556633552161,
      "grad_norm": 2.1037206649780273,
      "learning_rate": 2.2758219000776597e-05,
      "loss": 1.3416,
      "step": 9430
    },
    {
      "epoch": 0.7312302716938748,
      "grad_norm": 2.1836817264556885,
      "learning_rate": 2.2750453015790835e-05,
      "loss": 1.3355,
      "step": 9440
    },
    {
      "epoch": 0.7320048800325335,
      "grad_norm": 1.9860427379608154,
      "learning_rate": 2.2742687030805073e-05,
      "loss": 1.3638,
      "step": 9450
    },
    {
      "epoch": 0.7327794883711923,
      "grad_norm": 2.0100626945495605,
      "learning_rate": 2.273492104581931e-05,
      "loss": 1.2905,
      "step": 9460
    },
    {
      "epoch": 0.7335540967098511,
      "grad_norm": 2.1365108489990234,
      "learning_rate": 2.272715506083355e-05,
      "loss": 1.3624,
      "step": 9470
    },
    {
      "epoch": 0.7343287050485099,
      "grad_norm": 2.2819573879241943,
      "learning_rate": 2.2719389075847787e-05,
      "loss": 1.289,
      "step": 9480
    },
    {
      "epoch": 0.7351033133871686,
      "grad_norm": 2.2584986686706543,
      "learning_rate": 2.2711623090862025e-05,
      "loss": 1.2792,
      "step": 9490
    },
    {
      "epoch": 0.7358779217258273,
      "grad_norm": 3.0749521255493164,
      "learning_rate": 2.2703857105876263e-05,
      "loss": 1.3154,
      "step": 9500
    },
    {
      "epoch": 0.7366525300644862,
      "grad_norm": 2.962078809738159,
      "learning_rate": 2.2696091120890497e-05,
      "loss": 1.226,
      "step": 9510
    },
    {
      "epoch": 0.7374271384031449,
      "grad_norm": 2.8277175426483154,
      "learning_rate": 2.2688325135904738e-05,
      "loss": 1.409,
      "step": 9520
    },
    {
      "epoch": 0.7382017467418037,
      "grad_norm": 2.282010793685913,
      "learning_rate": 2.2680559150918976e-05,
      "loss": 1.2281,
      "step": 9530
    },
    {
      "epoch": 0.7389763550804624,
      "grad_norm": 2.3351519107818604,
      "learning_rate": 2.2672793165933214e-05,
      "loss": 1.2596,
      "step": 9540
    },
    {
      "epoch": 0.7397509634191212,
      "grad_norm": 2.3366754055023193,
      "learning_rate": 2.2665027180947452e-05,
      "loss": 1.2026,
      "step": 9550
    },
    {
      "epoch": 0.74052557175778,
      "grad_norm": 2.3232998847961426,
      "learning_rate": 2.265726119596169e-05,
      "loss": 1.4001,
      "step": 9560
    },
    {
      "epoch": 0.7413001800964387,
      "grad_norm": 2.6682322025299072,
      "learning_rate": 2.2649495210975928e-05,
      "loss": 1.2918,
      "step": 9570
    },
    {
      "epoch": 0.7420747884350976,
      "grad_norm": 2.5813939571380615,
      "learning_rate": 2.2641729225990165e-05,
      "loss": 1.2071,
      "step": 9580
    },
    {
      "epoch": 0.7428493967737563,
      "grad_norm": 2.241421699523926,
      "learning_rate": 2.2633963241004403e-05,
      "loss": 1.2168,
      "step": 9590
    },
    {
      "epoch": 0.743624005112415,
      "grad_norm": 2.7520089149475098,
      "learning_rate": 2.2626197256018638e-05,
      "loss": 1.3061,
      "step": 9600
    },
    {
      "epoch": 0.7443986134510738,
      "grad_norm": 3.546180486679077,
      "learning_rate": 2.2618431271032876e-05,
      "loss": 1.283,
      "step": 9610
    },
    {
      "epoch": 0.7451732217897326,
      "grad_norm": 2.885976552963257,
      "learning_rate": 2.2610665286047114e-05,
      "loss": 1.2505,
      "step": 9620
    },
    {
      "epoch": 0.7459478301283914,
      "grad_norm": 2.2933290004730225,
      "learning_rate": 2.260289930106135e-05,
      "loss": 1.2481,
      "step": 9630
    },
    {
      "epoch": 0.7467224384670501,
      "grad_norm": 2.070769786834717,
      "learning_rate": 2.259513331607559e-05,
      "loss": 1.2406,
      "step": 9640
    },
    {
      "epoch": 0.7474970468057088,
      "grad_norm": 2.8149325847625732,
      "learning_rate": 2.2587367331089827e-05,
      "loss": 1.2755,
      "step": 9650
    },
    {
      "epoch": 0.7482716551443677,
      "grad_norm": 2.925539970397949,
      "learning_rate": 2.2579601346104065e-05,
      "loss": 1.2528,
      "step": 9660
    },
    {
      "epoch": 0.7490462634830264,
      "grad_norm": 2.6098825931549072,
      "learning_rate": 2.2571835361118303e-05,
      "loss": 1.3201,
      "step": 9670
    },
    {
      "epoch": 0.7498208718216851,
      "grad_norm": 2.0382602214813232,
      "learning_rate": 2.2564069376132537e-05,
      "loss": 1.1655,
      "step": 9680
    },
    {
      "epoch": 0.7505954801603439,
      "grad_norm": 1.9609084129333496,
      "learning_rate": 2.2556303391146775e-05,
      "loss": 1.3687,
      "step": 9690
    },
    {
      "epoch": 0.7513700884990027,
      "grad_norm": 2.7913498878479004,
      "learning_rate": 2.2548537406161013e-05,
      "loss": 1.3778,
      "step": 9700
    },
    {
      "epoch": 0.7521446968376615,
      "grad_norm": 2.699354648590088,
      "learning_rate": 2.254077142117525e-05,
      "loss": 1.3068,
      "step": 9710
    },
    {
      "epoch": 0.7529193051763202,
      "grad_norm": 2.100296974182129,
      "learning_rate": 2.2533005436189492e-05,
      "loss": 1.2943,
      "step": 9720
    },
    {
      "epoch": 0.7536939135149789,
      "grad_norm": 3.5418615341186523,
      "learning_rate": 2.252523945120373e-05,
      "loss": 1.2204,
      "step": 9730
    },
    {
      "epoch": 0.7544685218536378,
      "grad_norm": 2.7750766277313232,
      "learning_rate": 2.2517473466217968e-05,
      "loss": 1.2737,
      "step": 9740
    },
    {
      "epoch": 0.7552431301922965,
      "grad_norm": 3.2907447814941406,
      "learning_rate": 2.2509707481232206e-05,
      "loss": 1.1679,
      "step": 9750
    },
    {
      "epoch": 0.7560177385309553,
      "grad_norm": 2.3781557083129883,
      "learning_rate": 2.2501941496246444e-05,
      "loss": 1.2392,
      "step": 9760
    },
    {
      "epoch": 0.756792346869614,
      "grad_norm": 2.5277152061462402,
      "learning_rate": 2.2494175511260678e-05,
      "loss": 1.1742,
      "step": 9770
    },
    {
      "epoch": 0.7575669552082728,
      "grad_norm": 2.1934266090393066,
      "learning_rate": 2.2486409526274916e-05,
      "loss": 1.3046,
      "step": 9780
    },
    {
      "epoch": 0.7583415635469316,
      "grad_norm": 3.155816078186035,
      "learning_rate": 2.2478643541289154e-05,
      "loss": 1.3132,
      "step": 9790
    },
    {
      "epoch": 0.7591161718855903,
      "grad_norm": 2.0538835525512695,
      "learning_rate": 2.2470877556303392e-05,
      "loss": 1.2026,
      "step": 9800
    },
    {
      "epoch": 0.7598907802242492,
      "grad_norm": 2.4900295734405518,
      "learning_rate": 2.246311157131763e-05,
      "loss": 1.2528,
      "step": 9810
    },
    {
      "epoch": 0.7606653885629079,
      "grad_norm": 2.355851411819458,
      "learning_rate": 2.2455345586331867e-05,
      "loss": 1.2914,
      "step": 9820
    },
    {
      "epoch": 0.7614399969015666,
      "grad_norm": 2.7516608238220215,
      "learning_rate": 2.2447579601346105e-05,
      "loss": 1.3728,
      "step": 9830
    },
    {
      "epoch": 0.7622146052402254,
      "grad_norm": 2.145432472229004,
      "learning_rate": 2.2439813616360343e-05,
      "loss": 1.2186,
      "step": 9840
    },
    {
      "epoch": 0.7629892135788842,
      "grad_norm": 2.445873737335205,
      "learning_rate": 2.2432047631374578e-05,
      "loss": 1.3554,
      "step": 9850
    },
    {
      "epoch": 0.763763821917543,
      "grad_norm": 2.5286781787872314,
      "learning_rate": 2.2424281646388816e-05,
      "loss": 1.2502,
      "step": 9860
    },
    {
      "epoch": 0.7645384302562017,
      "grad_norm": 2.15671443939209,
      "learning_rate": 2.2416515661403053e-05,
      "loss": 1.2724,
      "step": 9870
    },
    {
      "epoch": 0.7653130385948604,
      "grad_norm": 2.5884275436401367,
      "learning_rate": 2.240874967641729e-05,
      "loss": 1.3162,
      "step": 9880
    },
    {
      "epoch": 0.7660876469335193,
      "grad_norm": 2.6709911823272705,
      "learning_rate": 2.240098369143153e-05,
      "loss": 1.2933,
      "step": 9890
    },
    {
      "epoch": 0.766862255272178,
      "grad_norm": 1.8639508485794067,
      "learning_rate": 2.2393217706445767e-05,
      "loss": 1.3715,
      "step": 9900
    },
    {
      "epoch": 0.7676368636108367,
      "grad_norm": 2.727976083755493,
      "learning_rate": 2.2385451721460008e-05,
      "loss": 1.169,
      "step": 9910
    },
    {
      "epoch": 0.7684114719494956,
      "grad_norm": 2.1897642612457275,
      "learning_rate": 2.2377685736474246e-05,
      "loss": 1.286,
      "step": 9920
    },
    {
      "epoch": 0.7691860802881543,
      "grad_norm": 2.5859007835388184,
      "learning_rate": 2.2369919751488484e-05,
      "loss": 1.2634,
      "step": 9930
    },
    {
      "epoch": 0.7699606886268131,
      "grad_norm": 2.7414534091949463,
      "learning_rate": 2.236215376650272e-05,
      "loss": 1.2247,
      "step": 9940
    },
    {
      "epoch": 0.7707352969654718,
      "grad_norm": 3.0204970836639404,
      "learning_rate": 2.2354387781516956e-05,
      "loss": 1.2768,
      "step": 9950
    },
    {
      "epoch": 0.7715099053041305,
      "grad_norm": 2.3164215087890625,
      "learning_rate": 2.2346621796531194e-05,
      "loss": 1.2647,
      "step": 9960
    },
    {
      "epoch": 0.7722845136427894,
      "grad_norm": 2.527306318283081,
      "learning_rate": 2.2338855811545432e-05,
      "loss": 1.2736,
      "step": 9970
    },
    {
      "epoch": 0.7730591219814481,
      "grad_norm": 2.893017530441284,
      "learning_rate": 2.233108982655967e-05,
      "loss": 1.3201,
      "step": 9980
    },
    {
      "epoch": 0.7738337303201069,
      "grad_norm": 2.511920928955078,
      "learning_rate": 2.2323323841573908e-05,
      "loss": 1.2194,
      "step": 9990
    },
    {
      "epoch": 0.7746083386587657,
      "grad_norm": 2.031079053878784,
      "learning_rate": 2.2315557856588146e-05,
      "loss": 1.3107,
      "step": 10000
    },
    {
      "epoch": 0.7753829469974244,
      "grad_norm": 3.156320571899414,
      "learning_rate": 2.2307791871602384e-05,
      "loss": 1.2817,
      "step": 10010
    },
    {
      "epoch": 0.7761575553360832,
      "grad_norm": 2.212003231048584,
      "learning_rate": 2.2300025886616618e-05,
      "loss": 1.3081,
      "step": 10020
    },
    {
      "epoch": 0.7769321636747419,
      "grad_norm": 2.5983595848083496,
      "learning_rate": 2.2292259901630856e-05,
      "loss": 1.2695,
      "step": 10030
    },
    {
      "epoch": 0.7777067720134008,
      "grad_norm": 1.8544639348983765,
      "learning_rate": 2.2284493916645094e-05,
      "loss": 1.3741,
      "step": 10040
    },
    {
      "epoch": 0.7784813803520595,
      "grad_norm": 3.1404197216033936,
      "learning_rate": 2.227672793165933e-05,
      "loss": 1.2365,
      "step": 10050
    },
    {
      "epoch": 0.7792559886907182,
      "grad_norm": 2.4579503536224365,
      "learning_rate": 2.226896194667357e-05,
      "loss": 1.2933,
      "step": 10060
    },
    {
      "epoch": 0.780030597029377,
      "grad_norm": 2.30818247795105,
      "learning_rate": 2.2261195961687807e-05,
      "loss": 1.3884,
      "step": 10070
    },
    {
      "epoch": 0.7808052053680358,
      "grad_norm": 2.936479330062866,
      "learning_rate": 2.2253429976702045e-05,
      "loss": 1.3465,
      "step": 10080
    },
    {
      "epoch": 0.7815798137066946,
      "grad_norm": 3.065310478210449,
      "learning_rate": 2.2245663991716283e-05,
      "loss": 1.4053,
      "step": 10090
    },
    {
      "epoch": 0.7823544220453533,
      "grad_norm": 2.435455083847046,
      "learning_rate": 2.223789800673052e-05,
      "loss": 1.3184,
      "step": 10100
    },
    {
      "epoch": 0.783129030384012,
      "grad_norm": 3.0803425312042236,
      "learning_rate": 2.223013202174476e-05,
      "loss": 1.3889,
      "step": 10110
    },
    {
      "epoch": 0.7839036387226709,
      "grad_norm": 2.1191775798797607,
      "learning_rate": 2.2222366036758997e-05,
      "loss": 1.3634,
      "step": 10120
    },
    {
      "epoch": 0.7846782470613296,
      "grad_norm": 3.3051908016204834,
      "learning_rate": 2.2214600051773235e-05,
      "loss": 1.4089,
      "step": 10130
    },
    {
      "epoch": 0.7854528553999883,
      "grad_norm": 2.2271687984466553,
      "learning_rate": 2.2206834066787472e-05,
      "loss": 1.346,
      "step": 10140
    },
    {
      "epoch": 0.7862274637386472,
      "grad_norm": 2.3251824378967285,
      "learning_rate": 2.219906808180171e-05,
      "loss": 1.3187,
      "step": 10150
    },
    {
      "epoch": 0.7870020720773059,
      "grad_norm": 2.109025239944458,
      "learning_rate": 2.2191302096815948e-05,
      "loss": 1.2702,
      "step": 10160
    },
    {
      "epoch": 0.7877766804159647,
      "grad_norm": 2.776437282562256,
      "learning_rate": 2.2183536111830186e-05,
      "loss": 1.2542,
      "step": 10170
    },
    {
      "epoch": 0.7885512887546234,
      "grad_norm": 2.171099901199341,
      "learning_rate": 2.2175770126844424e-05,
      "loss": 1.3635,
      "step": 10180
    },
    {
      "epoch": 0.7893258970932822,
      "grad_norm": 1.938481330871582,
      "learning_rate": 2.216800414185866e-05,
      "loss": 1.2455,
      "step": 10190
    },
    {
      "epoch": 0.790100505431941,
      "grad_norm": 2.163984775543213,
      "learning_rate": 2.2160238156872896e-05,
      "loss": 1.2247,
      "step": 10200
    },
    {
      "epoch": 0.7908751137705997,
      "grad_norm": 2.3317363262176514,
      "learning_rate": 2.2152472171887134e-05,
      "loss": 1.2384,
      "step": 10210
    },
    {
      "epoch": 0.7916497221092585,
      "grad_norm": 2.3051042556762695,
      "learning_rate": 2.2144706186901372e-05,
      "loss": 1.1868,
      "step": 10220
    },
    {
      "epoch": 0.7924243304479173,
      "grad_norm": 2.7321743965148926,
      "learning_rate": 2.213694020191561e-05,
      "loss": 1.2101,
      "step": 10230
    },
    {
      "epoch": 0.793198938786576,
      "grad_norm": 2.0934855937957764,
      "learning_rate": 2.2129174216929848e-05,
      "loss": 1.2053,
      "step": 10240
    },
    {
      "epoch": 0.7939735471252348,
      "grad_norm": 2.043536901473999,
      "learning_rate": 2.2121408231944086e-05,
      "loss": 1.3253,
      "step": 10250
    },
    {
      "epoch": 0.7947481554638935,
      "grad_norm": 2.4161322116851807,
      "learning_rate": 2.2113642246958323e-05,
      "loss": 1.2473,
      "step": 10260
    },
    {
      "epoch": 0.7955227638025524,
      "grad_norm": 3.073171377182007,
      "learning_rate": 2.210587626197256e-05,
      "loss": 1.2157,
      "step": 10270
    },
    {
      "epoch": 0.7962973721412111,
      "grad_norm": 2.2871975898742676,
      "learning_rate": 2.2098110276986796e-05,
      "loss": 1.3391,
      "step": 10280
    },
    {
      "epoch": 0.7970719804798698,
      "grad_norm": 2.7313332557678223,
      "learning_rate": 2.2090344292001034e-05,
      "loss": 1.3575,
      "step": 10290
    },
    {
      "epoch": 0.7978465888185287,
      "grad_norm": 2.2519047260284424,
      "learning_rate": 2.208257830701527e-05,
      "loss": 1.2963,
      "step": 10300
    },
    {
      "epoch": 0.7986211971571874,
      "grad_norm": 1.8874015808105469,
      "learning_rate": 2.2074812322029513e-05,
      "loss": 1.35,
      "step": 10310
    },
    {
      "epoch": 0.7993958054958462,
      "grad_norm": 2.84747314453125,
      "learning_rate": 2.206704633704375e-05,
      "loss": 1.3376,
      "step": 10320
    },
    {
      "epoch": 0.8001704138345049,
      "grad_norm": 2.3021373748779297,
      "learning_rate": 2.205928035205799e-05,
      "loss": 1.2296,
      "step": 10330
    },
    {
      "epoch": 0.8009450221731637,
      "grad_norm": 2.0357449054718018,
      "learning_rate": 2.2051514367072226e-05,
      "loss": 1.2097,
      "step": 10340
    },
    {
      "epoch": 0.8017196305118225,
      "grad_norm": 1.704638123512268,
      "learning_rate": 2.2043748382086464e-05,
      "loss": 1.2351,
      "step": 10350
    },
    {
      "epoch": 0.8024942388504812,
      "grad_norm": 2.3679776191711426,
      "learning_rate": 2.20359823971007e-05,
      "loss": 1.2063,
      "step": 10360
    },
    {
      "epoch": 0.80326884718914,
      "grad_norm": 2.687678337097168,
      "learning_rate": 2.2028216412114937e-05,
      "loss": 1.1279,
      "step": 10370
    },
    {
      "epoch": 0.8040434555277988,
      "grad_norm": 2.029087543487549,
      "learning_rate": 2.2020450427129174e-05,
      "loss": 1.2368,
      "step": 10380
    },
    {
      "epoch": 0.8048180638664575,
      "grad_norm": 2.8398032188415527,
      "learning_rate": 2.2012684442143412e-05,
      "loss": 1.2861,
      "step": 10390
    },
    {
      "epoch": 0.8055926722051163,
      "grad_norm": 2.8501574993133545,
      "learning_rate": 2.200491845715765e-05,
      "loss": 1.2367,
      "step": 10400
    },
    {
      "epoch": 0.806367280543775,
      "grad_norm": 2.675351858139038,
      "learning_rate": 2.1997152472171888e-05,
      "loss": 1.3561,
      "step": 10410
    },
    {
      "epoch": 0.8071418888824338,
      "grad_norm": 2.5396945476531982,
      "learning_rate": 2.1989386487186126e-05,
      "loss": 1.219,
      "step": 10420
    },
    {
      "epoch": 0.8079164972210926,
      "grad_norm": 3.058601140975952,
      "learning_rate": 2.1981620502200364e-05,
      "loss": 1.302,
      "step": 10430
    },
    {
      "epoch": 0.8086911055597513,
      "grad_norm": 2.650031566619873,
      "learning_rate": 2.19738545172146e-05,
      "loss": 1.1734,
      "step": 10440
    },
    {
      "epoch": 0.8094657138984102,
      "grad_norm": 2.1029350757598877,
      "learning_rate": 2.1966088532228836e-05,
      "loss": 1.3885,
      "step": 10450
    },
    {
      "epoch": 0.8102403222370689,
      "grad_norm": 2.51570725440979,
      "learning_rate": 2.1958322547243074e-05,
      "loss": 1.261,
      "step": 10460
    },
    {
      "epoch": 0.8110149305757276,
      "grad_norm": 2.338371753692627,
      "learning_rate": 2.1950556562257312e-05,
      "loss": 1.2501,
      "step": 10470
    },
    {
      "epoch": 0.8117895389143864,
      "grad_norm": 2.444999933242798,
      "learning_rate": 2.194279057727155e-05,
      "loss": 1.2764,
      "step": 10480
    },
    {
      "epoch": 0.8125641472530452,
      "grad_norm": 2.062087297439575,
      "learning_rate": 2.1935024592285788e-05,
      "loss": 1.3166,
      "step": 10490
    },
    {
      "epoch": 0.813338755591704,
      "grad_norm": 2.8403093814849854,
      "learning_rate": 2.192725860730003e-05,
      "loss": 1.3672,
      "step": 10500
    },
    {
      "epoch": 0.8141133639303627,
      "grad_norm": 2.3380348682403564,
      "learning_rate": 2.1919492622314267e-05,
      "loss": 1.2844,
      "step": 10510
    },
    {
      "epoch": 0.8148879722690214,
      "grad_norm": 2.057069778442383,
      "learning_rate": 2.1911726637328505e-05,
      "loss": 1.2842,
      "step": 10520
    },
    {
      "epoch": 0.8156625806076803,
      "grad_norm": 2.118267774581909,
      "learning_rate": 2.190396065234274e-05,
      "loss": 1.2053,
      "step": 10530
    },
    {
      "epoch": 0.816437188946339,
      "grad_norm": 1.907852053642273,
      "learning_rate": 2.1896194667356977e-05,
      "loss": 1.1113,
      "step": 10540
    },
    {
      "epoch": 0.8172117972849978,
      "grad_norm": 3.0817151069641113,
      "learning_rate": 2.1888428682371215e-05,
      "loss": 1.1533,
      "step": 10550
    },
    {
      "epoch": 0.8179864056236565,
      "grad_norm": 2.2261803150177,
      "learning_rate": 2.1880662697385453e-05,
      "loss": 1.3098,
      "step": 10560
    },
    {
      "epoch": 0.8187610139623153,
      "grad_norm": 2.76332688331604,
      "learning_rate": 2.187289671239969e-05,
      "loss": 1.2384,
      "step": 10570
    },
    {
      "epoch": 0.8195356223009741,
      "grad_norm": 2.391226291656494,
      "learning_rate": 2.186513072741393e-05,
      "loss": 1.2279,
      "step": 10580
    },
    {
      "epoch": 0.8203102306396328,
      "grad_norm": 2.8636977672576904,
      "learning_rate": 2.1857364742428166e-05,
      "loss": 1.281,
      "step": 10590
    },
    {
      "epoch": 0.8210848389782917,
      "grad_norm": 2.7738356590270996,
      "learning_rate": 2.1849598757442404e-05,
      "loss": 1.24,
      "step": 10600
    },
    {
      "epoch": 0.8218594473169504,
      "grad_norm": 2.7525625228881836,
      "learning_rate": 2.184183277245664e-05,
      "loss": 1.1999,
      "step": 10610
    },
    {
      "epoch": 0.8226340556556091,
      "grad_norm": 2.689797878265381,
      "learning_rate": 2.1834066787470876e-05,
      "loss": 1.1557,
      "step": 10620
    },
    {
      "epoch": 0.8234086639942679,
      "grad_norm": 1.9523206949234009,
      "learning_rate": 2.1826300802485114e-05,
      "loss": 1.23,
      "step": 10630
    },
    {
      "epoch": 0.8241832723329267,
      "grad_norm": 2.483247995376587,
      "learning_rate": 2.1818534817499352e-05,
      "loss": 1.18,
      "step": 10640
    },
    {
      "epoch": 0.8249578806715854,
      "grad_norm": 2.810852289199829,
      "learning_rate": 2.181076883251359e-05,
      "loss": 1.3028,
      "step": 10650
    },
    {
      "epoch": 0.8257324890102442,
      "grad_norm": 2.6693601608276367,
      "learning_rate": 2.1803002847527828e-05,
      "loss": 1.1739,
      "step": 10660
    },
    {
      "epoch": 0.8265070973489029,
      "grad_norm": 2.56107497215271,
      "learning_rate": 2.1795236862542066e-05,
      "loss": 1.3098,
      "step": 10670
    },
    {
      "epoch": 0.8272817056875618,
      "grad_norm": 2.1372764110565186,
      "learning_rate": 2.1787470877556304e-05,
      "loss": 1.2479,
      "step": 10680
    },
    {
      "epoch": 0.8280563140262205,
      "grad_norm": 2.328138828277588,
      "learning_rate": 2.177970489257054e-05,
      "loss": 1.3007,
      "step": 10690
    },
    {
      "epoch": 0.8288309223648792,
      "grad_norm": 3.1866531372070312,
      "learning_rate": 2.177193890758478e-05,
      "loss": 1.2393,
      "step": 10700
    },
    {
      "epoch": 0.829605530703538,
      "grad_norm": 2.3491148948669434,
      "learning_rate": 2.1764172922599017e-05,
      "loss": 1.3236,
      "step": 10710
    },
    {
      "epoch": 0.8303801390421968,
      "grad_norm": 2.56168270111084,
      "learning_rate": 2.1756406937613255e-05,
      "loss": 1.3084,
      "step": 10720
    },
    {
      "epoch": 0.8311547473808556,
      "grad_norm": 1.7000248432159424,
      "learning_rate": 2.1748640952627493e-05,
      "loss": 1.2405,
      "step": 10730
    },
    {
      "epoch": 0.8319293557195143,
      "grad_norm": 3.151287078857422,
      "learning_rate": 2.174087496764173e-05,
      "loss": 1.2887,
      "step": 10740
    },
    {
      "epoch": 0.832703964058173,
      "grad_norm": 2.6782288551330566,
      "learning_rate": 2.173310898265597e-05,
      "loss": 1.2126,
      "step": 10750
    },
    {
      "epoch": 0.8334785723968319,
      "grad_norm": 2.2237911224365234,
      "learning_rate": 2.1725342997670207e-05,
      "loss": 1.2393,
      "step": 10760
    },
    {
      "epoch": 0.8342531807354906,
      "grad_norm": 2.364053964614868,
      "learning_rate": 2.1717577012684444e-05,
      "loss": 1.3197,
      "step": 10770
    },
    {
      "epoch": 0.8350277890741494,
      "grad_norm": 3.2451016902923584,
      "learning_rate": 2.170981102769868e-05,
      "loss": 1.2412,
      "step": 10780
    },
    {
      "epoch": 0.8358023974128082,
      "grad_norm": 2.1889266967773438,
      "learning_rate": 2.1702045042712917e-05,
      "loss": 1.2669,
      "step": 10790
    },
    {
      "epoch": 0.8365770057514669,
      "grad_norm": 2.414703369140625,
      "learning_rate": 2.1694279057727155e-05,
      "loss": 1.2182,
      "step": 10800
    },
    {
      "epoch": 0.8373516140901257,
      "grad_norm": 2.239847421646118,
      "learning_rate": 2.1686513072741392e-05,
      "loss": 1.253,
      "step": 10810
    },
    {
      "epoch": 0.8381262224287844,
      "grad_norm": 2.8549644947052,
      "learning_rate": 2.167874708775563e-05,
      "loss": 1.2101,
      "step": 10820
    },
    {
      "epoch": 0.8389008307674433,
      "grad_norm": 2.748605728149414,
      "learning_rate": 2.1670981102769868e-05,
      "loss": 1.215,
      "step": 10830
    },
    {
      "epoch": 0.839675439106102,
      "grad_norm": 1.9380658864974976,
      "learning_rate": 2.1663215117784106e-05,
      "loss": 1.4235,
      "step": 10840
    },
    {
      "epoch": 0.8404500474447607,
      "grad_norm": 2.2404282093048096,
      "learning_rate": 2.1655449132798344e-05,
      "loss": 1.2973,
      "step": 10850
    },
    {
      "epoch": 0.8412246557834195,
      "grad_norm": 2.7775046825408936,
      "learning_rate": 2.1647683147812582e-05,
      "loss": 1.4138,
      "step": 10860
    },
    {
      "epoch": 0.8419992641220783,
      "grad_norm": 2.8598473072052,
      "learning_rate": 2.1639917162826816e-05,
      "loss": 1.3541,
      "step": 10870
    },
    {
      "epoch": 0.8427738724607371,
      "grad_norm": 2.8863916397094727,
      "learning_rate": 2.1632151177841054e-05,
      "loss": 1.2831,
      "step": 10880
    },
    {
      "epoch": 0.8435484807993958,
      "grad_norm": 1.9983396530151367,
      "learning_rate": 2.1624385192855292e-05,
      "loss": 1.2143,
      "step": 10890
    },
    {
      "epoch": 0.8443230891380545,
      "grad_norm": 2.8372809886932373,
      "learning_rate": 2.1616619207869533e-05,
      "loss": 1.3228,
      "step": 10900
    },
    {
      "epoch": 0.8450976974767134,
      "grad_norm": 2.5752477645874023,
      "learning_rate": 2.160885322288377e-05,
      "loss": 1.2311,
      "step": 10910
    },
    {
      "epoch": 0.8458723058153721,
      "grad_norm": 2.5800275802612305,
      "learning_rate": 2.160108723789801e-05,
      "loss": 1.2949,
      "step": 10920
    },
    {
      "epoch": 0.8466469141540308,
      "grad_norm": 2.4623892307281494,
      "learning_rate": 2.1593321252912247e-05,
      "loss": 1.3436,
      "step": 10930
    },
    {
      "epoch": 0.8474215224926896,
      "grad_norm": 2.1150271892547607,
      "learning_rate": 2.1585555267926485e-05,
      "loss": 1.2297,
      "step": 10940
    },
    {
      "epoch": 0.8481961308313484,
      "grad_norm": 2.4555139541625977,
      "learning_rate": 2.157778928294072e-05,
      "loss": 1.3007,
      "step": 10950
    },
    {
      "epoch": 0.8489707391700072,
      "grad_norm": 2.5394248962402344,
      "learning_rate": 2.1570023297954957e-05,
      "loss": 1.215,
      "step": 10960
    },
    {
      "epoch": 0.8497453475086659,
      "grad_norm": 1.986092448234558,
      "learning_rate": 2.1562257312969195e-05,
      "loss": 1.3934,
      "step": 10970
    },
    {
      "epoch": 0.8505199558473246,
      "grad_norm": 2.1086342334747314,
      "learning_rate": 2.1554491327983433e-05,
      "loss": 1.2929,
      "step": 10980
    },
    {
      "epoch": 0.8512945641859835,
      "grad_norm": 1.9896130561828613,
      "learning_rate": 2.154672534299767e-05,
      "loss": 1.4154,
      "step": 10990
    },
    {
      "epoch": 0.8520691725246422,
      "grad_norm": 2.9169201850891113,
      "learning_rate": 2.153895935801191e-05,
      "loss": 1.2177,
      "step": 11000
    },
    {
      "epoch": 0.852843780863301,
      "grad_norm": 2.5841240882873535,
      "learning_rate": 2.1531193373026146e-05,
      "loss": 1.2822,
      "step": 11010
    },
    {
      "epoch": 0.8536183892019598,
      "grad_norm": 2.074867010116577,
      "learning_rate": 2.1523427388040384e-05,
      "loss": 1.1583,
      "step": 11020
    },
    {
      "epoch": 0.8543929975406185,
      "grad_norm": 2.6259517669677734,
      "learning_rate": 2.1515661403054622e-05,
      "loss": 1.2847,
      "step": 11030
    },
    {
      "epoch": 0.8551676058792773,
      "grad_norm": 2.8118233680725098,
      "learning_rate": 2.1507895418068857e-05,
      "loss": 1.3,
      "step": 11040
    },
    {
      "epoch": 0.855942214217936,
      "grad_norm": 2.8211944103240967,
      "learning_rate": 2.1500129433083094e-05,
      "loss": 1.2789,
      "step": 11050
    },
    {
      "epoch": 0.8567168225565949,
      "grad_norm": 2.3937296867370605,
      "learning_rate": 2.1492363448097332e-05,
      "loss": 1.3094,
      "step": 11060
    },
    {
      "epoch": 0.8574914308952536,
      "grad_norm": 3.244858503341675,
      "learning_rate": 2.148459746311157e-05,
      "loss": 1.3198,
      "step": 11070
    },
    {
      "epoch": 0.8582660392339123,
      "grad_norm": 3.576721668243408,
      "learning_rate": 2.1476831478125808e-05,
      "loss": 1.2826,
      "step": 11080
    },
    {
      "epoch": 0.8590406475725711,
      "grad_norm": 3.014411211013794,
      "learning_rate": 2.146906549314005e-05,
      "loss": 1.336,
      "step": 11090
    },
    {
      "epoch": 0.8598152559112299,
      "grad_norm": 2.280606508255005,
      "learning_rate": 2.1461299508154287e-05,
      "loss": 1.2277,
      "step": 11100
    },
    {
      "epoch": 0.8605898642498887,
      "grad_norm": 3.5448837280273438,
      "learning_rate": 2.1453533523168525e-05,
      "loss": 1.2887,
      "step": 11110
    },
    {
      "epoch": 0.8613644725885474,
      "grad_norm": 2.0830159187316895,
      "learning_rate": 2.144576753818276e-05,
      "loss": 1.2715,
      "step": 11120
    },
    {
      "epoch": 0.8621390809272061,
      "grad_norm": 2.417100429534912,
      "learning_rate": 2.1438001553196997e-05,
      "loss": 1.29,
      "step": 11130
    },
    {
      "epoch": 0.862913689265865,
      "grad_norm": 2.9669690132141113,
      "learning_rate": 2.1430235568211235e-05,
      "loss": 1.2473,
      "step": 11140
    },
    {
      "epoch": 0.8636882976045237,
      "grad_norm": 2.6016695499420166,
      "learning_rate": 2.1422469583225473e-05,
      "loss": 1.189,
      "step": 11150
    },
    {
      "epoch": 0.8644629059431824,
      "grad_norm": 2.540616750717163,
      "learning_rate": 2.141470359823971e-05,
      "loss": 1.2237,
      "step": 11160
    },
    {
      "epoch": 0.8652375142818413,
      "grad_norm": 2.3321778774261475,
      "learning_rate": 2.140693761325395e-05,
      "loss": 1.2629,
      "step": 11170
    },
    {
      "epoch": 0.8660121226205,
      "grad_norm": 2.236783266067505,
      "learning_rate": 2.1399171628268187e-05,
      "loss": 1.2314,
      "step": 11180
    },
    {
      "epoch": 0.8667867309591588,
      "grad_norm": 2.1482222080230713,
      "learning_rate": 2.1391405643282425e-05,
      "loss": 1.295,
      "step": 11190
    },
    {
      "epoch": 0.8675613392978175,
      "grad_norm": 2.198561668395996,
      "learning_rate": 2.1383639658296662e-05,
      "loss": 1.2636,
      "step": 11200
    },
    {
      "epoch": 0.8683359476364763,
      "grad_norm": 3.1708452701568604,
      "learning_rate": 2.1375873673310897e-05,
      "loss": 1.2997,
      "step": 11210
    },
    {
      "epoch": 0.8691105559751351,
      "grad_norm": 2.034827709197998,
      "learning_rate": 2.1368107688325135e-05,
      "loss": 1.3163,
      "step": 11220
    },
    {
      "epoch": 0.8698851643137938,
      "grad_norm": 2.610471725463867,
      "learning_rate": 2.1360341703339373e-05,
      "loss": 1.286,
      "step": 11230
    },
    {
      "epoch": 0.8706597726524526,
      "grad_norm": 2.1695616245269775,
      "learning_rate": 2.135257571835361e-05,
      "loss": 1.2815,
      "step": 11240
    },
    {
      "epoch": 0.8714343809911114,
      "grad_norm": 2.8369853496551514,
      "learning_rate": 2.134480973336785e-05,
      "loss": 1.264,
      "step": 11250
    },
    {
      "epoch": 0.8722089893297701,
      "grad_norm": 2.9820926189422607,
      "learning_rate": 2.1337043748382086e-05,
      "loss": 1.2474,
      "step": 11260
    },
    {
      "epoch": 0.8729835976684289,
      "grad_norm": 2.2634987831115723,
      "learning_rate": 2.1329277763396324e-05,
      "loss": 1.3638,
      "step": 11270
    },
    {
      "epoch": 0.8737582060070876,
      "grad_norm": 2.4442179203033447,
      "learning_rate": 2.1321511778410562e-05,
      "loss": 1.1217,
      "step": 11280
    },
    {
      "epoch": 0.8745328143457465,
      "grad_norm": 3.1301417350769043,
      "learning_rate": 2.13137457934248e-05,
      "loss": 1.3195,
      "step": 11290
    },
    {
      "epoch": 0.8753074226844052,
      "grad_norm": 2.542952060699463,
      "learning_rate": 2.1305979808439038e-05,
      "loss": 1.2798,
      "step": 11300
    },
    {
      "epoch": 0.8760820310230639,
      "grad_norm": 3.098759174346924,
      "learning_rate": 2.1298213823453276e-05,
      "loss": 1.2657,
      "step": 11310
    },
    {
      "epoch": 0.8768566393617228,
      "grad_norm": 3.1490466594696045,
      "learning_rate": 2.1290447838467513e-05,
      "loss": 1.1895,
      "step": 11320
    },
    {
      "epoch": 0.8776312477003815,
      "grad_norm": 2.20383882522583,
      "learning_rate": 2.128268185348175e-05,
      "loss": 1.2201,
      "step": 11330
    },
    {
      "epoch": 0.8784058560390403,
      "grad_norm": 1.8598895072937012,
      "learning_rate": 2.127491586849599e-05,
      "loss": 1.2567,
      "step": 11340
    },
    {
      "epoch": 0.879180464377699,
      "grad_norm": 2.2686069011688232,
      "learning_rate": 2.1267149883510227e-05,
      "loss": 1.342,
      "step": 11350
    },
    {
      "epoch": 0.8799550727163578,
      "grad_norm": 3.035513401031494,
      "learning_rate": 2.1259383898524465e-05,
      "loss": 1.3603,
      "step": 11360
    },
    {
      "epoch": 0.8807296810550166,
      "grad_norm": 3.1123268604278564,
      "learning_rate": 2.1251617913538703e-05,
      "loss": 1.2795,
      "step": 11370
    },
    {
      "epoch": 0.8815042893936753,
      "grad_norm": 2.1617116928100586,
      "learning_rate": 2.1243851928552937e-05,
      "loss": 1.3169,
      "step": 11380
    },
    {
      "epoch": 0.882278897732334,
      "grad_norm": 2.2655885219573975,
      "learning_rate": 2.1236085943567175e-05,
      "loss": 1.2308,
      "step": 11390
    },
    {
      "epoch": 0.8830535060709929,
      "grad_norm": 2.2392916679382324,
      "learning_rate": 2.1228319958581413e-05,
      "loss": 1.2726,
      "step": 11400
    },
    {
      "epoch": 0.8838281144096516,
      "grad_norm": 2.5859286785125732,
      "learning_rate": 2.122055397359565e-05,
      "loss": 1.2277,
      "step": 11410
    },
    {
      "epoch": 0.8846027227483104,
      "grad_norm": 2.4813692569732666,
      "learning_rate": 2.121278798860989e-05,
      "loss": 1.2664,
      "step": 11420
    },
    {
      "epoch": 0.8853773310869691,
      "grad_norm": 2.2409586906433105,
      "learning_rate": 2.1205022003624127e-05,
      "loss": 1.1982,
      "step": 11430
    },
    {
      "epoch": 0.8861519394256279,
      "grad_norm": 2.397109270095825,
      "learning_rate": 2.1197256018638364e-05,
      "loss": 1.2938,
      "step": 11440
    },
    {
      "epoch": 0.8869265477642867,
      "grad_norm": 2.141331911087036,
      "learning_rate": 2.1189490033652602e-05,
      "loss": 1.2286,
      "step": 11450
    },
    {
      "epoch": 0.8877011561029454,
      "grad_norm": 2.3200385570526123,
      "learning_rate": 2.1181724048666837e-05,
      "loss": 1.1729,
      "step": 11460
    },
    {
      "epoch": 0.8884757644416043,
      "grad_norm": 2.3085098266601562,
      "learning_rate": 2.1173958063681075e-05,
      "loss": 1.2333,
      "step": 11470
    },
    {
      "epoch": 0.889250372780263,
      "grad_norm": 1.8366847038269043,
      "learning_rate": 2.1166192078695316e-05,
      "loss": 1.256,
      "step": 11480
    },
    {
      "epoch": 0.8900249811189217,
      "grad_norm": 3.136352062225342,
      "learning_rate": 2.1158426093709554e-05,
      "loss": 1.1767,
      "step": 11490
    },
    {
      "epoch": 0.8907995894575805,
      "grad_norm": 2.1051924228668213,
      "learning_rate": 2.115066010872379e-05,
      "loss": 1.3666,
      "step": 11500
    },
    {
      "epoch": 0.8915741977962393,
      "grad_norm": 2.7491698265075684,
      "learning_rate": 2.114289412373803e-05,
      "loss": 1.3024,
      "step": 11510
    },
    {
      "epoch": 0.8923488061348981,
      "grad_norm": 2.36263108253479,
      "learning_rate": 2.1135128138752267e-05,
      "loss": 1.2954,
      "step": 11520
    },
    {
      "epoch": 0.8931234144735568,
      "grad_norm": 2.6277220249176025,
      "learning_rate": 2.1127362153766505e-05,
      "loss": 1.244,
      "step": 11530
    },
    {
      "epoch": 0.8938980228122155,
      "grad_norm": 2.2202186584472656,
      "learning_rate": 2.1119596168780743e-05,
      "loss": 1.2663,
      "step": 11540
    },
    {
      "epoch": 0.8946726311508744,
      "grad_norm": 1.8914339542388916,
      "learning_rate": 2.1111830183794978e-05,
      "loss": 1.1802,
      "step": 11550
    },
    {
      "epoch": 0.8954472394895331,
      "grad_norm": 2.2999706268310547,
      "learning_rate": 2.1104064198809215e-05,
      "loss": 1.3449,
      "step": 11560
    },
    {
      "epoch": 0.8962218478281919,
      "grad_norm": 3.3274784088134766,
      "learning_rate": 2.1096298213823453e-05,
      "loss": 1.2586,
      "step": 11570
    },
    {
      "epoch": 0.8969964561668506,
      "grad_norm": 3.3734562397003174,
      "learning_rate": 2.108853222883769e-05,
      "loss": 1.1219,
      "step": 11580
    },
    {
      "epoch": 0.8977710645055094,
      "grad_norm": 2.1304173469543457,
      "learning_rate": 2.108076624385193e-05,
      "loss": 1.3313,
      "step": 11590
    },
    {
      "epoch": 0.8985456728441682,
      "grad_norm": 2.923457384109497,
      "learning_rate": 2.1073000258866167e-05,
      "loss": 1.2741,
      "step": 11600
    },
    {
      "epoch": 0.8993202811828269,
      "grad_norm": 2.1799519062042236,
      "learning_rate": 2.1065234273880405e-05,
      "loss": 1.2891,
      "step": 11610
    },
    {
      "epoch": 0.9000948895214858,
      "grad_norm": 2.4721362590789795,
      "learning_rate": 2.1057468288894643e-05,
      "loss": 1.2077,
      "step": 11620
    },
    {
      "epoch": 0.9008694978601445,
      "grad_norm": 2.0159480571746826,
      "learning_rate": 2.1049702303908877e-05,
      "loss": 1.1809,
      "step": 11630
    },
    {
      "epoch": 0.9016441061988032,
      "grad_norm": 2.00341796875,
      "learning_rate": 2.1041936318923115e-05,
      "loss": 1.22,
      "step": 11640
    },
    {
      "epoch": 0.902418714537462,
      "grad_norm": 2.3762617111206055,
      "learning_rate": 2.1034170333937353e-05,
      "loss": 1.2604,
      "step": 11650
    },
    {
      "epoch": 0.9031933228761208,
      "grad_norm": 3.3733136653900146,
      "learning_rate": 2.102640434895159e-05,
      "loss": 1.2634,
      "step": 11660
    },
    {
      "epoch": 0.9039679312147795,
      "grad_norm": 2.250791311264038,
      "learning_rate": 2.101863836396583e-05,
      "loss": 1.3174,
      "step": 11670
    },
    {
      "epoch": 0.9047425395534383,
      "grad_norm": 2.5688328742980957,
      "learning_rate": 2.101087237898007e-05,
      "loss": 1.3855,
      "step": 11680
    },
    {
      "epoch": 0.905517147892097,
      "grad_norm": 2.4753377437591553,
      "learning_rate": 2.1003106393994308e-05,
      "loss": 1.2983,
      "step": 11690
    },
    {
      "epoch": 0.9062917562307559,
      "grad_norm": 2.984311819076538,
      "learning_rate": 2.0995340409008546e-05,
      "loss": 1.3565,
      "step": 11700
    },
    {
      "epoch": 0.9070663645694146,
      "grad_norm": 2.53794264793396,
      "learning_rate": 2.0987574424022783e-05,
      "loss": 1.1766,
      "step": 11710
    },
    {
      "epoch": 0.9078409729080733,
      "grad_norm": 2.1429572105407715,
      "learning_rate": 2.0979808439037018e-05,
      "loss": 1.3858,
      "step": 11720
    },
    {
      "epoch": 0.9086155812467321,
      "grad_norm": 3.061988115310669,
      "learning_rate": 2.0972042454051256e-05,
      "loss": 1.3067,
      "step": 11730
    },
    {
      "epoch": 0.9093901895853909,
      "grad_norm": 2.1047990322113037,
      "learning_rate": 2.0964276469065494e-05,
      "loss": 1.2422,
      "step": 11740
    },
    {
      "epoch": 0.9101647979240497,
      "grad_norm": 2.658141613006592,
      "learning_rate": 2.095651048407973e-05,
      "loss": 1.1916,
      "step": 11750
    },
    {
      "epoch": 0.9109394062627084,
      "grad_norm": 2.8142974376678467,
      "learning_rate": 2.094874449909397e-05,
      "loss": 1.2498,
      "step": 11760
    },
    {
      "epoch": 0.9117140146013671,
      "grad_norm": 2.241081714630127,
      "learning_rate": 2.0940978514108207e-05,
      "loss": 1.3113,
      "step": 11770
    },
    {
      "epoch": 0.912488622940026,
      "grad_norm": 3.076714277267456,
      "learning_rate": 2.0933212529122445e-05,
      "loss": 1.3963,
      "step": 11780
    },
    {
      "epoch": 0.9132632312786847,
      "grad_norm": 2.7275142669677734,
      "learning_rate": 2.0925446544136683e-05,
      "loss": 1.2823,
      "step": 11790
    },
    {
      "epoch": 0.9140378396173435,
      "grad_norm": 2.6178009510040283,
      "learning_rate": 2.0917680559150918e-05,
      "loss": 1.3567,
      "step": 11800
    },
    {
      "epoch": 0.9148124479560023,
      "grad_norm": 2.469637393951416,
      "learning_rate": 2.0909914574165155e-05,
      "loss": 1.1893,
      "step": 11810
    },
    {
      "epoch": 0.915587056294661,
      "grad_norm": 2.8584954738616943,
      "learning_rate": 2.0902148589179393e-05,
      "loss": 1.2739,
      "step": 11820
    },
    {
      "epoch": 0.9163616646333198,
      "grad_norm": 2.27346134185791,
      "learning_rate": 2.089438260419363e-05,
      "loss": 1.3172,
      "step": 11830
    },
    {
      "epoch": 0.9171362729719785,
      "grad_norm": 2.6667063236236572,
      "learning_rate": 2.088661661920787e-05,
      "loss": 1.1954,
      "step": 11840
    },
    {
      "epoch": 0.9179108813106374,
      "grad_norm": 2.192472457885742,
      "learning_rate": 2.0878850634222107e-05,
      "loss": 1.2015,
      "step": 11850
    },
    {
      "epoch": 0.9186854896492961,
      "grad_norm": 2.6853373050689697,
      "learning_rate": 2.0871084649236345e-05,
      "loss": 1.2656,
      "step": 11860
    },
    {
      "epoch": 0.9194600979879548,
      "grad_norm": 2.47707200050354,
      "learning_rate": 2.0863318664250583e-05,
      "loss": 1.3989,
      "step": 11870
    },
    {
      "epoch": 0.9202347063266136,
      "grad_norm": 2.6623170375823975,
      "learning_rate": 2.085555267926482e-05,
      "loss": 1.2789,
      "step": 11880
    },
    {
      "epoch": 0.9210093146652724,
      "grad_norm": 2.678043842315674,
      "learning_rate": 2.0847786694279058e-05,
      "loss": 1.2284,
      "step": 11890
    },
    {
      "epoch": 0.9217839230039311,
      "grad_norm": 2.3391644954681396,
      "learning_rate": 2.0840020709293296e-05,
      "loss": 1.2751,
      "step": 11900
    },
    {
      "epoch": 0.9225585313425899,
      "grad_norm": 1.9559872150421143,
      "learning_rate": 2.0832254724307534e-05,
      "loss": 1.2297,
      "step": 11910
    },
    {
      "epoch": 0.9233331396812486,
      "grad_norm": 3.482266664505005,
      "learning_rate": 2.0824488739321772e-05,
      "loss": 1.4045,
      "step": 11920
    },
    {
      "epoch": 0.9241077480199075,
      "grad_norm": 2.721585750579834,
      "learning_rate": 2.081672275433601e-05,
      "loss": 1.2587,
      "step": 11930
    },
    {
      "epoch": 0.9248823563585662,
      "grad_norm": 2.999692916870117,
      "learning_rate": 2.0808956769350248e-05,
      "loss": 1.2728,
      "step": 11940
    },
    {
      "epoch": 0.9256569646972249,
      "grad_norm": 2.3992903232574463,
      "learning_rate": 2.0801190784364485e-05,
      "loss": 1.3209,
      "step": 11950
    },
    {
      "epoch": 0.9264315730358837,
      "grad_norm": 2.73764705657959,
      "learning_rate": 2.0793424799378723e-05,
      "loss": 1.3282,
      "step": 11960
    },
    {
      "epoch": 0.9272061813745425,
      "grad_norm": 2.3448410034179688,
      "learning_rate": 2.0785658814392958e-05,
      "loss": 1.2352,
      "step": 11970
    },
    {
      "epoch": 0.9279807897132013,
      "grad_norm": 3.140110492706299,
      "learning_rate": 2.0777892829407196e-05,
      "loss": 1.2775,
      "step": 11980
    },
    {
      "epoch": 0.92875539805186,
      "grad_norm": 2.462710380554199,
      "learning_rate": 2.0770126844421434e-05,
      "loss": 1.3416,
      "step": 11990
    },
    {
      "epoch": 0.9295300063905187,
      "grad_norm": 2.9138262271881104,
      "learning_rate": 2.076236085943567e-05,
      "loss": 1.2919,
      "step": 12000
    },
    {
      "epoch": 0.9303046147291776,
      "grad_norm": 2.5653836727142334,
      "learning_rate": 2.075459487444991e-05,
      "loss": 1.3048,
      "step": 12010
    },
    {
      "epoch": 0.9310792230678363,
      "grad_norm": 2.213094711303711,
      "learning_rate": 2.0746828889464147e-05,
      "loss": 1.3394,
      "step": 12020
    },
    {
      "epoch": 0.9318538314064951,
      "grad_norm": 3.743868350982666,
      "learning_rate": 2.0739062904478385e-05,
      "loss": 1.4222,
      "step": 12030
    },
    {
      "epoch": 0.9326284397451539,
      "grad_norm": 2.371041774749756,
      "learning_rate": 2.0731296919492623e-05,
      "loss": 1.2646,
      "step": 12040
    },
    {
      "epoch": 0.9334030480838126,
      "grad_norm": 2.6870148181915283,
      "learning_rate": 2.072353093450686e-05,
      "loss": 1.2721,
      "step": 12050
    },
    {
      "epoch": 0.9341776564224714,
      "grad_norm": 2.0161244869232178,
      "learning_rate": 2.0715764949521095e-05,
      "loss": 1.2169,
      "step": 12060
    },
    {
      "epoch": 0.9349522647611301,
      "grad_norm": 1.9541183710098267,
      "learning_rate": 2.0707998964535337e-05,
      "loss": 1.129,
      "step": 12070
    },
    {
      "epoch": 0.935726873099789,
      "grad_norm": 2.261892557144165,
      "learning_rate": 2.0700232979549574e-05,
      "loss": 1.2971,
      "step": 12080
    },
    {
      "epoch": 0.9365014814384477,
      "grad_norm": 2.8905081748962402,
      "learning_rate": 2.0692466994563812e-05,
      "loss": 1.2089,
      "step": 12090
    },
    {
      "epoch": 0.9372760897771064,
      "grad_norm": 2.108515501022339,
      "learning_rate": 2.068470100957805e-05,
      "loss": 1.2142,
      "step": 12100
    },
    {
      "epoch": 0.9380506981157652,
      "grad_norm": 2.175079345703125,
      "learning_rate": 2.0676935024592288e-05,
      "loss": 1.3728,
      "step": 12110
    },
    {
      "epoch": 0.938825306454424,
      "grad_norm": 1.8229824304580688,
      "learning_rate": 2.0669169039606526e-05,
      "loss": 1.2698,
      "step": 12120
    },
    {
      "epoch": 0.9395999147930828,
      "grad_norm": 2.134409189224243,
      "learning_rate": 2.0661403054620764e-05,
      "loss": 1.2496,
      "step": 12130
    },
    {
      "epoch": 0.9403745231317415,
      "grad_norm": 2.4933698177337646,
      "learning_rate": 2.0653637069634998e-05,
      "loss": 1.2505,
      "step": 12140
    },
    {
      "epoch": 0.9411491314704002,
      "grad_norm": 2.3316452503204346,
      "learning_rate": 2.0645871084649236e-05,
      "loss": 1.2435,
      "step": 12150
    },
    {
      "epoch": 0.9419237398090591,
      "grad_norm": 2.900334119796753,
      "learning_rate": 2.0638105099663474e-05,
      "loss": 1.1779,
      "step": 12160
    },
    {
      "epoch": 0.9426983481477178,
      "grad_norm": 3.6779584884643555,
      "learning_rate": 2.0630339114677712e-05,
      "loss": 1.3681,
      "step": 12170
    },
    {
      "epoch": 0.9434729564863765,
      "grad_norm": 2.4782474040985107,
      "learning_rate": 2.062257312969195e-05,
      "loss": 1.2544,
      "step": 12180
    },
    {
      "epoch": 0.9442475648250354,
      "grad_norm": 2.9203245639801025,
      "learning_rate": 2.0614807144706188e-05,
      "loss": 1.2758,
      "step": 12190
    },
    {
      "epoch": 0.9450221731636941,
      "grad_norm": 2.592827796936035,
      "learning_rate": 2.0607041159720425e-05,
      "loss": 1.2817,
      "step": 12200
    },
    {
      "epoch": 0.9457967815023529,
      "grad_norm": 2.9106926918029785,
      "learning_rate": 2.0599275174734663e-05,
      "loss": 1.2587,
      "step": 12210
    },
    {
      "epoch": 0.9465713898410116,
      "grad_norm": 2.104698896408081,
      "learning_rate": 2.0591509189748898e-05,
      "loss": 1.2526,
      "step": 12220
    },
    {
      "epoch": 0.9473459981796704,
      "grad_norm": 2.588226318359375,
      "learning_rate": 2.0583743204763136e-05,
      "loss": 1.1988,
      "step": 12230
    },
    {
      "epoch": 0.9481206065183292,
      "grad_norm": 2.3729100227355957,
      "learning_rate": 2.0575977219777373e-05,
      "loss": 1.2797,
      "step": 12240
    },
    {
      "epoch": 0.9488952148569879,
      "grad_norm": 1.9987857341766357,
      "learning_rate": 2.056821123479161e-05,
      "loss": 1.2618,
      "step": 12250
    },
    {
      "epoch": 0.9496698231956467,
      "grad_norm": 2.0219438076019287,
      "learning_rate": 2.056044524980585e-05,
      "loss": 1.3071,
      "step": 12260
    },
    {
      "epoch": 0.9504444315343055,
      "grad_norm": 2.2339954376220703,
      "learning_rate": 2.055267926482009e-05,
      "loss": 1.3832,
      "step": 12270
    },
    {
      "epoch": 0.9512190398729642,
      "grad_norm": 2.1792004108428955,
      "learning_rate": 2.0544913279834328e-05,
      "loss": 1.2932,
      "step": 12280
    },
    {
      "epoch": 0.951993648211623,
      "grad_norm": 2.9463424682617188,
      "learning_rate": 2.0537147294848566e-05,
      "loss": 1.2665,
      "step": 12290
    },
    {
      "epoch": 0.9527682565502817,
      "grad_norm": 2.2822699546813965,
      "learning_rate": 2.0529381309862804e-05,
      "loss": 1.2595,
      "step": 12300
    },
    {
      "epoch": 0.9535428648889406,
      "grad_norm": 2.8100533485412598,
      "learning_rate": 2.052161532487704e-05,
      "loss": 1.2156,
      "step": 12310
    },
    {
      "epoch": 0.9543174732275993,
      "grad_norm": 2.6837713718414307,
      "learning_rate": 2.0513849339891276e-05,
      "loss": 1.2625,
      "step": 12320
    },
    {
      "epoch": 0.955092081566258,
      "grad_norm": 2.0670337677001953,
      "learning_rate": 2.0506083354905514e-05,
      "loss": 1.3137,
      "step": 12330
    },
    {
      "epoch": 0.9558666899049169,
      "grad_norm": 3.0085549354553223,
      "learning_rate": 2.0498317369919752e-05,
      "loss": 1.2167,
      "step": 12340
    },
    {
      "epoch": 0.9566412982435756,
      "grad_norm": 1.8176360130310059,
      "learning_rate": 2.049055138493399e-05,
      "loss": 1.2858,
      "step": 12350
    },
    {
      "epoch": 0.9574159065822344,
      "grad_norm": 2.767551898956299,
      "learning_rate": 2.0482785399948228e-05,
      "loss": 1.2416,
      "step": 12360
    },
    {
      "epoch": 0.9581905149208931,
      "grad_norm": 2.211894989013672,
      "learning_rate": 2.0475019414962466e-05,
      "loss": 1.2769,
      "step": 12370
    },
    {
      "epoch": 0.9589651232595519,
      "grad_norm": 2.6976478099823,
      "learning_rate": 2.0467253429976704e-05,
      "loss": 1.2263,
      "step": 12380
    },
    {
      "epoch": 0.9597397315982107,
      "grad_norm": 2.649709939956665,
      "learning_rate": 2.0459487444990938e-05,
      "loss": 1.3176,
      "step": 12390
    },
    {
      "epoch": 0.9605143399368694,
      "grad_norm": 2.687892436981201,
      "learning_rate": 2.0451721460005176e-05,
      "loss": 1.1967,
      "step": 12400
    },
    {
      "epoch": 0.9612889482755281,
      "grad_norm": 2.7550392150878906,
      "learning_rate": 2.0443955475019414e-05,
      "loss": 1.2616,
      "step": 12410
    },
    {
      "epoch": 0.962063556614187,
      "grad_norm": 2.3872523307800293,
      "learning_rate": 2.043618949003365e-05,
      "loss": 1.272,
      "step": 12420
    },
    {
      "epoch": 0.9628381649528457,
      "grad_norm": 2.7919094562530518,
      "learning_rate": 2.042842350504789e-05,
      "loss": 1.2801,
      "step": 12430
    },
    {
      "epoch": 0.9636127732915045,
      "grad_norm": 2.883286237716675,
      "learning_rate": 2.0420657520062127e-05,
      "loss": 1.3056,
      "step": 12440
    },
    {
      "epoch": 0.9643873816301632,
      "grad_norm": 2.0255026817321777,
      "learning_rate": 2.0412891535076365e-05,
      "loss": 1.3347,
      "step": 12450
    },
    {
      "epoch": 0.965161989968822,
      "grad_norm": 2.7934508323669434,
      "learning_rate": 2.0405125550090607e-05,
      "loss": 1.3111,
      "step": 12460
    },
    {
      "epoch": 0.9659365983074808,
      "grad_norm": 2.1986618041992188,
      "learning_rate": 2.0397359565104844e-05,
      "loss": 1.4055,
      "step": 12470
    },
    {
      "epoch": 0.9667112066461395,
      "grad_norm": 3.264542579650879,
      "learning_rate": 2.038959358011908e-05,
      "loss": 1.1894,
      "step": 12480
    },
    {
      "epoch": 0.9674858149847984,
      "grad_norm": 2.6726064682006836,
      "learning_rate": 2.0381827595133317e-05,
      "loss": 1.2682,
      "step": 12490
    },
    {
      "epoch": 0.9682604233234571,
      "grad_norm": 2.9937920570373535,
      "learning_rate": 2.0374061610147555e-05,
      "loss": 1.243,
      "step": 12500
    },
    {
      "epoch": 0.9690350316621158,
      "grad_norm": 2.381481647491455,
      "learning_rate": 2.0366295625161792e-05,
      "loss": 1.3429,
      "step": 12510
    },
    {
      "epoch": 0.9698096400007746,
      "grad_norm": 2.272294521331787,
      "learning_rate": 2.035852964017603e-05,
      "loss": 1.2794,
      "step": 12520
    },
    {
      "epoch": 0.9705842483394334,
      "grad_norm": 2.417653799057007,
      "learning_rate": 2.0350763655190268e-05,
      "loss": 1.2754,
      "step": 12530
    },
    {
      "epoch": 0.9713588566780922,
      "grad_norm": 2.7002358436584473,
      "learning_rate": 2.0342997670204506e-05,
      "loss": 1.3089,
      "step": 12540
    },
    {
      "epoch": 0.9721334650167509,
      "grad_norm": 2.460782766342163,
      "learning_rate": 2.0335231685218744e-05,
      "loss": 1.4059,
      "step": 12550
    },
    {
      "epoch": 0.9729080733554096,
      "grad_norm": 1.9802428483963013,
      "learning_rate": 2.032746570023298e-05,
      "loss": 1.3269,
      "step": 12560
    },
    {
      "epoch": 0.9736826816940685,
      "grad_norm": 2.5761868953704834,
      "learning_rate": 2.0319699715247216e-05,
      "loss": 1.2697,
      "step": 12570
    },
    {
      "epoch": 0.9744572900327272,
      "grad_norm": 2.072571039199829,
      "learning_rate": 2.0311933730261454e-05,
      "loss": 1.3646,
      "step": 12580
    },
    {
      "epoch": 0.975231898371386,
      "grad_norm": 2.313552141189575,
      "learning_rate": 2.0304167745275692e-05,
      "loss": 1.2223,
      "step": 12590
    },
    {
      "epoch": 0.9760065067100447,
      "grad_norm": 2.115280866622925,
      "learning_rate": 2.029640176028993e-05,
      "loss": 1.2934,
      "step": 12600
    },
    {
      "epoch": 0.9767811150487035,
      "grad_norm": 2.051689624786377,
      "learning_rate": 2.0288635775304168e-05,
      "loss": 1.2755,
      "step": 12610
    },
    {
      "epoch": 0.9775557233873623,
      "grad_norm": 2.183954954147339,
      "learning_rate": 2.0280869790318406e-05,
      "loss": 1.1704,
      "step": 12620
    },
    {
      "epoch": 0.978330331726021,
      "grad_norm": 2.414470911026001,
      "learning_rate": 2.0273103805332643e-05,
      "loss": 1.2988,
      "step": 12630
    },
    {
      "epoch": 0.9791049400646799,
      "grad_norm": 2.6118175983428955,
      "learning_rate": 2.026533782034688e-05,
      "loss": 1.2964,
      "step": 12640
    },
    {
      "epoch": 0.9798795484033386,
      "grad_norm": 3.074286460876465,
      "learning_rate": 2.0257571835361116e-05,
      "loss": 1.3783,
      "step": 12650
    },
    {
      "epoch": 0.9806541567419973,
      "grad_norm": 2.9766340255737305,
      "learning_rate": 2.0249805850375357e-05,
      "loss": 1.3604,
      "step": 12660
    },
    {
      "epoch": 0.9814287650806561,
      "grad_norm": 2.7098159790039062,
      "learning_rate": 2.0242039865389595e-05,
      "loss": 1.3135,
      "step": 12670
    },
    {
      "epoch": 0.9822033734193149,
      "grad_norm": 2.131803512573242,
      "learning_rate": 2.0234273880403833e-05,
      "loss": 1.2274,
      "step": 12680
    },
    {
      "epoch": 0.9829779817579736,
      "grad_norm": 2.320986747741699,
      "learning_rate": 2.022650789541807e-05,
      "loss": 1.2961,
      "step": 12690
    },
    {
      "epoch": 0.9837525900966324,
      "grad_norm": 3.2414755821228027,
      "learning_rate": 2.021874191043231e-05,
      "loss": 1.3671,
      "step": 12700
    },
    {
      "epoch": 0.9845271984352911,
      "grad_norm": 3.022953510284424,
      "learning_rate": 2.0210975925446546e-05,
      "loss": 1.2155,
      "step": 12710
    },
    {
      "epoch": 0.98530180677395,
      "grad_norm": 2.6262242794036865,
      "learning_rate": 2.0203209940460784e-05,
      "loss": 1.2437,
      "step": 12720
    },
    {
      "epoch": 0.9860764151126087,
      "grad_norm": 2.6879351139068604,
      "learning_rate": 2.019544395547502e-05,
      "loss": 1.2997,
      "step": 12730
    },
    {
      "epoch": 0.9868510234512674,
      "grad_norm": 2.0368337631225586,
      "learning_rate": 2.0187677970489257e-05,
      "loss": 1.3398,
      "step": 12740
    },
    {
      "epoch": 0.9876256317899262,
      "grad_norm": 2.405226707458496,
      "learning_rate": 2.0179911985503494e-05,
      "loss": 1.279,
      "step": 12750
    },
    {
      "epoch": 0.988400240128585,
      "grad_norm": 2.0737311840057373,
      "learning_rate": 2.0172146000517732e-05,
      "loss": 1.1854,
      "step": 12760
    },
    {
      "epoch": 0.9891748484672438,
      "grad_norm": 2.7065317630767822,
      "learning_rate": 2.016438001553197e-05,
      "loss": 1.2342,
      "step": 12770
    },
    {
      "epoch": 0.9899494568059025,
      "grad_norm": 2.301159143447876,
      "learning_rate": 2.0156614030546208e-05,
      "loss": 1.2006,
      "step": 12780
    },
    {
      "epoch": 0.9907240651445612,
      "grad_norm": 2.033067226409912,
      "learning_rate": 2.0148848045560446e-05,
      "loss": 1.1897,
      "step": 12790
    },
    {
      "epoch": 0.9914986734832201,
      "grad_norm": 2.639368772506714,
      "learning_rate": 2.0141082060574684e-05,
      "loss": 1.2365,
      "step": 12800
    },
    {
      "epoch": 0.9922732818218788,
      "grad_norm": 1.6201894283294678,
      "learning_rate": 2.013331607558892e-05,
      "loss": 1.305,
      "step": 12810
    },
    {
      "epoch": 0.9930478901605376,
      "grad_norm": 2.5087056159973145,
      "learning_rate": 2.0125550090603156e-05,
      "loss": 1.1684,
      "step": 12820
    },
    {
      "epoch": 0.9938224984991963,
      "grad_norm": 2.2607977390289307,
      "learning_rate": 2.0117784105617394e-05,
      "loss": 1.3145,
      "step": 12830
    },
    {
      "epoch": 0.9945971068378551,
      "grad_norm": 2.8731584548950195,
      "learning_rate": 2.0110018120631632e-05,
      "loss": 1.2715,
      "step": 12840
    },
    {
      "epoch": 0.9953717151765139,
      "grad_norm": 3.0004193782806396,
      "learning_rate": 2.010225213564587e-05,
      "loss": 1.2942,
      "step": 12850
    },
    {
      "epoch": 0.9961463235151726,
      "grad_norm": 2.351365804672241,
      "learning_rate": 2.009448615066011e-05,
      "loss": 1.2918,
      "step": 12860
    },
    {
      "epoch": 0.9969209318538315,
      "grad_norm": 2.728700637817383,
      "learning_rate": 2.008672016567435e-05,
      "loss": 1.329,
      "step": 12870
    },
    {
      "epoch": 0.9976955401924902,
      "grad_norm": 2.772559404373169,
      "learning_rate": 2.0078954180688587e-05,
      "loss": 1.0921,
      "step": 12880
    },
    {
      "epoch": 0.9984701485311489,
      "grad_norm": 2.7056775093078613,
      "learning_rate": 2.0071188195702825e-05,
      "loss": 1.3448,
      "step": 12890
    },
    {
      "epoch": 0.9992447568698077,
      "grad_norm": 2.3044636249542236,
      "learning_rate": 2.006342221071706e-05,
      "loss": 1.1973,
      "step": 12900
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.408944845199585,
      "learning_rate": 2.0055656225731297e-05,
      "loss": 1.223,
      "step": 12910
    },
    {
      "epoch": 1.0007746083386588,
      "grad_norm": 2.284778118133545,
      "learning_rate": 2.0047890240745535e-05,
      "loss": 1.3199,
      "step": 12920
    },
    {
      "epoch": 1.0015492166773174,
      "grad_norm": 2.7872722148895264,
      "learning_rate": 2.0040124255759773e-05,
      "loss": 1.3177,
      "step": 12930
    },
    {
      "epoch": 1.0023238250159763,
      "grad_norm": 2.740978240966797,
      "learning_rate": 2.003235827077401e-05,
      "loss": 1.2819,
      "step": 12940
    },
    {
      "epoch": 1.003098433354635,
      "grad_norm": 2.2700302600860596,
      "learning_rate": 2.002459228578825e-05,
      "loss": 1.2741,
      "step": 12950
    },
    {
      "epoch": 1.0038730416932937,
      "grad_norm": 2.3191778659820557,
      "learning_rate": 2.0016826300802486e-05,
      "loss": 1.3767,
      "step": 12960
    },
    {
      "epoch": 1.0046476500319526,
      "grad_norm": 2.4578969478607178,
      "learning_rate": 2.0009060315816724e-05,
      "loss": 1.2421,
      "step": 12970
    },
    {
      "epoch": 1.0054222583706114,
      "grad_norm": 2.4812729358673096,
      "learning_rate": 2.0001294330830962e-05,
      "loss": 1.285,
      "step": 12980
    },
    {
      "epoch": 1.0061968667092702,
      "grad_norm": 2.2450695037841797,
      "learning_rate": 1.9993528345845196e-05,
      "loss": 1.2357,
      "step": 12990
    },
    {
      "epoch": 1.0069714750479288,
      "grad_norm": 3.4515907764434814,
      "learning_rate": 1.9985762360859434e-05,
      "loss": 1.2721,
      "step": 13000
    },
    {
      "epoch": 1.0077460833865877,
      "grad_norm": 2.5683817863464355,
      "learning_rate": 1.9977996375873672e-05,
      "loss": 1.347,
      "step": 13010
    },
    {
      "epoch": 1.0085206917252465,
      "grad_norm": 2.3391010761260986,
      "learning_rate": 1.997023039088791e-05,
      "loss": 1.4545,
      "step": 13020
    },
    {
      "epoch": 1.009295300063905,
      "grad_norm": 2.4503350257873535,
      "learning_rate": 1.9962464405902148e-05,
      "loss": 1.1911,
      "step": 13030
    },
    {
      "epoch": 1.010069908402564,
      "grad_norm": 2.469191551208496,
      "learning_rate": 1.9954698420916386e-05,
      "loss": 1.2735,
      "step": 13040
    },
    {
      "epoch": 1.0108445167412228,
      "grad_norm": 1.9766134023666382,
      "learning_rate": 1.9946932435930627e-05,
      "loss": 1.2779,
      "step": 13050
    },
    {
      "epoch": 1.0116191250798814,
      "grad_norm": 2.1563405990600586,
      "learning_rate": 1.9939166450944865e-05,
      "loss": 1.2453,
      "step": 13060
    },
    {
      "epoch": 1.0123937334185402,
      "grad_norm": 1.902521014213562,
      "learning_rate": 1.99314004659591e-05,
      "loss": 1.3128,
      "step": 13070
    },
    {
      "epoch": 1.013168341757199,
      "grad_norm": 1.9934871196746826,
      "learning_rate": 1.9923634480973337e-05,
      "loss": 1.2783,
      "step": 13080
    },
    {
      "epoch": 1.0139429500958579,
      "grad_norm": 2.889014959335327,
      "learning_rate": 1.9915868495987575e-05,
      "loss": 1.2488,
      "step": 13090
    },
    {
      "epoch": 1.0147175584345165,
      "grad_norm": 2.6559958457946777,
      "learning_rate": 1.9908102511001813e-05,
      "loss": 1.2438,
      "step": 13100
    },
    {
      "epoch": 1.0154921667731753,
      "grad_norm": 2.8662781715393066,
      "learning_rate": 1.990033652601605e-05,
      "loss": 1.2467,
      "step": 13110
    },
    {
      "epoch": 1.0162667751118342,
      "grad_norm": 3.46897292137146,
      "learning_rate": 1.989257054103029e-05,
      "loss": 1.2158,
      "step": 13120
    },
    {
      "epoch": 1.0170413834504928,
      "grad_norm": 2.1159777641296387,
      "learning_rate": 1.9884804556044527e-05,
      "loss": 1.1348,
      "step": 13130
    },
    {
      "epoch": 1.0178159917891516,
      "grad_norm": 2.8000707626342773,
      "learning_rate": 1.9877038571058764e-05,
      "loss": 1.2776,
      "step": 13140
    },
    {
      "epoch": 1.0185906001278104,
      "grad_norm": 2.407175064086914,
      "learning_rate": 1.9869272586073002e-05,
      "loss": 1.194,
      "step": 13150
    },
    {
      "epoch": 1.019365208466469,
      "grad_norm": 2.377819538116455,
      "learning_rate": 1.9861506601087237e-05,
      "loss": 1.2883,
      "step": 13160
    },
    {
      "epoch": 1.0201398168051279,
      "grad_norm": 2.874530076980591,
      "learning_rate": 1.9853740616101475e-05,
      "loss": 1.2553,
      "step": 13170
    },
    {
      "epoch": 1.0209144251437867,
      "grad_norm": 2.7315709590911865,
      "learning_rate": 1.9845974631115713e-05,
      "loss": 1.161,
      "step": 13180
    },
    {
      "epoch": 1.0216890334824453,
      "grad_norm": 2.7757012844085693,
      "learning_rate": 1.983820864612995e-05,
      "loss": 1.3037,
      "step": 13190
    },
    {
      "epoch": 1.0224636418211042,
      "grad_norm": 2.868516206741333,
      "learning_rate": 1.9830442661144188e-05,
      "loss": 1.2976,
      "step": 13200
    },
    {
      "epoch": 1.023238250159763,
      "grad_norm": 2.1654629707336426,
      "learning_rate": 1.9822676676158426e-05,
      "loss": 1.3023,
      "step": 13210
    },
    {
      "epoch": 1.0240128584984218,
      "grad_norm": 2.283193826675415,
      "learning_rate": 1.9814910691172664e-05,
      "loss": 1.2704,
      "step": 13220
    },
    {
      "epoch": 1.0247874668370804,
      "grad_norm": 2.43436598777771,
      "learning_rate": 1.9807144706186902e-05,
      "loss": 1.3783,
      "step": 13230
    },
    {
      "epoch": 1.0255620751757393,
      "grad_norm": 2.5501043796539307,
      "learning_rate": 1.9799378721201136e-05,
      "loss": 1.2721,
      "step": 13240
    },
    {
      "epoch": 1.026336683514398,
      "grad_norm": 2.5640125274658203,
      "learning_rate": 1.9791612736215378e-05,
      "loss": 1.2714,
      "step": 13250
    },
    {
      "epoch": 1.0271112918530567,
      "grad_norm": 2.4096992015838623,
      "learning_rate": 1.9783846751229615e-05,
      "loss": 1.2777,
      "step": 13260
    },
    {
      "epoch": 1.0278859001917156,
      "grad_norm": 2.3113343715667725,
      "learning_rate": 1.9776080766243853e-05,
      "loss": 1.2442,
      "step": 13270
    },
    {
      "epoch": 1.0286605085303744,
      "grad_norm": 2.3633687496185303,
      "learning_rate": 1.976831478125809e-05,
      "loss": 1.2449,
      "step": 13280
    },
    {
      "epoch": 1.029435116869033,
      "grad_norm": 2.711765766143799,
      "learning_rate": 1.976054879627233e-05,
      "loss": 1.2392,
      "step": 13290
    },
    {
      "epoch": 1.0302097252076918,
      "grad_norm": 2.6052005290985107,
      "learning_rate": 1.9752782811286567e-05,
      "loss": 1.2695,
      "step": 13300
    },
    {
      "epoch": 1.0309843335463507,
      "grad_norm": 2.639763593673706,
      "learning_rate": 1.9745016826300805e-05,
      "loss": 1.2312,
      "step": 13310
    },
    {
      "epoch": 1.0317589418850095,
      "grad_norm": 3.391188383102417,
      "learning_rate": 1.9737250841315043e-05,
      "loss": 1.2845,
      "step": 13320
    },
    {
      "epoch": 1.032533550223668,
      "grad_norm": 2.446542739868164,
      "learning_rate": 1.9729484856329277e-05,
      "loss": 1.245,
      "step": 13330
    },
    {
      "epoch": 1.033308158562327,
      "grad_norm": 2.3347766399383545,
      "learning_rate": 1.9721718871343515e-05,
      "loss": 1.2247,
      "step": 13340
    },
    {
      "epoch": 1.0340827669009858,
      "grad_norm": 3.3346564769744873,
      "learning_rate": 1.9713952886357753e-05,
      "loss": 1.1852,
      "step": 13350
    },
    {
      "epoch": 1.0348573752396444,
      "grad_norm": 1.983227252960205,
      "learning_rate": 1.970618690137199e-05,
      "loss": 1.3244,
      "step": 13360
    },
    {
      "epoch": 1.0356319835783032,
      "grad_norm": 2.5963234901428223,
      "learning_rate": 1.969842091638623e-05,
      "loss": 1.182,
      "step": 13370
    },
    {
      "epoch": 1.036406591916962,
      "grad_norm": 2.480079412460327,
      "learning_rate": 1.9690654931400466e-05,
      "loss": 1.1936,
      "step": 13380
    },
    {
      "epoch": 1.0371812002556207,
      "grad_norm": 2.250643014907837,
      "learning_rate": 1.9682888946414704e-05,
      "loss": 1.3142,
      "step": 13390
    },
    {
      "epoch": 1.0379558085942795,
      "grad_norm": 2.9698970317840576,
      "learning_rate": 1.9675122961428942e-05,
      "loss": 1.3078,
      "step": 13400
    },
    {
      "epoch": 1.0387304169329383,
      "grad_norm": 2.340137004852295,
      "learning_rate": 1.9667356976443177e-05,
      "loss": 1.2149,
      "step": 13410
    },
    {
      "epoch": 1.039505025271597,
      "grad_norm": 2.103149890899658,
      "learning_rate": 1.9659590991457415e-05,
      "loss": 1.2893,
      "step": 13420
    },
    {
      "epoch": 1.0402796336102558,
      "grad_norm": 3.0332345962524414,
      "learning_rate": 1.9651825006471652e-05,
      "loss": 1.2422,
      "step": 13430
    },
    {
      "epoch": 1.0410542419489146,
      "grad_norm": 2.7005374431610107,
      "learning_rate": 1.9644059021485894e-05,
      "loss": 1.2953,
      "step": 13440
    },
    {
      "epoch": 1.0418288502875734,
      "grad_norm": 2.0312752723693848,
      "learning_rate": 1.963629303650013e-05,
      "loss": 1.2997,
      "step": 13450
    },
    {
      "epoch": 1.042603458626232,
      "grad_norm": 1.7733203172683716,
      "learning_rate": 1.962852705151437e-05,
      "loss": 1.2762,
      "step": 13460
    },
    {
      "epoch": 1.0433780669648909,
      "grad_norm": 2.0305755138397217,
      "learning_rate": 1.9620761066528607e-05,
      "loss": 1.2408,
      "step": 13470
    },
    {
      "epoch": 1.0441526753035497,
      "grad_norm": 2.340308427810669,
      "learning_rate": 1.9612995081542845e-05,
      "loss": 1.376,
      "step": 13480
    },
    {
      "epoch": 1.0449272836422083,
      "grad_norm": 2.0485270023345947,
      "learning_rate": 1.960522909655708e-05,
      "loss": 1.2852,
      "step": 13490
    },
    {
      "epoch": 1.0457018919808672,
      "grad_norm": 3.0285263061523438,
      "learning_rate": 1.9597463111571317e-05,
      "loss": 1.1975,
      "step": 13500
    },
    {
      "epoch": 1.046476500319526,
      "grad_norm": 2.1634175777435303,
      "learning_rate": 1.9589697126585555e-05,
      "loss": 1.2926,
      "step": 13510
    },
    {
      "epoch": 1.0472511086581846,
      "grad_norm": 2.3891282081604004,
      "learning_rate": 1.9581931141599793e-05,
      "loss": 1.2841,
      "step": 13520
    },
    {
      "epoch": 1.0480257169968434,
      "grad_norm": 2.4916131496429443,
      "learning_rate": 1.957416515661403e-05,
      "loss": 1.3628,
      "step": 13530
    },
    {
      "epoch": 1.0488003253355023,
      "grad_norm": 2.8181796073913574,
      "learning_rate": 1.956639917162827e-05,
      "loss": 1.2762,
      "step": 13540
    },
    {
      "epoch": 1.049574933674161,
      "grad_norm": 2.8585057258605957,
      "learning_rate": 1.9558633186642507e-05,
      "loss": 1.2626,
      "step": 13550
    },
    {
      "epoch": 1.0503495420128197,
      "grad_norm": 2.7319302558898926,
      "learning_rate": 1.9550867201656745e-05,
      "loss": 1.2805,
      "step": 13560
    },
    {
      "epoch": 1.0511241503514785,
      "grad_norm": 2.52058744430542,
      "learning_rate": 1.9543101216670983e-05,
      "loss": 1.307,
      "step": 13570
    },
    {
      "epoch": 1.0518987586901374,
      "grad_norm": 1.9035664796829224,
      "learning_rate": 1.9535335231685217e-05,
      "loss": 1.24,
      "step": 13580
    },
    {
      "epoch": 1.052673367028796,
      "grad_norm": 2.6321375370025635,
      "learning_rate": 1.9527569246699455e-05,
      "loss": 1.2369,
      "step": 13590
    },
    {
      "epoch": 1.0534479753674548,
      "grad_norm": 2.7175514698028564,
      "learning_rate": 1.9519803261713693e-05,
      "loss": 1.2099,
      "step": 13600
    },
    {
      "epoch": 1.0542225837061137,
      "grad_norm": 1.944189429283142,
      "learning_rate": 1.951203727672793e-05,
      "loss": 1.2446,
      "step": 13610
    },
    {
      "epoch": 1.0549971920447723,
      "grad_norm": 1.9176445007324219,
      "learning_rate": 1.950427129174217e-05,
      "loss": 1.3228,
      "step": 13620
    },
    {
      "epoch": 1.055771800383431,
      "grad_norm": 3.4933745861053467,
      "learning_rate": 1.9496505306756406e-05,
      "loss": 1.2538,
      "step": 13630
    },
    {
      "epoch": 1.05654640872209,
      "grad_norm": 2.44048810005188,
      "learning_rate": 1.9488739321770648e-05,
      "loss": 1.2017,
      "step": 13640
    },
    {
      "epoch": 1.0573210170607488,
      "grad_norm": 2.7615840435028076,
      "learning_rate": 1.9480973336784885e-05,
      "loss": 1.2063,
      "step": 13650
    },
    {
      "epoch": 1.0580956253994074,
      "grad_norm": 2.7427256107330322,
      "learning_rate": 1.947320735179912e-05,
      "loss": 1.2709,
      "step": 13660
    },
    {
      "epoch": 1.0588702337380662,
      "grad_norm": 2.4244585037231445,
      "learning_rate": 1.9465441366813358e-05,
      "loss": 1.3061,
      "step": 13670
    },
    {
      "epoch": 1.059644842076725,
      "grad_norm": 3.2321572303771973,
      "learning_rate": 1.9457675381827596e-05,
      "loss": 1.2366,
      "step": 13680
    },
    {
      "epoch": 1.0604194504153837,
      "grad_norm": 2.10078763961792,
      "learning_rate": 1.9449909396841834e-05,
      "loss": 1.2037,
      "step": 13690
    },
    {
      "epoch": 1.0611940587540425,
      "grad_norm": 2.3084187507629395,
      "learning_rate": 1.944214341185607e-05,
      "loss": 1.2818,
      "step": 13700
    },
    {
      "epoch": 1.0619686670927013,
      "grad_norm": 2.806544780731201,
      "learning_rate": 1.943437742687031e-05,
      "loss": 1.323,
      "step": 13710
    },
    {
      "epoch": 1.06274327543136,
      "grad_norm": 2.6747031211853027,
      "learning_rate": 1.9426611441884547e-05,
      "loss": 1.3919,
      "step": 13720
    },
    {
      "epoch": 1.0635178837700188,
      "grad_norm": 2.363356113433838,
      "learning_rate": 1.9418845456898785e-05,
      "loss": 1.3191,
      "step": 13730
    },
    {
      "epoch": 1.0642924921086776,
      "grad_norm": 2.40775203704834,
      "learning_rate": 1.9411079471913023e-05,
      "loss": 1.2518,
      "step": 13740
    },
    {
      "epoch": 1.0650671004473362,
      "grad_norm": 1.8804254531860352,
      "learning_rate": 1.9403313486927257e-05,
      "loss": 1.2623,
      "step": 13750
    },
    {
      "epoch": 1.065841708785995,
      "grad_norm": 2.7912909984588623,
      "learning_rate": 1.9395547501941495e-05,
      "loss": 1.2852,
      "step": 13760
    },
    {
      "epoch": 1.0666163171246539,
      "grad_norm": 2.9205782413482666,
      "learning_rate": 1.9387781516955733e-05,
      "loss": 1.1441,
      "step": 13770
    },
    {
      "epoch": 1.0673909254633127,
      "grad_norm": 3.059946298599243,
      "learning_rate": 1.938001553196997e-05,
      "loss": 1.183,
      "step": 13780
    },
    {
      "epoch": 1.0681655338019713,
      "grad_norm": 2.6960222721099854,
      "learning_rate": 1.937224954698421e-05,
      "loss": 1.3134,
      "step": 13790
    },
    {
      "epoch": 1.0689401421406302,
      "grad_norm": 2.4791293144226074,
      "learning_rate": 1.9364483561998447e-05,
      "loss": 1.3463,
      "step": 13800
    },
    {
      "epoch": 1.069714750479289,
      "grad_norm": 2.3112149238586426,
      "learning_rate": 1.9356717577012685e-05,
      "loss": 1.2493,
      "step": 13810
    },
    {
      "epoch": 1.0704893588179476,
      "grad_norm": 1.7755070924758911,
      "learning_rate": 1.9348951592026922e-05,
      "loss": 1.1588,
      "step": 13820
    },
    {
      "epoch": 1.0712639671566064,
      "grad_norm": 2.5993008613586426,
      "learning_rate": 1.9341185607041157e-05,
      "loss": 1.3425,
      "step": 13830
    },
    {
      "epoch": 1.0720385754952653,
      "grad_norm": 2.317589282989502,
      "learning_rate": 1.9333419622055398e-05,
      "loss": 1.3877,
      "step": 13840
    },
    {
      "epoch": 1.0728131838339239,
      "grad_norm": 2.4550516605377197,
      "learning_rate": 1.9325653637069636e-05,
      "loss": 1.2401,
      "step": 13850
    },
    {
      "epoch": 1.0735877921725827,
      "grad_norm": 2.2666258811950684,
      "learning_rate": 1.9317887652083874e-05,
      "loss": 1.2537,
      "step": 13860
    },
    {
      "epoch": 1.0743624005112415,
      "grad_norm": 2.343451499938965,
      "learning_rate": 1.9310121667098112e-05,
      "loss": 1.2316,
      "step": 13870
    },
    {
      "epoch": 1.0751370088499002,
      "grad_norm": 2.8370938301086426,
      "learning_rate": 1.930235568211235e-05,
      "loss": 1.2658,
      "step": 13880
    },
    {
      "epoch": 1.075911617188559,
      "grad_norm": 2.449575901031494,
      "learning_rate": 1.9294589697126587e-05,
      "loss": 1.1964,
      "step": 13890
    },
    {
      "epoch": 1.0766862255272178,
      "grad_norm": 3.2421300411224365,
      "learning_rate": 1.9286823712140825e-05,
      "loss": 1.286,
      "step": 13900
    },
    {
      "epoch": 1.0774608338658767,
      "grad_norm": 2.8561854362487793,
      "learning_rate": 1.9279057727155063e-05,
      "loss": 1.2697,
      "step": 13910
    },
    {
      "epoch": 1.0782354422045353,
      "grad_norm": 2.4550070762634277,
      "learning_rate": 1.9271291742169298e-05,
      "loss": 1.1948,
      "step": 13920
    },
    {
      "epoch": 1.079010050543194,
      "grad_norm": 2.6915409564971924,
      "learning_rate": 1.9263525757183536e-05,
      "loss": 1.1997,
      "step": 13930
    },
    {
      "epoch": 1.079784658881853,
      "grad_norm": 2.8959639072418213,
      "learning_rate": 1.9255759772197773e-05,
      "loss": 1.2661,
      "step": 13940
    },
    {
      "epoch": 1.0805592672205115,
      "grad_norm": 2.6847469806671143,
      "learning_rate": 1.924799378721201e-05,
      "loss": 1.305,
      "step": 13950
    },
    {
      "epoch": 1.0813338755591704,
      "grad_norm": 2.3833882808685303,
      "learning_rate": 1.924022780222625e-05,
      "loss": 1.2684,
      "step": 13960
    },
    {
      "epoch": 1.0821084838978292,
      "grad_norm": 1.9563889503479004,
      "learning_rate": 1.9232461817240487e-05,
      "loss": 1.2993,
      "step": 13970
    },
    {
      "epoch": 1.082883092236488,
      "grad_norm": 2.494457960128784,
      "learning_rate": 1.9224695832254725e-05,
      "loss": 1.3096,
      "step": 13980
    },
    {
      "epoch": 1.0836577005751467,
      "grad_norm": 3.427250862121582,
      "learning_rate": 1.9216929847268963e-05,
      "loss": 1.2763,
      "step": 13990
    },
    {
      "epoch": 1.0844323089138055,
      "grad_norm": 2.963271141052246,
      "learning_rate": 1.9209163862283197e-05,
      "loss": 1.2959,
      "step": 14000
    },
    {
      "epoch": 1.0852069172524643,
      "grad_norm": 2.268202066421509,
      "learning_rate": 1.9201397877297435e-05,
      "loss": 1.2125,
      "step": 14010
    },
    {
      "epoch": 1.085981525591123,
      "grad_norm": 2.8515565395355225,
      "learning_rate": 1.9193631892311673e-05,
      "loss": 1.2735,
      "step": 14020
    },
    {
      "epoch": 1.0867561339297818,
      "grad_norm": 3.1171722412109375,
      "learning_rate": 1.9185865907325914e-05,
      "loss": 1.4399,
      "step": 14030
    },
    {
      "epoch": 1.0875307422684406,
      "grad_norm": 2.6871445178985596,
      "learning_rate": 1.9178099922340152e-05,
      "loss": 1.1985,
      "step": 14040
    },
    {
      "epoch": 1.0883053506070992,
      "grad_norm": 2.8654494285583496,
      "learning_rate": 1.917033393735439e-05,
      "loss": 1.3123,
      "step": 14050
    },
    {
      "epoch": 1.089079958945758,
      "grad_norm": 1.9714255332946777,
      "learning_rate": 1.9162567952368628e-05,
      "loss": 1.2883,
      "step": 14060
    },
    {
      "epoch": 1.0898545672844169,
      "grad_norm": 2.9274744987487793,
      "learning_rate": 1.9154801967382866e-05,
      "loss": 1.1982,
      "step": 14070
    },
    {
      "epoch": 1.0906291756230755,
      "grad_norm": 2.9410386085510254,
      "learning_rate": 1.9147035982397104e-05,
      "loss": 1.415,
      "step": 14080
    },
    {
      "epoch": 1.0914037839617343,
      "grad_norm": 2.3871865272521973,
      "learning_rate": 1.9139269997411338e-05,
      "loss": 1.2846,
      "step": 14090
    },
    {
      "epoch": 1.0921783923003932,
      "grad_norm": 2.5727951526641846,
      "learning_rate": 1.9131504012425576e-05,
      "loss": 1.258,
      "step": 14100
    },
    {
      "epoch": 1.092953000639052,
      "grad_norm": 2.1899030208587646,
      "learning_rate": 1.9123738027439814e-05,
      "loss": 1.3686,
      "step": 14110
    },
    {
      "epoch": 1.0937276089777106,
      "grad_norm": 3.052799701690674,
      "learning_rate": 1.911597204245405e-05,
      "loss": 1.1424,
      "step": 14120
    },
    {
      "epoch": 1.0945022173163694,
      "grad_norm": 2.425424337387085,
      "learning_rate": 1.910820605746829e-05,
      "loss": 1.2402,
      "step": 14130
    },
    {
      "epoch": 1.0952768256550283,
      "grad_norm": 2.061584234237671,
      "learning_rate": 1.9100440072482527e-05,
      "loss": 1.3068,
      "step": 14140
    },
    {
      "epoch": 1.0960514339936869,
      "grad_norm": 2.078331708908081,
      "learning_rate": 1.9092674087496765e-05,
      "loss": 1.2147,
      "step": 14150
    },
    {
      "epoch": 1.0968260423323457,
      "grad_norm": 2.4772400856018066,
      "learning_rate": 1.9084908102511003e-05,
      "loss": 1.3308,
      "step": 14160
    },
    {
      "epoch": 1.0976006506710045,
      "grad_norm": 3.1250498294830322,
      "learning_rate": 1.9077142117525238e-05,
      "loss": 1.3247,
      "step": 14170
    },
    {
      "epoch": 1.0983752590096632,
      "grad_norm": 2.5856473445892334,
      "learning_rate": 1.9069376132539475e-05,
      "loss": 1.199,
      "step": 14180
    },
    {
      "epoch": 1.099149867348322,
      "grad_norm": 2.0465102195739746,
      "learning_rate": 1.9061610147553713e-05,
      "loss": 1.2647,
      "step": 14190
    },
    {
      "epoch": 1.0999244756869808,
      "grad_norm": 2.9064435958862305,
      "learning_rate": 1.905384416256795e-05,
      "loss": 1.2764,
      "step": 14200
    },
    {
      "epoch": 1.1006990840256394,
      "grad_norm": 3.302793264389038,
      "learning_rate": 1.904607817758219e-05,
      "loss": 1.226,
      "step": 14210
    },
    {
      "epoch": 1.1014736923642983,
      "grad_norm": 2.6543350219726562,
      "learning_rate": 1.9038312192596427e-05,
      "loss": 1.3258,
      "step": 14220
    },
    {
      "epoch": 1.102248300702957,
      "grad_norm": 2.874884605407715,
      "learning_rate": 1.9030546207610668e-05,
      "loss": 1.245,
      "step": 14230
    },
    {
      "epoch": 1.103022909041616,
      "grad_norm": 2.0517873764038086,
      "learning_rate": 1.9022780222624906e-05,
      "loss": 1.2596,
      "step": 14240
    },
    {
      "epoch": 1.1037975173802745,
      "grad_norm": 2.6026227474212646,
      "learning_rate": 1.9015014237639144e-05,
      "loss": 1.2961,
      "step": 14250
    },
    {
      "epoch": 1.1045721257189334,
      "grad_norm": 1.7279726266860962,
      "learning_rate": 1.900724825265338e-05,
      "loss": 1.3307,
      "step": 14260
    },
    {
      "epoch": 1.1053467340575922,
      "grad_norm": 3.484379768371582,
      "learning_rate": 1.8999482267667616e-05,
      "loss": 1.2267,
      "step": 14270
    },
    {
      "epoch": 1.1061213423962508,
      "grad_norm": 2.838082790374756,
      "learning_rate": 1.8991716282681854e-05,
      "loss": 1.158,
      "step": 14280
    },
    {
      "epoch": 1.1068959507349097,
      "grad_norm": 2.2637205123901367,
      "learning_rate": 1.8983950297696092e-05,
      "loss": 1.2881,
      "step": 14290
    },
    {
      "epoch": 1.1076705590735685,
      "grad_norm": 1.8309944868087769,
      "learning_rate": 1.897618431271033e-05,
      "loss": 1.1332,
      "step": 14300
    },
    {
      "epoch": 1.108445167412227,
      "grad_norm": 2.509004592895508,
      "learning_rate": 1.8968418327724568e-05,
      "loss": 1.2239,
      "step": 14310
    },
    {
      "epoch": 1.109219775750886,
      "grad_norm": 2.026057720184326,
      "learning_rate": 1.8960652342738806e-05,
      "loss": 1.2288,
      "step": 14320
    },
    {
      "epoch": 1.1099943840895448,
      "grad_norm": 2.6492679119110107,
      "learning_rate": 1.8952886357753043e-05,
      "loss": 1.2204,
      "step": 14330
    },
    {
      "epoch": 1.1107689924282034,
      "grad_norm": 2.1420249938964844,
      "learning_rate": 1.8945120372767278e-05,
      "loss": 1.1957,
      "step": 14340
    },
    {
      "epoch": 1.1115436007668622,
      "grad_norm": 2.1811931133270264,
      "learning_rate": 1.8937354387781516e-05,
      "loss": 1.3101,
      "step": 14350
    },
    {
      "epoch": 1.112318209105521,
      "grad_norm": 2.3066630363464355,
      "learning_rate": 1.8929588402795754e-05,
      "loss": 1.2889,
      "step": 14360
    },
    {
      "epoch": 1.1130928174441799,
      "grad_norm": 2.8530197143554688,
      "learning_rate": 1.892182241780999e-05,
      "loss": 1.3233,
      "step": 14370
    },
    {
      "epoch": 1.1138674257828385,
      "grad_norm": 1.9575036764144897,
      "learning_rate": 1.891405643282423e-05,
      "loss": 1.2526,
      "step": 14380
    },
    {
      "epoch": 1.1146420341214973,
      "grad_norm": 2.475661516189575,
      "learning_rate": 1.8906290447838467e-05,
      "loss": 1.3469,
      "step": 14390
    },
    {
      "epoch": 1.1154166424601561,
      "grad_norm": 2.4803760051727295,
      "learning_rate": 1.8898524462852705e-05,
      "loss": 1.2951,
      "step": 14400
    },
    {
      "epoch": 1.1161912507988148,
      "grad_norm": 2.31211519241333,
      "learning_rate": 1.8890758477866943e-05,
      "loss": 1.2284,
      "step": 14410
    },
    {
      "epoch": 1.1169658591374736,
      "grad_norm": 2.284036636352539,
      "learning_rate": 1.8882992492881184e-05,
      "loss": 1.222,
      "step": 14420
    },
    {
      "epoch": 1.1177404674761324,
      "grad_norm": 2.721630096435547,
      "learning_rate": 1.887522650789542e-05,
      "loss": 1.3047,
      "step": 14430
    },
    {
      "epoch": 1.1185150758147913,
      "grad_norm": 2.4513909816741943,
      "learning_rate": 1.8867460522909657e-05,
      "loss": 1.3032,
      "step": 14440
    },
    {
      "epoch": 1.1192896841534499,
      "grad_norm": 2.226496696472168,
      "learning_rate": 1.8859694537923894e-05,
      "loss": 1.2066,
      "step": 14450
    },
    {
      "epoch": 1.1200642924921087,
      "grad_norm": 3.4293394088745117,
      "learning_rate": 1.8851928552938132e-05,
      "loss": 1.2026,
      "step": 14460
    },
    {
      "epoch": 1.1208389008307675,
      "grad_norm": 2.4557220935821533,
      "learning_rate": 1.884416256795237e-05,
      "loss": 1.175,
      "step": 14470
    },
    {
      "epoch": 1.1216135091694261,
      "grad_norm": 2.4358255863189697,
      "learning_rate": 1.8836396582966608e-05,
      "loss": 1.2256,
      "step": 14480
    },
    {
      "epoch": 1.122388117508085,
      "grad_norm": 2.6192524433135986,
      "learning_rate": 1.8828630597980846e-05,
      "loss": 1.1937,
      "step": 14490
    },
    {
      "epoch": 1.1231627258467438,
      "grad_norm": 2.2457659244537354,
      "learning_rate": 1.8820864612995084e-05,
      "loss": 1.3025,
      "step": 14500
    },
    {
      "epoch": 1.1239373341854024,
      "grad_norm": 2.877563714981079,
      "learning_rate": 1.8813098628009318e-05,
      "loss": 1.2035,
      "step": 14510
    },
    {
      "epoch": 1.1247119425240613,
      "grad_norm": 2.885728120803833,
      "learning_rate": 1.8805332643023556e-05,
      "loss": 1.2924,
      "step": 14520
    },
    {
      "epoch": 1.12548655086272,
      "grad_norm": 2.814124822616577,
      "learning_rate": 1.8797566658037794e-05,
      "loss": 1.219,
      "step": 14530
    },
    {
      "epoch": 1.1262611592013787,
      "grad_norm": 1.9214855432510376,
      "learning_rate": 1.8789800673052032e-05,
      "loss": 1.131,
      "step": 14540
    },
    {
      "epoch": 1.1270357675400375,
      "grad_norm": 2.2679991722106934,
      "learning_rate": 1.878203468806627e-05,
      "loss": 1.2994,
      "step": 14550
    },
    {
      "epoch": 1.1278103758786964,
      "grad_norm": 2.514237403869629,
      "learning_rate": 1.8774268703080508e-05,
      "loss": 1.207,
      "step": 14560
    },
    {
      "epoch": 1.1285849842173552,
      "grad_norm": 2.615278959274292,
      "learning_rate": 1.8766502718094745e-05,
      "loss": 1.2617,
      "step": 14570
    },
    {
      "epoch": 1.1293595925560138,
      "grad_norm": 2.0407400131225586,
      "learning_rate": 1.8758736733108983e-05,
      "loss": 1.2357,
      "step": 14580
    },
    {
      "epoch": 1.1301342008946726,
      "grad_norm": 2.5936596393585205,
      "learning_rate": 1.875097074812322e-05,
      "loss": 1.3151,
      "step": 14590
    },
    {
      "epoch": 1.1309088092333315,
      "grad_norm": 2.309074878692627,
      "learning_rate": 1.8743204763137456e-05,
      "loss": 1.3075,
      "step": 14600
    },
    {
      "epoch": 1.13168341757199,
      "grad_norm": 3.420708656311035,
      "learning_rate": 1.8735438778151694e-05,
      "loss": 1.2321,
      "step": 14610
    },
    {
      "epoch": 1.132458025910649,
      "grad_norm": 2.388056516647339,
      "learning_rate": 1.8727672793165935e-05,
      "loss": 1.4199,
      "step": 14620
    },
    {
      "epoch": 1.1332326342493078,
      "grad_norm": 2.591918468475342,
      "learning_rate": 1.8719906808180173e-05,
      "loss": 1.2567,
      "step": 14630
    },
    {
      "epoch": 1.1340072425879664,
      "grad_norm": 2.9815256595611572,
      "learning_rate": 1.871214082319441e-05,
      "loss": 1.2291,
      "step": 14640
    },
    {
      "epoch": 1.1347818509266252,
      "grad_norm": 2.621328353881836,
      "learning_rate": 1.870437483820865e-05,
      "loss": 1.23,
      "step": 14650
    },
    {
      "epoch": 1.135556459265284,
      "grad_norm": 2.250363826751709,
      "learning_rate": 1.8696608853222886e-05,
      "loss": 1.2278,
      "step": 14660
    },
    {
      "epoch": 1.1363310676039426,
      "grad_norm": 2.6238956451416016,
      "learning_rate": 1.8688842868237124e-05,
      "loss": 1.2818,
      "step": 14670
    },
    {
      "epoch": 1.1371056759426015,
      "grad_norm": 2.8198509216308594,
      "learning_rate": 1.868107688325136e-05,
      "loss": 1.227,
      "step": 14680
    },
    {
      "epoch": 1.1378802842812603,
      "grad_norm": 2.3073227405548096,
      "learning_rate": 1.8673310898265596e-05,
      "loss": 1.2465,
      "step": 14690
    },
    {
      "epoch": 1.1386548926199191,
      "grad_norm": 2.4200286865234375,
      "learning_rate": 1.8665544913279834e-05,
      "loss": 1.3309,
      "step": 14700
    },
    {
      "epoch": 1.1394295009585778,
      "grad_norm": 1.869128704071045,
      "learning_rate": 1.8657778928294072e-05,
      "loss": 1.2718,
      "step": 14710
    },
    {
      "epoch": 1.1402041092972366,
      "grad_norm": 1.7154022455215454,
      "learning_rate": 1.865001294330831e-05,
      "loss": 1.313,
      "step": 14720
    },
    {
      "epoch": 1.1409787176358954,
      "grad_norm": 2.3500545024871826,
      "learning_rate": 1.8642246958322548e-05,
      "loss": 1.2746,
      "step": 14730
    },
    {
      "epoch": 1.141753325974554,
      "grad_norm": 2.6761586666107178,
      "learning_rate": 1.8634480973336786e-05,
      "loss": 1.2688,
      "step": 14740
    },
    {
      "epoch": 1.1425279343132129,
      "grad_norm": 2.033820629119873,
      "learning_rate": 1.8626714988351024e-05,
      "loss": 1.2037,
      "step": 14750
    },
    {
      "epoch": 1.1433025426518717,
      "grad_norm": 2.6270055770874023,
      "learning_rate": 1.861894900336526e-05,
      "loss": 1.2335,
      "step": 14760
    },
    {
      "epoch": 1.1440771509905305,
      "grad_norm": 2.7784149646759033,
      "learning_rate": 1.8611183018379496e-05,
      "loss": 1.2557,
      "step": 14770
    },
    {
      "epoch": 1.1448517593291891,
      "grad_norm": 2.916982889175415,
      "learning_rate": 1.8603417033393734e-05,
      "loss": 1.2474,
      "step": 14780
    },
    {
      "epoch": 1.145626367667848,
      "grad_norm": 3.1577816009521484,
      "learning_rate": 1.8595651048407972e-05,
      "loss": 1.2128,
      "step": 14790
    },
    {
      "epoch": 1.1464009760065066,
      "grad_norm": 2.6214451789855957,
      "learning_rate": 1.858788506342221e-05,
      "loss": 1.2129,
      "step": 14800
    },
    {
      "epoch": 1.1471755843451654,
      "grad_norm": 2.499842643737793,
      "learning_rate": 1.8580119078436447e-05,
      "loss": 1.2826,
      "step": 14810
    },
    {
      "epoch": 1.1479501926838243,
      "grad_norm": 2.4220151901245117,
      "learning_rate": 1.857235309345069e-05,
      "loss": 1.2603,
      "step": 14820
    },
    {
      "epoch": 1.148724801022483,
      "grad_norm": 2.4300851821899414,
      "learning_rate": 1.8564587108464927e-05,
      "loss": 1.3281,
      "step": 14830
    },
    {
      "epoch": 1.1494994093611417,
      "grad_norm": 2.1743721961975098,
      "learning_rate": 1.8556821123479164e-05,
      "loss": 1.2987,
      "step": 14840
    },
    {
      "epoch": 1.1502740176998005,
      "grad_norm": 2.4860174655914307,
      "learning_rate": 1.85490551384934e-05,
      "loss": 1.2663,
      "step": 14850
    },
    {
      "epoch": 1.1510486260384594,
      "grad_norm": 2.3924720287323,
      "learning_rate": 1.8541289153507637e-05,
      "loss": 1.2285,
      "step": 14860
    },
    {
      "epoch": 1.151823234377118,
      "grad_norm": 2.595390796661377,
      "learning_rate": 1.8533523168521875e-05,
      "loss": 1.3063,
      "step": 14870
    },
    {
      "epoch": 1.1525978427157768,
      "grad_norm": 3.1749308109283447,
      "learning_rate": 1.8525757183536113e-05,
      "loss": 1.3195,
      "step": 14880
    },
    {
      "epoch": 1.1533724510544356,
      "grad_norm": 2.83139705657959,
      "learning_rate": 1.851799119855035e-05,
      "loss": 1.2696,
      "step": 14890
    },
    {
      "epoch": 1.1541470593930945,
      "grad_norm": 3.3202364444732666,
      "learning_rate": 1.8510225213564588e-05,
      "loss": 1.2769,
      "step": 14900
    },
    {
      "epoch": 1.154921667731753,
      "grad_norm": 2.998389720916748,
      "learning_rate": 1.8502459228578826e-05,
      "loss": 1.3223,
      "step": 14910
    },
    {
      "epoch": 1.155696276070412,
      "grad_norm": 2.9233007431030273,
      "learning_rate": 1.8494693243593064e-05,
      "loss": 1.22,
      "step": 14920
    },
    {
      "epoch": 1.1564708844090708,
      "grad_norm": 3.020925760269165,
      "learning_rate": 1.8486927258607302e-05,
      "loss": 1.2462,
      "step": 14930
    },
    {
      "epoch": 1.1572454927477294,
      "grad_norm": 2.8002848625183105,
      "learning_rate": 1.8479161273621536e-05,
      "loss": 1.1918,
      "step": 14940
    },
    {
      "epoch": 1.1580201010863882,
      "grad_norm": 2.629739761352539,
      "learning_rate": 1.8471395288635774e-05,
      "loss": 1.3538,
      "step": 14950
    },
    {
      "epoch": 1.158794709425047,
      "grad_norm": 2.216657876968384,
      "learning_rate": 1.8463629303650012e-05,
      "loss": 1.2706,
      "step": 14960
    },
    {
      "epoch": 1.1595693177637056,
      "grad_norm": 2.464198589324951,
      "learning_rate": 1.845586331866425e-05,
      "loss": 1.2266,
      "step": 14970
    },
    {
      "epoch": 1.1603439261023645,
      "grad_norm": 2.7452096939086914,
      "learning_rate": 1.8448097333678488e-05,
      "loss": 1.2505,
      "step": 14980
    },
    {
      "epoch": 1.1611185344410233,
      "grad_norm": 3.152519702911377,
      "learning_rate": 1.8440331348692726e-05,
      "loss": 1.2948,
      "step": 14990
    },
    {
      "epoch": 1.161893142779682,
      "grad_norm": 1.7737290859222412,
      "learning_rate": 1.8432565363706964e-05,
      "loss": 1.231,
      "step": 15000
    },
    {
      "epoch": 1.1626677511183408,
      "grad_norm": 2.4586336612701416,
      "learning_rate": 1.8424799378721205e-05,
      "loss": 1.2833,
      "step": 15010
    },
    {
      "epoch": 1.1634423594569996,
      "grad_norm": 2.4964311122894287,
      "learning_rate": 1.841703339373544e-05,
      "loss": 1.1644,
      "step": 15020
    },
    {
      "epoch": 1.1642169677956584,
      "grad_norm": 2.183922052383423,
      "learning_rate": 1.8409267408749677e-05,
      "loss": 1.1924,
      "step": 15030
    },
    {
      "epoch": 1.164991576134317,
      "grad_norm": 2.5173089504241943,
      "learning_rate": 1.8401501423763915e-05,
      "loss": 1.3559,
      "step": 15040
    },
    {
      "epoch": 1.1657661844729759,
      "grad_norm": 2.5115418434143066,
      "learning_rate": 1.8393735438778153e-05,
      "loss": 1.2387,
      "step": 15050
    },
    {
      "epoch": 1.1665407928116347,
      "grad_norm": 2.735616445541382,
      "learning_rate": 1.838596945379239e-05,
      "loss": 1.3524,
      "step": 15060
    },
    {
      "epoch": 1.1673154011502933,
      "grad_norm": 3.0454912185668945,
      "learning_rate": 1.837820346880663e-05,
      "loss": 1.384,
      "step": 15070
    },
    {
      "epoch": 1.1680900094889521,
      "grad_norm": 2.4928793907165527,
      "learning_rate": 1.8370437483820866e-05,
      "loss": 1.3491,
      "step": 15080
    },
    {
      "epoch": 1.168864617827611,
      "grad_norm": 2.597867012023926,
      "learning_rate": 1.8362671498835104e-05,
      "loss": 1.3813,
      "step": 15090
    },
    {
      "epoch": 1.1696392261662696,
      "grad_norm": 2.156574010848999,
      "learning_rate": 1.835490551384934e-05,
      "loss": 1.2167,
      "step": 15100
    },
    {
      "epoch": 1.1704138345049284,
      "grad_norm": 3.434666872024536,
      "learning_rate": 1.8347139528863577e-05,
      "loss": 1.2849,
      "step": 15110
    },
    {
      "epoch": 1.1711884428435873,
      "grad_norm": 2.18622088432312,
      "learning_rate": 1.8339373543877815e-05,
      "loss": 1.2867,
      "step": 15120
    },
    {
      "epoch": 1.1719630511822459,
      "grad_norm": 2.17423939704895,
      "learning_rate": 1.8331607558892052e-05,
      "loss": 1.3494,
      "step": 15130
    },
    {
      "epoch": 1.1727376595209047,
      "grad_norm": 2.817023992538452,
      "learning_rate": 1.832384157390629e-05,
      "loss": 1.4459,
      "step": 15140
    },
    {
      "epoch": 1.1735122678595635,
      "grad_norm": 2.564420700073242,
      "learning_rate": 1.8316075588920528e-05,
      "loss": 1.2712,
      "step": 15150
    },
    {
      "epoch": 1.1742868761982224,
      "grad_norm": 2.190857410430908,
      "learning_rate": 1.8308309603934766e-05,
      "loss": 1.3184,
      "step": 15160
    },
    {
      "epoch": 1.175061484536881,
      "grad_norm": 1.903279423713684,
      "learning_rate": 1.8300543618949004e-05,
      "loss": 1.2399,
      "step": 15170
    },
    {
      "epoch": 1.1758360928755398,
      "grad_norm": 2.400873899459839,
      "learning_rate": 1.8292777633963242e-05,
      "loss": 1.2454,
      "step": 15180
    },
    {
      "epoch": 1.1766107012141986,
      "grad_norm": 2.1723105907440186,
      "learning_rate": 1.8285011648977476e-05,
      "loss": 1.2353,
      "step": 15190
    },
    {
      "epoch": 1.1773853095528573,
      "grad_norm": 2.2924246788024902,
      "learning_rate": 1.8277245663991714e-05,
      "loss": 1.3114,
      "step": 15200
    },
    {
      "epoch": 1.178159917891516,
      "grad_norm": 2.7797842025756836,
      "learning_rate": 1.8269479679005955e-05,
      "loss": 1.2868,
      "step": 15210
    },
    {
      "epoch": 1.178934526230175,
      "grad_norm": 2.3617990016937256,
      "learning_rate": 1.8261713694020193e-05,
      "loss": 1.2988,
      "step": 15220
    },
    {
      "epoch": 1.1797091345688338,
      "grad_norm": 3.070697069168091,
      "learning_rate": 1.825394770903443e-05,
      "loss": 1.1984,
      "step": 15230
    },
    {
      "epoch": 1.1804837429074924,
      "grad_norm": 3.2804553508758545,
      "learning_rate": 1.824618172404867e-05,
      "loss": 1.1945,
      "step": 15240
    },
    {
      "epoch": 1.1812583512461512,
      "grad_norm": 2.3944945335388184,
      "learning_rate": 1.8238415739062907e-05,
      "loss": 1.1977,
      "step": 15250
    },
    {
      "epoch": 1.1820329595848098,
      "grad_norm": 2.2112414836883545,
      "learning_rate": 1.8230649754077145e-05,
      "loss": 1.2871,
      "step": 15260
    },
    {
      "epoch": 1.1828075679234686,
      "grad_norm": 2.660550117492676,
      "learning_rate": 1.822288376909138e-05,
      "loss": 1.1992,
      "step": 15270
    },
    {
      "epoch": 1.1835821762621275,
      "grad_norm": 2.4063751697540283,
      "learning_rate": 1.8215117784105617e-05,
      "loss": 1.1955,
      "step": 15280
    },
    {
      "epoch": 1.1843567846007863,
      "grad_norm": 2.58020281791687,
      "learning_rate": 1.8207351799119855e-05,
      "loss": 1.3213,
      "step": 15290
    },
    {
      "epoch": 1.185131392939445,
      "grad_norm": 2.0182037353515625,
      "learning_rate": 1.8199585814134093e-05,
      "loss": 1.2129,
      "step": 15300
    },
    {
      "epoch": 1.1859060012781037,
      "grad_norm": 2.4761321544647217,
      "learning_rate": 1.819181982914833e-05,
      "loss": 1.2782,
      "step": 15310
    },
    {
      "epoch": 1.1866806096167626,
      "grad_norm": 2.8820993900299072,
      "learning_rate": 1.818405384416257e-05,
      "loss": 1.3618,
      "step": 15320
    },
    {
      "epoch": 1.1874552179554212,
      "grad_norm": 2.5602781772613525,
      "learning_rate": 1.8176287859176806e-05,
      "loss": 1.2693,
      "step": 15330
    },
    {
      "epoch": 1.18822982629408,
      "grad_norm": 2.3928492069244385,
      "learning_rate": 1.8168521874191044e-05,
      "loss": 1.2832,
      "step": 15340
    },
    {
      "epoch": 1.1890044346327389,
      "grad_norm": 2.5915842056274414,
      "learning_rate": 1.8160755889205282e-05,
      "loss": 1.3101,
      "step": 15350
    },
    {
      "epoch": 1.1897790429713977,
      "grad_norm": 2.45068621635437,
      "learning_rate": 1.8152989904219517e-05,
      "loss": 1.2941,
      "step": 15360
    },
    {
      "epoch": 1.1905536513100563,
      "grad_norm": 2.94425368309021,
      "learning_rate": 1.8145223919233754e-05,
      "loss": 1.21,
      "step": 15370
    },
    {
      "epoch": 1.1913282596487151,
      "grad_norm": 2.5406525135040283,
      "learning_rate": 1.8137457934247992e-05,
      "loss": 1.2717,
      "step": 15380
    },
    {
      "epoch": 1.192102867987374,
      "grad_norm": 2.7616140842437744,
      "learning_rate": 1.812969194926223e-05,
      "loss": 1.294,
      "step": 15390
    },
    {
      "epoch": 1.1928774763260326,
      "grad_norm": 2.151881456375122,
      "learning_rate": 1.8121925964276468e-05,
      "loss": 1.2741,
      "step": 15400
    },
    {
      "epoch": 1.1936520846646914,
      "grad_norm": 2.3353271484375,
      "learning_rate": 1.811415997929071e-05,
      "loss": 1.2763,
      "step": 15410
    },
    {
      "epoch": 1.1944266930033502,
      "grad_norm": 3.5391156673431396,
      "learning_rate": 1.8106393994304947e-05,
      "loss": 1.2877,
      "step": 15420
    },
    {
      "epoch": 1.1952013013420089,
      "grad_norm": 2.4496004581451416,
      "learning_rate": 1.8098628009319185e-05,
      "loss": 1.3319,
      "step": 15430
    },
    {
      "epoch": 1.1959759096806677,
      "grad_norm": 2.3486666679382324,
      "learning_rate": 1.809086202433342e-05,
      "loss": 1.1704,
      "step": 15440
    },
    {
      "epoch": 1.1967505180193265,
      "grad_norm": 2.4016475677490234,
      "learning_rate": 1.8083096039347657e-05,
      "loss": 1.2989,
      "step": 15450
    },
    {
      "epoch": 1.1975251263579851,
      "grad_norm": 3.3421430587768555,
      "learning_rate": 1.8075330054361895e-05,
      "loss": 1.2181,
      "step": 15460
    },
    {
      "epoch": 1.198299734696644,
      "grad_norm": 2.539559841156006,
      "learning_rate": 1.8067564069376133e-05,
      "loss": 1.2555,
      "step": 15470
    },
    {
      "epoch": 1.1990743430353028,
      "grad_norm": 2.4068527221679688,
      "learning_rate": 1.805979808439037e-05,
      "loss": 1.3824,
      "step": 15480
    },
    {
      "epoch": 1.1998489513739616,
      "grad_norm": 2.8659489154815674,
      "learning_rate": 1.805203209940461e-05,
      "loss": 1.2702,
      "step": 15490
    },
    {
      "epoch": 1.2006235597126202,
      "grad_norm": 2.1476337909698486,
      "learning_rate": 1.8044266114418847e-05,
      "loss": 1.1566,
      "step": 15500
    },
    {
      "epoch": 1.201398168051279,
      "grad_norm": 2.5979199409484863,
      "learning_rate": 1.8036500129433085e-05,
      "loss": 1.3408,
      "step": 15510
    },
    {
      "epoch": 1.202172776389938,
      "grad_norm": 2.5137181282043457,
      "learning_rate": 1.8028734144447322e-05,
      "loss": 1.2443,
      "step": 15520
    },
    {
      "epoch": 1.2029473847285965,
      "grad_norm": 2.6900134086608887,
      "learning_rate": 1.8020968159461557e-05,
      "loss": 1.2608,
      "step": 15530
    },
    {
      "epoch": 1.2037219930672554,
      "grad_norm": 2.73075270652771,
      "learning_rate": 1.8013202174475795e-05,
      "loss": 1.3601,
      "step": 15540
    },
    {
      "epoch": 1.2044966014059142,
      "grad_norm": 2.6073055267333984,
      "learning_rate": 1.8005436189490033e-05,
      "loss": 1.302,
      "step": 15550
    },
    {
      "epoch": 1.205271209744573,
      "grad_norm": 2.7181482315063477,
      "learning_rate": 1.799767020450427e-05,
      "loss": 1.3194,
      "step": 15560
    },
    {
      "epoch": 1.2060458180832316,
      "grad_norm": 3.0120248794555664,
      "learning_rate": 1.798990421951851e-05,
      "loss": 1.2453,
      "step": 15570
    },
    {
      "epoch": 1.2068204264218905,
      "grad_norm": 2.303557872772217,
      "learning_rate": 1.7982138234532746e-05,
      "loss": 1.2787,
      "step": 15580
    },
    {
      "epoch": 1.207595034760549,
      "grad_norm": 2.1144819259643555,
      "learning_rate": 1.7974372249546984e-05,
      "loss": 1.2607,
      "step": 15590
    },
    {
      "epoch": 1.208369643099208,
      "grad_norm": 2.3034214973449707,
      "learning_rate": 1.7966606264561225e-05,
      "loss": 1.3287,
      "step": 15600
    },
    {
      "epoch": 1.2091442514378667,
      "grad_norm": 2.3361544609069824,
      "learning_rate": 1.795884027957546e-05,
      "loss": 1.2685,
      "step": 15610
    },
    {
      "epoch": 1.2099188597765256,
      "grad_norm": 3.1515090465545654,
      "learning_rate": 1.7951074294589698e-05,
      "loss": 1.2962,
      "step": 15620
    },
    {
      "epoch": 1.2106934681151842,
      "grad_norm": 2.375530242919922,
      "learning_rate": 1.7943308309603936e-05,
      "loss": 1.2836,
      "step": 15630
    },
    {
      "epoch": 1.211468076453843,
      "grad_norm": 2.314441680908203,
      "learning_rate": 1.7935542324618173e-05,
      "loss": 1.3291,
      "step": 15640
    },
    {
      "epoch": 1.2122426847925019,
      "grad_norm": 3.0039587020874023,
      "learning_rate": 1.792777633963241e-05,
      "loss": 1.3288,
      "step": 15650
    },
    {
      "epoch": 1.2130172931311605,
      "grad_norm": 2.6033694744110107,
      "learning_rate": 1.792001035464665e-05,
      "loss": 1.291,
      "step": 15660
    },
    {
      "epoch": 1.2137919014698193,
      "grad_norm": 2.2524824142456055,
      "learning_rate": 1.7912244369660887e-05,
      "loss": 1.2407,
      "step": 15670
    },
    {
      "epoch": 1.2145665098084781,
      "grad_norm": 2.4155776500701904,
      "learning_rate": 1.7904478384675125e-05,
      "loss": 1.3216,
      "step": 15680
    },
    {
      "epoch": 1.215341118147137,
      "grad_norm": 2.0967471599578857,
      "learning_rate": 1.7896712399689363e-05,
      "loss": 1.1778,
      "step": 15690
    },
    {
      "epoch": 1.2161157264857956,
      "grad_norm": 2.394106864929199,
      "learning_rate": 1.7888946414703597e-05,
      "loss": 1.2411,
      "step": 15700
    },
    {
      "epoch": 1.2168903348244544,
      "grad_norm": 2.333036422729492,
      "learning_rate": 1.788195702821641e-05,
      "loss": 1.307,
      "step": 15710
    },
    {
      "epoch": 1.2176649431631132,
      "grad_norm": 3.4708633422851562,
      "learning_rate": 1.787419104323065e-05,
      "loss": 1.2113,
      "step": 15720
    },
    {
      "epoch": 1.2184395515017719,
      "grad_norm": 2.63420033454895,
      "learning_rate": 1.7866425058244886e-05,
      "loss": 1.2729,
      "step": 15730
    },
    {
      "epoch": 1.2192141598404307,
      "grad_norm": 2.665296792984009,
      "learning_rate": 1.7858659073259128e-05,
      "loss": 1.1745,
      "step": 15740
    },
    {
      "epoch": 1.2199887681790895,
      "grad_norm": 2.906548023223877,
      "learning_rate": 1.7850893088273365e-05,
      "loss": 1.2447,
      "step": 15750
    },
    {
      "epoch": 1.2207633765177481,
      "grad_norm": 2.5056490898132324,
      "learning_rate": 1.7843127103287603e-05,
      "loss": 1.2225,
      "step": 15760
    },
    {
      "epoch": 1.221537984856407,
      "grad_norm": 2.9018735885620117,
      "learning_rate": 1.7835361118301838e-05,
      "loss": 1.3105,
      "step": 15770
    },
    {
      "epoch": 1.2223125931950658,
      "grad_norm": 2.615896463394165,
      "learning_rate": 1.7827595133316076e-05,
      "loss": 1.1511,
      "step": 15780
    },
    {
      "epoch": 1.2230872015337244,
      "grad_norm": 2.1825785636901855,
      "learning_rate": 1.7819829148330314e-05,
      "loss": 1.289,
      "step": 15790
    },
    {
      "epoch": 1.2238618098723832,
      "grad_norm": 2.7533202171325684,
      "learning_rate": 1.781206316334455e-05,
      "loss": 1.2772,
      "step": 15800
    },
    {
      "epoch": 1.224636418211042,
      "grad_norm": 2.355600357055664,
      "learning_rate": 1.780429717835879e-05,
      "loss": 1.2003,
      "step": 15810
    },
    {
      "epoch": 1.225411026549701,
      "grad_norm": 3.014937162399292,
      "learning_rate": 1.7796531193373027e-05,
      "loss": 1.2321,
      "step": 15820
    },
    {
      "epoch": 1.2261856348883595,
      "grad_norm": 2.631875991821289,
      "learning_rate": 1.7788765208387265e-05,
      "loss": 1.2379,
      "step": 15830
    },
    {
      "epoch": 1.2269602432270184,
      "grad_norm": 2.4876229763031006,
      "learning_rate": 1.7780999223401503e-05,
      "loss": 1.2205,
      "step": 15840
    },
    {
      "epoch": 1.2277348515656772,
      "grad_norm": 2.4012887477874756,
      "learning_rate": 1.777323323841574e-05,
      "loss": 1.2292,
      "step": 15850
    },
    {
      "epoch": 1.2285094599043358,
      "grad_norm": 2.689868688583374,
      "learning_rate": 1.7765467253429975e-05,
      "loss": 1.3166,
      "step": 15860
    },
    {
      "epoch": 1.2292840682429946,
      "grad_norm": 2.3520917892456055,
      "learning_rate": 1.7757701268444213e-05,
      "loss": 1.1977,
      "step": 15870
    },
    {
      "epoch": 1.2300586765816535,
      "grad_norm": 2.2581212520599365,
      "learning_rate": 1.774993528345845e-05,
      "loss": 1.1289,
      "step": 15880
    },
    {
      "epoch": 1.230833284920312,
      "grad_norm": 2.4241585731506348,
      "learning_rate": 1.774216929847269e-05,
      "loss": 1.1682,
      "step": 15890
    },
    {
      "epoch": 1.231607893258971,
      "grad_norm": 2.263484477996826,
      "learning_rate": 1.7734403313486927e-05,
      "loss": 1.2335,
      "step": 15900
    },
    {
      "epoch": 1.2323825015976297,
      "grad_norm": 3.002664566040039,
      "learning_rate": 1.7726637328501165e-05,
      "loss": 1.304,
      "step": 15910
    },
    {
      "epoch": 1.2331571099362884,
      "grad_norm": 3.112123966217041,
      "learning_rate": 1.7718871343515402e-05,
      "loss": 1.2867,
      "step": 15920
    },
    {
      "epoch": 1.2339317182749472,
      "grad_norm": 2.2829360961914062,
      "learning_rate": 1.771110535852964e-05,
      "loss": 1.1967,
      "step": 15930
    },
    {
      "epoch": 1.234706326613606,
      "grad_norm": 2.6370913982391357,
      "learning_rate": 1.7703339373543878e-05,
      "loss": 1.2604,
      "step": 15940
    },
    {
      "epoch": 1.2354809349522649,
      "grad_norm": 2.1853909492492676,
      "learning_rate": 1.7695573388558116e-05,
      "loss": 1.3275,
      "step": 15950
    },
    {
      "epoch": 1.2362555432909235,
      "grad_norm": 2.7325682640075684,
      "learning_rate": 1.7687807403572354e-05,
      "loss": 1.2809,
      "step": 15960
    },
    {
      "epoch": 1.2370301516295823,
      "grad_norm": 2.2638394832611084,
      "learning_rate": 1.7680041418586592e-05,
      "loss": 1.2831,
      "step": 15970
    },
    {
      "epoch": 1.2378047599682411,
      "grad_norm": 2.912722587585449,
      "learning_rate": 1.767227543360083e-05,
      "loss": 1.2465,
      "step": 15980
    },
    {
      "epoch": 1.2385793683068997,
      "grad_norm": 2.4769530296325684,
      "learning_rate": 1.7664509448615067e-05,
      "loss": 1.3306,
      "step": 15990
    },
    {
      "epoch": 1.2393539766455586,
      "grad_norm": 2.765347480773926,
      "learning_rate": 1.7656743463629305e-05,
      "loss": 1.2887,
      "step": 16000
    },
    {
      "epoch": 1.2401285849842174,
      "grad_norm": 2.1882846355438232,
      "learning_rate": 1.7648977478643543e-05,
      "loss": 1.1692,
      "step": 16010
    },
    {
      "epoch": 1.2409031933228762,
      "grad_norm": 2.3267245292663574,
      "learning_rate": 1.764121149365778e-05,
      "loss": 1.2852,
      "step": 16020
    },
    {
      "epoch": 1.2416778016615349,
      "grad_norm": 2.281423330307007,
      "learning_rate": 1.7633445508672016e-05,
      "loss": 1.1782,
      "step": 16030
    },
    {
      "epoch": 1.2424524100001937,
      "grad_norm": 2.900850296020508,
      "learning_rate": 1.7625679523686253e-05,
      "loss": 1.204,
      "step": 16040
    },
    {
      "epoch": 1.2432270183388523,
      "grad_norm": 2.027177572250366,
      "learning_rate": 1.761791353870049e-05,
      "loss": 1.3234,
      "step": 16050
    },
    {
      "epoch": 1.2440016266775111,
      "grad_norm": 2.3950610160827637,
      "learning_rate": 1.761014755371473e-05,
      "loss": 1.1497,
      "step": 16060
    },
    {
      "epoch": 1.24477623501617,
      "grad_norm": 2.258007287979126,
      "learning_rate": 1.7602381568728967e-05,
      "loss": 1.1911,
      "step": 16070
    },
    {
      "epoch": 1.2455508433548288,
      "grad_norm": 2.5259082317352295,
      "learning_rate": 1.7594615583743205e-05,
      "loss": 1.3521,
      "step": 16080
    },
    {
      "epoch": 1.2463254516934874,
      "grad_norm": 2.531369209289551,
      "learning_rate": 1.7586849598757443e-05,
      "loss": 1.2384,
      "step": 16090
    },
    {
      "epoch": 1.2471000600321462,
      "grad_norm": 3.347332000732422,
      "learning_rate": 1.757908361377168e-05,
      "loss": 1.3354,
      "step": 16100
    },
    {
      "epoch": 1.247874668370805,
      "grad_norm": 2.7297682762145996,
      "learning_rate": 1.7571317628785915e-05,
      "loss": 1.3732,
      "step": 16110
    },
    {
      "epoch": 1.2486492767094637,
      "grad_norm": 1.9647947549819946,
      "learning_rate": 1.7563551643800153e-05,
      "loss": 1.2547,
      "step": 16120
    },
    {
      "epoch": 1.2494238850481225,
      "grad_norm": 2.4663660526275635,
      "learning_rate": 1.7555785658814394e-05,
      "loss": 1.2543,
      "step": 16130
    },
    {
      "epoch": 1.2501984933867814,
      "grad_norm": 2.104012966156006,
      "learning_rate": 1.7548019673828632e-05,
      "loss": 1.261,
      "step": 16140
    },
    {
      "epoch": 1.2509731017254402,
      "grad_norm": 3.0455479621887207,
      "learning_rate": 1.754025368884287e-05,
      "loss": 1.3742,
      "step": 16150
    },
    {
      "epoch": 1.2517477100640988,
      "grad_norm": 1.8752442598342896,
      "learning_rate": 1.7532487703857108e-05,
      "loss": 1.3675,
      "step": 16160
    },
    {
      "epoch": 1.2525223184027576,
      "grad_norm": 2.450035333633423,
      "learning_rate": 1.7524721718871346e-05,
      "loss": 1.3236,
      "step": 16170
    },
    {
      "epoch": 1.2532969267414162,
      "grad_norm": 2.1870017051696777,
      "learning_rate": 1.7516955733885584e-05,
      "loss": 1.3568,
      "step": 16180
    },
    {
      "epoch": 1.254071535080075,
      "grad_norm": 2.4594461917877197,
      "learning_rate": 1.750918974889982e-05,
      "loss": 1.3352,
      "step": 16190
    },
    {
      "epoch": 1.254846143418734,
      "grad_norm": 2.2491824626922607,
      "learning_rate": 1.7501423763914056e-05,
      "loss": 1.3176,
      "step": 16200
    },
    {
      "epoch": 1.2556207517573927,
      "grad_norm": 2.032045364379883,
      "learning_rate": 1.7493657778928294e-05,
      "loss": 1.2407,
      "step": 16210
    },
    {
      "epoch": 1.2563953600960516,
      "grad_norm": 2.644491195678711,
      "learning_rate": 1.748589179394253e-05,
      "loss": 1.2082,
      "step": 16220
    },
    {
      "epoch": 1.2571699684347102,
      "grad_norm": 2.903015613555908,
      "learning_rate": 1.747812580895677e-05,
      "loss": 1.3155,
      "step": 16230
    },
    {
      "epoch": 1.257944576773369,
      "grad_norm": 1.909873366355896,
      "learning_rate": 1.7470359823971007e-05,
      "loss": 1.271,
      "step": 16240
    },
    {
      "epoch": 1.2587191851120276,
      "grad_norm": 2.575347661972046,
      "learning_rate": 1.7462593838985245e-05,
      "loss": 1.2486,
      "step": 16250
    },
    {
      "epoch": 1.2594937934506865,
      "grad_norm": 1.8622475862503052,
      "learning_rate": 1.7454827853999483e-05,
      "loss": 1.2821,
      "step": 16260
    },
    {
      "epoch": 1.2602684017893453,
      "grad_norm": 2.71687912940979,
      "learning_rate": 1.744706186901372e-05,
      "loss": 1.3225,
      "step": 16270
    },
    {
      "epoch": 1.2610430101280041,
      "grad_norm": 3.260287284851074,
      "learning_rate": 1.7439295884027955e-05,
      "loss": 1.3469,
      "step": 16280
    },
    {
      "epoch": 1.2618176184666627,
      "grad_norm": 2.5523111820220947,
      "learning_rate": 1.7431529899042193e-05,
      "loss": 1.2235,
      "step": 16290
    },
    {
      "epoch": 1.2625922268053216,
      "grad_norm": 2.1590166091918945,
      "learning_rate": 1.742376391405643e-05,
      "loss": 1.2387,
      "step": 16300
    },
    {
      "epoch": 1.2633668351439804,
      "grad_norm": 2.4864487648010254,
      "learning_rate": 1.741599792907067e-05,
      "loss": 1.2856,
      "step": 16310
    },
    {
      "epoch": 1.264141443482639,
      "grad_norm": 2.892542600631714,
      "learning_rate": 1.7408231944084907e-05,
      "loss": 1.254,
      "step": 16320
    },
    {
      "epoch": 1.2649160518212978,
      "grad_norm": 3.270982027053833,
      "learning_rate": 1.7400465959099148e-05,
      "loss": 1.3106,
      "step": 16330
    },
    {
      "epoch": 1.2656906601599567,
      "grad_norm": 2.2077131271362305,
      "learning_rate": 1.7392699974113386e-05,
      "loss": 1.1771,
      "step": 16340
    },
    {
      "epoch": 1.2664652684986155,
      "grad_norm": 2.401200532913208,
      "learning_rate": 1.7384933989127624e-05,
      "loss": 1.2984,
      "step": 16350
    },
    {
      "epoch": 1.2672398768372741,
      "grad_norm": 2.3245604038238525,
      "learning_rate": 1.7377168004141862e-05,
      "loss": 1.4018,
      "step": 16360
    },
    {
      "epoch": 1.268014485175933,
      "grad_norm": 2.184211254119873,
      "learning_rate": 1.7369402019156096e-05,
      "loss": 1.2667,
      "step": 16370
    },
    {
      "epoch": 1.2687890935145916,
      "grad_norm": 2.3540871143341064,
      "learning_rate": 1.7361636034170334e-05,
      "loss": 1.2614,
      "step": 16380
    },
    {
      "epoch": 1.2695637018532504,
      "grad_norm": 1.7414872646331787,
      "learning_rate": 1.7353870049184572e-05,
      "loss": 1.1869,
      "step": 16390
    },
    {
      "epoch": 1.2703383101919092,
      "grad_norm": 2.8378403186798096,
      "learning_rate": 1.734610406419881e-05,
      "loss": 1.2809,
      "step": 16400
    },
    {
      "epoch": 1.271112918530568,
      "grad_norm": 3.138296365737915,
      "learning_rate": 1.7338338079213048e-05,
      "loss": 1.2173,
      "step": 16410
    },
    {
      "epoch": 1.2718875268692267,
      "grad_norm": 3.21622896194458,
      "learning_rate": 1.7330572094227286e-05,
      "loss": 1.2458,
      "step": 16420
    },
    {
      "epoch": 1.2726621352078855,
      "grad_norm": 2.7376065254211426,
      "learning_rate": 1.7322806109241523e-05,
      "loss": 1.3237,
      "step": 16430
    },
    {
      "epoch": 1.2734367435465443,
      "grad_norm": 2.3886566162109375,
      "learning_rate": 1.731504012425576e-05,
      "loss": 1.2638,
      "step": 16440
    },
    {
      "epoch": 1.274211351885203,
      "grad_norm": 2.2835938930511475,
      "learning_rate": 1.7307274139269996e-05,
      "loss": 1.211,
      "step": 16450
    },
    {
      "epoch": 1.2749859602238618,
      "grad_norm": 2.1510250568389893,
      "learning_rate": 1.7299508154284234e-05,
      "loss": 1.3202,
      "step": 16460
    },
    {
      "epoch": 1.2757605685625206,
      "grad_norm": 2.2061893939971924,
      "learning_rate": 1.729174216929847e-05,
      "loss": 1.2947,
      "step": 16470
    },
    {
      "epoch": 1.2765351769011795,
      "grad_norm": 2.320344924926758,
      "learning_rate": 1.728397618431271e-05,
      "loss": 1.304,
      "step": 16480
    },
    {
      "epoch": 1.277309785239838,
      "grad_norm": 2.785599708557129,
      "learning_rate": 1.7276210199326947e-05,
      "loss": 1.2577,
      "step": 16490
    },
    {
      "epoch": 1.278084393578497,
      "grad_norm": 2.746051788330078,
      "learning_rate": 1.7268444214341185e-05,
      "loss": 1.1855,
      "step": 16500
    },
    {
      "epoch": 1.2788590019171555,
      "grad_norm": 2.2826802730560303,
      "learning_rate": 1.7260678229355423e-05,
      "loss": 1.2383,
      "step": 16510
    },
    {
      "epoch": 1.2796336102558143,
      "grad_norm": 3.2624382972717285,
      "learning_rate": 1.7252912244369664e-05,
      "loss": 1.2275,
      "step": 16520
    },
    {
      "epoch": 1.2804082185944732,
      "grad_norm": 2.4911224842071533,
      "learning_rate": 1.7245146259383902e-05,
      "loss": 1.2243,
      "step": 16530
    },
    {
      "epoch": 1.281182826933132,
      "grad_norm": 3.280102252960205,
      "learning_rate": 1.7237380274398137e-05,
      "loss": 1.236,
      "step": 16540
    },
    {
      "epoch": 1.2819574352717906,
      "grad_norm": 1.9795451164245605,
      "learning_rate": 1.7229614289412374e-05,
      "loss": 1.29,
      "step": 16550
    },
    {
      "epoch": 1.2827320436104495,
      "grad_norm": 2.4090826511383057,
      "learning_rate": 1.7221848304426612e-05,
      "loss": 1.3445,
      "step": 16560
    },
    {
      "epoch": 1.2835066519491083,
      "grad_norm": 2.4479236602783203,
      "learning_rate": 1.721408231944085e-05,
      "loss": 1.33,
      "step": 16570
    },
    {
      "epoch": 1.284281260287767,
      "grad_norm": 2.2462825775146484,
      "learning_rate": 1.7206316334455088e-05,
      "loss": 1.2926,
      "step": 16580
    },
    {
      "epoch": 1.2850558686264257,
      "grad_norm": 2.399182081222534,
      "learning_rate": 1.71993269479679e-05,
      "loss": 1.2501,
      "step": 16590
    },
    {
      "epoch": 1.2858304769650846,
      "grad_norm": 2.9269330501556396,
      "learning_rate": 1.719156096298214e-05,
      "loss": 1.2746,
      "step": 16600
    },
    {
      "epoch": 1.2866050853037434,
      "grad_norm": 2.963714599609375,
      "learning_rate": 1.7183794977996374e-05,
      "loss": 1.2412,
      "step": 16610
    },
    {
      "epoch": 1.287379693642402,
      "grad_norm": 2.756079912185669,
      "learning_rate": 1.717602899301061e-05,
      "loss": 1.2442,
      "step": 16620
    },
    {
      "epoch": 1.2881543019810608,
      "grad_norm": 2.740928888320923,
      "learning_rate": 1.716826300802485e-05,
      "loss": 1.235,
      "step": 16630
    },
    {
      "epoch": 1.2889289103197195,
      "grad_norm": 2.1874401569366455,
      "learning_rate": 1.7160497023039087e-05,
      "loss": 1.2276,
      "step": 16640
    },
    {
      "epoch": 1.2897035186583783,
      "grad_norm": 2.1909334659576416,
      "learning_rate": 1.7152731038053325e-05,
      "loss": 1.2032,
      "step": 16650
    },
    {
      "epoch": 1.2904781269970371,
      "grad_norm": 2.761866331100464,
      "learning_rate": 1.7144965053067566e-05,
      "loss": 1.2409,
      "step": 16660
    },
    {
      "epoch": 1.291252735335696,
      "grad_norm": 2.7452328205108643,
      "learning_rate": 1.7137199068081804e-05,
      "loss": 1.2686,
      "step": 16670
    },
    {
      "epoch": 1.2920273436743548,
      "grad_norm": 2.4574368000030518,
      "learning_rate": 1.7129433083096042e-05,
      "loss": 1.3443,
      "step": 16680
    },
    {
      "epoch": 1.2928019520130134,
      "grad_norm": 2.2542293071746826,
      "learning_rate": 1.712166709811028e-05,
      "loss": 1.3585,
      "step": 16690
    },
    {
      "epoch": 1.2935765603516722,
      "grad_norm": 2.0104284286499023,
      "learning_rate": 1.7113901113124515e-05,
      "loss": 1.3497,
      "step": 16700
    },
    {
      "epoch": 1.2943511686903308,
      "grad_norm": 3.320213794708252,
      "learning_rate": 1.7106135128138752e-05,
      "loss": 1.2083,
      "step": 16710
    },
    {
      "epoch": 1.2951257770289897,
      "grad_norm": 3.3454434871673584,
      "learning_rate": 1.709836914315299e-05,
      "loss": 1.2113,
      "step": 16720
    },
    {
      "epoch": 1.2959003853676485,
      "grad_norm": 2.2343170642852783,
      "learning_rate": 1.7090603158167228e-05,
      "loss": 1.1624,
      "step": 16730
    },
    {
      "epoch": 1.2966749937063073,
      "grad_norm": 2.442742347717285,
      "learning_rate": 1.7082837173181466e-05,
      "loss": 1.2714,
      "step": 16740
    },
    {
      "epoch": 1.297449602044966,
      "grad_norm": 2.211961269378662,
      "learning_rate": 1.7075071188195704e-05,
      "loss": 1.2233,
      "step": 16750
    },
    {
      "epoch": 1.2982242103836248,
      "grad_norm": 2.1100385189056396,
      "learning_rate": 1.7067305203209942e-05,
      "loss": 1.2418,
      "step": 16760
    },
    {
      "epoch": 1.2989988187222836,
      "grad_norm": 2.7002112865448,
      "learning_rate": 1.705953921822418e-05,
      "loss": 1.2798,
      "step": 16770
    },
    {
      "epoch": 1.2997734270609422,
      "grad_norm": 2.342707395553589,
      "learning_rate": 1.7051773233238414e-05,
      "loss": 1.2741,
      "step": 16780
    },
    {
      "epoch": 1.300548035399601,
      "grad_norm": 2.3756418228149414,
      "learning_rate": 1.7044007248252652e-05,
      "loss": 1.1986,
      "step": 16790
    },
    {
      "epoch": 1.30132264373826,
      "grad_norm": 2.2559263706207275,
      "learning_rate": 1.703624126326689e-05,
      "loss": 1.3734,
      "step": 16800
    },
    {
      "epoch": 1.3020972520769187,
      "grad_norm": 3.1814379692077637,
      "learning_rate": 1.7028475278281128e-05,
      "loss": 1.0991,
      "step": 16810
    },
    {
      "epoch": 1.3028718604155773,
      "grad_norm": 3.052398443222046,
      "learning_rate": 1.7020709293295366e-05,
      "loss": 1.167,
      "step": 16820
    },
    {
      "epoch": 1.3036464687542362,
      "grad_norm": 2.773747444152832,
      "learning_rate": 1.7012943308309603e-05,
      "loss": 1.2151,
      "step": 16830
    },
    {
      "epoch": 1.3044210770928948,
      "grad_norm": 3.747527837753296,
      "learning_rate": 1.700517732332384e-05,
      "loss": 1.2815,
      "step": 16840
    },
    {
      "epoch": 1.3051956854315536,
      "grad_norm": 2.2486047744750977,
      "learning_rate": 1.699741133833808e-05,
      "loss": 1.2906,
      "step": 16850
    },
    {
      "epoch": 1.3059702937702125,
      "grad_norm": 2.23207688331604,
      "learning_rate": 1.698964535335232e-05,
      "loss": 1.3215,
      "step": 16860
    },
    {
      "epoch": 1.3067449021088713,
      "grad_norm": 2.2528109550476074,
      "learning_rate": 1.6981879368366555e-05,
      "loss": 1.2671,
      "step": 16870
    },
    {
      "epoch": 1.30751951044753,
      "grad_norm": 2.796217441558838,
      "learning_rate": 1.6974113383380793e-05,
      "loss": 1.2312,
      "step": 16880
    },
    {
      "epoch": 1.3082941187861887,
      "grad_norm": 2.2283031940460205,
      "learning_rate": 1.696634739839503e-05,
      "loss": 1.3432,
      "step": 16890
    },
    {
      "epoch": 1.3090687271248476,
      "grad_norm": 1.819122076034546,
      "learning_rate": 1.695858141340927e-05,
      "loss": 1.2613,
      "step": 16900
    },
    {
      "epoch": 1.3098433354635062,
      "grad_norm": 3.3355395793914795,
      "learning_rate": 1.6950815428423506e-05,
      "loss": 1.349,
      "step": 16910
    },
    {
      "epoch": 1.310617943802165,
      "grad_norm": 2.3559505939483643,
      "learning_rate": 1.6943049443437744e-05,
      "loss": 1.2821,
      "step": 16920
    },
    {
      "epoch": 1.3113925521408238,
      "grad_norm": 2.5627853870391846,
      "learning_rate": 1.6935283458451982e-05,
      "loss": 1.2024,
      "step": 16930
    },
    {
      "epoch": 1.3121671604794827,
      "grad_norm": 3.3939008712768555,
      "learning_rate": 1.692751747346622e-05,
      "loss": 1.2256,
      "step": 16940
    },
    {
      "epoch": 1.3129417688181413,
      "grad_norm": 2.559553623199463,
      "learning_rate": 1.6919751488480454e-05,
      "loss": 1.2075,
      "step": 16950
    },
    {
      "epoch": 1.3137163771568001,
      "grad_norm": 2.5945489406585693,
      "learning_rate": 1.6911985503494692e-05,
      "loss": 1.2506,
      "step": 16960
    },
    {
      "epoch": 1.3144909854954587,
      "grad_norm": 3.4599313735961914,
      "learning_rate": 1.690421951850893e-05,
      "loss": 1.106,
      "step": 16970
    },
    {
      "epoch": 1.3152655938341176,
      "grad_norm": 3.2724616527557373,
      "learning_rate": 1.6896453533523168e-05,
      "loss": 1.2801,
      "step": 16980
    },
    {
      "epoch": 1.3160402021727764,
      "grad_norm": 2.1919307708740234,
      "learning_rate": 1.6888687548537406e-05,
      "loss": 1.3249,
      "step": 16990
    },
    {
      "epoch": 1.3168148105114352,
      "grad_norm": 2.3598756790161133,
      "learning_rate": 1.6880921563551644e-05,
      "loss": 1.2186,
      "step": 17000
    },
    {
      "epoch": 1.3175894188500938,
      "grad_norm": 1.8601683378219604,
      "learning_rate": 1.687315557856588e-05,
      "loss": 1.2255,
      "step": 17010
    },
    {
      "epoch": 1.3183640271887527,
      "grad_norm": 2.439777135848999,
      "learning_rate": 1.686538959358012e-05,
      "loss": 1.2614,
      "step": 17020
    },
    {
      "epoch": 1.3191386355274115,
      "grad_norm": 1.9909402132034302,
      "learning_rate": 1.6857623608594357e-05,
      "loss": 1.1745,
      "step": 17030
    },
    {
      "epoch": 1.3199132438660701,
      "grad_norm": 2.050729990005493,
      "learning_rate": 1.6849857623608592e-05,
      "loss": 1.2786,
      "step": 17040
    },
    {
      "epoch": 1.320687852204729,
      "grad_norm": 2.741492986679077,
      "learning_rate": 1.6842091638622833e-05,
      "loss": 1.3017,
      "step": 17050
    },
    {
      "epoch": 1.3214624605433878,
      "grad_norm": 2.881566286087036,
      "learning_rate": 1.683432565363707e-05,
      "loss": 1.3159,
      "step": 17060
    },
    {
      "epoch": 1.3222370688820466,
      "grad_norm": 2.4376423358917236,
      "learning_rate": 1.682655966865131e-05,
      "loss": 1.1521,
      "step": 17070
    },
    {
      "epoch": 1.3230116772207052,
      "grad_norm": 2.436025381088257,
      "learning_rate": 1.6818793683665547e-05,
      "loss": 1.3021,
      "step": 17080
    },
    {
      "epoch": 1.323786285559364,
      "grad_norm": 2.3671836853027344,
      "learning_rate": 1.6811027698679785e-05,
      "loss": 1.267,
      "step": 17090
    },
    {
      "epoch": 1.324560893898023,
      "grad_norm": 2.3514816761016846,
      "learning_rate": 1.6803261713694022e-05,
      "loss": 1.1692,
      "step": 17100
    },
    {
      "epoch": 1.3253355022366815,
      "grad_norm": 1.9354223012924194,
      "learning_rate": 1.679549572870826e-05,
      "loss": 1.3152,
      "step": 17110
    },
    {
      "epoch": 1.3261101105753403,
      "grad_norm": 2.489208936691284,
      "learning_rate": 1.6787729743722495e-05,
      "loss": 1.2816,
      "step": 17120
    },
    {
      "epoch": 1.3268847189139992,
      "grad_norm": 2.2949280738830566,
      "learning_rate": 1.6779963758736733e-05,
      "loss": 1.2607,
      "step": 17130
    },
    {
      "epoch": 1.327659327252658,
      "grad_norm": 2.972055196762085,
      "learning_rate": 1.677219777375097e-05,
      "loss": 1.252,
      "step": 17140
    },
    {
      "epoch": 1.3284339355913166,
      "grad_norm": 2.714911699295044,
      "learning_rate": 1.676443178876521e-05,
      "loss": 1.2616,
      "step": 17150
    },
    {
      "epoch": 1.3292085439299755,
      "grad_norm": 2.0125372409820557,
      "learning_rate": 1.6756665803779446e-05,
      "loss": 1.3014,
      "step": 17160
    },
    {
      "epoch": 1.329983152268634,
      "grad_norm": 3.0069351196289062,
      "learning_rate": 1.6748899818793684e-05,
      "loss": 1.2756,
      "step": 17170
    },
    {
      "epoch": 1.330757760607293,
      "grad_norm": 2.3030476570129395,
      "learning_rate": 1.6741133833807922e-05,
      "loss": 1.2251,
      "step": 17180
    },
    {
      "epoch": 1.3315323689459517,
      "grad_norm": 2.9623377323150635,
      "learning_rate": 1.673336784882216e-05,
      "loss": 1.2022,
      "step": 17190
    },
    {
      "epoch": 1.3323069772846106,
      "grad_norm": 3.5551507472991943,
      "learning_rate": 1.6725601863836398e-05,
      "loss": 1.2856,
      "step": 17200
    },
    {
      "epoch": 1.3330815856232692,
      "grad_norm": 2.5778021812438965,
      "learning_rate": 1.6717835878850632e-05,
      "loss": 1.2641,
      "step": 17210
    },
    {
      "epoch": 1.333856193961928,
      "grad_norm": 2.6678006649017334,
      "learning_rate": 1.671006989386487e-05,
      "loss": 1.2708,
      "step": 17220
    },
    {
      "epoch": 1.3346308023005868,
      "grad_norm": 2.4692277908325195,
      "learning_rate": 1.6702303908879108e-05,
      "loss": 1.3518,
      "step": 17230
    },
    {
      "epoch": 1.3354054106392454,
      "grad_norm": 2.3578343391418457,
      "learning_rate": 1.6694537923893346e-05,
      "loss": 1.1949,
      "step": 17240
    },
    {
      "epoch": 1.3361800189779043,
      "grad_norm": 2.1127216815948486,
      "learning_rate": 1.6686771938907587e-05,
      "loss": 1.3187,
      "step": 17250
    },
    {
      "epoch": 1.3369546273165631,
      "grad_norm": 2.5119097232818604,
      "learning_rate": 1.6679005953921825e-05,
      "loss": 1.324,
      "step": 17260
    },
    {
      "epoch": 1.337729235655222,
      "grad_norm": 3.500593423843384,
      "learning_rate": 1.6671239968936063e-05,
      "loss": 1.2974,
      "step": 17270
    },
    {
      "epoch": 1.3385038439938806,
      "grad_norm": 2.3949756622314453,
      "learning_rate": 1.66634739839503e-05,
      "loss": 1.2329,
      "step": 17280
    },
    {
      "epoch": 1.3392784523325394,
      "grad_norm": 2.432720422744751,
      "learning_rate": 1.6655707998964535e-05,
      "loss": 1.1922,
      "step": 17290
    },
    {
      "epoch": 1.340053060671198,
      "grad_norm": 2.072359323501587,
      "learning_rate": 1.6647942013978773e-05,
      "loss": 1.264,
      "step": 17300
    },
    {
      "epoch": 1.3408276690098568,
      "grad_norm": 2.268594741821289,
      "learning_rate": 1.664017602899301e-05,
      "loss": 1.2386,
      "step": 17310
    },
    {
      "epoch": 1.3416022773485157,
      "grad_norm": 2.4794921875,
      "learning_rate": 1.663241004400725e-05,
      "loss": 1.2211,
      "step": 17320
    },
    {
      "epoch": 1.3423768856871745,
      "grad_norm": 2.1205215454101562,
      "learning_rate": 1.6624644059021487e-05,
      "loss": 1.2709,
      "step": 17330
    },
    {
      "epoch": 1.3431514940258331,
      "grad_norm": 2.4589874744415283,
      "learning_rate": 1.6616878074035724e-05,
      "loss": 1.2369,
      "step": 17340
    },
    {
      "epoch": 1.343926102364492,
      "grad_norm": 2.236632823944092,
      "learning_rate": 1.6609112089049962e-05,
      "loss": 1.2179,
      "step": 17350
    },
    {
      "epoch": 1.3447007107031508,
      "grad_norm": 2.0955448150634766,
      "learning_rate": 1.66013461040642e-05,
      "loss": 1.3331,
      "step": 17360
    },
    {
      "epoch": 1.3454753190418094,
      "grad_norm": 3.0352933406829834,
      "learning_rate": 1.6593580119078438e-05,
      "loss": 1.3414,
      "step": 17370
    },
    {
      "epoch": 1.3462499273804682,
      "grad_norm": 2.0828466415405273,
      "learning_rate": 1.6585814134092673e-05,
      "loss": 1.2557,
      "step": 17380
    },
    {
      "epoch": 1.347024535719127,
      "grad_norm": 1.8376070261001587,
      "learning_rate": 1.657804814910691e-05,
      "loss": 1.3271,
      "step": 17390
    },
    {
      "epoch": 1.347799144057786,
      "grad_norm": 2.305760383605957,
      "learning_rate": 1.6570282164121148e-05,
      "loss": 1.3155,
      "step": 17400
    },
    {
      "epoch": 1.3485737523964445,
      "grad_norm": 1.8228545188903809,
      "learning_rate": 1.6562516179135386e-05,
      "loss": 1.3081,
      "step": 17410
    },
    {
      "epoch": 1.3493483607351033,
      "grad_norm": 3.342933416366577,
      "learning_rate": 1.6554750194149624e-05,
      "loss": 1.1985,
      "step": 17420
    },
    {
      "epoch": 1.350122969073762,
      "grad_norm": 2.0253143310546875,
      "learning_rate": 1.6546984209163862e-05,
      "loss": 1.3551,
      "step": 17430
    },
    {
      "epoch": 1.3508975774124208,
      "grad_norm": 2.7479770183563232,
      "learning_rate": 1.65392182241781e-05,
      "loss": 1.3529,
      "step": 17440
    },
    {
      "epoch": 1.3516721857510796,
      "grad_norm": 2.903312921524048,
      "learning_rate": 1.653145223919234e-05,
      "loss": 1.2519,
      "step": 17450
    },
    {
      "epoch": 1.3524467940897384,
      "grad_norm": 2.358335256576538,
      "learning_rate": 1.6523686254206575e-05,
      "loss": 1.2142,
      "step": 17460
    },
    {
      "epoch": 1.3532214024283973,
      "grad_norm": 2.038174629211426,
      "learning_rate": 1.6515920269220813e-05,
      "loss": 1.1602,
      "step": 17470
    },
    {
      "epoch": 1.353996010767056,
      "grad_norm": 2.564399480819702,
      "learning_rate": 1.650815428423505e-05,
      "loss": 1.2967,
      "step": 17480
    },
    {
      "epoch": 1.3547706191057147,
      "grad_norm": 2.521456003189087,
      "learning_rate": 1.650038829924929e-05,
      "loss": 1.2655,
      "step": 17490
    },
    {
      "epoch": 1.3555452274443733,
      "grad_norm": 2.052565097808838,
      "learning_rate": 1.6492622314263527e-05,
      "loss": 1.4032,
      "step": 17500
    },
    {
      "epoch": 1.3563198357830322,
      "grad_norm": 3.089188814163208,
      "learning_rate": 1.6484856329277765e-05,
      "loss": 1.1772,
      "step": 17510
    },
    {
      "epoch": 1.357094444121691,
      "grad_norm": 1.9659086465835571,
      "learning_rate": 1.6477090344292003e-05,
      "loss": 1.2734,
      "step": 17520
    },
    {
      "epoch": 1.3578690524603498,
      "grad_norm": 1.949747085571289,
      "learning_rate": 1.646932435930624e-05,
      "loss": 1.2176,
      "step": 17530
    },
    {
      "epoch": 1.3586436607990084,
      "grad_norm": 2.4716808795928955,
      "learning_rate": 1.646155837432048e-05,
      "loss": 1.2533,
      "step": 17540
    },
    {
      "epoch": 1.3594182691376673,
      "grad_norm": 3.01041841506958,
      "learning_rate": 1.6453792389334713e-05,
      "loss": 1.2235,
      "step": 17550
    },
    {
      "epoch": 1.3601928774763261,
      "grad_norm": 2.131549119949341,
      "learning_rate": 1.644602640434895e-05,
      "loss": 1.2732,
      "step": 17560
    },
    {
      "epoch": 1.3609674858149847,
      "grad_norm": 2.3952038288116455,
      "learning_rate": 1.643826041936319e-05,
      "loss": 1.29,
      "step": 17570
    },
    {
      "epoch": 1.3617420941536436,
      "grad_norm": 2.754183530807495,
      "learning_rate": 1.6430494434377426e-05,
      "loss": 1.2609,
      "step": 17580
    },
    {
      "epoch": 1.3625167024923024,
      "grad_norm": 2.547353744506836,
      "learning_rate": 1.6422728449391664e-05,
      "loss": 1.2382,
      "step": 17590
    },
    {
      "epoch": 1.3632913108309612,
      "grad_norm": 2.467923879623413,
      "learning_rate": 1.6414962464405902e-05,
      "loss": 1.3041,
      "step": 17600
    },
    {
      "epoch": 1.3640659191696198,
      "grad_norm": 2.2547593116760254,
      "learning_rate": 1.640719647942014e-05,
      "loss": 1.3609,
      "step": 17610
    },
    {
      "epoch": 1.3648405275082787,
      "grad_norm": 2.5217323303222656,
      "learning_rate": 1.6399430494434378e-05,
      "loss": 1.2944,
      "step": 17620
    },
    {
      "epoch": 1.3656151358469373,
      "grad_norm": 2.180116891860962,
      "learning_rate": 1.6391664509448612e-05,
      "loss": 1.2084,
      "step": 17630
    },
    {
      "epoch": 1.3663897441855961,
      "grad_norm": 2.146198034286499,
      "learning_rate": 1.6383898524462854e-05,
      "loss": 1.2118,
      "step": 17640
    },
    {
      "epoch": 1.367164352524255,
      "grad_norm": 2.2632665634155273,
      "learning_rate": 1.637613253947709e-05,
      "loss": 1.3127,
      "step": 17650
    },
    {
      "epoch": 1.3679389608629138,
      "grad_norm": 2.4559879302978516,
      "learning_rate": 1.636836655449133e-05,
      "loss": 1.2401,
      "step": 17660
    },
    {
      "epoch": 1.3687135692015724,
      "grad_norm": 2.7143092155456543,
      "learning_rate": 1.6360600569505567e-05,
      "loss": 1.3224,
      "step": 17670
    },
    {
      "epoch": 1.3694881775402312,
      "grad_norm": 3.5086019039154053,
      "learning_rate": 1.6352834584519805e-05,
      "loss": 1.1778,
      "step": 17680
    },
    {
      "epoch": 1.37026278587889,
      "grad_norm": 2.3959801197052,
      "learning_rate": 1.6345068599534043e-05,
      "loss": 1.3092,
      "step": 17690
    },
    {
      "epoch": 1.3710373942175487,
      "grad_norm": 2.7491812705993652,
      "learning_rate": 1.633730261454828e-05,
      "loss": 1.2245,
      "step": 17700
    },
    {
      "epoch": 1.3718120025562075,
      "grad_norm": 2.6357831954956055,
      "learning_rate": 1.632953662956252e-05,
      "loss": 1.2729,
      "step": 17710
    },
    {
      "epoch": 1.3725866108948663,
      "grad_norm": 2.758168935775757,
      "learning_rate": 1.6321770644576753e-05,
      "loss": 1.3109,
      "step": 17720
    },
    {
      "epoch": 1.3733612192335252,
      "grad_norm": 2.1600492000579834,
      "learning_rate": 1.631400465959099e-05,
      "loss": 1.2634,
      "step": 17730
    },
    {
      "epoch": 1.3741358275721838,
      "grad_norm": 3.488616943359375,
      "learning_rate": 1.630623867460523e-05,
      "loss": 1.2566,
      "step": 17740
    },
    {
      "epoch": 1.3749104359108426,
      "grad_norm": 2.9122753143310547,
      "learning_rate": 1.6298472689619467e-05,
      "loss": 1.2155,
      "step": 17750
    },
    {
      "epoch": 1.3756850442495012,
      "grad_norm": 2.5154333114624023,
      "learning_rate": 1.6290706704633705e-05,
      "loss": 1.368,
      "step": 17760
    },
    {
      "epoch": 1.37645965258816,
      "grad_norm": 2.3311233520507812,
      "learning_rate": 1.6282940719647943e-05,
      "loss": 1.3825,
      "step": 17770
    },
    {
      "epoch": 1.3772342609268189,
      "grad_norm": 2.2258923053741455,
      "learning_rate": 1.627517473466218e-05,
      "loss": 1.3018,
      "step": 17780
    },
    {
      "epoch": 1.3780088692654777,
      "grad_norm": 2.254674196243286,
      "learning_rate": 1.6267408749676418e-05,
      "loss": 1.3115,
      "step": 17790
    },
    {
      "epoch": 1.3787834776041363,
      "grad_norm": 2.422285795211792,
      "learning_rate": 1.6259642764690653e-05,
      "loss": 1.2632,
      "step": 17800
    },
    {
      "epoch": 1.3795580859427952,
      "grad_norm": 2.568173885345459,
      "learning_rate": 1.625187677970489e-05,
      "loss": 1.2091,
      "step": 17810
    },
    {
      "epoch": 1.380332694281454,
      "grad_norm": 2.8342044353485107,
      "learning_rate": 1.624411079471913e-05,
      "loss": 1.2474,
      "step": 17820
    },
    {
      "epoch": 1.3811073026201126,
      "grad_norm": 2.333609104156494,
      "learning_rate": 1.6236344809733366e-05,
      "loss": 1.3215,
      "step": 17830
    },
    {
      "epoch": 1.3818819109587714,
      "grad_norm": 3.494884729385376,
      "learning_rate": 1.6228578824747608e-05,
      "loss": 1.2459,
      "step": 17840
    },
    {
      "epoch": 1.3826565192974303,
      "grad_norm": 2.3783090114593506,
      "learning_rate": 1.6220812839761845e-05,
      "loss": 1.2804,
      "step": 17850
    },
    {
      "epoch": 1.383431127636089,
      "grad_norm": 2.4432601928710938,
      "learning_rate": 1.6213046854776083e-05,
      "loss": 1.2686,
      "step": 17860
    },
    {
      "epoch": 1.3842057359747477,
      "grad_norm": 2.6301281452178955,
      "learning_rate": 1.620528086979032e-05,
      "loss": 1.2698,
      "step": 17870
    },
    {
      "epoch": 1.3849803443134066,
      "grad_norm": 2.5751895904541016,
      "learning_rate": 1.6197514884804556e-05,
      "loss": 1.1969,
      "step": 17880
    },
    {
      "epoch": 1.3857549526520652,
      "grad_norm": 2.684929609298706,
      "learning_rate": 1.6189748899818794e-05,
      "loss": 1.2557,
      "step": 17890
    },
    {
      "epoch": 1.386529560990724,
      "grad_norm": 2.635124921798706,
      "learning_rate": 1.618198291483303e-05,
      "loss": 1.1359,
      "step": 17900
    },
    {
      "epoch": 1.3873041693293828,
      "grad_norm": 2.1122186183929443,
      "learning_rate": 1.617421692984727e-05,
      "loss": 1.1534,
      "step": 17910
    },
    {
      "epoch": 1.3880787776680417,
      "grad_norm": 3.0007801055908203,
      "learning_rate": 1.6166450944861507e-05,
      "loss": 1.2633,
      "step": 17920
    },
    {
      "epoch": 1.3888533860067005,
      "grad_norm": 2.634079694747925,
      "learning_rate": 1.6158684959875745e-05,
      "loss": 1.3285,
      "step": 17930
    },
    {
      "epoch": 1.389627994345359,
      "grad_norm": 2.230760335922241,
      "learning_rate": 1.6150918974889983e-05,
      "loss": 1.2172,
      "step": 17940
    },
    {
      "epoch": 1.390402602684018,
      "grad_norm": 2.1072170734405518,
      "learning_rate": 1.614315298990422e-05,
      "loss": 1.3214,
      "step": 17950
    },
    {
      "epoch": 1.3911772110226766,
      "grad_norm": 2.2227251529693604,
      "learning_rate": 1.613538700491846e-05,
      "loss": 1.233,
      "step": 17960
    },
    {
      "epoch": 1.3919518193613354,
      "grad_norm": 2.394893169403076,
      "learning_rate": 1.6127621019932693e-05,
      "loss": 1.2327,
      "step": 17970
    },
    {
      "epoch": 1.3927264276999942,
      "grad_norm": 2.949195384979248,
      "learning_rate": 1.611985503494693e-05,
      "loss": 1.2556,
      "step": 17980
    },
    {
      "epoch": 1.393501036038653,
      "grad_norm": 2.1430463790893555,
      "learning_rate": 1.611208904996117e-05,
      "loss": 1.1566,
      "step": 17990
    },
    {
      "epoch": 1.3942756443773117,
      "grad_norm": 2.9172439575195312,
      "learning_rate": 1.6104323064975407e-05,
      "loss": 1.2619,
      "step": 18000
    },
    {
      "epoch": 1.3950502527159705,
      "grad_norm": 2.0244364738464355,
      "learning_rate": 1.6096557079989645e-05,
      "loss": 1.2786,
      "step": 18010
    },
    {
      "epoch": 1.3958248610546293,
      "grad_norm": 3.1057662963867188,
      "learning_rate": 1.6088791095003882e-05,
      "loss": 1.2095,
      "step": 18020
    },
    {
      "epoch": 1.396599469393288,
      "grad_norm": 2.2272627353668213,
      "learning_rate": 1.6081025110018124e-05,
      "loss": 1.3591,
      "step": 18030
    },
    {
      "epoch": 1.3973740777319468,
      "grad_norm": 2.2171711921691895,
      "learning_rate": 1.607325912503236e-05,
      "loss": 1.3119,
      "step": 18040
    },
    {
      "epoch": 1.3981486860706056,
      "grad_norm": 3.332383871078491,
      "learning_rate": 1.6065493140046596e-05,
      "loss": 1.3152,
      "step": 18050
    },
    {
      "epoch": 1.3989232944092644,
      "grad_norm": 1.7477242946624756,
      "learning_rate": 1.6057727155060834e-05,
      "loss": 1.2869,
      "step": 18060
    },
    {
      "epoch": 1.399697902747923,
      "grad_norm": 1.9324522018432617,
      "learning_rate": 1.6049961170075072e-05,
      "loss": 1.2936,
      "step": 18070
    },
    {
      "epoch": 1.4004725110865819,
      "grad_norm": 2.1882846355438232,
      "learning_rate": 1.604219518508931e-05,
      "loss": 1.1857,
      "step": 18080
    },
    {
      "epoch": 1.4012471194252405,
      "grad_norm": 3.1304876804351807,
      "learning_rate": 1.6034429200103547e-05,
      "loss": 1.2709,
      "step": 18090
    },
    {
      "epoch": 1.4020217277638993,
      "grad_norm": 2.8504467010498047,
      "learning_rate": 1.6026663215117785e-05,
      "loss": 1.2443,
      "step": 18100
    },
    {
      "epoch": 1.4027963361025582,
      "grad_norm": 2.5297110080718994,
      "learning_rate": 1.6018897230132023e-05,
      "loss": 1.2713,
      "step": 18110
    },
    {
      "epoch": 1.403570944441217,
      "grad_norm": 2.1649606227874756,
      "learning_rate": 1.601113124514626e-05,
      "loss": 1.311,
      "step": 18120
    },
    {
      "epoch": 1.4043455527798756,
      "grad_norm": 3.149773120880127,
      "learning_rate": 1.60033652601605e-05,
      "loss": 1.2984,
      "step": 18130
    },
    {
      "epoch": 1.4051201611185344,
      "grad_norm": 2.597285270690918,
      "learning_rate": 1.5995599275174733e-05,
      "loss": 1.3477,
      "step": 18140
    },
    {
      "epoch": 1.4058947694571933,
      "grad_norm": 2.5366547107696533,
      "learning_rate": 1.598783329018897e-05,
      "loss": 1.2576,
      "step": 18150
    },
    {
      "epoch": 1.4066693777958519,
      "grad_norm": 2.0616695880889893,
      "learning_rate": 1.598006730520321e-05,
      "loss": 1.2337,
      "step": 18160
    },
    {
      "epoch": 1.4074439861345107,
      "grad_norm": 1.9020735025405884,
      "learning_rate": 1.5972301320217447e-05,
      "loss": 1.1922,
      "step": 18170
    },
    {
      "epoch": 1.4082185944731695,
      "grad_norm": 2.457871675491333,
      "learning_rate": 1.5964535335231685e-05,
      "loss": 1.322,
      "step": 18180
    },
    {
      "epoch": 1.4089932028118284,
      "grad_norm": 2.6461308002471924,
      "learning_rate": 1.5956769350245923e-05,
      "loss": 1.2526,
      "step": 18190
    },
    {
      "epoch": 1.409767811150487,
      "grad_norm": 2.051494598388672,
      "learning_rate": 1.594900336526016e-05,
      "loss": 1.1866,
      "step": 18200
    },
    {
      "epoch": 1.4105424194891458,
      "grad_norm": 2.702281951904297,
      "learning_rate": 1.59412373802744e-05,
      "loss": 1.3252,
      "step": 18210
    },
    {
      "epoch": 1.4113170278278044,
      "grad_norm": 2.498896598815918,
      "learning_rate": 1.5933471395288633e-05,
      "loss": 1.3189,
      "step": 18220
    },
    {
      "epoch": 1.4120916361664633,
      "grad_norm": 2.4944169521331787,
      "learning_rate": 1.5925705410302874e-05,
      "loss": 1.1735,
      "step": 18230
    },
    {
      "epoch": 1.412866244505122,
      "grad_norm": 2.897348642349243,
      "learning_rate": 1.5917939425317112e-05,
      "loss": 1.2077,
      "step": 18240
    },
    {
      "epoch": 1.413640852843781,
      "grad_norm": 2.651259422302246,
      "learning_rate": 1.591017344033135e-05,
      "loss": 1.2973,
      "step": 18250
    },
    {
      "epoch": 1.4144154611824395,
      "grad_norm": 3.0529885292053223,
      "learning_rate": 1.5902407455345588e-05,
      "loss": 1.232,
      "step": 18260
    },
    {
      "epoch": 1.4151900695210984,
      "grad_norm": 2.5718374252319336,
      "learning_rate": 1.5894641470359826e-05,
      "loss": 1.156,
      "step": 18270
    },
    {
      "epoch": 1.4159646778597572,
      "grad_norm": 2.288245677947998,
      "learning_rate": 1.5886875485374064e-05,
      "loss": 1.2035,
      "step": 18280
    },
    {
      "epoch": 1.4167392861984158,
      "grad_norm": 2.9703116416931152,
      "learning_rate": 1.58791095003883e-05,
      "loss": 1.1866,
      "step": 18290
    },
    {
      "epoch": 1.4175138945370747,
      "grad_norm": 2.870126247406006,
      "learning_rate": 1.587134351540254e-05,
      "loss": 1.3093,
      "step": 18300
    },
    {
      "epoch": 1.4182885028757335,
      "grad_norm": 2.021641969680786,
      "learning_rate": 1.5863577530416774e-05,
      "loss": 1.2682,
      "step": 18310
    },
    {
      "epoch": 1.4190631112143923,
      "grad_norm": 2.0719027519226074,
      "learning_rate": 1.585581154543101e-05,
      "loss": 1.2392,
      "step": 18320
    },
    {
      "epoch": 1.419837719553051,
      "grad_norm": 2.1302413940429688,
      "learning_rate": 1.584804556044525e-05,
      "loss": 1.3296,
      "step": 18330
    },
    {
      "epoch": 1.4206123278917098,
      "grad_norm": 2.7426202297210693,
      "learning_rate": 1.5840279575459487e-05,
      "loss": 1.1875,
      "step": 18340
    },
    {
      "epoch": 1.4213869362303686,
      "grad_norm": 3.1777822971343994,
      "learning_rate": 1.5832513590473725e-05,
      "loss": 1.2691,
      "step": 18350
    },
    {
      "epoch": 1.4221615445690272,
      "grad_norm": 2.458364486694336,
      "learning_rate": 1.5824747605487963e-05,
      "loss": 1.3664,
      "step": 18360
    },
    {
      "epoch": 1.422936152907686,
      "grad_norm": 2.2612316608428955,
      "learning_rate": 1.58169816205022e-05,
      "loss": 1.2744,
      "step": 18370
    },
    {
      "epoch": 1.4237107612463449,
      "grad_norm": 2.4861204624176025,
      "learning_rate": 1.580921563551644e-05,
      "loss": 1.2459,
      "step": 18380
    },
    {
      "epoch": 1.4244853695850037,
      "grad_norm": 2.3873531818389893,
      "learning_rate": 1.5801449650530673e-05,
      "loss": 1.2532,
      "step": 18390
    },
    {
      "epoch": 1.4252599779236623,
      "grad_norm": 1.578721284866333,
      "learning_rate": 1.579368366554491e-05,
      "loss": 1.2152,
      "step": 18400
    },
    {
      "epoch": 1.4260345862623212,
      "grad_norm": 2.09676456451416,
      "learning_rate": 1.578591768055915e-05,
      "loss": 1.3101,
      "step": 18410
    },
    {
      "epoch": 1.4268091946009798,
      "grad_norm": 2.0891342163085938,
      "learning_rate": 1.5778151695573387e-05,
      "loss": 1.2691,
      "step": 18420
    },
    {
      "epoch": 1.4275838029396386,
      "grad_norm": 2.2193758487701416,
      "learning_rate": 1.5770385710587628e-05,
      "loss": 1.2762,
      "step": 18430
    },
    {
      "epoch": 1.4283584112782974,
      "grad_norm": 1.7486952543258667,
      "learning_rate": 1.5762619725601866e-05,
      "loss": 1.1874,
      "step": 18440
    },
    {
      "epoch": 1.4291330196169563,
      "grad_norm": 2.692946672439575,
      "learning_rate": 1.5754853740616104e-05,
      "loss": 1.2771,
      "step": 18450
    },
    {
      "epoch": 1.4299076279556149,
      "grad_norm": 3.245044708251953,
      "learning_rate": 1.5747087755630342e-05,
      "loss": 1.1947,
      "step": 18460
    },
    {
      "epoch": 1.4306822362942737,
      "grad_norm": 2.717820405960083,
      "learning_rate": 1.573932177064458e-05,
      "loss": 1.2609,
      "step": 18470
    },
    {
      "epoch": 1.4314568446329325,
      "grad_norm": 2.6853322982788086,
      "learning_rate": 1.5731555785658814e-05,
      "loss": 1.1886,
      "step": 18480
    },
    {
      "epoch": 1.4322314529715912,
      "grad_norm": 2.878354549407959,
      "learning_rate": 1.5723789800673052e-05,
      "loss": 1.2702,
      "step": 18490
    },
    {
      "epoch": 1.43300606131025,
      "grad_norm": 1.900839924812317,
      "learning_rate": 1.571602381568729e-05,
      "loss": 1.2689,
      "step": 18500
    },
    {
      "epoch": 1.4337806696489088,
      "grad_norm": 3.10050630569458,
      "learning_rate": 1.5708257830701528e-05,
      "loss": 1.2914,
      "step": 18510
    },
    {
      "epoch": 1.4345552779875677,
      "grad_norm": 2.4975368976593018,
      "learning_rate": 1.5700491845715766e-05,
      "loss": 1.2753,
      "step": 18520
    },
    {
      "epoch": 1.4353298863262263,
      "grad_norm": 2.3305270671844482,
      "learning_rate": 1.5692725860730003e-05,
      "loss": 1.3254,
      "step": 18530
    },
    {
      "epoch": 1.436104494664885,
      "grad_norm": 1.7097865343093872,
      "learning_rate": 1.568495987574424e-05,
      "loss": 1.251,
      "step": 18540
    },
    {
      "epoch": 1.4368791030035437,
      "grad_norm": 2.7126176357269287,
      "learning_rate": 1.567719389075848e-05,
      "loss": 1.338,
      "step": 18550
    },
    {
      "epoch": 1.4376537113422025,
      "grad_norm": 2.0615055561065674,
      "learning_rate": 1.5669427905772714e-05,
      "loss": 1.3256,
      "step": 18560
    },
    {
      "epoch": 1.4384283196808614,
      "grad_norm": 3.0661509037017822,
      "learning_rate": 1.566166192078695e-05,
      "loss": 1.3036,
      "step": 18570
    },
    {
      "epoch": 1.4392029280195202,
      "grad_norm": 2.399064540863037,
      "learning_rate": 1.565389593580119e-05,
      "loss": 1.2862,
      "step": 18580
    },
    {
      "epoch": 1.4399775363581788,
      "grad_norm": 2.2602672576904297,
      "learning_rate": 1.5646129950815427e-05,
      "loss": 1.1916,
      "step": 18590
    },
    {
      "epoch": 1.4407521446968377,
      "grad_norm": 3.0665245056152344,
      "learning_rate": 1.5638363965829665e-05,
      "loss": 1.1845,
      "step": 18600
    },
    {
      "epoch": 1.4415267530354965,
      "grad_norm": 3.8264527320861816,
      "learning_rate": 1.5630597980843903e-05,
      "loss": 1.2938,
      "step": 18610
    },
    {
      "epoch": 1.442301361374155,
      "grad_norm": 3.3662538528442383,
      "learning_rate": 1.5622831995858144e-05,
      "loss": 1.1569,
      "step": 18620
    },
    {
      "epoch": 1.443075969712814,
      "grad_norm": 2.2399301528930664,
      "learning_rate": 1.5615066010872382e-05,
      "loss": 1.2791,
      "step": 18630
    },
    {
      "epoch": 1.4438505780514728,
      "grad_norm": 2.00679349899292,
      "learning_rate": 1.560730002588662e-05,
      "loss": 1.3433,
      "step": 18640
    },
    {
      "epoch": 1.4446251863901316,
      "grad_norm": 2.08528995513916,
      "learning_rate": 1.5599534040900854e-05,
      "loss": 1.3557,
      "step": 18650
    },
    {
      "epoch": 1.4453997947287902,
      "grad_norm": 2.7487545013427734,
      "learning_rate": 1.5591768055915092e-05,
      "loss": 1.1912,
      "step": 18660
    },
    {
      "epoch": 1.446174403067449,
      "grad_norm": 2.4768149852752686,
      "learning_rate": 1.558400207092933e-05,
      "loss": 1.1934,
      "step": 18670
    },
    {
      "epoch": 1.4469490114061077,
      "grad_norm": 3.0457136631011963,
      "learning_rate": 1.5576236085943568e-05,
      "loss": 1.2465,
      "step": 18680
    },
    {
      "epoch": 1.4477236197447665,
      "grad_norm": 2.143113851547241,
      "learning_rate": 1.5568470100957806e-05,
      "loss": 1.2385,
      "step": 18690
    },
    {
      "epoch": 1.4484982280834253,
      "grad_norm": 1.8337907791137695,
      "learning_rate": 1.5560704115972044e-05,
      "loss": 1.4175,
      "step": 18700
    },
    {
      "epoch": 1.4492728364220842,
      "grad_norm": 2.4677932262420654,
      "learning_rate": 1.555293813098628e-05,
      "loss": 1.307,
      "step": 18710
    },
    {
      "epoch": 1.450047444760743,
      "grad_norm": 3.1146159172058105,
      "learning_rate": 1.554517214600052e-05,
      "loss": 1.2644,
      "step": 18720
    },
    {
      "epoch": 1.4508220530994016,
      "grad_norm": 2.08833384513855,
      "learning_rate": 1.5537406161014754e-05,
      "loss": 1.373,
      "step": 18730
    },
    {
      "epoch": 1.4515966614380604,
      "grad_norm": 2.6586365699768066,
      "learning_rate": 1.5529640176028992e-05,
      "loss": 1.2625,
      "step": 18740
    },
    {
      "epoch": 1.452371269776719,
      "grad_norm": 2.152489423751831,
      "learning_rate": 1.552187419104323e-05,
      "loss": 1.2714,
      "step": 18750
    },
    {
      "epoch": 1.4531458781153779,
      "grad_norm": 2.582923650741577,
      "learning_rate": 1.5514108206057468e-05,
      "loss": 1.2784,
      "step": 18760
    },
    {
      "epoch": 1.4539204864540367,
      "grad_norm": 2.539374589920044,
      "learning_rate": 1.5506342221071705e-05,
      "loss": 1.2972,
      "step": 18770
    },
    {
      "epoch": 1.4546950947926955,
      "grad_norm": 3.048227071762085,
      "learning_rate": 1.5498576236085943e-05,
      "loss": 1.2329,
      "step": 18780
    },
    {
      "epoch": 1.4554697031313542,
      "grad_norm": 1.9854570627212524,
      "learning_rate": 1.549081025110018e-05,
      "loss": 1.3586,
      "step": 18790
    },
    {
      "epoch": 1.456244311470013,
      "grad_norm": 2.411491870880127,
      "learning_rate": 1.548304426611442e-05,
      "loss": 1.1882,
      "step": 18800
    },
    {
      "epoch": 1.4570189198086718,
      "grad_norm": 2.4830169677734375,
      "learning_rate": 1.5475278281128657e-05,
      "loss": 1.2262,
      "step": 18810
    },
    {
      "epoch": 1.4577935281473304,
      "grad_norm": 2.3420472145080566,
      "learning_rate": 1.5467512296142895e-05,
      "loss": 1.2964,
      "step": 18820
    },
    {
      "epoch": 1.4585681364859893,
      "grad_norm": 2.061896562576294,
      "learning_rate": 1.5459746311157133e-05,
      "loss": 1.2314,
      "step": 18830
    },
    {
      "epoch": 1.459342744824648,
      "grad_norm": 3.0267651081085205,
      "learning_rate": 1.545198032617137e-05,
      "loss": 1.1903,
      "step": 18840
    },
    {
      "epoch": 1.460117353163307,
      "grad_norm": 2.369370460510254,
      "learning_rate": 1.544421434118561e-05,
      "loss": 1.1793,
      "step": 18850
    },
    {
      "epoch": 1.4608919615019655,
      "grad_norm": 3.4122660160064697,
      "learning_rate": 1.5436448356199846e-05,
      "loss": 1.1399,
      "step": 18860
    },
    {
      "epoch": 1.4616665698406244,
      "grad_norm": 2.179717540740967,
      "learning_rate": 1.5428682371214084e-05,
      "loss": 1.3192,
      "step": 18870
    },
    {
      "epoch": 1.462441178179283,
      "grad_norm": 2.279550790786743,
      "learning_rate": 1.5420916386228322e-05,
      "loss": 1.1648,
      "step": 18880
    },
    {
      "epoch": 1.4632157865179418,
      "grad_norm": 2.2549846172332764,
      "learning_rate": 1.541315040124256e-05,
      "loss": 1.2349,
      "step": 18890
    },
    {
      "epoch": 1.4639903948566007,
      "grad_norm": 2.632267951965332,
      "learning_rate": 1.5405384416256794e-05,
      "loss": 1.316,
      "step": 18900
    },
    {
      "epoch": 1.4647650031952595,
      "grad_norm": 2.178745985031128,
      "learning_rate": 1.5397618431271032e-05,
      "loss": 1.2154,
      "step": 18910
    },
    {
      "epoch": 1.465539611533918,
      "grad_norm": 3.0739731788635254,
      "learning_rate": 1.538985244628527e-05,
      "loss": 1.205,
      "step": 18920
    },
    {
      "epoch": 1.466314219872577,
      "grad_norm": 2.481281280517578,
      "learning_rate": 1.5382086461299508e-05,
      "loss": 1.3551,
      "step": 18930
    },
    {
      "epoch": 1.4670888282112358,
      "grad_norm": 2.7818796634674072,
      "learning_rate": 1.5374320476313746e-05,
      "loss": 1.2061,
      "step": 18940
    },
    {
      "epoch": 1.4678634365498944,
      "grad_norm": 2.5881474018096924,
      "learning_rate": 1.5366554491327984e-05,
      "loss": 1.2658,
      "step": 18950
    },
    {
      "epoch": 1.4686380448885532,
      "grad_norm": 3.061274290084839,
      "learning_rate": 1.535878850634222e-05,
      "loss": 1.2612,
      "step": 18960
    },
    {
      "epoch": 1.469412653227212,
      "grad_norm": 2.3794095516204834,
      "learning_rate": 1.535102252135646e-05,
      "loss": 1.1906,
      "step": 18970
    },
    {
      "epoch": 1.4701872615658709,
      "grad_norm": 2.6238465309143066,
      "learning_rate": 1.5343256536370697e-05,
      "loss": 1.2545,
      "step": 18980
    },
    {
      "epoch": 1.4709618699045295,
      "grad_norm": 2.2262983322143555,
      "learning_rate": 1.533549055138493e-05,
      "loss": 1.3145,
      "step": 18990
    },
    {
      "epoch": 1.4717364782431883,
      "grad_norm": 2.4979329109191895,
      "learning_rate": 1.532772456639917e-05,
      "loss": 1.1716,
      "step": 19000
    },
    {
      "epoch": 1.472511086581847,
      "grad_norm": 2.569469451904297,
      "learning_rate": 1.5319958581413407e-05,
      "loss": 1.2155,
      "step": 19010
    },
    {
      "epoch": 1.4732856949205058,
      "grad_norm": 2.731412887573242,
      "learning_rate": 1.531219259642765e-05,
      "loss": 1.2221,
      "step": 19020
    },
    {
      "epoch": 1.4740603032591646,
      "grad_norm": 2.6652894020080566,
      "learning_rate": 1.5304426611441887e-05,
      "loss": 1.2568,
      "step": 19030
    },
    {
      "epoch": 1.4748349115978234,
      "grad_norm": 2.129544734954834,
      "learning_rate": 1.5296660626456124e-05,
      "loss": 1.2315,
      "step": 19040
    },
    {
      "epoch": 1.475609519936482,
      "grad_norm": 2.2177884578704834,
      "learning_rate": 1.5288894641470362e-05,
      "loss": 1.2638,
      "step": 19050
    },
    {
      "epoch": 1.4763841282751409,
      "grad_norm": 2.4224040508270264,
      "learning_rate": 1.52811286564846e-05,
      "loss": 1.3006,
      "step": 19060
    },
    {
      "epoch": 1.4771587366137997,
      "grad_norm": 2.534893274307251,
      "learning_rate": 1.5273362671498835e-05,
      "loss": 1.2615,
      "step": 19070
    },
    {
      "epoch": 1.4779333449524583,
      "grad_norm": 2.2074947357177734,
      "learning_rate": 1.5265596686513072e-05,
      "loss": 1.2871,
      "step": 19080
    },
    {
      "epoch": 1.4787079532911171,
      "grad_norm": 2.8368825912475586,
      "learning_rate": 1.525783070152731e-05,
      "loss": 1.2287,
      "step": 19090
    },
    {
      "epoch": 1.479482561629776,
      "grad_norm": 2.664808750152588,
      "learning_rate": 1.5250064716541548e-05,
      "loss": 1.205,
      "step": 19100
    },
    {
      "epoch": 1.4802571699684348,
      "grad_norm": 2.5066120624542236,
      "learning_rate": 1.5242298731555786e-05,
      "loss": 1.2381,
      "step": 19110
    },
    {
      "epoch": 1.4810317783070934,
      "grad_norm": 2.269361734390259,
      "learning_rate": 1.5234532746570024e-05,
      "loss": 1.2302,
      "step": 19120
    },
    {
      "epoch": 1.4818063866457523,
      "grad_norm": 2.9492547512054443,
      "learning_rate": 1.522676676158426e-05,
      "loss": 1.1591,
      "step": 19130
    },
    {
      "epoch": 1.4825809949844109,
      "grad_norm": 2.7657220363616943,
      "learning_rate": 1.5219000776598498e-05,
      "loss": 1.1803,
      "step": 19140
    },
    {
      "epoch": 1.4833556033230697,
      "grad_norm": 2.779592514038086,
      "learning_rate": 1.5211234791612736e-05,
      "loss": 1.1575,
      "step": 19150
    },
    {
      "epoch": 1.4841302116617285,
      "grad_norm": 2.456139326095581,
      "learning_rate": 1.5204245405125553e-05,
      "loss": 1.2374,
      "step": 19160
    },
    {
      "epoch": 1.4849048200003874,
      "grad_norm": 3.1110897064208984,
      "learning_rate": 1.5196479420139789e-05,
      "loss": 1.3311,
      "step": 19170
    },
    {
      "epoch": 1.4856794283390462,
      "grad_norm": 2.727389097213745,
      "learning_rate": 1.5188713435154027e-05,
      "loss": 1.218,
      "step": 19180
    },
    {
      "epoch": 1.4864540366777048,
      "grad_norm": 2.021867036819458,
      "learning_rate": 1.5180947450168265e-05,
      "loss": 1.2805,
      "step": 19190
    },
    {
      "epoch": 1.4872286450163636,
      "grad_norm": 2.921402931213379,
      "learning_rate": 1.5173181465182502e-05,
      "loss": 1.1402,
      "step": 19200
    },
    {
      "epoch": 1.4880032533550223,
      "grad_norm": 2.0964484214782715,
      "learning_rate": 1.5165415480196739e-05,
      "loss": 1.2256,
      "step": 19210
    },
    {
      "epoch": 1.488777861693681,
      "grad_norm": 2.323272466659546,
      "learning_rate": 1.5157649495210976e-05,
      "loss": 1.1881,
      "step": 19220
    },
    {
      "epoch": 1.48955247003234,
      "grad_norm": 2.9235568046569824,
      "learning_rate": 1.5149883510225214e-05,
      "loss": 1.2158,
      "step": 19230
    },
    {
      "epoch": 1.4903270783709988,
      "grad_norm": 2.38889741897583,
      "learning_rate": 1.5142117525239452e-05,
      "loss": 1.3183,
      "step": 19240
    },
    {
      "epoch": 1.4911016867096574,
      "grad_norm": 2.3394646644592285,
      "learning_rate": 1.5134351540253688e-05,
      "loss": 1.2637,
      "step": 19250
    },
    {
      "epoch": 1.4918762950483162,
      "grad_norm": 2.586592674255371,
      "learning_rate": 1.5126585555267926e-05,
      "loss": 1.2558,
      "step": 19260
    },
    {
      "epoch": 1.492650903386975,
      "grad_norm": 2.808560371398926,
      "learning_rate": 1.5118819570282164e-05,
      "loss": 1.3183,
      "step": 19270
    },
    {
      "epoch": 1.4934255117256336,
      "grad_norm": 2.144463539123535,
      "learning_rate": 1.5111053585296402e-05,
      "loss": 1.2951,
      "step": 19280
    },
    {
      "epoch": 1.4942001200642925,
      "grad_norm": 2.46463942527771,
      "learning_rate": 1.510328760031064e-05,
      "loss": 1.2781,
      "step": 19290
    },
    {
      "epoch": 1.4949747284029513,
      "grad_norm": 2.5173115730285645,
      "learning_rate": 1.5095521615324876e-05,
      "loss": 1.1786,
      "step": 19300
    },
    {
      "epoch": 1.4957493367416101,
      "grad_norm": 2.340296745300293,
      "learning_rate": 1.5087755630339114e-05,
      "loss": 1.2394,
      "step": 19310
    },
    {
      "epoch": 1.4965239450802688,
      "grad_norm": 2.5650882720947266,
      "learning_rate": 1.5079989645353352e-05,
      "loss": 1.2569,
      "step": 19320
    },
    {
      "epoch": 1.4972985534189276,
      "grad_norm": 2.7510337829589844,
      "learning_rate": 1.507222366036759e-05,
      "loss": 1.2186,
      "step": 19330
    },
    {
      "epoch": 1.4980731617575862,
      "grad_norm": 2.401541233062744,
      "learning_rate": 1.5064457675381826e-05,
      "loss": 1.1512,
      "step": 19340
    },
    {
      "epoch": 1.498847770096245,
      "grad_norm": 2.846485137939453,
      "learning_rate": 1.5056691690396067e-05,
      "loss": 1.1818,
      "step": 19350
    },
    {
      "epoch": 1.4996223784349039,
      "grad_norm": 2.1766836643218994,
      "learning_rate": 1.5048925705410305e-05,
      "loss": 1.2765,
      "step": 19360
    },
    {
      "epoch": 1.5003969867735627,
      "grad_norm": 3.1476027965545654,
      "learning_rate": 1.5041159720424543e-05,
      "loss": 1.2781,
      "step": 19370
    },
    {
      "epoch": 1.5011715951122215,
      "grad_norm": 2.9410719871520996,
      "learning_rate": 1.5033393735438779e-05,
      "loss": 1.336,
      "step": 19380
    },
    {
      "epoch": 1.5019462034508801,
      "grad_norm": 2.166750431060791,
      "learning_rate": 1.5025627750453017e-05,
      "loss": 1.2874,
      "step": 19390
    },
    {
      "epoch": 1.5027208117895388,
      "grad_norm": 2.9785640239715576,
      "learning_rate": 1.5017861765467255e-05,
      "loss": 1.2498,
      "step": 19400
    },
    {
      "epoch": 1.5034954201281976,
      "grad_norm": 2.3441290855407715,
      "learning_rate": 1.5010095780481493e-05,
      "loss": 1.2478,
      "step": 19410
    },
    {
      "epoch": 1.5042700284668564,
      "grad_norm": 2.671323776245117,
      "learning_rate": 1.5002329795495729e-05,
      "loss": 1.2986,
      "step": 19420
    },
    {
      "epoch": 1.5050446368055153,
      "grad_norm": 2.933703899383545,
      "learning_rate": 1.4994563810509967e-05,
      "loss": 1.2827,
      "step": 19430
    },
    {
      "epoch": 1.505819245144174,
      "grad_norm": 2.3826191425323486,
      "learning_rate": 1.4986797825524204e-05,
      "loss": 1.1215,
      "step": 19440
    },
    {
      "epoch": 1.5065938534828327,
      "grad_norm": 2.4936978816986084,
      "learning_rate": 1.4979031840538442e-05,
      "loss": 1.3158,
      "step": 19450
    },
    {
      "epoch": 1.5073684618214915,
      "grad_norm": 2.435026168823242,
      "learning_rate": 1.497126585555268e-05,
      "loss": 1.1812,
      "step": 19460
    },
    {
      "epoch": 1.5081430701601501,
      "grad_norm": 2.2800843715667725,
      "learning_rate": 1.4963499870566916e-05,
      "loss": 1.2393,
      "step": 19470
    },
    {
      "epoch": 1.508917678498809,
      "grad_norm": 2.306715488433838,
      "learning_rate": 1.4955733885581154e-05,
      "loss": 1.3067,
      "step": 19480
    },
    {
      "epoch": 1.5096922868374678,
      "grad_norm": 2.1575987339019775,
      "learning_rate": 1.4947967900595392e-05,
      "loss": 1.2385,
      "step": 19490
    },
    {
      "epoch": 1.5104668951761266,
      "grad_norm": 2.816318988800049,
      "learning_rate": 1.4940201915609632e-05,
      "loss": 1.2139,
      "step": 19500
    },
    {
      "epoch": 1.5112415035147855,
      "grad_norm": 3.231818437576294,
      "learning_rate": 1.4932435930623868e-05,
      "loss": 1.2562,
      "step": 19510
    },
    {
      "epoch": 1.512016111853444,
      "grad_norm": 2.2648708820343018,
      "learning_rate": 1.4924669945638106e-05,
      "loss": 1.2068,
      "step": 19520
    },
    {
      "epoch": 1.512790720192103,
      "grad_norm": 2.7012252807617188,
      "learning_rate": 1.4916903960652344e-05,
      "loss": 1.2947,
      "step": 19530
    },
    {
      "epoch": 1.5135653285307615,
      "grad_norm": 2.3787002563476562,
      "learning_rate": 1.4909137975666581e-05,
      "loss": 1.2398,
      "step": 19540
    },
    {
      "epoch": 1.5143399368694204,
      "grad_norm": 2.2362565994262695,
      "learning_rate": 1.4901371990680818e-05,
      "loss": 1.2681,
      "step": 19550
    },
    {
      "epoch": 1.5151145452080792,
      "grad_norm": 1.9631296396255493,
      "learning_rate": 1.4893606005695055e-05,
      "loss": 1.3171,
      "step": 19560
    },
    {
      "epoch": 1.515889153546738,
      "grad_norm": 2.4925882816314697,
      "learning_rate": 1.4885840020709293e-05,
      "loss": 1.3813,
      "step": 19570
    },
    {
      "epoch": 1.5166637618853966,
      "grad_norm": 2.5963261127471924,
      "learning_rate": 1.4878074035723531e-05,
      "loss": 1.1445,
      "step": 19580
    },
    {
      "epoch": 1.5174383702240555,
      "grad_norm": 2.3548741340637207,
      "learning_rate": 1.4870308050737769e-05,
      "loss": 1.3582,
      "step": 19590
    },
    {
      "epoch": 1.518212978562714,
      "grad_norm": 2.9948291778564453,
      "learning_rate": 1.4862542065752007e-05,
      "loss": 1.1527,
      "step": 19600
    },
    {
      "epoch": 1.518987586901373,
      "grad_norm": 3.081028461456299,
      "learning_rate": 1.4854776080766245e-05,
      "loss": 1.335,
      "step": 19610
    },
    {
      "epoch": 1.5197621952400318,
      "grad_norm": 2.389066457748413,
      "learning_rate": 1.4847010095780483e-05,
      "loss": 1.2217,
      "step": 19620
    },
    {
      "epoch": 1.5205368035786906,
      "grad_norm": 2.4058525562286377,
      "learning_rate": 1.483924411079472e-05,
      "loss": 1.3682,
      "step": 19630
    },
    {
      "epoch": 1.5213114119173494,
      "grad_norm": 2.623277187347412,
      "learning_rate": 1.4831478125808957e-05,
      "loss": 1.2532,
      "step": 19640
    },
    {
      "epoch": 1.522086020256008,
      "grad_norm": 2.6584324836730957,
      "learning_rate": 1.4823712140823195e-05,
      "loss": 1.3246,
      "step": 19650
    },
    {
      "epoch": 1.5228606285946669,
      "grad_norm": 2.078122615814209,
      "learning_rate": 1.4815946155837432e-05,
      "loss": 1.2226,
      "step": 19660
    },
    {
      "epoch": 1.5236352369333255,
      "grad_norm": 2.1008055210113525,
      "learning_rate": 1.480818017085167e-05,
      "loss": 1.3549,
      "step": 19670
    },
    {
      "epoch": 1.5244098452719843,
      "grad_norm": 2.353008508682251,
      "learning_rate": 1.4800414185865906e-05,
      "loss": 1.2468,
      "step": 19680
    },
    {
      "epoch": 1.5251844536106431,
      "grad_norm": 2.6857848167419434,
      "learning_rate": 1.4792648200880146e-05,
      "loss": 1.2566,
      "step": 19690
    },
    {
      "epoch": 1.525959061949302,
      "grad_norm": 2.026318311691284,
      "learning_rate": 1.4784882215894384e-05,
      "loss": 1.2162,
      "step": 19700
    },
    {
      "epoch": 1.5267336702879608,
      "grad_norm": 3.4778101444244385,
      "learning_rate": 1.4777116230908622e-05,
      "loss": 1.2809,
      "step": 19710
    },
    {
      "epoch": 1.5275082786266194,
      "grad_norm": 2.500131368637085,
      "learning_rate": 1.4769350245922858e-05,
      "loss": 1.2126,
      "step": 19720
    },
    {
      "epoch": 1.528282886965278,
      "grad_norm": 2.8930861949920654,
      "learning_rate": 1.4761584260937096e-05,
      "loss": 1.251,
      "step": 19730
    },
    {
      "epoch": 1.5290574953039369,
      "grad_norm": 2.56434965133667,
      "learning_rate": 1.4753818275951334e-05,
      "loss": 1.161,
      "step": 19740
    },
    {
      "epoch": 1.5298321036425957,
      "grad_norm": 1.989249348640442,
      "learning_rate": 1.4746052290965571e-05,
      "loss": 1.3333,
      "step": 19750
    },
    {
      "epoch": 1.5306067119812545,
      "grad_norm": 2.2750051021575928,
      "learning_rate": 1.4738286305979808e-05,
      "loss": 1.3059,
      "step": 19760
    },
    {
      "epoch": 1.5313813203199134,
      "grad_norm": 3.077418565750122,
      "learning_rate": 1.4730520320994046e-05,
      "loss": 1.4015,
      "step": 19770
    },
    {
      "epoch": 1.532155928658572,
      "grad_norm": 2.311129570007324,
      "learning_rate": 1.4722754336008283e-05,
      "loss": 1.2516,
      "step": 19780
    },
    {
      "epoch": 1.5329305369972308,
      "grad_norm": 2.1887316703796387,
      "learning_rate": 1.4714988351022523e-05,
      "loss": 1.3294,
      "step": 19790
    },
    {
      "epoch": 1.5337051453358894,
      "grad_norm": 2.8804118633270264,
      "learning_rate": 1.4707222366036759e-05,
      "loss": 1.3236,
      "step": 19800
    },
    {
      "epoch": 1.5344797536745483,
      "grad_norm": 2.8898191452026367,
      "learning_rate": 1.4699456381050997e-05,
      "loss": 1.2225,
      "step": 19810
    },
    {
      "epoch": 1.535254362013207,
      "grad_norm": 1.9796503782272339,
      "learning_rate": 1.4691690396065235e-05,
      "loss": 1.2021,
      "step": 19820
    },
    {
      "epoch": 1.536028970351866,
      "grad_norm": 2.2184548377990723,
      "learning_rate": 1.4683924411079473e-05,
      "loss": 1.2447,
      "step": 19830
    },
    {
      "epoch": 1.5368035786905248,
      "grad_norm": 2.203066825866699,
      "learning_rate": 1.467615842609371e-05,
      "loss": 1.252,
      "step": 19840
    },
    {
      "epoch": 1.5375781870291834,
      "grad_norm": 2.2332255840301514,
      "learning_rate": 1.4668392441107947e-05,
      "loss": 1.2304,
      "step": 19850
    },
    {
      "epoch": 1.538352795367842,
      "grad_norm": 2.6342663764953613,
      "learning_rate": 1.4660626456122185e-05,
      "loss": 1.2976,
      "step": 19860
    },
    {
      "epoch": 1.5391274037065008,
      "grad_norm": 2.5571861267089844,
      "learning_rate": 1.4652860471136422e-05,
      "loss": 1.184,
      "step": 19870
    },
    {
      "epoch": 1.5399020120451596,
      "grad_norm": 1.9130641222000122,
      "learning_rate": 1.464509448615066e-05,
      "loss": 1.3252,
      "step": 19880
    },
    {
      "epoch": 1.5406766203838185,
      "grad_norm": 2.030116081237793,
      "learning_rate": 1.4637328501164898e-05,
      "loss": 1.2736,
      "step": 19890
    },
    {
      "epoch": 1.5414512287224773,
      "grad_norm": 1.9205334186553955,
      "learning_rate": 1.4629562516179136e-05,
      "loss": 1.3178,
      "step": 19900
    },
    {
      "epoch": 1.542225837061136,
      "grad_norm": 2.40539288520813,
      "learning_rate": 1.4621796531193374e-05,
      "loss": 1.2826,
      "step": 19910
    },
    {
      "epoch": 1.5430004453997948,
      "grad_norm": 2.339930772781372,
      "learning_rate": 1.4614030546207612e-05,
      "loss": 1.2891,
      "step": 19920
    },
    {
      "epoch": 1.5437750537384534,
      "grad_norm": 2.023555040359497,
      "learning_rate": 1.4606264561221848e-05,
      "loss": 1.2836,
      "step": 19930
    },
    {
      "epoch": 1.5445496620771122,
      "grad_norm": 2.728736400604248,
      "learning_rate": 1.4598498576236086e-05,
      "loss": 1.2115,
      "step": 19940
    },
    {
      "epoch": 1.545324270415771,
      "grad_norm": 3.292771577835083,
      "learning_rate": 1.4590732591250324e-05,
      "loss": 1.2795,
      "step": 19950
    },
    {
      "epoch": 1.5460988787544299,
      "grad_norm": 2.561847448348999,
      "learning_rate": 1.4582966606264562e-05,
      "loss": 1.266,
      "step": 19960
    },
    {
      "epoch": 1.5468734870930887,
      "grad_norm": 2.756308078765869,
      "learning_rate": 1.4575200621278798e-05,
      "loss": 1.2053,
      "step": 19970
    },
    {
      "epoch": 1.5476480954317473,
      "grad_norm": 2.590250253677368,
      "learning_rate": 1.4567434636293036e-05,
      "loss": 1.3586,
      "step": 19980
    },
    {
      "epoch": 1.5484227037704061,
      "grad_norm": 2.151014566421509,
      "learning_rate": 1.4559668651307275e-05,
      "loss": 1.2561,
      "step": 19990
    },
    {
      "epoch": 1.5491973121090648,
      "grad_norm": 2.686870574951172,
      "learning_rate": 1.4551902666321513e-05,
      "loss": 1.3278,
      "step": 20000
    },
    {
      "epoch": 1.5499719204477236,
      "grad_norm": 2.508718967437744,
      "learning_rate": 1.4544136681335751e-05,
      "loss": 1.3294,
      "step": 20010
    },
    {
      "epoch": 1.5507465287863824,
      "grad_norm": 2.262685775756836,
      "learning_rate": 1.4536370696349987e-05,
      "loss": 1.3428,
      "step": 20020
    },
    {
      "epoch": 1.5515211371250412,
      "grad_norm": 2.50935959815979,
      "learning_rate": 1.4528604711364225e-05,
      "loss": 1.3735,
      "step": 20030
    },
    {
      "epoch": 1.5522957454637,
      "grad_norm": 2.798041820526123,
      "learning_rate": 1.4520838726378463e-05,
      "loss": 1.1699,
      "step": 20040
    },
    {
      "epoch": 1.5530703538023587,
      "grad_norm": 2.5783603191375732,
      "learning_rate": 1.45130727413927e-05,
      "loss": 1.2598,
      "step": 20050
    },
    {
      "epoch": 1.5538449621410173,
      "grad_norm": 2.437028646469116,
      "learning_rate": 1.4505306756406937e-05,
      "loss": 1.1928,
      "step": 20060
    },
    {
      "epoch": 1.5546195704796761,
      "grad_norm": 2.7993578910827637,
      "learning_rate": 1.4497540771421175e-05,
      "loss": 1.2426,
      "step": 20070
    },
    {
      "epoch": 1.555394178818335,
      "grad_norm": 2.269158363342285,
      "learning_rate": 1.4489774786435414e-05,
      "loss": 1.2292,
      "step": 20080
    },
    {
      "epoch": 1.5561687871569938,
      "grad_norm": 3.155423879623413,
      "learning_rate": 1.4482008801449652e-05,
      "loss": 1.1959,
      "step": 20090
    },
    {
      "epoch": 1.5569433954956526,
      "grad_norm": 2.603975296020508,
      "learning_rate": 1.4474242816463888e-05,
      "loss": 1.2344,
      "step": 20100
    },
    {
      "epoch": 1.5577180038343112,
      "grad_norm": 2.1008102893829346,
      "learning_rate": 1.4466476831478126e-05,
      "loss": 1.2991,
      "step": 20110
    },
    {
      "epoch": 1.55849261217297,
      "grad_norm": 2.98339581489563,
      "learning_rate": 1.4458710846492364e-05,
      "loss": 1.2706,
      "step": 20120
    },
    {
      "epoch": 1.5592672205116287,
      "grad_norm": 2.539478302001953,
      "learning_rate": 1.4450944861506602e-05,
      "loss": 1.2446,
      "step": 20130
    },
    {
      "epoch": 1.5600418288502875,
      "grad_norm": 2.350351572036743,
      "learning_rate": 1.4443178876520838e-05,
      "loss": 1.2915,
      "step": 20140
    },
    {
      "epoch": 1.5608164371889464,
      "grad_norm": 3.4328908920288086,
      "learning_rate": 1.4435412891535076e-05,
      "loss": 1.2716,
      "step": 20150
    },
    {
      "epoch": 1.5615910455276052,
      "grad_norm": 2.3147099018096924,
      "learning_rate": 1.4427646906549314e-05,
      "loss": 1.2112,
      "step": 20160
    },
    {
      "epoch": 1.562365653866264,
      "grad_norm": 3.2690975666046143,
      "learning_rate": 1.4419880921563552e-05,
      "loss": 1.2325,
      "step": 20170
    },
    {
      "epoch": 1.5631402622049226,
      "grad_norm": 1.8082594871520996,
      "learning_rate": 1.4412114936577791e-05,
      "loss": 1.2684,
      "step": 20180
    },
    {
      "epoch": 1.5639148705435812,
      "grad_norm": 2.6028764247894287,
      "learning_rate": 1.4404348951592027e-05,
      "loss": 1.2543,
      "step": 20190
    },
    {
      "epoch": 1.56468947888224,
      "grad_norm": 2.2260499000549316,
      "learning_rate": 1.4396582966606265e-05,
      "loss": 1.2495,
      "step": 20200
    },
    {
      "epoch": 1.565464087220899,
      "grad_norm": 3.170480728149414,
      "learning_rate": 1.4388816981620503e-05,
      "loss": 1.2477,
      "step": 20210
    },
    {
      "epoch": 1.5662386955595577,
      "grad_norm": 2.182535409927368,
      "learning_rate": 1.4381050996634741e-05,
      "loss": 1.2998,
      "step": 20220
    },
    {
      "epoch": 1.5670133038982166,
      "grad_norm": 1.9879313707351685,
      "learning_rate": 1.4373285011648977e-05,
      "loss": 1.1699,
      "step": 20230
    },
    {
      "epoch": 1.5677879122368752,
      "grad_norm": 2.6086723804473877,
      "learning_rate": 1.4365519026663215e-05,
      "loss": 1.2423,
      "step": 20240
    },
    {
      "epoch": 1.568562520575534,
      "grad_norm": 2.461031675338745,
      "learning_rate": 1.4357753041677453e-05,
      "loss": 1.395,
      "step": 20250
    },
    {
      "epoch": 1.5693371289141926,
      "grad_norm": 2.4857194423675537,
      "learning_rate": 1.434998705669169e-05,
      "loss": 1.2254,
      "step": 20260
    },
    {
      "epoch": 1.5701117372528515,
      "grad_norm": 2.2242753505706787,
      "learning_rate": 1.4342221071705927e-05,
      "loss": 1.2628,
      "step": 20270
    },
    {
      "epoch": 1.5708863455915103,
      "grad_norm": 2.658069372177124,
      "learning_rate": 1.4334455086720167e-05,
      "loss": 1.2441,
      "step": 20280
    },
    {
      "epoch": 1.5716609539301691,
      "grad_norm": 2.4290575981140137,
      "learning_rate": 1.4326689101734404e-05,
      "loss": 1.2086,
      "step": 20290
    },
    {
      "epoch": 1.572435562268828,
      "grad_norm": 2.7441227436065674,
      "learning_rate": 1.4318923116748642e-05,
      "loss": 1.2209,
      "step": 20300
    },
    {
      "epoch": 1.5732101706074866,
      "grad_norm": 2.474365234375,
      "learning_rate": 1.4311157131762878e-05,
      "loss": 1.2424,
      "step": 20310
    },
    {
      "epoch": 1.5739847789461452,
      "grad_norm": 2.5862598419189453,
      "learning_rate": 1.4303391146777116e-05,
      "loss": 1.2982,
      "step": 20320
    },
    {
      "epoch": 1.574759387284804,
      "grad_norm": 1.7678223848342896,
      "learning_rate": 1.4295625161791354e-05,
      "loss": 1.3465,
      "step": 20330
    },
    {
      "epoch": 1.5755339956234629,
      "grad_norm": 2.7519664764404297,
      "learning_rate": 1.4287859176805592e-05,
      "loss": 1.3048,
      "step": 20340
    },
    {
      "epoch": 1.5763086039621217,
      "grad_norm": 2.3448219299316406,
      "learning_rate": 1.428009319181983e-05,
      "loss": 1.2692,
      "step": 20350
    },
    {
      "epoch": 1.5770832123007805,
      "grad_norm": 2.7247400283813477,
      "learning_rate": 1.4272327206834066e-05,
      "loss": 1.2054,
      "step": 20360
    },
    {
      "epoch": 1.5778578206394391,
      "grad_norm": 3.471745729446411,
      "learning_rate": 1.4264561221848304e-05,
      "loss": 1.2965,
      "step": 20370
    },
    {
      "epoch": 1.578632428978098,
      "grad_norm": 2.5689923763275146,
      "learning_rate": 1.4256795236862543e-05,
      "loss": 1.1745,
      "step": 20380
    },
    {
      "epoch": 1.5794070373167566,
      "grad_norm": 2.045229196548462,
      "learning_rate": 1.4249029251876781e-05,
      "loss": 1.2346,
      "step": 20390
    },
    {
      "epoch": 1.5801816456554154,
      "grad_norm": 2.5815234184265137,
      "learning_rate": 1.4241263266891018e-05,
      "loss": 1.2745,
      "step": 20400
    },
    {
      "epoch": 1.5809562539940742,
      "grad_norm": 2.2160446643829346,
      "learning_rate": 1.4233497281905255e-05,
      "loss": 1.2721,
      "step": 20410
    },
    {
      "epoch": 1.581730862332733,
      "grad_norm": 2.128143072128296,
      "learning_rate": 1.4225731296919493e-05,
      "loss": 1.2625,
      "step": 20420
    },
    {
      "epoch": 1.582505470671392,
      "grad_norm": 2.1155998706817627,
      "learning_rate": 1.4217965311933731e-05,
      "loss": 1.2757,
      "step": 20430
    },
    {
      "epoch": 1.5832800790100505,
      "grad_norm": 2.8521275520324707,
      "learning_rate": 1.4210199326947967e-05,
      "loss": 1.1695,
      "step": 20440
    },
    {
      "epoch": 1.5840546873487094,
      "grad_norm": 2.3337502479553223,
      "learning_rate": 1.4202433341962205e-05,
      "loss": 1.2292,
      "step": 20450
    },
    {
      "epoch": 1.584829295687368,
      "grad_norm": 2.5861995220184326,
      "learning_rate": 1.4194667356976443e-05,
      "loss": 1.3016,
      "step": 20460
    },
    {
      "epoch": 1.5856039040260268,
      "grad_norm": 2.433887481689453,
      "learning_rate": 1.4186901371990681e-05,
      "loss": 1.3162,
      "step": 20470
    },
    {
      "epoch": 1.5863785123646856,
      "grad_norm": 2.3562679290771484,
      "learning_rate": 1.4179135387004919e-05,
      "loss": 1.2923,
      "step": 20480
    },
    {
      "epoch": 1.5871531207033445,
      "grad_norm": 1.9194344282150269,
      "learning_rate": 1.4171369402019157e-05,
      "loss": 1.1552,
      "step": 20490
    },
    {
      "epoch": 1.5879277290420033,
      "grad_norm": 3.0244300365448,
      "learning_rate": 1.4163603417033395e-05,
      "loss": 1.2615,
      "step": 20500
    },
    {
      "epoch": 1.588702337380662,
      "grad_norm": 2.9450695514678955,
      "learning_rate": 1.4155837432047632e-05,
      "loss": 1.3164,
      "step": 20510
    },
    {
      "epoch": 1.5894769457193205,
      "grad_norm": 2.3368799686431885,
      "learning_rate": 1.414807144706187e-05,
      "loss": 1.3901,
      "step": 20520
    },
    {
      "epoch": 1.5902515540579794,
      "grad_norm": 2.1885764598846436,
      "learning_rate": 1.4140305462076106e-05,
      "loss": 1.2628,
      "step": 20530
    },
    {
      "epoch": 1.5910261623966382,
      "grad_norm": 2.963878870010376,
      "learning_rate": 1.4132539477090344e-05,
      "loss": 1.169,
      "step": 20540
    },
    {
      "epoch": 1.591800770735297,
      "grad_norm": 2.7161149978637695,
      "learning_rate": 1.4124773492104582e-05,
      "loss": 1.1408,
      "step": 20550
    },
    {
      "epoch": 1.5925753790739559,
      "grad_norm": 2.6789822578430176,
      "learning_rate": 1.411700750711882e-05,
      "loss": 1.3249,
      "step": 20560
    },
    {
      "epoch": 1.5933499874126145,
      "grad_norm": 2.3569862842559814,
      "learning_rate": 1.4109241522133056e-05,
      "loss": 1.2361,
      "step": 20570
    },
    {
      "epoch": 1.5941245957512733,
      "grad_norm": 2.5353357791900635,
      "learning_rate": 1.4101475537147296e-05,
      "loss": 1.2621,
      "step": 20580
    },
    {
      "epoch": 1.594899204089932,
      "grad_norm": 2.8646814823150635,
      "learning_rate": 1.4093709552161534e-05,
      "loss": 1.246,
      "step": 20590
    },
    {
      "epoch": 1.5956738124285907,
      "grad_norm": 2.550985813140869,
      "learning_rate": 1.4085943567175771e-05,
      "loss": 1.25,
      "step": 20600
    },
    {
      "epoch": 1.5964484207672496,
      "grad_norm": 2.8349008560180664,
      "learning_rate": 1.4078177582190008e-05,
      "loss": 1.23,
      "step": 20610
    },
    {
      "epoch": 1.5972230291059084,
      "grad_norm": 2.3515384197235107,
      "learning_rate": 1.4070411597204246e-05,
      "loss": 1.2559,
      "step": 20620
    },
    {
      "epoch": 1.5979976374445672,
      "grad_norm": 2.724294662475586,
      "learning_rate": 1.4062645612218483e-05,
      "loss": 1.3203,
      "step": 20630
    },
    {
      "epoch": 1.5987722457832259,
      "grad_norm": 2.919384479522705,
      "learning_rate": 1.4054879627232721e-05,
      "loss": 1.2154,
      "step": 20640
    },
    {
      "epoch": 1.5995468541218845,
      "grad_norm": 2.2251851558685303,
      "learning_rate": 1.4047113642246957e-05,
      "loss": 1.2726,
      "step": 20650
    },
    {
      "epoch": 1.6003214624605433,
      "grad_norm": 2.2834155559539795,
      "learning_rate": 1.4039347657261195e-05,
      "loss": 1.1893,
      "step": 20660
    },
    {
      "epoch": 1.6010960707992021,
      "grad_norm": 2.210545063018799,
      "learning_rate": 1.4031581672275435e-05,
      "loss": 1.333,
      "step": 20670
    },
    {
      "epoch": 1.601870679137861,
      "grad_norm": 4.830907821655273,
      "learning_rate": 1.4023815687289673e-05,
      "loss": 1.064,
      "step": 20680
    },
    {
      "epoch": 1.6026452874765198,
      "grad_norm": 2.215773820877075,
      "learning_rate": 1.4016049702303909e-05,
      "loss": 1.2551,
      "step": 20690
    },
    {
      "epoch": 1.6034198958151784,
      "grad_norm": 2.3620517253875732,
      "learning_rate": 1.4008283717318147e-05,
      "loss": 1.1773,
      "step": 20700
    },
    {
      "epoch": 1.6041945041538372,
      "grad_norm": 2.2307071685791016,
      "learning_rate": 1.4000517732332385e-05,
      "loss": 1.2909,
      "step": 20710
    },
    {
      "epoch": 1.6049691124924959,
      "grad_norm": 2.4191529750823975,
      "learning_rate": 1.3992751747346622e-05,
      "loss": 1.2096,
      "step": 20720
    },
    {
      "epoch": 1.6057437208311547,
      "grad_norm": 2.516957998275757,
      "learning_rate": 1.398498576236086e-05,
      "loss": 1.2988,
      "step": 20730
    },
    {
      "epoch": 1.6065183291698135,
      "grad_norm": 2.1658480167388916,
      "learning_rate": 1.3977219777375097e-05,
      "loss": 1.1779,
      "step": 20740
    },
    {
      "epoch": 1.6072929375084724,
      "grad_norm": 2.4426398277282715,
      "learning_rate": 1.3969453792389334e-05,
      "loss": 1.3001,
      "step": 20750
    },
    {
      "epoch": 1.6080675458471312,
      "grad_norm": 2.4955697059631348,
      "learning_rate": 1.3961687807403572e-05,
      "loss": 1.3205,
      "step": 20760
    },
    {
      "epoch": 1.6088421541857898,
      "grad_norm": 1.8612195253372192,
      "learning_rate": 1.3953921822417812e-05,
      "loss": 1.3064,
      "step": 20770
    },
    {
      "epoch": 1.6096167625244486,
      "grad_norm": 2.717228412628174,
      "learning_rate": 1.3946155837432048e-05,
      "loss": 1.1627,
      "step": 20780
    },
    {
      "epoch": 1.6103913708631072,
      "grad_norm": 3.175485372543335,
      "learning_rate": 1.3938389852446286e-05,
      "loss": 1.2361,
      "step": 20790
    },
    {
      "epoch": 1.611165979201766,
      "grad_norm": 2.9541115760803223,
      "learning_rate": 1.3930623867460524e-05,
      "loss": 1.2604,
      "step": 20800
    },
    {
      "epoch": 1.611940587540425,
      "grad_norm": 3.427419900894165,
      "learning_rate": 1.3922857882474762e-05,
      "loss": 1.2297,
      "step": 20810
    },
    {
      "epoch": 1.6127151958790837,
      "grad_norm": 3.5277416706085205,
      "learning_rate": 1.3915091897488998e-05,
      "loss": 1.226,
      "step": 20820
    },
    {
      "epoch": 1.6134898042177424,
      "grad_norm": 2.2891921997070312,
      "learning_rate": 1.3907325912503236e-05,
      "loss": 1.1465,
      "step": 20830
    },
    {
      "epoch": 1.6142644125564012,
      "grad_norm": 2.3093574047088623,
      "learning_rate": 1.3899559927517473e-05,
      "loss": 1.2255,
      "step": 20840
    },
    {
      "epoch": 1.6150390208950598,
      "grad_norm": 2.959527015686035,
      "learning_rate": 1.3891793942531711e-05,
      "loss": 1.2517,
      "step": 20850
    },
    {
      "epoch": 1.6158136292337186,
      "grad_norm": 3.2455520629882812,
      "learning_rate": 1.3884027957545948e-05,
      "loss": 1.2649,
      "step": 20860
    },
    {
      "epoch": 1.6165882375723775,
      "grad_norm": 2.4373679161071777,
      "learning_rate": 1.3876261972560187e-05,
      "loss": 1.2945,
      "step": 20870
    },
    {
      "epoch": 1.6173628459110363,
      "grad_norm": 2.101954936981201,
      "learning_rate": 1.3868495987574425e-05,
      "loss": 1.2796,
      "step": 20880
    },
    {
      "epoch": 1.6181374542496951,
      "grad_norm": 2.8975980281829834,
      "learning_rate": 1.3860730002588663e-05,
      "loss": 1.294,
      "step": 20890
    },
    {
      "epoch": 1.6189120625883537,
      "grad_norm": 2.7336082458496094,
      "learning_rate": 1.38529640176029e-05,
      "loss": 1.2844,
      "step": 20900
    },
    {
      "epoch": 1.6196866709270126,
      "grad_norm": 1.9420230388641357,
      "learning_rate": 1.3845198032617137e-05,
      "loss": 1.2412,
      "step": 20910
    },
    {
      "epoch": 1.6204612792656712,
      "grad_norm": 2.7693660259246826,
      "learning_rate": 1.3837432047631375e-05,
      "loss": 1.3713,
      "step": 20920
    },
    {
      "epoch": 1.62123588760433,
      "grad_norm": 2.263115882873535,
      "learning_rate": 1.3829666062645613e-05,
      "loss": 1.226,
      "step": 20930
    },
    {
      "epoch": 1.6220104959429889,
      "grad_norm": 2.223189115524292,
      "learning_rate": 1.382190007765985e-05,
      "loss": 1.2478,
      "step": 20940
    },
    {
      "epoch": 1.6227851042816477,
      "grad_norm": 2.1513314247131348,
      "learning_rate": 1.3814134092674087e-05,
      "loss": 1.3017,
      "step": 20950
    },
    {
      "epoch": 1.6235597126203065,
      "grad_norm": 2.180631160736084,
      "learning_rate": 1.3806368107688324e-05,
      "loss": 1.2235,
      "step": 20960
    },
    {
      "epoch": 1.6243343209589651,
      "grad_norm": 2.2005515098571777,
      "learning_rate": 1.3798602122702564e-05,
      "loss": 1.2345,
      "step": 20970
    },
    {
      "epoch": 1.6251089292976237,
      "grad_norm": 2.4182984828948975,
      "learning_rate": 1.3790836137716802e-05,
      "loss": 1.2472,
      "step": 20980
    },
    {
      "epoch": 1.6258835376362826,
      "grad_norm": 3.2907416820526123,
      "learning_rate": 1.3783070152731038e-05,
      "loss": 1.3562,
      "step": 20990
    },
    {
      "epoch": 1.6266581459749414,
      "grad_norm": 2.754772663116455,
      "learning_rate": 1.3775304167745276e-05,
      "loss": 1.2736,
      "step": 21000
    },
    {
      "epoch": 1.6274327543136002,
      "grad_norm": 2.3168702125549316,
      "learning_rate": 1.3767538182759514e-05,
      "loss": 1.1849,
      "step": 21010
    },
    {
      "epoch": 1.628207362652259,
      "grad_norm": 2.8701446056365967,
      "learning_rate": 1.3759772197773752e-05,
      "loss": 1.2815,
      "step": 21020
    },
    {
      "epoch": 1.6289819709909177,
      "grad_norm": 3.301623582839966,
      "learning_rate": 1.3752006212787988e-05,
      "loss": 1.3234,
      "step": 21030
    },
    {
      "epoch": 1.6297565793295765,
      "grad_norm": 2.991956949234009,
      "learning_rate": 1.3744240227802226e-05,
      "loss": 1.1856,
      "step": 21040
    },
    {
      "epoch": 1.6305311876682351,
      "grad_norm": 2.391578197479248,
      "learning_rate": 1.3736474242816464e-05,
      "loss": 1.2551,
      "step": 21050
    },
    {
      "epoch": 1.631305796006894,
      "grad_norm": 2.61844801902771,
      "learning_rate": 1.3728708257830701e-05,
      "loss": 1.2036,
      "step": 21060
    },
    {
      "epoch": 1.6320804043455528,
      "grad_norm": 2.1403391361236572,
      "learning_rate": 1.3720942272844941e-05,
      "loss": 1.2557,
      "step": 21070
    },
    {
      "epoch": 1.6328550126842116,
      "grad_norm": 2.6915411949157715,
      "learning_rate": 1.3713176287859177e-05,
      "loss": 1.2088,
      "step": 21080
    },
    {
      "epoch": 1.6336296210228705,
      "grad_norm": 2.9503254890441895,
      "learning_rate": 1.3705410302873415e-05,
      "loss": 1.2185,
      "step": 21090
    },
    {
      "epoch": 1.634404229361529,
      "grad_norm": 2.3450052738189697,
      "learning_rate": 1.3697644317887653e-05,
      "loss": 1.2862,
      "step": 21100
    },
    {
      "epoch": 1.6351788377001877,
      "grad_norm": 2.588122606277466,
      "learning_rate": 1.368987833290189e-05,
      "loss": 1.2738,
      "step": 21110
    },
    {
      "epoch": 1.6359534460388465,
      "grad_norm": 2.689743995666504,
      "learning_rate": 1.3682112347916127e-05,
      "loss": 1.363,
      "step": 21120
    },
    {
      "epoch": 1.6367280543775053,
      "grad_norm": 2.2249317169189453,
      "learning_rate": 1.3674346362930365e-05,
      "loss": 1.2253,
      "step": 21130
    },
    {
      "epoch": 1.6375026627161642,
      "grad_norm": 2.10317063331604,
      "learning_rate": 1.3666580377944603e-05,
      "loss": 1.3666,
      "step": 21140
    },
    {
      "epoch": 1.638277271054823,
      "grad_norm": 2.053946018218994,
      "learning_rate": 1.365881439295884e-05,
      "loss": 1.2403,
      "step": 21150
    },
    {
      "epoch": 1.6390518793934816,
      "grad_norm": 2.706115245819092,
      "learning_rate": 1.3651048407973078e-05,
      "loss": 1.2185,
      "step": 21160
    },
    {
      "epoch": 1.6398264877321405,
      "grad_norm": 2.7344870567321777,
      "learning_rate": 1.3643282422987316e-05,
      "loss": 1.2788,
      "step": 21170
    },
    {
      "epoch": 1.640601096070799,
      "grad_norm": 2.2840967178344727,
      "learning_rate": 1.3635516438001554e-05,
      "loss": 1.2789,
      "step": 21180
    },
    {
      "epoch": 1.641375704409458,
      "grad_norm": 2.43717885017395,
      "learning_rate": 1.3627750453015792e-05,
      "loss": 1.2738,
      "step": 21190
    },
    {
      "epoch": 1.6421503127481167,
      "grad_norm": 1.7917089462280273,
      "learning_rate": 1.3619984468030028e-05,
      "loss": 1.2604,
      "step": 21200
    },
    {
      "epoch": 1.6429249210867756,
      "grad_norm": 2.246891975402832,
      "learning_rate": 1.3612218483044266e-05,
      "loss": 1.2609,
      "step": 21210
    },
    {
      "epoch": 1.6436995294254344,
      "grad_norm": 2.546290874481201,
      "learning_rate": 1.3604452498058504e-05,
      "loss": 1.3124,
      "step": 21220
    },
    {
      "epoch": 1.644474137764093,
      "grad_norm": 2.7900967597961426,
      "learning_rate": 1.3596686513072742e-05,
      "loss": 1.2282,
      "step": 21230
    },
    {
      "epoch": 1.6452487461027518,
      "grad_norm": 2.4287781715393066,
      "learning_rate": 1.358892052808698e-05,
      "loss": 1.3045,
      "step": 21240
    },
    {
      "epoch": 1.6460233544414105,
      "grad_norm": 3.215473175048828,
      "learning_rate": 1.3581154543101216e-05,
      "loss": 1.2643,
      "step": 21250
    },
    {
      "epoch": 1.6467979627800693,
      "grad_norm": 2.87829327583313,
      "learning_rate": 1.3573388558115455e-05,
      "loss": 1.3159,
      "step": 21260
    },
    {
      "epoch": 1.6475725711187281,
      "grad_norm": 1.699436068534851,
      "learning_rate": 1.3565622573129693e-05,
      "loss": 1.2623,
      "step": 21270
    },
    {
      "epoch": 1.648347179457387,
      "grad_norm": 2.353806495666504,
      "learning_rate": 1.3557856588143931e-05,
      "loss": 1.2277,
      "step": 21280
    },
    {
      "epoch": 1.6491217877960458,
      "grad_norm": 2.0271358489990234,
      "learning_rate": 1.3550090603158167e-05,
      "loss": 1.2522,
      "step": 21290
    },
    {
      "epoch": 1.6498963961347044,
      "grad_norm": 2.421851873397827,
      "learning_rate": 1.3542324618172405e-05,
      "loss": 1.1982,
      "step": 21300
    },
    {
      "epoch": 1.650671004473363,
      "grad_norm": 2.424055814743042,
      "learning_rate": 1.3534558633186643e-05,
      "loss": 1.1991,
      "step": 21310
    },
    {
      "epoch": 1.6514456128120218,
      "grad_norm": 2.1166598796844482,
      "learning_rate": 1.3526792648200881e-05,
      "loss": 1.2665,
      "step": 21320
    },
    {
      "epoch": 1.6522202211506807,
      "grad_norm": 3.5122528076171875,
      "learning_rate": 1.3519026663215117e-05,
      "loss": 1.1727,
      "step": 21330
    },
    {
      "epoch": 1.6529948294893395,
      "grad_norm": 2.240356922149658,
      "learning_rate": 1.3511260678229355e-05,
      "loss": 1.2703,
      "step": 21340
    },
    {
      "epoch": 1.6537694378279983,
      "grad_norm": 2.719123601913452,
      "learning_rate": 1.3503494693243593e-05,
      "loss": 1.2388,
      "step": 21350
    },
    {
      "epoch": 1.654544046166657,
      "grad_norm": 2.7883427143096924,
      "learning_rate": 1.3495728708257832e-05,
      "loss": 1.3446,
      "step": 21360
    },
    {
      "epoch": 1.6553186545053158,
      "grad_norm": 2.332062005996704,
      "learning_rate": 1.3487962723272069e-05,
      "loss": 1.1801,
      "step": 21370
    },
    {
      "epoch": 1.6560932628439744,
      "grad_norm": 2.1422524452209473,
      "learning_rate": 1.3480196738286306e-05,
      "loss": 1.3593,
      "step": 21380
    },
    {
      "epoch": 1.6568678711826332,
      "grad_norm": 2.84338116645813,
      "learning_rate": 1.3472430753300544e-05,
      "loss": 1.2719,
      "step": 21390
    },
    {
      "epoch": 1.657642479521292,
      "grad_norm": 2.3922672271728516,
      "learning_rate": 1.3464664768314782e-05,
      "loss": 1.1851,
      "step": 21400
    },
    {
      "epoch": 1.658417087859951,
      "grad_norm": 2.742779016494751,
      "learning_rate": 1.3456898783329018e-05,
      "loss": 1.294,
      "step": 21410
    },
    {
      "epoch": 1.6591916961986097,
      "grad_norm": 2.311441421508789,
      "learning_rate": 1.3449132798343256e-05,
      "loss": 1.2924,
      "step": 21420
    },
    {
      "epoch": 1.6599663045372683,
      "grad_norm": 2.4522063732147217,
      "learning_rate": 1.3441366813357494e-05,
      "loss": 1.1376,
      "step": 21430
    },
    {
      "epoch": 1.660740912875927,
      "grad_norm": 2.3053231239318848,
      "learning_rate": 1.3433600828371732e-05,
      "loss": 1.4067,
      "step": 21440
    },
    {
      "epoch": 1.6615155212145858,
      "grad_norm": 2.5677664279937744,
      "learning_rate": 1.342583484338597e-05,
      "loss": 1.2838,
      "step": 21450
    },
    {
      "epoch": 1.6622901295532446,
      "grad_norm": 2.8992886543273926,
      "learning_rate": 1.3418068858400208e-05,
      "loss": 1.273,
      "step": 21460
    },
    {
      "epoch": 1.6630647378919035,
      "grad_norm": 2.432676076889038,
      "learning_rate": 1.3410302873414445e-05,
      "loss": 1.221,
      "step": 21470
    },
    {
      "epoch": 1.6638393462305623,
      "grad_norm": 2.620081663131714,
      "learning_rate": 1.3402536888428683e-05,
      "loss": 1.2888,
      "step": 21480
    },
    {
      "epoch": 1.664613954569221,
      "grad_norm": 2.4395813941955566,
      "learning_rate": 1.3394770903442921e-05,
      "loss": 1.2411,
      "step": 21490
    },
    {
      "epoch": 1.6653885629078797,
      "grad_norm": 2.4048349857330322,
      "learning_rate": 1.3387004918457157e-05,
      "loss": 1.2741,
      "step": 21500
    },
    {
      "epoch": 1.6661631712465383,
      "grad_norm": 2.129128932952881,
      "learning_rate": 1.3379238933471395e-05,
      "loss": 1.3938,
      "step": 21510
    },
    {
      "epoch": 1.6669377795851972,
      "grad_norm": 2.5943899154663086,
      "learning_rate": 1.3371472948485633e-05,
      "loss": 1.2514,
      "step": 21520
    },
    {
      "epoch": 1.667712387923856,
      "grad_norm": 2.490283966064453,
      "learning_rate": 1.3363706963499871e-05,
      "loss": 1.3015,
      "step": 21530
    },
    {
      "epoch": 1.6684869962625148,
      "grad_norm": 2.1551666259765625,
      "learning_rate": 1.3355940978514107e-05,
      "loss": 1.2427,
      "step": 21540
    },
    {
      "epoch": 1.6692616046011737,
      "grad_norm": 2.0920193195343018,
      "learning_rate": 1.3348174993528345e-05,
      "loss": 1.2718,
      "step": 21550
    },
    {
      "epoch": 1.6700362129398323,
      "grad_norm": 2.420213460922241,
      "learning_rate": 1.3340409008542585e-05,
      "loss": 1.1504,
      "step": 21560
    },
    {
      "epoch": 1.670810821278491,
      "grad_norm": 2.309100389480591,
      "learning_rate": 1.3332643023556822e-05,
      "loss": 1.2755,
      "step": 21570
    },
    {
      "epoch": 1.6715854296171497,
      "grad_norm": 2.728019952774048,
      "learning_rate": 1.3324877038571059e-05,
      "loss": 1.2979,
      "step": 21580
    },
    {
      "epoch": 1.6723600379558086,
      "grad_norm": 2.1455891132354736,
      "learning_rate": 1.3317111053585296e-05,
      "loss": 1.2362,
      "step": 21590
    },
    {
      "epoch": 1.6731346462944674,
      "grad_norm": 2.048119068145752,
      "learning_rate": 1.3309345068599534e-05,
      "loss": 1.2642,
      "step": 21600
    },
    {
      "epoch": 1.6739092546331262,
      "grad_norm": 2.129546642303467,
      "learning_rate": 1.3301579083613772e-05,
      "loss": 1.2115,
      "step": 21610
    },
    {
      "epoch": 1.6746838629717848,
      "grad_norm": 2.758152961730957,
      "learning_rate": 1.329381309862801e-05,
      "loss": 1.2963,
      "step": 21620
    },
    {
      "epoch": 1.6754584713104437,
      "grad_norm": 2.1870994567871094,
      "learning_rate": 1.3286047113642246e-05,
      "loss": 1.37,
      "step": 21630
    },
    {
      "epoch": 1.6762330796491023,
      "grad_norm": 2.797693967819214,
      "learning_rate": 1.3278281128656484e-05,
      "loss": 1.1735,
      "step": 21640
    },
    {
      "epoch": 1.6770076879877611,
      "grad_norm": 3.295400381088257,
      "learning_rate": 1.3270515143670724e-05,
      "loss": 1.2108,
      "step": 21650
    },
    {
      "epoch": 1.67778229632642,
      "grad_norm": 2.6684248447418213,
      "learning_rate": 1.3262749158684962e-05,
      "loss": 1.2804,
      "step": 21660
    },
    {
      "epoch": 1.6785569046650788,
      "grad_norm": 2.1433842182159424,
      "learning_rate": 1.3254983173699198e-05,
      "loss": 1.1828,
      "step": 21670
    },
    {
      "epoch": 1.6793315130037376,
      "grad_norm": 1.9240564107894897,
      "learning_rate": 1.3247217188713436e-05,
      "loss": 1.234,
      "step": 21680
    },
    {
      "epoch": 1.6801061213423962,
      "grad_norm": 3.5325815677642822,
      "learning_rate": 1.3239451203727673e-05,
      "loss": 1.1397,
      "step": 21690
    },
    {
      "epoch": 1.680880729681055,
      "grad_norm": 2.9136135578155518,
      "learning_rate": 1.3231685218741911e-05,
      "loss": 1.2776,
      "step": 21700
    },
    {
      "epoch": 1.6816553380197137,
      "grad_norm": 2.446030855178833,
      "learning_rate": 1.3223919233756148e-05,
      "loss": 1.2491,
      "step": 21710
    },
    {
      "epoch": 1.6824299463583725,
      "grad_norm": 2.498687505722046,
      "learning_rate": 1.3216153248770385e-05,
      "loss": 1.1932,
      "step": 21720
    },
    {
      "epoch": 1.6832045546970313,
      "grad_norm": 2.3825178146362305,
      "learning_rate": 1.3208387263784623e-05,
      "loss": 1.3043,
      "step": 21730
    },
    {
      "epoch": 1.6839791630356902,
      "grad_norm": 3.2970521450042725,
      "learning_rate": 1.3200621278798861e-05,
      "loss": 1.2365,
      "step": 21740
    },
    {
      "epoch": 1.684753771374349,
      "grad_norm": 2.5467629432678223,
      "learning_rate": 1.3192855293813099e-05,
      "loss": 1.2413,
      "step": 21750
    },
    {
      "epoch": 1.6855283797130076,
      "grad_norm": 2.496107339859009,
      "learning_rate": 1.3185089308827337e-05,
      "loss": 1.2115,
      "step": 21760
    },
    {
      "epoch": 1.6863029880516662,
      "grad_norm": 2.5988330841064453,
      "learning_rate": 1.3177323323841575e-05,
      "loss": 1.3233,
      "step": 21770
    },
    {
      "epoch": 1.687077596390325,
      "grad_norm": 2.293583393096924,
      "learning_rate": 1.3169557338855813e-05,
      "loss": 1.2978,
      "step": 21780
    },
    {
      "epoch": 1.687852204728984,
      "grad_norm": 2.363734245300293,
      "learning_rate": 1.316179135387005e-05,
      "loss": 1.2522,
      "step": 21790
    },
    {
      "epoch": 1.6886268130676427,
      "grad_norm": 4.988167762756348,
      "learning_rate": 1.3154025368884287e-05,
      "loss": 1.2138,
      "step": 21800
    },
    {
      "epoch": 1.6894014214063016,
      "grad_norm": 2.9516007900238037,
      "learning_rate": 1.3146259383898524e-05,
      "loss": 1.2214,
      "step": 21810
    },
    {
      "epoch": 1.6901760297449602,
      "grad_norm": 2.15810489654541,
      "learning_rate": 1.3138493398912762e-05,
      "loss": 1.2738,
      "step": 21820
    },
    {
      "epoch": 1.690950638083619,
      "grad_norm": 1.9637051820755005,
      "learning_rate": 1.3130727413927e-05,
      "loss": 1.2055,
      "step": 21830
    },
    {
      "epoch": 1.6917252464222776,
      "grad_norm": 2.4415183067321777,
      "learning_rate": 1.3122961428941236e-05,
      "loss": 1.2423,
      "step": 21840
    },
    {
      "epoch": 1.6924998547609365,
      "grad_norm": 2.0045838356018066,
      "learning_rate": 1.3115195443955476e-05,
      "loss": 1.2339,
      "step": 21850
    },
    {
      "epoch": 1.6932744630995953,
      "grad_norm": 2.6403918266296387,
      "learning_rate": 1.3107429458969714e-05,
      "loss": 1.2855,
      "step": 21860
    },
    {
      "epoch": 1.6940490714382541,
      "grad_norm": 2.2272191047668457,
      "learning_rate": 1.3099663473983952e-05,
      "loss": 1.2729,
      "step": 21870
    },
    {
      "epoch": 1.694823679776913,
      "grad_norm": 2.374643564224243,
      "learning_rate": 1.3092674087496763e-05,
      "loss": 1.2488,
      "step": 21880
    },
    {
      "epoch": 1.6955982881155716,
      "grad_norm": 2.124319553375244,
      "learning_rate": 1.3084908102511003e-05,
      "loss": 1.308,
      "step": 21890
    },
    {
      "epoch": 1.6963728964542302,
      "grad_norm": 2.561593770980835,
      "learning_rate": 1.307714211752524e-05,
      "loss": 1.2944,
      "step": 21900
    },
    {
      "epoch": 1.697147504792889,
      "grad_norm": 2.428887367248535,
      "learning_rate": 1.3069376132539479e-05,
      "loss": 1.2041,
      "step": 21910
    },
    {
      "epoch": 1.6979221131315478,
      "grad_norm": 2.657278060913086,
      "learning_rate": 1.3061610147553715e-05,
      "loss": 1.3564,
      "step": 21920
    },
    {
      "epoch": 1.6986967214702067,
      "grad_norm": 2.8946950435638428,
      "learning_rate": 1.3053844162567953e-05,
      "loss": 1.2714,
      "step": 21930
    },
    {
      "epoch": 1.6994713298088655,
      "grad_norm": 3.159761905670166,
      "learning_rate": 1.304607817758219e-05,
      "loss": 1.2502,
      "step": 21940
    },
    {
      "epoch": 1.7002459381475241,
      "grad_norm": 1.8643733263015747,
      "learning_rate": 1.3038312192596428e-05,
      "loss": 1.3106,
      "step": 21950
    },
    {
      "epoch": 1.701020546486183,
      "grad_norm": 2.455167293548584,
      "learning_rate": 1.3030546207610665e-05,
      "loss": 1.2363,
      "step": 21960
    },
    {
      "epoch": 1.7017951548248416,
      "grad_norm": 2.133436679840088,
      "learning_rate": 1.3022780222624902e-05,
      "loss": 1.3416,
      "step": 21970
    },
    {
      "epoch": 1.7025697631635004,
      "grad_norm": 1.8461815118789673,
      "learning_rate": 1.301501423763914e-05,
      "loss": 1.3192,
      "step": 21980
    },
    {
      "epoch": 1.7033443715021592,
      "grad_norm": 2.6906111240386963,
      "learning_rate": 1.300724825265338e-05,
      "loss": 1.3035,
      "step": 21990
    },
    {
      "epoch": 1.704118979840818,
      "grad_norm": 2.040741443634033,
      "learning_rate": 1.2999482267667616e-05,
      "loss": 1.1503,
      "step": 22000
    },
    {
      "epoch": 1.704893588179477,
      "grad_norm": 2.1346230506896973,
      "learning_rate": 1.2991716282681854e-05,
      "loss": 1.2525,
      "step": 22010
    },
    {
      "epoch": 1.7056681965181355,
      "grad_norm": 2.671146869659424,
      "learning_rate": 1.2983950297696092e-05,
      "loss": 1.2674,
      "step": 22020
    },
    {
      "epoch": 1.7064428048567943,
      "grad_norm": 2.691807746887207,
      "learning_rate": 1.297618431271033e-05,
      "loss": 1.2381,
      "step": 22030
    },
    {
      "epoch": 1.707217413195453,
      "grad_norm": 2.674238681793213,
      "learning_rate": 1.2968418327724566e-05,
      "loss": 1.3161,
      "step": 22040
    },
    {
      "epoch": 1.7079920215341118,
      "grad_norm": 2.2658534049987793,
      "learning_rate": 1.2960652342738804e-05,
      "loss": 1.3008,
      "step": 22050
    },
    {
      "epoch": 1.7087666298727706,
      "grad_norm": 2.3315677642822266,
      "learning_rate": 1.2952886357753042e-05,
      "loss": 1.2961,
      "step": 22060
    },
    {
      "epoch": 1.7095412382114294,
      "grad_norm": 3.046710252761841,
      "learning_rate": 1.294512037276728e-05,
      "loss": 1.2336,
      "step": 22070
    },
    {
      "epoch": 1.710315846550088,
      "grad_norm": 2.0676827430725098,
      "learning_rate": 1.2937354387781517e-05,
      "loss": 1.1622,
      "step": 22080
    },
    {
      "epoch": 1.711090454888747,
      "grad_norm": 3.1409835815429688,
      "learning_rate": 1.2929588402795755e-05,
      "loss": 1.3512,
      "step": 22090
    },
    {
      "epoch": 1.7118650632274055,
      "grad_norm": 2.846522092819214,
      "learning_rate": 1.2921822417809993e-05,
      "loss": 1.2872,
      "step": 22100
    },
    {
      "epoch": 1.7126396715660643,
      "grad_norm": 2.5203638076782227,
      "learning_rate": 1.2914056432824231e-05,
      "loss": 1.1829,
      "step": 22110
    },
    {
      "epoch": 1.7134142799047232,
      "grad_norm": 2.8402233123779297,
      "learning_rate": 1.2906290447838469e-05,
      "loss": 1.2283,
      "step": 22120
    },
    {
      "epoch": 1.714188888243382,
      "grad_norm": 2.6423513889312744,
      "learning_rate": 1.2898524462852705e-05,
      "loss": 1.1577,
      "step": 22130
    },
    {
      "epoch": 1.7149634965820408,
      "grad_norm": 2.2290449142456055,
      "learning_rate": 1.2890758477866943e-05,
      "loss": 1.2577,
      "step": 22140
    },
    {
      "epoch": 1.7157381049206994,
      "grad_norm": 2.83505916595459,
      "learning_rate": 1.288299249288118e-05,
      "loss": 1.3247,
      "step": 22150
    },
    {
      "epoch": 1.7165127132593583,
      "grad_norm": 2.7487785816192627,
      "learning_rate": 1.2875226507895419e-05,
      "loss": 1.2119,
      "step": 22160
    },
    {
      "epoch": 1.717287321598017,
      "grad_norm": 2.276662826538086,
      "learning_rate": 1.2867460522909655e-05,
      "loss": 1.3602,
      "step": 22170
    },
    {
      "epoch": 1.7180619299366757,
      "grad_norm": 2.0351598262786865,
      "learning_rate": 1.2859694537923894e-05,
      "loss": 1.3089,
      "step": 22180
    },
    {
      "epoch": 1.7188365382753346,
      "grad_norm": 3.346140146255493,
      "learning_rate": 1.2851928552938132e-05,
      "loss": 1.3029,
      "step": 22190
    },
    {
      "epoch": 1.7196111466139934,
      "grad_norm": 2.4932212829589844,
      "learning_rate": 1.284416256795237e-05,
      "loss": 1.2716,
      "step": 22200
    },
    {
      "epoch": 1.7203857549526522,
      "grad_norm": 2.5561106204986572,
      "learning_rate": 1.2836396582966606e-05,
      "loss": 1.2518,
      "step": 22210
    },
    {
      "epoch": 1.7211603632913108,
      "grad_norm": 2.5098416805267334,
      "learning_rate": 1.2828630597980844e-05,
      "loss": 1.1304,
      "step": 22220
    },
    {
      "epoch": 1.7219349716299694,
      "grad_norm": 2.1141390800476074,
      "learning_rate": 1.2820864612995082e-05,
      "loss": 1.2298,
      "step": 22230
    },
    {
      "epoch": 1.7227095799686283,
      "grad_norm": 2.5326008796691895,
      "learning_rate": 1.281309862800932e-05,
      "loss": 1.265,
      "step": 22240
    },
    {
      "epoch": 1.7234841883072871,
      "grad_norm": 2.1089048385620117,
      "learning_rate": 1.2805332643023556e-05,
      "loss": 1.1749,
      "step": 22250
    },
    {
      "epoch": 1.724258796645946,
      "grad_norm": 2.6566593647003174,
      "learning_rate": 1.2797566658037794e-05,
      "loss": 1.2916,
      "step": 22260
    },
    {
      "epoch": 1.7250334049846048,
      "grad_norm": 2.1601269245147705,
      "learning_rate": 1.2789800673052032e-05,
      "loss": 1.1997,
      "step": 22270
    },
    {
      "epoch": 1.7258080133232634,
      "grad_norm": 2.399050712585449,
      "learning_rate": 1.2782034688066271e-05,
      "loss": 1.2752,
      "step": 22280
    },
    {
      "epoch": 1.7265826216619222,
      "grad_norm": 2.5782275199890137,
      "learning_rate": 1.2774268703080509e-05,
      "loss": 1.1955,
      "step": 22290
    },
    {
      "epoch": 1.7273572300005808,
      "grad_norm": 2.1881401538848877,
      "learning_rate": 1.2766502718094745e-05,
      "loss": 1.2187,
      "step": 22300
    },
    {
      "epoch": 1.7281318383392397,
      "grad_norm": 2.1373138427734375,
      "learning_rate": 1.2758736733108983e-05,
      "loss": 1.2726,
      "step": 22310
    },
    {
      "epoch": 1.7289064466778985,
      "grad_norm": 2.0466573238372803,
      "learning_rate": 1.2750970748123221e-05,
      "loss": 1.3097,
      "step": 22320
    },
    {
      "epoch": 1.7296810550165573,
      "grad_norm": 1.9660643339157104,
      "learning_rate": 1.2743204763137459e-05,
      "loss": 1.318,
      "step": 22330
    },
    {
      "epoch": 1.7304556633552162,
      "grad_norm": 3.0275254249572754,
      "learning_rate": 1.2735438778151695e-05,
      "loss": 1.3399,
      "step": 22340
    },
    {
      "epoch": 1.7312302716938748,
      "grad_norm": 3.54988956451416,
      "learning_rate": 1.2727672793165933e-05,
      "loss": 1.1976,
      "step": 22350
    },
    {
      "epoch": 1.7320048800325334,
      "grad_norm": 2.3910422325134277,
      "learning_rate": 1.271990680818017e-05,
      "loss": 1.3271,
      "step": 22360
    },
    {
      "epoch": 1.7327794883711922,
      "grad_norm": 2.142812967300415,
      "learning_rate": 1.2712140823194409e-05,
      "loss": 1.3583,
      "step": 22370
    },
    {
      "epoch": 1.733554096709851,
      "grad_norm": 2.2008001804351807,
      "learning_rate": 1.2704374838208647e-05,
      "loss": 1.345,
      "step": 22380
    },
    {
      "epoch": 1.7343287050485099,
      "grad_norm": 2.5012824535369873,
      "learning_rate": 1.2696608853222884e-05,
      "loss": 1.2742,
      "step": 22390
    },
    {
      "epoch": 1.7351033133871687,
      "grad_norm": 2.335007429122925,
      "learning_rate": 1.2688842868237122e-05,
      "loss": 1.2373,
      "step": 22400
    },
    {
      "epoch": 1.7358779217258273,
      "grad_norm": 2.6812095642089844,
      "learning_rate": 1.268107688325136e-05,
      "loss": 1.2011,
      "step": 22410
    },
    {
      "epoch": 1.7366525300644862,
      "grad_norm": 2.7200756072998047,
      "learning_rate": 1.2673310898265596e-05,
      "loss": 1.1538,
      "step": 22420
    },
    {
      "epoch": 1.7374271384031448,
      "grad_norm": 2.241671085357666,
      "learning_rate": 1.2665544913279834e-05,
      "loss": 1.2594,
      "step": 22430
    },
    {
      "epoch": 1.7382017467418036,
      "grad_norm": 2.3249332904815674,
      "learning_rate": 1.2657778928294072e-05,
      "loss": 1.2588,
      "step": 22440
    },
    {
      "epoch": 1.7389763550804624,
      "grad_norm": 2.4525792598724365,
      "learning_rate": 1.265001294330831e-05,
      "loss": 1.1825,
      "step": 22450
    },
    {
      "epoch": 1.7397509634191213,
      "grad_norm": 2.79471755027771,
      "learning_rate": 1.2642246958322548e-05,
      "loss": 1.2453,
      "step": 22460
    },
    {
      "epoch": 1.74052557175778,
      "grad_norm": 2.760528087615967,
      "learning_rate": 1.2634480973336784e-05,
      "loss": 1.3363,
      "step": 22470
    },
    {
      "epoch": 1.7413001800964387,
      "grad_norm": 2.2666451930999756,
      "learning_rate": 1.2626714988351023e-05,
      "loss": 1.2921,
      "step": 22480
    },
    {
      "epoch": 1.7420747884350976,
      "grad_norm": 2.5394468307495117,
      "learning_rate": 1.2618949003365261e-05,
      "loss": 1.1955,
      "step": 22490
    },
    {
      "epoch": 1.7428493967737562,
      "grad_norm": 2.219550609588623,
      "learning_rate": 1.26111830183795e-05,
      "loss": 1.1444,
      "step": 22500
    },
    {
      "epoch": 1.743624005112415,
      "grad_norm": 2.5677318572998047,
      "learning_rate": 1.2603417033393735e-05,
      "loss": 1.1479,
      "step": 22510
    },
    {
      "epoch": 1.7443986134510738,
      "grad_norm": 2.0444698333740234,
      "learning_rate": 1.2595651048407973e-05,
      "loss": 1.18,
      "step": 22520
    },
    {
      "epoch": 1.7451732217897327,
      "grad_norm": 2.2721469402313232,
      "learning_rate": 1.2587885063422211e-05,
      "loss": 1.3025,
      "step": 22530
    },
    {
      "epoch": 1.7459478301283915,
      "grad_norm": 2.365278720855713,
      "learning_rate": 1.2580119078436449e-05,
      "loss": 1.237,
      "step": 22540
    },
    {
      "epoch": 1.74672243846705,
      "grad_norm": 3.042372941970825,
      "learning_rate": 1.2572353093450685e-05,
      "loss": 1.2473,
      "step": 22550
    },
    {
      "epoch": 1.7474970468057087,
      "grad_norm": 2.5809249877929688,
      "learning_rate": 1.2564587108464923e-05,
      "loss": 1.2525,
      "step": 22560
    },
    {
      "epoch": 1.7482716551443676,
      "grad_norm": 2.0642011165618896,
      "learning_rate": 1.2556821123479161e-05,
      "loss": 1.261,
      "step": 22570
    },
    {
      "epoch": 1.7490462634830264,
      "grad_norm": 2.266986131668091,
      "learning_rate": 1.25490551384934e-05,
      "loss": 1.2262,
      "step": 22580
    },
    {
      "epoch": 1.7498208718216852,
      "grad_norm": 2.1886961460113525,
      "learning_rate": 1.2541289153507637e-05,
      "loss": 1.2458,
      "step": 22590
    },
    {
      "epoch": 1.750595480160344,
      "grad_norm": 2.2199325561523438,
      "learning_rate": 1.2533523168521874e-05,
      "loss": 1.2755,
      "step": 22600
    },
    {
      "epoch": 1.7513700884990027,
      "grad_norm": 2.157095193862915,
      "learning_rate": 1.2525757183536112e-05,
      "loss": 1.3274,
      "step": 22610
    },
    {
      "epoch": 1.7521446968376615,
      "grad_norm": 2.106661081314087,
      "learning_rate": 1.251799119855035e-05,
      "loss": 1.3078,
      "step": 22620
    },
    {
      "epoch": 1.75291930517632,
      "grad_norm": 1.9256187677383423,
      "learning_rate": 1.2510225213564588e-05,
      "loss": 1.3045,
      "step": 22630
    },
    {
      "epoch": 1.753693913514979,
      "grad_norm": 2.5972399711608887,
      "learning_rate": 1.2502459228578824e-05,
      "loss": 1.1717,
      "step": 22640
    },
    {
      "epoch": 1.7544685218536378,
      "grad_norm": 2.5133984088897705,
      "learning_rate": 1.2494693243593062e-05,
      "loss": 1.3396,
      "step": 22650
    },
    {
      "epoch": 1.7552431301922966,
      "grad_norm": 2.19966721534729,
      "learning_rate": 1.24869272586073e-05,
      "loss": 1.2643,
      "step": 22660
    },
    {
      "epoch": 1.7560177385309554,
      "grad_norm": 2.060588836669922,
      "learning_rate": 1.247916127362154e-05,
      "loss": 1.2253,
      "step": 22670
    },
    {
      "epoch": 1.756792346869614,
      "grad_norm": 3.0392305850982666,
      "learning_rate": 1.2471395288635776e-05,
      "loss": 1.1383,
      "step": 22680
    },
    {
      "epoch": 1.7575669552082727,
      "grad_norm": 2.375760793685913,
      "learning_rate": 1.2463629303650014e-05,
      "loss": 1.2223,
      "step": 22690
    },
    {
      "epoch": 1.7583415635469315,
      "grad_norm": 2.964268922805786,
      "learning_rate": 1.2455863318664251e-05,
      "loss": 1.2826,
      "step": 22700
    },
    {
      "epoch": 1.7591161718855903,
      "grad_norm": 2.791395425796509,
      "learning_rate": 1.244809733367849e-05,
      "loss": 1.2809,
      "step": 22710
    },
    {
      "epoch": 1.7598907802242492,
      "grad_norm": 2.3228280544281006,
      "learning_rate": 1.2440331348692725e-05,
      "loss": 1.2735,
      "step": 22720
    },
    {
      "epoch": 1.760665388562908,
      "grad_norm": 2.1224348545074463,
      "learning_rate": 1.2432565363706963e-05,
      "loss": 1.2854,
      "step": 22730
    },
    {
      "epoch": 1.7614399969015666,
      "grad_norm": 1.9345954656600952,
      "learning_rate": 1.2424799378721201e-05,
      "loss": 1.2528,
      "step": 22740
    },
    {
      "epoch": 1.7622146052402254,
      "grad_norm": 2.7663638591766357,
      "learning_rate": 1.2417033393735439e-05,
      "loss": 1.3187,
      "step": 22750
    },
    {
      "epoch": 1.762989213578884,
      "grad_norm": 2.696502447128296,
      "learning_rate": 1.2409267408749675e-05,
      "loss": 1.276,
      "step": 22760
    },
    {
      "epoch": 1.7637638219175429,
      "grad_norm": 2.5621814727783203,
      "learning_rate": 1.2401501423763915e-05,
      "loss": 1.251,
      "step": 22770
    },
    {
      "epoch": 1.7645384302562017,
      "grad_norm": 2.1179347038269043,
      "learning_rate": 1.2393735438778153e-05,
      "loss": 1.2523,
      "step": 22780
    },
    {
      "epoch": 1.7653130385948606,
      "grad_norm": 2.945643901824951,
      "learning_rate": 1.238596945379239e-05,
      "loss": 1.2489,
      "step": 22790
    },
    {
      "epoch": 1.7660876469335194,
      "grad_norm": 2.6662442684173584,
      "learning_rate": 1.2378203468806627e-05,
      "loss": 1.181,
      "step": 22800
    },
    {
      "epoch": 1.766862255272178,
      "grad_norm": 2.342130661010742,
      "learning_rate": 1.2370437483820865e-05,
      "loss": 1.2722,
      "step": 22810
    },
    {
      "epoch": 1.7676368636108366,
      "grad_norm": 2.331312656402588,
      "learning_rate": 1.2362671498835102e-05,
      "loss": 1.2446,
      "step": 22820
    },
    {
      "epoch": 1.7684114719494954,
      "grad_norm": 2.3740735054016113,
      "learning_rate": 1.235490551384934e-05,
      "loss": 1.3447,
      "step": 22830
    },
    {
      "epoch": 1.7691860802881543,
      "grad_norm": 2.2017948627471924,
      "learning_rate": 1.2347139528863578e-05,
      "loss": 1.3701,
      "step": 22840
    },
    {
      "epoch": 1.769960688626813,
      "grad_norm": 2.5196123123168945,
      "learning_rate": 1.2339373543877814e-05,
      "loss": 1.1255,
      "step": 22850
    },
    {
      "epoch": 1.770735296965472,
      "grad_norm": 2.7801551818847656,
      "learning_rate": 1.2331607558892052e-05,
      "loss": 1.3498,
      "step": 22860
    },
    {
      "epoch": 1.7715099053041305,
      "grad_norm": 1.9000916481018066,
      "learning_rate": 1.2323841573906292e-05,
      "loss": 1.2494,
      "step": 22870
    },
    {
      "epoch": 1.7722845136427894,
      "grad_norm": 2.623323440551758,
      "learning_rate": 1.231607558892053e-05,
      "loss": 1.2836,
      "step": 22880
    },
    {
      "epoch": 1.773059121981448,
      "grad_norm": 2.8020639419555664,
      "learning_rate": 1.2308309603934766e-05,
      "loss": 1.2751,
      "step": 22890
    },
    {
      "epoch": 1.7738337303201068,
      "grad_norm": 2.670523166656494,
      "learning_rate": 1.2300543618949004e-05,
      "loss": 1.2165,
      "step": 22900
    },
    {
      "epoch": 1.7746083386587657,
      "grad_norm": 2.9446027278900146,
      "learning_rate": 1.2292777633963242e-05,
      "loss": 1.1752,
      "step": 22910
    },
    {
      "epoch": 1.7753829469974245,
      "grad_norm": 2.58335542678833,
      "learning_rate": 1.228501164897748e-05,
      "loss": 1.289,
      "step": 22920
    },
    {
      "epoch": 1.7761575553360833,
      "grad_norm": 2.2640445232391357,
      "learning_rate": 1.2277245663991716e-05,
      "loss": 1.3046,
      "step": 22930
    },
    {
      "epoch": 1.776932163674742,
      "grad_norm": 2.260129928588867,
      "learning_rate": 1.2269479679005953e-05,
      "loss": 1.3385,
      "step": 22940
    },
    {
      "epoch": 1.7777067720134008,
      "grad_norm": 2.1812498569488525,
      "learning_rate": 1.2261713694020191e-05,
      "loss": 1.2215,
      "step": 22950
    },
    {
      "epoch": 1.7784813803520594,
      "grad_norm": 2.6681599617004395,
      "learning_rate": 1.225394770903443e-05,
      "loss": 1.2493,
      "step": 22960
    },
    {
      "epoch": 1.7792559886907182,
      "grad_norm": 2.7768542766571045,
      "learning_rate": 1.2246181724048667e-05,
      "loss": 1.3203,
      "step": 22970
    },
    {
      "epoch": 1.780030597029377,
      "grad_norm": 2.330706834793091,
      "learning_rate": 1.2238415739062905e-05,
      "loss": 1.3145,
      "step": 22980
    },
    {
      "epoch": 1.7808052053680359,
      "grad_norm": 3.054018020629883,
      "learning_rate": 1.2230649754077143e-05,
      "loss": 1.2954,
      "step": 22990
    },
    {
      "epoch": 1.7815798137066947,
      "grad_norm": 2.9791598320007324,
      "learning_rate": 1.222288376909138e-05,
      "loss": 1.2065,
      "step": 23000
    },
    {
      "epoch": 1.7823544220453533,
      "grad_norm": 2.2517735958099365,
      "learning_rate": 1.2215117784105619e-05,
      "loss": 1.2659,
      "step": 23010
    },
    {
      "epoch": 1.783129030384012,
      "grad_norm": 2.3446168899536133,
      "learning_rate": 1.2207351799119855e-05,
      "loss": 1.1968,
      "step": 23020
    },
    {
      "epoch": 1.7839036387226708,
      "grad_norm": 2.3711612224578857,
      "learning_rate": 1.2199585814134093e-05,
      "loss": 1.3926,
      "step": 23030
    },
    {
      "epoch": 1.7846782470613296,
      "grad_norm": 2.5251479148864746,
      "learning_rate": 1.219181982914833e-05,
      "loss": 1.2982,
      "step": 23040
    },
    {
      "epoch": 1.7854528553999884,
      "grad_norm": 2.3582818508148193,
      "learning_rate": 1.2184053844162568e-05,
      "loss": 1.2422,
      "step": 23050
    },
    {
      "epoch": 1.7862274637386473,
      "grad_norm": 2.775729179382324,
      "learning_rate": 1.2176287859176804e-05,
      "loss": 1.2306,
      "step": 23060
    },
    {
      "epoch": 1.7870020720773059,
      "grad_norm": 2.1684744358062744,
      "learning_rate": 1.2168521874191044e-05,
      "loss": 1.27,
      "step": 23070
    },
    {
      "epoch": 1.7877766804159647,
      "grad_norm": 3.1196796894073486,
      "learning_rate": 1.2160755889205282e-05,
      "loss": 1.1949,
      "step": 23080
    },
    {
      "epoch": 1.7885512887546233,
      "grad_norm": 2.2446887493133545,
      "learning_rate": 1.215298990421952e-05,
      "loss": 1.3461,
      "step": 23090
    },
    {
      "epoch": 1.7893258970932822,
      "grad_norm": 2.417935848236084,
      "learning_rate": 1.2145223919233756e-05,
      "loss": 1.1968,
      "step": 23100
    },
    {
      "epoch": 1.790100505431941,
      "grad_norm": 2.3522770404815674,
      "learning_rate": 1.2137457934247994e-05,
      "loss": 1.3182,
      "step": 23110
    },
    {
      "epoch": 1.7908751137705998,
      "grad_norm": 2.3104145526885986,
      "learning_rate": 1.2129691949262232e-05,
      "loss": 1.2708,
      "step": 23120
    },
    {
      "epoch": 1.7916497221092587,
      "grad_norm": 2.339931011199951,
      "learning_rate": 1.212192596427647e-05,
      "loss": 1.3318,
      "step": 23130
    },
    {
      "epoch": 1.7924243304479173,
      "grad_norm": 2.3797249794006348,
      "learning_rate": 1.2114159979290706e-05,
      "loss": 1.2803,
      "step": 23140
    },
    {
      "epoch": 1.7931989387865759,
      "grad_norm": 2.725940465927124,
      "learning_rate": 1.2106393994304944e-05,
      "loss": 1.1822,
      "step": 23150
    },
    {
      "epoch": 1.7939735471252347,
      "grad_norm": 3.271026134490967,
      "learning_rate": 1.2098628009319183e-05,
      "loss": 1.2861,
      "step": 23160
    },
    {
      "epoch": 1.7947481554638935,
      "grad_norm": 2.235102415084839,
      "learning_rate": 1.2090862024333421e-05,
      "loss": 1.201,
      "step": 23170
    },
    {
      "epoch": 1.7955227638025524,
      "grad_norm": 1.9262906312942505,
      "learning_rate": 1.2083096039347659e-05,
      "loss": 1.2384,
      "step": 23180
    },
    {
      "epoch": 1.7962973721412112,
      "grad_norm": 3.0517046451568604,
      "learning_rate": 1.2075330054361895e-05,
      "loss": 1.3362,
      "step": 23190
    },
    {
      "epoch": 1.7970719804798698,
      "grad_norm": 1.8492460250854492,
      "learning_rate": 1.2067564069376133e-05,
      "loss": 1.24,
      "step": 23200
    },
    {
      "epoch": 1.7978465888185287,
      "grad_norm": 2.9376609325408936,
      "learning_rate": 1.205979808439037e-05,
      "loss": 1.149,
      "step": 23210
    },
    {
      "epoch": 1.7986211971571873,
      "grad_norm": 2.6192474365234375,
      "learning_rate": 1.2052032099404609e-05,
      "loss": 1.2295,
      "step": 23220
    },
    {
      "epoch": 1.799395805495846,
      "grad_norm": 2.5496208667755127,
      "learning_rate": 1.2044266114418845e-05,
      "loss": 1.2659,
      "step": 23230
    },
    {
      "epoch": 1.800170413834505,
      "grad_norm": 2.475193738937378,
      "learning_rate": 1.2036500129433083e-05,
      "loss": 1.231,
      "step": 23240
    },
    {
      "epoch": 1.8009450221731638,
      "grad_norm": 2.282738447189331,
      "learning_rate": 1.202873414444732e-05,
      "loss": 1.2712,
      "step": 23250
    },
    {
      "epoch": 1.8017196305118226,
      "grad_norm": 3.1738383769989014,
      "learning_rate": 1.202096815946156e-05,
      "loss": 1.2416,
      "step": 23260
    },
    {
      "epoch": 1.8024942388504812,
      "grad_norm": 2.5753173828125,
      "learning_rate": 1.2013202174475796e-05,
      "loss": 1.2482,
      "step": 23270
    },
    {
      "epoch": 1.80326884718914,
      "grad_norm": 2.1624417304992676,
      "learning_rate": 1.2005436189490034e-05,
      "loss": 1.3071,
      "step": 23280
    },
    {
      "epoch": 1.8040434555277987,
      "grad_norm": 2.548926591873169,
      "learning_rate": 1.1997670204504272e-05,
      "loss": 1.2538,
      "step": 23290
    },
    {
      "epoch": 1.8048180638664575,
      "grad_norm": 3.4079389572143555,
      "learning_rate": 1.198990421951851e-05,
      "loss": 1.2387,
      "step": 23300
    },
    {
      "epoch": 1.8055926722051163,
      "grad_norm": 2.89751935005188,
      "learning_rate": 1.1982138234532746e-05,
      "loss": 1.3256,
      "step": 23310
    },
    {
      "epoch": 1.8063672805437752,
      "grad_norm": 2.493168830871582,
      "learning_rate": 1.1974372249546984e-05,
      "loss": 1.3432,
      "step": 23320
    },
    {
      "epoch": 1.8071418888824338,
      "grad_norm": 2.6603341102600098,
      "learning_rate": 1.1966606264561222e-05,
      "loss": 1.2587,
      "step": 23330
    },
    {
      "epoch": 1.8079164972210926,
      "grad_norm": 2.795693874359131,
      "learning_rate": 1.195884027957546e-05,
      "loss": 1.3509,
      "step": 23340
    },
    {
      "epoch": 1.8086911055597512,
      "grad_norm": 1.9536083936691284,
      "learning_rate": 1.1951074294589698e-05,
      "loss": 1.2372,
      "step": 23350
    },
    {
      "epoch": 1.80946571389841,
      "grad_norm": 2.2884111404418945,
      "learning_rate": 1.1943308309603935e-05,
      "loss": 1.2626,
      "step": 23360
    },
    {
      "epoch": 1.8102403222370689,
      "grad_norm": 2.5409834384918213,
      "learning_rate": 1.1935542324618173e-05,
      "loss": 1.1893,
      "step": 23370
    },
    {
      "epoch": 1.8110149305757277,
      "grad_norm": 2.2190563678741455,
      "learning_rate": 1.1927776339632411e-05,
      "loss": 1.2541,
      "step": 23380
    },
    {
      "epoch": 1.8117895389143865,
      "grad_norm": 2.2872512340545654,
      "learning_rate": 1.1920010354646649e-05,
      "loss": 1.2899,
      "step": 23390
    },
    {
      "epoch": 1.8125641472530452,
      "grad_norm": 2.5683672428131104,
      "learning_rate": 1.1912244369660885e-05,
      "loss": 1.2027,
      "step": 23400
    },
    {
      "epoch": 1.813338755591704,
      "grad_norm": 2.7758212089538574,
      "learning_rate": 1.1904478384675123e-05,
      "loss": 1.2974,
      "step": 23410
    },
    {
      "epoch": 1.8141133639303626,
      "grad_norm": 2.449488878250122,
      "learning_rate": 1.1896712399689361e-05,
      "loss": 1.1911,
      "step": 23420
    },
    {
      "epoch": 1.8148879722690214,
      "grad_norm": 2.504906177520752,
      "learning_rate": 1.1888946414703599e-05,
      "loss": 1.2879,
      "step": 23430
    },
    {
      "epoch": 1.8156625806076803,
      "grad_norm": 2.1795153617858887,
      "learning_rate": 1.1881180429717835e-05,
      "loss": 1.245,
      "step": 23440
    },
    {
      "epoch": 1.816437188946339,
      "grad_norm": 2.2777280807495117,
      "learning_rate": 1.1873414444732073e-05,
      "loss": 1.3454,
      "step": 23450
    },
    {
      "epoch": 1.817211797284998,
      "grad_norm": 2.572816848754883,
      "learning_rate": 1.1865648459746312e-05,
      "loss": 1.265,
      "step": 23460
    },
    {
      "epoch": 1.8179864056236565,
      "grad_norm": 2.6072161197662354,
      "learning_rate": 1.185788247476055e-05,
      "loss": 1.2256,
      "step": 23470
    },
    {
      "epoch": 1.8187610139623152,
      "grad_norm": 3.651639223098755,
      "learning_rate": 1.1850116489774786e-05,
      "loss": 1.2058,
      "step": 23480
    },
    {
      "epoch": 1.819535622300974,
      "grad_norm": 2.298275947570801,
      "learning_rate": 1.1842350504789024e-05,
      "loss": 1.3248,
      "step": 23490
    },
    {
      "epoch": 1.8203102306396328,
      "grad_norm": 2.254854202270508,
      "learning_rate": 1.1834584519803262e-05,
      "loss": 1.2631,
      "step": 23500
    },
    {
      "epoch": 1.8210848389782917,
      "grad_norm": 2.0522043704986572,
      "learning_rate": 1.18268185348175e-05,
      "loss": 1.3074,
      "step": 23510
    },
    {
      "epoch": 1.8218594473169505,
      "grad_norm": 2.298767566680908,
      "learning_rate": 1.1819052549831738e-05,
      "loss": 1.3522,
      "step": 23520
    },
    {
      "epoch": 1.822634055655609,
      "grad_norm": 2.6669936180114746,
      "learning_rate": 1.1811286564845974e-05,
      "loss": 1.3233,
      "step": 23530
    },
    {
      "epoch": 1.823408663994268,
      "grad_norm": 2.4377596378326416,
      "learning_rate": 1.1803520579860212e-05,
      "loss": 1.2031,
      "step": 23540
    },
    {
      "epoch": 1.8241832723329265,
      "grad_norm": 2.2130913734436035,
      "learning_rate": 1.179575459487445e-05,
      "loss": 1.1299,
      "step": 23550
    },
    {
      "epoch": 1.8249578806715854,
      "grad_norm": 2.3562369346618652,
      "learning_rate": 1.178798860988869e-05,
      "loss": 1.2161,
      "step": 23560
    },
    {
      "epoch": 1.8257324890102442,
      "grad_norm": 2.178969621658325,
      "learning_rate": 1.1780222624902925e-05,
      "loss": 1.221,
      "step": 23570
    },
    {
      "epoch": 1.826507097348903,
      "grad_norm": 2.6927521228790283,
      "learning_rate": 1.1772456639917163e-05,
      "loss": 1.2172,
      "step": 23580
    },
    {
      "epoch": 1.8272817056875619,
      "grad_norm": 2.792433738708496,
      "learning_rate": 1.1764690654931401e-05,
      "loss": 1.2301,
      "step": 23590
    },
    {
      "epoch": 1.8280563140262205,
      "grad_norm": 2.5866875648498535,
      "learning_rate": 1.1756924669945639e-05,
      "loss": 1.3985,
      "step": 23600
    },
    {
      "epoch": 1.828830922364879,
      "grad_norm": 2.2340919971466064,
      "learning_rate": 1.1749158684959875e-05,
      "loss": 1.2302,
      "step": 23610
    },
    {
      "epoch": 1.829605530703538,
      "grad_norm": 3.0299413204193115,
      "learning_rate": 1.1741392699974113e-05,
      "loss": 1.3709,
      "step": 23620
    },
    {
      "epoch": 1.8303801390421968,
      "grad_norm": 2.5085225105285645,
      "learning_rate": 1.1733626714988351e-05,
      "loss": 1.247,
      "step": 23630
    },
    {
      "epoch": 1.8311547473808556,
      "grad_norm": 2.414586305618286,
      "learning_rate": 1.1725860730002589e-05,
      "loss": 1.2655,
      "step": 23640
    },
    {
      "epoch": 1.8319293557195144,
      "grad_norm": 2.3948066234588623,
      "learning_rate": 1.1718094745016827e-05,
      "loss": 1.2649,
      "step": 23650
    },
    {
      "epoch": 1.832703964058173,
      "grad_norm": 2.522547960281372,
      "learning_rate": 1.1710328760031065e-05,
      "loss": 1.2775,
      "step": 23660
    },
    {
      "epoch": 1.8334785723968319,
      "grad_norm": 2.361489772796631,
      "learning_rate": 1.1702562775045302e-05,
      "loss": 1.2325,
      "step": 23670
    },
    {
      "epoch": 1.8342531807354905,
      "grad_norm": 2.489154100418091,
      "learning_rate": 1.169479679005954e-05,
      "loss": 1.1637,
      "step": 23680
    },
    {
      "epoch": 1.8350277890741493,
      "grad_norm": 2.1091952323913574,
      "learning_rate": 1.1687030805073776e-05,
      "loss": 1.2433,
      "step": 23690
    },
    {
      "epoch": 1.8358023974128082,
      "grad_norm": 2.075941562652588,
      "learning_rate": 1.1679264820088014e-05,
      "loss": 1.1276,
      "step": 23700
    },
    {
      "epoch": 1.836577005751467,
      "grad_norm": 2.419553756713867,
      "learning_rate": 1.1671498835102252e-05,
      "loss": 1.2091,
      "step": 23710
    },
    {
      "epoch": 1.8373516140901258,
      "grad_norm": 2.3567636013031006,
      "learning_rate": 1.166373285011649e-05,
      "loss": 1.2786,
      "step": 23720
    },
    {
      "epoch": 1.8381262224287844,
      "grad_norm": 2.3237175941467285,
      "learning_rate": 1.1655966865130728e-05,
      "loss": 1.2477,
      "step": 23730
    },
    {
      "epoch": 1.8389008307674433,
      "grad_norm": 2.54001784324646,
      "learning_rate": 1.1648200880144964e-05,
      "loss": 1.285,
      "step": 23740
    },
    {
      "epoch": 1.8396754391061019,
      "grad_norm": 2.0703396797180176,
      "learning_rate": 1.1640434895159204e-05,
      "loss": 1.283,
      "step": 23750
    },
    {
      "epoch": 1.8404500474447607,
      "grad_norm": 2.7655723094940186,
      "learning_rate": 1.1632668910173442e-05,
      "loss": 1.2914,
      "step": 23760
    },
    {
      "epoch": 1.8412246557834195,
      "grad_norm": 3.5531764030456543,
      "learning_rate": 1.162490292518768e-05,
      "loss": 1.3205,
      "step": 23770
    },
    {
      "epoch": 1.8419992641220784,
      "grad_norm": 2.5383965969085693,
      "learning_rate": 1.1617136940201916e-05,
      "loss": 1.3961,
      "step": 23780
    },
    {
      "epoch": 1.8427738724607372,
      "grad_norm": 1.9016965627670288,
      "learning_rate": 1.1609370955216153e-05,
      "loss": 1.262,
      "step": 23790
    },
    {
      "epoch": 1.8435484807993958,
      "grad_norm": 1.9769102334976196,
      "learning_rate": 1.1601604970230391e-05,
      "loss": 1.2577,
      "step": 23800
    },
    {
      "epoch": 1.8443230891380544,
      "grad_norm": 2.008772134780884,
      "learning_rate": 1.159383898524463e-05,
      "loss": 1.2364,
      "step": 23810
    },
    {
      "epoch": 1.8450976974767133,
      "grad_norm": 1.8533539772033691,
      "learning_rate": 1.1586073000258865e-05,
      "loss": 1.2301,
      "step": 23820
    },
    {
      "epoch": 1.845872305815372,
      "grad_norm": 2.60196852684021,
      "learning_rate": 1.1578307015273103e-05,
      "loss": 1.298,
      "step": 23830
    },
    {
      "epoch": 1.846646914154031,
      "grad_norm": 2.2431163787841797,
      "learning_rate": 1.1570541030287341e-05,
      "loss": 1.2372,
      "step": 23840
    },
    {
      "epoch": 1.8474215224926898,
      "grad_norm": 2.4274797439575195,
      "learning_rate": 1.156277504530158e-05,
      "loss": 1.3093,
      "step": 23850
    },
    {
      "epoch": 1.8481961308313484,
      "grad_norm": 2.5969414710998535,
      "learning_rate": 1.1555009060315817e-05,
      "loss": 1.2235,
      "step": 23860
    },
    {
      "epoch": 1.8489707391700072,
      "grad_norm": 2.564206838607788,
      "learning_rate": 1.1547243075330055e-05,
      "loss": 1.2999,
      "step": 23870
    },
    {
      "epoch": 1.8497453475086658,
      "grad_norm": 2.6215484142303467,
      "learning_rate": 1.1539477090344293e-05,
      "loss": 1.2276,
      "step": 23880
    },
    {
      "epoch": 1.8505199558473246,
      "grad_norm": 2.4852402210235596,
      "learning_rate": 1.153171110535853e-05,
      "loss": 1.224,
      "step": 23890
    },
    {
      "epoch": 1.8512945641859835,
      "grad_norm": 2.9371747970581055,
      "learning_rate": 1.1523945120372768e-05,
      "loss": 1.3053,
      "step": 23900
    },
    {
      "epoch": 1.8520691725246423,
      "grad_norm": 2.020385503768921,
      "learning_rate": 1.1516179135387004e-05,
      "loss": 1.2957,
      "step": 23910
    },
    {
      "epoch": 1.8528437808633011,
      "grad_norm": 2.209184408187866,
      "learning_rate": 1.1508413150401242e-05,
      "loss": 1.3478,
      "step": 23920
    },
    {
      "epoch": 1.8536183892019598,
      "grad_norm": 2.6767756938934326,
      "learning_rate": 1.150064716541548e-05,
      "loss": 1.2216,
      "step": 23930
    },
    {
      "epoch": 1.8543929975406184,
      "grad_norm": 2.452777624130249,
      "learning_rate": 1.1492881180429718e-05,
      "loss": 1.1527,
      "step": 23940
    },
    {
      "epoch": 1.8551676058792772,
      "grad_norm": 2.7298696041107178,
      "learning_rate": 1.1485115195443956e-05,
      "loss": 1.2833,
      "step": 23950
    },
    {
      "epoch": 1.855942214217936,
      "grad_norm": 2.3918230533599854,
      "learning_rate": 1.1477349210458194e-05,
      "loss": 1.244,
      "step": 23960
    },
    {
      "epoch": 1.8567168225565949,
      "grad_norm": 2.3345720767974854,
      "learning_rate": 1.1469583225472432e-05,
      "loss": 1.2942,
      "step": 23970
    },
    {
      "epoch": 1.8574914308952537,
      "grad_norm": 2.476431369781494,
      "learning_rate": 1.146181724048667e-05,
      "loss": 1.246,
      "step": 23980
    },
    {
      "epoch": 1.8582660392339123,
      "grad_norm": 2.5068607330322266,
      "learning_rate": 1.1454051255500906e-05,
      "loss": 1.2413,
      "step": 23990
    },
    {
      "epoch": 1.8590406475725711,
      "grad_norm": 3.1903395652770996,
      "learning_rate": 1.1446285270515144e-05,
      "loss": 1.2651,
      "step": 24000
    },
    {
      "epoch": 1.8598152559112298,
      "grad_norm": 3.1786465644836426,
      "learning_rate": 1.1438519285529381e-05,
      "loss": 1.1464,
      "step": 24010
    },
    {
      "epoch": 1.8605898642498886,
      "grad_norm": 2.7688753604888916,
      "learning_rate": 1.143075330054362e-05,
      "loss": 1.3331,
      "step": 24020
    },
    {
      "epoch": 1.8613644725885474,
      "grad_norm": 2.2875001430511475,
      "learning_rate": 1.1422987315557855e-05,
      "loss": 1.2471,
      "step": 24030
    },
    {
      "epoch": 1.8621390809272063,
      "grad_norm": 3.453036308288574,
      "learning_rate": 1.1415221330572093e-05,
      "loss": 1.278,
      "step": 24040
    },
    {
      "epoch": 1.862913689265865,
      "grad_norm": 1.9554425477981567,
      "learning_rate": 1.1407455345586333e-05,
      "loss": 1.1548,
      "step": 24050
    },
    {
      "epoch": 1.8636882976045237,
      "grad_norm": 1.7765679359436035,
      "learning_rate": 1.139968936060057e-05,
      "loss": 1.2654,
      "step": 24060
    },
    {
      "epoch": 1.8644629059431823,
      "grad_norm": 2.7240967750549316,
      "learning_rate": 1.1391923375614809e-05,
      "loss": 1.3005,
      "step": 24070
    },
    {
      "epoch": 1.8652375142818411,
      "grad_norm": 2.3243515491485596,
      "learning_rate": 1.1384157390629045e-05,
      "loss": 1.1456,
      "step": 24080
    },
    {
      "epoch": 1.8660121226205,
      "grad_norm": 2.3543243408203125,
      "learning_rate": 1.1376391405643283e-05,
      "loss": 1.238,
      "step": 24090
    },
    {
      "epoch": 1.8667867309591588,
      "grad_norm": 2.5244226455688477,
      "learning_rate": 1.136862542065752e-05,
      "loss": 1.3342,
      "step": 24100
    },
    {
      "epoch": 1.8675613392978176,
      "grad_norm": 2.3371715545654297,
      "learning_rate": 1.1360859435671758e-05,
      "loss": 1.3566,
      "step": 24110
    },
    {
      "epoch": 1.8683359476364763,
      "grad_norm": 2.2188076972961426,
      "learning_rate": 1.1353093450685995e-05,
      "loss": 1.2793,
      "step": 24120
    },
    {
      "epoch": 1.869110555975135,
      "grad_norm": 2.510925769805908,
      "learning_rate": 1.1345327465700232e-05,
      "loss": 1.2167,
      "step": 24130
    },
    {
      "epoch": 1.8698851643137937,
      "grad_norm": 2.0374560356140137,
      "learning_rate": 1.1337561480714472e-05,
      "loss": 1.2528,
      "step": 24140
    },
    {
      "epoch": 1.8706597726524525,
      "grad_norm": 2.775291919708252,
      "learning_rate": 1.132979549572871e-05,
      "loss": 1.4337,
      "step": 24150
    },
    {
      "epoch": 1.8714343809911114,
      "grad_norm": 2.1253774166107178,
      "learning_rate": 1.1322029510742946e-05,
      "loss": 1.2615,
      "step": 24160
    },
    {
      "epoch": 1.8722089893297702,
      "grad_norm": 3.23126220703125,
      "learning_rate": 1.1314263525757184e-05,
      "loss": 1.1552,
      "step": 24170
    },
    {
      "epoch": 1.872983597668429,
      "grad_norm": 2.722601890563965,
      "learning_rate": 1.1306497540771422e-05,
      "loss": 1.2407,
      "step": 24180
    },
    {
      "epoch": 1.8737582060070876,
      "grad_norm": 2.2088751792907715,
      "learning_rate": 1.129873155578566e-05,
      "loss": 1.2637,
      "step": 24190
    },
    {
      "epoch": 1.8745328143457465,
      "grad_norm": 2.4659743309020996,
      "learning_rate": 1.1290965570799896e-05,
      "loss": 1.2315,
      "step": 24200
    },
    {
      "epoch": 1.875307422684405,
      "grad_norm": 2.7909700870513916,
      "learning_rate": 1.1283199585814134e-05,
      "loss": 1.2834,
      "step": 24210
    },
    {
      "epoch": 1.876082031023064,
      "grad_norm": 2.5495247840881348,
      "learning_rate": 1.1275433600828372e-05,
      "loss": 1.3073,
      "step": 24220
    },
    {
      "epoch": 1.8768566393617228,
      "grad_norm": 3.1400997638702393,
      "learning_rate": 1.126766761584261e-05,
      "loss": 1.2548,
      "step": 24230
    },
    {
      "epoch": 1.8776312477003816,
      "grad_norm": 1.9507830142974854,
      "learning_rate": 1.1259901630856847e-05,
      "loss": 1.2623,
      "step": 24240
    },
    {
      "epoch": 1.8784058560390404,
      "grad_norm": 2.336336374282837,
      "learning_rate": 1.1252135645871085e-05,
      "loss": 1.2071,
      "step": 24250
    },
    {
      "epoch": 1.879180464377699,
      "grad_norm": 1.9881703853607178,
      "learning_rate": 1.1244369660885323e-05,
      "loss": 1.2824,
      "step": 24260
    },
    {
      "epoch": 1.8799550727163576,
      "grad_norm": 2.552126169204712,
      "learning_rate": 1.1236603675899561e-05,
      "loss": 1.1365,
      "step": 24270
    },
    {
      "epoch": 1.8807296810550165,
      "grad_norm": 3.23691987991333,
      "learning_rate": 1.1228837690913799e-05,
      "loss": 1.3201,
      "step": 24280
    },
    {
      "epoch": 1.8815042893936753,
      "grad_norm": 2.375842809677124,
      "learning_rate": 1.1221071705928035e-05,
      "loss": 1.2111,
      "step": 24290
    },
    {
      "epoch": 1.8822788977323341,
      "grad_norm": 2.466597080230713,
      "learning_rate": 1.1213305720942273e-05,
      "loss": 1.3391,
      "step": 24300
    },
    {
      "epoch": 1.883053506070993,
      "grad_norm": 2.226529836654663,
      "learning_rate": 1.120553973595651e-05,
      "loss": 1.1819,
      "step": 24310
    },
    {
      "epoch": 1.8838281144096516,
      "grad_norm": 2.1928694248199463,
      "learning_rate": 1.1197773750970748e-05,
      "loss": 1.1857,
      "step": 24320
    },
    {
      "epoch": 1.8846027227483104,
      "grad_norm": 3.033566474914551,
      "learning_rate": 1.1190007765984985e-05,
      "loss": 1.1248,
      "step": 24330
    },
    {
      "epoch": 1.885377331086969,
      "grad_norm": 3.1496174335479736,
      "learning_rate": 1.1182241780999224e-05,
      "loss": 1.2735,
      "step": 24340
    },
    {
      "epoch": 1.8861519394256279,
      "grad_norm": 2.9568254947662354,
      "learning_rate": 1.1174475796013462e-05,
      "loss": 1.3138,
      "step": 24350
    },
    {
      "epoch": 1.8869265477642867,
      "grad_norm": 4.1579999923706055,
      "learning_rate": 1.11667098110277e-05,
      "loss": 1.2009,
      "step": 24360
    },
    {
      "epoch": 1.8877011561029455,
      "grad_norm": 2.573805570602417,
      "learning_rate": 1.1158943826041936e-05,
      "loss": 1.2849,
      "step": 24370
    },
    {
      "epoch": 1.8884757644416044,
      "grad_norm": 2.047104597091675,
      "learning_rate": 1.1151177841056174e-05,
      "loss": 1.2874,
      "step": 24380
    },
    {
      "epoch": 1.889250372780263,
      "grad_norm": 2.3775365352630615,
      "learning_rate": 1.1143411856070412e-05,
      "loss": 1.3809,
      "step": 24390
    },
    {
      "epoch": 1.8900249811189216,
      "grad_norm": 2.4857125282287598,
      "learning_rate": 1.113564587108465e-05,
      "loss": 1.1909,
      "step": 24400
    },
    {
      "epoch": 1.8907995894575804,
      "grad_norm": 2.1692922115325928,
      "learning_rate": 1.1127879886098886e-05,
      "loss": 1.2723,
      "step": 24410
    },
    {
      "epoch": 1.8915741977962393,
      "grad_norm": 2.2672080993652344,
      "learning_rate": 1.1120113901113124e-05,
      "loss": 1.2357,
      "step": 24420
    },
    {
      "epoch": 1.892348806134898,
      "grad_norm": 2.730814218521118,
      "learning_rate": 1.1112347916127362e-05,
      "loss": 1.1596,
      "step": 24430
    },
    {
      "epoch": 1.893123414473557,
      "grad_norm": 3.0110769271850586,
      "learning_rate": 1.1104581931141601e-05,
      "loss": 1.2508,
      "step": 24440
    },
    {
      "epoch": 1.8938980228122155,
      "grad_norm": 3.4934473037719727,
      "learning_rate": 1.1096815946155839e-05,
      "loss": 1.1218,
      "step": 24450
    },
    {
      "epoch": 1.8946726311508744,
      "grad_norm": 3.0638933181762695,
      "learning_rate": 1.1089049961170075e-05,
      "loss": 1.2369,
      "step": 24460
    },
    {
      "epoch": 1.895447239489533,
      "grad_norm": 2.7754569053649902,
      "learning_rate": 1.1081283976184313e-05,
      "loss": 1.2698,
      "step": 24470
    },
    {
      "epoch": 1.8962218478281918,
      "grad_norm": 2.569702386856079,
      "learning_rate": 1.1073517991198551e-05,
      "loss": 1.3332,
      "step": 24480
    },
    {
      "epoch": 1.8969964561668506,
      "grad_norm": 2.3579530715942383,
      "learning_rate": 1.1065752006212789e-05,
      "loss": 1.2405,
      "step": 24490
    },
    {
      "epoch": 1.8977710645055095,
      "grad_norm": 3.1411335468292236,
      "learning_rate": 1.1057986021227025e-05,
      "loss": 1.2916,
      "step": 24500
    },
    {
      "epoch": 1.8985456728441683,
      "grad_norm": 1.9782187938690186,
      "learning_rate": 1.1050220036241263e-05,
      "loss": 1.2074,
      "step": 24510
    },
    {
      "epoch": 1.899320281182827,
      "grad_norm": 2.9982078075408936,
      "learning_rate": 1.10424540512555e-05,
      "loss": 1.3166,
      "step": 24520
    },
    {
      "epoch": 1.9000948895214858,
      "grad_norm": 2.665461540222168,
      "learning_rate": 1.1034688066269739e-05,
      "loss": 1.1394,
      "step": 24530
    },
    {
      "epoch": 1.9008694978601444,
      "grad_norm": 2.1815476417541504,
      "learning_rate": 1.1026922081283976e-05,
      "loss": 1.2352,
      "step": 24540
    },
    {
      "epoch": 1.9016441061988032,
      "grad_norm": 2.4372925758361816,
      "learning_rate": 1.1019156096298214e-05,
      "loss": 1.3008,
      "step": 24550
    },
    {
      "epoch": 1.902418714537462,
      "grad_norm": 2.031564235687256,
      "learning_rate": 1.1011390111312452e-05,
      "loss": 1.1899,
      "step": 24560
    },
    {
      "epoch": 1.9031933228761209,
      "grad_norm": 2.3119595050811768,
      "learning_rate": 1.100362412632669e-05,
      "loss": 1.3389,
      "step": 24570
    },
    {
      "epoch": 1.9039679312147795,
      "grad_norm": 3.059014320373535,
      "learning_rate": 1.0995858141340926e-05,
      "loss": 1.2062,
      "step": 24580
    },
    {
      "epoch": 1.9047425395534383,
      "grad_norm": 2.408726930618286,
      "learning_rate": 1.0988092156355164e-05,
      "loss": 1.1965,
      "step": 24590
    },
    {
      "epoch": 1.905517147892097,
      "grad_norm": 2.3758745193481445,
      "learning_rate": 1.0980326171369402e-05,
      "loss": 1.2577,
      "step": 24600
    },
    {
      "epoch": 1.9062917562307558,
      "grad_norm": 4.509647369384766,
      "learning_rate": 1.097256018638364e-05,
      "loss": 1.1881,
      "step": 24610
    },
    {
      "epoch": 1.9070663645694146,
      "grad_norm": 1.9770203828811646,
      "learning_rate": 1.0964794201397878e-05,
      "loss": 1.2142,
      "step": 24620
    },
    {
      "epoch": 1.9078409729080734,
      "grad_norm": 2.192415475845337,
      "learning_rate": 1.0957028216412114e-05,
      "loss": 1.2671,
      "step": 24630
    },
    {
      "epoch": 1.9086155812467323,
      "grad_norm": 2.6302690505981445,
      "learning_rate": 1.0949262231426353e-05,
      "loss": 1.3138,
      "step": 24640
    },
    {
      "epoch": 1.9093901895853909,
      "grad_norm": 2.8228108882904053,
      "learning_rate": 1.0941496246440591e-05,
      "loss": 1.2077,
      "step": 24650
    },
    {
      "epoch": 1.9101647979240497,
      "grad_norm": 3.0129952430725098,
      "learning_rate": 1.093373026145483e-05,
      "loss": 1.1869,
      "step": 24660
    },
    {
      "epoch": 1.9109394062627083,
      "grad_norm": 2.93556809425354,
      "learning_rate": 1.0925964276469065e-05,
      "loss": 1.2916,
      "step": 24670
    },
    {
      "epoch": 1.9117140146013671,
      "grad_norm": 2.4520411491394043,
      "learning_rate": 1.0918198291483303e-05,
      "loss": 1.255,
      "step": 24680
    },
    {
      "epoch": 1.912488622940026,
      "grad_norm": 2.5056393146514893,
      "learning_rate": 1.0910432306497541e-05,
      "loss": 1.4038,
      "step": 24690
    },
    {
      "epoch": 1.9132632312786848,
      "grad_norm": 2.8798775672912598,
      "learning_rate": 1.0902666321511779e-05,
      "loss": 1.3328,
      "step": 24700
    },
    {
      "epoch": 1.9140378396173436,
      "grad_norm": 2.5088953971862793,
      "learning_rate": 1.0894900336526015e-05,
      "loss": 1.243,
      "step": 24710
    },
    {
      "epoch": 1.9148124479560023,
      "grad_norm": 2.9202921390533447,
      "learning_rate": 1.0887134351540253e-05,
      "loss": 1.2865,
      "step": 24720
    },
    {
      "epoch": 1.9155870562946609,
      "grad_norm": 2.2895960807800293,
      "learning_rate": 1.0879368366554493e-05,
      "loss": 1.1894,
      "step": 24730
    },
    {
      "epoch": 1.9163616646333197,
      "grad_norm": 2.5908567905426025,
      "learning_rate": 1.087160238156873e-05,
      "loss": 1.219,
      "step": 24740
    },
    {
      "epoch": 1.9171362729719785,
      "grad_norm": 2.216141700744629,
      "learning_rate": 1.0863836396582967e-05,
      "loss": 1.2635,
      "step": 24750
    },
    {
      "epoch": 1.9179108813106374,
      "grad_norm": 3.4387669563293457,
      "learning_rate": 1.0856070411597204e-05,
      "loss": 1.3397,
      "step": 24760
    },
    {
      "epoch": 1.9186854896492962,
      "grad_norm": 2.3323566913604736,
      "learning_rate": 1.0848304426611442e-05,
      "loss": 1.1313,
      "step": 24770
    },
    {
      "epoch": 1.9194600979879548,
      "grad_norm": 2.274484157562256,
      "learning_rate": 1.084053844162568e-05,
      "loss": 1.2358,
      "step": 24780
    },
    {
      "epoch": 1.9202347063266136,
      "grad_norm": 2.608544111251831,
      "learning_rate": 1.0832772456639918e-05,
      "loss": 1.3016,
      "step": 24790
    },
    {
      "epoch": 1.9210093146652722,
      "grad_norm": 2.6308610439300537,
      "learning_rate": 1.0825006471654154e-05,
      "loss": 1.212,
      "step": 24800
    },
    {
      "epoch": 1.921783923003931,
      "grad_norm": 2.5147595405578613,
      "learning_rate": 1.0817240486668392e-05,
      "loss": 1.2277,
      "step": 24810
    },
    {
      "epoch": 1.92255853134259,
      "grad_norm": 2.7557408809661865,
      "learning_rate": 1.080947450168263e-05,
      "loss": 1.313,
      "step": 24820
    },
    {
      "epoch": 1.9233331396812487,
      "grad_norm": 2.938532829284668,
      "learning_rate": 1.080170851669687e-05,
      "loss": 1.2213,
      "step": 24830
    },
    {
      "epoch": 1.9241077480199076,
      "grad_norm": 3.0682742595672607,
      "learning_rate": 1.0793942531711106e-05,
      "loss": 1.3696,
      "step": 24840
    },
    {
      "epoch": 1.9248823563585662,
      "grad_norm": 2.5136539936065674,
      "learning_rate": 1.0786176546725344e-05,
      "loss": 1.2145,
      "step": 24850
    },
    {
      "epoch": 1.9256569646972248,
      "grad_norm": 2.0683958530426025,
      "learning_rate": 1.0778410561739581e-05,
      "loss": 1.1958,
      "step": 24860
    },
    {
      "epoch": 1.9264315730358836,
      "grad_norm": NaN,
      "learning_rate": 1.077064457675382e-05,
      "loss": 1.3052,
      "step": 24870
    },
    {
      "epoch": 1.9272061813745425,
      "grad_norm": 1.7065355777740479,
      "learning_rate": 1.0763655190266633e-05,
      "loss": 1.2584,
      "step": 24880
    },
    {
      "epoch": 1.9279807897132013,
      "grad_norm": 2.180952787399292,
      "learning_rate": 1.075588920528087e-05,
      "loss": 1.4124,
      "step": 24890
    },
    {
      "epoch": 1.9287553980518601,
      "grad_norm": 2.4559600353240967,
      "learning_rate": 1.0748123220295108e-05,
      "loss": 1.2588,
      "step": 24900
    },
    {
      "epoch": 1.9295300063905187,
      "grad_norm": 2.6538426876068115,
      "learning_rate": 1.0740357235309346e-05,
      "loss": 1.2296,
      "step": 24910
    },
    {
      "epoch": 1.9303046147291776,
      "grad_norm": 3.154895305633545,
      "learning_rate": 1.0732591250323582e-05,
      "loss": 1.2918,
      "step": 24920
    },
    {
      "epoch": 1.9310792230678362,
      "grad_norm": 2.718129873275757,
      "learning_rate": 1.072482526533782e-05,
      "loss": 1.2931,
      "step": 24930
    },
    {
      "epoch": 1.931853831406495,
      "grad_norm": 2.077004909515381,
      "learning_rate": 1.0717059280352058e-05,
      "loss": 1.2246,
      "step": 24940
    },
    {
      "epoch": 1.9326284397451539,
      "grad_norm": 3.2282373905181885,
      "learning_rate": 1.0709293295366296e-05,
      "loss": 1.1977,
      "step": 24950
    },
    {
      "epoch": 1.9334030480838127,
      "grad_norm": 2.9608280658721924,
      "learning_rate": 1.0701527310380532e-05,
      "loss": 1.2487,
      "step": 24960
    },
    {
      "epoch": 1.9341776564224715,
      "grad_norm": 2.916107177734375,
      "learning_rate": 1.0693761325394772e-05,
      "loss": 1.1567,
      "step": 24970
    },
    {
      "epoch": 1.9349522647611301,
      "grad_norm": 2.957993507385254,
      "learning_rate": 1.068599534040901e-05,
      "loss": 1.2348,
      "step": 24980
    },
    {
      "epoch": 1.935726873099789,
      "grad_norm": 2.9400720596313477,
      "learning_rate": 1.0678229355423248e-05,
      "loss": 1.2328,
      "step": 24990
    },
    {
      "epoch": 1.9365014814384476,
      "grad_norm": 2.272174596786499,
      "learning_rate": 1.0670463370437484e-05,
      "loss": 1.2794,
      "step": 25000
    },
    {
      "epoch": 1.9372760897771064,
      "grad_norm": 2.683095693588257,
      "learning_rate": 1.0662697385451722e-05,
      "loss": 1.2575,
      "step": 25010
    },
    {
      "epoch": 1.9380506981157652,
      "grad_norm": 1.803268551826477,
      "learning_rate": 1.065493140046596e-05,
      "loss": 1.2239,
      "step": 25020
    },
    {
      "epoch": 1.938825306454424,
      "grad_norm": 2.4070873260498047,
      "learning_rate": 1.0647165415480197e-05,
      "loss": 1.1265,
      "step": 25030
    },
    {
      "epoch": 1.939599914793083,
      "grad_norm": 2.7771880626678467,
      "learning_rate": 1.0639399430494433e-05,
      "loss": 1.2422,
      "step": 25040
    },
    {
      "epoch": 1.9403745231317415,
      "grad_norm": 3.877140522003174,
      "learning_rate": 1.0631633445508671e-05,
      "loss": 1.3845,
      "step": 25050
    },
    {
      "epoch": 1.9411491314704001,
      "grad_norm": 2.465053081512451,
      "learning_rate": 1.062386746052291e-05,
      "loss": 1.2445,
      "step": 25060
    },
    {
      "epoch": 1.941923739809059,
      "grad_norm": 1.9393728971481323,
      "learning_rate": 1.0616101475537149e-05,
      "loss": 1.3317,
      "step": 25070
    },
    {
      "epoch": 1.9426983481477178,
      "grad_norm": 2.9806160926818848,
      "learning_rate": 1.0608335490551385e-05,
      "loss": 1.2801,
      "step": 25080
    },
    {
      "epoch": 1.9434729564863766,
      "grad_norm": 2.2260794639587402,
      "learning_rate": 1.0600569505565623e-05,
      "loss": 1.1987,
      "step": 25090
    },
    {
      "epoch": 1.9442475648250355,
      "grad_norm": 3.4691669940948486,
      "learning_rate": 1.059280352057986e-05,
      "loss": 1.232,
      "step": 25100
    },
    {
      "epoch": 1.945022173163694,
      "grad_norm": 2.059152603149414,
      "learning_rate": 1.0585037535594099e-05,
      "loss": 1.2685,
      "step": 25110
    },
    {
      "epoch": 1.945796781502353,
      "grad_norm": 2.0986168384552,
      "learning_rate": 1.0577271550608336e-05,
      "loss": 1.2134,
      "step": 25120
    },
    {
      "epoch": 1.9465713898410115,
      "grad_norm": 2.844633102416992,
      "learning_rate": 1.0569505565622573e-05,
      "loss": 1.2506,
      "step": 25130
    },
    {
      "epoch": 1.9473459981796704,
      "grad_norm": 2.3969709873199463,
      "learning_rate": 1.056173958063681e-05,
      "loss": 1.2674,
      "step": 25140
    },
    {
      "epoch": 1.9481206065183292,
      "grad_norm": 2.188325881958008,
      "learning_rate": 1.0553973595651048e-05,
      "loss": 1.1503,
      "step": 25150
    },
    {
      "epoch": 1.948895214856988,
      "grad_norm": 3.0614795684814453,
      "learning_rate": 1.0546207610665286e-05,
      "loss": 1.2588,
      "step": 25160
    },
    {
      "epoch": 1.9496698231956469,
      "grad_norm": 2.3458759784698486,
      "learning_rate": 1.0538441625679524e-05,
      "loss": 1.2214,
      "step": 25170
    },
    {
      "epoch": 1.9504444315343055,
      "grad_norm": 2.950580358505249,
      "learning_rate": 1.0530675640693762e-05,
      "loss": 1.2739,
      "step": 25180
    },
    {
      "epoch": 1.951219039872964,
      "grad_norm": 3.543242931365967,
      "learning_rate": 1.0522909655708e-05,
      "loss": 1.2955,
      "step": 25190
    },
    {
      "epoch": 1.951993648211623,
      "grad_norm": 4.469767093658447,
      "learning_rate": 1.0515143670722238e-05,
      "loss": 1.3068,
      "step": 25200
    },
    {
      "epoch": 1.9527682565502817,
      "grad_norm": 2.0289549827575684,
      "learning_rate": 1.0507377685736474e-05,
      "loss": 1.2373,
      "step": 25210
    },
    {
      "epoch": 1.9535428648889406,
      "grad_norm": 2.9921483993530273,
      "learning_rate": 1.0499611700750712e-05,
      "loss": 1.2251,
      "step": 25220
    },
    {
      "epoch": 1.9543174732275994,
      "grad_norm": 2.7276511192321777,
      "learning_rate": 1.049184571576495e-05,
      "loss": 1.2582,
      "step": 25230
    },
    {
      "epoch": 1.955092081566258,
      "grad_norm": 2.533125400543213,
      "learning_rate": 1.0484079730779187e-05,
      "loss": 1.3214,
      "step": 25240
    },
    {
      "epoch": 1.9558666899049169,
      "grad_norm": 2.0149481296539307,
      "learning_rate": 1.0476313745793424e-05,
      "loss": 1.3144,
      "step": 25250
    },
    {
      "epoch": 1.9566412982435755,
      "grad_norm": 2.9578030109405518,
      "learning_rate": 1.0468547760807663e-05,
      "loss": 1.2207,
      "step": 25260
    },
    {
      "epoch": 1.9574159065822343,
      "grad_norm": 2.3871779441833496,
      "learning_rate": 1.0460781775821901e-05,
      "loss": 1.3362,
      "step": 25270
    },
    {
      "epoch": 1.9581905149208931,
      "grad_norm": 2.361558437347412,
      "learning_rate": 1.0453015790836139e-05,
      "loss": 1.269,
      "step": 25280
    },
    {
      "epoch": 1.958965123259552,
      "grad_norm": 2.3760125637054443,
      "learning_rate": 1.0445249805850377e-05,
      "loss": 1.201,
      "step": 25290
    },
    {
      "epoch": 1.9597397315982108,
      "grad_norm": 2.6007726192474365,
      "learning_rate": 1.0437483820864613e-05,
      "loss": 1.2755,
      "step": 25300
    },
    {
      "epoch": 1.9605143399368694,
      "grad_norm": 2.086120843887329,
      "learning_rate": 1.042971783587885e-05,
      "loss": 1.3828,
      "step": 25310
    },
    {
      "epoch": 1.961288948275528,
      "grad_norm": 2.9747400283813477,
      "learning_rate": 1.0421951850893089e-05,
      "loss": 1.2827,
      "step": 25320
    },
    {
      "epoch": 1.9620635566141869,
      "grad_norm": 2.2873525619506836,
      "learning_rate": 1.0414185865907326e-05,
      "loss": 1.3613,
      "step": 25330
    },
    {
      "epoch": 1.9628381649528457,
      "grad_norm": 2.858278751373291,
      "learning_rate": 1.0406419880921563e-05,
      "loss": 1.2273,
      "step": 25340
    },
    {
      "epoch": 1.9636127732915045,
      "grad_norm": 2.4665565490722656,
      "learning_rate": 1.03986538959358e-05,
      "loss": 1.2254,
      "step": 25350
    },
    {
      "epoch": 1.9643873816301634,
      "grad_norm": 2.1108195781707764,
      "learning_rate": 1.039088791095004e-05,
      "loss": 1.2184,
      "step": 25360
    },
    {
      "epoch": 1.965161989968822,
      "grad_norm": 2.7585906982421875,
      "learning_rate": 1.0383121925964278e-05,
      "loss": 1.3503,
      "step": 25370
    },
    {
      "epoch": 1.9659365983074808,
      "grad_norm": 2.893890619277954,
      "learning_rate": 1.0375355940978514e-05,
      "loss": 1.2406,
      "step": 25380
    },
    {
      "epoch": 1.9667112066461394,
      "grad_norm": 2.8641912937164307,
      "learning_rate": 1.0367589955992752e-05,
      "loss": 1.2417,
      "step": 25390
    },
    {
      "epoch": 1.9674858149847982,
      "grad_norm": 2.5936901569366455,
      "learning_rate": 1.035982397100699e-05,
      "loss": 1.1729,
      "step": 25400
    },
    {
      "epoch": 1.968260423323457,
      "grad_norm": 2.535731077194214,
      "learning_rate": 1.0352057986021228e-05,
      "loss": 1.3202,
      "step": 25410
    },
    {
      "epoch": 1.969035031662116,
      "grad_norm": 2.355083465576172,
      "learning_rate": 1.0344292001035464e-05,
      "loss": 1.2939,
      "step": 25420
    },
    {
      "epoch": 1.9698096400007747,
      "grad_norm": 2.9706742763519287,
      "learning_rate": 1.0336526016049702e-05,
      "loss": 1.2229,
      "step": 25430
    },
    {
      "epoch": 1.9705842483394334,
      "grad_norm": 2.204035520553589,
      "learning_rate": 1.032876003106394e-05,
      "loss": 1.3221,
      "step": 25440
    },
    {
      "epoch": 1.9713588566780922,
      "grad_norm": 2.5506606101989746,
      "learning_rate": 1.0320994046078177e-05,
      "loss": 1.2215,
      "step": 25450
    },
    {
      "epoch": 1.9721334650167508,
      "grad_norm": 3.165731191635132,
      "learning_rate": 1.0313228061092417e-05,
      "loss": 1.2588,
      "step": 25460
    },
    {
      "epoch": 1.9729080733554096,
      "grad_norm": 1.9934927225112915,
      "learning_rate": 1.0305462076106653e-05,
      "loss": 1.2946,
      "step": 25470
    },
    {
      "epoch": 1.9736826816940685,
      "grad_norm": 2.2187623977661133,
      "learning_rate": 1.0297696091120891e-05,
      "loss": 1.2434,
      "step": 25480
    },
    {
      "epoch": 1.9744572900327273,
      "grad_norm": 2.2167186737060547,
      "learning_rate": 1.0289930106135129e-05,
      "loss": 1.2577,
      "step": 25490
    },
    {
      "epoch": 1.9752318983713861,
      "grad_norm": 2.2904083728790283,
      "learning_rate": 1.0282164121149367e-05,
      "loss": 1.2278,
      "step": 25500
    },
    {
      "epoch": 1.9760065067100447,
      "grad_norm": 2.516555070877075,
      "learning_rate": 1.0274398136163603e-05,
      "loss": 1.2528,
      "step": 25510
    },
    {
      "epoch": 1.9767811150487034,
      "grad_norm": 2.1957039833068848,
      "learning_rate": 1.0266632151177841e-05,
      "loss": 1.2295,
      "step": 25520
    },
    {
      "epoch": 1.9775557233873622,
      "grad_norm": 2.9691002368927,
      "learning_rate": 1.0258866166192079e-05,
      "loss": 1.3126,
      "step": 25530
    },
    {
      "epoch": 1.978330331726021,
      "grad_norm": 2.0600717067718506,
      "learning_rate": 1.0251100181206317e-05,
      "loss": 1.2011,
      "step": 25540
    },
    {
      "epoch": 1.9791049400646799,
      "grad_norm": 1.8869884014129639,
      "learning_rate": 1.0243334196220553e-05,
      "loss": 1.2715,
      "step": 25550
    },
    {
      "epoch": 1.9798795484033387,
      "grad_norm": 2.2864928245544434,
      "learning_rate": 1.0235568211234792e-05,
      "loss": 1.1715,
      "step": 25560
    },
    {
      "epoch": 1.9806541567419973,
      "grad_norm": 3.5208053588867188,
      "learning_rate": 1.022780222624903e-05,
      "loss": 1.3005,
      "step": 25570
    },
    {
      "epoch": 1.9814287650806561,
      "grad_norm": 2.220987319946289,
      "learning_rate": 1.0220036241263268e-05,
      "loss": 1.3538,
      "step": 25580
    },
    {
      "epoch": 1.9822033734193147,
      "grad_norm": 2.139713764190674,
      "learning_rate": 1.0212270256277504e-05,
      "loss": 1.2354,
      "step": 25590
    },
    {
      "epoch": 1.9829779817579736,
      "grad_norm": 2.2710094451904297,
      "learning_rate": 1.0204504271291742e-05,
      "loss": 1.2528,
      "step": 25600
    },
    {
      "epoch": 1.9837525900966324,
      "grad_norm": 2.6679270267486572,
      "learning_rate": 1.019673828630598e-05,
      "loss": 1.1856,
      "step": 25610
    },
    {
      "epoch": 1.9845271984352912,
      "grad_norm": 2.8763790130615234,
      "learning_rate": 1.0188972301320218e-05,
      "loss": 1.1726,
      "step": 25620
    },
    {
      "epoch": 1.98530180677395,
      "grad_norm": 2.015829563140869,
      "learning_rate": 1.0181206316334456e-05,
      "loss": 1.2616,
      "step": 25630
    },
    {
      "epoch": 1.9860764151126087,
      "grad_norm": 1.922398328781128,
      "learning_rate": 1.0173440331348692e-05,
      "loss": 1.2454,
      "step": 25640
    },
    {
      "epoch": 1.9868510234512673,
      "grad_norm": 3.103055000305176,
      "learning_rate": 1.016567434636293e-05,
      "loss": 1.3079,
      "step": 25650
    },
    {
      "epoch": 1.9876256317899261,
      "grad_norm": 2.514613389968872,
      "learning_rate": 1.015790836137717e-05,
      "loss": 1.2452,
      "step": 25660
    },
    {
      "epoch": 1.988400240128585,
      "grad_norm": 2.8849499225616455,
      "learning_rate": 1.0150142376391407e-05,
      "loss": 1.2878,
      "step": 25670
    },
    {
      "epoch": 1.9891748484672438,
      "grad_norm": 2.4653496742248535,
      "learning_rate": 1.0142376391405643e-05,
      "loss": 1.2122,
      "step": 25680
    },
    {
      "epoch": 1.9899494568059026,
      "grad_norm": 2.644789457321167,
      "learning_rate": 1.0134610406419881e-05,
      "loss": 1.1812,
      "step": 25690
    },
    {
      "epoch": 1.9907240651445612,
      "grad_norm": 2.3917315006256104,
      "learning_rate": 1.0126844421434119e-05,
      "loss": 1.2572,
      "step": 25700
    },
    {
      "epoch": 1.99149867348322,
      "grad_norm": 2.5022263526916504,
      "learning_rate": 1.0119078436448357e-05,
      "loss": 1.2526,
      "step": 25710
    },
    {
      "epoch": 1.9922732818218787,
      "grad_norm": 2.2566566467285156,
      "learning_rate": 1.0111312451462593e-05,
      "loss": 1.2494,
      "step": 25720
    },
    {
      "epoch": 1.9930478901605375,
      "grad_norm": 3.109952688217163,
      "learning_rate": 1.0103546466476831e-05,
      "loss": 1.4199,
      "step": 25730
    },
    {
      "epoch": 1.9938224984991963,
      "grad_norm": 2.4537174701690674,
      "learning_rate": 1.0095780481491069e-05,
      "loss": 1.3407,
      "step": 25740
    },
    {
      "epoch": 1.9945971068378552,
      "grad_norm": 2.3592991828918457,
      "learning_rate": 1.0088014496505308e-05,
      "loss": 1.196,
      "step": 25750
    },
    {
      "epoch": 1.995371715176514,
      "grad_norm": 2.374159812927246,
      "learning_rate": 1.0080248511519545e-05,
      "loss": 1.3842,
      "step": 25760
    },
    {
      "epoch": 1.9961463235151726,
      "grad_norm": 2.733914852142334,
      "learning_rate": 1.0072482526533782e-05,
      "loss": 1.2977,
      "step": 25770
    },
    {
      "epoch": 1.9969209318538315,
      "grad_norm": 2.618896484375,
      "learning_rate": 1.006471654154802e-05,
      "loss": 1.1354,
      "step": 25780
    },
    {
      "epoch": 1.99769554019249,
      "grad_norm": 2.652722120285034,
      "learning_rate": 1.0056950556562258e-05,
      "loss": 1.2759,
      "step": 25790
    },
    {
      "epoch": 1.998470148531149,
      "grad_norm": 2.3816847801208496,
      "learning_rate": 1.0049184571576494e-05,
      "loss": 1.3512,
      "step": 25800
    },
    {
      "epoch": 1.9992447568698077,
      "grad_norm": 2.5560011863708496,
      "learning_rate": 1.0041418586590732e-05,
      "loss": 1.2506,
      "step": 25810
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.633446455001831,
      "learning_rate": 1.003365260160497e-05,
      "loss": 1.2709,
      "step": 25820
    },
    {
      "epoch": 2.000774608338659,
      "grad_norm": 3.216104745864868,
      "learning_rate": 1.0025886616619208e-05,
      "loss": 1.3149,
      "step": 25830
    },
    {
      "epoch": 2.0015492166773177,
      "grad_norm": 2.366720676422119,
      "learning_rate": 1.0018120631633446e-05,
      "loss": 1.2969,
      "step": 25840
    },
    {
      "epoch": 2.0023238250159765,
      "grad_norm": 3.219010353088379,
      "learning_rate": 1.0010354646647684e-05,
      "loss": 1.0564,
      "step": 25850
    },
    {
      "epoch": 2.003098433354635,
      "grad_norm": 2.3416154384613037,
      "learning_rate": 1.0002588661661922e-05,
      "loss": 1.1752,
      "step": 25860
    },
    {
      "epoch": 2.0038730416932937,
      "grad_norm": 3.1829628944396973,
      "learning_rate": 9.99482267667616e-06,
      "loss": 1.2418,
      "step": 25870
    },
    {
      "epoch": 2.0046476500319526,
      "grad_norm": 2.506701946258545,
      "learning_rate": 9.987056691690397e-06,
      "loss": 1.3148,
      "step": 25880
    },
    {
      "epoch": 2.0054222583706114,
      "grad_norm": 2.0396604537963867,
      "learning_rate": 9.979290706704633e-06,
      "loss": 1.3588,
      "step": 25890
    },
    {
      "epoch": 2.00619686670927,
      "grad_norm": 2.0830647945404053,
      "learning_rate": 9.971524721718871e-06,
      "loss": 1.2765,
      "step": 25900
    },
    {
      "epoch": 2.006971475047929,
      "grad_norm": 2.540665626525879,
      "learning_rate": 9.96375873673311e-06,
      "loss": 1.3305,
      "step": 25910
    },
    {
      "epoch": 2.0077460833865874,
      "grad_norm": 2.609175682067871,
      "learning_rate": 9.955992751747347e-06,
      "loss": 1.2569,
      "step": 25920
    },
    {
      "epoch": 2.0085206917252463,
      "grad_norm": 2.973907709121704,
      "learning_rate": 9.948226766761583e-06,
      "loss": 1.228,
      "step": 25930
    },
    {
      "epoch": 2.009295300063905,
      "grad_norm": 1.898255467414856,
      "learning_rate": 9.940460781775821e-06,
      "loss": 1.2991,
      "step": 25940
    },
    {
      "epoch": 2.010069908402564,
      "grad_norm": 2.3787269592285156,
      "learning_rate": 9.93269479679006e-06,
      "loss": 1.1844,
      "step": 25950
    },
    {
      "epoch": 2.0108445167412228,
      "grad_norm": 2.2194442749023438,
      "learning_rate": 9.924928811804298e-06,
      "loss": 1.2596,
      "step": 25960
    },
    {
      "epoch": 2.0116191250798816,
      "grad_norm": 2.3716542720794678,
      "learning_rate": 9.917162826818535e-06,
      "loss": 1.2368,
      "step": 25970
    },
    {
      "epoch": 2.0123937334185404,
      "grad_norm": 2.359546422958374,
      "learning_rate": 9.909396841832773e-06,
      "loss": 1.2378,
      "step": 25980
    },
    {
      "epoch": 2.013168341757199,
      "grad_norm": 2.3983044624328613,
      "learning_rate": 9.90163085684701e-06,
      "loss": 1.2022,
      "step": 25990
    },
    {
      "epoch": 2.0139429500958577,
      "grad_norm": 2.075277328491211,
      "learning_rate": 9.893864871861248e-06,
      "loss": 1.3198,
      "step": 26000
    },
    {
      "epoch": 2.0147175584345165,
      "grad_norm": 2.751728057861328,
      "learning_rate": 9.886098886875486e-06,
      "loss": 1.232,
      "step": 26010
    },
    {
      "epoch": 2.0154921667731753,
      "grad_norm": 2.3647046089172363,
      "learning_rate": 9.878332901889722e-06,
      "loss": 1.2842,
      "step": 26020
    },
    {
      "epoch": 2.016266775111834,
      "grad_norm": 2.8863894939422607,
      "learning_rate": 9.87056691690396e-06,
      "loss": 1.1774,
      "step": 26030
    },
    {
      "epoch": 2.017041383450493,
      "grad_norm": 1.873661756515503,
      "learning_rate": 9.862800931918198e-06,
      "loss": 1.2432,
      "step": 26040
    },
    {
      "epoch": 2.017815991789152,
      "grad_norm": 2.557905912399292,
      "learning_rate": 9.855034946932438e-06,
      "loss": 1.3273,
      "step": 26050
    },
    {
      "epoch": 2.01859060012781,
      "grad_norm": 1.8927116394042969,
      "learning_rate": 9.847268961946674e-06,
      "loss": 1.2336,
      "step": 26060
    },
    {
      "epoch": 2.019365208466469,
      "grad_norm": 2.411726474761963,
      "learning_rate": 9.839502976960912e-06,
      "loss": 1.267,
      "step": 26070
    },
    {
      "epoch": 2.020139816805128,
      "grad_norm": 2.919771909713745,
      "learning_rate": 9.83173699197515e-06,
      "loss": 1.2343,
      "step": 26080
    },
    {
      "epoch": 2.0209144251437867,
      "grad_norm": 2.8232812881469727,
      "learning_rate": 9.823971006989387e-06,
      "loss": 1.183,
      "step": 26090
    },
    {
      "epoch": 2.0216890334824456,
      "grad_norm": 2.4531362056732178,
      "learning_rate": 9.816205022003624e-06,
      "loss": 1.3467,
      "step": 26100
    },
    {
      "epoch": 2.0224636418211044,
      "grad_norm": 2.881906509399414,
      "learning_rate": 9.808439037017861e-06,
      "loss": 1.2792,
      "step": 26110
    },
    {
      "epoch": 2.0232382501597628,
      "grad_norm": 1.9815552234649658,
      "learning_rate": 9.8006730520321e-06,
      "loss": 1.29,
      "step": 26120
    },
    {
      "epoch": 2.0240128584984216,
      "grad_norm": 2.600085496902466,
      "learning_rate": 9.792907067046337e-06,
      "loss": 1.2258,
      "step": 26130
    },
    {
      "epoch": 2.0247874668370804,
      "grad_norm": 4.065118789672852,
      "learning_rate": 9.785141082060573e-06,
      "loss": 1.2431,
      "step": 26140
    },
    {
      "epoch": 2.0255620751757393,
      "grad_norm": 2.320887327194214,
      "learning_rate": 9.777375097074813e-06,
      "loss": 1.2285,
      "step": 26150
    },
    {
      "epoch": 2.026336683514398,
      "grad_norm": 2.565131187438965,
      "learning_rate": 9.76960911208905e-06,
      "loss": 1.1852,
      "step": 26160
    },
    {
      "epoch": 2.027111291853057,
      "grad_norm": 1.9264315366744995,
      "learning_rate": 9.761843127103289e-06,
      "loss": 1.2598,
      "step": 26170
    },
    {
      "epoch": 2.0278859001917158,
      "grad_norm": 2.614572525024414,
      "learning_rate": 9.754077142117526e-06,
      "loss": 1.1889,
      "step": 26180
    },
    {
      "epoch": 2.028660508530374,
      "grad_norm": 2.7428038120269775,
      "learning_rate": 9.746311157131763e-06,
      "loss": 1.1535,
      "step": 26190
    },
    {
      "epoch": 2.029435116869033,
      "grad_norm": 2.121351480484009,
      "learning_rate": 9.738545172146e-06,
      "loss": 1.3077,
      "step": 26200
    },
    {
      "epoch": 2.030209725207692,
      "grad_norm": 2.4466512203216553,
      "learning_rate": 9.730779187160238e-06,
      "loss": 1.3052,
      "step": 26210
    },
    {
      "epoch": 2.0309843335463507,
      "grad_norm": 3.498751163482666,
      "learning_rate": 9.723013202174476e-06,
      "loss": 1.2091,
      "step": 26220
    },
    {
      "epoch": 2.0317589418850095,
      "grad_norm": 2.25850772857666,
      "learning_rate": 9.715247217188712e-06,
      "loss": 1.2862,
      "step": 26230
    },
    {
      "epoch": 2.0325335502236683,
      "grad_norm": 2.504770040512085,
      "learning_rate": 9.707481232202952e-06,
      "loss": 1.248,
      "step": 26240
    },
    {
      "epoch": 2.0333081585623267,
      "grad_norm": 2.490980386734009,
      "learning_rate": 9.69971524721719e-06,
      "loss": 1.2946,
      "step": 26250
    },
    {
      "epoch": 2.0340827669009856,
      "grad_norm": 2.9329216480255127,
      "learning_rate": 9.691949262231428e-06,
      "loss": 1.2786,
      "step": 26260
    },
    {
      "epoch": 2.0348573752396444,
      "grad_norm": 2.00067138671875,
      "learning_rate": 9.684183277245664e-06,
      "loss": 1.3199,
      "step": 26270
    },
    {
      "epoch": 2.035631983578303,
      "grad_norm": 2.578564167022705,
      "learning_rate": 9.676417292259902e-06,
      "loss": 1.0548,
      "step": 26280
    },
    {
      "epoch": 2.036406591916962,
      "grad_norm": 2.9816701412200928,
      "learning_rate": 9.66865130727414e-06,
      "loss": 1.2102,
      "step": 26290
    },
    {
      "epoch": 2.037181200255621,
      "grad_norm": 2.9463627338409424,
      "learning_rate": 9.660885322288377e-06,
      "loss": 1.3336,
      "step": 26300
    },
    {
      "epoch": 2.0379558085942797,
      "grad_norm": 2.321955442428589,
      "learning_rate": 9.653119337302614e-06,
      "loss": 1.2643,
      "step": 26310
    },
    {
      "epoch": 2.038730416932938,
      "grad_norm": 3.3148815631866455,
      "learning_rate": 9.645353352316852e-06,
      "loss": 1.2688,
      "step": 26320
    },
    {
      "epoch": 2.039505025271597,
      "grad_norm": 2.771432399749756,
      "learning_rate": 9.63758736733109e-06,
      "loss": 1.205,
      "step": 26330
    },
    {
      "epoch": 2.0402796336102558,
      "grad_norm": 2.8641631603240967,
      "learning_rate": 9.629821382345329e-06,
      "loss": 1.2303,
      "step": 26340
    },
    {
      "epoch": 2.0410542419489146,
      "grad_norm": 2.5819602012634277,
      "learning_rate": 9.622055397359567e-06,
      "loss": 1.2276,
      "step": 26350
    },
    {
      "epoch": 2.0418288502875734,
      "grad_norm": 3.532869338989258,
      "learning_rate": 9.614289412373803e-06,
      "loss": 1.218,
      "step": 26360
    },
    {
      "epoch": 2.0426034586262323,
      "grad_norm": 2.439772844314575,
      "learning_rate": 9.60652342738804e-06,
      "loss": 1.2537,
      "step": 26370
    },
    {
      "epoch": 2.0433780669648907,
      "grad_norm": 2.442126750946045,
      "learning_rate": 9.598757442402279e-06,
      "loss": 1.2407,
      "step": 26380
    },
    {
      "epoch": 2.0441526753035495,
      "grad_norm": 2.227827310562134,
      "learning_rate": 9.590991457416517e-06,
      "loss": 1.2479,
      "step": 26390
    },
    {
      "epoch": 2.0449272836422083,
      "grad_norm": 2.3069040775299072,
      "learning_rate": 9.583225472430753e-06,
      "loss": 1.2776,
      "step": 26400
    },
    {
      "epoch": 2.045701891980867,
      "grad_norm": 3.055830478668213,
      "learning_rate": 9.57545948744499e-06,
      "loss": 1.2513,
      "step": 26410
    },
    {
      "epoch": 2.046476500319526,
      "grad_norm": 2.1650869846343994,
      "learning_rate": 9.567693502459228e-06,
      "loss": 1.2749,
      "step": 26420
    },
    {
      "epoch": 2.047251108658185,
      "grad_norm": 2.200929880142212,
      "learning_rate": 9.559927517473466e-06,
      "loss": 1.2515,
      "step": 26430
    },
    {
      "epoch": 2.0480257169968437,
      "grad_norm": 3.8310859203338623,
      "learning_rate": 9.552161532487704e-06,
      "loss": 1.267,
      "step": 26440
    },
    {
      "epoch": 2.048800325335502,
      "grad_norm": 2.8957388401031494,
      "learning_rate": 9.544395547501942e-06,
      "loss": 1.1704,
      "step": 26450
    },
    {
      "epoch": 2.049574933674161,
      "grad_norm": 1.950995922088623,
      "learning_rate": 9.53662956251618e-06,
      "loss": 1.2718,
      "step": 26460
    },
    {
      "epoch": 2.0503495420128197,
      "grad_norm": 2.886401414871216,
      "learning_rate": 9.528863577530418e-06,
      "loss": 1.2913,
      "step": 26470
    },
    {
      "epoch": 2.0511241503514785,
      "grad_norm": 2.5817277431488037,
      "learning_rate": 9.521097592544654e-06,
      "loss": 1.2029,
      "step": 26480
    },
    {
      "epoch": 2.0518987586901374,
      "grad_norm": 1.950278639793396,
      "learning_rate": 9.513331607558892e-06,
      "loss": 1.241,
      "step": 26490
    },
    {
      "epoch": 2.052673367028796,
      "grad_norm": 2.0895540714263916,
      "learning_rate": 9.50556562257313e-06,
      "loss": 1.258,
      "step": 26500
    },
    {
      "epoch": 2.053447975367455,
      "grad_norm": 2.9006571769714355,
      "learning_rate": 9.497799637587368e-06,
      "loss": 1.1466,
      "step": 26510
    },
    {
      "epoch": 2.0542225837061134,
      "grad_norm": 2.739868402481079,
      "learning_rate": 9.490033652601605e-06,
      "loss": 1.2641,
      "step": 26520
    },
    {
      "epoch": 2.0549971920447723,
      "grad_norm": 2.4987285137176514,
      "learning_rate": 9.482267667615842e-06,
      "loss": 1.2181,
      "step": 26530
    },
    {
      "epoch": 2.055771800383431,
      "grad_norm": 2.492767810821533,
      "learning_rate": 9.474501682630081e-06,
      "loss": 1.2881,
      "step": 26540
    },
    {
      "epoch": 2.05654640872209,
      "grad_norm": 2.272287607192993,
      "learning_rate": 9.466735697644319e-06,
      "loss": 1.2818,
      "step": 26550
    },
    {
      "epoch": 2.0573210170607488,
      "grad_norm": 2.257826805114746,
      "learning_rate": 9.458969712658557e-06,
      "loss": 1.1896,
      "step": 26560
    },
    {
      "epoch": 2.0580956253994076,
      "grad_norm": 2.787034273147583,
      "learning_rate": 9.451203727672793e-06,
      "loss": 1.1653,
      "step": 26570
    },
    {
      "epoch": 2.058870233738066,
      "grad_norm": 2.488485813140869,
      "learning_rate": 9.443437742687031e-06,
      "loss": 1.2161,
      "step": 26580
    },
    {
      "epoch": 2.059644842076725,
      "grad_norm": 2.4453392028808594,
      "learning_rate": 9.435671757701269e-06,
      "loss": 1.2426,
      "step": 26590
    },
    {
      "epoch": 2.0604194504153837,
      "grad_norm": 2.690849542617798,
      "learning_rate": 9.427905772715507e-06,
      "loss": 1.1591,
      "step": 26600
    },
    {
      "epoch": 2.0611940587540425,
      "grad_norm": 2.1522419452667236,
      "learning_rate": 9.420139787729743e-06,
      "loss": 1.2748,
      "step": 26610
    },
    {
      "epoch": 2.0619686670927013,
      "grad_norm": 2.4220924377441406,
      "learning_rate": 9.41237380274398e-06,
      "loss": 1.2675,
      "step": 26620
    },
    {
      "epoch": 2.06274327543136,
      "grad_norm": 2.054734706878662,
      "learning_rate": 9.404607817758219e-06,
      "loss": 1.2168,
      "step": 26630
    },
    {
      "epoch": 2.063517883770019,
      "grad_norm": 3.16140079498291,
      "learning_rate": 9.396841832772458e-06,
      "loss": 1.2389,
      "step": 26640
    },
    {
      "epoch": 2.0642924921086774,
      "grad_norm": 2.20371675491333,
      "learning_rate": 9.389075847786694e-06,
      "loss": 1.2403,
      "step": 26650
    },
    {
      "epoch": 2.065067100447336,
      "grad_norm": 2.3657662868499756,
      "learning_rate": 9.381309862800932e-06,
      "loss": 1.1564,
      "step": 26660
    },
    {
      "epoch": 2.065841708785995,
      "grad_norm": 2.3321616649627686,
      "learning_rate": 9.37354387781517e-06,
      "loss": 1.2238,
      "step": 26670
    },
    {
      "epoch": 2.066616317124654,
      "grad_norm": 2.209369421005249,
      "learning_rate": 9.365777892829408e-06,
      "loss": 1.2596,
      "step": 26680
    },
    {
      "epoch": 2.0673909254633127,
      "grad_norm": 2.6808159351348877,
      "learning_rate": 9.358011907843644e-06,
      "loss": 1.333,
      "step": 26690
    },
    {
      "epoch": 2.0681655338019715,
      "grad_norm": 1.9967913627624512,
      "learning_rate": 9.350245922857882e-06,
      "loss": 1.1974,
      "step": 26700
    },
    {
      "epoch": 2.06894014214063,
      "grad_norm": 2.812004327774048,
      "learning_rate": 9.34247993787212e-06,
      "loss": 1.2675,
      "step": 26710
    },
    {
      "epoch": 2.0697147504792888,
      "grad_norm": 3.1948370933532715,
      "learning_rate": 9.334713952886358e-06,
      "loss": 1.2914,
      "step": 26720
    },
    {
      "epoch": 2.0704893588179476,
      "grad_norm": 2.104865074157715,
      "learning_rate": 9.326947967900597e-06,
      "loss": 1.2117,
      "step": 26730
    },
    {
      "epoch": 2.0712639671566064,
      "grad_norm": 3.1943259239196777,
      "learning_rate": 9.319181982914833e-06,
      "loss": 1.2309,
      "step": 26740
    },
    {
      "epoch": 2.0720385754952653,
      "grad_norm": 2.5038952827453613,
      "learning_rate": 9.311415997929071e-06,
      "loss": 1.2399,
      "step": 26750
    },
    {
      "epoch": 2.072813183833924,
      "grad_norm": 3.4786953926086426,
      "learning_rate": 9.303650012943309e-06,
      "loss": 1.1607,
      "step": 26760
    },
    {
      "epoch": 2.073587792172583,
      "grad_norm": 2.2054810523986816,
      "learning_rate": 9.295884027957547e-06,
      "loss": 1.0643,
      "step": 26770
    },
    {
      "epoch": 2.0743624005112413,
      "grad_norm": 2.4074175357818604,
      "learning_rate": 9.288118042971783e-06,
      "loss": 1.3089,
      "step": 26780
    },
    {
      "epoch": 2.0751370088499,
      "grad_norm": 3.0026473999023438,
      "learning_rate": 9.280352057986021e-06,
      "loss": 1.2909,
      "step": 26790
    },
    {
      "epoch": 2.075911617188559,
      "grad_norm": 2.4177844524383545,
      "learning_rate": 9.272586073000259e-06,
      "loss": 1.1863,
      "step": 26800
    },
    {
      "epoch": 2.076686225527218,
      "grad_norm": 1.7153871059417725,
      "learning_rate": 9.264820088014497e-06,
      "loss": 1.1082,
      "step": 26810
    },
    {
      "epoch": 2.0774608338658767,
      "grad_norm": 2.306987762451172,
      "learning_rate": 9.257054103028733e-06,
      "loss": 1.3516,
      "step": 26820
    },
    {
      "epoch": 2.0782354422045355,
      "grad_norm": 2.451303720474243,
      "learning_rate": 9.249288118042973e-06,
      "loss": 1.3053,
      "step": 26830
    },
    {
      "epoch": 2.079010050543194,
      "grad_norm": 2.3556041717529297,
      "learning_rate": 9.24152213305721e-06,
      "loss": 1.2704,
      "step": 26840
    },
    {
      "epoch": 2.0797846588818527,
      "grad_norm": 1.8760005235671997,
      "learning_rate": 9.233756148071448e-06,
      "loss": 1.3454,
      "step": 26850
    },
    {
      "epoch": 2.0805592672205115,
      "grad_norm": 2.6572132110595703,
      "learning_rate": 9.225990163085684e-06,
      "loss": 1.2431,
      "step": 26860
    },
    {
      "epoch": 2.0813338755591704,
      "grad_norm": 2.922297954559326,
      "learning_rate": 9.218224178099922e-06,
      "loss": 1.2329,
      "step": 26870
    },
    {
      "epoch": 2.082108483897829,
      "grad_norm": 2.1137776374816895,
      "learning_rate": 9.21045819311416e-06,
      "loss": 1.212,
      "step": 26880
    },
    {
      "epoch": 2.082883092236488,
      "grad_norm": 2.5319199562072754,
      "learning_rate": 9.202692208128398e-06,
      "loss": 1.2489,
      "step": 26890
    },
    {
      "epoch": 2.083657700575147,
      "grad_norm": 2.790362596511841,
      "learning_rate": 9.194926223142636e-06,
      "loss": 1.2423,
      "step": 26900
    },
    {
      "epoch": 2.0844323089138053,
      "grad_norm": 2.5272302627563477,
      "learning_rate": 9.187160238156872e-06,
      "loss": 1.3246,
      "step": 26910
    },
    {
      "epoch": 2.085206917252464,
      "grad_norm": 2.777076482772827,
      "learning_rate": 9.17939425317111e-06,
      "loss": 1.2152,
      "step": 26920
    },
    {
      "epoch": 2.085981525591123,
      "grad_norm": 2.0807337760925293,
      "learning_rate": 9.17162826818535e-06,
      "loss": 1.3165,
      "step": 26930
    },
    {
      "epoch": 2.0867561339297818,
      "grad_norm": 2.247321844100952,
      "learning_rate": 9.163862283199587e-06,
      "loss": 1.2431,
      "step": 26940
    },
    {
      "epoch": 2.0875307422684406,
      "grad_norm": 2.0604312419891357,
      "learning_rate": 9.156096298213824e-06,
      "loss": 1.2665,
      "step": 26950
    },
    {
      "epoch": 2.0883053506070994,
      "grad_norm": 3.035949468612671,
      "learning_rate": 9.148330313228061e-06,
      "loss": 1.2374,
      "step": 26960
    },
    {
      "epoch": 2.0890799589457583,
      "grad_norm": 2.713083267211914,
      "learning_rate": 9.1405643282423e-06,
      "loss": 1.2171,
      "step": 26970
    },
    {
      "epoch": 2.0898545672844167,
      "grad_norm": 3.0883493423461914,
      "learning_rate": 9.132798343256537e-06,
      "loss": 1.279,
      "step": 26980
    },
    {
      "epoch": 2.0906291756230755,
      "grad_norm": 2.429880380630493,
      "learning_rate": 9.125032358270773e-06,
      "loss": 1.2181,
      "step": 26990
    },
    {
      "epoch": 2.0914037839617343,
      "grad_norm": 2.681234836578369,
      "learning_rate": 9.117266373285011e-06,
      "loss": 1.2753,
      "step": 27000
    },
    {
      "epoch": 2.092178392300393,
      "grad_norm": 2.780914783477783,
      "learning_rate": 9.109500388299249e-06,
      "loss": 1.2917,
      "step": 27010
    },
    {
      "epoch": 2.092953000639052,
      "grad_norm": 2.135319709777832,
      "learning_rate": 9.101734403313487e-06,
      "loss": 1.2957,
      "step": 27020
    },
    {
      "epoch": 2.093727608977711,
      "grad_norm": 2.762528896331787,
      "learning_rate": 9.093968418327725e-06,
      "loss": 1.2453,
      "step": 27030
    },
    {
      "epoch": 2.094502217316369,
      "grad_norm": 3.228140354156494,
      "learning_rate": 9.086202433341963e-06,
      "loss": 1.3026,
      "step": 27040
    },
    {
      "epoch": 2.095276825655028,
      "grad_norm": 2.8042056560516357,
      "learning_rate": 9.0784364483562e-06,
      "loss": 1.2469,
      "step": 27050
    },
    {
      "epoch": 2.096051433993687,
      "grad_norm": 2.5523555278778076,
      "learning_rate": 9.070670463370438e-06,
      "loss": 1.2706,
      "step": 27060
    },
    {
      "epoch": 2.0968260423323457,
      "grad_norm": 2.232508659362793,
      "learning_rate": 9.062904478384676e-06,
      "loss": 1.1742,
      "step": 27070
    },
    {
      "epoch": 2.0976006506710045,
      "grad_norm": 2.8494625091552734,
      "learning_rate": 9.055138493398912e-06,
      "loss": 1.3177,
      "step": 27080
    },
    {
      "epoch": 2.0983752590096634,
      "grad_norm": 2.521379232406616,
      "learning_rate": 9.04737250841315e-06,
      "loss": 1.2155,
      "step": 27090
    },
    {
      "epoch": 2.099149867348322,
      "grad_norm": 2.140385389328003,
      "learning_rate": 9.039606523427388e-06,
      "loss": 1.3309,
      "step": 27100
    },
    {
      "epoch": 2.0999244756869806,
      "grad_norm": 2.025920867919922,
      "learning_rate": 9.031840538441626e-06,
      "loss": 1.2102,
      "step": 27110
    },
    {
      "epoch": 2.1006990840256394,
      "grad_norm": 2.1153275966644287,
      "learning_rate": 9.024074553455862e-06,
      "loss": 1.2307,
      "step": 27120
    },
    {
      "epoch": 2.1014736923642983,
      "grad_norm": 2.365480422973633,
      "learning_rate": 9.016308568470102e-06,
      "loss": 1.247,
      "step": 27130
    },
    {
      "epoch": 2.102248300702957,
      "grad_norm": 3.2782444953918457,
      "learning_rate": 9.00854258348434e-06,
      "loss": 1.2263,
      "step": 27140
    },
    {
      "epoch": 2.103022909041616,
      "grad_norm": 2.602851629257202,
      "learning_rate": 9.000776598498577e-06,
      "loss": 1.2266,
      "step": 27150
    },
    {
      "epoch": 2.1037975173802748,
      "grad_norm": 2.6810061931610107,
      "learning_rate": 8.993010613512814e-06,
      "loss": 1.2915,
      "step": 27160
    },
    {
      "epoch": 2.104572125718933,
      "grad_norm": 2.139223098754883,
      "learning_rate": 8.985244628527051e-06,
      "loss": 1.1862,
      "step": 27170
    },
    {
      "epoch": 2.105346734057592,
      "grad_norm": 2.5189523696899414,
      "learning_rate": 8.97747864354129e-06,
      "loss": 1.2229,
      "step": 27180
    },
    {
      "epoch": 2.106121342396251,
      "grad_norm": 1.9941291809082031,
      "learning_rate": 8.969712658555527e-06,
      "loss": 1.1589,
      "step": 27190
    },
    {
      "epoch": 2.1068959507349097,
      "grad_norm": 2.496610164642334,
      "learning_rate": 8.961946673569763e-06,
      "loss": 1.2689,
      "step": 27200
    },
    {
      "epoch": 2.1076705590735685,
      "grad_norm": 2.5047802925109863,
      "learning_rate": 8.954180688584001e-06,
      "loss": 1.3471,
      "step": 27210
    },
    {
      "epoch": 2.1084451674122273,
      "grad_norm": 2.3498270511627197,
      "learning_rate": 8.94641470359824e-06,
      "loss": 1.3054,
      "step": 27220
    },
    {
      "epoch": 2.109219775750886,
      "grad_norm": 2.8811724185943604,
      "learning_rate": 8.938648718612479e-06,
      "loss": 1.2152,
      "step": 27230
    },
    {
      "epoch": 2.1099943840895445,
      "grad_norm": 2.7517154216766357,
      "learning_rate": 8.930882733626715e-06,
      "loss": 1.2767,
      "step": 27240
    },
    {
      "epoch": 2.1107689924282034,
      "grad_norm": 2.292255401611328,
      "learning_rate": 8.923116748640953e-06,
      "loss": 1.2805,
      "step": 27250
    },
    {
      "epoch": 2.111543600766862,
      "grad_norm": 2.2938599586486816,
      "learning_rate": 8.91535076365519e-06,
      "loss": 1.2274,
      "step": 27260
    },
    {
      "epoch": 2.112318209105521,
      "grad_norm": 2.892749786376953,
      "learning_rate": 8.907584778669428e-06,
      "loss": 1.3127,
      "step": 27270
    },
    {
      "epoch": 2.11309281744418,
      "grad_norm": 2.518648147583008,
      "learning_rate": 8.899818793683666e-06,
      "loss": 1.1633,
      "step": 27280
    },
    {
      "epoch": 2.1138674257828387,
      "grad_norm": 2.520761013031006,
      "learning_rate": 8.892052808697903e-06,
      "loss": 1.2388,
      "step": 27290
    },
    {
      "epoch": 2.1146420341214975,
      "grad_norm": 2.6600100994110107,
      "learning_rate": 8.88428682371214e-06,
      "loss": 1.1733,
      "step": 27300
    },
    {
      "epoch": 2.115416642460156,
      "grad_norm": 2.102569103240967,
      "learning_rate": 8.876520838726378e-06,
      "loss": 1.2374,
      "step": 27310
    },
    {
      "epoch": 2.1161912507988148,
      "grad_norm": 2.223025321960449,
      "learning_rate": 8.868754853740618e-06,
      "loss": 1.2431,
      "step": 27320
    },
    {
      "epoch": 2.1169658591374736,
      "grad_norm": 2.6171207427978516,
      "learning_rate": 8.860988868754854e-06,
      "loss": 1.2237,
      "step": 27330
    },
    {
      "epoch": 2.1177404674761324,
      "grad_norm": 2.597482681274414,
      "learning_rate": 8.853222883769092e-06,
      "loss": 1.2666,
      "step": 27340
    },
    {
      "epoch": 2.1185150758147913,
      "grad_norm": 2.101278781890869,
      "learning_rate": 8.84545689878333e-06,
      "loss": 1.1813,
      "step": 27350
    },
    {
      "epoch": 2.11928968415345,
      "grad_norm": 2.2894697189331055,
      "learning_rate": 8.837690913797568e-06,
      "loss": 1.2606,
      "step": 27360
    },
    {
      "epoch": 2.1200642924921085,
      "grad_norm": 2.023167610168457,
      "learning_rate": 8.829924928811804e-06,
      "loss": 1.2788,
      "step": 27370
    },
    {
      "epoch": 2.1208389008307673,
      "grad_norm": 1.6529650688171387,
      "learning_rate": 8.822158943826042e-06,
      "loss": 1.2193,
      "step": 27380
    },
    {
      "epoch": 2.121613509169426,
      "grad_norm": 2.8127849102020264,
      "learning_rate": 8.81439295884028e-06,
      "loss": 1.2622,
      "step": 27390
    },
    {
      "epoch": 2.122388117508085,
      "grad_norm": 2.290449380874634,
      "learning_rate": 8.806626973854517e-06,
      "loss": 1.3045,
      "step": 27400
    },
    {
      "epoch": 2.123162725846744,
      "grad_norm": 2.115478277206421,
      "learning_rate": 8.798860988868754e-06,
      "loss": 1.2641,
      "step": 27410
    },
    {
      "epoch": 2.1239373341854026,
      "grad_norm": 2.076460123062134,
      "learning_rate": 8.791095003882993e-06,
      "loss": 1.1988,
      "step": 27420
    },
    {
      "epoch": 2.1247119425240615,
      "grad_norm": 4.236457824707031,
      "learning_rate": 8.783329018897231e-06,
      "loss": 1.2572,
      "step": 27430
    },
    {
      "epoch": 2.12548655086272,
      "grad_norm": 2.3523874282836914,
      "learning_rate": 8.775563033911469e-06,
      "loss": 1.2038,
      "step": 27440
    },
    {
      "epoch": 2.1262611592013787,
      "grad_norm": 3.0193161964416504,
      "learning_rate": 8.767797048925707e-06,
      "loss": 1.2243,
      "step": 27450
    },
    {
      "epoch": 2.1270357675400375,
      "grad_norm": 2.362250328063965,
      "learning_rate": 8.760031063939943e-06,
      "loss": 1.3656,
      "step": 27460
    },
    {
      "epoch": 2.1278103758786964,
      "grad_norm": 2.5181591510772705,
      "learning_rate": 8.75226507895418e-06,
      "loss": 1.2331,
      "step": 27470
    },
    {
      "epoch": 2.128584984217355,
      "grad_norm": 2.5403454303741455,
      "learning_rate": 8.744499093968419e-06,
      "loss": 1.3184,
      "step": 27480
    },
    {
      "epoch": 2.129359592556014,
      "grad_norm": 2.6461095809936523,
      "learning_rate": 8.736733108982656e-06,
      "loss": 1.3422,
      "step": 27490
    },
    {
      "epoch": 2.1301342008946724,
      "grad_norm": 3.4510843753814697,
      "learning_rate": 8.728967123996893e-06,
      "loss": 1.143,
      "step": 27500
    },
    {
      "epoch": 2.1309088092333313,
      "grad_norm": 2.0724198818206787,
      "learning_rate": 8.72120113901113e-06,
      "loss": 1.2848,
      "step": 27510
    },
    {
      "epoch": 2.13168341757199,
      "grad_norm": 3.185619831085205,
      "learning_rate": 8.71343515402537e-06,
      "loss": 1.1493,
      "step": 27520
    },
    {
      "epoch": 2.132458025910649,
      "grad_norm": 2.144291877746582,
      "learning_rate": 8.705669169039608e-06,
      "loss": 1.1461,
      "step": 27530
    },
    {
      "epoch": 2.1332326342493078,
      "grad_norm": 2.0007948875427246,
      "learning_rate": 8.697903184053844e-06,
      "loss": 1.2954,
      "step": 27540
    },
    {
      "epoch": 2.1340072425879666,
      "grad_norm": 2.414489984512329,
      "learning_rate": 8.690137199068082e-06,
      "loss": 1.2672,
      "step": 27550
    },
    {
      "epoch": 2.1347818509266254,
      "grad_norm": 1.9472975730895996,
      "learning_rate": 8.68237121408232e-06,
      "loss": 1.2734,
      "step": 27560
    },
    {
      "epoch": 2.135556459265284,
      "grad_norm": 2.7119131088256836,
      "learning_rate": 8.674605229096558e-06,
      "loss": 1.3085,
      "step": 27570
    },
    {
      "epoch": 2.1363310676039426,
      "grad_norm": 3.5410537719726562,
      "learning_rate": 8.666839244110794e-06,
      "loss": 1.1911,
      "step": 27580
    },
    {
      "epoch": 2.1371056759426015,
      "grad_norm": 2.1898868083953857,
      "learning_rate": 8.659073259125032e-06,
      "loss": 1.3331,
      "step": 27590
    },
    {
      "epoch": 2.1378802842812603,
      "grad_norm": 2.1438605785369873,
      "learning_rate": 8.65130727413927e-06,
      "loss": 1.313,
      "step": 27600
    },
    {
      "epoch": 2.138654892619919,
      "grad_norm": 2.500540256500244,
      "learning_rate": 8.643541289153507e-06,
      "loss": 1.1766,
      "step": 27610
    },
    {
      "epoch": 2.139429500958578,
      "grad_norm": 2.549403190612793,
      "learning_rate": 8.635775304167747e-06,
      "loss": 1.2955,
      "step": 27620
    },
    {
      "epoch": 2.140204109297237,
      "grad_norm": 2.409060478210449,
      "learning_rate": 8.628009319181983e-06,
      "loss": 1.2523,
      "step": 27630
    },
    {
      "epoch": 2.140978717635895,
      "grad_norm": 3.2060563564300537,
      "learning_rate": 8.620243334196221e-06,
      "loss": 1.1857,
      "step": 27640
    },
    {
      "epoch": 2.141753325974554,
      "grad_norm": 2.727468252182007,
      "learning_rate": 8.612477349210459e-06,
      "loss": 1.3086,
      "step": 27650
    },
    {
      "epoch": 2.142527934313213,
      "grad_norm": 2.523104667663574,
      "learning_rate": 8.604711364224697e-06,
      "loss": 1.295,
      "step": 27660
    },
    {
      "epoch": 2.1433025426518717,
      "grad_norm": 2.480095863342285,
      "learning_rate": 8.596945379238933e-06,
      "loss": 1.2234,
      "step": 27670
    },
    {
      "epoch": 2.1440771509905305,
      "grad_norm": 2.772516965866089,
      "learning_rate": 8.58917939425317e-06,
      "loss": 1.2794,
      "step": 27680
    },
    {
      "epoch": 2.1448517593291894,
      "grad_norm": 2.6466104984283447,
      "learning_rate": 8.581413409267409e-06,
      "loss": 1.335,
      "step": 27690
    },
    {
      "epoch": 2.1456263676678478,
      "grad_norm": 2.0773556232452393,
      "learning_rate": 8.573647424281647e-06,
      "loss": 1.1628,
      "step": 27700
    },
    {
      "epoch": 2.1464009760065066,
      "grad_norm": 1.8039143085479736,
      "learning_rate": 8.565881439295884e-06,
      "loss": 1.1971,
      "step": 27710
    },
    {
      "epoch": 2.1471755843451654,
      "grad_norm": 2.000290870666504,
      "learning_rate": 8.558115454310122e-06,
      "loss": 1.3478,
      "step": 27720
    },
    {
      "epoch": 2.1479501926838243,
      "grad_norm": 2.408538341522217,
      "learning_rate": 8.55034946932436e-06,
      "loss": 1.3266,
      "step": 27730
    },
    {
      "epoch": 2.148724801022483,
      "grad_norm": 3.122673273086548,
      "learning_rate": 8.542583484338598e-06,
      "loss": 1.3728,
      "step": 27740
    },
    {
      "epoch": 2.149499409361142,
      "grad_norm": 2.5469791889190674,
      "learning_rate": 8.534817499352834e-06,
      "loss": 1.3098,
      "step": 27750
    },
    {
      "epoch": 2.1502740176998003,
      "grad_norm": 2.8274989128112793,
      "learning_rate": 8.527051514367072e-06,
      "loss": 1.2498,
      "step": 27760
    },
    {
      "epoch": 2.151048626038459,
      "grad_norm": 2.8613197803497314,
      "learning_rate": 8.51928552938131e-06,
      "loss": 1.1996,
      "step": 27770
    },
    {
      "epoch": 2.151823234377118,
      "grad_norm": 2.3395934104919434,
      "learning_rate": 8.511519544395548e-06,
      "loss": 1.2868,
      "step": 27780
    },
    {
      "epoch": 2.152597842715777,
      "grad_norm": 1.9488333463668823,
      "learning_rate": 8.503753559409786e-06,
      "loss": 1.2191,
      "step": 27790
    },
    {
      "epoch": 2.1533724510544356,
      "grad_norm": 3.4489917755126953,
      "learning_rate": 8.495987574424022e-06,
      "loss": 1.2659,
      "step": 27800
    },
    {
      "epoch": 2.1541470593930945,
      "grad_norm": 2.725116014480591,
      "learning_rate": 8.488221589438261e-06,
      "loss": 1.2428,
      "step": 27810
    },
    {
      "epoch": 2.1549216677317533,
      "grad_norm": 2.7393012046813965,
      "learning_rate": 8.4804556044525e-06,
      "loss": 1.2987,
      "step": 27820
    },
    {
      "epoch": 2.1556962760704117,
      "grad_norm": 2.070422410964966,
      "learning_rate": 8.472689619466737e-06,
      "loss": 1.2671,
      "step": 27830
    },
    {
      "epoch": 2.1564708844090705,
      "grad_norm": 2.4684715270996094,
      "learning_rate": 8.464923634480973e-06,
      "loss": 1.2647,
      "step": 27840
    },
    {
      "epoch": 2.1572454927477294,
      "grad_norm": 2.2142796516418457,
      "learning_rate": 8.457157649495211e-06,
      "loss": 1.2585,
      "step": 27850
    },
    {
      "epoch": 2.158020101086388,
      "grad_norm": 2.6929428577423096,
      "learning_rate": 8.449391664509449e-06,
      "loss": 1.2389,
      "step": 27860
    },
    {
      "epoch": 2.158794709425047,
      "grad_norm": 2.2946267127990723,
      "learning_rate": 8.441625679523687e-06,
      "loss": 1.2044,
      "step": 27870
    },
    {
      "epoch": 2.159569317763706,
      "grad_norm": 3.0218300819396973,
      "learning_rate": 8.433859694537923e-06,
      "loss": 1.1551,
      "step": 27880
    },
    {
      "epoch": 2.1603439261023647,
      "grad_norm": 2.87380051612854,
      "learning_rate": 8.426093709552161e-06,
      "loss": 1.2278,
      "step": 27890
    },
    {
      "epoch": 2.161118534441023,
      "grad_norm": 2.7213680744171143,
      "learning_rate": 8.418327724566399e-06,
      "loss": 1.2444,
      "step": 27900
    },
    {
      "epoch": 2.161893142779682,
      "grad_norm": 3.158207654953003,
      "learning_rate": 8.410561739580638e-06,
      "loss": 1.2441,
      "step": 27910
    },
    {
      "epoch": 2.1626677511183408,
      "grad_norm": 2.290156841278076,
      "learning_rate": 8.402795754594875e-06,
      "loss": 1.2197,
      "step": 27920
    },
    {
      "epoch": 2.1634423594569996,
      "grad_norm": 1.8502789735794067,
      "learning_rate": 8.395029769609112e-06,
      "loss": 1.2441,
      "step": 27930
    },
    {
      "epoch": 2.1642169677956584,
      "grad_norm": 2.5676400661468506,
      "learning_rate": 8.38726378462335e-06,
      "loss": 1.2787,
      "step": 27940
    },
    {
      "epoch": 2.1649915761343173,
      "grad_norm": 2.880662441253662,
      "learning_rate": 8.379497799637588e-06,
      "loss": 1.2899,
      "step": 27950
    },
    {
      "epoch": 2.165766184472976,
      "grad_norm": 2.6892547607421875,
      "learning_rate": 8.371731814651826e-06,
      "loss": 1.2265,
      "step": 27960
    },
    {
      "epoch": 2.1665407928116345,
      "grad_norm": 2.08613657951355,
      "learning_rate": 8.363965829666062e-06,
      "loss": 1.237,
      "step": 27970
    },
    {
      "epoch": 2.1673154011502933,
      "grad_norm": 2.727536678314209,
      "learning_rate": 8.3561998446803e-06,
      "loss": 1.3144,
      "step": 27980
    },
    {
      "epoch": 2.168090009488952,
      "grad_norm": 2.2350685596466064,
      "learning_rate": 8.348433859694538e-06,
      "loss": 1.2212,
      "step": 27990
    },
    {
      "epoch": 2.168864617827611,
      "grad_norm": 2.977783203125,
      "learning_rate": 8.340667874708776e-06,
      "loss": 1.3409,
      "step": 28000
    },
    {
      "epoch": 2.16963922616627,
      "grad_norm": 2.375840902328491,
      "learning_rate": 8.332901889723014e-06,
      "loss": 1.292,
      "step": 28010
    },
    {
      "epoch": 2.1704138345049286,
      "grad_norm": 3.3432250022888184,
      "learning_rate": 8.325135904737251e-06,
      "loss": 1.2987,
      "step": 28020
    },
    {
      "epoch": 2.171188442843587,
      "grad_norm": 2.648967742919922,
      "learning_rate": 8.31736991975149e-06,
      "loss": 1.2244,
      "step": 28030
    },
    {
      "epoch": 2.171963051182246,
      "grad_norm": 2.4933106899261475,
      "learning_rate": 8.309603934765727e-06,
      "loss": 1.1965,
      "step": 28040
    },
    {
      "epoch": 2.1727376595209047,
      "grad_norm": 2.6302523612976074,
      "learning_rate": 8.301837949779963e-06,
      "loss": 1.2708,
      "step": 28050
    },
    {
      "epoch": 2.1735122678595635,
      "grad_norm": 2.1933789253234863,
      "learning_rate": 8.294071964794201e-06,
      "loss": 1.2819,
      "step": 28060
    },
    {
      "epoch": 2.1742868761982224,
      "grad_norm": 2.7251338958740234,
      "learning_rate": 8.286305979808439e-06,
      "loss": 1.1504,
      "step": 28070
    },
    {
      "epoch": 2.175061484536881,
      "grad_norm": 2.57890248298645,
      "learning_rate": 8.278539994822677e-06,
      "loss": 1.2535,
      "step": 28080
    },
    {
      "epoch": 2.1758360928755396,
      "grad_norm": 2.485185384750366,
      "learning_rate": 8.270774009836913e-06,
      "loss": 1.2615,
      "step": 28090
    },
    {
      "epoch": 2.1766107012141984,
      "grad_norm": 2.053936719894409,
      "learning_rate": 8.263008024851151e-06,
      "loss": 1.1971,
      "step": 28100
    },
    {
      "epoch": 2.1773853095528573,
      "grad_norm": 3.499455451965332,
      "learning_rate": 8.25524203986539e-06,
      "loss": 1.3353,
      "step": 28110
    },
    {
      "epoch": 2.178159917891516,
      "grad_norm": 1.649131178855896,
      "learning_rate": 8.247476054879628e-06,
      "loss": 1.2252,
      "step": 28120
    },
    {
      "epoch": 2.178934526230175,
      "grad_norm": 2.751716136932373,
      "learning_rate": 8.239710069893865e-06,
      "loss": 1.2233,
      "step": 28130
    },
    {
      "epoch": 2.1797091345688338,
      "grad_norm": 2.2915163040161133,
      "learning_rate": 8.231944084908102e-06,
      "loss": 1.2556,
      "step": 28140
    },
    {
      "epoch": 2.1804837429074926,
      "grad_norm": 2.453676462173462,
      "learning_rate": 8.22417809992234e-06,
      "loss": 1.2209,
      "step": 28150
    },
    {
      "epoch": 2.181258351246151,
      "grad_norm": 2.0439982414245605,
      "learning_rate": 8.216412114936578e-06,
      "loss": 1.2391,
      "step": 28160
    },
    {
      "epoch": 2.18203295958481,
      "grad_norm": 2.34757399559021,
      "learning_rate": 8.208646129950816e-06,
      "loss": 1.1915,
      "step": 28170
    },
    {
      "epoch": 2.1828075679234686,
      "grad_norm": 3.6167614459991455,
      "learning_rate": 8.200880144965052e-06,
      "loss": 1.2915,
      "step": 28180
    },
    {
      "epoch": 2.1835821762621275,
      "grad_norm": 2.8508667945861816,
      "learning_rate": 8.19311415997929e-06,
      "loss": 1.2075,
      "step": 28190
    },
    {
      "epoch": 2.1843567846007863,
      "grad_norm": 2.1763904094696045,
      "learning_rate": 8.185348174993528e-06,
      "loss": 1.2313,
      "step": 28200
    },
    {
      "epoch": 2.185131392939445,
      "grad_norm": 2.385322093963623,
      "learning_rate": 8.177582190007768e-06,
      "loss": 1.215,
      "step": 28210
    },
    {
      "epoch": 2.185906001278104,
      "grad_norm": 2.4226396083831787,
      "learning_rate": 8.169816205022004e-06,
      "loss": 1.1692,
      "step": 28220
    },
    {
      "epoch": 2.1866806096167624,
      "grad_norm": 2.081052303314209,
      "learning_rate": 8.162050220036242e-06,
      "loss": 1.331,
      "step": 28230
    },
    {
      "epoch": 2.187455217955421,
      "grad_norm": 2.497837543487549,
      "learning_rate": 8.15428423505048e-06,
      "loss": 1.2328,
      "step": 28240
    },
    {
      "epoch": 2.18822982629408,
      "grad_norm": 2.323791980743408,
      "learning_rate": 8.146518250064717e-06,
      "loss": 1.3017,
      "step": 28250
    },
    {
      "epoch": 2.189004434632739,
      "grad_norm": 2.6112635135650635,
      "learning_rate": 8.138752265078953e-06,
      "loss": 1.266,
      "step": 28260
    },
    {
      "epoch": 2.1897790429713977,
      "grad_norm": 2.5743637084960938,
      "learning_rate": 8.130986280093191e-06,
      "loss": 1.2,
      "step": 28270
    },
    {
      "epoch": 2.1905536513100565,
      "grad_norm": 2.446369171142578,
      "learning_rate": 8.12322029510743e-06,
      "loss": 1.3065,
      "step": 28280
    },
    {
      "epoch": 2.191328259648715,
      "grad_norm": 2.112684726715088,
      "learning_rate": 8.115454310121667e-06,
      "loss": 1.2989,
      "step": 28290
    },
    {
      "epoch": 2.1921028679873737,
      "grad_norm": 3.1942355632781982,
      "learning_rate": 8.107688325135905e-06,
      "loss": 1.1574,
      "step": 28300
    },
    {
      "epoch": 2.1928774763260326,
      "grad_norm": 2.608201026916504,
      "learning_rate": 8.099922340150143e-06,
      "loss": 1.3645,
      "step": 28310
    },
    {
      "epoch": 2.1936520846646914,
      "grad_norm": 2.2468924522399902,
      "learning_rate": 8.09215635516438e-06,
      "loss": 1.2602,
      "step": 28320
    },
    {
      "epoch": 2.1944266930033502,
      "grad_norm": 2.071268081665039,
      "learning_rate": 8.084390370178619e-06,
      "loss": 1.2993,
      "step": 28330
    },
    {
      "epoch": 2.195201301342009,
      "grad_norm": 2.455772876739502,
      "learning_rate": 8.076624385192856e-06,
      "loss": 1.2261,
      "step": 28340
    },
    {
      "epoch": 2.195975909680668,
      "grad_norm": 2.2393276691436768,
      "learning_rate": 8.068858400207093e-06,
      "loss": 1.3139,
      "step": 28350
    },
    {
      "epoch": 2.1967505180193263,
      "grad_norm": 2.778747320175171,
      "learning_rate": 8.06109241522133e-06,
      "loss": 1.2856,
      "step": 28360
    },
    {
      "epoch": 2.197525126357985,
      "grad_norm": 2.071659564971924,
      "learning_rate": 8.053326430235568e-06,
      "loss": 1.218,
      "step": 28370
    },
    {
      "epoch": 2.198299734696644,
      "grad_norm": 2.7591254711151123,
      "learning_rate": 8.045560445249806e-06,
      "loss": 1.2366,
      "step": 28380
    },
    {
      "epoch": 2.199074343035303,
      "grad_norm": 2.948146343231201,
      "learning_rate": 8.037794460264042e-06,
      "loss": 1.2211,
      "step": 28390
    },
    {
      "epoch": 2.1998489513739616,
      "grad_norm": 2.8718419075012207,
      "learning_rate": 8.030028475278282e-06,
      "loss": 1.2873,
      "step": 28400
    },
    {
      "epoch": 2.2006235597126205,
      "grad_norm": 2.3880043029785156,
      "learning_rate": 8.02226249029252e-06,
      "loss": 1.2704,
      "step": 28410
    },
    {
      "epoch": 2.201398168051279,
      "grad_norm": 2.5421066284179688,
      "learning_rate": 8.014496505306758e-06,
      "loss": 1.3236,
      "step": 28420
    },
    {
      "epoch": 2.2021727763899377,
      "grad_norm": 3.2275218963623047,
      "learning_rate": 8.006730520320994e-06,
      "loss": 1.2319,
      "step": 28430
    },
    {
      "epoch": 2.2029473847285965,
      "grad_norm": 2.33672833442688,
      "learning_rate": 7.998964535335232e-06,
      "loss": 1.255,
      "step": 28440
    },
    {
      "epoch": 2.2037219930672554,
      "grad_norm": 2.7788736820220947,
      "learning_rate": 7.99119855034947e-06,
      "loss": 1.1496,
      "step": 28450
    },
    {
      "epoch": 2.204496601405914,
      "grad_norm": 2.084869623184204,
      "learning_rate": 7.983432565363707e-06,
      "loss": 1.2771,
      "step": 28460
    },
    {
      "epoch": 2.205271209744573,
      "grad_norm": 2.434438705444336,
      "learning_rate": 7.975666580377944e-06,
      "loss": 1.254,
      "step": 28470
    },
    {
      "epoch": 2.206045818083232,
      "grad_norm": 2.821507215499878,
      "learning_rate": 7.967900595392181e-06,
      "loss": 1.165,
      "step": 28480
    },
    {
      "epoch": 2.2068204264218902,
      "grad_norm": 3.1997265815734863,
      "learning_rate": 7.96013461040642e-06,
      "loss": 1.2083,
      "step": 28490
    },
    {
      "epoch": 2.207595034760549,
      "grad_norm": 2.374258041381836,
      "learning_rate": 7.952368625420659e-06,
      "loss": 1.1945,
      "step": 28500
    },
    {
      "epoch": 2.208369643099208,
      "grad_norm": 2.1983489990234375,
      "learning_rate": 7.944602640434897e-06,
      "loss": 1.1286,
      "step": 28510
    },
    {
      "epoch": 2.2091442514378667,
      "grad_norm": 2.1072239875793457,
      "learning_rate": 7.936836655449133e-06,
      "loss": 1.3188,
      "step": 28520
    },
    {
      "epoch": 2.2099188597765256,
      "grad_norm": 2.8915884494781494,
      "learning_rate": 7.92907067046337e-06,
      "loss": 1.1832,
      "step": 28530
    },
    {
      "epoch": 2.2106934681151844,
      "grad_norm": 2.355563163757324,
      "learning_rate": 7.921304685477609e-06,
      "loss": 1.2401,
      "step": 28540
    },
    {
      "epoch": 2.2114680764538432,
      "grad_norm": 2.4148683547973633,
      "learning_rate": 7.913538700491847e-06,
      "loss": 1.2698,
      "step": 28550
    },
    {
      "epoch": 2.2122426847925016,
      "grad_norm": 2.2819392681121826,
      "learning_rate": 7.905772715506083e-06,
      "loss": 1.2042,
      "step": 28560
    },
    {
      "epoch": 2.2130172931311605,
      "grad_norm": 1.6518051624298096,
      "learning_rate": 7.89800673052032e-06,
      "loss": 1.2362,
      "step": 28570
    },
    {
      "epoch": 2.2137919014698193,
      "grad_norm": 2.168179750442505,
      "learning_rate": 7.890240745534558e-06,
      "loss": 1.2876,
      "step": 28580
    },
    {
      "epoch": 2.214566509808478,
      "grad_norm": 2.584440231323242,
      "learning_rate": 7.882474760548796e-06,
      "loss": 1.1646,
      "step": 28590
    },
    {
      "epoch": 2.215341118147137,
      "grad_norm": 2.2017712593078613,
      "learning_rate": 7.874708775563034e-06,
      "loss": 1.2104,
      "step": 28600
    },
    {
      "epoch": 2.216115726485796,
      "grad_norm": 3.3670990467071533,
      "learning_rate": 7.866942790577272e-06,
      "loss": 1.2781,
      "step": 28610
    },
    {
      "epoch": 2.216890334824454,
      "grad_norm": 2.606182813644409,
      "learning_rate": 7.85917680559151e-06,
      "loss": 1.3092,
      "step": 28620
    },
    {
      "epoch": 2.217664943163113,
      "grad_norm": 2.6622884273529053,
      "learning_rate": 7.851410820605748e-06,
      "loss": 1.232,
      "step": 28630
    },
    {
      "epoch": 2.218439551501772,
      "grad_norm": 3.3034682273864746,
      "learning_rate": 7.843644835619984e-06,
      "loss": 1.1636,
      "step": 28640
    },
    {
      "epoch": 2.2192141598404307,
      "grad_norm": 2.3589162826538086,
      "learning_rate": 7.835878850634222e-06,
      "loss": 1.3272,
      "step": 28650
    },
    {
      "epoch": 2.2199887681790895,
      "grad_norm": 2.4133529663085938,
      "learning_rate": 7.82811286564846e-06,
      "loss": 1.293,
      "step": 28660
    },
    {
      "epoch": 2.2207633765177484,
      "grad_norm": 2.371638298034668,
      "learning_rate": 7.820346880662698e-06,
      "loss": 1.3227,
      "step": 28670
    },
    {
      "epoch": 2.2215379848564067,
      "grad_norm": 2.9684736728668213,
      "learning_rate": 7.812580895676935e-06,
      "loss": 1.2033,
      "step": 28680
    },
    {
      "epoch": 2.2223125931950656,
      "grad_norm": 2.396376132965088,
      "learning_rate": 7.804814910691172e-06,
      "loss": 1.2153,
      "step": 28690
    },
    {
      "epoch": 2.2230872015337244,
      "grad_norm": 2.9694952964782715,
      "learning_rate": 7.797048925705411e-06,
      "loss": 1.2199,
      "step": 28700
    },
    {
      "epoch": 2.2238618098723832,
      "grad_norm": 2.1536011695861816,
      "learning_rate": 7.789282940719649e-06,
      "loss": 1.2353,
      "step": 28710
    },
    {
      "epoch": 2.224636418211042,
      "grad_norm": 2.5977301597595215,
      "learning_rate": 7.781516955733887e-06,
      "loss": 1.3152,
      "step": 28720
    },
    {
      "epoch": 2.225411026549701,
      "grad_norm": 2.478299856185913,
      "learning_rate": 7.773750970748123e-06,
      "loss": 1.2264,
      "step": 28730
    },
    {
      "epoch": 2.2261856348883597,
      "grad_norm": 2.992244005203247,
      "learning_rate": 7.765984985762361e-06,
      "loss": 1.2461,
      "step": 28740
    },
    {
      "epoch": 2.226960243227018,
      "grad_norm": 2.028945207595825,
      "learning_rate": 7.758219000776599e-06,
      "loss": 1.2801,
      "step": 28750
    },
    {
      "epoch": 2.227734851565677,
      "grad_norm": 2.9908368587493896,
      "learning_rate": 7.750453015790837e-06,
      "loss": 1.244,
      "step": 28760
    },
    {
      "epoch": 2.228509459904336,
      "grad_norm": 2.785249948501587,
      "learning_rate": 7.742687030805073e-06,
      "loss": 1.2588,
      "step": 28770
    },
    {
      "epoch": 2.2292840682429946,
      "grad_norm": 2.3583481311798096,
      "learning_rate": 7.73492104581931e-06,
      "loss": 1.2802,
      "step": 28780
    },
    {
      "epoch": 2.2300586765816535,
      "grad_norm": 2.3181114196777344,
      "learning_rate": 7.72715506083355e-06,
      "loss": 1.3235,
      "step": 28790
    },
    {
      "epoch": 2.2308332849203123,
      "grad_norm": 2.331702470779419,
      "learning_rate": 7.719389075847788e-06,
      "loss": 1.3148,
      "step": 28800
    },
    {
      "epoch": 2.231607893258971,
      "grad_norm": 2.7608206272125244,
      "learning_rate": 7.711623090862024e-06,
      "loss": 1.2456,
      "step": 28810
    },
    {
      "epoch": 2.2323825015976295,
      "grad_norm": 2.682905912399292,
      "learning_rate": 7.703857105876262e-06,
      "loss": 1.2837,
      "step": 28820
    },
    {
      "epoch": 2.2331571099362884,
      "grad_norm": 2.25970721244812,
      "learning_rate": 7.6960911208905e-06,
      "loss": 1.2541,
      "step": 28830
    },
    {
      "epoch": 2.233931718274947,
      "grad_norm": 2.834105968475342,
      "learning_rate": 7.688325135904738e-06,
      "loss": 1.1741,
      "step": 28840
    },
    {
      "epoch": 2.234706326613606,
      "grad_norm": 2.649477481842041,
      "learning_rate": 7.680559150918974e-06,
      "loss": 1.1948,
      "step": 28850
    },
    {
      "epoch": 2.235480934952265,
      "grad_norm": 2.3505499362945557,
      "learning_rate": 7.672793165933212e-06,
      "loss": 1.3051,
      "step": 28860
    },
    {
      "epoch": 2.2362555432909237,
      "grad_norm": 2.120361089706421,
      "learning_rate": 7.66502718094745e-06,
      "loss": 1.1624,
      "step": 28870
    },
    {
      "epoch": 2.2370301516295825,
      "grad_norm": 2.086892604827881,
      "learning_rate": 7.657261195961688e-06,
      "loss": 1.1757,
      "step": 28880
    },
    {
      "epoch": 2.237804759968241,
      "grad_norm": 2.654906988143921,
      "learning_rate": 7.649495210975927e-06,
      "loss": 1.2754,
      "step": 28890
    },
    {
      "epoch": 2.2385793683068997,
      "grad_norm": 2.1225483417510986,
      "learning_rate": 7.641729225990163e-06,
      "loss": 1.2133,
      "step": 28900
    },
    {
      "epoch": 2.2393539766455586,
      "grad_norm": 2.7449402809143066,
      "learning_rate": 7.633963241004401e-06,
      "loss": 1.242,
      "step": 28910
    },
    {
      "epoch": 2.2401285849842174,
      "grad_norm": 2.7777066230773926,
      "learning_rate": 7.626197256018639e-06,
      "loss": 1.3138,
      "step": 28920
    },
    {
      "epoch": 2.2409031933228762,
      "grad_norm": 2.540566921234131,
      "learning_rate": 7.618431271032876e-06,
      "loss": 1.1537,
      "step": 28930
    },
    {
      "epoch": 2.241677801661535,
      "grad_norm": 2.544358253479004,
      "learning_rate": 7.610665286047114e-06,
      "loss": 1.2546,
      "step": 28940
    },
    {
      "epoch": 2.2424524100001935,
      "grad_norm": 2.828587770462036,
      "learning_rate": 7.602899301061351e-06,
      "loss": 1.2213,
      "step": 28950
    },
    {
      "epoch": 2.2432270183388523,
      "grad_norm": 2.831221103668213,
      "learning_rate": 7.595133316075589e-06,
      "loss": 1.1249,
      "step": 28960
    },
    {
      "epoch": 2.244001626677511,
      "grad_norm": 3.2633230686187744,
      "learning_rate": 7.587367331089826e-06,
      "loss": 1.3314,
      "step": 28970
    },
    {
      "epoch": 2.24477623501617,
      "grad_norm": 2.040220022201538,
      "learning_rate": 7.579601346104064e-06,
      "loss": 1.3687,
      "step": 28980
    },
    {
      "epoch": 2.245550843354829,
      "grad_norm": 2.7609496116638184,
      "learning_rate": 7.5718353611183025e-06,
      "loss": 1.2802,
      "step": 28990
    },
    {
      "epoch": 2.2463254516934876,
      "grad_norm": 2.501957416534424,
      "learning_rate": 7.56406937613254e-06,
      "loss": 1.1404,
      "step": 29000
    },
    {
      "epoch": 2.247100060032146,
      "grad_norm": 2.8507449626922607,
      "learning_rate": 7.556303391146777e-06,
      "loss": 1.2563,
      "step": 29010
    },
    {
      "epoch": 2.247874668370805,
      "grad_norm": 2.495295524597168,
      "learning_rate": 7.548537406161015e-06,
      "loss": 1.2148,
      "step": 29020
    },
    {
      "epoch": 2.2486492767094637,
      "grad_norm": 2.1888723373413086,
      "learning_rate": 7.540771421175252e-06,
      "loss": 1.2892,
      "step": 29030
    },
    {
      "epoch": 2.2494238850481225,
      "grad_norm": 2.8582165241241455,
      "learning_rate": 7.53300543618949e-06,
      "loss": 1.184,
      "step": 29040
    },
    {
      "epoch": 2.2501984933867814,
      "grad_norm": 2.904104709625244,
      "learning_rate": 7.525239451203728e-06,
      "loss": 1.2623,
      "step": 29050
    },
    {
      "epoch": 2.25097310172544,
      "grad_norm": 1.9434324502944946,
      "learning_rate": 7.517473466217965e-06,
      "loss": 1.3193,
      "step": 29060
    },
    {
      "epoch": 2.251747710064099,
      "grad_norm": 3.6485705375671387,
      "learning_rate": 7.509707481232203e-06,
      "loss": 1.2107,
      "step": 29070
    },
    {
      "epoch": 2.2525223184027574,
      "grad_norm": 2.726534128189087,
      "learning_rate": 7.50194149624644e-06,
      "loss": 1.2446,
      "step": 29080
    },
    {
      "epoch": 2.2532969267414162,
      "grad_norm": 2.1993257999420166,
      "learning_rate": 7.494175511260679e-06,
      "loss": 1.2284,
      "step": 29090
    },
    {
      "epoch": 2.254071535080075,
      "grad_norm": 2.852722644805908,
      "learning_rate": 7.486409526274916e-06,
      "loss": 1.191,
      "step": 29100
    },
    {
      "epoch": 2.254846143418734,
      "grad_norm": 2.2921559810638428,
      "learning_rate": 7.478643541289154e-06,
      "loss": 1.221,
      "step": 29110
    },
    {
      "epoch": 2.2556207517573927,
      "grad_norm": 2.802743673324585,
      "learning_rate": 7.470877556303391e-06,
      "loss": 1.2501,
      "step": 29120
    },
    {
      "epoch": 2.2563953600960516,
      "grad_norm": 2.6489903926849365,
      "learning_rate": 7.463111571317629e-06,
      "loss": 1.3203,
      "step": 29130
    },
    {
      "epoch": 2.2571699684347104,
      "grad_norm": 2.414524793624878,
      "learning_rate": 7.455345586331866e-06,
      "loss": 1.3063,
      "step": 29140
    },
    {
      "epoch": 2.257944576773369,
      "grad_norm": 1.9403178691864014,
      "learning_rate": 7.447579601346104e-06,
      "loss": 1.2743,
      "step": 29150
    },
    {
      "epoch": 2.2587191851120276,
      "grad_norm": 2.554333209991455,
      "learning_rate": 7.439813616360342e-06,
      "loss": 1.2216,
      "step": 29160
    },
    {
      "epoch": 2.2594937934506865,
      "grad_norm": 2.3140153884887695,
      "learning_rate": 7.43204763137458e-06,
      "loss": 1.2989,
      "step": 29170
    },
    {
      "epoch": 2.2602684017893453,
      "grad_norm": 2.2173948287963867,
      "learning_rate": 7.424281646388817e-06,
      "loss": 1.306,
      "step": 29180
    },
    {
      "epoch": 2.261043010128004,
      "grad_norm": 2.658766508102417,
      "learning_rate": 7.416515661403055e-06,
      "loss": 1.2938,
      "step": 29190
    },
    {
      "epoch": 2.261817618466663,
      "grad_norm": 2.3172061443328857,
      "learning_rate": 7.408749676417292e-06,
      "loss": 1.2175,
      "step": 29200
    },
    {
      "epoch": 2.262592226805322,
      "grad_norm": 2.335601568222046,
      "learning_rate": 7.4009836914315304e-06,
      "loss": 1.3225,
      "step": 29210
    },
    {
      "epoch": 2.26336683514398,
      "grad_norm": 2.394390344619751,
      "learning_rate": 7.393217706445768e-06,
      "loss": 1.1678,
      "step": 29220
    },
    {
      "epoch": 2.264141443482639,
      "grad_norm": 2.3771519660949707,
      "learning_rate": 7.385451721460005e-06,
      "loss": 1.2606,
      "step": 29230
    },
    {
      "epoch": 2.264916051821298,
      "grad_norm": 2.9071967601776123,
      "learning_rate": 7.377685736474243e-06,
      "loss": 1.2329,
      "step": 29240
    },
    {
      "epoch": 2.2656906601599567,
      "grad_norm": 2.5642406940460205,
      "learning_rate": 7.36991975148848e-06,
      "loss": 1.3447,
      "step": 29250
    },
    {
      "epoch": 2.2664652684986155,
      "grad_norm": 2.615042209625244,
      "learning_rate": 7.362153766502719e-06,
      "loss": 1.2562,
      "step": 29260
    },
    {
      "epoch": 2.267239876837274,
      "grad_norm": 2.7339212894439697,
      "learning_rate": 7.354387781516956e-06,
      "loss": 1.2703,
      "step": 29270
    },
    {
      "epoch": 2.2680144851759327,
      "grad_norm": 2.617374897003174,
      "learning_rate": 7.346621796531194e-06,
      "loss": 1.2161,
      "step": 29280
    },
    {
      "epoch": 2.2687890935145916,
      "grad_norm": 2.3333284854888916,
      "learning_rate": 7.338855811545431e-06,
      "loss": 1.2696,
      "step": 29290
    },
    {
      "epoch": 2.2695637018532504,
      "grad_norm": 2.5947365760803223,
      "learning_rate": 7.331089826559669e-06,
      "loss": 1.2607,
      "step": 29300
    },
    {
      "epoch": 2.2703383101919092,
      "grad_norm": 2.7531251907348633,
      "learning_rate": 7.3233238415739066e-06,
      "loss": 1.2629,
      "step": 29310
    },
    {
      "epoch": 2.271112918530568,
      "grad_norm": 2.214627504348755,
      "learning_rate": 7.3155578565881444e-06,
      "loss": 1.2012,
      "step": 29320
    },
    {
      "epoch": 2.271887526869227,
      "grad_norm": 2.9175970554351807,
      "learning_rate": 7.3077918716023814e-06,
      "loss": 1.2769,
      "step": 29330
    },
    {
      "epoch": 2.2726621352078853,
      "grad_norm": 2.4714717864990234,
      "learning_rate": 7.300025886616619e-06,
      "loss": 1.2528,
      "step": 29340
    },
    {
      "epoch": 2.273436743546544,
      "grad_norm": 2.4425289630889893,
      "learning_rate": 7.292259901630856e-06,
      "loss": 1.1981,
      "step": 29350
    },
    {
      "epoch": 2.274211351885203,
      "grad_norm": 2.135525941848755,
      "learning_rate": 7.284493916645095e-06,
      "loss": 1.3248,
      "step": 29360
    },
    {
      "epoch": 2.274985960223862,
      "grad_norm": 2.024752378463745,
      "learning_rate": 7.276727931659332e-06,
      "loss": 1.1955,
      "step": 29370
    },
    {
      "epoch": 2.2757605685625206,
      "grad_norm": 2.673847198486328,
      "learning_rate": 7.26896194667357e-06,
      "loss": 1.3022,
      "step": 29380
    },
    {
      "epoch": 2.2765351769011795,
      "grad_norm": 2.7158758640289307,
      "learning_rate": 7.261972560186383e-06,
      "loss": 1.2992,
      "step": 29390
    },
    {
      "epoch": 2.2773097852398383,
      "grad_norm": 2.952104330062866,
      "learning_rate": 7.254206575200622e-06,
      "loss": 1.2195,
      "step": 29400
    },
    {
      "epoch": 2.2780843935784967,
      "grad_norm": 2.9174389839172363,
      "learning_rate": 7.246440590214859e-06,
      "loss": 1.3293,
      "step": 29410
    },
    {
      "epoch": 2.2788590019171555,
      "grad_norm": 2.2561697959899902,
      "learning_rate": 7.238674605229097e-06,
      "loss": 1.1104,
      "step": 29420
    },
    {
      "epoch": 2.2796336102558143,
      "grad_norm": 2.585628032684326,
      "learning_rate": 7.230908620243334e-06,
      "loss": 1.3405,
      "step": 29430
    },
    {
      "epoch": 2.280408218594473,
      "grad_norm": 2.781831979751587,
      "learning_rate": 7.223142635257572e-06,
      "loss": 1.311,
      "step": 29440
    },
    {
      "epoch": 2.281182826933132,
      "grad_norm": 2.584632396697998,
      "learning_rate": 7.21537665027181e-06,
      "loss": 1.3043,
      "step": 29450
    },
    {
      "epoch": 2.281957435271791,
      "grad_norm": 3.325852155685425,
      "learning_rate": 7.2076106652860475e-06,
      "loss": 1.3978,
      "step": 29460
    },
    {
      "epoch": 2.2827320436104497,
      "grad_norm": 2.647435188293457,
      "learning_rate": 7.1998446803002846e-06,
      "loss": 1.1599,
      "step": 29470
    },
    {
      "epoch": 2.283506651949108,
      "grad_norm": 2.3140578269958496,
      "learning_rate": 7.1920786953145224e-06,
      "loss": 1.3143,
      "step": 29480
    },
    {
      "epoch": 2.284281260287767,
      "grad_norm": 2.4351983070373535,
      "learning_rate": 7.1843127103287595e-06,
      "loss": 1.2786,
      "step": 29490
    },
    {
      "epoch": 2.2850558686264257,
      "grad_norm": 2.50522518157959,
      "learning_rate": 7.176546725342998e-06,
      "loss": 1.1814,
      "step": 29500
    },
    {
      "epoch": 2.2858304769650846,
      "grad_norm": 2.4693210124969482,
      "learning_rate": 7.168780740357235e-06,
      "loss": 1.1797,
      "step": 29510
    },
    {
      "epoch": 2.2866050853037434,
      "grad_norm": 2.4271435737609863,
      "learning_rate": 7.161014755371473e-06,
      "loss": 1.2932,
      "step": 29520
    },
    {
      "epoch": 2.2873796936424022,
      "grad_norm": 2.141223907470703,
      "learning_rate": 7.153248770385711e-06,
      "loss": 1.1988,
      "step": 29530
    },
    {
      "epoch": 2.288154301981061,
      "grad_norm": 2.6150712966918945,
      "learning_rate": 7.145482785399949e-06,
      "loss": 1.2667,
      "step": 29540
    },
    {
      "epoch": 2.2889289103197195,
      "grad_norm": 2.2216336727142334,
      "learning_rate": 7.137716800414187e-06,
      "loss": 1.1451,
      "step": 29550
    },
    {
      "epoch": 2.2897035186583783,
      "grad_norm": 2.923957586288452,
      "learning_rate": 7.129950815428424e-06,
      "loss": 1.2219,
      "step": 29560
    },
    {
      "epoch": 2.290478126997037,
      "grad_norm": 2.522853374481201,
      "learning_rate": 7.1221848304426615e-06,
      "loss": 1.2446,
      "step": 29570
    },
    {
      "epoch": 2.291252735335696,
      "grad_norm": 2.3758742809295654,
      "learning_rate": 7.1144188454568986e-06,
      "loss": 1.1683,
      "step": 29580
    },
    {
      "epoch": 2.292027343674355,
      "grad_norm": 2.1992530822753906,
      "learning_rate": 7.106652860471137e-06,
      "loss": 1.149,
      "step": 29590
    },
    {
      "epoch": 2.292801952013013,
      "grad_norm": 3.2669365406036377,
      "learning_rate": 7.098886875485374e-06,
      "loss": 1.2271,
      "step": 29600
    },
    {
      "epoch": 2.293576560351672,
      "grad_norm": 2.2591845989227295,
      "learning_rate": 7.091120890499612e-06,
      "loss": 1.3,
      "step": 29610
    },
    {
      "epoch": 2.294351168690331,
      "grad_norm": 2.4797043800354004,
      "learning_rate": 7.083354905513849e-06,
      "loss": 1.2434,
      "step": 29620
    },
    {
      "epoch": 2.2951257770289897,
      "grad_norm": 2.2604756355285645,
      "learning_rate": 7.075588920528087e-06,
      "loss": 1.2786,
      "step": 29630
    },
    {
      "epoch": 2.2959003853676485,
      "grad_norm": 2.6415443420410156,
      "learning_rate": 7.067822935542325e-06,
      "loss": 1.304,
      "step": 29640
    },
    {
      "epoch": 2.2966749937063073,
      "grad_norm": 2.388235092163086,
      "learning_rate": 7.060056950556563e-06,
      "loss": 1.2873,
      "step": 29650
    },
    {
      "epoch": 2.297449602044966,
      "grad_norm": 2.5025229454040527,
      "learning_rate": 7.0522909655708e-06,
      "loss": 1.2498,
      "step": 29660
    },
    {
      "epoch": 2.2982242103836246,
      "grad_norm": 2.2028748989105225,
      "learning_rate": 7.044524980585038e-06,
      "loss": 1.2798,
      "step": 29670
    },
    {
      "epoch": 2.2989988187222834,
      "grad_norm": 2.100649833679199,
      "learning_rate": 7.036758995599275e-06,
      "loss": 1.2935,
      "step": 29680
    },
    {
      "epoch": 2.2997734270609422,
      "grad_norm": 2.4361846446990967,
      "learning_rate": 7.028993010613513e-06,
      "loss": 1.2584,
      "step": 29690
    },
    {
      "epoch": 2.300548035399601,
      "grad_norm": 2.376018762588501,
      "learning_rate": 7.02122702562775e-06,
      "loss": 1.1944,
      "step": 29700
    },
    {
      "epoch": 2.30132264373826,
      "grad_norm": 2.5857532024383545,
      "learning_rate": 7.013461040641988e-06,
      "loss": 1.263,
      "step": 29710
    },
    {
      "epoch": 2.3020972520769187,
      "grad_norm": 2.077881097793579,
      "learning_rate": 7.005695055656226e-06,
      "loss": 1.275,
      "step": 29720
    },
    {
      "epoch": 2.3028718604155776,
      "grad_norm": 2.24897837638855,
      "learning_rate": 6.997929070670463e-06,
      "loss": 1.2984,
      "step": 29730
    },
    {
      "epoch": 2.303646468754236,
      "grad_norm": 2.740328550338745,
      "learning_rate": 6.990163085684702e-06,
      "loss": 1.257,
      "step": 29740
    },
    {
      "epoch": 2.304421077092895,
      "grad_norm": 2.6791741847991943,
      "learning_rate": 6.982397100698939e-06,
      "loss": 1.2691,
      "step": 29750
    },
    {
      "epoch": 2.3051956854315536,
      "grad_norm": 2.5927178859710693,
      "learning_rate": 6.974631115713177e-06,
      "loss": 1.2025,
      "step": 29760
    },
    {
      "epoch": 2.3059702937702125,
      "grad_norm": 2.3851094245910645,
      "learning_rate": 6.966865130727414e-06,
      "loss": 1.2325,
      "step": 29770
    },
    {
      "epoch": 2.3067449021088713,
      "grad_norm": 2.5111217498779297,
      "learning_rate": 6.959099145741652e-06,
      "loss": 1.3336,
      "step": 29780
    },
    {
      "epoch": 2.30751951044753,
      "grad_norm": 2.035137414932251,
      "learning_rate": 6.9513331607558895e-06,
      "loss": 1.2312,
      "step": 29790
    },
    {
      "epoch": 2.308294118786189,
      "grad_norm": 3.325448513031006,
      "learning_rate": 6.943567175770127e-06,
      "loss": 1.2613,
      "step": 29800
    },
    {
      "epoch": 2.3090687271248473,
      "grad_norm": 2.385352373123169,
      "learning_rate": 6.935801190784364e-06,
      "loss": 1.2881,
      "step": 29810
    },
    {
      "epoch": 2.309843335463506,
      "grad_norm": 3.470940351486206,
      "learning_rate": 6.928035205798602e-06,
      "loss": 1.2245,
      "step": 29820
    },
    {
      "epoch": 2.310617943802165,
      "grad_norm": 2.574925422668457,
      "learning_rate": 6.920269220812839e-06,
      "loss": 1.1906,
      "step": 29830
    },
    {
      "epoch": 2.311392552140824,
      "grad_norm": 2.529350996017456,
      "learning_rate": 6.912503235827078e-06,
      "loss": 1.3602,
      "step": 29840
    },
    {
      "epoch": 2.3121671604794827,
      "grad_norm": 2.0388681888580322,
      "learning_rate": 6.904737250841315e-06,
      "loss": 1.2842,
      "step": 29850
    },
    {
      "epoch": 2.3129417688181415,
      "grad_norm": 2.6640849113464355,
      "learning_rate": 6.896971265855553e-06,
      "loss": 1.2587,
      "step": 29860
    },
    {
      "epoch": 2.3137163771568003,
      "grad_norm": 2.046135902404785,
      "learning_rate": 6.88920528086979e-06,
      "loss": 1.2543,
      "step": 29870
    },
    {
      "epoch": 2.3144909854954587,
      "grad_norm": 3.2595529556274414,
      "learning_rate": 6.881439295884028e-06,
      "loss": 1.1511,
      "step": 29880
    },
    {
      "epoch": 2.3152655938341176,
      "grad_norm": 2.8226418495178223,
      "learning_rate": 6.8736733108982665e-06,
      "loss": 1.2728,
      "step": 29890
    },
    {
      "epoch": 2.3160402021727764,
      "grad_norm": 2.1401009559631348,
      "learning_rate": 6.8659073259125035e-06,
      "loss": 1.1533,
      "step": 29900
    },
    {
      "epoch": 2.3168148105114352,
      "grad_norm": 2.0595438480377197,
      "learning_rate": 6.858141340926741e-06,
      "loss": 1.1324,
      "step": 29910
    },
    {
      "epoch": 2.317589418850094,
      "grad_norm": 2.816642999649048,
      "learning_rate": 6.850375355940978e-06,
      "loss": 1.2978,
      "step": 29920
    },
    {
      "epoch": 2.3183640271887525,
      "grad_norm": 2.2144525051116943,
      "learning_rate": 6.842609370955216e-06,
      "loss": 1.1001,
      "step": 29930
    },
    {
      "epoch": 2.3191386355274113,
      "grad_norm": 3.192476272583008,
      "learning_rate": 6.834843385969454e-06,
      "loss": 1.3144,
      "step": 29940
    },
    {
      "epoch": 2.31991324386607,
      "grad_norm": 2.3631532192230225,
      "learning_rate": 6.827077400983692e-06,
      "loss": 1.3219,
      "step": 29950
    },
    {
      "epoch": 2.320687852204729,
      "grad_norm": 2.5794146060943604,
      "learning_rate": 6.819311415997929e-06,
      "loss": 1.2829,
      "step": 29960
    },
    {
      "epoch": 2.321462460543388,
      "grad_norm": 2.6784465312957764,
      "learning_rate": 6.811545431012167e-06,
      "loss": 1.2338,
      "step": 29970
    },
    {
      "epoch": 2.3222370688820466,
      "grad_norm": 3.058105707168579,
      "learning_rate": 6.803779446026404e-06,
      "loss": 1.2484,
      "step": 29980
    },
    {
      "epoch": 2.3230116772207055,
      "grad_norm": 2.48129940032959,
      "learning_rate": 6.796013461040643e-06,
      "loss": 1.2709,
      "step": 29990
    },
    {
      "epoch": 2.323786285559364,
      "grad_norm": 2.388659715652466,
      "learning_rate": 6.78824747605488e-06,
      "loss": 1.216,
      "step": 30000
    },
    {
      "epoch": 2.3245608938980227,
      "grad_norm": 2.1423864364624023,
      "learning_rate": 6.7804814910691175e-06,
      "loss": 1.2038,
      "step": 30010
    },
    {
      "epoch": 2.3253355022366815,
      "grad_norm": 2.3828227519989014,
      "learning_rate": 6.7727155060833545e-06,
      "loss": 1.3133,
      "step": 30020
    },
    {
      "epoch": 2.3261101105753403,
      "grad_norm": 3.701016664505005,
      "learning_rate": 6.764949521097593e-06,
      "loss": 1.1927,
      "step": 30030
    },
    {
      "epoch": 2.326884718913999,
      "grad_norm": 2.6208996772766113,
      "learning_rate": 6.75718353611183e-06,
      "loss": 1.1206,
      "step": 30040
    },
    {
      "epoch": 2.327659327252658,
      "grad_norm": 2.7210233211517334,
      "learning_rate": 6.749417551126068e-06,
      "loss": 1.3338,
      "step": 30050
    },
    {
      "epoch": 2.328433935591317,
      "grad_norm": 3.459566593170166,
      "learning_rate": 6.741651566140305e-06,
      "loss": 1.2406,
      "step": 30060
    },
    {
      "epoch": 2.3292085439299752,
      "grad_norm": 2.2759170532226562,
      "learning_rate": 6.733885581154543e-06,
      "loss": 1.2539,
      "step": 30070
    },
    {
      "epoch": 2.329983152268634,
      "grad_norm": 2.3362929821014404,
      "learning_rate": 6.726119596168782e-06,
      "loss": 1.2556,
      "step": 30080
    },
    {
      "epoch": 2.330757760607293,
      "grad_norm": 2.121433734893799,
      "learning_rate": 6.718353611183019e-06,
      "loss": 1.2548,
      "step": 30090
    },
    {
      "epoch": 2.3315323689459517,
      "grad_norm": 2.7928736209869385,
      "learning_rate": 6.710587626197257e-06,
      "loss": 1.1635,
      "step": 30100
    },
    {
      "epoch": 2.3323069772846106,
      "grad_norm": 4.14199161529541,
      "learning_rate": 6.702821641211494e-06,
      "loss": 1.2882,
      "step": 30110
    },
    {
      "epoch": 2.3330815856232694,
      "grad_norm": 2.5458836555480957,
      "learning_rate": 6.6950556562257315e-06,
      "loss": 1.2831,
      "step": 30120
    },
    {
      "epoch": 2.3338561939619282,
      "grad_norm": 2.4724903106689453,
      "learning_rate": 6.687289671239969e-06,
      "loss": 1.2152,
      "step": 30130
    },
    {
      "epoch": 2.3346308023005866,
      "grad_norm": 2.138979911804199,
      "learning_rate": 6.679523686254207e-06,
      "loss": 1.2979,
      "step": 30140
    },
    {
      "epoch": 2.3354054106392454,
      "grad_norm": 2.4377663135528564,
      "learning_rate": 6.671757701268444e-06,
      "loss": 1.2399,
      "step": 30150
    },
    {
      "epoch": 2.3361800189779043,
      "grad_norm": 2.785179615020752,
      "learning_rate": 6.663991716282682e-06,
      "loss": 1.2637,
      "step": 30160
    },
    {
      "epoch": 2.336954627316563,
      "grad_norm": 2.7749743461608887,
      "learning_rate": 6.656225731296919e-06,
      "loss": 1.2234,
      "step": 30170
    },
    {
      "epoch": 2.337729235655222,
      "grad_norm": 3.3102707862854004,
      "learning_rate": 6.648459746311158e-06,
      "loss": 1.2312,
      "step": 30180
    },
    {
      "epoch": 2.338503843993881,
      "grad_norm": 3.616987943649292,
      "learning_rate": 6.640693761325395e-06,
      "loss": 1.2436,
      "step": 30190
    },
    {
      "epoch": 2.339278452332539,
      "grad_norm": 2.2575185298919678,
      "learning_rate": 6.632927776339633e-06,
      "loss": 1.2295,
      "step": 30200
    },
    {
      "epoch": 2.340053060671198,
      "grad_norm": 2.3468503952026367,
      "learning_rate": 6.62516179135387e-06,
      "loss": 1.2993,
      "step": 30210
    },
    {
      "epoch": 2.340827669009857,
      "grad_norm": 2.5046238899230957,
      "learning_rate": 6.617395806368108e-06,
      "loss": 1.275,
      "step": 30220
    },
    {
      "epoch": 2.3416022773485157,
      "grad_norm": 2.407923936843872,
      "learning_rate": 6.6096298213823455e-06,
      "loss": 1.1697,
      "step": 30230
    },
    {
      "epoch": 2.3423768856871745,
      "grad_norm": 2.1724908351898193,
      "learning_rate": 6.601863836396583e-06,
      "loss": 1.2036,
      "step": 30240
    },
    {
      "epoch": 2.3431514940258333,
      "grad_norm": 4.104913234710693,
      "learning_rate": 6.594097851410821e-06,
      "loss": 1.2243,
      "step": 30250
    },
    {
      "epoch": 2.3439261023644917,
      "grad_norm": 2.2703306674957275,
      "learning_rate": 6.586331866425058e-06,
      "loss": 1.242,
      "step": 30260
    },
    {
      "epoch": 2.3447007107031506,
      "grad_norm": 2.4845480918884277,
      "learning_rate": 6.578565881439296e-06,
      "loss": 1.165,
      "step": 30270
    },
    {
      "epoch": 2.3454753190418094,
      "grad_norm": 2.3898277282714844,
      "learning_rate": 6.570799896453534e-06,
      "loss": 1.2703,
      "step": 30280
    },
    {
      "epoch": 2.3462499273804682,
      "grad_norm": 2.4183385372161865,
      "learning_rate": 6.563033911467772e-06,
      "loss": 1.2556,
      "step": 30290
    },
    {
      "epoch": 2.347024535719127,
      "grad_norm": 2.162970781326294,
      "learning_rate": 6.555267926482009e-06,
      "loss": 1.2234,
      "step": 30300
    },
    {
      "epoch": 2.347799144057786,
      "grad_norm": 2.0412607192993164,
      "learning_rate": 6.547501941496247e-06,
      "loss": 1.2125,
      "step": 30310
    },
    {
      "epoch": 2.3485737523964447,
      "grad_norm": 2.675217628479004,
      "learning_rate": 6.539735956510484e-06,
      "loss": 1.2588,
      "step": 30320
    },
    {
      "epoch": 2.349348360735103,
      "grad_norm": 1.95456862449646,
      "learning_rate": 6.531969971524722e-06,
      "loss": 1.2879,
      "step": 30330
    },
    {
      "epoch": 2.350122969073762,
      "grad_norm": 2.927403688430786,
      "learning_rate": 6.5242039865389594e-06,
      "loss": 1.2527,
      "step": 30340
    },
    {
      "epoch": 2.350897577412421,
      "grad_norm": 2.3941125869750977,
      "learning_rate": 6.516438001553197e-06,
      "loss": 1.243,
      "step": 30350
    },
    {
      "epoch": 2.3516721857510796,
      "grad_norm": 2.920945644378662,
      "learning_rate": 6.508672016567434e-06,
      "loss": 1.2251,
      "step": 30360
    },
    {
      "epoch": 2.3524467940897384,
      "grad_norm": 2.195490598678589,
      "learning_rate": 6.500906031581672e-06,
      "loss": 1.3285,
      "step": 30370
    },
    {
      "epoch": 2.3532214024283973,
      "grad_norm": 2.2507197856903076,
      "learning_rate": 6.49314004659591e-06,
      "loss": 1.2641,
      "step": 30380
    },
    {
      "epoch": 2.353996010767056,
      "grad_norm": 2.2507574558258057,
      "learning_rate": 6.485374061610148e-06,
      "loss": 1.2122,
      "step": 30390
    },
    {
      "epoch": 2.3547706191057145,
      "grad_norm": 2.198465585708618,
      "learning_rate": 6.477608076624385e-06,
      "loss": 1.2585,
      "step": 30400
    },
    {
      "epoch": 2.3555452274443733,
      "grad_norm": 2.415081024169922,
      "learning_rate": 6.469842091638623e-06,
      "loss": 1.185,
      "step": 30410
    },
    {
      "epoch": 2.356319835783032,
      "grad_norm": 2.431230306625366,
      "learning_rate": 6.462076106652861e-06,
      "loss": 1.3524,
      "step": 30420
    },
    {
      "epoch": 2.357094444121691,
      "grad_norm": 2.3594911098480225,
      "learning_rate": 6.4543101216670985e-06,
      "loss": 1.3216,
      "step": 30430
    },
    {
      "epoch": 2.35786905246035,
      "grad_norm": 2.618131160736084,
      "learning_rate": 6.446544136681336e-06,
      "loss": 1.2146,
      "step": 30440
    },
    {
      "epoch": 2.3586436607990087,
      "grad_norm": 2.6344165802001953,
      "learning_rate": 6.438778151695573e-06,
      "loss": 1.2261,
      "step": 30450
    },
    {
      "epoch": 2.3594182691376675,
      "grad_norm": 2.7658145427703857,
      "learning_rate": 6.431012166709811e-06,
      "loss": 1.2043,
      "step": 30460
    },
    {
      "epoch": 2.360192877476326,
      "grad_norm": 2.6601614952087402,
      "learning_rate": 6.423246181724048e-06,
      "loss": 1.3411,
      "step": 30470
    },
    {
      "epoch": 2.3609674858149847,
      "grad_norm": 3.6061418056488037,
      "learning_rate": 6.415480196738287e-06,
      "loss": 1.2643,
      "step": 30480
    },
    {
      "epoch": 2.3617420941536436,
      "grad_norm": 2.123187780380249,
      "learning_rate": 6.407714211752524e-06,
      "loss": 1.1883,
      "step": 30490
    },
    {
      "epoch": 2.3625167024923024,
      "grad_norm": 2.6933562755584717,
      "learning_rate": 6.399948226766762e-06,
      "loss": 1.2414,
      "step": 30500
    },
    {
      "epoch": 2.3632913108309612,
      "grad_norm": 2.24826979637146,
      "learning_rate": 6.392182241780999e-06,
      "loss": 1.2419,
      "step": 30510
    },
    {
      "epoch": 2.3640659191696196,
      "grad_norm": 2.2088418006896973,
      "learning_rate": 6.384416256795238e-06,
      "loss": 1.2331,
      "step": 30520
    },
    {
      "epoch": 2.3648405275082784,
      "grad_norm": 2.626429557800293,
      "learning_rate": 6.376650271809475e-06,
      "loss": 1.3518,
      "step": 30530
    },
    {
      "epoch": 2.3656151358469373,
      "grad_norm": 3.0909013748168945,
      "learning_rate": 6.3688842868237125e-06,
      "loss": 1.3028,
      "step": 30540
    },
    {
      "epoch": 2.366389744185596,
      "grad_norm": 3.089257001876831,
      "learning_rate": 6.3611183018379495e-06,
      "loss": 1.1733,
      "step": 30550
    },
    {
      "epoch": 2.367164352524255,
      "grad_norm": 2.3059818744659424,
      "learning_rate": 6.353352316852187e-06,
      "loss": 1.1424,
      "step": 30560
    },
    {
      "epoch": 2.3679389608629138,
      "grad_norm": 3.3241167068481445,
      "learning_rate": 6.345586331866425e-06,
      "loss": 1.265,
      "step": 30570
    },
    {
      "epoch": 2.3687135692015726,
      "grad_norm": 2.348353624343872,
      "learning_rate": 6.337820346880663e-06,
      "loss": 1.2665,
      "step": 30580
    },
    {
      "epoch": 2.369488177540231,
      "grad_norm": 2.3759074211120605,
      "learning_rate": 6.3300543618949e-06,
      "loss": 1.3753,
      "step": 30590
    },
    {
      "epoch": 2.37026278587889,
      "grad_norm": 2.3213987350463867,
      "learning_rate": 6.322288376909138e-06,
      "loss": 1.1105,
      "step": 30600
    },
    {
      "epoch": 2.3710373942175487,
      "grad_norm": 2.2404048442840576,
      "learning_rate": 6.314522391923376e-06,
      "loss": 1.3808,
      "step": 30610
    },
    {
      "epoch": 2.3718120025562075,
      "grad_norm": 2.0440382957458496,
      "learning_rate": 6.306756406937614e-06,
      "loss": 1.2516,
      "step": 30620
    },
    {
      "epoch": 2.3725866108948663,
      "grad_norm": 2.4315590858459473,
      "learning_rate": 6.298990421951852e-06,
      "loss": 1.2656,
      "step": 30630
    },
    {
      "epoch": 2.373361219233525,
      "grad_norm": 2.0247199535369873,
      "learning_rate": 6.291224436966089e-06,
      "loss": 1.2532,
      "step": 30640
    },
    {
      "epoch": 2.374135827572184,
      "grad_norm": 2.350066661834717,
      "learning_rate": 6.2834584519803265e-06,
      "loss": 1.3573,
      "step": 30650
    },
    {
      "epoch": 2.3749104359108424,
      "grad_norm": 2.7150447368621826,
      "learning_rate": 6.2756924669945635e-06,
      "loss": 1.2858,
      "step": 30660
    },
    {
      "epoch": 2.375685044249501,
      "grad_norm": 2.6219024658203125,
      "learning_rate": 6.267926482008802e-06,
      "loss": 1.2281,
      "step": 30670
    },
    {
      "epoch": 2.37645965258816,
      "grad_norm": 1.7919741868972778,
      "learning_rate": 6.260160497023039e-06,
      "loss": 1.3248,
      "step": 30680
    },
    {
      "epoch": 2.377234260926819,
      "grad_norm": 2.340989589691162,
      "learning_rate": 6.252394512037277e-06,
      "loss": 1.2068,
      "step": 30690
    },
    {
      "epoch": 2.3780088692654777,
      "grad_norm": 2.4681057929992676,
      "learning_rate": 6.244628527051514e-06,
      "loss": 1.2693,
      "step": 30700
    },
    {
      "epoch": 2.3787834776041366,
      "grad_norm": 2.2923014163970947,
      "learning_rate": 6.236862542065752e-06,
      "loss": 1.3579,
      "step": 30710
    },
    {
      "epoch": 2.3795580859427954,
      "grad_norm": 3.431640148162842,
      "learning_rate": 6.22909655707999e-06,
      "loss": 1.2143,
      "step": 30720
    },
    {
      "epoch": 2.3803326942814538,
      "grad_norm": 2.3441309928894043,
      "learning_rate": 6.221330572094228e-06,
      "loss": 1.2179,
      "step": 30730
    },
    {
      "epoch": 2.3811073026201126,
      "grad_norm": 2.7396585941314697,
      "learning_rate": 6.213564587108465e-06,
      "loss": 1.1905,
      "step": 30740
    },
    {
      "epoch": 2.3818819109587714,
      "grad_norm": 2.9492993354797363,
      "learning_rate": 6.205798602122703e-06,
      "loss": 1.2064,
      "step": 30750
    },
    {
      "epoch": 2.3826565192974303,
      "grad_norm": 2.103322744369507,
      "learning_rate": 6.19803261713694e-06,
      "loss": 1.2717,
      "step": 30760
    },
    {
      "epoch": 2.383431127636089,
      "grad_norm": 2.8368866443634033,
      "learning_rate": 6.190266632151178e-06,
      "loss": 1.265,
      "step": 30770
    },
    {
      "epoch": 2.384205735974748,
      "grad_norm": 2.1877806186676025,
      "learning_rate": 6.182500647165415e-06,
      "loss": 1.2719,
      "step": 30780
    },
    {
      "epoch": 2.3849803443134068,
      "grad_norm": 2.7799923419952393,
      "learning_rate": 6.174734662179653e-06,
      "loss": 1.2516,
      "step": 30790
    },
    {
      "epoch": 2.385754952652065,
      "grad_norm": 2.610222339630127,
      "learning_rate": 6.166968677193891e-06,
      "loss": 1.3159,
      "step": 30800
    },
    {
      "epoch": 2.386529560990724,
      "grad_norm": 2.2836108207702637,
      "learning_rate": 6.159202692208128e-06,
      "loss": 1.3498,
      "step": 30810
    },
    {
      "epoch": 2.387304169329383,
      "grad_norm": 3.101923704147339,
      "learning_rate": 6.151436707222367e-06,
      "loss": 1.2245,
      "step": 30820
    },
    {
      "epoch": 2.3880787776680417,
      "grad_norm": 2.3497660160064697,
      "learning_rate": 6.143670722236604e-06,
      "loss": 1.2292,
      "step": 30830
    },
    {
      "epoch": 2.3888533860067005,
      "grad_norm": 2.507072687149048,
      "learning_rate": 6.135904737250842e-06,
      "loss": 1.3459,
      "step": 30840
    },
    {
      "epoch": 2.389627994345359,
      "grad_norm": 2.5695295333862305,
      "learning_rate": 6.128138752265079e-06,
      "loss": 1.3056,
      "step": 30850
    },
    {
      "epoch": 2.3904026026840177,
      "grad_norm": 1.8184269666671753,
      "learning_rate": 6.120372767279317e-06,
      "loss": 1.1638,
      "step": 30860
    },
    {
      "epoch": 2.3911772110226766,
      "grad_norm": 2.4729630947113037,
      "learning_rate": 6.1126067822935545e-06,
      "loss": 1.1858,
      "step": 30870
    },
    {
      "epoch": 2.3919518193613354,
      "grad_norm": 3.093022346496582,
      "learning_rate": 6.104840797307792e-06,
      "loss": 1.231,
      "step": 30880
    },
    {
      "epoch": 2.392726427699994,
      "grad_norm": 2.638777017593384,
      "learning_rate": 6.097074812322029e-06,
      "loss": 1.0966,
      "step": 30890
    },
    {
      "epoch": 2.393501036038653,
      "grad_norm": 2.3332715034484863,
      "learning_rate": 6.089308827336267e-06,
      "loss": 1.2417,
      "step": 30900
    },
    {
      "epoch": 2.394275644377312,
      "grad_norm": 3.158158779144287,
      "learning_rate": 6.081542842350504e-06,
      "loss": 1.3303,
      "step": 30910
    },
    {
      "epoch": 2.3950502527159703,
      "grad_norm": 2.8890702724456787,
      "learning_rate": 6.073776857364743e-06,
      "loss": 1.1492,
      "step": 30920
    },
    {
      "epoch": 2.395824861054629,
      "grad_norm": 2.425696611404419,
      "learning_rate": 6.06601087237898e-06,
      "loss": 1.2734,
      "step": 30930
    },
    {
      "epoch": 2.396599469393288,
      "grad_norm": 1.9114714860916138,
      "learning_rate": 6.058244887393218e-06,
      "loss": 1.2269,
      "step": 30940
    },
    {
      "epoch": 2.3973740777319468,
      "grad_norm": 2.775106430053711,
      "learning_rate": 6.050478902407455e-06,
      "loss": 1.2542,
      "step": 30950
    },
    {
      "epoch": 2.3981486860706056,
      "grad_norm": 3.164353847503662,
      "learning_rate": 6.042712917421693e-06,
      "loss": 1.235,
      "step": 30960
    },
    {
      "epoch": 2.3989232944092644,
      "grad_norm": 2.4307210445404053,
      "learning_rate": 6.0349469324359314e-06,
      "loss": 1.1909,
      "step": 30970
    },
    {
      "epoch": 2.3996979027479233,
      "grad_norm": 1.9620267152786255,
      "learning_rate": 6.0271809474501685e-06,
      "loss": 1.3241,
      "step": 30980
    },
    {
      "epoch": 2.4004725110865817,
      "grad_norm": 2.4797749519348145,
      "learning_rate": 6.019414962464406e-06,
      "loss": 1.2719,
      "step": 30990
    },
    {
      "epoch": 2.4012471194252405,
      "grad_norm": 2.222811222076416,
      "learning_rate": 6.011648977478643e-06,
      "loss": 1.2484,
      "step": 31000
    },
    {
      "epoch": 2.4020217277638993,
      "grad_norm": 2.274951457977295,
      "learning_rate": 6.003882992492881e-06,
      "loss": 1.2843,
      "step": 31010
    },
    {
      "epoch": 2.402796336102558,
      "grad_norm": 2.7672674655914307,
      "learning_rate": 5.996117007507119e-06,
      "loss": 1.2556,
      "step": 31020
    },
    {
      "epoch": 2.403570944441217,
      "grad_norm": 1.9439607858657837,
      "learning_rate": 5.988351022521357e-06,
      "loss": 1.2699,
      "step": 31030
    },
    {
      "epoch": 2.404345552779876,
      "grad_norm": 2.356936454772949,
      "learning_rate": 5.980585037535594e-06,
      "loss": 1.236,
      "step": 31040
    },
    {
      "epoch": 2.4051201611185347,
      "grad_norm": 2.0133872032165527,
      "learning_rate": 5.972819052549832e-06,
      "loss": 1.3219,
      "step": 31050
    },
    {
      "epoch": 2.405894769457193,
      "grad_norm": 2.6131069660186768,
      "learning_rate": 5.96505306756407e-06,
      "loss": 1.2811,
      "step": 31060
    },
    {
      "epoch": 2.406669377795852,
      "grad_norm": 2.789700984954834,
      "learning_rate": 5.9572870825783076e-06,
      "loss": 1.2308,
      "step": 31070
    },
    {
      "epoch": 2.4074439861345107,
      "grad_norm": 2.5017356872558594,
      "learning_rate": 5.949521097592545e-06,
      "loss": 1.2165,
      "step": 31080
    },
    {
      "epoch": 2.4082185944731695,
      "grad_norm": 2.1566309928894043,
      "learning_rate": 5.9417551126067825e-06,
      "loss": 1.2244,
      "step": 31090
    },
    {
      "epoch": 2.4089932028118284,
      "grad_norm": 2.7139530181884766,
      "learning_rate": 5.9339891276210195e-06,
      "loss": 1.2446,
      "step": 31100
    },
    {
      "epoch": 2.409767811150487,
      "grad_norm": 3.073314666748047,
      "learning_rate": 5.926223142635258e-06,
      "loss": 1.3151,
      "step": 31110
    },
    {
      "epoch": 2.410542419489146,
      "grad_norm": 2.775336980819702,
      "learning_rate": 5.918457157649495e-06,
      "loss": 1.2578,
      "step": 31120
    },
    {
      "epoch": 2.4113170278278044,
      "grad_norm": 3.132715940475464,
      "learning_rate": 5.910691172663733e-06,
      "loss": 1.1807,
      "step": 31130
    },
    {
      "epoch": 2.4120916361664633,
      "grad_norm": 2.3890130519866943,
      "learning_rate": 5.902925187677971e-06,
      "loss": 1.2025,
      "step": 31140
    },
    {
      "epoch": 2.412866244505122,
      "grad_norm": 4.2013258934021,
      "learning_rate": 5.895159202692208e-06,
      "loss": 1.2853,
      "step": 31150
    },
    {
      "epoch": 2.413640852843781,
      "grad_norm": 2.1904618740081787,
      "learning_rate": 5.887393217706447e-06,
      "loss": 1.2338,
      "step": 31160
    },
    {
      "epoch": 2.4144154611824398,
      "grad_norm": 3.0574560165405273,
      "learning_rate": 5.879627232720684e-06,
      "loss": 1.1997,
      "step": 31170
    },
    {
      "epoch": 2.415190069521098,
      "grad_norm": 2.3116888999938965,
      "learning_rate": 5.8718612477349216e-06,
      "loss": 1.1845,
      "step": 31180
    },
    {
      "epoch": 2.415964677859757,
      "grad_norm": 3.236905097961426,
      "learning_rate": 5.8640952627491586e-06,
      "loss": 1.2247,
      "step": 31190
    },
    {
      "epoch": 2.416739286198416,
      "grad_norm": 2.7108981609344482,
      "learning_rate": 5.8563292777633964e-06,
      "loss": 1.2358,
      "step": 31200
    },
    {
      "epoch": 2.4175138945370747,
      "grad_norm": 2.32261061668396,
      "learning_rate": 5.848563292777634e-06,
      "loss": 1.2781,
      "step": 31210
    },
    {
      "epoch": 2.4182885028757335,
      "grad_norm": 2.7443904876708984,
      "learning_rate": 5.840797307791872e-06,
      "loss": 1.1999,
      "step": 31220
    },
    {
      "epoch": 2.4190631112143923,
      "grad_norm": 3.629014015197754,
      "learning_rate": 5.833031322806109e-06,
      "loss": 1.3063,
      "step": 31230
    },
    {
      "epoch": 2.419837719553051,
      "grad_norm": 2.4369304180145264,
      "learning_rate": 5.825265337820347e-06,
      "loss": 1.1764,
      "step": 31240
    },
    {
      "epoch": 2.4206123278917095,
      "grad_norm": 3.432605743408203,
      "learning_rate": 5.817499352834584e-06,
      "loss": 1.2391,
      "step": 31250
    },
    {
      "epoch": 2.4213869362303684,
      "grad_norm": 2.9036903381347656,
      "learning_rate": 5.809733367848823e-06,
      "loss": 1.233,
      "step": 31260
    },
    {
      "epoch": 2.422161544569027,
      "grad_norm": 2.416149139404297,
      "learning_rate": 5.80196738286306e-06,
      "loss": 1.2611,
      "step": 31270
    },
    {
      "epoch": 2.422936152907686,
      "grad_norm": 1.69282865524292,
      "learning_rate": 5.794201397877298e-06,
      "loss": 1.2827,
      "step": 31280
    },
    {
      "epoch": 2.423710761246345,
      "grad_norm": 2.9049456119537354,
      "learning_rate": 5.786435412891535e-06,
      "loss": 1.1036,
      "step": 31290
    },
    {
      "epoch": 2.4244853695850037,
      "grad_norm": 2.442075490951538,
      "learning_rate": 5.7786694279057726e-06,
      "loss": 1.2597,
      "step": 31300
    },
    {
      "epoch": 2.4252599779236625,
      "grad_norm": 2.9356489181518555,
      "learning_rate": 5.7709034429200104e-06,
      "loss": 1.2863,
      "step": 31310
    },
    {
      "epoch": 2.426034586262321,
      "grad_norm": 2.217329740524292,
      "learning_rate": 5.763137457934248e-06,
      "loss": 1.2864,
      "step": 31320
    },
    {
      "epoch": 2.4268091946009798,
      "grad_norm": 2.8148410320281982,
      "learning_rate": 5.755371472948486e-06,
      "loss": 1.248,
      "step": 31330
    },
    {
      "epoch": 2.4275838029396386,
      "grad_norm": 3.000615119934082,
      "learning_rate": 5.747605487962723e-06,
      "loss": 1.2649,
      "step": 31340
    },
    {
      "epoch": 2.4283584112782974,
      "grad_norm": 2.539184093475342,
      "learning_rate": 5.739839502976961e-06,
      "loss": 1.2118,
      "step": 31350
    },
    {
      "epoch": 2.4291330196169563,
      "grad_norm": 2.443930149078369,
      "learning_rate": 5.732073517991199e-06,
      "loss": 1.26,
      "step": 31360
    },
    {
      "epoch": 2.429907627955615,
      "grad_norm": 3.157036304473877,
      "learning_rate": 5.724307533005437e-06,
      "loss": 1.242,
      "step": 31370
    },
    {
      "epoch": 2.430682236294274,
      "grad_norm": 2.8246688842773438,
      "learning_rate": 5.716541548019674e-06,
      "loss": 1.2622,
      "step": 31380
    },
    {
      "epoch": 2.4314568446329323,
      "grad_norm": 2.081383228302002,
      "learning_rate": 5.708775563033912e-06,
      "loss": 1.2539,
      "step": 31390
    },
    {
      "epoch": 2.432231452971591,
      "grad_norm": 2.0185675621032715,
      "learning_rate": 5.701009578048149e-06,
      "loss": 1.2588,
      "step": 31400
    },
    {
      "epoch": 2.43300606131025,
      "grad_norm": 3.3848867416381836,
      "learning_rate": 5.693243593062387e-06,
      "loss": 1.2139,
      "step": 31410
    },
    {
      "epoch": 2.433780669648909,
      "grad_norm": 3.6176140308380127,
      "learning_rate": 5.685477608076624e-06,
      "loss": 1.1608,
      "step": 31420
    },
    {
      "epoch": 2.4345552779875677,
      "grad_norm": 2.3042495250701904,
      "learning_rate": 5.677711623090862e-06,
      "loss": 1.2253,
      "step": 31430
    },
    {
      "epoch": 2.4353298863262265,
      "grad_norm": 3.296192169189453,
      "learning_rate": 5.669945638105099e-06,
      "loss": 1.1339,
      "step": 31440
    },
    {
      "epoch": 2.436104494664885,
      "grad_norm": 3.164353370666504,
      "learning_rate": 5.662179653119337e-06,
      "loss": 1.2423,
      "step": 31450
    },
    {
      "epoch": 2.4368791030035437,
      "grad_norm": 2.699310064315796,
      "learning_rate": 5.654413668133575e-06,
      "loss": 1.3086,
      "step": 31460
    },
    {
      "epoch": 2.4376537113422025,
      "grad_norm": 1.988004446029663,
      "learning_rate": 5.646647683147813e-06,
      "loss": 1.3627,
      "step": 31470
    },
    {
      "epoch": 2.4384283196808614,
      "grad_norm": 2.6822378635406494,
      "learning_rate": 5.63888169816205e-06,
      "loss": 1.3083,
      "step": 31480
    },
    {
      "epoch": 2.43920292801952,
      "grad_norm": 2.127394676208496,
      "learning_rate": 5.631115713176288e-06,
      "loss": 1.2824,
      "step": 31490
    },
    {
      "epoch": 2.439977536358179,
      "grad_norm": 2.467471122741699,
      "learning_rate": 5.623349728190526e-06,
      "loss": 1.1926,
      "step": 31500
    },
    {
      "epoch": 2.4407521446968374,
      "grad_norm": 1.9154199361801147,
      "learning_rate": 5.6155837432047635e-06,
      "loss": 1.2031,
      "step": 31510
    },
    {
      "epoch": 2.4415267530354963,
      "grad_norm": 3.0240581035614014,
      "learning_rate": 5.607817758219001e-06,
      "loss": 1.2146,
      "step": 31520
    },
    {
      "epoch": 2.442301361374155,
      "grad_norm": 2.3896477222442627,
      "learning_rate": 5.600051773233238e-06,
      "loss": 1.1652,
      "step": 31530
    },
    {
      "epoch": 2.443075969712814,
      "grad_norm": 2.137619733810425,
      "learning_rate": 5.592285788247476e-06,
      "loss": 1.2798,
      "step": 31540
    },
    {
      "epoch": 2.4438505780514728,
      "grad_norm": 1.7989604473114014,
      "learning_rate": 5.584519803261714e-06,
      "loss": 1.182,
      "step": 31550
    },
    {
      "epoch": 2.4446251863901316,
      "grad_norm": 2.8966920375823975,
      "learning_rate": 5.576753818275952e-06,
      "loss": 1.2577,
      "step": 31560
    },
    {
      "epoch": 2.4453997947287904,
      "grad_norm": 1.8490959405899048,
      "learning_rate": 5.568987833290189e-06,
      "loss": 1.2365,
      "step": 31570
    },
    {
      "epoch": 2.446174403067449,
      "grad_norm": 2.2016122341156006,
      "learning_rate": 5.561998446803002e-06,
      "loss": 1.3436,
      "step": 31580
    },
    {
      "epoch": 2.4469490114061077,
      "grad_norm": 2.7361443042755127,
      "learning_rate": 5.554232461817241e-06,
      "loss": 1.241,
      "step": 31590
    },
    {
      "epoch": 2.4477236197447665,
      "grad_norm": 2.1681666374206543,
      "learning_rate": 5.546466476831478e-06,
      "loss": 1.2838,
      "step": 31600
    },
    {
      "epoch": 2.4484982280834253,
      "grad_norm": 3.054924726486206,
      "learning_rate": 5.538700491845716e-06,
      "loss": 1.3012,
      "step": 31610
    },
    {
      "epoch": 2.449272836422084,
      "grad_norm": 2.896524429321289,
      "learning_rate": 5.530934506859953e-06,
      "loss": 1.1885,
      "step": 31620
    },
    {
      "epoch": 2.450047444760743,
      "grad_norm": 2.1940343379974365,
      "learning_rate": 5.523168521874191e-06,
      "loss": 1.378,
      "step": 31630
    },
    {
      "epoch": 2.450822053099402,
      "grad_norm": 3.395259141921997,
      "learning_rate": 5.51540253688843e-06,
      "loss": 1.3081,
      "step": 31640
    },
    {
      "epoch": 2.45159666143806,
      "grad_norm": 2.2971067428588867,
      "learning_rate": 5.507636551902667e-06,
      "loss": 1.2168,
      "step": 31650
    },
    {
      "epoch": 2.452371269776719,
      "grad_norm": 2.1356472969055176,
      "learning_rate": 5.4998705669169045e-06,
      "loss": 1.3668,
      "step": 31660
    },
    {
      "epoch": 2.453145878115378,
      "grad_norm": 2.7022016048431396,
      "learning_rate": 5.4921045819311415e-06,
      "loss": 1.2214,
      "step": 31670
    },
    {
      "epoch": 2.4539204864540367,
      "grad_norm": 2.0999162197113037,
      "learning_rate": 5.484338596945379e-06,
      "loss": 1.3022,
      "step": 31680
    },
    {
      "epoch": 2.4546950947926955,
      "grad_norm": 3.0291709899902344,
      "learning_rate": 5.476572611959617e-06,
      "loss": 1.2032,
      "step": 31690
    },
    {
      "epoch": 2.4554697031313544,
      "grad_norm": 2.8759329319000244,
      "learning_rate": 5.468806626973855e-06,
      "loss": 1.2276,
      "step": 31700
    },
    {
      "epoch": 2.456244311470013,
      "grad_norm": 3.3817994594573975,
      "learning_rate": 5.461040641988092e-06,
      "loss": 1.2378,
      "step": 31710
    },
    {
      "epoch": 2.4570189198086716,
      "grad_norm": 2.4063527584075928,
      "learning_rate": 5.45327465700233e-06,
      "loss": 1.265,
      "step": 31720
    },
    {
      "epoch": 2.4577935281473304,
      "grad_norm": 2.4764416217803955,
      "learning_rate": 5.445508672016567e-06,
      "loss": 1.2391,
      "step": 31730
    },
    {
      "epoch": 2.4585681364859893,
      "grad_norm": 2.1042909622192383,
      "learning_rate": 5.437742687030806e-06,
      "loss": 1.2337,
      "step": 31740
    },
    {
      "epoch": 2.459342744824648,
      "grad_norm": 2.223109722137451,
      "learning_rate": 5.429976702045043e-06,
      "loss": 1.2438,
      "step": 31750
    },
    {
      "epoch": 2.460117353163307,
      "grad_norm": 2.3135299682617188,
      "learning_rate": 5.422210717059281e-06,
      "loss": 1.3123,
      "step": 31760
    },
    {
      "epoch": 2.4608919615019653,
      "grad_norm": 2.718635320663452,
      "learning_rate": 5.414444732073518e-06,
      "loss": 1.2322,
      "step": 31770
    },
    {
      "epoch": 2.461666569840624,
      "grad_norm": 2.111321449279785,
      "learning_rate": 5.4066787470877555e-06,
      "loss": 1.1779,
      "step": 31780
    },
    {
      "epoch": 2.462441178179283,
      "grad_norm": 2.547513246536255,
      "learning_rate": 5.398912762101993e-06,
      "loss": 1.2896,
      "step": 31790
    },
    {
      "epoch": 2.463215786517942,
      "grad_norm": 3.400272846221924,
      "learning_rate": 5.391146777116231e-06,
      "loss": 1.189,
      "step": 31800
    },
    {
      "epoch": 2.4639903948566007,
      "grad_norm": 2.3486881256103516,
      "learning_rate": 5.383380792130469e-06,
      "loss": 1.2392,
      "step": 31810
    },
    {
      "epoch": 2.4647650031952595,
      "grad_norm": 2.931342601776123,
      "learning_rate": 5.375614807144706e-06,
      "loss": 1.2403,
      "step": 31820
    },
    {
      "epoch": 2.4655396115339183,
      "grad_norm": 2.3186099529266357,
      "learning_rate": 5.367848822158944e-06,
      "loss": 1.2769,
      "step": 31830
    },
    {
      "epoch": 2.4663142198725767,
      "grad_norm": 2.572305202484131,
      "learning_rate": 5.360082837173182e-06,
      "loss": 1.2767,
      "step": 31840
    },
    {
      "epoch": 2.4670888282112355,
      "grad_norm": 2.785332441329956,
      "learning_rate": 5.35231685218742e-06,
      "loss": 1.2766,
      "step": 31850
    },
    {
      "epoch": 2.4678634365498944,
      "grad_norm": 2.717177391052246,
      "learning_rate": 5.344550867201657e-06,
      "loss": 1.2985,
      "step": 31860
    },
    {
      "epoch": 2.468638044888553,
      "grad_norm": 3.032308578491211,
      "learning_rate": 5.336784882215895e-06,
      "loss": 1.2984,
      "step": 31870
    },
    {
      "epoch": 2.469412653227212,
      "grad_norm": 3.7260754108428955,
      "learning_rate": 5.329018897230132e-06,
      "loss": 1.2654,
      "step": 31880
    },
    {
      "epoch": 2.470187261565871,
      "grad_norm": 2.1472084522247314,
      "learning_rate": 5.32125291224437e-06,
      "loss": 1.2807,
      "step": 31890
    },
    {
      "epoch": 2.4709618699045297,
      "grad_norm": 2.8118741512298584,
      "learning_rate": 5.313486927258607e-06,
      "loss": 1.286,
      "step": 31900
    },
    {
      "epoch": 2.471736478243188,
      "grad_norm": 2.5260603427886963,
      "learning_rate": 5.305720942272845e-06,
      "loss": 1.2534,
      "step": 31910
    },
    {
      "epoch": 2.472511086581847,
      "grad_norm": 1.9396040439605713,
      "learning_rate": 5.297954957287082e-06,
      "loss": 1.2217,
      "step": 31920
    },
    {
      "epoch": 2.4732856949205058,
      "grad_norm": 2.5580198764801025,
      "learning_rate": 5.29018897230132e-06,
      "loss": 1.2742,
      "step": 31930
    },
    {
      "epoch": 2.4740603032591646,
      "grad_norm": 3.16384220123291,
      "learning_rate": 5.282422987315558e-06,
      "loss": 1.1918,
      "step": 31940
    },
    {
      "epoch": 2.4748349115978234,
      "grad_norm": 1.7906032800674438,
      "learning_rate": 5.274657002329796e-06,
      "loss": 1.2596,
      "step": 31950
    },
    {
      "epoch": 2.4756095199364823,
      "grad_norm": 3.002852439880371,
      "learning_rate": 5.266891017344033e-06,
      "loss": 1.2434,
      "step": 31960
    },
    {
      "epoch": 2.476384128275141,
      "grad_norm": 2.2192459106445312,
      "learning_rate": 5.259125032358271e-06,
      "loss": 1.266,
      "step": 31970
    },
    {
      "epoch": 2.4771587366137995,
      "grad_norm": 2.2635533809661865,
      "learning_rate": 5.251359047372508e-06,
      "loss": 1.2242,
      "step": 31980
    },
    {
      "epoch": 2.4779333449524583,
      "grad_norm": 2.7556204795837402,
      "learning_rate": 5.2435930623867465e-06,
      "loss": 1.2464,
      "step": 31990
    },
    {
      "epoch": 2.478707953291117,
      "grad_norm": 2.9078023433685303,
      "learning_rate": 5.235827077400984e-06,
      "loss": 1.1863,
      "step": 32000
    },
    {
      "epoch": 2.479482561629776,
      "grad_norm": 2.3869035243988037,
      "learning_rate": 5.228061092415221e-06,
      "loss": 1.4073,
      "step": 32010
    },
    {
      "epoch": 2.480257169968435,
      "grad_norm": 2.393545389175415,
      "learning_rate": 5.220295107429459e-06,
      "loss": 1.1719,
      "step": 32020
    },
    {
      "epoch": 2.4810317783070936,
      "grad_norm": 2.4226176738739014,
      "learning_rate": 5.212529122443696e-06,
      "loss": 1.1935,
      "step": 32030
    },
    {
      "epoch": 2.4818063866457525,
      "grad_norm": 3.16742205619812,
      "learning_rate": 5.204763137457935e-06,
      "loss": 1.1915,
      "step": 32040
    },
    {
      "epoch": 2.482580994984411,
      "grad_norm": 2.1521317958831787,
      "learning_rate": 5.196997152472172e-06,
      "loss": 1.3124,
      "step": 32050
    },
    {
      "epoch": 2.4833556033230697,
      "grad_norm": 2.4349491596221924,
      "learning_rate": 5.18923116748641e-06,
      "loss": 1.1915,
      "step": 32060
    },
    {
      "epoch": 2.4841302116617285,
      "grad_norm": 2.4385268688201904,
      "learning_rate": 5.181465182500647e-06,
      "loss": 1.2883,
      "step": 32070
    },
    {
      "epoch": 2.4849048200003874,
      "grad_norm": 1.938876986503601,
      "learning_rate": 5.1736991975148856e-06,
      "loss": 1.2209,
      "step": 32080
    },
    {
      "epoch": 2.485679428339046,
      "grad_norm": 2.376110315322876,
      "learning_rate": 5.165933212529123e-06,
      "loss": 1.3161,
      "step": 32090
    },
    {
      "epoch": 2.4864540366777046,
      "grad_norm": 3.137514591217041,
      "learning_rate": 5.1581672275433604e-06,
      "loss": 1.2183,
      "step": 32100
    },
    {
      "epoch": 2.4872286450163634,
      "grad_norm": 2.4249401092529297,
      "learning_rate": 5.1504012425575975e-06,
      "loss": 1.3384,
      "step": 32110
    },
    {
      "epoch": 2.4880032533550223,
      "grad_norm": 2.075018882751465,
      "learning_rate": 5.142635257571835e-06,
      "loss": 1.2887,
      "step": 32120
    },
    {
      "epoch": 2.488777861693681,
      "grad_norm": 2.680738687515259,
      "learning_rate": 5.134869272586073e-06,
      "loss": 1.1351,
      "step": 32130
    },
    {
      "epoch": 2.48955247003234,
      "grad_norm": 2.8705437183380127,
      "learning_rate": 5.127103287600311e-06,
      "loss": 1.242,
      "step": 32140
    },
    {
      "epoch": 2.4903270783709988,
      "grad_norm": 2.7779955863952637,
      "learning_rate": 5.119337302614548e-06,
      "loss": 1.2863,
      "step": 32150
    },
    {
      "epoch": 2.4911016867096576,
      "grad_norm": 2.8450124263763428,
      "learning_rate": 5.111571317628786e-06,
      "loss": 1.2754,
      "step": 32160
    },
    {
      "epoch": 2.491876295048316,
      "grad_norm": 2.3185503482818604,
      "learning_rate": 5.103805332643024e-06,
      "loss": 1.2241,
      "step": 32170
    },
    {
      "epoch": 2.492650903386975,
      "grad_norm": 2.306110143661499,
      "learning_rate": 5.096039347657262e-06,
      "loss": 1.2335,
      "step": 32180
    },
    {
      "epoch": 2.4934255117256336,
      "grad_norm": 2.16225004196167,
      "learning_rate": 5.0882733626714995e-06,
      "loss": 1.1991,
      "step": 32190
    },
    {
      "epoch": 2.4942001200642925,
      "grad_norm": 2.757991075515747,
      "learning_rate": 5.0805073776857366e-06,
      "loss": 1.2133,
      "step": 32200
    },
    {
      "epoch": 2.4949747284029513,
      "grad_norm": 2.817542314529419,
      "learning_rate": 5.0727413926999744e-06,
      "loss": 1.1792,
      "step": 32210
    },
    {
      "epoch": 2.49574933674161,
      "grad_norm": 2.99175763130188,
      "learning_rate": 5.0649754077142114e-06,
      "loss": 1.2312,
      "step": 32220
    },
    {
      "epoch": 2.496523945080269,
      "grad_norm": 2.278526544570923,
      "learning_rate": 5.05720942272845e-06,
      "loss": 1.2854,
      "step": 32230
    },
    {
      "epoch": 2.4972985534189274,
      "grad_norm": 2.1700472831726074,
      "learning_rate": 5.049443437742687e-06,
      "loss": 1.2061,
      "step": 32240
    },
    {
      "epoch": 2.498073161757586,
      "grad_norm": 2.632457733154297,
      "learning_rate": 5.041677452756925e-06,
      "loss": 1.2507,
      "step": 32250
    },
    {
      "epoch": 2.498847770096245,
      "grad_norm": 3.9703965187072754,
      "learning_rate": 5.033911467771162e-06,
      "loss": 1.2939,
      "step": 32260
    },
    {
      "epoch": 2.499622378434904,
      "grad_norm": 2.5437870025634766,
      "learning_rate": 5.0261454827854e-06,
      "loss": 1.3037,
      "step": 32270
    },
    {
      "epoch": 2.5003969867735627,
      "grad_norm": 2.940855026245117,
      "learning_rate": 5.018379497799638e-06,
      "loss": 1.2612,
      "step": 32280
    },
    {
      "epoch": 2.5011715951122215,
      "grad_norm": 2.9139623641967773,
      "learning_rate": 5.010613512813876e-06,
      "loss": 1.301,
      "step": 32290
    },
    {
      "epoch": 2.5019462034508804,
      "grad_norm": 2.107534408569336,
      "learning_rate": 5.002847527828113e-06,
      "loss": 1.2881,
      "step": 32300
    },
    {
      "epoch": 2.5027208117895388,
      "grad_norm": 2.2963294982910156,
      "learning_rate": 4.9950815428423505e-06,
      "loss": 1.2547,
      "step": 32310
    },
    {
      "epoch": 2.5034954201281976,
      "grad_norm": 2.3178603649139404,
      "learning_rate": 4.9873155578565876e-06,
      "loss": 1.2116,
      "step": 32320
    },
    {
      "epoch": 2.5042700284668564,
      "grad_norm": 3.4171507358551025,
      "learning_rate": 4.979549572870826e-06,
      "loss": 1.2585,
      "step": 32330
    },
    {
      "epoch": 2.5050446368055153,
      "grad_norm": 2.5584189891815186,
      "learning_rate": 4.971783587885063e-06,
      "loss": 1.2797,
      "step": 32340
    },
    {
      "epoch": 2.505819245144174,
      "grad_norm": 2.323309898376465,
      "learning_rate": 4.964017602899301e-06,
      "loss": 1.3201,
      "step": 32350
    },
    {
      "epoch": 2.5065938534828325,
      "grad_norm": 3.05060076713562,
      "learning_rate": 4.956251617913539e-06,
      "loss": 1.2389,
      "step": 32360
    },
    {
      "epoch": 2.5073684618214918,
      "grad_norm": 2.643343925476074,
      "learning_rate": 4.948485632927776e-06,
      "loss": 1.214,
      "step": 32370
    },
    {
      "epoch": 2.50814307016015,
      "grad_norm": 2.8075945377349854,
      "learning_rate": 4.940719647942015e-06,
      "loss": 1.2771,
      "step": 32380
    },
    {
      "epoch": 2.508917678498809,
      "grad_norm": 2.1179208755493164,
      "learning_rate": 4.932953662956252e-06,
      "loss": 1.2401,
      "step": 32390
    },
    {
      "epoch": 2.509692286837468,
      "grad_norm": 2.2593891620635986,
      "learning_rate": 4.92518767797049e-06,
      "loss": 1.4125,
      "step": 32400
    },
    {
      "epoch": 2.5104668951761266,
      "grad_norm": 2.287646532058716,
      "learning_rate": 4.917421692984727e-06,
      "loss": 1.2102,
      "step": 32410
    },
    {
      "epoch": 2.5112415035147855,
      "grad_norm": 2.5724289417266846,
      "learning_rate": 4.9096557079989645e-06,
      "loss": 1.1809,
      "step": 32420
    },
    {
      "epoch": 2.512016111853444,
      "grad_norm": 2.9076790809631348,
      "learning_rate": 4.901889723013202e-06,
      "loss": 1.2714,
      "step": 32430
    },
    {
      "epoch": 2.512790720192103,
      "grad_norm": 1.8961877822875977,
      "learning_rate": 4.89412373802744e-06,
      "loss": 1.2246,
      "step": 32440
    },
    {
      "epoch": 2.5135653285307615,
      "grad_norm": 3.264498233795166,
      "learning_rate": 4.886357753041677e-06,
      "loss": 1.2041,
      "step": 32450
    },
    {
      "epoch": 2.5143399368694204,
      "grad_norm": 1.7373757362365723,
      "learning_rate": 4.878591768055915e-06,
      "loss": 1.1912,
      "step": 32460
    },
    {
      "epoch": 2.515114545208079,
      "grad_norm": 2.282727003097534,
      "learning_rate": 4.870825783070152e-06,
      "loss": 1.239,
      "step": 32470
    },
    {
      "epoch": 2.515889153546738,
      "grad_norm": 2.9051032066345215,
      "learning_rate": 4.863059798084391e-06,
      "loss": 1.2667,
      "step": 32480
    },
    {
      "epoch": 2.516663761885397,
      "grad_norm": 3.3698906898498535,
      "learning_rate": 4.855293813098628e-06,
      "loss": 1.2121,
      "step": 32490
    },
    {
      "epoch": 2.5174383702240553,
      "grad_norm": 2.7226052284240723,
      "learning_rate": 4.847527828112866e-06,
      "loss": 1.239,
      "step": 32500
    },
    {
      "epoch": 2.518212978562714,
      "grad_norm": 2.77994704246521,
      "learning_rate": 4.839761843127103e-06,
      "loss": 1.309,
      "step": 32510
    },
    {
      "epoch": 2.518987586901373,
      "grad_norm": 2.57985782623291,
      "learning_rate": 4.831995858141341e-06,
      "loss": 1.3108,
      "step": 32520
    },
    {
      "epoch": 2.5197621952400318,
      "grad_norm": 2.32814884185791,
      "learning_rate": 4.824229873155579e-06,
      "loss": 1.2598,
      "step": 32530
    },
    {
      "epoch": 2.5205368035786906,
      "grad_norm": 2.355404853820801,
      "learning_rate": 4.816463888169816e-06,
      "loss": 1.3396,
      "step": 32540
    },
    {
      "epoch": 2.5213114119173494,
      "grad_norm": 2.8276309967041016,
      "learning_rate": 4.808697903184054e-06,
      "loss": 1.2776,
      "step": 32550
    },
    {
      "epoch": 2.5220860202560083,
      "grad_norm": 2.741699457168579,
      "learning_rate": 4.800931918198291e-06,
      "loss": 1.2329,
      "step": 32560
    },
    {
      "epoch": 2.5228606285946666,
      "grad_norm": 2.3566269874572754,
      "learning_rate": 4.793165933212529e-06,
      "loss": 1.1848,
      "step": 32570
    },
    {
      "epoch": 2.5236352369333255,
      "grad_norm": 3.1548659801483154,
      "learning_rate": 4.785399948226767e-06,
      "loss": 1.3371,
      "step": 32580
    },
    {
      "epoch": 2.5244098452719843,
      "grad_norm": 2.995393991470337,
      "learning_rate": 4.777633963241005e-06,
      "loss": 1.2705,
      "step": 32590
    },
    {
      "epoch": 2.525184453610643,
      "grad_norm": 2.372734785079956,
      "learning_rate": 4.769867978255242e-06,
      "loss": 1.2503,
      "step": 32600
    },
    {
      "epoch": 2.525959061949302,
      "grad_norm": 2.748361349105835,
      "learning_rate": 4.76210199326948e-06,
      "loss": 1.2579,
      "step": 32610
    },
    {
      "epoch": 2.526733670287961,
      "grad_norm": 2.7827694416046143,
      "learning_rate": 4.754336008283718e-06,
      "loss": 1.1498,
      "step": 32620
    },
    {
      "epoch": 2.5275082786266196,
      "grad_norm": 2.6230413913726807,
      "learning_rate": 4.7465700232979555e-06,
      "loss": 1.2499,
      "step": 32630
    },
    {
      "epoch": 2.528282886965278,
      "grad_norm": 2.8098065853118896,
      "learning_rate": 4.7388040383121925e-06,
      "loss": 1.37,
      "step": 32640
    },
    {
      "epoch": 2.529057495303937,
      "grad_norm": 2.3508665561676025,
      "learning_rate": 4.73103805332643e-06,
      "loss": 1.2899,
      "step": 32650
    },
    {
      "epoch": 2.5298321036425957,
      "grad_norm": 2.594588279724121,
      "learning_rate": 4.723272068340667e-06,
      "loss": 1.1829,
      "step": 32660
    },
    {
      "epoch": 2.5306067119812545,
      "grad_norm": 2.58736252784729,
      "learning_rate": 4.715506083354906e-06,
      "loss": 1.2644,
      "step": 32670
    },
    {
      "epoch": 2.5313813203199134,
      "grad_norm": 2.3707828521728516,
      "learning_rate": 4.707740098369143e-06,
      "loss": 1.2253,
      "step": 32680
    },
    {
      "epoch": 2.5321559286585718,
      "grad_norm": 2.6959192752838135,
      "learning_rate": 4.699974113383381e-06,
      "loss": 1.2175,
      "step": 32690
    },
    {
      "epoch": 2.532930536997231,
      "grad_norm": 3.257272958755493,
      "learning_rate": 4.692208128397618e-06,
      "loss": 1.1824,
      "step": 32700
    },
    {
      "epoch": 2.5337051453358894,
      "grad_norm": 3.2166547775268555,
      "learning_rate": 4.684442143411856e-06,
      "loss": 1.3631,
      "step": 32710
    },
    {
      "epoch": 2.5344797536745483,
      "grad_norm": 2.5188114643096924,
      "learning_rate": 4.676676158426095e-06,
      "loss": 1.2034,
      "step": 32720
    },
    {
      "epoch": 2.535254362013207,
      "grad_norm": 2.564181089401245,
      "learning_rate": 4.668910173440332e-06,
      "loss": 1.3666,
      "step": 32730
    },
    {
      "epoch": 2.536028970351866,
      "grad_norm": 3.068220853805542,
      "learning_rate": 4.6611441884545695e-06,
      "loss": 1.2424,
      "step": 32740
    },
    {
      "epoch": 2.5368035786905248,
      "grad_norm": 2.32159161567688,
      "learning_rate": 4.6533782034688065e-06,
      "loss": 1.2235,
      "step": 32750
    },
    {
      "epoch": 2.537578187029183,
      "grad_norm": 2.522554874420166,
      "learning_rate": 4.645612218483044e-06,
      "loss": 1.2271,
      "step": 32760
    },
    {
      "epoch": 2.538352795367842,
      "grad_norm": 2.4559504985809326,
      "learning_rate": 4.637846233497282e-06,
      "loss": 1.1723,
      "step": 32770
    },
    {
      "epoch": 2.539127403706501,
      "grad_norm": 1.685799241065979,
      "learning_rate": 4.63008024851152e-06,
      "loss": 1.2408,
      "step": 32780
    },
    {
      "epoch": 2.5399020120451596,
      "grad_norm": 3.036273956298828,
      "learning_rate": 4.622314263525757e-06,
      "loss": 1.2264,
      "step": 32790
    },
    {
      "epoch": 2.5406766203838185,
      "grad_norm": 3.3778443336486816,
      "learning_rate": 4.614548278539995e-06,
      "loss": 1.1975,
      "step": 32800
    },
    {
      "epoch": 2.5414512287224773,
      "grad_norm": 2.2524893283843994,
      "learning_rate": 4.606782293554232e-06,
      "loss": 1.2028,
      "step": 32810
    },
    {
      "epoch": 2.542225837061136,
      "grad_norm": 2.28574800491333,
      "learning_rate": 4.599016308568471e-06,
      "loss": 1.2779,
      "step": 32820
    },
    {
      "epoch": 2.5430004453997945,
      "grad_norm": 3.1547303199768066,
      "learning_rate": 4.591250323582708e-06,
      "loss": 1.2378,
      "step": 32830
    },
    {
      "epoch": 2.5437750537384534,
      "grad_norm": 2.766629219055176,
      "learning_rate": 4.583484338596946e-06,
      "loss": 1.3029,
      "step": 32840
    },
    {
      "epoch": 2.544549662077112,
      "grad_norm": 2.8824515342712402,
      "learning_rate": 4.575718353611183e-06,
      "loss": 1.3268,
      "step": 32850
    },
    {
      "epoch": 2.545324270415771,
      "grad_norm": 2.496835470199585,
      "learning_rate": 4.5679523686254205e-06,
      "loss": 1.3736,
      "step": 32860
    },
    {
      "epoch": 2.54609887875443,
      "grad_norm": 2.2303619384765625,
      "learning_rate": 4.560186383639658e-06,
      "loss": 1.2396,
      "step": 32870
    },
    {
      "epoch": 2.5468734870930887,
      "grad_norm": 1.8477494716644287,
      "learning_rate": 4.552420398653896e-06,
      "loss": 1.3164,
      "step": 32880
    },
    {
      "epoch": 2.5476480954317475,
      "grad_norm": 3.19705867767334,
      "learning_rate": 4.544654413668134e-06,
      "loss": 1.2296,
      "step": 32890
    },
    {
      "epoch": 2.548422703770406,
      "grad_norm": 2.1035146713256836,
      "learning_rate": 4.536888428682371e-06,
      "loss": 1.3047,
      "step": 32900
    },
    {
      "epoch": 2.5491973121090648,
      "grad_norm": 2.689589023590088,
      "learning_rate": 4.529122443696609e-06,
      "loss": 1.2016,
      "step": 32910
    },
    {
      "epoch": 2.5499719204477236,
      "grad_norm": 1.9272047281265259,
      "learning_rate": 4.521356458710847e-06,
      "loss": 1.2803,
      "step": 32920
    },
    {
      "epoch": 2.5507465287863824,
      "grad_norm": 2.225393533706665,
      "learning_rate": 4.513590473725085e-06,
      "loss": 1.2828,
      "step": 32930
    },
    {
      "epoch": 2.5515211371250412,
      "grad_norm": 2.610524892807007,
      "learning_rate": 4.505824488739322e-06,
      "loss": 1.2493,
      "step": 32940
    },
    {
      "epoch": 2.5522957454637,
      "grad_norm": 2.962444543838501,
      "learning_rate": 4.49805850375356e-06,
      "loss": 1.2881,
      "step": 32950
    },
    {
      "epoch": 2.553070353802359,
      "grad_norm": 3.4684791564941406,
      "learning_rate": 4.490292518767797e-06,
      "loss": 1.2401,
      "step": 32960
    },
    {
      "epoch": 2.5538449621410173,
      "grad_norm": 2.6247494220733643,
      "learning_rate": 4.482526533782035e-06,
      "loss": 1.2711,
      "step": 32970
    },
    {
      "epoch": 2.554619570479676,
      "grad_norm": 2.22345232963562,
      "learning_rate": 4.474760548796272e-06,
      "loss": 1.1915,
      "step": 32980
    },
    {
      "epoch": 2.555394178818335,
      "grad_norm": 3.200180768966675,
      "learning_rate": 4.46699456381051e-06,
      "loss": 1.1716,
      "step": 32990
    },
    {
      "epoch": 2.556168787156994,
      "grad_norm": 2.789128303527832,
      "learning_rate": 4.459228578824747e-06,
      "loss": 1.2833,
      "step": 33000
    },
    {
      "epoch": 2.5569433954956526,
      "grad_norm": 3.604583978652954,
      "learning_rate": 4.451462593838985e-06,
      "loss": 1.2333,
      "step": 33010
    },
    {
      "epoch": 2.557718003834311,
      "grad_norm": 2.554837465286255,
      "learning_rate": 4.443696608853223e-06,
      "loss": 1.2111,
      "step": 33020
    },
    {
      "epoch": 2.5584926121729703,
      "grad_norm": 2.650642156600952,
      "learning_rate": 4.435930623867461e-06,
      "loss": 1.2847,
      "step": 33030
    },
    {
      "epoch": 2.5592672205116287,
      "grad_norm": 2.2395150661468506,
      "learning_rate": 4.428164638881698e-06,
      "loss": 1.2114,
      "step": 33040
    },
    {
      "epoch": 2.5600418288502875,
      "grad_norm": 2.9620392322540283,
      "learning_rate": 4.420398653895936e-06,
      "loss": 1.2752,
      "step": 33050
    },
    {
      "epoch": 2.5608164371889464,
      "grad_norm": 3.7873573303222656,
      "learning_rate": 4.412632668910173e-06,
      "loss": 1.316,
      "step": 33060
    },
    {
      "epoch": 2.561591045527605,
      "grad_norm": 2.1247124671936035,
      "learning_rate": 4.4048666839244114e-06,
      "loss": 1.2687,
      "step": 33070
    },
    {
      "epoch": 2.562365653866264,
      "grad_norm": 2.1921849250793457,
      "learning_rate": 4.397100698938649e-06,
      "loss": 1.1639,
      "step": 33080
    },
    {
      "epoch": 2.5631402622049224,
      "grad_norm": 2.2755749225616455,
      "learning_rate": 4.389334713952886e-06,
      "loss": 1.3129,
      "step": 33090
    },
    {
      "epoch": 2.5639148705435812,
      "grad_norm": 2.499612331390381,
      "learning_rate": 4.381568728967124e-06,
      "loss": 1.3004,
      "step": 33100
    },
    {
      "epoch": 2.56468947888224,
      "grad_norm": 2.1838769912719727,
      "learning_rate": 4.373802743981362e-06,
      "loss": 1.2905,
      "step": 33110
    },
    {
      "epoch": 2.565464087220899,
      "grad_norm": 2.12807559967041,
      "learning_rate": 4.3660367589956e-06,
      "loss": 1.2014,
      "step": 33120
    },
    {
      "epoch": 2.5662386955595577,
      "grad_norm": 2.3376083374023438,
      "learning_rate": 4.358270774009837e-06,
      "loss": 1.3248,
      "step": 33130
    },
    {
      "epoch": 2.5670133038982166,
      "grad_norm": 2.1639132499694824,
      "learning_rate": 4.350504789024075e-06,
      "loss": 1.2639,
      "step": 33140
    },
    {
      "epoch": 2.5677879122368754,
      "grad_norm": 2.8369503021240234,
      "learning_rate": 4.342738804038312e-06,
      "loss": 1.3038,
      "step": 33150
    },
    {
      "epoch": 2.568562520575534,
      "grad_norm": 2.112137794494629,
      "learning_rate": 4.3349728190525505e-06,
      "loss": 1.3196,
      "step": 33160
    },
    {
      "epoch": 2.5693371289141926,
      "grad_norm": 2.0477874279022217,
      "learning_rate": 4.3272068340667876e-06,
      "loss": 1.264,
      "step": 33170
    },
    {
      "epoch": 2.5701117372528515,
      "grad_norm": 2.178001642227173,
      "learning_rate": 4.319440849081025e-06,
      "loss": 1.3073,
      "step": 33180
    },
    {
      "epoch": 2.5708863455915103,
      "grad_norm": 2.5620851516723633,
      "learning_rate": 4.3116748640952624e-06,
      "loss": 1.2804,
      "step": 33190
    },
    {
      "epoch": 2.571660953930169,
      "grad_norm": 2.43239164352417,
      "learning_rate": 4.3039088791095e-06,
      "loss": 1.1931,
      "step": 33200
    },
    {
      "epoch": 2.572435562268828,
      "grad_norm": 2.6718506813049316,
      "learning_rate": 4.296142894123738e-06,
      "loss": 1.3275,
      "step": 33210
    },
    {
      "epoch": 2.573210170607487,
      "grad_norm": 2.5054872035980225,
      "learning_rate": 4.288376909137976e-06,
      "loss": 1.2085,
      "step": 33220
    },
    {
      "epoch": 2.573984778946145,
      "grad_norm": 2.3964169025421143,
      "learning_rate": 4.280610924152213e-06,
      "loss": 1.1928,
      "step": 33230
    },
    {
      "epoch": 2.574759387284804,
      "grad_norm": 2.2604403495788574,
      "learning_rate": 4.272844939166451e-06,
      "loss": 1.2515,
      "step": 33240
    },
    {
      "epoch": 2.575533995623463,
      "grad_norm": 2.7821826934814453,
      "learning_rate": 4.265078954180689e-06,
      "loss": 1.2051,
      "step": 33250
    },
    {
      "epoch": 2.5763086039621217,
      "grad_norm": 2.5552284717559814,
      "learning_rate": 4.257312969194927e-06,
      "loss": 1.1502,
      "step": 33260
    },
    {
      "epoch": 2.5770832123007805,
      "grad_norm": 3.0008277893066406,
      "learning_rate": 4.2495469842091645e-06,
      "loss": 1.3309,
      "step": 33270
    },
    {
      "epoch": 2.577857820639439,
      "grad_norm": 2.1519811153411865,
      "learning_rate": 4.2417809992234015e-06,
      "loss": 1.2631,
      "step": 33280
    },
    {
      "epoch": 2.578632428978098,
      "grad_norm": 2.7034735679626465,
      "learning_rate": 4.234015014237639e-06,
      "loss": 1.2587,
      "step": 33290
    },
    {
      "epoch": 2.5794070373167566,
      "grad_norm": 2.3131885528564453,
      "learning_rate": 4.2262490292518764e-06,
      "loss": 1.3218,
      "step": 33300
    },
    {
      "epoch": 2.5801816456554154,
      "grad_norm": 3.5211434364318848,
      "learning_rate": 4.218483044266115e-06,
      "loss": 1.2686,
      "step": 33310
    },
    {
      "epoch": 2.5809562539940742,
      "grad_norm": 2.212831497192383,
      "learning_rate": 4.210717059280352e-06,
      "loss": 1.2153,
      "step": 33320
    },
    {
      "epoch": 2.581730862332733,
      "grad_norm": 2.6577529907226562,
      "learning_rate": 4.20295107429459e-06,
      "loss": 1.1957,
      "step": 33330
    },
    {
      "epoch": 2.582505470671392,
      "grad_norm": 2.624457597732544,
      "learning_rate": 4.195185089308827e-06,
      "loss": 1.0679,
      "step": 33340
    },
    {
      "epoch": 2.5832800790100503,
      "grad_norm": 3.1204967498779297,
      "learning_rate": 4.187419104323065e-06,
      "loss": 1.3858,
      "step": 33350
    },
    {
      "epoch": 2.5840546873487096,
      "grad_norm": 3.1451268196105957,
      "learning_rate": 4.179653119337303e-06,
      "loss": 1.282,
      "step": 33360
    },
    {
      "epoch": 2.584829295687368,
      "grad_norm": 3.5189335346221924,
      "learning_rate": 4.171887134351541e-06,
      "loss": 1.2023,
      "step": 33370
    },
    {
      "epoch": 2.585603904026027,
      "grad_norm": 2.212467670440674,
      "learning_rate": 4.164121149365778e-06,
      "loss": 1.1843,
      "step": 33380
    },
    {
      "epoch": 2.5863785123646856,
      "grad_norm": 2.1866343021392822,
      "learning_rate": 4.1563551643800155e-06,
      "loss": 1.1952,
      "step": 33390
    },
    {
      "epoch": 2.5871531207033445,
      "grad_norm": 1.8741257190704346,
      "learning_rate": 4.1485891793942525e-06,
      "loss": 1.1849,
      "step": 33400
    },
    {
      "epoch": 2.5879277290420033,
      "grad_norm": 2.1055076122283936,
      "learning_rate": 4.140823194408491e-06,
      "loss": 1.1898,
      "step": 33410
    },
    {
      "epoch": 2.5887023373806617,
      "grad_norm": 2.5294811725616455,
      "learning_rate": 4.133057209422728e-06,
      "loss": 1.3351,
      "step": 33420
    },
    {
      "epoch": 2.5894769457193205,
      "grad_norm": 2.386066436767578,
      "learning_rate": 4.125291224436966e-06,
      "loss": 1.2576,
      "step": 33430
    },
    {
      "epoch": 2.5902515540579794,
      "grad_norm": 3.292782783508301,
      "learning_rate": 4.117525239451204e-06,
      "loss": 1.3017,
      "step": 33440
    },
    {
      "epoch": 2.591026162396638,
      "grad_norm": 2.5015780925750732,
      "learning_rate": 4.109759254465441e-06,
      "loss": 1.2564,
      "step": 33450
    },
    {
      "epoch": 2.591800770735297,
      "grad_norm": 2.2444374561309814,
      "learning_rate": 4.10199326947968e-06,
      "loss": 1.1652,
      "step": 33460
    },
    {
      "epoch": 2.592575379073956,
      "grad_norm": 2.5582714080810547,
      "learning_rate": 4.094227284493917e-06,
      "loss": 1.2225,
      "step": 33470
    },
    {
      "epoch": 2.5933499874126147,
      "grad_norm": 3.7493488788604736,
      "learning_rate": 4.086461299508155e-06,
      "loss": 1.242,
      "step": 33480
    },
    {
      "epoch": 2.594124595751273,
      "grad_norm": 2.3209621906280518,
      "learning_rate": 4.078695314522392e-06,
      "loss": 1.327,
      "step": 33490
    },
    {
      "epoch": 2.594899204089932,
      "grad_norm": 2.076606035232544,
      "learning_rate": 4.0709293295366295e-06,
      "loss": 1.1067,
      "step": 33500
    },
    {
      "epoch": 2.5956738124285907,
      "grad_norm": 2.2636749744415283,
      "learning_rate": 4.063163344550867e-06,
      "loss": 1.2704,
      "step": 33510
    },
    {
      "epoch": 2.5964484207672496,
      "grad_norm": 3.4933505058288574,
      "learning_rate": 4.055397359565105e-06,
      "loss": 1.2486,
      "step": 33520
    },
    {
      "epoch": 2.5972230291059084,
      "grad_norm": 2.2557802200317383,
      "learning_rate": 4.047631374579342e-06,
      "loss": 1.3036,
      "step": 33530
    },
    {
      "epoch": 2.5979976374445672,
      "grad_norm": 1.844380259513855,
      "learning_rate": 4.03986538959358e-06,
      "loss": 1.2989,
      "step": 33540
    },
    {
      "epoch": 2.598772245783226,
      "grad_norm": 2.6354568004608154,
      "learning_rate": 4.032099404607817e-06,
      "loss": 1.2497,
      "step": 33550
    },
    {
      "epoch": 2.5995468541218845,
      "grad_norm": 3.184140205383301,
      "learning_rate": 4.024333419622056e-06,
      "loss": 1.2584,
      "step": 33560
    },
    {
      "epoch": 2.6003214624605433,
      "grad_norm": 2.2375707626342773,
      "learning_rate": 4.016567434636293e-06,
      "loss": 1.248,
      "step": 33570
    },
    {
      "epoch": 2.601096070799202,
      "grad_norm": 3.3390538692474365,
      "learning_rate": 4.008801449650531e-06,
      "loss": 1.3446,
      "step": 33580
    },
    {
      "epoch": 2.601870679137861,
      "grad_norm": 1.9270600080490112,
      "learning_rate": 4.001035464664768e-06,
      "loss": 1.2652,
      "step": 33590
    },
    {
      "epoch": 2.60264528747652,
      "grad_norm": 3.461228370666504,
      "learning_rate": 3.9932694796790065e-06,
      "loss": 1.1802,
      "step": 33600
    },
    {
      "epoch": 2.603419895815178,
      "grad_norm": 2.1337597370147705,
      "learning_rate": 3.985503494693244e-06,
      "loss": 1.2931,
      "step": 33610
    },
    {
      "epoch": 2.6041945041538375,
      "grad_norm": 2.760709285736084,
      "learning_rate": 3.977737509707481e-06,
      "loss": 1.2089,
      "step": 33620
    },
    {
      "epoch": 2.604969112492496,
      "grad_norm": 2.327533721923828,
      "learning_rate": 3.969971524721719e-06,
      "loss": 1.2271,
      "step": 33630
    },
    {
      "epoch": 2.6057437208311547,
      "grad_norm": 2.149240493774414,
      "learning_rate": 3.962205539735956e-06,
      "loss": 1.2435,
      "step": 33640
    },
    {
      "epoch": 2.6065183291698135,
      "grad_norm": 2.520531415939331,
      "learning_rate": 3.954439554750195e-06,
      "loss": 1.2851,
      "step": 33650
    },
    {
      "epoch": 2.6072929375084724,
      "grad_norm": 2.12986159324646,
      "learning_rate": 3.946673569764432e-06,
      "loss": 1.246,
      "step": 33660
    },
    {
      "epoch": 2.608067545847131,
      "grad_norm": 2.094632863998413,
      "learning_rate": 3.93890758477867e-06,
      "loss": 1.2983,
      "step": 33670
    },
    {
      "epoch": 2.6088421541857896,
      "grad_norm": 2.9781603813171387,
      "learning_rate": 3.931141599792907e-06,
      "loss": 1.2388,
      "step": 33680
    },
    {
      "epoch": 2.609616762524449,
      "grad_norm": 2.315920352935791,
      "learning_rate": 3.923375614807145e-06,
      "loss": 1.2511,
      "step": 33690
    },
    {
      "epoch": 2.6103913708631072,
      "grad_norm": 2.597339153289795,
      "learning_rate": 3.915609629821383e-06,
      "loss": 1.2637,
      "step": 33700
    },
    {
      "epoch": 2.611165979201766,
      "grad_norm": 2.672024965286255,
      "learning_rate": 3.9078436448356205e-06,
      "loss": 1.2467,
      "step": 33710
    },
    {
      "epoch": 2.611940587540425,
      "grad_norm": 2.749291181564331,
      "learning_rate": 3.9000776598498575e-06,
      "loss": 1.3116,
      "step": 33720
    },
    {
      "epoch": 2.6127151958790837,
      "grad_norm": 3.4913017749786377,
      "learning_rate": 3.892311674864095e-06,
      "loss": 1.1183,
      "step": 33730
    },
    {
      "epoch": 2.6134898042177426,
      "grad_norm": 2.4935550689697266,
      "learning_rate": 3.884545689878332e-06,
      "loss": 1.267,
      "step": 33740
    },
    {
      "epoch": 2.614264412556401,
      "grad_norm": 2.9137589931488037,
      "learning_rate": 3.876779704892571e-06,
      "loss": 1.1503,
      "step": 33750
    },
    {
      "epoch": 2.61503902089506,
      "grad_norm": 2.6491971015930176,
      "learning_rate": 3.869013719906808e-06,
      "loss": 1.1569,
      "step": 33760
    },
    {
      "epoch": 2.6158136292337186,
      "grad_norm": 2.943390369415283,
      "learning_rate": 3.861247734921046e-06,
      "loss": 1.328,
      "step": 33770
    },
    {
      "epoch": 2.6165882375723775,
      "grad_norm": 2.529318332672119,
      "learning_rate": 3.853481749935283e-06,
      "loss": 1.3237,
      "step": 33780
    },
    {
      "epoch": 2.6173628459110363,
      "grad_norm": 2.4878668785095215,
      "learning_rate": 3.845715764949521e-06,
      "loss": 1.1915,
      "step": 33790
    },
    {
      "epoch": 2.618137454249695,
      "grad_norm": 2.464682102203369,
      "learning_rate": 3.8379497799637596e-06,
      "loss": 1.2381,
      "step": 33800
    },
    {
      "epoch": 2.618912062588354,
      "grad_norm": 2.423922061920166,
      "learning_rate": 3.830183794977997e-06,
      "loss": 1.1422,
      "step": 33810
    },
    {
      "epoch": 2.6196866709270124,
      "grad_norm": 2.0651462078094482,
      "learning_rate": 3.8224178099922344e-06,
      "loss": 1.3206,
      "step": 33820
    },
    {
      "epoch": 2.620461279265671,
      "grad_norm": 1.842348337173462,
      "learning_rate": 3.8146518250064715e-06,
      "loss": 1.2389,
      "step": 33830
    },
    {
      "epoch": 2.62123588760433,
      "grad_norm": 2.8658108711242676,
      "learning_rate": 3.806885840020709e-06,
      "loss": 1.2395,
      "step": 33840
    },
    {
      "epoch": 2.622010495942989,
      "grad_norm": 2.5829710960388184,
      "learning_rate": 3.799119855034947e-06,
      "loss": 1.2876,
      "step": 33850
    },
    {
      "epoch": 2.6227851042816477,
      "grad_norm": 2.7274720668792725,
      "learning_rate": 3.7913538700491846e-06,
      "loss": 1.2966,
      "step": 33860
    },
    {
      "epoch": 2.6235597126203065,
      "grad_norm": 3.569690704345703,
      "learning_rate": 3.783587885063422e-06,
      "loss": 1.3454,
      "step": 33870
    },
    {
      "epoch": 2.6243343209589653,
      "grad_norm": 2.80703067779541,
      "learning_rate": 3.77582190007766e-06,
      "loss": 1.3012,
      "step": 33880
    },
    {
      "epoch": 2.6251089292976237,
      "grad_norm": 2.54880952835083,
      "learning_rate": 3.7680559150918974e-06,
      "loss": 1.1359,
      "step": 33890
    },
    {
      "epoch": 2.6258835376362826,
      "grad_norm": 2.1600277423858643,
      "learning_rate": 3.7602899301061357e-06,
      "loss": 1.2562,
      "step": 33900
    },
    {
      "epoch": 2.6266581459749414,
      "grad_norm": 2.792905330657959,
      "learning_rate": 3.752523945120373e-06,
      "loss": 1.2993,
      "step": 33910
    },
    {
      "epoch": 2.6274327543136002,
      "grad_norm": 2.7467894554138184,
      "learning_rate": 3.7447579601346106e-06,
      "loss": 1.3995,
      "step": 33920
    },
    {
      "epoch": 2.628207362652259,
      "grad_norm": 2.8194942474365234,
      "learning_rate": 3.736991975148848e-06,
      "loss": 1.266,
      "step": 33930
    },
    {
      "epoch": 2.6289819709909175,
      "grad_norm": 2.603097915649414,
      "learning_rate": 3.729225990163086e-06,
      "loss": 1.2441,
      "step": 33940
    },
    {
      "epoch": 2.6297565793295767,
      "grad_norm": 2.3129708766937256,
      "learning_rate": 3.7214600051773233e-06,
      "loss": 1.3169,
      "step": 33950
    },
    {
      "epoch": 2.630531187668235,
      "grad_norm": 2.699054002761841,
      "learning_rate": 3.713694020191561e-06,
      "loss": 1.2756,
      "step": 33960
    },
    {
      "epoch": 2.631305796006894,
      "grad_norm": 1.9501864910125732,
      "learning_rate": 3.7067046337043746e-06,
      "loss": 1.2431,
      "step": 33970
    },
    {
      "epoch": 2.632080404345553,
      "grad_norm": 2.415194511413574,
      "learning_rate": 3.6989386487186125e-06,
      "loss": 1.2272,
      "step": 33980
    },
    {
      "epoch": 2.6328550126842116,
      "grad_norm": 2.7241549491882324,
      "learning_rate": 3.69117266373285e-06,
      "loss": 1.231,
      "step": 33990
    },
    {
      "epoch": 2.6336296210228705,
      "grad_norm": 2.4900217056274414,
      "learning_rate": 3.6834066787470878e-06,
      "loss": 1.3142,
      "step": 34000
    },
    {
      "epoch": 2.634404229361529,
      "grad_norm": 2.501185417175293,
      "learning_rate": 3.6756406937613256e-06,
      "loss": 1.1972,
      "step": 34010
    },
    {
      "epoch": 2.6351788377001877,
      "grad_norm": 3.062239408493042,
      "learning_rate": 3.6678747087755635e-06,
      "loss": 1.2241,
      "step": 34020
    },
    {
      "epoch": 2.6359534460388465,
      "grad_norm": 3.7215778827667236,
      "learning_rate": 3.660108723789801e-06,
      "loss": 1.2958,
      "step": 34030
    },
    {
      "epoch": 2.6367280543775053,
      "grad_norm": 2.7110509872436523,
      "learning_rate": 3.6523427388040384e-06,
      "loss": 1.1984,
      "step": 34040
    },
    {
      "epoch": 2.637502662716164,
      "grad_norm": 2.5338480472564697,
      "learning_rate": 3.6445767538182762e-06,
      "loss": 1.321,
      "step": 34050
    },
    {
      "epoch": 2.638277271054823,
      "grad_norm": 2.6435532569885254,
      "learning_rate": 3.6368107688325137e-06,
      "loss": 1.2662,
      "step": 34060
    },
    {
      "epoch": 2.639051879393482,
      "grad_norm": 1.9151097536087036,
      "learning_rate": 3.6290447838467516e-06,
      "loss": 1.2937,
      "step": 34070
    },
    {
      "epoch": 2.6398264877321402,
      "grad_norm": 2.453401803970337,
      "learning_rate": 3.621278798860989e-06,
      "loss": 1.4012,
      "step": 34080
    },
    {
      "epoch": 2.640601096070799,
      "grad_norm": 2.660696029663086,
      "learning_rate": 3.6135128138752264e-06,
      "loss": 1.3047,
      "step": 34090
    },
    {
      "epoch": 2.641375704409458,
      "grad_norm": 2.8836421966552734,
      "learning_rate": 3.6057468288894643e-06,
      "loss": 1.3389,
      "step": 34100
    },
    {
      "epoch": 2.6421503127481167,
      "grad_norm": 2.932389497756958,
      "learning_rate": 3.5979808439037018e-06,
      "loss": 1.3436,
      "step": 34110
    },
    {
      "epoch": 2.6429249210867756,
      "grad_norm": 2.5176711082458496,
      "learning_rate": 3.5902148589179396e-06,
      "loss": 1.2206,
      "step": 34120
    },
    {
      "epoch": 2.6436995294254344,
      "grad_norm": 2.9161765575408936,
      "learning_rate": 3.582448873932177e-06,
      "loss": 1.2719,
      "step": 34130
    },
    {
      "epoch": 2.6444741377640932,
      "grad_norm": 2.8660476207733154,
      "learning_rate": 3.5746828889464145e-06,
      "loss": 1.2151,
      "step": 34140
    },
    {
      "epoch": 2.6452487461027516,
      "grad_norm": 1.979087471961975,
      "learning_rate": 3.5669169039606524e-06,
      "loss": 1.3267,
      "step": 34150
    },
    {
      "epoch": 2.6460233544414105,
      "grad_norm": 2.125068426132202,
      "learning_rate": 3.55915091897489e-06,
      "loss": 1.2459,
      "step": 34160
    },
    {
      "epoch": 2.6467979627800693,
      "grad_norm": 2.3296079635620117,
      "learning_rate": 3.5513849339891277e-06,
      "loss": 1.3073,
      "step": 34170
    },
    {
      "epoch": 2.647572571118728,
      "grad_norm": 2.0264463424682617,
      "learning_rate": 3.5436189490033655e-06,
      "loss": 1.1665,
      "step": 34180
    },
    {
      "epoch": 2.648347179457387,
      "grad_norm": 3.084897756576538,
      "learning_rate": 3.535852964017603e-06,
      "loss": 1.187,
      "step": 34190
    },
    {
      "epoch": 2.649121787796046,
      "grad_norm": 1.9691489934921265,
      "learning_rate": 3.528086979031841e-06,
      "loss": 1.1373,
      "step": 34200
    },
    {
      "epoch": 2.6498963961347046,
      "grad_norm": 2.3269219398498535,
      "learning_rate": 3.5203209940460783e-06,
      "loss": 1.2456,
      "step": 34210
    },
    {
      "epoch": 2.650671004473363,
      "grad_norm": 2.8182404041290283,
      "learning_rate": 3.512555009060316e-06,
      "loss": 1.1716,
      "step": 34220
    },
    {
      "epoch": 2.651445612812022,
      "grad_norm": 2.845815420150757,
      "learning_rate": 3.5047890240745536e-06,
      "loss": 1.4363,
      "step": 34230
    },
    {
      "epoch": 2.6522202211506807,
      "grad_norm": 3.867501735687256,
      "learning_rate": 3.497023039088791e-06,
      "loss": 1.2814,
      "step": 34240
    },
    {
      "epoch": 2.6529948294893395,
      "grad_norm": 2.2556309700012207,
      "learning_rate": 3.489257054103029e-06,
      "loss": 1.179,
      "step": 34250
    },
    {
      "epoch": 2.6537694378279983,
      "grad_norm": 2.6393847465515137,
      "learning_rate": 3.4814910691172664e-06,
      "loss": 1.2438,
      "step": 34260
    },
    {
      "epoch": 2.6545440461666567,
      "grad_norm": 2.0009825229644775,
      "learning_rate": 3.4737250841315042e-06,
      "loss": 1.3297,
      "step": 34270
    },
    {
      "epoch": 2.655318654505316,
      "grad_norm": 2.7774596214294434,
      "learning_rate": 3.4659590991457417e-06,
      "loss": 1.2863,
      "step": 34280
    },
    {
      "epoch": 2.6560932628439744,
      "grad_norm": 2.6968588829040527,
      "learning_rate": 3.4581931141599795e-06,
      "loss": 1.2642,
      "step": 34290
    },
    {
      "epoch": 2.6568678711826332,
      "grad_norm": 3.2375943660736084,
      "learning_rate": 3.450427129174217e-06,
      "loss": 1.163,
      "step": 34300
    },
    {
      "epoch": 2.657642479521292,
      "grad_norm": 3.1712210178375244,
      "learning_rate": 3.4426611441884544e-06,
      "loss": 1.2061,
      "step": 34310
    },
    {
      "epoch": 2.658417087859951,
      "grad_norm": 2.22455096244812,
      "learning_rate": 3.4348951592026923e-06,
      "loss": 1.2997,
      "step": 34320
    },
    {
      "epoch": 2.6591916961986097,
      "grad_norm": 3.6897449493408203,
      "learning_rate": 3.4271291742169297e-06,
      "loss": 1.2307,
      "step": 34330
    },
    {
      "epoch": 2.659966304537268,
      "grad_norm": 3.0949559211730957,
      "learning_rate": 3.4193631892311676e-06,
      "loss": 1.2717,
      "step": 34340
    },
    {
      "epoch": 2.660740912875927,
      "grad_norm": 3.111348867416382,
      "learning_rate": 3.411597204245405e-06,
      "loss": 1.277,
      "step": 34350
    },
    {
      "epoch": 2.661515521214586,
      "grad_norm": 2.684532642364502,
      "learning_rate": 3.403831219259643e-06,
      "loss": 1.1721,
      "step": 34360
    },
    {
      "epoch": 2.6622901295532446,
      "grad_norm": 2.146345376968384,
      "learning_rate": 3.3960652342738808e-06,
      "loss": 1.1944,
      "step": 34370
    },
    {
      "epoch": 2.6630647378919035,
      "grad_norm": 2.4176878929138184,
      "learning_rate": 3.388299249288118e-06,
      "loss": 1.2086,
      "step": 34380
    },
    {
      "epoch": 2.6638393462305623,
      "grad_norm": 3.224546194076538,
      "learning_rate": 3.380533264302356e-06,
      "loss": 1.1637,
      "step": 34390
    },
    {
      "epoch": 2.664613954569221,
      "grad_norm": 2.7272984981536865,
      "learning_rate": 3.3727672793165935e-06,
      "loss": 1.2275,
      "step": 34400
    },
    {
      "epoch": 2.6653885629078795,
      "grad_norm": 2.597015142440796,
      "learning_rate": 3.365001294330831e-06,
      "loss": 1.1853,
      "step": 34410
    },
    {
      "epoch": 2.6661631712465383,
      "grad_norm": 2.557849884033203,
      "learning_rate": 3.357235309345069e-06,
      "loss": 1.2732,
      "step": 34420
    },
    {
      "epoch": 2.666937779585197,
      "grad_norm": 2.143803119659424,
      "learning_rate": 3.3494693243593063e-06,
      "loss": 1.2397,
      "step": 34430
    },
    {
      "epoch": 2.667712387923856,
      "grad_norm": 2.832468032836914,
      "learning_rate": 3.341703339373544e-06,
      "loss": 1.3798,
      "step": 34440
    },
    {
      "epoch": 2.668486996262515,
      "grad_norm": 2.006645441055298,
      "learning_rate": 3.3339373543877816e-06,
      "loss": 1.2356,
      "step": 34450
    },
    {
      "epoch": 2.6692616046011737,
      "grad_norm": 2.650759220123291,
      "learning_rate": 3.326171369402019e-06,
      "loss": 1.2408,
      "step": 34460
    },
    {
      "epoch": 2.6700362129398325,
      "grad_norm": 1.6976008415222168,
      "learning_rate": 3.318405384416257e-06,
      "loss": 1.2145,
      "step": 34470
    },
    {
      "epoch": 2.670810821278491,
      "grad_norm": 2.260819435119629,
      "learning_rate": 3.3106393994304943e-06,
      "loss": 1.2426,
      "step": 34480
    },
    {
      "epoch": 2.6715854296171497,
      "grad_norm": 2.502289056777954,
      "learning_rate": 3.302873414444732e-06,
      "loss": 1.2164,
      "step": 34490
    },
    {
      "epoch": 2.6723600379558086,
      "grad_norm": 2.654165506362915,
      "learning_rate": 3.2951074294589696e-06,
      "loss": 1.2353,
      "step": 34500
    },
    {
      "epoch": 2.6731346462944674,
      "grad_norm": 2.677856683731079,
      "learning_rate": 3.2873414444732075e-06,
      "loss": 1.3051,
      "step": 34510
    },
    {
      "epoch": 2.6739092546331262,
      "grad_norm": 2.5891706943511963,
      "learning_rate": 3.279575459487445e-06,
      "loss": 1.2796,
      "step": 34520
    },
    {
      "epoch": 2.6746838629717846,
      "grad_norm": 2.6679906845092773,
      "learning_rate": 3.2718094745016824e-06,
      "loss": 1.2403,
      "step": 34530
    },
    {
      "epoch": 2.675458471310444,
      "grad_norm": 2.3019635677337646,
      "learning_rate": 3.2640434895159207e-06,
      "loss": 1.225,
      "step": 34540
    },
    {
      "epoch": 2.6762330796491023,
      "grad_norm": 2.744192123413086,
      "learning_rate": 3.256277504530158e-06,
      "loss": 1.3305,
      "step": 34550
    },
    {
      "epoch": 2.677007687987761,
      "grad_norm": 3.276669979095459,
      "learning_rate": 3.248511519544396e-06,
      "loss": 1.1964,
      "step": 34560
    },
    {
      "epoch": 2.67778229632642,
      "grad_norm": 2.914318799972534,
      "learning_rate": 3.2407455345586334e-06,
      "loss": 1.2936,
      "step": 34570
    },
    {
      "epoch": 2.678556904665079,
      "grad_norm": 2.516845703125,
      "learning_rate": 3.232979549572871e-06,
      "loss": 1.3934,
      "step": 34580
    },
    {
      "epoch": 2.6793315130037376,
      "grad_norm": 2.4669532775878906,
      "learning_rate": 3.2252135645871087e-06,
      "loss": 1.2659,
      "step": 34590
    },
    {
      "epoch": 2.680106121342396,
      "grad_norm": 2.4674503803253174,
      "learning_rate": 3.217447579601346e-06,
      "loss": 1.3347,
      "step": 34600
    },
    {
      "epoch": 2.6808807296810553,
      "grad_norm": 2.866347312927246,
      "learning_rate": 3.209681594615584e-06,
      "loss": 1.3048,
      "step": 34610
    },
    {
      "epoch": 2.6816553380197137,
      "grad_norm": 2.174619197845459,
      "learning_rate": 3.2019156096298215e-06,
      "loss": 1.2315,
      "step": 34620
    },
    {
      "epoch": 2.6824299463583725,
      "grad_norm": 1.9040958881378174,
      "learning_rate": 3.194149624644059e-06,
      "loss": 1.2937,
      "step": 34630
    },
    {
      "epoch": 2.6832045546970313,
      "grad_norm": 2.6177375316619873,
      "learning_rate": 3.186383639658297e-06,
      "loss": 1.3203,
      "step": 34640
    },
    {
      "epoch": 2.68397916303569,
      "grad_norm": 2.5633349418640137,
      "learning_rate": 3.1786176546725342e-06,
      "loss": 1.2684,
      "step": 34650
    },
    {
      "epoch": 2.684753771374349,
      "grad_norm": 2.377307891845703,
      "learning_rate": 3.170851669686772e-06,
      "loss": 1.3224,
      "step": 34660
    },
    {
      "epoch": 2.6855283797130074,
      "grad_norm": 2.2862112522125244,
      "learning_rate": 3.1630856847010095e-06,
      "loss": 1.232,
      "step": 34670
    },
    {
      "epoch": 2.6863029880516662,
      "grad_norm": 2.8449904918670654,
      "learning_rate": 3.155319699715247e-06,
      "loss": 1.2497,
      "step": 34680
    },
    {
      "epoch": 2.687077596390325,
      "grad_norm": 2.9466264247894287,
      "learning_rate": 3.147553714729485e-06,
      "loss": 1.2931,
      "step": 34690
    },
    {
      "epoch": 2.687852204728984,
      "grad_norm": 2.5380911827087402,
      "learning_rate": 3.1397877297437223e-06,
      "loss": 1.2457,
      "step": 34700
    },
    {
      "epoch": 2.6886268130676427,
      "grad_norm": 2.4567394256591797,
      "learning_rate": 3.13202174475796e-06,
      "loss": 1.2317,
      "step": 34710
    },
    {
      "epoch": 2.6894014214063016,
      "grad_norm": 2.395221471786499,
      "learning_rate": 3.124255759772198e-06,
      "loss": 1.2744,
      "step": 34720
    },
    {
      "epoch": 2.6901760297449604,
      "grad_norm": 2.139726400375366,
      "learning_rate": 3.1164897747864355e-06,
      "loss": 1.1781,
      "step": 34730
    },
    {
      "epoch": 2.690950638083619,
      "grad_norm": 3.2920114994049072,
      "learning_rate": 3.1087237898006733e-06,
      "loss": 1.2568,
      "step": 34740
    },
    {
      "epoch": 2.6917252464222776,
      "grad_norm": 2.7085251808166504,
      "learning_rate": 3.1009578048149108e-06,
      "loss": 1.1301,
      "step": 34750
    },
    {
      "epoch": 2.6924998547609365,
      "grad_norm": 2.2019858360290527,
      "learning_rate": 3.0931918198291486e-06,
      "loss": 1.2265,
      "step": 34760
    },
    {
      "epoch": 2.6932744630995953,
      "grad_norm": 2.6620242595672607,
      "learning_rate": 3.085425834843386e-06,
      "loss": 1.2433,
      "step": 34770
    },
    {
      "epoch": 2.694049071438254,
      "grad_norm": 3.0266995429992676,
      "learning_rate": 3.077659849857624e-06,
      "loss": 1.263,
      "step": 34780
    },
    {
      "epoch": 2.694823679776913,
      "grad_norm": 2.629142999649048,
      "learning_rate": 3.0698938648718614e-06,
      "loss": 1.2582,
      "step": 34790
    },
    {
      "epoch": 2.695598288115572,
      "grad_norm": 3.1063010692596436,
      "learning_rate": 3.062127879886099e-06,
      "loss": 1.1704,
      "step": 34800
    },
    {
      "epoch": 2.69637289645423,
      "grad_norm": 2.1131114959716797,
      "learning_rate": 3.0543618949003367e-06,
      "loss": 1.1785,
      "step": 34810
    },
    {
      "epoch": 2.697147504792889,
      "grad_norm": 2.61392879486084,
      "learning_rate": 3.046595909914574e-06,
      "loss": 1.1818,
      "step": 34820
    },
    {
      "epoch": 2.697922113131548,
      "grad_norm": 2.1088178157806396,
      "learning_rate": 3.038829924928812e-06,
      "loss": 1.2752,
      "step": 34830
    },
    {
      "epoch": 2.6986967214702067,
      "grad_norm": 2.509023666381836,
      "learning_rate": 3.0310639399430495e-06,
      "loss": 1.3781,
      "step": 34840
    },
    {
      "epoch": 2.6994713298088655,
      "grad_norm": 2.482741355895996,
      "learning_rate": 3.023297954957287e-06,
      "loss": 1.2666,
      "step": 34850
    },
    {
      "epoch": 2.700245938147524,
      "grad_norm": 2.520216464996338,
      "learning_rate": 3.0155319699715248e-06,
      "loss": 1.1826,
      "step": 34860
    },
    {
      "epoch": 2.701020546486183,
      "grad_norm": 1.947350263595581,
      "learning_rate": 3.007765984985762e-06,
      "loss": 1.2813,
      "step": 34870
    },
    {
      "epoch": 2.7017951548248416,
      "grad_norm": 2.621899127960205,
      "learning_rate": 3e-06,
      "loss": 1.2125,
      "step": 34880
    },
    {
      "epoch": 2.7025697631635004,
      "grad_norm": 1.9700618982315063,
      "learning_rate": 2.9922340150142375e-06,
      "loss": 1.2412,
      "step": 34890
    },
    {
      "epoch": 2.7033443715021592,
      "grad_norm": 2.2763724327087402,
      "learning_rate": 2.9844680300284754e-06,
      "loss": 1.3569,
      "step": 34900
    },
    {
      "epoch": 2.704118979840818,
      "grad_norm": 2.4760899543762207,
      "learning_rate": 2.9767020450427133e-06,
      "loss": 1.1982,
      "step": 34910
    },
    {
      "epoch": 2.704893588179477,
      "grad_norm": 2.4691286087036133,
      "learning_rate": 2.9689360600569507e-06,
      "loss": 1.2412,
      "step": 34920
    },
    {
      "epoch": 2.7056681965181353,
      "grad_norm": 3.521450996398926,
      "learning_rate": 2.9611700750711886e-06,
      "loss": 1.1295,
      "step": 34930
    },
    {
      "epoch": 2.7064428048567946,
      "grad_norm": 2.532883882522583,
      "learning_rate": 2.953404090085426e-06,
      "loss": 1.2499,
      "step": 34940
    },
    {
      "epoch": 2.707217413195453,
      "grad_norm": 2.743110179901123,
      "learning_rate": 2.9456381050996634e-06,
      "loss": 1.263,
      "step": 34950
    },
    {
      "epoch": 2.707992021534112,
      "grad_norm": 2.314418077468872,
      "learning_rate": 2.9378721201139013e-06,
      "loss": 1.2199,
      "step": 34960
    },
    {
      "epoch": 2.7087666298727706,
      "grad_norm": 4.092759132385254,
      "learning_rate": 2.9301061351281388e-06,
      "loss": 1.214,
      "step": 34970
    },
    {
      "epoch": 2.7095412382114294,
      "grad_norm": 4.390844821929932,
      "learning_rate": 2.9223401501423766e-06,
      "loss": 1.2407,
      "step": 34980
    },
    {
      "epoch": 2.7103158465500883,
      "grad_norm": 2.838918447494507,
      "learning_rate": 2.914574165156614e-06,
      "loss": 1.2836,
      "step": 34990
    },
    {
      "epoch": 2.7110904548887467,
      "grad_norm": 2.413886308670044,
      "learning_rate": 2.9068081801708515e-06,
      "loss": 1.2227,
      "step": 35000
    },
    {
      "epoch": 2.7118650632274055,
      "grad_norm": 2.976694107055664,
      "learning_rate": 2.8990421951850894e-06,
      "loss": 1.2586,
      "step": 35010
    },
    {
      "epoch": 2.7126396715660643,
      "grad_norm": 2.5740065574645996,
      "learning_rate": 2.891276210199327e-06,
      "loss": 1.2491,
      "step": 35020
    },
    {
      "epoch": 2.713414279904723,
      "grad_norm": 2.244414806365967,
      "learning_rate": 2.8835102252135647e-06,
      "loss": 1.2699,
      "step": 35030
    },
    {
      "epoch": 2.714188888243382,
      "grad_norm": 2.418099880218506,
      "learning_rate": 2.875744240227802e-06,
      "loss": 1.3346,
      "step": 35040
    },
    {
      "epoch": 2.714963496582041,
      "grad_norm": 1.9890252351760864,
      "learning_rate": 2.86797825524204e-06,
      "loss": 1.3599,
      "step": 35050
    },
    {
      "epoch": 2.7157381049206997,
      "grad_norm": 2.552515745162964,
      "learning_rate": 2.8602122702562774e-06,
      "loss": 1.2489,
      "step": 35060
    },
    {
      "epoch": 2.716512713259358,
      "grad_norm": 2.656297206878662,
      "learning_rate": 2.852446285270515e-06,
      "loss": 1.1802,
      "step": 35070
    },
    {
      "epoch": 2.717287321598017,
      "grad_norm": 2.574099063873291,
      "learning_rate": 2.844680300284753e-06,
      "loss": 1.1816,
      "step": 35080
    },
    {
      "epoch": 2.7180619299366757,
      "grad_norm": 2.5022659301757812,
      "learning_rate": 2.8369143152989906e-06,
      "loss": 1.2178,
      "step": 35090
    },
    {
      "epoch": 2.7188365382753346,
      "grad_norm": 2.376614570617676,
      "learning_rate": 2.8291483303132285e-06,
      "loss": 1.3128,
      "step": 35100
    },
    {
      "epoch": 2.7196111466139934,
      "grad_norm": 2.2043228149414062,
      "learning_rate": 2.821382345327466e-06,
      "loss": 1.1524,
      "step": 35110
    },
    {
      "epoch": 2.7203857549526522,
      "grad_norm": 2.4589478969573975,
      "learning_rate": 2.8136163603417034e-06,
      "loss": 1.2877,
      "step": 35120
    },
    {
      "epoch": 2.721160363291311,
      "grad_norm": 2.342540740966797,
      "learning_rate": 2.8058503753559412e-06,
      "loss": 1.2213,
      "step": 35130
    },
    {
      "epoch": 2.7219349716299694,
      "grad_norm": 2.596890926361084,
      "learning_rate": 2.7980843903701787e-06,
      "loss": 1.2472,
      "step": 35140
    },
    {
      "epoch": 2.7227095799686283,
      "grad_norm": 2.492793321609497,
      "learning_rate": 2.7903184053844165e-06,
      "loss": 1.3479,
      "step": 35150
    },
    {
      "epoch": 2.723484188307287,
      "grad_norm": 2.393728017807007,
      "learning_rate": 2.782552420398654e-06,
      "loss": 1.2712,
      "step": 35160
    },
    {
      "epoch": 2.724258796645946,
      "grad_norm": 2.5177927017211914,
      "learning_rate": 2.7747864354128914e-06,
      "loss": 1.2998,
      "step": 35170
    },
    {
      "epoch": 2.725033404984605,
      "grad_norm": 2.7371044158935547,
      "learning_rate": 2.7670204504271293e-06,
      "loss": 1.329,
      "step": 35180
    },
    {
      "epoch": 2.725808013323263,
      "grad_norm": 2.5067310333251953,
      "learning_rate": 2.7592544654413667e-06,
      "loss": 1.2343,
      "step": 35190
    },
    {
      "epoch": 2.7265826216619224,
      "grad_norm": 2.372692584991455,
      "learning_rate": 2.7514884804556046e-06,
      "loss": 1.2965,
      "step": 35200
    },
    {
      "epoch": 2.727357230000581,
      "grad_norm": 2.2251193523406982,
      "learning_rate": 2.743722495469842e-06,
      "loss": 1.222,
      "step": 35210
    },
    {
      "epoch": 2.7281318383392397,
      "grad_norm": 1.9714319705963135,
      "learning_rate": 2.7359565104840795e-06,
      "loss": 1.2263,
      "step": 35220
    },
    {
      "epoch": 2.7289064466778985,
      "grad_norm": 2.7351291179656982,
      "learning_rate": 2.7281905254983173e-06,
      "loss": 1.2832,
      "step": 35230
    },
    {
      "epoch": 2.7296810550165573,
      "grad_norm": 2.665924549102783,
      "learning_rate": 2.7204245405125548e-06,
      "loss": 1.2107,
      "step": 35240
    },
    {
      "epoch": 2.730455663355216,
      "grad_norm": 2.274737596511841,
      "learning_rate": 2.7126585555267927e-06,
      "loss": 1.3514,
      "step": 35250
    },
    {
      "epoch": 2.7312302716938746,
      "grad_norm": 2.8321611881256104,
      "learning_rate": 2.7048925705410305e-06,
      "loss": 1.2779,
      "step": 35260
    },
    {
      "epoch": 2.7320048800325334,
      "grad_norm": 3.0839507579803467,
      "learning_rate": 2.6971265855552684e-06,
      "loss": 1.3263,
      "step": 35270
    },
    {
      "epoch": 2.7327794883711922,
      "grad_norm": 2.2773866653442383,
      "learning_rate": 2.689360600569506e-06,
      "loss": 1.1255,
      "step": 35280
    },
    {
      "epoch": 2.733554096709851,
      "grad_norm": 2.4816346168518066,
      "learning_rate": 2.6815946155837433e-06,
      "loss": 1.2014,
      "step": 35290
    },
    {
      "epoch": 2.73432870504851,
      "grad_norm": 3.7749154567718506,
      "learning_rate": 2.673828630597981e-06,
      "loss": 1.1968,
      "step": 35300
    },
    {
      "epoch": 2.7351033133871687,
      "grad_norm": 2.0563862323760986,
      "learning_rate": 2.6660626456122186e-06,
      "loss": 1.3328,
      "step": 35310
    },
    {
      "epoch": 2.7358779217258276,
      "grad_norm": 2.122663974761963,
      "learning_rate": 2.6582966606264564e-06,
      "loss": 1.3224,
      "step": 35320
    },
    {
      "epoch": 2.736652530064486,
      "grad_norm": 2.662532329559326,
      "learning_rate": 2.650530675640694e-06,
      "loss": 1.2561,
      "step": 35330
    },
    {
      "epoch": 2.7374271384031448,
      "grad_norm": 2.5177505016326904,
      "learning_rate": 2.6427646906549313e-06,
      "loss": 1.2781,
      "step": 35340
    },
    {
      "epoch": 2.7382017467418036,
      "grad_norm": 2.4497432708740234,
      "learning_rate": 2.634998705669169e-06,
      "loss": 1.2634,
      "step": 35350
    },
    {
      "epoch": 2.7389763550804624,
      "grad_norm": 2.5484793186187744,
      "learning_rate": 2.6272327206834066e-06,
      "loss": 1.1891,
      "step": 35360
    },
    {
      "epoch": 2.7397509634191213,
      "grad_norm": 3.0315592288970947,
      "learning_rate": 2.6194667356976445e-06,
      "loss": 1.3114,
      "step": 35370
    },
    {
      "epoch": 2.74052557175778,
      "grad_norm": 2.8820273876190186,
      "learning_rate": 2.611700750711882e-06,
      "loss": 1.2462,
      "step": 35380
    },
    {
      "epoch": 2.741300180096439,
      "grad_norm": 2.241828203201294,
      "learning_rate": 2.6039347657261194e-06,
      "loss": 1.3058,
      "step": 35390
    },
    {
      "epoch": 2.7420747884350973,
      "grad_norm": 2.473081350326538,
      "learning_rate": 2.5961687807403573e-06,
      "loss": 1.2828,
      "step": 35400
    },
    {
      "epoch": 2.742849396773756,
      "grad_norm": 2.7905049324035645,
      "learning_rate": 2.5884027957545947e-06,
      "loss": 1.3059,
      "step": 35410
    },
    {
      "epoch": 2.743624005112415,
      "grad_norm": 3.591989517211914,
      "learning_rate": 2.5806368107688326e-06,
      "loss": 1.2772,
      "step": 35420
    },
    {
      "epoch": 2.744398613451074,
      "grad_norm": 2.6995034217834473,
      "learning_rate": 2.57287082578307e-06,
      "loss": 1.1459,
      "step": 35430
    },
    {
      "epoch": 2.7451732217897327,
      "grad_norm": 2.6964588165283203,
      "learning_rate": 2.565104840797308e-06,
      "loss": 1.2073,
      "step": 35440
    },
    {
      "epoch": 2.7459478301283915,
      "grad_norm": 2.6343772411346436,
      "learning_rate": 2.5573388558115457e-06,
      "loss": 1.2621,
      "step": 35450
    },
    {
      "epoch": 2.7467224384670503,
      "grad_norm": 2.2764275074005127,
      "learning_rate": 2.549572870825783e-06,
      "loss": 1.2593,
      "step": 35460
    },
    {
      "epoch": 2.7474970468057087,
      "grad_norm": 2.3633415699005127,
      "learning_rate": 2.541806885840021e-06,
      "loss": 1.256,
      "step": 35470
    },
    {
      "epoch": 2.7482716551443676,
      "grad_norm": 2.2691240310668945,
      "learning_rate": 2.5340409008542585e-06,
      "loss": 1.287,
      "step": 35480
    },
    {
      "epoch": 2.7490462634830264,
      "grad_norm": 2.307703971862793,
      "learning_rate": 2.526274915868496e-06,
      "loss": 1.2915,
      "step": 35490
    },
    {
      "epoch": 2.749820871821685,
      "grad_norm": 2.7443010807037354,
      "learning_rate": 2.518508930882734e-06,
      "loss": 1.1517,
      "step": 35500
    },
    {
      "epoch": 2.750595480160344,
      "grad_norm": 2.387122392654419,
      "learning_rate": 2.5107429458969712e-06,
      "loss": 1.2381,
      "step": 35510
    },
    {
      "epoch": 2.7513700884990024,
      "grad_norm": 2.939255952835083,
      "learning_rate": 2.502976960911209e-06,
      "loss": 1.2884,
      "step": 35520
    },
    {
      "epoch": 2.7521446968376617,
      "grad_norm": 2.0988266468048096,
      "learning_rate": 2.4952109759254465e-06,
      "loss": 1.2705,
      "step": 35530
    },
    {
      "epoch": 2.75291930517632,
      "grad_norm": 2.1666855812072754,
      "learning_rate": 2.4874449909396844e-06,
      "loss": 1.3309,
      "step": 35540
    },
    {
      "epoch": 2.753693913514979,
      "grad_norm": 2.8309106826782227,
      "learning_rate": 2.479679005953922e-06,
      "loss": 1.21,
      "step": 35550
    },
    {
      "epoch": 2.7544685218536378,
      "grad_norm": 2.6785225868225098,
      "learning_rate": 2.4719130209681593e-06,
      "loss": 1.3728,
      "step": 35560
    },
    {
      "epoch": 2.7552431301922966,
      "grad_norm": 2.2409508228302,
      "learning_rate": 2.464147035982397e-06,
      "loss": 1.3411,
      "step": 35570
    },
    {
      "epoch": 2.7560177385309554,
      "grad_norm": 2.4335646629333496,
      "learning_rate": 2.4563810509966346e-06,
      "loss": 1.2163,
      "step": 35580
    },
    {
      "epoch": 2.756792346869614,
      "grad_norm": 2.1914148330688477,
      "learning_rate": 2.4486150660108725e-06,
      "loss": 1.2395,
      "step": 35590
    },
    {
      "epoch": 2.7575669552082727,
      "grad_norm": 2.1940383911132812,
      "learning_rate": 2.44084908102511e-06,
      "loss": 1.1506,
      "step": 35600
    },
    {
      "epoch": 2.7583415635469315,
      "grad_norm": 2.6409521102905273,
      "learning_rate": 2.4330830960393474e-06,
      "loss": 1.2224,
      "step": 35610
    },
    {
      "epoch": 2.7591161718855903,
      "grad_norm": 2.2311904430389404,
      "learning_rate": 2.4253171110535857e-06,
      "loss": 1.3243,
      "step": 35620
    },
    {
      "epoch": 2.759890780224249,
      "grad_norm": 2.921445369720459,
      "learning_rate": 2.417551126067823e-06,
      "loss": 1.2897,
      "step": 35630
    },
    {
      "epoch": 2.760665388562908,
      "grad_norm": 2.223555326461792,
      "learning_rate": 2.409785141082061e-06,
      "loss": 1.3185,
      "step": 35640
    },
    {
      "epoch": 2.761439996901567,
      "grad_norm": 2.476571559906006,
      "learning_rate": 2.4020191560962984e-06,
      "loss": 1.2975,
      "step": 35650
    },
    {
      "epoch": 2.762214605240225,
      "grad_norm": 2.6076550483703613,
      "learning_rate": 2.394253171110536e-06,
      "loss": 1.3434,
      "step": 35660
    },
    {
      "epoch": 2.762989213578884,
      "grad_norm": 2.034914493560791,
      "learning_rate": 2.3864871861247737e-06,
      "loss": 1.2965,
      "step": 35670
    },
    {
      "epoch": 2.763763821917543,
      "grad_norm": 2.6336870193481445,
      "learning_rate": 2.378721201139011e-06,
      "loss": 1.1543,
      "step": 35680
    },
    {
      "epoch": 2.7645384302562017,
      "grad_norm": 2.359689235687256,
      "learning_rate": 2.370955216153249e-06,
      "loss": 1.2908,
      "step": 35690
    },
    {
      "epoch": 2.7653130385948606,
      "grad_norm": 2.8419225215911865,
      "learning_rate": 2.3631892311674865e-06,
      "loss": 1.1435,
      "step": 35700
    },
    {
      "epoch": 2.7660876469335194,
      "grad_norm": 2.5143930912017822,
      "learning_rate": 2.355423246181724e-06,
      "loss": 1.2241,
      "step": 35710
    },
    {
      "epoch": 2.766862255272178,
      "grad_norm": 2.1573874950408936,
      "learning_rate": 2.3476572611959618e-06,
      "loss": 1.2461,
      "step": 35720
    },
    {
      "epoch": 2.7676368636108366,
      "grad_norm": 3.285039186477661,
      "learning_rate": 2.3398912762101992e-06,
      "loss": 1.2982,
      "step": 35730
    },
    {
      "epoch": 2.7684114719494954,
      "grad_norm": 2.8807287216186523,
      "learning_rate": 2.332125291224437e-06,
      "loss": 1.2458,
      "step": 35740
    },
    {
      "epoch": 2.7691860802881543,
      "grad_norm": 2.756960391998291,
      "learning_rate": 2.3243593062386745e-06,
      "loss": 1.2108,
      "step": 35750
    },
    {
      "epoch": 2.769960688626813,
      "grad_norm": 2.236550807952881,
      "learning_rate": 2.3165933212529124e-06,
      "loss": 1.2247,
      "step": 35760
    },
    {
      "epoch": 2.770735296965472,
      "grad_norm": 2.4847238063812256,
      "learning_rate": 2.30882733626715e-06,
      "loss": 1.266,
      "step": 35770
    },
    {
      "epoch": 2.7715099053041303,
      "grad_norm": 2.873786449432373,
      "learning_rate": 2.3010613512813873e-06,
      "loss": 1.2059,
      "step": 35780
    },
    {
      "epoch": 2.7722845136427896,
      "grad_norm": 2.3386178016662598,
      "learning_rate": 2.293295366295625e-06,
      "loss": 1.3134,
      "step": 35790
    },
    {
      "epoch": 2.773059121981448,
      "grad_norm": 3.034435749053955,
      "learning_rate": 2.285529381309863e-06,
      "loss": 1.2339,
      "step": 35800
    },
    {
      "epoch": 2.773833730320107,
      "grad_norm": 3.1950948238372803,
      "learning_rate": 2.277763396324101e-06,
      "loss": 1.2804,
      "step": 35810
    },
    {
      "epoch": 2.7746083386587657,
      "grad_norm": 2.07795786857605,
      "learning_rate": 2.2699974113383383e-06,
      "loss": 1.2422,
      "step": 35820
    },
    {
      "epoch": 2.7753829469974245,
      "grad_norm": 2.6688320636749268,
      "learning_rate": 2.2622314263525758e-06,
      "loss": 1.2486,
      "step": 35830
    },
    {
      "epoch": 2.7761575553360833,
      "grad_norm": 2.281123638153076,
      "learning_rate": 2.2544654413668136e-06,
      "loss": 1.1757,
      "step": 35840
    },
    {
      "epoch": 2.7769321636747417,
      "grad_norm": 2.3585004806518555,
      "learning_rate": 2.246699456381051e-06,
      "loss": 1.231,
      "step": 35850
    },
    {
      "epoch": 2.777706772013401,
      "grad_norm": 3.162862777709961,
      "learning_rate": 2.238933471395289e-06,
      "loss": 1.2621,
      "step": 35860
    },
    {
      "epoch": 2.7784813803520594,
      "grad_norm": 1.9648163318634033,
      "learning_rate": 2.2311674864095264e-06,
      "loss": 1.2613,
      "step": 35870
    },
    {
      "epoch": 2.779255988690718,
      "grad_norm": 2.821753740310669,
      "learning_rate": 2.223401501423764e-06,
      "loss": 1.0935,
      "step": 35880
    },
    {
      "epoch": 2.780030597029377,
      "grad_norm": 2.2783002853393555,
      "learning_rate": 2.2156355164380017e-06,
      "loss": 1.3421,
      "step": 35890
    },
    {
      "epoch": 2.780805205368036,
      "grad_norm": 2.4702703952789307,
      "learning_rate": 2.207869531452239e-06,
      "loss": 1.2766,
      "step": 35900
    },
    {
      "epoch": 2.7815798137066947,
      "grad_norm": 2.817538261413574,
      "learning_rate": 2.200103546466477e-06,
      "loss": 1.1808,
      "step": 35910
    },
    {
      "epoch": 2.782354422045353,
      "grad_norm": 1.8421285152435303,
      "learning_rate": 2.1923375614807144e-06,
      "loss": 1.1896,
      "step": 35920
    },
    {
      "epoch": 2.783129030384012,
      "grad_norm": 2.465428113937378,
      "learning_rate": 2.184571576494952e-06,
      "loss": 1.2704,
      "step": 35930
    },
    {
      "epoch": 2.7839036387226708,
      "grad_norm": 3.2093262672424316,
      "learning_rate": 2.1768055915091897e-06,
      "loss": 1.3104,
      "step": 35940
    },
    {
      "epoch": 2.7846782470613296,
      "grad_norm": 2.8580236434936523,
      "learning_rate": 2.169039606523427e-06,
      "loss": 1.2225,
      "step": 35950
    },
    {
      "epoch": 2.7854528553999884,
      "grad_norm": 2.413449764251709,
      "learning_rate": 2.161273621537665e-06,
      "loss": 1.2414,
      "step": 35960
    },
    {
      "epoch": 2.7862274637386473,
      "grad_norm": 2.223952054977417,
      "learning_rate": 2.1535076365519025e-06,
      "loss": 1.2284,
      "step": 35970
    },
    {
      "epoch": 2.787002072077306,
      "grad_norm": 3.2866051197052,
      "learning_rate": 2.1457416515661404e-06,
      "loss": 1.2591,
      "step": 35980
    },
    {
      "epoch": 2.7877766804159645,
      "grad_norm": 2.167056083679199,
      "learning_rate": 2.1379756665803782e-06,
      "loss": 1.216,
      "step": 35990
    },
    {
      "epoch": 2.7885512887546233,
      "grad_norm": 3.3700780868530273,
      "learning_rate": 2.1302096815946157e-06,
      "loss": 1.3213,
      "step": 36000
    },
    {
      "epoch": 2.789325897093282,
      "grad_norm": 2.168168544769287,
      "learning_rate": 2.1224436966088535e-06,
      "loss": 1.3801,
      "step": 36010
    },
    {
      "epoch": 2.790100505431941,
      "grad_norm": 2.9354195594787598,
      "learning_rate": 2.114677711623091e-06,
      "loss": 1.1811,
      "step": 36020
    },
    {
      "epoch": 2.7908751137706,
      "grad_norm": 2.4591808319091797,
      "learning_rate": 2.106911726637329e-06,
      "loss": 1.2569,
      "step": 36030
    },
    {
      "epoch": 2.7916497221092587,
      "grad_norm": 3.156938314437866,
      "learning_rate": 2.0991457416515663e-06,
      "loss": 1.2293,
      "step": 36040
    },
    {
      "epoch": 2.7924243304479175,
      "grad_norm": 3.2531182765960693,
      "learning_rate": 2.0913797566658037e-06,
      "loss": 1.1763,
      "step": 36050
    },
    {
      "epoch": 2.793198938786576,
      "grad_norm": 2.4358105659484863,
      "learning_rate": 2.0836137716800416e-06,
      "loss": 1.2508,
      "step": 36060
    },
    {
      "epoch": 2.7939735471252347,
      "grad_norm": 2.539064407348633,
      "learning_rate": 2.075847786694279e-06,
      "loss": 1.293,
      "step": 36070
    },
    {
      "epoch": 2.7947481554638935,
      "grad_norm": 2.3522236347198486,
      "learning_rate": 2.068081801708517e-06,
      "loss": 1.1758,
      "step": 36080
    },
    {
      "epoch": 2.7955227638025524,
      "grad_norm": 2.9256019592285156,
      "learning_rate": 2.0603158167227543e-06,
      "loss": 1.1533,
      "step": 36090
    },
    {
      "epoch": 2.796297372141211,
      "grad_norm": 2.21545672416687,
      "learning_rate": 2.052549831736992e-06,
      "loss": 1.1614,
      "step": 36100
    },
    {
      "epoch": 2.7970719804798696,
      "grad_norm": 2.2363247871398926,
      "learning_rate": 2.0447838467512297e-06,
      "loss": 1.2706,
      "step": 36110
    },
    {
      "epoch": 2.797846588818529,
      "grad_norm": 2.2958824634552,
      "learning_rate": 2.037017861765467e-06,
      "loss": 1.1988,
      "step": 36120
    },
    {
      "epoch": 2.7986211971571873,
      "grad_norm": 2.3124077320098877,
      "learning_rate": 2.029251876779705e-06,
      "loss": 1.3058,
      "step": 36130
    },
    {
      "epoch": 2.799395805495846,
      "grad_norm": 2.7233071327209473,
      "learning_rate": 2.0214858917939424e-06,
      "loss": 1.2092,
      "step": 36140
    },
    {
      "epoch": 2.800170413834505,
      "grad_norm": 2.041933536529541,
      "learning_rate": 2.01371990680818e-06,
      "loss": 1.2771,
      "step": 36150
    },
    {
      "epoch": 2.8009450221731638,
      "grad_norm": 2.628469705581665,
      "learning_rate": 2.005953921822418e-06,
      "loss": 1.2991,
      "step": 36160
    },
    {
      "epoch": 2.8017196305118226,
      "grad_norm": 3.3796777725219727,
      "learning_rate": 1.9981879368366556e-06,
      "loss": 1.2049,
      "step": 36170
    },
    {
      "epoch": 2.802494238850481,
      "grad_norm": 3.256159543991089,
      "learning_rate": 1.9904219518508934e-06,
      "loss": 1.2827,
      "step": 36180
    },
    {
      "epoch": 2.8032688471891403,
      "grad_norm": 2.1210286617279053,
      "learning_rate": 1.982655966865131e-06,
      "loss": 1.2236,
      "step": 36190
    },
    {
      "epoch": 2.8040434555277987,
      "grad_norm": 2.4296536445617676,
      "learning_rate": 1.9748899818793683e-06,
      "loss": 1.2201,
      "step": 36200
    },
    {
      "epoch": 2.8048180638664575,
      "grad_norm": 3.4354238510131836,
      "learning_rate": 1.967123996893606e-06,
      "loss": 1.2427,
      "step": 36210
    },
    {
      "epoch": 2.8055926722051163,
      "grad_norm": 2.97819185256958,
      "learning_rate": 1.9593580119078436e-06,
      "loss": 1.29,
      "step": 36220
    },
    {
      "epoch": 2.806367280543775,
      "grad_norm": 2.292621612548828,
      "learning_rate": 1.9515920269220815e-06,
      "loss": 1.2674,
      "step": 36230
    },
    {
      "epoch": 2.807141888882434,
      "grad_norm": 1.889117956161499,
      "learning_rate": 1.943826041936319e-06,
      "loss": 1.2699,
      "step": 36240
    },
    {
      "epoch": 2.8079164972210924,
      "grad_norm": 2.6870429515838623,
      "learning_rate": 1.9360600569505564e-06,
      "loss": 1.2413,
      "step": 36250
    },
    {
      "epoch": 2.808691105559751,
      "grad_norm": 2.355212450027466,
      "learning_rate": 1.9282940719647943e-06,
      "loss": 1.2396,
      "step": 36260
    },
    {
      "epoch": 2.80946571389841,
      "grad_norm": 2.4278604984283447,
      "learning_rate": 1.9205280869790317e-06,
      "loss": 1.3105,
      "step": 36270
    },
    {
      "epoch": 2.810240322237069,
      "grad_norm": 2.7706873416900635,
      "learning_rate": 1.9127621019932696e-06,
      "loss": 1.2551,
      "step": 36280
    },
    {
      "epoch": 2.8110149305757277,
      "grad_norm": 2.465589761734009,
      "learning_rate": 1.904996117007507e-06,
      "loss": 1.1782,
      "step": 36290
    },
    {
      "epoch": 2.8117895389143865,
      "grad_norm": 2.972989797592163,
      "learning_rate": 1.897230132021745e-06,
      "loss": 1.2334,
      "step": 36300
    },
    {
      "epoch": 2.8125641472530454,
      "grad_norm": 2.285726308822632,
      "learning_rate": 1.8902407455345585e-06,
      "loss": 1.2124,
      "step": 36310
    },
    {
      "epoch": 2.8133387555917038,
      "grad_norm": 2.495513677597046,
      "learning_rate": 1.8824747605487964e-06,
      "loss": 1.2575,
      "step": 36320
    },
    {
      "epoch": 2.8141133639303626,
      "grad_norm": 2.2482569217681885,
      "learning_rate": 1.874708775563034e-06,
      "loss": 1.2385,
      "step": 36330
    },
    {
      "epoch": 2.8148879722690214,
      "grad_norm": 2.4571261405944824,
      "learning_rate": 1.8669427905772717e-06,
      "loss": 1.2861,
      "step": 36340
    },
    {
      "epoch": 2.8156625806076803,
      "grad_norm": 2.4150924682617188,
      "learning_rate": 1.8591768055915093e-06,
      "loss": 1.158,
      "step": 36350
    },
    {
      "epoch": 2.816437188946339,
      "grad_norm": 2.1562607288360596,
      "learning_rate": 1.851410820605747e-06,
      "loss": 1.3094,
      "step": 36360
    },
    {
      "epoch": 2.817211797284998,
      "grad_norm": 2.372114419937134,
      "learning_rate": 1.8436448356199844e-06,
      "loss": 1.2981,
      "step": 36370
    },
    {
      "epoch": 2.8179864056236568,
      "grad_norm": 1.7948071956634521,
      "learning_rate": 1.835878850634222e-06,
      "loss": 1.2319,
      "step": 36380
    },
    {
      "epoch": 2.818761013962315,
      "grad_norm": 3.256272554397583,
      "learning_rate": 1.8281128656484597e-06,
      "loss": 1.0918,
      "step": 36390
    },
    {
      "epoch": 2.819535622300974,
      "grad_norm": 2.52871036529541,
      "learning_rate": 1.8203468806626974e-06,
      "loss": 1.2443,
      "step": 36400
    },
    {
      "epoch": 2.820310230639633,
      "grad_norm": 2.1203207969665527,
      "learning_rate": 1.812580895676935e-06,
      "loss": 1.1323,
      "step": 36410
    },
    {
      "epoch": 2.8210848389782917,
      "grad_norm": 2.8260204792022705,
      "learning_rate": 1.804814910691173e-06,
      "loss": 1.3021,
      "step": 36420
    },
    {
      "epoch": 2.8218594473169505,
      "grad_norm": 2.51309871673584,
      "learning_rate": 1.7970489257054103e-06,
      "loss": 1.3845,
      "step": 36430
    },
    {
      "epoch": 2.822634055655609,
      "grad_norm": 2.8364439010620117,
      "learning_rate": 1.789282940719648e-06,
      "loss": 1.22,
      "step": 36440
    },
    {
      "epoch": 2.823408663994268,
      "grad_norm": 2.2836086750030518,
      "learning_rate": 1.7815169557338857e-06,
      "loss": 1.14,
      "step": 36450
    },
    {
      "epoch": 2.8241832723329265,
      "grad_norm": 3.017449140548706,
      "learning_rate": 1.7737509707481233e-06,
      "loss": 1.2531,
      "step": 36460
    },
    {
      "epoch": 2.8249578806715854,
      "grad_norm": 3.1923017501831055,
      "learning_rate": 1.765984985762361e-06,
      "loss": 1.2467,
      "step": 36470
    },
    {
      "epoch": 2.825732489010244,
      "grad_norm": 2.7432780265808105,
      "learning_rate": 1.7582190007765984e-06,
      "loss": 1.2295,
      "step": 36480
    },
    {
      "epoch": 2.826507097348903,
      "grad_norm": 2.939220428466797,
      "learning_rate": 1.750453015790836e-06,
      "loss": 1.332,
      "step": 36490
    },
    {
      "epoch": 2.827281705687562,
      "grad_norm": 2.852299213409424,
      "learning_rate": 1.7426870308050737e-06,
      "loss": 1.1792,
      "step": 36500
    },
    {
      "epoch": 2.8280563140262203,
      "grad_norm": 3.7280333042144775,
      "learning_rate": 1.7349210458193116e-06,
      "loss": 1.2841,
      "step": 36510
    },
    {
      "epoch": 2.828830922364879,
      "grad_norm": 3.7311229705810547,
      "learning_rate": 1.7271550608335492e-06,
      "loss": 1.222,
      "step": 36520
    },
    {
      "epoch": 2.829605530703538,
      "grad_norm": 2.1884593963623047,
      "learning_rate": 1.7193890758477869e-06,
      "loss": 1.2343,
      "step": 36530
    },
    {
      "epoch": 2.8303801390421968,
      "grad_norm": 3.2609152793884277,
      "learning_rate": 1.7116230908620243e-06,
      "loss": 1.1602,
      "step": 36540
    },
    {
      "epoch": 2.8311547473808556,
      "grad_norm": 2.566023111343384,
      "learning_rate": 1.703857105876262e-06,
      "loss": 1.2187,
      "step": 36550
    },
    {
      "epoch": 2.8319293557195144,
      "grad_norm": 2.319959878921509,
      "learning_rate": 1.6960911208904996e-06,
      "loss": 1.2441,
      "step": 36560
    },
    {
      "epoch": 2.8327039640581733,
      "grad_norm": 2.0465645790100098,
      "learning_rate": 1.6883251359047373e-06,
      "loss": 1.1912,
      "step": 36570
    },
    {
      "epoch": 2.8334785723968317,
      "grad_norm": 2.990511178970337,
      "learning_rate": 1.680559150918975e-06,
      "loss": 1.2426,
      "step": 36580
    },
    {
      "epoch": 2.8342531807354905,
      "grad_norm": 2.7387406826019287,
      "learning_rate": 1.6727931659332124e-06,
      "loss": 1.2355,
      "step": 36590
    },
    {
      "epoch": 2.8350277890741493,
      "grad_norm": 2.3752763271331787,
      "learning_rate": 1.6650271809474503e-06,
      "loss": 1.2892,
      "step": 36600
    },
    {
      "epoch": 2.835802397412808,
      "grad_norm": 2.6748135089874268,
      "learning_rate": 1.657261195961688e-06,
      "loss": 1.293,
      "step": 36610
    },
    {
      "epoch": 2.836577005751467,
      "grad_norm": 2.6479833126068115,
      "learning_rate": 1.6494952109759256e-06,
      "loss": 1.2327,
      "step": 36620
    },
    {
      "epoch": 2.837351614090126,
      "grad_norm": 2.3876423835754395,
      "learning_rate": 1.6417292259901632e-06,
      "loss": 1.2841,
      "step": 36630
    },
    {
      "epoch": 2.8381262224287847,
      "grad_norm": 2.122269630432129,
      "learning_rate": 1.6339632410044009e-06,
      "loss": 1.1778,
      "step": 36640
    },
    {
      "epoch": 2.838900830767443,
      "grad_norm": 3.05108904838562,
      "learning_rate": 1.6261972560186383e-06,
      "loss": 1.3121,
      "step": 36650
    },
    {
      "epoch": 2.839675439106102,
      "grad_norm": 3.0465991497039795,
      "learning_rate": 1.618431271032876e-06,
      "loss": 1.2324,
      "step": 36660
    },
    {
      "epoch": 2.8404500474447607,
      "grad_norm": 2.6694188117980957,
      "learning_rate": 1.6106652860471136e-06,
      "loss": 1.2198,
      "step": 36670
    },
    {
      "epoch": 2.8412246557834195,
      "grad_norm": 2.402482271194458,
      "learning_rate": 1.6028993010613513e-06,
      "loss": 1.2278,
      "step": 36680
    },
    {
      "epoch": 2.8419992641220784,
      "grad_norm": 2.2813401222229004,
      "learning_rate": 1.5951333160755891e-06,
      "loss": 1.2092,
      "step": 36690
    },
    {
      "epoch": 2.842773872460737,
      "grad_norm": 3.0922186374664307,
      "learning_rate": 1.5873673310898266e-06,
      "loss": 1.2802,
      "step": 36700
    },
    {
      "epoch": 2.843548480799396,
      "grad_norm": 2.597031354904175,
      "learning_rate": 1.5796013461040642e-06,
      "loss": 1.378,
      "step": 36710
    },
    {
      "epoch": 2.8443230891380544,
      "grad_norm": 2.331810712814331,
      "learning_rate": 1.5718353611183019e-06,
      "loss": 1.29,
      "step": 36720
    },
    {
      "epoch": 2.8450976974767133,
      "grad_norm": 1.9406380653381348,
      "learning_rate": 1.5640693761325395e-06,
      "loss": 1.2398,
      "step": 36730
    },
    {
      "epoch": 2.845872305815372,
      "grad_norm": 2.720440149307251,
      "learning_rate": 1.5563033911467772e-06,
      "loss": 1.1705,
      "step": 36740
    },
    {
      "epoch": 2.846646914154031,
      "grad_norm": 3.3418407440185547,
      "learning_rate": 1.5485374061610149e-06,
      "loss": 1.2127,
      "step": 36750
    },
    {
      "epoch": 2.8474215224926898,
      "grad_norm": 2.122070074081421,
      "learning_rate": 1.5407714211752523e-06,
      "loss": 1.2502,
      "step": 36760
    },
    {
      "epoch": 2.848196130831348,
      "grad_norm": 3.092095375061035,
      "learning_rate": 1.53300543618949e-06,
      "loss": 1.3221,
      "step": 36770
    },
    {
      "epoch": 2.8489707391700074,
      "grad_norm": 2.619082450866699,
      "learning_rate": 1.5252394512037278e-06,
      "loss": 1.277,
      "step": 36780
    },
    {
      "epoch": 2.849745347508666,
      "grad_norm": 2.6061794757843018,
      "learning_rate": 1.5174734662179655e-06,
      "loss": 1.2553,
      "step": 36790
    },
    {
      "epoch": 2.8505199558473246,
      "grad_norm": 3.4149954319000244,
      "learning_rate": 1.5097074812322031e-06,
      "loss": 1.2263,
      "step": 36800
    },
    {
      "epoch": 2.8512945641859835,
      "grad_norm": 2.968912124633789,
      "learning_rate": 1.5019414962464406e-06,
      "loss": 1.2629,
      "step": 36810
    },
    {
      "epoch": 2.8520691725246423,
      "grad_norm": 2.75278377532959,
      "learning_rate": 1.4941755112606782e-06,
      "loss": 1.105,
      "step": 36820
    },
    {
      "epoch": 2.852843780863301,
      "grad_norm": 5.314520835876465,
      "learning_rate": 1.4864095262749159e-06,
      "loss": 1.3569,
      "step": 36830
    },
    {
      "epoch": 2.8536183892019595,
      "grad_norm": 2.8434653282165527,
      "learning_rate": 1.4786435412891535e-06,
      "loss": 1.2999,
      "step": 36840
    },
    {
      "epoch": 2.8543929975406184,
      "grad_norm": 3.083564281463623,
      "learning_rate": 1.4708775563033912e-06,
      "loss": 1.2614,
      "step": 36850
    },
    {
      "epoch": 2.855167605879277,
      "grad_norm": 4.404290199279785,
      "learning_rate": 1.4631115713176286e-06,
      "loss": 1.2563,
      "step": 36860
    },
    {
      "epoch": 2.855942214217936,
      "grad_norm": 2.0986545085906982,
      "learning_rate": 1.4553455863318665e-06,
      "loss": 1.3733,
      "step": 36870
    },
    {
      "epoch": 2.856716822556595,
      "grad_norm": 2.6545045375823975,
      "learning_rate": 1.4475796013461042e-06,
      "loss": 1.2686,
      "step": 36880
    },
    {
      "epoch": 2.8574914308952537,
      "grad_norm": 2.8365702629089355,
      "learning_rate": 1.4398136163603418e-06,
      "loss": 1.3315,
      "step": 36890
    },
    {
      "epoch": 2.8582660392339125,
      "grad_norm": 3.439817428588867,
      "learning_rate": 1.4320476313745795e-06,
      "loss": 1.2012,
      "step": 36900
    },
    {
      "epoch": 2.859040647572571,
      "grad_norm": 2.517660140991211,
      "learning_rate": 1.4242816463888171e-06,
      "loss": 1.2691,
      "step": 36910
    },
    {
      "epoch": 2.8598152559112298,
      "grad_norm": 2.4782843589782715,
      "learning_rate": 1.4165156614030546e-06,
      "loss": 1.2255,
      "step": 36920
    },
    {
      "epoch": 2.8605898642498886,
      "grad_norm": 2.708824872970581,
      "learning_rate": 1.4087496764172922e-06,
      "loss": 1.2488,
      "step": 36930
    },
    {
      "epoch": 2.8613644725885474,
      "grad_norm": 2.541456937789917,
      "learning_rate": 1.4009836914315299e-06,
      "loss": 1.2353,
      "step": 36940
    },
    {
      "epoch": 2.8621390809272063,
      "grad_norm": 3.0217459201812744,
      "learning_rate": 1.3932177064457675e-06,
      "loss": 1.2946,
      "step": 36950
    },
    {
      "epoch": 2.862913689265865,
      "grad_norm": 2.1947500705718994,
      "learning_rate": 1.3854517214600054e-06,
      "loss": 1.2635,
      "step": 36960
    },
    {
      "epoch": 2.863688297604524,
      "grad_norm": 2.708570718765259,
      "learning_rate": 1.3776857364742428e-06,
      "loss": 1.3494,
      "step": 36970
    },
    {
      "epoch": 2.8644629059431823,
      "grad_norm": 2.290006399154663,
      "learning_rate": 1.3699197514884805e-06,
      "loss": 1.2441,
      "step": 36980
    },
    {
      "epoch": 2.865237514281841,
      "grad_norm": 2.5809006690979004,
      "learning_rate": 1.3621537665027181e-06,
      "loss": 1.1802,
      "step": 36990
    },
    {
      "epoch": 2.8660121226205,
      "grad_norm": 2.829742908477783,
      "learning_rate": 1.3543877815169558e-06,
      "loss": 1.2693,
      "step": 37000
    },
    {
      "epoch": 2.866786730959159,
      "grad_norm": 2.416903018951416,
      "learning_rate": 1.3466217965311934e-06,
      "loss": 1.219,
      "step": 37010
    },
    {
      "epoch": 2.8675613392978176,
      "grad_norm": 2.251919984817505,
      "learning_rate": 1.338855811545431e-06,
      "loss": 1.2443,
      "step": 37020
    },
    {
      "epoch": 2.868335947636476,
      "grad_norm": 1.9922606945037842,
      "learning_rate": 1.3310898265596685e-06,
      "loss": 1.1909,
      "step": 37030
    },
    {
      "epoch": 2.8691105559751353,
      "grad_norm": 2.3554413318634033,
      "learning_rate": 1.3233238415739062e-06,
      "loss": 1.2372,
      "step": 37040
    },
    {
      "epoch": 2.8698851643137937,
      "grad_norm": 2.685506582260132,
      "learning_rate": 1.315557856588144e-06,
      "loss": 1.2337,
      "step": 37050
    },
    {
      "epoch": 2.8706597726524525,
      "grad_norm": 2.5208017826080322,
      "learning_rate": 1.3077918716023817e-06,
      "loss": 1.2073,
      "step": 37060
    },
    {
      "epoch": 2.8714343809911114,
      "grad_norm": 2.0170509815216064,
      "learning_rate": 1.3000258866166194e-06,
      "loss": 1.3933,
      "step": 37070
    },
    {
      "epoch": 2.87220898932977,
      "grad_norm": 3.758209466934204,
      "learning_rate": 1.2922599016308568e-06,
      "loss": 1.1712,
      "step": 37080
    },
    {
      "epoch": 2.872983597668429,
      "grad_norm": 2.6420583724975586,
      "learning_rate": 1.2844939166450945e-06,
      "loss": 1.2924,
      "step": 37090
    },
    {
      "epoch": 2.8737582060070874,
      "grad_norm": 2.573622226715088,
      "learning_rate": 1.2767279316593321e-06,
      "loss": 1.1731,
      "step": 37100
    },
    {
      "epoch": 2.8745328143457467,
      "grad_norm": 2.3641040325164795,
      "learning_rate": 1.2689619466735698e-06,
      "loss": 1.1792,
      "step": 37110
    },
    {
      "epoch": 2.875307422684405,
      "grad_norm": 2.8095924854278564,
      "learning_rate": 1.2611959616878074e-06,
      "loss": 1.2838,
      "step": 37120
    },
    {
      "epoch": 2.876082031023064,
      "grad_norm": 2.0415077209472656,
      "learning_rate": 1.253429976702045e-06,
      "loss": 1.2619,
      "step": 37130
    },
    {
      "epoch": 2.8768566393617228,
      "grad_norm": 2.916961908340454,
      "learning_rate": 1.2456639917162827e-06,
      "loss": 1.223,
      "step": 37140
    },
    {
      "epoch": 2.8776312477003816,
      "grad_norm": 2.8357784748077393,
      "learning_rate": 1.2378980067305204e-06,
      "loss": 1.0927,
      "step": 37150
    },
    {
      "epoch": 2.8784058560390404,
      "grad_norm": 2.444854736328125,
      "learning_rate": 1.230132021744758e-06,
      "loss": 1.176,
      "step": 37160
    },
    {
      "epoch": 2.879180464377699,
      "grad_norm": 3.0183732509613037,
      "learning_rate": 1.2223660367589957e-06,
      "loss": 1.3028,
      "step": 37170
    },
    {
      "epoch": 2.8799550727163576,
      "grad_norm": 3.7499196529388428,
      "learning_rate": 1.2146000517732334e-06,
      "loss": 1.2657,
      "step": 37180
    },
    {
      "epoch": 2.8807296810550165,
      "grad_norm": 2.726155996322632,
      "learning_rate": 1.2068340667874708e-06,
      "loss": 1.2724,
      "step": 37190
    },
    {
      "epoch": 2.8815042893936753,
      "grad_norm": 3.367983102798462,
      "learning_rate": 1.1990680818017085e-06,
      "loss": 1.2816,
      "step": 37200
    },
    {
      "epoch": 2.882278897732334,
      "grad_norm": 2.8140032291412354,
      "learning_rate": 1.1913020968159461e-06,
      "loss": 1.1539,
      "step": 37210
    },
    {
      "epoch": 2.883053506070993,
      "grad_norm": 2.4870550632476807,
      "learning_rate": 1.1835361118301838e-06,
      "loss": 1.2702,
      "step": 37220
    },
    {
      "epoch": 2.883828114409652,
      "grad_norm": 2.4858953952789307,
      "learning_rate": 1.1757701268444216e-06,
      "loss": 1.2348,
      "step": 37230
    },
    {
      "epoch": 2.88460272274831,
      "grad_norm": 3.027423620223999,
      "learning_rate": 1.168004141858659e-06,
      "loss": 1.2874,
      "step": 37240
    },
    {
      "epoch": 2.885377331086969,
      "grad_norm": 3.3607161045074463,
      "learning_rate": 1.1602381568728967e-06,
      "loss": 1.2546,
      "step": 37250
    },
    {
      "epoch": 2.886151939425628,
      "grad_norm": 2.6617183685302734,
      "learning_rate": 1.1524721718871344e-06,
      "loss": 1.3095,
      "step": 37260
    },
    {
      "epoch": 2.8869265477642867,
      "grad_norm": 2.6175928115844727,
      "learning_rate": 1.144706186901372e-06,
      "loss": 1.252,
      "step": 37270
    },
    {
      "epoch": 2.8877011561029455,
      "grad_norm": 2.3158555030822754,
      "learning_rate": 1.1369402019156097e-06,
      "loss": 1.0838,
      "step": 37280
    },
    {
      "epoch": 2.8884757644416044,
      "grad_norm": 2.523176431655884,
      "learning_rate": 1.1291742169298473e-06,
      "loss": 1.307,
      "step": 37290
    },
    {
      "epoch": 2.889250372780263,
      "grad_norm": 3.904261350631714,
      "learning_rate": 1.1214082319440848e-06,
      "loss": 1.2815,
      "step": 37300
    },
    {
      "epoch": 2.8900249811189216,
      "grad_norm": 2.8414645195007324,
      "learning_rate": 1.1136422469583224e-06,
      "loss": 1.2686,
      "step": 37310
    },
    {
      "epoch": 2.8907995894575804,
      "grad_norm": 2.831392526626587,
      "learning_rate": 1.1058762619725603e-06,
      "loss": 1.2855,
      "step": 37320
    },
    {
      "epoch": 2.8915741977962393,
      "grad_norm": 2.6934540271759033,
      "learning_rate": 1.098110276986798e-06,
      "loss": 1.2727,
      "step": 37330
    },
    {
      "epoch": 2.892348806134898,
      "grad_norm": 3.425394058227539,
      "learning_rate": 1.0903442920010356e-06,
      "loss": 1.2082,
      "step": 37340
    },
    {
      "epoch": 2.893123414473557,
      "grad_norm": 2.313443899154663,
      "learning_rate": 1.082578307015273e-06,
      "loss": 1.3105,
      "step": 37350
    },
    {
      "epoch": 2.8938980228122153,
      "grad_norm": 2.937060594558716,
      "learning_rate": 1.0748123220295107e-06,
      "loss": 1.2787,
      "step": 37360
    },
    {
      "epoch": 2.8946726311508746,
      "grad_norm": 2.280092239379883,
      "learning_rate": 1.0670463370437484e-06,
      "loss": 1.342,
      "step": 37370
    },
    {
      "epoch": 2.895447239489533,
      "grad_norm": 2.6099233627319336,
      "learning_rate": 1.059280352057986e-06,
      "loss": 1.3402,
      "step": 37380
    },
    {
      "epoch": 2.896221847828192,
      "grad_norm": 2.359283685684204,
      "learning_rate": 1.0515143670722237e-06,
      "loss": 1.1861,
      "step": 37390
    },
    {
      "epoch": 2.8969964561668506,
      "grad_norm": 2.6531946659088135,
      "learning_rate": 1.0437483820864613e-06,
      "loss": 1.2018,
      "step": 37400
    },
    {
      "epoch": 2.8977710645055095,
      "grad_norm": 2.2960991859436035,
      "learning_rate": 1.035982397100699e-06,
      "loss": 1.1994,
      "step": 37410
    },
    {
      "epoch": 2.8985456728441683,
      "grad_norm": 3.423186779022217,
      "learning_rate": 1.0282164121149366e-06,
      "loss": 1.2546,
      "step": 37420
    },
    {
      "epoch": 2.8993202811828267,
      "grad_norm": 2.5357062816619873,
      "learning_rate": 1.0204504271291743e-06,
      "loss": 1.3867,
      "step": 37430
    },
    {
      "epoch": 2.900094889521486,
      "grad_norm": 2.9663491249084473,
      "learning_rate": 1.012684442143412e-06,
      "loss": 1.1602,
      "step": 37440
    },
    {
      "epoch": 2.9008694978601444,
      "grad_norm": 3.0772933959960938,
      "learning_rate": 1.0049184571576496e-06,
      "loss": 1.2783,
      "step": 37450
    },
    {
      "epoch": 2.901644106198803,
      "grad_norm": 2.3894543647766113,
      "learning_rate": 9.97152472171887e-07,
      "loss": 1.2953,
      "step": 37460
    },
    {
      "epoch": 2.902418714537462,
      "grad_norm": 2.728618621826172,
      "learning_rate": 9.893864871861247e-07,
      "loss": 1.2224,
      "step": 37470
    },
    {
      "epoch": 2.903193322876121,
      "grad_norm": 2.899456024169922,
      "learning_rate": 9.816205022003624e-07,
      "loss": 1.3676,
      "step": 37480
    },
    {
      "epoch": 2.9039679312147797,
      "grad_norm": 2.8714048862457275,
      "learning_rate": 9.738545172146e-07,
      "loss": 1.2758,
      "step": 37490
    },
    {
      "epoch": 2.904742539553438,
      "grad_norm": 2.3625802993774414,
      "learning_rate": 9.660885322288379e-07,
      "loss": 1.2123,
      "step": 37500
    },
    {
      "epoch": 2.905517147892097,
      "grad_norm": 2.9909842014312744,
      "learning_rate": 9.583225472430755e-07,
      "loss": 1.1774,
      "step": 37510
    },
    {
      "epoch": 2.9062917562307558,
      "grad_norm": 3.2389726638793945,
      "learning_rate": 9.50556562257313e-07,
      "loss": 1.238,
      "step": 37520
    },
    {
      "epoch": 2.9070663645694146,
      "grad_norm": 2.492771625518799,
      "learning_rate": 9.427905772715506e-07,
      "loss": 1.3078,
      "step": 37530
    },
    {
      "epoch": 2.9078409729080734,
      "grad_norm": 2.816880464553833,
      "learning_rate": 9.350245922857883e-07,
      "loss": 1.1416,
      "step": 37540
    },
    {
      "epoch": 2.9086155812467323,
      "grad_norm": 2.635265350341797,
      "learning_rate": 9.272586073000259e-07,
      "loss": 1.297,
      "step": 37550
    },
    {
      "epoch": 2.909390189585391,
      "grad_norm": 3.2725417613983154,
      "learning_rate": 9.194926223142635e-07,
      "loss": 1.2119,
      "step": 37560
    },
    {
      "epoch": 2.9101647979240495,
      "grad_norm": 2.753875732421875,
      "learning_rate": 9.117266373285012e-07,
      "loss": 1.2496,
      "step": 37570
    },
    {
      "epoch": 2.9109394062627083,
      "grad_norm": 2.156430244445801,
      "learning_rate": 9.039606523427388e-07,
      "loss": 1.1639,
      "step": 37580
    },
    {
      "epoch": 2.911714014601367,
      "grad_norm": 2.4065933227539062,
      "learning_rate": 8.961946673569764e-07,
      "loss": 1.1491,
      "step": 37590
    },
    {
      "epoch": 2.912488622940026,
      "grad_norm": 3.3809969425201416,
      "learning_rate": 8.884286823712141e-07,
      "loss": 1.2791,
      "step": 37600
    },
    {
      "epoch": 2.913263231278685,
      "grad_norm": 2.809541940689087,
      "learning_rate": 8.806626973854518e-07,
      "loss": 1.2459,
      "step": 37610
    },
    {
      "epoch": 2.9140378396173436,
      "grad_norm": 2.9369471073150635,
      "learning_rate": 8.728967123996894e-07,
      "loss": 1.1052,
      "step": 37620
    },
    {
      "epoch": 2.9148124479560025,
      "grad_norm": 2.4649219512939453,
      "learning_rate": 8.651307274139271e-07,
      "loss": 1.2844,
      "step": 37630
    },
    {
      "epoch": 2.915587056294661,
      "grad_norm": 2.5073249340057373,
      "learning_rate": 8.573647424281646e-07,
      "loss": 1.2525,
      "step": 37640
    },
    {
      "epoch": 2.9163616646333197,
      "grad_norm": 2.643620729446411,
      "learning_rate": 8.495987574424023e-07,
      "loss": 1.2486,
      "step": 37650
    },
    {
      "epoch": 2.9171362729719785,
      "grad_norm": 2.833003282546997,
      "learning_rate": 8.4183277245664e-07,
      "loss": 1.2465,
      "step": 37660
    },
    {
      "epoch": 2.9179108813106374,
      "grad_norm": NaN,
      "learning_rate": 8.340667874708776e-07,
      "loss": 1.2595,
      "step": 37670
    },
    {
      "epoch": 2.918685489649296,
      "grad_norm": 2.264570951461792,
      "learning_rate": 8.270774009836914e-07,
      "loss": 1.2924,
      "step": 37680
    },
    {
      "epoch": 2.9194600979879546,
      "grad_norm": 2.290741443634033,
      "learning_rate": 8.193114159979292e-07,
      "loss": 1.3152,
      "step": 37690
    },
    {
      "epoch": 2.920234706326614,
      "grad_norm": 2.030743360519409,
      "learning_rate": 8.115454310121667e-07,
      "loss": 1.274,
      "step": 37700
    },
    {
      "epoch": 2.9210093146652722,
      "grad_norm": 2.934929132461548,
      "learning_rate": 8.037794460264044e-07,
      "loss": 1.2787,
      "step": 37710
    },
    {
      "epoch": 2.921783923003931,
      "grad_norm": 2.5154428482055664,
      "learning_rate": 7.96013461040642e-07,
      "loss": 1.3142,
      "step": 37720
    },
    {
      "epoch": 2.92255853134259,
      "grad_norm": 2.742811679840088,
      "learning_rate": 7.882474760548796e-07,
      "loss": 1.2383,
      "step": 37730
    },
    {
      "epoch": 2.9233331396812487,
      "grad_norm": 2.5097012519836426,
      "learning_rate": 7.804814910691173e-07,
      "loss": 1.2341,
      "step": 37740
    },
    {
      "epoch": 2.9241077480199076,
      "grad_norm": 2.860907554626465,
      "learning_rate": 7.72715506083355e-07,
      "loss": 1.2721,
      "step": 37750
    },
    {
      "epoch": 2.924882356358566,
      "grad_norm": 2.7380471229553223,
      "learning_rate": 7.649495210975925e-07,
      "loss": 1.2926,
      "step": 37760
    },
    {
      "epoch": 2.925656964697225,
      "grad_norm": 1.8894779682159424,
      "learning_rate": 7.571835361118302e-07,
      "loss": 1.2821,
      "step": 37770
    },
    {
      "epoch": 2.9264315730358836,
      "grad_norm": 2.236121892929077,
      "learning_rate": 7.494175511260679e-07,
      "loss": 1.2434,
      "step": 37780
    },
    {
      "epoch": 2.9272061813745425,
      "grad_norm": 2.8785560131073,
      "learning_rate": 7.416515661403055e-07,
      "loss": 1.3092,
      "step": 37790
    },
    {
      "epoch": 2.9279807897132013,
      "grad_norm": 1.9796632528305054,
      "learning_rate": 7.338855811545431e-07,
      "loss": 1.3298,
      "step": 37800
    },
    {
      "epoch": 2.92875539805186,
      "grad_norm": 2.2699437141418457,
      "learning_rate": 7.261195961687807e-07,
      "loss": 1.2202,
      "step": 37810
    },
    {
      "epoch": 2.929530006390519,
      "grad_norm": 2.1637468338012695,
      "learning_rate": 7.183536111830183e-07,
      "loss": 1.2147,
      "step": 37820
    },
    {
      "epoch": 2.9303046147291774,
      "grad_norm": 2.704538106918335,
      "learning_rate": 7.105876261972561e-07,
      "loss": 1.1805,
      "step": 37830
    },
    {
      "epoch": 2.931079223067836,
      "grad_norm": 2.303030252456665,
      "learning_rate": 7.028216412114937e-07,
      "loss": 1.2247,
      "step": 37840
    },
    {
      "epoch": 2.931853831406495,
      "grad_norm": 1.8487995862960815,
      "learning_rate": 6.950556562257313e-07,
      "loss": 1.2435,
      "step": 37850
    },
    {
      "epoch": 2.932628439745154,
      "grad_norm": 2.2289090156555176,
      "learning_rate": 6.87289671239969e-07,
      "loss": 1.2657,
      "step": 37860
    },
    {
      "epoch": 2.9334030480838127,
      "grad_norm": 2.676405191421509,
      "learning_rate": 6.795236862542066e-07,
      "loss": 1.2261,
      "step": 37870
    },
    {
      "epoch": 2.9341776564224715,
      "grad_norm": 2.315683603286743,
      "learning_rate": 6.717577012684443e-07,
      "loss": 1.2726,
      "step": 37880
    },
    {
      "epoch": 2.9349522647611304,
      "grad_norm": 2.692772150039673,
      "learning_rate": 6.639917162826818e-07,
      "loss": 1.1799,
      "step": 37890
    },
    {
      "epoch": 2.9357268730997887,
      "grad_norm": 2.6697800159454346,
      "learning_rate": 6.562257312969195e-07,
      "loss": 1.3395,
      "step": 37900
    },
    {
      "epoch": 2.9365014814384476,
      "grad_norm": 2.200004816055298,
      "learning_rate": 6.484597463111571e-07,
      "loss": 1.307,
      "step": 37910
    },
    {
      "epoch": 2.9372760897771064,
      "grad_norm": 2.4516074657440186,
      "learning_rate": 6.406937613253948e-07,
      "loss": 1.2814,
      "step": 37920
    },
    {
      "epoch": 2.9380506981157652,
      "grad_norm": 3.5936243534088135,
      "learning_rate": 6.329277763396324e-07,
      "loss": 1.2414,
      "step": 37930
    },
    {
      "epoch": 2.938825306454424,
      "grad_norm": 3.0282914638519287,
      "learning_rate": 6.251617913538701e-07,
      "loss": 1.346,
      "step": 37940
    },
    {
      "epoch": 2.939599914793083,
      "grad_norm": 2.200514316558838,
      "learning_rate": 6.173958063681076e-07,
      "loss": 1.3374,
      "step": 37950
    },
    {
      "epoch": 2.9403745231317417,
      "grad_norm": 3.093459367752075,
      "learning_rate": 6.096298213823454e-07,
      "loss": 1.2529,
      "step": 37960
    },
    {
      "epoch": 2.9411491314704,
      "grad_norm": 2.317033529281616,
      "learning_rate": 6.018638363965831e-07,
      "loss": 1.1596,
      "step": 37970
    },
    {
      "epoch": 2.941923739809059,
      "grad_norm": 3.7950003147125244,
      "learning_rate": 5.940978514108206e-07,
      "loss": 1.1705,
      "step": 37980
    },
    {
      "epoch": 2.942698348147718,
      "grad_norm": 2.2559969425201416,
      "learning_rate": 5.863318664250583e-07,
      "loss": 1.2692,
      "step": 37990
    },
    {
      "epoch": 2.9434729564863766,
      "grad_norm": 3.8406920433044434,
      "learning_rate": 5.785658814392958e-07,
      "loss": 1.2613,
      "step": 38000
    },
    {
      "epoch": 2.9442475648250355,
      "grad_norm": 2.4884896278381348,
      "learning_rate": 5.707998964535336e-07,
      "loss": 1.1894,
      "step": 38010
    },
    {
      "epoch": 2.945022173163694,
      "grad_norm": 2.252760648727417,
      "learning_rate": 5.630339114677712e-07,
      "loss": 1.3208,
      "step": 38020
    },
    {
      "epoch": 2.945796781502353,
      "grad_norm": 2.890584707260132,
      "learning_rate": 5.552679264820088e-07,
      "loss": 1.2613,
      "step": 38030
    },
    {
      "epoch": 2.9465713898410115,
      "grad_norm": 2.627936363220215,
      "learning_rate": 5.475019414962464e-07,
      "loss": 1.2664,
      "step": 38040
    },
    {
      "epoch": 2.9473459981796704,
      "grad_norm": 2.2692532539367676,
      "learning_rate": 5.397359565104842e-07,
      "loss": 1.2585,
      "step": 38050
    },
    {
      "epoch": 2.948120606518329,
      "grad_norm": 2.7236742973327637,
      "learning_rate": 5.319699715247217e-07,
      "loss": 1.2282,
      "step": 38060
    },
    {
      "epoch": 2.948895214856988,
      "grad_norm": 2.6171178817749023,
      "learning_rate": 5.242039865389594e-07,
      "loss": 1.2919,
      "step": 38070
    },
    {
      "epoch": 2.949669823195647,
      "grad_norm": 2.920700788497925,
      "learning_rate": 5.164380015531969e-07,
      "loss": 1.2618,
      "step": 38080
    },
    {
      "epoch": 2.9504444315343052,
      "grad_norm": 3.878946542739868,
      "learning_rate": 5.086720165674346e-07,
      "loss": 1.2548,
      "step": 38090
    },
    {
      "epoch": 2.951219039872964,
      "grad_norm": 2.7953436374664307,
      "learning_rate": 5.009060315816724e-07,
      "loss": 1.284,
      "step": 38100
    },
    {
      "epoch": 2.951993648211623,
      "grad_norm": 2.5148661136627197,
      "learning_rate": 4.931400465959099e-07,
      "loss": 1.1992,
      "step": 38110
    },
    {
      "epoch": 2.9527682565502817,
      "grad_norm": 1.9771367311477661,
      "learning_rate": 4.853740616101476e-07,
      "loss": 1.2554,
      "step": 38120
    },
    {
      "epoch": 2.9535428648889406,
      "grad_norm": 5.138307571411133,
      "learning_rate": 4.776080766243852e-07,
      "loss": 1.2271,
      "step": 38130
    },
    {
      "epoch": 2.9543174732275994,
      "grad_norm": 2.8608920574188232,
      "learning_rate": 4.698420916386228e-07,
      "loss": 1.2346,
      "step": 38140
    },
    {
      "epoch": 2.9550920815662582,
      "grad_norm": 2.59896183013916,
      "learning_rate": 4.620761066528605e-07,
      "loss": 1.2388,
      "step": 38150
    },
    {
      "epoch": 2.9558666899049166,
      "grad_norm": 2.3980860710144043,
      "learning_rate": 4.543101216670981e-07,
      "loss": 1.3229,
      "step": 38160
    },
    {
      "epoch": 2.9566412982435755,
      "grad_norm": 1.9965187311172485,
      "learning_rate": 4.465441366813357e-07,
      "loss": 1.2436,
      "step": 38170
    },
    {
      "epoch": 2.9574159065822343,
      "grad_norm": 2.7479004859924316,
      "learning_rate": 4.3877815169557343e-07,
      "loss": 1.3399,
      "step": 38180
    },
    {
      "epoch": 2.958190514920893,
      "grad_norm": 2.51147198677063,
      "learning_rate": 4.3101216670981103e-07,
      "loss": 1.2549,
      "step": 38190
    },
    {
      "epoch": 2.958965123259552,
      "grad_norm": 2.3591156005859375,
      "learning_rate": 4.232461817240487e-07,
      "loss": 1.2631,
      "step": 38200
    },
    {
      "epoch": 2.959739731598211,
      "grad_norm": 2.9576847553253174,
      "learning_rate": 4.154801967382863e-07,
      "loss": 1.2196,
      "step": 38210
    },
    {
      "epoch": 2.9605143399368696,
      "grad_norm": 2.358098030090332,
      "learning_rate": 4.07714211752524e-07,
      "loss": 1.3608,
      "step": 38220
    },
    {
      "epoch": 2.961288948275528,
      "grad_norm": 2.391573905944824,
      "learning_rate": 3.999482267667616e-07,
      "loss": 1.3133,
      "step": 38230
    },
    {
      "epoch": 2.962063556614187,
      "grad_norm": 2.4384427070617676,
      "learning_rate": 3.9218224178099925e-07,
      "loss": 1.1232,
      "step": 38240
    },
    {
      "epoch": 2.9628381649528457,
      "grad_norm": 2.059058427810669,
      "learning_rate": 3.8441625679523685e-07,
      "loss": 1.3319,
      "step": 38250
    },
    {
      "epoch": 2.9636127732915045,
      "grad_norm": 2.6829867362976074,
      "learning_rate": 3.766502718094745e-07,
      "loss": 1.2666,
      "step": 38260
    },
    {
      "epoch": 2.9643873816301634,
      "grad_norm": 2.6013741493225098,
      "learning_rate": 3.6888428682371216e-07,
      "loss": 1.3386,
      "step": 38270
    },
    {
      "epoch": 2.9651619899688217,
      "grad_norm": 2.461050510406494,
      "learning_rate": 3.6111830183794976e-07,
      "loss": 1.1897,
      "step": 38280
    },
    {
      "epoch": 2.965936598307481,
      "grad_norm": 1.8227548599243164,
      "learning_rate": 3.533523168521874e-07,
      "loss": 1.1718,
      "step": 38290
    },
    {
      "epoch": 2.9667112066461394,
      "grad_norm": 2.404160976409912,
      "learning_rate": 3.4558633186642507e-07,
      "loss": 1.3968,
      "step": 38300
    },
    {
      "epoch": 2.9674858149847982,
      "grad_norm": 2.5484347343444824,
      "learning_rate": 3.378203468806627e-07,
      "loss": 1.1449,
      "step": 38310
    },
    {
      "epoch": 2.968260423323457,
      "grad_norm": 2.317443609237671,
      "learning_rate": 3.300543618949003e-07,
      "loss": 1.2648,
      "step": 38320
    },
    {
      "epoch": 2.969035031662116,
      "grad_norm": 2.8536078929901123,
      "learning_rate": 3.2228837690913803e-07,
      "loss": 1.1601,
      "step": 38330
    },
    {
      "epoch": 2.9698096400007747,
      "grad_norm": 2.65899920463562,
      "learning_rate": 3.1452239192337563e-07,
      "loss": 1.3132,
      "step": 38340
    },
    {
      "epoch": 2.970584248339433,
      "grad_norm": 2.741842269897461,
      "learning_rate": 3.0675640693761324e-07,
      "loss": 1.2383,
      "step": 38350
    },
    {
      "epoch": 2.9713588566780924,
      "grad_norm": 2.552241325378418,
      "learning_rate": 2.989904219518509e-07,
      "loss": 1.2273,
      "step": 38360
    },
    {
      "epoch": 2.972133465016751,
      "grad_norm": 2.877899408340454,
      "learning_rate": 2.9122443696608854e-07,
      "loss": 1.1771,
      "step": 38370
    },
    {
      "epoch": 2.9729080733554096,
      "grad_norm": 2.6570353507995605,
      "learning_rate": 2.834584519803262e-07,
      "loss": 1.3622,
      "step": 38380
    },
    {
      "epoch": 2.9736826816940685,
      "grad_norm": 1.9779311418533325,
      "learning_rate": 2.756924669945638e-07,
      "loss": 1.23,
      "step": 38390
    },
    {
      "epoch": 2.9744572900327273,
      "grad_norm": 2.5300724506378174,
      "learning_rate": 2.6792648200880145e-07,
      "loss": 1.2818,
      "step": 38400
    }
  ],
  "logging_steps": 10,
  "max_steps": 38730,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 9.491305844860518e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
