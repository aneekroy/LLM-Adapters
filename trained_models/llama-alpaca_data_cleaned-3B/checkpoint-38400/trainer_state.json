{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9744572900327273,
  "eval_steps": 500,
  "global_step": 38400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007746083386587656,
      "grad_norm": 1.0637038946151733,
      "learning_rate": 2.7e-06,
      "loss": 2.517,
      "step": 10
    },
    {
      "epoch": 0.0015492166773175312,
      "grad_norm": 0.9616508483886719,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 2.4482,
      "step": 20
    },
    {
      "epoch": 0.002323825015976297,
      "grad_norm": 0.7512243390083313,
      "learning_rate": 8.7e-06,
      "loss": 2.2748,
      "step": 30
    },
    {
      "epoch": 0.0030984333546350625,
      "grad_norm": 0.9848512411117554,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 2.5352,
      "step": 40
    },
    {
      "epoch": 0.0038730416932938284,
      "grad_norm": 1.2072309255599976,
      "learning_rate": 1.47e-05,
      "loss": 2.5441,
      "step": 50
    },
    {
      "epoch": 0.004647650031952594,
      "grad_norm": 0.8899471759796143,
      "learning_rate": 1.77e-05,
      "loss": 2.2986,
      "step": 60
    },
    {
      "epoch": 0.005422258370611359,
      "grad_norm": 0.9284977316856384,
      "learning_rate": 2.07e-05,
      "loss": 2.3554,
      "step": 70
    },
    {
      "epoch": 0.006196866709270125,
      "grad_norm": 1.1369482278823853,
      "learning_rate": 2.37e-05,
      "loss": 2.1338,
      "step": 80
    },
    {
      "epoch": 0.006971475047928891,
      "grad_norm": 1.4266988039016724,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 1.9531,
      "step": 90
    },
    {
      "epoch": 0.007746083386587657,
      "grad_norm": 1.2385414838790894,
      "learning_rate": 2.97e-05,
      "loss": 1.8388,
      "step": 100
    },
    {
      "epoch": 0.008520691725246422,
      "grad_norm": 1.1072245836257935,
      "learning_rate": 2.9993010613512814e-05,
      "loss": 1.7496,
      "step": 110
    },
    {
      "epoch": 0.009295300063905188,
      "grad_norm": 4.5997114181518555,
      "learning_rate": 2.9985244628527052e-05,
      "loss": 1.5626,
      "step": 120
    },
    {
      "epoch": 0.010069908402563953,
      "grad_norm": 4.694903373718262,
      "learning_rate": 2.997747864354129e-05,
      "loss": 1.4456,
      "step": 130
    },
    {
      "epoch": 0.010844516741222719,
      "grad_norm": 2.491339921951294,
      "learning_rate": 2.9969712658555524e-05,
      "loss": 1.5184,
      "step": 140
    },
    {
      "epoch": 0.011619125079881484,
      "grad_norm": 6.501546859741211,
      "learning_rate": 2.9961946673569766e-05,
      "loss": 1.4052,
      "step": 150
    },
    {
      "epoch": 0.01239373341854025,
      "grad_norm": 5.056972026824951,
      "learning_rate": 2.9954180688584003e-05,
      "loss": 1.3668,
      "step": 160
    },
    {
      "epoch": 0.013168341757199017,
      "grad_norm": 1.0204228162765503,
      "learning_rate": 2.994641470359824e-05,
      "loss": 1.5246,
      "step": 170
    },
    {
      "epoch": 0.013942950095857783,
      "grad_norm": 1.2437949180603027,
      "learning_rate": 2.993864871861248e-05,
      "loss": 1.3859,
      "step": 180
    },
    {
      "epoch": 0.014717558434516548,
      "grad_norm": 0.9079254269599915,
      "learning_rate": 2.9930882733626717e-05,
      "loss": 1.4956,
      "step": 190
    },
    {
      "epoch": 0.015492166773175314,
      "grad_norm": 2.394608736038208,
      "learning_rate": 2.9923116748640955e-05,
      "loss": 1.3719,
      "step": 200
    },
    {
      "epoch": 0.016266775111834077,
      "grad_norm": 0.7449519038200378,
      "learning_rate": 2.9915350763655193e-05,
      "loss": 1.3509,
      "step": 210
    },
    {
      "epoch": 0.017041383450492845,
      "grad_norm": 1.102213740348816,
      "learning_rate": 2.9907584778669427e-05,
      "loss": 1.4075,
      "step": 220
    },
    {
      "epoch": 0.017815991789151612,
      "grad_norm": 1.1860580444335938,
      "learning_rate": 2.9899818793683665e-05,
      "loss": 1.4826,
      "step": 230
    },
    {
      "epoch": 0.018590600127810376,
      "grad_norm": 1.1514952182769775,
      "learning_rate": 2.9892052808697903e-05,
      "loss": 1.3555,
      "step": 240
    },
    {
      "epoch": 0.019365208466469143,
      "grad_norm": 1.5855756998062134,
      "learning_rate": 2.988428682371214e-05,
      "loss": 1.3132,
      "step": 250
    },
    {
      "epoch": 0.020139816805127907,
      "grad_norm": 1.044258713722229,
      "learning_rate": 2.987652083872638e-05,
      "loss": 1.3519,
      "step": 260
    },
    {
      "epoch": 0.020914425143786674,
      "grad_norm": 0.819170355796814,
      "learning_rate": 2.9868754853740617e-05,
      "loss": 1.3446,
      "step": 270
    },
    {
      "epoch": 0.021689033482445438,
      "grad_norm": 0.9554175138473511,
      "learning_rate": 2.9860988868754854e-05,
      "loss": 1.3383,
      "step": 280
    },
    {
      "epoch": 0.022463641821104205,
      "grad_norm": 1.0611108541488647,
      "learning_rate": 2.9853222883769092e-05,
      "loss": 1.3548,
      "step": 290
    },
    {
      "epoch": 0.02323825015976297,
      "grad_norm": 1.1888786554336548,
      "learning_rate": 2.984545689878333e-05,
      "loss": 1.2839,
      "step": 300
    },
    {
      "epoch": 0.024012858498421736,
      "grad_norm": 1.4521076679229736,
      "learning_rate": 2.9837690913797565e-05,
      "loss": 1.3823,
      "step": 310
    },
    {
      "epoch": 0.0247874668370805,
      "grad_norm": 1.1750624179840088,
      "learning_rate": 2.9829924928811803e-05,
      "loss": 1.3923,
      "step": 320
    },
    {
      "epoch": 0.025562075175739267,
      "grad_norm": 0.8499524593353271,
      "learning_rate": 2.982215894382604e-05,
      "loss": 1.3114,
      "step": 330
    },
    {
      "epoch": 0.026336683514398034,
      "grad_norm": 0.8502392768859863,
      "learning_rate": 2.9814392958840282e-05,
      "loss": 1.3821,
      "step": 340
    },
    {
      "epoch": 0.027111291853056798,
      "grad_norm": 0.8363734483718872,
      "learning_rate": 2.980662697385452e-05,
      "loss": 1.4449,
      "step": 350
    },
    {
      "epoch": 0.027885900191715565,
      "grad_norm": 1.1202062368392944,
      "learning_rate": 2.9798860988868757e-05,
      "loss": 1.3336,
      "step": 360
    },
    {
      "epoch": 0.02866050853037433,
      "grad_norm": 1.1084903478622437,
      "learning_rate": 2.9791095003882995e-05,
      "loss": 1.3177,
      "step": 370
    },
    {
      "epoch": 0.029435116869033096,
      "grad_norm": 1.0608707666397095,
      "learning_rate": 2.9783329018897233e-05,
      "loss": 1.3534,
      "step": 380
    },
    {
      "epoch": 0.03020972520769186,
      "grad_norm": 1.6875858306884766,
      "learning_rate": 2.9775563033911468e-05,
      "loss": 1.3224,
      "step": 390
    },
    {
      "epoch": 0.030984333546350627,
      "grad_norm": 1.1207588911056519,
      "learning_rate": 2.9767797048925706e-05,
      "loss": 1.3131,
      "step": 400
    },
    {
      "epoch": 0.03175894188500939,
      "grad_norm": 1.4070357084274292,
      "learning_rate": 2.9760031063939943e-05,
      "loss": 1.43,
      "step": 410
    },
    {
      "epoch": 0.032533550223668155,
      "grad_norm": 1.0975104570388794,
      "learning_rate": 2.975226507895418e-05,
      "loss": 1.3339,
      "step": 420
    },
    {
      "epoch": 0.033308158562326926,
      "grad_norm": 0.9109235405921936,
      "learning_rate": 2.974449909396842e-05,
      "loss": 1.3282,
      "step": 430
    },
    {
      "epoch": 0.03408276690098569,
      "grad_norm": 0.9431204199790955,
      "learning_rate": 2.9736733108982657e-05,
      "loss": 1.2916,
      "step": 440
    },
    {
      "epoch": 0.03485737523964445,
      "grad_norm": 1.3324538469314575,
      "learning_rate": 2.9728967123996895e-05,
      "loss": 1.4024,
      "step": 450
    },
    {
      "epoch": 0.035631983578303224,
      "grad_norm": 1.017778992652893,
      "learning_rate": 2.9721201139011133e-05,
      "loss": 1.3661,
      "step": 460
    },
    {
      "epoch": 0.03640659191696199,
      "grad_norm": 1.1143510341644287,
      "learning_rate": 2.971343515402537e-05,
      "loss": 1.2731,
      "step": 470
    },
    {
      "epoch": 0.03718120025562075,
      "grad_norm": 1.0877256393432617,
      "learning_rate": 2.9705669169039605e-05,
      "loss": 1.3548,
      "step": 480
    },
    {
      "epoch": 0.037955808594279515,
      "grad_norm": 1.157142162322998,
      "learning_rate": 2.9697903184053843e-05,
      "loss": 1.397,
      "step": 490
    },
    {
      "epoch": 0.038730416932938286,
      "grad_norm": 1.3103857040405273,
      "learning_rate": 2.969013719906808e-05,
      "loss": 1.3604,
      "step": 500
    },
    {
      "epoch": 0.03950502527159705,
      "grad_norm": 1.115139365196228,
      "learning_rate": 2.968237121408232e-05,
      "loss": 1.4254,
      "step": 510
    },
    {
      "epoch": 0.04027963361025581,
      "grad_norm": 1.786105990409851,
      "learning_rate": 2.9674605229096557e-05,
      "loss": 1.2801,
      "step": 520
    },
    {
      "epoch": 0.04105424194891458,
      "grad_norm": 1.1397695541381836,
      "learning_rate": 2.9666839244110794e-05,
      "loss": 1.2307,
      "step": 530
    },
    {
      "epoch": 0.04182885028757335,
      "grad_norm": 2.1032955646514893,
      "learning_rate": 2.9659073259125036e-05,
      "loss": 1.2638,
      "step": 540
    },
    {
      "epoch": 0.04260345862623211,
      "grad_norm": 1.011383056640625,
      "learning_rate": 2.9651307274139273e-05,
      "loss": 1.361,
      "step": 550
    },
    {
      "epoch": 0.043378066964890875,
      "grad_norm": 1.2220537662506104,
      "learning_rate": 2.9643541289153508e-05,
      "loss": 1.2431,
      "step": 560
    },
    {
      "epoch": 0.044152675303549646,
      "grad_norm": 1.7872142791748047,
      "learning_rate": 2.9635775304167746e-05,
      "loss": 1.2723,
      "step": 570
    },
    {
      "epoch": 0.04492728364220841,
      "grad_norm": 1.3945485353469849,
      "learning_rate": 2.9628009319181984e-05,
      "loss": 1.2837,
      "step": 580
    },
    {
      "epoch": 0.045701891980867174,
      "grad_norm": 1.0296506881713867,
      "learning_rate": 2.962024333419622e-05,
      "loss": 1.3822,
      "step": 590
    },
    {
      "epoch": 0.04647650031952594,
      "grad_norm": 0.9950292110443115,
      "learning_rate": 2.961247734921046e-05,
      "loss": 1.2585,
      "step": 600
    },
    {
      "epoch": 0.04725110865818471,
      "grad_norm": 1.0066769123077393,
      "learning_rate": 2.9604711364224697e-05,
      "loss": 1.3461,
      "step": 610
    },
    {
      "epoch": 0.04802571699684347,
      "grad_norm": 1.0394623279571533,
      "learning_rate": 2.9596945379238935e-05,
      "loss": 1.3563,
      "step": 620
    },
    {
      "epoch": 0.048800325335502236,
      "grad_norm": 1.1282753944396973,
      "learning_rate": 2.9589179394253173e-05,
      "loss": 1.3369,
      "step": 630
    },
    {
      "epoch": 0.049574933674161,
      "grad_norm": 1.318602442741394,
      "learning_rate": 2.958141340926741e-05,
      "loss": 1.2978,
      "step": 640
    },
    {
      "epoch": 0.05034954201281977,
      "grad_norm": 1.4763509035110474,
      "learning_rate": 2.9573647424281645e-05,
      "loss": 1.3757,
      "step": 650
    },
    {
      "epoch": 0.051124150351478534,
      "grad_norm": 1.04408597946167,
      "learning_rate": 2.9565881439295883e-05,
      "loss": 1.3306,
      "step": 660
    },
    {
      "epoch": 0.0518987586901373,
      "grad_norm": 1.256852626800537,
      "learning_rate": 2.955811545431012e-05,
      "loss": 1.3479,
      "step": 670
    },
    {
      "epoch": 0.05267336702879607,
      "grad_norm": 2.329198122024536,
      "learning_rate": 2.955034946932436e-05,
      "loss": 1.3433,
      "step": 680
    },
    {
      "epoch": 0.05344797536745483,
      "grad_norm": 0.9249210953712463,
      "learning_rate": 2.9542583484338597e-05,
      "loss": 1.3192,
      "step": 690
    },
    {
      "epoch": 0.054222583706113596,
      "grad_norm": 1.0122497081756592,
      "learning_rate": 2.9534817499352835e-05,
      "loss": 1.2352,
      "step": 700
    },
    {
      "epoch": 0.05499719204477236,
      "grad_norm": 1.482330083847046,
      "learning_rate": 2.9527051514367073e-05,
      "loss": 1.3463,
      "step": 710
    },
    {
      "epoch": 0.05577180038343113,
      "grad_norm": 1.8518928289413452,
      "learning_rate": 2.951928552938131e-05,
      "loss": 1.3767,
      "step": 720
    },
    {
      "epoch": 0.056546408722089894,
      "grad_norm": 1.3973132371902466,
      "learning_rate": 2.951151954439555e-05,
      "loss": 1.3423,
      "step": 730
    },
    {
      "epoch": 0.05732101706074866,
      "grad_norm": 0.837640643119812,
      "learning_rate": 2.9503753559409786e-05,
      "loss": 1.3233,
      "step": 740
    },
    {
      "epoch": 0.05809562539940742,
      "grad_norm": 1.6314640045166016,
      "learning_rate": 2.9495987574424024e-05,
      "loss": 1.3089,
      "step": 750
    },
    {
      "epoch": 0.05887023373806619,
      "grad_norm": 1.4477083683013916,
      "learning_rate": 2.9488221589438262e-05,
      "loss": 1.3195,
      "step": 760
    },
    {
      "epoch": 0.059644842076724956,
      "grad_norm": 1.280137538909912,
      "learning_rate": 2.94804556044525e-05,
      "loss": 1.2868,
      "step": 770
    },
    {
      "epoch": 0.06041945041538372,
      "grad_norm": 1.6196849346160889,
      "learning_rate": 2.9472689619466738e-05,
      "loss": 1.2757,
      "step": 780
    },
    {
      "epoch": 0.06119405875404249,
      "grad_norm": 1.4891855716705322,
      "learning_rate": 2.9464923634480976e-05,
      "loss": 1.3028,
      "step": 790
    },
    {
      "epoch": 0.061968667092701255,
      "grad_norm": 0.9084972143173218,
      "learning_rate": 2.9457157649495213e-05,
      "loss": 1.3691,
      "step": 800
    },
    {
      "epoch": 0.06274327543136002,
      "grad_norm": 1.4303545951843262,
      "learning_rate": 2.944939166450945e-05,
      "loss": 1.2792,
      "step": 810
    },
    {
      "epoch": 0.06351788377001878,
      "grad_norm": 0.8863781690597534,
      "learning_rate": 2.9441625679523686e-05,
      "loss": 1.2923,
      "step": 820
    },
    {
      "epoch": 0.06429249210867755,
      "grad_norm": 1.1481741666793823,
      "learning_rate": 2.9433859694537924e-05,
      "loss": 1.3417,
      "step": 830
    },
    {
      "epoch": 0.06506710044733631,
      "grad_norm": 1.9159015417099,
      "learning_rate": 2.942609370955216e-05,
      "loss": 1.3042,
      "step": 840
    },
    {
      "epoch": 0.06584170878599509,
      "grad_norm": 1.2030564546585083,
      "learning_rate": 2.94183277245664e-05,
      "loss": 1.2815,
      "step": 850
    },
    {
      "epoch": 0.06661631712465385,
      "grad_norm": 1.1362019777297974,
      "learning_rate": 2.9410561739580637e-05,
      "loss": 1.2596,
      "step": 860
    },
    {
      "epoch": 0.06739092546331261,
      "grad_norm": 1.1533139944076538,
      "learning_rate": 2.9402795754594875e-05,
      "loss": 1.2808,
      "step": 870
    },
    {
      "epoch": 0.06816553380197138,
      "grad_norm": 0.9476397037506104,
      "learning_rate": 2.9395029769609113e-05,
      "loss": 1.2707,
      "step": 880
    },
    {
      "epoch": 0.06894014214063014,
      "grad_norm": 1.0068562030792236,
      "learning_rate": 2.938726378462335e-05,
      "loss": 1.2021,
      "step": 890
    },
    {
      "epoch": 0.0697147504792889,
      "grad_norm": 1.2506667375564575,
      "learning_rate": 2.9379497799637585e-05,
      "loss": 1.265,
      "step": 900
    },
    {
      "epoch": 0.07048935881794767,
      "grad_norm": 1.6245167255401611,
      "learning_rate": 2.9371731814651823e-05,
      "loss": 1.2964,
      "step": 910
    },
    {
      "epoch": 0.07126396715660645,
      "grad_norm": 1.1240088939666748,
      "learning_rate": 2.936396582966606e-05,
      "loss": 1.2962,
      "step": 920
    },
    {
      "epoch": 0.07203857549526521,
      "grad_norm": 1.7750049829483032,
      "learning_rate": 2.9356199844680302e-05,
      "loss": 1.3237,
      "step": 930
    },
    {
      "epoch": 0.07281318383392398,
      "grad_norm": 1.1739282608032227,
      "learning_rate": 2.934843385969454e-05,
      "loss": 1.3565,
      "step": 940
    },
    {
      "epoch": 0.07358779217258274,
      "grad_norm": 0.9414814710617065,
      "learning_rate": 2.9340667874708778e-05,
      "loss": 1.3366,
      "step": 950
    },
    {
      "epoch": 0.0743624005112415,
      "grad_norm": 1.1936330795288086,
      "learning_rate": 2.9332901889723016e-05,
      "loss": 1.2299,
      "step": 960
    },
    {
      "epoch": 0.07513700884990027,
      "grad_norm": 1.3021787405014038,
      "learning_rate": 2.9325135904737254e-05,
      "loss": 1.2781,
      "step": 970
    },
    {
      "epoch": 0.07591161718855903,
      "grad_norm": 1.2825464010238647,
      "learning_rate": 2.931736991975149e-05,
      "loss": 1.2526,
      "step": 980
    },
    {
      "epoch": 0.0766862255272178,
      "grad_norm": 1.348453164100647,
      "learning_rate": 2.9309603934765726e-05,
      "loss": 1.3858,
      "step": 990
    },
    {
      "epoch": 0.07746083386587657,
      "grad_norm": 1.2305246591567993,
      "learning_rate": 2.9301837949779964e-05,
      "loss": 1.2809,
      "step": 1000
    },
    {
      "epoch": 0.07823544220453534,
      "grad_norm": 1.1788983345031738,
      "learning_rate": 2.9294071964794202e-05,
      "loss": 1.3062,
      "step": 1010
    },
    {
      "epoch": 0.0790100505431941,
      "grad_norm": 0.9642294645309448,
      "learning_rate": 2.928630597980844e-05,
      "loss": 1.3474,
      "step": 1020
    },
    {
      "epoch": 0.07978465888185286,
      "grad_norm": 1.1478468179702759,
      "learning_rate": 2.9278539994822678e-05,
      "loss": 1.3395,
      "step": 1030
    },
    {
      "epoch": 0.08055926722051163,
      "grad_norm": 1.1614954471588135,
      "learning_rate": 2.9270774009836915e-05,
      "loss": 1.3325,
      "step": 1040
    },
    {
      "epoch": 0.08133387555917039,
      "grad_norm": 1.2942653894424438,
      "learning_rate": 2.9263008024851153e-05,
      "loss": 1.2848,
      "step": 1050
    },
    {
      "epoch": 0.08210848389782915,
      "grad_norm": 1.4360815286636353,
      "learning_rate": 2.925524203986539e-05,
      "loss": 1.3234,
      "step": 1060
    },
    {
      "epoch": 0.08288309223648793,
      "grad_norm": 1.167305588722229,
      "learning_rate": 2.9247476054879626e-05,
      "loss": 1.3319,
      "step": 1070
    },
    {
      "epoch": 0.0836577005751467,
      "grad_norm": 2.14581298828125,
      "learning_rate": 2.9239710069893863e-05,
      "loss": 1.2571,
      "step": 1080
    },
    {
      "epoch": 0.08443230891380546,
      "grad_norm": 1.9350974559783936,
      "learning_rate": 2.92319440849081e-05,
      "loss": 1.2652,
      "step": 1090
    },
    {
      "epoch": 0.08520691725246422,
      "grad_norm": 1.3007692098617554,
      "learning_rate": 2.922417809992234e-05,
      "loss": 1.3055,
      "step": 1100
    },
    {
      "epoch": 0.08598152559112299,
      "grad_norm": 2.525991439819336,
      "learning_rate": 2.9216412114936577e-05,
      "loss": 1.2359,
      "step": 1110
    },
    {
      "epoch": 0.08675613392978175,
      "grad_norm": 1.3342430591583252,
      "learning_rate": 2.9208646129950815e-05,
      "loss": 1.3226,
      "step": 1120
    },
    {
      "epoch": 0.08753074226844051,
      "grad_norm": 1.2812079191207886,
      "learning_rate": 2.9200880144965056e-05,
      "loss": 1.1607,
      "step": 1130
    },
    {
      "epoch": 0.08830535060709929,
      "grad_norm": 2.215477466583252,
      "learning_rate": 2.9193114159979294e-05,
      "loss": 1.1909,
      "step": 1140
    },
    {
      "epoch": 0.08907995894575806,
      "grad_norm": 1.4352145195007324,
      "learning_rate": 2.9185348174993532e-05,
      "loss": 1.2523,
      "step": 1150
    },
    {
      "epoch": 0.08985456728441682,
      "grad_norm": 1.3748221397399902,
      "learning_rate": 2.9177582190007766e-05,
      "loss": 1.3406,
      "step": 1160
    },
    {
      "epoch": 0.09062917562307558,
      "grad_norm": 1.32708740234375,
      "learning_rate": 2.9169816205022004e-05,
      "loss": 1.3101,
      "step": 1170
    },
    {
      "epoch": 0.09140378396173435,
      "grad_norm": 1.6890510320663452,
      "learning_rate": 2.9162050220036242e-05,
      "loss": 1.3985,
      "step": 1180
    },
    {
      "epoch": 0.09217839230039311,
      "grad_norm": 1.4734174013137817,
      "learning_rate": 2.915428423505048e-05,
      "loss": 1.3759,
      "step": 1190
    },
    {
      "epoch": 0.09295300063905187,
      "grad_norm": 1.239983320236206,
      "learning_rate": 2.9146518250064718e-05,
      "loss": 1.2619,
      "step": 1200
    },
    {
      "epoch": 0.09372760897771064,
      "grad_norm": 1.50492262840271,
      "learning_rate": 2.9138752265078956e-05,
      "loss": 1.2476,
      "step": 1210
    },
    {
      "epoch": 0.09450221731636942,
      "grad_norm": 1.2730562686920166,
      "learning_rate": 2.9130986280093194e-05,
      "loss": 1.279,
      "step": 1220
    },
    {
      "epoch": 0.09527682565502818,
      "grad_norm": 1.4040343761444092,
      "learning_rate": 2.912322029510743e-05,
      "loss": 1.3598,
      "step": 1230
    },
    {
      "epoch": 0.09605143399368694,
      "grad_norm": 1.4739104509353638,
      "learning_rate": 2.9115454310121666e-05,
      "loss": 1.279,
      "step": 1240
    },
    {
      "epoch": 0.09682604233234571,
      "grad_norm": 1.146691083908081,
      "learning_rate": 2.9107688325135904e-05,
      "loss": 1.2669,
      "step": 1250
    },
    {
      "epoch": 0.09760065067100447,
      "grad_norm": 1.1951788663864136,
      "learning_rate": 2.909992234015014e-05,
      "loss": 1.2881,
      "step": 1260
    },
    {
      "epoch": 0.09837525900966324,
      "grad_norm": 1.7083438634872437,
      "learning_rate": 2.909215635516438e-05,
      "loss": 1.2902,
      "step": 1270
    },
    {
      "epoch": 0.099149867348322,
      "grad_norm": 1.171812891960144,
      "learning_rate": 2.9084390370178617e-05,
      "loss": 1.2567,
      "step": 1280
    },
    {
      "epoch": 0.09992447568698078,
      "grad_norm": 1.347637414932251,
      "learning_rate": 2.9076624385192855e-05,
      "loss": 1.3779,
      "step": 1290
    },
    {
      "epoch": 0.10069908402563954,
      "grad_norm": 1.704977035522461,
      "learning_rate": 2.9068858400207093e-05,
      "loss": 1.3864,
      "step": 1300
    },
    {
      "epoch": 0.1014736923642983,
      "grad_norm": 1.5001561641693115,
      "learning_rate": 2.906109241522133e-05,
      "loss": 1.2763,
      "step": 1310
    },
    {
      "epoch": 0.10224830070295707,
      "grad_norm": 1.4269099235534668,
      "learning_rate": 2.9053326430235572e-05,
      "loss": 1.167,
      "step": 1320
    },
    {
      "epoch": 0.10302290904161583,
      "grad_norm": 1.5406874418258667,
      "learning_rate": 2.9045560445249807e-05,
      "loss": 1.3447,
      "step": 1330
    },
    {
      "epoch": 0.1037975173802746,
      "grad_norm": 1.5538475513458252,
      "learning_rate": 2.9037794460264045e-05,
      "loss": 1.316,
      "step": 1340
    },
    {
      "epoch": 0.10457212571893336,
      "grad_norm": 1.0766730308532715,
      "learning_rate": 2.9030028475278282e-05,
      "loss": 1.3098,
      "step": 1350
    },
    {
      "epoch": 0.10534673405759214,
      "grad_norm": 1.281996250152588,
      "learning_rate": 2.902226249029252e-05,
      "loss": 1.3325,
      "step": 1360
    },
    {
      "epoch": 0.1061213423962509,
      "grad_norm": 1.2537440061569214,
      "learning_rate": 2.9014496505306758e-05,
      "loss": 1.3859,
      "step": 1370
    },
    {
      "epoch": 0.10689595073490966,
      "grad_norm": 1.4062025547027588,
      "learning_rate": 2.9006730520320996e-05,
      "loss": 1.2576,
      "step": 1380
    },
    {
      "epoch": 0.10767055907356843,
      "grad_norm": 1.2680118083953857,
      "learning_rate": 2.8998964535335234e-05,
      "loss": 1.2922,
      "step": 1390
    },
    {
      "epoch": 0.10844516741222719,
      "grad_norm": 1.3493753671646118,
      "learning_rate": 2.8991198550349472e-05,
      "loss": 1.2774,
      "step": 1400
    },
    {
      "epoch": 0.10921977575088596,
      "grad_norm": 1.51399564743042,
      "learning_rate": 2.8983432565363706e-05,
      "loss": 1.2785,
      "step": 1410
    },
    {
      "epoch": 0.10999438408954472,
      "grad_norm": 1.4044779539108276,
      "learning_rate": 2.8975666580377944e-05,
      "loss": 1.2662,
      "step": 1420
    },
    {
      "epoch": 0.11076899242820348,
      "grad_norm": 1.2027262449264526,
      "learning_rate": 2.8967900595392182e-05,
      "loss": 1.3382,
      "step": 1430
    },
    {
      "epoch": 0.11154360076686226,
      "grad_norm": 1.2223645448684692,
      "learning_rate": 2.896013461040642e-05,
      "loss": 1.2815,
      "step": 1440
    },
    {
      "epoch": 0.11231820910552102,
      "grad_norm": 1.633152961730957,
      "learning_rate": 2.8952368625420658e-05,
      "loss": 1.2554,
      "step": 1450
    },
    {
      "epoch": 0.11309281744417979,
      "grad_norm": 1.6981678009033203,
      "learning_rate": 2.8944602640434896e-05,
      "loss": 1.2706,
      "step": 1460
    },
    {
      "epoch": 0.11386742578283855,
      "grad_norm": 1.2426410913467407,
      "learning_rate": 2.8936836655449133e-05,
      "loss": 1.3142,
      "step": 1470
    },
    {
      "epoch": 0.11464203412149732,
      "grad_norm": 1.0940792560577393,
      "learning_rate": 2.892907067046337e-05,
      "loss": 1.2075,
      "step": 1480
    },
    {
      "epoch": 0.11541664246015608,
      "grad_norm": 1.1805329322814941,
      "learning_rate": 2.892130468547761e-05,
      "loss": 1.311,
      "step": 1490
    },
    {
      "epoch": 0.11619125079881484,
      "grad_norm": 1.3219314813613892,
      "learning_rate": 2.8913538700491844e-05,
      "loss": 1.2283,
      "step": 1500
    },
    {
      "epoch": 0.11696585913747362,
      "grad_norm": 1.3273518085479736,
      "learning_rate": 2.890577271550608e-05,
      "loss": 1.3533,
      "step": 1510
    },
    {
      "epoch": 0.11774046747613238,
      "grad_norm": 1.959630012512207,
      "learning_rate": 2.8898006730520323e-05,
      "loss": 1.3049,
      "step": 1520
    },
    {
      "epoch": 0.11851507581479115,
      "grad_norm": 1.2217069864273071,
      "learning_rate": 2.889024074553456e-05,
      "loss": 1.1743,
      "step": 1530
    },
    {
      "epoch": 0.11928968415344991,
      "grad_norm": 1.0627069473266602,
      "learning_rate": 2.88824747605488e-05,
      "loss": 1.3125,
      "step": 1540
    },
    {
      "epoch": 0.12006429249210868,
      "grad_norm": 1.1661843061447144,
      "learning_rate": 2.8874708775563036e-05,
      "loss": 1.2471,
      "step": 1550
    },
    {
      "epoch": 0.12083890083076744,
      "grad_norm": 1.5260611772537231,
      "learning_rate": 2.8866942790577274e-05,
      "loss": 1.1984,
      "step": 1560
    },
    {
      "epoch": 0.1216135091694262,
      "grad_norm": 1.006364345550537,
      "learning_rate": 2.8859176805591512e-05,
      "loss": 1.199,
      "step": 1570
    },
    {
      "epoch": 0.12238811750808498,
      "grad_norm": 1.3123036623001099,
      "learning_rate": 2.8851410820605747e-05,
      "loss": 1.2343,
      "step": 1580
    },
    {
      "epoch": 0.12316272584674375,
      "grad_norm": 1.2269130945205688,
      "learning_rate": 2.8843644835619984e-05,
      "loss": 1.172,
      "step": 1590
    },
    {
      "epoch": 0.12393733418540251,
      "grad_norm": 1.3836359977722168,
      "learning_rate": 2.8835878850634222e-05,
      "loss": 1.1159,
      "step": 1600
    },
    {
      "epoch": 0.12471194252406127,
      "grad_norm": 1.8613090515136719,
      "learning_rate": 2.882811286564846e-05,
      "loss": 1.2268,
      "step": 1610
    },
    {
      "epoch": 0.12548655086272004,
      "grad_norm": 1.8301506042480469,
      "learning_rate": 2.8820346880662698e-05,
      "loss": 1.2629,
      "step": 1620
    },
    {
      "epoch": 0.12626115920137881,
      "grad_norm": 1.080778956413269,
      "learning_rate": 2.8812580895676936e-05,
      "loss": 1.2394,
      "step": 1630
    },
    {
      "epoch": 0.12703576754003756,
      "grad_norm": 1.7235348224639893,
      "learning_rate": 2.8804814910691174e-05,
      "loss": 1.3316,
      "step": 1640
    },
    {
      "epoch": 0.12781037587869634,
      "grad_norm": 1.342621922492981,
      "learning_rate": 2.879704892570541e-05,
      "loss": 1.2726,
      "step": 1650
    },
    {
      "epoch": 0.1285849842173551,
      "grad_norm": 1.9160062074661255,
      "learning_rate": 2.878928294071965e-05,
      "loss": 1.148,
      "step": 1660
    },
    {
      "epoch": 0.12935959255601387,
      "grad_norm": 1.1707749366760254,
      "learning_rate": 2.8781516955733884e-05,
      "loss": 1.1555,
      "step": 1670
    },
    {
      "epoch": 0.13013420089467262,
      "grad_norm": 1.3919719457626343,
      "learning_rate": 2.8773750970748122e-05,
      "loss": 1.2313,
      "step": 1680
    },
    {
      "epoch": 0.1309088092333314,
      "grad_norm": 1.306657314300537,
      "learning_rate": 2.876598498576236e-05,
      "loss": 1.2543,
      "step": 1690
    },
    {
      "epoch": 0.13168341757199017,
      "grad_norm": 2.2497308254241943,
      "learning_rate": 2.8758219000776598e-05,
      "loss": 1.2204,
      "step": 1700
    },
    {
      "epoch": 0.13245802591064892,
      "grad_norm": 1.7307748794555664,
      "learning_rate": 2.875045301579084e-05,
      "loss": 1.2009,
      "step": 1710
    },
    {
      "epoch": 0.1332326342493077,
      "grad_norm": 1.5722466707229614,
      "learning_rate": 2.8742687030805077e-05,
      "loss": 1.196,
      "step": 1720
    },
    {
      "epoch": 0.13400724258796645,
      "grad_norm": 1.276963710784912,
      "learning_rate": 2.8734921045819315e-05,
      "loss": 1.1968,
      "step": 1730
    },
    {
      "epoch": 0.13478185092662523,
      "grad_norm": 1.3212554454803467,
      "learning_rate": 2.8727155060833552e-05,
      "loss": 1.1664,
      "step": 1740
    },
    {
      "epoch": 0.13555645926528398,
      "grad_norm": 2.6391937732696533,
      "learning_rate": 2.8719389075847787e-05,
      "loss": 1.2505,
      "step": 1750
    },
    {
      "epoch": 0.13633106760394276,
      "grad_norm": 1.4208271503448486,
      "learning_rate": 2.8711623090862025e-05,
      "loss": 1.224,
      "step": 1760
    },
    {
      "epoch": 0.13710567594260153,
      "grad_norm": 1.2547314167022705,
      "learning_rate": 2.8703857105876263e-05,
      "loss": 1.314,
      "step": 1770
    },
    {
      "epoch": 0.13788028428126028,
      "grad_norm": 1.2986804246902466,
      "learning_rate": 2.86960911208905e-05,
      "loss": 1.2341,
      "step": 1780
    },
    {
      "epoch": 0.13865489261991906,
      "grad_norm": 1.7366328239440918,
      "learning_rate": 2.868832513590474e-05,
      "loss": 1.3093,
      "step": 1790
    },
    {
      "epoch": 0.1394295009585778,
      "grad_norm": 1.7048649787902832,
      "learning_rate": 2.8680559150918976e-05,
      "loss": 1.1978,
      "step": 1800
    },
    {
      "epoch": 0.1402041092972366,
      "grad_norm": 1.3655462265014648,
      "learning_rate": 2.8672793165933214e-05,
      "loss": 1.2031,
      "step": 1810
    },
    {
      "epoch": 0.14097871763589534,
      "grad_norm": 1.2422316074371338,
      "learning_rate": 2.8665027180947452e-05,
      "loss": 1.2354,
      "step": 1820
    },
    {
      "epoch": 0.14175332597455412,
      "grad_norm": 1.5066242218017578,
      "learning_rate": 2.8657261195961686e-05,
      "loss": 1.326,
      "step": 1830
    },
    {
      "epoch": 0.1425279343132129,
      "grad_norm": 0.9593862295150757,
      "learning_rate": 2.8649495210975924e-05,
      "loss": 1.17,
      "step": 1840
    },
    {
      "epoch": 0.14330254265187164,
      "grad_norm": 1.628326654434204,
      "learning_rate": 2.8641729225990162e-05,
      "loss": 1.2519,
      "step": 1850
    },
    {
      "epoch": 0.14407715099053042,
      "grad_norm": 1.3749421834945679,
      "learning_rate": 2.86339632410044e-05,
      "loss": 1.1987,
      "step": 1860
    },
    {
      "epoch": 0.14485175932918917,
      "grad_norm": 1.1767815351486206,
      "learning_rate": 2.8626197256018638e-05,
      "loss": 1.2372,
      "step": 1870
    },
    {
      "epoch": 0.14562636766784795,
      "grad_norm": 1.3258941173553467,
      "learning_rate": 2.8618431271032876e-05,
      "loss": 1.1531,
      "step": 1880
    },
    {
      "epoch": 0.1464009760065067,
      "grad_norm": 1.4960964918136597,
      "learning_rate": 2.8610665286047114e-05,
      "loss": 1.151,
      "step": 1890
    },
    {
      "epoch": 0.14717558434516548,
      "grad_norm": 1.8860305547714233,
      "learning_rate": 2.860289930106135e-05,
      "loss": 1.266,
      "step": 1900
    },
    {
      "epoch": 0.14795019268382423,
      "grad_norm": 2.5800609588623047,
      "learning_rate": 2.8595133316075593e-05,
      "loss": 1.2259,
      "step": 1910
    },
    {
      "epoch": 0.148724801022483,
      "grad_norm": 1.3462145328521729,
      "learning_rate": 2.8587367331089827e-05,
      "loss": 1.2546,
      "step": 1920
    },
    {
      "epoch": 0.14949940936114178,
      "grad_norm": 1.3325098752975464,
      "learning_rate": 2.8579601346104065e-05,
      "loss": 1.2058,
      "step": 1930
    },
    {
      "epoch": 0.15027401769980053,
      "grad_norm": 1.560577154159546,
      "learning_rate": 2.8571835361118303e-05,
      "loss": 1.2936,
      "step": 1940
    },
    {
      "epoch": 0.1510486260384593,
      "grad_norm": 1.2425129413604736,
      "learning_rate": 2.856406937613254e-05,
      "loss": 1.2187,
      "step": 1950
    },
    {
      "epoch": 0.15182323437711806,
      "grad_norm": 1.1670886278152466,
      "learning_rate": 2.855630339114678e-05,
      "loss": 1.1827,
      "step": 1960
    },
    {
      "epoch": 0.15259784271577684,
      "grad_norm": 1.385136365890503,
      "learning_rate": 2.8548537406161017e-05,
      "loss": 1.2944,
      "step": 1970
    },
    {
      "epoch": 0.1533724510544356,
      "grad_norm": 1.2771373987197876,
      "learning_rate": 2.8540771421175254e-05,
      "loss": 1.3251,
      "step": 1980
    },
    {
      "epoch": 0.15414705939309437,
      "grad_norm": 1.5464202165603638,
      "learning_rate": 2.8533005436189492e-05,
      "loss": 1.2393,
      "step": 1990
    },
    {
      "epoch": 0.15492166773175314,
      "grad_norm": 1.2323740720748901,
      "learning_rate": 2.8525239451203727e-05,
      "loss": 1.2947,
      "step": 2000
    },
    {
      "epoch": 0.1556962760704119,
      "grad_norm": 1.2010831832885742,
      "learning_rate": 2.8517473466217965e-05,
      "loss": 1.253,
      "step": 2010
    },
    {
      "epoch": 0.15647088440907067,
      "grad_norm": 1.196761131286621,
      "learning_rate": 2.8509707481232203e-05,
      "loss": 1.2583,
      "step": 2020
    },
    {
      "epoch": 0.15724549274772942,
      "grad_norm": 1.4890421628952026,
      "learning_rate": 2.850194149624644e-05,
      "loss": 1.2456,
      "step": 2030
    },
    {
      "epoch": 0.1580201010863882,
      "grad_norm": 1.5303229093551636,
      "learning_rate": 2.8494175511260678e-05,
      "loss": 1.1829,
      "step": 2040
    },
    {
      "epoch": 0.15879470942504695,
      "grad_norm": 1.3150051832199097,
      "learning_rate": 2.8486409526274916e-05,
      "loss": 1.1964,
      "step": 2050
    },
    {
      "epoch": 0.15956931776370573,
      "grad_norm": 2.2993597984313965,
      "learning_rate": 2.8478643541289154e-05,
      "loss": 1.2561,
      "step": 2060
    },
    {
      "epoch": 0.1603439261023645,
      "grad_norm": 1.2593640089035034,
      "learning_rate": 2.847165415480197e-05,
      "loss": 1.2135,
      "step": 2070
    },
    {
      "epoch": 0.16111853444102325,
      "grad_norm": 1.6896073818206787,
      "learning_rate": 2.8463888169816205e-05,
      "loss": 1.1226,
      "step": 2080
    },
    {
      "epoch": 0.16189314277968203,
      "grad_norm": 1.7629001140594482,
      "learning_rate": 2.8456122184830443e-05,
      "loss": 1.1904,
      "step": 2090
    },
    {
      "epoch": 0.16266775111834078,
      "grad_norm": 1.5596379041671753,
      "learning_rate": 2.844835619984468e-05,
      "loss": 1.1949,
      "step": 2100
    },
    {
      "epoch": 0.16344235945699956,
      "grad_norm": 1.1365787982940674,
      "learning_rate": 2.844059021485892e-05,
      "loss": 1.2197,
      "step": 2110
    },
    {
      "epoch": 0.1642169677956583,
      "grad_norm": 1.9793205261230469,
      "learning_rate": 2.8432824229873157e-05,
      "loss": 1.1704,
      "step": 2120
    },
    {
      "epoch": 0.1649915761343171,
      "grad_norm": 1.6242634057998657,
      "learning_rate": 2.8425058244887395e-05,
      "loss": 1.2329,
      "step": 2130
    },
    {
      "epoch": 0.16576618447297586,
      "grad_norm": 1.2651419639587402,
      "learning_rate": 2.8417292259901632e-05,
      "loss": 1.1774,
      "step": 2140
    },
    {
      "epoch": 0.1665407928116346,
      "grad_norm": 1.1640055179595947,
      "learning_rate": 2.840952627491587e-05,
      "loss": 1.1711,
      "step": 2150
    },
    {
      "epoch": 0.1673154011502934,
      "grad_norm": 1.6262831687927246,
      "learning_rate": 2.8401760289930108e-05,
      "loss": 1.183,
      "step": 2160
    },
    {
      "epoch": 0.16809000948895214,
      "grad_norm": 1.2030079364776611,
      "learning_rate": 2.8393994304944343e-05,
      "loss": 1.2097,
      "step": 2170
    },
    {
      "epoch": 0.16886461782761092,
      "grad_norm": 1.4063634872436523,
      "learning_rate": 2.838622831995858e-05,
      "loss": 1.2133,
      "step": 2180
    },
    {
      "epoch": 0.16963922616626967,
      "grad_norm": 2.0104522705078125,
      "learning_rate": 2.837846233497282e-05,
      "loss": 1.1799,
      "step": 2190
    },
    {
      "epoch": 0.17041383450492845,
      "grad_norm": 1.3570404052734375,
      "learning_rate": 2.8370696349987056e-05,
      "loss": 1.2519,
      "step": 2200
    },
    {
      "epoch": 0.17118844284358722,
      "grad_norm": 1.1871577501296997,
      "learning_rate": 2.8362930365001294e-05,
      "loss": 1.2786,
      "step": 2210
    },
    {
      "epoch": 0.17196305118224597,
      "grad_norm": 1.3508363962173462,
      "learning_rate": 2.8355164380015532e-05,
      "loss": 1.1997,
      "step": 2220
    },
    {
      "epoch": 0.17273765952090475,
      "grad_norm": 1.3861100673675537,
      "learning_rate": 2.834739839502977e-05,
      "loss": 1.2013,
      "step": 2230
    },
    {
      "epoch": 0.1735122678595635,
      "grad_norm": 1.179400086402893,
      "learning_rate": 2.833963241004401e-05,
      "loss": 1.2239,
      "step": 2240
    },
    {
      "epoch": 0.17428687619822228,
      "grad_norm": 1.676238775253296,
      "learning_rate": 2.8331866425058246e-05,
      "loss": 1.1902,
      "step": 2250
    },
    {
      "epoch": 0.17506148453688103,
      "grad_norm": 1.327133297920227,
      "learning_rate": 2.8324100440072483e-05,
      "loss": 1.2171,
      "step": 2260
    },
    {
      "epoch": 0.1758360928755398,
      "grad_norm": 1.2584736347198486,
      "learning_rate": 2.831633445508672e-05,
      "loss": 1.2233,
      "step": 2270
    },
    {
      "epoch": 0.17661070121419858,
      "grad_norm": 1.4916027784347534,
      "learning_rate": 2.830856847010096e-05,
      "loss": 1.2328,
      "step": 2280
    },
    {
      "epoch": 0.17738530955285733,
      "grad_norm": 1.6763660907745361,
      "learning_rate": 2.8300802485115197e-05,
      "loss": 1.2417,
      "step": 2290
    },
    {
      "epoch": 0.1781599178915161,
      "grad_norm": 1.478444218635559,
      "learning_rate": 2.8293036500129435e-05,
      "loss": 1.2025,
      "step": 2300
    },
    {
      "epoch": 0.17893452623017486,
      "grad_norm": 1.3504810333251953,
      "learning_rate": 2.8285270515143673e-05,
      "loss": 1.1861,
      "step": 2310
    },
    {
      "epoch": 0.17970913456883364,
      "grad_norm": 1.2408814430236816,
      "learning_rate": 2.827750453015791e-05,
      "loss": 1.1723,
      "step": 2320
    },
    {
      "epoch": 0.1804837429074924,
      "grad_norm": 1.30754554271698,
      "learning_rate": 2.8269738545172145e-05,
      "loss": 1.245,
      "step": 2330
    },
    {
      "epoch": 0.18125835124615117,
      "grad_norm": 1.6224416494369507,
      "learning_rate": 2.8261972560186383e-05,
      "loss": 1.2115,
      "step": 2340
    },
    {
      "epoch": 0.18203295958480992,
      "grad_norm": 1.3393012285232544,
      "learning_rate": 2.825420657520062e-05,
      "loss": 1.2125,
      "step": 2350
    },
    {
      "epoch": 0.1828075679234687,
      "grad_norm": 1.3260940313339233,
      "learning_rate": 2.824644059021486e-05,
      "loss": 1.308,
      "step": 2360
    },
    {
      "epoch": 0.18358217626212747,
      "grad_norm": 1.0222382545471191,
      "learning_rate": 2.8238674605229097e-05,
      "loss": 1.2476,
      "step": 2370
    },
    {
      "epoch": 0.18435678460078622,
      "grad_norm": 1.5737627744674683,
      "learning_rate": 2.8230908620243334e-05,
      "loss": 1.1414,
      "step": 2380
    },
    {
      "epoch": 0.185131392939445,
      "grad_norm": 1.3507094383239746,
      "learning_rate": 2.8223142635257572e-05,
      "loss": 1.2668,
      "step": 2390
    },
    {
      "epoch": 0.18590600127810375,
      "grad_norm": 2.296400308609009,
      "learning_rate": 2.821537665027181e-05,
      "loss": 1.1909,
      "step": 2400
    },
    {
      "epoch": 0.18668060961676253,
      "grad_norm": 1.449217677116394,
      "learning_rate": 2.8207610665286048e-05,
      "loss": 1.254,
      "step": 2410
    },
    {
      "epoch": 0.18745521795542128,
      "grad_norm": 1.733045220375061,
      "learning_rate": 2.8199844680300283e-05,
      "loss": 1.2207,
      "step": 2420
    },
    {
      "epoch": 0.18822982629408005,
      "grad_norm": 1.518309473991394,
      "learning_rate": 2.819207869531452e-05,
      "loss": 1.2132,
      "step": 2430
    },
    {
      "epoch": 0.18900443463273883,
      "grad_norm": 1.9870127439498901,
      "learning_rate": 2.818431271032876e-05,
      "loss": 1.2599,
      "step": 2440
    },
    {
      "epoch": 0.18977904297139758,
      "grad_norm": 1.4707716703414917,
      "learning_rate": 2.8176546725343e-05,
      "loss": 1.1326,
      "step": 2450
    },
    {
      "epoch": 0.19055365131005636,
      "grad_norm": 1.5383926630020142,
      "learning_rate": 2.8168780740357237e-05,
      "loss": 1.2107,
      "step": 2460
    },
    {
      "epoch": 0.1913282596487151,
      "grad_norm": 1.3497892618179321,
      "learning_rate": 2.8161014755371475e-05,
      "loss": 1.1145,
      "step": 2470
    },
    {
      "epoch": 0.1921028679873739,
      "grad_norm": 1.2868610620498657,
      "learning_rate": 2.8153248770385713e-05,
      "loss": 1.3267,
      "step": 2480
    },
    {
      "epoch": 0.19287747632603264,
      "grad_norm": 0.9848746061325073,
      "learning_rate": 2.814548278539995e-05,
      "loss": 1.2824,
      "step": 2490
    },
    {
      "epoch": 0.19365208466469142,
      "grad_norm": 2.9665796756744385,
      "learning_rate": 2.8137716800414185e-05,
      "loss": 1.2534,
      "step": 2500
    },
    {
      "epoch": 0.1944266930033502,
      "grad_norm": 1.5640219449996948,
      "learning_rate": 2.8129950815428423e-05,
      "loss": 1.2006,
      "step": 2510
    },
    {
      "epoch": 0.19520130134200894,
      "grad_norm": 1.3973791599273682,
      "learning_rate": 2.812218483044266e-05,
      "loss": 1.2206,
      "step": 2520
    },
    {
      "epoch": 0.19597590968066772,
      "grad_norm": 1.3014109134674072,
      "learning_rate": 2.81144188454569e-05,
      "loss": 1.2139,
      "step": 2530
    },
    {
      "epoch": 0.19675051801932647,
      "grad_norm": 1.327660322189331,
      "learning_rate": 2.8106652860471137e-05,
      "loss": 1.2212,
      "step": 2540
    },
    {
      "epoch": 0.19752512635798525,
      "grad_norm": 1.4710030555725098,
      "learning_rate": 2.8098886875485375e-05,
      "loss": 1.223,
      "step": 2550
    },
    {
      "epoch": 0.198299734696644,
      "grad_norm": 1.2976078987121582,
      "learning_rate": 2.8091120890499613e-05,
      "loss": 1.131,
      "step": 2560
    },
    {
      "epoch": 0.19907434303530278,
      "grad_norm": 1.7265846729278564,
      "learning_rate": 2.808335490551385e-05,
      "loss": 1.1987,
      "step": 2570
    },
    {
      "epoch": 0.19984895137396155,
      "grad_norm": 1.31431245803833,
      "learning_rate": 2.807558892052809e-05,
      "loss": 1.1786,
      "step": 2580
    },
    {
      "epoch": 0.2006235597126203,
      "grad_norm": 1.6338894367218018,
      "learning_rate": 2.8067822935542323e-05,
      "loss": 1.1589,
      "step": 2590
    },
    {
      "epoch": 0.20139816805127908,
      "grad_norm": 1.6947832107543945,
      "learning_rate": 2.806005695055656e-05,
      "loss": 1.2058,
      "step": 2600
    },
    {
      "epoch": 0.20217277638993783,
      "grad_norm": 1.9087358713150024,
      "learning_rate": 2.80522909655708e-05,
      "loss": 1.1214,
      "step": 2610
    },
    {
      "epoch": 0.2029473847285966,
      "grad_norm": 2.003037452697754,
      "learning_rate": 2.8044524980585036e-05,
      "loss": 1.2132,
      "step": 2620
    },
    {
      "epoch": 0.20372199306725536,
      "grad_norm": 1.2002403736114502,
      "learning_rate": 2.8036758995599274e-05,
      "loss": 1.1869,
      "step": 2630
    },
    {
      "epoch": 0.20449660140591414,
      "grad_norm": 1.2994061708450317,
      "learning_rate": 2.8028993010613516e-05,
      "loss": 1.1267,
      "step": 2640
    },
    {
      "epoch": 0.2052712097445729,
      "grad_norm": 1.2917959690093994,
      "learning_rate": 2.8021227025627753e-05,
      "loss": 1.2957,
      "step": 2650
    },
    {
      "epoch": 0.20604581808323166,
      "grad_norm": 2.384093999862671,
      "learning_rate": 2.801346104064199e-05,
      "loss": 1.2185,
      "step": 2660
    },
    {
      "epoch": 0.20682042642189044,
      "grad_norm": 1.8734513521194458,
      "learning_rate": 2.8005695055656226e-05,
      "loss": 1.1179,
      "step": 2670
    },
    {
      "epoch": 0.2075950347605492,
      "grad_norm": 1.4200388193130493,
      "learning_rate": 2.7997929070670464e-05,
      "loss": 1.1887,
      "step": 2680
    },
    {
      "epoch": 0.20836964309920797,
      "grad_norm": 1.3857085704803467,
      "learning_rate": 2.79901630856847e-05,
      "loss": 1.244,
      "step": 2690
    },
    {
      "epoch": 0.20914425143786672,
      "grad_norm": 1.4796793460845947,
      "learning_rate": 2.798239710069894e-05,
      "loss": 1.2016,
      "step": 2700
    },
    {
      "epoch": 0.2099188597765255,
      "grad_norm": 1.8141679763793945,
      "learning_rate": 2.7974631115713177e-05,
      "loss": 1.2551,
      "step": 2710
    },
    {
      "epoch": 0.21069346811518427,
      "grad_norm": 1.0242401361465454,
      "learning_rate": 2.7966865130727415e-05,
      "loss": 1.303,
      "step": 2720
    },
    {
      "epoch": 0.21146807645384302,
      "grad_norm": 1.6331944465637207,
      "learning_rate": 2.7959099145741653e-05,
      "loss": 1.3,
      "step": 2730
    },
    {
      "epoch": 0.2122426847925018,
      "grad_norm": 2.0751423835754395,
      "learning_rate": 2.795133316075589e-05,
      "loss": 1.2336,
      "step": 2740
    },
    {
      "epoch": 0.21301729313116055,
      "grad_norm": 1.3400402069091797,
      "learning_rate": 2.794356717577013e-05,
      "loss": 1.2596,
      "step": 2750
    },
    {
      "epoch": 0.21379190146981933,
      "grad_norm": 1.433127999305725,
      "learning_rate": 2.7935801190784363e-05,
      "loss": 1.2441,
      "step": 2760
    },
    {
      "epoch": 0.21456650980847808,
      "grad_norm": 1.196757435798645,
      "learning_rate": 2.79280352057986e-05,
      "loss": 1.1577,
      "step": 2770
    },
    {
      "epoch": 0.21534111814713686,
      "grad_norm": 1.1921285390853882,
      "learning_rate": 2.792026922081284e-05,
      "loss": 1.155,
      "step": 2780
    },
    {
      "epoch": 0.2161157264857956,
      "grad_norm": 2.050990581512451,
      "learning_rate": 2.7912503235827077e-05,
      "loss": 1.246,
      "step": 2790
    },
    {
      "epoch": 0.21689033482445438,
      "grad_norm": 1.1767494678497314,
      "learning_rate": 2.7904737250841315e-05,
      "loss": 1.1682,
      "step": 2800
    },
    {
      "epoch": 0.21766494316311316,
      "grad_norm": 1.458603858947754,
      "learning_rate": 2.7896971265855553e-05,
      "loss": 1.1623,
      "step": 2810
    },
    {
      "epoch": 0.2184395515017719,
      "grad_norm": 1.4329379796981812,
      "learning_rate": 2.788920528086979e-05,
      "loss": 1.2353,
      "step": 2820
    },
    {
      "epoch": 0.2192141598404307,
      "grad_norm": 1.6977241039276123,
      "learning_rate": 2.788143929588403e-05,
      "loss": 1.1567,
      "step": 2830
    },
    {
      "epoch": 0.21998876817908944,
      "grad_norm": 1.8083648681640625,
      "learning_rate": 2.7873673310898266e-05,
      "loss": 1.2814,
      "step": 2840
    },
    {
      "epoch": 0.22076337651774822,
      "grad_norm": 1.4820410013198853,
      "learning_rate": 2.7865907325912504e-05,
      "loss": 1.2507,
      "step": 2850
    },
    {
      "epoch": 0.22153798485640697,
      "grad_norm": 1.2547478675842285,
      "learning_rate": 2.7858141340926742e-05,
      "loss": 1.1419,
      "step": 2860
    },
    {
      "epoch": 0.22231259319506574,
      "grad_norm": 1.5173314809799194,
      "learning_rate": 2.785037535594098e-05,
      "loss": 1.2928,
      "step": 2870
    },
    {
      "epoch": 0.22308720153372452,
      "grad_norm": 1.3816007375717163,
      "learning_rate": 2.7842609370955218e-05,
      "loss": 1.1871,
      "step": 2880
    },
    {
      "epoch": 0.22386180987238327,
      "grad_norm": 1.6144529581069946,
      "learning_rate": 2.7834843385969455e-05,
      "loss": 1.209,
      "step": 2890
    },
    {
      "epoch": 0.22463641821104205,
      "grad_norm": 2.0403144359588623,
      "learning_rate": 2.7827077400983693e-05,
      "loss": 1.2186,
      "step": 2900
    },
    {
      "epoch": 0.2254110265497008,
      "grad_norm": 1.5397770404815674,
      "learning_rate": 2.781931141599793e-05,
      "loss": 1.1876,
      "step": 2910
    },
    {
      "epoch": 0.22618563488835958,
      "grad_norm": 1.7560207843780518,
      "learning_rate": 2.781154543101217e-05,
      "loss": 1.2219,
      "step": 2920
    },
    {
      "epoch": 0.22696024322701833,
      "grad_norm": 1.4838476181030273,
      "learning_rate": 2.7803779446026404e-05,
      "loss": 1.2172,
      "step": 2930
    },
    {
      "epoch": 0.2277348515656771,
      "grad_norm": 1.3595770597457886,
      "learning_rate": 2.779601346104064e-05,
      "loss": 1.2072,
      "step": 2940
    },
    {
      "epoch": 0.22850945990433588,
      "grad_norm": 1.2786403894424438,
      "learning_rate": 2.778824747605488e-05,
      "loss": 1.278,
      "step": 2950
    },
    {
      "epoch": 0.22928406824299463,
      "grad_norm": 1.5218044519424438,
      "learning_rate": 2.7780481491069117e-05,
      "loss": 1.22,
      "step": 2960
    },
    {
      "epoch": 0.2300586765816534,
      "grad_norm": 2.2143664360046387,
      "learning_rate": 2.7772715506083355e-05,
      "loss": 1.3011,
      "step": 2970
    },
    {
      "epoch": 0.23083328492031216,
      "grad_norm": 1.4723498821258545,
      "learning_rate": 2.7764949521097593e-05,
      "loss": 1.2059,
      "step": 2980
    },
    {
      "epoch": 0.23160789325897094,
      "grad_norm": 1.607140064239502,
      "learning_rate": 2.775718353611183e-05,
      "loss": 1.1692,
      "step": 2990
    },
    {
      "epoch": 0.2323825015976297,
      "grad_norm": 1.8184442520141602,
      "learning_rate": 2.774941755112607e-05,
      "loss": 1.256,
      "step": 3000
    },
    {
      "epoch": 0.23315710993628846,
      "grad_norm": 2.0392370223999023,
      "learning_rate": 2.7741651566140303e-05,
      "loss": 1.2493,
      "step": 3010
    },
    {
      "epoch": 0.23393171827494724,
      "grad_norm": 1.3045331239700317,
      "learning_rate": 2.773388558115454e-05,
      "loss": 1.293,
      "step": 3020
    },
    {
      "epoch": 0.234706326613606,
      "grad_norm": 1.840510368347168,
      "learning_rate": 2.7726119596168782e-05,
      "loss": 1.1948,
      "step": 3030
    },
    {
      "epoch": 0.23548093495226477,
      "grad_norm": 1.91966712474823,
      "learning_rate": 2.771835361118302e-05,
      "loss": 1.2293,
      "step": 3040
    },
    {
      "epoch": 0.23625554329092352,
      "grad_norm": 1.5311893224716187,
      "learning_rate": 2.7710587626197258e-05,
      "loss": 1.2414,
      "step": 3050
    },
    {
      "epoch": 0.2370301516295823,
      "grad_norm": 2.9023451805114746,
      "learning_rate": 2.7702821641211496e-05,
      "loss": 1.2285,
      "step": 3060
    },
    {
      "epoch": 0.23780475996824105,
      "grad_norm": 1.9069933891296387,
      "learning_rate": 2.7695055656225734e-05,
      "loss": 1.1533,
      "step": 3070
    },
    {
      "epoch": 0.23857936830689982,
      "grad_norm": 1.315039038658142,
      "learning_rate": 2.768728967123997e-05,
      "loss": 1.1523,
      "step": 3080
    },
    {
      "epoch": 0.2393539766455586,
      "grad_norm": 1.3972867727279663,
      "learning_rate": 2.767952368625421e-05,
      "loss": 1.1467,
      "step": 3090
    },
    {
      "epoch": 0.24012858498421735,
      "grad_norm": 1.7311549186706543,
      "learning_rate": 2.7671757701268444e-05,
      "loss": 1.1804,
      "step": 3100
    },
    {
      "epoch": 0.24090319332287613,
      "grad_norm": 1.3029530048370361,
      "learning_rate": 2.7663991716282682e-05,
      "loss": 1.19,
      "step": 3110
    },
    {
      "epoch": 0.24167780166153488,
      "grad_norm": 1.546355962753296,
      "learning_rate": 2.765622573129692e-05,
      "loss": 1.1068,
      "step": 3120
    },
    {
      "epoch": 0.24245241000019366,
      "grad_norm": 1.568985939025879,
      "learning_rate": 2.7648459746311157e-05,
      "loss": 1.1392,
      "step": 3130
    },
    {
      "epoch": 0.2432270183388524,
      "grad_norm": 1.6415237188339233,
      "learning_rate": 2.7640693761325395e-05,
      "loss": 1.1719,
      "step": 3140
    },
    {
      "epoch": 0.24400162667751119,
      "grad_norm": 1.536138653755188,
      "learning_rate": 2.7632927776339633e-05,
      "loss": 1.234,
      "step": 3150
    },
    {
      "epoch": 0.24477623501616996,
      "grad_norm": 1.8093020915985107,
      "learning_rate": 2.762516179135387e-05,
      "loss": 1.1965,
      "step": 3160
    },
    {
      "epoch": 0.2455508433548287,
      "grad_norm": 1.934756875038147,
      "learning_rate": 2.761739580636811e-05,
      "loss": 1.1644,
      "step": 3170
    },
    {
      "epoch": 0.2463254516934875,
      "grad_norm": 1.7341676950454712,
      "learning_rate": 2.7609629821382343e-05,
      "loss": 1.1067,
      "step": 3180
    },
    {
      "epoch": 0.24710006003214624,
      "grad_norm": 1.3598670959472656,
      "learning_rate": 2.760186383639658e-05,
      "loss": 1.2114,
      "step": 3190
    },
    {
      "epoch": 0.24787466837080502,
      "grad_norm": 1.770603060722351,
      "learning_rate": 2.759409785141082e-05,
      "loss": 1.203,
      "step": 3200
    },
    {
      "epoch": 0.24864927670946377,
      "grad_norm": 1.7100672721862793,
      "learning_rate": 2.7586331866425057e-05,
      "loss": 1.1505,
      "step": 3210
    },
    {
      "epoch": 0.24942388504812255,
      "grad_norm": 1.1098963022232056,
      "learning_rate": 2.7578565881439298e-05,
      "loss": 1.2437,
      "step": 3220
    },
    {
      "epoch": 0.2501984933867813,
      "grad_norm": 1.315194010734558,
      "learning_rate": 2.7570799896453536e-05,
      "loss": 1.2113,
      "step": 3230
    },
    {
      "epoch": 0.2509731017254401,
      "grad_norm": 1.2700024843215942,
      "learning_rate": 2.7563033911467774e-05,
      "loss": 1.2576,
      "step": 3240
    },
    {
      "epoch": 0.25174771006409885,
      "grad_norm": 1.4496656656265259,
      "learning_rate": 2.7555267926482012e-05,
      "loss": 1.1566,
      "step": 3250
    },
    {
      "epoch": 0.25252231840275763,
      "grad_norm": 1.786487340927124,
      "learning_rate": 2.754750194149625e-05,
      "loss": 1.1277,
      "step": 3260
    },
    {
      "epoch": 0.25329692674141635,
      "grad_norm": 1.2609308958053589,
      "learning_rate": 2.7539735956510484e-05,
      "loss": 1.2067,
      "step": 3270
    },
    {
      "epoch": 0.25407153508007513,
      "grad_norm": 1.452635645866394,
      "learning_rate": 2.7531969971524722e-05,
      "loss": 1.2038,
      "step": 3280
    },
    {
      "epoch": 0.2548461434187339,
      "grad_norm": 1.7158868312835693,
      "learning_rate": 2.752420398653896e-05,
      "loss": 1.1329,
      "step": 3290
    },
    {
      "epoch": 0.2556207517573927,
      "grad_norm": 1.8371989727020264,
      "learning_rate": 2.7516438001553198e-05,
      "loss": 1.2239,
      "step": 3300
    },
    {
      "epoch": 0.25639536009605146,
      "grad_norm": 1.275698184967041,
      "learning_rate": 2.7508672016567436e-05,
      "loss": 1.2242,
      "step": 3310
    },
    {
      "epoch": 0.2571699684347102,
      "grad_norm": 2.2117245197296143,
      "learning_rate": 2.7500906031581674e-05,
      "loss": 1.24,
      "step": 3320
    },
    {
      "epoch": 0.25794457677336896,
      "grad_norm": 1.6474637985229492,
      "learning_rate": 2.749314004659591e-05,
      "loss": 1.333,
      "step": 3330
    },
    {
      "epoch": 0.25871918511202774,
      "grad_norm": 1.5536010265350342,
      "learning_rate": 2.748537406161015e-05,
      "loss": 1.1548,
      "step": 3340
    },
    {
      "epoch": 0.2594937934506865,
      "grad_norm": 1.4304450750350952,
      "learning_rate": 2.7477608076624384e-05,
      "loss": 1.2118,
      "step": 3350
    },
    {
      "epoch": 0.26026840178934524,
      "grad_norm": 1.4214575290679932,
      "learning_rate": 2.746984209163862e-05,
      "loss": 1.1647,
      "step": 3360
    },
    {
      "epoch": 0.261043010128004,
      "grad_norm": 1.2810070514678955,
      "learning_rate": 2.746207610665286e-05,
      "loss": 1.1575,
      "step": 3370
    },
    {
      "epoch": 0.2618176184666628,
      "grad_norm": 1.0110219717025757,
      "learning_rate": 2.7454310121667097e-05,
      "loss": 1.3317,
      "step": 3380
    },
    {
      "epoch": 0.26259222680532157,
      "grad_norm": 1.4298580884933472,
      "learning_rate": 2.7446544136681335e-05,
      "loss": 1.2375,
      "step": 3390
    },
    {
      "epoch": 0.26336683514398035,
      "grad_norm": 1.4743903875350952,
      "learning_rate": 2.7438778151695573e-05,
      "loss": 1.151,
      "step": 3400
    },
    {
      "epoch": 0.26414144348263907,
      "grad_norm": 1.6761975288391113,
      "learning_rate": 2.743101216670981e-05,
      "loss": 1.2183,
      "step": 3410
    },
    {
      "epoch": 0.26491605182129785,
      "grad_norm": 1.8281562328338623,
      "learning_rate": 2.7423246181724052e-05,
      "loss": 1.2066,
      "step": 3420
    },
    {
      "epoch": 0.2656906601599566,
      "grad_norm": 1.8550373315811157,
      "learning_rate": 2.741548019673829e-05,
      "loss": 1.249,
      "step": 3430
    },
    {
      "epoch": 0.2664652684986154,
      "grad_norm": 1.7755365371704102,
      "learning_rate": 2.7407714211752525e-05,
      "loss": 1.1852,
      "step": 3440
    },
    {
      "epoch": 0.2672398768372741,
      "grad_norm": 1.75336492061615,
      "learning_rate": 2.7399948226766762e-05,
      "loss": 1.1752,
      "step": 3450
    },
    {
      "epoch": 0.2680144851759329,
      "grad_norm": 1.2584971189498901,
      "learning_rate": 2.7392182241781e-05,
      "loss": 1.2962,
      "step": 3460
    },
    {
      "epoch": 0.2687890935145917,
      "grad_norm": 2.456353187561035,
      "learning_rate": 2.7384416256795238e-05,
      "loss": 1.1768,
      "step": 3470
    },
    {
      "epoch": 0.26956370185325046,
      "grad_norm": 1.0867506265640259,
      "learning_rate": 2.7376650271809476e-05,
      "loss": 1.2449,
      "step": 3480
    },
    {
      "epoch": 0.27033831019190924,
      "grad_norm": 1.8991470336914062,
      "learning_rate": 2.7368884286823714e-05,
      "loss": 1.2345,
      "step": 3490
    },
    {
      "epoch": 0.27111291853056796,
      "grad_norm": 1.1067317724227905,
      "learning_rate": 2.7361118301837952e-05,
      "loss": 1.1911,
      "step": 3500
    },
    {
      "epoch": 0.27188752686922674,
      "grad_norm": 1.1380075216293335,
      "learning_rate": 2.735335231685219e-05,
      "loss": 1.1976,
      "step": 3510
    },
    {
      "epoch": 0.2726621352078855,
      "grad_norm": 1.9120886325836182,
      "learning_rate": 2.7345586331866424e-05,
      "loss": 1.2048,
      "step": 3520
    },
    {
      "epoch": 0.2734367435465443,
      "grad_norm": 1.269295334815979,
      "learning_rate": 2.7337820346880662e-05,
      "loss": 1.1721,
      "step": 3530
    },
    {
      "epoch": 0.27421135188520307,
      "grad_norm": 1.6111819744110107,
      "learning_rate": 2.73300543618949e-05,
      "loss": 1.1844,
      "step": 3540
    },
    {
      "epoch": 0.2749859602238618,
      "grad_norm": 1.7076529264450073,
      "learning_rate": 2.7322288376909138e-05,
      "loss": 1.2263,
      "step": 3550
    },
    {
      "epoch": 0.27576056856252057,
      "grad_norm": 1.7409964799880981,
      "learning_rate": 2.7314522391923376e-05,
      "loss": 1.2822,
      "step": 3560
    },
    {
      "epoch": 0.27653517690117935,
      "grad_norm": 2.95678448677063,
      "learning_rate": 2.7306756406937613e-05,
      "loss": 1.252,
      "step": 3570
    },
    {
      "epoch": 0.2773097852398381,
      "grad_norm": 2.0041656494140625,
      "learning_rate": 2.729899042195185e-05,
      "loss": 1.2314,
      "step": 3580
    },
    {
      "epoch": 0.27808439357849685,
      "grad_norm": 2.02593994140625,
      "learning_rate": 2.729122443696609e-05,
      "loss": 1.1916,
      "step": 3590
    },
    {
      "epoch": 0.2788590019171556,
      "grad_norm": 1.9667601585388184,
      "learning_rate": 2.7283458451980327e-05,
      "loss": 1.2338,
      "step": 3600
    },
    {
      "epoch": 0.2796336102558144,
      "grad_norm": 1.520942211151123,
      "learning_rate": 2.727569246699456e-05,
      "loss": 1.2256,
      "step": 3610
    },
    {
      "epoch": 0.2804082185944732,
      "grad_norm": 1.5583264827728271,
      "learning_rate": 2.7267926482008803e-05,
      "loss": 1.2485,
      "step": 3620
    },
    {
      "epoch": 0.28118282693313196,
      "grad_norm": 1.6202821731567383,
      "learning_rate": 2.726016049702304e-05,
      "loss": 1.3652,
      "step": 3630
    },
    {
      "epoch": 0.2819574352717907,
      "grad_norm": 1.4708428382873535,
      "learning_rate": 2.725239451203728e-05,
      "loss": 1.2211,
      "step": 3640
    },
    {
      "epoch": 0.28273204361044946,
      "grad_norm": 1.3255724906921387,
      "learning_rate": 2.7244628527051516e-05,
      "loss": 1.1853,
      "step": 3650
    },
    {
      "epoch": 0.28350665194910823,
      "grad_norm": 1.6845674514770508,
      "learning_rate": 2.7236862542065754e-05,
      "loss": 1.1862,
      "step": 3660
    },
    {
      "epoch": 0.284281260287767,
      "grad_norm": 1.2241144180297852,
      "learning_rate": 2.7229096557079992e-05,
      "loss": 1.1745,
      "step": 3670
    },
    {
      "epoch": 0.2850558686264258,
      "grad_norm": 1.407267689704895,
      "learning_rate": 2.722133057209423e-05,
      "loss": 1.1179,
      "step": 3680
    },
    {
      "epoch": 0.2858304769650845,
      "grad_norm": 1.8670250177383423,
      "learning_rate": 2.7213564587108464e-05,
      "loss": 1.3254,
      "step": 3690
    },
    {
      "epoch": 0.2866050853037433,
      "grad_norm": 1.2009518146514893,
      "learning_rate": 2.7205798602122702e-05,
      "loss": 1.2576,
      "step": 3700
    },
    {
      "epoch": 0.28737969364240207,
      "grad_norm": 2.020207166671753,
      "learning_rate": 2.719803261713694e-05,
      "loss": 1.1808,
      "step": 3710
    },
    {
      "epoch": 0.28815430198106085,
      "grad_norm": 1.8510771989822388,
      "learning_rate": 2.7190266632151178e-05,
      "loss": 1.2136,
      "step": 3720
    },
    {
      "epoch": 0.28892891031971957,
      "grad_norm": 1.380924940109253,
      "learning_rate": 2.7182500647165416e-05,
      "loss": 1.1625,
      "step": 3730
    },
    {
      "epoch": 0.28970351865837835,
      "grad_norm": 1.6930750608444214,
      "learning_rate": 2.7174734662179654e-05,
      "loss": 1.2581,
      "step": 3740
    },
    {
      "epoch": 0.2904781269970371,
      "grad_norm": 1.8570656776428223,
      "learning_rate": 2.716696867719389e-05,
      "loss": 1.2274,
      "step": 3750
    },
    {
      "epoch": 0.2912527353356959,
      "grad_norm": 1.7270159721374512,
      "learning_rate": 2.715920269220813e-05,
      "loss": 1.2627,
      "step": 3760
    },
    {
      "epoch": 0.2920273436743547,
      "grad_norm": 1.693095326423645,
      "learning_rate": 2.7151436707222367e-05,
      "loss": 1.2181,
      "step": 3770
    },
    {
      "epoch": 0.2928019520130134,
      "grad_norm": 1.1536837816238403,
      "learning_rate": 2.7143670722236602e-05,
      "loss": 1.199,
      "step": 3780
    },
    {
      "epoch": 0.2935765603516722,
      "grad_norm": 1.6762526035308838,
      "learning_rate": 2.713590473725084e-05,
      "loss": 1.2723,
      "step": 3790
    },
    {
      "epoch": 0.29435116869033096,
      "grad_norm": 1.5715174674987793,
      "learning_rate": 2.7128138752265078e-05,
      "loss": 1.1898,
      "step": 3800
    },
    {
      "epoch": 0.29512577702898973,
      "grad_norm": 1.7386072874069214,
      "learning_rate": 2.712037276727932e-05,
      "loss": 1.2359,
      "step": 3810
    },
    {
      "epoch": 0.29590038536764846,
      "grad_norm": 1.2483445405960083,
      "learning_rate": 2.7112606782293557e-05,
      "loss": 1.1904,
      "step": 3820
    },
    {
      "epoch": 0.29667499370630723,
      "grad_norm": 2.2332584857940674,
      "learning_rate": 2.7104840797307795e-05,
      "loss": 1.2515,
      "step": 3830
    },
    {
      "epoch": 0.297449602044966,
      "grad_norm": 1.627479910850525,
      "learning_rate": 2.7097074812322032e-05,
      "loss": 1.2177,
      "step": 3840
    },
    {
      "epoch": 0.2982242103836248,
      "grad_norm": 1.5647094249725342,
      "learning_rate": 2.708930882733627e-05,
      "loss": 1.2344,
      "step": 3850
    },
    {
      "epoch": 0.29899881872228357,
      "grad_norm": 1.5903829336166382,
      "learning_rate": 2.7081542842350505e-05,
      "loss": 1.2618,
      "step": 3860
    },
    {
      "epoch": 0.2997734270609423,
      "grad_norm": 1.4133228063583374,
      "learning_rate": 2.7073776857364743e-05,
      "loss": 1.1711,
      "step": 3870
    },
    {
      "epoch": 0.30054803539960107,
      "grad_norm": 1.8540558815002441,
      "learning_rate": 2.706601087237898e-05,
      "loss": 1.2148,
      "step": 3880
    },
    {
      "epoch": 0.30132264373825984,
      "grad_norm": 1.6311936378479004,
      "learning_rate": 2.705824488739322e-05,
      "loss": 1.3,
      "step": 3890
    },
    {
      "epoch": 0.3020972520769186,
      "grad_norm": 1.54327392578125,
      "learning_rate": 2.7050478902407456e-05,
      "loss": 1.2384,
      "step": 3900
    },
    {
      "epoch": 0.3028718604155774,
      "grad_norm": 1.63038969039917,
      "learning_rate": 2.7042712917421694e-05,
      "loss": 1.1559,
      "step": 3910
    },
    {
      "epoch": 0.3036464687542361,
      "grad_norm": 1.2232040166854858,
      "learning_rate": 2.7034946932435932e-05,
      "loss": 1.1261,
      "step": 3920
    },
    {
      "epoch": 0.3044210770928949,
      "grad_norm": 1.5910025835037231,
      "learning_rate": 2.702718094745017e-05,
      "loss": 1.2531,
      "step": 3930
    },
    {
      "epoch": 0.3051956854315537,
      "grad_norm": 1.5333201885223389,
      "learning_rate": 2.7019414962464404e-05,
      "loss": 1.1286,
      "step": 3940
    },
    {
      "epoch": 0.30597029377021245,
      "grad_norm": 1.4954622983932495,
      "learning_rate": 2.7011648977478642e-05,
      "loss": 1.2078,
      "step": 3950
    },
    {
      "epoch": 0.3067449021088712,
      "grad_norm": 1.390753149986267,
      "learning_rate": 2.700388299249288e-05,
      "loss": 1.2896,
      "step": 3960
    },
    {
      "epoch": 0.30751951044752995,
      "grad_norm": 2.0321245193481445,
      "learning_rate": 2.6996117007507118e-05,
      "loss": 1.2085,
      "step": 3970
    },
    {
      "epoch": 0.30829411878618873,
      "grad_norm": 1.5218051671981812,
      "learning_rate": 2.6988351022521356e-05,
      "loss": 1.2812,
      "step": 3980
    },
    {
      "epoch": 0.3090687271248475,
      "grad_norm": 1.4268710613250732,
      "learning_rate": 2.6980585037535594e-05,
      "loss": 1.3084,
      "step": 3990
    },
    {
      "epoch": 0.3098433354635063,
      "grad_norm": 1.5824133157730103,
      "learning_rate": 2.697281905254983e-05,
      "loss": 1.1218,
      "step": 4000
    },
    {
      "epoch": 0.310617943802165,
      "grad_norm": 1.0591837167739868,
      "learning_rate": 2.6965053067564073e-05,
      "loss": 1.1799,
      "step": 4010
    },
    {
      "epoch": 0.3113925521408238,
      "grad_norm": 1.3348534107208252,
      "learning_rate": 2.695728708257831e-05,
      "loss": 1.1894,
      "step": 4020
    },
    {
      "epoch": 0.31216716047948256,
      "grad_norm": 1.027396559715271,
      "learning_rate": 2.6949521097592545e-05,
      "loss": 1.0711,
      "step": 4030
    },
    {
      "epoch": 0.31294176881814134,
      "grad_norm": 1.6238009929656982,
      "learning_rate": 2.6941755112606783e-05,
      "loss": 1.2091,
      "step": 4040
    },
    {
      "epoch": 0.3137163771568001,
      "grad_norm": 1.6943529844284058,
      "learning_rate": 2.693398912762102e-05,
      "loss": 1.2874,
      "step": 4050
    },
    {
      "epoch": 0.31449098549545884,
      "grad_norm": 1.5264713764190674,
      "learning_rate": 2.692622314263526e-05,
      "loss": 1.2797,
      "step": 4060
    },
    {
      "epoch": 0.3152655938341176,
      "grad_norm": 1.4443527460098267,
      "learning_rate": 2.6918457157649497e-05,
      "loss": 1.1632,
      "step": 4070
    },
    {
      "epoch": 0.3160402021727764,
      "grad_norm": 1.0559179782867432,
      "learning_rate": 2.6910691172663734e-05,
      "loss": 1.2419,
      "step": 4080
    },
    {
      "epoch": 0.3168148105114352,
      "grad_norm": 1.851130723953247,
      "learning_rate": 2.6902925187677972e-05,
      "loss": 1.1955,
      "step": 4090
    },
    {
      "epoch": 0.3175894188500939,
      "grad_norm": 1.2622231245040894,
      "learning_rate": 2.689515920269221e-05,
      "loss": 1.2169,
      "step": 4100
    },
    {
      "epoch": 0.3183640271887527,
      "grad_norm": 1.8555222749710083,
      "learning_rate": 2.6887393217706445e-05,
      "loss": 1.1667,
      "step": 4110
    },
    {
      "epoch": 0.31913863552741145,
      "grad_norm": 1.4405736923217773,
      "learning_rate": 2.6879627232720683e-05,
      "loss": 1.1719,
      "step": 4120
    },
    {
      "epoch": 0.31991324386607023,
      "grad_norm": 1.7726002931594849,
      "learning_rate": 2.687186124773492e-05,
      "loss": 1.2601,
      "step": 4130
    },
    {
      "epoch": 0.320687852204729,
      "grad_norm": 1.8039309978485107,
      "learning_rate": 2.6864095262749158e-05,
      "loss": 1.2352,
      "step": 4140
    },
    {
      "epoch": 0.32146246054338773,
      "grad_norm": 0.9805991053581238,
      "learning_rate": 2.6856329277763396e-05,
      "loss": 1.202,
      "step": 4150
    },
    {
      "epoch": 0.3222370688820465,
      "grad_norm": 1.7070392370224,
      "learning_rate": 2.6848563292777634e-05,
      "loss": 1.1789,
      "step": 4160
    },
    {
      "epoch": 0.3230116772207053,
      "grad_norm": 1.2878113985061646,
      "learning_rate": 2.6840797307791872e-05,
      "loss": 1.2899,
      "step": 4170
    },
    {
      "epoch": 0.32378628555936406,
      "grad_norm": 1.3429499864578247,
      "learning_rate": 2.683303132280611e-05,
      "loss": 1.2075,
      "step": 4180
    },
    {
      "epoch": 0.32456089389802284,
      "grad_norm": 2.0086352825164795,
      "learning_rate": 2.6825265337820348e-05,
      "loss": 1.2561,
      "step": 4190
    },
    {
      "epoch": 0.32533550223668156,
      "grad_norm": 1.4463554620742798,
      "learning_rate": 2.6817499352834582e-05,
      "loss": 1.1702,
      "step": 4200
    },
    {
      "epoch": 0.32611011057534034,
      "grad_norm": 1.5061944723129272,
      "learning_rate": 2.6809733367848823e-05,
      "loss": 1.204,
      "step": 4210
    },
    {
      "epoch": 0.3268847189139991,
      "grad_norm": 1.352953553199768,
      "learning_rate": 2.680196738286306e-05,
      "loss": 1.1486,
      "step": 4220
    },
    {
      "epoch": 0.3276593272526579,
      "grad_norm": 1.9993840456008911,
      "learning_rate": 2.67942013978773e-05,
      "loss": 1.1689,
      "step": 4230
    },
    {
      "epoch": 0.3284339355913166,
      "grad_norm": 1.6770564317703247,
      "learning_rate": 2.6786435412891537e-05,
      "loss": 1.2313,
      "step": 4240
    },
    {
      "epoch": 0.3292085439299754,
      "grad_norm": 1.5360150337219238,
      "learning_rate": 2.6778669427905775e-05,
      "loss": 1.2096,
      "step": 4250
    },
    {
      "epoch": 0.3299831522686342,
      "grad_norm": 1.7344911098480225,
      "learning_rate": 2.6770903442920013e-05,
      "loss": 1.2419,
      "step": 4260
    },
    {
      "epoch": 0.33075776060729295,
      "grad_norm": 2.0198395252227783,
      "learning_rate": 2.676313745793425e-05,
      "loss": 1.21,
      "step": 4270
    },
    {
      "epoch": 0.3315323689459517,
      "grad_norm": 1.5252659320831299,
      "learning_rate": 2.6755371472948485e-05,
      "loss": 1.0948,
      "step": 4280
    },
    {
      "epoch": 0.33230697728461045,
      "grad_norm": 1.51076078414917,
      "learning_rate": 2.6747605487962723e-05,
      "loss": 1.1575,
      "step": 4290
    },
    {
      "epoch": 0.3330815856232692,
      "grad_norm": 1.3142845630645752,
      "learning_rate": 2.673983950297696e-05,
      "loss": 1.2151,
      "step": 4300
    },
    {
      "epoch": 0.333856193961928,
      "grad_norm": 1.7493700981140137,
      "learning_rate": 2.67320735179912e-05,
      "loss": 1.1845,
      "step": 4310
    },
    {
      "epoch": 0.3346308023005868,
      "grad_norm": 1.625110387802124,
      "learning_rate": 2.6724307533005436e-05,
      "loss": 1.1983,
      "step": 4320
    },
    {
      "epoch": 0.3354054106392455,
      "grad_norm": 1.3128554821014404,
      "learning_rate": 2.6716541548019674e-05,
      "loss": 1.2046,
      "step": 4330
    },
    {
      "epoch": 0.3361800189779043,
      "grad_norm": 1.4620623588562012,
      "learning_rate": 2.6708775563033912e-05,
      "loss": 1.183,
      "step": 4340
    },
    {
      "epoch": 0.33695462731656306,
      "grad_norm": 2.0683343410491943,
      "learning_rate": 2.670100957804815e-05,
      "loss": 1.3053,
      "step": 4350
    },
    {
      "epoch": 0.33772923565522184,
      "grad_norm": 1.679007649421692,
      "learning_rate": 2.6693243593062388e-05,
      "loss": 1.1984,
      "step": 4360
    },
    {
      "epoch": 0.3385038439938806,
      "grad_norm": 1.6218794584274292,
      "learning_rate": 2.6685477608076622e-05,
      "loss": 1.1569,
      "step": 4370
    },
    {
      "epoch": 0.33927845233253934,
      "grad_norm": 1.3549660444259644,
      "learning_rate": 2.667771162309086e-05,
      "loss": 1.1742,
      "step": 4380
    },
    {
      "epoch": 0.3400530606711981,
      "grad_norm": 1.460240364074707,
      "learning_rate": 2.6669945638105098e-05,
      "loss": 1.2597,
      "step": 4390
    },
    {
      "epoch": 0.3408276690098569,
      "grad_norm": 1.4724632501602173,
      "learning_rate": 2.666217965311934e-05,
      "loss": 1.1669,
      "step": 4400
    },
    {
      "epoch": 0.34160227734851567,
      "grad_norm": 1.4467312097549438,
      "learning_rate": 2.6654413668133577e-05,
      "loss": 1.2206,
      "step": 4410
    },
    {
      "epoch": 0.34237688568717445,
      "grad_norm": 1.5417180061340332,
      "learning_rate": 2.6646647683147815e-05,
      "loss": 1.141,
      "step": 4420
    },
    {
      "epoch": 0.34315149402583317,
      "grad_norm": 1.3129366636276245,
      "learning_rate": 2.6638881698162053e-05,
      "loss": 1.2366,
      "step": 4430
    },
    {
      "epoch": 0.34392610236449195,
      "grad_norm": 1.5791515111923218,
      "learning_rate": 2.6631892311674866e-05,
      "loss": 1.2117,
      "step": 4440
    },
    {
      "epoch": 0.3447007107031507,
      "grad_norm": 1.758039116859436,
      "learning_rate": 2.66241263266891e-05,
      "loss": 1.2866,
      "step": 4450
    },
    {
      "epoch": 0.3454753190418095,
      "grad_norm": 1.1784509420394897,
      "learning_rate": 2.661636034170334e-05,
      "loss": 1.1356,
      "step": 4460
    },
    {
      "epoch": 0.3462499273804682,
      "grad_norm": 1.9273343086242676,
      "learning_rate": 2.6608594356717577e-05,
      "loss": 1.164,
      "step": 4470
    },
    {
      "epoch": 0.347024535719127,
      "grad_norm": 1.5631206035614014,
      "learning_rate": 2.6600828371731814e-05,
      "loss": 1.2532,
      "step": 4480
    },
    {
      "epoch": 0.3477991440577858,
      "grad_norm": 1.9410964250564575,
      "learning_rate": 2.6593062386746052e-05,
      "loss": 1.2378,
      "step": 4490
    },
    {
      "epoch": 0.34857375239644456,
      "grad_norm": 1.359539270401001,
      "learning_rate": 2.658529640176029e-05,
      "loss": 1.1199,
      "step": 4500
    },
    {
      "epoch": 0.34934836073510334,
      "grad_norm": 1.8660117387771606,
      "learning_rate": 2.6577530416774528e-05,
      "loss": 1.1785,
      "step": 4510
    },
    {
      "epoch": 0.35012296907376206,
      "grad_norm": 1.9632512331008911,
      "learning_rate": 2.6569764431788766e-05,
      "loss": 1.2395,
      "step": 4520
    },
    {
      "epoch": 0.35089757741242084,
      "grad_norm": 1.2123908996582031,
      "learning_rate": 2.6561998446803e-05,
      "loss": 1.2014,
      "step": 4530
    },
    {
      "epoch": 0.3516721857510796,
      "grad_norm": 1.508022665977478,
      "learning_rate": 2.655423246181724e-05,
      "loss": 1.1851,
      "step": 4540
    },
    {
      "epoch": 0.3524467940897384,
      "grad_norm": 1.8602063655853271,
      "learning_rate": 2.654646647683148e-05,
      "loss": 1.1737,
      "step": 4550
    },
    {
      "epoch": 0.35322140242839717,
      "grad_norm": 1.8624167442321777,
      "learning_rate": 2.6538700491845717e-05,
      "loss": 1.2096,
      "step": 4560
    },
    {
      "epoch": 0.3539960107670559,
      "grad_norm": 1.362227201461792,
      "learning_rate": 2.6530934506859955e-05,
      "loss": 1.2345,
      "step": 4570
    },
    {
      "epoch": 0.35477061910571467,
      "grad_norm": 1.469247579574585,
      "learning_rate": 2.6523168521874193e-05,
      "loss": 1.257,
      "step": 4580
    },
    {
      "epoch": 0.35554522744437345,
      "grad_norm": 1.321815848350525,
      "learning_rate": 2.651540253688843e-05,
      "loss": 1.1359,
      "step": 4590
    },
    {
      "epoch": 0.3563198357830322,
      "grad_norm": 1.438785433769226,
      "learning_rate": 2.650763655190267e-05,
      "loss": 1.1713,
      "step": 4600
    },
    {
      "epoch": 0.35709444412169095,
      "grad_norm": 1.8225975036621094,
      "learning_rate": 2.6499870566916903e-05,
      "loss": 1.3017,
      "step": 4610
    },
    {
      "epoch": 0.3578690524603497,
      "grad_norm": 1.4610843658447266,
      "learning_rate": 2.649210458193114e-05,
      "loss": 1.1216,
      "step": 4620
    },
    {
      "epoch": 0.3586436607990085,
      "grad_norm": 1.4518356323242188,
      "learning_rate": 2.648433859694538e-05,
      "loss": 1.217,
      "step": 4630
    },
    {
      "epoch": 0.3594182691376673,
      "grad_norm": 1.838087558746338,
      "learning_rate": 2.6476572611959617e-05,
      "loss": 1.2169,
      "step": 4640
    },
    {
      "epoch": 0.36019287747632606,
      "grad_norm": 1.3698259592056274,
      "learning_rate": 2.6468806626973855e-05,
      "loss": 1.1563,
      "step": 4650
    },
    {
      "epoch": 0.3609674858149848,
      "grad_norm": 1.451678991317749,
      "learning_rate": 2.6461040641988093e-05,
      "loss": 1.1871,
      "step": 4660
    },
    {
      "epoch": 0.36174209415364356,
      "grad_norm": 1.6041744947433472,
      "learning_rate": 2.645327465700233e-05,
      "loss": 1.1873,
      "step": 4670
    },
    {
      "epoch": 0.36251670249230233,
      "grad_norm": 1.3922423124313354,
      "learning_rate": 2.644550867201657e-05,
      "loss": 1.1892,
      "step": 4680
    },
    {
      "epoch": 0.3632913108309611,
      "grad_norm": 1.5258007049560547,
      "learning_rate": 2.6437742687030806e-05,
      "loss": 1.2371,
      "step": 4690
    },
    {
      "epoch": 0.36406591916961983,
      "grad_norm": 1.3401075601577759,
      "learning_rate": 2.642997670204504e-05,
      "loss": 1.1687,
      "step": 4700
    },
    {
      "epoch": 0.3648405275082786,
      "grad_norm": 1.4594990015029907,
      "learning_rate": 2.642221071705928e-05,
      "loss": 1.2219,
      "step": 4710
    },
    {
      "epoch": 0.3656151358469374,
      "grad_norm": 1.619369387626648,
      "learning_rate": 2.6414444732073516e-05,
      "loss": 1.1971,
      "step": 4720
    },
    {
      "epoch": 0.36638974418559617,
      "grad_norm": 1.5052928924560547,
      "learning_rate": 2.6406678747087754e-05,
      "loss": 1.2177,
      "step": 4730
    },
    {
      "epoch": 0.36716435252425494,
      "grad_norm": 1.3122919797897339,
      "learning_rate": 2.6398912762101996e-05,
      "loss": 1.2519,
      "step": 4740
    },
    {
      "epoch": 0.36793896086291367,
      "grad_norm": 1.7548315525054932,
      "learning_rate": 2.6391146777116233e-05,
      "loss": 1.2178,
      "step": 4750
    },
    {
      "epoch": 0.36871356920157244,
      "grad_norm": 1.3512393236160278,
      "learning_rate": 2.638338079213047e-05,
      "loss": 1.0952,
      "step": 4760
    },
    {
      "epoch": 0.3694881775402312,
      "grad_norm": 1.578896403312683,
      "learning_rate": 2.637561480714471e-05,
      "loss": 1.2849,
      "step": 4770
    },
    {
      "epoch": 0.37026278587889,
      "grad_norm": 1.6872621774673462,
      "learning_rate": 2.6367848822158944e-05,
      "loss": 1.2307,
      "step": 4780
    },
    {
      "epoch": 0.3710373942175488,
      "grad_norm": 1.7508366107940674,
      "learning_rate": 2.636008283717318e-05,
      "loss": 1.1841,
      "step": 4790
    },
    {
      "epoch": 0.3718120025562075,
      "grad_norm": 1.5152161121368408,
      "learning_rate": 2.635231685218742e-05,
      "loss": 1.1152,
      "step": 4800
    },
    {
      "epoch": 0.3725866108948663,
      "grad_norm": 1.7930526733398438,
      "learning_rate": 2.6344550867201657e-05,
      "loss": 1.087,
      "step": 4810
    },
    {
      "epoch": 0.37336121923352505,
      "grad_norm": 1.3029916286468506,
      "learning_rate": 2.6336784882215895e-05,
      "loss": 1.1685,
      "step": 4820
    },
    {
      "epoch": 0.37413582757218383,
      "grad_norm": 1.4423290491104126,
      "learning_rate": 2.6329018897230133e-05,
      "loss": 1.2822,
      "step": 4830
    },
    {
      "epoch": 0.37491043591084255,
      "grad_norm": 1.7593176364898682,
      "learning_rate": 2.632125291224437e-05,
      "loss": 1.1586,
      "step": 4840
    },
    {
      "epoch": 0.37568504424950133,
      "grad_norm": 1.645912528038025,
      "learning_rate": 2.631348692725861e-05,
      "loss": 1.1234,
      "step": 4850
    },
    {
      "epoch": 0.3764596525881601,
      "grad_norm": 1.5288959741592407,
      "learning_rate": 2.6305720942272847e-05,
      "loss": 1.1925,
      "step": 4860
    },
    {
      "epoch": 0.3772342609268189,
      "grad_norm": 2.080869197845459,
      "learning_rate": 2.629795495728708e-05,
      "loss": 1.379,
      "step": 4870
    },
    {
      "epoch": 0.37800886926547766,
      "grad_norm": 1.4372466802597046,
      "learning_rate": 2.629018897230132e-05,
      "loss": 1.2391,
      "step": 4880
    },
    {
      "epoch": 0.3787834776041364,
      "grad_norm": 1.2035248279571533,
      "learning_rate": 2.6282422987315557e-05,
      "loss": 1.2085,
      "step": 4890
    },
    {
      "epoch": 0.37955808594279516,
      "grad_norm": 1.7362477779388428,
      "learning_rate": 2.6274657002329795e-05,
      "loss": 1.2637,
      "step": 4900
    },
    {
      "epoch": 0.38033269428145394,
      "grad_norm": 1.1078966856002808,
      "learning_rate": 2.6266891017344033e-05,
      "loss": 1.1043,
      "step": 4910
    },
    {
      "epoch": 0.3811073026201127,
      "grad_norm": 1.7770121097564697,
      "learning_rate": 2.625912503235827e-05,
      "loss": 1.2345,
      "step": 4920
    },
    {
      "epoch": 0.3818819109587715,
      "grad_norm": 1.9191561937332153,
      "learning_rate": 2.625135904737251e-05,
      "loss": 1.2005,
      "step": 4930
    },
    {
      "epoch": 0.3826565192974302,
      "grad_norm": 1.6119866371154785,
      "learning_rate": 2.624359306238675e-05,
      "loss": 1.2315,
      "step": 4940
    },
    {
      "epoch": 0.383431127636089,
      "grad_norm": 1.5697009563446045,
      "learning_rate": 2.6235827077400984e-05,
      "loss": 1.1191,
      "step": 4950
    },
    {
      "epoch": 0.3842057359747478,
      "grad_norm": 1.2030342817306519,
      "learning_rate": 2.6228061092415222e-05,
      "loss": 1.2323,
      "step": 4960
    },
    {
      "epoch": 0.38498034431340655,
      "grad_norm": 1.3235690593719482,
      "learning_rate": 2.622029510742946e-05,
      "loss": 1.1059,
      "step": 4970
    },
    {
      "epoch": 0.3857549526520653,
      "grad_norm": 1.164844036102295,
      "learning_rate": 2.6212529122443698e-05,
      "loss": 1.2669,
      "step": 4980
    },
    {
      "epoch": 0.38652956099072405,
      "grad_norm": 1.8034507036209106,
      "learning_rate": 2.6204763137457935e-05,
      "loss": 1.2267,
      "step": 4990
    },
    {
      "epoch": 0.38730416932938283,
      "grad_norm": 2.0312399864196777,
      "learning_rate": 2.6196997152472173e-05,
      "loss": 1.1728,
      "step": 5000
    },
    {
      "epoch": 0.3880787776680416,
      "grad_norm": 1.1709941625595093,
      "learning_rate": 2.618923116748641e-05,
      "loss": 1.2049,
      "step": 5010
    },
    {
      "epoch": 0.3888533860067004,
      "grad_norm": 1.1434922218322754,
      "learning_rate": 2.618146518250065e-05,
      "loss": 1.1951,
      "step": 5020
    },
    {
      "epoch": 0.3896279943453591,
      "grad_norm": 1.5051931142807007,
      "learning_rate": 2.6173699197514887e-05,
      "loss": 1.3185,
      "step": 5030
    },
    {
      "epoch": 0.3904026026840179,
      "grad_norm": 1.249313473701477,
      "learning_rate": 2.616593321252912e-05,
      "loss": 1.1643,
      "step": 5040
    },
    {
      "epoch": 0.39117721102267666,
      "grad_norm": 1.7378408908843994,
      "learning_rate": 2.615816722754336e-05,
      "loss": 1.2533,
      "step": 5050
    },
    {
      "epoch": 0.39195181936133544,
      "grad_norm": 1.380317211151123,
      "learning_rate": 2.6150401242557597e-05,
      "loss": 1.2475,
      "step": 5060
    },
    {
      "epoch": 0.39272642769999416,
      "grad_norm": 1.413940191268921,
      "learning_rate": 2.6142635257571835e-05,
      "loss": 1.1988,
      "step": 5070
    },
    {
      "epoch": 0.39350103603865294,
      "grad_norm": 1.6940995454788208,
      "learning_rate": 2.6134869272586073e-05,
      "loss": 1.1572,
      "step": 5080
    },
    {
      "epoch": 0.3942756443773117,
      "grad_norm": 1.454809546470642,
      "learning_rate": 2.612710328760031e-05,
      "loss": 1.1721,
      "step": 5090
    },
    {
      "epoch": 0.3950502527159705,
      "grad_norm": 1.4963175058364868,
      "learning_rate": 2.611933730261455e-05,
      "loss": 1.1494,
      "step": 5100
    },
    {
      "epoch": 0.3958248610546293,
      "grad_norm": 1.62242591381073,
      "learning_rate": 2.6111571317628786e-05,
      "loss": 1.204,
      "step": 5110
    },
    {
      "epoch": 0.396599469393288,
      "grad_norm": 1.6217581033706665,
      "learning_rate": 2.610380533264302e-05,
      "loss": 1.1771,
      "step": 5120
    },
    {
      "epoch": 0.3973740777319468,
      "grad_norm": 1.4447025060653687,
      "learning_rate": 2.6096039347657262e-05,
      "loss": 1.2773,
      "step": 5130
    },
    {
      "epoch": 0.39814868607060555,
      "grad_norm": 2.0380399227142334,
      "learning_rate": 2.60882733626715e-05,
      "loss": 1.1369,
      "step": 5140
    },
    {
      "epoch": 0.39892329440926433,
      "grad_norm": 1.5846095085144043,
      "learning_rate": 2.6080507377685738e-05,
      "loss": 1.1996,
      "step": 5150
    },
    {
      "epoch": 0.3996979027479231,
      "grad_norm": 1.797593116760254,
      "learning_rate": 2.6072741392699976e-05,
      "loss": 1.2093,
      "step": 5160
    },
    {
      "epoch": 0.40047251108658183,
      "grad_norm": 1.662302017211914,
      "learning_rate": 2.6064975407714214e-05,
      "loss": 1.2538,
      "step": 5170
    },
    {
      "epoch": 0.4012471194252406,
      "grad_norm": 1.5607385635375977,
      "learning_rate": 2.605720942272845e-05,
      "loss": 1.1645,
      "step": 5180
    },
    {
      "epoch": 0.4020217277638994,
      "grad_norm": 1.6449497938156128,
      "learning_rate": 2.604944343774269e-05,
      "loss": 1.1356,
      "step": 5190
    },
    {
      "epoch": 0.40279633610255816,
      "grad_norm": 2.225774049758911,
      "learning_rate": 2.6041677452756927e-05,
      "loss": 1.1129,
      "step": 5200
    },
    {
      "epoch": 0.4035709444412169,
      "grad_norm": 1.3709543943405151,
      "learning_rate": 2.6033911467771162e-05,
      "loss": 1.1905,
      "step": 5210
    },
    {
      "epoch": 0.40434555277987566,
      "grad_norm": 1.5013216733932495,
      "learning_rate": 2.60261454827854e-05,
      "loss": 1.1079,
      "step": 5220
    },
    {
      "epoch": 0.40512016111853444,
      "grad_norm": 1.5398468971252441,
      "learning_rate": 2.6018379497799637e-05,
      "loss": 1.1681,
      "step": 5230
    },
    {
      "epoch": 0.4058947694571932,
      "grad_norm": 1.3545171022415161,
      "learning_rate": 2.6010613512813875e-05,
      "loss": 1.1885,
      "step": 5240
    },
    {
      "epoch": 0.406669377795852,
      "grad_norm": 1.3974101543426514,
      "learning_rate": 2.6002847527828113e-05,
      "loss": 1.15,
      "step": 5250
    },
    {
      "epoch": 0.4074439861345107,
      "grad_norm": 1.409600019454956,
      "learning_rate": 2.599508154284235e-05,
      "loss": 1.1482,
      "step": 5260
    },
    {
      "epoch": 0.4082185944731695,
      "grad_norm": 1.7729164361953735,
      "learning_rate": 2.598731555785659e-05,
      "loss": 1.1819,
      "step": 5270
    },
    {
      "epoch": 0.40899320281182827,
      "grad_norm": 1.4406211376190186,
      "learning_rate": 2.5979549572870827e-05,
      "loss": 1.2012,
      "step": 5280
    },
    {
      "epoch": 0.40976781115048705,
      "grad_norm": 1.426755666732788,
      "learning_rate": 2.597178358788506e-05,
      "loss": 1.2148,
      "step": 5290
    },
    {
      "epoch": 0.4105424194891458,
      "grad_norm": 2.0459659099578857,
      "learning_rate": 2.59640176028993e-05,
      "loss": 1.2075,
      "step": 5300
    },
    {
      "epoch": 0.41131702782780455,
      "grad_norm": 1.2639837265014648,
      "learning_rate": 2.5956251617913537e-05,
      "loss": 1.1679,
      "step": 5310
    },
    {
      "epoch": 0.4120916361664633,
      "grad_norm": 1.753723382949829,
      "learning_rate": 2.5948485632927778e-05,
      "loss": 1.2747,
      "step": 5320
    },
    {
      "epoch": 0.4128662445051221,
      "grad_norm": 1.539730191230774,
      "learning_rate": 2.5940719647942016e-05,
      "loss": 1.1236,
      "step": 5330
    },
    {
      "epoch": 0.4136408528437809,
      "grad_norm": 1.2286635637283325,
      "learning_rate": 2.5932953662956254e-05,
      "loss": 1.245,
      "step": 5340
    },
    {
      "epoch": 0.4144154611824396,
      "grad_norm": 1.2225642204284668,
      "learning_rate": 2.5925187677970492e-05,
      "loss": 1.2863,
      "step": 5350
    },
    {
      "epoch": 0.4151900695210984,
      "grad_norm": 1.7365225553512573,
      "learning_rate": 2.591742169298473e-05,
      "loss": 1.1974,
      "step": 5360
    },
    {
      "epoch": 0.41596467785975716,
      "grad_norm": 1.986498475074768,
      "learning_rate": 2.5909655707998968e-05,
      "loss": 1.1537,
      "step": 5370
    },
    {
      "epoch": 0.41673928619841594,
      "grad_norm": 1.3387945890426636,
      "learning_rate": 2.5901889723013202e-05,
      "loss": 1.2787,
      "step": 5380
    },
    {
      "epoch": 0.4175138945370747,
      "grad_norm": 1.3058569431304932,
      "learning_rate": 2.589412373802744e-05,
      "loss": 1.1778,
      "step": 5390
    },
    {
      "epoch": 0.41828850287573344,
      "grad_norm": 1.4617053270339966,
      "learning_rate": 2.5886357753041678e-05,
      "loss": 1.1727,
      "step": 5400
    },
    {
      "epoch": 0.4190631112143922,
      "grad_norm": 1.652139663696289,
      "learning_rate": 2.5878591768055916e-05,
      "loss": 1.1252,
      "step": 5410
    },
    {
      "epoch": 0.419837719553051,
      "grad_norm": 1.296617865562439,
      "learning_rate": 2.5870825783070154e-05,
      "loss": 1.1866,
      "step": 5420
    },
    {
      "epoch": 0.42061232789170977,
      "grad_norm": 1.7069889307022095,
      "learning_rate": 2.586305979808439e-05,
      "loss": 1.1661,
      "step": 5430
    },
    {
      "epoch": 0.42138693623036855,
      "grad_norm": 1.2832742929458618,
      "learning_rate": 2.585529381309863e-05,
      "loss": 1.2726,
      "step": 5440
    },
    {
      "epoch": 0.42216154456902727,
      "grad_norm": 1.2956491708755493,
      "learning_rate": 2.5847527828112867e-05,
      "loss": 1.1953,
      "step": 5450
    },
    {
      "epoch": 0.42293615290768605,
      "grad_norm": 1.4474347829818726,
      "learning_rate": 2.58397618431271e-05,
      "loss": 1.1333,
      "step": 5460
    },
    {
      "epoch": 0.4237107612463448,
      "grad_norm": 1.9288256168365479,
      "learning_rate": 2.583199585814134e-05,
      "loss": 1.2546,
      "step": 5470
    },
    {
      "epoch": 0.4244853695850036,
      "grad_norm": 1.618109941482544,
      "learning_rate": 2.5824229873155577e-05,
      "loss": 1.2225,
      "step": 5480
    },
    {
      "epoch": 0.4252599779236623,
      "grad_norm": 1.350289225578308,
      "learning_rate": 2.5816463888169815e-05,
      "loss": 1.2246,
      "step": 5490
    },
    {
      "epoch": 0.4260345862623211,
      "grad_norm": 1.8195970058441162,
      "learning_rate": 2.5808697903184053e-05,
      "loss": 1.2215,
      "step": 5500
    },
    {
      "epoch": 0.4268091946009799,
      "grad_norm": 1.5220723152160645,
      "learning_rate": 2.580093191819829e-05,
      "loss": 1.1865,
      "step": 5510
    },
    {
      "epoch": 0.42758380293963866,
      "grad_norm": 1.6735162734985352,
      "learning_rate": 2.5793165933212532e-05,
      "loss": 1.1276,
      "step": 5520
    },
    {
      "epoch": 0.42835841127829744,
      "grad_norm": 1.4988948106765747,
      "learning_rate": 2.578539994822677e-05,
      "loss": 1.2231,
      "step": 5530
    },
    {
      "epoch": 0.42913301961695616,
      "grad_norm": 1.3323124647140503,
      "learning_rate": 2.5777633963241008e-05,
      "loss": 1.1918,
      "step": 5540
    },
    {
      "epoch": 0.42990762795561493,
      "grad_norm": 1.4113956689834595,
      "learning_rate": 2.5769867978255242e-05,
      "loss": 1.1684,
      "step": 5550
    },
    {
      "epoch": 0.4306822362942737,
      "grad_norm": 1.5640738010406494,
      "learning_rate": 2.576210199326948e-05,
      "loss": 1.1403,
      "step": 5560
    },
    {
      "epoch": 0.4314568446329325,
      "grad_norm": 1.5197241306304932,
      "learning_rate": 2.5754336008283718e-05,
      "loss": 1.2056,
      "step": 5570
    },
    {
      "epoch": 0.4322314529715912,
      "grad_norm": 1.8076945543289185,
      "learning_rate": 2.5746570023297956e-05,
      "loss": 1.0844,
      "step": 5580
    },
    {
      "epoch": 0.43300606131025,
      "grad_norm": 2.050513982772827,
      "learning_rate": 2.5738804038312194e-05,
      "loss": 1.1765,
      "step": 5590
    },
    {
      "epoch": 0.43378066964890877,
      "grad_norm": 1.634657382965088,
      "learning_rate": 2.5731038053326432e-05,
      "loss": 1.2737,
      "step": 5600
    },
    {
      "epoch": 0.43455527798756755,
      "grad_norm": 1.2218793630599976,
      "learning_rate": 2.572327206834067e-05,
      "loss": 1.2222,
      "step": 5610
    },
    {
      "epoch": 0.4353298863262263,
      "grad_norm": 2.097200632095337,
      "learning_rate": 2.5715506083354907e-05,
      "loss": 1.3272,
      "step": 5620
    },
    {
      "epoch": 0.43610449466488505,
      "grad_norm": 1.3983285427093506,
      "learning_rate": 2.5707740098369142e-05,
      "loss": 1.2144,
      "step": 5630
    },
    {
      "epoch": 0.4368791030035438,
      "grad_norm": 1.0529509782791138,
      "learning_rate": 2.569997411338338e-05,
      "loss": 1.1942,
      "step": 5640
    },
    {
      "epoch": 0.4376537113422026,
      "grad_norm": 1.5591275691986084,
      "learning_rate": 2.5692208128397618e-05,
      "loss": 1.2122,
      "step": 5650
    },
    {
      "epoch": 0.4384283196808614,
      "grad_norm": 1.454269528388977,
      "learning_rate": 2.5684442143411856e-05,
      "loss": 1.26,
      "step": 5660
    },
    {
      "epoch": 0.43920292801952016,
      "grad_norm": 1.4006582498550415,
      "learning_rate": 2.5676676158426093e-05,
      "loss": 1.3092,
      "step": 5670
    },
    {
      "epoch": 0.4399775363581789,
      "grad_norm": 1.0992199182510376,
      "learning_rate": 2.566891017344033e-05,
      "loss": 1.1482,
      "step": 5680
    },
    {
      "epoch": 0.44075214469683766,
      "grad_norm": 1.6095110177993774,
      "learning_rate": 2.566114418845457e-05,
      "loss": 1.2654,
      "step": 5690
    },
    {
      "epoch": 0.44152675303549643,
      "grad_norm": 2.05204439163208,
      "learning_rate": 2.5653378203468807e-05,
      "loss": 1.1397,
      "step": 5700
    },
    {
      "epoch": 0.4423013613741552,
      "grad_norm": 1.5559593439102173,
      "learning_rate": 2.5645612218483045e-05,
      "loss": 1.2083,
      "step": 5710
    },
    {
      "epoch": 0.44307596971281393,
      "grad_norm": 1.6583245992660522,
      "learning_rate": 2.5637846233497283e-05,
      "loss": 1.2066,
      "step": 5720
    },
    {
      "epoch": 0.4438505780514727,
      "grad_norm": 2.0707409381866455,
      "learning_rate": 2.563008024851152e-05,
      "loss": 1.077,
      "step": 5730
    },
    {
      "epoch": 0.4446251863901315,
      "grad_norm": 1.3524513244628906,
      "learning_rate": 2.562231426352576e-05,
      "loss": 1.1104,
      "step": 5740
    },
    {
      "epoch": 0.44539979472879027,
      "grad_norm": 1.3004509210586548,
      "learning_rate": 2.5614548278539996e-05,
      "loss": 1.1557,
      "step": 5750
    },
    {
      "epoch": 0.44617440306744904,
      "grad_norm": 1.5411583185195923,
      "learning_rate": 2.5606782293554234e-05,
      "loss": 1.166,
      "step": 5760
    },
    {
      "epoch": 0.44694901140610777,
      "grad_norm": 1.8057771921157837,
      "learning_rate": 2.5599016308568472e-05,
      "loss": 1.1818,
      "step": 5770
    },
    {
      "epoch": 0.44772361974476654,
      "grad_norm": 1.5761321783065796,
      "learning_rate": 2.559125032358271e-05,
      "loss": 1.2328,
      "step": 5780
    },
    {
      "epoch": 0.4484982280834253,
      "grad_norm": 1.4107990264892578,
      "learning_rate": 2.5583484338596948e-05,
      "loss": 1.1526,
      "step": 5790
    },
    {
      "epoch": 0.4492728364220841,
      "grad_norm": 2.5925233364105225,
      "learning_rate": 2.5575718353611182e-05,
      "loss": 1.3011,
      "step": 5800
    },
    {
      "epoch": 0.4500474447607429,
      "grad_norm": 1.6243369579315186,
      "learning_rate": 2.556795236862542e-05,
      "loss": 1.1762,
      "step": 5810
    },
    {
      "epoch": 0.4508220530994016,
      "grad_norm": 1.6765868663787842,
      "learning_rate": 2.5560186383639658e-05,
      "loss": 1.0872,
      "step": 5820
    },
    {
      "epoch": 0.4515966614380604,
      "grad_norm": 1.6121207475662231,
      "learning_rate": 2.5552420398653896e-05,
      "loss": 1.2539,
      "step": 5830
    },
    {
      "epoch": 0.45237126977671915,
      "grad_norm": 1.2609599828720093,
      "learning_rate": 2.5544654413668134e-05,
      "loss": 1.2643,
      "step": 5840
    },
    {
      "epoch": 0.45314587811537793,
      "grad_norm": 1.4064232110977173,
      "learning_rate": 2.553688842868237e-05,
      "loss": 1.2526,
      "step": 5850
    },
    {
      "epoch": 0.45392048645403665,
      "grad_norm": 1.4558436870574951,
      "learning_rate": 2.552912244369661e-05,
      "loss": 1.2436,
      "step": 5860
    },
    {
      "epoch": 0.45469509479269543,
      "grad_norm": 1.4250837564468384,
      "learning_rate": 2.5521356458710847e-05,
      "loss": 1.3094,
      "step": 5870
    },
    {
      "epoch": 0.4554697031313542,
      "grad_norm": 1.7840100526809692,
      "learning_rate": 2.5513590473725085e-05,
      "loss": 1.1796,
      "step": 5880
    },
    {
      "epoch": 0.456244311470013,
      "grad_norm": 1.2385939359664917,
      "learning_rate": 2.550582448873932e-05,
      "loss": 1.3092,
      "step": 5890
    },
    {
      "epoch": 0.45701891980867176,
      "grad_norm": 1.6369708776474,
      "learning_rate": 2.5498058503753558e-05,
      "loss": 1.2584,
      "step": 5900
    },
    {
      "epoch": 0.4577935281473305,
      "grad_norm": 2.0164506435394287,
      "learning_rate": 2.54902925187678e-05,
      "loss": 1.2609,
      "step": 5910
    },
    {
      "epoch": 0.45856813648598926,
      "grad_norm": 2.1117231845855713,
      "learning_rate": 2.5482526533782037e-05,
      "loss": 1.2199,
      "step": 5920
    },
    {
      "epoch": 0.45934274482464804,
      "grad_norm": 1.2973976135253906,
      "learning_rate": 2.5474760548796275e-05,
      "loss": 1.1604,
      "step": 5930
    },
    {
      "epoch": 0.4601173531633068,
      "grad_norm": 1.0976786613464355,
      "learning_rate": 2.5466994563810512e-05,
      "loss": 1.2203,
      "step": 5940
    },
    {
      "epoch": 0.46089196150196554,
      "grad_norm": 1.6099573373794556,
      "learning_rate": 2.545922857882475e-05,
      "loss": 1.2646,
      "step": 5950
    },
    {
      "epoch": 0.4616665698406243,
      "grad_norm": 1.2786741256713867,
      "learning_rate": 2.5451462593838988e-05,
      "loss": 1.1672,
      "step": 5960
    },
    {
      "epoch": 0.4624411781792831,
      "grad_norm": 2.497708559036255,
      "learning_rate": 2.5443696608853223e-05,
      "loss": 1.1831,
      "step": 5970
    },
    {
      "epoch": 0.4632157865179419,
      "grad_norm": 1.3114666938781738,
      "learning_rate": 2.543593062386746e-05,
      "loss": 1.2223,
      "step": 5980
    },
    {
      "epoch": 0.46399039485660065,
      "grad_norm": 1.9113681316375732,
      "learning_rate": 2.54281646388817e-05,
      "loss": 1.2264,
      "step": 5990
    },
    {
      "epoch": 0.4647650031952594,
      "grad_norm": 2.589662551879883,
      "learning_rate": 2.5420398653895936e-05,
      "loss": 1.1659,
      "step": 6000
    },
    {
      "epoch": 0.46553961153391815,
      "grad_norm": 1.364531397819519,
      "learning_rate": 2.5412632668910174e-05,
      "loss": 1.2554,
      "step": 6010
    },
    {
      "epoch": 0.46631421987257693,
      "grad_norm": 1.371913194656372,
      "learning_rate": 2.5404866683924412e-05,
      "loss": 1.2245,
      "step": 6020
    },
    {
      "epoch": 0.4670888282112357,
      "grad_norm": 1.561071753501892,
      "learning_rate": 2.539710069893865e-05,
      "loss": 1.1337,
      "step": 6030
    },
    {
      "epoch": 0.4678634365498945,
      "grad_norm": 1.4048951864242554,
      "learning_rate": 2.5389334713952888e-05,
      "loss": 1.1861,
      "step": 6040
    },
    {
      "epoch": 0.4686380448885532,
      "grad_norm": 1.416434407234192,
      "learning_rate": 2.5381568728967126e-05,
      "loss": 1.1751,
      "step": 6050
    },
    {
      "epoch": 0.469412653227212,
      "grad_norm": 1.724895715713501,
      "learning_rate": 2.537380274398136e-05,
      "loss": 1.1501,
      "step": 6060
    },
    {
      "epoch": 0.47018726156587076,
      "grad_norm": 1.537132740020752,
      "learning_rate": 2.5366036758995598e-05,
      "loss": 1.3031,
      "step": 6070
    },
    {
      "epoch": 0.47096186990452954,
      "grad_norm": 1.1676032543182373,
      "learning_rate": 2.5358270774009836e-05,
      "loss": 1.1687,
      "step": 6080
    },
    {
      "epoch": 0.47173647824318826,
      "grad_norm": 1.7676538228988647,
      "learning_rate": 2.5350504789024074e-05,
      "loss": 1.2222,
      "step": 6090
    },
    {
      "epoch": 0.47251108658184704,
      "grad_norm": 1.3942192792892456,
      "learning_rate": 2.534273880403831e-05,
      "loss": 1.2267,
      "step": 6100
    },
    {
      "epoch": 0.4732856949205058,
      "grad_norm": 1.540042757987976,
      "learning_rate": 2.5334972819052553e-05,
      "loss": 1.2394,
      "step": 6110
    },
    {
      "epoch": 0.4740603032591646,
      "grad_norm": 1.3673311471939087,
      "learning_rate": 2.532720683406679e-05,
      "loss": 1.2019,
      "step": 6120
    },
    {
      "epoch": 0.4748349115978234,
      "grad_norm": 1.9442698955535889,
      "learning_rate": 2.531944084908103e-05,
      "loss": 1.2516,
      "step": 6130
    },
    {
      "epoch": 0.4756095199364821,
      "grad_norm": 1.4008874893188477,
      "learning_rate": 2.5311674864095263e-05,
      "loss": 1.2474,
      "step": 6140
    },
    {
      "epoch": 0.4763841282751409,
      "grad_norm": 1.6225827932357788,
      "learning_rate": 2.53039088791095e-05,
      "loss": 1.1948,
      "step": 6150
    },
    {
      "epoch": 0.47715873661379965,
      "grad_norm": 2.047559976577759,
      "learning_rate": 2.529614289412374e-05,
      "loss": 1.2696,
      "step": 6160
    },
    {
      "epoch": 0.4779333449524584,
      "grad_norm": 1.7015241384506226,
      "learning_rate": 2.5288376909137977e-05,
      "loss": 1.2661,
      "step": 6170
    },
    {
      "epoch": 0.4787079532911172,
      "grad_norm": 1.460932970046997,
      "learning_rate": 2.5280610924152214e-05,
      "loss": 1.1911,
      "step": 6180
    },
    {
      "epoch": 0.4794825616297759,
      "grad_norm": 1.411473035812378,
      "learning_rate": 2.5272844939166452e-05,
      "loss": 1.2283,
      "step": 6190
    },
    {
      "epoch": 0.4802571699684347,
      "grad_norm": 1.1047945022583008,
      "learning_rate": 2.526507895418069e-05,
      "loss": 1.1775,
      "step": 6200
    },
    {
      "epoch": 0.4810317783070935,
      "grad_norm": 1.3672548532485962,
      "learning_rate": 2.5257312969194928e-05,
      "loss": 1.1621,
      "step": 6210
    },
    {
      "epoch": 0.48180638664575226,
      "grad_norm": 1.6363718509674072,
      "learning_rate": 2.5249546984209163e-05,
      "loss": 1.2559,
      "step": 6220
    },
    {
      "epoch": 0.482580994984411,
      "grad_norm": 1.5362979173660278,
      "learning_rate": 2.52417809992234e-05,
      "loss": 1.2857,
      "step": 6230
    },
    {
      "epoch": 0.48335560332306976,
      "grad_norm": 1.4587520360946655,
      "learning_rate": 2.5234015014237638e-05,
      "loss": 1.1397,
      "step": 6240
    },
    {
      "epoch": 0.48413021166172854,
      "grad_norm": 1.4322558641433716,
      "learning_rate": 2.5226249029251876e-05,
      "loss": 1.1994,
      "step": 6250
    },
    {
      "epoch": 0.4849048200003873,
      "grad_norm": 1.9822559356689453,
      "learning_rate": 2.5218483044266114e-05,
      "loss": 1.197,
      "step": 6260
    },
    {
      "epoch": 0.4856794283390461,
      "grad_norm": 1.739428997039795,
      "learning_rate": 2.5210717059280352e-05,
      "loss": 1.2533,
      "step": 6270
    },
    {
      "epoch": 0.4864540366777048,
      "grad_norm": 1.3784499168395996,
      "learning_rate": 2.520295107429459e-05,
      "loss": 1.2454,
      "step": 6280
    },
    {
      "epoch": 0.4872286450163636,
      "grad_norm": 1.1711963415145874,
      "learning_rate": 2.5195185089308828e-05,
      "loss": 1.2071,
      "step": 6290
    },
    {
      "epoch": 0.48800325335502237,
      "grad_norm": 1.4546012878417969,
      "learning_rate": 2.518741910432307e-05,
      "loss": 1.2397,
      "step": 6300
    },
    {
      "epoch": 0.48877786169368115,
      "grad_norm": 1.4699418544769287,
      "learning_rate": 2.5179653119337303e-05,
      "loss": 1.2266,
      "step": 6310
    },
    {
      "epoch": 0.4895524700323399,
      "grad_norm": 1.3557648658752441,
      "learning_rate": 2.517188713435154e-05,
      "loss": 1.1955,
      "step": 6320
    },
    {
      "epoch": 0.49032707837099865,
      "grad_norm": 1.2845282554626465,
      "learning_rate": 2.516412114936578e-05,
      "loss": 1.1128,
      "step": 6330
    },
    {
      "epoch": 0.4911016867096574,
      "grad_norm": 1.7059810161590576,
      "learning_rate": 2.5156355164380017e-05,
      "loss": 1.2965,
      "step": 6340
    },
    {
      "epoch": 0.4918762950483162,
      "grad_norm": 1.906538724899292,
      "learning_rate": 2.5148589179394255e-05,
      "loss": 1.2198,
      "step": 6350
    },
    {
      "epoch": 0.492650903386975,
      "grad_norm": 1.3439085483551025,
      "learning_rate": 2.5140823194408493e-05,
      "loss": 1.1837,
      "step": 6360
    },
    {
      "epoch": 0.4934255117256337,
      "grad_norm": 1.5358649492263794,
      "learning_rate": 2.513305720942273e-05,
      "loss": 1.2217,
      "step": 6370
    },
    {
      "epoch": 0.4942001200642925,
      "grad_norm": 1.9124568700790405,
      "learning_rate": 2.512529122443697e-05,
      "loss": 1.2091,
      "step": 6380
    },
    {
      "epoch": 0.49497472840295126,
      "grad_norm": 1.6064927577972412,
      "learning_rate": 2.5117525239451203e-05,
      "loss": 1.2184,
      "step": 6390
    },
    {
      "epoch": 0.49574933674161004,
      "grad_norm": 1.622723937034607,
      "learning_rate": 2.510975925446544e-05,
      "loss": 1.1589,
      "step": 6400
    },
    {
      "epoch": 0.4965239450802688,
      "grad_norm": 1.3661788702011108,
      "learning_rate": 2.510199326947968e-05,
      "loss": 1.1841,
      "step": 6410
    },
    {
      "epoch": 0.49729855341892754,
      "grad_norm": 1.680344581604004,
      "learning_rate": 2.5094227284493916e-05,
      "loss": 1.201,
      "step": 6420
    },
    {
      "epoch": 0.4980731617575863,
      "grad_norm": 1.8959702253341675,
      "learning_rate": 2.5086461299508154e-05,
      "loss": 1.1516,
      "step": 6430
    },
    {
      "epoch": 0.4988477700962451,
      "grad_norm": 1.53858482837677,
      "learning_rate": 2.5078695314522392e-05,
      "loss": 1.2697,
      "step": 6440
    },
    {
      "epoch": 0.49962237843490387,
      "grad_norm": 1.4318288564682007,
      "learning_rate": 2.507092932953663e-05,
      "loss": 1.1199,
      "step": 6450
    },
    {
      "epoch": 0.5003969867735626,
      "grad_norm": 1.7203670740127563,
      "learning_rate": 2.5063163344550868e-05,
      "loss": 1.2594,
      "step": 6460
    },
    {
      "epoch": 0.5011715951122214,
      "grad_norm": 1.6934658288955688,
      "learning_rate": 2.5055397359565106e-05,
      "loss": 1.1424,
      "step": 6470
    },
    {
      "epoch": 0.5019462034508801,
      "grad_norm": 1.1050492525100708,
      "learning_rate": 2.504763137457934e-05,
      "loss": 1.2314,
      "step": 6480
    },
    {
      "epoch": 0.5027208117895389,
      "grad_norm": 1.592876672744751,
      "learning_rate": 2.5039865389593578e-05,
      "loss": 1.3307,
      "step": 6490
    },
    {
      "epoch": 0.5034954201281977,
      "grad_norm": 1.6814683675765991,
      "learning_rate": 2.503209940460782e-05,
      "loss": 1.208,
      "step": 6500
    },
    {
      "epoch": 0.5042700284668564,
      "grad_norm": 1.4269663095474243,
      "learning_rate": 2.5024333419622057e-05,
      "loss": 1.2215,
      "step": 6510
    },
    {
      "epoch": 0.5050446368055153,
      "grad_norm": 1.6147027015686035,
      "learning_rate": 2.5016567434636295e-05,
      "loss": 1.2252,
      "step": 6520
    },
    {
      "epoch": 0.505819245144174,
      "grad_norm": 1.6842938661575317,
      "learning_rate": 2.5008801449650533e-05,
      "loss": 1.2749,
      "step": 6530
    },
    {
      "epoch": 0.5065938534828327,
      "grad_norm": 1.4008032083511353,
      "learning_rate": 2.500103546466477e-05,
      "loss": 1.1694,
      "step": 6540
    },
    {
      "epoch": 0.5073684618214915,
      "grad_norm": 1.6563302278518677,
      "learning_rate": 2.499326947967901e-05,
      "loss": 1.2529,
      "step": 6550
    },
    {
      "epoch": 0.5081430701601503,
      "grad_norm": 1.8976620435714722,
      "learning_rate": 2.4985503494693243e-05,
      "loss": 1.1922,
      "step": 6560
    },
    {
      "epoch": 0.5089176784988091,
      "grad_norm": 1.7210228443145752,
      "learning_rate": 2.497773750970748e-05,
      "loss": 1.256,
      "step": 6570
    },
    {
      "epoch": 0.5096922868374678,
      "grad_norm": 1.434531331062317,
      "learning_rate": 2.496997152472172e-05,
      "loss": 1.1444,
      "step": 6580
    },
    {
      "epoch": 0.5104668951761265,
      "grad_norm": 1.2506755590438843,
      "learning_rate": 2.4962205539735957e-05,
      "loss": 1.1679,
      "step": 6590
    },
    {
      "epoch": 0.5112415035147854,
      "grad_norm": 1.4173580408096313,
      "learning_rate": 2.4954439554750195e-05,
      "loss": 1.1279,
      "step": 6600
    },
    {
      "epoch": 0.5120161118534441,
      "grad_norm": 1.4437860250473022,
      "learning_rate": 2.4946673569764433e-05,
      "loss": 1.2504,
      "step": 6610
    },
    {
      "epoch": 0.5127907201921029,
      "grad_norm": 2.4004175662994385,
      "learning_rate": 2.493890758477867e-05,
      "loss": 1.2582,
      "step": 6620
    },
    {
      "epoch": 0.5135653285307616,
      "grad_norm": 2.10703444480896,
      "learning_rate": 2.4931141599792908e-05,
      "loss": 1.1585,
      "step": 6630
    },
    {
      "epoch": 0.5143399368694204,
      "grad_norm": 1.3984519243240356,
      "learning_rate": 2.4923375614807146e-05,
      "loss": 1.2105,
      "step": 6640
    },
    {
      "epoch": 0.5151145452080792,
      "grad_norm": 1.4006162881851196,
      "learning_rate": 2.491560962982138e-05,
      "loss": 1.1971,
      "step": 6650
    },
    {
      "epoch": 0.5158891535467379,
      "grad_norm": 1.248311161994934,
      "learning_rate": 2.490784364483562e-05,
      "loss": 1.2283,
      "step": 6660
    },
    {
      "epoch": 0.5166637618853966,
      "grad_norm": 1.3346738815307617,
      "learning_rate": 2.4900077659849856e-05,
      "loss": 1.1803,
      "step": 6670
    },
    {
      "epoch": 0.5174383702240555,
      "grad_norm": 1.27830970287323,
      "learning_rate": 2.4892311674864094e-05,
      "loss": 1.1102,
      "step": 6680
    },
    {
      "epoch": 0.5182129785627142,
      "grad_norm": 1.853623628616333,
      "learning_rate": 2.4884545689878332e-05,
      "loss": 1.2375,
      "step": 6690
    },
    {
      "epoch": 0.518987586901373,
      "grad_norm": NaN,
      "learning_rate": 2.4876779704892573e-05,
      "loss": 1.2099,
      "step": 6700
    },
    {
      "epoch": 0.5197621952400318,
      "grad_norm": 1.5339887142181396,
      "learning_rate": 2.4869790318405387e-05,
      "loss": 1.1991,
      "step": 6710
    },
    {
      "epoch": 0.5205368035786905,
      "grad_norm": 1.9181392192840576,
      "learning_rate": 2.4862024333419625e-05,
      "loss": 1.1665,
      "step": 6720
    },
    {
      "epoch": 0.5213114119173493,
      "grad_norm": 1.0775216817855835,
      "learning_rate": 2.485425834843386e-05,
      "loss": 1.1861,
      "step": 6730
    },
    {
      "epoch": 0.522086020256008,
      "grad_norm": 2.3205676078796387,
      "learning_rate": 2.4846492363448097e-05,
      "loss": 1.1084,
      "step": 6740
    },
    {
      "epoch": 0.5228606285946669,
      "grad_norm": 1.3543509244918823,
      "learning_rate": 2.4838726378462335e-05,
      "loss": 1.1545,
      "step": 6750
    },
    {
      "epoch": 0.5236352369333256,
      "grad_norm": 1.3881226778030396,
      "learning_rate": 2.4830960393476573e-05,
      "loss": 1.1509,
      "step": 6760
    },
    {
      "epoch": 0.5244098452719843,
      "grad_norm": 1.3989059925079346,
      "learning_rate": 2.482319440849081e-05,
      "loss": 1.3522,
      "step": 6770
    },
    {
      "epoch": 0.5251844536106431,
      "grad_norm": 2.6037466526031494,
      "learning_rate": 2.481542842350505e-05,
      "loss": 1.1573,
      "step": 6780
    },
    {
      "epoch": 0.5259590619493019,
      "grad_norm": 1.5888172388076782,
      "learning_rate": 2.4807662438519286e-05,
      "loss": 1.1333,
      "step": 6790
    },
    {
      "epoch": 0.5267336702879607,
      "grad_norm": 1.4781715869903564,
      "learning_rate": 2.4799896453533524e-05,
      "loss": 1.2002,
      "step": 6800
    },
    {
      "epoch": 0.5275082786266194,
      "grad_norm": 1.291920781135559,
      "learning_rate": 2.479213046854776e-05,
      "loss": 1.2207,
      "step": 6810
    },
    {
      "epoch": 0.5282828869652781,
      "grad_norm": 1.2763875722885132,
      "learning_rate": 2.4784364483561996e-05,
      "loss": 1.2123,
      "step": 6820
    },
    {
      "epoch": 0.529057495303937,
      "grad_norm": 1.124023199081421,
      "learning_rate": 2.4776598498576238e-05,
      "loss": 1.2138,
      "step": 6830
    },
    {
      "epoch": 0.5298321036425957,
      "grad_norm": 1.6321178674697876,
      "learning_rate": 2.4768832513590476e-05,
      "loss": 1.2036,
      "step": 6840
    },
    {
      "epoch": 0.5306067119812545,
      "grad_norm": 1.3626389503479004,
      "learning_rate": 2.4761066528604713e-05,
      "loss": 1.3449,
      "step": 6850
    },
    {
      "epoch": 0.5313813203199133,
      "grad_norm": 1.5132092237472534,
      "learning_rate": 2.475330054361895e-05,
      "loss": 1.1486,
      "step": 6860
    },
    {
      "epoch": 0.532155928658572,
      "grad_norm": 1.3871440887451172,
      "learning_rate": 2.474553455863319e-05,
      "loss": 1.1474,
      "step": 6870
    },
    {
      "epoch": 0.5329305369972308,
      "grad_norm": 1.534398078918457,
      "learning_rate": 2.4737768573647427e-05,
      "loss": 1.2554,
      "step": 6880
    },
    {
      "epoch": 0.5337051453358895,
      "grad_norm": 1.2952356338500977,
      "learning_rate": 2.473000258866166e-05,
      "loss": 1.1521,
      "step": 6890
    },
    {
      "epoch": 0.5344797536745483,
      "grad_norm": 1.7360180616378784,
      "learning_rate": 2.47222366036759e-05,
      "loss": 1.1757,
      "step": 6900
    },
    {
      "epoch": 0.5352543620132071,
      "grad_norm": 1.777482032775879,
      "learning_rate": 2.4714470618690137e-05,
      "loss": 1.2028,
      "step": 6910
    },
    {
      "epoch": 0.5360289703518658,
      "grad_norm": 1.4445136785507202,
      "learning_rate": 2.4706704633704375e-05,
      "loss": 1.1649,
      "step": 6920
    },
    {
      "epoch": 0.5368035786905246,
      "grad_norm": 1.7226006984710693,
      "learning_rate": 2.4698938648718613e-05,
      "loss": 1.2232,
      "step": 6930
    },
    {
      "epoch": 0.5375781870291834,
      "grad_norm": 1.3355368375778198,
      "learning_rate": 2.469117266373285e-05,
      "loss": 1.1856,
      "step": 6940
    },
    {
      "epoch": 0.5383527953678421,
      "grad_norm": 1.424484372138977,
      "learning_rate": 2.468340667874709e-05,
      "loss": 1.1763,
      "step": 6950
    },
    {
      "epoch": 0.5391274037065009,
      "grad_norm": 1.1682668924331665,
      "learning_rate": 2.4675640693761327e-05,
      "loss": 1.1839,
      "step": 6960
    },
    {
      "epoch": 0.5399020120451596,
      "grad_norm": 1.491316318511963,
      "learning_rate": 2.4667874708775564e-05,
      "loss": 1.2,
      "step": 6970
    },
    {
      "epoch": 0.5406766203838185,
      "grad_norm": 1.286333680152893,
      "learning_rate": 2.46601087237898e-05,
      "loss": 1.2775,
      "step": 6980
    },
    {
      "epoch": 0.5414512287224772,
      "grad_norm": 1.4224845170974731,
      "learning_rate": 2.4652342738804037e-05,
      "loss": 1.1685,
      "step": 6990
    },
    {
      "epoch": 0.5422258370611359,
      "grad_norm": 1.7569667100906372,
      "learning_rate": 2.4644576753818275e-05,
      "loss": 1.1612,
      "step": 7000
    },
    {
      "epoch": 0.5430004453997948,
      "grad_norm": 1.9445737600326538,
      "learning_rate": 2.4636810768832513e-05,
      "loss": 1.2131,
      "step": 7010
    },
    {
      "epoch": 0.5437750537384535,
      "grad_norm": 1.9150965213775635,
      "learning_rate": 2.462904478384675e-05,
      "loss": 1.1612,
      "step": 7020
    },
    {
      "epoch": 0.5445496620771123,
      "grad_norm": 0.9876465797424316,
      "learning_rate": 2.462127879886099e-05,
      "loss": 1.1916,
      "step": 7030
    },
    {
      "epoch": 0.545324270415771,
      "grad_norm": 1.0935906171798706,
      "learning_rate": 2.461351281387523e-05,
      "loss": 1.2032,
      "step": 7040
    },
    {
      "epoch": 0.5460988787544298,
      "grad_norm": 1.2709101438522339,
      "learning_rate": 2.4605746828889467e-05,
      "loss": 1.1837,
      "step": 7050
    },
    {
      "epoch": 0.5468734870930886,
      "grad_norm": 1.6897563934326172,
      "learning_rate": 2.4597980843903702e-05,
      "loss": 1.2378,
      "step": 7060
    },
    {
      "epoch": 0.5476480954317473,
      "grad_norm": 1.8181794881820679,
      "learning_rate": 2.459021485891794e-05,
      "loss": 1.2147,
      "step": 7070
    },
    {
      "epoch": 0.5484227037704061,
      "grad_norm": 1.6311250925064087,
      "learning_rate": 2.4582448873932178e-05,
      "loss": 1.1934,
      "step": 7080
    },
    {
      "epoch": 0.5491973121090649,
      "grad_norm": 2.214918851852417,
      "learning_rate": 2.4574682888946415e-05,
      "loss": 1.1551,
      "step": 7090
    },
    {
      "epoch": 0.5499719204477236,
      "grad_norm": 1.9589216709136963,
      "learning_rate": 2.4566916903960653e-05,
      "loss": 1.1443,
      "step": 7100
    },
    {
      "epoch": 0.5507465287863824,
      "grad_norm": 1.9728679656982422,
      "learning_rate": 2.455915091897489e-05,
      "loss": 1.1663,
      "step": 7110
    },
    {
      "epoch": 0.5515211371250411,
      "grad_norm": 1.100936770439148,
      "learning_rate": 2.455138493398913e-05,
      "loss": 1.1794,
      "step": 7120
    },
    {
      "epoch": 0.5522957454637,
      "grad_norm": 1.4295005798339844,
      "learning_rate": 2.4543618949003367e-05,
      "loss": 1.1562,
      "step": 7130
    },
    {
      "epoch": 0.5530703538023587,
      "grad_norm": 1.4792673587799072,
      "learning_rate": 2.4535852964017605e-05,
      "loss": 1.2458,
      "step": 7140
    },
    {
      "epoch": 0.5538449621410174,
      "grad_norm": 1.8661965131759644,
      "learning_rate": 2.452808697903184e-05,
      "loss": 1.2606,
      "step": 7150
    },
    {
      "epoch": 0.5546195704796762,
      "grad_norm": 1.385290503501892,
      "learning_rate": 2.4520320994046077e-05,
      "loss": 1.1961,
      "step": 7160
    },
    {
      "epoch": 0.555394178818335,
      "grad_norm": 1.522316336631775,
      "learning_rate": 2.4512555009060315e-05,
      "loss": 1.2052,
      "step": 7170
    },
    {
      "epoch": 0.5561687871569937,
      "grad_norm": 1.713446021080017,
      "learning_rate": 2.4504789024074553e-05,
      "loss": 1.2065,
      "step": 7180
    },
    {
      "epoch": 0.5569433954956525,
      "grad_norm": 1.2743631601333618,
      "learning_rate": 2.449702303908879e-05,
      "loss": 1.2337,
      "step": 7190
    },
    {
      "epoch": 0.5577180038343112,
      "grad_norm": 1.3218860626220703,
      "learning_rate": 2.448925705410303e-05,
      "loss": 1.1195,
      "step": 7200
    },
    {
      "epoch": 0.5584926121729701,
      "grad_norm": 1.4049073457717896,
      "learning_rate": 2.4481491069117266e-05,
      "loss": 1.1643,
      "step": 7210
    },
    {
      "epoch": 0.5592672205116288,
      "grad_norm": 1.7970075607299805,
      "learning_rate": 2.4473725084131504e-05,
      "loss": 1.2301,
      "step": 7220
    },
    {
      "epoch": 0.5600418288502875,
      "grad_norm": 1.3457664251327515,
      "learning_rate": 2.4465959099145742e-05,
      "loss": 1.2396,
      "step": 7230
    },
    {
      "epoch": 0.5608164371889464,
      "grad_norm": 1.5833630561828613,
      "learning_rate": 2.445819311415998e-05,
      "loss": 1.3205,
      "step": 7240
    },
    {
      "epoch": 0.5615910455276051,
      "grad_norm": 1.5006847381591797,
      "learning_rate": 2.4450427129174218e-05,
      "loss": 1.1551,
      "step": 7250
    },
    {
      "epoch": 0.5623656538662639,
      "grad_norm": 1.23092782497406,
      "learning_rate": 2.4442661144188456e-05,
      "loss": 1.1501,
      "step": 7260
    },
    {
      "epoch": 0.5631402622049226,
      "grad_norm": 1.3314166069030762,
      "learning_rate": 2.4434895159202694e-05,
      "loss": 1.1252,
      "step": 7270
    },
    {
      "epoch": 0.5639148705435814,
      "grad_norm": 1.9151912927627563,
      "learning_rate": 2.442712917421693e-05,
      "loss": 1.2115,
      "step": 7280
    },
    {
      "epoch": 0.5646894788822402,
      "grad_norm": 1.2697030305862427,
      "learning_rate": 2.441936318923117e-05,
      "loss": 1.2037,
      "step": 7290
    },
    {
      "epoch": 0.5654640872208989,
      "grad_norm": 2.1232986450195312,
      "learning_rate": 2.4411597204245407e-05,
      "loss": 1.1934,
      "step": 7300
    },
    {
      "epoch": 0.5662386955595577,
      "grad_norm": 1.3722684383392334,
      "learning_rate": 2.4403831219259645e-05,
      "loss": 1.1875,
      "step": 7310
    },
    {
      "epoch": 0.5670133038982165,
      "grad_norm": 1.5648773908615112,
      "learning_rate": 2.439606523427388e-05,
      "loss": 1.2004,
      "step": 7320
    },
    {
      "epoch": 0.5677879122368752,
      "grad_norm": 1.630725622177124,
      "learning_rate": 2.4388299249288117e-05,
      "loss": 1.214,
      "step": 7330
    },
    {
      "epoch": 0.568562520575534,
      "grad_norm": 1.6046948432922363,
      "learning_rate": 2.4380533264302355e-05,
      "loss": 1.2574,
      "step": 7340
    },
    {
      "epoch": 0.5693371289141927,
      "grad_norm": 1.7101925611495972,
      "learning_rate": 2.4372767279316593e-05,
      "loss": 1.133,
      "step": 7350
    },
    {
      "epoch": 0.5701117372528516,
      "grad_norm": 1.267094373703003,
      "learning_rate": 2.436500129433083e-05,
      "loss": 1.1507,
      "step": 7360
    },
    {
      "epoch": 0.5708863455915103,
      "grad_norm": 1.667472004890442,
      "learning_rate": 2.435723530934507e-05,
      "loss": 1.1988,
      "step": 7370
    },
    {
      "epoch": 0.571660953930169,
      "grad_norm": 1.2216871976852417,
      "learning_rate": 2.4349469324359307e-05,
      "loss": 1.2315,
      "step": 7380
    },
    {
      "epoch": 0.5724355622688279,
      "grad_norm": 1.702462077140808,
      "learning_rate": 2.4341703339373545e-05,
      "loss": 1.1818,
      "step": 7390
    },
    {
      "epoch": 0.5732101706074866,
      "grad_norm": 1.3344303369522095,
      "learning_rate": 2.433393735438778e-05,
      "loss": 1.2189,
      "step": 7400
    },
    {
      "epoch": 0.5739847789461453,
      "grad_norm": 1.3774421215057373,
      "learning_rate": 2.4326171369402017e-05,
      "loss": 1.2294,
      "step": 7410
    },
    {
      "epoch": 0.5747593872848041,
      "grad_norm": 1.4137848615646362,
      "learning_rate": 2.4318405384416258e-05,
      "loss": 1.2099,
      "step": 7420
    },
    {
      "epoch": 0.5755339956234629,
      "grad_norm": 1.5138119459152222,
      "learning_rate": 2.4310639399430496e-05,
      "loss": 1.211,
      "step": 7430
    },
    {
      "epoch": 0.5763086039621217,
      "grad_norm": 1.870269536972046,
      "learning_rate": 2.4302873414444734e-05,
      "loss": 1.1497,
      "step": 7440
    },
    {
      "epoch": 0.5770832123007804,
      "grad_norm": 1.8459651470184326,
      "learning_rate": 2.4295107429458972e-05,
      "loss": 1.2071,
      "step": 7450
    },
    {
      "epoch": 0.5778578206394391,
      "grad_norm": 1.4137201309204102,
      "learning_rate": 2.428734144447321e-05,
      "loss": 1.2556,
      "step": 7460
    },
    {
      "epoch": 0.578632428978098,
      "grad_norm": 1.5368400812149048,
      "learning_rate": 2.4279575459487448e-05,
      "loss": 1.224,
      "step": 7470
    },
    {
      "epoch": 0.5794070373167567,
      "grad_norm": 1.8203175067901611,
      "learning_rate": 2.4271809474501685e-05,
      "loss": 1.1281,
      "step": 7480
    },
    {
      "epoch": 0.5801816456554155,
      "grad_norm": 1.4108000993728638,
      "learning_rate": 2.426404348951592e-05,
      "loss": 1.0714,
      "step": 7490
    },
    {
      "epoch": 0.5809562539940742,
      "grad_norm": 1.3730494976043701,
      "learning_rate": 2.4256277504530158e-05,
      "loss": 1.0773,
      "step": 7500
    },
    {
      "epoch": 0.581730862332733,
      "grad_norm": 1.4432116746902466,
      "learning_rate": 2.4248511519544396e-05,
      "loss": 1.2024,
      "step": 7510
    },
    {
      "epoch": 0.5825054706713918,
      "grad_norm": 1.5003652572631836,
      "learning_rate": 2.4240745534558634e-05,
      "loss": 1.1265,
      "step": 7520
    },
    {
      "epoch": 0.5832800790100505,
      "grad_norm": 1.7267882823944092,
      "learning_rate": 2.423297954957287e-05,
      "loss": 1.2186,
      "step": 7530
    },
    {
      "epoch": 0.5840546873487094,
      "grad_norm": 2.00571608543396,
      "learning_rate": 2.422521356458711e-05,
      "loss": 1.1978,
      "step": 7540
    },
    {
      "epoch": 0.5848292956873681,
      "grad_norm": 1.8761134147644043,
      "learning_rate": 2.4217447579601347e-05,
      "loss": 1.1603,
      "step": 7550
    },
    {
      "epoch": 0.5856039040260268,
      "grad_norm": 2.2065038681030273,
      "learning_rate": 2.4209681594615585e-05,
      "loss": 1.1735,
      "step": 7560
    },
    {
      "epoch": 0.5863785123646856,
      "grad_norm": 1.2239131927490234,
      "learning_rate": 2.420191560962982e-05,
      "loss": 1.1991,
      "step": 7570
    },
    {
      "epoch": 0.5871531207033444,
      "grad_norm": 1.1935416460037231,
      "learning_rate": 2.4194149624644057e-05,
      "loss": 1.1892,
      "step": 7580
    },
    {
      "epoch": 0.5879277290420032,
      "grad_norm": 1.4638423919677734,
      "learning_rate": 2.4186383639658295e-05,
      "loss": 1.1664,
      "step": 7590
    },
    {
      "epoch": 0.5887023373806619,
      "grad_norm": 1.4551568031311035,
      "learning_rate": 2.4178617654672533e-05,
      "loss": 1.1989,
      "step": 7600
    },
    {
      "epoch": 0.5894769457193206,
      "grad_norm": 1.9341440200805664,
      "learning_rate": 2.417085166968677e-05,
      "loss": 1.1924,
      "step": 7610
    },
    {
      "epoch": 0.5902515540579795,
      "grad_norm": 1.308472752571106,
      "learning_rate": 2.4163085684701012e-05,
      "loss": 1.2353,
      "step": 7620
    },
    {
      "epoch": 0.5910261623966382,
      "grad_norm": 1.5139281749725342,
      "learning_rate": 2.415531969971525e-05,
      "loss": 1.0627,
      "step": 7630
    },
    {
      "epoch": 0.5918007707352969,
      "grad_norm": 1.8607771396636963,
      "learning_rate": 2.4147553714729488e-05,
      "loss": 1.2929,
      "step": 7640
    },
    {
      "epoch": 0.5925753790739557,
      "grad_norm": 1.1692997217178345,
      "learning_rate": 2.4139787729743726e-05,
      "loss": 1.1769,
      "step": 7650
    },
    {
      "epoch": 0.5933499874126145,
      "grad_norm": 1.4739799499511719,
      "learning_rate": 2.413202174475796e-05,
      "loss": 1.2288,
      "step": 7660
    },
    {
      "epoch": 0.5941245957512733,
      "grad_norm": 1.4658197164535522,
      "learning_rate": 2.4124255759772198e-05,
      "loss": 1.126,
      "step": 7670
    },
    {
      "epoch": 0.594899204089932,
      "grad_norm": 1.271121859550476,
      "learning_rate": 2.4116489774786436e-05,
      "loss": 1.1294,
      "step": 7680
    },
    {
      "epoch": 0.5956738124285907,
      "grad_norm": 1.4481241703033447,
      "learning_rate": 2.4108723789800674e-05,
      "loss": 1.1996,
      "step": 7690
    },
    {
      "epoch": 0.5964484207672496,
      "grad_norm": 1.5129058361053467,
      "learning_rate": 2.4100957804814912e-05,
      "loss": 1.2055,
      "step": 7700
    },
    {
      "epoch": 0.5972230291059083,
      "grad_norm": 2.003166437149048,
      "learning_rate": 2.409319181982915e-05,
      "loss": 1.1408,
      "step": 7710
    },
    {
      "epoch": 0.5979976374445671,
      "grad_norm": 1.436111330986023,
      "learning_rate": 2.4085425834843387e-05,
      "loss": 1.1711,
      "step": 7720
    },
    {
      "epoch": 0.5987722457832259,
      "grad_norm": 1.374014139175415,
      "learning_rate": 2.4077659849857625e-05,
      "loss": 1.2139,
      "step": 7730
    },
    {
      "epoch": 0.5995468541218846,
      "grad_norm": 2.2111124992370605,
      "learning_rate": 2.406989386487186e-05,
      "loss": 1.1695,
      "step": 7740
    },
    {
      "epoch": 0.6003214624605434,
      "grad_norm": 1.2918009757995605,
      "learning_rate": 2.4062127879886098e-05,
      "loss": 1.1085,
      "step": 7750
    },
    {
      "epoch": 0.6010960707992021,
      "grad_norm": 1.8358004093170166,
      "learning_rate": 2.4054361894900336e-05,
      "loss": 1.2052,
      "step": 7760
    },
    {
      "epoch": 0.601870679137861,
      "grad_norm": 1.6599534749984741,
      "learning_rate": 2.4046595909914573e-05,
      "loss": 1.1735,
      "step": 7770
    },
    {
      "epoch": 0.6026452874765197,
      "grad_norm": 1.4562675952911377,
      "learning_rate": 2.403882992492881e-05,
      "loss": 1.1148,
      "step": 7780
    },
    {
      "epoch": 0.6034198958151784,
      "grad_norm": 1.2123606204986572,
      "learning_rate": 2.403106393994305e-05,
      "loss": 1.2818,
      "step": 7790
    },
    {
      "epoch": 0.6041945041538372,
      "grad_norm": 1.340819239616394,
      "learning_rate": 2.4023297954957287e-05,
      "loss": 1.2306,
      "step": 7800
    },
    {
      "epoch": 0.604969112492496,
      "grad_norm": 1.4155179262161255,
      "learning_rate": 2.4015531969971525e-05,
      "loss": 1.1907,
      "step": 7810
    },
    {
      "epoch": 0.6057437208311548,
      "grad_norm": 1.346207857131958,
      "learning_rate": 2.4007765984985766e-05,
      "loss": 1.2047,
      "step": 7820
    },
    {
      "epoch": 0.6065183291698135,
      "grad_norm": 1.6173360347747803,
      "learning_rate": 2.4e-05,
      "loss": 1.1955,
      "step": 7830
    },
    {
      "epoch": 0.6072929375084722,
      "grad_norm": 1.8282992839813232,
      "learning_rate": 2.399223401501424e-05,
      "loss": 1.2312,
      "step": 7840
    },
    {
      "epoch": 0.6080675458471311,
      "grad_norm": 1.301487684249878,
      "learning_rate": 2.3984468030028476e-05,
      "loss": 1.195,
      "step": 7850
    },
    {
      "epoch": 0.6088421541857898,
      "grad_norm": 1.3521924018859863,
      "learning_rate": 2.3976702045042714e-05,
      "loss": 1.2234,
      "step": 7860
    },
    {
      "epoch": 0.6096167625244486,
      "grad_norm": 1.3381363153457642,
      "learning_rate": 2.3968936060056952e-05,
      "loss": 1.1723,
      "step": 7870
    },
    {
      "epoch": 0.6103913708631074,
      "grad_norm": 1.7987793684005737,
      "learning_rate": 2.396117007507119e-05,
      "loss": 1.1737,
      "step": 7880
    },
    {
      "epoch": 0.6111659792017661,
      "grad_norm": 1.4657986164093018,
      "learning_rate": 2.3953404090085428e-05,
      "loss": 1.128,
      "step": 7890
    },
    {
      "epoch": 0.6119405875404249,
      "grad_norm": 1.0049951076507568,
      "learning_rate": 2.3945638105099666e-05,
      "loss": 1.1808,
      "step": 7900
    },
    {
      "epoch": 0.6127151958790836,
      "grad_norm": 1.4575297832489014,
      "learning_rate": 2.39378721201139e-05,
      "loss": 1.2575,
      "step": 7910
    },
    {
      "epoch": 0.6134898042177424,
      "grad_norm": 1.7650846242904663,
      "learning_rate": 2.3930106135128138e-05,
      "loss": 1.2059,
      "step": 7920
    },
    {
      "epoch": 0.6142644125564012,
      "grad_norm": 1.6911365985870361,
      "learning_rate": 2.3922340150142376e-05,
      "loss": 1.2013,
      "step": 7930
    },
    {
      "epoch": 0.6150390208950599,
      "grad_norm": 1.795275330543518,
      "learning_rate": 2.3914574165156614e-05,
      "loss": 1.1612,
      "step": 7940
    },
    {
      "epoch": 0.6158136292337187,
      "grad_norm": 1.5717304944992065,
      "learning_rate": 2.390680818017085e-05,
      "loss": 1.1625,
      "step": 7950
    },
    {
      "epoch": 0.6165882375723775,
      "grad_norm": 1.9590349197387695,
      "learning_rate": 2.389904219518509e-05,
      "loss": 1.1526,
      "step": 7960
    },
    {
      "epoch": 0.6173628459110362,
      "grad_norm": 1.7993940114974976,
      "learning_rate": 2.3891276210199327e-05,
      "loss": 1.2458,
      "step": 7970
    },
    {
      "epoch": 0.618137454249695,
      "grad_norm": 1.6559029817581177,
      "learning_rate": 2.3883510225213565e-05,
      "loss": 1.1489,
      "step": 7980
    },
    {
      "epoch": 0.6189120625883537,
      "grad_norm": 1.1658891439437866,
      "learning_rate": 2.3875744240227803e-05,
      "loss": 1.1144,
      "step": 7990
    },
    {
      "epoch": 0.6196866709270126,
      "grad_norm": 1.684525728225708,
      "learning_rate": 2.3867978255242038e-05,
      "loss": 1.191,
      "step": 8000
    },
    {
      "epoch": 0.6204612792656713,
      "grad_norm": 1.200019121170044,
      "learning_rate": 2.386021227025628e-05,
      "loss": 1.2706,
      "step": 8010
    },
    {
      "epoch": 0.62123588760433,
      "grad_norm": 1.8605068922042847,
      "learning_rate": 2.3852446285270517e-05,
      "loss": 1.1764,
      "step": 8020
    },
    {
      "epoch": 0.6220104959429889,
      "grad_norm": 1.359407901763916,
      "learning_rate": 2.3844680300284755e-05,
      "loss": 1.2624,
      "step": 8030
    },
    {
      "epoch": 0.6227851042816476,
      "grad_norm": 1.7955479621887207,
      "learning_rate": 2.3836914315298992e-05,
      "loss": 1.0603,
      "step": 8040
    },
    {
      "epoch": 0.6235597126203064,
      "grad_norm": 1.4329127073287964,
      "learning_rate": 2.382914833031323e-05,
      "loss": 1.2224,
      "step": 8050
    },
    {
      "epoch": 0.6243343209589651,
      "grad_norm": 1.2780907154083252,
      "learning_rate": 2.3821382345327468e-05,
      "loss": 1.2645,
      "step": 8060
    },
    {
      "epoch": 0.6251089292976238,
      "grad_norm": 1.696286678314209,
      "learning_rate": 2.3813616360341706e-05,
      "loss": 1.2206,
      "step": 8070
    },
    {
      "epoch": 0.6258835376362827,
      "grad_norm": 1.26748788356781,
      "learning_rate": 2.380585037535594e-05,
      "loss": 1.1059,
      "step": 8080
    },
    {
      "epoch": 0.6266581459749414,
      "grad_norm": 1.3678123950958252,
      "learning_rate": 2.379808439037018e-05,
      "loss": 1.146,
      "step": 8090
    },
    {
      "epoch": 0.6274327543136002,
      "grad_norm": 1.3392260074615479,
      "learning_rate": 2.3790318405384416e-05,
      "loss": 1.0839,
      "step": 8100
    },
    {
      "epoch": 0.628207362652259,
      "grad_norm": 1.3812216520309448,
      "learning_rate": 2.3782552420398654e-05,
      "loss": 1.3543,
      "step": 8110
    },
    {
      "epoch": 0.6289819709909177,
      "grad_norm": 1.260270118713379,
      "learning_rate": 2.3774786435412892e-05,
      "loss": 1.3137,
      "step": 8120
    },
    {
      "epoch": 0.6297565793295765,
      "grad_norm": 1.3323220014572144,
      "learning_rate": 2.376702045042713e-05,
      "loss": 1.1773,
      "step": 8130
    },
    {
      "epoch": 0.6305311876682352,
      "grad_norm": 1.6529624462127686,
      "learning_rate": 2.3759254465441368e-05,
      "loss": 1.1744,
      "step": 8140
    },
    {
      "epoch": 0.631305796006894,
      "grad_norm": 1.1805609464645386,
      "learning_rate": 2.3751488480455606e-05,
      "loss": 1.1538,
      "step": 8150
    },
    {
      "epoch": 0.6320804043455528,
      "grad_norm": 1.4466584920883179,
      "learning_rate": 2.3743722495469843e-05,
      "loss": 1.186,
      "step": 8160
    },
    {
      "epoch": 0.6328550126842115,
      "grad_norm": 1.4420832395553589,
      "learning_rate": 2.3735956510484078e-05,
      "loss": 1.1159,
      "step": 8170
    },
    {
      "epoch": 0.6336296210228703,
      "grad_norm": 1.7202982902526855,
      "learning_rate": 2.3728190525498316e-05,
      "loss": 1.1764,
      "step": 8180
    },
    {
      "epoch": 0.6344042293615291,
      "grad_norm": 1.772416353225708,
      "learning_rate": 2.3720424540512554e-05,
      "loss": 1.1317,
      "step": 8190
    },
    {
      "epoch": 0.6351788377001878,
      "grad_norm": 1.5913357734680176,
      "learning_rate": 2.371265855552679e-05,
      "loss": 1.1602,
      "step": 8200
    },
    {
      "epoch": 0.6359534460388466,
      "grad_norm": 1.6776847839355469,
      "learning_rate": 2.3704892570541033e-05,
      "loss": 1.1534,
      "step": 8210
    },
    {
      "epoch": 0.6367280543775053,
      "grad_norm": 1.4252442121505737,
      "learning_rate": 2.369712658555527e-05,
      "loss": 1.2292,
      "step": 8220
    },
    {
      "epoch": 0.6375026627161642,
      "grad_norm": 1.0669831037521362,
      "learning_rate": 2.368936060056951e-05,
      "loss": 1.1665,
      "step": 8230
    },
    {
      "epoch": 0.6382772710548229,
      "grad_norm": 1.4685229063034058,
      "learning_rate": 2.3681594615583746e-05,
      "loss": 1.1723,
      "step": 8240
    },
    {
      "epoch": 0.6390518793934816,
      "grad_norm": 1.615679383277893,
      "learning_rate": 2.367382863059798e-05,
      "loss": 1.1128,
      "step": 8250
    },
    {
      "epoch": 0.6398264877321405,
      "grad_norm": 1.6893287897109985,
      "learning_rate": 2.366606264561222e-05,
      "loss": 1.1669,
      "step": 8260
    },
    {
      "epoch": 0.6406010960707992,
      "grad_norm": 1.6391412019729614,
      "learning_rate": 2.3658296660626457e-05,
      "loss": 1.0713,
      "step": 8270
    },
    {
      "epoch": 0.641375704409458,
      "grad_norm": 1.3642525672912598,
      "learning_rate": 2.3650530675640694e-05,
      "loss": 1.1746,
      "step": 8280
    },
    {
      "epoch": 0.6421503127481167,
      "grad_norm": 1.4293296337127686,
      "learning_rate": 2.3642764690654932e-05,
      "loss": 1.2408,
      "step": 8290
    },
    {
      "epoch": 0.6429249210867755,
      "grad_norm": 1.4893410205841064,
      "learning_rate": 2.363499870566917e-05,
      "loss": 1.0904,
      "step": 8300
    },
    {
      "epoch": 0.6436995294254343,
      "grad_norm": 1.3686773777008057,
      "learning_rate": 2.3627232720683408e-05,
      "loss": 1.1856,
      "step": 8310
    },
    {
      "epoch": 0.644474137764093,
      "grad_norm": 1.6234172582626343,
      "learning_rate": 2.3619466735697646e-05,
      "loss": 1.2121,
      "step": 8320
    },
    {
      "epoch": 0.6452487461027518,
      "grad_norm": 1.411532998085022,
      "learning_rate": 2.3611700750711884e-05,
      "loss": 1.1367,
      "step": 8330
    },
    {
      "epoch": 0.6460233544414106,
      "grad_norm": 1.5807735919952393,
      "learning_rate": 2.3603934765726118e-05,
      "loss": 1.2494,
      "step": 8340
    },
    {
      "epoch": 0.6467979627800693,
      "grad_norm": 1.6006180047988892,
      "learning_rate": 2.3596168780740356e-05,
      "loss": 1.0582,
      "step": 8350
    },
    {
      "epoch": 0.6475725711187281,
      "grad_norm": 1.570951223373413,
      "learning_rate": 2.3588402795754594e-05,
      "loss": 1.1242,
      "step": 8360
    },
    {
      "epoch": 0.6483471794573868,
      "grad_norm": 2.0189156532287598,
      "learning_rate": 2.3580636810768832e-05,
      "loss": 1.2267,
      "step": 8370
    },
    {
      "epoch": 0.6491217877960457,
      "grad_norm": 1.7897536754608154,
      "learning_rate": 2.357287082578307e-05,
      "loss": 1.1404,
      "step": 8380
    },
    {
      "epoch": 0.6498963961347044,
      "grad_norm": 1.1494990587234497,
      "learning_rate": 2.3565104840797308e-05,
      "loss": 1.1777,
      "step": 8390
    },
    {
      "epoch": 0.6506710044733631,
      "grad_norm": 1.4926719665527344,
      "learning_rate": 2.355733885581155e-05,
      "loss": 1.2238,
      "step": 8400
    },
    {
      "epoch": 0.651445612812022,
      "grad_norm": 1.531723141670227,
      "learning_rate": 2.3549572870825787e-05,
      "loss": 1.1529,
      "step": 8410
    },
    {
      "epoch": 0.6522202211506807,
      "grad_norm": 1.6409189701080322,
      "learning_rate": 2.354180688584002e-05,
      "loss": 1.155,
      "step": 8420
    },
    {
      "epoch": 0.6529948294893394,
      "grad_norm": 1.5343331098556519,
      "learning_rate": 2.353404090085426e-05,
      "loss": 1.117,
      "step": 8430
    },
    {
      "epoch": 0.6537694378279982,
      "grad_norm": 1.2559497356414795,
      "learning_rate": 2.3526274915868497e-05,
      "loss": 1.1786,
      "step": 8440
    },
    {
      "epoch": 0.654544046166657,
      "grad_norm": 1.252707839012146,
      "learning_rate": 2.3518508930882735e-05,
      "loss": 1.1755,
      "step": 8450
    },
    {
      "epoch": 0.6553186545053158,
      "grad_norm": 2.1920666694641113,
      "learning_rate": 2.3510742945896973e-05,
      "loss": 1.128,
      "step": 8460
    },
    {
      "epoch": 0.6560932628439745,
      "grad_norm": 1.3593971729278564,
      "learning_rate": 2.350297696091121e-05,
      "loss": 1.0581,
      "step": 8470
    },
    {
      "epoch": 0.6568678711826332,
      "grad_norm": 1.3103997707366943,
      "learning_rate": 2.349521097592545e-05,
      "loss": 1.2267,
      "step": 8480
    },
    {
      "epoch": 0.6576424795212921,
      "grad_norm": 1.8665932416915894,
      "learning_rate": 2.3487444990939686e-05,
      "loss": 1.1928,
      "step": 8490
    },
    {
      "epoch": 0.6584170878599508,
      "grad_norm": 1.3707506656646729,
      "learning_rate": 2.347967900595392e-05,
      "loss": 1.2143,
      "step": 8500
    },
    {
      "epoch": 0.6591916961986096,
      "grad_norm": 1.3469910621643066,
      "learning_rate": 2.347191302096816e-05,
      "loss": 1.1289,
      "step": 8510
    },
    {
      "epoch": 0.6599663045372683,
      "grad_norm": 1.6049240827560425,
      "learning_rate": 2.3464147035982396e-05,
      "loss": 1.094,
      "step": 8520
    },
    {
      "epoch": 0.6607409128759271,
      "grad_norm": 2.0852928161621094,
      "learning_rate": 2.3456381050996634e-05,
      "loss": 1.2342,
      "step": 8530
    },
    {
      "epoch": 0.6615155212145859,
      "grad_norm": 1.6170287132263184,
      "learning_rate": 2.3448615066010872e-05,
      "loss": 1.1773,
      "step": 8540
    },
    {
      "epoch": 0.6622901295532446,
      "grad_norm": 1.7730923891067505,
      "learning_rate": 2.344084908102511e-05,
      "loss": 1.1932,
      "step": 8550
    },
    {
      "epoch": 0.6630647378919035,
      "grad_norm": 1.2779760360717773,
      "learning_rate": 2.3433083096039348e-05,
      "loss": 1.1803,
      "step": 8560
    },
    {
      "epoch": 0.6638393462305622,
      "grad_norm": 1.948965311050415,
      "learning_rate": 2.3425317111053586e-05,
      "loss": 1.1287,
      "step": 8570
    },
    {
      "epoch": 0.6646139545692209,
      "grad_norm": 1.3147724866867065,
      "learning_rate": 2.3417551126067824e-05,
      "loss": 1.2005,
      "step": 8580
    },
    {
      "epoch": 0.6653885629078797,
      "grad_norm": 2.259366750717163,
      "learning_rate": 2.3409785141082058e-05,
      "loss": 1.1496,
      "step": 8590
    },
    {
      "epoch": 0.6661631712465385,
      "grad_norm": 1.650252342224121,
      "learning_rate": 2.34020191560963e-05,
      "loss": 1.2108,
      "step": 8600
    },
    {
      "epoch": 0.6669377795851973,
      "grad_norm": 1.3571027517318726,
      "learning_rate": 2.3394253171110537e-05,
      "loss": 1.2656,
      "step": 8610
    },
    {
      "epoch": 0.667712387923856,
      "grad_norm": 1.226642370223999,
      "learning_rate": 2.3386487186124775e-05,
      "loss": 1.2063,
      "step": 8620
    },
    {
      "epoch": 0.6684869962625147,
      "grad_norm": 1.7662252187728882,
      "learning_rate": 2.3378721201139013e-05,
      "loss": 1.2125,
      "step": 8630
    },
    {
      "epoch": 0.6692616046011736,
      "grad_norm": 1.6399084329605103,
      "learning_rate": 2.337095521615325e-05,
      "loss": 1.1342,
      "step": 8640
    },
    {
      "epoch": 0.6700362129398323,
      "grad_norm": 1.378027319908142,
      "learning_rate": 2.336318923116749e-05,
      "loss": 1.2439,
      "step": 8650
    },
    {
      "epoch": 0.670810821278491,
      "grad_norm": 1.4361435174942017,
      "learning_rate": 2.3355423246181727e-05,
      "loss": 1.2372,
      "step": 8660
    },
    {
      "epoch": 0.6715854296171498,
      "grad_norm": 1.5069553852081299,
      "learning_rate": 2.334765726119596e-05,
      "loss": 1.1213,
      "step": 8670
    },
    {
      "epoch": 0.6723600379558086,
      "grad_norm": 2.3164761066436768,
      "learning_rate": 2.33398912762102e-05,
      "loss": 1.1906,
      "step": 8680
    },
    {
      "epoch": 0.6731346462944674,
      "grad_norm": 1.6369508504867554,
      "learning_rate": 2.3332125291224437e-05,
      "loss": 1.1832,
      "step": 8690
    },
    {
      "epoch": 0.6739092546331261,
      "grad_norm": 1.561255693435669,
      "learning_rate": 2.3324359306238675e-05,
      "loss": 1.1933,
      "step": 8700
    },
    {
      "epoch": 0.6746838629717848,
      "grad_norm": 1.2727062702178955,
      "learning_rate": 2.3316593321252912e-05,
      "loss": 1.2262,
      "step": 8710
    },
    {
      "epoch": 0.6754584713104437,
      "grad_norm": 1.8161139488220215,
      "learning_rate": 2.330882733626715e-05,
      "loss": 1.2889,
      "step": 8720
    },
    {
      "epoch": 0.6762330796491024,
      "grad_norm": 1.6757830381393433,
      "learning_rate": 2.3301061351281388e-05,
      "loss": 1.1863,
      "step": 8730
    },
    {
      "epoch": 0.6770076879877612,
      "grad_norm": 1.8817697763442993,
      "learning_rate": 2.3293295366295626e-05,
      "loss": 1.154,
      "step": 8740
    },
    {
      "epoch": 0.67778229632642,
      "grad_norm": 1.2333869934082031,
      "learning_rate": 2.3285529381309864e-05,
      "loss": 1.1543,
      "step": 8750
    },
    {
      "epoch": 0.6785569046650787,
      "grad_norm": 1.1139147281646729,
      "learning_rate": 2.32777633963241e-05,
      "loss": 1.2829,
      "step": 8760
    },
    {
      "epoch": 0.6793315130037375,
      "grad_norm": 1.1926552057266235,
      "learning_rate": 2.3269997411338336e-05,
      "loss": 1.2105,
      "step": 8770
    },
    {
      "epoch": 0.6801061213423962,
      "grad_norm": 1.5981707572937012,
      "learning_rate": 2.3262231426352574e-05,
      "loss": 1.1246,
      "step": 8780
    },
    {
      "epoch": 0.6808807296810551,
      "grad_norm": 1.3503518104553223,
      "learning_rate": 2.3254465441366812e-05,
      "loss": 1.1777,
      "step": 8790
    },
    {
      "epoch": 0.6816553380197138,
      "grad_norm": 1.4138102531433105,
      "learning_rate": 2.3246699456381053e-05,
      "loss": 1.2436,
      "step": 8800
    },
    {
      "epoch": 0.6824299463583725,
      "grad_norm": 1.4541014432907104,
      "learning_rate": 2.323893347139529e-05,
      "loss": 1.1933,
      "step": 8810
    },
    {
      "epoch": 0.6832045546970313,
      "grad_norm": 1.9286044836044312,
      "learning_rate": 2.323116748640953e-05,
      "loss": 1.216,
      "step": 8820
    },
    {
      "epoch": 0.6839791630356901,
      "grad_norm": 1.8818936347961426,
      "learning_rate": 2.3223401501423767e-05,
      "loss": 1.2025,
      "step": 8830
    },
    {
      "epoch": 0.6847537713743489,
      "grad_norm": 1.6723333597183228,
      "learning_rate": 2.3215635516438e-05,
      "loss": 1.2667,
      "step": 8840
    },
    {
      "epoch": 0.6855283797130076,
      "grad_norm": 1.5628806352615356,
      "learning_rate": 2.320786953145224e-05,
      "loss": 1.1929,
      "step": 8850
    },
    {
      "epoch": 0.6863029880516663,
      "grad_norm": 1.5128483772277832,
      "learning_rate": 2.3200103546466477e-05,
      "loss": 1.1933,
      "step": 8860
    },
    {
      "epoch": 0.6870775963903252,
      "grad_norm": 1.276167392730713,
      "learning_rate": 2.3192337561480715e-05,
      "loss": 1.2214,
      "step": 8870
    },
    {
      "epoch": 0.6878522047289839,
      "grad_norm": 1.4634748697280884,
      "learning_rate": 2.3184571576494953e-05,
      "loss": 1.181,
      "step": 8880
    },
    {
      "epoch": 0.6886268130676426,
      "grad_norm": 1.2998954057693481,
      "learning_rate": 2.317680559150919e-05,
      "loss": 1.2696,
      "step": 8890
    },
    {
      "epoch": 0.6894014214063015,
      "grad_norm": 2.078810930252075,
      "learning_rate": 2.316903960652343e-05,
      "loss": 1.1382,
      "step": 8900
    },
    {
      "epoch": 0.6901760297449602,
      "grad_norm": 1.0680099725723267,
      "learning_rate": 2.3161273621537666e-05,
      "loss": 1.0439,
      "step": 8910
    },
    {
      "epoch": 0.690950638083619,
      "grad_norm": 1.520723819732666,
      "learning_rate": 2.3153507636551904e-05,
      "loss": 1.2239,
      "step": 8920
    },
    {
      "epoch": 0.6917252464222777,
      "grad_norm": 1.8263368606567383,
      "learning_rate": 2.314574165156614e-05,
      "loss": 1.2171,
      "step": 8930
    },
    {
      "epoch": 0.6924998547609365,
      "grad_norm": 2.054863214492798,
      "learning_rate": 2.3137975666580377e-05,
      "loss": 1.2017,
      "step": 8940
    },
    {
      "epoch": 0.6932744630995953,
      "grad_norm": 2.069061279296875,
      "learning_rate": 2.3130209681594615e-05,
      "loss": 1.1785,
      "step": 8950
    },
    {
      "epoch": 0.694049071438254,
      "grad_norm": 1.6119295358657837,
      "learning_rate": 2.3122443696608852e-05,
      "loss": 1.1681,
      "step": 8960
    },
    {
      "epoch": 0.6948236797769128,
      "grad_norm": 1.4360973834991455,
      "learning_rate": 2.311467771162309e-05,
      "loss": 1.2044,
      "step": 8970
    },
    {
      "epoch": 0.6955982881155716,
      "grad_norm": 1.474670171737671,
      "learning_rate": 2.3106911726637328e-05,
      "loss": 1.1192,
      "step": 8980
    },
    {
      "epoch": 0.6963728964542303,
      "grad_norm": 2.513098955154419,
      "learning_rate": 2.309914574165157e-05,
      "loss": 1.2222,
      "step": 8990
    },
    {
      "epoch": 0.6971475047928891,
      "grad_norm": 1.7238132953643799,
      "learning_rate": 2.3091379756665807e-05,
      "loss": 1.238,
      "step": 9000
    },
    {
      "epoch": 0.6979221131315478,
      "grad_norm": 1.5370253324508667,
      "learning_rate": 2.3083613771680042e-05,
      "loss": 1.2362,
      "step": 9010
    },
    {
      "epoch": 0.6986967214702067,
      "grad_norm": 1.6325675249099731,
      "learning_rate": 2.307584778669428e-05,
      "loss": 1.1661,
      "step": 9020
    },
    {
      "epoch": 0.6994713298088654,
      "grad_norm": 1.7519270181655884,
      "learning_rate": 2.3068081801708517e-05,
      "loss": 1.1721,
      "step": 9030
    },
    {
      "epoch": 0.7002459381475241,
      "grad_norm": 1.8838908672332764,
      "learning_rate": 2.3060315816722755e-05,
      "loss": 1.2632,
      "step": 9040
    },
    {
      "epoch": 0.701020546486183,
      "grad_norm": 1.798216462135315,
      "learning_rate": 2.3052549831736993e-05,
      "loss": 1.3365,
      "step": 9050
    },
    {
      "epoch": 0.7017951548248417,
      "grad_norm": 1.2487999200820923,
      "learning_rate": 2.304478384675123e-05,
      "loss": 1.2699,
      "step": 9060
    },
    {
      "epoch": 0.7025697631635005,
      "grad_norm": 0.9635313749313354,
      "learning_rate": 2.303701786176547e-05,
      "loss": 1.1577,
      "step": 9070
    },
    {
      "epoch": 0.7033443715021592,
      "grad_norm": 1.4354510307312012,
      "learning_rate": 2.3029251876779707e-05,
      "loss": 1.2088,
      "step": 9080
    },
    {
      "epoch": 0.704118979840818,
      "grad_norm": 1.5919909477233887,
      "learning_rate": 2.3021485891793945e-05,
      "loss": 1.202,
      "step": 9090
    },
    {
      "epoch": 0.7048935881794768,
      "grad_norm": 1.2085217237472534,
      "learning_rate": 2.301371990680818e-05,
      "loss": 1.1626,
      "step": 9100
    },
    {
      "epoch": 0.7056681965181355,
      "grad_norm": 1.5335716009140015,
      "learning_rate": 2.3005953921822417e-05,
      "loss": 1.0713,
      "step": 9110
    },
    {
      "epoch": 0.7064428048567943,
      "grad_norm": 1.4655321836471558,
      "learning_rate": 2.2998187936836655e-05,
      "loss": 1.2117,
      "step": 9120
    },
    {
      "epoch": 0.7072174131954531,
      "grad_norm": 1.5344637632369995,
      "learning_rate": 2.2990421951850893e-05,
      "loss": 1.1287,
      "step": 9130
    },
    {
      "epoch": 0.7079920215341118,
      "grad_norm": 1.2532014846801758,
      "learning_rate": 2.298265596686513e-05,
      "loss": 1.1184,
      "step": 9140
    },
    {
      "epoch": 0.7087666298727706,
      "grad_norm": 2.153304100036621,
      "learning_rate": 2.297488998187937e-05,
      "loss": 1.1158,
      "step": 9150
    },
    {
      "epoch": 0.7095412382114293,
      "grad_norm": 1.3340500593185425,
      "learning_rate": 2.2967123996893606e-05,
      "loss": 1.0617,
      "step": 9160
    },
    {
      "epoch": 0.7103158465500881,
      "grad_norm": 1.647279977798462,
      "learning_rate": 2.2959358011907844e-05,
      "loss": 1.251,
      "step": 9170
    },
    {
      "epoch": 0.7110904548887469,
      "grad_norm": 1.2273120880126953,
      "learning_rate": 2.295159202692208e-05,
      "loss": 1.1365,
      "step": 9180
    },
    {
      "epoch": 0.7118650632274056,
      "grad_norm": 1.2467049360275269,
      "learning_rate": 2.294382604193632e-05,
      "loss": 1.1752,
      "step": 9190
    },
    {
      "epoch": 0.7126396715660644,
      "grad_norm": 1.1929235458374023,
      "learning_rate": 2.2936060056950558e-05,
      "loss": 1.276,
      "step": 9200
    },
    {
      "epoch": 0.7134142799047232,
      "grad_norm": 2.1161065101623535,
      "learning_rate": 2.2928294071964796e-05,
      "loss": 1.1483,
      "step": 9210
    },
    {
      "epoch": 0.7141888882433819,
      "grad_norm": 1.9031031131744385,
      "learning_rate": 2.2920528086979034e-05,
      "loss": 1.2873,
      "step": 9220
    },
    {
      "epoch": 0.7149634965820407,
      "grad_norm": 1.8741577863693237,
      "learning_rate": 2.291276210199327e-05,
      "loss": 1.1979,
      "step": 9230
    },
    {
      "epoch": 0.7157381049206994,
      "grad_norm": 1.4696050882339478,
      "learning_rate": 2.2905772715506085e-05,
      "loss": 1.1667,
      "step": 9240
    },
    {
      "epoch": 0.7165127132593583,
      "grad_norm": 1.3742281198501587,
      "learning_rate": 2.2898006730520323e-05,
      "loss": 1.155,
      "step": 9250
    },
    {
      "epoch": 0.717287321598017,
      "grad_norm": 1.5426045656204224,
      "learning_rate": 2.2890240745534557e-05,
      "loss": 1.1101,
      "step": 9260
    },
    {
      "epoch": 0.7180619299366757,
      "grad_norm": 1.5573451519012451,
      "learning_rate": 2.2882474760548795e-05,
      "loss": 1.2845,
      "step": 9270
    },
    {
      "epoch": 0.7188365382753346,
      "grad_norm": 1.5963952541351318,
      "learning_rate": 2.2874708775563033e-05,
      "loss": 1.1405,
      "step": 9280
    },
    {
      "epoch": 0.7196111466139933,
      "grad_norm": 1.4417965412139893,
      "learning_rate": 2.286694279057727e-05,
      "loss": 1.1795,
      "step": 9290
    },
    {
      "epoch": 0.7203857549526521,
      "grad_norm": 1.4914060831069946,
      "learning_rate": 2.285917680559151e-05,
      "loss": 1.0927,
      "step": 9300
    },
    {
      "epoch": 0.7211603632913108,
      "grad_norm": 1.9238033294677734,
      "learning_rate": 2.2851410820605746e-05,
      "loss": 1.1886,
      "step": 9310
    },
    {
      "epoch": 0.7219349716299696,
      "grad_norm": 1.578262209892273,
      "learning_rate": 2.2843644835619984e-05,
      "loss": 1.3011,
      "step": 9320
    },
    {
      "epoch": 0.7227095799686284,
      "grad_norm": 1.29883873462677,
      "learning_rate": 2.2835878850634226e-05,
      "loss": 1.1933,
      "step": 9330
    },
    {
      "epoch": 0.7234841883072871,
      "grad_norm": 1.3245283365249634,
      "learning_rate": 2.282811286564846e-05,
      "loss": 1.0953,
      "step": 9340
    },
    {
      "epoch": 0.724258796645946,
      "grad_norm": 1.2448828220367432,
      "learning_rate": 2.2820346880662698e-05,
      "loss": 1.212,
      "step": 9350
    },
    {
      "epoch": 0.7250334049846047,
      "grad_norm": 1.3882627487182617,
      "learning_rate": 2.2812580895676936e-05,
      "loss": 1.2573,
      "step": 9360
    },
    {
      "epoch": 0.7258080133232634,
      "grad_norm": 1.3330825567245483,
      "learning_rate": 2.2804814910691174e-05,
      "loss": 1.2058,
      "step": 9370
    },
    {
      "epoch": 0.7265826216619222,
      "grad_norm": 1.405900239944458,
      "learning_rate": 2.279704892570541e-05,
      "loss": 1.2092,
      "step": 9380
    },
    {
      "epoch": 0.727357230000581,
      "grad_norm": 1.2610862255096436,
      "learning_rate": 2.278928294071965e-05,
      "loss": 1.2853,
      "step": 9390
    },
    {
      "epoch": 0.7281318383392397,
      "grad_norm": 1.1713285446166992,
      "learning_rate": 2.2781516955733887e-05,
      "loss": 1.2064,
      "step": 9400
    },
    {
      "epoch": 0.7289064466778985,
      "grad_norm": 1.8620398044586182,
      "learning_rate": 2.2773750970748125e-05,
      "loss": 1.1186,
      "step": 9410
    },
    {
      "epoch": 0.7296810550165572,
      "grad_norm": 1.6404592990875244,
      "learning_rate": 2.2765984985762363e-05,
      "loss": 1.2355,
      "step": 9420
    },
    {
      "epoch": 0.7304556633552161,
      "grad_norm": 0.9921746850013733,
      "learning_rate": 2.2758219000776597e-05,
      "loss": 1.198,
      "step": 9430
    },
    {
      "epoch": 0.7312302716938748,
      "grad_norm": 1.7395018339157104,
      "learning_rate": 2.2750453015790835e-05,
      "loss": 1.2562,
      "step": 9440
    },
    {
      "epoch": 0.7320048800325335,
      "grad_norm": 1.6260035037994385,
      "learning_rate": 2.2742687030805073e-05,
      "loss": 1.161,
      "step": 9450
    },
    {
      "epoch": 0.7327794883711923,
      "grad_norm": 1.749476671218872,
      "learning_rate": 2.273492104581931e-05,
      "loss": 1.1961,
      "step": 9460
    },
    {
      "epoch": 0.7335540967098511,
      "grad_norm": 1.5596964359283447,
      "learning_rate": 2.272715506083355e-05,
      "loss": 1.2231,
      "step": 9470
    },
    {
      "epoch": 0.7343287050485099,
      "grad_norm": 1.6654953956604004,
      "learning_rate": 2.2719389075847787e-05,
      "loss": 1.1607,
      "step": 9480
    },
    {
      "epoch": 0.7351033133871686,
      "grad_norm": 1.2884595394134521,
      "learning_rate": 2.2711623090862025e-05,
      "loss": 1.2441,
      "step": 9490
    },
    {
      "epoch": 0.7358779217258273,
      "grad_norm": 1.7570818662643433,
      "learning_rate": 2.2703857105876263e-05,
      "loss": 1.1523,
      "step": 9500
    },
    {
      "epoch": 0.7366525300644862,
      "grad_norm": 1.215391993522644,
      "learning_rate": 2.2696091120890497e-05,
      "loss": 1.1825,
      "step": 9510
    },
    {
      "epoch": 0.7374271384031449,
      "grad_norm": 1.4154448509216309,
      "learning_rate": 2.2688325135904738e-05,
      "loss": 1.1644,
      "step": 9520
    },
    {
      "epoch": 0.7382017467418037,
      "grad_norm": 1.6627171039581299,
      "learning_rate": 2.2680559150918976e-05,
      "loss": 1.1195,
      "step": 9530
    },
    {
      "epoch": 0.7389763550804624,
      "grad_norm": 1.422126054763794,
      "learning_rate": 2.2672793165933214e-05,
      "loss": 1.1948,
      "step": 9540
    },
    {
      "epoch": 0.7397509634191212,
      "grad_norm": 1.41410493850708,
      "learning_rate": 2.2665027180947452e-05,
      "loss": 1.1752,
      "step": 9550
    },
    {
      "epoch": 0.74052557175778,
      "grad_norm": 1.4403940439224243,
      "learning_rate": 2.265726119596169e-05,
      "loss": 1.1042,
      "step": 9560
    },
    {
      "epoch": 0.7413001800964387,
      "grad_norm": 1.4182649850845337,
      "learning_rate": 2.2649495210975928e-05,
      "loss": 1.2322,
      "step": 9570
    },
    {
      "epoch": 0.7420747884350976,
      "grad_norm": 1.7019904851913452,
      "learning_rate": 2.2641729225990165e-05,
      "loss": 1.1795,
      "step": 9580
    },
    {
      "epoch": 0.7428493967737563,
      "grad_norm": 1.441786766052246,
      "learning_rate": 2.2633963241004403e-05,
      "loss": 1.1881,
      "step": 9590
    },
    {
      "epoch": 0.743624005112415,
      "grad_norm": 1.346357822418213,
      "learning_rate": 2.2626197256018638e-05,
      "loss": 1.2114,
      "step": 9600
    },
    {
      "epoch": 0.7443986134510738,
      "grad_norm": 1.3497273921966553,
      "learning_rate": 2.2618431271032876e-05,
      "loss": 1.148,
      "step": 9610
    },
    {
      "epoch": 0.7451732217897326,
      "grad_norm": 1.4619008302688599,
      "learning_rate": 2.2610665286047114e-05,
      "loss": 1.2821,
      "step": 9620
    },
    {
      "epoch": 0.7459478301283914,
      "grad_norm": 1.3989392518997192,
      "learning_rate": 2.260289930106135e-05,
      "loss": 1.3156,
      "step": 9630
    },
    {
      "epoch": 0.7467224384670501,
      "grad_norm": 1.7248587608337402,
      "learning_rate": 2.259513331607559e-05,
      "loss": 1.2718,
      "step": 9640
    },
    {
      "epoch": 0.7474970468057088,
      "grad_norm": 1.4899955987930298,
      "learning_rate": 2.2587367331089827e-05,
      "loss": 1.1458,
      "step": 9650
    },
    {
      "epoch": 0.7482716551443677,
      "grad_norm": 1.422068476676941,
      "learning_rate": 2.2579601346104065e-05,
      "loss": 1.1945,
      "step": 9660
    },
    {
      "epoch": 0.7490462634830264,
      "grad_norm": 1.5397624969482422,
      "learning_rate": 2.2571835361118303e-05,
      "loss": 1.2106,
      "step": 9670
    },
    {
      "epoch": 0.7498208718216851,
      "grad_norm": 1.3368909358978271,
      "learning_rate": 2.2564069376132537e-05,
      "loss": 1.208,
      "step": 9680
    },
    {
      "epoch": 0.7505954801603439,
      "grad_norm": 1.488169550895691,
      "learning_rate": 2.2556303391146775e-05,
      "loss": 1.2734,
      "step": 9690
    },
    {
      "epoch": 0.7513700884990027,
      "grad_norm": 2.065500497817993,
      "learning_rate": 2.2548537406161013e-05,
      "loss": 1.1213,
      "step": 9700
    },
    {
      "epoch": 0.7521446968376615,
      "grad_norm": 1.658232569694519,
      "learning_rate": 2.254077142117525e-05,
      "loss": 1.1763,
      "step": 9710
    },
    {
      "epoch": 0.7529193051763202,
      "grad_norm": 2.3107986450195312,
      "learning_rate": 2.2533005436189492e-05,
      "loss": 1.1574,
      "step": 9720
    },
    {
      "epoch": 0.7536939135149789,
      "grad_norm": 1.5208899974822998,
      "learning_rate": 2.252523945120373e-05,
      "loss": 1.1713,
      "step": 9730
    },
    {
      "epoch": 0.7544685218536378,
      "grad_norm": 1.5651870965957642,
      "learning_rate": 2.2517473466217968e-05,
      "loss": 1.2586,
      "step": 9740
    },
    {
      "epoch": 0.7552431301922965,
      "grad_norm": 1.4364244937896729,
      "learning_rate": 2.2509707481232206e-05,
      "loss": 1.1721,
      "step": 9750
    },
    {
      "epoch": 0.7560177385309553,
      "grad_norm": 1.4640111923217773,
      "learning_rate": 2.2501941496246444e-05,
      "loss": 1.1653,
      "step": 9760
    },
    {
      "epoch": 0.756792346869614,
      "grad_norm": 1.3695976734161377,
      "learning_rate": 2.2494175511260678e-05,
      "loss": 1.2142,
      "step": 9770
    },
    {
      "epoch": 0.7575669552082728,
      "grad_norm": 2.214834451675415,
      "learning_rate": 2.2486409526274916e-05,
      "loss": 1.2046,
      "step": 9780
    },
    {
      "epoch": 0.7583415635469316,
      "grad_norm": 1.517844557762146,
      "learning_rate": 2.2478643541289154e-05,
      "loss": 1.149,
      "step": 9790
    },
    {
      "epoch": 0.7591161718855903,
      "grad_norm": 1.4561711549758911,
      "learning_rate": 2.2470877556303392e-05,
      "loss": 1.2622,
      "step": 9800
    },
    {
      "epoch": 0.7598907802242492,
      "grad_norm": 1.6270948648452759,
      "learning_rate": 2.246311157131763e-05,
      "loss": 1.2078,
      "step": 9810
    },
    {
      "epoch": 0.7606653885629079,
      "grad_norm": 1.548692226409912,
      "learning_rate": 2.2455345586331867e-05,
      "loss": 1.1935,
      "step": 9820
    },
    {
      "epoch": 0.7614399969015666,
      "grad_norm": 1.5431673526763916,
      "learning_rate": 2.2447579601346105e-05,
      "loss": 1.1962,
      "step": 9830
    },
    {
      "epoch": 0.7622146052402254,
      "grad_norm": 1.9799076318740845,
      "learning_rate": 2.2439813616360343e-05,
      "loss": 1.2113,
      "step": 9840
    },
    {
      "epoch": 0.7629892135788842,
      "grad_norm": 1.5230613946914673,
      "learning_rate": 2.2432047631374578e-05,
      "loss": 1.2409,
      "step": 9850
    },
    {
      "epoch": 0.763763821917543,
      "grad_norm": 1.8967702388763428,
      "learning_rate": 2.2424281646388816e-05,
      "loss": 1.2202,
      "step": 9860
    },
    {
      "epoch": 0.7645384302562017,
      "grad_norm": 1.545198678970337,
      "learning_rate": 2.2416515661403053e-05,
      "loss": 1.1651,
      "step": 9870
    },
    {
      "epoch": 0.7653130385948604,
      "grad_norm": 1.476965308189392,
      "learning_rate": 2.240874967641729e-05,
      "loss": 1.1933,
      "step": 9880
    },
    {
      "epoch": 0.7660876469335193,
      "grad_norm": 1.7663403749465942,
      "learning_rate": 2.240098369143153e-05,
      "loss": 1.1593,
      "step": 9890
    },
    {
      "epoch": 0.766862255272178,
      "grad_norm": 1.1494542360305786,
      "learning_rate": 2.2393217706445767e-05,
      "loss": 1.2042,
      "step": 9900
    },
    {
      "epoch": 0.7676368636108367,
      "grad_norm": 1.738741397857666,
      "learning_rate": 2.2385451721460008e-05,
      "loss": 1.1954,
      "step": 9910
    },
    {
      "epoch": 0.7684114719494956,
      "grad_norm": 1.4514689445495605,
      "learning_rate": 2.2377685736474246e-05,
      "loss": 1.2086,
      "step": 9920
    },
    {
      "epoch": 0.7691860802881543,
      "grad_norm": 1.317740559577942,
      "learning_rate": 2.2369919751488484e-05,
      "loss": 1.1564,
      "step": 9930
    },
    {
      "epoch": 0.7699606886268131,
      "grad_norm": 1.8845374584197998,
      "learning_rate": 2.236215376650272e-05,
      "loss": 1.2034,
      "step": 9940
    },
    {
      "epoch": 0.7707352969654718,
      "grad_norm": 1.6214547157287598,
      "learning_rate": 2.2354387781516956e-05,
      "loss": 1.2047,
      "step": 9950
    },
    {
      "epoch": 0.7715099053041305,
      "grad_norm": 1.1834678649902344,
      "learning_rate": 2.2346621796531194e-05,
      "loss": 1.1327,
      "step": 9960
    },
    {
      "epoch": 0.7722845136427894,
      "grad_norm": 1.9756511449813843,
      "learning_rate": 2.2338855811545432e-05,
      "loss": 1.122,
      "step": 9970
    },
    {
      "epoch": 0.7730591219814481,
      "grad_norm": 1.8090648651123047,
      "learning_rate": 2.233108982655967e-05,
      "loss": 1.1296,
      "step": 9980
    },
    {
      "epoch": 0.7738337303201069,
      "grad_norm": 1.275415301322937,
      "learning_rate": 2.2323323841573908e-05,
      "loss": 1.2383,
      "step": 9990
    },
    {
      "epoch": 0.7746083386587657,
      "grad_norm": 1.3918029069900513,
      "learning_rate": 2.2315557856588146e-05,
      "loss": 1.1725,
      "step": 10000
    },
    {
      "epoch": 0.7753829469974244,
      "grad_norm": 1.9643537998199463,
      "learning_rate": 2.2307791871602384e-05,
      "loss": 1.175,
      "step": 10010
    },
    {
      "epoch": 0.7761575553360832,
      "grad_norm": 1.3072099685668945,
      "learning_rate": 2.2300025886616618e-05,
      "loss": 1.2174,
      "step": 10020
    },
    {
      "epoch": 0.7769321636747419,
      "grad_norm": 1.3985874652862549,
      "learning_rate": 2.2292259901630856e-05,
      "loss": 1.1603,
      "step": 10030
    },
    {
      "epoch": 0.7777067720134008,
      "grad_norm": 1.1327250003814697,
      "learning_rate": 2.2284493916645094e-05,
      "loss": 1.2885,
      "step": 10040
    },
    {
      "epoch": 0.7784813803520595,
      "grad_norm": 1.9073628187179565,
      "learning_rate": 2.227672793165933e-05,
      "loss": 1.1813,
      "step": 10050
    },
    {
      "epoch": 0.7792559886907182,
      "grad_norm": 1.7259761095046997,
      "learning_rate": 2.226896194667357e-05,
      "loss": 1.19,
      "step": 10060
    },
    {
      "epoch": 0.780030597029377,
      "grad_norm": 1.7034245729446411,
      "learning_rate": 2.2261195961687807e-05,
      "loss": 1.1763,
      "step": 10070
    },
    {
      "epoch": 0.7808052053680358,
      "grad_norm": 1.2993416786193848,
      "learning_rate": 2.2253429976702045e-05,
      "loss": 1.2063,
      "step": 10080
    },
    {
      "epoch": 0.7815798137066946,
      "grad_norm": 1.3504443168640137,
      "learning_rate": 2.2245663991716283e-05,
      "loss": 1.2072,
      "step": 10090
    },
    {
      "epoch": 0.7823544220453533,
      "grad_norm": 1.411816954612732,
      "learning_rate": 2.223789800673052e-05,
      "loss": 1.1658,
      "step": 10100
    },
    {
      "epoch": 0.783129030384012,
      "grad_norm": 1.8854153156280518,
      "learning_rate": 2.223013202174476e-05,
      "loss": 1.117,
      "step": 10110
    },
    {
      "epoch": 0.7839036387226709,
      "grad_norm": 1.4277325868606567,
      "learning_rate": 2.2222366036758997e-05,
      "loss": 1.1335,
      "step": 10120
    },
    {
      "epoch": 0.7846782470613296,
      "grad_norm": 1.3896011114120483,
      "learning_rate": 2.2214600051773235e-05,
      "loss": 1.2399,
      "step": 10130
    },
    {
      "epoch": 0.7854528553999883,
      "grad_norm": 1.3656940460205078,
      "learning_rate": 2.2206834066787472e-05,
      "loss": 1.2495,
      "step": 10140
    },
    {
      "epoch": 0.7862274637386472,
      "grad_norm": 1.501602292060852,
      "learning_rate": 2.219906808180171e-05,
      "loss": 1.2172,
      "step": 10150
    },
    {
      "epoch": 0.7870020720773059,
      "grad_norm": 1.0820505619049072,
      "learning_rate": 2.2191302096815948e-05,
      "loss": 1.1176,
      "step": 10160
    },
    {
      "epoch": 0.7877766804159647,
      "grad_norm": 1.3898096084594727,
      "learning_rate": 2.2183536111830186e-05,
      "loss": 1.2224,
      "step": 10170
    },
    {
      "epoch": 0.7885512887546234,
      "grad_norm": 1.3812848329544067,
      "learning_rate": 2.2175770126844424e-05,
      "loss": 1.2114,
      "step": 10180
    },
    {
      "epoch": 0.7893258970932822,
      "grad_norm": 1.6000521183013916,
      "learning_rate": 2.216800414185866e-05,
      "loss": 1.1542,
      "step": 10190
    },
    {
      "epoch": 0.790100505431941,
      "grad_norm": 1.3715078830718994,
      "learning_rate": 2.2160238156872896e-05,
      "loss": 1.2071,
      "step": 10200
    },
    {
      "epoch": 0.7908751137705997,
      "grad_norm": 2.126077651977539,
      "learning_rate": 2.2152472171887134e-05,
      "loss": 1.2729,
      "step": 10210
    },
    {
      "epoch": 0.7916497221092585,
      "grad_norm": 1.8992416858673096,
      "learning_rate": 2.2144706186901372e-05,
      "loss": 1.1716,
      "step": 10220
    },
    {
      "epoch": 0.7924243304479173,
      "grad_norm": 2.1124236583709717,
      "learning_rate": 2.213694020191561e-05,
      "loss": 1.166,
      "step": 10230
    },
    {
      "epoch": 0.793198938786576,
      "grad_norm": 1.6165485382080078,
      "learning_rate": 2.2129174216929848e-05,
      "loss": 1.1197,
      "step": 10240
    },
    {
      "epoch": 0.7939735471252348,
      "grad_norm": 2.0282092094421387,
      "learning_rate": 2.2121408231944086e-05,
      "loss": 1.1786,
      "step": 10250
    },
    {
      "epoch": 0.7947481554638935,
      "grad_norm": 1.6266483068466187,
      "learning_rate": 2.2113642246958323e-05,
      "loss": 1.2336,
      "step": 10260
    },
    {
      "epoch": 0.7955227638025524,
      "grad_norm": 1.2098172903060913,
      "learning_rate": 2.210587626197256e-05,
      "loss": 1.2905,
      "step": 10270
    },
    {
      "epoch": 0.7962973721412111,
      "grad_norm": 1.598862648010254,
      "learning_rate": 2.2098110276986796e-05,
      "loss": 1.1404,
      "step": 10280
    },
    {
      "epoch": 0.7970719804798698,
      "grad_norm": 1.7276201248168945,
      "learning_rate": 2.2090344292001034e-05,
      "loss": 1.2457,
      "step": 10290
    },
    {
      "epoch": 0.7978465888185287,
      "grad_norm": 1.3729099035263062,
      "learning_rate": 2.208257830701527e-05,
      "loss": 1.1147,
      "step": 10300
    },
    {
      "epoch": 0.7986211971571874,
      "grad_norm": 1.313423752784729,
      "learning_rate": 2.2074812322029513e-05,
      "loss": 1.1806,
      "step": 10310
    },
    {
      "epoch": 0.7993958054958462,
      "grad_norm": 1.5267863273620605,
      "learning_rate": 2.206704633704375e-05,
      "loss": 1.1952,
      "step": 10320
    },
    {
      "epoch": 0.8001704138345049,
      "grad_norm": 1.3096575736999512,
      "learning_rate": 2.205928035205799e-05,
      "loss": 1.0904,
      "step": 10330
    },
    {
      "epoch": 0.8009450221731637,
      "grad_norm": 1.7552168369293213,
      "learning_rate": 2.2051514367072226e-05,
      "loss": 1.1455,
      "step": 10340
    },
    {
      "epoch": 0.8017196305118225,
      "grad_norm": 1.5209084749221802,
      "learning_rate": 2.2043748382086464e-05,
      "loss": 1.2919,
      "step": 10350
    },
    {
      "epoch": 0.8024942388504812,
      "grad_norm": 1.2335484027862549,
      "learning_rate": 2.20359823971007e-05,
      "loss": 1.2585,
      "step": 10360
    },
    {
      "epoch": 0.80326884718914,
      "grad_norm": 1.2680554389953613,
      "learning_rate": 2.2028216412114937e-05,
      "loss": 1.1679,
      "step": 10370
    },
    {
      "epoch": 0.8040434555277988,
      "grad_norm": 1.725859522819519,
      "learning_rate": 2.2020450427129174e-05,
      "loss": 1.1781,
      "step": 10380
    },
    {
      "epoch": 0.8048180638664575,
      "grad_norm": 1.2534948587417603,
      "learning_rate": 2.2012684442143412e-05,
      "loss": 1.185,
      "step": 10390
    },
    {
      "epoch": 0.8055926722051163,
      "grad_norm": 1.4196867942810059,
      "learning_rate": 2.200491845715765e-05,
      "loss": 1.1853,
      "step": 10400
    },
    {
      "epoch": 0.806367280543775,
      "grad_norm": 1.5641707181930542,
      "learning_rate": 2.1997152472171888e-05,
      "loss": 1.1559,
      "step": 10410
    },
    {
      "epoch": 0.8071418888824338,
      "grad_norm": 1.6370500326156616,
      "learning_rate": 2.1989386487186126e-05,
      "loss": 1.2328,
      "step": 10420
    },
    {
      "epoch": 0.8079164972210926,
      "grad_norm": 1.3047211170196533,
      "learning_rate": 2.1981620502200364e-05,
      "loss": 1.1851,
      "step": 10430
    },
    {
      "epoch": 0.8086911055597513,
      "grad_norm": 1.5370608568191528,
      "learning_rate": 2.19738545172146e-05,
      "loss": 1.2475,
      "step": 10440
    },
    {
      "epoch": 0.8094657138984102,
      "grad_norm": 1.3307875394821167,
      "learning_rate": 2.1966088532228836e-05,
      "loss": 1.2451,
      "step": 10450
    },
    {
      "epoch": 0.8102403222370689,
      "grad_norm": 1.180361270904541,
      "learning_rate": 2.1958322547243074e-05,
      "loss": 1.1986,
      "step": 10460
    },
    {
      "epoch": 0.8110149305757276,
      "grad_norm": 1.559493899345398,
      "learning_rate": 2.1950556562257312e-05,
      "loss": 1.1244,
      "step": 10470
    },
    {
      "epoch": 0.8117895389143864,
      "grad_norm": 1.3627151250839233,
      "learning_rate": 2.194279057727155e-05,
      "loss": 1.2377,
      "step": 10480
    },
    {
      "epoch": 0.8125641472530452,
      "grad_norm": 1.4470409154891968,
      "learning_rate": 2.1935024592285788e-05,
      "loss": 1.1988,
      "step": 10490
    },
    {
      "epoch": 0.813338755591704,
      "grad_norm": 2.1112446784973145,
      "learning_rate": 2.192725860730003e-05,
      "loss": 1.2908,
      "step": 10500
    },
    {
      "epoch": 0.8141133639303627,
      "grad_norm": 1.2653876543045044,
      "learning_rate": 2.1919492622314267e-05,
      "loss": 1.1938,
      "step": 10510
    },
    {
      "epoch": 0.8148879722690214,
      "grad_norm": 1.1308754682540894,
      "learning_rate": 2.1911726637328505e-05,
      "loss": 1.1661,
      "step": 10520
    },
    {
      "epoch": 0.8156625806076803,
      "grad_norm": 1.9234524965286255,
      "learning_rate": 2.190396065234274e-05,
      "loss": 1.1174,
      "step": 10530
    },
    {
      "epoch": 0.816437188946339,
      "grad_norm": 1.4502599239349365,
      "learning_rate": 2.1896194667356977e-05,
      "loss": 1.1901,
      "step": 10540
    },
    {
      "epoch": 0.8172117972849978,
      "grad_norm": 1.3650277853012085,
      "learning_rate": 2.1888428682371215e-05,
      "loss": 1.1717,
      "step": 10550
    },
    {
      "epoch": 0.8179864056236565,
      "grad_norm": 1.7141647338867188,
      "learning_rate": 2.1880662697385453e-05,
      "loss": 1.2205,
      "step": 10560
    },
    {
      "epoch": 0.8187610139623153,
      "grad_norm": 1.448870062828064,
      "learning_rate": 2.187289671239969e-05,
      "loss": 1.2069,
      "step": 10570
    },
    {
      "epoch": 0.8195356223009741,
      "grad_norm": 1.3596653938293457,
      "learning_rate": 2.186513072741393e-05,
      "loss": 1.2118,
      "step": 10580
    },
    {
      "epoch": 0.8203102306396328,
      "grad_norm": 1.3422061204910278,
      "learning_rate": 2.1857364742428166e-05,
      "loss": 1.1792,
      "step": 10590
    },
    {
      "epoch": 0.8210848389782917,
      "grad_norm": 2.1304240226745605,
      "learning_rate": 2.1849598757442404e-05,
      "loss": 1.2527,
      "step": 10600
    },
    {
      "epoch": 0.8218594473169504,
      "grad_norm": 1.2989563941955566,
      "learning_rate": 2.184183277245664e-05,
      "loss": 1.2574,
      "step": 10610
    },
    {
      "epoch": 0.8226340556556091,
      "grad_norm": 1.6957740783691406,
      "learning_rate": 2.1834066787470876e-05,
      "loss": 1.1666,
      "step": 10620
    },
    {
      "epoch": 0.8234086639942679,
      "grad_norm": 1.5862313508987427,
      "learning_rate": 2.1826300802485114e-05,
      "loss": 1.249,
      "step": 10630
    },
    {
      "epoch": 0.8241832723329267,
      "grad_norm": 1.4576541185379028,
      "learning_rate": 2.1818534817499352e-05,
      "loss": 1.1971,
      "step": 10640
    },
    {
      "epoch": 0.8249578806715854,
      "grad_norm": 1.5607621669769287,
      "learning_rate": 2.181076883251359e-05,
      "loss": 1.2576,
      "step": 10650
    },
    {
      "epoch": 0.8257324890102442,
      "grad_norm": 1.733086109161377,
      "learning_rate": 2.1803002847527828e-05,
      "loss": 1.1805,
      "step": 10660
    },
    {
      "epoch": 0.8265070973489029,
      "grad_norm": 1.411409854888916,
      "learning_rate": 2.1795236862542066e-05,
      "loss": 1.2643,
      "step": 10670
    },
    {
      "epoch": 0.8272817056875618,
      "grad_norm": 1.4094210863113403,
      "learning_rate": 2.1787470877556304e-05,
      "loss": 1.2013,
      "step": 10680
    },
    {
      "epoch": 0.8280563140262205,
      "grad_norm": 1.114976406097412,
      "learning_rate": 2.177970489257054e-05,
      "loss": 1.2001,
      "step": 10690
    },
    {
      "epoch": 0.8288309223648792,
      "grad_norm": 1.6956713199615479,
      "learning_rate": 2.177193890758478e-05,
      "loss": 1.2396,
      "step": 10700
    },
    {
      "epoch": 0.829605530703538,
      "grad_norm": 1.4120644330978394,
      "learning_rate": 2.1764172922599017e-05,
      "loss": 1.1901,
      "step": 10710
    },
    {
      "epoch": 0.8303801390421968,
      "grad_norm": 1.124030351638794,
      "learning_rate": 2.1756406937613255e-05,
      "loss": 1.3215,
      "step": 10720
    },
    {
      "epoch": 0.8311547473808556,
      "grad_norm": 1.4700446128845215,
      "learning_rate": 2.1748640952627493e-05,
      "loss": 1.1922,
      "step": 10730
    },
    {
      "epoch": 0.8319293557195143,
      "grad_norm": 1.1739556789398193,
      "learning_rate": 2.174087496764173e-05,
      "loss": 1.2187,
      "step": 10740
    },
    {
      "epoch": 0.832703964058173,
      "grad_norm": 1.2031704187393188,
      "learning_rate": 2.173310898265597e-05,
      "loss": 1.3055,
      "step": 10750
    },
    {
      "epoch": 0.8334785723968319,
      "grad_norm": 1.4707485437393188,
      "learning_rate": 2.1725342997670207e-05,
      "loss": 1.1317,
      "step": 10760
    },
    {
      "epoch": 0.8342531807354906,
      "grad_norm": 1.5441395044326782,
      "learning_rate": 2.1717577012684444e-05,
      "loss": 1.1867,
      "step": 10770
    },
    {
      "epoch": 0.8350277890741494,
      "grad_norm": 1.6066279411315918,
      "learning_rate": 2.170981102769868e-05,
      "loss": 1.0912,
      "step": 10780
    },
    {
      "epoch": 0.8358023974128082,
      "grad_norm": 1.5589637756347656,
      "learning_rate": 2.1702045042712917e-05,
      "loss": 1.2176,
      "step": 10790
    },
    {
      "epoch": 0.8365770057514669,
      "grad_norm": 1.2774330377578735,
      "learning_rate": 2.1694279057727155e-05,
      "loss": 1.2168,
      "step": 10800
    },
    {
      "epoch": 0.8373516140901257,
      "grad_norm": 1.2808367013931274,
      "learning_rate": 2.1686513072741392e-05,
      "loss": 1.158,
      "step": 10810
    },
    {
      "epoch": 0.8381262224287844,
      "grad_norm": 1.7047579288482666,
      "learning_rate": 2.167874708775563e-05,
      "loss": 1.1217,
      "step": 10820
    },
    {
      "epoch": 0.8389008307674433,
      "grad_norm": 1.4482247829437256,
      "learning_rate": 2.1670981102769868e-05,
      "loss": 1.266,
      "step": 10830
    },
    {
      "epoch": 0.839675439106102,
      "grad_norm": 1.1486502885818481,
      "learning_rate": 2.1663215117784106e-05,
      "loss": 1.178,
      "step": 10840
    },
    {
      "epoch": 0.8404500474447607,
      "grad_norm": 1.2151505947113037,
      "learning_rate": 2.1655449132798344e-05,
      "loss": 1.2757,
      "step": 10850
    },
    {
      "epoch": 0.8412246557834195,
      "grad_norm": 1.3308517932891846,
      "learning_rate": 2.1647683147812582e-05,
      "loss": 1.2246,
      "step": 10860
    },
    {
      "epoch": 0.8419992641220783,
      "grad_norm": 1.6499543190002441,
      "learning_rate": 2.1639917162826816e-05,
      "loss": 1.1974,
      "step": 10870
    },
    {
      "epoch": 0.8427738724607371,
      "grad_norm": 1.4041966199874878,
      "learning_rate": 2.1632151177841054e-05,
      "loss": 1.2009,
      "step": 10880
    },
    {
      "epoch": 0.8435484807993958,
      "grad_norm": 1.4046273231506348,
      "learning_rate": 2.1624385192855292e-05,
      "loss": 1.1934,
      "step": 10890
    },
    {
      "epoch": 0.8443230891380545,
      "grad_norm": 2.093885898590088,
      "learning_rate": 2.1616619207869533e-05,
      "loss": 1.1577,
      "step": 10900
    },
    {
      "epoch": 0.8450976974767134,
      "grad_norm": 1.108339548110962,
      "learning_rate": 2.160885322288377e-05,
      "loss": 1.1881,
      "step": 10910
    },
    {
      "epoch": 0.8458723058153721,
      "grad_norm": 1.478884220123291,
      "learning_rate": 2.160108723789801e-05,
      "loss": 1.1236,
      "step": 10920
    },
    {
      "epoch": 0.8466469141540308,
      "grad_norm": 1.6159933805465698,
      "learning_rate": 2.1593321252912247e-05,
      "loss": 1.3003,
      "step": 10930
    },
    {
      "epoch": 0.8474215224926896,
      "grad_norm": 1.4784272909164429,
      "learning_rate": 2.1585555267926485e-05,
      "loss": 1.2205,
      "step": 10940
    },
    {
      "epoch": 0.8481961308313484,
      "grad_norm": 1.4778906106948853,
      "learning_rate": 2.157778928294072e-05,
      "loss": 1.2003,
      "step": 10950
    },
    {
      "epoch": 0.8489707391700072,
      "grad_norm": 1.6113550662994385,
      "learning_rate": 2.1570023297954957e-05,
      "loss": 1.3238,
      "step": 10960
    },
    {
      "epoch": 0.8497453475086659,
      "grad_norm": 1.0933024883270264,
      "learning_rate": 2.1562257312969195e-05,
      "loss": 1.2072,
      "step": 10970
    },
    {
      "epoch": 0.8505199558473246,
      "grad_norm": 1.6336926221847534,
      "learning_rate": 2.1554491327983433e-05,
      "loss": 1.3434,
      "step": 10980
    },
    {
      "epoch": 0.8512945641859835,
      "grad_norm": 1.9133892059326172,
      "learning_rate": 2.154672534299767e-05,
      "loss": 1.1505,
      "step": 10990
    },
    {
      "epoch": 0.8520691725246422,
      "grad_norm": 1.1863174438476562,
      "learning_rate": 2.153895935801191e-05,
      "loss": 1.1089,
      "step": 11000
    },
    {
      "epoch": 0.852843780863301,
      "grad_norm": 1.5794495344161987,
      "learning_rate": 2.1531193373026146e-05,
      "loss": 1.3092,
      "step": 11010
    },
    {
      "epoch": 0.8536183892019598,
      "grad_norm": 1.2299593687057495,
      "learning_rate": 2.1523427388040384e-05,
      "loss": 1.2811,
      "step": 11020
    },
    {
      "epoch": 0.8543929975406185,
      "grad_norm": 1.5124777555465698,
      "learning_rate": 2.1515661403054622e-05,
      "loss": 1.182,
      "step": 11030
    },
    {
      "epoch": 0.8551676058792773,
      "grad_norm": 2.332380533218384,
      "learning_rate": 2.1507895418068857e-05,
      "loss": 1.14,
      "step": 11040
    },
    {
      "epoch": 0.855942214217936,
      "grad_norm": 1.8140441179275513,
      "learning_rate": 2.1500129433083094e-05,
      "loss": 1.195,
      "step": 11050
    },
    {
      "epoch": 0.8567168225565949,
      "grad_norm": 1.3362622261047363,
      "learning_rate": 2.1492363448097332e-05,
      "loss": 1.2243,
      "step": 11060
    },
    {
      "epoch": 0.8574914308952536,
      "grad_norm": 1.382735252380371,
      "learning_rate": 2.148459746311157e-05,
      "loss": 1.0924,
      "step": 11070
    },
    {
      "epoch": 0.8582660392339123,
      "grad_norm": 1.4321579933166504,
      "learning_rate": 2.1476831478125808e-05,
      "loss": 1.2277,
      "step": 11080
    },
    {
      "epoch": 0.8590406475725711,
      "grad_norm": 1.5192276239395142,
      "learning_rate": 2.146906549314005e-05,
      "loss": 1.1636,
      "step": 11090
    },
    {
      "epoch": 0.8598152559112299,
      "grad_norm": 1.4626615047454834,
      "learning_rate": 2.1461299508154287e-05,
      "loss": 1.2096,
      "step": 11100
    },
    {
      "epoch": 0.8605898642498887,
      "grad_norm": 1.6282463073730469,
      "learning_rate": 2.1453533523168525e-05,
      "loss": 1.2608,
      "step": 11110
    },
    {
      "epoch": 0.8613644725885474,
      "grad_norm": 1.5142263174057007,
      "learning_rate": 2.144576753818276e-05,
      "loss": 1.1093,
      "step": 11120
    },
    {
      "epoch": 0.8621390809272061,
      "grad_norm": 1.1927416324615479,
      "learning_rate": 2.1438001553196997e-05,
      "loss": 1.2315,
      "step": 11130
    },
    {
      "epoch": 0.862913689265865,
      "grad_norm": 1.573303461074829,
      "learning_rate": 2.1430235568211235e-05,
      "loss": 1.1779,
      "step": 11140
    },
    {
      "epoch": 0.8636882976045237,
      "grad_norm": 2.2927658557891846,
      "learning_rate": 2.1422469583225473e-05,
      "loss": 1.1713,
      "step": 11150
    },
    {
      "epoch": 0.8644629059431824,
      "grad_norm": 2.011254072189331,
      "learning_rate": 2.141470359823971e-05,
      "loss": 1.1194,
      "step": 11160
    },
    {
      "epoch": 0.8652375142818413,
      "grad_norm": 1.3785693645477295,
      "learning_rate": 2.140693761325395e-05,
      "loss": 1.2198,
      "step": 11170
    },
    {
      "epoch": 0.8660121226205,
      "grad_norm": 1.6209888458251953,
      "learning_rate": 2.1399171628268187e-05,
      "loss": 1.2155,
      "step": 11180
    },
    {
      "epoch": 0.8667867309591588,
      "grad_norm": 1.7873353958129883,
      "learning_rate": 2.1391405643282425e-05,
      "loss": 1.146,
      "step": 11190
    },
    {
      "epoch": 0.8675613392978175,
      "grad_norm": 1.6993073225021362,
      "learning_rate": 2.1383639658296662e-05,
      "loss": 1.2401,
      "step": 11200
    },
    {
      "epoch": 0.8683359476364763,
      "grad_norm": 1.5951693058013916,
      "learning_rate": 2.1375873673310897e-05,
      "loss": 1.1868,
      "step": 11210
    },
    {
      "epoch": 0.8691105559751351,
      "grad_norm": 1.4969958066940308,
      "learning_rate": 2.1368107688325135e-05,
      "loss": 1.1389,
      "step": 11220
    },
    {
      "epoch": 0.8698851643137938,
      "grad_norm": 1.4220681190490723,
      "learning_rate": 2.1360341703339373e-05,
      "loss": 1.1226,
      "step": 11230
    },
    {
      "epoch": 0.8706597726524526,
      "grad_norm": 2.0095677375793457,
      "learning_rate": 2.135257571835361e-05,
      "loss": 1.2079,
      "step": 11240
    },
    {
      "epoch": 0.8714343809911114,
      "grad_norm": 1.1183362007141113,
      "learning_rate": 2.134480973336785e-05,
      "loss": 1.0011,
      "step": 11250
    },
    {
      "epoch": 0.8722089893297701,
      "grad_norm": 1.5402719974517822,
      "learning_rate": 2.1337043748382086e-05,
      "loss": 1.2291,
      "step": 11260
    },
    {
      "epoch": 0.8729835976684289,
      "grad_norm": 1.3327440023422241,
      "learning_rate": 2.1329277763396324e-05,
      "loss": 1.1717,
      "step": 11270
    },
    {
      "epoch": 0.8737582060070876,
      "grad_norm": 1.3120795488357544,
      "learning_rate": 2.1321511778410562e-05,
      "loss": 1.1699,
      "step": 11280
    },
    {
      "epoch": 0.8745328143457465,
      "grad_norm": 1.3449113368988037,
      "learning_rate": 2.13137457934248e-05,
      "loss": 1.1986,
      "step": 11290
    },
    {
      "epoch": 0.8753074226844052,
      "grad_norm": 1.4564508199691772,
      "learning_rate": 2.1305979808439038e-05,
      "loss": 1.2493,
      "step": 11300
    },
    {
      "epoch": 0.8760820310230639,
      "grad_norm": 2.2582767009735107,
      "learning_rate": 2.1298213823453276e-05,
      "loss": 1.183,
      "step": 11310
    },
    {
      "epoch": 0.8768566393617228,
      "grad_norm": 1.1239080429077148,
      "learning_rate": 2.1290447838467513e-05,
      "loss": 1.2062,
      "step": 11320
    },
    {
      "epoch": 0.8776312477003815,
      "grad_norm": 1.4707279205322266,
      "learning_rate": 2.128268185348175e-05,
      "loss": 1.2431,
      "step": 11330
    },
    {
      "epoch": 0.8784058560390403,
      "grad_norm": 1.4167896509170532,
      "learning_rate": 2.127491586849599e-05,
      "loss": 1.1971,
      "step": 11340
    },
    {
      "epoch": 0.879180464377699,
      "grad_norm": 1.4306373596191406,
      "learning_rate": 2.1267149883510227e-05,
      "loss": 1.1813,
      "step": 11350
    },
    {
      "epoch": 0.8799550727163578,
      "grad_norm": 1.6387338638305664,
      "learning_rate": 2.1259383898524465e-05,
      "loss": 1.2077,
      "step": 11360
    },
    {
      "epoch": 0.8807296810550166,
      "grad_norm": 1.5568017959594727,
      "learning_rate": 2.1251617913538703e-05,
      "loss": 1.2149,
      "step": 11370
    },
    {
      "epoch": 0.8815042893936753,
      "grad_norm": 1.962249994277954,
      "learning_rate": 2.1243851928552937e-05,
      "loss": 1.2081,
      "step": 11380
    },
    {
      "epoch": 0.882278897732334,
      "grad_norm": 1.618544101715088,
      "learning_rate": 2.1236085943567175e-05,
      "loss": 1.2568,
      "step": 11390
    },
    {
      "epoch": 0.8830535060709929,
      "grad_norm": 1.4098365306854248,
      "learning_rate": 2.1228319958581413e-05,
      "loss": 1.1573,
      "step": 11400
    },
    {
      "epoch": 0.8838281144096516,
      "grad_norm": 2.0709540843963623,
      "learning_rate": 2.122055397359565e-05,
      "loss": 1.2859,
      "step": 11410
    },
    {
      "epoch": 0.8846027227483104,
      "grad_norm": 1.3392789363861084,
      "learning_rate": 2.121278798860989e-05,
      "loss": 1.1586,
      "step": 11420
    },
    {
      "epoch": 0.8853773310869691,
      "grad_norm": 1.295913815498352,
      "learning_rate": 2.1205022003624127e-05,
      "loss": 1.1197,
      "step": 11430
    },
    {
      "epoch": 0.8861519394256279,
      "grad_norm": 1.6514204740524292,
      "learning_rate": 2.1197256018638364e-05,
      "loss": 1.322,
      "step": 11440
    },
    {
      "epoch": 0.8869265477642867,
      "grad_norm": 2.0404484272003174,
      "learning_rate": 2.1189490033652602e-05,
      "loss": 1.2449,
      "step": 11450
    },
    {
      "epoch": 0.8877011561029454,
      "grad_norm": 1.6011106967926025,
      "learning_rate": 2.1181724048666837e-05,
      "loss": 1.2381,
      "step": 11460
    },
    {
      "epoch": 0.8884757644416043,
      "grad_norm": 1.618615746498108,
      "learning_rate": 2.1173958063681075e-05,
      "loss": 1.2081,
      "step": 11470
    },
    {
      "epoch": 0.889250372780263,
      "grad_norm": 1.2714954614639282,
      "learning_rate": 2.1166192078695316e-05,
      "loss": 1.2747,
      "step": 11480
    },
    {
      "epoch": 0.8900249811189217,
      "grad_norm": 1.19551682472229,
      "learning_rate": 2.1158426093709554e-05,
      "loss": 1.2503,
      "step": 11490
    },
    {
      "epoch": 0.8907995894575805,
      "grad_norm": 1.3507438898086548,
      "learning_rate": 2.115066010872379e-05,
      "loss": 1.132,
      "step": 11500
    },
    {
      "epoch": 0.8915741977962393,
      "grad_norm": 1.6043684482574463,
      "learning_rate": 2.114289412373803e-05,
      "loss": 1.2675,
      "step": 11510
    },
    {
      "epoch": 0.8923488061348981,
      "grad_norm": 1.6445714235305786,
      "learning_rate": 2.1135128138752267e-05,
      "loss": 1.0889,
      "step": 11520
    },
    {
      "epoch": 0.8931234144735568,
      "grad_norm": 1.0067360401153564,
      "learning_rate": 2.1127362153766505e-05,
      "loss": 1.1722,
      "step": 11530
    },
    {
      "epoch": 0.8938980228122155,
      "grad_norm": 2.740053653717041,
      "learning_rate": 2.1119596168780743e-05,
      "loss": 1.2894,
      "step": 11540
    },
    {
      "epoch": 0.8946726311508744,
      "grad_norm": 1.5261684656143188,
      "learning_rate": 2.1111830183794978e-05,
      "loss": 1.2737,
      "step": 11550
    },
    {
      "epoch": 0.8954472394895331,
      "grad_norm": 1.4641873836517334,
      "learning_rate": 2.1104064198809215e-05,
      "loss": 1.0997,
      "step": 11560
    },
    {
      "epoch": 0.8962218478281919,
      "grad_norm": 1.4454681873321533,
      "learning_rate": 2.1096298213823453e-05,
      "loss": 1.1734,
      "step": 11570
    },
    {
      "epoch": 0.8969964561668506,
      "grad_norm": 1.5520780086517334,
      "learning_rate": 2.108853222883769e-05,
      "loss": 1.1444,
      "step": 11580
    },
    {
      "epoch": 0.8977710645055094,
      "grad_norm": 1.1998482942581177,
      "learning_rate": 2.108076624385193e-05,
      "loss": 1.1493,
      "step": 11590
    },
    {
      "epoch": 0.8985456728441682,
      "grad_norm": 1.6145122051239014,
      "learning_rate": 2.1073000258866167e-05,
      "loss": 1.153,
      "step": 11600
    },
    {
      "epoch": 0.8993202811828269,
      "grad_norm": 1.3390333652496338,
      "learning_rate": 2.1065234273880405e-05,
      "loss": 1.2374,
      "step": 11610
    },
    {
      "epoch": 0.9000948895214858,
      "grad_norm": 1.4818888902664185,
      "learning_rate": 2.1057468288894643e-05,
      "loss": 1.1601,
      "step": 11620
    },
    {
      "epoch": 0.9008694978601445,
      "grad_norm": 1.7712887525558472,
      "learning_rate": 2.1049702303908877e-05,
      "loss": 1.2492,
      "step": 11630
    },
    {
      "epoch": 0.9016441061988032,
      "grad_norm": 1.2777819633483887,
      "learning_rate": 2.1041936318923115e-05,
      "loss": 1.2463,
      "step": 11640
    },
    {
      "epoch": 0.902418714537462,
      "grad_norm": 1.3074285984039307,
      "learning_rate": 2.1034170333937353e-05,
      "loss": 1.1012,
      "step": 11650
    },
    {
      "epoch": 0.9031933228761208,
      "grad_norm": 1.5438158512115479,
      "learning_rate": 2.102718094745017e-05,
      "loss": 1.1674,
      "step": 11660
    },
    {
      "epoch": 0.9039679312147795,
      "grad_norm": 1.3667693138122559,
      "learning_rate": 2.1019414962464408e-05,
      "loss": 1.1932,
      "step": 11670
    },
    {
      "epoch": 0.9047425395534383,
      "grad_norm": 1.3520753383636475,
      "learning_rate": 2.1011648977478645e-05,
      "loss": 1.1858,
      "step": 11680
    },
    {
      "epoch": 0.905517147892097,
      "grad_norm": 1.444031000137329,
      "learning_rate": 2.1003882992492883e-05,
      "loss": 1.1861,
      "step": 11690
    },
    {
      "epoch": 0.9062917562307559,
      "grad_norm": 1.3320988416671753,
      "learning_rate": 2.099611700750712e-05,
      "loss": 1.2019,
      "step": 11700
    },
    {
      "epoch": 0.9070663645694146,
      "grad_norm": 1.546194314956665,
      "learning_rate": 2.0988351022521356e-05,
      "loss": 1.1897,
      "step": 11710
    },
    {
      "epoch": 0.9078409729080733,
      "grad_norm": 1.3336139917373657,
      "learning_rate": 2.0980585037535594e-05,
      "loss": 1.1622,
      "step": 11720
    },
    {
      "epoch": 0.9086155812467321,
      "grad_norm": 1.7309963703155518,
      "learning_rate": 2.097281905254983e-05,
      "loss": 1.1531,
      "step": 11730
    },
    {
      "epoch": 0.9093901895853909,
      "grad_norm": 1.7357605695724487,
      "learning_rate": 2.096505306756407e-05,
      "loss": 1.1822,
      "step": 11740
    },
    {
      "epoch": 0.9101647979240497,
      "grad_norm": 1.2989332675933838,
      "learning_rate": 2.0957287082578307e-05,
      "loss": 1.1775,
      "step": 11750
    },
    {
      "epoch": 0.9109394062627084,
      "grad_norm": 2.132927417755127,
      "learning_rate": 2.0949521097592545e-05,
      "loss": 1.1265,
      "step": 11760
    },
    {
      "epoch": 0.9117140146013671,
      "grad_norm": 3.450340986251831,
      "learning_rate": 2.0941755112606783e-05,
      "loss": 1.29,
      "step": 11770
    },
    {
      "epoch": 0.912488622940026,
      "grad_norm": 1.5645525455474854,
      "learning_rate": 2.093398912762102e-05,
      "loss": 1.2461,
      "step": 11780
    },
    {
      "epoch": 0.9132632312786847,
      "grad_norm": 1.3964508771896362,
      "learning_rate": 2.0926223142635255e-05,
      "loss": 1.0964,
      "step": 11790
    },
    {
      "epoch": 0.9140378396173435,
      "grad_norm": 1.5508557558059692,
      "learning_rate": 2.0918457157649493e-05,
      "loss": 1.2561,
      "step": 11800
    },
    {
      "epoch": 0.9148124479560023,
      "grad_norm": 1.2648296356201172,
      "learning_rate": 2.091069117266373e-05,
      "loss": 1.1562,
      "step": 11810
    },
    {
      "epoch": 0.915587056294661,
      "grad_norm": 1.2177977561950684,
      "learning_rate": 2.0902925187677972e-05,
      "loss": 1.2475,
      "step": 11820
    },
    {
      "epoch": 0.9163616646333198,
      "grad_norm": 1.4281566143035889,
      "learning_rate": 2.089515920269221e-05,
      "loss": 1.2654,
      "step": 11830
    },
    {
      "epoch": 0.9171362729719785,
      "grad_norm": 1.6973007917404175,
      "learning_rate": 2.0887393217706448e-05,
      "loss": 1.2468,
      "step": 11840
    },
    {
      "epoch": 0.9179108813106374,
      "grad_norm": 1.9143563508987427,
      "learning_rate": 2.0879627232720686e-05,
      "loss": 1.2312,
      "step": 11850
    },
    {
      "epoch": 0.9186854896492961,
      "grad_norm": 1.4128429889678955,
      "learning_rate": 2.0871861247734924e-05,
      "loss": 1.2971,
      "step": 11860
    },
    {
      "epoch": 0.9194600979879548,
      "grad_norm": 1.5052211284637451,
      "learning_rate": 2.086409526274916e-05,
      "loss": 1.2747,
      "step": 11870
    },
    {
      "epoch": 0.9202347063266136,
      "grad_norm": 1.626490592956543,
      "learning_rate": 2.0856329277763396e-05,
      "loss": 1.2292,
      "step": 11880
    },
    {
      "epoch": 0.9210093146652724,
      "grad_norm": 1.2493699789047241,
      "learning_rate": 2.0848563292777634e-05,
      "loss": 1.1664,
      "step": 11890
    },
    {
      "epoch": 0.9217839230039311,
      "grad_norm": 1.4667987823486328,
      "learning_rate": 2.0840797307791872e-05,
      "loss": 1.2095,
      "step": 11900
    },
    {
      "epoch": 0.9225585313425899,
      "grad_norm": 2.179436445236206,
      "learning_rate": 2.083303132280611e-05,
      "loss": 1.0996,
      "step": 11910
    },
    {
      "epoch": 0.9233331396812486,
      "grad_norm": 1.6358065605163574,
      "learning_rate": 2.0825265337820347e-05,
      "loss": 1.1427,
      "step": 11920
    },
    {
      "epoch": 0.9241077480199075,
      "grad_norm": 1.7083572149276733,
      "learning_rate": 2.0817499352834585e-05,
      "loss": 1.1878,
      "step": 11930
    },
    {
      "epoch": 0.9248823563585662,
      "grad_norm": 1.9514977931976318,
      "learning_rate": 2.0809733367848823e-05,
      "loss": 1.1993,
      "step": 11940
    },
    {
      "epoch": 0.9256569646972249,
      "grad_norm": 2.5270392894744873,
      "learning_rate": 2.080196738286306e-05,
      "loss": 1.183,
      "step": 11950
    },
    {
      "epoch": 0.9264315730358837,
      "grad_norm": 2.271475076675415,
      "learning_rate": 2.0794201397877296e-05,
      "loss": 1.1723,
      "step": 11960
    },
    {
      "epoch": 0.9272061813745425,
      "grad_norm": 1.1848511695861816,
      "learning_rate": 2.0786435412891533e-05,
      "loss": 1.2545,
      "step": 11970
    },
    {
      "epoch": 0.9279807897132013,
      "grad_norm": 1.349149227142334,
      "learning_rate": 2.077866942790577e-05,
      "loss": 1.1939,
      "step": 11980
    },
    {
      "epoch": 0.92875539805186,
      "grad_norm": 1.2393794059753418,
      "learning_rate": 2.077090344292001e-05,
      "loss": 1.1967,
      "step": 11990
    },
    {
      "epoch": 0.9295300063905187,
      "grad_norm": 1.3668323755264282,
      "learning_rate": 2.0763137457934247e-05,
      "loss": 1.1941,
      "step": 12000
    },
    {
      "epoch": 0.9303046147291776,
      "grad_norm": 1.383750557899475,
      "learning_rate": 2.0755371472948488e-05,
      "loss": 1.2166,
      "step": 12010
    },
    {
      "epoch": 0.9310792230678363,
      "grad_norm": 2.021426200866699,
      "learning_rate": 2.0747605487962726e-05,
      "loss": 1.1941,
      "step": 12020
    },
    {
      "epoch": 0.9318538314064951,
      "grad_norm": 1.980490803718567,
      "learning_rate": 2.0739839502976964e-05,
      "loss": 1.1413,
      "step": 12030
    },
    {
      "epoch": 0.9326284397451539,
      "grad_norm": 1.7307875156402588,
      "learning_rate": 2.0732073517991202e-05,
      "loss": 1.2113,
      "step": 12040
    },
    {
      "epoch": 0.9334030480838126,
      "grad_norm": 1.5007153749465942,
      "learning_rate": 2.0724307533005436e-05,
      "loss": 1.0956,
      "step": 12050
    },
    {
      "epoch": 0.9341776564224714,
      "grad_norm": 1.3064790964126587,
      "learning_rate": 2.0716541548019674e-05,
      "loss": 1.1421,
      "step": 12060
    },
    {
      "epoch": 0.9349522647611301,
      "grad_norm": 1.6436666250228882,
      "learning_rate": 2.0708775563033912e-05,
      "loss": 1.2036,
      "step": 12070
    },
    {
      "epoch": 0.935726873099789,
      "grad_norm": 1.5193737745285034,
      "learning_rate": 2.070100957804815e-05,
      "loss": 1.2371,
      "step": 12080
    },
    {
      "epoch": 0.9365014814384477,
      "grad_norm": 1.6529942750930786,
      "learning_rate": 2.0693243593062388e-05,
      "loss": 1.2135,
      "step": 12090
    },
    {
      "epoch": 0.9372760897771064,
      "grad_norm": 1.4804574251174927,
      "learning_rate": 2.0685477608076626e-05,
      "loss": 1.1974,
      "step": 12100
    },
    {
      "epoch": 0.9380506981157652,
      "grad_norm": 1.646767258644104,
      "learning_rate": 2.0677711623090864e-05,
      "loss": 1.2196,
      "step": 12110
    },
    {
      "epoch": 0.938825306454424,
      "grad_norm": 1.1759624481201172,
      "learning_rate": 2.06699456381051e-05,
      "loss": 1.128,
      "step": 12120
    },
    {
      "epoch": 0.9395999147930828,
      "grad_norm": 1.541172981262207,
      "learning_rate": 2.0662179653119336e-05,
      "loss": 1.1328,
      "step": 12130
    },
    {
      "epoch": 0.9403745231317415,
      "grad_norm": 1.252039909362793,
      "learning_rate": 2.0654413668133574e-05,
      "loss": 1.1939,
      "step": 12140
    },
    {
      "epoch": 0.9411491314704002,
      "grad_norm": 1.514426827430725,
      "learning_rate": 2.064664768314781e-05,
      "loss": 1.1743,
      "step": 12150
    },
    {
      "epoch": 0.9419237398090591,
      "grad_norm": 2.43497633934021,
      "learning_rate": 2.063888169816205e-05,
      "loss": 1.2582,
      "step": 12160
    },
    {
      "epoch": 0.9426983481477178,
      "grad_norm": 1.2878484725952148,
      "learning_rate": 2.0631115713176287e-05,
      "loss": 1.1358,
      "step": 12170
    },
    {
      "epoch": 0.9434729564863765,
      "grad_norm": 1.679251790046692,
      "learning_rate": 2.0623349728190525e-05,
      "loss": 1.1207,
      "step": 12180
    },
    {
      "epoch": 0.9442475648250354,
      "grad_norm": 1.9125351905822754,
      "learning_rate": 2.0615583743204763e-05,
      "loss": 1.2033,
      "step": 12190
    },
    {
      "epoch": 0.9450221731636941,
      "grad_norm": 1.218144416809082,
      "learning_rate": 2.0607817758219e-05,
      "loss": 1.1274,
      "step": 12200
    },
    {
      "epoch": 0.9457967815023529,
      "grad_norm": 1.299854040145874,
      "learning_rate": 2.0600051773233242e-05,
      "loss": 1.1961,
      "step": 12210
    },
    {
      "epoch": 0.9465713898410116,
      "grad_norm": 1.9275989532470703,
      "learning_rate": 2.0592285788247477e-05,
      "loss": 1.1866,
      "step": 12220
    },
    {
      "epoch": 0.9473459981796704,
      "grad_norm": 1.4279401302337646,
      "learning_rate": 2.0584519803261715e-05,
      "loss": 1.1176,
      "step": 12230
    },
    {
      "epoch": 0.9481206065183292,
      "grad_norm": 1.2796670198440552,
      "learning_rate": 2.0576753818275952e-05,
      "loss": 1.1568,
      "step": 12240
    },
    {
      "epoch": 0.9488952148569879,
      "grad_norm": 1.438808798789978,
      "learning_rate": 2.056898783329019e-05,
      "loss": 1.2637,
      "step": 12250
    },
    {
      "epoch": 0.9496698231956467,
      "grad_norm": 1.6817187070846558,
      "learning_rate": 2.0561221848304428e-05,
      "loss": 1.1371,
      "step": 12260
    },
    {
      "epoch": 0.9504444315343055,
      "grad_norm": 1.4022917747497559,
      "learning_rate": 2.0553455863318666e-05,
      "loss": 1.2826,
      "step": 12270
    },
    {
      "epoch": 0.9512190398729642,
      "grad_norm": 1.5064574480056763,
      "learning_rate": 2.0545689878332904e-05,
      "loss": 1.1595,
      "step": 12280
    },
    {
      "epoch": 0.951993648211623,
      "grad_norm": 1.4940733909606934,
      "learning_rate": 2.0537923893347142e-05,
      "loss": 1.1514,
      "step": 12290
    },
    {
      "epoch": 0.9527682565502817,
      "grad_norm": 1.5027906894683838,
      "learning_rate": 2.0530157908361376e-05,
      "loss": 1.1854,
      "step": 12300
    },
    {
      "epoch": 0.9535428648889406,
      "grad_norm": 1.508583664894104,
      "learning_rate": 2.0522391923375614e-05,
      "loss": 1.2293,
      "step": 12310
    },
    {
      "epoch": 0.9543174732275993,
      "grad_norm": 1.2993619441986084,
      "learning_rate": 2.0514625938389852e-05,
      "loss": 1.2684,
      "step": 12320
    },
    {
      "epoch": 0.955092081566258,
      "grad_norm": 1.4143556356430054,
      "learning_rate": 2.050685995340409e-05,
      "loss": 1.1403,
      "step": 12330
    },
    {
      "epoch": 0.9558666899049169,
      "grad_norm": 1.6929678916931152,
      "learning_rate": 2.0499093968418328e-05,
      "loss": 1.0315,
      "step": 12340
    },
    {
      "epoch": 0.9566412982435756,
      "grad_norm": 1.596621036529541,
      "learning_rate": 2.0491327983432566e-05,
      "loss": 1.2591,
      "step": 12350
    },
    {
      "epoch": 0.9574159065822344,
      "grad_norm": 1.3667367696762085,
      "learning_rate": 2.0483561998446803e-05,
      "loss": 1.2701,
      "step": 12360
    },
    {
      "epoch": 0.9581905149208931,
      "grad_norm": 1.369799256324768,
      "learning_rate": 2.047579601346104e-05,
      "loss": 1.2071,
      "step": 12370
    },
    {
      "epoch": 0.9589651232595519,
      "grad_norm": 1.4096235036849976,
      "learning_rate": 2.046803002847528e-05,
      "loss": 1.2398,
      "step": 12380
    },
    {
      "epoch": 0.9597397315982107,
      "grad_norm": 1.4070137739181519,
      "learning_rate": 2.0460264043489514e-05,
      "loss": 1.1653,
      "step": 12390
    },
    {
      "epoch": 0.9605143399368694,
      "grad_norm": 1.304322600364685,
      "learning_rate": 2.045249805850375e-05,
      "loss": 1.2452,
      "step": 12400
    },
    {
      "epoch": 0.9612889482755281,
      "grad_norm": 1.380059003829956,
      "learning_rate": 2.0444732073517993e-05,
      "loss": 1.1523,
      "step": 12410
    },
    {
      "epoch": 0.962063556614187,
      "grad_norm": 1.5143818855285645,
      "learning_rate": 2.043696608853223e-05,
      "loss": 1.1486,
      "step": 12420
    },
    {
      "epoch": 0.9628381649528457,
      "grad_norm": 1.7320390939712524,
      "learning_rate": 2.042920010354647e-05,
      "loss": 1.1816,
      "step": 12430
    },
    {
      "epoch": 0.9636127732915045,
      "grad_norm": 1.20899498462677,
      "learning_rate": 2.0421434118560706e-05,
      "loss": 1.2366,
      "step": 12440
    },
    {
      "epoch": 0.9643873816301632,
      "grad_norm": 2.1041712760925293,
      "learning_rate": 2.0413668133574944e-05,
      "loss": 1.2026,
      "step": 12450
    },
    {
      "epoch": 0.965161989968822,
      "grad_norm": 1.319044828414917,
      "learning_rate": 2.0405902148589182e-05,
      "loss": 1.2415,
      "step": 12460
    },
    {
      "epoch": 0.9659365983074808,
      "grad_norm": 1.5837429761886597,
      "learning_rate": 2.0398136163603417e-05,
      "loss": 1.1511,
      "step": 12470
    },
    {
      "epoch": 0.9667112066461395,
      "grad_norm": 1.4273113012313843,
      "learning_rate": 2.0390370178617654e-05,
      "loss": 1.2552,
      "step": 12480
    },
    {
      "epoch": 0.9674858149847984,
      "grad_norm": 1.7085044384002686,
      "learning_rate": 2.0382604193631892e-05,
      "loss": 1.1078,
      "step": 12490
    },
    {
      "epoch": 0.9682604233234571,
      "grad_norm": 1.5415927171707153,
      "learning_rate": 2.037483820864613e-05,
      "loss": 1.0947,
      "step": 12500
    },
    {
      "epoch": 0.9690350316621158,
      "grad_norm": 2.125840425491333,
      "learning_rate": 2.0367072223660368e-05,
      "loss": 1.1506,
      "step": 12510
    },
    {
      "epoch": 0.9698096400007746,
      "grad_norm": 1.4554204940795898,
      "learning_rate": 2.0359306238674606e-05,
      "loss": 1.1704,
      "step": 12520
    },
    {
      "epoch": 0.9705842483394334,
      "grad_norm": 2.052753448486328,
      "learning_rate": 2.0351540253688844e-05,
      "loss": 1.233,
      "step": 12530
    },
    {
      "epoch": 0.9713588566780922,
      "grad_norm": 1.429398775100708,
      "learning_rate": 2.034377426870308e-05,
      "loss": 1.1155,
      "step": 12540
    },
    {
      "epoch": 0.9721334650167509,
      "grad_norm": 1.608000636100769,
      "learning_rate": 2.033600828371732e-05,
      "loss": 1.2546,
      "step": 12550
    },
    {
      "epoch": 0.9729080733554096,
      "grad_norm": 1.5308033227920532,
      "learning_rate": 2.0328242298731554e-05,
      "loss": 1.0727,
      "step": 12560
    },
    {
      "epoch": 0.9736826816940685,
      "grad_norm": 1.9211699962615967,
      "learning_rate": 2.0320476313745792e-05,
      "loss": 1.2194,
      "step": 12570
    },
    {
      "epoch": 0.9744572900327272,
      "grad_norm": 1.3390182256698608,
      "learning_rate": 2.031271032876003e-05,
      "loss": 1.1826,
      "step": 12580
    },
    {
      "epoch": 0.975231898371386,
      "grad_norm": 1.2902740240097046,
      "learning_rate": 2.0304944343774268e-05,
      "loss": 1.1846,
      "step": 12590
    },
    {
      "epoch": 0.9760065067100447,
      "grad_norm": 1.371085524559021,
      "learning_rate": 2.029717835878851e-05,
      "loss": 1.2196,
      "step": 12600
    },
    {
      "epoch": 0.9767811150487035,
      "grad_norm": 1.709421157836914,
      "learning_rate": 2.0289412373802747e-05,
      "loss": 1.2228,
      "step": 12610
    },
    {
      "epoch": 0.9775557233873623,
      "grad_norm": 1.9432896375656128,
      "learning_rate": 2.0281646388816985e-05,
      "loss": 1.1395,
      "step": 12620
    },
    {
      "epoch": 0.978330331726021,
      "grad_norm": 1.4801775217056274,
      "learning_rate": 2.0273880403831222e-05,
      "loss": 1.2068,
      "step": 12630
    },
    {
      "epoch": 0.9791049400646799,
      "grad_norm": 1.3414239883422852,
      "learning_rate": 2.0266114418845457e-05,
      "loss": 1.1691,
      "step": 12640
    },
    {
      "epoch": 0.9798795484033386,
      "grad_norm": 1.326912760734558,
      "learning_rate": 2.0258348433859695e-05,
      "loss": 1.227,
      "step": 12650
    },
    {
      "epoch": 0.9806541567419973,
      "grad_norm": 1.371048092842102,
      "learning_rate": 2.0250582448873933e-05,
      "loss": 1.3454,
      "step": 12660
    },
    {
      "epoch": 0.9814287650806561,
      "grad_norm": 1.7067525386810303,
      "learning_rate": 2.024281646388817e-05,
      "loss": 1.1367,
      "step": 12670
    },
    {
      "epoch": 0.9822033734193149,
      "grad_norm": 1.6768896579742432,
      "learning_rate": 2.023505047890241e-05,
      "loss": 1.2203,
      "step": 12680
    },
    {
      "epoch": 0.9829779817579736,
      "grad_norm": 1.4108070135116577,
      "learning_rate": 2.0227284493916646e-05,
      "loss": 1.1537,
      "step": 12690
    },
    {
      "epoch": 0.9837525900966324,
      "grad_norm": 1.8949261903762817,
      "learning_rate": 2.0219518508930884e-05,
      "loss": 1.231,
      "step": 12700
    },
    {
      "epoch": 0.9845271984352911,
      "grad_norm": 1.4425214529037476,
      "learning_rate": 2.0211752523945122e-05,
      "loss": 1.1553,
      "step": 12710
    },
    {
      "epoch": 0.98530180677395,
      "grad_norm": 1.4047328233718872,
      "learning_rate": 2.020398653895936e-05,
      "loss": 1.1783,
      "step": 12720
    },
    {
      "epoch": 0.9860764151126087,
      "grad_norm": 1.5722469091415405,
      "learning_rate": 2.0196220553973594e-05,
      "loss": 1.2034,
      "step": 12730
    },
    {
      "epoch": 0.9868510234512674,
      "grad_norm": 1.3304789066314697,
      "learning_rate": 2.0188454568987832e-05,
      "loss": 1.1188,
      "step": 12740
    },
    {
      "epoch": 0.9876256317899262,
      "grad_norm": 1.1215381622314453,
      "learning_rate": 2.018068858400207e-05,
      "loss": 1.1872,
      "step": 12750
    },
    {
      "epoch": 0.988400240128585,
      "grad_norm": 1.2090470790863037,
      "learning_rate": 2.0172922599016308e-05,
      "loss": 1.2337,
      "step": 12760
    },
    {
      "epoch": 0.9891748484672438,
      "grad_norm": 1.267317771911621,
      "learning_rate": 2.0165156614030546e-05,
      "loss": 1.1515,
      "step": 12770
    },
    {
      "epoch": 0.9899494568059025,
      "grad_norm": 1.4963116645812988,
      "learning_rate": 2.0157390629044784e-05,
      "loss": 1.3187,
      "step": 12780
    },
    {
      "epoch": 0.9907240651445612,
      "grad_norm": 1.1222890615463257,
      "learning_rate": 2.014962464405902e-05,
      "loss": 1.1772,
      "step": 12790
    },
    {
      "epoch": 0.9914986734832201,
      "grad_norm": 1.414851188659668,
      "learning_rate": 2.0141858659073263e-05,
      "loss": 1.1116,
      "step": 12800
    },
    {
      "epoch": 0.9922732818218788,
      "grad_norm": 1.5894538164138794,
      "learning_rate": 2.0134092674087497e-05,
      "loss": 1.2319,
      "step": 12810
    },
    {
      "epoch": 0.9930478901605376,
      "grad_norm": 1.3242771625518799,
      "learning_rate": 2.0126326689101735e-05,
      "loss": 1.1452,
      "step": 12820
    },
    {
      "epoch": 0.9938224984991963,
      "grad_norm": 1.5829336643218994,
      "learning_rate": 2.0118560704115973e-05,
      "loss": 1.1955,
      "step": 12830
    },
    {
      "epoch": 0.9945971068378551,
      "grad_norm": 1.1931767463684082,
      "learning_rate": 2.011079471913021e-05,
      "loss": 1.0833,
      "step": 12840
    },
    {
      "epoch": 0.9953717151765139,
      "grad_norm": 1.3627358675003052,
      "learning_rate": 2.010302873414445e-05,
      "loss": 1.1997,
      "step": 12850
    },
    {
      "epoch": 0.9961463235151726,
      "grad_norm": 1.963989019393921,
      "learning_rate": 2.0095262749158687e-05,
      "loss": 1.1906,
      "step": 12860
    },
    {
      "epoch": 0.9969209318538315,
      "grad_norm": 1.484774112701416,
      "learning_rate": 2.0087496764172924e-05,
      "loss": 1.1741,
      "step": 12870
    },
    {
      "epoch": 0.9976955401924902,
      "grad_norm": 1.7802003622055054,
      "learning_rate": 2.0079730779187162e-05,
      "loss": 1.2304,
      "step": 12880
    },
    {
      "epoch": 0.9984701485311489,
      "grad_norm": 1.6541599035263062,
      "learning_rate": 2.0071964794201397e-05,
      "loss": 1.1752,
      "step": 12890
    },
    {
      "epoch": 0.9992447568698077,
      "grad_norm": 1.4689885377883911,
      "learning_rate": 2.0064198809215635e-05,
      "loss": 1.2008,
      "step": 12900
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.3419297933578491,
      "learning_rate": 2.0056432824229872e-05,
      "loss": 1.1073,
      "step": 12910
    },
    {
      "epoch": 1.0007746083386588,
      "grad_norm": 1.3106478452682495,
      "learning_rate": 2.004866683924411e-05,
      "loss": 1.2464,
      "step": 12920
    },
    {
      "epoch": 1.0015492166773174,
      "grad_norm": 2.2620997428894043,
      "learning_rate": 2.0040900854258348e-05,
      "loss": 1.2359,
      "step": 12930
    },
    {
      "epoch": 1.0023238250159763,
      "grad_norm": 1.4082767963409424,
      "learning_rate": 2.0033134869272586e-05,
      "loss": 1.2067,
      "step": 12940
    },
    {
      "epoch": 1.003098433354635,
      "grad_norm": 1.3339933156967163,
      "learning_rate": 2.0025368884286824e-05,
      "loss": 1.1326,
      "step": 12950
    },
    {
      "epoch": 1.0038730416932937,
      "grad_norm": 1.4973926544189453,
      "learning_rate": 2.0017602899301062e-05,
      "loss": 1.2052,
      "step": 12960
    },
    {
      "epoch": 1.0046476500319526,
      "grad_norm": 1.5039682388305664,
      "learning_rate": 2.00098369143153e-05,
      "loss": 1.2185,
      "step": 12970
    },
    {
      "epoch": 1.0054222583706114,
      "grad_norm": 1.3011853694915771,
      "learning_rate": 2.0002070929329534e-05,
      "loss": 1.2559,
      "step": 12980
    },
    {
      "epoch": 1.0061968667092702,
      "grad_norm": 2.018913507461548,
      "learning_rate": 1.9994304944343775e-05,
      "loss": 1.1684,
      "step": 12990
    },
    {
      "epoch": 1.0069714750479288,
      "grad_norm": 1.5514253377914429,
      "learning_rate": 1.9986538959358013e-05,
      "loss": 1.176,
      "step": 13000
    },
    {
      "epoch": 1.0077460833865877,
      "grad_norm": 1.4294573068618774,
      "learning_rate": 1.997877297437225e-05,
      "loss": 1.256,
      "step": 13010
    },
    {
      "epoch": 1.0085206917252465,
      "grad_norm": 1.1986889839172363,
      "learning_rate": 1.997100698938649e-05,
      "loss": 1.1786,
      "step": 13020
    },
    {
      "epoch": 1.009295300063905,
      "grad_norm": 2.203148126602173,
      "learning_rate": 1.9963241004400727e-05,
      "loss": 1.1135,
      "step": 13030
    },
    {
      "epoch": 1.010069908402564,
      "grad_norm": 1.410215973854065,
      "learning_rate": 1.9955475019414965e-05,
      "loss": 1.1614,
      "step": 13040
    },
    {
      "epoch": 1.0108445167412228,
      "grad_norm": 1.3683428764343262,
      "learning_rate": 1.9947709034429203e-05,
      "loss": 1.2606,
      "step": 13050
    },
    {
      "epoch": 1.0116191250798814,
      "grad_norm": 1.4155491590499878,
      "learning_rate": 1.9939943049443437e-05,
      "loss": 1.2049,
      "step": 13060
    },
    {
      "epoch": 1.0123937334185402,
      "grad_norm": 1.6072123050689697,
      "learning_rate": 1.9932177064457675e-05,
      "loss": 1.1387,
      "step": 13070
    },
    {
      "epoch": 1.013168341757199,
      "grad_norm": 2.2106964588165283,
      "learning_rate": 1.9924411079471913e-05,
      "loss": 1.1774,
      "step": 13080
    },
    {
      "epoch": 1.0139429500958579,
      "grad_norm": 1.5117603540420532,
      "learning_rate": 1.991664509448615e-05,
      "loss": 1.1966,
      "step": 13090
    },
    {
      "epoch": 1.0147175584345165,
      "grad_norm": 1.4223461151123047,
      "learning_rate": 1.990887910950039e-05,
      "loss": 1.0938,
      "step": 13100
    },
    {
      "epoch": 1.0154921667731753,
      "grad_norm": 1.2062127590179443,
      "learning_rate": 1.9901113124514626e-05,
      "loss": 1.1095,
      "step": 13110
    },
    {
      "epoch": 1.0162667751118342,
      "grad_norm": 1.790235996246338,
      "learning_rate": 1.9893347139528864e-05,
      "loss": 1.1778,
      "step": 13120
    },
    {
      "epoch": 1.0170413834504928,
      "grad_norm": 1.5914021730422974,
      "learning_rate": 1.9885581154543102e-05,
      "loss": 1.1917,
      "step": 13130
    },
    {
      "epoch": 1.0178159917891516,
      "grad_norm": 1.2807724475860596,
      "learning_rate": 1.987781516955734e-05,
      "loss": 1.1843,
      "step": 13140
    },
    {
      "epoch": 1.0185906001278104,
      "grad_norm": 1.442615032196045,
      "learning_rate": 1.9870049184571574e-05,
      "loss": 1.1983,
      "step": 13150
    },
    {
      "epoch": 1.019365208466469,
      "grad_norm": 1.7134944200515747,
      "learning_rate": 1.9862283199585812e-05,
      "loss": 1.183,
      "step": 13160
    },
    {
      "epoch": 1.0201398168051279,
      "grad_norm": 1.74130117893219,
      "learning_rate": 1.985451721460005e-05,
      "loss": 1.1893,
      "step": 13170
    },
    {
      "epoch": 1.0209144251437867,
      "grad_norm": 1.6064929962158203,
      "learning_rate": 1.9846751229614288e-05,
      "loss": 1.0901,
      "step": 13180
    },
    {
      "epoch": 1.0216890334824453,
      "grad_norm": 1.402861475944519,
      "learning_rate": 1.983898524462853e-05,
      "loss": 1.1664,
      "step": 13190
    },
    {
      "epoch": 1.0224636418211042,
      "grad_norm": 1.581162691116333,
      "learning_rate": 1.9831219259642767e-05,
      "loss": 1.211,
      "step": 13200
    },
    {
      "epoch": 1.023238250159763,
      "grad_norm": 1.6931008100509644,
      "learning_rate": 1.9823453274657005e-05,
      "loss": 1.1536,
      "step": 13210
    },
    {
      "epoch": 1.0240128584984218,
      "grad_norm": 1.4011040925979614,
      "learning_rate": 1.9815687289671243e-05,
      "loss": 1.1926,
      "step": 13220
    },
    {
      "epoch": 1.0247874668370804,
      "grad_norm": 1.3516225814819336,
      "learning_rate": 1.9807921304685477e-05,
      "loss": 1.0732,
      "step": 13230
    },
    {
      "epoch": 1.0255620751757393,
      "grad_norm": 1.4261198043823242,
      "learning_rate": 1.9800155319699715e-05,
      "loss": 1.1679,
      "step": 13240
    },
    {
      "epoch": 1.026336683514398,
      "grad_norm": 1.558476448059082,
      "learning_rate": 1.9792389334713953e-05,
      "loss": 1.2005,
      "step": 13250
    },
    {
      "epoch": 1.0271112918530567,
      "grad_norm": 1.3190491199493408,
      "learning_rate": 1.978462334972819e-05,
      "loss": 1.3649,
      "step": 13260
    },
    {
      "epoch": 1.0278859001917156,
      "grad_norm": 1.5845118761062622,
      "learning_rate": 1.977685736474243e-05,
      "loss": 1.1109,
      "step": 13270
    },
    {
      "epoch": 1.0286605085303744,
      "grad_norm": 1.5166162252426147,
      "learning_rate": 1.9769091379756667e-05,
      "loss": 1.2009,
      "step": 13280
    },
    {
      "epoch": 1.029435116869033,
      "grad_norm": 1.5385262966156006,
      "learning_rate": 1.9761325394770905e-05,
      "loss": 1.2081,
      "step": 13290
    },
    {
      "epoch": 1.0302097252076918,
      "grad_norm": 1.6554582118988037,
      "learning_rate": 1.9753559409785142e-05,
      "loss": 1.3327,
      "step": 13300
    },
    {
      "epoch": 1.0309843335463507,
      "grad_norm": 1.657751202583313,
      "learning_rate": 1.974579342479938e-05,
      "loss": 1.3108,
      "step": 13310
    },
    {
      "epoch": 1.0317589418850095,
      "grad_norm": 1.3607538938522339,
      "learning_rate": 1.9738027439813615e-05,
      "loss": 1.1763,
      "step": 13320
    },
    {
      "epoch": 1.032533550223668,
      "grad_norm": 1.0984764099121094,
      "learning_rate": 1.9730261454827853e-05,
      "loss": 1.2795,
      "step": 13330
    },
    {
      "epoch": 1.033308158562327,
      "grad_norm": 1.4956296682357788,
      "learning_rate": 1.972249546984209e-05,
      "loss": 1.2142,
      "step": 13340
    },
    {
      "epoch": 1.0340827669009858,
      "grad_norm": 1.4545574188232422,
      "learning_rate": 1.971472948485633e-05,
      "loss": 1.2119,
      "step": 13350
    },
    {
      "epoch": 1.0348573752396444,
      "grad_norm": 1.6420444250106812,
      "learning_rate": 1.9706963499870566e-05,
      "loss": 1.1062,
      "step": 13360
    },
    {
      "epoch": 1.0356319835783032,
      "grad_norm": 1.3971948623657227,
      "learning_rate": 1.9699197514884804e-05,
      "loss": 1.1598,
      "step": 13370
    },
    {
      "epoch": 1.036406591916962,
      "grad_norm": 1.45657479763031,
      "learning_rate": 1.9691431529899042e-05,
      "loss": 1.231,
      "step": 13380
    },
    {
      "epoch": 1.0371812002556207,
      "grad_norm": 1.7680765390396118,
      "learning_rate": 1.9683665544913283e-05,
      "loss": 1.1656,
      "step": 13390
    },
    {
      "epoch": 1.0379558085942795,
      "grad_norm": 1.6069403886795044,
      "learning_rate": 1.9675899559927518e-05,
      "loss": 1.156,
      "step": 13400
    },
    {
      "epoch": 1.0387304169329383,
      "grad_norm": 1.3297169208526611,
      "learning_rate": 1.9668133574941756e-05,
      "loss": 1.2038,
      "step": 13410
    },
    {
      "epoch": 1.039505025271597,
      "grad_norm": 1.1517072916030884,
      "learning_rate": 1.9660367589955993e-05,
      "loss": 1.2058,
      "step": 13420
    },
    {
      "epoch": 1.0402796336102558,
      "grad_norm": 2.1718592643737793,
      "learning_rate": 1.965260160497023e-05,
      "loss": 1.2006,
      "step": 13430
    },
    {
      "epoch": 1.0410542419489146,
      "grad_norm": 1.6509960889816284,
      "learning_rate": 1.964483561998447e-05,
      "loss": 1.1794,
      "step": 13440
    },
    {
      "epoch": 1.0418288502875734,
      "grad_norm": 1.4064439535140991,
      "learning_rate": 1.9637069634998707e-05,
      "loss": 1.1032,
      "step": 13450
    },
    {
      "epoch": 1.042603458626232,
      "grad_norm": 2.583437919616699,
      "learning_rate": 1.9629303650012945e-05,
      "loss": 1.1521,
      "step": 13460
    },
    {
      "epoch": 1.0433780669648909,
      "grad_norm": 1.286333441734314,
      "learning_rate": 1.9621537665027183e-05,
      "loss": 1.1477,
      "step": 13470
    },
    {
      "epoch": 1.0441526753035497,
      "grad_norm": 2.1480822563171387,
      "learning_rate": 1.961377168004142e-05,
      "loss": 1.1387,
      "step": 13480
    },
    {
      "epoch": 1.0449272836422083,
      "grad_norm": 2.011129140853882,
      "learning_rate": 1.9606005695055655e-05,
      "loss": 1.1636,
      "step": 13490
    },
    {
      "epoch": 1.0457018919808672,
      "grad_norm": 1.9520841836929321,
      "learning_rate": 1.9598239710069893e-05,
      "loss": 1.1981,
      "step": 13500
    },
    {
      "epoch": 1.046476500319526,
      "grad_norm": 1.3744854927062988,
      "learning_rate": 1.959047372508413e-05,
      "loss": 1.2051,
      "step": 13510
    },
    {
      "epoch": 1.0472511086581846,
      "grad_norm": 1.4589979648590088,
      "learning_rate": 1.958270774009837e-05,
      "loss": 1.206,
      "step": 13520
    },
    {
      "epoch": 1.0480257169968434,
      "grad_norm": 2.3053159713745117,
      "learning_rate": 1.9574941755112607e-05,
      "loss": 1.1816,
      "step": 13530
    },
    {
      "epoch": 1.0488003253355023,
      "grad_norm": 1.848618745803833,
      "learning_rate": 1.9567175770126844e-05,
      "loss": 1.2258,
      "step": 13540
    },
    {
      "epoch": 1.049574933674161,
      "grad_norm": 1.2628049850463867,
      "learning_rate": 1.9559409785141082e-05,
      "loss": 1.1954,
      "step": 13550
    },
    {
      "epoch": 1.0503495420128197,
      "grad_norm": 2.290081024169922,
      "learning_rate": 1.955164380015532e-05,
      "loss": 1.1073,
      "step": 13560
    },
    {
      "epoch": 1.0511241503514785,
      "grad_norm": 1.367095947265625,
      "learning_rate": 1.9543877815169555e-05,
      "loss": 1.1705,
      "step": 13570
    },
    {
      "epoch": 1.0518987586901374,
      "grad_norm": 1.6403405666351318,
      "learning_rate": 1.9536111830183796e-05,
      "loss": 1.2701,
      "step": 13580
    },
    {
      "epoch": 1.052673367028796,
      "grad_norm": 1.6059391498565674,
      "learning_rate": 1.9528345845198034e-05,
      "loss": 1.1974,
      "step": 13590
    },
    {
      "epoch": 1.0534479753674548,
      "grad_norm": 1.2867414951324463,
      "learning_rate": 1.952057986021227e-05,
      "loss": 1.1585,
      "step": 13600
    },
    {
      "epoch": 1.0542225837061137,
      "grad_norm": 1.5146726369857788,
      "learning_rate": 1.951281387522651e-05,
      "loss": 1.2201,
      "step": 13610
    },
    {
      "epoch": 1.0549971920447723,
      "grad_norm": 1.4179044961929321,
      "learning_rate": 1.9505047890240747e-05,
      "loss": 1.1652,
      "step": 13620
    },
    {
      "epoch": 1.055771800383431,
      "grad_norm": 1.6117734909057617,
      "learning_rate": 1.9497281905254985e-05,
      "loss": 1.196,
      "step": 13630
    },
    {
      "epoch": 1.05654640872209,
      "grad_norm": 2.0245909690856934,
      "learning_rate": 1.9489515920269223e-05,
      "loss": 1.2079,
      "step": 13640
    },
    {
      "epoch": 1.0573210170607488,
      "grad_norm": 1.4210002422332764,
      "learning_rate": 1.948174993528346e-05,
      "loss": 1.1919,
      "step": 13650
    },
    {
      "epoch": 1.0580956253994074,
      "grad_norm": 1.5182905197143555,
      "learning_rate": 1.9473983950297695e-05,
      "loss": 1.1724,
      "step": 13660
    },
    {
      "epoch": 1.0588702337380662,
      "grad_norm": NaN,
      "learning_rate": 1.9466217965311933e-05,
      "loss": 1.2155,
      "step": 13670
    },
    {
      "epoch": 1.059644842076725,
      "grad_norm": 1.39827561378479,
      "learning_rate": 1.9459228578824747e-05,
      "loss": 1.193,
      "step": 13680
    },
    {
      "epoch": 1.0604194504153837,
      "grad_norm": 1.3083611726760864,
      "learning_rate": 1.9451462593838985e-05,
      "loss": 1.1659,
      "step": 13690
    },
    {
      "epoch": 1.0611940587540425,
      "grad_norm": 1.533568024635315,
      "learning_rate": 1.9443696608853222e-05,
      "loss": 1.1944,
      "step": 13700
    },
    {
      "epoch": 1.0619686670927013,
      "grad_norm": 1.335539698600769,
      "learning_rate": 1.943593062386746e-05,
      "loss": 1.122,
      "step": 13710
    },
    {
      "epoch": 1.06274327543136,
      "grad_norm": 1.6818139553070068,
      "learning_rate": 1.94281646388817e-05,
      "loss": 1.2244,
      "step": 13720
    },
    {
      "epoch": 1.0635178837700188,
      "grad_norm": 1.4512336254119873,
      "learning_rate": 1.9420398653895936e-05,
      "loss": 1.1784,
      "step": 13730
    },
    {
      "epoch": 1.0642924921086776,
      "grad_norm": 1.4859362840652466,
      "learning_rate": 1.9412632668910174e-05,
      "loss": 1.1606,
      "step": 13740
    },
    {
      "epoch": 1.0650671004473362,
      "grad_norm": 1.392539381980896,
      "learning_rate": 1.9404866683924412e-05,
      "loss": 1.263,
      "step": 13750
    },
    {
      "epoch": 1.065841708785995,
      "grad_norm": 1.7561798095703125,
      "learning_rate": 1.939710069893865e-05,
      "loss": 1.1842,
      "step": 13760
    },
    {
      "epoch": 1.0666163171246539,
      "grad_norm": 1.641392469406128,
      "learning_rate": 1.9389334713952888e-05,
      "loss": 1.1865,
      "step": 13770
    },
    {
      "epoch": 1.0673909254633127,
      "grad_norm": 1.5835366249084473,
      "learning_rate": 1.9381568728967125e-05,
      "loss": 1.0828,
      "step": 13780
    },
    {
      "epoch": 1.0681655338019713,
      "grad_norm": 1.2479370832443237,
      "learning_rate": 1.9373802743981363e-05,
      "loss": 1.1106,
      "step": 13790
    },
    {
      "epoch": 1.0689401421406302,
      "grad_norm": 1.5377535820007324,
      "learning_rate": 1.93660367589956e-05,
      "loss": 1.241,
      "step": 13800
    },
    {
      "epoch": 1.069714750479289,
      "grad_norm": 1.4554885625839233,
      "learning_rate": 1.935827077400984e-05,
      "loss": 1.2425,
      "step": 13810
    },
    {
      "epoch": 1.0704893588179476,
      "grad_norm": 1.4295902252197266,
      "learning_rate": 1.9350504789024073e-05,
      "loss": 1.1323,
      "step": 13820
    },
    {
      "epoch": 1.0712639671566064,
      "grad_norm": 1.3346655368804932,
      "learning_rate": 1.934273880403831e-05,
      "loss": 1.2365,
      "step": 13830
    },
    {
      "epoch": 1.0720385754952653,
      "grad_norm": 1.3157960176467896,
      "learning_rate": 1.933497281905255e-05,
      "loss": 1.158,
      "step": 13840
    },
    {
      "epoch": 1.0728131838339239,
      "grad_norm": 1.5469623804092407,
      "learning_rate": 1.9327206834066787e-05,
      "loss": 1.1931,
      "step": 13850
    },
    {
      "epoch": 1.0735877921725827,
      "grad_norm": 1.516636848449707,
      "learning_rate": 1.9319440849081025e-05,
      "loss": 1.1739,
      "step": 13860
    },
    {
      "epoch": 1.0743624005112415,
      "grad_norm": 1.429050326347351,
      "learning_rate": 1.9311674864095263e-05,
      "loss": 1.229,
      "step": 13870
    },
    {
      "epoch": 1.0751370088499002,
      "grad_norm": 2.2336413860321045,
      "learning_rate": 1.93039088791095e-05,
      "loss": 1.2059,
      "step": 13880
    },
    {
      "epoch": 1.075911617188559,
      "grad_norm": 1.1340936422348022,
      "learning_rate": 1.929614289412374e-05,
      "loss": 1.1815,
      "step": 13890
    },
    {
      "epoch": 1.0766862255272178,
      "grad_norm": 1.6410592794418335,
      "learning_rate": 1.9288376909137973e-05,
      "loss": 1.2523,
      "step": 13900
    },
    {
      "epoch": 1.0774608338658767,
      "grad_norm": 1.4408329725265503,
      "learning_rate": 1.928061092415221e-05,
      "loss": 1.2018,
      "step": 13910
    },
    {
      "epoch": 1.0782354422045353,
      "grad_norm": 1.319504976272583,
      "learning_rate": 1.9272844939166452e-05,
      "loss": 1.2074,
      "step": 13920
    },
    {
      "epoch": 1.079010050543194,
      "grad_norm": 1.341062307357788,
      "learning_rate": 1.926507895418069e-05,
      "loss": 1.1951,
      "step": 13930
    },
    {
      "epoch": 1.079784658881853,
      "grad_norm": 1.3600603342056274,
      "learning_rate": 1.9257312969194928e-05,
      "loss": 1.1871,
      "step": 13940
    },
    {
      "epoch": 1.0805592672205115,
      "grad_norm": 1.1954659223556519,
      "learning_rate": 1.9249546984209166e-05,
      "loss": 1.1215,
      "step": 13950
    },
    {
      "epoch": 1.0813338755591704,
      "grad_norm": 1.2181222438812256,
      "learning_rate": 1.9241780999223404e-05,
      "loss": 1.0984,
      "step": 13960
    },
    {
      "epoch": 1.0821084838978292,
      "grad_norm": 1.4907649755477905,
      "learning_rate": 1.923401501423764e-05,
      "loss": 1.2973,
      "step": 13970
    },
    {
      "epoch": 1.082883092236488,
      "grad_norm": 1.6275179386138916,
      "learning_rate": 1.922624902925188e-05,
      "loss": 1.2098,
      "step": 13980
    },
    {
      "epoch": 1.0836577005751467,
      "grad_norm": 1.5469788312911987,
      "learning_rate": 1.9218483044266114e-05,
      "loss": 1.1733,
      "step": 13990
    },
    {
      "epoch": 1.0844323089138055,
      "grad_norm": 1.2596073150634766,
      "learning_rate": 1.921071705928035e-05,
      "loss": 1.2568,
      "step": 14000
    },
    {
      "epoch": 1.0852069172524643,
      "grad_norm": 1.8956161737442017,
      "learning_rate": 1.920295107429459e-05,
      "loss": 1.1036,
      "step": 14010
    },
    {
      "epoch": 1.085981525591123,
      "grad_norm": 2.193122148513794,
      "learning_rate": 1.9195185089308827e-05,
      "loss": 1.1453,
      "step": 14020
    },
    {
      "epoch": 1.0867561339297818,
      "grad_norm": 1.4533230066299438,
      "learning_rate": 1.9187419104323065e-05,
      "loss": 1.1649,
      "step": 14030
    },
    {
      "epoch": 1.0875307422684406,
      "grad_norm": 1.5518152713775635,
      "learning_rate": 1.9179653119337303e-05,
      "loss": 1.1571,
      "step": 14040
    },
    {
      "epoch": 1.0883053506070992,
      "grad_norm": 1.3715218305587769,
      "learning_rate": 1.917188713435154e-05,
      "loss": 1.2194,
      "step": 14050
    },
    {
      "epoch": 1.089079958945758,
      "grad_norm": 1.5599493980407715,
      "learning_rate": 1.916412114936578e-05,
      "loss": 1.1964,
      "step": 14060
    },
    {
      "epoch": 1.0898545672844169,
      "grad_norm": 1.7098159790039062,
      "learning_rate": 1.9156355164380013e-05,
      "loss": 1.0982,
      "step": 14070
    },
    {
      "epoch": 1.0906291756230755,
      "grad_norm": 2.041351318359375,
      "learning_rate": 1.914858917939425e-05,
      "loss": 1.1485,
      "step": 14080
    },
    {
      "epoch": 1.0914037839617343,
      "grad_norm": 1.504209280014038,
      "learning_rate": 1.914082319440849e-05,
      "loss": 1.1286,
      "step": 14090
    },
    {
      "epoch": 1.0921783923003932,
      "grad_norm": 1.8816864490509033,
      "learning_rate": 1.9133057209422727e-05,
      "loss": 1.1516,
      "step": 14100
    },
    {
      "epoch": 1.092953000639052,
      "grad_norm": 1.6550594568252563,
      "learning_rate": 1.9125291224436968e-05,
      "loss": 1.0948,
      "step": 14110
    },
    {
      "epoch": 1.0937276089777106,
      "grad_norm": 1.3094455003738403,
      "learning_rate": 1.9117525239451206e-05,
      "loss": 1.2248,
      "step": 14120
    },
    {
      "epoch": 1.0945022173163694,
      "grad_norm": 1.448500633239746,
      "learning_rate": 1.9109759254465444e-05,
      "loss": 1.1324,
      "step": 14130
    },
    {
      "epoch": 1.0952768256550283,
      "grad_norm": 1.1482418775558472,
      "learning_rate": 1.9101993269479682e-05,
      "loss": 1.2198,
      "step": 14140
    },
    {
      "epoch": 1.0960514339936869,
      "grad_norm": 1.311413288116455,
      "learning_rate": 1.909422728449392e-05,
      "loss": 1.1227,
      "step": 14150
    },
    {
      "epoch": 1.0968260423323457,
      "grad_norm": 1.7497214078903198,
      "learning_rate": 1.9086461299508154e-05,
      "loss": 1.1476,
      "step": 14160
    },
    {
      "epoch": 1.0976006506710045,
      "grad_norm": 1.8247640132904053,
      "learning_rate": 1.9078695314522392e-05,
      "loss": 1.2054,
      "step": 14170
    },
    {
      "epoch": 1.0983752590096632,
      "grad_norm": 1.4543558359146118,
      "learning_rate": 1.907092932953663e-05,
      "loss": 1.2296,
      "step": 14180
    },
    {
      "epoch": 1.099149867348322,
      "grad_norm": 1.8665555715560913,
      "learning_rate": 1.9063163344550868e-05,
      "loss": 1.1523,
      "step": 14190
    },
    {
      "epoch": 1.0999244756869808,
      "grad_norm": 1.547441005706787,
      "learning_rate": 1.9055397359565106e-05,
      "loss": 1.1869,
      "step": 14200
    },
    {
      "epoch": 1.1006990840256394,
      "grad_norm": 1.6806432008743286,
      "learning_rate": 1.9047631374579343e-05,
      "loss": 1.2192,
      "step": 14210
    },
    {
      "epoch": 1.1014736923642983,
      "grad_norm": 1.6044871807098389,
      "learning_rate": 1.903986538959358e-05,
      "loss": 1.2076,
      "step": 14220
    },
    {
      "epoch": 1.102248300702957,
      "grad_norm": 2.717471122741699,
      "learning_rate": 1.903209940460782e-05,
      "loss": 1.1442,
      "step": 14230
    },
    {
      "epoch": 1.103022909041616,
      "grad_norm": 1.4513444900512695,
      "learning_rate": 1.9024333419622054e-05,
      "loss": 1.2665,
      "step": 14240
    },
    {
      "epoch": 1.1037975173802745,
      "grad_norm": 2.108227491378784,
      "learning_rate": 1.901656743463629e-05,
      "loss": 1.2379,
      "step": 14250
    },
    {
      "epoch": 1.1045721257189334,
      "grad_norm": 1.7039374113082886,
      "learning_rate": 1.900880144965053e-05,
      "loss": 1.2311,
      "step": 14260
    },
    {
      "epoch": 1.1053467340575922,
      "grad_norm": 1.5207476615905762,
      "learning_rate": 1.9001035464664767e-05,
      "loss": 1.1573,
      "step": 14270
    },
    {
      "epoch": 1.1061213423962508,
      "grad_norm": 1.5414421558380127,
      "learning_rate": 1.8993269479679005e-05,
      "loss": 1.1747,
      "step": 14280
    },
    {
      "epoch": 1.1068959507349097,
      "grad_norm": 1.3298481702804565,
      "learning_rate": 1.8985503494693243e-05,
      "loss": 1.2661,
      "step": 14290
    },
    {
      "epoch": 1.1076705590735685,
      "grad_norm": 1.3253737688064575,
      "learning_rate": 1.897773750970748e-05,
      "loss": 1.2868,
      "step": 14300
    },
    {
      "epoch": 1.108445167412227,
      "grad_norm": 1.7057408094406128,
      "learning_rate": 1.8969971524721722e-05,
      "loss": 1.2061,
      "step": 14310
    },
    {
      "epoch": 1.109219775750886,
      "grad_norm": 1.4387227296829224,
      "learning_rate": 1.896220553973596e-05,
      "loss": 1.208,
      "step": 14320
    },
    {
      "epoch": 1.1099943840895448,
      "grad_norm": 1.3210445642471313,
      "learning_rate": 1.8954439554750194e-05,
      "loss": 1.2388,
      "step": 14330
    },
    {
      "epoch": 1.1107689924282034,
      "grad_norm": 1.3064277172088623,
      "learning_rate": 1.8946673569764432e-05,
      "loss": 1.1865,
      "step": 14340
    },
    {
      "epoch": 1.1115436007668622,
      "grad_norm": 1.5021252632141113,
      "learning_rate": 1.893890758477867e-05,
      "loss": 1.1613,
      "step": 14350
    },
    {
      "epoch": 1.112318209105521,
      "grad_norm": 1.1443580389022827,
      "learning_rate": 1.8931141599792908e-05,
      "loss": 1.153,
      "step": 14360
    },
    {
      "epoch": 1.1130928174441799,
      "grad_norm": 1.8182116746902466,
      "learning_rate": 1.8923375614807146e-05,
      "loss": 1.1664,
      "step": 14370
    },
    {
      "epoch": 1.1138674257828385,
      "grad_norm": 2.2643940448760986,
      "learning_rate": 1.8915609629821384e-05,
      "loss": 1.1684,
      "step": 14380
    },
    {
      "epoch": 1.1146420341214973,
      "grad_norm": 1.2332234382629395,
      "learning_rate": 1.890784364483562e-05,
      "loss": 1.1767,
      "step": 14390
    },
    {
      "epoch": 1.1154166424601561,
      "grad_norm": 1.6736383438110352,
      "learning_rate": 1.890007765984986e-05,
      "loss": 1.151,
      "step": 14400
    },
    {
      "epoch": 1.1161912507988148,
      "grad_norm": 1.5041265487670898,
      "learning_rate": 1.8892311674864094e-05,
      "loss": 1.1183,
      "step": 14410
    },
    {
      "epoch": 1.1169658591374736,
      "grad_norm": 1.4662517309188843,
      "learning_rate": 1.8884545689878332e-05,
      "loss": 1.2327,
      "step": 14420
    },
    {
      "epoch": 1.1177404674761324,
      "grad_norm": 2.049614429473877,
      "learning_rate": 1.887677970489257e-05,
      "loss": 1.1526,
      "step": 14430
    },
    {
      "epoch": 1.1185150758147913,
      "grad_norm": 1.5102307796478271,
      "learning_rate": 1.8869013719906808e-05,
      "loss": 1.0769,
      "step": 14440
    },
    {
      "epoch": 1.1192896841534499,
      "grad_norm": 1.9327136278152466,
      "learning_rate": 1.8861247734921045e-05,
      "loss": 1.2967,
      "step": 14450
    },
    {
      "epoch": 1.1200642924921087,
      "grad_norm": 1.5665470361709595,
      "learning_rate": 1.8853481749935283e-05,
      "loss": 1.1365,
      "step": 14460
    },
    {
      "epoch": 1.1208389008307675,
      "grad_norm": 1.4778310060501099,
      "learning_rate": 1.884571576494952e-05,
      "loss": 1.1805,
      "step": 14470
    },
    {
      "epoch": 1.1216135091694261,
      "grad_norm": 1.6566927433013916,
      "learning_rate": 1.883794977996376e-05,
      "loss": 1.2202,
      "step": 14480
    },
    {
      "epoch": 1.122388117508085,
      "grad_norm": 1.6855698823928833,
      "learning_rate": 1.8830183794977997e-05,
      "loss": 1.1988,
      "step": 14490
    },
    {
      "epoch": 1.1231627258467438,
      "grad_norm": 1.7702678442001343,
      "learning_rate": 1.882241780999223e-05,
      "loss": 1.1776,
      "step": 14500
    },
    {
      "epoch": 1.1239373341854024,
      "grad_norm": 1.847755789756775,
      "learning_rate": 1.8814651825006473e-05,
      "loss": 1.2187,
      "step": 14510
    },
    {
      "epoch": 1.1247119425240613,
      "grad_norm": 1.7444971799850464,
      "learning_rate": 1.880688584002071e-05,
      "loss": 1.1184,
      "step": 14520
    },
    {
      "epoch": 1.12548655086272,
      "grad_norm": 1.9041340351104736,
      "learning_rate": 1.879911985503495e-05,
      "loss": 1.1781,
      "step": 14530
    },
    {
      "epoch": 1.1262611592013787,
      "grad_norm": 1.3531873226165771,
      "learning_rate": 1.8791353870049186e-05,
      "loss": 1.1513,
      "step": 14540
    },
    {
      "epoch": 1.1270357675400375,
      "grad_norm": 1.666095495223999,
      "learning_rate": 1.8783587885063424e-05,
      "loss": 1.1892,
      "step": 14550
    },
    {
      "epoch": 1.1278103758786964,
      "grad_norm": 1.4222248792648315,
      "learning_rate": 1.8775821900077662e-05,
      "loss": 1.19,
      "step": 14560
    },
    {
      "epoch": 1.1285849842173552,
      "grad_norm": 1.6054270267486572,
      "learning_rate": 1.87680559150919e-05,
      "loss": 1.1569,
      "step": 14570
    },
    {
      "epoch": 1.1293595925560138,
      "grad_norm": 1.3168823719024658,
      "learning_rate": 1.8760289930106134e-05,
      "loss": 1.1785,
      "step": 14580
    },
    {
      "epoch": 1.1301342008946726,
      "grad_norm": 1.4116237163543701,
      "learning_rate": 1.8752523945120372e-05,
      "loss": 1.2258,
      "step": 14590
    },
    {
      "epoch": 1.1309088092333315,
      "grad_norm": 1.7199476957321167,
      "learning_rate": 1.874475796013461e-05,
      "loss": 1.1789,
      "step": 14600
    },
    {
      "epoch": 1.13168341757199,
      "grad_norm": 1.424066424369812,
      "learning_rate": 1.8736991975148848e-05,
      "loss": 1.2273,
      "step": 14610
    },
    {
      "epoch": 1.132458025910649,
      "grad_norm": 1.6217443943023682,
      "learning_rate": 1.8729225990163086e-05,
      "loss": 1.207,
      "step": 14620
    },
    {
      "epoch": 1.1332326342493078,
      "grad_norm": 1.433146595954895,
      "learning_rate": 1.8721460005177324e-05,
      "loss": 1.2646,
      "step": 14630
    },
    {
      "epoch": 1.1340072425879664,
      "grad_norm": 1.555315375328064,
      "learning_rate": 1.871369402019156e-05,
      "loss": 1.1844,
      "step": 14640
    },
    {
      "epoch": 1.1347818509266252,
      "grad_norm": 1.5767383575439453,
      "learning_rate": 1.87059280352058e-05,
      "loss": 1.1696,
      "step": 14650
    },
    {
      "epoch": 1.135556459265284,
      "grad_norm": 1.668771743774414,
      "learning_rate": 1.8698162050220037e-05,
      "loss": 1.0812,
      "step": 14660
    },
    {
      "epoch": 1.1363310676039426,
      "grad_norm": 1.5871295928955078,
      "learning_rate": 1.8690396065234272e-05,
      "loss": 1.1953,
      "step": 14670
    },
    {
      "epoch": 1.1371056759426015,
      "grad_norm": 1.2370089292526245,
      "learning_rate": 1.868263008024851e-05,
      "loss": 1.1576,
      "step": 14680
    },
    {
      "epoch": 1.1378802842812603,
      "grad_norm": 1.1865627765655518,
      "learning_rate": 1.8674864095262748e-05,
      "loss": 1.1763,
      "step": 14690
    },
    {
      "epoch": 1.1386548926199191,
      "grad_norm": 1.4667562246322632,
      "learning_rate": 1.866709811027699e-05,
      "loss": 1.2009,
      "step": 14700
    },
    {
      "epoch": 1.1394295009585778,
      "grad_norm": 1.817219853401184,
      "learning_rate": 1.8659332125291227e-05,
      "loss": 1.1932,
      "step": 14710
    },
    {
      "epoch": 1.1402041092972366,
      "grad_norm": 1.5978511571884155,
      "learning_rate": 1.8651566140305464e-05,
      "loss": 1.2161,
      "step": 14720
    },
    {
      "epoch": 1.1409787176358954,
      "grad_norm": 1.3365862369537354,
      "learning_rate": 1.8643800155319702e-05,
      "loss": 1.1362,
      "step": 14730
    },
    {
      "epoch": 1.141753325974554,
      "grad_norm": 1.5625903606414795,
      "learning_rate": 1.863603417033394e-05,
      "loss": 1.1028,
      "step": 14740
    },
    {
      "epoch": 1.1425279343132129,
      "grad_norm": 1.3435325622558594,
      "learning_rate": 1.8628268185348175e-05,
      "loss": 1.2643,
      "step": 14750
    },
    {
      "epoch": 1.1433025426518717,
      "grad_norm": 1.2771918773651123,
      "learning_rate": 1.8620502200362413e-05,
      "loss": 1.1525,
      "step": 14760
    },
    {
      "epoch": 1.1440771509905305,
      "grad_norm": 1.766194462776184,
      "learning_rate": 1.861273621537665e-05,
      "loss": 1.1857,
      "step": 14770
    },
    {
      "epoch": 1.1448517593291891,
      "grad_norm": 1.3863780498504639,
      "learning_rate": 1.8604970230390888e-05,
      "loss": 1.1908,
      "step": 14780
    },
    {
      "epoch": 1.145626367667848,
      "grad_norm": 1.295801043510437,
      "learning_rate": 1.8597204245405126e-05,
      "loss": 1.1795,
      "step": 14790
    },
    {
      "epoch": 1.1464009760065066,
      "grad_norm": 1.5082350969314575,
      "learning_rate": 1.8589438260419364e-05,
      "loss": 1.2501,
      "step": 14800
    },
    {
      "epoch": 1.1471755843451654,
      "grad_norm": 1.4732383489608765,
      "learning_rate": 1.8581672275433602e-05,
      "loss": 1.1973,
      "step": 14810
    },
    {
      "epoch": 1.1479501926838243,
      "grad_norm": 1.41475510597229,
      "learning_rate": 1.857390629044784e-05,
      "loss": 1.1863,
      "step": 14820
    },
    {
      "epoch": 1.148724801022483,
      "grad_norm": 1.6036263704299927,
      "learning_rate": 1.8566140305462078e-05,
      "loss": 1.1088,
      "step": 14830
    },
    {
      "epoch": 1.1494994093611417,
      "grad_norm": 1.2906230688095093,
      "learning_rate": 1.8558374320476312e-05,
      "loss": 1.2034,
      "step": 14840
    },
    {
      "epoch": 1.1502740176998005,
      "grad_norm": 1.3382830619812012,
      "learning_rate": 1.855060833549055e-05,
      "loss": 1.1701,
      "step": 14850
    },
    {
      "epoch": 1.1510486260384594,
      "grad_norm": 1.345231294631958,
      "learning_rate": 1.8542842350504788e-05,
      "loss": 1.0874,
      "step": 14860
    },
    {
      "epoch": 1.151823234377118,
      "grad_norm": 1.8362163305282593,
      "learning_rate": 1.8535076365519026e-05,
      "loss": 1.2102,
      "step": 14870
    },
    {
      "epoch": 1.1525978427157768,
      "grad_norm": 1.3567196130752563,
      "learning_rate": 1.8527310380533264e-05,
      "loss": 1.1547,
      "step": 14880
    },
    {
      "epoch": 1.1533724510544356,
      "grad_norm": 2.0216257572174072,
      "learning_rate": 1.85195443955475e-05,
      "loss": 1.1923,
      "step": 14890
    },
    {
      "epoch": 1.1541470593930945,
      "grad_norm": 1.4454408884048462,
      "learning_rate": 1.8511778410561743e-05,
      "loss": 1.1955,
      "step": 14900
    },
    {
      "epoch": 1.154921667731753,
      "grad_norm": 1.7795662879943848,
      "learning_rate": 1.850401242557598e-05,
      "loss": 1.2444,
      "step": 14910
    },
    {
      "epoch": 1.155696276070412,
      "grad_norm": 1.5877342224121094,
      "learning_rate": 1.8496246440590215e-05,
      "loss": 1.139,
      "step": 14920
    },
    {
      "epoch": 1.1564708844090708,
      "grad_norm": 1.402299165725708,
      "learning_rate": 1.8488480455604453e-05,
      "loss": 1.2979,
      "step": 14930
    },
    {
      "epoch": 1.1572454927477294,
      "grad_norm": 1.4774903059005737,
      "learning_rate": 1.848071447061869e-05,
      "loss": 1.2109,
      "step": 14940
    },
    {
      "epoch": 1.1580201010863882,
      "grad_norm": 1.3419787883758545,
      "learning_rate": 1.847294848563293e-05,
      "loss": 1.2182,
      "step": 14950
    },
    {
      "epoch": 1.158794709425047,
      "grad_norm": 1.5998420715332031,
      "learning_rate": 1.8465182500647167e-05,
      "loss": 1.1804,
      "step": 14960
    },
    {
      "epoch": 1.1595693177637056,
      "grad_norm": 1.409760594367981,
      "learning_rate": 1.8457416515661404e-05,
      "loss": 1.0957,
      "step": 14970
    },
    {
      "epoch": 1.1603439261023645,
      "grad_norm": 1.2540520429611206,
      "learning_rate": 1.8449650530675642e-05,
      "loss": 1.2067,
      "step": 14980
    },
    {
      "epoch": 1.1611185344410233,
      "grad_norm": 2.458244562149048,
      "learning_rate": 1.844188454568988e-05,
      "loss": 1.1873,
      "step": 14990
    },
    {
      "epoch": 1.161893142779682,
      "grad_norm": 1.8543089628219604,
      "learning_rate": 1.8434118560704115e-05,
      "loss": 1.1305,
      "step": 15000
    },
    {
      "epoch": 1.1626677511183408,
      "grad_norm": 1.7561818361282349,
      "learning_rate": 1.8426352575718352e-05,
      "loss": 1.2408,
      "step": 15010
    },
    {
      "epoch": 1.1634423594569996,
      "grad_norm": 1.4399456977844238,
      "learning_rate": 1.841858659073259e-05,
      "loss": 1.1754,
      "step": 15020
    },
    {
      "epoch": 1.1642169677956584,
      "grad_norm": 1.5214391946792603,
      "learning_rate": 1.8410820605746828e-05,
      "loss": 1.2285,
      "step": 15030
    },
    {
      "epoch": 1.164991576134317,
      "grad_norm": 1.3474266529083252,
      "learning_rate": 1.8403054620761066e-05,
      "loss": 1.2233,
      "step": 15040
    },
    {
      "epoch": 1.1657661844729759,
      "grad_norm": 1.9103437662124634,
      "learning_rate": 1.8395288635775304e-05,
      "loss": 1.2036,
      "step": 15050
    },
    {
      "epoch": 1.1665407928116347,
      "grad_norm": 1.60930597782135,
      "learning_rate": 1.8387522650789542e-05,
      "loss": 1.1648,
      "step": 15060
    },
    {
      "epoch": 1.1673154011502933,
      "grad_norm": 1.3071092367172241,
      "learning_rate": 1.837975666580378e-05,
      "loss": 1.1756,
      "step": 15070
    },
    {
      "epoch": 1.1680900094889521,
      "grad_norm": 1.8098642826080322,
      "learning_rate": 1.8371990680818018e-05,
      "loss": 1.1207,
      "step": 15080
    },
    {
      "epoch": 1.168864617827611,
      "grad_norm": 1.7928900718688965,
      "learning_rate": 1.8364224695832255e-05,
      "loss": 1.2074,
      "step": 15090
    },
    {
      "epoch": 1.1696392261662696,
      "grad_norm": 1.4599169492721558,
      "learning_rate": 1.8356458710846493e-05,
      "loss": 1.1572,
      "step": 15100
    },
    {
      "epoch": 1.1704138345049284,
      "grad_norm": 1.3115595579147339,
      "learning_rate": 1.834869272586073e-05,
      "loss": 1.1805,
      "step": 15110
    },
    {
      "epoch": 1.1711884428435873,
      "grad_norm": 1.298392653465271,
      "learning_rate": 1.834092674087497e-05,
      "loss": 1.2192,
      "step": 15120
    },
    {
      "epoch": 1.1719630511822459,
      "grad_norm": 1.3346976041793823,
      "learning_rate": 1.8333160755889207e-05,
      "loss": 1.1631,
      "step": 15130
    },
    {
      "epoch": 1.1727376595209047,
      "grad_norm": 1.2587076425552368,
      "learning_rate": 1.8325394770903445e-05,
      "loss": 1.1589,
      "step": 15140
    },
    {
      "epoch": 1.1735122678595635,
      "grad_norm": 1.696656584739685,
      "learning_rate": 1.8317628785917683e-05,
      "loss": 1.1194,
      "step": 15150
    },
    {
      "epoch": 1.1742868761982224,
      "grad_norm": 1.4990290403366089,
      "learning_rate": 1.830986280093192e-05,
      "loss": 1.1743,
      "step": 15160
    },
    {
      "epoch": 1.175061484536881,
      "grad_norm": 1.5505403280258179,
      "learning_rate": 1.8302096815946155e-05,
      "loss": 1.1674,
      "step": 15170
    },
    {
      "epoch": 1.1758360928755398,
      "grad_norm": 1.6059232950210571,
      "learning_rate": 1.8294330830960393e-05,
      "loss": 1.296,
      "step": 15180
    },
    {
      "epoch": 1.1766107012141986,
      "grad_norm": 1.9483814239501953,
      "learning_rate": 1.828656484597463e-05,
      "loss": 1.167,
      "step": 15190
    },
    {
      "epoch": 1.1773853095528573,
      "grad_norm": 1.158862829208374,
      "learning_rate": 1.827879886098887e-05,
      "loss": 1.1805,
      "step": 15200
    },
    {
      "epoch": 1.178159917891516,
      "grad_norm": 1.322401523590088,
      "learning_rate": 1.8271032876003106e-05,
      "loss": 1.137,
      "step": 15210
    },
    {
      "epoch": 1.178934526230175,
      "grad_norm": 1.470552921295166,
      "learning_rate": 1.8263266891017344e-05,
      "loss": 1.1352,
      "step": 15220
    },
    {
      "epoch": 1.1797091345688338,
      "grad_norm": 1.3196700811386108,
      "learning_rate": 1.8255500906031582e-05,
      "loss": 1.2394,
      "step": 15230
    },
    {
      "epoch": 1.1804837429074924,
      "grad_norm": 1.7427715063095093,
      "learning_rate": 1.824773492104582e-05,
      "loss": 1.1773,
      "step": 15240
    },
    {
      "epoch": 1.1812583512461512,
      "grad_norm": 1.1090404987335205,
      "learning_rate": 1.8239968936060058e-05,
      "loss": 1.1236,
      "step": 15250
    },
    {
      "epoch": 1.1820329595848098,
      "grad_norm": 1.4784865379333496,
      "learning_rate": 1.8232202951074292e-05,
      "loss": 1.157,
      "step": 15260
    },
    {
      "epoch": 1.1828075679234686,
      "grad_norm": 1.5212764739990234,
      "learning_rate": 1.822443696608853e-05,
      "loss": 1.1835,
      "step": 15270
    },
    {
      "epoch": 1.1835821762621275,
      "grad_norm": 2.093991756439209,
      "learning_rate": 1.8216670981102768e-05,
      "loss": 1.1873,
      "step": 15280
    },
    {
      "epoch": 1.1843567846007863,
      "grad_norm": 1.791339635848999,
      "learning_rate": 1.820890499611701e-05,
      "loss": 1.197,
      "step": 15290
    },
    {
      "epoch": 1.185131392939445,
      "grad_norm": 1.6994802951812744,
      "learning_rate": 1.8201139011131247e-05,
      "loss": 1.2309,
      "step": 15300
    },
    {
      "epoch": 1.1859060012781037,
      "grad_norm": 1.4633270502090454,
      "learning_rate": 1.8193373026145485e-05,
      "loss": 1.1648,
      "step": 15310
    },
    {
      "epoch": 1.1866806096167626,
      "grad_norm": 1.5390236377716064,
      "learning_rate": 1.8185607041159723e-05,
      "loss": 1.1209,
      "step": 15320
    },
    {
      "epoch": 1.1874552179554212,
      "grad_norm": 2.028874397277832,
      "learning_rate": 1.817784105617396e-05,
      "loss": 1.1763,
      "step": 15330
    },
    {
      "epoch": 1.18822982629408,
      "grad_norm": 1.305238962173462,
      "learning_rate": 1.8170075071188195e-05,
      "loss": 1.2265,
      "step": 15340
    },
    {
      "epoch": 1.1890044346327389,
      "grad_norm": 1.0744760036468506,
      "learning_rate": 1.8162309086202433e-05,
      "loss": 1.184,
      "step": 15350
    },
    {
      "epoch": 1.1897790429713977,
      "grad_norm": 1.6433571577072144,
      "learning_rate": 1.815454310121667e-05,
      "loss": 1.1061,
      "step": 15360
    },
    {
      "epoch": 1.1905536513100563,
      "grad_norm": 1.5258549451828003,
      "learning_rate": 1.814677711623091e-05,
      "loss": 1.306,
      "step": 15370
    },
    {
      "epoch": 1.1913282596487151,
      "grad_norm": 1.4270597696304321,
      "learning_rate": 1.8139011131245147e-05,
      "loss": 1.2046,
      "step": 15380
    },
    {
      "epoch": 1.192102867987374,
      "grad_norm": 1.3900052309036255,
      "learning_rate": 1.8131245146259385e-05,
      "loss": 1.2307,
      "step": 15390
    },
    {
      "epoch": 1.1928774763260326,
      "grad_norm": 1.7563937902450562,
      "learning_rate": 1.8123479161273622e-05,
      "loss": 1.1454,
      "step": 15400
    },
    {
      "epoch": 1.1936520846646914,
      "grad_norm": 1.5354554653167725,
      "learning_rate": 1.811571317628786e-05,
      "loss": 1.2158,
      "step": 15410
    },
    {
      "epoch": 1.1944266930033502,
      "grad_norm": 1.8179606199264526,
      "learning_rate": 1.8107947191302098e-05,
      "loss": 1.0398,
      "step": 15420
    },
    {
      "epoch": 1.1952013013420089,
      "grad_norm": 1.5851753950119019,
      "learning_rate": 1.8100181206316333e-05,
      "loss": 1.2191,
      "step": 15430
    },
    {
      "epoch": 1.1959759096806677,
      "grad_norm": 2.2742865085601807,
      "learning_rate": 1.809241522133057e-05,
      "loss": 1.1917,
      "step": 15440
    },
    {
      "epoch": 1.1967505180193265,
      "grad_norm": 1.4368776082992554,
      "learning_rate": 1.808464923634481e-05,
      "loss": 1.2933,
      "step": 15450
    },
    {
      "epoch": 1.1975251263579851,
      "grad_norm": 1.4466595649719238,
      "learning_rate": 1.8076883251359046e-05,
      "loss": 1.1638,
      "step": 15460
    },
    {
      "epoch": 1.198299734696644,
      "grad_norm": 1.586379885673523,
      "learning_rate": 1.8069117266373284e-05,
      "loss": 1.2105,
      "step": 15470
    },
    {
      "epoch": 1.1990743430353028,
      "grad_norm": 1.3445605039596558,
      "learning_rate": 1.8061351281387522e-05,
      "loss": 1.2472,
      "step": 15480
    },
    {
      "epoch": 1.1998489513739616,
      "grad_norm": 1.6061768531799316,
      "learning_rate": 1.8053585296401763e-05,
      "loss": 1.1689,
      "step": 15490
    },
    {
      "epoch": 1.2006235597126202,
      "grad_norm": 1.2772932052612305,
      "learning_rate": 1.8045819311416e-05,
      "loss": 1.2264,
      "step": 15500
    },
    {
      "epoch": 1.201398168051279,
      "grad_norm": 1.8453017473220825,
      "learning_rate": 1.8038053326430236e-05,
      "loss": 1.1304,
      "step": 15510
    },
    {
      "epoch": 1.202172776389938,
      "grad_norm": 1.7698155641555786,
      "learning_rate": 1.8030287341444473e-05,
      "loss": 1.2373,
      "step": 15520
    },
    {
      "epoch": 1.2029473847285965,
      "grad_norm": 1.518371820449829,
      "learning_rate": 1.802252135645871e-05,
      "loss": 1.0815,
      "step": 15530
    },
    {
      "epoch": 1.2037219930672554,
      "grad_norm": 1.4386343955993652,
      "learning_rate": 1.801475537147295e-05,
      "loss": 1.1789,
      "step": 15540
    },
    {
      "epoch": 1.2044966014059142,
      "grad_norm": 1.467927098274231,
      "learning_rate": 1.8006989386487187e-05,
      "loss": 1.1696,
      "step": 15550
    },
    {
      "epoch": 1.205271209744573,
      "grad_norm": 1.784209966659546,
      "learning_rate": 1.7999223401501425e-05,
      "loss": 1.1823,
      "step": 15560
    },
    {
      "epoch": 1.2060458180832316,
      "grad_norm": 1.6360747814178467,
      "learning_rate": 1.7991457416515663e-05,
      "loss": 1.2225,
      "step": 15570
    },
    {
      "epoch": 1.2068204264218905,
      "grad_norm": 1.546448826789856,
      "learning_rate": 1.79836914315299e-05,
      "loss": 1.2345,
      "step": 15580
    },
    {
      "epoch": 1.207595034760549,
      "grad_norm": 1.5897544622421265,
      "learning_rate": 1.797592544654414e-05,
      "loss": 1.1337,
      "step": 15590
    },
    {
      "epoch": 1.208369643099208,
      "grad_norm": 1.640648603439331,
      "learning_rate": 1.7968159461558373e-05,
      "loss": 1.26,
      "step": 15600
    },
    {
      "epoch": 1.2091442514378667,
      "grad_norm": 1.5707643032073975,
      "learning_rate": 1.796039347657261e-05,
      "loss": 1.1335,
      "step": 15610
    },
    {
      "epoch": 1.2099188597765256,
      "grad_norm": 1.6641545295715332,
      "learning_rate": 1.795262749158685e-05,
      "loss": 1.2219,
      "step": 15620
    },
    {
      "epoch": 1.2106934681151842,
      "grad_norm": 1.6566499471664429,
      "learning_rate": 1.7944861506601087e-05,
      "loss": 1.1245,
      "step": 15630
    },
    {
      "epoch": 1.211468076453843,
      "grad_norm": 1.815075159072876,
      "learning_rate": 1.7937095521615324e-05,
      "loss": 1.1145,
      "step": 15640
    },
    {
      "epoch": 1.2122426847925019,
      "grad_norm": 1.2026996612548828,
      "learning_rate": 1.7929329536629562e-05,
      "loss": 1.2122,
      "step": 15650
    },
    {
      "epoch": 1.2130172931311605,
      "grad_norm": 1.5713294744491577,
      "learning_rate": 1.79215635516438e-05,
      "loss": 1.2303,
      "step": 15660
    },
    {
      "epoch": 1.2137919014698193,
      "grad_norm": 1.7568708658218384,
      "learning_rate": 1.7913797566658038e-05,
      "loss": 1.1712,
      "step": 15670
    },
    {
      "epoch": 1.2145665098084781,
      "grad_norm": 1.5486091375350952,
      "learning_rate": 1.7906031581672276e-05,
      "loss": 1.1818,
      "step": 15680
    },
    {
      "epoch": 1.215341118147137,
      "grad_norm": 1.6512255668640137,
      "learning_rate": 1.7898265596686514e-05,
      "loss": 1.3196,
      "step": 15690
    },
    {
      "epoch": 1.2161157264857956,
      "grad_norm": 1.7783867120742798,
      "learning_rate": 1.789049961170075e-05,
      "loss": 1.1349,
      "step": 15700
    },
    {
      "epoch": 1.2168903348244544,
      "grad_norm": 1.6868789196014404,
      "learning_rate": 1.788273362671499e-05,
      "loss": 1.2112,
      "step": 15710
    },
    {
      "epoch": 1.2176649431631132,
      "grad_norm": 1.3366990089416504,
      "learning_rate": 1.7874967641729227e-05,
      "loss": 1.1443,
      "step": 15720
    },
    {
      "epoch": 1.2184395515017719,
      "grad_norm": 1.528411626815796,
      "learning_rate": 1.7867201656743465e-05,
      "loss": 1.1377,
      "step": 15730
    },
    {
      "epoch": 1.2192141598404307,
      "grad_norm": 1.5048788785934448,
      "learning_rate": 1.7859435671757703e-05,
      "loss": 1.1909,
      "step": 15740
    },
    {
      "epoch": 1.2199887681790895,
      "grad_norm": 1.4188545942306519,
      "learning_rate": 1.785166968677194e-05,
      "loss": 1.2646,
      "step": 15750
    },
    {
      "epoch": 1.2207633765177481,
      "grad_norm": 1.698777437210083,
      "learning_rate": 1.784390370178618e-05,
      "loss": 1.2514,
      "step": 15760
    },
    {
      "epoch": 1.221537984856407,
      "grad_norm": 1.474915623664856,
      "learning_rate": 1.7836137716800413e-05,
      "loss": 1.2507,
      "step": 15770
    },
    {
      "epoch": 1.2223125931950658,
      "grad_norm": 1.6795704364776611,
      "learning_rate": 1.782837173181465e-05,
      "loss": 1.2869,
      "step": 15780
    },
    {
      "epoch": 1.2230872015337244,
      "grad_norm": 1.4131697416305542,
      "learning_rate": 1.782060574682889e-05,
      "loss": 1.1815,
      "step": 15790
    },
    {
      "epoch": 1.2238618098723832,
      "grad_norm": 1.7144666910171509,
      "learning_rate": 1.7812839761843127e-05,
      "loss": 1.2289,
      "step": 15800
    },
    {
      "epoch": 1.224636418211042,
      "grad_norm": 1.1507080793380737,
      "learning_rate": 1.7805073776857365e-05,
      "loss": 1.1687,
      "step": 15810
    },
    {
      "epoch": 1.225411026549701,
      "grad_norm": 1.5630534887313843,
      "learning_rate": 1.7797307791871603e-05,
      "loss": 1.24,
      "step": 15820
    },
    {
      "epoch": 1.2261856348883595,
      "grad_norm": 1.452368974685669,
      "learning_rate": 1.778954180688584e-05,
      "loss": 1.2225,
      "step": 15830
    },
    {
      "epoch": 1.2269602432270184,
      "grad_norm": 1.5153106451034546,
      "learning_rate": 1.778177582190008e-05,
      "loss": 1.2381,
      "step": 15840
    },
    {
      "epoch": 1.2277348515656772,
      "grad_norm": 1.4427032470703125,
      "learning_rate": 1.7774009836914313e-05,
      "loss": 1.1336,
      "step": 15850
    },
    {
      "epoch": 1.2285094599043358,
      "grad_norm": 1.4741833209991455,
      "learning_rate": 1.776624385192855e-05,
      "loss": 1.1846,
      "step": 15860
    },
    {
      "epoch": 1.2292840682429946,
      "grad_norm": 1.2773834466934204,
      "learning_rate": 1.775847786694279e-05,
      "loss": 1.1752,
      "step": 15870
    },
    {
      "epoch": 1.2300586765816535,
      "grad_norm": 1.6528770923614502,
      "learning_rate": 1.775071188195703e-05,
      "loss": 1.1879,
      "step": 15880
    },
    {
      "epoch": 1.230833284920312,
      "grad_norm": 1.357354998588562,
      "learning_rate": 1.7742945896971268e-05,
      "loss": 1.2327,
      "step": 15890
    },
    {
      "epoch": 1.231607893258971,
      "grad_norm": 1.9254918098449707,
      "learning_rate": 1.7735179911985506e-05,
      "loss": 1.1368,
      "step": 15900
    },
    {
      "epoch": 1.2323825015976297,
      "grad_norm": 1.316709280014038,
      "learning_rate": 1.772819052549832e-05,
      "loss": 1.2233,
      "step": 15910
    },
    {
      "epoch": 1.2331571099362884,
      "grad_norm": 1.5120006799697876,
      "learning_rate": 1.7720424540512557e-05,
      "loss": 1.188,
      "step": 15920
    },
    {
      "epoch": 1.2339317182749472,
      "grad_norm": 1.7723720073699951,
      "learning_rate": 1.771265855552679e-05,
      "loss": 1.2211,
      "step": 15930
    },
    {
      "epoch": 1.234706326613606,
      "grad_norm": 1.3475074768066406,
      "learning_rate": 1.770489257054103e-05,
      "loss": 1.062,
      "step": 15940
    },
    {
      "epoch": 1.2354809349522649,
      "grad_norm": 1.4169472455978394,
      "learning_rate": 1.7697126585555267e-05,
      "loss": 1.1447,
      "step": 15950
    },
    {
      "epoch": 1.2362555432909235,
      "grad_norm": 1.7748239040374756,
      "learning_rate": 1.7689360600569505e-05,
      "loss": 1.1579,
      "step": 15960
    },
    {
      "epoch": 1.2370301516295823,
      "grad_norm": 1.8099439144134521,
      "learning_rate": 1.7681594615583743e-05,
      "loss": 1.1002,
      "step": 15970
    },
    {
      "epoch": 1.2378047599682411,
      "grad_norm": 1.2731887102127075,
      "learning_rate": 1.767382863059798e-05,
      "loss": 1.1656,
      "step": 15980
    },
    {
      "epoch": 1.2385793683068997,
      "grad_norm": 1.8512822389602661,
      "learning_rate": 1.766606264561222e-05,
      "loss": 1.2002,
      "step": 15990
    },
    {
      "epoch": 1.2393539766455586,
      "grad_norm": 2.08516001701355,
      "learning_rate": 1.7658296660626456e-05,
      "loss": 1.2164,
      "step": 16000
    },
    {
      "epoch": 1.2401285849842174,
      "grad_norm": 1.6290565729141235,
      "learning_rate": 1.765053067564069e-05,
      "loss": 1.1651,
      "step": 16010
    },
    {
      "epoch": 1.2409031933228762,
      "grad_norm": 1.9279344081878662,
      "learning_rate": 1.7642764690654932e-05,
      "loss": 1.1776,
      "step": 16020
    },
    {
      "epoch": 1.2416778016615349,
      "grad_norm": 1.5238322019577026,
      "learning_rate": 1.763499870566917e-05,
      "loss": 1.101,
      "step": 16030
    },
    {
      "epoch": 1.2424524100001937,
      "grad_norm": 1.525251865386963,
      "learning_rate": 1.7627232720683408e-05,
      "loss": 1.153,
      "step": 16040
    },
    {
      "epoch": 1.2432270183388523,
      "grad_norm": 1.5020627975463867,
      "learning_rate": 1.7619466735697646e-05,
      "loss": 1.171,
      "step": 16050
    },
    {
      "epoch": 1.2440016266775111,
      "grad_norm": 1.5476107597351074,
      "learning_rate": 1.7611700750711884e-05,
      "loss": 1.1594,
      "step": 16060
    },
    {
      "epoch": 1.24477623501617,
      "grad_norm": 1.2387614250183105,
      "learning_rate": 1.760393476572612e-05,
      "loss": 1.0902,
      "step": 16070
    },
    {
      "epoch": 1.2455508433548288,
      "grad_norm": 1.441787838935852,
      "learning_rate": 1.759616878074036e-05,
      "loss": 1.0797,
      "step": 16080
    },
    {
      "epoch": 1.2463254516934874,
      "grad_norm": 1.4921410083770752,
      "learning_rate": 1.7588402795754597e-05,
      "loss": 1.1713,
      "step": 16090
    },
    {
      "epoch": 1.2471000600321462,
      "grad_norm": 1.5458838939666748,
      "learning_rate": 1.758063681076883e-05,
      "loss": 1.1291,
      "step": 16100
    },
    {
      "epoch": 1.247874668370805,
      "grad_norm": 1.9034377336502075,
      "learning_rate": 1.757287082578307e-05,
      "loss": 1.1457,
      "step": 16110
    },
    {
      "epoch": 1.2486492767094637,
      "grad_norm": 1.5592085123062134,
      "learning_rate": 1.7565104840797307e-05,
      "loss": 1.1314,
      "step": 16120
    },
    {
      "epoch": 1.2494238850481225,
      "grad_norm": 1.5775344371795654,
      "learning_rate": 1.7557338855811545e-05,
      "loss": 1.1106,
      "step": 16130
    },
    {
      "epoch": 1.2501984933867814,
      "grad_norm": 1.7556424140930176,
      "learning_rate": 1.7549572870825783e-05,
      "loss": 1.1567,
      "step": 16140
    },
    {
      "epoch": 1.2509731017254402,
      "grad_norm": 1.6175626516342163,
      "learning_rate": 1.754180688584002e-05,
      "loss": 1.2234,
      "step": 16150
    },
    {
      "epoch": 1.2517477100640988,
      "grad_norm": 1.675744652748108,
      "learning_rate": 1.753404090085426e-05,
      "loss": 1.1539,
      "step": 16160
    },
    {
      "epoch": 1.2525223184027576,
      "grad_norm": 1.4844915866851807,
      "learning_rate": 1.7526274915868497e-05,
      "loss": 1.2668,
      "step": 16170
    },
    {
      "epoch": 1.2532969267414162,
      "grad_norm": 1.5451602935791016,
      "learning_rate": 1.751850893088273e-05,
      "loss": 1.1594,
      "step": 16180
    },
    {
      "epoch": 1.254071535080075,
      "grad_norm": 1.497984528541565,
      "learning_rate": 1.751074294589697e-05,
      "loss": 1.1521,
      "step": 16190
    },
    {
      "epoch": 1.254846143418734,
      "grad_norm": 1.7831381559371948,
      "learning_rate": 1.7502976960911207e-05,
      "loss": 1.2017,
      "step": 16200
    },
    {
      "epoch": 1.2556207517573927,
      "grad_norm": 1.60955810546875,
      "learning_rate": 1.7495210975925448e-05,
      "loss": 1.2546,
      "step": 16210
    },
    {
      "epoch": 1.2563953600960516,
      "grad_norm": 1.6938453912734985,
      "learning_rate": 1.7487444990939686e-05,
      "loss": 1.2099,
      "step": 16220
    },
    {
      "epoch": 1.2571699684347102,
      "grad_norm": 1.7379038333892822,
      "learning_rate": 1.7479679005953924e-05,
      "loss": 1.2319,
      "step": 16230
    },
    {
      "epoch": 1.257944576773369,
      "grad_norm": 2.212489366531372,
      "learning_rate": 1.7471913020968162e-05,
      "loss": 1.1968,
      "step": 16240
    },
    {
      "epoch": 1.2587191851120276,
      "grad_norm": 1.5271937847137451,
      "learning_rate": 1.74641470359824e-05,
      "loss": 1.2833,
      "step": 16250
    },
    {
      "epoch": 1.2594937934506865,
      "grad_norm": 1.4399120807647705,
      "learning_rate": 1.7456381050996638e-05,
      "loss": 1.2769,
      "step": 16260
    },
    {
      "epoch": 1.2602684017893453,
      "grad_norm": 1.6702085733413696,
      "learning_rate": 1.7448615066010872e-05,
      "loss": 1.2111,
      "step": 16270
    },
    {
      "epoch": 1.2610430101280041,
      "grad_norm": 1.42874014377594,
      "learning_rate": 1.744084908102511e-05,
      "loss": 1.1627,
      "step": 16280
    },
    {
      "epoch": 1.2618176184666627,
      "grad_norm": 2.9320969581604004,
      "learning_rate": 1.7433083096039348e-05,
      "loss": 1.2312,
      "step": 16290
    },
    {
      "epoch": 1.2625922268053216,
      "grad_norm": 1.5088413953781128,
      "learning_rate": 1.7425317111053586e-05,
      "loss": 1.1835,
      "step": 16300
    },
    {
      "epoch": 1.2633668351439804,
      "grad_norm": 1.36732017993927,
      "learning_rate": 1.7417551126067823e-05,
      "loss": 1.148,
      "step": 16310
    },
    {
      "epoch": 1.264141443482639,
      "grad_norm": 1.6689026355743408,
      "learning_rate": 1.740978514108206e-05,
      "loss": 1.0905,
      "step": 16320
    },
    {
      "epoch": 1.2649160518212978,
      "grad_norm": 2.265307664871216,
      "learning_rate": 1.74020191560963e-05,
      "loss": 1.279,
      "step": 16330
    },
    {
      "epoch": 1.2656906601599567,
      "grad_norm": 1.8828943967819214,
      "learning_rate": 1.7394253171110537e-05,
      "loss": 1.1687,
      "step": 16340
    },
    {
      "epoch": 1.2664652684986155,
      "grad_norm": 1.8696578741073608,
      "learning_rate": 1.738648718612477e-05,
      "loss": 1.1574,
      "step": 16350
    },
    {
      "epoch": 1.2672398768372741,
      "grad_norm": 1.2886805534362793,
      "learning_rate": 1.737872120113901e-05,
      "loss": 1.1375,
      "step": 16360
    },
    {
      "epoch": 1.268014485175933,
      "grad_norm": 1.8995766639709473,
      "learning_rate": 1.7370955216153247e-05,
      "loss": 1.1681,
      "step": 16370
    },
    {
      "epoch": 1.2687890935145916,
      "grad_norm": 1.3171007633209229,
      "learning_rate": 1.7363189231167485e-05,
      "loss": 1.1543,
      "step": 16380
    },
    {
      "epoch": 1.2695637018532504,
      "grad_norm": 1.6325968503952026,
      "learning_rate": 1.7355423246181723e-05,
      "loss": 1.1763,
      "step": 16390
    },
    {
      "epoch": 1.2703383101919092,
      "grad_norm": 1.4913640022277832,
      "learning_rate": 1.734765726119596e-05,
      "loss": 1.155,
      "step": 16400
    },
    {
      "epoch": 1.271112918530568,
      "grad_norm": 1.3525818586349487,
      "learning_rate": 1.7339891276210202e-05,
      "loss": 1.2005,
      "step": 16410
    },
    {
      "epoch": 1.2718875268692267,
      "grad_norm": 1.2770315408706665,
      "learning_rate": 1.733212529122444e-05,
      "loss": 1.1903,
      "step": 16420
    },
    {
      "epoch": 1.2726621352078855,
      "grad_norm": 1.770729422569275,
      "learning_rate": 1.7324359306238678e-05,
      "loss": 1.2138,
      "step": 16430
    },
    {
      "epoch": 1.2734367435465443,
      "grad_norm": 1.3227245807647705,
      "learning_rate": 1.7316593321252912e-05,
      "loss": 1.1754,
      "step": 16440
    },
    {
      "epoch": 1.274211351885203,
      "grad_norm": 1.4994428157806396,
      "learning_rate": 1.730882733626715e-05,
      "loss": 1.0759,
      "step": 16450
    },
    {
      "epoch": 1.2749859602238618,
      "grad_norm": 1.4979002475738525,
      "learning_rate": 1.7301061351281388e-05,
      "loss": 1.183,
      "step": 16460
    },
    {
      "epoch": 1.2757605685625206,
      "grad_norm": 1.6998661756515503,
      "learning_rate": 1.7293295366295626e-05,
      "loss": 1.214,
      "step": 16470
    },
    {
      "epoch": 1.2765351769011795,
      "grad_norm": 1.538629412651062,
      "learning_rate": 1.7285529381309864e-05,
      "loss": 1.2438,
      "step": 16480
    },
    {
      "epoch": 1.277309785239838,
      "grad_norm": 1.4339566230773926,
      "learning_rate": 1.72777633963241e-05,
      "loss": 1.1449,
      "step": 16490
    },
    {
      "epoch": 1.278084393578497,
      "grad_norm": 1.4370567798614502,
      "learning_rate": 1.726999741133834e-05,
      "loss": 1.163,
      "step": 16500
    },
    {
      "epoch": 1.2788590019171555,
      "grad_norm": 1.4727368354797363,
      "learning_rate": 1.7262231426352577e-05,
      "loss": 1.2216,
      "step": 16510
    },
    {
      "epoch": 1.2796336102558143,
      "grad_norm": 1.5231499671936035,
      "learning_rate": 1.7254465441366812e-05,
      "loss": 1.1926,
      "step": 16520
    },
    {
      "epoch": 1.2804082185944732,
      "grad_norm": 1.6692712306976318,
      "learning_rate": 1.724669945638105e-05,
      "loss": 1.0954,
      "step": 16530
    },
    {
      "epoch": 1.281182826933132,
      "grad_norm": 1.4049392938613892,
      "learning_rate": 1.7238933471395288e-05,
      "loss": 1.0814,
      "step": 16540
    },
    {
      "epoch": 1.2819574352717906,
      "grad_norm": 1.3121590614318848,
      "learning_rate": 1.7231167486409525e-05,
      "loss": 1.1724,
      "step": 16550
    },
    {
      "epoch": 1.2827320436104495,
      "grad_norm": 1.2123191356658936,
      "learning_rate": 1.7223401501423763e-05,
      "loss": 1.2057,
      "step": 16560
    },
    {
      "epoch": 1.2835066519491083,
      "grad_norm": 1.4463043212890625,
      "learning_rate": 1.7215635516438e-05,
      "loss": 1.2292,
      "step": 16570
    },
    {
      "epoch": 1.284281260287767,
      "grad_norm": 1.2357425689697266,
      "learning_rate": 1.720786953145224e-05,
      "loss": 1.1505,
      "step": 16580
    },
    {
      "epoch": 1.2850558686264257,
      "grad_norm": 1.444731593132019,
      "learning_rate": 1.7200103546466477e-05,
      "loss": 1.2225,
      "step": 16590
    },
    {
      "epoch": 1.2858304769650846,
      "grad_norm": 1.6285637617111206,
      "learning_rate": 1.7192337561480718e-05,
      "loss": 1.2284,
      "step": 16600
    },
    {
      "epoch": 1.2866050853037434,
      "grad_norm": 1.95830237865448,
      "learning_rate": 1.7184571576494953e-05,
      "loss": 1.174,
      "step": 16610
    },
    {
      "epoch": 1.287379693642402,
      "grad_norm": 1.38027024269104,
      "learning_rate": 1.717680559150919e-05,
      "loss": 1.218,
      "step": 16620
    },
    {
      "epoch": 1.2881543019810608,
      "grad_norm": 1.4306151866912842,
      "learning_rate": 1.716903960652343e-05,
      "loss": 1.1574,
      "step": 16630
    },
    {
      "epoch": 1.2889289103197195,
      "grad_norm": 1.476659893989563,
      "learning_rate": 1.7161273621537666e-05,
      "loss": 1.1859,
      "step": 16640
    },
    {
      "epoch": 1.2897035186583783,
      "grad_norm": 1.7973847389221191,
      "learning_rate": 1.7153507636551904e-05,
      "loss": 1.1325,
      "step": 16650
    },
    {
      "epoch": 1.2904781269970371,
      "grad_norm": 1.297568440437317,
      "learning_rate": 1.7145741651566142e-05,
      "loss": 1.1634,
      "step": 16660
    },
    {
      "epoch": 1.291252735335696,
      "grad_norm": 1.802172064781189,
      "learning_rate": 1.713797566658038e-05,
      "loss": 1.0734,
      "step": 16670
    },
    {
      "epoch": 1.2920273436743548,
      "grad_norm": 1.5853898525238037,
      "learning_rate": 1.7130209681594618e-05,
      "loss": 1.2417,
      "step": 16680
    },
    {
      "epoch": 1.2928019520130134,
      "grad_norm": 1.7791261672973633,
      "learning_rate": 1.7122443696608852e-05,
      "loss": 1.1556,
      "step": 16690
    },
    {
      "epoch": 1.2935765603516722,
      "grad_norm": 2.078674077987671,
      "learning_rate": 1.711467771162309e-05,
      "loss": 1.1535,
      "step": 16700
    },
    {
      "epoch": 1.2943511686903308,
      "grad_norm": 2.090657949447632,
      "learning_rate": 1.7106911726637328e-05,
      "loss": 1.2145,
      "step": 16710
    },
    {
      "epoch": 1.2951257770289897,
      "grad_norm": 1.8146889209747314,
      "learning_rate": 1.7099145741651566e-05,
      "loss": 1.2082,
      "step": 16720
    },
    {
      "epoch": 1.2959003853676485,
      "grad_norm": 1.2631220817565918,
      "learning_rate": 1.7091379756665804e-05,
      "loss": 1.2283,
      "step": 16730
    },
    {
      "epoch": 1.2966749937063073,
      "grad_norm": 1.2590094804763794,
      "learning_rate": 1.708361377168004e-05,
      "loss": 1.1448,
      "step": 16740
    },
    {
      "epoch": 1.297449602044966,
      "grad_norm": 1.7376060485839844,
      "learning_rate": 1.707584778669428e-05,
      "loss": 1.177,
      "step": 16750
    },
    {
      "epoch": 1.2982242103836248,
      "grad_norm": 1.4124127626419067,
      "learning_rate": 1.7068081801708517e-05,
      "loss": 1.1262,
      "step": 16760
    },
    {
      "epoch": 1.2989988187222836,
      "grad_norm": 1.6589417457580566,
      "learning_rate": 1.7060315816722755e-05,
      "loss": 1.1224,
      "step": 16770
    },
    {
      "epoch": 1.2997734270609422,
      "grad_norm": 1.043051838874817,
      "learning_rate": 1.705254983173699e-05,
      "loss": 1.1914,
      "step": 16780
    },
    {
      "epoch": 1.300548035399601,
      "grad_norm": 1.7960126399993896,
      "learning_rate": 1.7044783846751227e-05,
      "loss": 1.1488,
      "step": 16790
    },
    {
      "epoch": 1.30132264373826,
      "grad_norm": 1.8783334493637085,
      "learning_rate": 1.703701786176547e-05,
      "loss": 1.1076,
      "step": 16800
    },
    {
      "epoch": 1.3020972520769187,
      "grad_norm": 1.4521150588989258,
      "learning_rate": 1.7029251876779707e-05,
      "loss": 1.2335,
      "step": 16810
    },
    {
      "epoch": 1.3028718604155773,
      "grad_norm": 1.287757396697998,
      "learning_rate": 1.7021485891793944e-05,
      "loss": 1.1686,
      "step": 16820
    },
    {
      "epoch": 1.3036464687542362,
      "grad_norm": 1.4637688398361206,
      "learning_rate": 1.7013719906808182e-05,
      "loss": 1.2378,
      "step": 16830
    },
    {
      "epoch": 1.3044210770928948,
      "grad_norm": 1.5052845478057861,
      "learning_rate": 1.700595392182242e-05,
      "loss": 1.1751,
      "step": 16840
    },
    {
      "epoch": 1.3051956854315536,
      "grad_norm": 1.2435206174850464,
      "learning_rate": 1.6998187936836658e-05,
      "loss": 1.1447,
      "step": 16850
    },
    {
      "epoch": 1.3059702937702125,
      "grad_norm": 1.7785656452178955,
      "learning_rate": 1.6990421951850893e-05,
      "loss": 1.262,
      "step": 16860
    },
    {
      "epoch": 1.3067449021088713,
      "grad_norm": 1.9841156005859375,
      "learning_rate": 1.698265596686513e-05,
      "loss": 1.1887,
      "step": 16870
    },
    {
      "epoch": 1.30751951044753,
      "grad_norm": 1.45492684841156,
      "learning_rate": 1.6974889981879368e-05,
      "loss": 1.2183,
      "step": 16880
    },
    {
      "epoch": 1.3082941187861887,
      "grad_norm": 2.066164493560791,
      "learning_rate": 1.6967123996893606e-05,
      "loss": 1.1724,
      "step": 16890
    },
    {
      "epoch": 1.3090687271248476,
      "grad_norm": 1.405561923980713,
      "learning_rate": 1.6959358011907844e-05,
      "loss": 1.1442,
      "step": 16900
    },
    {
      "epoch": 1.3098433354635062,
      "grad_norm": 1.5421628952026367,
      "learning_rate": 1.6951592026922082e-05,
      "loss": 1.1775,
      "step": 16910
    },
    {
      "epoch": 1.310617943802165,
      "grad_norm": 2.016294002532959,
      "learning_rate": 1.694382604193632e-05,
      "loss": 1.1423,
      "step": 16920
    },
    {
      "epoch": 1.3113925521408238,
      "grad_norm": 1.6963939666748047,
      "learning_rate": 1.6936060056950558e-05,
      "loss": 1.2525,
      "step": 16930
    },
    {
      "epoch": 1.3121671604794827,
      "grad_norm": 1.6818478107452393,
      "learning_rate": 1.6928294071964795e-05,
      "loss": 1.2148,
      "step": 16940
    },
    {
      "epoch": 1.3129417688181413,
      "grad_norm": 1.5870062112808228,
      "learning_rate": 1.692052808697903e-05,
      "loss": 1.113,
      "step": 16950
    },
    {
      "epoch": 1.3137163771568001,
      "grad_norm": 1.9127697944641113,
      "learning_rate": 1.6912762101993268e-05,
      "loss": 1.1329,
      "step": 16960
    },
    {
      "epoch": 1.3144909854954587,
      "grad_norm": 1.7571685314178467,
      "learning_rate": 1.6904996117007506e-05,
      "loss": 1.2189,
      "step": 16970
    },
    {
      "epoch": 1.3152655938341176,
      "grad_norm": 1.1957848072052002,
      "learning_rate": 1.6897230132021744e-05,
      "loss": 1.3101,
      "step": 16980
    },
    {
      "epoch": 1.3160402021727764,
      "grad_norm": 1.5036437511444092,
      "learning_rate": 1.688946414703598e-05,
      "loss": 1.1843,
      "step": 16990
    },
    {
      "epoch": 1.3168148105114352,
      "grad_norm": 1.6765468120574951,
      "learning_rate": 1.6881698162050223e-05,
      "loss": 1.1769,
      "step": 17000
    },
    {
      "epoch": 1.3175894188500938,
      "grad_norm": 1.4186980724334717,
      "learning_rate": 1.687393217706446e-05,
      "loss": 1.2172,
      "step": 17010
    },
    {
      "epoch": 1.3183640271887527,
      "grad_norm": 1.1039406061172485,
      "learning_rate": 1.68661661920787e-05,
      "loss": 1.1008,
      "step": 17020
    },
    {
      "epoch": 1.3191386355274115,
      "grad_norm": 1.669092059135437,
      "learning_rate": 1.6858400207092933e-05,
      "loss": 1.1603,
      "step": 17030
    },
    {
      "epoch": 1.3199132438660701,
      "grad_norm": 1.7455297708511353,
      "learning_rate": 1.685063422210717e-05,
      "loss": 1.1959,
      "step": 17040
    },
    {
      "epoch": 1.320687852204729,
      "grad_norm": 1.3536375761032104,
      "learning_rate": 1.684286823712141e-05,
      "loss": 1.1358,
      "step": 17050
    },
    {
      "epoch": 1.3214624605433878,
      "grad_norm": 1.258501648902893,
      "learning_rate": 1.6835102252135646e-05,
      "loss": 1.1363,
      "step": 17060
    },
    {
      "epoch": 1.3222370688820466,
      "grad_norm": 1.4548357725143433,
      "learning_rate": 1.6827336267149884e-05,
      "loss": 1.1315,
      "step": 17070
    },
    {
      "epoch": 1.3230116772207052,
      "grad_norm": 1.5700123310089111,
      "learning_rate": 1.6819570282164122e-05,
      "loss": 1.1862,
      "step": 17080
    },
    {
      "epoch": 1.323786285559364,
      "grad_norm": 1.5500829219818115,
      "learning_rate": 1.681180429717836e-05,
      "loss": 1.1097,
      "step": 17090
    },
    {
      "epoch": 1.324560893898023,
      "grad_norm": 1.7308266162872314,
      "learning_rate": 1.6804038312192598e-05,
      "loss": 1.2479,
      "step": 17100
    },
    {
      "epoch": 1.3253355022366815,
      "grad_norm": 1.4648066759109497,
      "learning_rate": 1.6796272327206836e-05,
      "loss": 1.1843,
      "step": 17110
    },
    {
      "epoch": 1.3261101105753403,
      "grad_norm": 1.5380254983901978,
      "learning_rate": 1.678850634222107e-05,
      "loss": 1.2207,
      "step": 17120
    },
    {
      "epoch": 1.3268847189139992,
      "grad_norm": 1.6303460597991943,
      "learning_rate": 1.6780740357235308e-05,
      "loss": 1.1703,
      "step": 17130
    },
    {
      "epoch": 1.327659327252658,
      "grad_norm": 2.0713837146759033,
      "learning_rate": 1.6772974372249546e-05,
      "loss": 1.11,
      "step": 17140
    },
    {
      "epoch": 1.3284339355913166,
      "grad_norm": 2.1651580333709717,
      "learning_rate": 1.6765208387263784e-05,
      "loss": 1.2133,
      "step": 17150
    },
    {
      "epoch": 1.3292085439299755,
      "grad_norm": 1.6098870038986206,
      "learning_rate": 1.6757442402278022e-05,
      "loss": 1.2944,
      "step": 17160
    },
    {
      "epoch": 1.329983152268634,
      "grad_norm": 1.6541879177093506,
      "learning_rate": 1.674967641729226e-05,
      "loss": 1.1241,
      "step": 17170
    },
    {
      "epoch": 1.330757760607293,
      "grad_norm": 1.273677110671997,
      "learning_rate": 1.6741910432306497e-05,
      "loss": 1.1898,
      "step": 17180
    },
    {
      "epoch": 1.3315323689459517,
      "grad_norm": 1.494936466217041,
      "learning_rate": 1.673414444732074e-05,
      "loss": 1.0893,
      "step": 17190
    },
    {
      "epoch": 1.3323069772846106,
      "grad_norm": 1.7120232582092285,
      "learning_rate": 1.6726378462334973e-05,
      "loss": 1.1497,
      "step": 17200
    },
    {
      "epoch": 1.3330815856232692,
      "grad_norm": 1.2351353168487549,
      "learning_rate": 1.671861247734921e-05,
      "loss": 1.1902,
      "step": 17210
    },
    {
      "epoch": 1.333856193961928,
      "grad_norm": 1.3086258172988892,
      "learning_rate": 1.671084649236345e-05,
      "loss": 1.1399,
      "step": 17220
    },
    {
      "epoch": 1.3346308023005868,
      "grad_norm": 1.3906753063201904,
      "learning_rate": 1.6703080507377687e-05,
      "loss": 1.2092,
      "step": 17230
    },
    {
      "epoch": 1.3354054106392454,
      "grad_norm": 1.6745901107788086,
      "learning_rate": 1.6695314522391925e-05,
      "loss": 1.1944,
      "step": 17240
    },
    {
      "epoch": 1.3361800189779043,
      "grad_norm": 1.3404914140701294,
      "learning_rate": 1.6687548537406163e-05,
      "loss": 1.1108,
      "step": 17250
    },
    {
      "epoch": 1.3369546273165631,
      "grad_norm": 1.7979673147201538,
      "learning_rate": 1.66797825524204e-05,
      "loss": 1.2092,
      "step": 17260
    },
    {
      "epoch": 1.337729235655222,
      "grad_norm": 2.1443793773651123,
      "learning_rate": 1.6672016567434638e-05,
      "loss": 1.1743,
      "step": 17270
    },
    {
      "epoch": 1.3385038439938806,
      "grad_norm": 1.5867140293121338,
      "learning_rate": 1.6664250582448873e-05,
      "loss": 1.2431,
      "step": 17280
    },
    {
      "epoch": 1.3392784523325394,
      "grad_norm": 1.570101261138916,
      "learning_rate": 1.665648459746311e-05,
      "loss": 1.1468,
      "step": 17290
    },
    {
      "epoch": 1.340053060671198,
      "grad_norm": 1.4759666919708252,
      "learning_rate": 1.664871861247735e-05,
      "loss": 1.2007,
      "step": 17300
    },
    {
      "epoch": 1.3408276690098568,
      "grad_norm": 1.631531834602356,
      "learning_rate": 1.6640952627491586e-05,
      "loss": 1.1222,
      "step": 17310
    },
    {
      "epoch": 1.3416022773485157,
      "grad_norm": 1.831348180770874,
      "learning_rate": 1.6633186642505824e-05,
      "loss": 1.1929,
      "step": 17320
    },
    {
      "epoch": 1.3423768856871745,
      "grad_norm": 1.4950122833251953,
      "learning_rate": 1.6625420657520062e-05,
      "loss": 1.2551,
      "step": 17330
    },
    {
      "epoch": 1.3431514940258331,
      "grad_norm": 1.6792374849319458,
      "learning_rate": 1.66176546725343e-05,
      "loss": 1.2118,
      "step": 17340
    },
    {
      "epoch": 1.343926102364492,
      "grad_norm": 1.2535532712936401,
      "learning_rate": 1.6609888687548538e-05,
      "loss": 1.1206,
      "step": 17350
    },
    {
      "epoch": 1.3447007107031508,
      "grad_norm": 2.184292793273926,
      "learning_rate": 1.6602122702562776e-05,
      "loss": 1.1953,
      "step": 17360
    },
    {
      "epoch": 1.3454753190418094,
      "grad_norm": 1.283692717552185,
      "learning_rate": 1.659435671757701e-05,
      "loss": 1.1441,
      "step": 17370
    },
    {
      "epoch": 1.3462499273804682,
      "grad_norm": 1.7238490581512451,
      "learning_rate": 1.6586590732591248e-05,
      "loss": 1.1796,
      "step": 17380
    },
    {
      "epoch": 1.347024535719127,
      "grad_norm": 1.6333351135253906,
      "learning_rate": 1.657882474760549e-05,
      "loss": 1.1493,
      "step": 17390
    },
    {
      "epoch": 1.347799144057786,
      "grad_norm": 1.471422553062439,
      "learning_rate": 1.6571058762619727e-05,
      "loss": 1.2432,
      "step": 17400
    },
    {
      "epoch": 1.3485737523964445,
      "grad_norm": 1.5555198192596436,
      "learning_rate": 1.6563292777633965e-05,
      "loss": 1.2626,
      "step": 17410
    },
    {
      "epoch": 1.3493483607351033,
      "grad_norm": 1.340223789215088,
      "learning_rate": 1.6555526792648203e-05,
      "loss": 1.184,
      "step": 17420
    },
    {
      "epoch": 1.350122969073762,
      "grad_norm": 1.8410378694534302,
      "learning_rate": 1.654776080766244e-05,
      "loss": 1.056,
      "step": 17430
    },
    {
      "epoch": 1.3508975774124208,
      "grad_norm": 1.4721640348434448,
      "learning_rate": 1.653999482267668e-05,
      "loss": 1.2455,
      "step": 17440
    },
    {
      "epoch": 1.3516721857510796,
      "grad_norm": 1.316541075706482,
      "learning_rate": 1.6532228837690913e-05,
      "loss": 1.1694,
      "step": 17450
    },
    {
      "epoch": 1.3524467940897384,
      "grad_norm": 1.6929031610488892,
      "learning_rate": 1.652446285270515e-05,
      "loss": 1.2261,
      "step": 17460
    },
    {
      "epoch": 1.3532214024283973,
      "grad_norm": 1.986008882522583,
      "learning_rate": 1.651669686771939e-05,
      "loss": 1.2271,
      "step": 17470
    },
    {
      "epoch": 1.353996010767056,
      "grad_norm": 1.3161033391952515,
      "learning_rate": 1.6508930882733627e-05,
      "loss": 1.2151,
      "step": 17480
    },
    {
      "epoch": 1.3547706191057147,
      "grad_norm": 1.8131952285766602,
      "learning_rate": 1.6501164897747865e-05,
      "loss": 1.1787,
      "step": 17490
    },
    {
      "epoch": 1.3555452274443733,
      "grad_norm": 1.4154285192489624,
      "learning_rate": 1.6493398912762102e-05,
      "loss": 1.1359,
      "step": 17500
    },
    {
      "epoch": 1.3563198357830322,
      "grad_norm": 1.220241665840149,
      "learning_rate": 1.648563292777634e-05,
      "loss": 1.2609,
      "step": 17510
    },
    {
      "epoch": 1.357094444121691,
      "grad_norm": 1.515673279762268,
      "learning_rate": 1.6477866942790578e-05,
      "loss": 1.1822,
      "step": 17520
    },
    {
      "epoch": 1.3578690524603498,
      "grad_norm": 1.4630457162857056,
      "learning_rate": 1.6470100957804816e-05,
      "loss": 1.2522,
      "step": 17530
    },
    {
      "epoch": 1.3586436607990084,
      "grad_norm": 1.6213260889053345,
      "learning_rate": 1.646233497281905e-05,
      "loss": 1.2226,
      "step": 17540
    },
    {
      "epoch": 1.3594182691376673,
      "grad_norm": 1.349286437034607,
      "learning_rate": 1.645456898783329e-05,
      "loss": 1.128,
      "step": 17550
    },
    {
      "epoch": 1.3601928774763261,
      "grad_norm": 1.8843060731887817,
      "learning_rate": 1.6446803002847526e-05,
      "loss": 1.1969,
      "step": 17560
    },
    {
      "epoch": 1.3609674858149847,
      "grad_norm": 1.968342661857605,
      "learning_rate": 1.6439037017861764e-05,
      "loss": 1.1375,
      "step": 17570
    },
    {
      "epoch": 1.3617420941536436,
      "grad_norm": 1.3131049871444702,
      "learning_rate": 1.6431271032876005e-05,
      "loss": 1.168,
      "step": 17580
    },
    {
      "epoch": 1.3625167024923024,
      "grad_norm": 1.4292895793914795,
      "learning_rate": 1.6423505047890243e-05,
      "loss": 1.072,
      "step": 17590
    },
    {
      "epoch": 1.3632913108309612,
      "grad_norm": 1.7530543804168701,
      "learning_rate": 1.641573906290448e-05,
      "loss": 1.2201,
      "step": 17600
    },
    {
      "epoch": 1.3640659191696198,
      "grad_norm": 1.695458173751831,
      "learning_rate": 1.640797307791872e-05,
      "loss": 1.1886,
      "step": 17610
    },
    {
      "epoch": 1.3648405275082787,
      "grad_norm": 2.048473358154297,
      "learning_rate": 1.6400207092932953e-05,
      "loss": 1.1999,
      "step": 17620
    },
    {
      "epoch": 1.3656151358469373,
      "grad_norm": 1.6288284063339233,
      "learning_rate": 1.639244110794719e-05,
      "loss": 1.2607,
      "step": 17630
    },
    {
      "epoch": 1.3663897441855961,
      "grad_norm": 1.5905243158340454,
      "learning_rate": 1.638467512296143e-05,
      "loss": 1.1802,
      "step": 17640
    },
    {
      "epoch": 1.367164352524255,
      "grad_norm": 1.9066853523254395,
      "learning_rate": 1.6376909137975667e-05,
      "loss": 1.175,
      "step": 17650
    },
    {
      "epoch": 1.3679389608629138,
      "grad_norm": 2.3095901012420654,
      "learning_rate": 1.6369143152989905e-05,
      "loss": 1.1177,
      "step": 17660
    },
    {
      "epoch": 1.3687135692015724,
      "grad_norm": 1.961869478225708,
      "learning_rate": 1.6361377168004143e-05,
      "loss": 1.228,
      "step": 17670
    },
    {
      "epoch": 1.3694881775402312,
      "grad_norm": 1.7396830320358276,
      "learning_rate": 1.635361118301838e-05,
      "loss": 1.0957,
      "step": 17680
    },
    {
      "epoch": 1.37026278587889,
      "grad_norm": 1.6797884702682495,
      "learning_rate": 1.634584519803262e-05,
      "loss": 1.1327,
      "step": 17690
    },
    {
      "epoch": 1.3710373942175487,
      "grad_norm": 1.609644889831543,
      "learning_rate": 1.6338079213046856e-05,
      "loss": 1.1987,
      "step": 17700
    },
    {
      "epoch": 1.3718120025562075,
      "grad_norm": 1.5314170122146606,
      "learning_rate": 1.633031322806109e-05,
      "loss": 1.1625,
      "step": 17710
    },
    {
      "epoch": 1.3725866108948663,
      "grad_norm": 1.909666657447815,
      "learning_rate": 1.632254724307533e-05,
      "loss": 1.2147,
      "step": 17720
    },
    {
      "epoch": 1.3733612192335252,
      "grad_norm": 2.919865846633911,
      "learning_rate": 1.6314781258089567e-05,
      "loss": 1.2086,
      "step": 17730
    },
    {
      "epoch": 1.3741358275721838,
      "grad_norm": 1.3836050033569336,
      "learning_rate": 1.6307015273103804e-05,
      "loss": 1.2075,
      "step": 17740
    },
    {
      "epoch": 1.3749104359108426,
      "grad_norm": 1.504400372505188,
      "learning_rate": 1.6299249288118042e-05,
      "loss": 1.1516,
      "step": 17750
    },
    {
      "epoch": 1.3756850442495012,
      "grad_norm": 1.3992623090744019,
      "learning_rate": 1.629148330313228e-05,
      "loss": 1.1748,
      "step": 17760
    },
    {
      "epoch": 1.37645965258816,
      "grad_norm": 1.182347059249878,
      "learning_rate": 1.6283717318146518e-05,
      "loss": 1.1979,
      "step": 17770
    },
    {
      "epoch": 1.3772342609268189,
      "grad_norm": 1.4721126556396484,
      "learning_rate": 1.627595133316076e-05,
      "loss": 1.1639,
      "step": 17780
    },
    {
      "epoch": 1.3780088692654777,
      "grad_norm": 2.4704504013061523,
      "learning_rate": 1.6268185348174994e-05,
      "loss": 1.2758,
      "step": 17790
    },
    {
      "epoch": 1.3787834776041363,
      "grad_norm": 1.9987117052078247,
      "learning_rate": 1.626041936318923e-05,
      "loss": 1.2622,
      "step": 17800
    },
    {
      "epoch": 1.3795580859427952,
      "grad_norm": 1.542371153831482,
      "learning_rate": 1.625265337820347e-05,
      "loss": 1.1539,
      "step": 17810
    },
    {
      "epoch": 1.380332694281454,
      "grad_norm": 1.7076325416564941,
      "learning_rate": 1.6244887393217707e-05,
      "loss": 1.1414,
      "step": 17820
    },
    {
      "epoch": 1.3811073026201126,
      "grad_norm": 1.4627126455307007,
      "learning_rate": 1.6237121408231945e-05,
      "loss": 1.1159,
      "step": 17830
    },
    {
      "epoch": 1.3818819109587714,
      "grad_norm": 1.5593280792236328,
      "learning_rate": 1.6229355423246183e-05,
      "loss": 1.1507,
      "step": 17840
    },
    {
      "epoch": 1.3826565192974303,
      "grad_norm": 2.487013101577759,
      "learning_rate": 1.622158943826042e-05,
      "loss": 1.2666,
      "step": 17850
    },
    {
      "epoch": 1.383431127636089,
      "grad_norm": 1.3427280187606812,
      "learning_rate": 1.621382345327466e-05,
      "loss": 1.2162,
      "step": 17860
    },
    {
      "epoch": 1.3842057359747477,
      "grad_norm": 1.8813915252685547,
      "learning_rate": 1.6206057468288897e-05,
      "loss": 1.141,
      "step": 17870
    },
    {
      "epoch": 1.3849803443134066,
      "grad_norm": 1.3703006505966187,
      "learning_rate": 1.619829148330313e-05,
      "loss": 1.1714,
      "step": 17880
    },
    {
      "epoch": 1.3857549526520652,
      "grad_norm": 1.2997281551361084,
      "learning_rate": 1.619052549831737e-05,
      "loss": 1.1313,
      "step": 17890
    },
    {
      "epoch": 1.386529560990724,
      "grad_norm": 1.3509330749511719,
      "learning_rate": 1.6182759513331607e-05,
      "loss": 1.2239,
      "step": 17900
    },
    {
      "epoch": 1.3873041693293828,
      "grad_norm": 1.9073278903961182,
      "learning_rate": 1.6174993528345845e-05,
      "loss": 1.1909,
      "step": 17910
    },
    {
      "epoch": 1.3880787776680417,
      "grad_norm": 1.4119588136672974,
      "learning_rate": 1.6167227543360083e-05,
      "loss": 1.2278,
      "step": 17920
    },
    {
      "epoch": 1.3888533860067005,
      "grad_norm": 2.144334316253662,
      "learning_rate": 1.615946155837432e-05,
      "loss": 1.2096,
      "step": 17930
    },
    {
      "epoch": 1.389627994345359,
      "grad_norm": 1.345781922340393,
      "learning_rate": 1.615169557338856e-05,
      "loss": 1.277,
      "step": 17940
    },
    {
      "epoch": 1.390402602684018,
      "grad_norm": 1.575779676437378,
      "learning_rate": 1.6143929588402796e-05,
      "loss": 1.1586,
      "step": 17950
    },
    {
      "epoch": 1.3911772110226766,
      "grad_norm": 1.474792242050171,
      "learning_rate": 1.613616360341703e-05,
      "loss": 1.1957,
      "step": 17960
    },
    {
      "epoch": 1.3919518193613354,
      "grad_norm": 1.5581872463226318,
      "learning_rate": 1.612839761843127e-05,
      "loss": 1.256,
      "step": 17970
    },
    {
      "epoch": 1.3927264276999942,
      "grad_norm": 1.3723634481430054,
      "learning_rate": 1.612063163344551e-05,
      "loss": 1.2072,
      "step": 17980
    },
    {
      "epoch": 1.393501036038653,
      "grad_norm": 1.6619006395339966,
      "learning_rate": 1.6112865648459748e-05,
      "loss": 1.208,
      "step": 17990
    },
    {
      "epoch": 1.3942756443773117,
      "grad_norm": 1.8784884214401245,
      "learning_rate": 1.6105099663473986e-05,
      "loss": 1.1856,
      "step": 18000
    },
    {
      "epoch": 1.3950502527159705,
      "grad_norm": 2.01887583732605,
      "learning_rate": 1.6097333678488223e-05,
      "loss": 1.1937,
      "step": 18010
    },
    {
      "epoch": 1.3958248610546293,
      "grad_norm": 1.569223403930664,
      "learning_rate": 1.608956769350246e-05,
      "loss": 1.1637,
      "step": 18020
    },
    {
      "epoch": 1.396599469393288,
      "grad_norm": 1.4419736862182617,
      "learning_rate": 1.60818017085167e-05,
      "loss": 1.1476,
      "step": 18030
    },
    {
      "epoch": 1.3973740777319468,
      "grad_norm": 1.0679898262023926,
      "learning_rate": 1.6074035723530937e-05,
      "loss": 1.1436,
      "step": 18040
    },
    {
      "epoch": 1.3981486860706056,
      "grad_norm": 1.7791815996170044,
      "learning_rate": 1.606626973854517e-05,
      "loss": 1.1947,
      "step": 18050
    },
    {
      "epoch": 1.3989232944092644,
      "grad_norm": 1.766324520111084,
      "learning_rate": 1.605850375355941e-05,
      "loss": 1.2507,
      "step": 18060
    },
    {
      "epoch": 1.399697902747923,
      "grad_norm": 1.2531101703643799,
      "learning_rate": 1.6050737768573647e-05,
      "loss": 1.1798,
      "step": 18070
    },
    {
      "epoch": 1.4004725110865819,
      "grad_norm": 1.1640907526016235,
      "learning_rate": 1.6042971783587885e-05,
      "loss": 1.1872,
      "step": 18080
    },
    {
      "epoch": 1.4012471194252405,
      "grad_norm": 1.1259846687316895,
      "learning_rate": 1.6035205798602123e-05,
      "loss": 1.2272,
      "step": 18090
    },
    {
      "epoch": 1.4020217277638993,
      "grad_norm": 1.4862505197525024,
      "learning_rate": 1.602743981361636e-05,
      "loss": 1.2401,
      "step": 18100
    },
    {
      "epoch": 1.4027963361025582,
      "grad_norm": 1.1310335397720337,
      "learning_rate": 1.60196738286306e-05,
      "loss": 1.12,
      "step": 18110
    },
    {
      "epoch": 1.403570944441217,
      "grad_norm": 1.5605731010437012,
      "learning_rate": 1.6011907843644837e-05,
      "loss": 1.2173,
      "step": 18120
    },
    {
      "epoch": 1.4043455527798756,
      "grad_norm": 1.6833953857421875,
      "learning_rate": 1.600414185865907e-05,
      "loss": 1.1612,
      "step": 18130
    },
    {
      "epoch": 1.4051201611185344,
      "grad_norm": 1.3382775783538818,
      "learning_rate": 1.599637587367331e-05,
      "loss": 1.1696,
      "step": 18140
    },
    {
      "epoch": 1.4058947694571933,
      "grad_norm": 1.641879916191101,
      "learning_rate": 1.5988609888687547e-05,
      "loss": 1.1652,
      "step": 18150
    },
    {
      "epoch": 1.4066693777958519,
      "grad_norm": 1.7732611894607544,
      "learning_rate": 1.5980843903701785e-05,
      "loss": 1.1781,
      "step": 18160
    },
    {
      "epoch": 1.4074439861345107,
      "grad_norm": 1.4536113739013672,
      "learning_rate": 1.5973077918716026e-05,
      "loss": 1.149,
      "step": 18170
    },
    {
      "epoch": 1.4082185944731695,
      "grad_norm": 1.203270673751831,
      "learning_rate": 1.596608853222884e-05,
      "loss": 1.2381,
      "step": 18180
    },
    {
      "epoch": 1.4089932028118284,
      "grad_norm": 1.5001276731491089,
      "learning_rate": 1.5958322547243077e-05,
      "loss": 1.1466,
      "step": 18190
    },
    {
      "epoch": 1.409767811150487,
      "grad_norm": 1.251029372215271,
      "learning_rate": 1.5950556562257315e-05,
      "loss": 1.1638,
      "step": 18200
    },
    {
      "epoch": 1.4105424194891458,
      "grad_norm": 2.3466804027557373,
      "learning_rate": 1.594279057727155e-05,
      "loss": 1.2185,
      "step": 18210
    },
    {
      "epoch": 1.4113170278278044,
      "grad_norm": 1.7689069509506226,
      "learning_rate": 1.5935024592285787e-05,
      "loss": 1.0768,
      "step": 18220
    },
    {
      "epoch": 1.4120916361664633,
      "grad_norm": 1.6567431688308716,
      "learning_rate": 1.5927258607300025e-05,
      "loss": 1.2553,
      "step": 18230
    },
    {
      "epoch": 1.412866244505122,
      "grad_norm": 1.4044990539550781,
      "learning_rate": 1.5919492622314263e-05,
      "loss": 1.2064,
      "step": 18240
    },
    {
      "epoch": 1.413640852843781,
      "grad_norm": 1.2360585927963257,
      "learning_rate": 1.59117266373285e-05,
      "loss": 1.1531,
      "step": 18250
    },
    {
      "epoch": 1.4144154611824395,
      "grad_norm": 1.410644292831421,
      "learning_rate": 1.590396065234274e-05,
      "loss": 1.1196,
      "step": 18260
    },
    {
      "epoch": 1.4151900695210984,
      "grad_norm": 1.8348115682601929,
      "learning_rate": 1.5896194667356977e-05,
      "loss": 1.2562,
      "step": 18270
    },
    {
      "epoch": 1.4159646778597572,
      "grad_norm": 1.252956748008728,
      "learning_rate": 1.5888428682371215e-05,
      "loss": 1.2641,
      "step": 18280
    },
    {
      "epoch": 1.4167392861984158,
      "grad_norm": 1.6952762603759766,
      "learning_rate": 1.588066269738545e-05,
      "loss": 1.2303,
      "step": 18290
    },
    {
      "epoch": 1.4175138945370747,
      "grad_norm": 1.5729223489761353,
      "learning_rate": 1.5872896712399687e-05,
      "loss": 1.0841,
      "step": 18300
    },
    {
      "epoch": 1.4182885028757335,
      "grad_norm": 1.623552918434143,
      "learning_rate": 1.5865130727413928e-05,
      "loss": 1.225,
      "step": 18310
    },
    {
      "epoch": 1.4190631112143923,
      "grad_norm": 1.4450725317001343,
      "learning_rate": 1.5857364742428166e-05,
      "loss": 1.2584,
      "step": 18320
    },
    {
      "epoch": 1.419837719553051,
      "grad_norm": 1.5556267499923706,
      "learning_rate": 1.5849598757442404e-05,
      "loss": 1.1633,
      "step": 18330
    },
    {
      "epoch": 1.4206123278917098,
      "grad_norm": 1.6512387990951538,
      "learning_rate": 1.5841832772456642e-05,
      "loss": 1.2084,
      "step": 18340
    },
    {
      "epoch": 1.4213869362303686,
      "grad_norm": 1.7319517135620117,
      "learning_rate": 1.583406678747088e-05,
      "loss": 1.2257,
      "step": 18350
    },
    {
      "epoch": 1.4221615445690272,
      "grad_norm": 1.5924055576324463,
      "learning_rate": 1.5826300802485118e-05,
      "loss": 1.257,
      "step": 18360
    },
    {
      "epoch": 1.422936152907686,
      "grad_norm": 1.5653738975524902,
      "learning_rate": 1.5818534817499355e-05,
      "loss": 1.1526,
      "step": 18370
    },
    {
      "epoch": 1.4237107612463449,
      "grad_norm": 1.4122897386550903,
      "learning_rate": 1.581076883251359e-05,
      "loss": 1.096,
      "step": 18380
    },
    {
      "epoch": 1.4244853695850037,
      "grad_norm": 1.5126456022262573,
      "learning_rate": 1.5803002847527828e-05,
      "loss": 1.1272,
      "step": 18390
    },
    {
      "epoch": 1.4252599779236623,
      "grad_norm": 1.575957179069519,
      "learning_rate": 1.5795236862542066e-05,
      "loss": 1.2091,
      "step": 18400
    },
    {
      "epoch": 1.4260345862623212,
      "grad_norm": 1.289339542388916,
      "learning_rate": 1.5787470877556303e-05,
      "loss": 1.1805,
      "step": 18410
    },
    {
      "epoch": 1.4268091946009798,
      "grad_norm": 1.6061128377914429,
      "learning_rate": 1.577970489257054e-05,
      "loss": 1.1622,
      "step": 18420
    },
    {
      "epoch": 1.4275838029396386,
      "grad_norm": 1.374699592590332,
      "learning_rate": 1.577193890758478e-05,
      "loss": 1.1951,
      "step": 18430
    },
    {
      "epoch": 1.4283584112782974,
      "grad_norm": 2.5425004959106445,
      "learning_rate": 1.5764172922599017e-05,
      "loss": 1.3349,
      "step": 18440
    },
    {
      "epoch": 1.4291330196169563,
      "grad_norm": 1.6125948429107666,
      "learning_rate": 1.5756406937613255e-05,
      "loss": 1.0777,
      "step": 18450
    },
    {
      "epoch": 1.4299076279556149,
      "grad_norm": 1.624509572982788,
      "learning_rate": 1.574864095262749e-05,
      "loss": 1.162,
      "step": 18460
    },
    {
      "epoch": 1.4306822362942737,
      "grad_norm": 4.05519437789917,
      "learning_rate": 1.5740874967641727e-05,
      "loss": 1.1596,
      "step": 18470
    },
    {
      "epoch": 1.4314568446329325,
      "grad_norm": 1.5860090255737305,
      "learning_rate": 1.5733108982655965e-05,
      "loss": 1.0946,
      "step": 18480
    },
    {
      "epoch": 1.4322314529715912,
      "grad_norm": 1.7722821235656738,
      "learning_rate": 1.5725342997670203e-05,
      "loss": 1.2371,
      "step": 18490
    },
    {
      "epoch": 1.43300606131025,
      "grad_norm": 1.6958696842193604,
      "learning_rate": 1.571757701268444e-05,
      "loss": 1.1713,
      "step": 18500
    },
    {
      "epoch": 1.4337806696489088,
      "grad_norm": 1.3820544481277466,
      "learning_rate": 1.5709811027698682e-05,
      "loss": 1.2055,
      "step": 18510
    },
    {
      "epoch": 1.4345552779875677,
      "grad_norm": 1.7809375524520874,
      "learning_rate": 1.570204504271292e-05,
      "loss": 1.1551,
      "step": 18520
    },
    {
      "epoch": 1.4353298863262263,
      "grad_norm": 2.171773672103882,
      "learning_rate": 1.5694279057727158e-05,
      "loss": 1.1537,
      "step": 18530
    },
    {
      "epoch": 1.436104494664885,
      "grad_norm": 1.1766437292099,
      "learning_rate": 1.5686513072741396e-05,
      "loss": 1.1905,
      "step": 18540
    },
    {
      "epoch": 1.4368791030035437,
      "grad_norm": 1.3111470937728882,
      "learning_rate": 1.567874708775563e-05,
      "loss": 1.168,
      "step": 18550
    },
    {
      "epoch": 1.4376537113422025,
      "grad_norm": 1.4107763767242432,
      "learning_rate": 1.5670981102769868e-05,
      "loss": 1.183,
      "step": 18560
    },
    {
      "epoch": 1.4384283196808614,
      "grad_norm": 1.3295812606811523,
      "learning_rate": 1.5663215117784106e-05,
      "loss": 1.2284,
      "step": 18570
    },
    {
      "epoch": 1.4392029280195202,
      "grad_norm": 1.2726290225982666,
      "learning_rate": 1.5655449132798344e-05,
      "loss": 1.1321,
      "step": 18580
    },
    {
      "epoch": 1.4399775363581788,
      "grad_norm": 1.4381250143051147,
      "learning_rate": 1.564768314781258e-05,
      "loss": 1.2516,
      "step": 18590
    },
    {
      "epoch": 1.4407521446968377,
      "grad_norm": 1.422793984413147,
      "learning_rate": 1.563991716282682e-05,
      "loss": 1.1063,
      "step": 18600
    },
    {
      "epoch": 1.4415267530354965,
      "grad_norm": 1.42891263961792,
      "learning_rate": 1.5632151177841057e-05,
      "loss": 1.2765,
      "step": 18610
    },
    {
      "epoch": 1.442301361374155,
      "grad_norm": 1.4769291877746582,
      "learning_rate": 1.5624385192855295e-05,
      "loss": 1.1573,
      "step": 18620
    },
    {
      "epoch": 1.443075969712814,
      "grad_norm": 1.6168293952941895,
      "learning_rate": 1.561661920786953e-05,
      "loss": 1.2775,
      "step": 18630
    },
    {
      "epoch": 1.4438505780514728,
      "grad_norm": 1.7931296825408936,
      "learning_rate": 1.5608853222883768e-05,
      "loss": 1.1667,
      "step": 18640
    },
    {
      "epoch": 1.4446251863901316,
      "grad_norm": 1.0924967527389526,
      "learning_rate": 1.5601087237898005e-05,
      "loss": 1.2035,
      "step": 18650
    },
    {
      "epoch": 1.4453997947287902,
      "grad_norm": 1.4914746284484863,
      "learning_rate": 1.5593321252912243e-05,
      "loss": 1.188,
      "step": 18660
    },
    {
      "epoch": 1.446174403067449,
      "grad_norm": 1.2820831537246704,
      "learning_rate": 1.558555526792648e-05,
      "loss": 1.1767,
      "step": 18670
    },
    {
      "epoch": 1.4469490114061077,
      "grad_norm": 1.1771868467330933,
      "learning_rate": 1.557778928294072e-05,
      "loss": 1.057,
      "step": 18680
    },
    {
      "epoch": 1.4477236197447665,
      "grad_norm": 1.5508055686950684,
      "learning_rate": 1.5570023297954957e-05,
      "loss": 1.2223,
      "step": 18690
    },
    {
      "epoch": 1.4484982280834253,
      "grad_norm": 1.2951489686965942,
      "learning_rate": 1.5562257312969198e-05,
      "loss": 1.1506,
      "step": 18700
    },
    {
      "epoch": 1.4492728364220842,
      "grad_norm": 1.8220405578613281,
      "learning_rate": 1.5554491327983436e-05,
      "loss": 1.1142,
      "step": 18710
    },
    {
      "epoch": 1.450047444760743,
      "grad_norm": 1.5116186141967773,
      "learning_rate": 1.554672534299767e-05,
      "loss": 1.1832,
      "step": 18720
    },
    {
      "epoch": 1.4508220530994016,
      "grad_norm": 1.4905041456222534,
      "learning_rate": 1.553895935801191e-05,
      "loss": 1.1465,
      "step": 18730
    },
    {
      "epoch": 1.4515966614380604,
      "grad_norm": 1.9451991319656372,
      "learning_rate": 1.5531193373026146e-05,
      "loss": 1.2046,
      "step": 18740
    },
    {
      "epoch": 1.452371269776719,
      "grad_norm": 1.8359922170639038,
      "learning_rate": 1.5523427388040384e-05,
      "loss": 1.2342,
      "step": 18750
    },
    {
      "epoch": 1.4531458781153779,
      "grad_norm": 1.5428287982940674,
      "learning_rate": 1.5515661403054622e-05,
      "loss": 1.2472,
      "step": 18760
    },
    {
      "epoch": 1.4539204864540367,
      "grad_norm": 1.9616408348083496,
      "learning_rate": 1.550789541806886e-05,
      "loss": 1.1281,
      "step": 18770
    },
    {
      "epoch": 1.4546950947926955,
      "grad_norm": 1.5533968210220337,
      "learning_rate": 1.5500129433083098e-05,
      "loss": 1.2332,
      "step": 18780
    },
    {
      "epoch": 1.4554697031313542,
      "grad_norm": 1.8998112678527832,
      "learning_rate": 1.5492363448097336e-05,
      "loss": 1.2479,
      "step": 18790
    },
    {
      "epoch": 1.456244311470013,
      "grad_norm": 1.435770034790039,
      "learning_rate": 1.548459746311157e-05,
      "loss": 1.2017,
      "step": 18800
    },
    {
      "epoch": 1.4570189198086718,
      "grad_norm": 1.6794517040252686,
      "learning_rate": 1.5476831478125808e-05,
      "loss": 1.1814,
      "step": 18810
    },
    {
      "epoch": 1.4577935281473304,
      "grad_norm": 1.5870121717453003,
      "learning_rate": 1.5469065493140046e-05,
      "loss": 1.2453,
      "step": 18820
    },
    {
      "epoch": 1.4585681364859893,
      "grad_norm": 1.3018161058425903,
      "learning_rate": 1.5461299508154284e-05,
      "loss": 1.2505,
      "step": 18830
    },
    {
      "epoch": 1.459342744824648,
      "grad_norm": 1.8408819437026978,
      "learning_rate": 1.545353352316852e-05,
      "loss": 1.1741,
      "step": 18840
    },
    {
      "epoch": 1.460117353163307,
      "grad_norm": 1.6905559301376343,
      "learning_rate": 1.544576753818276e-05,
      "loss": 1.2332,
      "step": 18850
    },
    {
      "epoch": 1.4608919615019655,
      "grad_norm": 1.6239285469055176,
      "learning_rate": 1.5438001553196997e-05,
      "loss": 1.1353,
      "step": 18860
    },
    {
      "epoch": 1.4616665698406244,
      "grad_norm": 1.3165749311447144,
      "learning_rate": 1.5430235568211235e-05,
      "loss": 1.1746,
      "step": 18870
    },
    {
      "epoch": 1.462441178179283,
      "grad_norm": 1.7336853742599487,
      "learning_rate": 1.5422469583225473e-05,
      "loss": 1.158,
      "step": 18880
    },
    {
      "epoch": 1.4632157865179418,
      "grad_norm": 1.326515555381775,
      "learning_rate": 1.5414703598239707e-05,
      "loss": 1.2091,
      "step": 18890
    },
    {
      "epoch": 1.4639903948566007,
      "grad_norm": 1.3133729696273804,
      "learning_rate": 1.540693761325395e-05,
      "loss": 1.1232,
      "step": 18900
    },
    {
      "epoch": 1.4647650031952595,
      "grad_norm": 1.9081488847732544,
      "learning_rate": 1.5399171628268187e-05,
      "loss": 1.1782,
      "step": 18910
    },
    {
      "epoch": 1.465539611533918,
      "grad_norm": 1.7721788883209229,
      "learning_rate": 1.5391405643282424e-05,
      "loss": 1.2532,
      "step": 18920
    },
    {
      "epoch": 1.466314219872577,
      "grad_norm": 1.3840895891189575,
      "learning_rate": 1.5383639658296662e-05,
      "loss": 1.2285,
      "step": 18930
    },
    {
      "epoch": 1.4670888282112358,
      "grad_norm": 1.8186070919036865,
      "learning_rate": 1.53758736733109e-05,
      "loss": 1.1031,
      "step": 18940
    },
    {
      "epoch": 1.4678634365498944,
      "grad_norm": 2.1301286220550537,
      "learning_rate": 1.5368107688325138e-05,
      "loss": 1.2882,
      "step": 18950
    },
    {
      "epoch": 1.4686380448885532,
      "grad_norm": 1.8974413871765137,
      "learning_rate": 1.5360341703339376e-05,
      "loss": 1.1568,
      "step": 18960
    },
    {
      "epoch": 1.469412653227212,
      "grad_norm": 1.4870102405548096,
      "learning_rate": 1.535257571835361e-05,
      "loss": 1.2121,
      "step": 18970
    },
    {
      "epoch": 1.4701872615658709,
      "grad_norm": 1.8223272562026978,
      "learning_rate": 1.5344809733367848e-05,
      "loss": 1.115,
      "step": 18980
    },
    {
      "epoch": 1.4709618699045295,
      "grad_norm": 1.6388850212097168,
      "learning_rate": 1.5337043748382086e-05,
      "loss": 1.0563,
      "step": 18990
    },
    {
      "epoch": 1.4717364782431883,
      "grad_norm": 1.1974308490753174,
      "learning_rate": 1.5329277763396324e-05,
      "loss": 1.1528,
      "step": 19000
    },
    {
      "epoch": 1.472511086581847,
      "grad_norm": 1.6971663236618042,
      "learning_rate": 1.5321511778410562e-05,
      "loss": 1.246,
      "step": 19010
    },
    {
      "epoch": 1.4732856949205058,
      "grad_norm": 1.553971529006958,
      "learning_rate": 1.53137457934248e-05,
      "loss": 1.2213,
      "step": 19020
    },
    {
      "epoch": 1.4740603032591646,
      "grad_norm": 1.7910691499710083,
      "learning_rate": 1.5305979808439038e-05,
      "loss": 1.1187,
      "step": 19030
    },
    {
      "epoch": 1.4748349115978234,
      "grad_norm": 1.8251750469207764,
      "learning_rate": 1.5298213823453275e-05,
      "loss": 1.1765,
      "step": 19040
    },
    {
      "epoch": 1.475609519936482,
      "grad_norm": 1.5405305624008179,
      "learning_rate": 1.5290447838467513e-05,
      "loss": 1.1563,
      "step": 19050
    },
    {
      "epoch": 1.4763841282751409,
      "grad_norm": 1.593578577041626,
      "learning_rate": 1.5282681853481748e-05,
      "loss": 1.1667,
      "step": 19060
    },
    {
      "epoch": 1.4771587366137997,
      "grad_norm": 1.456709861755371,
      "learning_rate": 1.5274915868495986e-05,
      "loss": 1.1242,
      "step": 19070
    },
    {
      "epoch": 1.4779333449524583,
      "grad_norm": 1.563730001449585,
      "learning_rate": 1.5267149883510224e-05,
      "loss": 1.12,
      "step": 19080
    },
    {
      "epoch": 1.4787079532911171,
      "grad_norm": 1.851865291595459,
      "learning_rate": 1.525938389852446e-05,
      "loss": 1.1528,
      "step": 19090
    },
    {
      "epoch": 1.479482561629776,
      "grad_norm": 1.4899895191192627,
      "learning_rate": 1.5251617913538703e-05,
      "loss": 1.1913,
      "step": 19100
    },
    {
      "epoch": 1.4802571699684348,
      "grad_norm": 1.4225060939788818,
      "learning_rate": 1.524385192855294e-05,
      "loss": 1.2091,
      "step": 19110
    },
    {
      "epoch": 1.4810317783070934,
      "grad_norm": 1.7795122861862183,
      "learning_rate": 1.5236085943567177e-05,
      "loss": 1.2358,
      "step": 19120
    },
    {
      "epoch": 1.4818063866457523,
      "grad_norm": 1.5447889566421509,
      "learning_rate": 1.5228319958581415e-05,
      "loss": 1.2134,
      "step": 19130
    },
    {
      "epoch": 1.4825809949844109,
      "grad_norm": 1.3242915868759155,
      "learning_rate": 1.5220553973595652e-05,
      "loss": 1.2178,
      "step": 19140
    },
    {
      "epoch": 1.4833556033230697,
      "grad_norm": 1.338602066040039,
      "learning_rate": 1.521278798860989e-05,
      "loss": 1.2349,
      "step": 19150
    },
    {
      "epoch": 1.4841302116617285,
      "grad_norm": 1.3246444463729858,
      "learning_rate": 1.5205022003624126e-05,
      "loss": 1.1928,
      "step": 19160
    },
    {
      "epoch": 1.4849048200003874,
      "grad_norm": 1.5839693546295166,
      "learning_rate": 1.5197256018638364e-05,
      "loss": 1.1995,
      "step": 19170
    },
    {
      "epoch": 1.4856794283390462,
      "grad_norm": 1.4888229370117188,
      "learning_rate": 1.5189490033652602e-05,
      "loss": 1.1602,
      "step": 19180
    },
    {
      "epoch": 1.4864540366777048,
      "grad_norm": 2.006185531616211,
      "learning_rate": 1.518172404866684e-05,
      "loss": 1.2818,
      "step": 19190
    },
    {
      "epoch": 1.4872286450163636,
      "grad_norm": 1.889270305633545,
      "learning_rate": 1.5173958063681076e-05,
      "loss": 1.1669,
      "step": 19200
    },
    {
      "epoch": 1.4880032533550223,
      "grad_norm": 2.338252544403076,
      "learning_rate": 1.5166192078695314e-05,
      "loss": 1.3275,
      "step": 19210
    },
    {
      "epoch": 1.488777861693681,
      "grad_norm": 1.5528651475906372,
      "learning_rate": 1.5158426093709552e-05,
      "loss": 1.2111,
      "step": 19220
    },
    {
      "epoch": 1.48955247003234,
      "grad_norm": 1.7102898359298706,
      "learning_rate": 1.515066010872379e-05,
      "loss": 1.1793,
      "step": 19230
    },
    {
      "epoch": 1.4903270783709988,
      "grad_norm": 1.3582580089569092,
      "learning_rate": 1.5142894123738028e-05,
      "loss": 1.1645,
      "step": 19240
    },
    {
      "epoch": 1.4911016867096574,
      "grad_norm": 1.3353160619735718,
      "learning_rate": 1.5135128138752264e-05,
      "loss": 1.1441,
      "step": 19250
    },
    {
      "epoch": 1.4918762950483162,
      "grad_norm": 1.7161805629730225,
      "learning_rate": 1.5127362153766502e-05,
      "loss": 1.1737,
      "step": 19260
    },
    {
      "epoch": 1.492650903386975,
      "grad_norm": 1.4519844055175781,
      "learning_rate": 1.511959616878074e-05,
      "loss": 1.1052,
      "step": 19270
    },
    {
      "epoch": 1.4934255117256336,
      "grad_norm": 1.4601774215698242,
      "learning_rate": 1.5111830183794977e-05,
      "loss": 1.163,
      "step": 19280
    },
    {
      "epoch": 1.4942001200642925,
      "grad_norm": 1.6641606092453003,
      "learning_rate": 1.5104064198809217e-05,
      "loss": 1.1806,
      "step": 19290
    },
    {
      "epoch": 1.4949747284029513,
      "grad_norm": 1.1344624757766724,
      "learning_rate": 1.5096298213823455e-05,
      "loss": 1.2411,
      "step": 19300
    },
    {
      "epoch": 1.4957493367416101,
      "grad_norm": 2.100381851196289,
      "learning_rate": 1.5088532228837693e-05,
      "loss": 1.2074,
      "step": 19310
    },
    {
      "epoch": 1.4965239450802688,
      "grad_norm": 1.7552868127822876,
      "learning_rate": 1.508076624385193e-05,
      "loss": 1.2006,
      "step": 19320
    },
    {
      "epoch": 1.4972985534189276,
      "grad_norm": 1.6779135465621948,
      "learning_rate": 1.5073000258866167e-05,
      "loss": 1.1167,
      "step": 19330
    },
    {
      "epoch": 1.4980731617575862,
      "grad_norm": 1.3346730470657349,
      "learning_rate": 1.5065234273880405e-05,
      "loss": 1.2107,
      "step": 19340
    },
    {
      "epoch": 1.498847770096245,
      "grad_norm": 1.450499415397644,
      "learning_rate": 1.5057468288894643e-05,
      "loss": 1.0883,
      "step": 19350
    },
    {
      "epoch": 1.4996223784349039,
      "grad_norm": 1.9406633377075195,
      "learning_rate": 1.504970230390888e-05,
      "loss": 1.2397,
      "step": 19360
    },
    {
      "epoch": 1.5003969867735627,
      "grad_norm": 1.7986785173416138,
      "learning_rate": 1.5041936318923117e-05,
      "loss": 1.0263,
      "step": 19370
    },
    {
      "epoch": 1.5011715951122215,
      "grad_norm": 2.4091696739196777,
      "learning_rate": 1.5034170333937354e-05,
      "loss": 1.1246,
      "step": 19380
    },
    {
      "epoch": 1.5019462034508801,
      "grad_norm": 1.7117470502853394,
      "learning_rate": 1.5026404348951592e-05,
      "loss": 1.2023,
      "step": 19390
    },
    {
      "epoch": 1.5027208117895388,
      "grad_norm": 1.466176986694336,
      "learning_rate": 1.501863836396583e-05,
      "loss": 1.1181,
      "step": 19400
    },
    {
      "epoch": 1.5034954201281976,
      "grad_norm": 1.9047834873199463,
      "learning_rate": 1.5010872378980068e-05,
      "loss": 1.0381,
      "step": 19410
    },
    {
      "epoch": 1.5042700284668564,
      "grad_norm": 1.7681337594985962,
      "learning_rate": 1.5003106393994304e-05,
      "loss": 1.2317,
      "step": 19420
    },
    {
      "epoch": 1.5050446368055153,
      "grad_norm": 1.3049509525299072,
      "learning_rate": 1.4995340409008542e-05,
      "loss": 1.1472,
      "step": 19430
    },
    {
      "epoch": 1.505819245144174,
      "grad_norm": 1.6581919193267822,
      "learning_rate": 1.4987574424022782e-05,
      "loss": 1.1687,
      "step": 19440
    },
    {
      "epoch": 1.5065938534828327,
      "grad_norm": 1.6469905376434326,
      "learning_rate": 1.497980843903702e-05,
      "loss": 1.2382,
      "step": 19450
    },
    {
      "epoch": 1.5073684618214915,
      "grad_norm": 1.4078031778335571,
      "learning_rate": 1.4972042454051256e-05,
      "loss": 1.2217,
      "step": 19460
    },
    {
      "epoch": 1.5081430701601501,
      "grad_norm": 1.5045799016952515,
      "learning_rate": 1.4964276469065494e-05,
      "loss": 1.1811,
      "step": 19470
    },
    {
      "epoch": 1.508917678498809,
      "grad_norm": 1.70643150806427,
      "learning_rate": 1.4956510484079731e-05,
      "loss": 1.1878,
      "step": 19480
    },
    {
      "epoch": 1.5096922868374678,
      "grad_norm": 1.368904948234558,
      "learning_rate": 1.494874449909397e-05,
      "loss": 1.1532,
      "step": 19490
    },
    {
      "epoch": 1.5104668951761266,
      "grad_norm": 2.1337716579437256,
      "learning_rate": 1.4940978514108205e-05,
      "loss": 1.2161,
      "step": 19500
    },
    {
      "epoch": 1.5112415035147855,
      "grad_norm": 2.1574771404266357,
      "learning_rate": 1.4933212529122443e-05,
      "loss": 1.0992,
      "step": 19510
    },
    {
      "epoch": 1.512016111853444,
      "grad_norm": 1.555474042892456,
      "learning_rate": 1.4925446544136681e-05,
      "loss": 1.2258,
      "step": 19520
    },
    {
      "epoch": 1.512790720192103,
      "grad_norm": 1.2254695892333984,
      "learning_rate": 1.4917680559150919e-05,
      "loss": 1.1731,
      "step": 19530
    },
    {
      "epoch": 1.5135653285307615,
      "grad_norm": 1.551007866859436,
      "learning_rate": 1.4909914574165157e-05,
      "loss": 1.1454,
      "step": 19540
    },
    {
      "epoch": 1.5143399368694204,
      "grad_norm": 1.4234809875488281,
      "learning_rate": 1.4902148589179395e-05,
      "loss": 1.1826,
      "step": 19550
    },
    {
      "epoch": 1.5151145452080792,
      "grad_norm": 1.4673992395401,
      "learning_rate": 1.4894382604193633e-05,
      "loss": 1.199,
      "step": 19560
    },
    {
      "epoch": 1.515889153546738,
      "grad_norm": 1.6049981117248535,
      "learning_rate": 1.488661661920787e-05,
      "loss": 1.1627,
      "step": 19570
    },
    {
      "epoch": 1.5166637618853966,
      "grad_norm": 1.871888518333435,
      "learning_rate": 1.4878850634222108e-05,
      "loss": 1.1381,
      "step": 19580
    },
    {
      "epoch": 1.5174383702240555,
      "grad_norm": 1.3603626489639282,
      "learning_rate": 1.4871084649236345e-05,
      "loss": 1.2183,
      "step": 19590
    },
    {
      "epoch": 1.518212978562714,
      "grad_norm": 1.4067274332046509,
      "learning_rate": 1.4863318664250582e-05,
      "loss": 1.1944,
      "step": 19600
    },
    {
      "epoch": 1.518987586901373,
      "grad_norm": 1.7265849113464355,
      "learning_rate": 1.485555267926482e-05,
      "loss": 1.1969,
      "step": 19610
    },
    {
      "epoch": 1.5197621952400318,
      "grad_norm": 1.395334243774414,
      "learning_rate": 1.4847786694279058e-05,
      "loss": 1.0992,
      "step": 19620
    },
    {
      "epoch": 1.5205368035786906,
      "grad_norm": 1.5238726139068604,
      "learning_rate": 1.4840020709293296e-05,
      "loss": 1.1973,
      "step": 19630
    },
    {
      "epoch": 1.5213114119173494,
      "grad_norm": 1.4552050828933716,
      "learning_rate": 1.4832254724307534e-05,
      "loss": 1.1493,
      "step": 19640
    },
    {
      "epoch": 1.522086020256008,
      "grad_norm": 2.0197038650512695,
      "learning_rate": 1.4824488739321772e-05,
      "loss": 1.2868,
      "step": 19650
    },
    {
      "epoch": 1.5228606285946669,
      "grad_norm": 1.4699277877807617,
      "learning_rate": 1.481672275433601e-05,
      "loss": 1.1352,
      "step": 19660
    },
    {
      "epoch": 1.5236352369333255,
      "grad_norm": 1.6524189710617065,
      "learning_rate": 1.4808956769350246e-05,
      "loss": 1.2131,
      "step": 19670
    },
    {
      "epoch": 1.5244098452719843,
      "grad_norm": 1.8234403133392334,
      "learning_rate": 1.4801190784364484e-05,
      "loss": 1.1373,
      "step": 19680
    },
    {
      "epoch": 1.5251844536106431,
      "grad_norm": 1.5759572982788086,
      "learning_rate": 1.4793424799378722e-05,
      "loss": 1.1686,
      "step": 19690
    },
    {
      "epoch": 1.525959061949302,
      "grad_norm": 1.4566305875778198,
      "learning_rate": 1.478565881439296e-05,
      "loss": 1.1338,
      "step": 19700
    },
    {
      "epoch": 1.5267336702879608,
      "grad_norm": 1.246658205986023,
      "learning_rate": 1.4777892829407196e-05,
      "loss": 1.2095,
      "step": 19710
    },
    {
      "epoch": 1.5275082786266194,
      "grad_norm": 2.0084753036499023,
      "learning_rate": 1.4770126844421433e-05,
      "loss": 1.1954,
      "step": 19720
    },
    {
      "epoch": 1.528282886965278,
      "grad_norm": 1.3553730249404907,
      "learning_rate": 1.4762360859435673e-05,
      "loss": 1.2357,
      "step": 19730
    },
    {
      "epoch": 1.5290574953039369,
      "grad_norm": 1.513960361480713,
      "learning_rate": 1.4754594874449911e-05,
      "loss": 1.1112,
      "step": 19740
    },
    {
      "epoch": 1.5298321036425957,
      "grad_norm": 1.6551947593688965,
      "learning_rate": 1.4746828889464147e-05,
      "loss": 1.0532,
      "step": 19750
    },
    {
      "epoch": 1.5306067119812545,
      "grad_norm": 1.590851902961731,
      "learning_rate": 1.4739062904478385e-05,
      "loss": 1.1389,
      "step": 19760
    },
    {
      "epoch": 1.5313813203199134,
      "grad_norm": 1.489441990852356,
      "learning_rate": 1.4731296919492623e-05,
      "loss": 1.2177,
      "step": 19770
    },
    {
      "epoch": 1.532155928658572,
      "grad_norm": 1.2504407167434692,
      "learning_rate": 1.472353093450686e-05,
      "loss": 1.1616,
      "step": 19780
    },
    {
      "epoch": 1.5329305369972308,
      "grad_norm": 1.4469631910324097,
      "learning_rate": 1.4715764949521098e-05,
      "loss": 1.1956,
      "step": 19790
    },
    {
      "epoch": 1.5337051453358894,
      "grad_norm": 1.548356056213379,
      "learning_rate": 1.4707998964535335e-05,
      "loss": 1.1491,
      "step": 19800
    },
    {
      "epoch": 1.5344797536745483,
      "grad_norm": 1.6403676271438599,
      "learning_rate": 1.4700232979549573e-05,
      "loss": 1.2025,
      "step": 19810
    },
    {
      "epoch": 1.535254362013207,
      "grad_norm": 1.4109572172164917,
      "learning_rate": 1.469246699456381e-05,
      "loss": 1.2034,
      "step": 19820
    },
    {
      "epoch": 1.536028970351866,
      "grad_norm": 1.6953288316726685,
      "learning_rate": 1.468470100957805e-05,
      "loss": 1.2728,
      "step": 19830
    },
    {
      "epoch": 1.5368035786905248,
      "grad_norm": 1.5497819185256958,
      "learning_rate": 1.4676935024592286e-05,
      "loss": 1.1398,
      "step": 19840
    },
    {
      "epoch": 1.5375781870291834,
      "grad_norm": 1.75310218334198,
      "learning_rate": 1.4669169039606524e-05,
      "loss": 1.2775,
      "step": 19850
    },
    {
      "epoch": 1.538352795367842,
      "grad_norm": 1.4537246227264404,
      "learning_rate": 1.4661403054620762e-05,
      "loss": 1.1568,
      "step": 19860
    },
    {
      "epoch": 1.5391274037065008,
      "grad_norm": 1.5230897665023804,
      "learning_rate": 1.4653637069635e-05,
      "loss": 1.1174,
      "step": 19870
    },
    {
      "epoch": 1.5399020120451596,
      "grad_norm": 1.5912449359893799,
      "learning_rate": 1.4645871084649236e-05,
      "loss": 1.152,
      "step": 19880
    },
    {
      "epoch": 1.5406766203838185,
      "grad_norm": 1.8506793975830078,
      "learning_rate": 1.4638105099663474e-05,
      "loss": 1.1877,
      "step": 19890
    },
    {
      "epoch": 1.5414512287224773,
      "grad_norm": 1.677125096321106,
      "learning_rate": 1.4630339114677712e-05,
      "loss": 1.1469,
      "step": 19900
    },
    {
      "epoch": 1.542225837061136,
      "grad_norm": 1.8344303369522095,
      "learning_rate": 1.462257312969195e-05,
      "loss": 1.1329,
      "step": 19910
    },
    {
      "epoch": 1.5430004453997948,
      "grad_norm": 1.259504795074463,
      "learning_rate": 1.4614807144706186e-05,
      "loss": 1.0945,
      "step": 19920
    },
    {
      "epoch": 1.5437750537384534,
      "grad_norm": 1.4529688358306885,
      "learning_rate": 1.4607041159720425e-05,
      "loss": 1.2547,
      "step": 19930
    },
    {
      "epoch": 1.5445496620771122,
      "grad_norm": 1.465327262878418,
      "learning_rate": 1.4599275174734663e-05,
      "loss": 1.1756,
      "step": 19940
    },
    {
      "epoch": 1.545324270415771,
      "grad_norm": 1.4581432342529297,
      "learning_rate": 1.4591509189748901e-05,
      "loss": 1.1037,
      "step": 19950
    },
    {
      "epoch": 1.5460988787544299,
      "grad_norm": 1.5527347326278687,
      "learning_rate": 1.4583743204763139e-05,
      "loss": 1.2094,
      "step": 19960
    },
    {
      "epoch": 1.5468734870930887,
      "grad_norm": 2.0757713317871094,
      "learning_rate": 1.4575977219777375e-05,
      "loss": 1.1842,
      "step": 19970
    },
    {
      "epoch": 1.5476480954317473,
      "grad_norm": 1.5768111944198608,
      "learning_rate": 1.4568211234791613e-05,
      "loss": 1.301,
      "step": 19980
    },
    {
      "epoch": 1.5484227037704061,
      "grad_norm": 1.6888854503631592,
      "learning_rate": 1.456044524980585e-05,
      "loss": 1.2052,
      "step": 19990
    },
    {
      "epoch": 1.5491973121090648,
      "grad_norm": 2.3446993827819824,
      "learning_rate": 1.4552679264820089e-05,
      "loss": 1.1519,
      "step": 20000
    },
    {
      "epoch": 1.5499719204477236,
      "grad_norm": 1.7636394500732422,
      "learning_rate": 1.4544913279834325e-05,
      "loss": 1.1814,
      "step": 20010
    },
    {
      "epoch": 1.5507465287863824,
      "grad_norm": 1.4278219938278198,
      "learning_rate": 1.4537147294848563e-05,
      "loss": 1.2342,
      "step": 20020
    },
    {
      "epoch": 1.5515211371250412,
      "grad_norm": 1.387581706047058,
      "learning_rate": 1.4529381309862802e-05,
      "loss": 1.1539,
      "step": 20030
    },
    {
      "epoch": 1.5522957454637,
      "grad_norm": 1.4897184371948242,
      "learning_rate": 1.452161532487704e-05,
      "loss": 1.2537,
      "step": 20040
    },
    {
      "epoch": 1.5530703538023587,
      "grad_norm": 1.4601598978042603,
      "learning_rate": 1.4513849339891276e-05,
      "loss": 1.2176,
      "step": 20050
    },
    {
      "epoch": 1.5538449621410173,
      "grad_norm": 1.399287223815918,
      "learning_rate": 1.4506083354905514e-05,
      "loss": 1.199,
      "step": 20060
    },
    {
      "epoch": 1.5546195704796761,
      "grad_norm": 1.5157113075256348,
      "learning_rate": 1.4498317369919752e-05,
      "loss": 1.2282,
      "step": 20070
    },
    {
      "epoch": 1.555394178818335,
      "grad_norm": 1.3294398784637451,
      "learning_rate": 1.449055138493399e-05,
      "loss": 1.2288,
      "step": 20080
    },
    {
      "epoch": 1.5561687871569938,
      "grad_norm": 1.6317384243011475,
      "learning_rate": 1.4482785399948226e-05,
      "loss": 1.2806,
      "step": 20090
    },
    {
      "epoch": 1.5569433954956526,
      "grad_norm": 1.8243629932403564,
      "learning_rate": 1.4475019414962464e-05,
      "loss": 1.0846,
      "step": 20100
    },
    {
      "epoch": 1.5577180038343112,
      "grad_norm": 1.5005613565444946,
      "learning_rate": 1.4467253429976702e-05,
      "loss": 1.2308,
      "step": 20110
    },
    {
      "epoch": 1.55849261217297,
      "grad_norm": 1.961868405342102,
      "learning_rate": 1.445948744499094e-05,
      "loss": 1.0891,
      "step": 20120
    },
    {
      "epoch": 1.5592672205116287,
      "grad_norm": 1.3878027200698853,
      "learning_rate": 1.445172146000518e-05,
      "loss": 1.1732,
      "step": 20130
    },
    {
      "epoch": 1.5600418288502875,
      "grad_norm": 1.2617027759552002,
      "learning_rate": 1.4443955475019415e-05,
      "loss": 1.1188,
      "step": 20140
    },
    {
      "epoch": 1.5608164371889464,
      "grad_norm": 1.6096808910369873,
      "learning_rate": 1.4436189490033653e-05,
      "loss": 1.2612,
      "step": 20150
    },
    {
      "epoch": 1.5615910455276052,
      "grad_norm": 1.565101146697998,
      "learning_rate": 1.4428423505047891e-05,
      "loss": 1.162,
      "step": 20160
    },
    {
      "epoch": 1.562365653866264,
      "grad_norm": 1.5972189903259277,
      "learning_rate": 1.4420657520062129e-05,
      "loss": 1.1643,
      "step": 20170
    },
    {
      "epoch": 1.5631402622049226,
      "grad_norm": 2.2366397380828857,
      "learning_rate": 1.4412891535076365e-05,
      "loss": 1.2459,
      "step": 20180
    },
    {
      "epoch": 1.5639148705435812,
      "grad_norm": 1.3089747428894043,
      "learning_rate": 1.4405125550090603e-05,
      "loss": 1.2051,
      "step": 20190
    },
    {
      "epoch": 1.56468947888224,
      "grad_norm": 1.6917290687561035,
      "learning_rate": 1.439735956510484e-05,
      "loss": 1.0923,
      "step": 20200
    },
    {
      "epoch": 1.565464087220899,
      "grad_norm": 1.5777255296707153,
      "learning_rate": 1.4389593580119079e-05,
      "loss": 1.1767,
      "step": 20210
    },
    {
      "epoch": 1.5662386955595577,
      "grad_norm": 1.7324532270431519,
      "learning_rate": 1.4381827595133317e-05,
      "loss": 1.1985,
      "step": 20220
    },
    {
      "epoch": 1.5670133038982166,
      "grad_norm": 1.5172383785247803,
      "learning_rate": 1.4374061610147554e-05,
      "loss": 1.0605,
      "step": 20230
    },
    {
      "epoch": 1.5677879122368752,
      "grad_norm": 1.2194340229034424,
      "learning_rate": 1.4366295625161792e-05,
      "loss": 1.1743,
      "step": 20240
    },
    {
      "epoch": 1.568562520575534,
      "grad_norm": 1.4070042371749878,
      "learning_rate": 1.4359306238674606e-05,
      "loss": 1.1753,
      "step": 20250
    },
    {
      "epoch": 1.5693371289141926,
      "grad_norm": 1.575372338294983,
      "learning_rate": 1.4351540253688844e-05,
      "loss": 1.227,
      "step": 20260
    },
    {
      "epoch": 1.5701117372528515,
      "grad_norm": 1.5167452096939087,
      "learning_rate": 1.4343774268703081e-05,
      "loss": 1.1635,
      "step": 20270
    },
    {
      "epoch": 1.5708863455915103,
      "grad_norm": 1.8415731191635132,
      "learning_rate": 1.433600828371732e-05,
      "loss": 1.2683,
      "step": 20280
    },
    {
      "epoch": 1.5716609539301691,
      "grad_norm": 1.6125723123550415,
      "learning_rate": 1.4328242298731557e-05,
      "loss": 1.1699,
      "step": 20290
    },
    {
      "epoch": 1.572435562268828,
      "grad_norm": 1.836551308631897,
      "learning_rate": 1.4320476313745793e-05,
      "loss": 1.2393,
      "step": 20300
    },
    {
      "epoch": 1.5732101706074866,
      "grad_norm": 1.9985655546188354,
      "learning_rate": 1.4312710328760031e-05,
      "loss": 1.1299,
      "step": 20310
    },
    {
      "epoch": 1.5739847789461452,
      "grad_norm": 1.5777523517608643,
      "learning_rate": 1.4304944343774269e-05,
      "loss": 1.0424,
      "step": 20320
    },
    {
      "epoch": 1.574759387284804,
      "grad_norm": 1.2437580823898315,
      "learning_rate": 1.4297178358788507e-05,
      "loss": 1.1988,
      "step": 20330
    },
    {
      "epoch": 1.5755339956234629,
      "grad_norm": 1.6652086973190308,
      "learning_rate": 1.4289412373802743e-05,
      "loss": 1.097,
      "step": 20340
    },
    {
      "epoch": 1.5763086039621217,
      "grad_norm": 1.4789776802062988,
      "learning_rate": 1.4281646388816981e-05,
      "loss": 1.1104,
      "step": 20350
    },
    {
      "epoch": 1.5770832123007805,
      "grad_norm": 1.4273571968078613,
      "learning_rate": 1.427388040383122e-05,
      "loss": 1.147,
      "step": 20360
    },
    {
      "epoch": 1.5778578206394391,
      "grad_norm": 1.4718756675720215,
      "learning_rate": 1.4266114418845458e-05,
      "loss": 1.1864,
      "step": 20370
    },
    {
      "epoch": 1.578632428978098,
      "grad_norm": 1.6983370780944824,
      "learning_rate": 1.4258348433859695e-05,
      "loss": 1.2506,
      "step": 20380
    },
    {
      "epoch": 1.5794070373167566,
      "grad_norm": 1.2597957849502563,
      "learning_rate": 1.4250582448873932e-05,
      "loss": 1.1685,
      "step": 20390
    },
    {
      "epoch": 1.5801816456554154,
      "grad_norm": 1.6511985063552856,
      "learning_rate": 1.424281646388817e-05,
      "loss": 1.1706,
      "step": 20400
    },
    {
      "epoch": 1.5809562539940742,
      "grad_norm": 1.674079179763794,
      "learning_rate": 1.4235050478902408e-05,
      "loss": 1.1434,
      "step": 20410
    },
    {
      "epoch": 1.581730862332733,
      "grad_norm": 1.7013225555419922,
      "learning_rate": 1.4227284493916646e-05,
      "loss": 1.2173,
      "step": 20420
    },
    {
      "epoch": 1.582505470671392,
      "grad_norm": 1.6867233514785767,
      "learning_rate": 1.4219518508930882e-05,
      "loss": 1.2037,
      "step": 20430
    },
    {
      "epoch": 1.5832800790100505,
      "grad_norm": 1.7201021909713745,
      "learning_rate": 1.421175252394512e-05,
      "loss": 1.2003,
      "step": 20440
    },
    {
      "epoch": 1.5840546873487094,
      "grad_norm": 1.3046889305114746,
      "learning_rate": 1.4203986538959358e-05,
      "loss": 1.2058,
      "step": 20450
    },
    {
      "epoch": 1.584829295687368,
      "grad_norm": 1.6049290895462036,
      "learning_rate": 1.4196220553973597e-05,
      "loss": 1.0741,
      "step": 20460
    },
    {
      "epoch": 1.5856039040260268,
      "grad_norm": 1.091223120689392,
      "learning_rate": 1.4188454568987834e-05,
      "loss": 1.1898,
      "step": 20470
    },
    {
      "epoch": 1.5863785123646856,
      "grad_norm": 1.8539035320281982,
      "learning_rate": 1.4180688584002072e-05,
      "loss": 1.0972,
      "step": 20480
    },
    {
      "epoch": 1.5871531207033445,
      "grad_norm": 1.278983473777771,
      "learning_rate": 1.417292259901631e-05,
      "loss": 1.1891,
      "step": 20490
    },
    {
      "epoch": 1.5879277290420033,
      "grad_norm": 1.442352533340454,
      "learning_rate": 1.4165156614030547e-05,
      "loss": 1.1826,
      "step": 20500
    },
    {
      "epoch": 1.588702337380662,
      "grad_norm": 1.6796762943267822,
      "learning_rate": 1.4157390629044783e-05,
      "loss": 1.1492,
      "step": 20510
    },
    {
      "epoch": 1.5894769457193205,
      "grad_norm": 1.3490514755249023,
      "learning_rate": 1.4149624644059021e-05,
      "loss": 1.2103,
      "step": 20520
    },
    {
      "epoch": 1.5902515540579794,
      "grad_norm": 2.371009111404419,
      "learning_rate": 1.414185865907326e-05,
      "loss": 1.139,
      "step": 20530
    },
    {
      "epoch": 1.5910261623966382,
      "grad_norm": 1.5548750162124634,
      "learning_rate": 1.4134092674087497e-05,
      "loss": 1.2031,
      "step": 20540
    },
    {
      "epoch": 1.591800770735297,
      "grad_norm": 1.512280821800232,
      "learning_rate": 1.4126326689101733e-05,
      "loss": 1.2064,
      "step": 20550
    },
    {
      "epoch": 1.5925753790739559,
      "grad_norm": 1.5570472478866577,
      "learning_rate": 1.4118560704115973e-05,
      "loss": 1.2384,
      "step": 20560
    },
    {
      "epoch": 1.5933499874126145,
      "grad_norm": 1.3277907371520996,
      "learning_rate": 1.411079471913021e-05,
      "loss": 1.2396,
      "step": 20570
    },
    {
      "epoch": 1.5941245957512733,
      "grad_norm": 1.4325450658798218,
      "learning_rate": 1.4103028734144449e-05,
      "loss": 1.2384,
      "step": 20580
    },
    {
      "epoch": 1.594899204089932,
      "grad_norm": 1.3095628023147583,
      "learning_rate": 1.4095262749158685e-05,
      "loss": 1.1199,
      "step": 20590
    },
    {
      "epoch": 1.5956738124285907,
      "grad_norm": 1.3246266841888428,
      "learning_rate": 1.4087496764172923e-05,
      "loss": 1.2839,
      "step": 20600
    },
    {
      "epoch": 1.5964484207672496,
      "grad_norm": 2.072784423828125,
      "learning_rate": 1.407973077918716e-05,
      "loss": 1.2603,
      "step": 20610
    },
    {
      "epoch": 1.5972230291059084,
      "grad_norm": 1.5614629983901978,
      "learning_rate": 1.4071964794201398e-05,
      "loss": 1.1333,
      "step": 20620
    },
    {
      "epoch": 1.5979976374445672,
      "grad_norm": 1.5851489305496216,
      "learning_rate": 1.4064198809215636e-05,
      "loss": 1.2275,
      "step": 20630
    },
    {
      "epoch": 1.5987722457832259,
      "grad_norm": 1.3350797891616821,
      "learning_rate": 1.4056432824229872e-05,
      "loss": 1.2152,
      "step": 20640
    },
    {
      "epoch": 1.5995468541218845,
      "grad_norm": 1.2678576707839966,
      "learning_rate": 1.404866683924411e-05,
      "loss": 1.1655,
      "step": 20650
    },
    {
      "epoch": 1.6003214624605433,
      "grad_norm": 1.5965559482574463,
      "learning_rate": 1.404090085425835e-05,
      "loss": 1.2712,
      "step": 20660
    },
    {
      "epoch": 1.6010960707992021,
      "grad_norm": 1.7340385913848877,
      "learning_rate": 1.4033134869272588e-05,
      "loss": 1.1593,
      "step": 20670
    },
    {
      "epoch": 1.601870679137861,
      "grad_norm": 1.3288755416870117,
      "learning_rate": 1.4025368884286824e-05,
      "loss": 1.2263,
      "step": 20680
    },
    {
      "epoch": 1.6026452874765198,
      "grad_norm": 1.6731808185577393,
      "learning_rate": 1.4017602899301062e-05,
      "loss": 1.1424,
      "step": 20690
    },
    {
      "epoch": 1.6034198958151784,
      "grad_norm": 1.6053262948989868,
      "learning_rate": 1.40098369143153e-05,
      "loss": 1.2048,
      "step": 20700
    },
    {
      "epoch": 1.6041945041538372,
      "grad_norm": 1.9893252849578857,
      "learning_rate": 1.4002070929329537e-05,
      "loss": 1.2578,
      "step": 20710
    },
    {
      "epoch": 1.6049691124924959,
      "grad_norm": 1.4900574684143066,
      "learning_rate": 1.3994304944343774e-05,
      "loss": 1.2661,
      "step": 20720
    },
    {
      "epoch": 1.6057437208311547,
      "grad_norm": 1.5265862941741943,
      "learning_rate": 1.3986538959358011e-05,
      "loss": 1.158,
      "step": 20730
    },
    {
      "epoch": 1.6065183291698135,
      "grad_norm": 1.4690853357315063,
      "learning_rate": 1.397877297437225e-05,
      "loss": 1.1817,
      "step": 20740
    },
    {
      "epoch": 1.6072929375084724,
      "grad_norm": 1.9799830913543701,
      "learning_rate": 1.3971006989386489e-05,
      "loss": 1.2425,
      "step": 20750
    },
    {
      "epoch": 1.6080675458471312,
      "grad_norm": 1.9497934579849243,
      "learning_rate": 1.3963241004400725e-05,
      "loss": 1.1502,
      "step": 20760
    },
    {
      "epoch": 1.6088421541857898,
      "grad_norm": 1.4207630157470703,
      "learning_rate": 1.3955475019414963e-05,
      "loss": 1.2372,
      "step": 20770
    },
    {
      "epoch": 1.6096167625244486,
      "grad_norm": 1.1894168853759766,
      "learning_rate": 1.39477090344292e-05,
      "loss": 1.2385,
      "step": 20780
    },
    {
      "epoch": 1.6103913708631072,
      "grad_norm": 1.449632167816162,
      "learning_rate": 1.3939943049443439e-05,
      "loss": 1.1553,
      "step": 20790
    },
    {
      "epoch": 1.611165979201766,
      "grad_norm": 1.6775507926940918,
      "learning_rate": 1.3932177064457676e-05,
      "loss": 1.2424,
      "step": 20800
    },
    {
      "epoch": 1.611940587540425,
      "grad_norm": 1.646328091621399,
      "learning_rate": 1.3924411079471913e-05,
      "loss": 1.2303,
      "step": 20810
    },
    {
      "epoch": 1.6127151958790837,
      "grad_norm": 2.6656086444854736,
      "learning_rate": 1.391664509448615e-05,
      "loss": 1.2279,
      "step": 20820
    },
    {
      "epoch": 1.6134898042177424,
      "grad_norm": 1.2823045253753662,
      "learning_rate": 1.3908879109500388e-05,
      "loss": 1.2248,
      "step": 20830
    },
    {
      "epoch": 1.6142644125564012,
      "grad_norm": 2.0889766216278076,
      "learning_rate": 1.3901113124514626e-05,
      "loss": 1.299,
      "step": 20840
    },
    {
      "epoch": 1.6150390208950598,
      "grad_norm": 1.5561494827270508,
      "learning_rate": 1.3893347139528864e-05,
      "loss": 1.1063,
      "step": 20850
    },
    {
      "epoch": 1.6158136292337186,
      "grad_norm": 1.4436464309692383,
      "learning_rate": 1.3885581154543102e-05,
      "loss": 1.1901,
      "step": 20860
    },
    {
      "epoch": 1.6165882375723775,
      "grad_norm": 1.9382729530334473,
      "learning_rate": 1.387781516955734e-05,
      "loss": 1.1574,
      "step": 20870
    },
    {
      "epoch": 1.6173628459110363,
      "grad_norm": 2.1457839012145996,
      "learning_rate": 1.3870049184571578e-05,
      "loss": 1.1421,
      "step": 20880
    },
    {
      "epoch": 1.6181374542496951,
      "grad_norm": 1.5629147291183472,
      "learning_rate": 1.3862283199585814e-05,
      "loss": 1.0855,
      "step": 20890
    },
    {
      "epoch": 1.6189120625883537,
      "grad_norm": 1.9610549211502075,
      "learning_rate": 1.3854517214600052e-05,
      "loss": 1.2204,
      "step": 20900
    },
    {
      "epoch": 1.6196866709270126,
      "grad_norm": 1.4375821352005005,
      "learning_rate": 1.384675122961429e-05,
      "loss": 1.1917,
      "step": 20910
    },
    {
      "epoch": 1.6204612792656712,
      "grad_norm": 1.4794543981552124,
      "learning_rate": 1.3838985244628527e-05,
      "loss": 1.2204,
      "step": 20920
    },
    {
      "epoch": 1.62123588760433,
      "grad_norm": 1.8289178609848022,
      "learning_rate": 1.3831219259642764e-05,
      "loss": 1.2077,
      "step": 20930
    },
    {
      "epoch": 1.6220104959429889,
      "grad_norm": 1.29447603225708,
      "learning_rate": 1.3823453274657002e-05,
      "loss": 1.2753,
      "step": 20940
    },
    {
      "epoch": 1.6227851042816477,
      "grad_norm": 1.8051522970199585,
      "learning_rate": 1.3815687289671241e-05,
      "loss": 1.2192,
      "step": 20950
    },
    {
      "epoch": 1.6235597126203065,
      "grad_norm": 2.061540126800537,
      "learning_rate": 1.3807921304685479e-05,
      "loss": 1.187,
      "step": 20960
    },
    {
      "epoch": 1.6243343209589651,
      "grad_norm": 1.5476495027542114,
      "learning_rate": 1.3800155319699717e-05,
      "loss": 1.1613,
      "step": 20970
    },
    {
      "epoch": 1.6251089292976237,
      "grad_norm": 1.6752821207046509,
      "learning_rate": 1.3792389334713953e-05,
      "loss": 1.2106,
      "step": 20980
    },
    {
      "epoch": 1.6258835376362826,
      "grad_norm": 1.3974934816360474,
      "learning_rate": 1.3784623349728191e-05,
      "loss": 1.1548,
      "step": 20990
    },
    {
      "epoch": 1.6266581459749414,
      "grad_norm": 1.3925424814224243,
      "learning_rate": 1.3776857364742429e-05,
      "loss": 1.201,
      "step": 21000
    },
    {
      "epoch": 1.6274327543136002,
      "grad_norm": 1.4586948156356812,
      "learning_rate": 1.3769091379756667e-05,
      "loss": 1.1964,
      "step": 21010
    },
    {
      "epoch": 1.628207362652259,
      "grad_norm": 1.4066966772079468,
      "learning_rate": 1.3761325394770903e-05,
      "loss": 1.1761,
      "step": 21020
    },
    {
      "epoch": 1.6289819709909177,
      "grad_norm": 1.5597299337387085,
      "learning_rate": 1.375355940978514e-05,
      "loss": 1.1508,
      "step": 21030
    },
    {
      "epoch": 1.6297565793295765,
      "grad_norm": 1.319406270980835,
      "learning_rate": 1.3745793424799378e-05,
      "loss": 1.1565,
      "step": 21040
    },
    {
      "epoch": 1.6305311876682351,
      "grad_norm": 1.7264468669891357,
      "learning_rate": 1.3738027439813618e-05,
      "loss": 1.2792,
      "step": 21050
    },
    {
      "epoch": 1.631305796006894,
      "grad_norm": 1.6086511611938477,
      "learning_rate": 1.3730261454827854e-05,
      "loss": 1.1587,
      "step": 21060
    },
    {
      "epoch": 1.6320804043455528,
      "grad_norm": 1.144919514656067,
      "learning_rate": 1.3722495469842092e-05,
      "loss": 1.1941,
      "step": 21070
    },
    {
      "epoch": 1.6328550126842116,
      "grad_norm": 1.4169979095458984,
      "learning_rate": 1.371472948485633e-05,
      "loss": 1.1426,
      "step": 21080
    },
    {
      "epoch": 1.6336296210228705,
      "grad_norm": 1.4855726957321167,
      "learning_rate": 1.3706963499870568e-05,
      "loss": 1.1383,
      "step": 21090
    },
    {
      "epoch": 1.634404229361529,
      "grad_norm": 1.2697091102600098,
      "learning_rate": 1.3699197514884804e-05,
      "loss": 1.1942,
      "step": 21100
    },
    {
      "epoch": 1.6351788377001877,
      "grad_norm": 1.631139874458313,
      "learning_rate": 1.3691431529899042e-05,
      "loss": 1.1831,
      "step": 21110
    },
    {
      "epoch": 1.6359534460388465,
      "grad_norm": 1.697846531867981,
      "learning_rate": 1.368366554491328e-05,
      "loss": 1.1965,
      "step": 21120
    },
    {
      "epoch": 1.6367280543775053,
      "grad_norm": 2.357975721359253,
      "learning_rate": 1.3675899559927518e-05,
      "loss": 1.2486,
      "step": 21130
    },
    {
      "epoch": 1.6375026627161642,
      "grad_norm": 1.3341137170791626,
      "learning_rate": 1.3668133574941755e-05,
      "loss": 1.2486,
      "step": 21140
    },
    {
      "epoch": 1.638277271054823,
      "grad_norm": 1.6602591276168823,
      "learning_rate": 1.3660367589955993e-05,
      "loss": 1.1678,
      "step": 21150
    },
    {
      "epoch": 1.6390518793934816,
      "grad_norm": 1.3215736150741577,
      "learning_rate": 1.3652601604970231e-05,
      "loss": 1.17,
      "step": 21160
    },
    {
      "epoch": 1.6398264877321405,
      "grad_norm": 1.2243592739105225,
      "learning_rate": 1.3644835619984469e-05,
      "loss": 1.2001,
      "step": 21170
    },
    {
      "epoch": 1.640601096070799,
      "grad_norm": 1.6899611949920654,
      "learning_rate": 1.3637069634998707e-05,
      "loss": 1.2004,
      "step": 21180
    },
    {
      "epoch": 1.641375704409458,
      "grad_norm": 1.497153639793396,
      "learning_rate": 1.3629303650012943e-05,
      "loss": 1.1753,
      "step": 21190
    },
    {
      "epoch": 1.6421503127481167,
      "grad_norm": 2.0287187099456787,
      "learning_rate": 1.3621537665027181e-05,
      "loss": 1.1171,
      "step": 21200
    },
    {
      "epoch": 1.6429249210867756,
      "grad_norm": 2.072890043258667,
      "learning_rate": 1.3613771680041419e-05,
      "loss": 1.1702,
      "step": 21210
    },
    {
      "epoch": 1.6436995294254344,
      "grad_norm": 1.552497386932373,
      "learning_rate": 1.3606005695055657e-05,
      "loss": 1.1577,
      "step": 21220
    },
    {
      "epoch": 1.644474137764093,
      "grad_norm": 1.403563380241394,
      "learning_rate": 1.3598239710069893e-05,
      "loss": 1.0814,
      "step": 21230
    },
    {
      "epoch": 1.6452487461027518,
      "grad_norm": 1.429530143737793,
      "learning_rate": 1.3590473725084132e-05,
      "loss": 1.0472,
      "step": 21240
    },
    {
      "epoch": 1.6460233544414105,
      "grad_norm": 1.8404096364974976,
      "learning_rate": 1.358270774009837e-05,
      "loss": 1.2449,
      "step": 21250
    },
    {
      "epoch": 1.6467979627800693,
      "grad_norm": 1.968069314956665,
      "learning_rate": 1.3574941755112608e-05,
      "loss": 1.1473,
      "step": 21260
    },
    {
      "epoch": 1.6475725711187281,
      "grad_norm": 1.072035551071167,
      "learning_rate": 1.3567175770126844e-05,
      "loss": 1.1308,
      "step": 21270
    },
    {
      "epoch": 1.648347179457387,
      "grad_norm": 1.2307054996490479,
      "learning_rate": 1.3559409785141082e-05,
      "loss": 1.2634,
      "step": 21280
    },
    {
      "epoch": 1.6491217877960458,
      "grad_norm": 1.477027416229248,
      "learning_rate": 1.355164380015532e-05,
      "loss": 1.3179,
      "step": 21290
    },
    {
      "epoch": 1.6498963961347044,
      "grad_norm": 1.68804931640625,
      "learning_rate": 1.3543877815169558e-05,
      "loss": 1.239,
      "step": 21300
    },
    {
      "epoch": 1.650671004473363,
      "grad_norm": 1.8532382249832153,
      "learning_rate": 1.3536111830183796e-05,
      "loss": 1.1508,
      "step": 21310
    },
    {
      "epoch": 1.6514456128120218,
      "grad_norm": 1.6986043453216553,
      "learning_rate": 1.3528345845198032e-05,
      "loss": 1.2608,
      "step": 21320
    },
    {
      "epoch": 1.6522202211506807,
      "grad_norm": 1.8697110414505005,
      "learning_rate": 1.352057986021227e-05,
      "loss": 1.1348,
      "step": 21330
    },
    {
      "epoch": 1.6529948294893395,
      "grad_norm": 1.4883058071136475,
      "learning_rate": 1.351281387522651e-05,
      "loss": 1.1892,
      "step": 21340
    },
    {
      "epoch": 1.6537694378279983,
      "grad_norm": 1.384006142616272,
      "learning_rate": 1.3505047890240747e-05,
      "loss": 1.1491,
      "step": 21350
    },
    {
      "epoch": 1.654544046166657,
      "grad_norm": 1.4407848119735718,
      "learning_rate": 1.3497281905254983e-05,
      "loss": 1.1812,
      "step": 21360
    },
    {
      "epoch": 1.6553186545053158,
      "grad_norm": 1.697973608970642,
      "learning_rate": 1.3489515920269221e-05,
      "loss": 1.1706,
      "step": 21370
    },
    {
      "epoch": 1.6560932628439744,
      "grad_norm": 1.1343567371368408,
      "learning_rate": 1.348174993528346e-05,
      "loss": 1.1767,
      "step": 21380
    },
    {
      "epoch": 1.6568678711826332,
      "grad_norm": 1.6886996030807495,
      "learning_rate": 1.3473983950297697e-05,
      "loss": 1.199,
      "step": 21390
    },
    {
      "epoch": 1.657642479521292,
      "grad_norm": 1.7224171161651611,
      "learning_rate": 1.3466217965311933e-05,
      "loss": 1.1616,
      "step": 21400
    },
    {
      "epoch": 1.658417087859951,
      "grad_norm": 1.4412399530410767,
      "learning_rate": 1.3458451980326171e-05,
      "loss": 1.1761,
      "step": 21410
    },
    {
      "epoch": 1.6591916961986097,
      "grad_norm": 1.7715436220169067,
      "learning_rate": 1.3450685995340409e-05,
      "loss": 1.1865,
      "step": 21420
    },
    {
      "epoch": 1.6599663045372683,
      "grad_norm": 1.6597893238067627,
      "learning_rate": 1.3442920010354647e-05,
      "loss": 1.1632,
      "step": 21430
    },
    {
      "epoch": 1.660740912875927,
      "grad_norm": 1.5610151290893555,
      "learning_rate": 1.3435154025368885e-05,
      "loss": 1.1683,
      "step": 21440
    },
    {
      "epoch": 1.6615155212145858,
      "grad_norm": 1.4418388605117798,
      "learning_rate": 1.3427388040383123e-05,
      "loss": 1.0963,
      "step": 21450
    },
    {
      "epoch": 1.6622901295532446,
      "grad_norm": 1.4032241106033325,
      "learning_rate": 1.341962205539736e-05,
      "loss": 1.2052,
      "step": 21460
    },
    {
      "epoch": 1.6630647378919035,
      "grad_norm": 1.6691590547561646,
      "learning_rate": 1.3411856070411598e-05,
      "loss": 1.1426,
      "step": 21470
    },
    {
      "epoch": 1.6638393462305623,
      "grad_norm": 1.2698854207992554,
      "learning_rate": 1.3404090085425834e-05,
      "loss": 1.1323,
      "step": 21480
    },
    {
      "epoch": 1.664613954569221,
      "grad_norm": 1.4706764221191406,
      "learning_rate": 1.3396324100440072e-05,
      "loss": 1.1332,
      "step": 21490
    },
    {
      "epoch": 1.6653885629078797,
      "grad_norm": 1.3714648485183716,
      "learning_rate": 1.338855811545431e-05,
      "loss": 1.2196,
      "step": 21500
    },
    {
      "epoch": 1.6661631712465383,
      "grad_norm": 1.7473512887954712,
      "learning_rate": 1.3380792130468548e-05,
      "loss": 1.1162,
      "step": 21510
    },
    {
      "epoch": 1.6669377795851972,
      "grad_norm": 1.2431025505065918,
      "learning_rate": 1.3373026145482786e-05,
      "loss": 1.1522,
      "step": 21520
    },
    {
      "epoch": 1.667712387923856,
      "grad_norm": 1.6078529357910156,
      "learning_rate": 1.3365260160497022e-05,
      "loss": 1.1471,
      "step": 21530
    },
    {
      "epoch": 1.6684869962625148,
      "grad_norm": 1.38014554977417,
      "learning_rate": 1.3357494175511262e-05,
      "loss": 1.2007,
      "step": 21540
    },
    {
      "epoch": 1.6692616046011737,
      "grad_norm": 1.8049899339675903,
      "learning_rate": 1.33497281905255e-05,
      "loss": 1.2306,
      "step": 21550
    },
    {
      "epoch": 1.6700362129398323,
      "grad_norm": 1.752541422843933,
      "learning_rate": 1.3341962205539737e-05,
      "loss": 1.1849,
      "step": 21560
    },
    {
      "epoch": 1.670810821278491,
      "grad_norm": 1.4085500240325928,
      "learning_rate": 1.3334196220553974e-05,
      "loss": 1.1319,
      "step": 21570
    },
    {
      "epoch": 1.6715854296171497,
      "grad_norm": 2.112628221511841,
      "learning_rate": 1.3326430235568211e-05,
      "loss": 1.1711,
      "step": 21580
    },
    {
      "epoch": 1.6723600379558086,
      "grad_norm": 1.7729251384735107,
      "learning_rate": 1.331866425058245e-05,
      "loss": 1.1581,
      "step": 21590
    },
    {
      "epoch": 1.6731346462944674,
      "grad_norm": 2.0631961822509766,
      "learning_rate": 1.3310898265596687e-05,
      "loss": 1.1283,
      "step": 21600
    },
    {
      "epoch": 1.6739092546331262,
      "grad_norm": 1.214382290840149,
      "learning_rate": 1.3303132280610923e-05,
      "loss": 1.1464,
      "step": 21610
    },
    {
      "epoch": 1.6746838629717848,
      "grad_norm": 1.7005198001861572,
      "learning_rate": 1.3295366295625161e-05,
      "loss": 1.1663,
      "step": 21620
    },
    {
      "epoch": 1.6754584713104437,
      "grad_norm": 1.8430025577545166,
      "learning_rate": 1.3287600310639399e-05,
      "loss": 1.3168,
      "step": 21630
    },
    {
      "epoch": 1.6762330796491023,
      "grad_norm": 1.4972622394561768,
      "learning_rate": 1.3279834325653639e-05,
      "loss": 1.2879,
      "step": 21640
    },
    {
      "epoch": 1.6770076879877611,
      "grad_norm": 1.7451432943344116,
      "learning_rate": 1.3272068340667875e-05,
      "loss": 1.2026,
      "step": 21650
    },
    {
      "epoch": 1.67778229632642,
      "grad_norm": 1.7317442893981934,
      "learning_rate": 1.3264302355682113e-05,
      "loss": 1.2778,
      "step": 21660
    },
    {
      "epoch": 1.6785569046650788,
      "grad_norm": 1.4080630540847778,
      "learning_rate": 1.325653637069635e-05,
      "loss": 1.1161,
      "step": 21670
    },
    {
      "epoch": 1.6793315130037376,
      "grad_norm": 1.4945485591888428,
      "learning_rate": 1.3248770385710588e-05,
      "loss": 1.137,
      "step": 21680
    },
    {
      "epoch": 1.6801061213423962,
      "grad_norm": 1.8310812711715698,
      "learning_rate": 1.3241004400724826e-05,
      "loss": 1.2509,
      "step": 21690
    },
    {
      "epoch": 1.680880729681055,
      "grad_norm": 1.4184434413909912,
      "learning_rate": 1.3233238415739062e-05,
      "loss": 1.2432,
      "step": 21700
    },
    {
      "epoch": 1.6816553380197137,
      "grad_norm": 1.4223085641860962,
      "learning_rate": 1.32254724307533e-05,
      "loss": 1.2529,
      "step": 21710
    },
    {
      "epoch": 1.6824299463583725,
      "grad_norm": 1.4753094911575317,
      "learning_rate": 1.3217706445767538e-05,
      "loss": 1.2695,
      "step": 21720
    },
    {
      "epoch": 1.6832045546970313,
      "grad_norm": 1.259178876876831,
      "learning_rate": 1.3209940460781778e-05,
      "loss": 1.1298,
      "step": 21730
    },
    {
      "epoch": 1.6839791630356902,
      "grad_norm": 1.2352240085601807,
      "learning_rate": 1.3202174475796014e-05,
      "loss": 1.2431,
      "step": 21740
    },
    {
      "epoch": 1.684753771374349,
      "grad_norm": 1.331418514251709,
      "learning_rate": 1.3194408490810252e-05,
      "loss": 1.1342,
      "step": 21750
    },
    {
      "epoch": 1.6855283797130076,
      "grad_norm": 1.7493971586227417,
      "learning_rate": 1.318664250582449e-05,
      "loss": 1.1102,
      "step": 21760
    },
    {
      "epoch": 1.6863029880516662,
      "grad_norm": 1.6251206398010254,
      "learning_rate": 1.3178876520838727e-05,
      "loss": 1.124,
      "step": 21770
    },
    {
      "epoch": 1.687077596390325,
      "grad_norm": 1.9934247732162476,
      "learning_rate": 1.3171110535852964e-05,
      "loss": 1.2351,
      "step": 21780
    },
    {
      "epoch": 1.687852204728984,
      "grad_norm": 1.7293808460235596,
      "learning_rate": 1.3163344550867202e-05,
      "loss": 1.0918,
      "step": 21790
    },
    {
      "epoch": 1.6886268130676427,
      "grad_norm": 1.6293524503707886,
      "learning_rate": 1.315557856588144e-05,
      "loss": 1.1707,
      "step": 21800
    },
    {
      "epoch": 1.6894014214063016,
      "grad_norm": 1.461277723312378,
      "learning_rate": 1.3147812580895677e-05,
      "loss": 1.2011,
      "step": 21810
    },
    {
      "epoch": 1.6901760297449602,
      "grad_norm": 1.6218817234039307,
      "learning_rate": 1.3140046595909913e-05,
      "loss": 1.2395,
      "step": 21820
    },
    {
      "epoch": 1.690950638083619,
      "grad_norm": 1.6935967206954956,
      "learning_rate": 1.3132280610924153e-05,
      "loss": 1.1759,
      "step": 21830
    },
    {
      "epoch": 1.6917252464222776,
      "grad_norm": 1.700081706047058,
      "learning_rate": 1.312451462593839e-05,
      "loss": 1.1433,
      "step": 21840
    },
    {
      "epoch": 1.6924998547609365,
      "grad_norm": 1.2819020748138428,
      "learning_rate": 1.3116748640952629e-05,
      "loss": 1.1501,
      "step": 21850
    },
    {
      "epoch": 1.6932744630995953,
      "grad_norm": 1.4012160301208496,
      "learning_rate": 1.3108982655966867e-05,
      "loss": 1.2002,
      "step": 21860
    },
    {
      "epoch": 1.6940490714382541,
      "grad_norm": 1.7060097455978394,
      "learning_rate": 1.3101216670981103e-05,
      "loss": 1.2065,
      "step": 21870
    },
    {
      "epoch": 1.694823679776913,
      "grad_norm": 1.613898754119873,
      "learning_rate": 1.309345068599534e-05,
      "loss": 1.2984,
      "step": 21880
    },
    {
      "epoch": 1.6955982881155716,
      "grad_norm": 1.4872887134552002,
      "learning_rate": 1.3085684701009578e-05,
      "loss": 1.2333,
      "step": 21890
    },
    {
      "epoch": 1.6963728964542302,
      "grad_norm": 2.0014305114746094,
      "learning_rate": 1.3077918716023816e-05,
      "loss": 1.2095,
      "step": 21900
    },
    {
      "epoch": 1.697147504792889,
      "grad_norm": 1.8273862600326538,
      "learning_rate": 1.3070152731038053e-05,
      "loss": 1.1443,
      "step": 21910
    },
    {
      "epoch": 1.6979221131315478,
      "grad_norm": 1.5286900997161865,
      "learning_rate": 1.306238674605229e-05,
      "loss": 1.2191,
      "step": 21920
    },
    {
      "epoch": 1.6986967214702067,
      "grad_norm": 1.6216579675674438,
      "learning_rate": 1.305462076106653e-05,
      "loss": 1.1674,
      "step": 21930
    },
    {
      "epoch": 1.6994713298088655,
      "grad_norm": 1.442418098449707,
      "learning_rate": 1.3046854776080768e-05,
      "loss": 1.1728,
      "step": 21940
    },
    {
      "epoch": 1.7002459381475241,
      "grad_norm": 1.3025997877120972,
      "learning_rate": 1.3039088791095004e-05,
      "loss": 1.1632,
      "step": 21950
    },
    {
      "epoch": 1.701020546486183,
      "grad_norm": 1.7106962203979492,
      "learning_rate": 1.3031322806109242e-05,
      "loss": 1.253,
      "step": 21960
    },
    {
      "epoch": 1.7017951548248416,
      "grad_norm": 1.5642716884613037,
      "learning_rate": 1.302355682112348e-05,
      "loss": 1.2321,
      "step": 21970
    },
    {
      "epoch": 1.7025697631635004,
      "grad_norm": 1.7713737487792969,
      "learning_rate": 1.3015790836137718e-05,
      "loss": 1.1297,
      "step": 21980
    },
    {
      "epoch": 1.7033443715021592,
      "grad_norm": 1.1573519706726074,
      "learning_rate": 1.3008024851151954e-05,
      "loss": 1.224,
      "step": 21990
    },
    {
      "epoch": 1.704118979840818,
      "grad_norm": 1.4896117448806763,
      "learning_rate": 1.3000258866166192e-05,
      "loss": 1.2855,
      "step": 22000
    },
    {
      "epoch": 1.704893588179477,
      "grad_norm": 1.7033823728561401,
      "learning_rate": 1.299249288118043e-05,
      "loss": 1.1832,
      "step": 22010
    },
    {
      "epoch": 1.7056681965181355,
      "grad_norm": 1.9788860082626343,
      "learning_rate": 1.2984726896194667e-05,
      "loss": 1.1849,
      "step": 22020
    },
    {
      "epoch": 1.7064428048567943,
      "grad_norm": 1.6396067142486572,
      "learning_rate": 1.2976960911208905e-05,
      "loss": 1.0871,
      "step": 22030
    },
    {
      "epoch": 1.707217413195453,
      "grad_norm": 1.7979291677474976,
      "learning_rate": 1.2969194926223143e-05,
      "loss": 1.1184,
      "step": 22040
    },
    {
      "epoch": 1.7079920215341118,
      "grad_norm": 1.300950050354004,
      "learning_rate": 1.2961428941237381e-05,
      "loss": 1.1681,
      "step": 22050
    },
    {
      "epoch": 1.7087666298727706,
      "grad_norm": 1.7020635604858398,
      "learning_rate": 1.2953662956251619e-05,
      "loss": 1.1671,
      "step": 22060
    },
    {
      "epoch": 1.7095412382114294,
      "grad_norm": 1.6421456336975098,
      "learning_rate": 1.2945896971265857e-05,
      "loss": 1.168,
      "step": 22070
    },
    {
      "epoch": 1.710315846550088,
      "grad_norm": 1.802322506904602,
      "learning_rate": 1.2938130986280093e-05,
      "loss": 1.1114,
      "step": 22080
    },
    {
      "epoch": 1.711090454888747,
      "grad_norm": 1.6684859991073608,
      "learning_rate": 1.293036500129433e-05,
      "loss": 1.2143,
      "step": 22090
    },
    {
      "epoch": 1.7118650632274055,
      "grad_norm": 1.576664686203003,
      "learning_rate": 1.2922599016308569e-05,
      "loss": 1.1581,
      "step": 22100
    },
    {
      "epoch": 1.7126396715660643,
      "grad_norm": 1.4272379875183105,
      "learning_rate": 1.2914833031322806e-05,
      "loss": 1.165,
      "step": 22110
    },
    {
      "epoch": 1.7134142799047232,
      "grad_norm": 1.347296953201294,
      "learning_rate": 1.2907067046337043e-05,
      "loss": 1.2104,
      "step": 22120
    },
    {
      "epoch": 1.714188888243382,
      "grad_norm": 1.306593894958496,
      "learning_rate": 1.2899301061351282e-05,
      "loss": 1.2038,
      "step": 22130
    },
    {
      "epoch": 1.7149634965820408,
      "grad_norm": 1.6289198398590088,
      "learning_rate": 1.289153507636552e-05,
      "loss": 1.2498,
      "step": 22140
    },
    {
      "epoch": 1.7157381049206994,
      "grad_norm": 1.6819908618927002,
      "learning_rate": 1.2883769091379758e-05,
      "loss": 1.2036,
      "step": 22150
    },
    {
      "epoch": 1.7165127132593583,
      "grad_norm": 1.5900046825408936,
      "learning_rate": 1.2876003106393994e-05,
      "loss": 1.2158,
      "step": 22160
    },
    {
      "epoch": 1.717287321598017,
      "grad_norm": 1.645579218864441,
      "learning_rate": 1.2868237121408232e-05,
      "loss": 1.0799,
      "step": 22170
    },
    {
      "epoch": 1.7180619299366757,
      "grad_norm": 1.600915789604187,
      "learning_rate": 1.286047113642247e-05,
      "loss": 1.1627,
      "step": 22180
    },
    {
      "epoch": 1.7188365382753346,
      "grad_norm": 1.7015154361724854,
      "learning_rate": 1.2852705151436708e-05,
      "loss": 1.169,
      "step": 22190
    },
    {
      "epoch": 1.7196111466139934,
      "grad_norm": 1.3029547929763794,
      "learning_rate": 1.2844939166450944e-05,
      "loss": 1.1734,
      "step": 22200
    },
    {
      "epoch": 1.7203857549526522,
      "grad_norm": 1.3983548879623413,
      "learning_rate": 1.2837173181465182e-05,
      "loss": 1.1319,
      "step": 22210
    },
    {
      "epoch": 1.7211603632913108,
      "grad_norm": 1.2236549854278564,
      "learning_rate": 1.2829407196479421e-05,
      "loss": 1.1651,
      "step": 22220
    },
    {
      "epoch": 1.7219349716299694,
      "grad_norm": 1.6259981393814087,
      "learning_rate": 1.2821641211493659e-05,
      "loss": 1.1845,
      "step": 22230
    },
    {
      "epoch": 1.7227095799686283,
      "grad_norm": 1.7901068925857544,
      "learning_rate": 1.2813875226507897e-05,
      "loss": 1.0821,
      "step": 22240
    },
    {
      "epoch": 1.7234841883072871,
      "grad_norm": 1.266373634338379,
      "learning_rate": 1.2806109241522133e-05,
      "loss": 1.1981,
      "step": 22250
    },
    {
      "epoch": 1.724258796645946,
      "grad_norm": 1.2024922370910645,
      "learning_rate": 1.2799119855034948e-05,
      "loss": 1.2155,
      "step": 22260
    },
    {
      "epoch": 1.7250334049846048,
      "grad_norm": 1.9899060726165771,
      "learning_rate": 1.2791353870049186e-05,
      "loss": 1.1042,
      "step": 22270
    },
    {
      "epoch": 1.7258080133232634,
      "grad_norm": 1.9979270696640015,
      "learning_rate": 1.2783587885063422e-05,
      "loss": 1.2262,
      "step": 22280
    },
    {
      "epoch": 1.7265826216619222,
      "grad_norm": 1.6541653871536255,
      "learning_rate": 1.277582190007766e-05,
      "loss": 1.2208,
      "step": 22290
    },
    {
      "epoch": 1.7273572300005808,
      "grad_norm": 1.8285406827926636,
      "learning_rate": 1.2768055915091898e-05,
      "loss": 1.2141,
      "step": 22300
    },
    {
      "epoch": 1.7281318383392397,
      "grad_norm": 1.3574298620224,
      "learning_rate": 1.2760289930106136e-05,
      "loss": 1.1511,
      "step": 22310
    },
    {
      "epoch": 1.7289064466778985,
      "grad_norm": 1.515783667564392,
      "learning_rate": 1.2752523945120372e-05,
      "loss": 1.1532,
      "step": 22320
    },
    {
      "epoch": 1.7296810550165573,
      "grad_norm": 1.5307013988494873,
      "learning_rate": 1.274475796013461e-05,
      "loss": 1.1522,
      "step": 22330
    },
    {
      "epoch": 1.7304556633552162,
      "grad_norm": 1.410845398902893,
      "learning_rate": 1.2736991975148848e-05,
      "loss": 1.2638,
      "step": 22340
    },
    {
      "epoch": 1.7312302716938748,
      "grad_norm": 1.6299136877059937,
      "learning_rate": 1.2729225990163086e-05,
      "loss": 1.1569,
      "step": 22350
    },
    {
      "epoch": 1.7320048800325334,
      "grad_norm": 1.3769928216934204,
      "learning_rate": 1.2721460005177325e-05,
      "loss": 1.1359,
      "step": 22360
    },
    {
      "epoch": 1.7327794883711922,
      "grad_norm": 1.5774060487747192,
      "learning_rate": 1.2713694020191561e-05,
      "loss": 1.192,
      "step": 22370
    },
    {
      "epoch": 1.733554096709851,
      "grad_norm": 1.335699200630188,
      "learning_rate": 1.27059280352058e-05,
      "loss": 1.1483,
      "step": 22380
    },
    {
      "epoch": 1.7343287050485099,
      "grad_norm": 1.4833157062530518,
      "learning_rate": 1.2698162050220037e-05,
      "loss": 1.2065,
      "step": 22390
    },
    {
      "epoch": 1.7351033133871687,
      "grad_norm": 1.457311749458313,
      "learning_rate": 1.2690396065234275e-05,
      "loss": 1.0418,
      "step": 22400
    },
    {
      "epoch": 1.7358779217258273,
      "grad_norm": 1.6443599462509155,
      "learning_rate": 1.2682630080248511e-05,
      "loss": 1.2293,
      "step": 22410
    },
    {
      "epoch": 1.7366525300644862,
      "grad_norm": 1.4557373523712158,
      "learning_rate": 1.2674864095262749e-05,
      "loss": 1.1297,
      "step": 22420
    },
    {
      "epoch": 1.7374271384031448,
      "grad_norm": 1.374599814414978,
      "learning_rate": 1.2667098110276987e-05,
      "loss": 1.2146,
      "step": 22430
    },
    {
      "epoch": 1.7382017467418036,
      "grad_norm": 1.6017143726348877,
      "learning_rate": 1.2659332125291225e-05,
      "loss": 1.2126,
      "step": 22440
    },
    {
      "epoch": 1.7389763550804624,
      "grad_norm": 1.357800841331482,
      "learning_rate": 1.2651566140305461e-05,
      "loss": 1.2905,
      "step": 22450
    },
    {
      "epoch": 1.7397509634191213,
      "grad_norm": 1.1846867799758911,
      "learning_rate": 1.26438001553197e-05,
      "loss": 1.153,
      "step": 22460
    },
    {
      "epoch": 1.74052557175778,
      "grad_norm": 1.739263653755188,
      "learning_rate": 1.2636034170333938e-05,
      "loss": 1.2022,
      "step": 22470
    },
    {
      "epoch": 1.7413001800964387,
      "grad_norm": 1.0686997175216675,
      "learning_rate": 1.2628268185348176e-05,
      "loss": 1.1703,
      "step": 22480
    },
    {
      "epoch": 1.7420747884350976,
      "grad_norm": 1.9488805532455444,
      "learning_rate": 1.2620502200362412e-05,
      "loss": 1.1427,
      "step": 22490
    },
    {
      "epoch": 1.7428493967737562,
      "grad_norm": 1.4968818426132202,
      "learning_rate": 1.261273621537665e-05,
      "loss": 1.1187,
      "step": 22500
    },
    {
      "epoch": 1.743624005112415,
      "grad_norm": 1.3065990209579468,
      "learning_rate": 1.2604970230390888e-05,
      "loss": 1.189,
      "step": 22510
    },
    {
      "epoch": 1.7443986134510738,
      "grad_norm": 1.3966490030288696,
      "learning_rate": 1.2597204245405126e-05,
      "loss": 1.2271,
      "step": 22520
    },
    {
      "epoch": 1.7451732217897327,
      "grad_norm": 1.5054585933685303,
      "learning_rate": 1.2589438260419364e-05,
      "loss": 1.0353,
      "step": 22530
    },
    {
      "epoch": 1.7459478301283915,
      "grad_norm": 0.9980659484863281,
      "learning_rate": 1.25816722754336e-05,
      "loss": 1.1168,
      "step": 22540
    },
    {
      "epoch": 1.74672243846705,
      "grad_norm": 1.5073614120483398,
      "learning_rate": 1.2573906290447838e-05,
      "loss": 1.1147,
      "step": 22550
    },
    {
      "epoch": 1.7474970468057087,
      "grad_norm": 2.020320177078247,
      "learning_rate": 1.2566140305462077e-05,
      "loss": 1.2454,
      "step": 22560
    },
    {
      "epoch": 1.7482716551443676,
      "grad_norm": 1.570586919784546,
      "learning_rate": 1.2558374320476315e-05,
      "loss": 1.2156,
      "step": 22570
    },
    {
      "epoch": 1.7490462634830264,
      "grad_norm": 1.3711374998092651,
      "learning_rate": 1.2550608335490552e-05,
      "loss": 1.2251,
      "step": 22580
    },
    {
      "epoch": 1.7498208718216852,
      "grad_norm": 1.4610453844070435,
      "learning_rate": 1.254284235050479e-05,
      "loss": 1.1616,
      "step": 22590
    },
    {
      "epoch": 1.750595480160344,
      "grad_norm": 1.369531512260437,
      "learning_rate": 1.2535076365519027e-05,
      "loss": 1.1824,
      "step": 22600
    },
    {
      "epoch": 1.7513700884990027,
      "grad_norm": 1.3186054229736328,
      "learning_rate": 1.2527310380533265e-05,
      "loss": 1.159,
      "step": 22610
    },
    {
      "epoch": 1.7521446968376615,
      "grad_norm": 1.4210453033447266,
      "learning_rate": 1.2519544395547501e-05,
      "loss": 1.1053,
      "step": 22620
    },
    {
      "epoch": 1.75291930517632,
      "grad_norm": 1.6610647439956665,
      "learning_rate": 1.251177841056174e-05,
      "loss": 1.1517,
      "step": 22630
    },
    {
      "epoch": 1.753693913514979,
      "grad_norm": 1.8899712562561035,
      "learning_rate": 1.2504012425575977e-05,
      "loss": 1.18,
      "step": 22640
    },
    {
      "epoch": 1.7544685218536378,
      "grad_norm": 1.3751780986785889,
      "learning_rate": 1.2496246440590215e-05,
      "loss": 1.1631,
      "step": 22650
    },
    {
      "epoch": 1.7552431301922966,
      "grad_norm": 1.714449167251587,
      "learning_rate": 1.2488480455604453e-05,
      "loss": 1.1925,
      "step": 22660
    },
    {
      "epoch": 1.7560177385309554,
      "grad_norm": 1.3463988304138184,
      "learning_rate": 1.248071447061869e-05,
      "loss": 1.1601,
      "step": 22670
    },
    {
      "epoch": 1.756792346869614,
      "grad_norm": 1.5105973482131958,
      "learning_rate": 1.2472948485632928e-05,
      "loss": 1.1077,
      "step": 22680
    },
    {
      "epoch": 1.7575669552082727,
      "grad_norm": 1.6977936029434204,
      "learning_rate": 1.2465182500647166e-05,
      "loss": 1.2391,
      "step": 22690
    },
    {
      "epoch": 1.7583415635469315,
      "grad_norm": 1.9138686656951904,
      "learning_rate": 1.2457416515661404e-05,
      "loss": 1.111,
      "step": 22700
    },
    {
      "epoch": 1.7591161718855903,
      "grad_norm": 1.6611677408218384,
      "learning_rate": 1.244965053067564e-05,
      "loss": 1.1667,
      "step": 22710
    },
    {
      "epoch": 1.7598907802242492,
      "grad_norm": 1.992046594619751,
      "learning_rate": 1.2441884545689878e-05,
      "loss": 1.1464,
      "step": 22720
    },
    {
      "epoch": 1.760665388562908,
      "grad_norm": 1.7607632875442505,
      "learning_rate": 1.2434118560704116e-05,
      "loss": 1.187,
      "step": 22730
    },
    {
      "epoch": 1.7614399969015666,
      "grad_norm": 1.5158743858337402,
      "learning_rate": 1.2426352575718354e-05,
      "loss": 1.2333,
      "step": 22740
    },
    {
      "epoch": 1.7622146052402254,
      "grad_norm": 1.3706169128417969,
      "learning_rate": 1.2418586590732592e-05,
      "loss": 1.1472,
      "step": 22750
    },
    {
      "epoch": 1.762989213578884,
      "grad_norm": 1.3651984930038452,
      "learning_rate": 1.241082060574683e-05,
      "loss": 1.2058,
      "step": 22760
    },
    {
      "epoch": 1.7637638219175429,
      "grad_norm": 1.7880053520202637,
      "learning_rate": 1.2403054620761068e-05,
      "loss": 1.2278,
      "step": 22770
    },
    {
      "epoch": 1.7645384302562017,
      "grad_norm": 1.5755969285964966,
      "learning_rate": 1.2395288635775305e-05,
      "loss": 1.1311,
      "step": 22780
    },
    {
      "epoch": 1.7653130385948606,
      "grad_norm": 1.6863703727722168,
      "learning_rate": 1.2387522650789542e-05,
      "loss": 1.2033,
      "step": 22790
    },
    {
      "epoch": 1.7660876469335194,
      "grad_norm": 1.4742144346237183,
      "learning_rate": 1.237975666580378e-05,
      "loss": 1.1112,
      "step": 22800
    },
    {
      "epoch": 1.766862255272178,
      "grad_norm": 1.3751152753829956,
      "learning_rate": 1.2371990680818017e-05,
      "loss": 1.1986,
      "step": 22810
    },
    {
      "epoch": 1.7676368636108366,
      "grad_norm": 1.4404544830322266,
      "learning_rate": 1.2364224695832255e-05,
      "loss": 1.127,
      "step": 22820
    },
    {
      "epoch": 1.7684114719494954,
      "grad_norm": 1.4988521337509155,
      "learning_rate": 1.2356458710846491e-05,
      "loss": 1.1738,
      "step": 22830
    },
    {
      "epoch": 1.7691860802881543,
      "grad_norm": 1.7363351583480835,
      "learning_rate": 1.234869272586073e-05,
      "loss": 1.1893,
      "step": 22840
    },
    {
      "epoch": 1.769960688626813,
      "grad_norm": 1.8962929248809814,
      "learning_rate": 1.2340926740874969e-05,
      "loss": 1.242,
      "step": 22850
    },
    {
      "epoch": 1.770735296965472,
      "grad_norm": 1.49884033203125,
      "learning_rate": 1.2333160755889207e-05,
      "loss": 1.1801,
      "step": 22860
    },
    {
      "epoch": 1.7715099053041305,
      "grad_norm": 1.3707911968231201,
      "learning_rate": 1.2325394770903443e-05,
      "loss": 1.2076,
      "step": 22870
    },
    {
      "epoch": 1.7722845136427894,
      "grad_norm": 1.5346648693084717,
      "learning_rate": 1.231762878591768e-05,
      "loss": 1.216,
      "step": 22880
    },
    {
      "epoch": 1.773059121981448,
      "grad_norm": 1.3735835552215576,
      "learning_rate": 1.2309862800931919e-05,
      "loss": 1.1844,
      "step": 22890
    },
    {
      "epoch": 1.7738337303201068,
      "grad_norm": 1.6411713361740112,
      "learning_rate": 1.2302096815946156e-05,
      "loss": 1.2009,
      "step": 22900
    },
    {
      "epoch": 1.7746083386587657,
      "grad_norm": 1.565475583076477,
      "learning_rate": 1.2294330830960394e-05,
      "loss": 1.1514,
      "step": 22910
    },
    {
      "epoch": 1.7753829469974245,
      "grad_norm": 1.2804237604141235,
      "learning_rate": 1.228656484597463e-05,
      "loss": 1.1625,
      "step": 22920
    },
    {
      "epoch": 1.7761575553360833,
      "grad_norm": 1.6677848100662231,
      "learning_rate": 1.2278798860988868e-05,
      "loss": 1.176,
      "step": 22930
    },
    {
      "epoch": 1.776932163674742,
      "grad_norm": 1.2257367372512817,
      "learning_rate": 1.2271032876003106e-05,
      "loss": 1.2208,
      "step": 22940
    },
    {
      "epoch": 1.7777067720134008,
      "grad_norm": 1.6587971448898315,
      "learning_rate": 1.2263266891017346e-05,
      "loss": 1.1593,
      "step": 22950
    },
    {
      "epoch": 1.7784813803520594,
      "grad_norm": 1.596396565437317,
      "learning_rate": 1.2255500906031582e-05,
      "loss": 1.1375,
      "step": 22960
    },
    {
      "epoch": 1.7792559886907182,
      "grad_norm": 1.7451916933059692,
      "learning_rate": 1.224773492104582e-05,
      "loss": 1.0328,
      "step": 22970
    },
    {
      "epoch": 1.780030597029377,
      "grad_norm": 1.4677690267562866,
      "learning_rate": 1.2239968936060058e-05,
      "loss": 1.2132,
      "step": 22980
    },
    {
      "epoch": 1.7808052053680359,
      "grad_norm": 1.3950469493865967,
      "learning_rate": 1.2232202951074296e-05,
      "loss": 1.1843,
      "step": 22990
    },
    {
      "epoch": 1.7815798137066947,
      "grad_norm": 1.8932890892028809,
      "learning_rate": 1.2224436966088532e-05,
      "loss": 1.2506,
      "step": 23000
    },
    {
      "epoch": 1.7823544220453533,
      "grad_norm": 1.4799896478652954,
      "learning_rate": 1.221667098110277e-05,
      "loss": 1.1935,
      "step": 23010
    },
    {
      "epoch": 1.783129030384012,
      "grad_norm": 1.727939486503601,
      "learning_rate": 1.2208904996117007e-05,
      "loss": 1.23,
      "step": 23020
    },
    {
      "epoch": 1.7839036387226708,
      "grad_norm": 1.4732071161270142,
      "learning_rate": 1.2201139011131245e-05,
      "loss": 1.1736,
      "step": 23030
    },
    {
      "epoch": 1.7846782470613296,
      "grad_norm": 1.2612262964248657,
      "learning_rate": 1.2193373026145482e-05,
      "loss": 1.1437,
      "step": 23040
    },
    {
      "epoch": 1.7854528553999884,
      "grad_norm": 1.3306751251220703,
      "learning_rate": 1.2185607041159721e-05,
      "loss": 1.1387,
      "step": 23050
    },
    {
      "epoch": 1.7862274637386473,
      "grad_norm": 1.6628694534301758,
      "learning_rate": 1.2177841056173959e-05,
      "loss": 1.1235,
      "step": 23060
    },
    {
      "epoch": 1.7870020720773059,
      "grad_norm": 1.591059684753418,
      "learning_rate": 1.2170075071188197e-05,
      "loss": 1.256,
      "step": 23070
    },
    {
      "epoch": 1.7877766804159647,
      "grad_norm": 1.7219945192337036,
      "learning_rate": 1.2162309086202435e-05,
      "loss": 1.1534,
      "step": 23080
    },
    {
      "epoch": 1.7885512887546233,
      "grad_norm": 1.651389479637146,
      "learning_rate": 1.215454310121667e-05,
      "loss": 1.1674,
      "step": 23090
    },
    {
      "epoch": 1.7893258970932822,
      "grad_norm": 1.6293383836746216,
      "learning_rate": 1.2146777116230909e-05,
      "loss": 1.1585,
      "step": 23100
    },
    {
      "epoch": 1.790100505431941,
      "grad_norm": 1.263221263885498,
      "learning_rate": 1.2139011131245147e-05,
      "loss": 1.1445,
      "step": 23110
    },
    {
      "epoch": 1.7908751137705998,
      "grad_norm": 1.4944995641708374,
      "learning_rate": 1.2131245146259384e-05,
      "loss": 1.1695,
      "step": 23120
    },
    {
      "epoch": 1.7916497221092587,
      "grad_norm": 1.283246636390686,
      "learning_rate": 1.212347916127362e-05,
      "loss": 1.1889,
      "step": 23130
    },
    {
      "epoch": 1.7924243304479173,
      "grad_norm": 1.3749080896377563,
      "learning_rate": 1.2115713176287858e-05,
      "loss": 1.1515,
      "step": 23140
    },
    {
      "epoch": 1.7931989387865759,
      "grad_norm": 1.3521499633789062,
      "learning_rate": 1.2107947191302098e-05,
      "loss": 1.2349,
      "step": 23150
    },
    {
      "epoch": 1.7939735471252347,
      "grad_norm": 1.9243931770324707,
      "learning_rate": 1.2100181206316336e-05,
      "loss": 1.1967,
      "step": 23160
    },
    {
      "epoch": 1.7947481554638935,
      "grad_norm": 1.8286964893341064,
      "learning_rate": 1.2092415221330572e-05,
      "loss": 1.2442,
      "step": 23170
    },
    {
      "epoch": 1.7955227638025524,
      "grad_norm": 1.409815788269043,
      "learning_rate": 1.208464923634481e-05,
      "loss": 1.2085,
      "step": 23180
    },
    {
      "epoch": 1.7962973721412112,
      "grad_norm": 1.2979506254196167,
      "learning_rate": 1.2076883251359048e-05,
      "loss": 1.1735,
      "step": 23190
    },
    {
      "epoch": 1.7970719804798698,
      "grad_norm": 1.43472421169281,
      "learning_rate": 1.2069117266373286e-05,
      "loss": 1.2423,
      "step": 23200
    },
    {
      "epoch": 1.7978465888185287,
      "grad_norm": 1.6010966300964355,
      "learning_rate": 1.2061351281387522e-05,
      "loss": 1.1875,
      "step": 23210
    },
    {
      "epoch": 1.7986211971571873,
      "grad_norm": 1.20364511013031,
      "learning_rate": 1.205358529640176e-05,
      "loss": 1.1316,
      "step": 23220
    },
    {
      "epoch": 1.799395805495846,
      "grad_norm": 2.6738688945770264,
      "learning_rate": 1.2045819311415998e-05,
      "loss": 1.2191,
      "step": 23230
    },
    {
      "epoch": 1.800170413834505,
      "grad_norm": 1.0646599531173706,
      "learning_rate": 1.2038053326430237e-05,
      "loss": 1.1638,
      "step": 23240
    },
    {
      "epoch": 1.8009450221731638,
      "grad_norm": 2.134223222732544,
      "learning_rate": 1.2030287341444475e-05,
      "loss": 1.224,
      "step": 23250
    },
    {
      "epoch": 1.8017196305118226,
      "grad_norm": 1.7532262802124023,
      "learning_rate": 1.2022521356458711e-05,
      "loss": 1.2314,
      "step": 23260
    },
    {
      "epoch": 1.8024942388504812,
      "grad_norm": 1.9073677062988281,
      "learning_rate": 1.2014755371472949e-05,
      "loss": 1.1576,
      "step": 23270
    },
    {
      "epoch": 1.80326884718914,
      "grad_norm": 1.59174382686615,
      "learning_rate": 1.2006989386487187e-05,
      "loss": 1.0949,
      "step": 23280
    },
    {
      "epoch": 1.8040434555277987,
      "grad_norm": 2.093817949295044,
      "learning_rate": 1.1999223401501425e-05,
      "loss": 1.1331,
      "step": 23290
    },
    {
      "epoch": 1.8048180638664575,
      "grad_norm": 1.5882676839828491,
      "learning_rate": 1.1991457416515661e-05,
      "loss": 1.2101,
      "step": 23300
    },
    {
      "epoch": 1.8055926722051163,
      "grad_norm": 1.9577146768569946,
      "learning_rate": 1.1983691431529899e-05,
      "loss": 1.2577,
      "step": 23310
    },
    {
      "epoch": 1.8063672805437752,
      "grad_norm": 1.5014944076538086,
      "learning_rate": 1.1975925446544137e-05,
      "loss": 1.1867,
      "step": 23320
    },
    {
      "epoch": 1.8071418888824338,
      "grad_norm": 1.4204603433609009,
      "learning_rate": 1.1968159461558375e-05,
      "loss": 1.2426,
      "step": 23330
    },
    {
      "epoch": 1.8079164972210926,
      "grad_norm": 1.3821830749511719,
      "learning_rate": 1.1960393476572612e-05,
      "loss": 1.1899,
      "step": 23340
    },
    {
      "epoch": 1.8086911055597512,
      "grad_norm": 1.402531623840332,
      "learning_rate": 1.195262749158685e-05,
      "loss": 1.0823,
      "step": 23350
    },
    {
      "epoch": 1.80946571389841,
      "grad_norm": 1.436227798461914,
      "learning_rate": 1.1944861506601088e-05,
      "loss": 1.2294,
      "step": 23360
    },
    {
      "epoch": 1.8102403222370689,
      "grad_norm": 1.6438626050949097,
      "learning_rate": 1.1937095521615326e-05,
      "loss": 1.1276,
      "step": 23370
    },
    {
      "epoch": 1.8110149305757277,
      "grad_norm": 1.7180209159851074,
      "learning_rate": 1.1929329536629562e-05,
      "loss": 1.2623,
      "step": 23380
    },
    {
      "epoch": 1.8117895389143865,
      "grad_norm": 1.575966238975525,
      "learning_rate": 1.19215635516438e-05,
      "loss": 1.1162,
      "step": 23390
    },
    {
      "epoch": 1.8125641472530452,
      "grad_norm": 1.7319899797439575,
      "learning_rate": 1.1913797566658038e-05,
      "loss": 1.1482,
      "step": 23400
    },
    {
      "epoch": 1.813338755591704,
      "grad_norm": 1.8473938703536987,
      "learning_rate": 1.1906031581672276e-05,
      "loss": 1.1937,
      "step": 23410
    },
    {
      "epoch": 1.8141133639303626,
      "grad_norm": 1.6325243711471558,
      "learning_rate": 1.1898265596686514e-05,
      "loss": 1.2258,
      "step": 23420
    },
    {
      "epoch": 1.8148879722690214,
      "grad_norm": 1.556967854499817,
      "learning_rate": 1.189049961170075e-05,
      "loss": 1.1464,
      "step": 23430
    },
    {
      "epoch": 1.8156625806076803,
      "grad_norm": 1.6395230293273926,
      "learning_rate": 1.188273362671499e-05,
      "loss": 1.1594,
      "step": 23440
    },
    {
      "epoch": 1.816437188946339,
      "grad_norm": 1.4341381788253784,
      "learning_rate": 1.1874967641729227e-05,
      "loss": 1.0566,
      "step": 23450
    },
    {
      "epoch": 1.817211797284998,
      "grad_norm": 1.6372877359390259,
      "learning_rate": 1.1867201656743465e-05,
      "loss": 1.1695,
      "step": 23460
    },
    {
      "epoch": 1.8179864056236565,
      "grad_norm": 2.000430107116699,
      "learning_rate": 1.1859435671757701e-05,
      "loss": 1.1753,
      "step": 23470
    },
    {
      "epoch": 1.8187610139623152,
      "grad_norm": 1.3034653663635254,
      "learning_rate": 1.1851669686771939e-05,
      "loss": 1.1449,
      "step": 23480
    },
    {
      "epoch": 1.819535622300974,
      "grad_norm": 1.2417429685592651,
      "learning_rate": 1.1843903701786177e-05,
      "loss": 1.1235,
      "step": 23490
    },
    {
      "epoch": 1.8203102306396328,
      "grad_norm": 1.6620333194732666,
      "learning_rate": 1.1836137716800415e-05,
      "loss": 1.2376,
      "step": 23500
    },
    {
      "epoch": 1.8210848389782917,
      "grad_norm": 1.3224354982376099,
      "learning_rate": 1.1828371731814651e-05,
      "loss": 1.1323,
      "step": 23510
    },
    {
      "epoch": 1.8218594473169505,
      "grad_norm": 1.29524564743042,
      "learning_rate": 1.1820605746828889e-05,
      "loss": 1.1578,
      "step": 23520
    },
    {
      "epoch": 1.822634055655609,
      "grad_norm": 1.627814769744873,
      "learning_rate": 1.1812839761843127e-05,
      "loss": 1.2129,
      "step": 23530
    },
    {
      "epoch": 1.823408663994268,
      "grad_norm": 1.76625657081604,
      "learning_rate": 1.1805073776857366e-05,
      "loss": 1.1505,
      "step": 23540
    },
    {
      "epoch": 1.8241832723329265,
      "grad_norm": 1.5495688915252686,
      "learning_rate": 1.1797307791871603e-05,
      "loss": 1.1733,
      "step": 23550
    },
    {
      "epoch": 1.8249578806715854,
      "grad_norm": 1.2288395166397095,
      "learning_rate": 1.178954180688584e-05,
      "loss": 1.1728,
      "step": 23560
    },
    {
      "epoch": 1.8257324890102442,
      "grad_norm": 1.418591022491455,
      "learning_rate": 1.1781775821900078e-05,
      "loss": 1.083,
      "step": 23570
    },
    {
      "epoch": 1.826507097348903,
      "grad_norm": 1.6723905801773071,
      "learning_rate": 1.1774009836914316e-05,
      "loss": 1.2116,
      "step": 23580
    },
    {
      "epoch": 1.8272817056875619,
      "grad_norm": 1.271865963935852,
      "learning_rate": 1.1766243851928552e-05,
      "loss": 1.1409,
      "step": 23590
    },
    {
      "epoch": 1.8280563140262205,
      "grad_norm": 1.6465277671813965,
      "learning_rate": 1.175847786694279e-05,
      "loss": 1.1595,
      "step": 23600
    },
    {
      "epoch": 1.828830922364879,
      "grad_norm": 1.6435840129852295,
      "learning_rate": 1.1750711881957028e-05,
      "loss": 1.1816,
      "step": 23610
    },
    {
      "epoch": 1.829605530703538,
      "grad_norm": 1.5243505239486694,
      "learning_rate": 1.1742945896971266e-05,
      "loss": 1.0816,
      "step": 23620
    },
    {
      "epoch": 1.8303801390421968,
      "grad_norm": 1.9288562536239624,
      "learning_rate": 1.1735179911985504e-05,
      "loss": 1.1716,
      "step": 23630
    },
    {
      "epoch": 1.8311547473808556,
      "grad_norm": 2.24100661277771,
      "learning_rate": 1.1727413926999742e-05,
      "loss": 1.158,
      "step": 23640
    },
    {
      "epoch": 1.8319293557195144,
      "grad_norm": 1.5652697086334229,
      "learning_rate": 1.171964794201398e-05,
      "loss": 1.1352,
      "step": 23650
    },
    {
      "epoch": 1.832703964058173,
      "grad_norm": 1.5480676889419556,
      "learning_rate": 1.1711881957028217e-05,
      "loss": 1.2628,
      "step": 23660
    },
    {
      "epoch": 1.8334785723968319,
      "grad_norm": 1.3913862705230713,
      "learning_rate": 1.1704115972042455e-05,
      "loss": 1.1456,
      "step": 23670
    },
    {
      "epoch": 1.8342531807354905,
      "grad_norm": 1.6854642629623413,
      "learning_rate": 1.1696349987056691e-05,
      "loss": 1.1469,
      "step": 23680
    },
    {
      "epoch": 1.8350277890741493,
      "grad_norm": 1.5366324186325073,
      "learning_rate": 1.168858400207093e-05,
      "loss": 1.1443,
      "step": 23690
    },
    {
      "epoch": 1.8358023974128082,
      "grad_norm": 1.5438919067382812,
      "learning_rate": 1.1680818017085167e-05,
      "loss": 1.1617,
      "step": 23700
    },
    {
      "epoch": 1.836577005751467,
      "grad_norm": 2.2241523265838623,
      "learning_rate": 1.1673052032099405e-05,
      "loss": 1.2331,
      "step": 23710
    },
    {
      "epoch": 1.8373516140901258,
      "grad_norm": 1.362812876701355,
      "learning_rate": 1.1665286047113641e-05,
      "loss": 1.182,
      "step": 23720
    },
    {
      "epoch": 1.8381262224287844,
      "grad_norm": 1.7810792922973633,
      "learning_rate": 1.1657520062127879e-05,
      "loss": 1.2088,
      "step": 23730
    },
    {
      "epoch": 1.8389008307674433,
      "grad_norm": 1.712074637413025,
      "learning_rate": 1.1649754077142119e-05,
      "loss": 1.21,
      "step": 23740
    },
    {
      "epoch": 1.8396754391061019,
      "grad_norm": 1.6419241428375244,
      "learning_rate": 1.1641988092156356e-05,
      "loss": 1.1969,
      "step": 23750
    },
    {
      "epoch": 1.8404500474447607,
      "grad_norm": 1.508281946182251,
      "learning_rate": 1.1634222107170593e-05,
      "loss": 1.1946,
      "step": 23760
    },
    {
      "epoch": 1.8412246557834195,
      "grad_norm": 1.9247491359710693,
      "learning_rate": 1.162645612218483e-05,
      "loss": 1.1996,
      "step": 23770
    },
    {
      "epoch": 1.8419992641220784,
      "grad_norm": 1.7563426494598389,
      "learning_rate": 1.1618690137199068e-05,
      "loss": 1.1809,
      "step": 23780
    },
    {
      "epoch": 1.8427738724607372,
      "grad_norm": 1.570681095123291,
      "learning_rate": 1.1610924152213306e-05,
      "loss": 1.127,
      "step": 23790
    },
    {
      "epoch": 1.8435484807993958,
      "grad_norm": 1.712291955947876,
      "learning_rate": 1.1603158167227544e-05,
      "loss": 1.2074,
      "step": 23800
    },
    {
      "epoch": 1.8443230891380544,
      "grad_norm": 1.3466310501098633,
      "learning_rate": 1.159539218224178e-05,
      "loss": 1.1826,
      "step": 23810
    },
    {
      "epoch": 1.8450976974767133,
      "grad_norm": 1.3785569667816162,
      "learning_rate": 1.1587626197256018e-05,
      "loss": 1.2082,
      "step": 23820
    },
    {
      "epoch": 1.845872305815372,
      "grad_norm": 1.5628292560577393,
      "learning_rate": 1.1579860212270258e-05,
      "loss": 1.17,
      "step": 23830
    },
    {
      "epoch": 1.846646914154031,
      "grad_norm": 1.598454475402832,
      "learning_rate": 1.1572094227284496e-05,
      "loss": 1.2513,
      "step": 23840
    },
    {
      "epoch": 1.8474215224926898,
      "grad_norm": 1.2656117677688599,
      "learning_rate": 1.1564328242298732e-05,
      "loss": 1.2211,
      "step": 23850
    },
    {
      "epoch": 1.8481961308313484,
      "grad_norm": 1.2406182289123535,
      "learning_rate": 1.155656225731297e-05,
      "loss": 1.1907,
      "step": 23860
    },
    {
      "epoch": 1.8489707391700072,
      "grad_norm": 1.6978408098220825,
      "learning_rate": 1.1548796272327207e-05,
      "loss": 1.1293,
      "step": 23870
    },
    {
      "epoch": 1.8497453475086658,
      "grad_norm": 1.4606724977493286,
      "learning_rate": 1.1541030287341445e-05,
      "loss": 1.2093,
      "step": 23880
    },
    {
      "epoch": 1.8505199558473246,
      "grad_norm": 2.8975844383239746,
      "learning_rate": 1.1533264302355681e-05,
      "loss": 1.15,
      "step": 23890
    },
    {
      "epoch": 1.8512945641859835,
      "grad_norm": 1.3928382396697998,
      "learning_rate": 1.152549831736992e-05,
      "loss": 1.1706,
      "step": 23900
    },
    {
      "epoch": 1.8520691725246423,
      "grad_norm": 1.3435856103897095,
      "learning_rate": 1.1517732332384157e-05,
      "loss": 1.1051,
      "step": 23910
    },
    {
      "epoch": 1.8528437808633011,
      "grad_norm": 1.5689398050308228,
      "learning_rate": 1.1509966347398395e-05,
      "loss": 1.0894,
      "step": 23920
    },
    {
      "epoch": 1.8536183892019598,
      "grad_norm": 1.4854931831359863,
      "learning_rate": 1.1502200362412633e-05,
      "loss": 1.1644,
      "step": 23930
    },
    {
      "epoch": 1.8543929975406184,
      "grad_norm": 1.6585465669631958,
      "learning_rate": 1.149443437742687e-05,
      "loss": 1.1904,
      "step": 23940
    },
    {
      "epoch": 1.8551676058792772,
      "grad_norm": 1.2604038715362549,
      "learning_rate": 1.1486668392441109e-05,
      "loss": 1.1311,
      "step": 23950
    },
    {
      "epoch": 1.855942214217936,
      "grad_norm": 1.3949007987976074,
      "learning_rate": 1.1478902407455347e-05,
      "loss": 1.2671,
      "step": 23960
    },
    {
      "epoch": 1.8567168225565949,
      "grad_norm": 1.5234123468399048,
      "learning_rate": 1.1471136422469584e-05,
      "loss": 1.1818,
      "step": 23970
    },
    {
      "epoch": 1.8574914308952537,
      "grad_norm": 1.8446587324142456,
      "learning_rate": 1.146337043748382e-05,
      "loss": 1.1361,
      "step": 23980
    },
    {
      "epoch": 1.8582660392339123,
      "grad_norm": 1.5536060333251953,
      "learning_rate": 1.1455604452498058e-05,
      "loss": 1.152,
      "step": 23990
    },
    {
      "epoch": 1.8590406475725711,
      "grad_norm": 1.6173344850540161,
      "learning_rate": 1.1447838467512296e-05,
      "loss": 1.1663,
      "step": 24000
    },
    {
      "epoch": 1.8598152559112298,
      "grad_norm": 1.87725031375885,
      "learning_rate": 1.1440072482526534e-05,
      "loss": 1.1736,
      "step": 24010
    },
    {
      "epoch": 1.8605898642498886,
      "grad_norm": 1.5080748796463013,
      "learning_rate": 1.143230649754077e-05,
      "loss": 1.276,
      "step": 24020
    },
    {
      "epoch": 1.8613644725885474,
      "grad_norm": 1.7394980192184448,
      "learning_rate": 1.142454051255501e-05,
      "loss": 1.2278,
      "step": 24030
    },
    {
      "epoch": 1.8621390809272063,
      "grad_norm": 1.4839907884597778,
      "learning_rate": 1.1416774527569248e-05,
      "loss": 1.2972,
      "step": 24040
    },
    {
      "epoch": 1.862913689265865,
      "grad_norm": 1.305985927581787,
      "learning_rate": 1.1409008542583486e-05,
      "loss": 1.2001,
      "step": 24050
    },
    {
      "epoch": 1.8636882976045237,
      "grad_norm": 1.726223349571228,
      "learning_rate": 1.1401242557597722e-05,
      "loss": 1.1886,
      "step": 24060
    },
    {
      "epoch": 1.8644629059431823,
      "grad_norm": 1.1795499324798584,
      "learning_rate": 1.139347657261196e-05,
      "loss": 1.2198,
      "step": 24070
    },
    {
      "epoch": 1.8652375142818411,
      "grad_norm": 1.656719446182251,
      "learning_rate": 1.1385710587626198e-05,
      "loss": 1.2055,
      "step": 24080
    },
    {
      "epoch": 1.8660121226205,
      "grad_norm": 1.2935644388198853,
      "learning_rate": 1.1377944602640435e-05,
      "loss": 1.2359,
      "step": 24090
    },
    {
      "epoch": 1.8667867309591588,
      "grad_norm": 1.7796680927276611,
      "learning_rate": 1.1370178617654672e-05,
      "loss": 1.2139,
      "step": 24100
    },
    {
      "epoch": 1.8675613392978176,
      "grad_norm": 1.3593676090240479,
      "learning_rate": 1.136241263266891e-05,
      "loss": 1.0634,
      "step": 24110
    },
    {
      "epoch": 1.8683359476364763,
      "grad_norm": 1.8843507766723633,
      "learning_rate": 1.1354646647683147e-05,
      "loss": 1.1813,
      "step": 24120
    },
    {
      "epoch": 1.869110555975135,
      "grad_norm": 1.1328861713409424,
      "learning_rate": 1.1346880662697387e-05,
      "loss": 1.1765,
      "step": 24130
    },
    {
      "epoch": 1.8698851643137937,
      "grad_norm": 1.423479676246643,
      "learning_rate": 1.1339114677711625e-05,
      "loss": 1.16,
      "step": 24140
    },
    {
      "epoch": 1.8706597726524525,
      "grad_norm": 1.5961048603057861,
      "learning_rate": 1.1331348692725861e-05,
      "loss": 1.1854,
      "step": 24150
    },
    {
      "epoch": 1.8714343809911114,
      "grad_norm": 1.3770421743392944,
      "learning_rate": 1.1323582707740099e-05,
      "loss": 1.1471,
      "step": 24160
    },
    {
      "epoch": 1.8722089893297702,
      "grad_norm": 1.6407054662704468,
      "learning_rate": 1.1315816722754337e-05,
      "loss": 1.1572,
      "step": 24170
    },
    {
      "epoch": 1.872983597668429,
      "grad_norm": 1.4193336963653564,
      "learning_rate": 1.1308050737768575e-05,
      "loss": 1.1687,
      "step": 24180
    },
    {
      "epoch": 1.8737582060070876,
      "grad_norm": 1.2607332468032837,
      "learning_rate": 1.130028475278281e-05,
      "loss": 1.0722,
      "step": 24190
    },
    {
      "epoch": 1.8745328143457465,
      "grad_norm": 1.3525832891464233,
      "learning_rate": 1.1292518767797049e-05,
      "loss": 1.2479,
      "step": 24200
    },
    {
      "epoch": 1.875307422684405,
      "grad_norm": 1.4340834617614746,
      "learning_rate": 1.1284752782811286e-05,
      "loss": 1.16,
      "step": 24210
    },
    {
      "epoch": 1.876082031023064,
      "grad_norm": 1.4327856302261353,
      "learning_rate": 1.1276986797825524e-05,
      "loss": 1.1873,
      "step": 24220
    },
    {
      "epoch": 1.8768566393617228,
      "grad_norm": 1.6419035196304321,
      "learning_rate": 1.1269220812839762e-05,
      "loss": 1.1494,
      "step": 24230
    },
    {
      "epoch": 1.8776312477003816,
      "grad_norm": 1.4975104331970215,
      "learning_rate": 1.1261454827854e-05,
      "loss": 1.1928,
      "step": 24240
    },
    {
      "epoch": 1.8784058560390404,
      "grad_norm": 1.9596039056777954,
      "learning_rate": 1.1253688842868238e-05,
      "loss": 1.1521,
      "step": 24250
    },
    {
      "epoch": 1.879180464377699,
      "grad_norm": 1.3831108808517456,
      "learning_rate": 1.1245922857882476e-05,
      "loss": 1.179,
      "step": 24260
    },
    {
      "epoch": 1.8799550727163576,
      "grad_norm": 1.3328503370285034,
      "learning_rate": 1.1238156872896712e-05,
      "loss": 1.1153,
      "step": 24270
    },
    {
      "epoch": 1.8807296810550165,
      "grad_norm": 2.354593515396118,
      "learning_rate": 1.123039088791095e-05,
      "loss": 1.1369,
      "step": 24280
    },
    {
      "epoch": 1.8815042893936753,
      "grad_norm": 1.5722540616989136,
      "learning_rate": 1.1222624902925188e-05,
      "loss": 1.1874,
      "step": 24290
    },
    {
      "epoch": 1.8822788977323341,
      "grad_norm": 1.2950407266616821,
      "learning_rate": 1.1214858917939426e-05,
      "loss": 1.1322,
      "step": 24300
    },
    {
      "epoch": 1.883053506070993,
      "grad_norm": 1.1996928453445435,
      "learning_rate": 1.1207092932953663e-05,
      "loss": 1.0882,
      "step": 24310
    },
    {
      "epoch": 1.8838281144096516,
      "grad_norm": 1.945759654045105,
      "learning_rate": 1.1199326947967901e-05,
      "loss": 1.1377,
      "step": 24320
    },
    {
      "epoch": 1.8846027227483104,
      "grad_norm": 1.9415541887283325,
      "learning_rate": 1.1191560962982139e-05,
      "loss": 1.1891,
      "step": 24330
    },
    {
      "epoch": 1.885377331086969,
      "grad_norm": 1.8230100870132446,
      "learning_rate": 1.1183794977996377e-05,
      "loss": 1.1066,
      "step": 24340
    },
    {
      "epoch": 1.8861519394256279,
      "grad_norm": 1.1668291091918945,
      "learning_rate": 1.1176028993010615e-05,
      "loss": 1.1366,
      "step": 24350
    },
    {
      "epoch": 1.8869265477642867,
      "grad_norm": 1.4626045227050781,
      "learning_rate": 1.1168263008024851e-05,
      "loss": 1.3193,
      "step": 24360
    },
    {
      "epoch": 1.8877011561029455,
      "grad_norm": 1.3912599086761475,
      "learning_rate": 1.1160497023039089e-05,
      "loss": 1.264,
      "step": 24370
    },
    {
      "epoch": 1.8884757644416044,
      "grad_norm": 1.3710078001022339,
      "learning_rate": 1.1152731038053327e-05,
      "loss": 1.1917,
      "step": 24380
    },
    {
      "epoch": 1.889250372780263,
      "grad_norm": 1.294569730758667,
      "learning_rate": 1.1144965053067565e-05,
      "loss": 1.2728,
      "step": 24390
    },
    {
      "epoch": 1.8900249811189216,
      "grad_norm": 1.7074310779571533,
      "learning_rate": 1.11371990680818e-05,
      "loss": 1.0712,
      "step": 24400
    },
    {
      "epoch": 1.8907995894575804,
      "grad_norm": 1.673329472541809,
      "learning_rate": 1.1129433083096039e-05,
      "loss": 1.1796,
      "step": 24410
    },
    {
      "epoch": 1.8915741977962393,
      "grad_norm": 1.5427756309509277,
      "learning_rate": 1.1121667098110278e-05,
      "loss": 1.2709,
      "step": 24420
    },
    {
      "epoch": 1.892348806134898,
      "grad_norm": 1.8281525373458862,
      "learning_rate": 1.1113901113124516e-05,
      "loss": 1.1553,
      "step": 24430
    },
    {
      "epoch": 1.893123414473557,
      "grad_norm": 1.4476600885391235,
      "learning_rate": 1.1106135128138752e-05,
      "loss": 1.1612,
      "step": 24440
    },
    {
      "epoch": 1.8938980228122155,
      "grad_norm": 2.074651002883911,
      "learning_rate": 1.109836914315299e-05,
      "loss": 1.2203,
      "step": 24450
    },
    {
      "epoch": 1.8946726311508744,
      "grad_norm": 1.3405864238739014,
      "learning_rate": 1.1090603158167228e-05,
      "loss": 1.1874,
      "step": 24460
    },
    {
      "epoch": 1.895447239489533,
      "grad_norm": 1.5280441045761108,
      "learning_rate": 1.1082837173181466e-05,
      "loss": 1.2367,
      "step": 24470
    },
    {
      "epoch": 1.8962218478281918,
      "grad_norm": 1.8071677684783936,
      "learning_rate": 1.1075071188195702e-05,
      "loss": 1.1753,
      "step": 24480
    },
    {
      "epoch": 1.8969964561668506,
      "grad_norm": 1.6961504220962524,
      "learning_rate": 1.106730520320994e-05,
      "loss": 1.1649,
      "step": 24490
    },
    {
      "epoch": 1.8977710645055095,
      "grad_norm": 2.040566921234131,
      "learning_rate": 1.1059539218224178e-05,
      "loss": 1.1779,
      "step": 24500
    },
    {
      "epoch": 1.8985456728441683,
      "grad_norm": 1.4037944078445435,
      "learning_rate": 1.1051773233238416e-05,
      "loss": 1.1364,
      "step": 24510
    },
    {
      "epoch": 1.899320281182827,
      "grad_norm": 1.8442835807800293,
      "learning_rate": 1.1044007248252655e-05,
      "loss": 1.1778,
      "step": 24520
    },
    {
      "epoch": 1.9000948895214858,
      "grad_norm": 1.1939057111740112,
      "learning_rate": 1.1036241263266891e-05,
      "loss": 1.1846,
      "step": 24530
    },
    {
      "epoch": 1.9008694978601444,
      "grad_norm": 1.708385944366455,
      "learning_rate": 1.102847527828113e-05,
      "loss": 1.2828,
      "step": 24540
    },
    {
      "epoch": 1.9016441061988032,
      "grad_norm": 2.161522626876831,
      "learning_rate": 1.1020709293295367e-05,
      "loss": 1.2293,
      "step": 24550
    },
    {
      "epoch": 1.902418714537462,
      "grad_norm": 2.1839826107025146,
      "learning_rate": 1.1012943308309605e-05,
      "loss": 1.1339,
      "step": 24560
    },
    {
      "epoch": 1.9031933228761209,
      "grad_norm": 1.5639029741287231,
      "learning_rate": 1.1005177323323841e-05,
      "loss": 1.1573,
      "step": 24570
    },
    {
      "epoch": 1.9039679312147795,
      "grad_norm": 1.5728198289871216,
      "learning_rate": 1.0997411338338079e-05,
      "loss": 1.0811,
      "step": 24580
    },
    {
      "epoch": 1.9047425395534383,
      "grad_norm": 1.6087052822113037,
      "learning_rate": 1.0989645353352317e-05,
      "loss": 1.1457,
      "step": 24590
    },
    {
      "epoch": 1.905517147892097,
      "grad_norm": 1.8217777013778687,
      "learning_rate": 1.0981879368366555e-05,
      "loss": 1.2507,
      "step": 24600
    },
    {
      "epoch": 1.9062917562307558,
      "grad_norm": 1.5330227613449097,
      "learning_rate": 1.0974113383380791e-05,
      "loss": 1.1207,
      "step": 24610
    },
    {
      "epoch": 1.9070663645694146,
      "grad_norm": 1.63575279712677,
      "learning_rate": 1.0967123996893606e-05,
      "loss": 1.1993,
      "step": 24620
    },
    {
      "epoch": 1.9078409729080734,
      "grad_norm": 1.7123963832855225,
      "learning_rate": 1.0959358011907844e-05,
      "loss": 1.2151,
      "step": 24630
    },
    {
      "epoch": 1.9086155812467323,
      "grad_norm": 1.5458455085754395,
      "learning_rate": 1.0951592026922082e-05,
      "loss": 1.1263,
      "step": 24640
    },
    {
      "epoch": 1.9093901895853909,
      "grad_norm": 2.0289809703826904,
      "learning_rate": 1.0943826041936318e-05,
      "loss": 1.1607,
      "step": 24650
    },
    {
      "epoch": 1.9101647979240497,
      "grad_norm": 1.6567314863204956,
      "learning_rate": 1.0936060056950557e-05,
      "loss": 1.1017,
      "step": 24660
    },
    {
      "epoch": 1.9109394062627083,
      "grad_norm": 1.732761263847351,
      "learning_rate": 1.0928294071964795e-05,
      "loss": 1.1773,
      "step": 24670
    },
    {
      "epoch": 1.9117140146013671,
      "grad_norm": 1.7429089546203613,
      "learning_rate": 1.0920528086979033e-05,
      "loss": 1.1488,
      "step": 24680
    },
    {
      "epoch": 1.912488622940026,
      "grad_norm": 1.7636971473693848,
      "learning_rate": 1.091276210199327e-05,
      "loss": 1.1756,
      "step": 24690
    },
    {
      "epoch": 1.9132632312786848,
      "grad_norm": 1.7471966743469238,
      "learning_rate": 1.0904996117007507e-05,
      "loss": 1.1837,
      "step": 24700
    },
    {
      "epoch": 1.9140378396173436,
      "grad_norm": 1.485926628112793,
      "learning_rate": 1.0897230132021745e-05,
      "loss": 1.1834,
      "step": 24710
    },
    {
      "epoch": 1.9148124479560023,
      "grad_norm": 1.1446772813796997,
      "learning_rate": 1.0889464147035983e-05,
      "loss": 1.2001,
      "step": 24720
    },
    {
      "epoch": 1.9155870562946609,
      "grad_norm": 1.2416292428970337,
      "learning_rate": 1.0881698162050219e-05,
      "loss": 1.0997,
      "step": 24730
    },
    {
      "epoch": 1.9163616646333197,
      "grad_norm": 1.417583703994751,
      "learning_rate": 1.0873932177064457e-05,
      "loss": 1.2894,
      "step": 24740
    },
    {
      "epoch": 1.9171362729719785,
      "grad_norm": 1.670358419418335,
      "learning_rate": 1.0866166192078695e-05,
      "loss": 1.2066,
      "step": 24750
    },
    {
      "epoch": 1.9179108813106374,
      "grad_norm": 1.6201330423355103,
      "learning_rate": 1.0858400207092934e-05,
      "loss": 1.1435,
      "step": 24760
    },
    {
      "epoch": 1.9186854896492962,
      "grad_norm": 2.0743038654327393,
      "learning_rate": 1.085063422210717e-05,
      "loss": 1.2012,
      "step": 24770
    },
    {
      "epoch": 1.9194600979879548,
      "grad_norm": 1.5437489748001099,
      "learning_rate": 1.0842868237121408e-05,
      "loss": 1.1893,
      "step": 24780
    },
    {
      "epoch": 1.9202347063266136,
      "grad_norm": 1.9114865064620972,
      "learning_rate": 1.0835102252135646e-05,
      "loss": 1.1017,
      "step": 24790
    },
    {
      "epoch": 1.9210093146652722,
      "grad_norm": 1.574073076248169,
      "learning_rate": 1.0827336267149884e-05,
      "loss": 1.2298,
      "step": 24800
    },
    {
      "epoch": 1.921783923003931,
      "grad_norm": 3.0452311038970947,
      "learning_rate": 1.0819570282164122e-05,
      "loss": 1.2493,
      "step": 24810
    },
    {
      "epoch": 1.92255853134259,
      "grad_norm": 1.399530053138733,
      "learning_rate": 1.0811804297178358e-05,
      "loss": 1.2382,
      "step": 24820
    },
    {
      "epoch": 1.9233331396812487,
      "grad_norm": 2.0065197944641113,
      "learning_rate": 1.0804038312192596e-05,
      "loss": 1.1473,
      "step": 24830
    },
    {
      "epoch": 1.9241077480199076,
      "grad_norm": 1.580051064491272,
      "learning_rate": 1.0796272327206834e-05,
      "loss": 1.2315,
      "step": 24840
    },
    {
      "epoch": 1.9248823563585662,
      "grad_norm": 1.626995325088501,
      "learning_rate": 1.0788506342221074e-05,
      "loss": 1.1003,
      "step": 24850
    },
    {
      "epoch": 1.9256569646972248,
      "grad_norm": 2.1683402061462402,
      "learning_rate": 1.078074035723531e-05,
      "loss": 1.2087,
      "step": 24860
    },
    {
      "epoch": 1.9264315730358836,
      "grad_norm": 1.6687061786651611,
      "learning_rate": 1.0772974372249548e-05,
      "loss": 1.2567,
      "step": 24870
    },
    {
      "epoch": 1.9272061813745425,
      "grad_norm": 2.0661065578460693,
      "learning_rate": 1.0765208387263785e-05,
      "loss": 1.1449,
      "step": 24880
    },
    {
      "epoch": 1.9279807897132013,
      "grad_norm": 2.1195034980773926,
      "learning_rate": 1.0757442402278023e-05,
      "loss": 1.1941,
      "step": 24890
    },
    {
      "epoch": 1.9287553980518601,
      "grad_norm": 1.592932105064392,
      "learning_rate": 1.074967641729226e-05,
      "loss": 1.1983,
      "step": 24900
    },
    {
      "epoch": 1.9295300063905187,
      "grad_norm": 1.9577889442443848,
      "learning_rate": 1.0741910432306497e-05,
      "loss": 1.2122,
      "step": 24910
    },
    {
      "epoch": 1.9303046147291776,
      "grad_norm": 1.121353030204773,
      "learning_rate": 1.0734144447320735e-05,
      "loss": 1.2156,
      "step": 24920
    },
    {
      "epoch": 1.9310792230678362,
      "grad_norm": 1.3745222091674805,
      "learning_rate": 1.0726378462334973e-05,
      "loss": 1.2104,
      "step": 24930
    },
    {
      "epoch": 1.931853831406495,
      "grad_norm": 1.590170979499817,
      "learning_rate": 1.071861247734921e-05,
      "loss": 1.1543,
      "step": 24940
    },
    {
      "epoch": 1.9326284397451539,
      "grad_norm": 1.2537838220596313,
      "learning_rate": 1.0710846492363449e-05,
      "loss": 1.1082,
      "step": 24950
    },
    {
      "epoch": 1.9334030480838127,
      "grad_norm": 1.6200464963912964,
      "learning_rate": 1.0703080507377687e-05,
      "loss": 1.1677,
      "step": 24960
    },
    {
      "epoch": 1.9341776564224715,
      "grad_norm": 1.2779221534729004,
      "learning_rate": 1.0695314522391925e-05,
      "loss": 1.1707,
      "step": 24970
    },
    {
      "epoch": 1.9349522647611301,
      "grad_norm": 1.4299260377883911,
      "learning_rate": 1.068754853740616e-05,
      "loss": 1.1684,
      "step": 24980
    },
    {
      "epoch": 1.935726873099789,
      "grad_norm": 1.2426880598068237,
      "learning_rate": 1.0679782552420399e-05,
      "loss": 1.3352,
      "step": 24990
    },
    {
      "epoch": 1.9365014814384476,
      "grad_norm": 1.9559508562088013,
      "learning_rate": 1.0672016567434636e-05,
      "loss": 1.1507,
      "step": 25000
    },
    {
      "epoch": 1.9372760897771064,
      "grad_norm": 1.9972267150878906,
      "learning_rate": 1.0664250582448874e-05,
      "loss": 1.2398,
      "step": 25010
    },
    {
      "epoch": 1.9380506981157652,
      "grad_norm": 1.6310155391693115,
      "learning_rate": 1.0656484597463112e-05,
      "loss": 1.2457,
      "step": 25020
    },
    {
      "epoch": 1.938825306454424,
      "grad_norm": 1.4834500551223755,
      "learning_rate": 1.0648718612477348e-05,
      "loss": 1.206,
      "step": 25030
    },
    {
      "epoch": 1.939599914793083,
      "grad_norm": 1.2821078300476074,
      "learning_rate": 1.0640952627491586e-05,
      "loss": 1.2488,
      "step": 25040
    },
    {
      "epoch": 1.9403745231317415,
      "grad_norm": 1.703495740890503,
      "learning_rate": 1.0633186642505826e-05,
      "loss": 1.2222,
      "step": 25050
    },
    {
      "epoch": 1.9411491314704001,
      "grad_norm": 1.6272501945495605,
      "learning_rate": 1.0625420657520064e-05,
      "loss": 1.1265,
      "step": 25060
    },
    {
      "epoch": 1.941923739809059,
      "grad_norm": 1.8156288862228394,
      "learning_rate": 1.06176546725343e-05,
      "loss": 1.1924,
      "step": 25070
    },
    {
      "epoch": 1.9426983481477178,
      "grad_norm": 1.3148505687713623,
      "learning_rate": 1.0609888687548538e-05,
      "loss": 1.2176,
      "step": 25080
    },
    {
      "epoch": 1.9434729564863766,
      "grad_norm": 2.127603054046631,
      "learning_rate": 1.0602122702562776e-05,
      "loss": 1.1945,
      "step": 25090
    },
    {
      "epoch": 1.9442475648250355,
      "grad_norm": 1.5058890581130981,
      "learning_rate": 1.0594356717577013e-05,
      "loss": 1.1788,
      "step": 25100
    },
    {
      "epoch": 1.945022173163694,
      "grad_norm": 1.5694369077682495,
      "learning_rate": 1.058659073259125e-05,
      "loss": 1.2043,
      "step": 25110
    },
    {
      "epoch": 1.945796781502353,
      "grad_norm": 1.496588110923767,
      "learning_rate": 1.0578824747605487e-05,
      "loss": 1.2051,
      "step": 25120
    },
    {
      "epoch": 1.9465713898410115,
      "grad_norm": 1.5778244733810425,
      "learning_rate": 1.0571058762619725e-05,
      "loss": 1.2178,
      "step": 25130
    },
    {
      "epoch": 1.9473459981796704,
      "grad_norm": 1.3298943042755127,
      "learning_rate": 1.0563292777633963e-05,
      "loss": 1.1917,
      "step": 25140
    },
    {
      "epoch": 1.9481206065183292,
      "grad_norm": 1.677238941192627,
      "learning_rate": 1.0555526792648201e-05,
      "loss": 1.2502,
      "step": 25150
    },
    {
      "epoch": 1.948895214856988,
      "grad_norm": 1.443076252937317,
      "learning_rate": 1.0547760807662439e-05,
      "loss": 1.1003,
      "step": 25160
    },
    {
      "epoch": 1.9496698231956469,
      "grad_norm": 1.1137633323669434,
      "learning_rate": 1.0539994822676677e-05,
      "loss": 1.2287,
      "step": 25170
    },
    {
      "epoch": 1.9504444315343055,
      "grad_norm": 1.4011743068695068,
      "learning_rate": 1.0532228837690915e-05,
      "loss": 1.3232,
      "step": 25180
    },
    {
      "epoch": 1.951219039872964,
      "grad_norm": 2.020988941192627,
      "learning_rate": 1.0524462852705153e-05,
      "loss": 1.1448,
      "step": 25190
    },
    {
      "epoch": 1.951993648211623,
      "grad_norm": 1.5856568813323975,
      "learning_rate": 1.0516696867719389e-05,
      "loss": 1.1475,
      "step": 25200
    },
    {
      "epoch": 1.9527682565502817,
      "grad_norm": 1.7787737846374512,
      "learning_rate": 1.0508930882733627e-05,
      "loss": 1.2993,
      "step": 25210
    },
    {
      "epoch": 1.9535428648889406,
      "grad_norm": 1.7092362642288208,
      "learning_rate": 1.0501164897747864e-05,
      "loss": 1.2383,
      "step": 25220
    },
    {
      "epoch": 1.9543174732275994,
      "grad_norm": 1.6978912353515625,
      "learning_rate": 1.0493398912762102e-05,
      "loss": 1.1816,
      "step": 25230
    },
    {
      "epoch": 1.955092081566258,
      "grad_norm": 2.0761613845825195,
      "learning_rate": 1.0485632927776338e-05,
      "loss": 1.2921,
      "step": 25240
    },
    {
      "epoch": 1.9558666899049169,
      "grad_norm": 1.4610131978988647,
      "learning_rate": 1.0477866942790578e-05,
      "loss": 1.1428,
      "step": 25250
    },
    {
      "epoch": 1.9566412982435755,
      "grad_norm": 2.029831886291504,
      "learning_rate": 1.0470100957804816e-05,
      "loss": 1.1697,
      "step": 25260
    },
    {
      "epoch": 1.9574159065822343,
      "grad_norm": 1.580223560333252,
      "learning_rate": 1.0462334972819054e-05,
      "loss": 1.2276,
      "step": 25270
    },
    {
      "epoch": 1.9581905149208931,
      "grad_norm": 1.397701621055603,
      "learning_rate": 1.045456898783329e-05,
      "loss": 1.177,
      "step": 25280
    },
    {
      "epoch": 1.958965123259552,
      "grad_norm": 1.3902751207351685,
      "learning_rate": 1.0446803002847528e-05,
      "loss": 1.2006,
      "step": 25290
    },
    {
      "epoch": 1.9597397315982108,
      "grad_norm": 1.2677648067474365,
      "learning_rate": 1.0439037017861766e-05,
      "loss": 1.1915,
      "step": 25300
    },
    {
      "epoch": 1.9605143399368694,
      "grad_norm": 1.6058974266052246,
      "learning_rate": 1.0431271032876004e-05,
      "loss": 1.1793,
      "step": 25310
    },
    {
      "epoch": 1.961288948275528,
      "grad_norm": 1.6617003679275513,
      "learning_rate": 1.042350504789024e-05,
      "loss": 1.1583,
      "step": 25320
    },
    {
      "epoch": 1.9620635566141869,
      "grad_norm": 1.5021034479141235,
      "learning_rate": 1.0415739062904478e-05,
      "loss": 1.212,
      "step": 25330
    },
    {
      "epoch": 1.9628381649528457,
      "grad_norm": 1.3211277723312378,
      "learning_rate": 1.0407973077918717e-05,
      "loss": 1.141,
      "step": 25340
    },
    {
      "epoch": 1.9636127732915045,
      "grad_norm": 1.4999641180038452,
      "learning_rate": 1.0400207092932955e-05,
      "loss": 1.167,
      "step": 25350
    },
    {
      "epoch": 1.9643873816301634,
      "grad_norm": 1.3204996585845947,
      "learning_rate": 1.0392441107947193e-05,
      "loss": 1.1413,
      "step": 25360
    },
    {
      "epoch": 1.965161989968822,
      "grad_norm": 1.9708824157714844,
      "learning_rate": 1.0384675122961429e-05,
      "loss": 1.1745,
      "step": 25370
    },
    {
      "epoch": 1.9659365983074808,
      "grad_norm": 1.5432710647583008,
      "learning_rate": 1.0376909137975667e-05,
      "loss": 1.2249,
      "step": 25380
    },
    {
      "epoch": 1.9667112066461394,
      "grad_norm": 2.6234145164489746,
      "learning_rate": 1.0369143152989905e-05,
      "loss": 1.2252,
      "step": 25390
    },
    {
      "epoch": 1.9674858149847982,
      "grad_norm": 1.8242781162261963,
      "learning_rate": 1.0361377168004143e-05,
      "loss": 1.242,
      "step": 25400
    },
    {
      "epoch": 1.968260423323457,
      "grad_norm": 1.340093970298767,
      "learning_rate": 1.0353611183018379e-05,
      "loss": 1.2551,
      "step": 25410
    },
    {
      "epoch": 1.969035031662116,
      "grad_norm": 1.4369924068450928,
      "learning_rate": 1.0345845198032617e-05,
      "loss": 1.2182,
      "step": 25420
    },
    {
      "epoch": 1.9698096400007747,
      "grad_norm": 1.5266560316085815,
      "learning_rate": 1.0338079213046855e-05,
      "loss": 1.1522,
      "step": 25430
    },
    {
      "epoch": 1.9705842483394334,
      "grad_norm": 1.6349223852157593,
      "learning_rate": 1.0330313228061094e-05,
      "loss": 1.1732,
      "step": 25440
    },
    {
      "epoch": 1.9713588566780922,
      "grad_norm": 1.7479538917541504,
      "learning_rate": 1.032254724307533e-05,
      "loss": 1.1903,
      "step": 25450
    },
    {
      "epoch": 1.9721334650167508,
      "grad_norm": 1.522397518157959,
      "learning_rate": 1.0314781258089568e-05,
      "loss": 1.2322,
      "step": 25460
    },
    {
      "epoch": 1.9729080733554096,
      "grad_norm": 2.008399724960327,
      "learning_rate": 1.0307015273103806e-05,
      "loss": 1.1637,
      "step": 25470
    },
    {
      "epoch": 1.9736826816940685,
      "grad_norm": 1.694149374961853,
      "learning_rate": 1.0299249288118044e-05,
      "loss": 1.2202,
      "step": 25480
    },
    {
      "epoch": 1.9744572900327273,
      "grad_norm": 1.0883954763412476,
      "learning_rate": 1.029148330313228e-05,
      "loss": 1.07,
      "step": 25490
    },
    {
      "epoch": 1.9752318983713861,
      "grad_norm": 1.7158563137054443,
      "learning_rate": 1.0283717318146518e-05,
      "loss": 1.2115,
      "step": 25500
    },
    {
      "epoch": 1.9760065067100447,
      "grad_norm": 1.5042301416397095,
      "learning_rate": 1.0275951333160756e-05,
      "loss": 1.1794,
      "step": 25510
    },
    {
      "epoch": 1.9767811150487034,
      "grad_norm": 1.5485798120498657,
      "learning_rate": 1.0268185348174994e-05,
      "loss": 1.0809,
      "step": 25520
    },
    {
      "epoch": 1.9775557233873622,
      "grad_norm": 1.5840877294540405,
      "learning_rate": 1.0260419363189231e-05,
      "loss": 1.2375,
      "step": 25530
    },
    {
      "epoch": 1.978330331726021,
      "grad_norm": 1.4472343921661377,
      "learning_rate": 1.025265337820347e-05,
      "loss": 1.1172,
      "step": 25540
    },
    {
      "epoch": 1.9791049400646799,
      "grad_norm": 2.061669111251831,
      "learning_rate": 1.0244887393217707e-05,
      "loss": 1.1817,
      "step": 25550
    },
    {
      "epoch": 1.9798795484033387,
      "grad_norm": 1.37826406955719,
      "learning_rate": 1.0237121408231945e-05,
      "loss": 1.2095,
      "step": 25560
    },
    {
      "epoch": 1.9806541567419973,
      "grad_norm": 1.623516321182251,
      "learning_rate": 1.0229355423246183e-05,
      "loss": 1.1408,
      "step": 25570
    },
    {
      "epoch": 1.9814287650806561,
      "grad_norm": 1.4206534624099731,
      "learning_rate": 1.0221589438260419e-05,
      "loss": 1.2116,
      "step": 25580
    },
    {
      "epoch": 1.9822033734193147,
      "grad_norm": 1.2293667793273926,
      "learning_rate": 1.0213823453274657e-05,
      "loss": 1.1579,
      "step": 25590
    },
    {
      "epoch": 1.9829779817579736,
      "grad_norm": 1.6150624752044678,
      "learning_rate": 1.0206057468288895e-05,
      "loss": 1.2554,
      "step": 25600
    },
    {
      "epoch": 1.9837525900966324,
      "grad_norm": 1.4374219179153442,
      "learning_rate": 1.0198291483303133e-05,
      "loss": 1.2195,
      "step": 25610
    },
    {
      "epoch": 1.9845271984352912,
      "grad_norm": 1.2268091440200806,
      "learning_rate": 1.0190525498317369e-05,
      "loss": 1.1589,
      "step": 25620
    },
    {
      "epoch": 1.98530180677395,
      "grad_norm": 1.5134742259979248,
      "learning_rate": 1.0182759513331607e-05,
      "loss": 1.1619,
      "step": 25630
    },
    {
      "epoch": 1.9860764151126087,
      "grad_norm": 2.4832634925842285,
      "learning_rate": 1.0174993528345846e-05,
      "loss": 1.1956,
      "step": 25640
    },
    {
      "epoch": 1.9868510234512673,
      "grad_norm": 1.4566168785095215,
      "learning_rate": 1.0167227543360084e-05,
      "loss": 1.1249,
      "step": 25650
    },
    {
      "epoch": 1.9876256317899261,
      "grad_norm": 1.5722593069076538,
      "learning_rate": 1.015946155837432e-05,
      "loss": 1.1813,
      "step": 25660
    },
    {
      "epoch": 1.988400240128585,
      "grad_norm": 1.960983157157898,
      "learning_rate": 1.0151695573388558e-05,
      "loss": 1.2278,
      "step": 25670
    },
    {
      "epoch": 1.9891748484672438,
      "grad_norm": 1.3938299417495728,
      "learning_rate": 1.0143929588402796e-05,
      "loss": 1.1554,
      "step": 25680
    },
    {
      "epoch": 1.9899494568059026,
      "grad_norm": 1.2988016605377197,
      "learning_rate": 1.0136163603417034e-05,
      "loss": 1.1908,
      "step": 25690
    },
    {
      "epoch": 1.9907240651445612,
      "grad_norm": 1.2581560611724854,
      "learning_rate": 1.0128397618431272e-05,
      "loss": 1.1246,
      "step": 25700
    },
    {
      "epoch": 1.99149867348322,
      "grad_norm": 1.3295339345932007,
      "learning_rate": 1.0120631633445508e-05,
      "loss": 1.1452,
      "step": 25710
    },
    {
      "epoch": 1.9922732818218787,
      "grad_norm": 1.4914774894714355,
      "learning_rate": 1.0112865648459746e-05,
      "loss": 1.0504,
      "step": 25720
    },
    {
      "epoch": 1.9930478901605375,
      "grad_norm": 2.226623773574829,
      "learning_rate": 1.0105099663473984e-05,
      "loss": 1.1855,
      "step": 25730
    },
    {
      "epoch": 1.9938224984991963,
      "grad_norm": 2.1107921600341797,
      "learning_rate": 1.0097333678488223e-05,
      "loss": 1.2432,
      "step": 25740
    },
    {
      "epoch": 1.9945971068378552,
      "grad_norm": 1.6438323259353638,
      "learning_rate": 1.008956769350246e-05,
      "loss": 1.1478,
      "step": 25750
    },
    {
      "epoch": 1.995371715176514,
      "grad_norm": 1.7387142181396484,
      "learning_rate": 1.0081801708516697e-05,
      "loss": 1.1,
      "step": 25760
    },
    {
      "epoch": 1.9961463235151726,
      "grad_norm": 1.374511480331421,
      "learning_rate": 1.0074035723530935e-05,
      "loss": 1.2286,
      "step": 25770
    },
    {
      "epoch": 1.9969209318538315,
      "grad_norm": 1.856398582458496,
      "learning_rate": 1.0066269738545173e-05,
      "loss": 1.136,
      "step": 25780
    },
    {
      "epoch": 1.99769554019249,
      "grad_norm": 1.6058999300003052,
      "learning_rate": 1.005850375355941e-05,
      "loss": 1.2323,
      "step": 25790
    },
    {
      "epoch": 1.998470148531149,
      "grad_norm": 1.7785369157791138,
      "learning_rate": 1.0050737768573647e-05,
      "loss": 1.2346,
      "step": 25800
    },
    {
      "epoch": 1.9992447568698077,
      "grad_norm": 1.3330696821212769,
      "learning_rate": 1.0042971783587885e-05,
      "loss": 1.1475,
      "step": 25810
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.10011887550354,
      "learning_rate": 1.0035205798602123e-05,
      "loss": 1.1722,
      "step": 25820
    },
    {
      "epoch": 2.000774608338659,
      "grad_norm": 1.344878911972046,
      "learning_rate": 1.002743981361636e-05,
      "loss": 1.156,
      "step": 25830
    },
    {
      "epoch": 2.0015492166773177,
      "grad_norm": 1.7476624250411987,
      "learning_rate": 1.0019673828630599e-05,
      "loss": 1.1668,
      "step": 25840
    },
    {
      "epoch": 2.0023238250159765,
      "grad_norm": 1.9946153163909912,
      "learning_rate": 1.0011907843644836e-05,
      "loss": 1.1776,
      "step": 25850
    },
    {
      "epoch": 2.003098433354635,
      "grad_norm": 1.283286452293396,
      "learning_rate": 1.0004141858659074e-05,
      "loss": 1.2753,
      "step": 25860
    },
    {
      "epoch": 2.0038730416932937,
      "grad_norm": 1.3180700540542603,
      "learning_rate": 9.99637587367331e-06,
      "loss": 1.1541,
      "step": 25870
    },
    {
      "epoch": 2.0046476500319526,
      "grad_norm": 1.3325687646865845,
      "learning_rate": 9.988609888687548e-06,
      "loss": 1.2418,
      "step": 25880
    },
    {
      "epoch": 2.0054222583706114,
      "grad_norm": 1.422013521194458,
      "learning_rate": 9.980843903701786e-06,
      "loss": 1.1892,
      "step": 25890
    },
    {
      "epoch": 2.00619686670927,
      "grad_norm": 1.328444242477417,
      "learning_rate": 9.973077918716024e-06,
      "loss": 1.2217,
      "step": 25900
    },
    {
      "epoch": 2.006971475047929,
      "grad_norm": 1.8050302267074585,
      "learning_rate": 9.965311933730262e-06,
      "loss": 1.1594,
      "step": 25910
    },
    {
      "epoch": 2.0077460833865874,
      "grad_norm": 1.3735164403915405,
      "learning_rate": 9.957545948744498e-06,
      "loss": 1.2224,
      "step": 25920
    },
    {
      "epoch": 2.0085206917252463,
      "grad_norm": 1.5475382804870605,
      "learning_rate": 9.949779963758738e-06,
      "loss": 1.2781,
      "step": 25930
    },
    {
      "epoch": 2.009295300063905,
      "grad_norm": 1.6575387716293335,
      "learning_rate": 9.942013978772976e-06,
      "loss": 1.2001,
      "step": 25940
    },
    {
      "epoch": 2.010069908402564,
      "grad_norm": 1.402093768119812,
      "learning_rate": 9.934247993787213e-06,
      "loss": 1.1915,
      "step": 25950
    },
    {
      "epoch": 2.0108445167412228,
      "grad_norm": 1.5822038650512695,
      "learning_rate": 9.92648200880145e-06,
      "loss": 1.1725,
      "step": 25960
    },
    {
      "epoch": 2.0116191250798816,
      "grad_norm": 1.363595724105835,
      "learning_rate": 9.918716023815687e-06,
      "loss": 1.1935,
      "step": 25970
    },
    {
      "epoch": 2.0123937334185404,
      "grad_norm": 2.4490537643432617,
      "learning_rate": 9.910950038829925e-06,
      "loss": 1.1566,
      "step": 25980
    },
    {
      "epoch": 2.013168341757199,
      "grad_norm": 1.4403846263885498,
      "learning_rate": 9.903184053844163e-06,
      "loss": 1.1291,
      "step": 25990
    },
    {
      "epoch": 2.0139429500958577,
      "grad_norm": 1.6304857730865479,
      "learning_rate": 9.8954180688584e-06,
      "loss": 1.2239,
      "step": 26000
    },
    {
      "epoch": 2.0147175584345165,
      "grad_norm": 1.2357182502746582,
      "learning_rate": 9.887652083872637e-06,
      "loss": 1.2872,
      "step": 26010
    },
    {
      "epoch": 2.0154921667731753,
      "grad_norm": 1.7043081521987915,
      "learning_rate": 9.879886098886875e-06,
      "loss": 1.2454,
      "step": 26020
    },
    {
      "epoch": 2.016266775111834,
      "grad_norm": 1.7041031122207642,
      "learning_rate": 9.872120113901115e-06,
      "loss": 1.1373,
      "step": 26030
    },
    {
      "epoch": 2.017041383450493,
      "grad_norm": 1.9083361625671387,
      "learning_rate": 9.86435412891535e-06,
      "loss": 1.1484,
      "step": 26040
    },
    {
      "epoch": 2.017815991789152,
      "grad_norm": 1.6861765384674072,
      "learning_rate": 9.856588143929589e-06,
      "loss": 1.102,
      "step": 26050
    },
    {
      "epoch": 2.01859060012781,
      "grad_norm": 1.9083250761032104,
      "learning_rate": 9.848822158943827e-06,
      "loss": 1.1992,
      "step": 26060
    },
    {
      "epoch": 2.019365208466469,
      "grad_norm": 1.49056875705719,
      "learning_rate": 9.841056173958064e-06,
      "loss": 1.2247,
      "step": 26070
    },
    {
      "epoch": 2.020139816805128,
      "grad_norm": 1.6561447381973267,
      "learning_rate": 9.833290188972302e-06,
      "loss": 1.2192,
      "step": 26080
    },
    {
      "epoch": 2.0209144251437867,
      "grad_norm": 1.4842734336853027,
      "learning_rate": 9.825524203986538e-06,
      "loss": 1.1354,
      "step": 26090
    },
    {
      "epoch": 2.0216890334824456,
      "grad_norm": 1.471594214439392,
      "learning_rate": 9.817758219000776e-06,
      "loss": 1.2211,
      "step": 26100
    },
    {
      "epoch": 2.0224636418211044,
      "grad_norm": 1.3950084447860718,
      "learning_rate": 9.809992234015014e-06,
      "loss": 1.1651,
      "step": 26110
    },
    {
      "epoch": 2.0232382501597628,
      "grad_norm": 1.5154889822006226,
      "learning_rate": 9.802226249029252e-06,
      "loss": 1.1802,
      "step": 26120
    },
    {
      "epoch": 2.0240128584984216,
      "grad_norm": 1.3952218294143677,
      "learning_rate": 9.79446026404349e-06,
      "loss": 1.1385,
      "step": 26130
    },
    {
      "epoch": 2.0247874668370804,
      "grad_norm": 1.3523869514465332,
      "learning_rate": 9.786694279057728e-06,
      "loss": 1.2072,
      "step": 26140
    },
    {
      "epoch": 2.0255620751757393,
      "grad_norm": 2.2029130458831787,
      "learning_rate": 9.778928294071966e-06,
      "loss": 1.1917,
      "step": 26150
    },
    {
      "epoch": 2.026336683514398,
      "grad_norm": 1.6855546236038208,
      "learning_rate": 9.771162309086204e-06,
      "loss": 1.0593,
      "step": 26160
    },
    {
      "epoch": 2.027111291853057,
      "grad_norm": 1.3389252424240112,
      "learning_rate": 9.76339632410044e-06,
      "loss": 1.2459,
      "step": 26170
    },
    {
      "epoch": 2.0278859001917158,
      "grad_norm": 1.5355814695358276,
      "learning_rate": 9.755630339114678e-06,
      "loss": 1.15,
      "step": 26180
    },
    {
      "epoch": 2.028660508530374,
      "grad_norm": 1.413514494895935,
      "learning_rate": 9.747864354128915e-06,
      "loss": 1.1609,
      "step": 26190
    },
    {
      "epoch": 2.029435116869033,
      "grad_norm": 1.2819035053253174,
      "learning_rate": 9.740098369143153e-06,
      "loss": 1.1437,
      "step": 26200
    },
    {
      "epoch": 2.030209725207692,
      "grad_norm": 1.6244202852249146,
      "learning_rate": 9.73233238415739e-06,
      "loss": 1.1768,
      "step": 26210
    },
    {
      "epoch": 2.0309843335463507,
      "grad_norm": 1.482805609703064,
      "learning_rate": 9.724566399171627e-06,
      "loss": 1.2024,
      "step": 26220
    },
    {
      "epoch": 2.0317589418850095,
      "grad_norm": 1.4530965089797974,
      "learning_rate": 9.716800414185867e-06,
      "loss": 1.1846,
      "step": 26230
    },
    {
      "epoch": 2.0325335502236683,
      "grad_norm": 1.6026936769485474,
      "learning_rate": 9.709034429200105e-06,
      "loss": 1.143,
      "step": 26240
    },
    {
      "epoch": 2.0333081585623267,
      "grad_norm": 1.3378846645355225,
      "learning_rate": 9.701268444214343e-06,
      "loss": 1.2313,
      "step": 26250
    },
    {
      "epoch": 2.0340827669009856,
      "grad_norm": 1.932992696762085,
      "learning_rate": 9.693502459228579e-06,
      "loss": 1.1601,
      "step": 26260
    },
    {
      "epoch": 2.0348573752396444,
      "grad_norm": 1.7419630289077759,
      "learning_rate": 9.685736474242817e-06,
      "loss": 1.1511,
      "step": 26270
    },
    {
      "epoch": 2.035631983578303,
      "grad_norm": 1.9954473972320557,
      "learning_rate": 9.677970489257055e-06,
      "loss": 1.1184,
      "step": 26280
    },
    {
      "epoch": 2.036406591916962,
      "grad_norm": 1.4718420505523682,
      "learning_rate": 9.670204504271292e-06,
      "loss": 1.176,
      "step": 26290
    },
    {
      "epoch": 2.037181200255621,
      "grad_norm": 1.4931695461273193,
      "learning_rate": 9.662438519285529e-06,
      "loss": 1.1302,
      "step": 26300
    },
    {
      "epoch": 2.0379558085942797,
      "grad_norm": 2.1639416217803955,
      "learning_rate": 9.654672534299766e-06,
      "loss": 1.2039,
      "step": 26310
    },
    {
      "epoch": 2.038730416932938,
      "grad_norm": 1.768798828125,
      "learning_rate": 9.646906549314006e-06,
      "loss": 1.1664,
      "step": 26320
    },
    {
      "epoch": 2.039505025271597,
      "grad_norm": 1.5063140392303467,
      "learning_rate": 9.639140564328244e-06,
      "loss": 1.1632,
      "step": 26330
    },
    {
      "epoch": 2.0402796336102558,
      "grad_norm": 1.3641917705535889,
      "learning_rate": 9.63137457934248e-06,
      "loss": 1.0892,
      "step": 26340
    },
    {
      "epoch": 2.0410542419489146,
      "grad_norm": 1.6688721179962158,
      "learning_rate": 9.623608594356718e-06,
      "loss": 1.1426,
      "step": 26350
    },
    {
      "epoch": 2.0418288502875734,
      "grad_norm": 1.6369240283966064,
      "learning_rate": 9.615842609370956e-06,
      "loss": 1.187,
      "step": 26360
    },
    {
      "epoch": 2.0426034586262323,
      "grad_norm": 1.6865675449371338,
      "learning_rate": 9.608076624385194e-06,
      "loss": 1.1638,
      "step": 26370
    },
    {
      "epoch": 2.0433780669648907,
      "grad_norm": 1.581007719039917,
      "learning_rate": 9.60031063939943e-06,
      "loss": 1.2036,
      "step": 26380
    },
    {
      "epoch": 2.0441526753035495,
      "grad_norm": 1.6010662317276,
      "learning_rate": 9.592544654413668e-06,
      "loss": 1.2193,
      "step": 26390
    },
    {
      "epoch": 2.0449272836422083,
      "grad_norm": 1.7969435453414917,
      "learning_rate": 9.584778669427906e-06,
      "loss": 1.2389,
      "step": 26400
    },
    {
      "epoch": 2.045701891980867,
      "grad_norm": 1.67229425907135,
      "learning_rate": 9.577012684442143e-06,
      "loss": 1.0977,
      "step": 26410
    },
    {
      "epoch": 2.046476500319526,
      "grad_norm": 1.4190689325332642,
      "learning_rate": 9.569246699456381e-06,
      "loss": 1.1951,
      "step": 26420
    },
    {
      "epoch": 2.047251108658185,
      "grad_norm": 1.6623733043670654,
      "learning_rate": 9.561480714470619e-06,
      "loss": 1.1659,
      "step": 26430
    },
    {
      "epoch": 2.0480257169968437,
      "grad_norm": 1.5561649799346924,
      "learning_rate": 9.553714729484857e-06,
      "loss": 1.1905,
      "step": 26440
    },
    {
      "epoch": 2.048800325335502,
      "grad_norm": 2.187188148498535,
      "learning_rate": 9.545948744499095e-06,
      "loss": 1.1876,
      "step": 26450
    },
    {
      "epoch": 2.049574933674161,
      "grad_norm": 1.8664309978485107,
      "learning_rate": 9.538182759513333e-06,
      "loss": 1.1295,
      "step": 26460
    },
    {
      "epoch": 2.0503495420128197,
      "grad_norm": 1.3379403352737427,
      "learning_rate": 9.530416774527569e-06,
      "loss": 1.1839,
      "step": 26470
    },
    {
      "epoch": 2.0511241503514785,
      "grad_norm": 2.0531442165374756,
      "learning_rate": 9.522650789541807e-06,
      "loss": 1.1846,
      "step": 26480
    },
    {
      "epoch": 2.0518987586901374,
      "grad_norm": 1.6819161176681519,
      "learning_rate": 9.514884804556045e-06,
      "loss": 1.1336,
      "step": 26490
    },
    {
      "epoch": 2.052673367028796,
      "grad_norm": 1.5665446519851685,
      "learning_rate": 9.507118819570282e-06,
      "loss": 1.0575,
      "step": 26500
    },
    {
      "epoch": 2.053447975367455,
      "grad_norm": 1.6206573247909546,
      "learning_rate": 9.499352834584519e-06,
      "loss": 1.1458,
      "step": 26510
    },
    {
      "epoch": 2.0542225837061134,
      "grad_norm": 1.8264751434326172,
      "learning_rate": 9.491586849598758e-06,
      "loss": 1.1624,
      "step": 26520
    },
    {
      "epoch": 2.0549971920447723,
      "grad_norm": 1.436371922492981,
      "learning_rate": 9.483820864612996e-06,
      "loss": 1.2202,
      "step": 26530
    },
    {
      "epoch": 2.055771800383431,
      "grad_norm": 1.8197214603424072,
      "learning_rate": 9.476054879627234e-06,
      "loss": 1.1645,
      "step": 26540
    },
    {
      "epoch": 2.05654640872209,
      "grad_norm": 1.6223487854003906,
      "learning_rate": 9.46828889464147e-06,
      "loss": 1.0416,
      "step": 26550
    },
    {
      "epoch": 2.0573210170607488,
      "grad_norm": 1.5433316230773926,
      "learning_rate": 9.460522909655708e-06,
      "loss": 1.2081,
      "step": 26560
    },
    {
      "epoch": 2.0580956253994076,
      "grad_norm": 1.463701844215393,
      "learning_rate": 9.452756924669946e-06,
      "loss": 1.2488,
      "step": 26570
    },
    {
      "epoch": 2.058870233738066,
      "grad_norm": 1.72011399269104,
      "learning_rate": 9.444990939684184e-06,
      "loss": 1.2917,
      "step": 26580
    },
    {
      "epoch": 2.059644842076725,
      "grad_norm": 1.6452696323394775,
      "learning_rate": 9.43722495469842e-06,
      "loss": 1.1662,
      "step": 26590
    },
    {
      "epoch": 2.0604194504153837,
      "grad_norm": 1.2917027473449707,
      "learning_rate": 9.429458969712658e-06,
      "loss": 1.1544,
      "step": 26600
    },
    {
      "epoch": 2.0611940587540425,
      "grad_norm": 1.2686961889266968,
      "learning_rate": 9.421692984726896e-06,
      "loss": 1.2555,
      "step": 26610
    },
    {
      "epoch": 2.0619686670927013,
      "grad_norm": 1.8601300716400146,
      "learning_rate": 9.413926999741135e-06,
      "loss": 1.236,
      "step": 26620
    },
    {
      "epoch": 2.06274327543136,
      "grad_norm": 1.5949866771697998,
      "learning_rate": 9.406161014755373e-06,
      "loss": 1.2444,
      "step": 26630
    },
    {
      "epoch": 2.063517883770019,
      "grad_norm": 1.7078896760940552,
      "learning_rate": 9.39839502976961e-06,
      "loss": 1.1563,
      "step": 26640
    },
    {
      "epoch": 2.0642924921086774,
      "grad_norm": 1.644761562347412,
      "learning_rate": 9.390629044783847e-06,
      "loss": 1.2527,
      "step": 26650
    },
    {
      "epoch": 2.065067100447336,
      "grad_norm": 1.5433785915374756,
      "learning_rate": 9.382863059798085e-06,
      "loss": 1.137,
      "step": 26660
    },
    {
      "epoch": 2.065841708785995,
      "grad_norm": 1.7655982971191406,
      "learning_rate": 9.375097074812323e-06,
      "loss": 1.1684,
      "step": 26670
    },
    {
      "epoch": 2.066616317124654,
      "grad_norm": 2.4742789268493652,
      "learning_rate": 9.367331089826559e-06,
      "loss": 1.1926,
      "step": 26680
    },
    {
      "epoch": 2.0673909254633127,
      "grad_norm": 1.6327027082443237,
      "learning_rate": 9.359565104840797e-06,
      "loss": 1.2352,
      "step": 26690
    },
    {
      "epoch": 2.0681655338019715,
      "grad_norm": 1.9490281343460083,
      "learning_rate": 9.351799119855035e-06,
      "loss": 1.2241,
      "step": 26700
    },
    {
      "epoch": 2.06894014214063,
      "grad_norm": 1.9255799055099487,
      "learning_rate": 9.344033134869273e-06,
      "loss": 1.1371,
      "step": 26710
    },
    {
      "epoch": 2.0697147504792888,
      "grad_norm": 1.5312377214431763,
      "learning_rate": 9.33626714988351e-06,
      "loss": 1.2013,
      "step": 26720
    },
    {
      "epoch": 2.0704893588179476,
      "grad_norm": 1.3441596031188965,
      "learning_rate": 9.328501164897748e-06,
      "loss": 1.1474,
      "step": 26730
    },
    {
      "epoch": 2.0712639671566064,
      "grad_norm": 1.2575883865356445,
      "learning_rate": 9.320735179911986e-06,
      "loss": 1.164,
      "step": 26740
    },
    {
      "epoch": 2.0720385754952653,
      "grad_norm": 1.6766633987426758,
      "learning_rate": 9.312969194926224e-06,
      "loss": 1.1818,
      "step": 26750
    },
    {
      "epoch": 2.072813183833924,
      "grad_norm": 1.8258157968521118,
      "learning_rate": 9.30520320994046e-06,
      "loss": 1.2521,
      "step": 26760
    },
    {
      "epoch": 2.073587792172583,
      "grad_norm": 1.724622368812561,
      "learning_rate": 9.297437224954698e-06,
      "loss": 1.1365,
      "step": 26770
    },
    {
      "epoch": 2.0743624005112413,
      "grad_norm": 1.6064280271530151,
      "learning_rate": 9.289671239968936e-06,
      "loss": 1.0503,
      "step": 26780
    },
    {
      "epoch": 2.0751370088499,
      "grad_norm": 1.5576202869415283,
      "learning_rate": 9.281905254983174e-06,
      "loss": 1.1306,
      "step": 26790
    },
    {
      "epoch": 2.075911617188559,
      "grad_norm": 1.251688838005066,
      "learning_rate": 9.274139269997412e-06,
      "loss": 1.1196,
      "step": 26800
    },
    {
      "epoch": 2.076686225527218,
      "grad_norm": 1.350820541381836,
      "learning_rate": 9.26637328501165e-06,
      "loss": 1.2738,
      "step": 26810
    },
    {
      "epoch": 2.0774608338658767,
      "grad_norm": 1.7064746618270874,
      "learning_rate": 9.258607300025887e-06,
      "loss": 1.2076,
      "step": 26820
    },
    {
      "epoch": 2.0782354422045355,
      "grad_norm": 1.5616203546524048,
      "learning_rate": 9.250841315040125e-06,
      "loss": 1.1643,
      "step": 26830
    },
    {
      "epoch": 2.079010050543194,
      "grad_norm": 1.3794994354248047,
      "learning_rate": 9.243075330054363e-06,
      "loss": 1.1648,
      "step": 26840
    },
    {
      "epoch": 2.0797846588818527,
      "grad_norm": 1.8007607460021973,
      "learning_rate": 9.2353093450686e-06,
      "loss": 1.119,
      "step": 26850
    },
    {
      "epoch": 2.0805592672205115,
      "grad_norm": 1.3886702060699463,
      "learning_rate": 9.227543360082837e-06,
      "loss": 1.2311,
      "step": 26860
    },
    {
      "epoch": 2.0813338755591704,
      "grad_norm": 1.2862944602966309,
      "learning_rate": 9.219777375097075e-06,
      "loss": 1.3045,
      "step": 26870
    },
    {
      "epoch": 2.082108483897829,
      "grad_norm": 1.5571935176849365,
      "learning_rate": 9.212011390111313e-06,
      "loss": 1.1927,
      "step": 26880
    },
    {
      "epoch": 2.082883092236488,
      "grad_norm": 1.493249773979187,
      "learning_rate": 9.204245405125549e-06,
      "loss": 1.1593,
      "step": 26890
    },
    {
      "epoch": 2.083657700575147,
      "grad_norm": 1.537501335144043,
      "learning_rate": 9.197256018638364e-06,
      "loss": 1.1071,
      "step": 26900
    },
    {
      "epoch": 2.0844323089138053,
      "grad_norm": 1.1517784595489502,
      "learning_rate": 9.189490033652602e-06,
      "loss": 1.0811,
      "step": 26910
    },
    {
      "epoch": 2.085206917252464,
      "grad_norm": 2.06911563873291,
      "learning_rate": 9.18172404866684e-06,
      "loss": 1.208,
      "step": 26920
    },
    {
      "epoch": 2.085981525591123,
      "grad_norm": 1.9411511421203613,
      "learning_rate": 9.173958063681076e-06,
      "loss": 1.1741,
      "step": 26930
    },
    {
      "epoch": 2.0867561339297818,
      "grad_norm": 1.5545746088027954,
      "learning_rate": 9.166192078695314e-06,
      "loss": 1.1559,
      "step": 26940
    },
    {
      "epoch": 2.0875307422684406,
      "grad_norm": 1.3606265783309937,
      "learning_rate": 9.158426093709554e-06,
      "loss": 1.2206,
      "step": 26950
    },
    {
      "epoch": 2.0883053506070994,
      "grad_norm": 1.5383261442184448,
      "learning_rate": 9.150660108723791e-06,
      "loss": 1.0761,
      "step": 26960
    },
    {
      "epoch": 2.0890799589457583,
      "grad_norm": 1.5183048248291016,
      "learning_rate": 9.142894123738028e-06,
      "loss": 1.0997,
      "step": 26970
    },
    {
      "epoch": 2.0898545672844167,
      "grad_norm": 1.8398741483688354,
      "learning_rate": 9.135128138752265e-06,
      "loss": 1.2067,
      "step": 26980
    },
    {
      "epoch": 2.0906291756230755,
      "grad_norm": 1.2238062620162964,
      "learning_rate": 9.127362153766503e-06,
      "loss": 1.1893,
      "step": 26990
    },
    {
      "epoch": 2.0914037839617343,
      "grad_norm": 2.228708028793335,
      "learning_rate": 9.119596168780741e-06,
      "loss": 1.2006,
      "step": 27000
    },
    {
      "epoch": 2.092178392300393,
      "grad_norm": 1.8517812490463257,
      "learning_rate": 9.111830183794977e-06,
      "loss": 1.1357,
      "step": 27010
    },
    {
      "epoch": 2.092953000639052,
      "grad_norm": 2.043109655380249,
      "learning_rate": 9.104064198809215e-06,
      "loss": 1.2034,
      "step": 27020
    },
    {
      "epoch": 2.093727608977711,
      "grad_norm": 1.8169968128204346,
      "learning_rate": 9.096298213823453e-06,
      "loss": 1.1734,
      "step": 27030
    },
    {
      "epoch": 2.094502217316369,
      "grad_norm": 1.485245943069458,
      "learning_rate": 9.088532228837691e-06,
      "loss": 1.1593,
      "step": 27040
    },
    {
      "epoch": 2.095276825655028,
      "grad_norm": 1.418209195137024,
      "learning_rate": 9.080766243851929e-06,
      "loss": 1.1954,
      "step": 27050
    },
    {
      "epoch": 2.096051433993687,
      "grad_norm": 1.625612735748291,
      "learning_rate": 9.073000258866167e-06,
      "loss": 1.0616,
      "step": 27060
    },
    {
      "epoch": 2.0968260423323457,
      "grad_norm": 1.2502962350845337,
      "learning_rate": 9.065234273880405e-06,
      "loss": 1.2421,
      "step": 27070
    },
    {
      "epoch": 2.0976006506710045,
      "grad_norm": 1.5278651714324951,
      "learning_rate": 9.057468288894642e-06,
      "loss": 1.2308,
      "step": 27080
    },
    {
      "epoch": 2.0983752590096634,
      "grad_norm": 1.452918529510498,
      "learning_rate": 9.04970230390888e-06,
      "loss": 1.2232,
      "step": 27090
    },
    {
      "epoch": 2.099149867348322,
      "grad_norm": 1.4429234266281128,
      "learning_rate": 9.041936318923116e-06,
      "loss": 1.2054,
      "step": 27100
    },
    {
      "epoch": 2.0999244756869806,
      "grad_norm": 1.6067794561386108,
      "learning_rate": 9.034170333937354e-06,
      "loss": 1.0986,
      "step": 27110
    },
    {
      "epoch": 2.1006990840256394,
      "grad_norm": 1.5042202472686768,
      "learning_rate": 9.026404348951592e-06,
      "loss": 1.1387,
      "step": 27120
    },
    {
      "epoch": 2.1014736923642983,
      "grad_norm": 1.1274628639221191,
      "learning_rate": 9.01863836396583e-06,
      "loss": 1.2328,
      "step": 27130
    },
    {
      "epoch": 2.102248300702957,
      "grad_norm": 1.7709765434265137,
      "learning_rate": 9.010872378980066e-06,
      "loss": 1.1189,
      "step": 27140
    },
    {
      "epoch": 2.103022909041616,
      "grad_norm": 1.7916131019592285,
      "learning_rate": 9.003106393994306e-06,
      "loss": 1.1163,
      "step": 27150
    },
    {
      "epoch": 2.1037975173802748,
      "grad_norm": 1.5068541765213013,
      "learning_rate": 8.995340409008544e-06,
      "loss": 1.2204,
      "step": 27160
    },
    {
      "epoch": 2.104572125718933,
      "grad_norm": 1.7260178327560425,
      "learning_rate": 8.987574424022781e-06,
      "loss": 1.2392,
      "step": 27170
    },
    {
      "epoch": 2.105346734057592,
      "grad_norm": 1.447714924812317,
      "learning_rate": 8.979808439037018e-06,
      "loss": 1.1659,
      "step": 27180
    },
    {
      "epoch": 2.106121342396251,
      "grad_norm": 1.6897494792938232,
      "learning_rate": 8.972042454051256e-06,
      "loss": 1.2164,
      "step": 27190
    },
    {
      "epoch": 2.1068959507349097,
      "grad_norm": 1.422363519668579,
      "learning_rate": 8.964276469065493e-06,
      "loss": 1.1518,
      "step": 27200
    },
    {
      "epoch": 2.1076705590735685,
      "grad_norm": 1.3803232908248901,
      "learning_rate": 8.956510484079731e-06,
      "loss": 1.1406,
      "step": 27210
    },
    {
      "epoch": 2.1084451674122273,
      "grad_norm": 2.050487756729126,
      "learning_rate": 8.948744499093967e-06,
      "loss": 1.0867,
      "step": 27220
    },
    {
      "epoch": 2.109219775750886,
      "grad_norm": 2.1653945446014404,
      "learning_rate": 8.940978514108205e-06,
      "loss": 1.2183,
      "step": 27230
    },
    {
      "epoch": 2.1099943840895445,
      "grad_norm": 1.631219744682312,
      "learning_rate": 8.933212529122443e-06,
      "loss": 1.1293,
      "step": 27240
    },
    {
      "epoch": 2.1107689924282034,
      "grad_norm": 1.62587308883667,
      "learning_rate": 8.925446544136683e-06,
      "loss": 1.2725,
      "step": 27250
    },
    {
      "epoch": 2.111543600766862,
      "grad_norm": 1.5405179262161255,
      "learning_rate": 8.917680559150919e-06,
      "loss": 1.1569,
      "step": 27260
    },
    {
      "epoch": 2.112318209105521,
      "grad_norm": 1.609445333480835,
      "learning_rate": 8.909914574165157e-06,
      "loss": 1.1171,
      "step": 27270
    },
    {
      "epoch": 2.11309281744418,
      "grad_norm": 1.6551975011825562,
      "learning_rate": 8.902148589179395e-06,
      "loss": 1.1224,
      "step": 27280
    },
    {
      "epoch": 2.1138674257828387,
      "grad_norm": 1.4101275205612183,
      "learning_rate": 8.894382604193632e-06,
      "loss": 1.2765,
      "step": 27290
    },
    {
      "epoch": 2.1146420341214975,
      "grad_norm": 1.831028699874878,
      "learning_rate": 8.88661661920787e-06,
      "loss": 1.2388,
      "step": 27300
    },
    {
      "epoch": 2.115416642460156,
      "grad_norm": 1.6643145084381104,
      "learning_rate": 8.878850634222107e-06,
      "loss": 1.1877,
      "step": 27310
    },
    {
      "epoch": 2.1161912507988148,
      "grad_norm": 1.577311635017395,
      "learning_rate": 8.871084649236344e-06,
      "loss": 1.1014,
      "step": 27320
    },
    {
      "epoch": 2.1169658591374736,
      "grad_norm": 2.011597156524658,
      "learning_rate": 8.863318664250582e-06,
      "loss": 1.2201,
      "step": 27330
    },
    {
      "epoch": 2.1177404674761324,
      "grad_norm": 1.8358336687088013,
      "learning_rate": 8.85555267926482e-06,
      "loss": 1.2189,
      "step": 27340
    },
    {
      "epoch": 2.1185150758147913,
      "grad_norm": 1.630154013633728,
      "learning_rate": 8.847786694279058e-06,
      "loss": 1.0937,
      "step": 27350
    },
    {
      "epoch": 2.11928968415345,
      "grad_norm": 1.9040255546569824,
      "learning_rate": 8.840020709293296e-06,
      "loss": 1.2807,
      "step": 27360
    },
    {
      "epoch": 2.1200642924921085,
      "grad_norm": 1.5014158487319946,
      "learning_rate": 8.832254724307534e-06,
      "loss": 1.1936,
      "step": 27370
    },
    {
      "epoch": 2.1208389008307673,
      "grad_norm": 1.7735145092010498,
      "learning_rate": 8.824488739321772e-06,
      "loss": 1.1813,
      "step": 27380
    },
    {
      "epoch": 2.121613509169426,
      "grad_norm": 1.646583080291748,
      "learning_rate": 8.816722754336008e-06,
      "loss": 1.1352,
      "step": 27390
    },
    {
      "epoch": 2.122388117508085,
      "grad_norm": 1.4928431510925293,
      "learning_rate": 8.808956769350246e-06,
      "loss": 1.131,
      "step": 27400
    },
    {
      "epoch": 2.123162725846744,
      "grad_norm": 1.3409106731414795,
      "learning_rate": 8.801190784364484e-06,
      "loss": 1.2212,
      "step": 27410
    },
    {
      "epoch": 2.1239373341854026,
      "grad_norm": 1.517791509628296,
      "learning_rate": 8.793424799378721e-06,
      "loss": 1.0663,
      "step": 27420
    },
    {
      "epoch": 2.1247119425240615,
      "grad_norm": 1.8258533477783203,
      "learning_rate": 8.785658814392958e-06,
      "loss": 1.1041,
      "step": 27430
    },
    {
      "epoch": 2.12548655086272,
      "grad_norm": 1.519376516342163,
      "learning_rate": 8.777892829407197e-06,
      "loss": 1.1884,
      "step": 27440
    },
    {
      "epoch": 2.1262611592013787,
      "grad_norm": 1.4063395261764526,
      "learning_rate": 8.770126844421435e-06,
      "loss": 1.2434,
      "step": 27450
    },
    {
      "epoch": 2.1270357675400375,
      "grad_norm": 1.531197428703308,
      "learning_rate": 8.762360859435673e-06,
      "loss": 1.1942,
      "step": 27460
    },
    {
      "epoch": 2.1278103758786964,
      "grad_norm": 1.9065525531768799,
      "learning_rate": 8.75459487444991e-06,
      "loss": 1.1096,
      "step": 27470
    },
    {
      "epoch": 2.128584984217355,
      "grad_norm": 2.161884069442749,
      "learning_rate": 8.746828889464147e-06,
      "loss": 1.0762,
      "step": 27480
    },
    {
      "epoch": 2.129359592556014,
      "grad_norm": 1.708757996559143,
      "learning_rate": 8.739062904478385e-06,
      "loss": 1.1977,
      "step": 27490
    },
    {
      "epoch": 2.1301342008946724,
      "grad_norm": 2.0765857696533203,
      "learning_rate": 8.731296919492623e-06,
      "loss": 1.2665,
      "step": 27500
    },
    {
      "epoch": 2.1309088092333313,
      "grad_norm": 2.273487091064453,
      "learning_rate": 8.72353093450686e-06,
      "loss": 1.1431,
      "step": 27510
    },
    {
      "epoch": 2.13168341757199,
      "grad_norm": 1.4989601373672485,
      "learning_rate": 8.715764949521097e-06,
      "loss": 1.1981,
      "step": 27520
    },
    {
      "epoch": 2.132458025910649,
      "grad_norm": 1.4342900514602661,
      "learning_rate": 8.707998964535335e-06,
      "loss": 1.2627,
      "step": 27530
    },
    {
      "epoch": 2.1332326342493078,
      "grad_norm": 1.8592034578323364,
      "learning_rate": 8.700232979549574e-06,
      "loss": 1.2388,
      "step": 27540
    },
    {
      "epoch": 2.1340072425879666,
      "grad_norm": 2.0141806602478027,
      "learning_rate": 8.692466994563812e-06,
      "loss": 1.1587,
      "step": 27550
    },
    {
      "epoch": 2.1347818509266254,
      "grad_norm": 1.6192084550857544,
      "learning_rate": 8.684701009578048e-06,
      "loss": 1.1773,
      "step": 27560
    },
    {
      "epoch": 2.135556459265284,
      "grad_norm": 1.8097128868103027,
      "learning_rate": 8.676935024592286e-06,
      "loss": 1.2534,
      "step": 27570
    },
    {
      "epoch": 2.1363310676039426,
      "grad_norm": 1.5701265335083008,
      "learning_rate": 8.669169039606524e-06,
      "loss": 1.2492,
      "step": 27580
    },
    {
      "epoch": 2.1371056759426015,
      "grad_norm": 1.3976563215255737,
      "learning_rate": 8.661403054620762e-06,
      "loss": 1.1671,
      "step": 27590
    },
    {
      "epoch": 2.1378802842812603,
      "grad_norm": 1.1837420463562012,
      "learning_rate": 8.653637069634998e-06,
      "loss": 1.2159,
      "step": 27600
    },
    {
      "epoch": 2.138654892619919,
      "grad_norm": 2.2649662494659424,
      "learning_rate": 8.645871084649236e-06,
      "loss": 1.2283,
      "step": 27610
    },
    {
      "epoch": 2.139429500958578,
      "grad_norm": 1.901807188987732,
      "learning_rate": 8.638105099663474e-06,
      "loss": 1.0696,
      "step": 27620
    },
    {
      "epoch": 2.140204109297237,
      "grad_norm": 1.4504656791687012,
      "learning_rate": 8.630339114677711e-06,
      "loss": 1.2062,
      "step": 27630
    },
    {
      "epoch": 2.140978717635895,
      "grad_norm": 1.506351113319397,
      "learning_rate": 8.622573129691951e-06,
      "loss": 1.2139,
      "step": 27640
    },
    {
      "epoch": 2.141753325974554,
      "grad_norm": 2.8299248218536377,
      "learning_rate": 8.614807144706187e-06,
      "loss": 1.208,
      "step": 27650
    },
    {
      "epoch": 2.142527934313213,
      "grad_norm": 1.5985430479049683,
      "learning_rate": 8.607041159720425e-06,
      "loss": 1.1594,
      "step": 27660
    },
    {
      "epoch": 2.1433025426518717,
      "grad_norm": 1.602292537689209,
      "learning_rate": 8.599275174734663e-06,
      "loss": 1.1266,
      "step": 27670
    },
    {
      "epoch": 2.1440771509905305,
      "grad_norm": 1.5444637537002563,
      "learning_rate": 8.5915091897489e-06,
      "loss": 1.1362,
      "step": 27680
    },
    {
      "epoch": 2.1448517593291894,
      "grad_norm": 1.8870902061462402,
      "learning_rate": 8.583743204763137e-06,
      "loss": 1.1614,
      "step": 27690
    },
    {
      "epoch": 2.1456263676678478,
      "grad_norm": 1.5267579555511475,
      "learning_rate": 8.575977219777375e-06,
      "loss": 1.1645,
      "step": 27700
    },
    {
      "epoch": 2.1464009760065066,
      "grad_norm": 1.760866403579712,
      "learning_rate": 8.568211234791613e-06,
      "loss": 1.2588,
      "step": 27710
    },
    {
      "epoch": 2.1471755843451654,
      "grad_norm": 1.8170039653778076,
      "learning_rate": 8.56044524980585e-06,
      "loss": 1.1932,
      "step": 27720
    },
    {
      "epoch": 2.1479501926838243,
      "grad_norm": 1.5195834636688232,
      "learning_rate": 8.552679264820087e-06,
      "loss": 1.2483,
      "step": 27730
    },
    {
      "epoch": 2.148724801022483,
      "grad_norm": 2.122729539871216,
      "learning_rate": 8.544913279834326e-06,
      "loss": 1.1464,
      "step": 27740
    },
    {
      "epoch": 2.149499409361142,
      "grad_norm": 1.5763047933578491,
      "learning_rate": 8.537147294848564e-06,
      "loss": 1.214,
      "step": 27750
    },
    {
      "epoch": 2.1502740176998003,
      "grad_norm": 1.6941946744918823,
      "learning_rate": 8.529381309862802e-06,
      "loss": 1.1027,
      "step": 27760
    },
    {
      "epoch": 2.151048626038459,
      "grad_norm": 1.4555081129074097,
      "learning_rate": 8.521615324877038e-06,
      "loss": 1.1454,
      "step": 27770
    },
    {
      "epoch": 2.151823234377118,
      "grad_norm": 1.5337543487548828,
      "learning_rate": 8.513849339891276e-06,
      "loss": 1.098,
      "step": 27780
    },
    {
      "epoch": 2.152597842715777,
      "grad_norm": 1.4667627811431885,
      "learning_rate": 8.506083354905514e-06,
      "loss": 1.1482,
      "step": 27790
    },
    {
      "epoch": 2.1533724510544356,
      "grad_norm": 1.8899296522140503,
      "learning_rate": 8.498317369919752e-06,
      "loss": 1.2116,
      "step": 27800
    },
    {
      "epoch": 2.1541470593930945,
      "grad_norm": 1.415490746498108,
      "learning_rate": 8.49055138493399e-06,
      "loss": 1.169,
      "step": 27810
    },
    {
      "epoch": 2.1549216677317533,
      "grad_norm": 1.1001728773117065,
      "learning_rate": 8.482785399948226e-06,
      "loss": 1.1282,
      "step": 27820
    },
    {
      "epoch": 2.1556962760704117,
      "grad_norm": 1.7176693677902222,
      "learning_rate": 8.475019414962464e-06,
      "loss": 1.1275,
      "step": 27830
    },
    {
      "epoch": 2.1564708844090705,
      "grad_norm": 1.9816733598709106,
      "learning_rate": 8.467253429976703e-06,
      "loss": 1.0759,
      "step": 27840
    },
    {
      "epoch": 2.1572454927477294,
      "grad_norm": 1.3835428953170776,
      "learning_rate": 8.459487444990941e-06,
      "loss": 1.0945,
      "step": 27850
    },
    {
      "epoch": 2.158020101086388,
      "grad_norm": 1.5953398942947388,
      "learning_rate": 8.451721460005177e-06,
      "loss": 1.1727,
      "step": 27860
    },
    {
      "epoch": 2.158794709425047,
      "grad_norm": 1.6192340850830078,
      "learning_rate": 8.443955475019415e-06,
      "loss": 1.2376,
      "step": 27870
    },
    {
      "epoch": 2.159569317763706,
      "grad_norm": 1.9681470394134521,
      "learning_rate": 8.436189490033653e-06,
      "loss": 1.29,
      "step": 27880
    },
    {
      "epoch": 2.1603439261023647,
      "grad_norm": 1.8496829271316528,
      "learning_rate": 8.428423505047891e-06,
      "loss": 1.2916,
      "step": 27890
    },
    {
      "epoch": 2.161118534441023,
      "grad_norm": 1.8199152946472168,
      "learning_rate": 8.420657520062127e-06,
      "loss": 1.1572,
      "step": 27900
    },
    {
      "epoch": 2.161893142779682,
      "grad_norm": 1.412449836730957,
      "learning_rate": 8.412891535076365e-06,
      "loss": 1.1296,
      "step": 27910
    },
    {
      "epoch": 2.1626677511183408,
      "grad_norm": 2.0701828002929688,
      "learning_rate": 8.405125550090603e-06,
      "loss": 1.1823,
      "step": 27920
    },
    {
      "epoch": 2.1634423594569996,
      "grad_norm": 1.4357377290725708,
      "learning_rate": 8.397359565104842e-06,
      "loss": 1.2189,
      "step": 27930
    },
    {
      "epoch": 2.1642169677956584,
      "grad_norm": 1.5784364938735962,
      "learning_rate": 8.389593580119079e-06,
      "loss": 1.1427,
      "step": 27940
    },
    {
      "epoch": 2.1649915761343173,
      "grad_norm": 1.5904110670089722,
      "learning_rate": 8.381827595133316e-06,
      "loss": 1.1848,
      "step": 27950
    },
    {
      "epoch": 2.165766184472976,
      "grad_norm": 1.1480971574783325,
      "learning_rate": 8.374061610147554e-06,
      "loss": 1.1931,
      "step": 27960
    },
    {
      "epoch": 2.1665407928116345,
      "grad_norm": 1.7509702444076538,
      "learning_rate": 8.366295625161792e-06,
      "loss": 1.1341,
      "step": 27970
    },
    {
      "epoch": 2.1673154011502933,
      "grad_norm": 1.599123239517212,
      "learning_rate": 8.358529640176028e-06,
      "loss": 1.1397,
      "step": 27980
    },
    {
      "epoch": 2.168090009488952,
      "grad_norm": 1.5575212240219116,
      "learning_rate": 8.350763655190266e-06,
      "loss": 1.153,
      "step": 27990
    },
    {
      "epoch": 2.168864617827611,
      "grad_norm": 1.146255612373352,
      "learning_rate": 8.342997670204504e-06,
      "loss": 1.3112,
      "step": 28000
    },
    {
      "epoch": 2.16963922616627,
      "grad_norm": 1.4505075216293335,
      "learning_rate": 8.335231685218742e-06,
      "loss": 1.2605,
      "step": 28010
    },
    {
      "epoch": 2.1704138345049286,
      "grad_norm": 1.3093624114990234,
      "learning_rate": 8.32746570023298e-06,
      "loss": 1.1695,
      "step": 28020
    },
    {
      "epoch": 2.171188442843587,
      "grad_norm": 1.498502254486084,
      "learning_rate": 8.319699715247218e-06,
      "loss": 1.1318,
      "step": 28030
    },
    {
      "epoch": 2.171963051182246,
      "grad_norm": 1.7980718612670898,
      "learning_rate": 8.311933730261456e-06,
      "loss": 1.1499,
      "step": 28040
    },
    {
      "epoch": 2.1727376595209047,
      "grad_norm": 1.9320415258407593,
      "learning_rate": 8.304167745275693e-06,
      "loss": 1.1156,
      "step": 28050
    },
    {
      "epoch": 2.1735122678595635,
      "grad_norm": 1.7986335754394531,
      "learning_rate": 8.296401760289931e-06,
      "loss": 1.1797,
      "step": 28060
    },
    {
      "epoch": 2.1742868761982224,
      "grad_norm": 1.454269289970398,
      "learning_rate": 8.288635775304167e-06,
      "loss": 1.1773,
      "step": 28070
    },
    {
      "epoch": 2.175061484536881,
      "grad_norm": 1.5463148355484009,
      "learning_rate": 8.280869790318405e-06,
      "loss": 1.0978,
      "step": 28080
    },
    {
      "epoch": 2.1758360928755396,
      "grad_norm": 1.3639765977859497,
      "learning_rate": 8.273103805332643e-06,
      "loss": 1.191,
      "step": 28090
    },
    {
      "epoch": 2.1766107012141984,
      "grad_norm": 1.7209874391555786,
      "learning_rate": 8.265337820346881e-06,
      "loss": 1.1818,
      "step": 28100
    },
    {
      "epoch": 2.1773853095528573,
      "grad_norm": 1.5316146612167358,
      "learning_rate": 8.257571835361117e-06,
      "loss": 1.2658,
      "step": 28110
    },
    {
      "epoch": 2.178159917891516,
      "grad_norm": 1.5575013160705566,
      "learning_rate": 8.249805850375355e-06,
      "loss": 1.1414,
      "step": 28120
    },
    {
      "epoch": 2.178934526230175,
      "grad_norm": 1.68779456615448,
      "learning_rate": 8.242039865389595e-06,
      "loss": 1.1654,
      "step": 28130
    },
    {
      "epoch": 2.1797091345688338,
      "grad_norm": 1.3263059854507446,
      "learning_rate": 8.234273880403832e-06,
      "loss": 1.1811,
      "step": 28140
    },
    {
      "epoch": 2.1804837429074926,
      "grad_norm": 1.3879209756851196,
      "learning_rate": 8.226507895418069e-06,
      "loss": 1.145,
      "step": 28150
    },
    {
      "epoch": 2.181258351246151,
      "grad_norm": 1.9980850219726562,
      "learning_rate": 8.218741910432307e-06,
      "loss": 1.0799,
      "step": 28160
    },
    {
      "epoch": 2.18203295958481,
      "grad_norm": 1.3402372598648071,
      "learning_rate": 8.210975925446544e-06,
      "loss": 1.1654,
      "step": 28170
    },
    {
      "epoch": 2.1828075679234686,
      "grad_norm": 1.8268094062805176,
      "learning_rate": 8.203209940460782e-06,
      "loss": 1.1573,
      "step": 28180
    },
    {
      "epoch": 2.1835821762621275,
      "grad_norm": 1.7290226221084595,
      "learning_rate": 8.19544395547502e-06,
      "loss": 1.1945,
      "step": 28190
    },
    {
      "epoch": 2.1843567846007863,
      "grad_norm": 1.5665580034255981,
      "learning_rate": 8.187677970489256e-06,
      "loss": 1.1826,
      "step": 28200
    },
    {
      "epoch": 2.185131392939445,
      "grad_norm": 1.4472116231918335,
      "learning_rate": 8.179911985503494e-06,
      "loss": 1.1666,
      "step": 28210
    },
    {
      "epoch": 2.185906001278104,
      "grad_norm": 1.7096593379974365,
      "learning_rate": 8.172146000517732e-06,
      "loss": 1.1497,
      "step": 28220
    },
    {
      "epoch": 2.1866806096167624,
      "grad_norm": 1.556465983390808,
      "learning_rate": 8.164380015531972e-06,
      "loss": 1.2268,
      "step": 28230
    },
    {
      "epoch": 2.187455217955421,
      "grad_norm": 1.8898680210113525,
      "learning_rate": 8.156614030546208e-06,
      "loss": 1.2128,
      "step": 28240
    },
    {
      "epoch": 2.18822982629408,
      "grad_norm": 1.4004228115081787,
      "learning_rate": 8.148848045560446e-06,
      "loss": 1.2668,
      "step": 28250
    },
    {
      "epoch": 2.189004434632739,
      "grad_norm": 1.5003536939620972,
      "learning_rate": 8.141082060574683e-06,
      "loss": 1.2169,
      "step": 28260
    },
    {
      "epoch": 2.1897790429713977,
      "grad_norm": 1.668017029762268,
      "learning_rate": 8.133316075588921e-06,
      "loss": 1.1523,
      "step": 28270
    },
    {
      "epoch": 2.1905536513100565,
      "grad_norm": 1.7686048746109009,
      "learning_rate": 8.125550090603158e-06,
      "loss": 1.2569,
      "step": 28280
    },
    {
      "epoch": 2.191328259648715,
      "grad_norm": 2.286693811416626,
      "learning_rate": 8.117784105617395e-06,
      "loss": 1.2353,
      "step": 28290
    },
    {
      "epoch": 2.1921028679873737,
      "grad_norm": 1.4356677532196045,
      "learning_rate": 8.110018120631633e-06,
      "loss": 1.2851,
      "step": 28300
    },
    {
      "epoch": 2.1928774763260326,
      "grad_norm": 2.1086692810058594,
      "learning_rate": 8.102252135645871e-06,
      "loss": 1.1438,
      "step": 28310
    },
    {
      "epoch": 2.1936520846646914,
      "grad_norm": 1.5122061967849731,
      "learning_rate": 8.094486150660107e-06,
      "loss": 1.2059,
      "step": 28320
    },
    {
      "epoch": 2.1944266930033502,
      "grad_norm": 1.3527594804763794,
      "learning_rate": 8.086720165674347e-06,
      "loss": 1.1127,
      "step": 28330
    },
    {
      "epoch": 2.195201301342009,
      "grad_norm": 1.852728247642517,
      "learning_rate": 8.078954180688585e-06,
      "loss": 1.2226,
      "step": 28340
    },
    {
      "epoch": 2.195975909680668,
      "grad_norm": 1.5858525037765503,
      "learning_rate": 8.071188195702823e-06,
      "loss": 1.1488,
      "step": 28350
    },
    {
      "epoch": 2.1967505180193263,
      "grad_norm": 1.6130082607269287,
      "learning_rate": 8.06342221071706e-06,
      "loss": 1.2317,
      "step": 28360
    },
    {
      "epoch": 2.197525126357985,
      "grad_norm": 1.6865733861923218,
      "learning_rate": 8.055656225731297e-06,
      "loss": 1.2083,
      "step": 28370
    },
    {
      "epoch": 2.198299734696644,
      "grad_norm": 1.5646405220031738,
      "learning_rate": 8.047890240745534e-06,
      "loss": 1.1279,
      "step": 28380
    },
    {
      "epoch": 2.199074343035303,
      "grad_norm": 1.3882958889007568,
      "learning_rate": 8.040124255759772e-06,
      "loss": 1.1209,
      "step": 28390
    },
    {
      "epoch": 2.1998489513739616,
      "grad_norm": 1.5612249374389648,
      "learning_rate": 8.03235827077401e-06,
      "loss": 1.2726,
      "step": 28400
    },
    {
      "epoch": 2.2006235597126205,
      "grad_norm": 2.0184109210968018,
      "learning_rate": 8.024592285788246e-06,
      "loss": 1.1833,
      "step": 28410
    },
    {
      "epoch": 2.201398168051279,
      "grad_norm": 1.330678105354309,
      "learning_rate": 8.016826300802486e-06,
      "loss": 1.1735,
      "step": 28420
    },
    {
      "epoch": 2.2021727763899377,
      "grad_norm": 1.3028202056884766,
      "learning_rate": 8.009060315816724e-06,
      "loss": 1.2048,
      "step": 28430
    },
    {
      "epoch": 2.2029473847285965,
      "grad_norm": 1.3873082399368286,
      "learning_rate": 8.001294330830962e-06,
      "loss": 1.1535,
      "step": 28440
    },
    {
      "epoch": 2.2037219930672554,
      "grad_norm": 1.3774008750915527,
      "learning_rate": 7.993528345845198e-06,
      "loss": 1.2086,
      "step": 28450
    },
    {
      "epoch": 2.204496601405914,
      "grad_norm": 1.3365726470947266,
      "learning_rate": 7.985762360859436e-06,
      "loss": 1.2173,
      "step": 28460
    },
    {
      "epoch": 2.205271209744573,
      "grad_norm": 1.4433820247650146,
      "learning_rate": 7.977996375873674e-06,
      "loss": 1.1399,
      "step": 28470
    },
    {
      "epoch": 2.206045818083232,
      "grad_norm": 1.529532790184021,
      "learning_rate": 7.970230390887911e-06,
      "loss": 1.1192,
      "step": 28480
    },
    {
      "epoch": 2.2068204264218902,
      "grad_norm": 2.025866746902466,
      "learning_rate": 7.962464405902148e-06,
      "loss": 1.1618,
      "step": 28490
    },
    {
      "epoch": 2.207595034760549,
      "grad_norm": 1.5628910064697266,
      "learning_rate": 7.954698420916385e-06,
      "loss": 1.1216,
      "step": 28500
    },
    {
      "epoch": 2.208369643099208,
      "grad_norm": 1.3008475303649902,
      "learning_rate": 7.946932435930623e-06,
      "loss": 1.1127,
      "step": 28510
    },
    {
      "epoch": 2.2091442514378667,
      "grad_norm": 1.73047935962677,
      "learning_rate": 7.939166450944863e-06,
      "loss": 1.1842,
      "step": 28520
    },
    {
      "epoch": 2.2099188597765256,
      "grad_norm": 1.179847002029419,
      "learning_rate": 7.9314004659591e-06,
      "loss": 1.1922,
      "step": 28530
    },
    {
      "epoch": 2.2106934681151844,
      "grad_norm": 1.235469937324524,
      "learning_rate": 7.923634480973337e-06,
      "loss": 1.2314,
      "step": 28540
    },
    {
      "epoch": 2.2114680764538432,
      "grad_norm": 1.6990962028503418,
      "learning_rate": 7.915868495987575e-06,
      "loss": 1.2605,
      "step": 28550
    },
    {
      "epoch": 2.2122426847925016,
      "grad_norm": 1.3442121744155884,
      "learning_rate": 7.908102511001813e-06,
      "loss": 1.1451,
      "step": 28560
    },
    {
      "epoch": 2.2130172931311605,
      "grad_norm": 1.5614349842071533,
      "learning_rate": 7.90033652601605e-06,
      "loss": 1.1488,
      "step": 28570
    },
    {
      "epoch": 2.2137919014698193,
      "grad_norm": 1.4737932682037354,
      "learning_rate": 7.892570541030287e-06,
      "loss": 1.117,
      "step": 28580
    },
    {
      "epoch": 2.214566509808478,
      "grad_norm": 1.9508726596832275,
      "learning_rate": 7.884804556044525e-06,
      "loss": 1.0878,
      "step": 28590
    },
    {
      "epoch": 2.215341118147137,
      "grad_norm": 1.4744718074798584,
      "learning_rate": 7.877038571058762e-06,
      "loss": 1.1664,
      "step": 28600
    },
    {
      "epoch": 2.216115726485796,
      "grad_norm": 1.781486988067627,
      "learning_rate": 7.869272586073e-06,
      "loss": 1.171,
      "step": 28610
    },
    {
      "epoch": 2.216890334824454,
      "grad_norm": 1.504450798034668,
      "learning_rate": 7.861506601087238e-06,
      "loss": 1.2001,
      "step": 28620
    },
    {
      "epoch": 2.217664943163113,
      "grad_norm": 1.8126164674758911,
      "learning_rate": 7.853740616101476e-06,
      "loss": 1.1625,
      "step": 28630
    },
    {
      "epoch": 2.218439551501772,
      "grad_norm": 1.3350218534469604,
      "learning_rate": 7.845974631115714e-06,
      "loss": 1.1753,
      "step": 28640
    },
    {
      "epoch": 2.2192141598404307,
      "grad_norm": 1.513261079788208,
      "learning_rate": 7.838208646129952e-06,
      "loss": 1.2035,
      "step": 28650
    },
    {
      "epoch": 2.2199887681790895,
      "grad_norm": 2.0425500869750977,
      "learning_rate": 7.830442661144188e-06,
      "loss": 1.1903,
      "step": 28660
    },
    {
      "epoch": 2.2207633765177484,
      "grad_norm": 1.5473371744155884,
      "learning_rate": 7.822676676158426e-06,
      "loss": 1.2805,
      "step": 28670
    },
    {
      "epoch": 2.2215379848564067,
      "grad_norm": 1.7402828931808472,
      "learning_rate": 7.814910691172664e-06,
      "loss": 1.1614,
      "step": 28680
    },
    {
      "epoch": 2.2223125931950656,
      "grad_norm": 1.5228469371795654,
      "learning_rate": 7.807144706186902e-06,
      "loss": 1.1685,
      "step": 28690
    },
    {
      "epoch": 2.2230872015337244,
      "grad_norm": 1.6527607440948486,
      "learning_rate": 7.79937872120114e-06,
      "loss": 1.1655,
      "step": 28700
    },
    {
      "epoch": 2.2238618098723832,
      "grad_norm": 1.9761897325515747,
      "learning_rate": 7.791612736215376e-06,
      "loss": 1.1282,
      "step": 28710
    },
    {
      "epoch": 2.224636418211042,
      "grad_norm": 1.682055115699768,
      "learning_rate": 7.783846751229615e-06,
      "loss": 1.1704,
      "step": 28720
    },
    {
      "epoch": 2.225411026549701,
      "grad_norm": 1.8270769119262695,
      "learning_rate": 7.776080766243853e-06,
      "loss": 1.2154,
      "step": 28730
    },
    {
      "epoch": 2.2261856348883597,
      "grad_norm": 1.9212840795516968,
      "learning_rate": 7.768314781258091e-06,
      "loss": 1.1399,
      "step": 28740
    },
    {
      "epoch": 2.226960243227018,
      "grad_norm": 2.0796303749084473,
      "learning_rate": 7.760548796272327e-06,
      "loss": 1.1837,
      "step": 28750
    },
    {
      "epoch": 2.227734851565677,
      "grad_norm": 1.5637190341949463,
      "learning_rate": 7.752782811286565e-06,
      "loss": 1.1255,
      "step": 28760
    },
    {
      "epoch": 2.228509459904336,
      "grad_norm": 1.7385354042053223,
      "learning_rate": 7.745016826300803e-06,
      "loss": 1.0852,
      "step": 28770
    },
    {
      "epoch": 2.2292840682429946,
      "grad_norm": 1.47655189037323,
      "learning_rate": 7.73725084131504e-06,
      "loss": 1.1938,
      "step": 28780
    },
    {
      "epoch": 2.2300586765816535,
      "grad_norm": 1.8007054328918457,
      "learning_rate": 7.729484856329277e-06,
      "loss": 1.2235,
      "step": 28790
    },
    {
      "epoch": 2.2308332849203123,
      "grad_norm": 1.691050410270691,
      "learning_rate": 7.721718871343515e-06,
      "loss": 1.1951,
      "step": 28800
    },
    {
      "epoch": 2.231607893258971,
      "grad_norm": 1.7452512979507446,
      "learning_rate": 7.713952886357753e-06,
      "loss": 1.127,
      "step": 28810
    },
    {
      "epoch": 2.2323825015976295,
      "grad_norm": 1.6706427335739136,
      "learning_rate": 7.706186901371992e-06,
      "loss": 1.0254,
      "step": 28820
    },
    {
      "epoch": 2.2331571099362884,
      "grad_norm": 1.3577373027801514,
      "learning_rate": 7.698420916386228e-06,
      "loss": 1.0578,
      "step": 28830
    },
    {
      "epoch": 2.233931718274947,
      "grad_norm": 1.6185686588287354,
      "learning_rate": 7.690654931400466e-06,
      "loss": 1.2027,
      "step": 28840
    },
    {
      "epoch": 2.234706326613606,
      "grad_norm": 1.358272671699524,
      "learning_rate": 7.682888946414704e-06,
      "loss": 1.1725,
      "step": 28850
    },
    {
      "epoch": 2.235480934952265,
      "grad_norm": 1.3872507810592651,
      "learning_rate": 7.675122961428942e-06,
      "loss": 1.1541,
      "step": 28860
    },
    {
      "epoch": 2.2362555432909237,
      "grad_norm": 1.5884897708892822,
      "learning_rate": 7.667356976443178e-06,
      "loss": 1.1934,
      "step": 28870
    },
    {
      "epoch": 2.2370301516295825,
      "grad_norm": 2.1508066654205322,
      "learning_rate": 7.659590991457416e-06,
      "loss": 1.1971,
      "step": 28880
    },
    {
      "epoch": 2.237804759968241,
      "grad_norm": 1.6987372636795044,
      "learning_rate": 7.651825006471654e-06,
      "loss": 1.1571,
      "step": 28890
    },
    {
      "epoch": 2.2385793683068997,
      "grad_norm": 1.5027421712875366,
      "learning_rate": 7.644059021485892e-06,
      "loss": 1.1324,
      "step": 28900
    },
    {
      "epoch": 2.2393539766455586,
      "grad_norm": 1.5141781568527222,
      "learning_rate": 7.636293036500131e-06,
      "loss": 1.1364,
      "step": 28910
    },
    {
      "epoch": 2.2401285849842174,
      "grad_norm": 1.4971047639846802,
      "learning_rate": 7.628527051514368e-06,
      "loss": 1.1584,
      "step": 28920
    },
    {
      "epoch": 2.2409031933228762,
      "grad_norm": 2.3505911827087402,
      "learning_rate": 7.620761066528605e-06,
      "loss": 1.1845,
      "step": 28930
    },
    {
      "epoch": 2.241677801661535,
      "grad_norm": 1.472306728363037,
      "learning_rate": 7.612995081542843e-06,
      "loss": 1.0975,
      "step": 28940
    },
    {
      "epoch": 2.2424524100001935,
      "grad_norm": 1.8338860273361206,
      "learning_rate": 7.60522909655708e-06,
      "loss": 1.1185,
      "step": 28950
    },
    {
      "epoch": 2.2432270183388523,
      "grad_norm": 1.397174596786499,
      "learning_rate": 7.597463111571318e-06,
      "loss": 1.2363,
      "step": 28960
    },
    {
      "epoch": 2.244001626677511,
      "grad_norm": 1.6350260972976685,
      "learning_rate": 7.589697126585555e-06,
      "loss": 1.2073,
      "step": 28970
    },
    {
      "epoch": 2.24477623501617,
      "grad_norm": 1.3812123537063599,
      "learning_rate": 7.581931141599793e-06,
      "loss": 1.2175,
      "step": 28980
    },
    {
      "epoch": 2.245550843354829,
      "grad_norm": 1.4265159368515015,
      "learning_rate": 7.57416515661403e-06,
      "loss": 1.1529,
      "step": 28990
    },
    {
      "epoch": 2.2463254516934876,
      "grad_norm": 1.3173774480819702,
      "learning_rate": 7.566399171628268e-06,
      "loss": 1.2219,
      "step": 29000
    },
    {
      "epoch": 2.247100060032146,
      "grad_norm": 2.0756356716156006,
      "learning_rate": 7.5586331866425065e-06,
      "loss": 1.1715,
      "step": 29010
    },
    {
      "epoch": 2.247874668370805,
      "grad_norm": 1.415611982345581,
      "learning_rate": 7.550867201656744e-06,
      "loss": 1.1312,
      "step": 29020
    },
    {
      "epoch": 2.2486492767094637,
      "grad_norm": 1.6505144834518433,
      "learning_rate": 7.543101216670981e-06,
      "loss": 1.2548,
      "step": 29030
    },
    {
      "epoch": 2.2494238850481225,
      "grad_norm": 1.9638915061950684,
      "learning_rate": 7.535335231685219e-06,
      "loss": 1.1493,
      "step": 29040
    },
    {
      "epoch": 2.2501984933867814,
      "grad_norm": 1.4723438024520874,
      "learning_rate": 7.527569246699456e-06,
      "loss": 1.1996,
      "step": 29050
    },
    {
      "epoch": 2.25097310172544,
      "grad_norm": 1.7684910297393799,
      "learning_rate": 7.519803261713694e-06,
      "loss": 1.1234,
      "step": 29060
    },
    {
      "epoch": 2.251747710064099,
      "grad_norm": 1.5862390995025635,
      "learning_rate": 7.512037276727932e-06,
      "loss": 1.1874,
      "step": 29070
    },
    {
      "epoch": 2.2525223184027574,
      "grad_norm": 1.6095516681671143,
      "learning_rate": 7.504271291742169e-06,
      "loss": 1.1967,
      "step": 29080
    },
    {
      "epoch": 2.2532969267414162,
      "grad_norm": 1.7588815689086914,
      "learning_rate": 7.496505306756408e-06,
      "loss": 1.1519,
      "step": 29090
    },
    {
      "epoch": 2.254071535080075,
      "grad_norm": 1.395883321762085,
      "learning_rate": 7.488739321770645e-06,
      "loss": 1.2145,
      "step": 29100
    },
    {
      "epoch": 2.254846143418734,
      "grad_norm": 1.289779782295227,
      "learning_rate": 7.480973336784883e-06,
      "loss": 1.1147,
      "step": 29110
    },
    {
      "epoch": 2.2556207517573927,
      "grad_norm": 1.559761881828308,
      "learning_rate": 7.473983950297696e-06,
      "loss": 1.2055,
      "step": 29120
    },
    {
      "epoch": 2.2563953600960516,
      "grad_norm": 1.4365973472595215,
      "learning_rate": 7.466217965311934e-06,
      "loss": 1.1192,
      "step": 29130
    },
    {
      "epoch": 2.2571699684347104,
      "grad_norm": 1.1568289995193481,
      "learning_rate": 7.458451980326172e-06,
      "loss": 1.1005,
      "step": 29140
    },
    {
      "epoch": 2.257944576773369,
      "grad_norm": 1.3281382322311401,
      "learning_rate": 7.450685995340409e-06,
      "loss": 1.2259,
      "step": 29150
    },
    {
      "epoch": 2.2587191851120276,
      "grad_norm": 1.8644360303878784,
      "learning_rate": 7.442920010354647e-06,
      "loss": 1.1326,
      "step": 29160
    },
    {
      "epoch": 2.2594937934506865,
      "grad_norm": 1.7653496265411377,
      "learning_rate": 7.4351540253688845e-06,
      "loss": 1.1183,
      "step": 29170
    },
    {
      "epoch": 2.2602684017893453,
      "grad_norm": 1.228684902191162,
      "learning_rate": 7.427388040383122e-06,
      "loss": 1.1189,
      "step": 29180
    },
    {
      "epoch": 2.261043010128004,
      "grad_norm": 1.6497673988342285,
      "learning_rate": 7.41962205539736e-06,
      "loss": 1.1399,
      "step": 29190
    },
    {
      "epoch": 2.261817618466663,
      "grad_norm": 1.837756633758545,
      "learning_rate": 7.411856070411597e-06,
      "loss": 1.1268,
      "step": 29200
    },
    {
      "epoch": 2.262592226805322,
      "grad_norm": 1.553408145904541,
      "learning_rate": 7.404090085425835e-06,
      "loss": 1.1774,
      "step": 29210
    },
    {
      "epoch": 2.26336683514398,
      "grad_norm": 2.2157816886901855,
      "learning_rate": 7.396324100440073e-06,
      "loss": 1.2013,
      "step": 29220
    },
    {
      "epoch": 2.264141443482639,
      "grad_norm": 1.6592423915863037,
      "learning_rate": 7.388558115454311e-06,
      "loss": 1.1364,
      "step": 29230
    },
    {
      "epoch": 2.264916051821298,
      "grad_norm": 1.3493603467941284,
      "learning_rate": 7.380792130468548e-06,
      "loss": 1.1398,
      "step": 29240
    },
    {
      "epoch": 2.2656906601599567,
      "grad_norm": 1.7666758298873901,
      "learning_rate": 7.373026145482786e-06,
      "loss": 1.2738,
      "step": 29250
    },
    {
      "epoch": 2.2664652684986155,
      "grad_norm": 1.15728759765625,
      "learning_rate": 7.365260160497023e-06,
      "loss": 1.192,
      "step": 29260
    },
    {
      "epoch": 2.267239876837274,
      "grad_norm": 1.4809132814407349,
      "learning_rate": 7.3574941755112615e-06,
      "loss": 1.2629,
      "step": 29270
    },
    {
      "epoch": 2.2680144851759327,
      "grad_norm": 1.765645146369934,
      "learning_rate": 7.3497281905254985e-06,
      "loss": 1.3097,
      "step": 29280
    },
    {
      "epoch": 2.2687890935145916,
      "grad_norm": 1.419235348701477,
      "learning_rate": 7.341962205539736e-06,
      "loss": 1.2561,
      "step": 29290
    },
    {
      "epoch": 2.2695637018532504,
      "grad_norm": 1.3716654777526855,
      "learning_rate": 7.334196220553973e-06,
      "loss": 1.1265,
      "step": 29300
    },
    {
      "epoch": 2.2703383101919092,
      "grad_norm": 1.4873369932174683,
      "learning_rate": 7.326430235568211e-06,
      "loss": 1.1338,
      "step": 29310
    },
    {
      "epoch": 2.271112918530568,
      "grad_norm": 1.5578628778457642,
      "learning_rate": 7.318664250582449e-06,
      "loss": 1.2026,
      "step": 29320
    },
    {
      "epoch": 2.271887526869227,
      "grad_norm": 1.6591477394104004,
      "learning_rate": 7.310898265596687e-06,
      "loss": 1.1926,
      "step": 29330
    },
    {
      "epoch": 2.2726621352078853,
      "grad_norm": 1.2299563884735107,
      "learning_rate": 7.303132280610924e-06,
      "loss": 1.1611,
      "step": 29340
    },
    {
      "epoch": 2.273436743546544,
      "grad_norm": 1.6414155960083008,
      "learning_rate": 7.295366295625162e-06,
      "loss": 1.1336,
      "step": 29350
    },
    {
      "epoch": 2.274211351885203,
      "grad_norm": 1.264148235321045,
      "learning_rate": 7.287600310639399e-06,
      "loss": 1.195,
      "step": 29360
    },
    {
      "epoch": 2.274985960223862,
      "grad_norm": 1.663918375968933,
      "learning_rate": 7.279834325653638e-06,
      "loss": 1.1391,
      "step": 29370
    },
    {
      "epoch": 2.2757605685625206,
      "grad_norm": 1.2121120691299438,
      "learning_rate": 7.2720683406678755e-06,
      "loss": 1.1207,
      "step": 29380
    },
    {
      "epoch": 2.2765351769011795,
      "grad_norm": 1.8341715335845947,
      "learning_rate": 7.2643023556821125e-06,
      "loss": 1.2298,
      "step": 29390
    },
    {
      "epoch": 2.2773097852398383,
      "grad_norm": 1.1602703332901,
      "learning_rate": 7.25653637069635e-06,
      "loss": 1.1334,
      "step": 29400
    },
    {
      "epoch": 2.2780843935784967,
      "grad_norm": 1.524827480316162,
      "learning_rate": 7.248770385710587e-06,
      "loss": 1.2631,
      "step": 29410
    },
    {
      "epoch": 2.2788590019171555,
      "grad_norm": 1.5233198404312134,
      "learning_rate": 7.241004400724826e-06,
      "loss": 1.2269,
      "step": 29420
    },
    {
      "epoch": 2.2796336102558143,
      "grad_norm": 1.9174634218215942,
      "learning_rate": 7.233238415739063e-06,
      "loss": 1.2216,
      "step": 29430
    },
    {
      "epoch": 2.280408218594473,
      "grad_norm": 1.6836934089660645,
      "learning_rate": 7.225472430753301e-06,
      "loss": 1.1357,
      "step": 29440
    },
    {
      "epoch": 2.281182826933132,
      "grad_norm": 1.7329837083816528,
      "learning_rate": 7.217706445767538e-06,
      "loss": 1.1173,
      "step": 29450
    },
    {
      "epoch": 2.281957435271791,
      "grad_norm": 1.65641450881958,
      "learning_rate": 7.209940460781776e-06,
      "loss": 1.2136,
      "step": 29460
    },
    {
      "epoch": 2.2827320436104497,
      "grad_norm": 1.4081861972808838,
      "learning_rate": 7.202174475796014e-06,
      "loss": 1.2099,
      "step": 29470
    },
    {
      "epoch": 2.283506651949108,
      "grad_norm": 1.3127326965332031,
      "learning_rate": 7.194408490810252e-06,
      "loss": 1.112,
      "step": 29480
    },
    {
      "epoch": 2.284281260287767,
      "grad_norm": 1.609717845916748,
      "learning_rate": 7.186642505824489e-06,
      "loss": 1.1182,
      "step": 29490
    },
    {
      "epoch": 2.2850558686264257,
      "grad_norm": 1.403710961341858,
      "learning_rate": 7.1788765208387265e-06,
      "loss": 1.1327,
      "step": 29500
    },
    {
      "epoch": 2.2858304769650846,
      "grad_norm": 1.636420726776123,
      "learning_rate": 7.1711105358529635e-06,
      "loss": 1.2282,
      "step": 29510
    },
    {
      "epoch": 2.2866050853037434,
      "grad_norm": 2.0115878582000732,
      "learning_rate": 7.163344550867202e-06,
      "loss": 1.2143,
      "step": 29520
    },
    {
      "epoch": 2.2873796936424022,
      "grad_norm": 1.363380789756775,
      "learning_rate": 7.155578565881439e-06,
      "loss": 1.0598,
      "step": 29530
    },
    {
      "epoch": 2.288154301981061,
      "grad_norm": 1.5854270458221436,
      "learning_rate": 7.147812580895677e-06,
      "loss": 1.1185,
      "step": 29540
    },
    {
      "epoch": 2.2889289103197195,
      "grad_norm": 1.4351921081542969,
      "learning_rate": 7.140046595909915e-06,
      "loss": 1.2036,
      "step": 29550
    },
    {
      "epoch": 2.2897035186583783,
      "grad_norm": 1.4357798099517822,
      "learning_rate": 7.132280610924152e-06,
      "loss": 1.0641,
      "step": 29560
    },
    {
      "epoch": 2.290478126997037,
      "grad_norm": 1.6073527336120605,
      "learning_rate": 7.124514625938391e-06,
      "loss": 1.1552,
      "step": 29570
    },
    {
      "epoch": 2.291252735335696,
      "grad_norm": 1.5749138593673706,
      "learning_rate": 7.116748640952628e-06,
      "loss": 1.1636,
      "step": 29580
    },
    {
      "epoch": 2.292027343674355,
      "grad_norm": 1.5090620517730713,
      "learning_rate": 7.1089826559668656e-06,
      "loss": 1.2221,
      "step": 29590
    },
    {
      "epoch": 2.292801952013013,
      "grad_norm": 2.2454304695129395,
      "learning_rate": 7.101216670981103e-06,
      "loss": 1.1418,
      "step": 29600
    },
    {
      "epoch": 2.293576560351672,
      "grad_norm": 1.5947598218917847,
      "learning_rate": 7.0934506859953405e-06,
      "loss": 1.2544,
      "step": 29610
    },
    {
      "epoch": 2.294351168690331,
      "grad_norm": 1.7700446844100952,
      "learning_rate": 7.085684701009578e-06,
      "loss": 1.0663,
      "step": 29620
    },
    {
      "epoch": 2.2951257770289897,
      "grad_norm": 1.5830142498016357,
      "learning_rate": 7.077918716023816e-06,
      "loss": 1.1971,
      "step": 29630
    },
    {
      "epoch": 2.2959003853676485,
      "grad_norm": 1.9266178607940674,
      "learning_rate": 7.070152731038053e-06,
      "loss": 1.2099,
      "step": 29640
    },
    {
      "epoch": 2.2966749937063073,
      "grad_norm": 1.8186142444610596,
      "learning_rate": 7.062386746052291e-06,
      "loss": 1.1428,
      "step": 29650
    },
    {
      "epoch": 2.297449602044966,
      "grad_norm": 1.987842321395874,
      "learning_rate": 7.054620761066528e-06,
      "loss": 1.2436,
      "step": 29660
    },
    {
      "epoch": 2.2982242103836246,
      "grad_norm": 1.7136828899383545,
      "learning_rate": 7.046854776080767e-06,
      "loss": 1.182,
      "step": 29670
    },
    {
      "epoch": 2.2989988187222834,
      "grad_norm": 1.647755742073059,
      "learning_rate": 7.039088791095004e-06,
      "loss": 1.1683,
      "step": 29680
    },
    {
      "epoch": 2.2997734270609422,
      "grad_norm": 1.5299062728881836,
      "learning_rate": 7.031322806109242e-06,
      "loss": 1.2522,
      "step": 29690
    },
    {
      "epoch": 2.300548035399601,
      "grad_norm": 1.288521409034729,
      "learning_rate": 7.023556821123479e-06,
      "loss": 1.0531,
      "step": 29700
    },
    {
      "epoch": 2.30132264373826,
      "grad_norm": 2.026440143585205,
      "learning_rate": 7.015790836137717e-06,
      "loss": 1.0816,
      "step": 29710
    },
    {
      "epoch": 2.3020972520769187,
      "grad_norm": 1.7546570301055908,
      "learning_rate": 7.0080248511519544e-06,
      "loss": 1.2119,
      "step": 29720
    },
    {
      "epoch": 2.3028718604155776,
      "grad_norm": 1.9101251363754272,
      "learning_rate": 7.000258866166192e-06,
      "loss": 1.285,
      "step": 29730
    },
    {
      "epoch": 2.303646468754236,
      "grad_norm": 1.6908284425735474,
      "learning_rate": 6.99249288118043e-06,
      "loss": 1.1696,
      "step": 29740
    },
    {
      "epoch": 2.304421077092895,
      "grad_norm": 1.8171005249023438,
      "learning_rate": 6.984726896194667e-06,
      "loss": 1.1932,
      "step": 29750
    },
    {
      "epoch": 2.3051956854315536,
      "grad_norm": 1.8240004777908325,
      "learning_rate": 6.976960911208906e-06,
      "loss": 1.2028,
      "step": 29760
    },
    {
      "epoch": 2.3059702937702125,
      "grad_norm": 1.8597934246063232,
      "learning_rate": 6.969194926223143e-06,
      "loss": 1.1874,
      "step": 29770
    },
    {
      "epoch": 2.3067449021088713,
      "grad_norm": 1.642439842224121,
      "learning_rate": 6.961428941237381e-06,
      "loss": 1.2069,
      "step": 29780
    },
    {
      "epoch": 2.30751951044753,
      "grad_norm": 2.732391834259033,
      "learning_rate": 6.953662956251618e-06,
      "loss": 1.2456,
      "step": 29790
    },
    {
      "epoch": 2.308294118786189,
      "grad_norm": 1.374954104423523,
      "learning_rate": 6.945896971265856e-06,
      "loss": 1.0915,
      "step": 29800
    },
    {
      "epoch": 2.3090687271248473,
      "grad_norm": 1.2705812454223633,
      "learning_rate": 6.9381309862800935e-06,
      "loss": 1.2787,
      "step": 29810
    },
    {
      "epoch": 2.309843335463506,
      "grad_norm": 1.5309885740280151,
      "learning_rate": 6.930365001294331e-06,
      "loss": 1.1576,
      "step": 29820
    },
    {
      "epoch": 2.310617943802165,
      "grad_norm": 1.4480029344558716,
      "learning_rate": 6.9225990163085684e-06,
      "loss": 1.143,
      "step": 29830
    },
    {
      "epoch": 2.311392552140824,
      "grad_norm": 1.5612363815307617,
      "learning_rate": 6.914833031322806e-06,
      "loss": 1.1566,
      "step": 29840
    },
    {
      "epoch": 2.3121671604794827,
      "grad_norm": 1.463186264038086,
      "learning_rate": 6.907067046337043e-06,
      "loss": 1.1256,
      "step": 29850
    },
    {
      "epoch": 2.3129417688181415,
      "grad_norm": 1.6906068325042725,
      "learning_rate": 6.899301061351282e-06,
      "loss": 1.1649,
      "step": 29860
    },
    {
      "epoch": 2.3137163771568003,
      "grad_norm": 1.8948016166687012,
      "learning_rate": 6.891535076365519e-06,
      "loss": 1.1837,
      "step": 29870
    },
    {
      "epoch": 2.3144909854954587,
      "grad_norm": 1.5328593254089355,
      "learning_rate": 6.883769091379757e-06,
      "loss": 1.249,
      "step": 29880
    },
    {
      "epoch": 2.3152655938341176,
      "grad_norm": 1.460758924484253,
      "learning_rate": 6.876003106393994e-06,
      "loss": 1.2002,
      "step": 29890
    },
    {
      "epoch": 2.3160402021727764,
      "grad_norm": 1.5689327716827393,
      "learning_rate": 6.868237121408232e-06,
      "loss": 1.2191,
      "step": 29900
    },
    {
      "epoch": 2.3168148105114352,
      "grad_norm": 1.3935495615005493,
      "learning_rate": 6.8604711364224705e-06,
      "loss": 1.1917,
      "step": 29910
    },
    {
      "epoch": 2.317589418850094,
      "grad_norm": 1.7568210363388062,
      "learning_rate": 6.8527051514367075e-06,
      "loss": 1.2103,
      "step": 29920
    },
    {
      "epoch": 2.3183640271887525,
      "grad_norm": 1.451335072517395,
      "learning_rate": 6.844939166450945e-06,
      "loss": 1.1776,
      "step": 29930
    },
    {
      "epoch": 2.3191386355274113,
      "grad_norm": 1.7736399173736572,
      "learning_rate": 6.837173181465182e-06,
      "loss": 1.1006,
      "step": 29940
    },
    {
      "epoch": 2.31991324386607,
      "grad_norm": 1.852625846862793,
      "learning_rate": 6.82940719647942e-06,
      "loss": 1.3086,
      "step": 29950
    },
    {
      "epoch": 2.320687852204729,
      "grad_norm": 1.789158582687378,
      "learning_rate": 6.821641211493658e-06,
      "loss": 1.1803,
      "step": 29960
    },
    {
      "epoch": 2.321462460543388,
      "grad_norm": 1.5385123491287231,
      "learning_rate": 6.813875226507896e-06,
      "loss": 1.1897,
      "step": 29970
    },
    {
      "epoch": 2.3222370688820466,
      "grad_norm": 1.684195637702942,
      "learning_rate": 6.806109241522133e-06,
      "loss": 1.1965,
      "step": 29980
    },
    {
      "epoch": 2.3230116772207055,
      "grad_norm": 1.6568530797958374,
      "learning_rate": 6.798343256536371e-06,
      "loss": 1.1537,
      "step": 29990
    },
    {
      "epoch": 2.323786285559364,
      "grad_norm": 1.4427236318588257,
      "learning_rate": 6.790577271550608e-06,
      "loss": 1.2202,
      "step": 30000
    },
    {
      "epoch": 2.3245608938980227,
      "grad_norm": 1.8459464311599731,
      "learning_rate": 6.782811286564847e-06,
      "loss": 1.2571,
      "step": 30010
    },
    {
      "epoch": 2.3253355022366815,
      "grad_norm": 1.2030513286590576,
      "learning_rate": 6.775045301579084e-06,
      "loss": 1.1202,
      "step": 30020
    },
    {
      "epoch": 2.3261101105753403,
      "grad_norm": 1.506138801574707,
      "learning_rate": 6.7672793165933215e-06,
      "loss": 1.1153,
      "step": 30030
    },
    {
      "epoch": 2.326884718913999,
      "grad_norm": 1.9448864459991455,
      "learning_rate": 6.7595133316075585e-06,
      "loss": 1.1462,
      "step": 30040
    },
    {
      "epoch": 2.327659327252658,
      "grad_norm": 1.6164005994796753,
      "learning_rate": 6.751747346621796e-06,
      "loss": 1.0964,
      "step": 30050
    },
    {
      "epoch": 2.328433935591317,
      "grad_norm": 1.9293527603149414,
      "learning_rate": 6.743981361636034e-06,
      "loss": 1.1327,
      "step": 30060
    },
    {
      "epoch": 2.3292085439299752,
      "grad_norm": 1.4580708742141724,
      "learning_rate": 6.736215376650272e-06,
      "loss": 1.1633,
      "step": 30070
    },
    {
      "epoch": 2.329983152268634,
      "grad_norm": 1.3267067670822144,
      "learning_rate": 6.728449391664509e-06,
      "loss": 1.1456,
      "step": 30080
    },
    {
      "epoch": 2.330757760607293,
      "grad_norm": 2.053812026977539,
      "learning_rate": 6.720683406678747e-06,
      "loss": 1.2704,
      "step": 30090
    },
    {
      "epoch": 2.3315323689459517,
      "grad_norm": 1.5775377750396729,
      "learning_rate": 6.712917421692985e-06,
      "loss": 1.1596,
      "step": 30100
    },
    {
      "epoch": 2.3323069772846106,
      "grad_norm": 1.628501534461975,
      "learning_rate": 6.705151436707223e-06,
      "loss": 1.1902,
      "step": 30110
    },
    {
      "epoch": 2.3330815856232694,
      "grad_norm": 1.4705063104629517,
      "learning_rate": 6.697385451721461e-06,
      "loss": 1.1564,
      "step": 30120
    },
    {
      "epoch": 2.3338561939619282,
      "grad_norm": 1.6416956186294556,
      "learning_rate": 6.689619466735698e-06,
      "loss": 1.1688,
      "step": 30130
    },
    {
      "epoch": 2.3346308023005866,
      "grad_norm": 1.522744059562683,
      "learning_rate": 6.6818534817499355e-06,
      "loss": 1.1455,
      "step": 30140
    },
    {
      "epoch": 2.3354054106392454,
      "grad_norm": 1.3835582733154297,
      "learning_rate": 6.6740874967641725e-06,
      "loss": 1.1051,
      "step": 30150
    },
    {
      "epoch": 2.3361800189779043,
      "grad_norm": 1.688170075416565,
      "learning_rate": 6.666321511778411e-06,
      "loss": 1.1522,
      "step": 30160
    },
    {
      "epoch": 2.336954627316563,
      "grad_norm": 1.214797854423523,
      "learning_rate": 6.658555526792648e-06,
      "loss": 1.188,
      "step": 30170
    },
    {
      "epoch": 2.337729235655222,
      "grad_norm": 2.3604018688201904,
      "learning_rate": 6.650789541806886e-06,
      "loss": 1.1831,
      "step": 30180
    },
    {
      "epoch": 2.338503843993881,
      "grad_norm": 1.7038203477859497,
      "learning_rate": 6.643023556821123e-06,
      "loss": 1.1002,
      "step": 30190
    },
    {
      "epoch": 2.339278452332539,
      "grad_norm": 1.8640028238296509,
      "learning_rate": 6.635257571835362e-06,
      "loss": 1.2037,
      "step": 30200
    },
    {
      "epoch": 2.340053060671198,
      "grad_norm": 1.7058320045471191,
      "learning_rate": 6.627491586849599e-06,
      "loss": 1.1484,
      "step": 30210
    },
    {
      "epoch": 2.340827669009857,
      "grad_norm": 1.5296268463134766,
      "learning_rate": 6.619725601863837e-06,
      "loss": 1.173,
      "step": 30220
    },
    {
      "epoch": 2.3416022773485157,
      "grad_norm": 1.3537673950195312,
      "learning_rate": 6.611959616878074e-06,
      "loss": 1.2049,
      "step": 30230
    },
    {
      "epoch": 2.3423768856871745,
      "grad_norm": 1.4125094413757324,
      "learning_rate": 6.604193631892312e-06,
      "loss": 1.2421,
      "step": 30240
    },
    {
      "epoch": 2.3431514940258333,
      "grad_norm": 1.3347809314727783,
      "learning_rate": 6.5964276469065495e-06,
      "loss": 1.2354,
      "step": 30250
    },
    {
      "epoch": 2.3439261023644917,
      "grad_norm": 1.2444875240325928,
      "learning_rate": 6.588661661920787e-06,
      "loss": 1.1956,
      "step": 30260
    },
    {
      "epoch": 2.3447007107031506,
      "grad_norm": 1.408100962638855,
      "learning_rate": 6.580895676935025e-06,
      "loss": 1.2482,
      "step": 30270
    },
    {
      "epoch": 2.3454753190418094,
      "grad_norm": 1.3341487646102905,
      "learning_rate": 6.573129691949262e-06,
      "loss": 1.1117,
      "step": 30280
    },
    {
      "epoch": 2.3462499273804682,
      "grad_norm": 1.365307331085205,
      "learning_rate": 6.5653637069635e-06,
      "loss": 1.2112,
      "step": 30290
    },
    {
      "epoch": 2.347024535719127,
      "grad_norm": 1.6140612363815308,
      "learning_rate": 6.557597721977738e-06,
      "loss": 1.1071,
      "step": 30300
    },
    {
      "epoch": 2.347799144057786,
      "grad_norm": 2.23508620262146,
      "learning_rate": 6.549831736991976e-06,
      "loss": 1.1469,
      "step": 30310
    },
    {
      "epoch": 2.3485737523964447,
      "grad_norm": 1.6263105869293213,
      "learning_rate": 6.542065752006213e-06,
      "loss": 1.1754,
      "step": 30320
    },
    {
      "epoch": 2.349348360735103,
      "grad_norm": 1.6635032892227173,
      "learning_rate": 6.534299767020451e-06,
      "loss": 1.1325,
      "step": 30330
    },
    {
      "epoch": 2.350122969073762,
      "grad_norm": 1.6250801086425781,
      "learning_rate": 6.526533782034688e-06,
      "loss": 1.2019,
      "step": 30340
    },
    {
      "epoch": 2.350897577412421,
      "grad_norm": 1.4220404624938965,
      "learning_rate": 6.5187677970489265e-06,
      "loss": 1.2182,
      "step": 30350
    },
    {
      "epoch": 2.3516721857510796,
      "grad_norm": 1.454086184501648,
      "learning_rate": 6.5110018120631635e-06,
      "loss": 1.2331,
      "step": 30360
    },
    {
      "epoch": 2.3524467940897384,
      "grad_norm": 1.2692692279815674,
      "learning_rate": 6.503235827077401e-06,
      "loss": 1.1519,
      "step": 30370
    },
    {
      "epoch": 2.3532214024283973,
      "grad_norm": 1.4989464282989502,
      "learning_rate": 6.495469842091638e-06,
      "loss": 1.1882,
      "step": 30380
    },
    {
      "epoch": 2.353996010767056,
      "grad_norm": 1.2471545934677124,
      "learning_rate": 6.487703857105876e-06,
      "loss": 1.1524,
      "step": 30390
    },
    {
      "epoch": 2.3547706191057145,
      "grad_norm": 1.5767467021942139,
      "learning_rate": 6.479937872120114e-06,
      "loss": 1.1601,
      "step": 30400
    },
    {
      "epoch": 2.3555452274443733,
      "grad_norm": 1.5434876680374146,
      "learning_rate": 6.472171887134352e-06,
      "loss": 1.1578,
      "step": 30410
    },
    {
      "epoch": 2.356319835783032,
      "grad_norm": 1.4503556489944458,
      "learning_rate": 6.464405902148589e-06,
      "loss": 1.1399,
      "step": 30420
    },
    {
      "epoch": 2.357094444121691,
      "grad_norm": 1.5291714668273926,
      "learning_rate": 6.456639917162827e-06,
      "loss": 1.212,
      "step": 30430
    },
    {
      "epoch": 2.35786905246035,
      "grad_norm": 1.6138174533843994,
      "learning_rate": 6.448873932177064e-06,
      "loss": 1.2301,
      "step": 30440
    },
    {
      "epoch": 2.3586436607990087,
      "grad_norm": 1.8764008283615112,
      "learning_rate": 6.4411079471913026e-06,
      "loss": 1.1311,
      "step": 30450
    },
    {
      "epoch": 2.3594182691376675,
      "grad_norm": 1.6441998481750488,
      "learning_rate": 6.4333419622055404e-06,
      "loss": 1.088,
      "step": 30460
    },
    {
      "epoch": 2.360192877476326,
      "grad_norm": 1.251161813735962,
      "learning_rate": 6.4255759772197775e-06,
      "loss": 1.1335,
      "step": 30470
    },
    {
      "epoch": 2.3609674858149847,
      "grad_norm": 1.3805837631225586,
      "learning_rate": 6.417809992234015e-06,
      "loss": 1.1094,
      "step": 30480
    },
    {
      "epoch": 2.3617420941536436,
      "grad_norm": 2.1227381229400635,
      "learning_rate": 6.410044007248252e-06,
      "loss": 1.118,
      "step": 30490
    },
    {
      "epoch": 2.3625167024923024,
      "grad_norm": 1.7989118099212646,
      "learning_rate": 6.402278022262491e-06,
      "loss": 1.157,
      "step": 30500
    },
    {
      "epoch": 2.3632913108309612,
      "grad_norm": 1.8750660419464111,
      "learning_rate": 6.394512037276728e-06,
      "loss": 1.2501,
      "step": 30510
    },
    {
      "epoch": 2.3640659191696196,
      "grad_norm": 1.6431972980499268,
      "learning_rate": 6.386746052290966e-06,
      "loss": 1.1486,
      "step": 30520
    },
    {
      "epoch": 2.3648405275082784,
      "grad_norm": 1.5186395645141602,
      "learning_rate": 6.378980067305203e-06,
      "loss": 1.1896,
      "step": 30530
    },
    {
      "epoch": 2.3656151358469373,
      "grad_norm": 1.801071286201477,
      "learning_rate": 6.371214082319441e-06,
      "loss": 1.1421,
      "step": 30540
    },
    {
      "epoch": 2.366389744185596,
      "grad_norm": 1.5255792140960693,
      "learning_rate": 6.363448097333679e-06,
      "loss": 1.1995,
      "step": 30550
    },
    {
      "epoch": 2.367164352524255,
      "grad_norm": 1.4489655494689941,
      "learning_rate": 6.3556821123479166e-06,
      "loss": 1.1725,
      "step": 30560
    },
    {
      "epoch": 2.3679389608629138,
      "grad_norm": 1.4621347188949585,
      "learning_rate": 6.3479161273621536e-06,
      "loss": 1.202,
      "step": 30570
    },
    {
      "epoch": 2.3687135692015726,
      "grad_norm": 2.0734758377075195,
      "learning_rate": 6.3401501423763914e-06,
      "loss": 1.1787,
      "step": 30580
    },
    {
      "epoch": 2.369488177540231,
      "grad_norm": 1.412442684173584,
      "learning_rate": 6.3323841573906285e-06,
      "loss": 1.1244,
      "step": 30590
    },
    {
      "epoch": 2.37026278587889,
      "grad_norm": 1.939324975013733,
      "learning_rate": 6.324618172404867e-06,
      "loss": 1.1642,
      "step": 30600
    },
    {
      "epoch": 2.3710373942175487,
      "grad_norm": 1.4830901622772217,
      "learning_rate": 6.316852187419104e-06,
      "loss": 1.1275,
      "step": 30610
    },
    {
      "epoch": 2.3718120025562075,
      "grad_norm": 1.3192728757858276,
      "learning_rate": 6.309086202433342e-06,
      "loss": 1.1371,
      "step": 30620
    },
    {
      "epoch": 2.3725866108948663,
      "grad_norm": 1.9349106550216675,
      "learning_rate": 6.30132021744758e-06,
      "loss": 1.1828,
      "step": 30630
    },
    {
      "epoch": 2.373361219233525,
      "grad_norm": 2.0136489868164062,
      "learning_rate": 6.293554232461817e-06,
      "loss": 1.2248,
      "step": 30640
    },
    {
      "epoch": 2.374135827572184,
      "grad_norm": 1.2000269889831543,
      "learning_rate": 6.285788247476056e-06,
      "loss": 1.0319,
      "step": 30650
    },
    {
      "epoch": 2.3749104359108424,
      "grad_norm": 1.5593420267105103,
      "learning_rate": 6.278022262490293e-06,
      "loss": 1.2062,
      "step": 30660
    },
    {
      "epoch": 2.375685044249501,
      "grad_norm": 1.8038434982299805,
      "learning_rate": 6.2702562775045305e-06,
      "loss": 1.2314,
      "step": 30670
    },
    {
      "epoch": 2.37645965258816,
      "grad_norm": 1.3821097612380981,
      "learning_rate": 6.2624902925187676e-06,
      "loss": 1.1771,
      "step": 30680
    },
    {
      "epoch": 2.377234260926819,
      "grad_norm": 1.564508080482483,
      "learning_rate": 6.254724307533006e-06,
      "loss": 1.2169,
      "step": 30690
    },
    {
      "epoch": 2.3780088692654777,
      "grad_norm": 1.7669093608856201,
      "learning_rate": 6.246958322547243e-06,
      "loss": 1.1348,
      "step": 30700
    },
    {
      "epoch": 2.3787834776041366,
      "grad_norm": 1.2408289909362793,
      "learning_rate": 6.239192337561481e-06,
      "loss": 1.1707,
      "step": 30710
    },
    {
      "epoch": 2.3795580859427954,
      "grad_norm": 1.6640363931655884,
      "learning_rate": 6.231426352575718e-06,
      "loss": 1.1569,
      "step": 30720
    },
    {
      "epoch": 2.3803326942814538,
      "grad_norm": 1.9159114360809326,
      "learning_rate": 6.223660367589956e-06,
      "loss": 1.0598,
      "step": 30730
    },
    {
      "epoch": 2.3811073026201126,
      "grad_norm": 1.582200288772583,
      "learning_rate": 6.215894382604194e-06,
      "loss": 1.1507,
      "step": 30740
    },
    {
      "epoch": 2.3818819109587714,
      "grad_norm": 1.866438865661621,
      "learning_rate": 6.208128397618432e-06,
      "loss": 1.2421,
      "step": 30750
    },
    {
      "epoch": 2.3826565192974303,
      "grad_norm": 1.803090214729309,
      "learning_rate": 6.200362412632669e-06,
      "loss": 1.1134,
      "step": 30760
    },
    {
      "epoch": 2.383431127636089,
      "grad_norm": 1.7565146684646606,
      "learning_rate": 6.192596427646907e-06,
      "loss": 1.1011,
      "step": 30770
    },
    {
      "epoch": 2.384205735974748,
      "grad_norm": 1.6807758808135986,
      "learning_rate": 6.184830442661144e-06,
      "loss": 1.1343,
      "step": 30780
    },
    {
      "epoch": 2.3849803443134068,
      "grad_norm": 1.3727766275405884,
      "learning_rate": 6.177064457675382e-06,
      "loss": 1.062,
      "step": 30790
    },
    {
      "epoch": 2.385754952652065,
      "grad_norm": 1.990038514137268,
      "learning_rate": 6.169298472689619e-06,
      "loss": 1.0834,
      "step": 30800
    },
    {
      "epoch": 2.386529560990724,
      "grad_norm": 1.30898118019104,
      "learning_rate": 6.161532487703857e-06,
      "loss": 1.2659,
      "step": 30810
    },
    {
      "epoch": 2.387304169329383,
      "grad_norm": 1.853034496307373,
      "learning_rate": 6.153766502718095e-06,
      "loss": 1.1608,
      "step": 30820
    },
    {
      "epoch": 2.3880787776680417,
      "grad_norm": 1.8363040685653687,
      "learning_rate": 6.146000517732332e-06,
      "loss": 1.1524,
      "step": 30830
    },
    {
      "epoch": 2.3888533860067005,
      "grad_norm": 1.5604950189590454,
      "learning_rate": 6.138234532746571e-06,
      "loss": 1.1594,
      "step": 30840
    },
    {
      "epoch": 2.389627994345359,
      "grad_norm": 1.6359151601791382,
      "learning_rate": 6.130468547760808e-06,
      "loss": 1.145,
      "step": 30850
    },
    {
      "epoch": 2.3904026026840177,
      "grad_norm": 1.748670220375061,
      "learning_rate": 6.122702562775046e-06,
      "loss": 1.1948,
      "step": 30860
    },
    {
      "epoch": 2.3911772110226766,
      "grad_norm": 1.3462553024291992,
      "learning_rate": 6.114936577789283e-06,
      "loss": 1.2189,
      "step": 30870
    },
    {
      "epoch": 2.3919518193613354,
      "grad_norm": 1.7024742364883423,
      "learning_rate": 6.107170592803521e-06,
      "loss": 1.2578,
      "step": 30880
    },
    {
      "epoch": 2.392726427699994,
      "grad_norm": 1.9892840385437012,
      "learning_rate": 6.0994046078177585e-06,
      "loss": 1.1085,
      "step": 30890
    },
    {
      "epoch": 2.393501036038653,
      "grad_norm": 2.145430326461792,
      "learning_rate": 6.091638622831996e-06,
      "loss": 1.2229,
      "step": 30900
    },
    {
      "epoch": 2.394275644377312,
      "grad_norm": 1.8795231580734253,
      "learning_rate": 6.083872637846233e-06,
      "loss": 1.138,
      "step": 30910
    },
    {
      "epoch": 2.3950502527159703,
      "grad_norm": 1.7291513681411743,
      "learning_rate": 6.076106652860471e-06,
      "loss": 1.1048,
      "step": 30920
    },
    {
      "epoch": 2.395824861054629,
      "grad_norm": 1.6221814155578613,
      "learning_rate": 6.068340667874708e-06,
      "loss": 1.1831,
      "step": 30930
    },
    {
      "epoch": 2.396599469393288,
      "grad_norm": 2.2534613609313965,
      "learning_rate": 6.060574682888947e-06,
      "loss": 1.1173,
      "step": 30940
    },
    {
      "epoch": 2.3973740777319468,
      "grad_norm": 1.3636562824249268,
      "learning_rate": 6.052808697903184e-06,
      "loss": 1.1503,
      "step": 30950
    },
    {
      "epoch": 2.3981486860706056,
      "grad_norm": 1.1378511190414429,
      "learning_rate": 6.045042712917422e-06,
      "loss": 1.1532,
      "step": 30960
    },
    {
      "epoch": 2.3989232944092644,
      "grad_norm": 1.8247634172439575,
      "learning_rate": 6.037276727931659e-06,
      "loss": 1.1489,
      "step": 30970
    },
    {
      "epoch": 2.3996979027479233,
      "grad_norm": 1.5158642530441284,
      "learning_rate": 6.029510742945897e-06,
      "loss": 1.1561,
      "step": 30980
    },
    {
      "epoch": 2.4004725110865817,
      "grad_norm": 1.8154710531234741,
      "learning_rate": 6.0217447579601355e-06,
      "loss": 1.2674,
      "step": 30990
    },
    {
      "epoch": 2.4012471194252405,
      "grad_norm": 1.4029269218444824,
      "learning_rate": 6.0139787729743725e-06,
      "loss": 1.2531,
      "step": 31000
    },
    {
      "epoch": 2.4020217277638993,
      "grad_norm": 1.790684700012207,
      "learning_rate": 6.00621278798861e-06,
      "loss": 1.1289,
      "step": 31010
    },
    {
      "epoch": 2.402796336102558,
      "grad_norm": 1.594369888305664,
      "learning_rate": 5.998446803002847e-06,
      "loss": 1.2214,
      "step": 31020
    },
    {
      "epoch": 2.403570944441217,
      "grad_norm": 1.3947581052780151,
      "learning_rate": 5.990680818017085e-06,
      "loss": 1.18,
      "step": 31030
    },
    {
      "epoch": 2.404345552779876,
      "grad_norm": 1.9071574211120605,
      "learning_rate": 5.982914833031323e-06,
      "loss": 1.3213,
      "step": 31040
    },
    {
      "epoch": 2.4051201611185347,
      "grad_norm": 1.1522377729415894,
      "learning_rate": 5.975148848045561e-06,
      "loss": 1.1736,
      "step": 31050
    },
    {
      "epoch": 2.405894769457193,
      "grad_norm": 1.8452420234680176,
      "learning_rate": 5.967382863059798e-06,
      "loss": 1.1005,
      "step": 31060
    },
    {
      "epoch": 2.406669377795852,
      "grad_norm": 1.9259355068206787,
      "learning_rate": 5.959616878074036e-06,
      "loss": 1.2364,
      "step": 31070
    },
    {
      "epoch": 2.4074439861345107,
      "grad_norm": 1.5621931552886963,
      "learning_rate": 5.951850893088273e-06,
      "loss": 1.1282,
      "step": 31080
    },
    {
      "epoch": 2.4082185944731695,
      "grad_norm": 1.807386040687561,
      "learning_rate": 5.944084908102512e-06,
      "loss": 1.1597,
      "step": 31090
    },
    {
      "epoch": 2.4089932028118284,
      "grad_norm": 1.920255184173584,
      "learning_rate": 5.936318923116749e-06,
      "loss": 1.1571,
      "step": 31100
    },
    {
      "epoch": 2.409767811150487,
      "grad_norm": 2.60799503326416,
      "learning_rate": 5.9285529381309865e-06,
      "loss": 1.2096,
      "step": 31110
    },
    {
      "epoch": 2.410542419489146,
      "grad_norm": 1.72052001953125,
      "learning_rate": 5.9207869531452235e-06,
      "loss": 1.0947,
      "step": 31120
    },
    {
      "epoch": 2.4113170278278044,
      "grad_norm": 1.4222396612167358,
      "learning_rate": 5.913020968159461e-06,
      "loss": 1.294,
      "step": 31130
    },
    {
      "epoch": 2.4120916361664633,
      "grad_norm": 1.619525671005249,
      "learning_rate": 5.905254983173699e-06,
      "loss": 1.0694,
      "step": 31140
    },
    {
      "epoch": 2.412866244505122,
      "grad_norm": 1.556484341621399,
      "learning_rate": 5.897488998187937e-06,
      "loss": 1.239,
      "step": 31150
    },
    {
      "epoch": 2.413640852843781,
      "grad_norm": 1.617663025856018,
      "learning_rate": 5.889723013202174e-06,
      "loss": 1.2351,
      "step": 31160
    },
    {
      "epoch": 2.4144154611824398,
      "grad_norm": 1.5183225870132446,
      "learning_rate": 5.881957028216412e-06,
      "loss": 1.1599,
      "step": 31170
    },
    {
      "epoch": 2.415190069521098,
      "grad_norm": 1.7220979928970337,
      "learning_rate": 5.874191043230651e-06,
      "loss": 1.1244,
      "step": 31180
    },
    {
      "epoch": 2.415964677859757,
      "grad_norm": 1.6077643632888794,
      "learning_rate": 5.866425058244888e-06,
      "loss": 1.2784,
      "step": 31190
    },
    {
      "epoch": 2.416739286198416,
      "grad_norm": 1.5877972841262817,
      "learning_rate": 5.858659073259126e-06,
      "loss": 1.1503,
      "step": 31200
    },
    {
      "epoch": 2.4175138945370747,
      "grad_norm": 1.6059789657592773,
      "learning_rate": 5.850893088273363e-06,
      "loss": 1.1287,
      "step": 31210
    },
    {
      "epoch": 2.4182885028757335,
      "grad_norm": 1.4114757776260376,
      "learning_rate": 5.8431271032876005e-06,
      "loss": 1.1847,
      "step": 31220
    },
    {
      "epoch": 2.4190631112143923,
      "grad_norm": 1.30898118019104,
      "learning_rate": 5.835361118301838e-06,
      "loss": 1.2306,
      "step": 31230
    },
    {
      "epoch": 2.419837719553051,
      "grad_norm": 1.3981117010116577,
      "learning_rate": 5.827595133316076e-06,
      "loss": 1.1614,
      "step": 31240
    },
    {
      "epoch": 2.4206123278917095,
      "grad_norm": 1.5004526376724243,
      "learning_rate": 5.819829148330313e-06,
      "loss": 1.225,
      "step": 31250
    },
    {
      "epoch": 2.4213869362303684,
      "grad_norm": 2.09187650680542,
      "learning_rate": 5.812063163344551e-06,
      "loss": 1.0864,
      "step": 31260
    },
    {
      "epoch": 2.422161544569027,
      "grad_norm": 1.606345534324646,
      "learning_rate": 5.804297178358788e-06,
      "loss": 1.1337,
      "step": 31270
    },
    {
      "epoch": 2.422936152907686,
      "grad_norm": 1.5801411867141724,
      "learning_rate": 5.796531193373027e-06,
      "loss": 1.1538,
      "step": 31280
    },
    {
      "epoch": 2.423710761246345,
      "grad_norm": 1.6556687355041504,
      "learning_rate": 5.788765208387264e-06,
      "loss": 1.0818,
      "step": 31290
    },
    {
      "epoch": 2.4244853695850037,
      "grad_norm": 1.5525925159454346,
      "learning_rate": 5.780999223401502e-06,
      "loss": 1.1636,
      "step": 31300
    },
    {
      "epoch": 2.4252599779236625,
      "grad_norm": 1.6640056371688843,
      "learning_rate": 5.773233238415739e-06,
      "loss": 1.1634,
      "step": 31310
    },
    {
      "epoch": 2.426034586262321,
      "grad_norm": 1.3243333101272583,
      "learning_rate": 5.765467253429977e-06,
      "loss": 1.1675,
      "step": 31320
    },
    {
      "epoch": 2.4268091946009798,
      "grad_norm": 1.820011019706726,
      "learning_rate": 5.7577012684442145e-06,
      "loss": 1.2253,
      "step": 31330
    },
    {
      "epoch": 2.4275838029396386,
      "grad_norm": 1.5524173974990845,
      "learning_rate": 5.749935283458452e-06,
      "loss": 1.1289,
      "step": 31340
    },
    {
      "epoch": 2.4283584112782974,
      "grad_norm": 1.4723920822143555,
      "learning_rate": 5.742945896971266e-06,
      "loss": 1.1606,
      "step": 31350
    },
    {
      "epoch": 2.4291330196169563,
      "grad_norm": 1.694070816040039,
      "learning_rate": 5.735179911985504e-06,
      "loss": 1.3078,
      "step": 31360
    },
    {
      "epoch": 2.429907627955615,
      "grad_norm": 1.7106437683105469,
      "learning_rate": 5.7274139269997415e-06,
      "loss": 1.2233,
      "step": 31370
    },
    {
      "epoch": 2.430682236294274,
      "grad_norm": 1.7647223472595215,
      "learning_rate": 5.719647942013979e-06,
      "loss": 1.2574,
      "step": 31380
    },
    {
      "epoch": 2.4314568446329323,
      "grad_norm": 2.3908402919769287,
      "learning_rate": 5.711881957028216e-06,
      "loss": 1.2176,
      "step": 31390
    },
    {
      "epoch": 2.432231452971591,
      "grad_norm": 1.435645580291748,
      "learning_rate": 5.704115972042454e-06,
      "loss": 1.1186,
      "step": 31400
    },
    {
      "epoch": 2.43300606131025,
      "grad_norm": 2.1733884811401367,
      "learning_rate": 5.696349987056691e-06,
      "loss": 1.2348,
      "step": 31410
    },
    {
      "epoch": 2.433780669648909,
      "grad_norm": 1.4820773601531982,
      "learning_rate": 5.68858400207093e-06,
      "loss": 1.1722,
      "step": 31420
    },
    {
      "epoch": 2.4345552779875677,
      "grad_norm": 1.7059110403060913,
      "learning_rate": 5.680818017085167e-06,
      "loss": 1.1738,
      "step": 31430
    },
    {
      "epoch": 2.4353298863262265,
      "grad_norm": 2.0238471031188965,
      "learning_rate": 5.673052032099405e-06,
      "loss": 1.1506,
      "step": 31440
    },
    {
      "epoch": 2.436104494664885,
      "grad_norm": 1.6328884363174438,
      "learning_rate": 5.665286047113642e-06,
      "loss": 1.2102,
      "step": 31450
    },
    {
      "epoch": 2.4368791030035437,
      "grad_norm": 1.5791391134262085,
      "learning_rate": 5.65752006212788e-06,
      "loss": 1.1696,
      "step": 31460
    },
    {
      "epoch": 2.4376537113422025,
      "grad_norm": 1.5414797067642212,
      "learning_rate": 5.649754077142118e-06,
      "loss": 1.1622,
      "step": 31470
    },
    {
      "epoch": 2.4384283196808614,
      "grad_norm": 1.5815353393554688,
      "learning_rate": 5.6419880921563554e-06,
      "loss": 1.1796,
      "step": 31480
    },
    {
      "epoch": 2.43920292801952,
      "grad_norm": 2.280639410018921,
      "learning_rate": 5.634222107170593e-06,
      "loss": 1.1713,
      "step": 31490
    },
    {
      "epoch": 2.439977536358179,
      "grad_norm": 1.8784428834915161,
      "learning_rate": 5.62645612218483e-06,
      "loss": 1.2462,
      "step": 31500
    },
    {
      "epoch": 2.4407521446968374,
      "grad_norm": 1.5387914180755615,
      "learning_rate": 5.618690137199068e-06,
      "loss": 1.1773,
      "step": 31510
    },
    {
      "epoch": 2.4415267530354963,
      "grad_norm": 2.2446134090423584,
      "learning_rate": 5.610924152213306e-06,
      "loss": 1.1601,
      "step": 31520
    },
    {
      "epoch": 2.442301361374155,
      "grad_norm": 1.6101354360580444,
      "learning_rate": 5.603158167227544e-06,
      "loss": 1.2251,
      "step": 31530
    },
    {
      "epoch": 2.443075969712814,
      "grad_norm": 1.422255516052246,
      "learning_rate": 5.595392182241781e-06,
      "loss": 1.1862,
      "step": 31540
    },
    {
      "epoch": 2.4438505780514728,
      "grad_norm": 1.578076958656311,
      "learning_rate": 5.587626197256019e-06,
      "loss": 1.1678,
      "step": 31550
    },
    {
      "epoch": 2.4446251863901316,
      "grad_norm": 1.2681000232696533,
      "learning_rate": 5.579860212270256e-06,
      "loss": 1.2458,
      "step": 31560
    },
    {
      "epoch": 2.4453997947287904,
      "grad_norm": 1.5644274950027466,
      "learning_rate": 5.5720942272844945e-06,
      "loss": 1.2033,
      "step": 31570
    },
    {
      "epoch": 2.446174403067449,
      "grad_norm": 1.7288272380828857,
      "learning_rate": 5.5643282422987316e-06,
      "loss": 1.2175,
      "step": 31580
    },
    {
      "epoch": 2.4469490114061077,
      "grad_norm": 1.4863041639328003,
      "learning_rate": 5.5565622573129694e-06,
      "loss": 1.167,
      "step": 31590
    },
    {
      "epoch": 2.4477236197447665,
      "grad_norm": 1.4573770761489868,
      "learning_rate": 5.5487962723272065e-06,
      "loss": 1.1477,
      "step": 31600
    },
    {
      "epoch": 2.4484982280834253,
      "grad_norm": 1.764686107635498,
      "learning_rate": 5.541030287341444e-06,
      "loss": 1.1144,
      "step": 31610
    },
    {
      "epoch": 2.449272836422084,
      "grad_norm": 2.135887861251831,
      "learning_rate": 5.533264302355682e-06,
      "loss": 1.1659,
      "step": 31620
    },
    {
      "epoch": 2.450047444760743,
      "grad_norm": 1.6992387771606445,
      "learning_rate": 5.52549831736992e-06,
      "loss": 1.1966,
      "step": 31630
    },
    {
      "epoch": 2.450822053099402,
      "grad_norm": 2.1492040157318115,
      "learning_rate": 5.517732332384157e-06,
      "loss": 1.2057,
      "step": 31640
    },
    {
      "epoch": 2.45159666143806,
      "grad_norm": 1.6305623054504395,
      "learning_rate": 5.509966347398395e-06,
      "loss": 1.1925,
      "step": 31650
    },
    {
      "epoch": 2.452371269776719,
      "grad_norm": 1.618077039718628,
      "learning_rate": 5.502200362412633e-06,
      "loss": 1.1692,
      "step": 31660
    },
    {
      "epoch": 2.453145878115378,
      "grad_norm": 1.902132272720337,
      "learning_rate": 5.494434377426871e-06,
      "loss": 1.1922,
      "step": 31670
    },
    {
      "epoch": 2.4539204864540367,
      "grad_norm": 1.3322174549102783,
      "learning_rate": 5.4866683924411085e-06,
      "loss": 1.1156,
      "step": 31680
    },
    {
      "epoch": 2.4546950947926955,
      "grad_norm": 1.6425879001617432,
      "learning_rate": 5.4789024074553456e-06,
      "loss": 1.076,
      "step": 31690
    },
    {
      "epoch": 2.4554697031313544,
      "grad_norm": 1.6016390323638916,
      "learning_rate": 5.471136422469583e-06,
      "loss": 1.1239,
      "step": 31700
    },
    {
      "epoch": 2.456244311470013,
      "grad_norm": 1.4266440868377686,
      "learning_rate": 5.4633704374838204e-06,
      "loss": 1.1959,
      "step": 31710
    },
    {
      "epoch": 2.4570189198086716,
      "grad_norm": 1.3993195295333862,
      "learning_rate": 5.455604452498059e-06,
      "loss": 1.1609,
      "step": 31720
    },
    {
      "epoch": 2.4577935281473304,
      "grad_norm": 1.9384227991104126,
      "learning_rate": 5.447838467512296e-06,
      "loss": 1.0718,
      "step": 31730
    },
    {
      "epoch": 2.4585681364859893,
      "grad_norm": 3.5705904960632324,
      "learning_rate": 5.440072482526534e-06,
      "loss": 1.1849,
      "step": 31740
    },
    {
      "epoch": 2.459342744824648,
      "grad_norm": 1.966094970703125,
      "learning_rate": 5.432306497540771e-06,
      "loss": 1.1489,
      "step": 31750
    },
    {
      "epoch": 2.460117353163307,
      "grad_norm": 1.2085341215133667,
      "learning_rate": 5.42454051255501e-06,
      "loss": 1.1177,
      "step": 31760
    },
    {
      "epoch": 2.4608919615019653,
      "grad_norm": 1.3128598928451538,
      "learning_rate": 5.416774527569247e-06,
      "loss": 1.0956,
      "step": 31770
    },
    {
      "epoch": 2.461666569840624,
      "grad_norm": 1.8419594764709473,
      "learning_rate": 5.409008542583485e-06,
      "loss": 1.1372,
      "step": 31780
    },
    {
      "epoch": 2.462441178179283,
      "grad_norm": 1.5225173234939575,
      "learning_rate": 5.401242557597722e-06,
      "loss": 1.0868,
      "step": 31790
    },
    {
      "epoch": 2.463215786517942,
      "grad_norm": 1.441602110862732,
      "learning_rate": 5.3934765726119595e-06,
      "loss": 1.2185,
      "step": 31800
    },
    {
      "epoch": 2.4639903948566007,
      "grad_norm": 1.3748235702514648,
      "learning_rate": 5.385710587626197e-06,
      "loss": 1.2245,
      "step": 31810
    },
    {
      "epoch": 2.4647650031952595,
      "grad_norm": 1.492652416229248,
      "learning_rate": 5.377944602640435e-06,
      "loss": 1.0978,
      "step": 31820
    },
    {
      "epoch": 2.4655396115339183,
      "grad_norm": 1.5191587209701538,
      "learning_rate": 5.370178617654673e-06,
      "loss": 1.1754,
      "step": 31830
    },
    {
      "epoch": 2.4663142198725767,
      "grad_norm": 1.3908475637435913,
      "learning_rate": 5.36241263266891e-06,
      "loss": 1.2102,
      "step": 31840
    },
    {
      "epoch": 2.4670888282112355,
      "grad_norm": 1.818945050239563,
      "learning_rate": 5.354646647683148e-06,
      "loss": 1.1839,
      "step": 31850
    },
    {
      "epoch": 2.4678634365498944,
      "grad_norm": 1.5001205205917358,
      "learning_rate": 5.346880662697386e-06,
      "loss": 1.0675,
      "step": 31860
    },
    {
      "epoch": 2.468638044888553,
      "grad_norm": 1.312131643295288,
      "learning_rate": 5.339114677711624e-06,
      "loss": 1.2638,
      "step": 31870
    },
    {
      "epoch": 2.469412653227212,
      "grad_norm": 1.5670166015625,
      "learning_rate": 5.331348692725861e-06,
      "loss": 1.1286,
      "step": 31880
    },
    {
      "epoch": 2.470187261565871,
      "grad_norm": 1.8024638891220093,
      "learning_rate": 5.323582707740099e-06,
      "loss": 1.1023,
      "step": 31890
    },
    {
      "epoch": 2.4709618699045297,
      "grad_norm": 1.4382752180099487,
      "learning_rate": 5.315816722754336e-06,
      "loss": 1.1489,
      "step": 31900
    },
    {
      "epoch": 2.471736478243188,
      "grad_norm": 1.531084656715393,
      "learning_rate": 5.308050737768574e-06,
      "loss": 1.1989,
      "step": 31910
    },
    {
      "epoch": 2.472511086581847,
      "grad_norm": 1.4335664510726929,
      "learning_rate": 5.300284752782811e-06,
      "loss": 1.0814,
      "step": 31920
    },
    {
      "epoch": 2.4732856949205058,
      "grad_norm": 1.8617031574249268,
      "learning_rate": 5.292518767797049e-06,
      "loss": 1.2437,
      "step": 31930
    },
    {
      "epoch": 2.4740603032591646,
      "grad_norm": 1.5045514106750488,
      "learning_rate": 5.284752782811286e-06,
      "loss": 1.115,
      "step": 31940
    },
    {
      "epoch": 2.4748349115978234,
      "grad_norm": 1.7489850521087646,
      "learning_rate": 5.276986797825524e-06,
      "loss": 1.1006,
      "step": 31950
    },
    {
      "epoch": 2.4756095199364823,
      "grad_norm": 2.474308967590332,
      "learning_rate": 5.269220812839762e-06,
      "loss": 1.2407,
      "step": 31960
    },
    {
      "epoch": 2.476384128275141,
      "grad_norm": 1.3297134637832642,
      "learning_rate": 5.261454827854e-06,
      "loss": 1.2378,
      "step": 31970
    },
    {
      "epoch": 2.4771587366137995,
      "grad_norm": 1.4647635221481323,
      "learning_rate": 5.253688842868237e-06,
      "loss": 1.2119,
      "step": 31980
    },
    {
      "epoch": 2.4779333449524583,
      "grad_norm": 1.8238446712493896,
      "learning_rate": 5.245922857882475e-06,
      "loss": 1.1687,
      "step": 31990
    },
    {
      "epoch": 2.478707953291117,
      "grad_norm": 1.9822747707366943,
      "learning_rate": 5.238156872896712e-06,
      "loss": 1.1259,
      "step": 32000
    },
    {
      "epoch": 2.479482561629776,
      "grad_norm": 1.464191198348999,
      "learning_rate": 5.2303908879109505e-06,
      "loss": 1.1445,
      "step": 32010
    },
    {
      "epoch": 2.480257169968435,
      "grad_norm": 1.2877100706100464,
      "learning_rate": 5.222624902925188e-06,
      "loss": 1.1575,
      "step": 32020
    },
    {
      "epoch": 2.4810317783070936,
      "grad_norm": 1.5510585308074951,
      "learning_rate": 5.214858917939425e-06,
      "loss": 1.0656,
      "step": 32030
    },
    {
      "epoch": 2.4818063866457525,
      "grad_norm": 2.465372323989868,
      "learning_rate": 5.207092932953663e-06,
      "loss": 1.1987,
      "step": 32040
    },
    {
      "epoch": 2.482580994984411,
      "grad_norm": 1.3106601238250732,
      "learning_rate": 5.1993269479679e-06,
      "loss": 1.2023,
      "step": 32050
    },
    {
      "epoch": 2.4833556033230697,
      "grad_norm": 1.6571158170700073,
      "learning_rate": 5.191560962982139e-06,
      "loss": 1.1048,
      "step": 32060
    },
    {
      "epoch": 2.4841302116617285,
      "grad_norm": 2.0663671493530273,
      "learning_rate": 5.183794977996376e-06,
      "loss": 1.1638,
      "step": 32070
    },
    {
      "epoch": 2.4849048200003874,
      "grad_norm": 1.1210798025131226,
      "learning_rate": 5.176028993010614e-06,
      "loss": 1.1648,
      "step": 32080
    },
    {
      "epoch": 2.485679428339046,
      "grad_norm": 1.5476458072662354,
      "learning_rate": 5.168263008024851e-06,
      "loss": 1.1771,
      "step": 32090
    },
    {
      "epoch": 2.4864540366777046,
      "grad_norm": 1.5417253971099854,
      "learning_rate": 5.160497023039089e-06,
      "loss": 1.18,
      "step": 32100
    },
    {
      "epoch": 2.4872286450163634,
      "grad_norm": 1.5326733589172363,
      "learning_rate": 5.152731038053327e-06,
      "loss": 1.1801,
      "step": 32110
    },
    {
      "epoch": 2.4880032533550223,
      "grad_norm": 1.4347761869430542,
      "learning_rate": 5.1449650530675645e-06,
      "loss": 1.187,
      "step": 32120
    },
    {
      "epoch": 2.488777861693681,
      "grad_norm": 1.63251793384552,
      "learning_rate": 5.1371990680818015e-06,
      "loss": 1.1179,
      "step": 32130
    },
    {
      "epoch": 2.48955247003234,
      "grad_norm": 1.416237473487854,
      "learning_rate": 5.129433083096039e-06,
      "loss": 1.2085,
      "step": 32140
    },
    {
      "epoch": 2.4903270783709988,
      "grad_norm": 1.3800617456436157,
      "learning_rate": 5.121667098110276e-06,
      "loss": 1.301,
      "step": 32150
    },
    {
      "epoch": 2.4911016867096576,
      "grad_norm": 1.351258397102356,
      "learning_rate": 5.113901113124515e-06,
      "loss": 1.2398,
      "step": 32160
    },
    {
      "epoch": 2.491876295048316,
      "grad_norm": 1.765932321548462,
      "learning_rate": 5.106135128138752e-06,
      "loss": 1.114,
      "step": 32170
    },
    {
      "epoch": 2.492650903386975,
      "grad_norm": 1.3889570236206055,
      "learning_rate": 5.09836914315299e-06,
      "loss": 1.1574,
      "step": 32180
    },
    {
      "epoch": 2.4934255117256336,
      "grad_norm": 1.6649432182312012,
      "learning_rate": 5.090603158167228e-06,
      "loss": 1.1977,
      "step": 32190
    },
    {
      "epoch": 2.4942001200642925,
      "grad_norm": 1.702985405921936,
      "learning_rate": 5.082837173181465e-06,
      "loss": 1.2556,
      "step": 32200
    },
    {
      "epoch": 2.4949747284029513,
      "grad_norm": 1.3596010208129883,
      "learning_rate": 5.075071188195704e-06,
      "loss": 1.1235,
      "step": 32210
    },
    {
      "epoch": 2.49574933674161,
      "grad_norm": 1.776078224182129,
      "learning_rate": 5.067305203209941e-06,
      "loss": 1.1325,
      "step": 32220
    },
    {
      "epoch": 2.496523945080269,
      "grad_norm": 1.7627723217010498,
      "learning_rate": 5.0595392182241785e-06,
      "loss": 1.1456,
      "step": 32230
    },
    {
      "epoch": 2.4972985534189274,
      "grad_norm": 1.588152289390564,
      "learning_rate": 5.0517732332384155e-06,
      "loss": 1.2559,
      "step": 32240
    },
    {
      "epoch": 2.498073161757586,
      "grad_norm": 1.8716949224472046,
      "learning_rate": 5.044007248252654e-06,
      "loss": 1.2224,
      "step": 32250
    },
    {
      "epoch": 2.498847770096245,
      "grad_norm": 1.7072159051895142,
      "learning_rate": 5.036241263266891e-06,
      "loss": 1.1638,
      "step": 32260
    },
    {
      "epoch": 2.499622378434904,
      "grad_norm": 1.8971363306045532,
      "learning_rate": 5.028475278281129e-06,
      "loss": 1.0535,
      "step": 32270
    },
    {
      "epoch": 2.5003969867735627,
      "grad_norm": 2.2056639194488525,
      "learning_rate": 5.020709293295366e-06,
      "loss": 1.2514,
      "step": 32280
    },
    {
      "epoch": 2.5011715951122215,
      "grad_norm": 1.566238284111023,
      "learning_rate": 5.012943308309604e-06,
      "loss": 1.1965,
      "step": 32290
    },
    {
      "epoch": 2.5019462034508804,
      "grad_norm": 1.5198639631271362,
      "learning_rate": 5.005177323323842e-06,
      "loss": 1.1289,
      "step": 32300
    },
    {
      "epoch": 2.5027208117895388,
      "grad_norm": 1.554860234260559,
      "learning_rate": 4.99741133833808e-06,
      "loss": 1.1694,
      "step": 32310
    },
    {
      "epoch": 2.5034954201281976,
      "grad_norm": 1.9478577375411987,
      "learning_rate": 4.989645353352317e-06,
      "loss": 1.0618,
      "step": 32320
    },
    {
      "epoch": 2.5042700284668564,
      "grad_norm": 1.6118595600128174,
      "learning_rate": 4.981879368366555e-06,
      "loss": 1.1306,
      "step": 32330
    },
    {
      "epoch": 2.5050446368055153,
      "grad_norm": 1.6178685426712036,
      "learning_rate": 4.974113383380792e-06,
      "loss": 1.1084,
      "step": 32340
    },
    {
      "epoch": 2.505819245144174,
      "grad_norm": 1.5361976623535156,
      "learning_rate": 4.96634739839503e-06,
      "loss": 1.1682,
      "step": 32350
    },
    {
      "epoch": 2.5065938534828325,
      "grad_norm": 1.7886722087860107,
      "learning_rate": 4.958581413409267e-06,
      "loss": 1.2229,
      "step": 32360
    },
    {
      "epoch": 2.5073684618214918,
      "grad_norm": 1.524194359779358,
      "learning_rate": 4.950815428423505e-06,
      "loss": 1.1315,
      "step": 32370
    },
    {
      "epoch": 2.50814307016015,
      "grad_norm": 1.8167887926101685,
      "learning_rate": 4.943049443437743e-06,
      "loss": 1.2147,
      "step": 32380
    },
    {
      "epoch": 2.508917678498809,
      "grad_norm": 1.4021066427230835,
      "learning_rate": 4.93528345845198e-06,
      "loss": 1.1885,
      "step": 32390
    },
    {
      "epoch": 2.509692286837468,
      "grad_norm": 2.0451619625091553,
      "learning_rate": 4.927517473466219e-06,
      "loss": 1.1956,
      "step": 32400
    },
    {
      "epoch": 2.5104668951761266,
      "grad_norm": 1.3642832040786743,
      "learning_rate": 4.919751488480456e-06,
      "loss": 1.269,
      "step": 32410
    },
    {
      "epoch": 2.5112415035147855,
      "grad_norm": 1.5265181064605713,
      "learning_rate": 4.911985503494694e-06,
      "loss": 1.2496,
      "step": 32420
    },
    {
      "epoch": 2.512016111853444,
      "grad_norm": 1.5313457250595093,
      "learning_rate": 4.904219518508931e-06,
      "loss": 1.0852,
      "step": 32430
    },
    {
      "epoch": 2.512790720192103,
      "grad_norm": 1.2071435451507568,
      "learning_rate": 4.8964535335231686e-06,
      "loss": 1.1349,
      "step": 32440
    },
    {
      "epoch": 2.5135653285307615,
      "grad_norm": 1.5853127241134644,
      "learning_rate": 4.8886875485374064e-06,
      "loss": 1.1636,
      "step": 32450
    },
    {
      "epoch": 2.5143399368694204,
      "grad_norm": 1.789578914642334,
      "learning_rate": 4.880921563551644e-06,
      "loss": 1.1845,
      "step": 32460
    },
    {
      "epoch": 2.515114545208079,
      "grad_norm": 1.560168981552124,
      "learning_rate": 4.873155578565881e-06,
      "loss": 1.1155,
      "step": 32470
    },
    {
      "epoch": 2.515889153546738,
      "grad_norm": 1.5563831329345703,
      "learning_rate": 4.865389593580119e-06,
      "loss": 1.1156,
      "step": 32480
    },
    {
      "epoch": 2.516663761885397,
      "grad_norm": 1.4707297086715698,
      "learning_rate": 4.857623608594356e-06,
      "loss": 1.155,
      "step": 32490
    },
    {
      "epoch": 2.5174383702240553,
      "grad_norm": 1.2206809520721436,
      "learning_rate": 4.849857623608595e-06,
      "loss": 1.1438,
      "step": 32500
    },
    {
      "epoch": 2.518212978562714,
      "grad_norm": 2.179539680480957,
      "learning_rate": 4.842091638622832e-06,
      "loss": 1.1848,
      "step": 32510
    },
    {
      "epoch": 2.518987586901373,
      "grad_norm": 1.4258896112442017,
      "learning_rate": 4.83432565363707e-06,
      "loss": 1.1398,
      "step": 32520
    },
    {
      "epoch": 2.5197621952400318,
      "grad_norm": 1.4283097982406616,
      "learning_rate": 4.826559668651307e-06,
      "loss": 1.0844,
      "step": 32530
    },
    {
      "epoch": 2.5205368035786906,
      "grad_norm": 1.4121521711349487,
      "learning_rate": 4.818793683665545e-06,
      "loss": 1.1522,
      "step": 32540
    },
    {
      "epoch": 2.5213114119173494,
      "grad_norm": 1.5846909284591675,
      "learning_rate": 4.811027698679783e-06,
      "loss": 1.1816,
      "step": 32550
    },
    {
      "epoch": 2.5220860202560083,
      "grad_norm": 1.6271476745605469,
      "learning_rate": 4.80326171369402e-06,
      "loss": 1.155,
      "step": 32560
    },
    {
      "epoch": 2.5228606285946666,
      "grad_norm": 1.516312837600708,
      "learning_rate": 4.795495728708258e-06,
      "loss": 1.2593,
      "step": 32570
    },
    {
      "epoch": 2.5236352369333255,
      "grad_norm": 1.4873429536819458,
      "learning_rate": 4.787729743722495e-06,
      "loss": 1.1299,
      "step": 32580
    },
    {
      "epoch": 2.5244098452719843,
      "grad_norm": 1.547661542892456,
      "learning_rate": 4.779963758736733e-06,
      "loss": 1.1714,
      "step": 32590
    },
    {
      "epoch": 2.525184453610643,
      "grad_norm": 1.261505126953125,
      "learning_rate": 4.772197773750971e-06,
      "loss": 1.138,
      "step": 32600
    },
    {
      "epoch": 2.525959061949302,
      "grad_norm": 1.3886899948120117,
      "learning_rate": 4.764431788765209e-06,
      "loss": 1.1579,
      "step": 32610
    },
    {
      "epoch": 2.526733670287961,
      "grad_norm": 1.6872777938842773,
      "learning_rate": 4.756665803779446e-06,
      "loss": 1.2135,
      "step": 32620
    },
    {
      "epoch": 2.5275082786266196,
      "grad_norm": 1.514257788658142,
      "learning_rate": 4.748899818793684e-06,
      "loss": 1.1861,
      "step": 32630
    },
    {
      "epoch": 2.528282886965278,
      "grad_norm": 1.970821738243103,
      "learning_rate": 4.741133833807921e-06,
      "loss": 1.0671,
      "step": 32640
    },
    {
      "epoch": 2.529057495303937,
      "grad_norm": 1.622258186340332,
      "learning_rate": 4.7333678488221595e-06,
      "loss": 1.3094,
      "step": 32650
    },
    {
      "epoch": 2.5298321036425957,
      "grad_norm": 1.8123127222061157,
      "learning_rate": 4.7256018638363965e-06,
      "loss": 1.125,
      "step": 32660
    },
    {
      "epoch": 2.5306067119812545,
      "grad_norm": 2.393389940261841,
      "learning_rate": 4.717835878850634e-06,
      "loss": 1.0835,
      "step": 32670
    },
    {
      "epoch": 2.5313813203199134,
      "grad_norm": 1.3850125074386597,
      "learning_rate": 4.7100698938648714e-06,
      "loss": 1.0371,
      "step": 32680
    },
    {
      "epoch": 2.5321559286585718,
      "grad_norm": 1.436055302619934,
      "learning_rate": 4.702303908879109e-06,
      "loss": 1.2363,
      "step": 32690
    },
    {
      "epoch": 2.532930536997231,
      "grad_norm": 1.7388278245925903,
      "learning_rate": 4.694537923893347e-06,
      "loss": 1.1145,
      "step": 32700
    },
    {
      "epoch": 2.5337051453358894,
      "grad_norm": 1.4467670917510986,
      "learning_rate": 4.686771938907585e-06,
      "loss": 1.0937,
      "step": 32710
    },
    {
      "epoch": 2.5344797536745483,
      "grad_norm": 1.5752804279327393,
      "learning_rate": 4.679005953921822e-06,
      "loss": 1.1905,
      "step": 32720
    },
    {
      "epoch": 2.535254362013207,
      "grad_norm": 1.6563196182250977,
      "learning_rate": 4.67123996893606e-06,
      "loss": 1.236,
      "step": 32730
    },
    {
      "epoch": 2.536028970351866,
      "grad_norm": 1.401485800743103,
      "learning_rate": 4.663473983950299e-06,
      "loss": 1.1837,
      "step": 32740
    },
    {
      "epoch": 2.5368035786905248,
      "grad_norm": 1.8914679288864136,
      "learning_rate": 4.655707998964536e-06,
      "loss": 1.1966,
      "step": 32750
    },
    {
      "epoch": 2.537578187029183,
      "grad_norm": 1.7500569820404053,
      "learning_rate": 4.6479420139787735e-06,
      "loss": 1.1437,
      "step": 32760
    },
    {
      "epoch": 2.538352795367842,
      "grad_norm": 1.9106069803237915,
      "learning_rate": 4.6401760289930105e-06,
      "loss": 1.1112,
      "step": 32770
    },
    {
      "epoch": 2.539127403706501,
      "grad_norm": 2.3885319232940674,
      "learning_rate": 4.632410044007248e-06,
      "loss": 1.1455,
      "step": 32780
    },
    {
      "epoch": 2.5399020120451596,
      "grad_norm": 1.530367374420166,
      "learning_rate": 4.624644059021486e-06,
      "loss": 1.1496,
      "step": 32790
    },
    {
      "epoch": 2.5406766203838185,
      "grad_norm": 1.9630823135375977,
      "learning_rate": 4.616878074035724e-06,
      "loss": 1.3158,
      "step": 32800
    },
    {
      "epoch": 2.5414512287224773,
      "grad_norm": 1.2792892456054688,
      "learning_rate": 4.609112089049961e-06,
      "loss": 1.126,
      "step": 32810
    },
    {
      "epoch": 2.542225837061136,
      "grad_norm": 1.2037981748580933,
      "learning_rate": 4.601346104064199e-06,
      "loss": 1.0427,
      "step": 32820
    },
    {
      "epoch": 2.5430004453997945,
      "grad_norm": 1.6367051601409912,
      "learning_rate": 4.593580119078436e-06,
      "loss": 1.1923,
      "step": 32830
    },
    {
      "epoch": 2.5437750537384534,
      "grad_norm": 1.4580069780349731,
      "learning_rate": 4.585814134092675e-06,
      "loss": 1.3292,
      "step": 32840
    },
    {
      "epoch": 2.544549662077112,
      "grad_norm": 1.525189757347107,
      "learning_rate": 4.578048149106912e-06,
      "loss": 1.232,
      "step": 32850
    },
    {
      "epoch": 2.545324270415771,
      "grad_norm": 1.5155127048492432,
      "learning_rate": 4.57028216412115e-06,
      "loss": 1.1336,
      "step": 32860
    },
    {
      "epoch": 2.54609887875443,
      "grad_norm": 1.910675048828125,
      "learning_rate": 4.562516179135387e-06,
      "loss": 1.1039,
      "step": 32870
    },
    {
      "epoch": 2.5468734870930887,
      "grad_norm": 1.3771164417266846,
      "learning_rate": 4.5547501941496245e-06,
      "loss": 1.1875,
      "step": 32880
    },
    {
      "epoch": 2.5476480954317475,
      "grad_norm": 1.4573184251785278,
      "learning_rate": 4.546984209163862e-06,
      "loss": 1.2183,
      "step": 32890
    },
    {
      "epoch": 2.548422703770406,
      "grad_norm": 1.652573823928833,
      "learning_rate": 4.5392182241781e-06,
      "loss": 1.1429,
      "step": 32900
    },
    {
      "epoch": 2.5491973121090648,
      "grad_norm": 1.497938632965088,
      "learning_rate": 4.531452239192338e-06,
      "loss": 1.1276,
      "step": 32910
    },
    {
      "epoch": 2.5499719204477236,
      "grad_norm": 1.5591793060302734,
      "learning_rate": 4.523686254206575e-06,
      "loss": 1.1407,
      "step": 32920
    },
    {
      "epoch": 2.5507465287863824,
      "grad_norm": 1.6418501138687134,
      "learning_rate": 4.515920269220813e-06,
      "loss": 1.2275,
      "step": 32930
    },
    {
      "epoch": 2.5515211371250412,
      "grad_norm": 1.6584290266036987,
      "learning_rate": 4.508154284235051e-06,
      "loss": 1.2539,
      "step": 32940
    },
    {
      "epoch": 2.5522957454637,
      "grad_norm": 1.9105421304702759,
      "learning_rate": 4.500388299249289e-06,
      "loss": 1.1476,
      "step": 32950
    },
    {
      "epoch": 2.553070353802359,
      "grad_norm": 1.6673346757888794,
      "learning_rate": 4.492622314263526e-06,
      "loss": 1.1544,
      "step": 32960
    },
    {
      "epoch": 2.5538449621410173,
      "grad_norm": 1.9447803497314453,
      "learning_rate": 4.484856329277764e-06,
      "loss": 1.1942,
      "step": 32970
    },
    {
      "epoch": 2.554619570479676,
      "grad_norm": 1.4678400754928589,
      "learning_rate": 4.477090344292001e-06,
      "loss": 1.2261,
      "step": 32980
    },
    {
      "epoch": 2.555394178818335,
      "grad_norm": 1.2171287536621094,
      "learning_rate": 4.469324359306239e-06,
      "loss": 1.1185,
      "step": 32990
    },
    {
      "epoch": 2.556168787156994,
      "grad_norm": 1.3241959810256958,
      "learning_rate": 4.461558374320476e-06,
      "loss": 1.1472,
      "step": 33000
    },
    {
      "epoch": 2.5569433954956526,
      "grad_norm": 1.6739373207092285,
      "learning_rate": 4.453792389334714e-06,
      "loss": 1.2488,
      "step": 33010
    },
    {
      "epoch": 2.557718003834311,
      "grad_norm": 2.505539894104004,
      "learning_rate": 4.446026404348951e-06,
      "loss": 1.2195,
      "step": 33020
    },
    {
      "epoch": 2.5584926121729703,
      "grad_norm": 1.4751949310302734,
      "learning_rate": 4.438260419363189e-06,
      "loss": 1.1353,
      "step": 33030
    },
    {
      "epoch": 2.5592672205116287,
      "grad_norm": 2.0442748069763184,
      "learning_rate": 4.430494434377427e-06,
      "loss": 1.1554,
      "step": 33040
    },
    {
      "epoch": 2.5600418288502875,
      "grad_norm": 1.6163537502288818,
      "learning_rate": 4.422728449391665e-06,
      "loss": 1.1825,
      "step": 33050
    },
    {
      "epoch": 2.5608164371889464,
      "grad_norm": 1.7244726419448853,
      "learning_rate": 4.414962464405902e-06,
      "loss": 1.0857,
      "step": 33060
    },
    {
      "epoch": 2.561591045527605,
      "grad_norm": 2.1924145221710205,
      "learning_rate": 4.40719647942014e-06,
      "loss": 1.1683,
      "step": 33070
    },
    {
      "epoch": 2.562365653866264,
      "grad_norm": 1.2124441862106323,
      "learning_rate": 4.399430494434377e-06,
      "loss": 1.1329,
      "step": 33080
    },
    {
      "epoch": 2.5631402622049224,
      "grad_norm": 1.6538130044937134,
      "learning_rate": 4.3916645094486155e-06,
      "loss": 1.1329,
      "step": 33090
    },
    {
      "epoch": 2.5639148705435812,
      "grad_norm": 2.540332794189453,
      "learning_rate": 4.383898524462853e-06,
      "loss": 1.1405,
      "step": 33100
    },
    {
      "epoch": 2.56468947888224,
      "grad_norm": 1.5999418497085571,
      "learning_rate": 4.37613253947709e-06,
      "loss": 1.0732,
      "step": 33110
    },
    {
      "epoch": 2.565464087220899,
      "grad_norm": 1.2940435409545898,
      "learning_rate": 4.368366554491328e-06,
      "loss": 1.156,
      "step": 33120
    },
    {
      "epoch": 2.5662386955595577,
      "grad_norm": 1.809248685836792,
      "learning_rate": 4.360600569505565e-06,
      "loss": 1.2002,
      "step": 33130
    },
    {
      "epoch": 2.5670133038982166,
      "grad_norm": 1.4117474555969238,
      "learning_rate": 4.352834584519804e-06,
      "loss": 1.1024,
      "step": 33140
    },
    {
      "epoch": 2.5677879122368754,
      "grad_norm": 1.9710687398910522,
      "learning_rate": 4.345068599534041e-06,
      "loss": 1.2057,
      "step": 33150
    },
    {
      "epoch": 2.568562520575534,
      "grad_norm": 1.4888783693313599,
      "learning_rate": 4.337302614548279e-06,
      "loss": 1.1645,
      "step": 33160
    },
    {
      "epoch": 2.5693371289141926,
      "grad_norm": 1.8981877565383911,
      "learning_rate": 4.329536629562516e-06,
      "loss": 1.086,
      "step": 33170
    },
    {
      "epoch": 2.5701117372528515,
      "grad_norm": 1.7694692611694336,
      "learning_rate": 4.321770644576754e-06,
      "loss": 1.168,
      "step": 33180
    },
    {
      "epoch": 2.5708863455915103,
      "grad_norm": 1.6432061195373535,
      "learning_rate": 4.314004659590992e-06,
      "loss": 1.1777,
      "step": 33190
    },
    {
      "epoch": 2.571660953930169,
      "grad_norm": 2.4785549640655518,
      "learning_rate": 4.3062386746052295e-06,
      "loss": 1.1602,
      "step": 33200
    },
    {
      "epoch": 2.572435562268828,
      "grad_norm": 1.7932193279266357,
      "learning_rate": 4.2984726896194665e-06,
      "loss": 1.0895,
      "step": 33210
    },
    {
      "epoch": 2.573210170607487,
      "grad_norm": 1.231957197189331,
      "learning_rate": 4.290706704633704e-06,
      "loss": 1.1725,
      "step": 33220
    },
    {
      "epoch": 2.573984778946145,
      "grad_norm": 2.0330684185028076,
      "learning_rate": 4.282940719647942e-06,
      "loss": 1.1307,
      "step": 33230
    },
    {
      "epoch": 2.574759387284804,
      "grad_norm": 1.7518912553787231,
      "learning_rate": 4.27517473466218e-06,
      "loss": 1.1459,
      "step": 33240
    },
    {
      "epoch": 2.575533995623463,
      "grad_norm": 1.481752634048462,
      "learning_rate": 4.267408749676417e-06,
      "loss": 1.1386,
      "step": 33250
    },
    {
      "epoch": 2.5763086039621217,
      "grad_norm": 1.8463797569274902,
      "learning_rate": 4.259642764690655e-06,
      "loss": 1.1539,
      "step": 33260
    },
    {
      "epoch": 2.5770832123007805,
      "grad_norm": 1.3869879245758057,
      "learning_rate": 4.251876779704893e-06,
      "loss": 1.1348,
      "step": 33270
    },
    {
      "epoch": 2.577857820639439,
      "grad_norm": 1.568683385848999,
      "learning_rate": 4.244110794719131e-06,
      "loss": 1.2779,
      "step": 33280
    },
    {
      "epoch": 2.578632428978098,
      "grad_norm": 2.080348014831543,
      "learning_rate": 4.2363448097333686e-06,
      "loss": 1.2093,
      "step": 33290
    },
    {
      "epoch": 2.5794070373167566,
      "grad_norm": 2.09808087348938,
      "learning_rate": 4.2285788247476056e-06,
      "loss": 1.1481,
      "step": 33300
    },
    {
      "epoch": 2.5801816456554154,
      "grad_norm": 1.3938279151916504,
      "learning_rate": 4.2208128397618434e-06,
      "loss": 1.1252,
      "step": 33310
    },
    {
      "epoch": 2.5809562539940742,
      "grad_norm": 1.8118022680282593,
      "learning_rate": 4.2130468547760805e-06,
      "loss": 1.2452,
      "step": 33320
    },
    {
      "epoch": 2.581730862332733,
      "grad_norm": 1.4548900127410889,
      "learning_rate": 4.205280869790319e-06,
      "loss": 1.1282,
      "step": 33330
    },
    {
      "epoch": 2.582505470671392,
      "grad_norm": 1.4379223585128784,
      "learning_rate": 4.197514884804556e-06,
      "loss": 1.1302,
      "step": 33340
    },
    {
      "epoch": 2.5832800790100503,
      "grad_norm": 1.8096916675567627,
      "learning_rate": 4.189748899818794e-06,
      "loss": 1.19,
      "step": 33350
    },
    {
      "epoch": 2.5840546873487096,
      "grad_norm": 1.2404489517211914,
      "learning_rate": 4.181982914833031e-06,
      "loss": 1.0994,
      "step": 33360
    },
    {
      "epoch": 2.584829295687368,
      "grad_norm": 1.3465977907180786,
      "learning_rate": 4.174993528345845e-06,
      "loss": 1.1237,
      "step": 33370
    },
    {
      "epoch": 2.585603904026027,
      "grad_norm": 1.8367944955825806,
      "learning_rate": 4.167227543360083e-06,
      "loss": 1.1911,
      "step": 33380
    },
    {
      "epoch": 2.5863785123646856,
      "grad_norm": 1.8298636674880981,
      "learning_rate": 4.15946155837432e-06,
      "loss": 1.1087,
      "step": 33390
    },
    {
      "epoch": 2.5871531207033445,
      "grad_norm": 1.88420569896698,
      "learning_rate": 4.151695573388558e-06,
      "loss": 1.1471,
      "step": 33400
    },
    {
      "epoch": 2.5879277290420033,
      "grad_norm": 0.9681097865104675,
      "learning_rate": 4.143929588402796e-06,
      "loss": 1.1372,
      "step": 33410
    },
    {
      "epoch": 2.5887023373806617,
      "grad_norm": 1.8797215223312378,
      "learning_rate": 4.136163603417034e-06,
      "loss": 1.2269,
      "step": 33420
    },
    {
      "epoch": 2.5894769457193205,
      "grad_norm": 1.3177202939987183,
      "learning_rate": 4.128397618431272e-06,
      "loss": 1.1989,
      "step": 33430
    },
    {
      "epoch": 2.5902515540579794,
      "grad_norm": 1.103522777557373,
      "learning_rate": 4.120631633445509e-06,
      "loss": 1.149,
      "step": 33440
    },
    {
      "epoch": 2.591026162396638,
      "grad_norm": 2.1318252086639404,
      "learning_rate": 4.1128656484597466e-06,
      "loss": 1.1453,
      "step": 33450
    },
    {
      "epoch": 2.591800770735297,
      "grad_norm": 2.238741159439087,
      "learning_rate": 4.105099663473984e-06,
      "loss": 1.148,
      "step": 33460
    },
    {
      "epoch": 2.592575379073956,
      "grad_norm": 1.5112618207931519,
      "learning_rate": 4.097333678488222e-06,
      "loss": 1.2224,
      "step": 33470
    },
    {
      "epoch": 2.5933499874126147,
      "grad_norm": 2.165614604949951,
      "learning_rate": 4.089567693502459e-06,
      "loss": 1.1599,
      "step": 33480
    },
    {
      "epoch": 2.594124595751273,
      "grad_norm": 2.1069648265838623,
      "learning_rate": 4.081801708516697e-06,
      "loss": 1.1928,
      "step": 33490
    },
    {
      "epoch": 2.594899204089932,
      "grad_norm": 1.3728361129760742,
      "learning_rate": 4.074035723530934e-06,
      "loss": 1.1577,
      "step": 33500
    },
    {
      "epoch": 2.5956738124285907,
      "grad_norm": 1.4728025197982788,
      "learning_rate": 4.066269738545172e-06,
      "loss": 1.1992,
      "step": 33510
    },
    {
      "epoch": 2.5964484207672496,
      "grad_norm": 2.486048698425293,
      "learning_rate": 4.05850375355941e-06,
      "loss": 1.2108,
      "step": 33520
    },
    {
      "epoch": 2.5972230291059084,
      "grad_norm": 2.2519898414611816,
      "learning_rate": 4.050737768573648e-06,
      "loss": 1.1469,
      "step": 33530
    },
    {
      "epoch": 2.5979976374445672,
      "grad_norm": 1.6594210863113403,
      "learning_rate": 4.042971783587885e-06,
      "loss": 1.143,
      "step": 33540
    },
    {
      "epoch": 2.598772245783226,
      "grad_norm": 1.4867526292800903,
      "learning_rate": 4.035205798602123e-06,
      "loss": 1.1268,
      "step": 33550
    },
    {
      "epoch": 2.5995468541218845,
      "grad_norm": 1.8617935180664062,
      "learning_rate": 4.02743981361636e-06,
      "loss": 1.2118,
      "step": 33560
    },
    {
      "epoch": 2.6003214624605433,
      "grad_norm": 2.0215835571289062,
      "learning_rate": 4.019673828630598e-06,
      "loss": 1.2294,
      "step": 33570
    },
    {
      "epoch": 2.601096070799202,
      "grad_norm": 1.4133806228637695,
      "learning_rate": 4.011907843644836e-06,
      "loss": 1.0781,
      "step": 33580
    },
    {
      "epoch": 2.601870679137861,
      "grad_norm": 1.6272895336151123,
      "learning_rate": 4.004141858659073e-06,
      "loss": 1.1125,
      "step": 33590
    },
    {
      "epoch": 2.60264528747652,
      "grad_norm": 1.995790958404541,
      "learning_rate": 3.996375873673311e-06,
      "loss": 1.1709,
      "step": 33600
    },
    {
      "epoch": 2.603419895815178,
      "grad_norm": 1.5041773319244385,
      "learning_rate": 3.988609888687548e-06,
      "loss": 1.1619,
      "step": 33610
    },
    {
      "epoch": 2.6041945041538375,
      "grad_norm": 2.495636224746704,
      "learning_rate": 3.980843903701787e-06,
      "loss": 1.1755,
      "step": 33620
    },
    {
      "epoch": 2.604969112492496,
      "grad_norm": 1.6647862195968628,
      "learning_rate": 3.973077918716024e-06,
      "loss": 1.0878,
      "step": 33630
    },
    {
      "epoch": 2.6057437208311547,
      "grad_norm": 1.622788429260254,
      "learning_rate": 3.965311933730262e-06,
      "loss": 1.203,
      "step": 33640
    },
    {
      "epoch": 2.6065183291698135,
      "grad_norm": 2.0655510425567627,
      "learning_rate": 3.957545948744499e-06,
      "loss": 1.2492,
      "step": 33650
    },
    {
      "epoch": 2.6072929375084724,
      "grad_norm": 1.600110411643982,
      "learning_rate": 3.949779963758737e-06,
      "loss": 1.0975,
      "step": 33660
    },
    {
      "epoch": 2.608067545847131,
      "grad_norm": 1.810708999633789,
      "learning_rate": 3.9420139787729745e-06,
      "loss": 1.2185,
      "step": 33670
    },
    {
      "epoch": 2.6088421541857896,
      "grad_norm": 1.9161927700042725,
      "learning_rate": 3.934247993787212e-06,
      "loss": 1.0966,
      "step": 33680
    },
    {
      "epoch": 2.609616762524449,
      "grad_norm": 1.7201056480407715,
      "learning_rate": 3.926482008801449e-06,
      "loss": 1.2203,
      "step": 33690
    },
    {
      "epoch": 2.6103913708631072,
      "grad_norm": 1.4925156831741333,
      "learning_rate": 3.918716023815687e-06,
      "loss": 1.061,
      "step": 33700
    },
    {
      "epoch": 2.611165979201766,
      "grad_norm": 1.6065022945404053,
      "learning_rate": 3.910950038829924e-06,
      "loss": 1.2447,
      "step": 33710
    },
    {
      "epoch": 2.611940587540425,
      "grad_norm": 1.5152220726013184,
      "learning_rate": 3.903184053844163e-06,
      "loss": 1.1758,
      "step": 33720
    },
    {
      "epoch": 2.6127151958790837,
      "grad_norm": 1.4766072034835815,
      "learning_rate": 3.8954180688584e-06,
      "loss": 1.0967,
      "step": 33730
    },
    {
      "epoch": 2.6134898042177426,
      "grad_norm": 1.3706294298171997,
      "learning_rate": 3.887652083872638e-06,
      "loss": 1.2569,
      "step": 33740
    },
    {
      "epoch": 2.614264412556401,
      "grad_norm": 1.7991955280303955,
      "learning_rate": 3.879886098886876e-06,
      "loss": 1.1634,
      "step": 33750
    },
    {
      "epoch": 2.61503902089506,
      "grad_norm": 1.9223930835723877,
      "learning_rate": 3.872120113901113e-06,
      "loss": 1.1688,
      "step": 33760
    },
    {
      "epoch": 2.6158136292337186,
      "grad_norm": 1.2913635969161987,
      "learning_rate": 3.8643541289153515e-06,
      "loss": 1.132,
      "step": 33770
    },
    {
      "epoch": 2.6165882375723775,
      "grad_norm": 1.2509684562683105,
      "learning_rate": 3.8565881439295885e-06,
      "loss": 1.2673,
      "step": 33780
    },
    {
      "epoch": 2.6173628459110363,
      "grad_norm": 1.5623235702514648,
      "learning_rate": 3.848822158943826e-06,
      "loss": 1.1624,
      "step": 33790
    },
    {
      "epoch": 2.618137454249695,
      "grad_norm": 1.7025967836380005,
      "learning_rate": 3.841056173958063e-06,
      "loss": 1.0942,
      "step": 33800
    },
    {
      "epoch": 2.618912062588354,
      "grad_norm": 1.5617071390151978,
      "learning_rate": 3.833290188972302e-06,
      "loss": 1.1202,
      "step": 33810
    },
    {
      "epoch": 2.6196866709270124,
      "grad_norm": 1.5469183921813965,
      "learning_rate": 3.825524203986539e-06,
      "loss": 1.2195,
      "step": 33820
    },
    {
      "epoch": 2.620461279265671,
      "grad_norm": 2.0942678451538086,
      "learning_rate": 3.817758219000777e-06,
      "loss": 1.183,
      "step": 33830
    },
    {
      "epoch": 2.62123588760433,
      "grad_norm": 1.5933740139007568,
      "learning_rate": 3.809992234015014e-06,
      "loss": 1.1811,
      "step": 33840
    },
    {
      "epoch": 2.622010495942989,
      "grad_norm": 1.8021515607833862,
      "learning_rate": 3.802226249029252e-06,
      "loss": 1.1908,
      "step": 33850
    },
    {
      "epoch": 2.6227851042816477,
      "grad_norm": 1.930801272392273,
      "learning_rate": 3.79446026404349e-06,
      "loss": 1.1496,
      "step": 33860
    },
    {
      "epoch": 2.6235597126203065,
      "grad_norm": 1.7909824848175049,
      "learning_rate": 3.7866942790577276e-06,
      "loss": 1.0921,
      "step": 33870
    },
    {
      "epoch": 2.6243343209589653,
      "grad_norm": 1.4179697036743164,
      "learning_rate": 3.778928294071965e-06,
      "loss": 1.186,
      "step": 33880
    },
    {
      "epoch": 2.6251089292976237,
      "grad_norm": 1.3157590627670288,
      "learning_rate": 3.7711623090862025e-06,
      "loss": 1.2363,
      "step": 33890
    },
    {
      "epoch": 2.6258835376362826,
      "grad_norm": 1.7372167110443115,
      "learning_rate": 3.76339632410044e-06,
      "loss": 1.1819,
      "step": 33900
    },
    {
      "epoch": 2.6266581459749414,
      "grad_norm": 1.8486096858978271,
      "learning_rate": 3.7556303391146782e-06,
      "loss": 1.2743,
      "step": 33910
    },
    {
      "epoch": 2.6274327543136002,
      "grad_norm": 1.4092097282409668,
      "learning_rate": 3.7478643541289157e-06,
      "loss": 1.3245,
      "step": 33920
    },
    {
      "epoch": 2.628207362652259,
      "grad_norm": 1.5187774896621704,
      "learning_rate": 3.740098369143153e-06,
      "loss": 1.1892,
      "step": 33930
    },
    {
      "epoch": 2.6289819709909175,
      "grad_norm": 1.4868141412734985,
      "learning_rate": 3.7323323841573906e-06,
      "loss": 1.1904,
      "step": 33940
    },
    {
      "epoch": 2.6297565793295767,
      "grad_norm": 1.378346562385559,
      "learning_rate": 3.7245663991716284e-06,
      "loss": 1.134,
      "step": 33950
    },
    {
      "epoch": 2.630531187668235,
      "grad_norm": 1.91453218460083,
      "learning_rate": 3.716800414185866e-06,
      "loss": 1.0964,
      "step": 33960
    },
    {
      "epoch": 2.631305796006894,
      "grad_norm": 3.3425464630126953,
      "learning_rate": 3.7090344292001037e-06,
      "loss": 1.1936,
      "step": 33970
    },
    {
      "epoch": 2.632080404345553,
      "grad_norm": 1.6286698579788208,
      "learning_rate": 3.701268444214341e-06,
      "loss": 1.1138,
      "step": 33980
    },
    {
      "epoch": 2.6328550126842116,
      "grad_norm": 2.1208133697509766,
      "learning_rate": 3.6935024592285786e-06,
      "loss": 1.1552,
      "step": 33990
    },
    {
      "epoch": 2.6336296210228705,
      "grad_norm": 1.5010101795196533,
      "learning_rate": 3.6857364742428165e-06,
      "loss": 1.1212,
      "step": 34000
    },
    {
      "epoch": 2.634404229361529,
      "grad_norm": 1.685842514038086,
      "learning_rate": 3.677970489257054e-06,
      "loss": 1.1404,
      "step": 34010
    },
    {
      "epoch": 2.6351788377001877,
      "grad_norm": 1.4101811647415161,
      "learning_rate": 3.670204504271292e-06,
      "loss": 1.164,
      "step": 34020
    },
    {
      "epoch": 2.6359534460388465,
      "grad_norm": 1.5838255882263184,
      "learning_rate": 3.6624385192855297e-06,
      "loss": 1.1672,
      "step": 34030
    },
    {
      "epoch": 2.6367280543775053,
      "grad_norm": 1.6272225379943848,
      "learning_rate": 3.654672534299767e-06,
      "loss": 1.1768,
      "step": 34040
    },
    {
      "epoch": 2.637502662716164,
      "grad_norm": 1.4552583694458008,
      "learning_rate": 3.646906549314005e-06,
      "loss": 1.2643,
      "step": 34050
    },
    {
      "epoch": 2.638277271054823,
      "grad_norm": 1.8988099098205566,
      "learning_rate": 3.6391405643282424e-06,
      "loss": 1.1357,
      "step": 34060
    },
    {
      "epoch": 2.639051879393482,
      "grad_norm": 1.7620704174041748,
      "learning_rate": 3.6313745793424803e-06,
      "loss": 1.2174,
      "step": 34070
    },
    {
      "epoch": 2.6398264877321402,
      "grad_norm": 1.9149508476257324,
      "learning_rate": 3.6236085943567177e-06,
      "loss": 1.2488,
      "step": 34080
    },
    {
      "epoch": 2.640601096070799,
      "grad_norm": 2.0473155975341797,
      "learning_rate": 3.615842609370955e-06,
      "loss": 1.128,
      "step": 34090
    },
    {
      "epoch": 2.641375704409458,
      "grad_norm": 1.8201645612716675,
      "learning_rate": 3.608076624385193e-06,
      "loss": 1.1558,
      "step": 34100
    },
    {
      "epoch": 2.6421503127481167,
      "grad_norm": 1.5488961935043335,
      "learning_rate": 3.6003106393994305e-06,
      "loss": 1.1906,
      "step": 34110
    },
    {
      "epoch": 2.6429249210867756,
      "grad_norm": 1.7409604787826538,
      "learning_rate": 3.5925446544136683e-06,
      "loss": 1.2188,
      "step": 34120
    },
    {
      "epoch": 2.6436995294254344,
      "grad_norm": 1.5225476026535034,
      "learning_rate": 3.5847786694279058e-06,
      "loss": 1.0565,
      "step": 34130
    },
    {
      "epoch": 2.6444741377640932,
      "grad_norm": 1.647405982017517,
      "learning_rate": 3.5770126844421432e-06,
      "loss": 1.1295,
      "step": 34140
    },
    {
      "epoch": 2.6452487461027516,
      "grad_norm": 2.1917154788970947,
      "learning_rate": 3.569246699456381e-06,
      "loss": 1.1374,
      "step": 34150
    },
    {
      "epoch": 2.6460233544414105,
      "grad_norm": 2.2209815979003906,
      "learning_rate": 3.5614807144706185e-06,
      "loss": 1.2262,
      "step": 34160
    },
    {
      "epoch": 2.6467979627800693,
      "grad_norm": 1.2489203214645386,
      "learning_rate": 3.5537147294848564e-06,
      "loss": 1.1921,
      "step": 34170
    },
    {
      "epoch": 2.647572571118728,
      "grad_norm": 1.9426674842834473,
      "learning_rate": 3.545948744499094e-06,
      "loss": 1.1068,
      "step": 34180
    },
    {
      "epoch": 2.648347179457387,
      "grad_norm": 1.4180595874786377,
      "learning_rate": 3.5381827595133317e-06,
      "loss": 1.2202,
      "step": 34190
    },
    {
      "epoch": 2.649121787796046,
      "grad_norm": 1.4502607583999634,
      "learning_rate": 3.530416774527569e-06,
      "loss": 1.199,
      "step": 34200
    },
    {
      "epoch": 2.6498963961347046,
      "grad_norm": 1.6326677799224854,
      "learning_rate": 3.522650789541807e-06,
      "loss": 1.1951,
      "step": 34210
    },
    {
      "epoch": 2.650671004473363,
      "grad_norm": 1.6373960971832275,
      "learning_rate": 3.514884804556045e-06,
      "loss": 1.2414,
      "step": 34220
    },
    {
      "epoch": 2.651445612812022,
      "grad_norm": 2.0578553676605225,
      "learning_rate": 3.5071188195702823e-06,
      "loss": 1.1306,
      "step": 34230
    },
    {
      "epoch": 2.6522202211506807,
      "grad_norm": 2.2171177864074707,
      "learning_rate": 3.49935283458452e-06,
      "loss": 1.181,
      "step": 34240
    },
    {
      "epoch": 2.6529948294893395,
      "grad_norm": 1.4466779232025146,
      "learning_rate": 3.4915868495987576e-06,
      "loss": 1.1525,
      "step": 34250
    },
    {
      "epoch": 2.6537694378279983,
      "grad_norm": 1.5174068212509155,
      "learning_rate": 3.483820864612995e-06,
      "loss": 1.1981,
      "step": 34260
    },
    {
      "epoch": 2.6545440461666567,
      "grad_norm": 1.233973741531372,
      "learning_rate": 3.476054879627233e-06,
      "loss": 1.1186,
      "step": 34270
    },
    {
      "epoch": 2.655318654505316,
      "grad_norm": 1.980388879776001,
      "learning_rate": 3.4682888946414704e-06,
      "loss": 1.1009,
      "step": 34280
    },
    {
      "epoch": 2.6560932628439744,
      "grad_norm": 1.9694520235061646,
      "learning_rate": 3.4605229096557083e-06,
      "loss": 1.2197,
      "step": 34290
    },
    {
      "epoch": 2.6568678711826332,
      "grad_norm": 1.537613034248352,
      "learning_rate": 3.4527569246699457e-06,
      "loss": 1.2024,
      "step": 34300
    },
    {
      "epoch": 2.657642479521292,
      "grad_norm": 1.633433222770691,
      "learning_rate": 3.444990939684183e-06,
      "loss": 1.198,
      "step": 34310
    },
    {
      "epoch": 2.658417087859951,
      "grad_norm": 1.3394426107406616,
      "learning_rate": 3.437224954698421e-06,
      "loss": 1.1903,
      "step": 34320
    },
    {
      "epoch": 2.6591916961986097,
      "grad_norm": 1.3514199256896973,
      "learning_rate": 3.4294589697126584e-06,
      "loss": 1.1996,
      "step": 34330
    },
    {
      "epoch": 2.659966304537268,
      "grad_norm": 1.590861201286316,
      "learning_rate": 3.4216929847268963e-06,
      "loss": 1.204,
      "step": 34340
    },
    {
      "epoch": 2.660740912875927,
      "grad_norm": 1.5503795146942139,
      "learning_rate": 3.4139269997411338e-06,
      "loss": 1.212,
      "step": 34350
    },
    {
      "epoch": 2.661515521214586,
      "grad_norm": 1.40831458568573,
      "learning_rate": 3.406161014755371e-06,
      "loss": 1.2332,
      "step": 34360
    },
    {
      "epoch": 2.6622901295532446,
      "grad_norm": 1.5662436485290527,
      "learning_rate": 3.398395029769609e-06,
      "loss": 1.1143,
      "step": 34370
    },
    {
      "epoch": 2.6630647378919035,
      "grad_norm": 1.581430435180664,
      "learning_rate": 3.390629044783847e-06,
      "loss": 1.159,
      "step": 34380
    },
    {
      "epoch": 2.6638393462305623,
      "grad_norm": 1.992186427116394,
      "learning_rate": 3.382863059798085e-06,
      "loss": 1.2067,
      "step": 34390
    },
    {
      "epoch": 2.664613954569221,
      "grad_norm": 1.5182448625564575,
      "learning_rate": 3.3750970748123222e-06,
      "loss": 1.2075,
      "step": 34400
    },
    {
      "epoch": 2.6653885629078795,
      "grad_norm": 1.7710615396499634,
      "learning_rate": 3.36733108982656e-06,
      "loss": 1.1555,
      "step": 34410
    },
    {
      "epoch": 2.6661631712465383,
      "grad_norm": 1.5411628484725952,
      "learning_rate": 3.3595651048407975e-06,
      "loss": 1.0637,
      "step": 34420
    },
    {
      "epoch": 2.666937779585197,
      "grad_norm": 1.6256190538406372,
      "learning_rate": 3.351799119855035e-06,
      "loss": 1.2349,
      "step": 34430
    },
    {
      "epoch": 2.667712387923856,
      "grad_norm": 1.3013677597045898,
      "learning_rate": 3.344033134869273e-06,
      "loss": 1.1149,
      "step": 34440
    },
    {
      "epoch": 2.668486996262515,
      "grad_norm": 1.872640609741211,
      "learning_rate": 3.3362671498835103e-06,
      "loss": 1.1425,
      "step": 34450
    },
    {
      "epoch": 2.6692616046011737,
      "grad_norm": 1.3215142488479614,
      "learning_rate": 3.328501164897748e-06,
      "loss": 1.1975,
      "step": 34460
    },
    {
      "epoch": 2.6700362129398325,
      "grad_norm": 1.8663737773895264,
      "learning_rate": 3.3207351799119856e-06,
      "loss": 1.1426,
      "step": 34470
    },
    {
      "epoch": 2.670810821278491,
      "grad_norm": 1.3972066640853882,
      "learning_rate": 3.312969194926223e-06,
      "loss": 1.1699,
      "step": 34480
    },
    {
      "epoch": 2.6715854296171497,
      "grad_norm": 1.6633732318878174,
      "learning_rate": 3.305203209940461e-06,
      "loss": 1.1446,
      "step": 34490
    },
    {
      "epoch": 2.6723600379558086,
      "grad_norm": 1.6882092952728271,
      "learning_rate": 3.2974372249546984e-06,
      "loss": 1.1615,
      "step": 34500
    },
    {
      "epoch": 2.6731346462944674,
      "grad_norm": 1.8522546291351318,
      "learning_rate": 3.2896712399689362e-06,
      "loss": 1.2416,
      "step": 34510
    },
    {
      "epoch": 2.6739092546331262,
      "grad_norm": 1.7318180799484253,
      "learning_rate": 3.2819052549831737e-06,
      "loss": 1.0391,
      "step": 34520
    },
    {
      "epoch": 2.6746838629717846,
      "grad_norm": 2.3854448795318604,
      "learning_rate": 3.274139269997411e-06,
      "loss": 1.1289,
      "step": 34530
    },
    {
      "epoch": 2.675458471310444,
      "grad_norm": 1.6058591604232788,
      "learning_rate": 3.266373285011649e-06,
      "loss": 1.1025,
      "step": 34540
    },
    {
      "epoch": 2.6762330796491023,
      "grad_norm": 1.8724689483642578,
      "learning_rate": 3.2586073000258864e-06,
      "loss": 1.1797,
      "step": 34550
    },
    {
      "epoch": 2.677007687987761,
      "grad_norm": 1.5917181968688965,
      "learning_rate": 3.2508413150401243e-06,
      "loss": 1.1005,
      "step": 34560
    },
    {
      "epoch": 2.67778229632642,
      "grad_norm": 1.7742811441421509,
      "learning_rate": 3.243075330054362e-06,
      "loss": 1.1708,
      "step": 34570
    },
    {
      "epoch": 2.678556904665079,
      "grad_norm": 1.87660551071167,
      "learning_rate": 3.2353093450685996e-06,
      "loss": 1.2423,
      "step": 34580
    },
    {
      "epoch": 2.6793315130037376,
      "grad_norm": 1.6512644290924072,
      "learning_rate": 3.2275433600828375e-06,
      "loss": 1.1519,
      "step": 34590
    },
    {
      "epoch": 2.680106121342396,
      "grad_norm": 1.9921133518218994,
      "learning_rate": 3.219777375097075e-06,
      "loss": 1.1416,
      "step": 34600
    },
    {
      "epoch": 2.6808807296810553,
      "grad_norm": 2.7061643600463867,
      "learning_rate": 3.2120113901113128e-06,
      "loss": 1.1318,
      "step": 34610
    },
    {
      "epoch": 2.6816553380197137,
      "grad_norm": 1.8227583169937134,
      "learning_rate": 3.20424540512555e-06,
      "loss": 1.0959,
      "step": 34620
    },
    {
      "epoch": 2.6824299463583725,
      "grad_norm": 1.6395946741104126,
      "learning_rate": 3.1964794201397877e-06,
      "loss": 1.1407,
      "step": 34630
    },
    {
      "epoch": 2.6832045546970313,
      "grad_norm": 1.6771533489227295,
      "learning_rate": 3.1887134351540255e-06,
      "loss": 1.1462,
      "step": 34640
    },
    {
      "epoch": 2.68397916303569,
      "grad_norm": 1.480352759361267,
      "learning_rate": 3.180947450168263e-06,
      "loss": 1.2018,
      "step": 34650
    },
    {
      "epoch": 2.684753771374349,
      "grad_norm": 1.6456609964370728,
      "learning_rate": 3.173181465182501e-06,
      "loss": 1.1536,
      "step": 34660
    },
    {
      "epoch": 2.6855283797130074,
      "grad_norm": 1.312346339225769,
      "learning_rate": 3.1654154801967383e-06,
      "loss": 1.2388,
      "step": 34670
    },
    {
      "epoch": 2.6863029880516662,
      "grad_norm": 1.638371229171753,
      "learning_rate": 3.157649495210976e-06,
      "loss": 1.1335,
      "step": 34680
    },
    {
      "epoch": 2.687077596390325,
      "grad_norm": 1.5755743980407715,
      "learning_rate": 3.1498835102252136e-06,
      "loss": 1.1943,
      "step": 34690
    },
    {
      "epoch": 2.687852204728984,
      "grad_norm": 1.8345117568969727,
      "learning_rate": 3.142117525239451e-06,
      "loss": 1.1723,
      "step": 34700
    },
    {
      "epoch": 2.6886268130676427,
      "grad_norm": 1.2985817193984985,
      "learning_rate": 3.134351540253689e-06,
      "loss": 1.108,
      "step": 34710
    },
    {
      "epoch": 2.6894014214063016,
      "grad_norm": 1.8993839025497437,
      "learning_rate": 3.1265855552679263e-06,
      "loss": 1.2327,
      "step": 34720
    },
    {
      "epoch": 2.6901760297449604,
      "grad_norm": 1.4385590553283691,
      "learning_rate": 3.118819570282164e-06,
      "loss": 1.1796,
      "step": 34730
    },
    {
      "epoch": 2.690950638083619,
      "grad_norm": 1.8299611806869507,
      "learning_rate": 3.111053585296402e-06,
      "loss": 1.1204,
      "step": 34740
    },
    {
      "epoch": 2.6917252464222776,
      "grad_norm": 2.1317384243011475,
      "learning_rate": 3.1032876003106395e-06,
      "loss": 1.2362,
      "step": 34750
    },
    {
      "epoch": 2.6924998547609365,
      "grad_norm": 1.9516774415969849,
      "learning_rate": 3.0955216153248774e-06,
      "loss": 1.1002,
      "step": 34760
    },
    {
      "epoch": 2.6932744630995953,
      "grad_norm": 1.6626317501068115,
      "learning_rate": 3.087755630339115e-06,
      "loss": 1.1143,
      "step": 34770
    },
    {
      "epoch": 2.694049071438254,
      "grad_norm": 1.2912659645080566,
      "learning_rate": 3.0799896453533527e-06,
      "loss": 1.1668,
      "step": 34780
    },
    {
      "epoch": 2.694823679776913,
      "grad_norm": 1.8958401679992676,
      "learning_rate": 3.07222366036759e-06,
      "loss": 1.1352,
      "step": 34790
    },
    {
      "epoch": 2.695598288115572,
      "grad_norm": 1.3704142570495605,
      "learning_rate": 3.0644576753818276e-06,
      "loss": 1.1059,
      "step": 34800
    },
    {
      "epoch": 2.69637289645423,
      "grad_norm": 1.864841341972351,
      "learning_rate": 3.0566916903960654e-06,
      "loss": 1.1902,
      "step": 34810
    },
    {
      "epoch": 2.697147504792889,
      "grad_norm": 1.7715648412704468,
      "learning_rate": 3.048925705410303e-06,
      "loss": 1.1072,
      "step": 34820
    },
    {
      "epoch": 2.697922113131548,
      "grad_norm": 1.3588389158248901,
      "learning_rate": 3.0411597204245407e-06,
      "loss": 1.2762,
      "step": 34830
    },
    {
      "epoch": 2.6986967214702067,
      "grad_norm": 1.6420350074768066,
      "learning_rate": 3.033393735438778e-06,
      "loss": 1.1789,
      "step": 34840
    },
    {
      "epoch": 2.6994713298088655,
      "grad_norm": 1.3319851160049438,
      "learning_rate": 3.0256277504530156e-06,
      "loss": 1.3014,
      "step": 34850
    },
    {
      "epoch": 2.700245938147524,
      "grad_norm": 2.10974383354187,
      "learning_rate": 3.0178617654672535e-06,
      "loss": 1.1038,
      "step": 34860
    },
    {
      "epoch": 2.701020546486183,
      "grad_norm": 2.287019729614258,
      "learning_rate": 3.010095780481491e-06,
      "loss": 1.1896,
      "step": 34870
    },
    {
      "epoch": 2.7017951548248416,
      "grad_norm": 1.608748435974121,
      "learning_rate": 3.002329795495729e-06,
      "loss": 1.2693,
      "step": 34880
    },
    {
      "epoch": 2.7025697631635004,
      "grad_norm": 1.400153398513794,
      "learning_rate": 2.9945638105099662e-06,
      "loss": 1.1563,
      "step": 34890
    },
    {
      "epoch": 2.7033443715021592,
      "grad_norm": 2.353933334350586,
      "learning_rate": 2.9867978255242037e-06,
      "loss": 1.1526,
      "step": 34900
    },
    {
      "epoch": 2.704118979840818,
      "grad_norm": 2.0774195194244385,
      "learning_rate": 2.9790318405384416e-06,
      "loss": 1.179,
      "step": 34910
    },
    {
      "epoch": 2.704893588179477,
      "grad_norm": 1.460452914237976,
      "learning_rate": 2.9712658555526794e-06,
      "loss": 1.2587,
      "step": 34920
    },
    {
      "epoch": 2.7056681965181353,
      "grad_norm": 1.6481647491455078,
      "learning_rate": 2.9634998705669173e-06,
      "loss": 1.1228,
      "step": 34930
    },
    {
      "epoch": 2.7064428048567946,
      "grad_norm": 1.848166584968567,
      "learning_rate": 2.9557338855811547e-06,
      "loss": 1.1819,
      "step": 34940
    },
    {
      "epoch": 2.707217413195453,
      "grad_norm": 1.4683213233947754,
      "learning_rate": 2.9479679005953926e-06,
      "loss": 1.0993,
      "step": 34950
    },
    {
      "epoch": 2.707992021534112,
      "grad_norm": 1.592928171157837,
      "learning_rate": 2.94020191560963e-06,
      "loss": 1.1275,
      "step": 34960
    },
    {
      "epoch": 2.7087666298727706,
      "grad_norm": 1.4216431379318237,
      "learning_rate": 2.9324359306238675e-06,
      "loss": 1.1064,
      "step": 34970
    },
    {
      "epoch": 2.7095412382114294,
      "grad_norm": 1.279510736465454,
      "learning_rate": 2.9246699456381053e-06,
      "loss": 1.178,
      "step": 34980
    },
    {
      "epoch": 2.7103158465500883,
      "grad_norm": 1.9211540222167969,
      "learning_rate": 2.9169039606523428e-06,
      "loss": 1.2628,
      "step": 34990
    },
    {
      "epoch": 2.7110904548887467,
      "grad_norm": 2.3938066959381104,
      "learning_rate": 2.9091379756665807e-06,
      "loss": 1.1918,
      "step": 35000
    },
    {
      "epoch": 2.7118650632274055,
      "grad_norm": 1.7813740968704224,
      "learning_rate": 2.901371990680818e-06,
      "loss": 1.1952,
      "step": 35010
    },
    {
      "epoch": 2.7126396715660643,
      "grad_norm": 1.8876967430114746,
      "learning_rate": 2.8936060056950555e-06,
      "loss": 1.084,
      "step": 35020
    },
    {
      "epoch": 2.713414279904723,
      "grad_norm": 1.9464223384857178,
      "learning_rate": 2.8858400207092934e-06,
      "loss": 1.1096,
      "step": 35030
    },
    {
      "epoch": 2.714188888243382,
      "grad_norm": 1.7104982137680054,
      "learning_rate": 2.878074035723531e-06,
      "loss": 1.042,
      "step": 35040
    },
    {
      "epoch": 2.714963496582041,
      "grad_norm": 1.3614606857299805,
      "learning_rate": 2.8703080507377687e-06,
      "loss": 1.1121,
      "step": 35050
    },
    {
      "epoch": 2.7157381049206997,
      "grad_norm": 1.4415881633758545,
      "learning_rate": 2.862542065752006e-06,
      "loss": 1.1932,
      "step": 35060
    },
    {
      "epoch": 2.716512713259358,
      "grad_norm": 1.528487205505371,
      "learning_rate": 2.8547760807662436e-06,
      "loss": 1.0781,
      "step": 35070
    },
    {
      "epoch": 2.717287321598017,
      "grad_norm": 1.425190806388855,
      "learning_rate": 2.8470100957804815e-06,
      "loss": 1.1548,
      "step": 35080
    },
    {
      "epoch": 2.7180619299366757,
      "grad_norm": 1.861743450164795,
      "learning_rate": 2.839244110794719e-06,
      "loss": 1.1222,
      "step": 35090
    },
    {
      "epoch": 2.7188365382753346,
      "grad_norm": 1.290597677230835,
      "learning_rate": 2.831478125808957e-06,
      "loss": 1.1422,
      "step": 35100
    },
    {
      "epoch": 2.7196111466139934,
      "grad_norm": 1.484173059463501,
      "learning_rate": 2.8237121408231946e-06,
      "loss": 1.0624,
      "step": 35110
    },
    {
      "epoch": 2.7203857549526522,
      "grad_norm": 1.7644709348678589,
      "learning_rate": 2.815946155837432e-06,
      "loss": 1.1893,
      "step": 35120
    },
    {
      "epoch": 2.721160363291311,
      "grad_norm": 1.5619782209396362,
      "learning_rate": 2.80818017085167e-06,
      "loss": 1.2527,
      "step": 35130
    },
    {
      "epoch": 2.7219349716299694,
      "grad_norm": 1.6115909814834595,
      "learning_rate": 2.8004141858659074e-06,
      "loss": 1.1357,
      "step": 35140
    },
    {
      "epoch": 2.7227095799686283,
      "grad_norm": 1.6761425733566284,
      "learning_rate": 2.7926482008801453e-06,
      "loss": 1.2027,
      "step": 35150
    },
    {
      "epoch": 2.723484188307287,
      "grad_norm": 2.0118939876556396,
      "learning_rate": 2.7848822158943827e-06,
      "loss": 1.1695,
      "step": 35160
    },
    {
      "epoch": 2.724258796645946,
      "grad_norm": 2.116344451904297,
      "learning_rate": 2.7771162309086206e-06,
      "loss": 1.1693,
      "step": 35170
    },
    {
      "epoch": 2.725033404984605,
      "grad_norm": 1.6520920991897583,
      "learning_rate": 2.769350245922858e-06,
      "loss": 1.0714,
      "step": 35180
    },
    {
      "epoch": 2.725808013323263,
      "grad_norm": 1.4447174072265625,
      "learning_rate": 2.7615842609370955e-06,
      "loss": 1.1595,
      "step": 35190
    },
    {
      "epoch": 2.7265826216619224,
      "grad_norm": 1.3008484840393066,
      "learning_rate": 2.7538182759513333e-06,
      "loss": 1.1996,
      "step": 35200
    },
    {
      "epoch": 2.727357230000581,
      "grad_norm": 2.0081918239593506,
      "learning_rate": 2.7460522909655708e-06,
      "loss": 1.2361,
      "step": 35210
    },
    {
      "epoch": 2.7281318383392397,
      "grad_norm": 1.5538296699523926,
      "learning_rate": 2.7382863059798086e-06,
      "loss": 1.3297,
      "step": 35220
    },
    {
      "epoch": 2.7289064466778985,
      "grad_norm": 1.7344834804534912,
      "learning_rate": 2.730520320994046e-06,
      "loss": 1.2951,
      "step": 35230
    },
    {
      "epoch": 2.7296810550165573,
      "grad_norm": 1.4414616823196411,
      "learning_rate": 2.7227543360082835e-06,
      "loss": 1.2166,
      "step": 35240
    },
    {
      "epoch": 2.730455663355216,
      "grad_norm": 1.2009717226028442,
      "learning_rate": 2.7149883510225214e-06,
      "loss": 1.2085,
      "step": 35250
    },
    {
      "epoch": 2.7312302716938746,
      "grad_norm": 2.2555158138275146,
      "learning_rate": 2.707222366036759e-06,
      "loss": 1.208,
      "step": 35260
    },
    {
      "epoch": 2.7320048800325334,
      "grad_norm": 2.1813220977783203,
      "learning_rate": 2.6994563810509967e-06,
      "loss": 1.188,
      "step": 35270
    },
    {
      "epoch": 2.7327794883711922,
      "grad_norm": 1.5879994630813599,
      "learning_rate": 2.6916903960652346e-06,
      "loss": 1.2545,
      "step": 35280
    },
    {
      "epoch": 2.733554096709851,
      "grad_norm": 1.5679209232330322,
      "learning_rate": 2.683924411079472e-06,
      "loss": 1.0917,
      "step": 35290
    },
    {
      "epoch": 2.73432870504851,
      "grad_norm": 1.7733595371246338,
      "learning_rate": 2.67615842609371e-06,
      "loss": 1.1881,
      "step": 35300
    },
    {
      "epoch": 2.7351033133871687,
      "grad_norm": 1.6716662645339966,
      "learning_rate": 2.6683924411079473e-06,
      "loss": 1.0957,
      "step": 35310
    },
    {
      "epoch": 2.7358779217258276,
      "grad_norm": 1.5797826051712036,
      "learning_rate": 2.660626456122185e-06,
      "loss": 1.2065,
      "step": 35320
    },
    {
      "epoch": 2.736652530064486,
      "grad_norm": 1.8855535984039307,
      "learning_rate": 2.6528604711364226e-06,
      "loss": 1.3073,
      "step": 35330
    },
    {
      "epoch": 2.7374271384031448,
      "grad_norm": 1.512294054031372,
      "learning_rate": 2.64509448615066e-06,
      "loss": 1.1852,
      "step": 35340
    },
    {
      "epoch": 2.7382017467418036,
      "grad_norm": 2.049058198928833,
      "learning_rate": 2.637328501164898e-06,
      "loss": 1.1117,
      "step": 35350
    },
    {
      "epoch": 2.7389763550804624,
      "grad_norm": 1.6420375108718872,
      "learning_rate": 2.6295625161791354e-06,
      "loss": 1.1092,
      "step": 35360
    },
    {
      "epoch": 2.7397509634191213,
      "grad_norm": 1.4270296096801758,
      "learning_rate": 2.6217965311933732e-06,
      "loss": 1.1639,
      "step": 35370
    },
    {
      "epoch": 2.74052557175778,
      "grad_norm": 1.6081913709640503,
      "learning_rate": 2.6140305462076107e-06,
      "loss": 1.1352,
      "step": 35380
    },
    {
      "epoch": 2.741300180096439,
      "grad_norm": 1.8408578634262085,
      "learning_rate": 2.606264561221848e-06,
      "loss": 1.2432,
      "step": 35390
    },
    {
      "epoch": 2.7420747884350973,
      "grad_norm": 2.0557773113250732,
      "learning_rate": 2.598498576236086e-06,
      "loss": 1.2805,
      "step": 35400
    },
    {
      "epoch": 2.742849396773756,
      "grad_norm": 1.3974403142929077,
      "learning_rate": 2.5907325912503234e-06,
      "loss": 1.1982,
      "step": 35410
    },
    {
      "epoch": 2.743624005112415,
      "grad_norm": 1.6094129085540771,
      "learning_rate": 2.5829666062645613e-06,
      "loss": 1.1517,
      "step": 35420
    },
    {
      "epoch": 2.744398613451074,
      "grad_norm": 1.8000109195709229,
      "learning_rate": 2.5752006212787987e-06,
      "loss": 1.0893,
      "step": 35430
    },
    {
      "epoch": 2.7451732217897327,
      "grad_norm": 1.6351126432418823,
      "learning_rate": 2.5674346362930366e-06,
      "loss": 1.14,
      "step": 35440
    },
    {
      "epoch": 2.7459478301283915,
      "grad_norm": 1.8561055660247803,
      "learning_rate": 2.559668651307274e-06,
      "loss": 1.0898,
      "step": 35450
    },
    {
      "epoch": 2.7467224384670503,
      "grad_norm": 1.9908905029296875,
      "learning_rate": 2.551902666321512e-06,
      "loss": 1.1189,
      "step": 35460
    },
    {
      "epoch": 2.7474970468057087,
      "grad_norm": 1.2759188413619995,
      "learning_rate": 2.5441366813357498e-06,
      "loss": 1.2147,
      "step": 35470
    },
    {
      "epoch": 2.7482716551443676,
      "grad_norm": 2.4733593463897705,
      "learning_rate": 2.5363706963499872e-06,
      "loss": 1.1229,
      "step": 35480
    },
    {
      "epoch": 2.7490462634830264,
      "grad_norm": 1.3631110191345215,
      "learning_rate": 2.528604711364225e-06,
      "loss": 1.1587,
      "step": 35490
    },
    {
      "epoch": 2.749820871821685,
      "grad_norm": 1.2863270044326782,
      "learning_rate": 2.5208387263784625e-06,
      "loss": 1.0676,
      "step": 35500
    },
    {
      "epoch": 2.750595480160344,
      "grad_norm": 1.576936960220337,
      "learning_rate": 2.5130727413927e-06,
      "loss": 1.1084,
      "step": 35510
    },
    {
      "epoch": 2.7513700884990024,
      "grad_norm": 1.416824460029602,
      "learning_rate": 2.505306756406938e-06,
      "loss": 1.1734,
      "step": 35520
    },
    {
      "epoch": 2.7521446968376617,
      "grad_norm": 1.510402798652649,
      "learning_rate": 2.4975407714211753e-06,
      "loss": 1.1715,
      "step": 35530
    },
    {
      "epoch": 2.75291930517632,
      "grad_norm": 1.5387656688690186,
      "learning_rate": 2.489774786435413e-06,
      "loss": 1.093,
      "step": 35540
    },
    {
      "epoch": 2.753693913514979,
      "grad_norm": 1.5173968076705933,
      "learning_rate": 2.4820088014496506e-06,
      "loss": 1.1456,
      "step": 35550
    },
    {
      "epoch": 2.7544685218536378,
      "grad_norm": 1.682227373123169,
      "learning_rate": 2.474242816463888e-06,
      "loss": 1.1167,
      "step": 35560
    },
    {
      "epoch": 2.7552431301922966,
      "grad_norm": 2.0224227905273438,
      "learning_rate": 2.466476831478126e-06,
      "loss": 1.1784,
      "step": 35570
    },
    {
      "epoch": 2.7560177385309554,
      "grad_norm": 1.6661418676376343,
      "learning_rate": 2.4587108464923633e-06,
      "loss": 1.2071,
      "step": 35580
    },
    {
      "epoch": 2.756792346869614,
      "grad_norm": 1.7830380201339722,
      "learning_rate": 2.450944861506601e-06,
      "loss": 1.22,
      "step": 35590
    },
    {
      "epoch": 2.7575669552082727,
      "grad_norm": 1.545670747756958,
      "learning_rate": 2.4431788765208386e-06,
      "loss": 1.2269,
      "step": 35600
    },
    {
      "epoch": 2.7583415635469315,
      "grad_norm": 1.5676292181015015,
      "learning_rate": 2.435412891535076e-06,
      "loss": 1.313,
      "step": 35610
    },
    {
      "epoch": 2.7591161718855903,
      "grad_norm": 1.5272458791732788,
      "learning_rate": 2.427646906549314e-06,
      "loss": 1.1935,
      "step": 35620
    },
    {
      "epoch": 2.759890780224249,
      "grad_norm": 1.8172807693481445,
      "learning_rate": 2.4198809215635514e-06,
      "loss": 1.17,
      "step": 35630
    },
    {
      "epoch": 2.760665388562908,
      "grad_norm": 1.6656193733215332,
      "learning_rate": 2.4121149365777897e-06,
      "loss": 1.1554,
      "step": 35640
    },
    {
      "epoch": 2.761439996901567,
      "grad_norm": 1.5711265802383423,
      "learning_rate": 2.404348951592027e-06,
      "loss": 1.1483,
      "step": 35650
    },
    {
      "epoch": 2.762214605240225,
      "grad_norm": 1.7593315839767456,
      "learning_rate": 2.3965829666062646e-06,
      "loss": 1.1645,
      "step": 35660
    },
    {
      "epoch": 2.762989213578884,
      "grad_norm": 2.073286533355713,
      "learning_rate": 2.3888169816205024e-06,
      "loss": 1.2638,
      "step": 35670
    },
    {
      "epoch": 2.763763821917543,
      "grad_norm": 1.270615577697754,
      "learning_rate": 2.38105099663474e-06,
      "loss": 1.1789,
      "step": 35680
    },
    {
      "epoch": 2.7645384302562017,
      "grad_norm": 1.4846121072769165,
      "learning_rate": 2.3732850116489777e-06,
      "loss": 1.0833,
      "step": 35690
    },
    {
      "epoch": 2.7653130385948606,
      "grad_norm": 2.1722805500030518,
      "learning_rate": 2.365519026663215e-06,
      "loss": 1.1493,
      "step": 35700
    },
    {
      "epoch": 2.7660876469335194,
      "grad_norm": 1.9123331308364868,
      "learning_rate": 2.357753041677453e-06,
      "loss": 1.2135,
      "step": 35710
    },
    {
      "epoch": 2.766862255272178,
      "grad_norm": 1.7606220245361328,
      "learning_rate": 2.3507636551902665e-06,
      "loss": 1.1086,
      "step": 35720
    },
    {
      "epoch": 2.7676368636108366,
      "grad_norm": 1.580189824104309,
      "learning_rate": 2.3429976702045043e-06,
      "loss": 1.2043,
      "step": 35730
    },
    {
      "epoch": 2.7684114719494954,
      "grad_norm": 1.2497400045394897,
      "learning_rate": 2.3352316852187418e-06,
      "loss": 1.1977,
      "step": 35740
    },
    {
      "epoch": 2.7691860802881543,
      "grad_norm": 1.5460339784622192,
      "learning_rate": 2.3274657002329796e-06,
      "loss": 1.2743,
      "step": 35750
    },
    {
      "epoch": 2.769960688626813,
      "grad_norm": 1.8997570276260376,
      "learning_rate": 2.319699715247217e-06,
      "loss": 1.2387,
      "step": 35760
    },
    {
      "epoch": 2.770735296965472,
      "grad_norm": 1.9728517532348633,
      "learning_rate": 2.311933730261455e-06,
      "loss": 1.1675,
      "step": 35770
    },
    {
      "epoch": 2.7715099053041303,
      "grad_norm": 1.747209072113037,
      "learning_rate": 2.304167745275693e-06,
      "loss": 1.1745,
      "step": 35780
    },
    {
      "epoch": 2.7722845136427896,
      "grad_norm": 1.5147006511688232,
      "learning_rate": 2.2964017602899302e-06,
      "loss": 1.2407,
      "step": 35790
    },
    {
      "epoch": 2.773059121981448,
      "grad_norm": 1.9167894124984741,
      "learning_rate": 2.288635775304168e-06,
      "loss": 1.2264,
      "step": 35800
    },
    {
      "epoch": 2.773833730320107,
      "grad_norm": 1.4736294746398926,
      "learning_rate": 2.2808697903184056e-06,
      "loss": 1.1408,
      "step": 35810
    },
    {
      "epoch": 2.7746083386587657,
      "grad_norm": 1.4731273651123047,
      "learning_rate": 2.273103805332643e-06,
      "loss": 1.2353,
      "step": 35820
    },
    {
      "epoch": 2.7753829469974245,
      "grad_norm": 2.3719284534454346,
      "learning_rate": 2.265337820346881e-06,
      "loss": 1.0685,
      "step": 35830
    },
    {
      "epoch": 2.7761575553360833,
      "grad_norm": 1.4379552602767944,
      "learning_rate": 2.2575718353611183e-06,
      "loss": 1.1553,
      "step": 35840
    },
    {
      "epoch": 2.7769321636747417,
      "grad_norm": 1.5180147886276245,
      "learning_rate": 2.249805850375356e-06,
      "loss": 1.1697,
      "step": 35850
    },
    {
      "epoch": 2.777706772013401,
      "grad_norm": 2.960352659225464,
      "learning_rate": 2.2420398653895936e-06,
      "loss": 1.2051,
      "step": 35860
    },
    {
      "epoch": 2.7784813803520594,
      "grad_norm": 1.7006912231445312,
      "learning_rate": 2.234273880403831e-06,
      "loss": 1.1472,
      "step": 35870
    },
    {
      "epoch": 2.779255988690718,
      "grad_norm": 2.3301305770874023,
      "learning_rate": 2.226507895418069e-06,
      "loss": 1.2387,
      "step": 35880
    },
    {
      "epoch": 2.780030597029377,
      "grad_norm": 1.535236120223999,
      "learning_rate": 2.2187419104323064e-06,
      "loss": 1.2051,
      "step": 35890
    },
    {
      "epoch": 2.780805205368036,
      "grad_norm": 1.3734182119369507,
      "learning_rate": 2.2109759254465442e-06,
      "loss": 1.1789,
      "step": 35900
    },
    {
      "epoch": 2.7815798137066947,
      "grad_norm": 1.4935142993927002,
      "learning_rate": 2.2032099404607817e-06,
      "loss": 1.1485,
      "step": 35910
    },
    {
      "epoch": 2.782354422045353,
      "grad_norm": 1.4472103118896484,
      "learning_rate": 2.195443955475019e-06,
      "loss": 1.1489,
      "step": 35920
    },
    {
      "epoch": 2.783129030384012,
      "grad_norm": 1.8079575300216675,
      "learning_rate": 2.187677970489257e-06,
      "loss": 1.2361,
      "step": 35930
    },
    {
      "epoch": 2.7839036387226708,
      "grad_norm": 1.5253287553787231,
      "learning_rate": 2.1799119855034944e-06,
      "loss": 1.1969,
      "step": 35940
    },
    {
      "epoch": 2.7846782470613296,
      "grad_norm": 1.5799118280410767,
      "learning_rate": 2.1721460005177327e-06,
      "loss": 1.2091,
      "step": 35950
    },
    {
      "epoch": 2.7854528553999884,
      "grad_norm": 1.5940903425216675,
      "learning_rate": 2.16438001553197e-06,
      "loss": 1.1502,
      "step": 35960
    },
    {
      "epoch": 2.7862274637386473,
      "grad_norm": 1.8904606103897095,
      "learning_rate": 2.156614030546208e-06,
      "loss": 1.1612,
      "step": 35970
    },
    {
      "epoch": 2.787002072077306,
      "grad_norm": 1.5921574831008911,
      "learning_rate": 2.1488480455604455e-06,
      "loss": 1.2158,
      "step": 35980
    },
    {
      "epoch": 2.7877766804159645,
      "grad_norm": 1.565631628036499,
      "learning_rate": 2.141082060574683e-06,
      "loss": 1.1613,
      "step": 35990
    },
    {
      "epoch": 2.7885512887546233,
      "grad_norm": 1.793691873550415,
      "learning_rate": 2.1333160755889208e-06,
      "loss": 1.2161,
      "step": 36000
    },
    {
      "epoch": 2.789325897093282,
      "grad_norm": 1.9473172426223755,
      "learning_rate": 2.1255500906031582e-06,
      "loss": 1.2075,
      "step": 36010
    },
    {
      "epoch": 2.790100505431941,
      "grad_norm": 1.4777580499649048,
      "learning_rate": 2.117784105617396e-06,
      "loss": 1.1302,
      "step": 36020
    },
    {
      "epoch": 2.7908751137706,
      "grad_norm": 2.1866869926452637,
      "learning_rate": 2.1100181206316335e-06,
      "loss": 1.1055,
      "step": 36030
    },
    {
      "epoch": 2.7916497221092587,
      "grad_norm": 1.6944434642791748,
      "learning_rate": 2.102252135645871e-06,
      "loss": 1.2615,
      "step": 36040
    },
    {
      "epoch": 2.7924243304479175,
      "grad_norm": 1.953728199005127,
      "learning_rate": 2.094486150660109e-06,
      "loss": 1.1231,
      "step": 36050
    },
    {
      "epoch": 2.793198938786576,
      "grad_norm": 1.431085467338562,
      "learning_rate": 2.0867201656743463e-06,
      "loss": 1.1762,
      "step": 36060
    },
    {
      "epoch": 2.7939735471252347,
      "grad_norm": 1.0598772764205933,
      "learning_rate": 2.078954180688584e-06,
      "loss": 1.0626,
      "step": 36070
    },
    {
      "epoch": 2.7947481554638935,
      "grad_norm": 1.7301069498062134,
      "learning_rate": 2.0711881957028216e-06,
      "loss": 1.2231,
      "step": 36080
    },
    {
      "epoch": 2.7955227638025524,
      "grad_norm": 1.724772334098816,
      "learning_rate": 2.063422210717059e-06,
      "loss": 1.1864,
      "step": 36090
    },
    {
      "epoch": 2.796297372141211,
      "grad_norm": 1.5791305303573608,
      "learning_rate": 2.055656225731297e-06,
      "loss": 1.2848,
      "step": 36100
    },
    {
      "epoch": 2.7970719804798696,
      "grad_norm": 1.7578155994415283,
      "learning_rate": 2.0478902407455343e-06,
      "loss": 1.0899,
      "step": 36110
    },
    {
      "epoch": 2.797846588818529,
      "grad_norm": 1.7444696426391602,
      "learning_rate": 2.040124255759772e-06,
      "loss": 1.1537,
      "step": 36120
    },
    {
      "epoch": 2.7986211971571873,
      "grad_norm": 1.956423044204712,
      "learning_rate": 2.03235827077401e-06,
      "loss": 1.2084,
      "step": 36130
    },
    {
      "epoch": 2.799395805495846,
      "grad_norm": 1.6659523248672485,
      "learning_rate": 2.0245922857882475e-06,
      "loss": 1.1484,
      "step": 36140
    },
    {
      "epoch": 2.800170413834505,
      "grad_norm": 1.4673547744750977,
      "learning_rate": 2.0168263008024854e-06,
      "loss": 1.2239,
      "step": 36150
    },
    {
      "epoch": 2.8009450221731638,
      "grad_norm": 1.847517490386963,
      "learning_rate": 2.009060315816723e-06,
      "loss": 1.1963,
      "step": 36160
    },
    {
      "epoch": 2.8017196305118226,
      "grad_norm": 1.7723017930984497,
      "learning_rate": 2.0012943308309607e-06,
      "loss": 1.2158,
      "step": 36170
    },
    {
      "epoch": 2.802494238850481,
      "grad_norm": 1.5387980937957764,
      "learning_rate": 1.993528345845198e-06,
      "loss": 1.1602,
      "step": 36180
    },
    {
      "epoch": 2.8032688471891403,
      "grad_norm": 2.452927827835083,
      "learning_rate": 1.9857623608594356e-06,
      "loss": 1.2487,
      "step": 36190
    },
    {
      "epoch": 2.8040434555277987,
      "grad_norm": 1.593733549118042,
      "learning_rate": 1.9779963758736734e-06,
      "loss": 1.2299,
      "step": 36200
    },
    {
      "epoch": 2.8048180638664575,
      "grad_norm": 1.5380213260650635,
      "learning_rate": 1.970230390887911e-06,
      "loss": 1.1799,
      "step": 36210
    },
    {
      "epoch": 2.8055926722051163,
      "grad_norm": 1.6547126770019531,
      "learning_rate": 1.9624644059021487e-06,
      "loss": 1.1929,
      "step": 36220
    },
    {
      "epoch": 2.806367280543775,
      "grad_norm": 2.020570755004883,
      "learning_rate": 1.954698420916386e-06,
      "loss": 1.2003,
      "step": 36230
    },
    {
      "epoch": 2.807141888882434,
      "grad_norm": 1.975539207458496,
      "learning_rate": 1.946932435930624e-06,
      "loss": 1.2669,
      "step": 36240
    },
    {
      "epoch": 2.8079164972210924,
      "grad_norm": 1.6029685735702515,
      "learning_rate": 1.9391664509448615e-06,
      "loss": 1.2385,
      "step": 36250
    },
    {
      "epoch": 2.808691105559751,
      "grad_norm": 1.346092939376831,
      "learning_rate": 1.931400465959099e-06,
      "loss": 1.1796,
      "step": 36260
    },
    {
      "epoch": 2.80946571389841,
      "grad_norm": 1.5041649341583252,
      "learning_rate": 1.923634480973337e-06,
      "loss": 1.2068,
      "step": 36270
    },
    {
      "epoch": 2.810240322237069,
      "grad_norm": 1.9647704362869263,
      "learning_rate": 1.9158684959875743e-06,
      "loss": 1.1647,
      "step": 36280
    },
    {
      "epoch": 2.8110149305757277,
      "grad_norm": 1.7691024541854858,
      "learning_rate": 1.908102511001812e-06,
      "loss": 1.1564,
      "step": 36290
    },
    {
      "epoch": 2.8117895389143865,
      "grad_norm": 1.316691517829895,
      "learning_rate": 1.9003365260160498e-06,
      "loss": 1.0756,
      "step": 36300
    },
    {
      "epoch": 2.8125641472530454,
      "grad_norm": 1.7632249593734741,
      "learning_rate": 1.8925705410302872e-06,
      "loss": 1.2179,
      "step": 36310
    },
    {
      "epoch": 2.8133387555917038,
      "grad_norm": 1.5367918014526367,
      "learning_rate": 1.884804556044525e-06,
      "loss": 1.1655,
      "step": 36320
    },
    {
      "epoch": 2.8141133639303626,
      "grad_norm": 2.2537288665771484,
      "learning_rate": 1.8770385710587625e-06,
      "loss": 1.2304,
      "step": 36330
    },
    {
      "epoch": 2.8148879722690214,
      "grad_norm": 1.329268455505371,
      "learning_rate": 1.8692725860730002e-06,
      "loss": 1.2097,
      "step": 36340
    },
    {
      "epoch": 2.8156625806076803,
      "grad_norm": 2.649291515350342,
      "learning_rate": 1.861506601087238e-06,
      "loss": 1.1793,
      "step": 36350
    },
    {
      "epoch": 2.816437188946339,
      "grad_norm": 1.3650915622711182,
      "learning_rate": 1.8537406161014757e-06,
      "loss": 1.0949,
      "step": 36360
    },
    {
      "epoch": 2.817211797284998,
      "grad_norm": 1.649888277053833,
      "learning_rate": 1.8459746311157134e-06,
      "loss": 1.128,
      "step": 36370
    },
    {
      "epoch": 2.8179864056236568,
      "grad_norm": 1.6930636167526245,
      "learning_rate": 1.8382086461299508e-06,
      "loss": 1.2098,
      "step": 36380
    },
    {
      "epoch": 2.818761013962315,
      "grad_norm": 1.3971545696258545,
      "learning_rate": 1.8304426611441884e-06,
      "loss": 1.2374,
      "step": 36390
    },
    {
      "epoch": 2.819535622300974,
      "grad_norm": 3.2412478923797607,
      "learning_rate": 1.822676676158426e-06,
      "loss": 1.1824,
      "step": 36400
    },
    {
      "epoch": 2.820310230639633,
      "grad_norm": 1.5749777555465698,
      "learning_rate": 1.8149106911726638e-06,
      "loss": 1.1934,
      "step": 36410
    },
    {
      "epoch": 2.8210848389782917,
      "grad_norm": 1.448175311088562,
      "learning_rate": 1.8071447061869014e-06,
      "loss": 1.0629,
      "step": 36420
    },
    {
      "epoch": 2.8218594473169505,
      "grad_norm": 1.6941757202148438,
      "learning_rate": 1.799378721201139e-06,
      "loss": 1.2429,
      "step": 36430
    },
    {
      "epoch": 2.822634055655609,
      "grad_norm": 1.6192883253097534,
      "learning_rate": 1.7916127362153767e-06,
      "loss": 1.2272,
      "step": 36440
    },
    {
      "epoch": 2.823408663994268,
      "grad_norm": 1.536802887916565,
      "learning_rate": 1.7838467512296144e-06,
      "loss": 1.16,
      "step": 36450
    },
    {
      "epoch": 2.8241832723329265,
      "grad_norm": 2.183119773864746,
      "learning_rate": 1.776080766243852e-06,
      "loss": 1.2088,
      "step": 36460
    },
    {
      "epoch": 2.8249578806715854,
      "grad_norm": 1.5805555582046509,
      "learning_rate": 1.7683147812580897e-06,
      "loss": 1.1751,
      "step": 36470
    },
    {
      "epoch": 2.825732489010244,
      "grad_norm": 1.6448094844818115,
      "learning_rate": 1.7605487962723273e-06,
      "loss": 1.1176,
      "step": 36480
    },
    {
      "epoch": 2.826507097348903,
      "grad_norm": 1.8572003841400146,
      "learning_rate": 1.7527828112865648e-06,
      "loss": 1.2413,
      "step": 36490
    },
    {
      "epoch": 2.827281705687562,
      "grad_norm": 1.4070745706558228,
      "learning_rate": 1.7450168263008024e-06,
      "loss": 1.1129,
      "step": 36500
    },
    {
      "epoch": 2.8280563140262203,
      "grad_norm": 1.4839351177215576,
      "learning_rate": 1.73725084131504e-06,
      "loss": 1.0656,
      "step": 36510
    },
    {
      "epoch": 2.828830922364879,
      "grad_norm": 1.6413730382919312,
      "learning_rate": 1.7294848563292777e-06,
      "loss": 1.2646,
      "step": 36520
    },
    {
      "epoch": 2.829605530703538,
      "grad_norm": 1.8602575063705444,
      "learning_rate": 1.7217188713435156e-06,
      "loss": 1.1618,
      "step": 36530
    },
    {
      "epoch": 2.8303801390421968,
      "grad_norm": 1.688112497329712,
      "learning_rate": 1.7139528863577533e-06,
      "loss": 1.183,
      "step": 36540
    },
    {
      "epoch": 2.8311547473808556,
      "grad_norm": 2.1602325439453125,
      "learning_rate": 1.7061869013719907e-06,
      "loss": 1.1995,
      "step": 36550
    },
    {
      "epoch": 2.8319293557195144,
      "grad_norm": 1.572177767753601,
      "learning_rate": 1.6984209163862284e-06,
      "loss": 1.2086,
      "step": 36560
    },
    {
      "epoch": 2.8327039640581733,
      "grad_norm": 1.5160166025161743,
      "learning_rate": 1.690654931400466e-06,
      "loss": 1.1591,
      "step": 36570
    },
    {
      "epoch": 2.8334785723968317,
      "grad_norm": 1.756299614906311,
      "learning_rate": 1.6828889464147037e-06,
      "loss": 1.021,
      "step": 36580
    },
    {
      "epoch": 2.8342531807354905,
      "grad_norm": 1.485985279083252,
      "learning_rate": 1.6751229614289413e-06,
      "loss": 1.1694,
      "step": 36590
    },
    {
      "epoch": 2.8350277890741493,
      "grad_norm": 1.1922687292099,
      "learning_rate": 1.6673569764431788e-06,
      "loss": 1.2248,
      "step": 36600
    },
    {
      "epoch": 2.835802397412808,
      "grad_norm": 1.508609414100647,
      "learning_rate": 1.6595909914574164e-06,
      "loss": 1.2072,
      "step": 36610
    },
    {
      "epoch": 2.836577005751467,
      "grad_norm": 2.009524345397949,
      "learning_rate": 1.6518250064716543e-06,
      "loss": 1.2232,
      "step": 36620
    },
    {
      "epoch": 2.837351614090126,
      "grad_norm": 1.4974530935287476,
      "learning_rate": 1.644059021485892e-06,
      "loss": 1.1945,
      "step": 36630
    },
    {
      "epoch": 2.8381262224287847,
      "grad_norm": 2.269519805908203,
      "learning_rate": 1.6362930365001296e-06,
      "loss": 1.1602,
      "step": 36640
    },
    {
      "epoch": 2.838900830767443,
      "grad_norm": 2.5199177265167236,
      "learning_rate": 1.628527051514367e-06,
      "loss": 1.1849,
      "step": 36650
    },
    {
      "epoch": 2.839675439106102,
      "grad_norm": 1.4195159673690796,
      "learning_rate": 1.6207610665286047e-06,
      "loss": 1.1253,
      "step": 36660
    },
    {
      "epoch": 2.8404500474447607,
      "grad_norm": 1.9004899263381958,
      "learning_rate": 1.6129950815428423e-06,
      "loss": 1.2795,
      "step": 36670
    },
    {
      "epoch": 2.8412246557834195,
      "grad_norm": 1.6384291648864746,
      "learning_rate": 1.60522909655708e-06,
      "loss": 1.1217,
      "step": 36680
    },
    {
      "epoch": 2.8419992641220784,
      "grad_norm": 2.004458427429199,
      "learning_rate": 1.5974631115713177e-06,
      "loss": 1.2236,
      "step": 36690
    },
    {
      "epoch": 2.842773872460737,
      "grad_norm": 1.3455888032913208,
      "learning_rate": 1.5896971265855553e-06,
      "loss": 1.2282,
      "step": 36700
    },
    {
      "epoch": 2.843548480799396,
      "grad_norm": 1.501383662223816,
      "learning_rate": 1.581931141599793e-06,
      "loss": 1.1653,
      "step": 36710
    },
    {
      "epoch": 2.8443230891380544,
      "grad_norm": 1.6346116065979004,
      "learning_rate": 1.5741651566140306e-06,
      "loss": 1.0824,
      "step": 36720
    },
    {
      "epoch": 2.8450976974767133,
      "grad_norm": 1.5857322216033936,
      "learning_rate": 1.5663991716282683e-06,
      "loss": 1.17,
      "step": 36730
    },
    {
      "epoch": 2.845872305815372,
      "grad_norm": 1.4666122198104858,
      "learning_rate": 1.558633186642506e-06,
      "loss": 1.1956,
      "step": 36740
    },
    {
      "epoch": 2.846646914154031,
      "grad_norm": 2.7768540382385254,
      "learning_rate": 1.5508672016567436e-06,
      "loss": 1.2313,
      "step": 36750
    },
    {
      "epoch": 2.8474215224926898,
      "grad_norm": 1.3745501041412354,
      "learning_rate": 1.543101216670981e-06,
      "loss": 1.1642,
      "step": 36760
    },
    {
      "epoch": 2.848196130831348,
      "grad_norm": 1.6607297658920288,
      "learning_rate": 1.5353352316852187e-06,
      "loss": 1.1701,
      "step": 36770
    },
    {
      "epoch": 2.8489707391700074,
      "grad_norm": 1.494235873222351,
      "learning_rate": 1.5275692466994563e-06,
      "loss": 1.1958,
      "step": 36780
    },
    {
      "epoch": 2.849745347508666,
      "grad_norm": 1.4160139560699463,
      "learning_rate": 1.519803261713694e-06,
      "loss": 1.1192,
      "step": 36790
    },
    {
      "epoch": 2.8505199558473246,
      "grad_norm": 1.928147792816162,
      "learning_rate": 1.5120372767279319e-06,
      "loss": 1.1615,
      "step": 36800
    },
    {
      "epoch": 2.8512945641859835,
      "grad_norm": 1.5826915502548218,
      "learning_rate": 1.5042712917421695e-06,
      "loss": 1.173,
      "step": 36810
    },
    {
      "epoch": 2.8520691725246423,
      "grad_norm": 1.3289940357208252,
      "learning_rate": 1.496505306756407e-06,
      "loss": 1.186,
      "step": 36820
    },
    {
      "epoch": 2.852843780863301,
      "grad_norm": 1.4597957134246826,
      "learning_rate": 1.4887393217706446e-06,
      "loss": 1.2128,
      "step": 36830
    },
    {
      "epoch": 2.8536183892019595,
      "grad_norm": 1.205390214920044,
      "learning_rate": 1.4809733367848823e-06,
      "loss": 1.1515,
      "step": 36840
    },
    {
      "epoch": 2.8543929975406184,
      "grad_norm": 2.1360960006713867,
      "learning_rate": 1.47320735179912e-06,
      "loss": 1.1803,
      "step": 36850
    },
    {
      "epoch": 2.855167605879277,
      "grad_norm": 2.1406779289245605,
      "learning_rate": 1.4654413668133576e-06,
      "loss": 1.2237,
      "step": 36860
    },
    {
      "epoch": 2.855942214217936,
      "grad_norm": 1.6121433973312378,
      "learning_rate": 1.457675381827595e-06,
      "loss": 1.1262,
      "step": 36870
    },
    {
      "epoch": 2.856716822556595,
      "grad_norm": 1.2533302307128906,
      "learning_rate": 1.4499093968418327e-06,
      "loss": 1.322,
      "step": 36880
    },
    {
      "epoch": 2.8574914308952537,
      "grad_norm": 1.5517297983169556,
      "learning_rate": 1.4421434118560705e-06,
      "loss": 1.1299,
      "step": 36890
    },
    {
      "epoch": 2.8582660392339125,
      "grad_norm": 1.629604697227478,
      "learning_rate": 1.4343774268703082e-06,
      "loss": 1.1512,
      "step": 36900
    },
    {
      "epoch": 2.859040647572571,
      "grad_norm": 1.5467712879180908,
      "learning_rate": 1.4266114418845458e-06,
      "loss": 1.2253,
      "step": 36910
    },
    {
      "epoch": 2.8598152559112298,
      "grad_norm": 1.5193631649017334,
      "learning_rate": 1.4188454568987835e-06,
      "loss": 1.2384,
      "step": 36920
    },
    {
      "epoch": 2.8605898642498886,
      "grad_norm": 1.7188893556594849,
      "learning_rate": 1.411079471913021e-06,
      "loss": 1.1819,
      "step": 36930
    },
    {
      "epoch": 2.8613644725885474,
      "grad_norm": 1.6874994039535522,
      "learning_rate": 1.4033134869272586e-06,
      "loss": 1.222,
      "step": 36940
    },
    {
      "epoch": 2.8621390809272063,
      "grad_norm": 1.9850716590881348,
      "learning_rate": 1.3955475019414962e-06,
      "loss": 1.1753,
      "step": 36950
    },
    {
      "epoch": 2.862913689265865,
      "grad_norm": 1.8560583591461182,
      "learning_rate": 1.387781516955734e-06,
      "loss": 1.1465,
      "step": 36960
    },
    {
      "epoch": 2.863688297604524,
      "grad_norm": 1.90372633934021,
      "learning_rate": 1.3800155319699716e-06,
      "loss": 1.2266,
      "step": 36970
    },
    {
      "epoch": 2.8644629059431823,
      "grad_norm": 1.6869128942489624,
      "learning_rate": 1.3722495469842092e-06,
      "loss": 1.1759,
      "step": 36980
    },
    {
      "epoch": 2.865237514281841,
      "grad_norm": 1.5457074642181396,
      "learning_rate": 1.3644835619984469e-06,
      "loss": 1.1877,
      "step": 36990
    },
    {
      "epoch": 2.8660121226205,
      "grad_norm": 1.5848665237426758,
      "learning_rate": 1.3567175770126845e-06,
      "loss": 1.1456,
      "step": 37000
    },
    {
      "epoch": 2.866786730959159,
      "grad_norm": 1.3704659938812256,
      "learning_rate": 1.3489515920269222e-06,
      "loss": 1.0296,
      "step": 37010
    },
    {
      "epoch": 2.8675613392978176,
      "grad_norm": 1.6434621810913086,
      "learning_rate": 1.3411856070411598e-06,
      "loss": 1.2134,
      "step": 37020
    },
    {
      "epoch": 2.868335947636476,
      "grad_norm": 2.0763092041015625,
      "learning_rate": 1.3334196220553975e-06,
      "loss": 1.1979,
      "step": 37030
    },
    {
      "epoch": 2.8691105559751353,
      "grad_norm": 1.9156478643417358,
      "learning_rate": 1.325653637069635e-06,
      "loss": 1.1883,
      "step": 37040
    },
    {
      "epoch": 2.8698851643137937,
      "grad_norm": 1.914720058441162,
      "learning_rate": 1.3178876520838726e-06,
      "loss": 1.2337,
      "step": 37050
    },
    {
      "epoch": 2.8706597726524525,
      "grad_norm": 1.4294977188110352,
      "learning_rate": 1.3101216670981102e-06,
      "loss": 1.1455,
      "step": 37060
    },
    {
      "epoch": 2.8714343809911114,
      "grad_norm": 1.5831812620162964,
      "learning_rate": 1.302355682112348e-06,
      "loss": 1.1852,
      "step": 37070
    },
    {
      "epoch": 2.87220898932977,
      "grad_norm": 10.009566307067871,
      "learning_rate": 1.2945896971265858e-06,
      "loss": 1.1232,
      "step": 37080
    },
    {
      "epoch": 2.872983597668429,
      "grad_norm": 1.7924246788024902,
      "learning_rate": 1.2868237121408232e-06,
      "loss": 1.0761,
      "step": 37090
    },
    {
      "epoch": 2.8737582060070874,
      "grad_norm": 1.5760563611984253,
      "learning_rate": 1.2790577271550608e-06,
      "loss": 1.1282,
      "step": 37100
    },
    {
      "epoch": 2.8745328143457467,
      "grad_norm": 1.6690274477005005,
      "learning_rate": 1.2712917421692985e-06,
      "loss": 1.1944,
      "step": 37110
    },
    {
      "epoch": 2.875307422684405,
      "grad_norm": 1.321432113647461,
      "learning_rate": 1.2635257571835362e-06,
      "loss": 1.1376,
      "step": 37120
    },
    {
      "epoch": 2.876082031023064,
      "grad_norm": 1.3554328680038452,
      "learning_rate": 1.2557597721977738e-06,
      "loss": 1.2172,
      "step": 37130
    },
    {
      "epoch": 2.8768566393617228,
      "grad_norm": 1.490007758140564,
      "learning_rate": 1.2479937872120113e-06,
      "loss": 1.1726,
      "step": 37140
    },
    {
      "epoch": 2.8776312477003816,
      "grad_norm": 1.2004284858703613,
      "learning_rate": 1.240227802226249e-06,
      "loss": 1.0649,
      "step": 37150
    },
    {
      "epoch": 2.8784058560390404,
      "grad_norm": 2.1151158809661865,
      "learning_rate": 1.2324618172404868e-06,
      "loss": 1.2815,
      "step": 37160
    },
    {
      "epoch": 2.879180464377699,
      "grad_norm": 1.5764367580413818,
      "learning_rate": 1.2246958322547244e-06,
      "loss": 1.1854,
      "step": 37170
    },
    {
      "epoch": 2.8799550727163576,
      "grad_norm": 1.6816867589950562,
      "learning_rate": 1.216929847268962e-06,
      "loss": 1.1216,
      "step": 37180
    },
    {
      "epoch": 2.8807296810550165,
      "grad_norm": 1.7061238288879395,
      "learning_rate": 1.2091638622831997e-06,
      "loss": 1.2192,
      "step": 37190
    },
    {
      "epoch": 2.8815042893936753,
      "grad_norm": 1.7144712209701538,
      "learning_rate": 1.2013978772974372e-06,
      "loss": 1.2159,
      "step": 37200
    },
    {
      "epoch": 2.882278897732334,
      "grad_norm": 1.7664260864257812,
      "learning_rate": 1.1936318923116748e-06,
      "loss": 1.2368,
      "step": 37210
    },
    {
      "epoch": 2.883053506070993,
      "grad_norm": 1.567310094833374,
      "learning_rate": 1.1858659073259125e-06,
      "loss": 1.2847,
      "step": 37220
    },
    {
      "epoch": 2.883828114409652,
      "grad_norm": 1.620688557624817,
      "learning_rate": 1.1780999223401501e-06,
      "loss": 1.2069,
      "step": 37230
    },
    {
      "epoch": 2.88460272274831,
      "grad_norm": 1.533392310142517,
      "learning_rate": 1.1703339373543878e-06,
      "loss": 1.206,
      "step": 37240
    },
    {
      "epoch": 2.885377331086969,
      "grad_norm": 1.841823935508728,
      "learning_rate": 1.1625679523686255e-06,
      "loss": 1.1974,
      "step": 37250
    },
    {
      "epoch": 2.886151939425628,
      "grad_norm": 1.6976946592330933,
      "learning_rate": 1.1548019673828631e-06,
      "loss": 1.2056,
      "step": 37260
    },
    {
      "epoch": 2.8869265477642867,
      "grad_norm": 1.3696955442428589,
      "learning_rate": 1.1470359823971008e-06,
      "loss": 1.114,
      "step": 37270
    },
    {
      "epoch": 2.8877011561029455,
      "grad_norm": 1.5716294050216675,
      "learning_rate": 1.1392699974113384e-06,
      "loss": 1.1768,
      "step": 37280
    },
    {
      "epoch": 2.8884757644416044,
      "grad_norm": 1.540298342704773,
      "learning_rate": 1.131504012425576e-06,
      "loss": 1.1651,
      "step": 37290
    },
    {
      "epoch": 2.889250372780263,
      "grad_norm": 1.7013132572174072,
      "learning_rate": 1.1237380274398137e-06,
      "loss": 1.1672,
      "step": 37300
    },
    {
      "epoch": 2.8900249811189216,
      "grad_norm": 1.6939034461975098,
      "learning_rate": 1.1159720424540512e-06,
      "loss": 1.2187,
      "step": 37310
    },
    {
      "epoch": 2.8907995894575804,
      "grad_norm": 1.7223659753799438,
      "learning_rate": 1.1082060574682888e-06,
      "loss": 1.1077,
      "step": 37320
    },
    {
      "epoch": 2.8915741977962393,
      "grad_norm": 2.0766305923461914,
      "learning_rate": 1.1004400724825265e-06,
      "loss": 1.1912,
      "step": 37330
    },
    {
      "epoch": 2.892348806134898,
      "grad_norm": 1.4221571683883667,
      "learning_rate": 1.0926740874967643e-06,
      "loss": 1.1767,
      "step": 37340
    },
    {
      "epoch": 2.893123414473557,
      "grad_norm": 1.1336184740066528,
      "learning_rate": 1.084908102511002e-06,
      "loss": 1.1987,
      "step": 37350
    },
    {
      "epoch": 2.8938980228122153,
      "grad_norm": 1.6524691581726074,
      "learning_rate": 1.0771421175252394e-06,
      "loss": 1.3063,
      "step": 37360
    },
    {
      "epoch": 2.8946726311508746,
      "grad_norm": 2.0240559577941895,
      "learning_rate": 1.069376132539477e-06,
      "loss": 1.2358,
      "step": 37370
    },
    {
      "epoch": 2.895447239489533,
      "grad_norm": 1.8603912591934204,
      "learning_rate": 1.0616101475537147e-06,
      "loss": 1.2624,
      "step": 37380
    },
    {
      "epoch": 2.896221847828192,
      "grad_norm": 1.558917760848999,
      "learning_rate": 1.0538441625679524e-06,
      "loss": 1.0747,
      "step": 37390
    },
    {
      "epoch": 2.8969964561668506,
      "grad_norm": 1.5799484252929688,
      "learning_rate": 1.04607817758219e-06,
      "loss": 1.0924,
      "step": 37400
    },
    {
      "epoch": 2.8977710645055095,
      "grad_norm": 2.503450870513916,
      "learning_rate": 1.0383121925964277e-06,
      "loss": 1.213,
      "step": 37410
    },
    {
      "epoch": 2.8985456728441683,
      "grad_norm": 2.0785341262817383,
      "learning_rate": 1.0305462076106652e-06,
      "loss": 1.1508,
      "step": 37420
    },
    {
      "epoch": 2.8993202811828267,
      "grad_norm": 1.6310406923294067,
      "learning_rate": 1.022780222624903e-06,
      "loss": 1.182,
      "step": 37430
    },
    {
      "epoch": 2.900094889521486,
      "grad_norm": 1.5104434490203857,
      "learning_rate": 1.0150142376391407e-06,
      "loss": 1.1573,
      "step": 37440
    },
    {
      "epoch": 2.9008694978601444,
      "grad_norm": 1.1821495294570923,
      "learning_rate": 1.0072482526533783e-06,
      "loss": 1.1371,
      "step": 37450
    },
    {
      "epoch": 2.901644106198803,
      "grad_norm": 1.6656941175460815,
      "learning_rate": 9.99482267667616e-07,
      "loss": 1.1535,
      "step": 37460
    },
    {
      "epoch": 2.902418714537462,
      "grad_norm": 1.6620804071426392,
      "learning_rate": 9.917162826818534e-07,
      "loss": 1.3307,
      "step": 37470
    },
    {
      "epoch": 2.903193322876121,
      "grad_norm": 1.4839062690734863,
      "learning_rate": 9.83950297696091e-07,
      "loss": 1.1157,
      "step": 37480
    },
    {
      "epoch": 2.9039679312147797,
      "grad_norm": 1.702043056488037,
      "learning_rate": 9.761843127103287e-07,
      "loss": 1.1496,
      "step": 37490
    },
    {
      "epoch": 2.904742539553438,
      "grad_norm": 2.1473166942596436,
      "learning_rate": 9.684183277245664e-07,
      "loss": 1.3103,
      "step": 37500
    },
    {
      "epoch": 2.905517147892097,
      "grad_norm": 1.6347225904464722,
      "learning_rate": 9.60652342738804e-07,
      "loss": 1.1659,
      "step": 37510
    },
    {
      "epoch": 2.9062917562307558,
      "grad_norm": 1.7539424896240234,
      "learning_rate": 9.528863577530416e-07,
      "loss": 1.1903,
      "step": 37520
    },
    {
      "epoch": 2.9070663645694146,
      "grad_norm": 2.03265643119812,
      "learning_rate": 9.451203727672792e-07,
      "loss": 1.1834,
      "step": 37530
    },
    {
      "epoch": 2.9078409729080734,
      "grad_norm": 1.7704728841781616,
      "learning_rate": 9.37354387781517e-07,
      "loss": 1.2843,
      "step": 37540
    },
    {
      "epoch": 2.9086155812467323,
      "grad_norm": 1.2966967821121216,
      "learning_rate": 9.295884027957547e-07,
      "loss": 1.1582,
      "step": 37550
    },
    {
      "epoch": 2.909390189585391,
      "grad_norm": 1.4418057203292847,
      "learning_rate": 9.218224178099922e-07,
      "loss": 1.3026,
      "step": 37560
    },
    {
      "epoch": 2.9101647979240495,
      "grad_norm": 1.5115532875061035,
      "learning_rate": 9.140564328242299e-07,
      "loss": 1.258,
      "step": 37570
    },
    {
      "epoch": 2.9109394062627083,
      "grad_norm": 1.534289002418518,
      "learning_rate": 9.062904478384675e-07,
      "loss": 1.2875,
      "step": 37580
    },
    {
      "epoch": 2.911714014601367,
      "grad_norm": 1.8067705631256104,
      "learning_rate": 8.985244628527052e-07,
      "loss": 1.0878,
      "step": 37590
    },
    {
      "epoch": 2.912488622940026,
      "grad_norm": 1.9535248279571533,
      "learning_rate": 8.907584778669428e-07,
      "loss": 1.1136,
      "step": 37600
    },
    {
      "epoch": 2.913263231278685,
      "grad_norm": 1.6313176155090332,
      "learning_rate": 8.829924928811805e-07,
      "loss": 1.2037,
      "step": 37610
    },
    {
      "epoch": 2.9140378396173436,
      "grad_norm": 1.610807180404663,
      "learning_rate": 8.75226507895418e-07,
      "loss": 1.0795,
      "step": 37620
    },
    {
      "epoch": 2.9148124479560025,
      "grad_norm": 1.8662062883377075,
      "learning_rate": 8.674605229096558e-07,
      "loss": 1.1755,
      "step": 37630
    },
    {
      "epoch": 2.915587056294661,
      "grad_norm": 1.487180233001709,
      "learning_rate": 8.596945379238934e-07,
      "loss": 1.1173,
      "step": 37640
    },
    {
      "epoch": 2.9163616646333197,
      "grad_norm": 2.817392349243164,
      "learning_rate": 8.51928552938131e-07,
      "loss": 1.156,
      "step": 37650
    },
    {
      "epoch": 2.9171362729719785,
      "grad_norm": 1.8319684267044067,
      "learning_rate": 8.441625679523686e-07,
      "loss": 1.1304,
      "step": 37660
    },
    {
      "epoch": 2.9179108813106374,
      "grad_norm": 1.6564449071884155,
      "learning_rate": 8.363965829666062e-07,
      "loss": 1.1982,
      "step": 37670
    },
    {
      "epoch": 2.918685489649296,
      "grad_norm": 1.732048749923706,
      "learning_rate": 8.28630597980844e-07,
      "loss": 1.164,
      "step": 37680
    },
    {
      "epoch": 2.9194600979879546,
      "grad_norm": 2.2234909534454346,
      "learning_rate": 8.208646129950816e-07,
      "loss": 1.187,
      "step": 37690
    },
    {
      "epoch": 2.920234706326614,
      "grad_norm": 1.7689160108566284,
      "learning_rate": 8.130986280093192e-07,
      "loss": 1.3142,
      "step": 37700
    },
    {
      "epoch": 2.9210093146652722,
      "grad_norm": 1.3322381973266602,
      "learning_rate": 8.053326430235568e-07,
      "loss": 1.2223,
      "step": 37710
    },
    {
      "epoch": 2.921783923003931,
      "grad_norm": 1.3700553178787231,
      "learning_rate": 7.975666580377946e-07,
      "loss": 1.242,
      "step": 37720
    },
    {
      "epoch": 2.92255853134259,
      "grad_norm": 1.731325387954712,
      "learning_rate": 7.898006730520321e-07,
      "loss": 1.2355,
      "step": 37730
    },
    {
      "epoch": 2.9233331396812487,
      "grad_norm": 1.8888850212097168,
      "learning_rate": 7.820346880662698e-07,
      "loss": 1.1475,
      "step": 37740
    },
    {
      "epoch": 2.9241077480199076,
      "grad_norm": 1.5147372484207153,
      "learning_rate": 7.742687030805074e-07,
      "loss": 1.2135,
      "step": 37750
    },
    {
      "epoch": 2.924882356358566,
      "grad_norm": 1.5168718099594116,
      "learning_rate": 7.66502718094745e-07,
      "loss": 1.2169,
      "step": 37760
    },
    {
      "epoch": 2.925656964697225,
      "grad_norm": 1.2557883262634277,
      "learning_rate": 7.587367331089827e-07,
      "loss": 1.1246,
      "step": 37770
    },
    {
      "epoch": 2.9264315730358836,
      "grad_norm": 1.4314993619918823,
      "learning_rate": 7.509707481232203e-07,
      "loss": 1.3227,
      "step": 37780
    },
    {
      "epoch": 2.9272061813745425,
      "grad_norm": 1.4407458305358887,
      "learning_rate": 7.432047631374579e-07,
      "loss": 1.2157,
      "step": 37790
    },
    {
      "epoch": 2.9279807897132013,
      "grad_norm": 1.6895301342010498,
      "learning_rate": 7.362153766502718e-07,
      "loss": 1.1416,
      "step": 37800
    },
    {
      "epoch": 2.92875539805186,
      "grad_norm": 1.8535641431808472,
      "learning_rate": 7.284493916645095e-07,
      "loss": 1.152,
      "step": 37810
    },
    {
      "epoch": 2.929530006390519,
      "grad_norm": 1.9084622859954834,
      "learning_rate": 7.206834066787471e-07,
      "loss": 1.173,
      "step": 37820
    },
    {
      "epoch": 2.9303046147291774,
      "grad_norm": 1.9319562911987305,
      "learning_rate": 7.129174216929847e-07,
      "loss": 1.0727,
      "step": 37830
    },
    {
      "epoch": 2.931079223067836,
      "grad_norm": 1.9573280811309814,
      "learning_rate": 7.051514367072224e-07,
      "loss": 1.2586,
      "step": 37840
    },
    {
      "epoch": 2.931853831406495,
      "grad_norm": 1.6296988725662231,
      "learning_rate": 6.9738545172146e-07,
      "loss": 1.1656,
      "step": 37850
    },
    {
      "epoch": 2.932628439745154,
      "grad_norm": 1.5252665281295776,
      "learning_rate": 6.896194667356977e-07,
      "loss": 1.1125,
      "step": 37860
    },
    {
      "epoch": 2.9334030480838127,
      "grad_norm": 1.456809401512146,
      "learning_rate": 6.818534817499353e-07,
      "loss": 1.213,
      "step": 37870
    },
    {
      "epoch": 2.9341776564224715,
      "grad_norm": 1.2551090717315674,
      "learning_rate": 6.740874967641729e-07,
      "loss": 1.1034,
      "step": 37880
    },
    {
      "epoch": 2.9349522647611304,
      "grad_norm": 1.5065267086029053,
      "learning_rate": 6.663215117784105e-07,
      "loss": 1.1025,
      "step": 37890
    },
    {
      "epoch": 2.9357268730997887,
      "grad_norm": 1.5065112113952637,
      "learning_rate": 6.585555267926482e-07,
      "loss": 1.1743,
      "step": 37900
    },
    {
      "epoch": 2.9365014814384476,
      "grad_norm": 1.6195085048675537,
      "learning_rate": 6.507895418068859e-07,
      "loss": 1.2301,
      "step": 37910
    },
    {
      "epoch": 2.9372760897771064,
      "grad_norm": 1.7535773515701294,
      "learning_rate": 6.430235568211235e-07,
      "loss": 1.0735,
      "step": 37920
    },
    {
      "epoch": 2.9380506981157652,
      "grad_norm": 1.495100736618042,
      "learning_rate": 6.352575718353611e-07,
      "loss": 1.1812,
      "step": 37930
    },
    {
      "epoch": 2.938825306454424,
      "grad_norm": 1.5972834825515747,
      "learning_rate": 6.274915868495988e-07,
      "loss": 1.1649,
      "step": 37940
    },
    {
      "epoch": 2.939599914793083,
      "grad_norm": 1.7414599657058716,
      "learning_rate": 6.197256018638365e-07,
      "loss": 1.1957,
      "step": 37950
    },
    {
      "epoch": 2.9403745231317417,
      "grad_norm": 2.130030632019043,
      "learning_rate": 6.11959616878074e-07,
      "loss": 1.2424,
      "step": 37960
    },
    {
      "epoch": 2.9411491314704,
      "grad_norm": 1.336258888244629,
      "learning_rate": 6.041936318923117e-07,
      "loss": 1.1776,
      "step": 37970
    },
    {
      "epoch": 2.941923739809059,
      "grad_norm": 2.390326976776123,
      "learning_rate": 5.964276469065493e-07,
      "loss": 1.152,
      "step": 37980
    },
    {
      "epoch": 2.942698348147718,
      "grad_norm": 1.928287386894226,
      "learning_rate": 5.88661661920787e-07,
      "loss": 1.128,
      "step": 37990
    },
    {
      "epoch": 2.9434729564863766,
      "grad_norm": 1.906221628189087,
      "learning_rate": 5.808956769350246e-07,
      "loss": 1.1848,
      "step": 38000
    },
    {
      "epoch": 2.9442475648250355,
      "grad_norm": 1.8768316507339478,
      "learning_rate": 5.731296919492622e-07,
      "loss": 1.0913,
      "step": 38010
    },
    {
      "epoch": 2.945022173163694,
      "grad_norm": 1.2047395706176758,
      "learning_rate": 5.653637069634998e-07,
      "loss": 1.2652,
      "step": 38020
    },
    {
      "epoch": 2.945796781502353,
      "grad_norm": 1.335097074508667,
      "learning_rate": 5.575977219777376e-07,
      "loss": 1.2394,
      "step": 38030
    },
    {
      "epoch": 2.9465713898410115,
      "grad_norm": 1.9062999486923218,
      "learning_rate": 5.498317369919752e-07,
      "loss": 1.1915,
      "step": 38040
    },
    {
      "epoch": 2.9473459981796704,
      "grad_norm": 1.461944580078125,
      "learning_rate": 5.420657520062128e-07,
      "loss": 1.1849,
      "step": 38050
    },
    {
      "epoch": 2.948120606518329,
      "grad_norm": 1.5697784423828125,
      "learning_rate": 5.342997670204505e-07,
      "loss": 1.2155,
      "step": 38060
    },
    {
      "epoch": 2.948895214856988,
      "grad_norm": 1.2575654983520508,
      "learning_rate": 5.265337820346881e-07,
      "loss": 1.1797,
      "step": 38070
    },
    {
      "epoch": 2.949669823195647,
      "grad_norm": 2.1352508068084717,
      "learning_rate": 5.187677970489258e-07,
      "loss": 1.2972,
      "step": 38080
    },
    {
      "epoch": 2.9504444315343052,
      "grad_norm": 1.8866428136825562,
      "learning_rate": 5.110018120631633e-07,
      "loss": 1.2126,
      "step": 38090
    },
    {
      "epoch": 2.951219039872964,
      "grad_norm": 1.8254865407943726,
      "learning_rate": 5.03235827077401e-07,
      "loss": 1.2392,
      "step": 38100
    },
    {
      "epoch": 2.951993648211623,
      "grad_norm": 1.8290094137191772,
      "learning_rate": 4.954698420916386e-07,
      "loss": 1.2231,
      "step": 38110
    },
    {
      "epoch": 2.9527682565502817,
      "grad_norm": 1.4237242937088013,
      "learning_rate": 4.877038571058763e-07,
      "loss": 1.1711,
      "step": 38120
    },
    {
      "epoch": 2.9535428648889406,
      "grad_norm": 1.6734790802001953,
      "learning_rate": 4.799378721201139e-07,
      "loss": 1.1156,
      "step": 38130
    },
    {
      "epoch": 2.9543174732275994,
      "grad_norm": 1.6049818992614746,
      "learning_rate": 4.721718871343516e-07,
      "loss": 1.2175,
      "step": 38140
    },
    {
      "epoch": 2.9550920815662582,
      "grad_norm": 2.3745763301849365,
      "learning_rate": 4.644059021485892e-07,
      "loss": 1.1428,
      "step": 38150
    },
    {
      "epoch": 2.9558666899049166,
      "grad_norm": 1.6042637825012207,
      "learning_rate": 4.5663991716282685e-07,
      "loss": 1.0902,
      "step": 38160
    },
    {
      "epoch": 2.9566412982435755,
      "grad_norm": 1.9016339778900146,
      "learning_rate": 4.4887393217706445e-07,
      "loss": 1.1434,
      "step": 38170
    },
    {
      "epoch": 2.9574159065822343,
      "grad_norm": 1.667121171951294,
      "learning_rate": 4.411079471913021e-07,
      "loss": 1.0871,
      "step": 38180
    },
    {
      "epoch": 2.958190514920893,
      "grad_norm": 1.6871867179870605,
      "learning_rate": 4.333419622055397e-07,
      "loss": 1.1537,
      "step": 38190
    },
    {
      "epoch": 2.958965123259552,
      "grad_norm": 1.913839340209961,
      "learning_rate": 4.255759772197774e-07,
      "loss": 1.2096,
      "step": 38200
    },
    {
      "epoch": 2.959739731598211,
      "grad_norm": 2.3200290203094482,
      "learning_rate": 4.17809992234015e-07,
      "loss": 1.1254,
      "step": 38210
    },
    {
      "epoch": 2.9605143399368696,
      "grad_norm": 1.3381116390228271,
      "learning_rate": 4.1004400724825267e-07,
      "loss": 1.1863,
      "step": 38220
    },
    {
      "epoch": 2.961288948275528,
      "grad_norm": 1.6027814149856567,
      "learning_rate": 4.022780222624903e-07,
      "loss": 1.2355,
      "step": 38230
    },
    {
      "epoch": 2.962063556614187,
      "grad_norm": 1.8556580543518066,
      "learning_rate": 3.94512037276728e-07,
      "loss": 1.2482,
      "step": 38240
    },
    {
      "epoch": 2.9628381649528457,
      "grad_norm": 1.4996147155761719,
      "learning_rate": 3.867460522909656e-07,
      "loss": 1.1663,
      "step": 38250
    },
    {
      "epoch": 2.9636127732915045,
      "grad_norm": 1.607851266860962,
      "learning_rate": 3.789800673052032e-07,
      "loss": 1.1611,
      "step": 38260
    },
    {
      "epoch": 2.9643873816301634,
      "grad_norm": 1.7885401248931885,
      "learning_rate": 3.712140823194409e-07,
      "loss": 1.1937,
      "step": 38270
    },
    {
      "epoch": 2.9651619899688217,
      "grad_norm": 1.8125905990600586,
      "learning_rate": 3.634480973336785e-07,
      "loss": 1.2703,
      "step": 38280
    },
    {
      "epoch": 2.965936598307481,
      "grad_norm": 2.0624232292175293,
      "learning_rate": 3.5568211234791614e-07,
      "loss": 1.186,
      "step": 38290
    },
    {
      "epoch": 2.9667112066461394,
      "grad_norm": 1.6223520040512085,
      "learning_rate": 3.4791612736215374e-07,
      "loss": 1.2316,
      "step": 38300
    },
    {
      "epoch": 2.9674858149847982,
      "grad_norm": 1.8593765497207642,
      "learning_rate": 3.4015014237639145e-07,
      "loss": 1.1436,
      "step": 38310
    },
    {
      "epoch": 2.968260423323457,
      "grad_norm": 1.3890796899795532,
      "learning_rate": 3.3238415739062905e-07,
      "loss": 1.1778,
      "step": 38320
    },
    {
      "epoch": 2.969035031662116,
      "grad_norm": 1.6351573467254639,
      "learning_rate": 3.246181724048667e-07,
      "loss": 1.2091,
      "step": 38330
    },
    {
      "epoch": 2.9698096400007747,
      "grad_norm": 1.7917110919952393,
      "learning_rate": 3.168521874191043e-07,
      "loss": 1.182,
      "step": 38340
    },
    {
      "epoch": 2.970584248339433,
      "grad_norm": 1.6491706371307373,
      "learning_rate": 3.0908620243334196e-07,
      "loss": 1.1595,
      "step": 38350
    },
    {
      "epoch": 2.9713588566780924,
      "grad_norm": 1.2932312488555908,
      "learning_rate": 3.013202174475796e-07,
      "loss": 1.1204,
      "step": 38360
    },
    {
      "epoch": 2.972133465016751,
      "grad_norm": 1.682733416557312,
      "learning_rate": 2.935542324618172e-07,
      "loss": 1.1475,
      "step": 38370
    },
    {
      "epoch": 2.9729080733554096,
      "grad_norm": 1.2563222646713257,
      "learning_rate": 2.8578824747605487e-07,
      "loss": 1.1381,
      "step": 38380
    },
    {
      "epoch": 2.9736826816940685,
      "grad_norm": 1.4945205450057983,
      "learning_rate": 2.780222624902925e-07,
      "loss": 1.2344,
      "step": 38390
    },
    {
      "epoch": 2.9744572900327273,
      "grad_norm": 1.4249718189239502,
      "learning_rate": 2.702562775045302e-07,
      "loss": 1.1706,
      "step": 38400
    }
  ],
  "logging_steps": 10,
  "max_steps": 38730,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.748597715242844e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
