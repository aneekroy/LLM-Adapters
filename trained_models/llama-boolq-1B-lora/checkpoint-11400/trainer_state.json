{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.899108198130439,
  "eval_steps": 500,
  "global_step": 11400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004297840335231546,
      "grad_norm": 2.2439193725585938,
      "learning_rate": 2.7e-06,
      "loss": 3.8523,
      "step": 10
    },
    {
      "epoch": 0.008595680670463093,
      "grad_norm": 2.1270856857299805,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 3.8315,
      "step": 20
    },
    {
      "epoch": 0.012893521005694639,
      "grad_norm": 2.0917789936065674,
      "learning_rate": 8.400000000000001e-06,
      "loss": 3.7844,
      "step": 30
    },
    {
      "epoch": 0.017191361340926185,
      "grad_norm": 2.099583387374878,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 3.7719,
      "step": 40
    },
    {
      "epoch": 0.02148920167615773,
      "grad_norm": 2.098783016204834,
      "learning_rate": 1.44e-05,
      "loss": 3.7045,
      "step": 50
    },
    {
      "epoch": 0.025787042011389278,
      "grad_norm": 2.1914336681365967,
      "learning_rate": 1.74e-05,
      "loss": 3.5445,
      "step": 60
    },
    {
      "epoch": 0.030084882346620823,
      "grad_norm": 2.4008970260620117,
      "learning_rate": 2.04e-05,
      "loss": 3.4395,
      "step": 70
    },
    {
      "epoch": 0.03438272268185237,
      "grad_norm": 2.646775245666504,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 3.2564,
      "step": 80
    },
    {
      "epoch": 0.03868056301708392,
      "grad_norm": 2.8657212257385254,
      "learning_rate": 2.64e-05,
      "loss": 3.0029,
      "step": 90
    },
    {
      "epoch": 0.04297840335231546,
      "grad_norm": 3.165053129196167,
      "learning_rate": 2.94e-05,
      "loss": 2.6844,
      "step": 100
    },
    {
      "epoch": 0.04727624368754701,
      "grad_norm": 3.6331276893615723,
      "learning_rate": 2.9979193758127437e-05,
      "loss": 2.3384,
      "step": 110
    },
    {
      "epoch": 0.051574084022778556,
      "grad_norm": 4.566567897796631,
      "learning_rate": 2.995318595578674e-05,
      "loss": 1.9197,
      "step": 120
    },
    {
      "epoch": 0.0558719243580101,
      "grad_norm": 3.8959412574768066,
      "learning_rate": 2.9927178153446036e-05,
      "loss": 1.4856,
      "step": 130
    },
    {
      "epoch": 0.060169764693241645,
      "grad_norm": 3.5852532386779785,
      "learning_rate": 2.9901170351105333e-05,
      "loss": 1.2154,
      "step": 140
    },
    {
      "epoch": 0.0644676050284732,
      "grad_norm": 2.251152515411377,
      "learning_rate": 2.987516254876463e-05,
      "loss": 1.0908,
      "step": 150
    },
    {
      "epoch": 0.06876544536370474,
      "grad_norm": 2.477384328842163,
      "learning_rate": 2.984915474642393e-05,
      "loss": 0.9624,
      "step": 160
    },
    {
      "epoch": 0.07306328569893629,
      "grad_norm": 1.6339043378829956,
      "learning_rate": 2.9823146944083226e-05,
      "loss": 0.9393,
      "step": 170
    },
    {
      "epoch": 0.07736112603416784,
      "grad_norm": 1.3877806663513184,
      "learning_rate": 2.9797139141742524e-05,
      "loss": 0.9549,
      "step": 180
    },
    {
      "epoch": 0.08165896636939937,
      "grad_norm": 1.8207998275756836,
      "learning_rate": 2.977113133940182e-05,
      "loss": 0.955,
      "step": 190
    },
    {
      "epoch": 0.08595680670463092,
      "grad_norm": 1.221486210823059,
      "learning_rate": 2.974512353706112e-05,
      "loss": 0.9096,
      "step": 200
    },
    {
      "epoch": 0.09025464703986247,
      "grad_norm": 1.6938754320144653,
      "learning_rate": 2.9719115734720416e-05,
      "loss": 0.9851,
      "step": 210
    },
    {
      "epoch": 0.09455248737509402,
      "grad_norm": 2.2557573318481445,
      "learning_rate": 2.9693107932379714e-05,
      "loss": 0.9584,
      "step": 220
    },
    {
      "epoch": 0.09885032771032556,
      "grad_norm": 1.5922590494155884,
      "learning_rate": 2.966710013003901e-05,
      "loss": 0.9186,
      "step": 230
    },
    {
      "epoch": 0.10314816804555711,
      "grad_norm": 2.7211382389068604,
      "learning_rate": 2.964109232769831e-05,
      "loss": 0.9183,
      "step": 240
    },
    {
      "epoch": 0.10744600838078866,
      "grad_norm": 2.013387680053711,
      "learning_rate": 2.961508452535761e-05,
      "loss": 0.9253,
      "step": 250
    },
    {
      "epoch": 0.1117438487160202,
      "grad_norm": 1.8031048774719238,
      "learning_rate": 2.9589076723016907e-05,
      "loss": 0.9193,
      "step": 260
    },
    {
      "epoch": 0.11604168905125174,
      "grad_norm": 1.9260324239730835,
      "learning_rate": 2.9563068920676205e-05,
      "loss": 0.9005,
      "step": 270
    },
    {
      "epoch": 0.12033952938648329,
      "grad_norm": 1.6062299013137817,
      "learning_rate": 2.9537061118335502e-05,
      "loss": 0.9417,
      "step": 280
    },
    {
      "epoch": 0.12463736972171484,
      "grad_norm": 2.0572381019592285,
      "learning_rate": 2.95110533159948e-05,
      "loss": 0.9025,
      "step": 290
    },
    {
      "epoch": 0.1289352100569464,
      "grad_norm": 1.6503078937530518,
      "learning_rate": 2.9485045513654097e-05,
      "loss": 0.9053,
      "step": 300
    },
    {
      "epoch": 0.13323305039217792,
      "grad_norm": 1.9801955223083496,
      "learning_rate": 2.9459037711313395e-05,
      "loss": 0.9321,
      "step": 310
    },
    {
      "epoch": 0.13753089072740948,
      "grad_norm": 1.8264697790145874,
      "learning_rate": 2.9433029908972693e-05,
      "loss": 0.872,
      "step": 320
    },
    {
      "epoch": 0.14182873106264102,
      "grad_norm": 2.2063682079315186,
      "learning_rate": 2.940702210663199e-05,
      "loss": 0.9091,
      "step": 330
    },
    {
      "epoch": 0.14612657139787258,
      "grad_norm": 2.4447221755981445,
      "learning_rate": 2.9381014304291288e-05,
      "loss": 0.8944,
      "step": 340
    },
    {
      "epoch": 0.1504244117331041,
      "grad_norm": 2.272000789642334,
      "learning_rate": 2.9355006501950585e-05,
      "loss": 0.9017,
      "step": 350
    },
    {
      "epoch": 0.15472225206833568,
      "grad_norm": 1.5556195974349976,
      "learning_rate": 2.9328998699609883e-05,
      "loss": 0.897,
      "step": 360
    },
    {
      "epoch": 0.1590200924035672,
      "grad_norm": 1.4963613748550415,
      "learning_rate": 2.930299089726918e-05,
      "loss": 0.8921,
      "step": 370
    },
    {
      "epoch": 0.16331793273879874,
      "grad_norm": 1.2491198778152466,
      "learning_rate": 2.9276983094928478e-05,
      "loss": 0.9138,
      "step": 380
    },
    {
      "epoch": 0.1676157730740303,
      "grad_norm": 2.2678825855255127,
      "learning_rate": 2.925097529258778e-05,
      "loss": 0.9234,
      "step": 390
    },
    {
      "epoch": 0.17191361340926184,
      "grad_norm": 1.7287007570266724,
      "learning_rate": 2.9224967490247076e-05,
      "loss": 0.9025,
      "step": 400
    },
    {
      "epoch": 0.1762114537444934,
      "grad_norm": 3.1042118072509766,
      "learning_rate": 2.9198959687906374e-05,
      "loss": 0.8964,
      "step": 410
    },
    {
      "epoch": 0.18050929407972494,
      "grad_norm": 1.8487852811813354,
      "learning_rate": 2.917295188556567e-05,
      "loss": 0.9077,
      "step": 420
    },
    {
      "epoch": 0.1848071344149565,
      "grad_norm": 1.9388614892959595,
      "learning_rate": 2.914694408322497e-05,
      "loss": 0.8892,
      "step": 430
    },
    {
      "epoch": 0.18910497475018803,
      "grad_norm": 2.078889846801758,
      "learning_rate": 2.9120936280884266e-05,
      "loss": 0.9436,
      "step": 440
    },
    {
      "epoch": 0.19340281508541957,
      "grad_norm": 2.1568830013275146,
      "learning_rate": 2.9094928478543564e-05,
      "loss": 0.9306,
      "step": 450
    },
    {
      "epoch": 0.19770065542065113,
      "grad_norm": 1.2709153890609741,
      "learning_rate": 2.906892067620286e-05,
      "loss": 0.8761,
      "step": 460
    },
    {
      "epoch": 0.20199849575588266,
      "grad_norm": 1.471901297569275,
      "learning_rate": 2.904291287386216e-05,
      "loss": 0.9449,
      "step": 470
    },
    {
      "epoch": 0.20629633609111422,
      "grad_norm": 1.8028442859649658,
      "learning_rate": 2.9016905071521457e-05,
      "loss": 0.9026,
      "step": 480
    },
    {
      "epoch": 0.21059417642634576,
      "grad_norm": 1.513261318206787,
      "learning_rate": 2.8990897269180754e-05,
      "loss": 0.8714,
      "step": 490
    },
    {
      "epoch": 0.21489201676157732,
      "grad_norm": 1.2649246454238892,
      "learning_rate": 2.896488946684005e-05,
      "loss": 0.8932,
      "step": 500
    },
    {
      "epoch": 0.21918985709680885,
      "grad_norm": 2.4855308532714844,
      "learning_rate": 2.893888166449935e-05,
      "loss": 0.8771,
      "step": 510
    },
    {
      "epoch": 0.2234876974320404,
      "grad_norm": 1.8490320444107056,
      "learning_rate": 2.891287386215865e-05,
      "loss": 0.888,
      "step": 520
    },
    {
      "epoch": 0.22778553776727195,
      "grad_norm": 1.7483315467834473,
      "learning_rate": 2.8886866059817944e-05,
      "loss": 0.8983,
      "step": 530
    },
    {
      "epoch": 0.23208337810250348,
      "grad_norm": 1.5140494108200073,
      "learning_rate": 2.8860858257477245e-05,
      "loss": 0.8713,
      "step": 540
    },
    {
      "epoch": 0.23638121843773505,
      "grad_norm": 1.174425721168518,
      "learning_rate": 2.8834850455136543e-05,
      "loss": 0.837,
      "step": 550
    },
    {
      "epoch": 0.24067905877296658,
      "grad_norm": 1.7094616889953613,
      "learning_rate": 2.880884265279584e-05,
      "loss": 0.8819,
      "step": 560
    },
    {
      "epoch": 0.24497689910819814,
      "grad_norm": 2.0715062618255615,
      "learning_rate": 2.8782834850455138e-05,
      "loss": 0.885,
      "step": 570
    },
    {
      "epoch": 0.24927473944342968,
      "grad_norm": 1.7991349697113037,
      "learning_rate": 2.8756827048114435e-05,
      "loss": 0.907,
      "step": 580
    },
    {
      "epoch": 0.2535725797786612,
      "grad_norm": 1.9705241918563843,
      "learning_rate": 2.8730819245773733e-05,
      "loss": 0.9146,
      "step": 590
    },
    {
      "epoch": 0.2578704201138928,
      "grad_norm": 3.261291742324829,
      "learning_rate": 2.870481144343303e-05,
      "loss": 0.9045,
      "step": 600
    },
    {
      "epoch": 0.26216826044912434,
      "grad_norm": 1.4352775812149048,
      "learning_rate": 2.8678803641092328e-05,
      "loss": 0.8821,
      "step": 610
    },
    {
      "epoch": 0.26646610078435584,
      "grad_norm": 1.8127928972244263,
      "learning_rate": 2.8652795838751626e-05,
      "loss": 0.8737,
      "step": 620
    },
    {
      "epoch": 0.2707639411195874,
      "grad_norm": 1.8089369535446167,
      "learning_rate": 2.8626788036410923e-05,
      "loss": 0.9088,
      "step": 630
    },
    {
      "epoch": 0.27506178145481897,
      "grad_norm": 1.258477807044983,
      "learning_rate": 2.860078023407022e-05,
      "loss": 0.8933,
      "step": 640
    },
    {
      "epoch": 0.27935962179005047,
      "grad_norm": 1.9984034299850464,
      "learning_rate": 2.857477243172952e-05,
      "loss": 0.875,
      "step": 650
    },
    {
      "epoch": 0.28365746212528203,
      "grad_norm": 1.457008719444275,
      "learning_rate": 2.8548764629388816e-05,
      "loss": 0.9295,
      "step": 660
    },
    {
      "epoch": 0.2879553024605136,
      "grad_norm": 1.4760674238204956,
      "learning_rate": 2.8522756827048117e-05,
      "loss": 0.8676,
      "step": 670
    },
    {
      "epoch": 0.29225314279574516,
      "grad_norm": 1.4483600854873657,
      "learning_rate": 2.849674902470741e-05,
      "loss": 0.9044,
      "step": 680
    },
    {
      "epoch": 0.29655098313097666,
      "grad_norm": 1.504328966140747,
      "learning_rate": 2.8470741222366712e-05,
      "loss": 0.9132,
      "step": 690
    },
    {
      "epoch": 0.3008488234662082,
      "grad_norm": 1.726001262664795,
      "learning_rate": 2.844473342002601e-05,
      "loss": 0.8866,
      "step": 700
    },
    {
      "epoch": 0.3051466638014398,
      "grad_norm": 1.914871096611023,
      "learning_rate": 2.8418725617685307e-05,
      "loss": 0.8923,
      "step": 710
    },
    {
      "epoch": 0.30944450413667135,
      "grad_norm": 1.831436276435852,
      "learning_rate": 2.8392717815344604e-05,
      "loss": 0.8738,
      "step": 720
    },
    {
      "epoch": 0.31374234447190286,
      "grad_norm": 2.512200117111206,
      "learning_rate": 2.8366710013003902e-05,
      "loss": 0.9101,
      "step": 730
    },
    {
      "epoch": 0.3180401848071344,
      "grad_norm": 1.2558857202529907,
      "learning_rate": 2.83407022106632e-05,
      "loss": 0.8796,
      "step": 740
    },
    {
      "epoch": 0.322338025142366,
      "grad_norm": 1.7246792316436768,
      "learning_rate": 2.8314694408322497e-05,
      "loss": 0.8539,
      "step": 750
    },
    {
      "epoch": 0.3266358654775975,
      "grad_norm": 1.5701310634613037,
      "learning_rate": 2.8288686605981795e-05,
      "loss": 0.8714,
      "step": 760
    },
    {
      "epoch": 0.33093370581282905,
      "grad_norm": 1.5575428009033203,
      "learning_rate": 2.8262678803641092e-05,
      "loss": 0.8588,
      "step": 770
    },
    {
      "epoch": 0.3352315461480606,
      "grad_norm": 1.7644492387771606,
      "learning_rate": 2.8236671001300393e-05,
      "loss": 0.8846,
      "step": 780
    },
    {
      "epoch": 0.3395293864832922,
      "grad_norm": 3.079484462738037,
      "learning_rate": 2.8210663198959687e-05,
      "loss": 0.8723,
      "step": 790
    },
    {
      "epoch": 0.3438272268185237,
      "grad_norm": 1.4718937873840332,
      "learning_rate": 2.8184655396618988e-05,
      "loss": 0.8669,
      "step": 800
    },
    {
      "epoch": 0.34812506715375524,
      "grad_norm": 1.7372475862503052,
      "learning_rate": 2.8158647594278282e-05,
      "loss": 0.9223,
      "step": 810
    },
    {
      "epoch": 0.3524229074889868,
      "grad_norm": 1.4305087327957153,
      "learning_rate": 2.8132639791937583e-05,
      "loss": 0.9024,
      "step": 820
    },
    {
      "epoch": 0.3567207478242183,
      "grad_norm": 3.3948540687561035,
      "learning_rate": 2.8106631989596877e-05,
      "loss": 0.9062,
      "step": 830
    },
    {
      "epoch": 0.36101858815944987,
      "grad_norm": 1.6377743482589722,
      "learning_rate": 2.8080624187256178e-05,
      "loss": 0.8567,
      "step": 840
    },
    {
      "epoch": 0.36531642849468143,
      "grad_norm": 1.3509950637817383,
      "learning_rate": 2.8054616384915476e-05,
      "loss": 0.8938,
      "step": 850
    },
    {
      "epoch": 0.369614268829913,
      "grad_norm": 1.4829307794570923,
      "learning_rate": 2.8028608582574773e-05,
      "loss": 0.9019,
      "step": 860
    },
    {
      "epoch": 0.3739121091651445,
      "grad_norm": 1.682952880859375,
      "learning_rate": 2.800260078023407e-05,
      "loss": 0.8832,
      "step": 870
    },
    {
      "epoch": 0.37820994950037606,
      "grad_norm": 2.082268476486206,
      "learning_rate": 2.797659297789337e-05,
      "loss": 0.9099,
      "step": 880
    },
    {
      "epoch": 0.3825077898356076,
      "grad_norm": 1.8859350681304932,
      "learning_rate": 2.7950585175552666e-05,
      "loss": 0.8685,
      "step": 890
    },
    {
      "epoch": 0.38680563017083913,
      "grad_norm": 2.061854124069214,
      "learning_rate": 2.7924577373211963e-05,
      "loss": 0.8487,
      "step": 900
    },
    {
      "epoch": 0.3911034705060707,
      "grad_norm": 1.595057725906372,
      "learning_rate": 2.7898569570871264e-05,
      "loss": 0.8676,
      "step": 910
    },
    {
      "epoch": 0.39540131084130226,
      "grad_norm": 1.3148223161697388,
      "learning_rate": 2.787256176853056e-05,
      "loss": 0.9031,
      "step": 920
    },
    {
      "epoch": 0.3996991511765338,
      "grad_norm": 2.0778708457946777,
      "learning_rate": 2.784655396618986e-05,
      "loss": 0.8634,
      "step": 930
    },
    {
      "epoch": 0.4039969915117653,
      "grad_norm": 1.2834926843643188,
      "learning_rate": 2.7820546163849154e-05,
      "loss": 0.8667,
      "step": 940
    },
    {
      "epoch": 0.4082948318469969,
      "grad_norm": 1.5207332372665405,
      "learning_rate": 2.7794538361508455e-05,
      "loss": 0.8923,
      "step": 950
    },
    {
      "epoch": 0.41259267218222845,
      "grad_norm": 2.4688920974731445,
      "learning_rate": 2.776853055916775e-05,
      "loss": 0.9037,
      "step": 960
    },
    {
      "epoch": 0.41689051251745995,
      "grad_norm": 1.421184778213501,
      "learning_rate": 2.774252275682705e-05,
      "loss": 0.8855,
      "step": 970
    },
    {
      "epoch": 0.4211883528526915,
      "grad_norm": 4.2580647468566895,
      "learning_rate": 2.7716514954486344e-05,
      "loss": 0.8783,
      "step": 980
    },
    {
      "epoch": 0.4254861931879231,
      "grad_norm": 1.7161500453948975,
      "learning_rate": 2.7690507152145645e-05,
      "loss": 0.8813,
      "step": 990
    },
    {
      "epoch": 0.42978403352315464,
      "grad_norm": 1.3755216598510742,
      "learning_rate": 2.7664499349804942e-05,
      "loss": 0.8933,
      "step": 1000
    },
    {
      "epoch": 0.43408187385838615,
      "grad_norm": 1.571014165878296,
      "learning_rate": 2.763849154746424e-05,
      "loss": 0.8963,
      "step": 1010
    },
    {
      "epoch": 0.4383797141936177,
      "grad_norm": 1.6853419542312622,
      "learning_rate": 2.7612483745123537e-05,
      "loss": 0.8592,
      "step": 1020
    },
    {
      "epoch": 0.44267755452884927,
      "grad_norm": 1.7665759325027466,
      "learning_rate": 2.7586475942782835e-05,
      "loss": 0.852,
      "step": 1030
    },
    {
      "epoch": 0.4469753948640808,
      "grad_norm": 1.651178002357483,
      "learning_rate": 2.7560468140442136e-05,
      "loss": 0.9122,
      "step": 1040
    },
    {
      "epoch": 0.45127323519931234,
      "grad_norm": 2.645949125289917,
      "learning_rate": 2.753446033810143e-05,
      "loss": 0.8864,
      "step": 1050
    },
    {
      "epoch": 0.4555710755345439,
      "grad_norm": 1.8713504076004028,
      "learning_rate": 2.750845253576073e-05,
      "loss": 0.9025,
      "step": 1060
    },
    {
      "epoch": 0.45986891586977546,
      "grad_norm": 1.5271580219268799,
      "learning_rate": 2.7482444733420025e-05,
      "loss": 0.9128,
      "step": 1070
    },
    {
      "epoch": 0.46416675620500697,
      "grad_norm": 1.330910086631775,
      "learning_rate": 2.7456436931079326e-05,
      "loss": 0.8863,
      "step": 1080
    },
    {
      "epoch": 0.46846459654023853,
      "grad_norm": 1.7527412176132202,
      "learning_rate": 2.743042912873862e-05,
      "loss": 0.8868,
      "step": 1090
    },
    {
      "epoch": 0.4727624368754701,
      "grad_norm": 1.734645128250122,
      "learning_rate": 2.740442132639792e-05,
      "loss": 0.8774,
      "step": 1100
    },
    {
      "epoch": 0.4770602772107016,
      "grad_norm": 1.425091028213501,
      "learning_rate": 2.7378413524057215e-05,
      "loss": 0.8572,
      "step": 1110
    },
    {
      "epoch": 0.48135811754593316,
      "grad_norm": 2.1365723609924316,
      "learning_rate": 2.7352405721716516e-05,
      "loss": 0.8716,
      "step": 1120
    },
    {
      "epoch": 0.4856559578811647,
      "grad_norm": 1.4031108617782593,
      "learning_rate": 2.732639791937581e-05,
      "loss": 0.8764,
      "step": 1130
    },
    {
      "epoch": 0.4899537982163963,
      "grad_norm": 1.5276724100112915,
      "learning_rate": 2.730039011703511e-05,
      "loss": 0.9004,
      "step": 1140
    },
    {
      "epoch": 0.4942516385516278,
      "grad_norm": 2.1624739170074463,
      "learning_rate": 2.7274382314694412e-05,
      "loss": 0.8759,
      "step": 1150
    },
    {
      "epoch": 0.49854947888685935,
      "grad_norm": 1.3980320692062378,
      "learning_rate": 2.7248374512353706e-05,
      "loss": 0.8773,
      "step": 1160
    },
    {
      "epoch": 0.5028473192220909,
      "grad_norm": 1.5873808860778809,
      "learning_rate": 2.7222366710013007e-05,
      "loss": 0.8824,
      "step": 1170
    },
    {
      "epoch": 0.5071451595573224,
      "grad_norm": 1.1939259767532349,
      "learning_rate": 2.71963589076723e-05,
      "loss": 0.8551,
      "step": 1180
    },
    {
      "epoch": 0.511442999892554,
      "grad_norm": 1.5269997119903564,
      "learning_rate": 2.7170351105331602e-05,
      "loss": 0.8938,
      "step": 1190
    },
    {
      "epoch": 0.5157408402277855,
      "grad_norm": 2.283552408218384,
      "learning_rate": 2.7144343302990896e-05,
      "loss": 0.8962,
      "step": 1200
    },
    {
      "epoch": 0.520038680563017,
      "grad_norm": 2.4669644832611084,
      "learning_rate": 2.7118335500650197e-05,
      "loss": 0.9075,
      "step": 1210
    },
    {
      "epoch": 0.5243365208982487,
      "grad_norm": 2.3498830795288086,
      "learning_rate": 2.709232769830949e-05,
      "loss": 0.839,
      "step": 1220
    },
    {
      "epoch": 0.5286343612334802,
      "grad_norm": 1.1803995370864868,
      "learning_rate": 2.7066319895968792e-05,
      "loss": 0.8763,
      "step": 1230
    },
    {
      "epoch": 0.5329322015687117,
      "grad_norm": 1.564141869544983,
      "learning_rate": 2.7040312093628087e-05,
      "loss": 0.8695,
      "step": 1240
    },
    {
      "epoch": 0.5372300419039433,
      "grad_norm": 2.769319772720337,
      "learning_rate": 2.7014304291287388e-05,
      "loss": 0.8805,
      "step": 1250
    },
    {
      "epoch": 0.5415278822391748,
      "grad_norm": 1.3853942155838013,
      "learning_rate": 2.6988296488946682e-05,
      "loss": 0.8857,
      "step": 1260
    },
    {
      "epoch": 0.5458257225744063,
      "grad_norm": 1.4380059242248535,
      "learning_rate": 2.6962288686605983e-05,
      "loss": 0.8886,
      "step": 1270
    },
    {
      "epoch": 0.5501235629096379,
      "grad_norm": 1.5870716571807861,
      "learning_rate": 2.6936280884265284e-05,
      "loss": 0.9061,
      "step": 1280
    },
    {
      "epoch": 0.5544214032448694,
      "grad_norm": 1.8280760049819946,
      "learning_rate": 2.6910273081924578e-05,
      "loss": 0.864,
      "step": 1290
    },
    {
      "epoch": 0.5587192435801009,
      "grad_norm": 2.3969814777374268,
      "learning_rate": 2.688426527958388e-05,
      "loss": 0.9249,
      "step": 1300
    },
    {
      "epoch": 0.5630170839153326,
      "grad_norm": 1.623030662536621,
      "learning_rate": 2.6858257477243173e-05,
      "loss": 0.8712,
      "step": 1310
    },
    {
      "epoch": 0.5673149242505641,
      "grad_norm": 1.23959219455719,
      "learning_rate": 2.6832249674902474e-05,
      "loss": 0.8728,
      "step": 1320
    },
    {
      "epoch": 0.5716127645857957,
      "grad_norm": 1.6381367444992065,
      "learning_rate": 2.6806241872561768e-05,
      "loss": 0.9242,
      "step": 1330
    },
    {
      "epoch": 0.5759106049210272,
      "grad_norm": 1.5897910594940186,
      "learning_rate": 2.678023407022107e-05,
      "loss": 0.8812,
      "step": 1340
    },
    {
      "epoch": 0.5802084452562587,
      "grad_norm": 1.4334756135940552,
      "learning_rate": 2.6754226267880363e-05,
      "loss": 0.8502,
      "step": 1350
    },
    {
      "epoch": 0.5845062855914903,
      "grad_norm": 1.9995590448379517,
      "learning_rate": 2.6728218465539664e-05,
      "loss": 0.9116,
      "step": 1360
    },
    {
      "epoch": 0.5888041259267218,
      "grad_norm": 1.817491054534912,
      "learning_rate": 2.6702210663198958e-05,
      "loss": 0.8883,
      "step": 1370
    },
    {
      "epoch": 0.5931019662619533,
      "grad_norm": 1.5452651977539062,
      "learning_rate": 2.667620286085826e-05,
      "loss": 0.9068,
      "step": 1380
    },
    {
      "epoch": 0.597399806597185,
      "grad_norm": 1.7019668817520142,
      "learning_rate": 2.6650195058517553e-05,
      "loss": 0.8911,
      "step": 1390
    },
    {
      "epoch": 0.6016976469324165,
      "grad_norm": 1.782210350036621,
      "learning_rate": 2.6624187256176854e-05,
      "loss": 0.8764,
      "step": 1400
    },
    {
      "epoch": 0.605995487267648,
      "grad_norm": 1.8373172283172607,
      "learning_rate": 2.659817945383615e-05,
      "loss": 0.8742,
      "step": 1410
    },
    {
      "epoch": 0.6102933276028796,
      "grad_norm": 2.0278995037078857,
      "learning_rate": 2.657217165149545e-05,
      "loss": 0.9105,
      "step": 1420
    },
    {
      "epoch": 0.6145911679381111,
      "grad_norm": 1.7124911546707153,
      "learning_rate": 2.654616384915475e-05,
      "loss": 0.8621,
      "step": 1430
    },
    {
      "epoch": 0.6188890082733427,
      "grad_norm": 1.9258427619934082,
      "learning_rate": 2.6520156046814044e-05,
      "loss": 0.8642,
      "step": 1440
    },
    {
      "epoch": 0.6231868486085742,
      "grad_norm": 1.0418362617492676,
      "learning_rate": 2.6494148244473345e-05,
      "loss": 0.8614,
      "step": 1450
    },
    {
      "epoch": 0.6274846889438057,
      "grad_norm": 2.066481828689575,
      "learning_rate": 2.646814044213264e-05,
      "loss": 0.8715,
      "step": 1460
    },
    {
      "epoch": 0.6317825292790373,
      "grad_norm": 2.010335922241211,
      "learning_rate": 2.644213263979194e-05,
      "loss": 0.8884,
      "step": 1470
    },
    {
      "epoch": 0.6360803696142688,
      "grad_norm": 1.5888879299163818,
      "learning_rate": 2.6416124837451234e-05,
      "loss": 0.8438,
      "step": 1480
    },
    {
      "epoch": 0.6403782099495003,
      "grad_norm": 2.1659128665924072,
      "learning_rate": 2.6390117035110535e-05,
      "loss": 0.8836,
      "step": 1490
    },
    {
      "epoch": 0.644676050284732,
      "grad_norm": 2.0463523864746094,
      "learning_rate": 2.636410923276983e-05,
      "loss": 0.8866,
      "step": 1500
    },
    {
      "epoch": 0.6489738906199635,
      "grad_norm": 2.78353214263916,
      "learning_rate": 2.633810143042913e-05,
      "loss": 0.9162,
      "step": 1510
    },
    {
      "epoch": 0.653271730955195,
      "grad_norm": 1.6259557008743286,
      "learning_rate": 2.6312093628088425e-05,
      "loss": 0.8786,
      "step": 1520
    },
    {
      "epoch": 0.6575695712904266,
      "grad_norm": 1.491129755973816,
      "learning_rate": 2.6286085825747725e-05,
      "loss": 0.8803,
      "step": 1530
    },
    {
      "epoch": 0.6618674116256581,
      "grad_norm": 1.7967478036880493,
      "learning_rate": 2.6260078023407023e-05,
      "loss": 0.9179,
      "step": 1540
    },
    {
      "epoch": 0.6661652519608896,
      "grad_norm": 1.8079509735107422,
      "learning_rate": 2.623407022106632e-05,
      "loss": 0.9016,
      "step": 1550
    },
    {
      "epoch": 0.6704630922961212,
      "grad_norm": 1.4530149698257446,
      "learning_rate": 2.6208062418725618e-05,
      "loss": 0.8509,
      "step": 1560
    },
    {
      "epoch": 0.6747609326313527,
      "grad_norm": 1.401450276374817,
      "learning_rate": 2.6182054616384916e-05,
      "loss": 0.8789,
      "step": 1570
    },
    {
      "epoch": 0.6790587729665843,
      "grad_norm": 2.0368266105651855,
      "learning_rate": 2.6156046814044217e-05,
      "loss": 0.8756,
      "step": 1580
    },
    {
      "epoch": 0.6833566133018159,
      "grad_norm": 1.337648868560791,
      "learning_rate": 2.613003901170351e-05,
      "loss": 0.8786,
      "step": 1590
    },
    {
      "epoch": 0.6876544536370474,
      "grad_norm": 2.4348111152648926,
      "learning_rate": 2.610403120936281e-05,
      "loss": 0.8835,
      "step": 1600
    },
    {
      "epoch": 0.691952293972279,
      "grad_norm": 1.8632820844650269,
      "learning_rate": 2.6078023407022106e-05,
      "loss": 0.811,
      "step": 1610
    },
    {
      "epoch": 0.6962501343075105,
      "grad_norm": 1.782188057899475,
      "learning_rate": 2.6052015604681407e-05,
      "loss": 0.8625,
      "step": 1620
    },
    {
      "epoch": 0.700547974642742,
      "grad_norm": 1.7481275796890259,
      "learning_rate": 2.60260078023407e-05,
      "loss": 0.8636,
      "step": 1630
    },
    {
      "epoch": 0.7048458149779736,
      "grad_norm": 1.6880429983139038,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.8753,
      "step": 1640
    },
    {
      "epoch": 0.7091436553132051,
      "grad_norm": 2.012465715408325,
      "learning_rate": 2.5973992197659296e-05,
      "loss": 0.9314,
      "step": 1650
    },
    {
      "epoch": 0.7134414956484366,
      "grad_norm": 1.1981308460235596,
      "learning_rate": 2.5947984395318597e-05,
      "loss": 0.8631,
      "step": 1660
    },
    {
      "epoch": 0.7177393359836682,
      "grad_norm": 2.22318959236145,
      "learning_rate": 2.5921976592977894e-05,
      "loss": 0.8928,
      "step": 1670
    },
    {
      "epoch": 0.7220371763188997,
      "grad_norm": 2.611795425415039,
      "learning_rate": 2.5895968790637192e-05,
      "loss": 0.897,
      "step": 1680
    },
    {
      "epoch": 0.7263350166541312,
      "grad_norm": 1.8722865581512451,
      "learning_rate": 2.586996098829649e-05,
      "loss": 0.8786,
      "step": 1690
    },
    {
      "epoch": 0.7306328569893629,
      "grad_norm": 1.8729095458984375,
      "learning_rate": 2.5843953185955787e-05,
      "loss": 0.8908,
      "step": 1700
    },
    {
      "epoch": 0.7349306973245944,
      "grad_norm": 2.977431535720825,
      "learning_rate": 2.5817945383615085e-05,
      "loss": 0.8482,
      "step": 1710
    },
    {
      "epoch": 0.739228537659826,
      "grad_norm": 2.1187310218811035,
      "learning_rate": 2.5791937581274382e-05,
      "loss": 0.8798,
      "step": 1720
    },
    {
      "epoch": 0.7435263779950575,
      "grad_norm": 2.9335086345672607,
      "learning_rate": 2.5765929778933683e-05,
      "loss": 0.862,
      "step": 1730
    },
    {
      "epoch": 0.747824218330289,
      "grad_norm": 2.0331037044525146,
      "learning_rate": 2.5739921976592977e-05,
      "loss": 0.8738,
      "step": 1740
    },
    {
      "epoch": 0.7521220586655206,
      "grad_norm": 1.7363224029541016,
      "learning_rate": 2.5713914174252278e-05,
      "loss": 0.871,
      "step": 1750
    },
    {
      "epoch": 0.7564198990007521,
      "grad_norm": 1.7314996719360352,
      "learning_rate": 2.5687906371911572e-05,
      "loss": 0.8917,
      "step": 1760
    },
    {
      "epoch": 0.7607177393359836,
      "grad_norm": 1.530348539352417,
      "learning_rate": 2.5661898569570873e-05,
      "loss": 0.8221,
      "step": 1770
    },
    {
      "epoch": 0.7650155796712153,
      "grad_norm": 1.9359831809997559,
      "learning_rate": 2.5635890767230167e-05,
      "loss": 0.8592,
      "step": 1780
    },
    {
      "epoch": 0.7693134200064468,
      "grad_norm": 1.4959254264831543,
      "learning_rate": 2.5609882964889468e-05,
      "loss": 0.8756,
      "step": 1790
    },
    {
      "epoch": 0.7736112603416783,
      "grad_norm": 1.7099897861480713,
      "learning_rate": 2.5583875162548766e-05,
      "loss": 0.8229,
      "step": 1800
    },
    {
      "epoch": 0.7779091006769099,
      "grad_norm": 3.5300166606903076,
      "learning_rate": 2.5557867360208063e-05,
      "loss": 0.8645,
      "step": 1810
    },
    {
      "epoch": 0.7822069410121414,
      "grad_norm": 1.3300458192825317,
      "learning_rate": 2.553185955786736e-05,
      "loss": 0.8363,
      "step": 1820
    },
    {
      "epoch": 0.7865047813473729,
      "grad_norm": 1.8559931516647339,
      "learning_rate": 2.550585175552666e-05,
      "loss": 0.8728,
      "step": 1830
    },
    {
      "epoch": 0.7908026216826045,
      "grad_norm": 1.994932770729065,
      "learning_rate": 2.5479843953185956e-05,
      "loss": 0.8587,
      "step": 1840
    },
    {
      "epoch": 0.795100462017836,
      "grad_norm": 1.5656005144119263,
      "learning_rate": 2.5453836150845254e-05,
      "loss": 0.8868,
      "step": 1850
    },
    {
      "epoch": 0.7993983023530676,
      "grad_norm": 1.6939618587493896,
      "learning_rate": 2.542782834850455e-05,
      "loss": 0.864,
      "step": 1860
    },
    {
      "epoch": 0.8036961426882991,
      "grad_norm": 1.6485602855682373,
      "learning_rate": 2.540182054616385e-05,
      "loss": 0.8777,
      "step": 1870
    },
    {
      "epoch": 0.8079939830235306,
      "grad_norm": 2.400416374206543,
      "learning_rate": 2.537581274382315e-05,
      "loss": 0.8635,
      "step": 1880
    },
    {
      "epoch": 0.8122918233587623,
      "grad_norm": 1.7604485750198364,
      "learning_rate": 2.5349804941482444e-05,
      "loss": 0.8169,
      "step": 1890
    },
    {
      "epoch": 0.8165896636939938,
      "grad_norm": 1.6736923456192017,
      "learning_rate": 2.5323797139141745e-05,
      "loss": 0.8698,
      "step": 1900
    },
    {
      "epoch": 0.8208875040292253,
      "grad_norm": 2.572523593902588,
      "learning_rate": 2.529778933680104e-05,
      "loss": 0.8627,
      "step": 1910
    },
    {
      "epoch": 0.8251853443644569,
      "grad_norm": 1.5722655057907104,
      "learning_rate": 2.527178153446034e-05,
      "loss": 0.8131,
      "step": 1920
    },
    {
      "epoch": 0.8294831846996884,
      "grad_norm": 1.7519451379776,
      "learning_rate": 2.5245773732119637e-05,
      "loss": 0.8824,
      "step": 1930
    },
    {
      "epoch": 0.8337810250349199,
      "grad_norm": 2.6309916973114014,
      "learning_rate": 2.5219765929778935e-05,
      "loss": 0.8612,
      "step": 1940
    },
    {
      "epoch": 0.8380788653701515,
      "grad_norm": 2.0151212215423584,
      "learning_rate": 2.5193758127438232e-05,
      "loss": 0.8437,
      "step": 1950
    },
    {
      "epoch": 0.842376705705383,
      "grad_norm": 2.029317617416382,
      "learning_rate": 2.516775032509753e-05,
      "loss": 0.8834,
      "step": 1960
    },
    {
      "epoch": 0.8466745460406145,
      "grad_norm": 1.8256421089172363,
      "learning_rate": 2.5141742522756827e-05,
      "loss": 0.8589,
      "step": 1970
    },
    {
      "epoch": 0.8509723863758462,
      "grad_norm": 3.154003858566284,
      "learning_rate": 2.5115734720416125e-05,
      "loss": 0.8299,
      "step": 1980
    },
    {
      "epoch": 0.8552702267110777,
      "grad_norm": 1.3150641918182373,
      "learning_rate": 2.5089726918075422e-05,
      "loss": 0.8258,
      "step": 1990
    },
    {
      "epoch": 0.8595680670463093,
      "grad_norm": 2.105128049850464,
      "learning_rate": 2.506371911573472e-05,
      "loss": 0.8735,
      "step": 2000
    },
    {
      "epoch": 0.8638659073815408,
      "grad_norm": 1.9501115083694458,
      "learning_rate": 2.5037711313394018e-05,
      "loss": 0.869,
      "step": 2010
    },
    {
      "epoch": 0.8681637477167723,
      "grad_norm": 1.9979995489120483,
      "learning_rate": 2.5011703511053315e-05,
      "loss": 0.8348,
      "step": 2020
    },
    {
      "epoch": 0.8724615880520039,
      "grad_norm": 3.6857316493988037,
      "learning_rate": 2.4985695708712616e-05,
      "loss": 0.9121,
      "step": 2030
    },
    {
      "epoch": 0.8767594283872354,
      "grad_norm": 1.4154495000839233,
      "learning_rate": 2.4959687906371914e-05,
      "loss": 0.8459,
      "step": 2040
    },
    {
      "epoch": 0.8810572687224669,
      "grad_norm": 3.119694948196411,
      "learning_rate": 2.493368010403121e-05,
      "loss": 0.8425,
      "step": 2050
    },
    {
      "epoch": 0.8853551090576985,
      "grad_norm": 1.5688881874084473,
      "learning_rate": 2.490767230169051e-05,
      "loss": 0.8341,
      "step": 2060
    },
    {
      "epoch": 0.88965294939293,
      "grad_norm": 1.722669243812561,
      "learning_rate": 2.4881664499349806e-05,
      "loss": 0.87,
      "step": 2070
    },
    {
      "epoch": 0.8939507897281616,
      "grad_norm": 1.4348880052566528,
      "learning_rate": 2.4855656697009104e-05,
      "loss": 0.8482,
      "step": 2080
    },
    {
      "epoch": 0.8982486300633932,
      "grad_norm": 1.803605079650879,
      "learning_rate": 2.48296488946684e-05,
      "loss": 0.8492,
      "step": 2090
    },
    {
      "epoch": 0.9025464703986247,
      "grad_norm": 1.6409341096878052,
      "learning_rate": 2.48036410923277e-05,
      "loss": 0.846,
      "step": 2100
    },
    {
      "epoch": 0.9068443107338562,
      "grad_norm": 1.875947117805481,
      "learning_rate": 2.4777633289986996e-05,
      "loss": 0.8526,
      "step": 2110
    },
    {
      "epoch": 0.9111421510690878,
      "grad_norm": 1.6677769422531128,
      "learning_rate": 2.4751625487646294e-05,
      "loss": 0.8489,
      "step": 2120
    },
    {
      "epoch": 0.9154399914043193,
      "grad_norm": 2.3566460609436035,
      "learning_rate": 2.472561768530559e-05,
      "loss": 0.8436,
      "step": 2130
    },
    {
      "epoch": 0.9197378317395509,
      "grad_norm": 2.425994634628296,
      "learning_rate": 2.469960988296489e-05,
      "loss": 0.9152,
      "step": 2140
    },
    {
      "epoch": 0.9240356720747824,
      "grad_norm": 1.7716435194015503,
      "learning_rate": 2.4673602080624187e-05,
      "loss": 0.8549,
      "step": 2150
    },
    {
      "epoch": 0.9283335124100139,
      "grad_norm": 1.8667963743209839,
      "learning_rate": 2.4647594278283487e-05,
      "loss": 0.8599,
      "step": 2160
    },
    {
      "epoch": 0.9326313527452456,
      "grad_norm": 1.7791329622268677,
      "learning_rate": 2.4621586475942785e-05,
      "loss": 0.8848,
      "step": 2170
    },
    {
      "epoch": 0.9369291930804771,
      "grad_norm": 2.4081404209136963,
      "learning_rate": 2.4595578673602083e-05,
      "loss": 0.8806,
      "step": 2180
    },
    {
      "epoch": 0.9412270334157086,
      "grad_norm": 1.9877556562423706,
      "learning_rate": 2.456957087126138e-05,
      "loss": 0.8597,
      "step": 2190
    },
    {
      "epoch": 0.9455248737509402,
      "grad_norm": 1.7890280485153198,
      "learning_rate": 2.4543563068920678e-05,
      "loss": 0.8747,
      "step": 2200
    },
    {
      "epoch": 0.9498227140861717,
      "grad_norm": 1.854243278503418,
      "learning_rate": 2.4517555266579975e-05,
      "loss": 0.8832,
      "step": 2210
    },
    {
      "epoch": 0.9541205544214032,
      "grad_norm": 1.790817141532898,
      "learning_rate": 2.4491547464239273e-05,
      "loss": 0.8814,
      "step": 2220
    },
    {
      "epoch": 0.9584183947566348,
      "grad_norm": 1.8040492534637451,
      "learning_rate": 2.446553966189857e-05,
      "loss": 0.8871,
      "step": 2230
    },
    {
      "epoch": 0.9627162350918663,
      "grad_norm": 2.0629522800445557,
      "learning_rate": 2.4439531859557868e-05,
      "loss": 0.9049,
      "step": 2240
    },
    {
      "epoch": 0.9670140754270978,
      "grad_norm": 1.7184243202209473,
      "learning_rate": 2.4413524057217165e-05,
      "loss": 0.8293,
      "step": 2250
    },
    {
      "epoch": 0.9713119157623294,
      "grad_norm": 1.8312731981277466,
      "learning_rate": 2.4387516254876463e-05,
      "loss": 0.894,
      "step": 2260
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 2.056771993637085,
      "learning_rate": 2.436150845253576e-05,
      "loss": 0.8979,
      "step": 2270
    },
    {
      "epoch": 0.9799075964327926,
      "grad_norm": 2.200263023376465,
      "learning_rate": 2.4335500650195058e-05,
      "loss": 0.8503,
      "step": 2280
    },
    {
      "epoch": 0.9842054367680241,
      "grad_norm": 1.8005846738815308,
      "learning_rate": 2.4309492847854355e-05,
      "loss": 0.8584,
      "step": 2290
    },
    {
      "epoch": 0.9885032771032556,
      "grad_norm": 1.5184662342071533,
      "learning_rate": 2.4283485045513656e-05,
      "loss": 0.866,
      "step": 2300
    },
    {
      "epoch": 0.9928011174384872,
      "grad_norm": 1.7222442626953125,
      "learning_rate": 2.4257477243172954e-05,
      "loss": 0.8549,
      "step": 2310
    },
    {
      "epoch": 0.9970989577737187,
      "grad_norm": 2.6276566982269287,
      "learning_rate": 2.423146944083225e-05,
      "loss": 0.8532,
      "step": 2320
    },
    {
      "epoch": 1.0012893521005695,
      "grad_norm": 2.892911911010742,
      "learning_rate": 2.420546163849155e-05,
      "loss": 0.8873,
      "step": 2330
    },
    {
      "epoch": 1.005587192435801,
      "grad_norm": 1.4807909727096558,
      "learning_rate": 2.4179453836150847e-05,
      "loss": 0.8743,
      "step": 2340
    },
    {
      "epoch": 1.0098850327710325,
      "grad_norm": 1.6499543190002441,
      "learning_rate": 2.4153446033810144e-05,
      "loss": 0.8374,
      "step": 2350
    },
    {
      "epoch": 1.014182873106264,
      "grad_norm": 2.3757448196411133,
      "learning_rate": 2.412743823146944e-05,
      "loss": 0.8361,
      "step": 2360
    },
    {
      "epoch": 1.0184807134414957,
      "grad_norm": 1.8433775901794434,
      "learning_rate": 2.410143042912874e-05,
      "loss": 0.8372,
      "step": 2370
    },
    {
      "epoch": 1.022778553776727,
      "grad_norm": 2.2961199283599854,
      "learning_rate": 2.4075422626788037e-05,
      "loss": 0.8502,
      "step": 2380
    },
    {
      "epoch": 1.0270763941119587,
      "grad_norm": 2.5799357891082764,
      "learning_rate": 2.4049414824447334e-05,
      "loss": 0.8474,
      "step": 2390
    },
    {
      "epoch": 1.0313742344471903,
      "grad_norm": 1.5607377290725708,
      "learning_rate": 2.4023407022106632e-05,
      "loss": 0.8901,
      "step": 2400
    },
    {
      "epoch": 1.0356720747824217,
      "grad_norm": 1.7967209815979004,
      "learning_rate": 2.399739921976593e-05,
      "loss": 0.8489,
      "step": 2410
    },
    {
      "epoch": 1.0399699151176534,
      "grad_norm": 1.6764206886291504,
      "learning_rate": 2.3971391417425227e-05,
      "loss": 0.8228,
      "step": 2420
    },
    {
      "epoch": 1.044267755452885,
      "grad_norm": 1.5853476524353027,
      "learning_rate": 2.3945383615084528e-05,
      "loss": 0.8384,
      "step": 2430
    },
    {
      "epoch": 1.0485655957881164,
      "grad_norm": 1.7456718683242798,
      "learning_rate": 2.3919375812743822e-05,
      "loss": 0.8419,
      "step": 2440
    },
    {
      "epoch": 1.052863436123348,
      "grad_norm": 2.566309928894043,
      "learning_rate": 2.3893368010403123e-05,
      "loss": 0.8215,
      "step": 2450
    },
    {
      "epoch": 1.0571612764585796,
      "grad_norm": 1.7233147621154785,
      "learning_rate": 2.386736020806242e-05,
      "loss": 0.8198,
      "step": 2460
    },
    {
      "epoch": 1.0614591167938112,
      "grad_norm": 2.4802796840667725,
      "learning_rate": 2.3841352405721718e-05,
      "loss": 0.8817,
      "step": 2470
    },
    {
      "epoch": 1.0657569571290426,
      "grad_norm": 1.4367990493774414,
      "learning_rate": 2.3815344603381016e-05,
      "loss": 0.8413,
      "step": 2480
    },
    {
      "epoch": 1.0700547974642742,
      "grad_norm": 1.8097659349441528,
      "learning_rate": 2.3789336801040313e-05,
      "loss": 0.8602,
      "step": 2490
    },
    {
      "epoch": 1.0743526377995058,
      "grad_norm": 2.9138147830963135,
      "learning_rate": 2.376332899869961e-05,
      "loss": 0.8305,
      "step": 2500
    },
    {
      "epoch": 1.0786504781347372,
      "grad_norm": 2.0294322967529297,
      "learning_rate": 2.3737321196358908e-05,
      "loss": 0.8427,
      "step": 2510
    },
    {
      "epoch": 1.0829483184699689,
      "grad_norm": 2.0641326904296875,
      "learning_rate": 2.3711313394018206e-05,
      "loss": 0.859,
      "step": 2520
    },
    {
      "epoch": 1.0872461588052005,
      "grad_norm": 2.2581496238708496,
      "learning_rate": 2.3685305591677503e-05,
      "loss": 0.8599,
      "step": 2530
    },
    {
      "epoch": 1.0915439991404319,
      "grad_norm": 1.8581123352050781,
      "learning_rate": 2.36592977893368e-05,
      "loss": 0.8665,
      "step": 2540
    },
    {
      "epoch": 1.0958418394756635,
      "grad_norm": 2.5537071228027344,
      "learning_rate": 2.3633289986996098e-05,
      "loss": 0.8652,
      "step": 2550
    },
    {
      "epoch": 1.100139679810895,
      "grad_norm": 1.9574370384216309,
      "learning_rate": 2.36072821846554e-05,
      "loss": 0.8514,
      "step": 2560
    },
    {
      "epoch": 1.1044375201461265,
      "grad_norm": 1.7719886302947998,
      "learning_rate": 2.3581274382314693e-05,
      "loss": 0.8734,
      "step": 2570
    },
    {
      "epoch": 1.1087353604813581,
      "grad_norm": 3.611604690551758,
      "learning_rate": 2.3555266579973994e-05,
      "loss": 0.8705,
      "step": 2580
    },
    {
      "epoch": 1.1130332008165897,
      "grad_norm": 1.984591007232666,
      "learning_rate": 2.352925877763329e-05,
      "loss": 0.8319,
      "step": 2590
    },
    {
      "epoch": 1.1173310411518211,
      "grad_norm": 2.5148088932037354,
      "learning_rate": 2.350325097529259e-05,
      "loss": 0.871,
      "step": 2600
    },
    {
      "epoch": 1.1216288814870528,
      "grad_norm": 2.4449338912963867,
      "learning_rate": 2.3477243172951887e-05,
      "loss": 0.8495,
      "step": 2610
    },
    {
      "epoch": 1.1259267218222844,
      "grad_norm": 2.82202410697937,
      "learning_rate": 2.3451235370611184e-05,
      "loss": 0.8185,
      "step": 2620
    },
    {
      "epoch": 1.1302245621575158,
      "grad_norm": 1.8514169454574585,
      "learning_rate": 2.3425227568270482e-05,
      "loss": 0.8484,
      "step": 2630
    },
    {
      "epoch": 1.1345224024927474,
      "grad_norm": 2.4035680294036865,
      "learning_rate": 2.339921976592978e-05,
      "loss": 0.8162,
      "step": 2640
    },
    {
      "epoch": 1.138820242827979,
      "grad_norm": 1.7463104724884033,
      "learning_rate": 2.3373211963589077e-05,
      "loss": 0.9021,
      "step": 2650
    },
    {
      "epoch": 1.1431180831632104,
      "grad_norm": 1.4814001321792603,
      "learning_rate": 2.3347204161248375e-05,
      "loss": 0.8543,
      "step": 2660
    },
    {
      "epoch": 1.147415923498442,
      "grad_norm": 2.523808240890503,
      "learning_rate": 2.3321196358907672e-05,
      "loss": 0.8926,
      "step": 2670
    },
    {
      "epoch": 1.1517137638336736,
      "grad_norm": 1.9545660018920898,
      "learning_rate": 2.329518855656697e-05,
      "loss": 0.8443,
      "step": 2680
    },
    {
      "epoch": 1.156011604168905,
      "grad_norm": 1.974391222000122,
      "learning_rate": 2.326918075422627e-05,
      "loss": 0.8696,
      "step": 2690
    },
    {
      "epoch": 1.1603094445041366,
      "grad_norm": 3.195638656616211,
      "learning_rate": 2.3243172951885565e-05,
      "loss": 0.8616,
      "step": 2700
    },
    {
      "epoch": 1.1646072848393683,
      "grad_norm": 1.5661324262619019,
      "learning_rate": 2.3217165149544866e-05,
      "loss": 0.8446,
      "step": 2710
    },
    {
      "epoch": 1.1689051251745997,
      "grad_norm": 2.7836809158325195,
      "learning_rate": 2.319115734720416e-05,
      "loss": 0.8724,
      "step": 2720
    },
    {
      "epoch": 1.1732029655098313,
      "grad_norm": 2.0062007904052734,
      "learning_rate": 2.316514954486346e-05,
      "loss": 0.8598,
      "step": 2730
    },
    {
      "epoch": 1.177500805845063,
      "grad_norm": 3.0623972415924072,
      "learning_rate": 2.3139141742522755e-05,
      "loss": 0.8675,
      "step": 2740
    },
    {
      "epoch": 1.1817986461802943,
      "grad_norm": 1.7845449447631836,
      "learning_rate": 2.3113133940182056e-05,
      "loss": 0.864,
      "step": 2750
    },
    {
      "epoch": 1.186096486515526,
      "grad_norm": 1.559212565422058,
      "learning_rate": 2.3087126137841353e-05,
      "loss": 0.8833,
      "step": 2760
    },
    {
      "epoch": 1.1903943268507575,
      "grad_norm": 2.618772268295288,
      "learning_rate": 2.306111833550065e-05,
      "loss": 0.856,
      "step": 2770
    },
    {
      "epoch": 1.194692167185989,
      "grad_norm": 1.578370451927185,
      "learning_rate": 2.303511053315995e-05,
      "loss": 0.8351,
      "step": 2780
    },
    {
      "epoch": 1.1989900075212205,
      "grad_norm": 2.1163361072540283,
      "learning_rate": 2.3009102730819246e-05,
      "loss": 0.8658,
      "step": 2790
    },
    {
      "epoch": 1.2032878478564522,
      "grad_norm": 1.3161565065383911,
      "learning_rate": 2.2983094928478544e-05,
      "loss": 0.8367,
      "step": 2800
    },
    {
      "epoch": 1.2075856881916838,
      "grad_norm": 1.4614791870117188,
      "learning_rate": 2.295708712613784e-05,
      "loss": 0.8356,
      "step": 2810
    },
    {
      "epoch": 1.2118835285269152,
      "grad_norm": 2.143406629562378,
      "learning_rate": 2.2931079323797142e-05,
      "loss": 0.8265,
      "step": 2820
    },
    {
      "epoch": 1.2161813688621468,
      "grad_norm": 3.758512258529663,
      "learning_rate": 2.2905071521456436e-05,
      "loss": 0.8434,
      "step": 2830
    },
    {
      "epoch": 1.2204792091973784,
      "grad_norm": 1.5412895679473877,
      "learning_rate": 2.2879063719115737e-05,
      "loss": 0.8555,
      "step": 2840
    },
    {
      "epoch": 1.2247770495326098,
      "grad_norm": 1.6586062908172607,
      "learning_rate": 2.285305591677503e-05,
      "loss": 0.8145,
      "step": 2850
    },
    {
      "epoch": 1.2290748898678414,
      "grad_norm": 2.0543086528778076,
      "learning_rate": 2.2827048114434332e-05,
      "loss": 0.899,
      "step": 2860
    },
    {
      "epoch": 1.233372730203073,
      "grad_norm": 2.275569438934326,
      "learning_rate": 2.2801040312093626e-05,
      "loss": 0.8693,
      "step": 2870
    },
    {
      "epoch": 1.2376705705383044,
      "grad_norm": 1.518336534500122,
      "learning_rate": 2.2775032509752927e-05,
      "loss": 0.8335,
      "step": 2880
    },
    {
      "epoch": 1.241968410873536,
      "grad_norm": 2.104902744293213,
      "learning_rate": 2.274902470741222e-05,
      "loss": 0.8767,
      "step": 2890
    },
    {
      "epoch": 1.2462662512087677,
      "grad_norm": 2.584160804748535,
      "learning_rate": 2.2723016905071522e-05,
      "loss": 0.8772,
      "step": 2900
    },
    {
      "epoch": 1.2505640915439993,
      "grad_norm": 2.1880643367767334,
      "learning_rate": 2.269700910273082e-05,
      "loss": 0.8696,
      "step": 2910
    },
    {
      "epoch": 1.2548619318792307,
      "grad_norm": 1.7208362817764282,
      "learning_rate": 2.2671001300390117e-05,
      "loss": 0.8745,
      "step": 2920
    },
    {
      "epoch": 1.2591597722144623,
      "grad_norm": 2.432770252227783,
      "learning_rate": 2.2644993498049415e-05,
      "loss": 0.8512,
      "step": 2930
    },
    {
      "epoch": 1.263457612549694,
      "grad_norm": 1.714034080505371,
      "learning_rate": 2.2618985695708713e-05,
      "loss": 0.8631,
      "step": 2940
    },
    {
      "epoch": 1.2677554528849253,
      "grad_norm": 2.5886051654815674,
      "learning_rate": 2.2592977893368013e-05,
      "loss": 0.8454,
      "step": 2950
    },
    {
      "epoch": 1.272053293220157,
      "grad_norm": 1.4578248262405396,
      "learning_rate": 2.2566970091027308e-05,
      "loss": 0.8485,
      "step": 2960
    },
    {
      "epoch": 1.2763511335553885,
      "grad_norm": 1.7042062282562256,
      "learning_rate": 2.254096228868661e-05,
      "loss": 0.8484,
      "step": 2970
    },
    {
      "epoch": 1.28064897389062,
      "grad_norm": 1.442399501800537,
      "learning_rate": 2.2514954486345903e-05,
      "loss": 0.8286,
      "step": 2980
    },
    {
      "epoch": 1.2849468142258516,
      "grad_norm": 1.678686261177063,
      "learning_rate": 2.2488946684005204e-05,
      "loss": 0.8239,
      "step": 2990
    },
    {
      "epoch": 1.2892446545610832,
      "grad_norm": 1.8662612438201904,
      "learning_rate": 2.2462938881664498e-05,
      "loss": 0.8363,
      "step": 3000
    },
    {
      "epoch": 1.2935424948963146,
      "grad_norm": 1.873789668083191,
      "learning_rate": 2.24369310793238e-05,
      "loss": 0.8585,
      "step": 3010
    },
    {
      "epoch": 1.2978403352315462,
      "grad_norm": 2.357743978500366,
      "learning_rate": 2.2410923276983093e-05,
      "loss": 0.8821,
      "step": 3020
    },
    {
      "epoch": 1.3021381755667778,
      "grad_norm": 1.725853443145752,
      "learning_rate": 2.2384915474642394e-05,
      "loss": 0.8225,
      "step": 3030
    },
    {
      "epoch": 1.3064360159020092,
      "grad_norm": 1.9082525968551636,
      "learning_rate": 2.2358907672301688e-05,
      "loss": 0.8705,
      "step": 3040
    },
    {
      "epoch": 1.3107338562372408,
      "grad_norm": 2.170088529586792,
      "learning_rate": 2.233289986996099e-05,
      "loss": 0.8541,
      "step": 3050
    },
    {
      "epoch": 1.3150316965724724,
      "grad_norm": 2.4072325229644775,
      "learning_rate": 2.2306892067620286e-05,
      "loss": 0.8603,
      "step": 3060
    },
    {
      "epoch": 1.3193295369077038,
      "grad_norm": 2.650087594985962,
      "learning_rate": 2.2280884265279584e-05,
      "loss": 0.8441,
      "step": 3070
    },
    {
      "epoch": 1.3236273772429354,
      "grad_norm": 1.3174396753311157,
      "learning_rate": 2.2254876462938885e-05,
      "loss": 0.8355,
      "step": 3080
    },
    {
      "epoch": 1.327925217578167,
      "grad_norm": 1.3162908554077148,
      "learning_rate": 2.222886866059818e-05,
      "loss": 0.8093,
      "step": 3090
    },
    {
      "epoch": 1.3322230579133985,
      "grad_norm": 3.1211609840393066,
      "learning_rate": 2.220286085825748e-05,
      "loss": 0.8668,
      "step": 3100
    },
    {
      "epoch": 1.33652089824863,
      "grad_norm": 3.4452528953552246,
      "learning_rate": 2.2176853055916774e-05,
      "loss": 0.8619,
      "step": 3110
    },
    {
      "epoch": 1.3408187385838617,
      "grad_norm": 2.367417097091675,
      "learning_rate": 2.2150845253576075e-05,
      "loss": 0.9008,
      "step": 3120
    },
    {
      "epoch": 1.345116578919093,
      "grad_norm": 2.4884450435638428,
      "learning_rate": 2.212483745123537e-05,
      "loss": 0.855,
      "step": 3130
    },
    {
      "epoch": 1.3494144192543247,
      "grad_norm": 2.0867679119110107,
      "learning_rate": 2.209882964889467e-05,
      "loss": 0.8716,
      "step": 3140
    },
    {
      "epoch": 1.3537122595895563,
      "grad_norm": 2.108508586883545,
      "learning_rate": 2.2072821846553964e-05,
      "loss": 0.8661,
      "step": 3150
    },
    {
      "epoch": 1.3580100999247877,
      "grad_norm": 2.0285420417785645,
      "learning_rate": 2.2046814044213265e-05,
      "loss": 0.8465,
      "step": 3160
    },
    {
      "epoch": 1.3623079402600193,
      "grad_norm": 1.5024068355560303,
      "learning_rate": 2.202080624187256e-05,
      "loss": 0.8506,
      "step": 3170
    },
    {
      "epoch": 1.366605780595251,
      "grad_norm": 1.7672098875045776,
      "learning_rate": 2.199479843953186e-05,
      "loss": 0.8947,
      "step": 3180
    },
    {
      "epoch": 1.3709036209304823,
      "grad_norm": 2.524399757385254,
      "learning_rate": 2.196879063719116e-05,
      "loss": 0.8699,
      "step": 3190
    },
    {
      "epoch": 1.375201461265714,
      "grad_norm": 2.1764533519744873,
      "learning_rate": 2.1942782834850455e-05,
      "loss": 0.8808,
      "step": 3200
    },
    {
      "epoch": 1.3794993016009456,
      "grad_norm": 1.9962279796600342,
      "learning_rate": 2.1916775032509756e-05,
      "loss": 0.8277,
      "step": 3210
    },
    {
      "epoch": 1.383797141936177,
      "grad_norm": 2.0350756645202637,
      "learning_rate": 2.189076723016905e-05,
      "loss": 0.8557,
      "step": 3220
    },
    {
      "epoch": 1.3880949822714086,
      "grad_norm": 2.011932611465454,
      "learning_rate": 2.186475942782835e-05,
      "loss": 0.8535,
      "step": 3230
    },
    {
      "epoch": 1.3923928226066402,
      "grad_norm": 1.6926153898239136,
      "learning_rate": 2.1838751625487646e-05,
      "loss": 0.8448,
      "step": 3240
    },
    {
      "epoch": 1.3966906629418716,
      "grad_norm": 1.964801549911499,
      "learning_rate": 2.1812743823146946e-05,
      "loss": 0.841,
      "step": 3250
    },
    {
      "epoch": 1.4009885032771032,
      "grad_norm": 2.13158917427063,
      "learning_rate": 2.178673602080624e-05,
      "loss": 0.8502,
      "step": 3260
    },
    {
      "epoch": 1.4052863436123348,
      "grad_norm": 2.726306676864624,
      "learning_rate": 2.176072821846554e-05,
      "loss": 0.856,
      "step": 3270
    },
    {
      "epoch": 1.4095841839475662,
      "grad_norm": 3.06606125831604,
      "learning_rate": 2.1734720416124836e-05,
      "loss": 0.8936,
      "step": 3280
    },
    {
      "epoch": 1.4138820242827979,
      "grad_norm": 1.8363112211227417,
      "learning_rate": 2.1708712613784137e-05,
      "loss": 0.8616,
      "step": 3290
    },
    {
      "epoch": 1.4181798646180295,
      "grad_norm": 1.451494812965393,
      "learning_rate": 2.168270481144343e-05,
      "loss": 0.8363,
      "step": 3300
    },
    {
      "epoch": 1.4224777049532609,
      "grad_norm": 2.3072173595428467,
      "learning_rate": 2.1656697009102732e-05,
      "loss": 0.9082,
      "step": 3310
    },
    {
      "epoch": 1.4267755452884925,
      "grad_norm": 2.076808452606201,
      "learning_rate": 2.163068920676203e-05,
      "loss": 0.8502,
      "step": 3320
    },
    {
      "epoch": 1.431073385623724,
      "grad_norm": 2.0619089603424072,
      "learning_rate": 2.1604681404421327e-05,
      "loss": 0.8491,
      "step": 3330
    },
    {
      "epoch": 1.4353712259589555,
      "grad_norm": 1.9419862031936646,
      "learning_rate": 2.1578673602080628e-05,
      "loss": 0.8826,
      "step": 3340
    },
    {
      "epoch": 1.4396690662941871,
      "grad_norm": 1.6688768863677979,
      "learning_rate": 2.1552665799739922e-05,
      "loss": 0.855,
      "step": 3350
    },
    {
      "epoch": 1.4439669066294187,
      "grad_norm": 2.1687190532684326,
      "learning_rate": 2.1526657997399223e-05,
      "loss": 0.8676,
      "step": 3360
    },
    {
      "epoch": 1.4482647469646501,
      "grad_norm": 1.66860032081604,
      "learning_rate": 2.1500650195058517e-05,
      "loss": 0.887,
      "step": 3370
    },
    {
      "epoch": 1.4525625872998817,
      "grad_norm": 1.769540786743164,
      "learning_rate": 2.1474642392717818e-05,
      "loss": 0.8633,
      "step": 3380
    },
    {
      "epoch": 1.4568604276351134,
      "grad_norm": 2.4283523559570312,
      "learning_rate": 2.1448634590377112e-05,
      "loss": 0.8614,
      "step": 3390
    },
    {
      "epoch": 1.461158267970345,
      "grad_norm": 1.6645978689193726,
      "learning_rate": 2.1422626788036413e-05,
      "loss": 0.8676,
      "step": 3400
    },
    {
      "epoch": 1.4654561083055764,
      "grad_norm": 1.4203821420669556,
      "learning_rate": 2.1396618985695707e-05,
      "loss": 0.8601,
      "step": 3410
    },
    {
      "epoch": 1.469753948640808,
      "grad_norm": 1.764943242073059,
      "learning_rate": 2.1370611183355008e-05,
      "loss": 0.8486,
      "step": 3420
    },
    {
      "epoch": 1.4740517889760396,
      "grad_norm": 2.2102954387664795,
      "learning_rate": 2.1344603381014302e-05,
      "loss": 0.8633,
      "step": 3430
    },
    {
      "epoch": 1.478349629311271,
      "grad_norm": 2.399289846420288,
      "learning_rate": 2.1318595578673603e-05,
      "loss": 0.8329,
      "step": 3440
    },
    {
      "epoch": 1.4826474696465026,
      "grad_norm": 2.5122244358062744,
      "learning_rate": 2.12925877763329e-05,
      "loss": 0.8698,
      "step": 3450
    },
    {
      "epoch": 1.4869453099817342,
      "grad_norm": 3.067595958709717,
      "learning_rate": 2.1266579973992198e-05,
      "loss": 0.8708,
      "step": 3460
    },
    {
      "epoch": 1.4912431503169659,
      "grad_norm": 1.7518510818481445,
      "learning_rate": 2.1240572171651496e-05,
      "loss": 0.7936,
      "step": 3470
    },
    {
      "epoch": 1.4955409906521973,
      "grad_norm": 2.564427614212036,
      "learning_rate": 2.1214564369310793e-05,
      "loss": 0.8561,
      "step": 3480
    },
    {
      "epoch": 1.4998388309874289,
      "grad_norm": 1.7492276430130005,
      "learning_rate": 2.1188556566970094e-05,
      "loss": 0.8376,
      "step": 3490
    },
    {
      "epoch": 1.5041366713226605,
      "grad_norm": 1.8672897815704346,
      "learning_rate": 2.116254876462939e-05,
      "loss": 0.8094,
      "step": 3500
    },
    {
      "epoch": 1.5084345116578919,
      "grad_norm": 2.2475333213806152,
      "learning_rate": 2.113654096228869e-05,
      "loss": 0.8616,
      "step": 3510
    },
    {
      "epoch": 1.5127323519931235,
      "grad_norm": 1.8386662006378174,
      "learning_rate": 2.1110533159947983e-05,
      "loss": 0.8426,
      "step": 3520
    },
    {
      "epoch": 1.5170301923283551,
      "grad_norm": 2.0957202911376953,
      "learning_rate": 2.1084525357607284e-05,
      "loss": 0.8577,
      "step": 3530
    },
    {
      "epoch": 1.5213280326635865,
      "grad_norm": 1.5686161518096924,
      "learning_rate": 2.105851755526658e-05,
      "loss": 0.8173,
      "step": 3540
    },
    {
      "epoch": 1.5256258729988181,
      "grad_norm": 1.8499078750610352,
      "learning_rate": 2.103250975292588e-05,
      "loss": 0.8759,
      "step": 3550
    },
    {
      "epoch": 1.5299237133340498,
      "grad_norm": 2.209566116333008,
      "learning_rate": 2.1006501950585174e-05,
      "loss": 0.8488,
      "step": 3560
    },
    {
      "epoch": 1.5342215536692811,
      "grad_norm": 1.6881401538848877,
      "learning_rate": 2.0980494148244475e-05,
      "loss": 0.8726,
      "step": 3570
    },
    {
      "epoch": 1.5385193940045128,
      "grad_norm": 1.800757646560669,
      "learning_rate": 2.0954486345903772e-05,
      "loss": 0.8179,
      "step": 3580
    },
    {
      "epoch": 1.5428172343397444,
      "grad_norm": 2.0352225303649902,
      "learning_rate": 2.092847854356307e-05,
      "loss": 0.881,
      "step": 3590
    },
    {
      "epoch": 1.5471150746749758,
      "grad_norm": 2.7968475818634033,
      "learning_rate": 2.0902470741222367e-05,
      "loss": 0.8329,
      "step": 3600
    },
    {
      "epoch": 1.5514129150102074,
      "grad_norm": 2.329953193664551,
      "learning_rate": 2.0876462938881665e-05,
      "loss": 0.8607,
      "step": 3610
    },
    {
      "epoch": 1.555710755345439,
      "grad_norm": 2.476667881011963,
      "learning_rate": 2.0850455136540962e-05,
      "loss": 0.8886,
      "step": 3620
    },
    {
      "epoch": 1.5600085956806704,
      "grad_norm": 1.942586064338684,
      "learning_rate": 2.082444733420026e-05,
      "loss": 0.857,
      "step": 3630
    },
    {
      "epoch": 1.564306436015902,
      "grad_norm": 1.6548830270767212,
      "learning_rate": 2.079843953185956e-05,
      "loss": 0.8504,
      "step": 3640
    },
    {
      "epoch": 1.5686042763511336,
      "grad_norm": 2.177999973297119,
      "learning_rate": 2.0772431729518855e-05,
      "loss": 0.8755,
      "step": 3650
    },
    {
      "epoch": 1.572902116686365,
      "grad_norm": 2.689058303833008,
      "learning_rate": 2.0746423927178156e-05,
      "loss": 0.8797,
      "step": 3660
    },
    {
      "epoch": 1.5771999570215967,
      "grad_norm": 1.8910014629364014,
      "learning_rate": 2.072041612483745e-05,
      "loss": 0.852,
      "step": 3670
    },
    {
      "epoch": 1.5814977973568283,
      "grad_norm": 1.7863155603408813,
      "learning_rate": 2.069440832249675e-05,
      "loss": 0.8657,
      "step": 3680
    },
    {
      "epoch": 1.5857956376920597,
      "grad_norm": 3.8353285789489746,
      "learning_rate": 2.0668400520156045e-05,
      "loss": 0.8936,
      "step": 3690
    },
    {
      "epoch": 1.5900934780272913,
      "grad_norm": 1.7059872150421143,
      "learning_rate": 2.0642392717815346e-05,
      "loss": 0.8157,
      "step": 3700
    },
    {
      "epoch": 1.594391318362523,
      "grad_norm": 1.5183550119400024,
      "learning_rate": 2.0616384915474643e-05,
      "loss": 0.8519,
      "step": 3710
    },
    {
      "epoch": 1.5986891586977543,
      "grad_norm": 1.9664701223373413,
      "learning_rate": 2.059037711313394e-05,
      "loss": 0.8605,
      "step": 3720
    },
    {
      "epoch": 1.602986999032986,
      "grad_norm": 1.8128840923309326,
      "learning_rate": 2.056436931079324e-05,
      "loss": 0.8437,
      "step": 3730
    },
    {
      "epoch": 1.6072848393682175,
      "grad_norm": 1.5457382202148438,
      "learning_rate": 2.0538361508452536e-05,
      "loss": 0.8763,
      "step": 3740
    },
    {
      "epoch": 1.611582679703449,
      "grad_norm": 1.5927428007125854,
      "learning_rate": 2.0512353706111834e-05,
      "loss": 0.8499,
      "step": 3750
    },
    {
      "epoch": 1.6158805200386805,
      "grad_norm": 1.99697744846344,
      "learning_rate": 2.048634590377113e-05,
      "loss": 0.8541,
      "step": 3760
    },
    {
      "epoch": 1.6201783603739122,
      "grad_norm": 1.7087615728378296,
      "learning_rate": 2.046033810143043e-05,
      "loss": 0.8629,
      "step": 3770
    },
    {
      "epoch": 1.6244762007091436,
      "grad_norm": 2.5878634452819824,
      "learning_rate": 2.0434330299089726e-05,
      "loss": 0.8835,
      "step": 3780
    },
    {
      "epoch": 1.6287740410443752,
      "grad_norm": 1.5024832487106323,
      "learning_rate": 2.0408322496749027e-05,
      "loss": 0.8314,
      "step": 3790
    },
    {
      "epoch": 1.6330718813796068,
      "grad_norm": 2.133141040802002,
      "learning_rate": 2.038231469440832e-05,
      "loss": 0.8513,
      "step": 3800
    },
    {
      "epoch": 1.6373697217148382,
      "grad_norm": 1.8160301446914673,
      "learning_rate": 2.0356306892067622e-05,
      "loss": 0.8429,
      "step": 3810
    },
    {
      "epoch": 1.6416675620500698,
      "grad_norm": 2.389288902282715,
      "learning_rate": 2.0330299089726916e-05,
      "loss": 0.858,
      "step": 3820
    },
    {
      "epoch": 1.6459654023853014,
      "grad_norm": 1.9045385122299194,
      "learning_rate": 2.0304291287386217e-05,
      "loss": 0.8518,
      "step": 3830
    },
    {
      "epoch": 1.6502632427205328,
      "grad_norm": 1.7280313968658447,
      "learning_rate": 2.0278283485045515e-05,
      "loss": 0.8427,
      "step": 3840
    },
    {
      "epoch": 1.6545610830557644,
      "grad_norm": 1.6552119255065918,
      "learning_rate": 2.0252275682704812e-05,
      "loss": 0.8204,
      "step": 3850
    },
    {
      "epoch": 1.658858923390996,
      "grad_norm": 1.9472748041152954,
      "learning_rate": 2.022626788036411e-05,
      "loss": 0.8562,
      "step": 3860
    },
    {
      "epoch": 1.6631567637262274,
      "grad_norm": 3.340712785720825,
      "learning_rate": 2.0200260078023408e-05,
      "loss": 0.8461,
      "step": 3870
    },
    {
      "epoch": 1.667454604061459,
      "grad_norm": 1.7531561851501465,
      "learning_rate": 2.0174252275682705e-05,
      "loss": 0.8547,
      "step": 3880
    },
    {
      "epoch": 1.6717524443966907,
      "grad_norm": 1.5790107250213623,
      "learning_rate": 2.0148244473342003e-05,
      "loss": 0.8333,
      "step": 3890
    },
    {
      "epoch": 1.676050284731922,
      "grad_norm": 2.7590365409851074,
      "learning_rate": 2.01222366710013e-05,
      "loss": 0.8335,
      "step": 3900
    },
    {
      "epoch": 1.680348125067154,
      "grad_norm": 2.1124913692474365,
      "learning_rate": 2.0096228868660598e-05,
      "loss": 0.8853,
      "step": 3910
    },
    {
      "epoch": 1.6846459654023853,
      "grad_norm": 1.6337229013442993,
      "learning_rate": 2.0070221066319895e-05,
      "loss": 0.8025,
      "step": 3920
    },
    {
      "epoch": 1.6889438057376167,
      "grad_norm": 1.7925525903701782,
      "learning_rate": 2.0044213263979193e-05,
      "loss": 0.8746,
      "step": 3930
    },
    {
      "epoch": 1.6932416460728485,
      "grad_norm": 1.5929923057556152,
      "learning_rate": 2.0018205461638494e-05,
      "loss": 0.8474,
      "step": 3940
    },
    {
      "epoch": 1.69753948640808,
      "grad_norm": 2.2484991550445557,
      "learning_rate": 1.9992197659297788e-05,
      "loss": 0.8547,
      "step": 3950
    },
    {
      "epoch": 1.7018373267433113,
      "grad_norm": 2.030111789703369,
      "learning_rate": 1.996618985695709e-05,
      "loss": 0.8119,
      "step": 3960
    },
    {
      "epoch": 1.7061351670785432,
      "grad_norm": 1.990085244178772,
      "learning_rate": 1.9940182054616386e-05,
      "loss": 0.842,
      "step": 3970
    },
    {
      "epoch": 1.7104330074137746,
      "grad_norm": 2.762894630432129,
      "learning_rate": 1.9914174252275684e-05,
      "loss": 0.837,
      "step": 3980
    },
    {
      "epoch": 1.714730847749006,
      "grad_norm": 2.4444236755371094,
      "learning_rate": 1.988816644993498e-05,
      "loss": 0.814,
      "step": 3990
    },
    {
      "epoch": 1.7190286880842378,
      "grad_norm": 1.7298673391342163,
      "learning_rate": 1.986215864759428e-05,
      "loss": 0.8268,
      "step": 4000
    },
    {
      "epoch": 1.7233265284194692,
      "grad_norm": 3.2950820922851562,
      "learning_rate": 1.9836150845253576e-05,
      "loss": 0.8596,
      "step": 4010
    },
    {
      "epoch": 1.7276243687547006,
      "grad_norm": 1.9794464111328125,
      "learning_rate": 1.9810143042912874e-05,
      "loss": 0.836,
      "step": 4020
    },
    {
      "epoch": 1.7319222090899324,
      "grad_norm": 2.734571933746338,
      "learning_rate": 1.978413524057217e-05,
      "loss": 0.8827,
      "step": 4030
    },
    {
      "epoch": 1.7362200494251638,
      "grad_norm": 4.074718475341797,
      "learning_rate": 1.975812743823147e-05,
      "loss": 0.8264,
      "step": 4040
    },
    {
      "epoch": 1.7405178897603955,
      "grad_norm": 1.8319445848464966,
      "learning_rate": 1.9732119635890767e-05,
      "loss": 0.8353,
      "step": 4050
    },
    {
      "epoch": 1.744815730095627,
      "grad_norm": 1.705244779586792,
      "learning_rate": 1.9706111833550064e-05,
      "loss": 0.8831,
      "step": 4060
    },
    {
      "epoch": 1.7491135704308585,
      "grad_norm": 2.168896198272705,
      "learning_rate": 1.9680104031209365e-05,
      "loss": 0.8609,
      "step": 4070
    },
    {
      "epoch": 1.75341141076609,
      "grad_norm": 3.2166390419006348,
      "learning_rate": 1.9654096228868663e-05,
      "loss": 0.8541,
      "step": 4080
    },
    {
      "epoch": 1.7577092511013217,
      "grad_norm": 1.4464665651321411,
      "learning_rate": 1.962808842652796e-05,
      "loss": 0.8593,
      "step": 4090
    },
    {
      "epoch": 1.762007091436553,
      "grad_norm": 2.10670804977417,
      "learning_rate": 1.960468140442133e-05,
      "loss": 0.8576,
      "step": 4100
    },
    {
      "epoch": 1.7663049317717847,
      "grad_norm": 2.0898663997650146,
      "learning_rate": 1.9578673602080623e-05,
      "loss": 0.822,
      "step": 4110
    },
    {
      "epoch": 1.7706027721070163,
      "grad_norm": 2.179518222808838,
      "learning_rate": 1.9552665799739924e-05,
      "loss": 0.8655,
      "step": 4120
    },
    {
      "epoch": 1.7749006124422477,
      "grad_norm": 3.541693925857544,
      "learning_rate": 1.9526657997399218e-05,
      "loss": 0.8507,
      "step": 4130
    },
    {
      "epoch": 1.7791984527774793,
      "grad_norm": 2.0014359951019287,
      "learning_rate": 1.950065019505852e-05,
      "loss": 0.8435,
      "step": 4140
    },
    {
      "epoch": 1.783496293112711,
      "grad_norm": 2.3078408241271973,
      "learning_rate": 1.9474642392717817e-05,
      "loss": 0.8616,
      "step": 4150
    },
    {
      "epoch": 1.7877941334479424,
      "grad_norm": 1.988430380821228,
      "learning_rate": 1.9448634590377114e-05,
      "loss": 0.8524,
      "step": 4160
    },
    {
      "epoch": 1.792091973783174,
      "grad_norm": 1.91830313205719,
      "learning_rate": 1.9422626788036412e-05,
      "loss": 0.8212,
      "step": 4170
    },
    {
      "epoch": 1.7963898141184056,
      "grad_norm": 1.7817449569702148,
      "learning_rate": 1.939661898569571e-05,
      "loss": 0.8448,
      "step": 4180
    },
    {
      "epoch": 1.800687654453637,
      "grad_norm": 1.9036628007888794,
      "learning_rate": 1.9370611183355007e-05,
      "loss": 0.8865,
      "step": 4190
    },
    {
      "epoch": 1.8049854947888686,
      "grad_norm": 2.618138313293457,
      "learning_rate": 1.9344603381014304e-05,
      "loss": 0.8706,
      "step": 4200
    },
    {
      "epoch": 1.8092833351241002,
      "grad_norm": 2.1741411685943604,
      "learning_rate": 1.9318595578673602e-05,
      "loss": 0.8462,
      "step": 4210
    },
    {
      "epoch": 1.8135811754593316,
      "grad_norm": 2.28216814994812,
      "learning_rate": 1.92925877763329e-05,
      "loss": 0.862,
      "step": 4220
    },
    {
      "epoch": 1.8178790157945632,
      "grad_norm": 2.5889861583709717,
      "learning_rate": 1.92665799739922e-05,
      "loss": 0.8578,
      "step": 4230
    },
    {
      "epoch": 1.8221768561297949,
      "grad_norm": 1.9213104248046875,
      "learning_rate": 1.9240572171651495e-05,
      "loss": 0.83,
      "step": 4240
    },
    {
      "epoch": 1.8264746964650262,
      "grad_norm": 1.6190590858459473,
      "learning_rate": 1.9214564369310795e-05,
      "loss": 0.8362,
      "step": 4250
    },
    {
      "epoch": 1.8307725368002579,
      "grad_norm": 2.0574851036071777,
      "learning_rate": 1.918855656697009e-05,
      "loss": 0.8101,
      "step": 4260
    },
    {
      "epoch": 1.8350703771354895,
      "grad_norm": 2.0041048526763916,
      "learning_rate": 1.916254876462939e-05,
      "loss": 0.8416,
      "step": 4270
    },
    {
      "epoch": 1.8393682174707209,
      "grad_norm": 2.200464963912964,
      "learning_rate": 1.9136540962288685e-05,
      "loss": 0.8292,
      "step": 4280
    },
    {
      "epoch": 1.8436660578059525,
      "grad_norm": 2.2413747310638428,
      "learning_rate": 1.9110533159947986e-05,
      "loss": 0.8671,
      "step": 4290
    },
    {
      "epoch": 1.8479638981411841,
      "grad_norm": 1.9621833562850952,
      "learning_rate": 1.9084525357607283e-05,
      "loss": 0.8429,
      "step": 4300
    },
    {
      "epoch": 1.8522617384764155,
      "grad_norm": 2.6728787422180176,
      "learning_rate": 1.905851755526658e-05,
      "loss": 0.85,
      "step": 4310
    },
    {
      "epoch": 1.8565595788116471,
      "grad_norm": 2.4146318435668945,
      "learning_rate": 1.9032509752925878e-05,
      "loss": 0.8315,
      "step": 4320
    },
    {
      "epoch": 1.8608574191468787,
      "grad_norm": 2.679011106491089,
      "learning_rate": 1.9006501950585176e-05,
      "loss": 0.8667,
      "step": 4330
    },
    {
      "epoch": 1.8651552594821101,
      "grad_norm": 1.840955138206482,
      "learning_rate": 1.8980494148244473e-05,
      "loss": 0.8343,
      "step": 4340
    },
    {
      "epoch": 1.8694530998173418,
      "grad_norm": 1.7659724950790405,
      "learning_rate": 1.895448634590377e-05,
      "loss": 0.8504,
      "step": 4350
    },
    {
      "epoch": 1.8737509401525734,
      "grad_norm": 1.718576431274414,
      "learning_rate": 1.8928478543563072e-05,
      "loss": 0.8427,
      "step": 4360
    },
    {
      "epoch": 1.8780487804878048,
      "grad_norm": 1.9237421751022339,
      "learning_rate": 1.8902470741222366e-05,
      "loss": 0.8149,
      "step": 4370
    },
    {
      "epoch": 1.8823466208230364,
      "grad_norm": 1.7462245225906372,
      "learning_rate": 1.8876462938881667e-05,
      "loss": 0.8592,
      "step": 4380
    },
    {
      "epoch": 1.886644461158268,
      "grad_norm": 2.0181562900543213,
      "learning_rate": 1.885045513654096e-05,
      "loss": 0.8418,
      "step": 4390
    },
    {
      "epoch": 1.8909423014934994,
      "grad_norm": 2.339862823486328,
      "learning_rate": 1.8824447334200262e-05,
      "loss": 0.8531,
      "step": 4400
    },
    {
      "epoch": 1.895240141828731,
      "grad_norm": 3.261646270751953,
      "learning_rate": 1.8798439531859556e-05,
      "loss": 0.8614,
      "step": 4410
    },
    {
      "epoch": 1.8995379821639626,
      "grad_norm": 1.9183802604675293,
      "learning_rate": 1.8772431729518857e-05,
      "loss": 0.8282,
      "step": 4420
    },
    {
      "epoch": 1.903835822499194,
      "grad_norm": 1.751499891281128,
      "learning_rate": 1.874642392717815e-05,
      "loss": 0.8212,
      "step": 4430
    },
    {
      "epoch": 1.9081336628344256,
      "grad_norm": 3.767835855484009,
      "learning_rate": 1.8720416124837452e-05,
      "loss": 0.8479,
      "step": 4440
    },
    {
      "epoch": 1.9124315031696573,
      "grad_norm": 2.233424663543701,
      "learning_rate": 1.869440832249675e-05,
      "loss": 0.8634,
      "step": 4450
    },
    {
      "epoch": 1.9167293435048887,
      "grad_norm": 2.0906262397766113,
      "learning_rate": 1.8668400520156047e-05,
      "loss": 0.842,
      "step": 4460
    },
    {
      "epoch": 1.9210271838401205,
      "grad_norm": 2.016065835952759,
      "learning_rate": 1.8642392717815345e-05,
      "loss": 0.8457,
      "step": 4470
    },
    {
      "epoch": 1.925325024175352,
      "grad_norm": 2.640810966491699,
      "learning_rate": 1.8616384915474642e-05,
      "loss": 0.8726,
      "step": 4480
    },
    {
      "epoch": 1.9296228645105833,
      "grad_norm": 2.141726493835449,
      "learning_rate": 1.8590377113133943e-05,
      "loss": 0.8304,
      "step": 4490
    },
    {
      "epoch": 1.9339207048458151,
      "grad_norm": 1.696556806564331,
      "learning_rate": 1.8564369310793237e-05,
      "loss": 0.8304,
      "step": 4500
    },
    {
      "epoch": 1.9382185451810465,
      "grad_norm": 1.923128366470337,
      "learning_rate": 1.853836150845254e-05,
      "loss": 0.8447,
      "step": 4510
    },
    {
      "epoch": 1.942516385516278,
      "grad_norm": 1.7213290929794312,
      "learning_rate": 1.8512353706111832e-05,
      "loss": 0.8474,
      "step": 4520
    },
    {
      "epoch": 1.9468142258515098,
      "grad_norm": 2.4403738975524902,
      "learning_rate": 1.8486345903771133e-05,
      "loss": 0.8732,
      "step": 4530
    },
    {
      "epoch": 1.9511120661867412,
      "grad_norm": 1.784554362297058,
      "learning_rate": 1.8460338101430428e-05,
      "loss": 0.8398,
      "step": 4540
    },
    {
      "epoch": 1.9554099065219726,
      "grad_norm": 1.7551789283752441,
      "learning_rate": 1.843433029908973e-05,
      "loss": 0.8539,
      "step": 4550
    },
    {
      "epoch": 1.9597077468572044,
      "grad_norm": 1.8657149076461792,
      "learning_rate": 1.8408322496749023e-05,
      "loss": 0.8558,
      "step": 4560
    },
    {
      "epoch": 1.9640055871924358,
      "grad_norm": 2.23642897605896,
      "learning_rate": 1.8382314694408324e-05,
      "loss": 0.8695,
      "step": 4570
    },
    {
      "epoch": 1.9683034275276672,
      "grad_norm": 2.2597157955169678,
      "learning_rate": 1.8356306892067618e-05,
      "loss": 0.8656,
      "step": 4580
    },
    {
      "epoch": 1.972601267862899,
      "grad_norm": 2.367718458175659,
      "learning_rate": 1.833029908972692e-05,
      "loss": 0.8312,
      "step": 4590
    },
    {
      "epoch": 1.9768991081981304,
      "grad_norm": 1.8452681303024292,
      "learning_rate": 1.8304291287386216e-05,
      "loss": 0.8556,
      "step": 4600
    },
    {
      "epoch": 1.981196948533362,
      "grad_norm": 3.216796875,
      "learning_rate": 1.8278283485045514e-05,
      "loss": 0.8488,
      "step": 4610
    },
    {
      "epoch": 1.9854947888685937,
      "grad_norm": 1.8492870330810547,
      "learning_rate": 1.8252275682704815e-05,
      "loss": 0.8431,
      "step": 4620
    },
    {
      "epoch": 1.989792629203825,
      "grad_norm": 1.6641911268234253,
      "learning_rate": 1.822626788036411e-05,
      "loss": 0.8407,
      "step": 4630
    },
    {
      "epoch": 1.9940904695390567,
      "grad_norm": 2.4198052883148193,
      "learning_rate": 1.820026007802341e-05,
      "loss": 0.8266,
      "step": 4640
    },
    {
      "epoch": 1.9983883098742883,
      "grad_norm": 2.0911526679992676,
      "learning_rate": 1.8174252275682704e-05,
      "loss": 0.8282,
      "step": 4650
    },
    {
      "epoch": 2.002578704201139,
      "grad_norm": 1.755746841430664,
      "learning_rate": 1.8148244473342005e-05,
      "loss": 0.8089,
      "step": 4660
    },
    {
      "epoch": 2.0068765445363703,
      "grad_norm": 2.865302324295044,
      "learning_rate": 1.81222366710013e-05,
      "loss": 0.8461,
      "step": 4670
    },
    {
      "epoch": 2.011174384871602,
      "grad_norm": 2.016209602355957,
      "learning_rate": 1.80962288686606e-05,
      "loss": 0.8569,
      "step": 4680
    },
    {
      "epoch": 2.0154722252068336,
      "grad_norm": 1.5126547813415527,
      "learning_rate": 1.8070221066319894e-05,
      "loss": 0.8338,
      "step": 4690
    },
    {
      "epoch": 2.019770065542065,
      "grad_norm": 2.600252151489258,
      "learning_rate": 1.8044213263979195e-05,
      "loss": 0.8387,
      "step": 4700
    },
    {
      "epoch": 2.024067905877297,
      "grad_norm": 1.817923665046692,
      "learning_rate": 1.801820546163849e-05,
      "loss": 0.8148,
      "step": 4710
    },
    {
      "epoch": 2.028365746212528,
      "grad_norm": 3.621769666671753,
      "learning_rate": 1.799219765929779e-05,
      "loss": 0.8552,
      "step": 4720
    },
    {
      "epoch": 2.0326635865477596,
      "grad_norm": 2.7649691104888916,
      "learning_rate": 1.7966189856957088e-05,
      "loss": 0.8094,
      "step": 4730
    },
    {
      "epoch": 2.0369614268829914,
      "grad_norm": 1.9126285314559937,
      "learning_rate": 1.7940182054616385e-05,
      "loss": 0.8219,
      "step": 4740
    },
    {
      "epoch": 2.041259267218223,
      "grad_norm": 1.9327517747879028,
      "learning_rate": 1.7914174252275686e-05,
      "loss": 0.8453,
      "step": 4750
    },
    {
      "epoch": 2.045557107553454,
      "grad_norm": 2.6915619373321533,
      "learning_rate": 1.788816644993498e-05,
      "loss": 0.8356,
      "step": 4760
    },
    {
      "epoch": 2.049854947888686,
      "grad_norm": 2.143939733505249,
      "learning_rate": 1.786215864759428e-05,
      "loss": 0.8107,
      "step": 4770
    },
    {
      "epoch": 2.0541527882239174,
      "grad_norm": 2.6061177253723145,
      "learning_rate": 1.7836150845253575e-05,
      "loss": 0.817,
      "step": 4780
    },
    {
      "epoch": 2.058450628559149,
      "grad_norm": 2.1958038806915283,
      "learning_rate": 1.7810143042912876e-05,
      "loss": 0.8552,
      "step": 4790
    },
    {
      "epoch": 2.0627484688943807,
      "grad_norm": 2.0069873332977295,
      "learning_rate": 1.778413524057217e-05,
      "loss": 0.7981,
      "step": 4800
    },
    {
      "epoch": 2.067046309229612,
      "grad_norm": 2.2662248611450195,
      "learning_rate": 1.775812743823147e-05,
      "loss": 0.8486,
      "step": 4810
    },
    {
      "epoch": 2.0713441495648435,
      "grad_norm": 1.8936169147491455,
      "learning_rate": 1.7732119635890765e-05,
      "loss": 0.8341,
      "step": 4820
    },
    {
      "epoch": 2.0756419899000753,
      "grad_norm": 1.9257597923278809,
      "learning_rate": 1.7706111833550066e-05,
      "loss": 0.8453,
      "step": 4830
    },
    {
      "epoch": 2.0799398302353067,
      "grad_norm": 1.89949369430542,
      "learning_rate": 1.768010403120936e-05,
      "loss": 0.8675,
      "step": 4840
    },
    {
      "epoch": 2.084237670570538,
      "grad_norm": 1.8590527772903442,
      "learning_rate": 1.765409622886866e-05,
      "loss": 0.8374,
      "step": 4850
    },
    {
      "epoch": 2.08853551090577,
      "grad_norm": 1.800724744796753,
      "learning_rate": 1.762808842652796e-05,
      "loss": 0.8173,
      "step": 4860
    },
    {
      "epoch": 2.0928333512410013,
      "grad_norm": 2.267780303955078,
      "learning_rate": 1.7602080624187257e-05,
      "loss": 0.8532,
      "step": 4870
    },
    {
      "epoch": 2.0971311915762327,
      "grad_norm": 1.943345546722412,
      "learning_rate": 1.7576072821846557e-05,
      "loss": 0.8562,
      "step": 4880
    },
    {
      "epoch": 2.1014290319114646,
      "grad_norm": 1.7881354093551636,
      "learning_rate": 1.755006501950585e-05,
      "loss": 0.8218,
      "step": 4890
    },
    {
      "epoch": 2.105726872246696,
      "grad_norm": 2.1733593940734863,
      "learning_rate": 1.7524057217165153e-05,
      "loss": 0.8238,
      "step": 4900
    },
    {
      "epoch": 2.110024712581928,
      "grad_norm": 2.075446844100952,
      "learning_rate": 1.7498049414824447e-05,
      "loss": 0.8633,
      "step": 4910
    },
    {
      "epoch": 2.114322552917159,
      "grad_norm": 2.345024824142456,
      "learning_rate": 1.7472041612483748e-05,
      "loss": 0.8336,
      "step": 4920
    },
    {
      "epoch": 2.1186203932523906,
      "grad_norm": 2.027916431427002,
      "learning_rate": 1.7446033810143042e-05,
      "loss": 0.8193,
      "step": 4930
    },
    {
      "epoch": 2.1229182335876224,
      "grad_norm": 2.6074066162109375,
      "learning_rate": 1.7420026007802343e-05,
      "loss": 0.8233,
      "step": 4940
    },
    {
      "epoch": 2.127216073922854,
      "grad_norm": 2.42826247215271,
      "learning_rate": 1.7394018205461637e-05,
      "loss": 0.8519,
      "step": 4950
    },
    {
      "epoch": 2.1315139142580852,
      "grad_norm": 2.3784759044647217,
      "learning_rate": 1.7368010403120938e-05,
      "loss": 0.8376,
      "step": 4960
    },
    {
      "epoch": 2.135811754593317,
      "grad_norm": 2.6086244583129883,
      "learning_rate": 1.7342002600780232e-05,
      "loss": 0.8154,
      "step": 4970
    },
    {
      "epoch": 2.1401095949285485,
      "grad_norm": 1.700882911682129,
      "learning_rate": 1.7315994798439533e-05,
      "loss": 0.8115,
      "step": 4980
    },
    {
      "epoch": 2.14440743526378,
      "grad_norm": 2.827244758605957,
      "learning_rate": 1.728998699609883e-05,
      "loss": 0.8922,
      "step": 4990
    },
    {
      "epoch": 2.1487052755990117,
      "grad_norm": 1.9213329553604126,
      "learning_rate": 1.7263979193758128e-05,
      "loss": 0.8442,
      "step": 5000
    },
    {
      "epoch": 2.153003115934243,
      "grad_norm": 3.5591232776641846,
      "learning_rate": 1.7237971391417426e-05,
      "loss": 0.866,
      "step": 5010
    },
    {
      "epoch": 2.1573009562694745,
      "grad_norm": 2.768948554992676,
      "learning_rate": 1.7211963589076723e-05,
      "loss": 0.841,
      "step": 5020
    },
    {
      "epoch": 2.1615987966047063,
      "grad_norm": 2.093421459197998,
      "learning_rate": 1.7185955786736024e-05,
      "loss": 0.8341,
      "step": 5030
    },
    {
      "epoch": 2.1658966369399377,
      "grad_norm": 2.224672317504883,
      "learning_rate": 1.7159947984395318e-05,
      "loss": 0.825,
      "step": 5040
    },
    {
      "epoch": 2.170194477275169,
      "grad_norm": 1.7963062524795532,
      "learning_rate": 1.713394018205462e-05,
      "loss": 0.8417,
      "step": 5050
    },
    {
      "epoch": 2.174492317610401,
      "grad_norm": 2.07151460647583,
      "learning_rate": 1.7107932379713913e-05,
      "loss": 0.8328,
      "step": 5060
    },
    {
      "epoch": 2.1787901579456324,
      "grad_norm": 3.24853253364563,
      "learning_rate": 1.7081924577373214e-05,
      "loss": 0.8179,
      "step": 5070
    },
    {
      "epoch": 2.1830879982808638,
      "grad_norm": 1.7244374752044678,
      "learning_rate": 1.7055916775032508e-05,
      "loss": 0.8249,
      "step": 5080
    },
    {
      "epoch": 2.1873858386160956,
      "grad_norm": 2.6132147312164307,
      "learning_rate": 1.702990897269181e-05,
      "loss": 0.8554,
      "step": 5090
    },
    {
      "epoch": 2.191683678951327,
      "grad_norm": 2.3131542205810547,
      "learning_rate": 1.7003901170351103e-05,
      "loss": 0.8602,
      "step": 5100
    },
    {
      "epoch": 2.1959815192865584,
      "grad_norm": 1.714519739151001,
      "learning_rate": 1.6977893368010404e-05,
      "loss": 0.8369,
      "step": 5110
    },
    {
      "epoch": 2.20027935962179,
      "grad_norm": 2.272261381149292,
      "learning_rate": 1.6951885565669702e-05,
      "loss": 0.8804,
      "step": 5120
    },
    {
      "epoch": 2.2045771999570216,
      "grad_norm": 3.92767071723938,
      "learning_rate": 1.6925877763329e-05,
      "loss": 0.8803,
      "step": 5130
    },
    {
      "epoch": 2.208875040292253,
      "grad_norm": 1.8476197719573975,
      "learning_rate": 1.6899869960988297e-05,
      "loss": 0.8139,
      "step": 5140
    },
    {
      "epoch": 2.213172880627485,
      "grad_norm": 2.344593048095703,
      "learning_rate": 1.6873862158647594e-05,
      "loss": 0.8679,
      "step": 5150
    },
    {
      "epoch": 2.2174707209627162,
      "grad_norm": 2.317490577697754,
      "learning_rate": 1.6847854356306892e-05,
      "loss": 0.8041,
      "step": 5160
    },
    {
      "epoch": 2.2217685612979476,
      "grad_norm": 2.788961887359619,
      "learning_rate": 1.682184655396619e-05,
      "loss": 0.8306,
      "step": 5170
    },
    {
      "epoch": 2.2260664016331795,
      "grad_norm": 1.8133071660995483,
      "learning_rate": 1.679583875162549e-05,
      "loss": 0.8129,
      "step": 5180
    },
    {
      "epoch": 2.230364241968411,
      "grad_norm": 1.9036056995391846,
      "learning_rate": 1.6769830949284785e-05,
      "loss": 0.8077,
      "step": 5190
    },
    {
      "epoch": 2.2346620823036423,
      "grad_norm": 2.7897422313690186,
      "learning_rate": 1.6743823146944086e-05,
      "loss": 0.846,
      "step": 5200
    },
    {
      "epoch": 2.238959922638874,
      "grad_norm": 3.532485008239746,
      "learning_rate": 1.671781534460338e-05,
      "loss": 0.8469,
      "step": 5210
    },
    {
      "epoch": 2.2432577629741055,
      "grad_norm": 2.0166537761688232,
      "learning_rate": 1.669180754226268e-05,
      "loss": 0.7938,
      "step": 5220
    },
    {
      "epoch": 2.247555603309337,
      "grad_norm": 2.857203483581543,
      "learning_rate": 1.6665799739921975e-05,
      "loss": 0.8357,
      "step": 5230
    },
    {
      "epoch": 2.2518534436445687,
      "grad_norm": 2.119642734527588,
      "learning_rate": 1.6639791937581276e-05,
      "loss": 0.8472,
      "step": 5240
    },
    {
      "epoch": 2.2561512839798,
      "grad_norm": 3.2269287109375,
      "learning_rate": 1.6613784135240573e-05,
      "loss": 0.8678,
      "step": 5250
    },
    {
      "epoch": 2.2604491243150315,
      "grad_norm": 1.8111190795898438,
      "learning_rate": 1.658777633289987e-05,
      "loss": 0.8132,
      "step": 5260
    },
    {
      "epoch": 2.2647469646502634,
      "grad_norm": 1.8971123695373535,
      "learning_rate": 1.656176853055917e-05,
      "loss": 0.8614,
      "step": 5270
    },
    {
      "epoch": 2.2690448049854948,
      "grad_norm": 1.9025448560714722,
      "learning_rate": 1.6535760728218466e-05,
      "loss": 0.8397,
      "step": 5280
    },
    {
      "epoch": 2.273342645320726,
      "grad_norm": 1.7242286205291748,
      "learning_rate": 1.6509752925877763e-05,
      "loss": 0.826,
      "step": 5290
    },
    {
      "epoch": 2.277640485655958,
      "grad_norm": 2.0419163703918457,
      "learning_rate": 1.648374512353706e-05,
      "loss": 0.8274,
      "step": 5300
    },
    {
      "epoch": 2.2819383259911894,
      "grad_norm": 2.23207950592041,
      "learning_rate": 1.645773732119636e-05,
      "loss": 0.889,
      "step": 5310
    },
    {
      "epoch": 2.286236166326421,
      "grad_norm": 1.9739634990692139,
      "learning_rate": 1.6431729518855656e-05,
      "loss": 0.8641,
      "step": 5320
    },
    {
      "epoch": 2.2905340066616526,
      "grad_norm": 2.317784070968628,
      "learning_rate": 1.6405721716514957e-05,
      "loss": 0.8067,
      "step": 5330
    },
    {
      "epoch": 2.294831846996884,
      "grad_norm": 2.9078609943389893,
      "learning_rate": 1.637971391417425e-05,
      "loss": 0.8607,
      "step": 5340
    },
    {
      "epoch": 2.2991296873321154,
      "grad_norm": 2.1412670612335205,
      "learning_rate": 1.6353706111833552e-05,
      "loss": 0.8238,
      "step": 5350
    },
    {
      "epoch": 2.3034275276673473,
      "grad_norm": 2.3732709884643555,
      "learning_rate": 1.6327698309492846e-05,
      "loss": 0.8278,
      "step": 5360
    },
    {
      "epoch": 2.3077253680025787,
      "grad_norm": 2.2960100173950195,
      "learning_rate": 1.6301690507152147e-05,
      "loss": 0.833,
      "step": 5370
    },
    {
      "epoch": 2.31202320833781,
      "grad_norm": 1.884372591972351,
      "learning_rate": 1.6275682704811445e-05,
      "loss": 0.8525,
      "step": 5380
    },
    {
      "epoch": 2.316321048673042,
      "grad_norm": 2.0460383892059326,
      "learning_rate": 1.6249674902470742e-05,
      "loss": 0.8233,
      "step": 5390
    },
    {
      "epoch": 2.3206188890082733,
      "grad_norm": 2.195345878601074,
      "learning_rate": 1.622366710013004e-05,
      "loss": 0.8311,
      "step": 5400
    },
    {
      "epoch": 2.3249167293435047,
      "grad_norm": 1.8183631896972656,
      "learning_rate": 1.6197659297789337e-05,
      "loss": 0.843,
      "step": 5410
    },
    {
      "epoch": 2.3292145696787365,
      "grad_norm": 1.982046365737915,
      "learning_rate": 1.6171651495448635e-05,
      "loss": 0.8246,
      "step": 5420
    },
    {
      "epoch": 2.333512410013968,
      "grad_norm": 2.066999912261963,
      "learning_rate": 1.6145643693107932e-05,
      "loss": 0.8176,
      "step": 5430
    },
    {
      "epoch": 2.3378102503491993,
      "grad_norm": 3.9742650985717773,
      "learning_rate": 1.611963589076723e-05,
      "loss": 0.8448,
      "step": 5440
    },
    {
      "epoch": 2.342108090684431,
      "grad_norm": 2.533623218536377,
      "learning_rate": 1.6093628088426527e-05,
      "loss": 0.8309,
      "step": 5450
    },
    {
      "epoch": 2.3464059310196625,
      "grad_norm": 2.126760721206665,
      "learning_rate": 1.6067620286085825e-05,
      "loss": 0.7975,
      "step": 5460
    },
    {
      "epoch": 2.350703771354894,
      "grad_norm": 2.197327136993408,
      "learning_rate": 1.6041612483745123e-05,
      "loss": 0.8496,
      "step": 5470
    },
    {
      "epoch": 2.355001611690126,
      "grad_norm": 1.7309150695800781,
      "learning_rate": 1.6015604681404423e-05,
      "loss": 0.8083,
      "step": 5480
    },
    {
      "epoch": 2.359299452025357,
      "grad_norm": 2.790698766708374,
      "learning_rate": 1.5989596879063718e-05,
      "loss": 0.839,
      "step": 5490
    },
    {
      "epoch": 2.3635972923605886,
      "grad_norm": 1.8246008157730103,
      "learning_rate": 1.596358907672302e-05,
      "loss": 0.825,
      "step": 5500
    },
    {
      "epoch": 2.3678951326958204,
      "grad_norm": 1.8696269989013672,
      "learning_rate": 1.5937581274382316e-05,
      "loss": 0.8191,
      "step": 5510
    },
    {
      "epoch": 2.372192973031052,
      "grad_norm": 2.048485279083252,
      "learning_rate": 1.5911573472041614e-05,
      "loss": 0.8425,
      "step": 5520
    },
    {
      "epoch": 2.376490813366283,
      "grad_norm": 1.9061790704727173,
      "learning_rate": 1.588556566970091e-05,
      "loss": 0.818,
      "step": 5530
    },
    {
      "epoch": 2.380788653701515,
      "grad_norm": 2.277283191680908,
      "learning_rate": 1.585955786736021e-05,
      "loss": 0.7947,
      "step": 5540
    },
    {
      "epoch": 2.3850864940367464,
      "grad_norm": 2.0266342163085938,
      "learning_rate": 1.5833550065019506e-05,
      "loss": 0.8379,
      "step": 5550
    },
    {
      "epoch": 2.389384334371978,
      "grad_norm": 1.8553252220153809,
      "learning_rate": 1.5807542262678804e-05,
      "loss": 0.8227,
      "step": 5560
    },
    {
      "epoch": 2.3936821747072097,
      "grad_norm": 1.9910932779312134,
      "learning_rate": 1.57815344603381e-05,
      "loss": 0.8753,
      "step": 5570
    },
    {
      "epoch": 2.397980015042441,
      "grad_norm": 1.6938420534133911,
      "learning_rate": 1.57555266579974e-05,
      "loss": 0.8628,
      "step": 5580
    },
    {
      "epoch": 2.402277855377673,
      "grad_norm": 1.9358924627304077,
      "learning_rate": 1.5729518855656696e-05,
      "loss": 0.8277,
      "step": 5590
    },
    {
      "epoch": 2.4065756957129043,
      "grad_norm": 7.336191177368164,
      "learning_rate": 1.5703511053315994e-05,
      "loss": 0.8476,
      "step": 5600
    },
    {
      "epoch": 2.4108735360481357,
      "grad_norm": 2.8806827068328857,
      "learning_rate": 1.567750325097529e-05,
      "loss": 0.8512,
      "step": 5610
    },
    {
      "epoch": 2.4151713763833675,
      "grad_norm": 2.4420716762542725,
      "learning_rate": 1.565149544863459e-05,
      "loss": 0.8226,
      "step": 5620
    },
    {
      "epoch": 2.419469216718599,
      "grad_norm": 3.224461555480957,
      "learning_rate": 1.562548764629389e-05,
      "loss": 0.8451,
      "step": 5630
    },
    {
      "epoch": 2.4237670570538303,
      "grad_norm": 2.4914772510528564,
      "learning_rate": 1.5599479843953187e-05,
      "loss": 0.8004,
      "step": 5640
    },
    {
      "epoch": 2.428064897389062,
      "grad_norm": 2.254756450653076,
      "learning_rate": 1.5573472041612485e-05,
      "loss": 0.8447,
      "step": 5650
    },
    {
      "epoch": 2.4323627377242936,
      "grad_norm": 2.264805793762207,
      "learning_rate": 1.5547464239271783e-05,
      "loss": 0.8479,
      "step": 5660
    },
    {
      "epoch": 2.436660578059525,
      "grad_norm": 2.2582733631134033,
      "learning_rate": 1.552145643693108e-05,
      "loss": 0.8541,
      "step": 5670
    },
    {
      "epoch": 2.440958418394757,
      "grad_norm": 2.4018824100494385,
      "learning_rate": 1.5495448634590378e-05,
      "loss": 0.8352,
      "step": 5680
    },
    {
      "epoch": 2.445256258729988,
      "grad_norm": 1.8728681802749634,
      "learning_rate": 1.5469440832249675e-05,
      "loss": 0.8143,
      "step": 5690
    },
    {
      "epoch": 2.4495540990652196,
      "grad_norm": 1.7589194774627686,
      "learning_rate": 1.5443433029908973e-05,
      "loss": 0.8176,
      "step": 5700
    },
    {
      "epoch": 2.4538519394004514,
      "grad_norm": 1.8795843124389648,
      "learning_rate": 1.541742522756827e-05,
      "loss": 0.8103,
      "step": 5710
    },
    {
      "epoch": 2.458149779735683,
      "grad_norm": 1.8767679929733276,
      "learning_rate": 1.5391417425227568e-05,
      "loss": 0.8156,
      "step": 5720
    },
    {
      "epoch": 2.4624476200709142,
      "grad_norm": 2.081376075744629,
      "learning_rate": 1.5365409622886865e-05,
      "loss": 0.8229,
      "step": 5730
    },
    {
      "epoch": 2.466745460406146,
      "grad_norm": 1.9182652235031128,
      "learning_rate": 1.5339401820546163e-05,
      "loss": 0.8475,
      "step": 5740
    },
    {
      "epoch": 2.4710433007413775,
      "grad_norm": 2.1970207691192627,
      "learning_rate": 1.5313394018205464e-05,
      "loss": 0.8386,
      "step": 5750
    },
    {
      "epoch": 2.475341141076609,
      "grad_norm": 2.2653651237487793,
      "learning_rate": 1.5287386215864758e-05,
      "loss": 0.8645,
      "step": 5760
    },
    {
      "epoch": 2.4796389814118407,
      "grad_norm": 3.5646309852600098,
      "learning_rate": 1.526137841352406e-05,
      "loss": 0.907,
      "step": 5770
    },
    {
      "epoch": 2.483936821747072,
      "grad_norm": 2.640089511871338,
      "learning_rate": 1.5235370611183356e-05,
      "loss": 0.8683,
      "step": 5780
    },
    {
      "epoch": 2.4882346620823035,
      "grad_norm": 2.1131584644317627,
      "learning_rate": 1.5209362808842652e-05,
      "loss": 0.8301,
      "step": 5790
    },
    {
      "epoch": 2.4925325024175353,
      "grad_norm": 2.2442245483398438,
      "learning_rate": 1.5183355006501952e-05,
      "loss": 0.8185,
      "step": 5800
    },
    {
      "epoch": 2.4968303427527667,
      "grad_norm": 3.470278739929199,
      "learning_rate": 1.5157347204161247e-05,
      "loss": 0.834,
      "step": 5810
    },
    {
      "epoch": 2.5011281830879986,
      "grad_norm": 2.0637283325195312,
      "learning_rate": 1.5131339401820548e-05,
      "loss": 0.8231,
      "step": 5820
    },
    {
      "epoch": 2.50542602342323,
      "grad_norm": 2.8878378868103027,
      "learning_rate": 1.5105331599479844e-05,
      "loss": 0.8254,
      "step": 5830
    },
    {
      "epoch": 2.5097238637584613,
      "grad_norm": 1.7346928119659424,
      "learning_rate": 1.5079323797139143e-05,
      "loss": 0.8144,
      "step": 5840
    },
    {
      "epoch": 2.514021704093693,
      "grad_norm": 2.0115489959716797,
      "learning_rate": 1.505331599479844e-05,
      "loss": 0.8441,
      "step": 5850
    },
    {
      "epoch": 2.5183195444289246,
      "grad_norm": 3.3221356868743896,
      "learning_rate": 1.5027308192457738e-05,
      "loss": 0.8227,
      "step": 5860
    },
    {
      "epoch": 2.522617384764156,
      "grad_norm": 2.0305612087249756,
      "learning_rate": 1.5001300390117034e-05,
      "loss": 0.841,
      "step": 5870
    },
    {
      "epoch": 2.526915225099388,
      "grad_norm": 1.8633432388305664,
      "learning_rate": 1.4975292587776334e-05,
      "loss": 0.8332,
      "step": 5880
    },
    {
      "epoch": 2.531213065434619,
      "grad_norm": 2.399090528488159,
      "learning_rate": 1.4949284785435631e-05,
      "loss": 0.8724,
      "step": 5890
    },
    {
      "epoch": 2.5355109057698506,
      "grad_norm": 2.018446922302246,
      "learning_rate": 1.4923276983094929e-05,
      "loss": 0.8168,
      "step": 5900
    },
    {
      "epoch": 2.5398087461050824,
      "grad_norm": 2.1548683643341064,
      "learning_rate": 1.4897269180754226e-05,
      "loss": 0.8562,
      "step": 5910
    },
    {
      "epoch": 2.544106586440314,
      "grad_norm": 4.101995468139648,
      "learning_rate": 1.4871261378413524e-05,
      "loss": 0.8327,
      "step": 5920
    },
    {
      "epoch": 2.5484044267755452,
      "grad_norm": 2.244936943054199,
      "learning_rate": 1.4845253576072821e-05,
      "loss": 0.8549,
      "step": 5930
    },
    {
      "epoch": 2.552702267110777,
      "grad_norm": 2.361332893371582,
      "learning_rate": 1.4819245773732119e-05,
      "loss": 0.8527,
      "step": 5940
    },
    {
      "epoch": 2.5570001074460085,
      "grad_norm": 1.9174724817276,
      "learning_rate": 1.4793237971391418e-05,
      "loss": 0.8377,
      "step": 5950
    },
    {
      "epoch": 2.56129794778124,
      "grad_norm": 2.3391590118408203,
      "learning_rate": 1.4767230169050716e-05,
      "loss": 0.8655,
      "step": 5960
    },
    {
      "epoch": 2.5655957881164717,
      "grad_norm": 1.7326821088790894,
      "learning_rate": 1.4741222366710013e-05,
      "loss": 0.8639,
      "step": 5970
    },
    {
      "epoch": 2.569893628451703,
      "grad_norm": 2.0442450046539307,
      "learning_rate": 1.471521456436931e-05,
      "loss": 0.8459,
      "step": 5980
    },
    {
      "epoch": 2.5741914687869345,
      "grad_norm": 2.425452470779419,
      "learning_rate": 1.468920676202861e-05,
      "loss": 0.8491,
      "step": 5990
    },
    {
      "epoch": 2.5784893091221663,
      "grad_norm": 1.8309391736984253,
      "learning_rate": 1.4663198959687907e-05,
      "loss": 0.8702,
      "step": 6000
    },
    {
      "epoch": 2.5827871494573977,
      "grad_norm": 2.087642192840576,
      "learning_rate": 1.4637191157347205e-05,
      "loss": 0.8586,
      "step": 6010
    },
    {
      "epoch": 2.587084989792629,
      "grad_norm": 1.9936331510543823,
      "learning_rate": 1.4611183355006503e-05,
      "loss": 0.8329,
      "step": 6020
    },
    {
      "epoch": 2.591382830127861,
      "grad_norm": 2.3392457962036133,
      "learning_rate": 1.45851755526658e-05,
      "loss": 0.8481,
      "step": 6030
    },
    {
      "epoch": 2.5956806704630924,
      "grad_norm": 2.4695334434509277,
      "learning_rate": 1.4559167750325098e-05,
      "loss": 0.8253,
      "step": 6040
    },
    {
      "epoch": 2.5999785107983238,
      "grad_norm": 2.6091296672821045,
      "learning_rate": 1.4533159947984395e-05,
      "loss": 0.8408,
      "step": 6050
    },
    {
      "epoch": 2.6042763511335556,
      "grad_norm": 2.1383988857269287,
      "learning_rate": 1.4507152145643693e-05,
      "loss": 0.8372,
      "step": 6060
    },
    {
      "epoch": 2.608574191468787,
      "grad_norm": 2.501603603363037,
      "learning_rate": 1.4481144343302992e-05,
      "loss": 0.8364,
      "step": 6070
    },
    {
      "epoch": 2.6128720318040184,
      "grad_norm": 1.6721731424331665,
      "learning_rate": 1.445513654096229e-05,
      "loss": 0.83,
      "step": 6080
    },
    {
      "epoch": 2.6171698721392502,
      "grad_norm": 2.347831964492798,
      "learning_rate": 1.4429128738621587e-05,
      "loss": 0.8447,
      "step": 6090
    },
    {
      "epoch": 2.6214677124744816,
      "grad_norm": 2.6728830337524414,
      "learning_rate": 1.4403120936280885e-05,
      "loss": 0.8547,
      "step": 6100
    },
    {
      "epoch": 2.625765552809713,
      "grad_norm": 1.5303648710250854,
      "learning_rate": 1.4377113133940182e-05,
      "loss": 0.8888,
      "step": 6110
    },
    {
      "epoch": 2.630063393144945,
      "grad_norm": 2.2400853633880615,
      "learning_rate": 1.435110533159948e-05,
      "loss": 0.8454,
      "step": 6120
    },
    {
      "epoch": 2.6343612334801763,
      "grad_norm": 2.6511406898498535,
      "learning_rate": 1.4325097529258779e-05,
      "loss": 0.8639,
      "step": 6130
    },
    {
      "epoch": 2.6386590738154077,
      "grad_norm": 2.502307176589966,
      "learning_rate": 1.4299089726918076e-05,
      "loss": 0.8505,
      "step": 6140
    },
    {
      "epoch": 2.6429569141506395,
      "grad_norm": 2.212013006210327,
      "learning_rate": 1.4273081924577374e-05,
      "loss": 0.8562,
      "step": 6150
    },
    {
      "epoch": 2.647254754485871,
      "grad_norm": 2.2604401111602783,
      "learning_rate": 1.4247074122236671e-05,
      "loss": 0.8682,
      "step": 6160
    },
    {
      "epoch": 2.6515525948211023,
      "grad_norm": 1.7996559143066406,
      "learning_rate": 1.4221066319895969e-05,
      "loss": 0.8392,
      "step": 6170
    },
    {
      "epoch": 2.655850435156334,
      "grad_norm": 2.42763614654541,
      "learning_rate": 1.4195058517555267e-05,
      "loss": 0.8415,
      "step": 6180
    },
    {
      "epoch": 2.6601482754915655,
      "grad_norm": 1.8488932847976685,
      "learning_rate": 1.4169050715214564e-05,
      "loss": 0.8322,
      "step": 6190
    },
    {
      "epoch": 2.664446115826797,
      "grad_norm": 1.936538577079773,
      "learning_rate": 1.4143042912873863e-05,
      "loss": 0.8615,
      "step": 6200
    },
    {
      "epoch": 2.6687439561620288,
      "grad_norm": 2.942472219467163,
      "learning_rate": 1.4117035110533161e-05,
      "loss": 0.8324,
      "step": 6210
    },
    {
      "epoch": 2.67304179649726,
      "grad_norm": 1.9425569772720337,
      "learning_rate": 1.4091027308192458e-05,
      "loss": 0.8172,
      "step": 6220
    },
    {
      "epoch": 2.6773396368324915,
      "grad_norm": 1.9994274377822876,
      "learning_rate": 1.4065019505851756e-05,
      "loss": 0.8622,
      "step": 6230
    },
    {
      "epoch": 2.6816374771677234,
      "grad_norm": 2.063232898712158,
      "learning_rate": 1.4039011703511053e-05,
      "loss": 0.8046,
      "step": 6240
    },
    {
      "epoch": 2.6859353175029548,
      "grad_norm": 2.5445046424865723,
      "learning_rate": 1.4013003901170351e-05,
      "loss": 0.8445,
      "step": 6250
    },
    {
      "epoch": 2.690233157838186,
      "grad_norm": 1.903300166130066,
      "learning_rate": 1.3986996098829649e-05,
      "loss": 0.8315,
      "step": 6260
    },
    {
      "epoch": 2.694530998173418,
      "grad_norm": 1.9516681432724,
      "learning_rate": 1.3960988296488946e-05,
      "loss": 0.8512,
      "step": 6270
    },
    {
      "epoch": 2.6988288385086494,
      "grad_norm": 2.9729251861572266,
      "learning_rate": 1.3934980494148245e-05,
      "loss": 0.9065,
      "step": 6280
    },
    {
      "epoch": 2.703126678843881,
      "grad_norm": 2.3370349407196045,
      "learning_rate": 1.3908972691807543e-05,
      "loss": 0.8321,
      "step": 6290
    },
    {
      "epoch": 2.7074245191791126,
      "grad_norm": 2.0578927993774414,
      "learning_rate": 1.388296488946684e-05,
      "loss": 0.8141,
      "step": 6300
    },
    {
      "epoch": 2.711722359514344,
      "grad_norm": 2.227670907974243,
      "learning_rate": 1.3856957087126138e-05,
      "loss": 0.8106,
      "step": 6310
    },
    {
      "epoch": 2.7160201998495754,
      "grad_norm": 2.9578027725219727,
      "learning_rate": 1.3830949284785435e-05,
      "loss": 0.8324,
      "step": 6320
    },
    {
      "epoch": 2.7203180401848073,
      "grad_norm": 2.2930986881256104,
      "learning_rate": 1.3804941482444735e-05,
      "loss": 0.8041,
      "step": 6330
    },
    {
      "epoch": 2.7246158805200387,
      "grad_norm": 2.3146274089813232,
      "learning_rate": 1.3778933680104032e-05,
      "loss": 0.8301,
      "step": 6340
    },
    {
      "epoch": 2.72891372085527,
      "grad_norm": 2.976332664489746,
      "learning_rate": 1.375292587776333e-05,
      "loss": 0.8711,
      "step": 6350
    },
    {
      "epoch": 2.733211561190502,
      "grad_norm": 2.9178059101104736,
      "learning_rate": 1.3726918075422627e-05,
      "loss": 0.7825,
      "step": 6360
    },
    {
      "epoch": 2.7375094015257333,
      "grad_norm": 2.603374481201172,
      "learning_rate": 1.3700910273081925e-05,
      "loss": 0.811,
      "step": 6370
    },
    {
      "epoch": 2.7418072418609647,
      "grad_norm": 1.9927624464035034,
      "learning_rate": 1.3674902470741222e-05,
      "loss": 0.8537,
      "step": 6380
    },
    {
      "epoch": 2.7461050821961965,
      "grad_norm": 1.9245107173919678,
      "learning_rate": 1.364889466840052e-05,
      "loss": 0.8663,
      "step": 6390
    },
    {
      "epoch": 2.750402922531428,
      "grad_norm": 2.465473175048828,
      "learning_rate": 1.3622886866059818e-05,
      "loss": 0.8368,
      "step": 6400
    },
    {
      "epoch": 2.7547007628666593,
      "grad_norm": 2.165015935897827,
      "learning_rate": 1.3596879063719115e-05,
      "loss": 0.8738,
      "step": 6410
    },
    {
      "epoch": 2.758998603201891,
      "grad_norm": 2.656083583831787,
      "learning_rate": 1.3570871261378413e-05,
      "loss": 0.8324,
      "step": 6420
    },
    {
      "epoch": 2.7632964435371226,
      "grad_norm": 2.5415213108062744,
      "learning_rate": 1.3544863459037712e-05,
      "loss": 0.8433,
      "step": 6430
    },
    {
      "epoch": 2.767594283872354,
      "grad_norm": 2.3042244911193848,
      "learning_rate": 1.351885565669701e-05,
      "loss": 0.8378,
      "step": 6440
    },
    {
      "epoch": 2.771892124207586,
      "grad_norm": 1.81434166431427,
      "learning_rate": 1.3492847854356307e-05,
      "loss": 0.8296,
      "step": 6450
    },
    {
      "epoch": 2.776189964542817,
      "grad_norm": 1.8286519050598145,
      "learning_rate": 1.3466840052015606e-05,
      "loss": 0.8193,
      "step": 6460
    },
    {
      "epoch": 2.7804878048780486,
      "grad_norm": 4.671245574951172,
      "learning_rate": 1.3440832249674904e-05,
      "loss": 0.8377,
      "step": 6470
    },
    {
      "epoch": 2.7847856452132804,
      "grad_norm": 2.294292688369751,
      "learning_rate": 1.3414824447334201e-05,
      "loss": 0.8012,
      "step": 6480
    },
    {
      "epoch": 2.789083485548512,
      "grad_norm": 2.3286867141723633,
      "learning_rate": 1.3388816644993499e-05,
      "loss": 0.8903,
      "step": 6490
    },
    {
      "epoch": 2.793381325883743,
      "grad_norm": 1.9204438924789429,
      "learning_rate": 1.3362808842652796e-05,
      "loss": 0.827,
      "step": 6500
    },
    {
      "epoch": 2.797679166218975,
      "grad_norm": 2.1733880043029785,
      "learning_rate": 1.3336801040312094e-05,
      "loss": 0.8159,
      "step": 6510
    },
    {
      "epoch": 2.8019770065542065,
      "grad_norm": 1.9933550357818604,
      "learning_rate": 1.3310793237971391e-05,
      "loss": 0.8157,
      "step": 6520
    },
    {
      "epoch": 2.806274846889438,
      "grad_norm": 1.7158734798431396,
      "learning_rate": 1.3284785435630689e-05,
      "loss": 0.8225,
      "step": 6530
    },
    {
      "epoch": 2.8105726872246697,
      "grad_norm": 1.8747084140777588,
      "learning_rate": 1.3258777633289986e-05,
      "loss": 0.8113,
      "step": 6540
    },
    {
      "epoch": 2.814870527559901,
      "grad_norm": 2.0481791496276855,
      "learning_rate": 1.3232769830949284e-05,
      "loss": 0.8443,
      "step": 6550
    },
    {
      "epoch": 2.8191683678951325,
      "grad_norm": 2.1141815185546875,
      "learning_rate": 1.3206762028608582e-05,
      "loss": 0.8442,
      "step": 6560
    },
    {
      "epoch": 2.8234662082303643,
      "grad_norm": 1.8887488842010498,
      "learning_rate": 1.318075422626788e-05,
      "loss": 0.8172,
      "step": 6570
    },
    {
      "epoch": 2.8277640485655957,
      "grad_norm": 2.2302682399749756,
      "learning_rate": 1.315474642392718e-05,
      "loss": 0.813,
      "step": 6580
    },
    {
      "epoch": 2.832061888900827,
      "grad_norm": 1.8239291906356812,
      "learning_rate": 1.3128738621586478e-05,
      "loss": 0.8387,
      "step": 6590
    },
    {
      "epoch": 2.836359729236059,
      "grad_norm": 2.2952709197998047,
      "learning_rate": 1.3102730819245775e-05,
      "loss": 0.8129,
      "step": 6600
    },
    {
      "epoch": 2.8406575695712903,
      "grad_norm": 1.9261474609375,
      "learning_rate": 1.3076723016905073e-05,
      "loss": 0.8546,
      "step": 6610
    },
    {
      "epoch": 2.8449554099065217,
      "grad_norm": 1.9417062997817993,
      "learning_rate": 1.305071521456437e-05,
      "loss": 0.8471,
      "step": 6620
    },
    {
      "epoch": 2.8492532502417536,
      "grad_norm": 1.9409371614456177,
      "learning_rate": 1.3024707412223668e-05,
      "loss": 0.8261,
      "step": 6630
    },
    {
      "epoch": 2.853551090576985,
      "grad_norm": 2.3218955993652344,
      "learning_rate": 1.2998699609882965e-05,
      "loss": 0.8507,
      "step": 6640
    },
    {
      "epoch": 2.8578489309122164,
      "grad_norm": 2.1245205402374268,
      "learning_rate": 1.2972691807542263e-05,
      "loss": 0.8422,
      "step": 6650
    },
    {
      "epoch": 2.862146771247448,
      "grad_norm": 2.355492353439331,
      "learning_rate": 1.294668400520156e-05,
      "loss": 0.8386,
      "step": 6660
    },
    {
      "epoch": 2.8664446115826796,
      "grad_norm": 2.877692937850952,
      "learning_rate": 1.2923276983094929e-05,
      "loss": 0.8149,
      "step": 6670
    },
    {
      "epoch": 2.870742451917911,
      "grad_norm": 2.6216773986816406,
      "learning_rate": 1.2897269180754227e-05,
      "loss": 0.827,
      "step": 6680
    },
    {
      "epoch": 2.875040292253143,
      "grad_norm": 2.0071592330932617,
      "learning_rate": 1.2871261378413524e-05,
      "loss": 0.8167,
      "step": 6690
    },
    {
      "epoch": 2.8793381325883742,
      "grad_norm": 1.850724697113037,
      "learning_rate": 1.2845253576072822e-05,
      "loss": 0.8512,
      "step": 6700
    },
    {
      "epoch": 2.8836359729236056,
      "grad_norm": 2.6591250896453857,
      "learning_rate": 1.281924577373212e-05,
      "loss": 0.8562,
      "step": 6710
    },
    {
      "epoch": 2.8879338132588375,
      "grad_norm": 2.1101179122924805,
      "learning_rate": 1.2793237971391419e-05,
      "loss": 0.8594,
      "step": 6720
    },
    {
      "epoch": 2.892231653594069,
      "grad_norm": 2.4941248893737793,
      "learning_rate": 1.2767230169050716e-05,
      "loss": 0.8556,
      "step": 6730
    },
    {
      "epoch": 2.8965294939293003,
      "grad_norm": 2.3820595741271973,
      "learning_rate": 1.2741222366710014e-05,
      "loss": 0.8042,
      "step": 6740
    },
    {
      "epoch": 2.900827334264532,
      "grad_norm": 2.3879141807556152,
      "learning_rate": 1.2715214564369311e-05,
      "loss": 0.8219,
      "step": 6750
    },
    {
      "epoch": 2.9051251745997635,
      "grad_norm": 1.9088221788406372,
      "learning_rate": 1.2689206762028609e-05,
      "loss": 0.8341,
      "step": 6760
    },
    {
      "epoch": 2.909423014934995,
      "grad_norm": 1.4908653497695923,
      "learning_rate": 1.2663198959687906e-05,
      "loss": 0.8409,
      "step": 6770
    },
    {
      "epoch": 2.9137208552702267,
      "grad_norm": 2.528444766998291,
      "learning_rate": 1.2637191157347204e-05,
      "loss": 0.8191,
      "step": 6780
    },
    {
      "epoch": 2.918018695605458,
      "grad_norm": 2.0630528926849365,
      "learning_rate": 1.2611183355006503e-05,
      "loss": 0.859,
      "step": 6790
    },
    {
      "epoch": 2.92231653594069,
      "grad_norm": 2.4844236373901367,
      "learning_rate": 1.25851755526658e-05,
      "loss": 0.7967,
      "step": 6800
    },
    {
      "epoch": 2.9266143762759214,
      "grad_norm": 2.5486648082733154,
      "learning_rate": 1.2559167750325098e-05,
      "loss": 0.838,
      "step": 6810
    },
    {
      "epoch": 2.9309122166111528,
      "grad_norm": 1.8010075092315674,
      "learning_rate": 1.2533159947984396e-05,
      "loss": 0.7904,
      "step": 6820
    },
    {
      "epoch": 2.9352100569463846,
      "grad_norm": 2.676269769668579,
      "learning_rate": 1.2507152145643693e-05,
      "loss": 0.8388,
      "step": 6830
    },
    {
      "epoch": 2.939507897281616,
      "grad_norm": 2.14870285987854,
      "learning_rate": 1.248114434330299e-05,
      "loss": 0.8727,
      "step": 6840
    },
    {
      "epoch": 2.9438057376168474,
      "grad_norm": 1.9494338035583496,
      "learning_rate": 1.245513654096229e-05,
      "loss": 0.8614,
      "step": 6850
    },
    {
      "epoch": 2.9481035779520792,
      "grad_norm": 2.946183681488037,
      "learning_rate": 1.2429128738621587e-05,
      "loss": 0.8353,
      "step": 6860
    },
    {
      "epoch": 2.9524014182873106,
      "grad_norm": 2.379324436187744,
      "learning_rate": 1.2403120936280885e-05,
      "loss": 0.845,
      "step": 6870
    },
    {
      "epoch": 2.956699258622542,
      "grad_norm": 2.0696635246276855,
      "learning_rate": 1.2377113133940183e-05,
      "loss": 0.8114,
      "step": 6880
    },
    {
      "epoch": 2.960997098957774,
      "grad_norm": 2.07360577583313,
      "learning_rate": 1.235110533159948e-05,
      "loss": 0.8324,
      "step": 6890
    },
    {
      "epoch": 2.9652949392930052,
      "grad_norm": 2.1506571769714355,
      "learning_rate": 1.2325097529258778e-05,
      "loss": 0.8277,
      "step": 6900
    },
    {
      "epoch": 2.9695927796282366,
      "grad_norm": 2.3600423336029053,
      "learning_rate": 1.2299089726918075e-05,
      "loss": 0.8476,
      "step": 6910
    },
    {
      "epoch": 2.9738906199634685,
      "grad_norm": 2.910858631134033,
      "learning_rate": 1.2273081924577373e-05,
      "loss": 0.8341,
      "step": 6920
    },
    {
      "epoch": 2.9781884602987,
      "grad_norm": 2.2574076652526855,
      "learning_rate": 1.224707412223667e-05,
      "loss": 0.8293,
      "step": 6930
    },
    {
      "epoch": 2.9824863006339317,
      "grad_norm": 2.9502387046813965,
      "learning_rate": 1.222106631989597e-05,
      "loss": 0.8322,
      "step": 6940
    },
    {
      "epoch": 2.986784140969163,
      "grad_norm": 2.43100905418396,
      "learning_rate": 1.2195058517555267e-05,
      "loss": 0.8134,
      "step": 6950
    },
    {
      "epoch": 2.9910819813043945,
      "grad_norm": 2.6545886993408203,
      "learning_rate": 1.2169050715214565e-05,
      "loss": 0.8338,
      "step": 6960
    },
    {
      "epoch": 2.9953798216396263,
      "grad_norm": 3.043407440185547,
      "learning_rate": 1.2143042912873862e-05,
      "loss": 0.859,
      "step": 6970
    },
    {
      "epoch": 2.9996776619748577,
      "grad_norm": 3.557896614074707,
      "learning_rate": 1.2117035110533161e-05,
      "loss": 0.8159,
      "step": 6980
    },
    {
      "epoch": 3.0038680563017084,
      "grad_norm": 2.024470329284668,
      "learning_rate": 1.2091027308192459e-05,
      "loss": 0.7895,
      "step": 6990
    },
    {
      "epoch": 3.00816589663694,
      "grad_norm": 1.860510230064392,
      "learning_rate": 1.2065019505851756e-05,
      "loss": 0.787,
      "step": 7000
    },
    {
      "epoch": 3.0124637369721716,
      "grad_norm": 2.042402505874634,
      "learning_rate": 1.2039011703511054e-05,
      "loss": 0.7864,
      "step": 7010
    },
    {
      "epoch": 3.016761577307403,
      "grad_norm": 1.696939468383789,
      "learning_rate": 1.2013003901170352e-05,
      "loss": 0.8223,
      "step": 7020
    },
    {
      "epoch": 3.0210594176426344,
      "grad_norm": 2.0660598278045654,
      "learning_rate": 1.1986996098829649e-05,
      "loss": 0.8379,
      "step": 7030
    },
    {
      "epoch": 3.0253572579778663,
      "grad_norm": 2.2530200481414795,
      "learning_rate": 1.1960988296488947e-05,
      "loss": 0.8144,
      "step": 7040
    },
    {
      "epoch": 3.0296550983130976,
      "grad_norm": 3.971562385559082,
      "learning_rate": 1.1934980494148244e-05,
      "loss": 0.8232,
      "step": 7050
    },
    {
      "epoch": 3.033952938648329,
      "grad_norm": 2.6129651069641113,
      "learning_rate": 1.1908972691807542e-05,
      "loss": 0.8419,
      "step": 7060
    },
    {
      "epoch": 3.038250778983561,
      "grad_norm": 2.086557149887085,
      "learning_rate": 1.188296488946684e-05,
      "loss": 0.8388,
      "step": 7070
    },
    {
      "epoch": 3.0425486193187923,
      "grad_norm": 2.1628611087799072,
      "learning_rate": 1.1856957087126137e-05,
      "loss": 0.8223,
      "step": 7080
    },
    {
      "epoch": 3.0468464596540237,
      "grad_norm": 3.3434057235717773,
      "learning_rate": 1.1830949284785436e-05,
      "loss": 0.8043,
      "step": 7090
    },
    {
      "epoch": 3.0511442999892555,
      "grad_norm": 2.514206647872925,
      "learning_rate": 1.1804941482444734e-05,
      "loss": 0.8451,
      "step": 7100
    },
    {
      "epoch": 3.055442140324487,
      "grad_norm": 1.8894104957580566,
      "learning_rate": 1.1778933680104033e-05,
      "loss": 0.8301,
      "step": 7110
    },
    {
      "epoch": 3.0597399806597183,
      "grad_norm": 2.266132354736328,
      "learning_rate": 1.175292587776333e-05,
      "loss": 0.8085,
      "step": 7120
    },
    {
      "epoch": 3.06403782099495,
      "grad_norm": 2.2788617610931396,
      "learning_rate": 1.1726918075422628e-05,
      "loss": 0.8739,
      "step": 7130
    },
    {
      "epoch": 3.0683356613301815,
      "grad_norm": 2.084709405899048,
      "learning_rate": 1.1700910273081925e-05,
      "loss": 0.8204,
      "step": 7140
    },
    {
      "epoch": 3.072633501665413,
      "grad_norm": 1.974876046180725,
      "learning_rate": 1.1674902470741223e-05,
      "loss": 0.8428,
      "step": 7150
    },
    {
      "epoch": 3.0769313420006448,
      "grad_norm": 2.421759605407715,
      "learning_rate": 1.164889466840052e-05,
      "loss": 0.8769,
      "step": 7160
    },
    {
      "epoch": 3.081229182335876,
      "grad_norm": 1.9922422170639038,
      "learning_rate": 1.1622886866059818e-05,
      "loss": 0.8288,
      "step": 7170
    },
    {
      "epoch": 3.0855270226711076,
      "grad_norm": 2.1146905422210693,
      "learning_rate": 1.1596879063719116e-05,
      "loss": 0.845,
      "step": 7180
    },
    {
      "epoch": 3.0898248630063394,
      "grad_norm": 2.7656190395355225,
      "learning_rate": 1.1570871261378413e-05,
      "loss": 0.8602,
      "step": 7190
    },
    {
      "epoch": 3.094122703341571,
      "grad_norm": 2.6112468242645264,
      "learning_rate": 1.154486345903771e-05,
      "loss": 0.8218,
      "step": 7200
    },
    {
      "epoch": 3.098420543676802,
      "grad_norm": 2.0153255462646484,
      "learning_rate": 1.1518855656697008e-05,
      "loss": 0.8232,
      "step": 7210
    },
    {
      "epoch": 3.102718384012034,
      "grad_norm": 2.0627946853637695,
      "learning_rate": 1.1492847854356306e-05,
      "loss": 0.8048,
      "step": 7220
    },
    {
      "epoch": 3.1070162243472654,
      "grad_norm": 2.7287638187408447,
      "learning_rate": 1.1466840052015605e-05,
      "loss": 0.8125,
      "step": 7230
    },
    {
      "epoch": 3.111314064682497,
      "grad_norm": 2.091202735900879,
      "learning_rate": 1.1440832249674904e-05,
      "loss": 0.8383,
      "step": 7240
    },
    {
      "epoch": 3.1156119050177287,
      "grad_norm": 2.5563840866088867,
      "learning_rate": 1.1414824447334202e-05,
      "loss": 0.8595,
      "step": 7250
    },
    {
      "epoch": 3.11990974535296,
      "grad_norm": 2.5393481254577637,
      "learning_rate": 1.13888166449935e-05,
      "loss": 0.8263,
      "step": 7260
    },
    {
      "epoch": 3.1242075856881915,
      "grad_norm": 3.096450090408325,
      "learning_rate": 1.1362808842652797e-05,
      "loss": 0.8456,
      "step": 7270
    },
    {
      "epoch": 3.1285054260234233,
      "grad_norm": 2.503580093383789,
      "learning_rate": 1.1336801040312094e-05,
      "loss": 0.8682,
      "step": 7280
    },
    {
      "epoch": 3.1328032663586547,
      "grad_norm": 2.7597365379333496,
      "learning_rate": 1.1310793237971392e-05,
      "loss": 0.8241,
      "step": 7290
    },
    {
      "epoch": 3.137101106693886,
      "grad_norm": 1.942299723625183,
      "learning_rate": 1.128478543563069e-05,
      "loss": 0.82,
      "step": 7300
    },
    {
      "epoch": 3.141398947029118,
      "grad_norm": 2.6169538497924805,
      "learning_rate": 1.1258777633289987e-05,
      "loss": 0.7864,
      "step": 7310
    },
    {
      "epoch": 3.1456967873643493,
      "grad_norm": 2.995603561401367,
      "learning_rate": 1.1232769830949285e-05,
      "loss": 0.846,
      "step": 7320
    },
    {
      "epoch": 3.149994627699581,
      "grad_norm": 2.4241340160369873,
      "learning_rate": 1.1206762028608582e-05,
      "loss": 0.8376,
      "step": 7330
    },
    {
      "epoch": 3.1542924680348126,
      "grad_norm": 2.5634195804595947,
      "learning_rate": 1.118075422626788e-05,
      "loss": 0.8236,
      "step": 7340
    },
    {
      "epoch": 3.158590308370044,
      "grad_norm": 2.059213399887085,
      "learning_rate": 1.1154746423927177e-05,
      "loss": 0.8072,
      "step": 7350
    },
    {
      "epoch": 3.162888148705276,
      "grad_norm": 2.7327308654785156,
      "learning_rate": 1.1128738621586476e-05,
      "loss": 0.8089,
      "step": 7360
    },
    {
      "epoch": 3.167185989040507,
      "grad_norm": 2.2895102500915527,
      "learning_rate": 1.1102730819245774e-05,
      "loss": 0.8053,
      "step": 7370
    },
    {
      "epoch": 3.1714838293757386,
      "grad_norm": 2.430119752883911,
      "learning_rate": 1.1076723016905073e-05,
      "loss": 0.8779,
      "step": 7380
    },
    {
      "epoch": 3.1757816697109704,
      "grad_norm": 2.6809425354003906,
      "learning_rate": 1.105071521456437e-05,
      "loss": 0.8245,
      "step": 7390
    },
    {
      "epoch": 3.180079510046202,
      "grad_norm": 2.0371809005737305,
      "learning_rate": 1.1024707412223668e-05,
      "loss": 0.8274,
      "step": 7400
    },
    {
      "epoch": 3.184377350381433,
      "grad_norm": 2.131962776184082,
      "learning_rate": 1.0998699609882966e-05,
      "loss": 0.826,
      "step": 7410
    },
    {
      "epoch": 3.188675190716665,
      "grad_norm": 2.4399635791778564,
      "learning_rate": 1.0972691807542263e-05,
      "loss": 0.8551,
      "step": 7420
    },
    {
      "epoch": 3.1929730310518964,
      "grad_norm": 3.0379292964935303,
      "learning_rate": 1.0946684005201561e-05,
      "loss": 0.8193,
      "step": 7430
    },
    {
      "epoch": 3.197270871387128,
      "grad_norm": 2.229238271713257,
      "learning_rate": 1.0920676202860858e-05,
      "loss": 0.8328,
      "step": 7440
    },
    {
      "epoch": 3.2015687117223597,
      "grad_norm": 2.4807183742523193,
      "learning_rate": 1.0894668400520156e-05,
      "loss": 0.8689,
      "step": 7450
    },
    {
      "epoch": 3.205866552057591,
      "grad_norm": 2.323064088821411,
      "learning_rate": 1.0868660598179453e-05,
      "loss": 0.8219,
      "step": 7460
    },
    {
      "epoch": 3.2101643923928225,
      "grad_norm": 2.211792469024658,
      "learning_rate": 1.0842652795838751e-05,
      "loss": 0.8568,
      "step": 7470
    },
    {
      "epoch": 3.2144622327280543,
      "grad_norm": 2.131065607070923,
      "learning_rate": 1.0816644993498049e-05,
      "loss": 0.8151,
      "step": 7480
    },
    {
      "epoch": 3.2187600730632857,
      "grad_norm": 2.016026496887207,
      "learning_rate": 1.0790637191157348e-05,
      "loss": 0.8398,
      "step": 7490
    },
    {
      "epoch": 3.223057913398517,
      "grad_norm": 2.6922519207000732,
      "learning_rate": 1.0764629388816645e-05,
      "loss": 0.8273,
      "step": 7500
    },
    {
      "epoch": 3.227355753733749,
      "grad_norm": 3.1272406578063965,
      "learning_rate": 1.0738621586475943e-05,
      "loss": 0.8666,
      "step": 7510
    },
    {
      "epoch": 3.2316535940689803,
      "grad_norm": 1.8319913148880005,
      "learning_rate": 1.071261378413524e-05,
      "loss": 0.8093,
      "step": 7520
    },
    {
      "epoch": 3.2359514344042117,
      "grad_norm": 1.9033701419830322,
      "learning_rate": 1.068660598179454e-05,
      "loss": 0.8246,
      "step": 7530
    },
    {
      "epoch": 3.2402492747394436,
      "grad_norm": 2.841714859008789,
      "learning_rate": 1.0660598179453837e-05,
      "loss": 0.8132,
      "step": 7540
    },
    {
      "epoch": 3.244547115074675,
      "grad_norm": 1.992087960243225,
      "learning_rate": 1.0634590377113135e-05,
      "loss": 0.8417,
      "step": 7550
    },
    {
      "epoch": 3.2488449554099064,
      "grad_norm": 2.1149866580963135,
      "learning_rate": 1.0608582574772432e-05,
      "loss": 0.8404,
      "step": 7560
    },
    {
      "epoch": 3.253142795745138,
      "grad_norm": 2.992253303527832,
      "learning_rate": 1.058257477243173e-05,
      "loss": 0.8341,
      "step": 7570
    },
    {
      "epoch": 3.2574406360803696,
      "grad_norm": 2.3603456020355225,
      "learning_rate": 1.0556566970091027e-05,
      "loss": 0.7966,
      "step": 7580
    },
    {
      "epoch": 3.261738476415601,
      "grad_norm": 2.294147253036499,
      "learning_rate": 1.0530559167750325e-05,
      "loss": 0.8459,
      "step": 7590
    },
    {
      "epoch": 3.266036316750833,
      "grad_norm": 2.292954921722412,
      "learning_rate": 1.0504551365409622e-05,
      "loss": 0.8175,
      "step": 7600
    },
    {
      "epoch": 3.2703341570860642,
      "grad_norm": 2.7061562538146973,
      "learning_rate": 1.047854356306892e-05,
      "loss": 0.8435,
      "step": 7610
    },
    {
      "epoch": 3.2746319974212956,
      "grad_norm": 2.2146809101104736,
      "learning_rate": 1.045253576072822e-05,
      "loss": 0.8242,
      "step": 7620
    },
    {
      "epoch": 3.2789298377565275,
      "grad_norm": 2.8174612522125244,
      "learning_rate": 1.0426527958387517e-05,
      "loss": 0.7991,
      "step": 7630
    },
    {
      "epoch": 3.283227678091759,
      "grad_norm": 2.6726443767547607,
      "learning_rate": 1.0400520156046814e-05,
      "loss": 0.8236,
      "step": 7640
    },
    {
      "epoch": 3.2875255184269903,
      "grad_norm": 2.866483688354492,
      "learning_rate": 1.0374512353706112e-05,
      "loss": 0.8567,
      "step": 7650
    },
    {
      "epoch": 3.291823358762222,
      "grad_norm": 1.9545886516571045,
      "learning_rate": 1.034850455136541e-05,
      "loss": 0.8207,
      "step": 7660
    },
    {
      "epoch": 3.2961211990974535,
      "grad_norm": 2.0908777713775635,
      "learning_rate": 1.0322496749024707e-05,
      "loss": 0.8078,
      "step": 7670
    },
    {
      "epoch": 3.300419039432685,
      "grad_norm": 2.28080153465271,
      "learning_rate": 1.0296488946684006e-05,
      "loss": 0.8491,
      "step": 7680
    },
    {
      "epoch": 3.3047168797679167,
      "grad_norm": 4.314270973205566,
      "learning_rate": 1.0270481144343304e-05,
      "loss": 0.7932,
      "step": 7690
    },
    {
      "epoch": 3.309014720103148,
      "grad_norm": 2.002147674560547,
      "learning_rate": 1.0244473342002601e-05,
      "loss": 0.8541,
      "step": 7700
    },
    {
      "epoch": 3.31331256043838,
      "grad_norm": 2.7952816486358643,
      "learning_rate": 1.0218465539661899e-05,
      "loss": 0.8075,
      "step": 7710
    },
    {
      "epoch": 3.3176104007736114,
      "grad_norm": 2.076509714126587,
      "learning_rate": 1.0192457737321196e-05,
      "loss": 0.83,
      "step": 7720
    },
    {
      "epoch": 3.3219082411088428,
      "grad_norm": 3.043917179107666,
      "learning_rate": 1.0166449934980494e-05,
      "loss": 0.86,
      "step": 7730
    },
    {
      "epoch": 3.3262060814440746,
      "grad_norm": 2.832225799560547,
      "learning_rate": 1.0140442132639793e-05,
      "loss": 0.8542,
      "step": 7740
    },
    {
      "epoch": 3.330503921779306,
      "grad_norm": 2.1683218479156494,
      "learning_rate": 1.011443433029909e-05,
      "loss": 0.8293,
      "step": 7750
    },
    {
      "epoch": 3.3348017621145374,
      "grad_norm": 2.3730475902557373,
      "learning_rate": 1.0088426527958388e-05,
      "loss": 0.8439,
      "step": 7760
    },
    {
      "epoch": 3.339099602449769,
      "grad_norm": 2.539515972137451,
      "learning_rate": 1.0062418725617686e-05,
      "loss": 0.8129,
      "step": 7770
    },
    {
      "epoch": 3.3433974427850006,
      "grad_norm": 1.996179223060608,
      "learning_rate": 1.0036410923276983e-05,
      "loss": 0.831,
      "step": 7780
    },
    {
      "epoch": 3.347695283120232,
      "grad_norm": 2.465106248855591,
      "learning_rate": 1.001040312093628e-05,
      "loss": 0.8317,
      "step": 7790
    },
    {
      "epoch": 3.351993123455464,
      "grad_norm": 1.9972130060195923,
      "learning_rate": 9.984395318595578e-06,
      "loss": 0.8035,
      "step": 7800
    },
    {
      "epoch": 3.3562909637906952,
      "grad_norm": 2.7597994804382324,
      "learning_rate": 9.958387516254876e-06,
      "loss": 0.8365,
      "step": 7810
    },
    {
      "epoch": 3.3605888041259266,
      "grad_norm": 1.8616628646850586,
      "learning_rate": 9.932379713914175e-06,
      "loss": 0.8308,
      "step": 7820
    },
    {
      "epoch": 3.3648866444611585,
      "grad_norm": 2.139962673187256,
      "learning_rate": 9.906371911573473e-06,
      "loss": 0.8262,
      "step": 7830
    },
    {
      "epoch": 3.36918448479639,
      "grad_norm": 2.3861114978790283,
      "learning_rate": 9.88036410923277e-06,
      "loss": 0.8189,
      "step": 7840
    },
    {
      "epoch": 3.3734823251316213,
      "grad_norm": 2.3144469261169434,
      "learning_rate": 9.854356306892068e-06,
      "loss": 0.8607,
      "step": 7850
    },
    {
      "epoch": 3.377780165466853,
      "grad_norm": 1.780122995376587,
      "learning_rate": 9.828348504551365e-06,
      "loss": 0.8332,
      "step": 7860
    },
    {
      "epoch": 3.3820780058020845,
      "grad_norm": 2.596065044403076,
      "learning_rate": 9.802340702210664e-06,
      "loss": 0.8262,
      "step": 7870
    },
    {
      "epoch": 3.386375846137316,
      "grad_norm": 2.355656623840332,
      "learning_rate": 9.776332899869962e-06,
      "loss": 0.8734,
      "step": 7880
    },
    {
      "epoch": 3.3906736864725477,
      "grad_norm": 2.7183563709259033,
      "learning_rate": 9.75032509752926e-06,
      "loss": 0.8191,
      "step": 7890
    },
    {
      "epoch": 3.394971526807779,
      "grad_norm": 2.553727626800537,
      "learning_rate": 9.724317295188557e-06,
      "loss": 0.7987,
      "step": 7900
    },
    {
      "epoch": 3.3992693671430105,
      "grad_norm": 2.2755966186523438,
      "learning_rate": 9.698309492847855e-06,
      "loss": 0.7956,
      "step": 7910
    },
    {
      "epoch": 3.4035672074782424,
      "grad_norm": 1.9395509958267212,
      "learning_rate": 9.672301690507152e-06,
      "loss": 0.8504,
      "step": 7920
    },
    {
      "epoch": 3.4078650478134738,
      "grad_norm": 2.062424421310425,
      "learning_rate": 9.64629388816645e-06,
      "loss": 0.8173,
      "step": 7930
    },
    {
      "epoch": 3.412162888148705,
      "grad_norm": 2.3288259506225586,
      "learning_rate": 9.620286085825747e-06,
      "loss": 0.8428,
      "step": 7940
    },
    {
      "epoch": 3.416460728483937,
      "grad_norm": 2.203564405441284,
      "learning_rate": 9.594278283485045e-06,
      "loss": 0.8312,
      "step": 7950
    },
    {
      "epoch": 3.4207585688191684,
      "grad_norm": 1.8323594331741333,
      "learning_rate": 9.568270481144342e-06,
      "loss": 0.8059,
      "step": 7960
    },
    {
      "epoch": 3.4250564091544,
      "grad_norm": 2.775157928466797,
      "learning_rate": 9.542262678803642e-06,
      "loss": 0.8419,
      "step": 7970
    },
    {
      "epoch": 3.4293542494896316,
      "grad_norm": 1.598055124282837,
      "learning_rate": 9.516254876462939e-06,
      "loss": 0.8036,
      "step": 7980
    },
    {
      "epoch": 3.433652089824863,
      "grad_norm": 2.267578125,
      "learning_rate": 9.490247074122237e-06,
      "loss": 0.8269,
      "step": 7990
    },
    {
      "epoch": 3.4379499301600944,
      "grad_norm": 2.4164369106292725,
      "learning_rate": 9.464239271781536e-06,
      "loss": 0.8181,
      "step": 8000
    },
    {
      "epoch": 3.4422477704953263,
      "grad_norm": 1.7315623760223389,
      "learning_rate": 9.438231469440833e-06,
      "loss": 0.8049,
      "step": 8010
    },
    {
      "epoch": 3.4465456108305577,
      "grad_norm": 2.3677570819854736,
      "learning_rate": 9.412223667100131e-06,
      "loss": 0.7818,
      "step": 8020
    },
    {
      "epoch": 3.450843451165789,
      "grad_norm": 2.290221929550171,
      "learning_rate": 9.386215864759429e-06,
      "loss": 0.8275,
      "step": 8030
    },
    {
      "epoch": 3.455141291501021,
      "grad_norm": 2.1171271800994873,
      "learning_rate": 9.360208062418726e-06,
      "loss": 0.8081,
      "step": 8040
    },
    {
      "epoch": 3.4594391318362523,
      "grad_norm": 2.1126463413238525,
      "learning_rate": 9.334200260078024e-06,
      "loss": 0.8212,
      "step": 8050
    },
    {
      "epoch": 3.4637369721714837,
      "grad_norm": 1.9094538688659668,
      "learning_rate": 9.308192457737321e-06,
      "loss": 0.8079,
      "step": 8060
    },
    {
      "epoch": 3.4680348125067155,
      "grad_norm": 2.7944271564483643,
      "learning_rate": 9.282184655396619e-06,
      "loss": 0.832,
      "step": 8070
    },
    {
      "epoch": 3.472332652841947,
      "grad_norm": 2.978090524673462,
      "learning_rate": 9.256176853055916e-06,
      "loss": 0.8241,
      "step": 8080
    },
    {
      "epoch": 3.4766304931771783,
      "grad_norm": 3.072603225708008,
      "learning_rate": 9.230169050715214e-06,
      "loss": 0.8134,
      "step": 8090
    },
    {
      "epoch": 3.48092833351241,
      "grad_norm": 4.253958702087402,
      "learning_rate": 9.204161248374511e-06,
      "loss": 0.8451,
      "step": 8100
    },
    {
      "epoch": 3.4852261738476416,
      "grad_norm": 3.052816152572632,
      "learning_rate": 9.178153446033809e-06,
      "loss": 0.8797,
      "step": 8110
    },
    {
      "epoch": 3.489524014182873,
      "grad_norm": 2.726505756378174,
      "learning_rate": 9.152145643693108e-06,
      "loss": 0.8132,
      "step": 8120
    },
    {
      "epoch": 3.493821854518105,
      "grad_norm": 2.604417562484741,
      "learning_rate": 9.126137841352407e-06,
      "loss": 0.822,
      "step": 8130
    },
    {
      "epoch": 3.498119694853336,
      "grad_norm": 3.2186293601989746,
      "learning_rate": 9.100130039011705e-06,
      "loss": 0.8667,
      "step": 8140
    },
    {
      "epoch": 3.5024175351885676,
      "grad_norm": 2.2375166416168213,
      "learning_rate": 9.074122236671002e-06,
      "loss": 0.7988,
      "step": 8150
    },
    {
      "epoch": 3.5067153755237994,
      "grad_norm": 2.989326238632202,
      "learning_rate": 9.0481144343303e-06,
      "loss": 0.816,
      "step": 8160
    },
    {
      "epoch": 3.511013215859031,
      "grad_norm": 2.7180869579315186,
      "learning_rate": 9.022106631989597e-06,
      "loss": 0.8046,
      "step": 8170
    },
    {
      "epoch": 3.515311056194262,
      "grad_norm": 2.3229308128356934,
      "learning_rate": 8.996098829648895e-06,
      "loss": 0.8233,
      "step": 8180
    },
    {
      "epoch": 3.519608896529494,
      "grad_norm": 2.281017780303955,
      "learning_rate": 8.970091027308193e-06,
      "loss": 0.8464,
      "step": 8190
    },
    {
      "epoch": 3.5239067368647254,
      "grad_norm": 2.65671968460083,
      "learning_rate": 8.94408322496749e-06,
      "loss": 0.8371,
      "step": 8200
    },
    {
      "epoch": 3.528204577199957,
      "grad_norm": 2.0153794288635254,
      "learning_rate": 8.918075422626788e-06,
      "loss": 0.8459,
      "step": 8210
    },
    {
      "epoch": 3.5325024175351887,
      "grad_norm": 1.954141616821289,
      "learning_rate": 8.892067620286085e-06,
      "loss": 0.8388,
      "step": 8220
    },
    {
      "epoch": 3.53680025787042,
      "grad_norm": 2.499011993408203,
      "learning_rate": 8.866059817945383e-06,
      "loss": 0.7992,
      "step": 8230
    },
    {
      "epoch": 3.5410980982056515,
      "grad_norm": 2.2292702198028564,
      "learning_rate": 8.84005201560468e-06,
      "loss": 0.8061,
      "step": 8240
    },
    {
      "epoch": 3.5453959385408833,
      "grad_norm": 2.362806797027588,
      "learning_rate": 8.81404421326398e-06,
      "loss": 0.8333,
      "step": 8250
    },
    {
      "epoch": 3.5496937788761147,
      "grad_norm": 2.234999895095825,
      "learning_rate": 8.788036410923279e-06,
      "loss": 0.8405,
      "step": 8260
    },
    {
      "epoch": 3.553991619211346,
      "grad_norm": 2.053683042526245,
      "learning_rate": 8.762028608582576e-06,
      "loss": 0.8087,
      "step": 8270
    },
    {
      "epoch": 3.558289459546578,
      "grad_norm": 2.564079523086548,
      "learning_rate": 8.736020806241874e-06,
      "loss": 0.8627,
      "step": 8280
    },
    {
      "epoch": 3.5625872998818093,
      "grad_norm": 2.4949774742126465,
      "learning_rate": 8.710013003901171e-06,
      "loss": 0.8267,
      "step": 8290
    },
    {
      "epoch": 3.5668851402170407,
      "grad_norm": 2.2918150424957275,
      "learning_rate": 8.684005201560469e-06,
      "loss": 0.822,
      "step": 8300
    },
    {
      "epoch": 3.5711829805522726,
      "grad_norm": 2.0078179836273193,
      "learning_rate": 8.657997399219766e-06,
      "loss": 0.796,
      "step": 8310
    },
    {
      "epoch": 3.575480820887504,
      "grad_norm": 2.080048084259033,
      "learning_rate": 8.631989596879064e-06,
      "loss": 0.8171,
      "step": 8320
    },
    {
      "epoch": 3.5797786612227354,
      "grad_norm": 1.7679299116134644,
      "learning_rate": 8.605981794538362e-06,
      "loss": 0.8229,
      "step": 8330
    },
    {
      "epoch": 3.584076501557967,
      "grad_norm": 2.427288293838501,
      "learning_rate": 8.579973992197659e-06,
      "loss": 0.7904,
      "step": 8340
    },
    {
      "epoch": 3.5883743418931986,
      "grad_norm": 2.287069797515869,
      "learning_rate": 8.553966189856957e-06,
      "loss": 0.8039,
      "step": 8350
    },
    {
      "epoch": 3.59267218222843,
      "grad_norm": 3.4274613857269287,
      "learning_rate": 8.527958387516254e-06,
      "loss": 0.8253,
      "step": 8360
    },
    {
      "epoch": 3.596970022563662,
      "grad_norm": 2.506869077682495,
      "learning_rate": 8.501950585175552e-06,
      "loss": 0.8461,
      "step": 8370
    },
    {
      "epoch": 3.6012678628988932,
      "grad_norm": 4.560885906219482,
      "learning_rate": 8.475942782834851e-06,
      "loss": 0.8719,
      "step": 8380
    },
    {
      "epoch": 3.6055657032341246,
      "grad_norm": 2.187072277069092,
      "learning_rate": 8.449934980494148e-06,
      "loss": 0.8302,
      "step": 8390
    },
    {
      "epoch": 3.6098635435693565,
      "grad_norm": 2.373382806777954,
      "learning_rate": 8.423927178153446e-06,
      "loss": 0.8107,
      "step": 8400
    },
    {
      "epoch": 3.614161383904588,
      "grad_norm": 2.464660406112671,
      "learning_rate": 8.397919375812745e-06,
      "loss": 0.8356,
      "step": 8410
    },
    {
      "epoch": 3.6184592242398192,
      "grad_norm": 2.206669807434082,
      "learning_rate": 8.371911573472043e-06,
      "loss": 0.8307,
      "step": 8420
    },
    {
      "epoch": 3.622757064575051,
      "grad_norm": 2.623462200164795,
      "learning_rate": 8.34590377113134e-06,
      "loss": 0.8224,
      "step": 8430
    },
    {
      "epoch": 3.6270549049102825,
      "grad_norm": 2.3635032176971436,
      "learning_rate": 8.319895968790638e-06,
      "loss": 0.8202,
      "step": 8440
    },
    {
      "epoch": 3.631352745245514,
      "grad_norm": 2.492725372314453,
      "learning_rate": 8.293888166449935e-06,
      "loss": 0.8092,
      "step": 8450
    },
    {
      "epoch": 3.6356505855807457,
      "grad_norm": 2.244466781616211,
      "learning_rate": 8.267880364109233e-06,
      "loss": 0.7999,
      "step": 8460
    },
    {
      "epoch": 3.639948425915977,
      "grad_norm": 2.049755096435547,
      "learning_rate": 8.24187256176853e-06,
      "loss": 0.8135,
      "step": 8470
    },
    {
      "epoch": 3.6442462662512085,
      "grad_norm": 2.556460380554199,
      "learning_rate": 8.215864759427828e-06,
      "loss": 0.8084,
      "step": 8480
    },
    {
      "epoch": 3.6485441065864403,
      "grad_norm": 2.2239139080047607,
      "learning_rate": 8.189856957087126e-06,
      "loss": 0.8321,
      "step": 8490
    },
    {
      "epoch": 3.6528419469216717,
      "grad_norm": 2.6177773475646973,
      "learning_rate": 8.163849154746423e-06,
      "loss": 0.8081,
      "step": 8500
    },
    {
      "epoch": 3.657139787256903,
      "grad_norm": 2.3924152851104736,
      "learning_rate": 8.137841352405722e-06,
      "loss": 0.8002,
      "step": 8510
    },
    {
      "epoch": 3.661437627592135,
      "grad_norm": 1.8663215637207031,
      "learning_rate": 8.11183355006502e-06,
      "loss": 0.8408,
      "step": 8520
    },
    {
      "epoch": 3.6657354679273664,
      "grad_norm": 1.928332805633545,
      "learning_rate": 8.085825747724317e-06,
      "loss": 0.7947,
      "step": 8530
    },
    {
      "epoch": 3.6700333082625978,
      "grad_norm": 2.443760871887207,
      "learning_rate": 8.059817945383615e-06,
      "loss": 0.8422,
      "step": 8540
    },
    {
      "epoch": 3.6743311485978296,
      "grad_norm": 2.8904833793640137,
      "learning_rate": 8.033810143042912e-06,
      "loss": 0.8208,
      "step": 8550
    },
    {
      "epoch": 3.678628988933061,
      "grad_norm": 2.169635534286499,
      "learning_rate": 8.007802340702212e-06,
      "loss": 0.8709,
      "step": 8560
    },
    {
      "epoch": 3.682926829268293,
      "grad_norm": 2.3419296741485596,
      "learning_rate": 7.98179453836151e-06,
      "loss": 0.822,
      "step": 8570
    },
    {
      "epoch": 3.6872246696035242,
      "grad_norm": 3.184398889541626,
      "learning_rate": 7.955786736020807e-06,
      "loss": 0.8137,
      "step": 8580
    },
    {
      "epoch": 3.6915225099387556,
      "grad_norm": 2.1120145320892334,
      "learning_rate": 7.929778933680104e-06,
      "loss": 0.808,
      "step": 8590
    },
    {
      "epoch": 3.6958203502739875,
      "grad_norm": 2.0573933124542236,
      "learning_rate": 7.903771131339402e-06,
      "loss": 0.8135,
      "step": 8600
    },
    {
      "epoch": 3.700118190609219,
      "grad_norm": 2.564368724822998,
      "learning_rate": 7.8777633289987e-06,
      "loss": 0.8181,
      "step": 8610
    },
    {
      "epoch": 3.7044160309444503,
      "grad_norm": 2.669804096221924,
      "learning_rate": 7.851755526657997e-06,
      "loss": 0.8018,
      "step": 8620
    },
    {
      "epoch": 3.708713871279682,
      "grad_norm": 2.4499809741973877,
      "learning_rate": 7.825747724317295e-06,
      "loss": 0.8191,
      "step": 8630
    },
    {
      "epoch": 3.7130117116149135,
      "grad_norm": 2.6498537063598633,
      "learning_rate": 7.799739921976594e-06,
      "loss": 0.8107,
      "step": 8640
    },
    {
      "epoch": 3.717309551950145,
      "grad_norm": 2.0455172061920166,
      "learning_rate": 7.773732119635891e-06,
      "loss": 0.8057,
      "step": 8650
    },
    {
      "epoch": 3.7216073922853767,
      "grad_norm": 2.734623908996582,
      "learning_rate": 7.747724317295189e-06,
      "loss": 0.8534,
      "step": 8660
    },
    {
      "epoch": 3.725905232620608,
      "grad_norm": 2.4609735012054443,
      "learning_rate": 7.721716514954486e-06,
      "loss": 0.814,
      "step": 8670
    },
    {
      "epoch": 3.7302030729558395,
      "grad_norm": 2.6835758686065674,
      "learning_rate": 7.695708712613784e-06,
      "loss": 0.8508,
      "step": 8680
    },
    {
      "epoch": 3.7345009132910714,
      "grad_norm": 2.176541328430176,
      "learning_rate": 7.669700910273081e-06,
      "loss": 0.817,
      "step": 8690
    },
    {
      "epoch": 3.7387987536263028,
      "grad_norm": 2.613600730895996,
      "learning_rate": 7.64629388816645e-06,
      "loss": 0.812,
      "step": 8700
    },
    {
      "epoch": 3.7430965939615346,
      "grad_norm": 2.812455892562866,
      "learning_rate": 7.620286085825748e-06,
      "loss": 0.795,
      "step": 8710
    },
    {
      "epoch": 3.747394434296766,
      "grad_norm": 3.174487829208374,
      "learning_rate": 7.594278283485045e-06,
      "loss": 0.8328,
      "step": 8720
    },
    {
      "epoch": 3.7516922746319974,
      "grad_norm": 2.543504476547241,
      "learning_rate": 7.568270481144343e-06,
      "loss": 0.8102,
      "step": 8730
    },
    {
      "epoch": 3.7559901149672292,
      "grad_norm": 2.139735698699951,
      "learning_rate": 7.542262678803641e-06,
      "loss": 0.7847,
      "step": 8740
    },
    {
      "epoch": 3.7602879553024606,
      "grad_norm": 2.952927589416504,
      "learning_rate": 7.516254876462939e-06,
      "loss": 0.778,
      "step": 8750
    },
    {
      "epoch": 3.764585795637692,
      "grad_norm": 2.310375213623047,
      "learning_rate": 7.490247074122237e-06,
      "loss": 0.8265,
      "step": 8760
    },
    {
      "epoch": 3.768883635972924,
      "grad_norm": 2.7693140506744385,
      "learning_rate": 7.464239271781535e-06,
      "loss": 0.8116,
      "step": 8770
    },
    {
      "epoch": 3.7731814763081553,
      "grad_norm": 2.0008654594421387,
      "learning_rate": 7.438231469440832e-06,
      "loss": 0.7977,
      "step": 8780
    },
    {
      "epoch": 3.7774793166433867,
      "grad_norm": 2.6171176433563232,
      "learning_rate": 7.41222366710013e-06,
      "loss": 0.8046,
      "step": 8790
    },
    {
      "epoch": 3.7817771569786185,
      "grad_norm": 2.6477599143981934,
      "learning_rate": 7.386215864759427e-06,
      "loss": 0.8222,
      "step": 8800
    },
    {
      "epoch": 3.78607499731385,
      "grad_norm": 1.9922062158584595,
      "learning_rate": 7.360208062418726e-06,
      "loss": 0.8213,
      "step": 8810
    },
    {
      "epoch": 3.7903728376490813,
      "grad_norm": 2.8664629459381104,
      "learning_rate": 7.334200260078024e-06,
      "loss": 0.8411,
      "step": 8820
    },
    {
      "epoch": 3.794670677984313,
      "grad_norm": 2.2665882110595703,
      "learning_rate": 7.308192457737322e-06,
      "loss": 0.8646,
      "step": 8830
    },
    {
      "epoch": 3.7989685183195445,
      "grad_norm": 3.1115024089813232,
      "learning_rate": 7.282184655396619e-06,
      "loss": 0.8048,
      "step": 8840
    },
    {
      "epoch": 3.803266358654776,
      "grad_norm": 2.0399694442749023,
      "learning_rate": 7.256176853055917e-06,
      "loss": 0.8617,
      "step": 8850
    },
    {
      "epoch": 3.8075641989900078,
      "grad_norm": 2.7581558227539062,
      "learning_rate": 7.230169050715214e-06,
      "loss": 0.8235,
      "step": 8860
    },
    {
      "epoch": 3.811862039325239,
      "grad_norm": 2.172062873840332,
      "learning_rate": 7.204161248374513e-06,
      "loss": 0.8073,
      "step": 8870
    },
    {
      "epoch": 3.8161598796604705,
      "grad_norm": 2.477853536605835,
      "learning_rate": 7.17815344603381e-06,
      "loss": 0.8258,
      "step": 8880
    },
    {
      "epoch": 3.8204577199957024,
      "grad_norm": 2.789459705352783,
      "learning_rate": 7.152145643693109e-06,
      "loss": 0.7982,
      "step": 8890
    },
    {
      "epoch": 3.824755560330934,
      "grad_norm": 2.1302778720855713,
      "learning_rate": 7.126137841352406e-06,
      "loss": 0.847,
      "step": 8900
    },
    {
      "epoch": 3.829053400666165,
      "grad_norm": 3.1122827529907227,
      "learning_rate": 7.100130039011704e-06,
      "loss": 0.8143,
      "step": 8910
    },
    {
      "epoch": 3.833351241001397,
      "grad_norm": 2.7121505737304688,
      "learning_rate": 7.074122236671001e-06,
      "loss": 0.8489,
      "step": 8920
    },
    {
      "epoch": 3.8376490813366284,
      "grad_norm": 2.925644636154175,
      "learning_rate": 7.048114434330299e-06,
      "loss": 0.8104,
      "step": 8930
    },
    {
      "epoch": 3.84194692167186,
      "grad_norm": 2.4807000160217285,
      "learning_rate": 7.022106631989597e-06,
      "loss": 0.8,
      "step": 8940
    },
    {
      "epoch": 3.8462447620070916,
      "grad_norm": 2.1339035034179688,
      "learning_rate": 6.996098829648895e-06,
      "loss": 0.8104,
      "step": 8950
    },
    {
      "epoch": 3.850542602342323,
      "grad_norm": 2.0116689205169678,
      "learning_rate": 6.970091027308193e-06,
      "loss": 0.8157,
      "step": 8960
    },
    {
      "epoch": 3.8548404426775544,
      "grad_norm": 3.8566207885742188,
      "learning_rate": 6.944083224967491e-06,
      "loss": 0.8823,
      "step": 8970
    },
    {
      "epoch": 3.8591382830127863,
      "grad_norm": 1.92936110496521,
      "learning_rate": 6.918075422626788e-06,
      "loss": 0.795,
      "step": 8980
    },
    {
      "epoch": 3.8634361233480177,
      "grad_norm": 2.0693180561065674,
      "learning_rate": 6.892067620286086e-06,
      "loss": 0.8121,
      "step": 8990
    },
    {
      "epoch": 3.867733963683249,
      "grad_norm": 2.308194160461426,
      "learning_rate": 6.866059817945384e-06,
      "loss": 0.8116,
      "step": 9000
    },
    {
      "epoch": 3.872031804018481,
      "grad_norm": 2.389037847518921,
      "learning_rate": 6.840052015604682e-06,
      "loss": 0.8117,
      "step": 9010
    },
    {
      "epoch": 3.8763296443537123,
      "grad_norm": 2.0813350677490234,
      "learning_rate": 6.814044213263979e-06,
      "loss": 0.8069,
      "step": 9020
    },
    {
      "epoch": 3.8806274846889437,
      "grad_norm": 2.3519508838653564,
      "learning_rate": 6.788036410923277e-06,
      "loss": 0.8145,
      "step": 9030
    },
    {
      "epoch": 3.8849253250241755,
      "grad_norm": 2.1154773235321045,
      "learning_rate": 6.762028608582575e-06,
      "loss": 0.7877,
      "step": 9040
    },
    {
      "epoch": 3.889223165359407,
      "grad_norm": 2.334584951400757,
      "learning_rate": 6.736020806241873e-06,
      "loss": 0.8165,
      "step": 9050
    },
    {
      "epoch": 3.8935210056946383,
      "grad_norm": 2.930453300476074,
      "learning_rate": 6.710013003901171e-06,
      "loss": 0.8677,
      "step": 9060
    },
    {
      "epoch": 3.89781884602987,
      "grad_norm": 3.9080984592437744,
      "learning_rate": 6.6840052015604686e-06,
      "loss": 0.8386,
      "step": 9070
    },
    {
      "epoch": 3.9021166863651016,
      "grad_norm": 3.0754964351654053,
      "learning_rate": 6.657997399219766e-06,
      "loss": 0.8268,
      "step": 9080
    },
    {
      "epoch": 3.906414526700333,
      "grad_norm": 2.5651051998138428,
      "learning_rate": 6.631989596879064e-06,
      "loss": 0.7906,
      "step": 9090
    },
    {
      "epoch": 3.910712367035565,
      "grad_norm": 3.444390058517456,
      "learning_rate": 6.605981794538361e-06,
      "loss": 0.8473,
      "step": 9100
    },
    {
      "epoch": 3.915010207370796,
      "grad_norm": 2.5650522708892822,
      "learning_rate": 6.57997399219766e-06,
      "loss": 0.8714,
      "step": 9110
    },
    {
      "epoch": 3.9193080477060276,
      "grad_norm": 2.591456174850464,
      "learning_rate": 6.553966189856957e-06,
      "loss": 0.846,
      "step": 9120
    },
    {
      "epoch": 3.9236058880412594,
      "grad_norm": 2.011995792388916,
      "learning_rate": 6.5279583875162555e-06,
      "loss": 0.813,
      "step": 9130
    },
    {
      "epoch": 3.927903728376491,
      "grad_norm": 2.0124306678771973,
      "learning_rate": 6.501950585175553e-06,
      "loss": 0.7673,
      "step": 9140
    },
    {
      "epoch": 3.932201568711722,
      "grad_norm": 2.1354434490203857,
      "learning_rate": 6.475942782834851e-06,
      "loss": 0.8338,
      "step": 9150
    },
    {
      "epoch": 3.936499409046954,
      "grad_norm": 2.6531057357788086,
      "learning_rate": 6.449934980494148e-06,
      "loss": 0.8372,
      "step": 9160
    },
    {
      "epoch": 3.9407972493821855,
      "grad_norm": 2.2042551040649414,
      "learning_rate": 6.423927178153446e-06,
      "loss": 0.8318,
      "step": 9170
    },
    {
      "epoch": 3.945095089717417,
      "grad_norm": 2.80143404006958,
      "learning_rate": 6.397919375812744e-06,
      "loss": 0.8503,
      "step": 9180
    },
    {
      "epoch": 3.9493929300526487,
      "grad_norm": 2.0806972980499268,
      "learning_rate": 6.3719115734720424e-06,
      "loss": 0.8402,
      "step": 9190
    },
    {
      "epoch": 3.95369077038788,
      "grad_norm": 2.1934611797332764,
      "learning_rate": 6.34590377113134e-06,
      "loss": 0.8161,
      "step": 9200
    },
    {
      "epoch": 3.9579886107231115,
      "grad_norm": 2.2027251720428467,
      "learning_rate": 6.3198959687906375e-06,
      "loss": 0.8059,
      "step": 9210
    },
    {
      "epoch": 3.9622864510583433,
      "grad_norm": 2.1583023071289062,
      "learning_rate": 6.293888166449935e-06,
      "loss": 0.8055,
      "step": 9220
    },
    {
      "epoch": 3.9665842913935747,
      "grad_norm": 2.734961986541748,
      "learning_rate": 6.267880364109233e-06,
      "loss": 0.8692,
      "step": 9230
    },
    {
      "epoch": 3.970882131728806,
      "grad_norm": 3.6727705001831055,
      "learning_rate": 6.24187256176853e-06,
      "loss": 0.814,
      "step": 9240
    },
    {
      "epoch": 3.975179972064038,
      "grad_norm": 2.4413564205169678,
      "learning_rate": 6.2158647594278285e-06,
      "loss": 0.8424,
      "step": 9250
    },
    {
      "epoch": 3.9794778123992693,
      "grad_norm": 2.406749725341797,
      "learning_rate": 6.189856957087127e-06,
      "loss": 0.8282,
      "step": 9260
    },
    {
      "epoch": 3.9837756527345007,
      "grad_norm": 2.1930606365203857,
      "learning_rate": 6.1638491547464245e-06,
      "loss": 0.793,
      "step": 9270
    },
    {
      "epoch": 3.9880734930697326,
      "grad_norm": 1.8547018766403198,
      "learning_rate": 6.137841352405722e-06,
      "loss": 0.7926,
      "step": 9280
    },
    {
      "epoch": 3.992371333404964,
      "grad_norm": 2.873354911804199,
      "learning_rate": 6.1118335500650195e-06,
      "loss": 0.8334,
      "step": 9290
    },
    {
      "epoch": 3.9966691737401954,
      "grad_norm": 2.3757948875427246,
      "learning_rate": 6.085825747724317e-06,
      "loss": 0.8332,
      "step": 9300
    },
    {
      "epoch": 4.000859568067046,
      "grad_norm": 2.222240447998047,
      "learning_rate": 6.059817945383615e-06,
      "loss": 0.8304,
      "step": 9310
    },
    {
      "epoch": 4.005157408402278,
      "grad_norm": 2.3563897609710693,
      "learning_rate": 6.033810143042913e-06,
      "loss": 0.8084,
      "step": 9320
    },
    {
      "epoch": 4.00945524873751,
      "grad_norm": 3.411773681640625,
      "learning_rate": 6.007802340702211e-06,
      "loss": 0.829,
      "step": 9330
    },
    {
      "epoch": 4.013753089072741,
      "grad_norm": 2.458143711090088,
      "learning_rate": 5.981794538361509e-06,
      "loss": 0.8216,
      "step": 9340
    },
    {
      "epoch": 4.0180509294079725,
      "grad_norm": 2.1002509593963623,
      "learning_rate": 5.9557867360208065e-06,
      "loss": 0.7969,
      "step": 9350
    },
    {
      "epoch": 4.022348769743204,
      "grad_norm": 2.8817431926727295,
      "learning_rate": 5.929778933680104e-06,
      "loss": 0.8176,
      "step": 9360
    },
    {
      "epoch": 4.026646610078435,
      "grad_norm": 2.861746311187744,
      "learning_rate": 5.9037711313394016e-06,
      "loss": 0.8282,
      "step": 9370
    },
    {
      "epoch": 4.030944450413667,
      "grad_norm": 2.134807586669922,
      "learning_rate": 5.8777633289987e-06,
      "loss": 0.8251,
      "step": 9380
    },
    {
      "epoch": 4.035242290748899,
      "grad_norm": 2.7956113815307617,
      "learning_rate": 5.8517555266579975e-06,
      "loss": 0.8197,
      "step": 9390
    },
    {
      "epoch": 4.03954013108413,
      "grad_norm": 2.5326361656188965,
      "learning_rate": 5.825747724317296e-06,
      "loss": 0.854,
      "step": 9400
    },
    {
      "epoch": 4.043837971419362,
      "grad_norm": 1.9541103839874268,
      "learning_rate": 5.799739921976593e-06,
      "loss": 0.8363,
      "step": 9410
    },
    {
      "epoch": 4.048135811754594,
      "grad_norm": 3.6883950233459473,
      "learning_rate": 5.773732119635891e-06,
      "loss": 0.8703,
      "step": 9420
    },
    {
      "epoch": 4.0524336520898245,
      "grad_norm": 2.3559811115264893,
      "learning_rate": 5.7477243172951885e-06,
      "loss": 0.8212,
      "step": 9430
    },
    {
      "epoch": 4.056731492425056,
      "grad_norm": 2.1226353645324707,
      "learning_rate": 5.721716514954486e-06,
      "loss": 0.8142,
      "step": 9440
    },
    {
      "epoch": 4.061029332760288,
      "grad_norm": 2.156466484069824,
      "learning_rate": 5.6957087126137844e-06,
      "loss": 0.7751,
      "step": 9450
    },
    {
      "epoch": 4.065327173095519,
      "grad_norm": 3.487733840942383,
      "learning_rate": 5.669700910273082e-06,
      "loss": 0.8523,
      "step": 9460
    },
    {
      "epoch": 4.069625013430751,
      "grad_norm": 1.7903190851211548,
      "learning_rate": 5.6436931079323795e-06,
      "loss": 0.8104,
      "step": 9470
    },
    {
      "epoch": 4.073922853765983,
      "grad_norm": 2.092482089996338,
      "learning_rate": 5.617685305591678e-06,
      "loss": 0.8091,
      "step": 9480
    },
    {
      "epoch": 4.078220694101214,
      "grad_norm": 2.525850534439087,
      "learning_rate": 5.5916775032509754e-06,
      "loss": 0.8122,
      "step": 9490
    },
    {
      "epoch": 4.082518534436446,
      "grad_norm": 2.3114068508148193,
      "learning_rate": 5.565669700910273e-06,
      "loss": 0.831,
      "step": 9500
    },
    {
      "epoch": 4.0868163747716775,
      "grad_norm": 2.5403075218200684,
      "learning_rate": 5.539661898569571e-06,
      "loss": 0.8461,
      "step": 9510
    },
    {
      "epoch": 4.091114215106908,
      "grad_norm": 2.627546787261963,
      "learning_rate": 5.513654096228869e-06,
      "loss": 0.825,
      "step": 9520
    },
    {
      "epoch": 4.09541205544214,
      "grad_norm": 2.3735604286193848,
      "learning_rate": 5.4876462938881664e-06,
      "loss": 0.871,
      "step": 9530
    },
    {
      "epoch": 4.099709895777372,
      "grad_norm": 2.2827980518341064,
      "learning_rate": 5.461638491547464e-06,
      "loss": 0.7638,
      "step": 9540
    },
    {
      "epoch": 4.104007736112603,
      "grad_norm": 3.035146713256836,
      "learning_rate": 5.435630689206762e-06,
      "loss": 0.815,
      "step": 9550
    },
    {
      "epoch": 4.108305576447835,
      "grad_norm": 2.6076133251190186,
      "learning_rate": 5.40962288686606e-06,
      "loss": 0.8158,
      "step": 9560
    },
    {
      "epoch": 4.112603416783067,
      "grad_norm": 2.283003568649292,
      "learning_rate": 5.383615084525358e-06,
      "loss": 0.8579,
      "step": 9570
    },
    {
      "epoch": 4.116901257118298,
      "grad_norm": 2.6728575229644775,
      "learning_rate": 5.357607282184656e-06,
      "loss": 0.8185,
      "step": 9580
    },
    {
      "epoch": 4.1211990974535295,
      "grad_norm": 1.9724738597869873,
      "learning_rate": 5.331599479843953e-06,
      "loss": 0.8016,
      "step": 9590
    },
    {
      "epoch": 4.125496937788761,
      "grad_norm": 4.058923721313477,
      "learning_rate": 5.305591677503251e-06,
      "loss": 0.8533,
      "step": 9600
    },
    {
      "epoch": 4.129794778123992,
      "grad_norm": 2.5088558197021484,
      "learning_rate": 5.2795838751625485e-06,
      "loss": 0.8201,
      "step": 9610
    },
    {
      "epoch": 4.134092618459224,
      "grad_norm": 2.025597333908081,
      "learning_rate": 5.253576072821847e-06,
      "loss": 0.8401,
      "step": 9620
    },
    {
      "epoch": 4.138390458794456,
      "grad_norm": 2.7202396392822266,
      "learning_rate": 5.227568270481144e-06,
      "loss": 0.8393,
      "step": 9630
    },
    {
      "epoch": 4.142688299129687,
      "grad_norm": 2.47406268119812,
      "learning_rate": 5.201560468140443e-06,
      "loss": 0.8227,
      "step": 9640
    },
    {
      "epoch": 4.146986139464919,
      "grad_norm": 2.462385416030884,
      "learning_rate": 5.17555266579974e-06,
      "loss": 0.7924,
      "step": 9650
    },
    {
      "epoch": 4.151283979800151,
      "grad_norm": 2.769827127456665,
      "learning_rate": 5.149544863459038e-06,
      "loss": 0.7976,
      "step": 9660
    },
    {
      "epoch": 4.155581820135382,
      "grad_norm": 2.2710795402526855,
      "learning_rate": 5.123537061118335e-06,
      "loss": 0.8026,
      "step": 9670
    },
    {
      "epoch": 4.159879660470613,
      "grad_norm": 2.1753058433532715,
      "learning_rate": 5.097529258777633e-06,
      "loss": 0.7836,
      "step": 9680
    },
    {
      "epoch": 4.164177500805845,
      "grad_norm": 3.045571804046631,
      "learning_rate": 5.0715214564369305e-06,
      "loss": 0.8245,
      "step": 9690
    },
    {
      "epoch": 4.168475341141076,
      "grad_norm": 2.249246835708618,
      "learning_rate": 5.04551365409623e-06,
      "loss": 0.8168,
      "step": 9700
    },
    {
      "epoch": 4.172773181476308,
      "grad_norm": 2.9510693550109863,
      "learning_rate": 5.019505851755527e-06,
      "loss": 0.8223,
      "step": 9710
    },
    {
      "epoch": 4.17707102181154,
      "grad_norm": 2.0890510082244873,
      "learning_rate": 4.993498049414825e-06,
      "loss": 0.8288,
      "step": 9720
    },
    {
      "epoch": 4.181368862146771,
      "grad_norm": 4.302569389343262,
      "learning_rate": 4.967490247074122e-06,
      "loss": 0.8184,
      "step": 9730
    },
    {
      "epoch": 4.185666702482003,
      "grad_norm": 2.120039463043213,
      "learning_rate": 4.94148244473342e-06,
      "loss": 0.7948,
      "step": 9740
    },
    {
      "epoch": 4.1899645428172345,
      "grad_norm": 2.1362144947052,
      "learning_rate": 4.915474642392717e-06,
      "loss": 0.8099,
      "step": 9750
    },
    {
      "epoch": 4.1942623831524655,
      "grad_norm": 2.2672994136810303,
      "learning_rate": 4.889466840052016e-06,
      "loss": 0.8148,
      "step": 9760
    },
    {
      "epoch": 4.198560223487697,
      "grad_norm": 3.5295913219451904,
      "learning_rate": 4.863459037711314e-06,
      "loss": 0.8406,
      "step": 9770
    },
    {
      "epoch": 4.202858063822929,
      "grad_norm": 2.41304349899292,
      "learning_rate": 4.837451235370612e-06,
      "loss": 0.8194,
      "step": 9780
    },
    {
      "epoch": 4.20715590415816,
      "grad_norm": 2.2897891998291016,
      "learning_rate": 4.811443433029909e-06,
      "loss": 0.8523,
      "step": 9790
    },
    {
      "epoch": 4.211453744493392,
      "grad_norm": 2.1467227935791016,
      "learning_rate": 4.785435630689207e-06,
      "loss": 0.7879,
      "step": 9800
    },
    {
      "epoch": 4.215751584828624,
      "grad_norm": 2.5855917930603027,
      "learning_rate": 4.759427828348504e-06,
      "loss": 0.8566,
      "step": 9810
    },
    {
      "epoch": 4.220049425163856,
      "grad_norm": 3.687962055206299,
      "learning_rate": 4.733420026007802e-06,
      "loss": 0.8091,
      "step": 9820
    },
    {
      "epoch": 4.224347265499087,
      "grad_norm": 4.630329608917236,
      "learning_rate": 4.7074122236671e-06,
      "loss": 0.8213,
      "step": 9830
    },
    {
      "epoch": 4.228645105834318,
      "grad_norm": 2.447089433670044,
      "learning_rate": 4.681404421326399e-06,
      "loss": 0.8015,
      "step": 9840
    },
    {
      "epoch": 4.232942946169549,
      "grad_norm": 2.080747365951538,
      "learning_rate": 4.655396618985696e-06,
      "loss": 0.8021,
      "step": 9850
    },
    {
      "epoch": 4.237240786504781,
      "grad_norm": 2.2503316402435303,
      "learning_rate": 4.629388816644994e-06,
      "loss": 0.7993,
      "step": 9860
    },
    {
      "epoch": 4.241538626840013,
      "grad_norm": 3.1901581287384033,
      "learning_rate": 4.603381014304291e-06,
      "loss": 0.8264,
      "step": 9870
    },
    {
      "epoch": 4.245836467175245,
      "grad_norm": 2.6685397624969482,
      "learning_rate": 4.577373211963589e-06,
      "loss": 0.7925,
      "step": 9880
    },
    {
      "epoch": 4.250134307510476,
      "grad_norm": 3.605973243713379,
      "learning_rate": 4.551365409622887e-06,
      "loss": 0.8116,
      "step": 9890
    },
    {
      "epoch": 4.254432147845708,
      "grad_norm": 2.5209689140319824,
      "learning_rate": 4.525357607282185e-06,
      "loss": 0.7967,
      "step": 9900
    },
    {
      "epoch": 4.258729988180939,
      "grad_norm": 2.4053897857666016,
      "learning_rate": 4.499349804941482e-06,
      "loss": 0.8175,
      "step": 9910
    },
    {
      "epoch": 4.2630278285161705,
      "grad_norm": 2.0998318195343018,
      "learning_rate": 4.473342002600781e-06,
      "loss": 0.7925,
      "step": 9920
    },
    {
      "epoch": 4.267325668851402,
      "grad_norm": 2.4148175716400146,
      "learning_rate": 4.447334200260078e-06,
      "loss": 0.7997,
      "step": 9930
    },
    {
      "epoch": 4.271623509186634,
      "grad_norm": 2.8764796257019043,
      "learning_rate": 4.421326397919376e-06,
      "loss": 0.8066,
      "step": 9940
    },
    {
      "epoch": 4.275921349521865,
      "grad_norm": 2.8387837409973145,
      "learning_rate": 4.395318595578673e-06,
      "loss": 0.776,
      "step": 9950
    },
    {
      "epoch": 4.280219189857097,
      "grad_norm": 2.9546329975128174,
      "learning_rate": 4.369310793237972e-06,
      "loss": 0.8323,
      "step": 9960
    },
    {
      "epoch": 4.284517030192329,
      "grad_norm": 2.3877484798431396,
      "learning_rate": 4.343302990897269e-06,
      "loss": 0.8134,
      "step": 9970
    },
    {
      "epoch": 4.28881487052756,
      "grad_norm": 2.0377254486083984,
      "learning_rate": 4.317295188556567e-06,
      "loss": 0.8104,
      "step": 9980
    },
    {
      "epoch": 4.293112710862792,
      "grad_norm": 2.1110565662384033,
      "learning_rate": 4.291287386215865e-06,
      "loss": 0.7936,
      "step": 9990
    },
    {
      "epoch": 4.297410551198023,
      "grad_norm": 2.715019464492798,
      "learning_rate": 4.265279583875163e-06,
      "loss": 0.8042,
      "step": 10000
    },
    {
      "epoch": 4.301708391533254,
      "grad_norm": 2.365293264389038,
      "learning_rate": 4.23927178153446e-06,
      "loss": 0.8138,
      "step": 10010
    },
    {
      "epoch": 4.306006231868486,
      "grad_norm": 2.515570640563965,
      "learning_rate": 4.213263979193759e-06,
      "loss": 0.8631,
      "step": 10020
    },
    {
      "epoch": 4.310304072203718,
      "grad_norm": 3.3437633514404297,
      "learning_rate": 4.187256176853056e-06,
      "loss": 0.8332,
      "step": 10030
    },
    {
      "epoch": 4.314601912538949,
      "grad_norm": 2.9369139671325684,
      "learning_rate": 4.161248374512354e-06,
      "loss": 0.8397,
      "step": 10040
    },
    {
      "epoch": 4.318899752874181,
      "grad_norm": 2.048322916030884,
      "learning_rate": 4.135240572171651e-06,
      "loss": 0.8037,
      "step": 10050
    },
    {
      "epoch": 4.323197593209413,
      "grad_norm": 2.2271320819854736,
      "learning_rate": 4.109232769830949e-06,
      "loss": 0.8019,
      "step": 10060
    },
    {
      "epoch": 4.327495433544644,
      "grad_norm": 3.769835948944092,
      "learning_rate": 4.083224967490247e-06,
      "loss": 0.8392,
      "step": 10070
    },
    {
      "epoch": 4.3317932738798754,
      "grad_norm": 2.3251876831054688,
      "learning_rate": 4.0572171651495456e-06,
      "loss": 0.79,
      "step": 10080
    },
    {
      "epoch": 4.336091114215107,
      "grad_norm": 2.4082841873168945,
      "learning_rate": 4.031209362808843e-06,
      "loss": 0.7992,
      "step": 10090
    },
    {
      "epoch": 4.340388954550338,
      "grad_norm": 2.4334561824798584,
      "learning_rate": 4.005201560468141e-06,
      "loss": 0.792,
      "step": 10100
    },
    {
      "epoch": 4.34468679488557,
      "grad_norm": 2.824934244155884,
      "learning_rate": 3.979193758127438e-06,
      "loss": 0.8359,
      "step": 10110
    },
    {
      "epoch": 4.348984635220802,
      "grad_norm": 2.528287649154663,
      "learning_rate": 3.953185955786736e-06,
      "loss": 0.8161,
      "step": 10120
    },
    {
      "epoch": 4.353282475556033,
      "grad_norm": 3.35382342338562,
      "learning_rate": 3.927178153446033e-06,
      "loss": 0.821,
      "step": 10130
    },
    {
      "epoch": 4.357580315891265,
      "grad_norm": 2.9618771076202393,
      "learning_rate": 3.901170351105332e-06,
      "loss": 0.7932,
      "step": 10140
    },
    {
      "epoch": 4.3618781562264966,
      "grad_norm": 2.2790169715881348,
      "learning_rate": 3.87516254876463e-06,
      "loss": 0.821,
      "step": 10150
    },
    {
      "epoch": 4.3661759965617275,
      "grad_norm": 1.9417461156845093,
      "learning_rate": 3.849154746423928e-06,
      "loss": 0.7987,
      "step": 10160
    },
    {
      "epoch": 4.370473836896959,
      "grad_norm": 3.4517974853515625,
      "learning_rate": 3.823146944083225e-06,
      "loss": 0.8356,
      "step": 10170
    },
    {
      "epoch": 4.374771677232191,
      "grad_norm": 2.043816089630127,
      "learning_rate": 3.7971391417425227e-06,
      "loss": 0.8021,
      "step": 10180
    },
    {
      "epoch": 4.379069517567422,
      "grad_norm": 2.21439266204834,
      "learning_rate": 3.7711313394018206e-06,
      "loss": 0.7965,
      "step": 10190
    },
    {
      "epoch": 4.383367357902654,
      "grad_norm": 3.1742184162139893,
      "learning_rate": 3.7451235370611186e-06,
      "loss": 0.7781,
      "step": 10200
    },
    {
      "epoch": 4.387665198237886,
      "grad_norm": 2.4832894802093506,
      "learning_rate": 3.719115734720416e-06,
      "loss": 0.8024,
      "step": 10210
    },
    {
      "epoch": 4.391963038573117,
      "grad_norm": 2.2827794551849365,
      "learning_rate": 3.6931079323797137e-06,
      "loss": 0.8448,
      "step": 10220
    },
    {
      "epoch": 4.396260878908349,
      "grad_norm": 2.706958055496216,
      "learning_rate": 3.667100130039012e-06,
      "loss": 0.8198,
      "step": 10230
    },
    {
      "epoch": 4.40055871924358,
      "grad_norm": 2.158862352371216,
      "learning_rate": 3.6410923276983096e-06,
      "loss": 0.7947,
      "step": 10240
    },
    {
      "epoch": 4.404856559578811,
      "grad_norm": 2.7275569438934326,
      "learning_rate": 3.615084525357607e-06,
      "loss": 0.8201,
      "step": 10250
    },
    {
      "epoch": 4.409154399914043,
      "grad_norm": 2.3184707164764404,
      "learning_rate": 3.589076723016905e-06,
      "loss": 0.8217,
      "step": 10260
    },
    {
      "epoch": 4.413452240249275,
      "grad_norm": 2.7770907878875732,
      "learning_rate": 3.563068920676203e-06,
      "loss": 0.8047,
      "step": 10270
    },
    {
      "epoch": 4.417750080584506,
      "grad_norm": 1.801823616027832,
      "learning_rate": 3.5370611183355006e-06,
      "loss": 0.8106,
      "step": 10280
    },
    {
      "epoch": 4.422047920919738,
      "grad_norm": 3.5761191844940186,
      "learning_rate": 3.5110533159947986e-06,
      "loss": 0.821,
      "step": 10290
    },
    {
      "epoch": 4.42634576125497,
      "grad_norm": 2.530055046081543,
      "learning_rate": 3.4850455136540965e-06,
      "loss": 0.7939,
      "step": 10300
    },
    {
      "epoch": 4.430643601590201,
      "grad_norm": 3.4115943908691406,
      "learning_rate": 3.459037711313394e-06,
      "loss": 0.8046,
      "step": 10310
    },
    {
      "epoch": 4.4349414419254325,
      "grad_norm": 2.4997403621673584,
      "learning_rate": 3.433029908972692e-06,
      "loss": 0.8441,
      "step": 10320
    },
    {
      "epoch": 4.439239282260664,
      "grad_norm": 2.8241126537323,
      "learning_rate": 3.4070221066319896e-06,
      "loss": 0.8359,
      "step": 10330
    },
    {
      "epoch": 4.443537122595895,
      "grad_norm": 2.59536075592041,
      "learning_rate": 3.3810143042912876e-06,
      "loss": 0.7869,
      "step": 10340
    },
    {
      "epoch": 4.447834962931127,
      "grad_norm": 2.5132572650909424,
      "learning_rate": 3.3550065019505855e-06,
      "loss": 0.8186,
      "step": 10350
    },
    {
      "epoch": 4.452132803266359,
      "grad_norm": 3.5301671028137207,
      "learning_rate": 3.328998699609883e-06,
      "loss": 0.8348,
      "step": 10360
    },
    {
      "epoch": 4.45643064360159,
      "grad_norm": 2.155583143234253,
      "learning_rate": 3.3029908972691806e-06,
      "loss": 0.8064,
      "step": 10370
    },
    {
      "epoch": 4.460728483936822,
      "grad_norm": 2.7505040168762207,
      "learning_rate": 3.2769830949284786e-06,
      "loss": 0.8321,
      "step": 10380
    },
    {
      "epoch": 4.465026324272054,
      "grad_norm": 2.5441160202026367,
      "learning_rate": 3.2509752925877765e-06,
      "loss": 0.8298,
      "step": 10390
    },
    {
      "epoch": 4.4693241646072845,
      "grad_norm": 2.9121451377868652,
      "learning_rate": 3.224967490247074e-06,
      "loss": 0.8034,
      "step": 10400
    },
    {
      "epoch": 4.473622004942516,
      "grad_norm": 2.4851670265197754,
      "learning_rate": 3.198959687906372e-06,
      "loss": 0.7868,
      "step": 10410
    },
    {
      "epoch": 4.477919845277748,
      "grad_norm": 2.8280279636383057,
      "learning_rate": 3.17295188556567e-06,
      "loss": 0.8182,
      "step": 10420
    },
    {
      "epoch": 4.482217685612979,
      "grad_norm": 2.9958221912384033,
      "learning_rate": 3.1469440832249675e-06,
      "loss": 0.8472,
      "step": 10430
    },
    {
      "epoch": 4.486515525948211,
      "grad_norm": 2.5353033542633057,
      "learning_rate": 3.120936280884265e-06,
      "loss": 0.7762,
      "step": 10440
    },
    {
      "epoch": 4.490813366283443,
      "grad_norm": 2.5972323417663574,
      "learning_rate": 3.0949284785435635e-06,
      "loss": 0.8391,
      "step": 10450
    },
    {
      "epoch": 4.495111206618674,
      "grad_norm": 2.4838643074035645,
      "learning_rate": 3.068920676202861e-06,
      "loss": 0.8334,
      "step": 10460
    },
    {
      "epoch": 4.499409046953906,
      "grad_norm": 2.6042685508728027,
      "learning_rate": 3.0429128738621585e-06,
      "loss": 0.8187,
      "step": 10470
    },
    {
      "epoch": 4.5037068872891375,
      "grad_norm": 3.62833309173584,
      "learning_rate": 3.0169050715214565e-06,
      "loss": 0.7889,
      "step": 10480
    },
    {
      "epoch": 4.508004727624368,
      "grad_norm": 2.3429243564605713,
      "learning_rate": 2.9908972691807545e-06,
      "loss": 0.8563,
      "step": 10490
    },
    {
      "epoch": 4.5123025679596,
      "grad_norm": 1.8648444414138794,
      "learning_rate": 2.964889466840052e-06,
      "loss": 0.8174,
      "step": 10500
    },
    {
      "epoch": 4.516600408294832,
      "grad_norm": 2.8812801837921143,
      "learning_rate": 2.93888166449935e-06,
      "loss": 0.8078,
      "step": 10510
    },
    {
      "epoch": 4.520898248630063,
      "grad_norm": 2.27990984916687,
      "learning_rate": 2.912873862158648e-06,
      "loss": 0.79,
      "step": 10520
    },
    {
      "epoch": 4.525196088965295,
      "grad_norm": 3.252237319946289,
      "learning_rate": 2.8868660598179455e-06,
      "loss": 0.8168,
      "step": 10530
    },
    {
      "epoch": 4.529493929300527,
      "grad_norm": 2.3535094261169434,
      "learning_rate": 2.860858257477243e-06,
      "loss": 0.8204,
      "step": 10540
    },
    {
      "epoch": 4.533791769635758,
      "grad_norm": 2.589115858078003,
      "learning_rate": 2.834850455136541e-06,
      "loss": 0.8309,
      "step": 10550
    },
    {
      "epoch": 4.5380896099709895,
      "grad_norm": 2.7474892139434814,
      "learning_rate": 2.808842652795839e-06,
      "loss": 0.8126,
      "step": 10560
    },
    {
      "epoch": 4.542387450306221,
      "grad_norm": 2.393087387084961,
      "learning_rate": 2.7828348504551365e-06,
      "loss": 0.8403,
      "step": 10570
    },
    {
      "epoch": 4.546685290641452,
      "grad_norm": 2.767324209213257,
      "learning_rate": 2.7568270481144345e-06,
      "loss": 0.8461,
      "step": 10580
    },
    {
      "epoch": 4.550983130976684,
      "grad_norm": 2.3588175773620605,
      "learning_rate": 2.730819245773732e-06,
      "loss": 0.8012,
      "step": 10590
    },
    {
      "epoch": 4.555280971311916,
      "grad_norm": 2.202869415283203,
      "learning_rate": 2.70481144343303e-06,
      "loss": 0.8308,
      "step": 10600
    },
    {
      "epoch": 4.559578811647147,
      "grad_norm": 3.2011337280273438,
      "learning_rate": 2.678803641092328e-06,
      "loss": 0.8094,
      "step": 10610
    },
    {
      "epoch": 4.563876651982379,
      "grad_norm": 2.768422842025757,
      "learning_rate": 2.6527958387516255e-06,
      "loss": 0.7994,
      "step": 10620
    },
    {
      "epoch": 4.568174492317611,
      "grad_norm": 2.8596317768096924,
      "learning_rate": 2.6267880364109234e-06,
      "loss": 0.8221,
      "step": 10630
    },
    {
      "epoch": 4.572472332652842,
      "grad_norm": 2.3996002674102783,
      "learning_rate": 2.6007802340702214e-06,
      "loss": 0.7889,
      "step": 10640
    },
    {
      "epoch": 4.576770172988073,
      "grad_norm": 1.9949451684951782,
      "learning_rate": 2.574772431729519e-06,
      "loss": 0.8069,
      "step": 10650
    },
    {
      "epoch": 4.581068013323305,
      "grad_norm": 2.084507703781128,
      "learning_rate": 2.5487646293888165e-06,
      "loss": 0.8107,
      "step": 10660
    },
    {
      "epoch": 4.585365853658536,
      "grad_norm": 2.9825692176818848,
      "learning_rate": 2.522756827048115e-06,
      "loss": 0.7969,
      "step": 10670
    },
    {
      "epoch": 4.589663693993768,
      "grad_norm": 2.9286739826202393,
      "learning_rate": 2.4967490247074124e-06,
      "loss": 0.8102,
      "step": 10680
    },
    {
      "epoch": 4.593961534329,
      "grad_norm": 2.9292266368865967,
      "learning_rate": 2.47074122236671e-06,
      "loss": 0.8346,
      "step": 10690
    },
    {
      "epoch": 4.598259374664231,
      "grad_norm": 2.590252637863159,
      "learning_rate": 2.444733420026008e-06,
      "loss": 0.8057,
      "step": 10700
    },
    {
      "epoch": 4.602557214999463,
      "grad_norm": 2.145315170288086,
      "learning_rate": 2.418725617685306e-06,
      "loss": 0.8051,
      "step": 10710
    },
    {
      "epoch": 4.6068550553346945,
      "grad_norm": 2.3770549297332764,
      "learning_rate": 2.3927178153446034e-06,
      "loss": 0.8202,
      "step": 10720
    },
    {
      "epoch": 4.6111528956699255,
      "grad_norm": 2.475691318511963,
      "learning_rate": 2.366710013003901e-06,
      "loss": 0.826,
      "step": 10730
    },
    {
      "epoch": 4.615450736005157,
      "grad_norm": 3.2441887855529785,
      "learning_rate": 2.3407022106631993e-06,
      "loss": 0.8192,
      "step": 10740
    },
    {
      "epoch": 4.619748576340389,
      "grad_norm": 2.495495080947876,
      "learning_rate": 2.3172951885565673e-06,
      "loss": 0.8276,
      "step": 10750
    },
    {
      "epoch": 4.62404641667562,
      "grad_norm": 3.022981643676758,
      "learning_rate": 2.291287386215865e-06,
      "loss": 0.8534,
      "step": 10760
    },
    {
      "epoch": 4.628344257010852,
      "grad_norm": 2.43552827835083,
      "learning_rate": 2.2652795838751624e-06,
      "loss": 0.8375,
      "step": 10770
    },
    {
      "epoch": 4.632642097346084,
      "grad_norm": 2.6198384761810303,
      "learning_rate": 2.2392717815344608e-06,
      "loss": 0.7927,
      "step": 10780
    },
    {
      "epoch": 4.636939937681316,
      "grad_norm": 2.594743251800537,
      "learning_rate": 2.2132639791937583e-06,
      "loss": 0.8079,
      "step": 10790
    },
    {
      "epoch": 4.641237778016547,
      "grad_norm": 2.383683204650879,
      "learning_rate": 2.187256176853056e-06,
      "loss": 0.8415,
      "step": 10800
    },
    {
      "epoch": 4.645535618351778,
      "grad_norm": 2.050044298171997,
      "learning_rate": 2.161248374512354e-06,
      "loss": 0.793,
      "step": 10810
    },
    {
      "epoch": 4.649833458687009,
      "grad_norm": 2.798903226852417,
      "learning_rate": 2.1352405721716518e-06,
      "loss": 0.8151,
      "step": 10820
    },
    {
      "epoch": 4.654131299022241,
      "grad_norm": 3.0557591915130615,
      "learning_rate": 2.1092327698309493e-06,
      "loss": 0.7976,
      "step": 10830
    },
    {
      "epoch": 4.658429139357473,
      "grad_norm": 2.5829110145568848,
      "learning_rate": 2.083224967490247e-06,
      "loss": 0.8087,
      "step": 10840
    },
    {
      "epoch": 4.662726979692705,
      "grad_norm": 2.418074369430542,
      "learning_rate": 2.057217165149545e-06,
      "loss": 0.7852,
      "step": 10850
    },
    {
      "epoch": 4.667024820027936,
      "grad_norm": 2.695817470550537,
      "learning_rate": 2.0312093628088428e-06,
      "loss": 0.8257,
      "step": 10860
    },
    {
      "epoch": 4.671322660363168,
      "grad_norm": 3.2456295490264893,
      "learning_rate": 2.0052015604681403e-06,
      "loss": 0.7985,
      "step": 10870
    },
    {
      "epoch": 4.675620500698399,
      "grad_norm": 2.8197731971740723,
      "learning_rate": 1.9791937581274383e-06,
      "loss": 0.8277,
      "step": 10880
    },
    {
      "epoch": 4.6799183410336305,
      "grad_norm": 2.4145874977111816,
      "learning_rate": 1.9531859557867362e-06,
      "loss": 0.8275,
      "step": 10890
    },
    {
      "epoch": 4.684216181368862,
      "grad_norm": 2.370753765106201,
      "learning_rate": 1.9271781534460338e-06,
      "loss": 0.8445,
      "step": 10900
    },
    {
      "epoch": 4.688514021704094,
      "grad_norm": 1.9043973684310913,
      "learning_rate": 1.9011703511053315e-06,
      "loss": 0.8052,
      "step": 10910
    },
    {
      "epoch": 4.692811862039325,
      "grad_norm": 2.985630989074707,
      "learning_rate": 1.8751625487646293e-06,
      "loss": 0.8711,
      "step": 10920
    },
    {
      "epoch": 4.697109702374557,
      "grad_norm": 3.161289930343628,
      "learning_rate": 1.8491547464239273e-06,
      "loss": 0.8472,
      "step": 10930
    },
    {
      "epoch": 4.701407542709788,
      "grad_norm": 2.6867127418518066,
      "learning_rate": 1.823146944083225e-06,
      "loss": 0.8404,
      "step": 10940
    },
    {
      "epoch": 4.70570538304502,
      "grad_norm": 2.7265355587005615,
      "learning_rate": 1.7971391417425228e-06,
      "loss": 0.7999,
      "step": 10950
    },
    {
      "epoch": 4.710003223380252,
      "grad_norm": 2.675044298171997,
      "learning_rate": 1.7711313394018205e-06,
      "loss": 0.794,
      "step": 10960
    },
    {
      "epoch": 4.714301063715483,
      "grad_norm": 2.4772238731384277,
      "learning_rate": 1.7451235370611183e-06,
      "loss": 0.7976,
      "step": 10970
    },
    {
      "epoch": 4.718598904050714,
      "grad_norm": 2.26298451423645,
      "learning_rate": 1.7191157347204162e-06,
      "loss": 0.784,
      "step": 10980
    },
    {
      "epoch": 4.722896744385946,
      "grad_norm": 3.236077308654785,
      "learning_rate": 1.693107932379714e-06,
      "loss": 0.8124,
      "step": 10990
    },
    {
      "epoch": 4.727194584721177,
      "grad_norm": 2.3039772510528564,
      "learning_rate": 1.6671001300390117e-06,
      "loss": 0.7948,
      "step": 11000
    },
    {
      "epoch": 4.731492425056409,
      "grad_norm": 2.368014097213745,
      "learning_rate": 1.6410923276983097e-06,
      "loss": 0.8109,
      "step": 11010
    },
    {
      "epoch": 4.735790265391641,
      "grad_norm": 2.1160569190979004,
      "learning_rate": 1.6150845253576072e-06,
      "loss": 0.7833,
      "step": 11020
    },
    {
      "epoch": 4.740088105726873,
      "grad_norm": 2.9305977821350098,
      "learning_rate": 1.5890767230169052e-06,
      "loss": 0.8391,
      "step": 11030
    },
    {
      "epoch": 4.744385946062104,
      "grad_norm": 2.6147043704986572,
      "learning_rate": 1.5630689206762027e-06,
      "loss": 0.8103,
      "step": 11040
    },
    {
      "epoch": 4.7486837863973355,
      "grad_norm": 2.6779232025146484,
      "learning_rate": 1.5370611183355007e-06,
      "loss": 0.8065,
      "step": 11050
    },
    {
      "epoch": 4.752981626732566,
      "grad_norm": 4.267576217651367,
      "learning_rate": 1.5110533159947985e-06,
      "loss": 0.8121,
      "step": 11060
    },
    {
      "epoch": 4.757279467067798,
      "grad_norm": 2.486156702041626,
      "learning_rate": 1.4850455136540962e-06,
      "loss": 0.8028,
      "step": 11070
    },
    {
      "epoch": 4.76157730740303,
      "grad_norm": 2.5177834033966064,
      "learning_rate": 1.459037711313394e-06,
      "loss": 0.8066,
      "step": 11080
    },
    {
      "epoch": 4.765875147738262,
      "grad_norm": 2.340644121170044,
      "learning_rate": 1.433029908972692e-06,
      "loss": 0.8335,
      "step": 11090
    },
    {
      "epoch": 4.770172988073493,
      "grad_norm": 2.6426727771759033,
      "learning_rate": 1.4070221066319897e-06,
      "loss": 0.83,
      "step": 11100
    },
    {
      "epoch": 4.774470828408725,
      "grad_norm": 2.123105049133301,
      "learning_rate": 1.3810143042912874e-06,
      "loss": 0.7857,
      "step": 11110
    },
    {
      "epoch": 4.778768668743956,
      "grad_norm": 2.3244404792785645,
      "learning_rate": 1.3550065019505852e-06,
      "loss": 0.8329,
      "step": 11120
    },
    {
      "epoch": 4.7830665090791875,
      "grad_norm": 3.912513017654419,
      "learning_rate": 1.328998699609883e-06,
      "loss": 0.8489,
      "step": 11130
    },
    {
      "epoch": 4.787364349414419,
      "grad_norm": 3.3236677646636963,
      "learning_rate": 1.302990897269181e-06,
      "loss": 0.8414,
      "step": 11140
    },
    {
      "epoch": 4.791662189749651,
      "grad_norm": 2.4710683822631836,
      "learning_rate": 1.2769830949284784e-06,
      "loss": 0.8269,
      "step": 11150
    },
    {
      "epoch": 4.795960030084882,
      "grad_norm": 2.9499592781066895,
      "learning_rate": 1.2509752925877764e-06,
      "loss": 0.8205,
      "step": 11160
    },
    {
      "epoch": 4.800257870420114,
      "grad_norm": 2.5057084560394287,
      "learning_rate": 1.2249674902470742e-06,
      "loss": 0.8045,
      "step": 11170
    },
    {
      "epoch": 4.804555710755346,
      "grad_norm": 3.883413076400757,
      "learning_rate": 1.198959687906372e-06,
      "loss": 0.79,
      "step": 11180
    },
    {
      "epoch": 4.808853551090577,
      "grad_norm": 2.3628926277160645,
      "learning_rate": 1.1729518855656697e-06,
      "loss": 0.8203,
      "step": 11190
    },
    {
      "epoch": 4.813151391425809,
      "grad_norm": 3.1127870082855225,
      "learning_rate": 1.1469440832249676e-06,
      "loss": 0.8573,
      "step": 11200
    },
    {
      "epoch": 4.8174492317610405,
      "grad_norm": 2.98063063621521,
      "learning_rate": 1.1209362808842654e-06,
      "loss": 0.8094,
      "step": 11210
    },
    {
      "epoch": 4.821747072096271,
      "grad_norm": 2.01924204826355,
      "learning_rate": 1.0949284785435631e-06,
      "loss": 0.7998,
      "step": 11220
    },
    {
      "epoch": 4.826044912431503,
      "grad_norm": 2.4002249240875244,
      "learning_rate": 1.0689206762028609e-06,
      "loss": 0.8033,
      "step": 11230
    },
    {
      "epoch": 4.830342752766735,
      "grad_norm": 2.2603423595428467,
      "learning_rate": 1.0429128738621586e-06,
      "loss": 0.8446,
      "step": 11240
    },
    {
      "epoch": 4.834640593101966,
      "grad_norm": 2.856149196624756,
      "learning_rate": 1.0169050715214566e-06,
      "loss": 0.7954,
      "step": 11250
    },
    {
      "epoch": 4.838938433437198,
      "grad_norm": 2.8752028942108154,
      "learning_rate": 9.908972691807541e-07,
      "loss": 0.8145,
      "step": 11260
    },
    {
      "epoch": 4.84323627377243,
      "grad_norm": 2.2863667011260986,
      "learning_rate": 9.64889466840052e-07,
      "loss": 0.7918,
      "step": 11270
    },
    {
      "epoch": 4.847534114107661,
      "grad_norm": 2.456040620803833,
      "learning_rate": 9.388816644993497e-07,
      "loss": 0.82,
      "step": 11280
    },
    {
      "epoch": 4.8518319544428925,
      "grad_norm": 2.197709798812866,
      "learning_rate": 9.128738621586476e-07,
      "loss": 0.8185,
      "step": 11290
    },
    {
      "epoch": 4.856129794778124,
      "grad_norm": 2.298238515853882,
      "learning_rate": 8.868660598179455e-07,
      "loss": 0.8242,
      "step": 11300
    },
    {
      "epoch": 4.860427635113355,
      "grad_norm": 2.3707809448242188,
      "learning_rate": 8.608582574772432e-07,
      "loss": 0.8151,
      "step": 11310
    },
    {
      "epoch": 4.864725475448587,
      "grad_norm": 2.1746296882629395,
      "learning_rate": 8.34850455136541e-07,
      "loss": 0.8337,
      "step": 11320
    },
    {
      "epoch": 4.869023315783819,
      "grad_norm": 2.5394933223724365,
      "learning_rate": 8.088426527958387e-07,
      "loss": 0.7937,
      "step": 11330
    },
    {
      "epoch": 4.87332115611905,
      "grad_norm": 2.5250463485717773,
      "learning_rate": 7.828348504551366e-07,
      "loss": 0.8182,
      "step": 11340
    },
    {
      "epoch": 4.877618996454282,
      "grad_norm": 2.8107399940490723,
      "learning_rate": 7.568270481144343e-07,
      "loss": 0.8035,
      "step": 11350
    },
    {
      "epoch": 4.881916836789514,
      "grad_norm": 2.605091094970703,
      "learning_rate": 7.308192457737321e-07,
      "loss": 0.7804,
      "step": 11360
    },
    {
      "epoch": 4.8862146771247446,
      "grad_norm": 2.9730045795440674,
      "learning_rate": 7.048114434330298e-07,
      "loss": 0.8277,
      "step": 11370
    },
    {
      "epoch": 4.890512517459976,
      "grad_norm": 2.326838731765747,
      "learning_rate": 6.788036410923278e-07,
      "loss": 0.8295,
      "step": 11380
    },
    {
      "epoch": 4.894810357795208,
      "grad_norm": 2.4002840518951416,
      "learning_rate": 6.527958387516256e-07,
      "loss": 0.8551,
      "step": 11390
    },
    {
      "epoch": 4.899108198130439,
      "grad_norm": 2.7466588020324707,
      "learning_rate": 6.267880364109233e-07,
      "loss": 0.8223,
      "step": 11400
    }
  ],
  "logging_steps": 10,
  "max_steps": 11635,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.7090954613751808e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
