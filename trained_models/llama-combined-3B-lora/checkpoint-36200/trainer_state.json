{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.5602631069839428,
  "eval_steps": 500,
  "global_step": 36200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00015476881408396209,
      "grad_norm": 0.5115175247192383,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 2.763,
      "step": 10
    },
    {
      "epoch": 0.00030953762816792417,
      "grad_norm": 0.5536526441574097,
      "learning_rate": 5.4e-06,
      "loss": 2.8013,
      "step": 20
    },
    {
      "epoch": 0.00046430644225188626,
      "grad_norm": 0.5471962094306946,
      "learning_rate": 8.400000000000001e-06,
      "loss": 2.3339,
      "step": 30
    },
    {
      "epoch": 0.0006190752563358483,
      "grad_norm": 0.6596537828445435,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 2.8247,
      "step": 40
    },
    {
      "epoch": 0.0007738440704198104,
      "grad_norm": 0.5303691029548645,
      "learning_rate": 1.44e-05,
      "loss": 2.9002,
      "step": 50
    },
    {
      "epoch": 0.0009286128845037725,
      "grad_norm": 1.0266377925872803,
      "learning_rate": 1.74e-05,
      "loss": 2.4865,
      "step": 60
    },
    {
      "epoch": 0.0010833816985877345,
      "grad_norm": 0.6757490038871765,
      "learning_rate": 2.04e-05,
      "loss": 2.7079,
      "step": 70
    },
    {
      "epoch": 0.0012381505126716967,
      "grad_norm": 0.7484861016273499,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 2.3888,
      "step": 80
    },
    {
      "epoch": 0.0013929193267556588,
      "grad_norm": 0.9840059280395508,
      "learning_rate": 2.64e-05,
      "loss": 2.0901,
      "step": 90
    },
    {
      "epoch": 0.0015476881408396208,
      "grad_norm": 1.2235761880874634,
      "learning_rate": 2.94e-05,
      "loss": 2.3623,
      "step": 100
    },
    {
      "epoch": 0.001702456954923583,
      "grad_norm": 1.2810251712799072,
      "learning_rate": 2.9998761219991846e-05,
      "loss": 1.9406,
      "step": 110
    },
    {
      "epoch": 0.001857225769007545,
      "grad_norm": 2.263233184814453,
      "learning_rate": 2.9997212744981653e-05,
      "loss": 1.5012,
      "step": 120
    },
    {
      "epoch": 0.002011994583091507,
      "grad_norm": 1.9955108165740967,
      "learning_rate": 2.9995664269971457e-05,
      "loss": 1.6185,
      "step": 130
    },
    {
      "epoch": 0.002166763397175469,
      "grad_norm": 0.9684050679206848,
      "learning_rate": 2.9994115794961264e-05,
      "loss": 1.8538,
      "step": 140
    },
    {
      "epoch": 0.0023215322112594312,
      "grad_norm": 1.3912330865859985,
      "learning_rate": 2.9992567319951068e-05,
      "loss": 1.7158,
      "step": 150
    },
    {
      "epoch": 0.0024763010253433934,
      "grad_norm": 1.1517486572265625,
      "learning_rate": 2.9991018844940875e-05,
      "loss": 1.2625,
      "step": 160
    },
    {
      "epoch": 0.0026310698394273555,
      "grad_norm": 1.0722101926803589,
      "learning_rate": 2.9989470369930682e-05,
      "loss": 1.1868,
      "step": 170
    },
    {
      "epoch": 0.0027858386535113177,
      "grad_norm": 1.8850860595703125,
      "learning_rate": 2.998792189492049e-05,
      "loss": 1.4366,
      "step": 180
    },
    {
      "epoch": 0.0029406074675952794,
      "grad_norm": 2.1348376274108887,
      "learning_rate": 2.9986373419910293e-05,
      "loss": 1.4206,
      "step": 190
    },
    {
      "epoch": 0.0030953762816792415,
      "grad_norm": 2.020841598510742,
      "learning_rate": 2.99848249449001e-05,
      "loss": 1.5882,
      "step": 200
    },
    {
      "epoch": 0.0032501450957632036,
      "grad_norm": 2.2651171684265137,
      "learning_rate": 2.9983276469889904e-05,
      "loss": 1.5724,
      "step": 210
    },
    {
      "epoch": 0.003404913909847166,
      "grad_norm": 1.6053963899612427,
      "learning_rate": 2.9981727994879707e-05,
      "loss": 1.4756,
      "step": 220
    },
    {
      "epoch": 0.003559682723931128,
      "grad_norm": 1.6305818557739258,
      "learning_rate": 2.9980179519869515e-05,
      "loss": 1.4191,
      "step": 230
    },
    {
      "epoch": 0.00371445153801509,
      "grad_norm": 1.585888147354126,
      "learning_rate": 2.997863104485932e-05,
      "loss": 1.3176,
      "step": 240
    },
    {
      "epoch": 0.003869220352099052,
      "grad_norm": 1.1234064102172852,
      "learning_rate": 2.997708256984913e-05,
      "loss": 1.5259,
      "step": 250
    },
    {
      "epoch": 0.004023989166183014,
      "grad_norm": 2.306378126144409,
      "learning_rate": 2.9975534094838933e-05,
      "loss": 1.3748,
      "step": 260
    },
    {
      "epoch": 0.004178757980266976,
      "grad_norm": 2.502915620803833,
      "learning_rate": 2.997398561982874e-05,
      "loss": 1.4657,
      "step": 270
    },
    {
      "epoch": 0.004333526794350938,
      "grad_norm": 1.9757884740829468,
      "learning_rate": 2.9972437144818543e-05,
      "loss": 1.3056,
      "step": 280
    },
    {
      "epoch": 0.0044882956084349,
      "grad_norm": 0.9909770488739014,
      "learning_rate": 2.997088866980835e-05,
      "loss": 1.4267,
      "step": 290
    },
    {
      "epoch": 0.0046430644225188625,
      "grad_norm": 1.6950268745422363,
      "learning_rate": 2.9969340194798154e-05,
      "loss": 1.4022,
      "step": 300
    },
    {
      "epoch": 0.004797833236602825,
      "grad_norm": 1.5109457969665527,
      "learning_rate": 2.9967791719787965e-05,
      "loss": 1.3574,
      "step": 310
    },
    {
      "epoch": 0.004952602050686787,
      "grad_norm": 1.112306833267212,
      "learning_rate": 2.996624324477777e-05,
      "loss": 1.2775,
      "step": 320
    },
    {
      "epoch": 0.005107370864770749,
      "grad_norm": 1.506690502166748,
      "learning_rate": 2.9964694769767576e-05,
      "loss": 1.1533,
      "step": 330
    },
    {
      "epoch": 0.005262139678854711,
      "grad_norm": 2.7271103858947754,
      "learning_rate": 2.996314629475738e-05,
      "loss": 1.3598,
      "step": 340
    },
    {
      "epoch": 0.005416908492938673,
      "grad_norm": 1.4995626211166382,
      "learning_rate": 2.9961597819747187e-05,
      "loss": 1.3104,
      "step": 350
    },
    {
      "epoch": 0.005571677307022635,
      "grad_norm": 1.6171196699142456,
      "learning_rate": 2.996004934473699e-05,
      "loss": 1.3831,
      "step": 360
    },
    {
      "epoch": 0.005726446121106597,
      "grad_norm": 1.584837555885315,
      "learning_rate": 2.99585008697268e-05,
      "loss": 1.3294,
      "step": 370
    },
    {
      "epoch": 0.005881214935190559,
      "grad_norm": 1.1541929244995117,
      "learning_rate": 2.9956952394716605e-05,
      "loss": 1.3247,
      "step": 380
    },
    {
      "epoch": 0.006035983749274521,
      "grad_norm": 1.0468014478683472,
      "learning_rate": 2.9955403919706412e-05,
      "loss": 1.303,
      "step": 390
    },
    {
      "epoch": 0.006190752563358483,
      "grad_norm": 1.1760997772216797,
      "learning_rate": 2.9953855444696216e-05,
      "loss": 1.4775,
      "step": 400
    },
    {
      "epoch": 0.006345521377442445,
      "grad_norm": 1.0867644548416138,
      "learning_rate": 2.9952306969686023e-05,
      "loss": 1.3109,
      "step": 410
    },
    {
      "epoch": 0.006500290191526407,
      "grad_norm": 2.367828845977783,
      "learning_rate": 2.9950758494675826e-05,
      "loss": 1.283,
      "step": 420
    },
    {
      "epoch": 0.0066550590056103694,
      "grad_norm": 3.192671775817871,
      "learning_rate": 2.9949210019665634e-05,
      "loss": 1.4747,
      "step": 430
    },
    {
      "epoch": 0.006809827819694332,
      "grad_norm": 0.8298603892326355,
      "learning_rate": 2.994766154465544e-05,
      "loss": 1.1726,
      "step": 440
    },
    {
      "epoch": 0.006964596633778294,
      "grad_norm": 1.5888168811798096,
      "learning_rate": 2.9946113069645248e-05,
      "loss": 1.1805,
      "step": 450
    },
    {
      "epoch": 0.007119365447862256,
      "grad_norm": 1.1183151006698608,
      "learning_rate": 2.994456459463505e-05,
      "loss": 1.2703,
      "step": 460
    },
    {
      "epoch": 0.007274134261946218,
      "grad_norm": 1.189394235610962,
      "learning_rate": 2.9943016119624855e-05,
      "loss": 1.3902,
      "step": 470
    },
    {
      "epoch": 0.00742890307603018,
      "grad_norm": 2.2616653442382812,
      "learning_rate": 2.9941467644614662e-05,
      "loss": 1.5577,
      "step": 480
    },
    {
      "epoch": 0.007583671890114142,
      "grad_norm": 1.7387137413024902,
      "learning_rate": 2.9939919169604466e-05,
      "loss": 1.3149,
      "step": 490
    },
    {
      "epoch": 0.007738440704198104,
      "grad_norm": 3.225752592086792,
      "learning_rate": 2.9938370694594273e-05,
      "loss": 1.4751,
      "step": 500
    },
    {
      "epoch": 0.007893209518282067,
      "grad_norm": 2.7189037799835205,
      "learning_rate": 2.993682221958408e-05,
      "loss": 1.5027,
      "step": 510
    },
    {
      "epoch": 0.008047978332366028,
      "grad_norm": 1.3952513933181763,
      "learning_rate": 2.9935273744573888e-05,
      "loss": 1.207,
      "step": 520
    },
    {
      "epoch": 0.00820274714644999,
      "grad_norm": 1.7070056200027466,
      "learning_rate": 2.993372526956369e-05,
      "loss": 1.2978,
      "step": 530
    },
    {
      "epoch": 0.008357515960533952,
      "grad_norm": 2.015385866165161,
      "learning_rate": 2.99321767945535e-05,
      "loss": 1.2025,
      "step": 540
    },
    {
      "epoch": 0.008512284774617915,
      "grad_norm": 1.0987461805343628,
      "learning_rate": 2.9930628319543302e-05,
      "loss": 1.1738,
      "step": 550
    },
    {
      "epoch": 0.008667053588701876,
      "grad_norm": 1.8022371530532837,
      "learning_rate": 2.992907984453311e-05,
      "loss": 1.2257,
      "step": 560
    },
    {
      "epoch": 0.00882182240278584,
      "grad_norm": 2.2208056449890137,
      "learning_rate": 2.9927531369522913e-05,
      "loss": 1.4106,
      "step": 570
    },
    {
      "epoch": 0.0089765912168698,
      "grad_norm": 2.09600830078125,
      "learning_rate": 2.9925982894512724e-05,
      "loss": 1.0988,
      "step": 580
    },
    {
      "epoch": 0.009131360030953764,
      "grad_norm": 1.9170016050338745,
      "learning_rate": 2.9924434419502527e-05,
      "loss": 1.1963,
      "step": 590
    },
    {
      "epoch": 0.009286128845037725,
      "grad_norm": 2.0211892127990723,
      "learning_rate": 2.9922885944492335e-05,
      "loss": 1.3807,
      "step": 600
    },
    {
      "epoch": 0.009440897659121686,
      "grad_norm": 1.512500524520874,
      "learning_rate": 2.992133746948214e-05,
      "loss": 1.2412,
      "step": 610
    },
    {
      "epoch": 0.00959566647320565,
      "grad_norm": 1.9090313911437988,
      "learning_rate": 2.9919788994471945e-05,
      "loss": 1.1814,
      "step": 620
    },
    {
      "epoch": 0.00975043528728961,
      "grad_norm": 2.04451060295105,
      "learning_rate": 2.991824051946175e-05,
      "loss": 1.0839,
      "step": 630
    },
    {
      "epoch": 0.009905204101373574,
      "grad_norm": 2.95900559425354,
      "learning_rate": 2.9916692044451556e-05,
      "loss": 1.1561,
      "step": 640
    },
    {
      "epoch": 0.010059972915457535,
      "grad_norm": 1.452073574066162,
      "learning_rate": 2.9915143569441364e-05,
      "loss": 1.1923,
      "step": 650
    },
    {
      "epoch": 0.010214741729541498,
      "grad_norm": 3.4508755207061768,
      "learning_rate": 2.991359509443117e-05,
      "loss": 1.3355,
      "step": 660
    },
    {
      "epoch": 0.010369510543625459,
      "grad_norm": 1.0116883516311646,
      "learning_rate": 2.9912046619420974e-05,
      "loss": 1.3633,
      "step": 670
    },
    {
      "epoch": 0.010524279357709422,
      "grad_norm": 1.3076163530349731,
      "learning_rate": 2.991049814441078e-05,
      "loss": 1.2849,
      "step": 680
    },
    {
      "epoch": 0.010679048171793383,
      "grad_norm": 1.5920976400375366,
      "learning_rate": 2.9908949669400585e-05,
      "loss": 1.4048,
      "step": 690
    },
    {
      "epoch": 0.010833816985877346,
      "grad_norm": 1.703381061553955,
      "learning_rate": 2.9907401194390392e-05,
      "loss": 1.4164,
      "step": 700
    },
    {
      "epoch": 0.010988585799961308,
      "grad_norm": 1.9107877016067505,
      "learning_rate": 2.9905852719380196e-05,
      "loss": 1.3523,
      "step": 710
    },
    {
      "epoch": 0.01114335461404527,
      "grad_norm": 2.133579969406128,
      "learning_rate": 2.9904304244370003e-05,
      "loss": 1.2994,
      "step": 720
    },
    {
      "epoch": 0.011298123428129232,
      "grad_norm": 1.4149787425994873,
      "learning_rate": 2.990275576935981e-05,
      "loss": 1.3533,
      "step": 730
    },
    {
      "epoch": 0.011452892242213193,
      "grad_norm": 1.8311854600906372,
      "learning_rate": 2.9901207294349614e-05,
      "loss": 1.1666,
      "step": 740
    },
    {
      "epoch": 0.011607661056297156,
      "grad_norm": 1.29277503490448,
      "learning_rate": 2.989965881933942e-05,
      "loss": 1.2735,
      "step": 750
    },
    {
      "epoch": 0.011762429870381117,
      "grad_norm": 2.1205015182495117,
      "learning_rate": 2.9898110344329225e-05,
      "loss": 1.3309,
      "step": 760
    },
    {
      "epoch": 0.01191719868446508,
      "grad_norm": 1.5208806991577148,
      "learning_rate": 2.9896561869319032e-05,
      "loss": 1.2332,
      "step": 770
    },
    {
      "epoch": 0.012071967498549042,
      "grad_norm": 1.3312580585479736,
      "learning_rate": 2.989501339430884e-05,
      "loss": 1.1266,
      "step": 780
    },
    {
      "epoch": 0.012226736312633005,
      "grad_norm": 1.9041941165924072,
      "learning_rate": 2.9893464919298647e-05,
      "loss": 1.2313,
      "step": 790
    },
    {
      "epoch": 0.012381505126716966,
      "grad_norm": 2.788487195968628,
      "learning_rate": 2.989191644428845e-05,
      "loss": 1.2848,
      "step": 800
    },
    {
      "epoch": 0.012536273940800929,
      "grad_norm": 1.475769281387329,
      "learning_rate": 2.9890367969278257e-05,
      "loss": 1.0822,
      "step": 810
    },
    {
      "epoch": 0.01269104275488489,
      "grad_norm": 2.7570879459381104,
      "learning_rate": 2.988881949426806e-05,
      "loss": 1.3261,
      "step": 820
    },
    {
      "epoch": 0.012845811568968853,
      "grad_norm": 2.349959373474121,
      "learning_rate": 2.988727101925787e-05,
      "loss": 1.247,
      "step": 830
    },
    {
      "epoch": 0.013000580383052815,
      "grad_norm": 1.9351321458816528,
      "learning_rate": 2.9885722544247672e-05,
      "loss": 1.4525,
      "step": 840
    },
    {
      "epoch": 0.013155349197136778,
      "grad_norm": 2.585984706878662,
      "learning_rate": 2.9884174069237483e-05,
      "loss": 1.3971,
      "step": 850
    },
    {
      "epoch": 0.013310118011220739,
      "grad_norm": 2.9948766231536865,
      "learning_rate": 2.9882625594227286e-05,
      "loss": 1.269,
      "step": 860
    },
    {
      "epoch": 0.013464886825304702,
      "grad_norm": 2.7638049125671387,
      "learning_rate": 2.9881077119217093e-05,
      "loss": 1.0902,
      "step": 870
    },
    {
      "epoch": 0.013619655639388663,
      "grad_norm": 1.629334807395935,
      "learning_rate": 2.9879528644206897e-05,
      "loss": 1.3315,
      "step": 880
    },
    {
      "epoch": 0.013774424453472624,
      "grad_norm": 1.4820114374160767,
      "learning_rate": 2.9877980169196704e-05,
      "loss": 1.1368,
      "step": 890
    },
    {
      "epoch": 0.013929193267556587,
      "grad_norm": 1.661082148551941,
      "learning_rate": 2.9876431694186508e-05,
      "loss": 1.2237,
      "step": 900
    },
    {
      "epoch": 0.014083962081640549,
      "grad_norm": 1.4050151109695435,
      "learning_rate": 2.9874883219176315e-05,
      "loss": 1.2819,
      "step": 910
    },
    {
      "epoch": 0.014238730895724512,
      "grad_norm": 3.3045012950897217,
      "learning_rate": 2.9873334744166122e-05,
      "loss": 1.1688,
      "step": 920
    },
    {
      "epoch": 0.014393499709808473,
      "grad_norm": 2.2572295665740967,
      "learning_rate": 2.987178626915593e-05,
      "loss": 1.3046,
      "step": 930
    },
    {
      "epoch": 0.014548268523892436,
      "grad_norm": 2.6962342262268066,
      "learning_rate": 2.9870237794145733e-05,
      "loss": 1.3685,
      "step": 940
    },
    {
      "epoch": 0.014703037337976397,
      "grad_norm": 2.701106309890747,
      "learning_rate": 2.986868931913554e-05,
      "loss": 1.0934,
      "step": 950
    },
    {
      "epoch": 0.01485780615206036,
      "grad_norm": 1.5741769075393677,
      "learning_rate": 2.9867140844125344e-05,
      "loss": 1.2985,
      "step": 960
    },
    {
      "epoch": 0.015012574966144322,
      "grad_norm": 2.607783317565918,
      "learning_rate": 2.9865592369115148e-05,
      "loss": 1.21,
      "step": 970
    },
    {
      "epoch": 0.015167343780228285,
      "grad_norm": 1.9792312383651733,
      "learning_rate": 2.9864043894104955e-05,
      "loss": 1.2942,
      "step": 980
    },
    {
      "epoch": 0.015322112594312246,
      "grad_norm": 2.412973403930664,
      "learning_rate": 2.9862495419094762e-05,
      "loss": 1.2969,
      "step": 990
    },
    {
      "epoch": 0.015476881408396209,
      "grad_norm": 1.9472633600234985,
      "learning_rate": 2.986094694408457e-05,
      "loss": 1.2625,
      "step": 1000
    },
    {
      "epoch": 0.01563165022248017,
      "grad_norm": 2.280488967895508,
      "learning_rate": 2.9859398469074373e-05,
      "loss": 1.1792,
      "step": 1010
    },
    {
      "epoch": 0.015786419036564133,
      "grad_norm": 1.9439212083816528,
      "learning_rate": 2.985784999406418e-05,
      "loss": 1.3494,
      "step": 1020
    },
    {
      "epoch": 0.015941187850648096,
      "grad_norm": 2.215904951095581,
      "learning_rate": 2.9856301519053984e-05,
      "loss": 1.1869,
      "step": 1030
    },
    {
      "epoch": 0.016095956664732056,
      "grad_norm": 1.771981120109558,
      "learning_rate": 2.985475304404379e-05,
      "loss": 1.2226,
      "step": 1040
    },
    {
      "epoch": 0.01625072547881602,
      "grad_norm": 1.5572229623794556,
      "learning_rate": 2.9853204569033595e-05,
      "loss": 1.2931,
      "step": 1050
    },
    {
      "epoch": 0.01640549429289998,
      "grad_norm": 2.474823236465454,
      "learning_rate": 2.9851656094023405e-05,
      "loss": 1.3084,
      "step": 1060
    },
    {
      "epoch": 0.01656026310698394,
      "grad_norm": 1.1217893362045288,
      "learning_rate": 2.985010761901321e-05,
      "loss": 1.2514,
      "step": 1070
    },
    {
      "epoch": 0.016715031921067904,
      "grad_norm": 3.570826768875122,
      "learning_rate": 2.9848559144003016e-05,
      "loss": 1.2201,
      "step": 1080
    },
    {
      "epoch": 0.016869800735151867,
      "grad_norm": 1.5987498760223389,
      "learning_rate": 2.984701066899282e-05,
      "loss": 1.2377,
      "step": 1090
    },
    {
      "epoch": 0.01702456954923583,
      "grad_norm": 1.1484265327453613,
      "learning_rate": 2.9845462193982627e-05,
      "loss": 1.2545,
      "step": 1100
    },
    {
      "epoch": 0.01717933836331979,
      "grad_norm": 2.4369306564331055,
      "learning_rate": 2.984391371897243e-05,
      "loss": 1.1435,
      "step": 1110
    },
    {
      "epoch": 0.017334107177403753,
      "grad_norm": 1.6910380125045776,
      "learning_rate": 2.9842365243962238e-05,
      "loss": 1.0927,
      "step": 1120
    },
    {
      "epoch": 0.017488875991487716,
      "grad_norm": 1.2047194242477417,
      "learning_rate": 2.9840816768952045e-05,
      "loss": 1.2569,
      "step": 1130
    },
    {
      "epoch": 0.01764364480557168,
      "grad_norm": 1.1245920658111572,
      "learning_rate": 2.9839268293941852e-05,
      "loss": 1.2366,
      "step": 1140
    },
    {
      "epoch": 0.01779841361965564,
      "grad_norm": 2.136812686920166,
      "learning_rate": 2.9837719818931656e-05,
      "loss": 1.2897,
      "step": 1150
    },
    {
      "epoch": 0.0179531824337396,
      "grad_norm": 1.341822862625122,
      "learning_rate": 2.9836171343921463e-05,
      "loss": 1.2072,
      "step": 1160
    },
    {
      "epoch": 0.018107951247823564,
      "grad_norm": 1.4193483591079712,
      "learning_rate": 2.9834622868911267e-05,
      "loss": 1.0978,
      "step": 1170
    },
    {
      "epoch": 0.018262720061907527,
      "grad_norm": 2.202836751937866,
      "learning_rate": 2.9833074393901074e-05,
      "loss": 1.2923,
      "step": 1180
    },
    {
      "epoch": 0.018417488875991487,
      "grad_norm": 1.8907815217971802,
      "learning_rate": 2.9831525918890878e-05,
      "loss": 1.4234,
      "step": 1190
    },
    {
      "epoch": 0.01857225769007545,
      "grad_norm": 2.0829052925109863,
      "learning_rate": 2.982997744388069e-05,
      "loss": 1.2116,
      "step": 1200
    },
    {
      "epoch": 0.018727026504159413,
      "grad_norm": 1.6718329191207886,
      "learning_rate": 2.9828428968870492e-05,
      "loss": 1.0987,
      "step": 1210
    },
    {
      "epoch": 0.018881795318243372,
      "grad_norm": 1.4563454389572144,
      "learning_rate": 2.9826880493860296e-05,
      "loss": 1.2392,
      "step": 1220
    },
    {
      "epoch": 0.019036564132327335,
      "grad_norm": 1.5491983890533447,
      "learning_rate": 2.9825332018850103e-05,
      "loss": 1.1309,
      "step": 1230
    },
    {
      "epoch": 0.0191913329464113,
      "grad_norm": 2.835667610168457,
      "learning_rate": 2.9823783543839907e-05,
      "loss": 1.266,
      "step": 1240
    },
    {
      "epoch": 0.01934610176049526,
      "grad_norm": 1.9923540353775024,
      "learning_rate": 2.9822235068829714e-05,
      "loss": 1.1303,
      "step": 1250
    },
    {
      "epoch": 0.01950087057457922,
      "grad_norm": 2.695256233215332,
      "learning_rate": 2.982068659381952e-05,
      "loss": 1.1768,
      "step": 1260
    },
    {
      "epoch": 0.019655639388663184,
      "grad_norm": 3.690488815307617,
      "learning_rate": 2.9819138118809328e-05,
      "loss": 1.1271,
      "step": 1270
    },
    {
      "epoch": 0.019810408202747147,
      "grad_norm": 1.8372979164123535,
      "learning_rate": 2.9817589643799132e-05,
      "loss": 1.2649,
      "step": 1280
    },
    {
      "epoch": 0.01996517701683111,
      "grad_norm": 2.2119131088256836,
      "learning_rate": 2.981604116878894e-05,
      "loss": 1.2077,
      "step": 1290
    },
    {
      "epoch": 0.02011994583091507,
      "grad_norm": 2.959815740585327,
      "learning_rate": 2.9814492693778743e-05,
      "loss": 1.2168,
      "step": 1300
    },
    {
      "epoch": 0.020274714644999033,
      "grad_norm": 2.7907767295837402,
      "learning_rate": 2.981294421876855e-05,
      "loss": 1.3694,
      "step": 1310
    },
    {
      "epoch": 0.020429483459082996,
      "grad_norm": 1.6151682138442993,
      "learning_rate": 2.9811395743758354e-05,
      "loss": 1.1016,
      "step": 1320
    },
    {
      "epoch": 0.020584252273166955,
      "grad_norm": 2.2248942852020264,
      "learning_rate": 2.9809847268748164e-05,
      "loss": 1.3806,
      "step": 1330
    },
    {
      "epoch": 0.020739021087250918,
      "grad_norm": 1.868956208229065,
      "learning_rate": 2.9808298793737968e-05,
      "loss": 1.2306,
      "step": 1340
    },
    {
      "epoch": 0.02089378990133488,
      "grad_norm": 2.3471367359161377,
      "learning_rate": 2.9806750318727775e-05,
      "loss": 1.2818,
      "step": 1350
    },
    {
      "epoch": 0.021048558715418844,
      "grad_norm": 2.2617573738098145,
      "learning_rate": 2.980520184371758e-05,
      "loss": 1.3373,
      "step": 1360
    },
    {
      "epoch": 0.021203327529502804,
      "grad_norm": 2.5557096004486084,
      "learning_rate": 2.9803653368707386e-05,
      "loss": 1.152,
      "step": 1370
    },
    {
      "epoch": 0.021358096343586767,
      "grad_norm": 2.209756374359131,
      "learning_rate": 2.980210489369719e-05,
      "loss": 1.2865,
      "step": 1380
    },
    {
      "epoch": 0.02151286515767073,
      "grad_norm": 1.6122915744781494,
      "learning_rate": 2.9800556418686997e-05,
      "loss": 1.2153,
      "step": 1390
    },
    {
      "epoch": 0.021667633971754693,
      "grad_norm": 2.4354610443115234,
      "learning_rate": 2.9799007943676804e-05,
      "loss": 1.3156,
      "step": 1400
    },
    {
      "epoch": 0.021822402785838652,
      "grad_norm": 1.6745096445083618,
      "learning_rate": 2.979745946866661e-05,
      "loss": 1.3065,
      "step": 1410
    },
    {
      "epoch": 0.021977171599922615,
      "grad_norm": 2.2443618774414062,
      "learning_rate": 2.9795910993656415e-05,
      "loss": 1.3036,
      "step": 1420
    },
    {
      "epoch": 0.02213194041400658,
      "grad_norm": 2.1776223182678223,
      "learning_rate": 2.9794362518646222e-05,
      "loss": 1.154,
      "step": 1430
    },
    {
      "epoch": 0.02228670922809054,
      "grad_norm": 2.5723774433135986,
      "learning_rate": 2.9792814043636026e-05,
      "loss": 1.2904,
      "step": 1440
    },
    {
      "epoch": 0.0224414780421745,
      "grad_norm": 2.586968421936035,
      "learning_rate": 2.9791265568625833e-05,
      "loss": 1.1549,
      "step": 1450
    },
    {
      "epoch": 0.022596246856258464,
      "grad_norm": 2.834245204925537,
      "learning_rate": 2.9789717093615637e-05,
      "loss": 1.1867,
      "step": 1460
    },
    {
      "epoch": 0.022751015670342427,
      "grad_norm": 1.474382996559143,
      "learning_rate": 2.9788168618605447e-05,
      "loss": 1.2454,
      "step": 1470
    },
    {
      "epoch": 0.022905784484426386,
      "grad_norm": 2.180161952972412,
      "learning_rate": 2.978662014359525e-05,
      "loss": 1.4693,
      "step": 1480
    },
    {
      "epoch": 0.02306055329851035,
      "grad_norm": 1.9207539558410645,
      "learning_rate": 2.9785071668585055e-05,
      "loss": 1.2655,
      "step": 1490
    },
    {
      "epoch": 0.023215322112594312,
      "grad_norm": 2.8263497352600098,
      "learning_rate": 2.9783523193574862e-05,
      "loss": 1.1617,
      "step": 1500
    },
    {
      "epoch": 0.023370090926678275,
      "grad_norm": 3.697256565093994,
      "learning_rate": 2.9781974718564666e-05,
      "loss": 1.1505,
      "step": 1510
    },
    {
      "epoch": 0.023524859740762235,
      "grad_norm": 1.4375494718551636,
      "learning_rate": 2.9780426243554473e-05,
      "loss": 1.2186,
      "step": 1520
    },
    {
      "epoch": 0.023679628554846198,
      "grad_norm": 1.3253034353256226,
      "learning_rate": 2.9778877768544277e-05,
      "loss": 1.1387,
      "step": 1530
    },
    {
      "epoch": 0.02383439736893016,
      "grad_norm": 1.8077772855758667,
      "learning_rate": 2.9777329293534087e-05,
      "loss": 1.3033,
      "step": 1540
    },
    {
      "epoch": 0.023989166183014124,
      "grad_norm": 2.3163628578186035,
      "learning_rate": 2.977578081852389e-05,
      "loss": 1.2707,
      "step": 1550
    },
    {
      "epoch": 0.024143934997098083,
      "grad_norm": 1.7672626972198486,
      "learning_rate": 2.9774232343513698e-05,
      "loss": 1.3599,
      "step": 1560
    },
    {
      "epoch": 0.024298703811182047,
      "grad_norm": 2.7621052265167236,
      "learning_rate": 2.9772683868503502e-05,
      "loss": 1.2555,
      "step": 1570
    },
    {
      "epoch": 0.02445347262526601,
      "grad_norm": 2.1824638843536377,
      "learning_rate": 2.977113539349331e-05,
      "loss": 1.2025,
      "step": 1580
    },
    {
      "epoch": 0.024608241439349973,
      "grad_norm": 2.1567156314849854,
      "learning_rate": 2.9769586918483113e-05,
      "loss": 1.3465,
      "step": 1590
    },
    {
      "epoch": 0.024763010253433932,
      "grad_norm": 1.6874110698699951,
      "learning_rate": 2.976803844347292e-05,
      "loss": 1.1086,
      "step": 1600
    },
    {
      "epoch": 0.024917779067517895,
      "grad_norm": 1.1569586992263794,
      "learning_rate": 2.9766489968462727e-05,
      "loss": 1.162,
      "step": 1610
    },
    {
      "epoch": 0.025072547881601858,
      "grad_norm": 1.1829065084457397,
      "learning_rate": 2.9764941493452534e-05,
      "loss": 1.2212,
      "step": 1620
    },
    {
      "epoch": 0.025227316695685818,
      "grad_norm": 1.6469030380249023,
      "learning_rate": 2.9763393018442338e-05,
      "loss": 1.276,
      "step": 1630
    },
    {
      "epoch": 0.02538208550976978,
      "grad_norm": 2.1734344959259033,
      "learning_rate": 2.9761844543432145e-05,
      "loss": 1.3289,
      "step": 1640
    },
    {
      "epoch": 0.025536854323853744,
      "grad_norm": 1.325565218925476,
      "learning_rate": 2.976029606842195e-05,
      "loss": 1.0676,
      "step": 1650
    },
    {
      "epoch": 0.025691623137937707,
      "grad_norm": 1.7755824327468872,
      "learning_rate": 2.9758747593411756e-05,
      "loss": 1.1036,
      "step": 1660
    },
    {
      "epoch": 0.025846391952021666,
      "grad_norm": 3.023685932159424,
      "learning_rate": 2.9757199118401563e-05,
      "loss": 1.1351,
      "step": 1670
    },
    {
      "epoch": 0.02600116076610563,
      "grad_norm": 1.320803165435791,
      "learning_rate": 2.975565064339137e-05,
      "loss": 1.2046,
      "step": 1680
    },
    {
      "epoch": 0.026155929580189592,
      "grad_norm": 1.4388914108276367,
      "learning_rate": 2.9754102168381174e-05,
      "loss": 1.2226,
      "step": 1690
    },
    {
      "epoch": 0.026310698394273555,
      "grad_norm": 2.236103057861328,
      "learning_rate": 2.975255369337098e-05,
      "loss": 1.2614,
      "step": 1700
    },
    {
      "epoch": 0.026465467208357515,
      "grad_norm": 2.855530023574829,
      "learning_rate": 2.9751005218360785e-05,
      "loss": 1.2178,
      "step": 1710
    },
    {
      "epoch": 0.026620236022441478,
      "grad_norm": 1.7041881084442139,
      "learning_rate": 2.9749456743350592e-05,
      "loss": 1.1637,
      "step": 1720
    },
    {
      "epoch": 0.02677500483652544,
      "grad_norm": 1.5228118896484375,
      "learning_rate": 2.9747908268340396e-05,
      "loss": 1.2648,
      "step": 1730
    },
    {
      "epoch": 0.026929773650609404,
      "grad_norm": 2.749290943145752,
      "learning_rate": 2.9746359793330203e-05,
      "loss": 1.4396,
      "step": 1740
    },
    {
      "epoch": 0.027084542464693363,
      "grad_norm": 2.281801223754883,
      "learning_rate": 2.974481131832001e-05,
      "loss": 1.2345,
      "step": 1750
    },
    {
      "epoch": 0.027239311278777326,
      "grad_norm": 2.3748886585235596,
      "learning_rate": 2.9743262843309814e-05,
      "loss": 1.2665,
      "step": 1760
    },
    {
      "epoch": 0.02739408009286129,
      "grad_norm": 1.6501202583312988,
      "learning_rate": 2.974171436829962e-05,
      "loss": 1.3327,
      "step": 1770
    },
    {
      "epoch": 0.02754884890694525,
      "grad_norm": 2.3718786239624023,
      "learning_rate": 2.9740165893289425e-05,
      "loss": 1.1132,
      "step": 1780
    },
    {
      "epoch": 0.027703617721029212,
      "grad_norm": 2.131737232208252,
      "learning_rate": 2.973861741827923e-05,
      "loss": 1.3702,
      "step": 1790
    },
    {
      "epoch": 0.027858386535113175,
      "grad_norm": 1.6204971075057983,
      "learning_rate": 2.9737068943269035e-05,
      "loss": 1.2635,
      "step": 1800
    },
    {
      "epoch": 0.028013155349197138,
      "grad_norm": 1.6553574800491333,
      "learning_rate": 2.9735520468258846e-05,
      "loss": 1.0745,
      "step": 1810
    },
    {
      "epoch": 0.028167924163281097,
      "grad_norm": 1.3404344320297241,
      "learning_rate": 2.973397199324865e-05,
      "loss": 1.149,
      "step": 1820
    },
    {
      "epoch": 0.02832269297736506,
      "grad_norm": 2.2369837760925293,
      "learning_rate": 2.9732423518238457e-05,
      "loss": 1.2314,
      "step": 1830
    },
    {
      "epoch": 0.028477461791449023,
      "grad_norm": 2.0281803607940674,
      "learning_rate": 2.973087504322826e-05,
      "loss": 1.2887,
      "step": 1840
    },
    {
      "epoch": 0.028632230605532986,
      "grad_norm": 1.8786187171936035,
      "learning_rate": 2.9729326568218068e-05,
      "loss": 1.2409,
      "step": 1850
    },
    {
      "epoch": 0.028786999419616946,
      "grad_norm": 1.276133418083191,
      "learning_rate": 2.972777809320787e-05,
      "loss": 1.2189,
      "step": 1860
    },
    {
      "epoch": 0.02894176823370091,
      "grad_norm": 1.8849518299102783,
      "learning_rate": 2.972622961819768e-05,
      "loss": 1.0561,
      "step": 1870
    },
    {
      "epoch": 0.029096537047784872,
      "grad_norm": 2.8426871299743652,
      "learning_rate": 2.9724681143187486e-05,
      "loss": 1.2638,
      "step": 1880
    },
    {
      "epoch": 0.029251305861868835,
      "grad_norm": 2.8124494552612305,
      "learning_rate": 2.9723132668177293e-05,
      "loss": 1.5142,
      "step": 1890
    },
    {
      "epoch": 0.029406074675952795,
      "grad_norm": 2.0074946880340576,
      "learning_rate": 2.9721584193167097e-05,
      "loss": 1.2197,
      "step": 1900
    },
    {
      "epoch": 0.029560843490036758,
      "grad_norm": 1.724070429801941,
      "learning_rate": 2.9720035718156904e-05,
      "loss": 1.0754,
      "step": 1910
    },
    {
      "epoch": 0.02971561230412072,
      "grad_norm": 2.076833963394165,
      "learning_rate": 2.9718487243146708e-05,
      "loss": 1.0736,
      "step": 1920
    },
    {
      "epoch": 0.02987038111820468,
      "grad_norm": 1.6210983991622925,
      "learning_rate": 2.9716938768136515e-05,
      "loss": 1.2026,
      "step": 1930
    },
    {
      "epoch": 0.030025149932288643,
      "grad_norm": 2.3424723148345947,
      "learning_rate": 2.971539029312632e-05,
      "loss": 1.338,
      "step": 1940
    },
    {
      "epoch": 0.030179918746372606,
      "grad_norm": 2.2634754180908203,
      "learning_rate": 2.971384181811613e-05,
      "loss": 1.2538,
      "step": 1950
    },
    {
      "epoch": 0.03033468756045657,
      "grad_norm": 2.195233106613159,
      "learning_rate": 2.9712293343105933e-05,
      "loss": 1.207,
      "step": 1960
    },
    {
      "epoch": 0.03048945637454053,
      "grad_norm": 1.2844352722167969,
      "learning_rate": 2.971074486809574e-05,
      "loss": 1.277,
      "step": 1970
    },
    {
      "epoch": 0.03064422518862449,
      "grad_norm": 1.328863501548767,
      "learning_rate": 2.9709196393085544e-05,
      "loss": 1.0994,
      "step": 1980
    },
    {
      "epoch": 0.030798994002708455,
      "grad_norm": 1.9301913976669312,
      "learning_rate": 2.9707647918075347e-05,
      "loss": 1.2794,
      "step": 1990
    },
    {
      "epoch": 0.030953762816792418,
      "grad_norm": 2.0875768661499023,
      "learning_rate": 2.9706099443065154e-05,
      "loss": 1.2647,
      "step": 2000
    },
    {
      "epoch": 0.031108531630876377,
      "grad_norm": 1.9259923696517944,
      "learning_rate": 2.970470581555598e-05,
      "loss": 1.4164,
      "step": 2010
    },
    {
      "epoch": 0.03126330044496034,
      "grad_norm": 1.288589358329773,
      "learning_rate": 2.9703157340545784e-05,
      "loss": 1.0388,
      "step": 2020
    },
    {
      "epoch": 0.0314180692590443,
      "grad_norm": 1.4895806312561035,
      "learning_rate": 2.9701608865535595e-05,
      "loss": 1.268,
      "step": 2030
    },
    {
      "epoch": 0.031572838073128266,
      "grad_norm": 1.6663479804992676,
      "learning_rate": 2.97000603905254e-05,
      "loss": 1.0861,
      "step": 2040
    },
    {
      "epoch": 0.031727606887212226,
      "grad_norm": 2.620811939239502,
      "learning_rate": 2.9698511915515206e-05,
      "loss": 1.0543,
      "step": 2050
    },
    {
      "epoch": 0.03188237570129619,
      "grad_norm": 1.8253209590911865,
      "learning_rate": 2.969696344050501e-05,
      "loss": 1.3407,
      "step": 2060
    },
    {
      "epoch": 0.03203714451538015,
      "grad_norm": 1.7981550693511963,
      "learning_rate": 2.9695414965494817e-05,
      "loss": 1.1258,
      "step": 2070
    },
    {
      "epoch": 0.03219191332946411,
      "grad_norm": 2.8854141235351562,
      "learning_rate": 2.969386649048462e-05,
      "loss": 1.1919,
      "step": 2080
    },
    {
      "epoch": 0.03234668214354808,
      "grad_norm": 2.473839282989502,
      "learning_rate": 2.9692318015474428e-05,
      "loss": 0.9231,
      "step": 2090
    },
    {
      "epoch": 0.03250145095763204,
      "grad_norm": 3.23508882522583,
      "learning_rate": 2.9690769540464235e-05,
      "loss": 1.2655,
      "step": 2100
    },
    {
      "epoch": 0.032656219771716,
      "grad_norm": 1.6069974899291992,
      "learning_rate": 2.9689221065454042e-05,
      "loss": 1.2362,
      "step": 2110
    },
    {
      "epoch": 0.03281098858579996,
      "grad_norm": 2.042388439178467,
      "learning_rate": 2.9687672590443846e-05,
      "loss": 1.1553,
      "step": 2120
    },
    {
      "epoch": 0.03296575739988392,
      "grad_norm": 1.8236215114593506,
      "learning_rate": 2.9686124115433653e-05,
      "loss": 1.3963,
      "step": 2130
    },
    {
      "epoch": 0.03312052621396788,
      "grad_norm": 1.5808284282684326,
      "learning_rate": 2.9684575640423456e-05,
      "loss": 1.0669,
      "step": 2140
    },
    {
      "epoch": 0.03327529502805185,
      "grad_norm": 1.60204017162323,
      "learning_rate": 2.9683027165413264e-05,
      "loss": 1.2051,
      "step": 2150
    },
    {
      "epoch": 0.03343006384213581,
      "grad_norm": 2.3463051319122314,
      "learning_rate": 2.9681478690403067e-05,
      "loss": 1.1504,
      "step": 2160
    },
    {
      "epoch": 0.033584832656219775,
      "grad_norm": 2.706207752227783,
      "learning_rate": 2.9679930215392874e-05,
      "loss": 1.1884,
      "step": 2170
    },
    {
      "epoch": 0.033739601470303734,
      "grad_norm": 2.0876076221466064,
      "learning_rate": 2.967838174038268e-05,
      "loss": 1.1983,
      "step": 2180
    },
    {
      "epoch": 0.033894370284387694,
      "grad_norm": 2.044696807861328,
      "learning_rate": 2.9676833265372485e-05,
      "loss": 1.2019,
      "step": 2190
    },
    {
      "epoch": 0.03404913909847166,
      "grad_norm": 2.673783779144287,
      "learning_rate": 2.9675284790362292e-05,
      "loss": 1.3555,
      "step": 2200
    },
    {
      "epoch": 0.03420390791255562,
      "grad_norm": 1.7657572031021118,
      "learning_rate": 2.9673736315352096e-05,
      "loss": 1.1961,
      "step": 2210
    },
    {
      "epoch": 0.03435867672663958,
      "grad_norm": 1.5196874141693115,
      "learning_rate": 2.9672187840341903e-05,
      "loss": 1.0358,
      "step": 2220
    },
    {
      "epoch": 0.034513445540723546,
      "grad_norm": 2.4899823665618896,
      "learning_rate": 2.9670639365331707e-05,
      "loss": 1.4034,
      "step": 2230
    },
    {
      "epoch": 0.034668214354807506,
      "grad_norm": 4.63454532623291,
      "learning_rate": 2.9669090890321518e-05,
      "loss": 1.2788,
      "step": 2240
    },
    {
      "epoch": 0.034822983168891465,
      "grad_norm": 1.4702142477035522,
      "learning_rate": 2.966754241531132e-05,
      "loss": 1.1219,
      "step": 2250
    },
    {
      "epoch": 0.03497775198297543,
      "grad_norm": 1.4194391965866089,
      "learning_rate": 2.966599394030113e-05,
      "loss": 1.2509,
      "step": 2260
    },
    {
      "epoch": 0.03513252079705939,
      "grad_norm": 3.1252622604370117,
      "learning_rate": 2.9664445465290932e-05,
      "loss": 1.1301,
      "step": 2270
    },
    {
      "epoch": 0.03528728961114336,
      "grad_norm": 1.3917244672775269,
      "learning_rate": 2.966289699028074e-05,
      "loss": 1.1407,
      "step": 2280
    },
    {
      "epoch": 0.03544205842522732,
      "grad_norm": 2.2536635398864746,
      "learning_rate": 2.9661348515270543e-05,
      "loss": 1.2077,
      "step": 2290
    },
    {
      "epoch": 0.03559682723931128,
      "grad_norm": 1.5803102254867554,
      "learning_rate": 2.965980004026035e-05,
      "loss": 1.2517,
      "step": 2300
    },
    {
      "epoch": 0.03575159605339524,
      "grad_norm": 2.9034721851348877,
      "learning_rate": 2.9658251565250157e-05,
      "loss": 1.1799,
      "step": 2310
    },
    {
      "epoch": 0.0359063648674792,
      "grad_norm": 3.1025025844573975,
      "learning_rate": 2.9656703090239965e-05,
      "loss": 1.1889,
      "step": 2320
    },
    {
      "epoch": 0.03606113368156316,
      "grad_norm": 1.2727704048156738,
      "learning_rate": 2.965515461522977e-05,
      "loss": 1.3894,
      "step": 2330
    },
    {
      "epoch": 0.03621590249564713,
      "grad_norm": 2.0651047229766846,
      "learning_rate": 2.9653606140219575e-05,
      "loss": 1.3593,
      "step": 2340
    },
    {
      "epoch": 0.03637067130973109,
      "grad_norm": 2.5057899951934814,
      "learning_rate": 2.965205766520938e-05,
      "loss": 1.1776,
      "step": 2350
    },
    {
      "epoch": 0.036525440123815055,
      "grad_norm": 2.943389415740967,
      "learning_rate": 2.9650509190199186e-05,
      "loss": 1.2514,
      "step": 2360
    },
    {
      "epoch": 0.036680208937899014,
      "grad_norm": 2.1550991535186768,
      "learning_rate": 2.9648960715188994e-05,
      "loss": 1.0881,
      "step": 2370
    },
    {
      "epoch": 0.036834977751982974,
      "grad_norm": 1.2223892211914062,
      "learning_rate": 2.96474122401788e-05,
      "loss": 1.136,
      "step": 2380
    },
    {
      "epoch": 0.03698974656606694,
      "grad_norm": 2.7748541831970215,
      "learning_rate": 2.9645863765168604e-05,
      "loss": 1.2256,
      "step": 2390
    },
    {
      "epoch": 0.0371445153801509,
      "grad_norm": 2.005077362060547,
      "learning_rate": 2.964431529015841e-05,
      "loss": 1.2437,
      "step": 2400
    },
    {
      "epoch": 0.03729928419423486,
      "grad_norm": 1.8013800382614136,
      "learning_rate": 2.9642766815148215e-05,
      "loss": 1.1425,
      "step": 2410
    },
    {
      "epoch": 0.037454053008318826,
      "grad_norm": 1.6363400220870972,
      "learning_rate": 2.964121834013802e-05,
      "loss": 1.019,
      "step": 2420
    },
    {
      "epoch": 0.037608821822402785,
      "grad_norm": 2.018256187438965,
      "learning_rate": 2.9639669865127826e-05,
      "loss": 1.2676,
      "step": 2430
    },
    {
      "epoch": 0.037763590636486745,
      "grad_norm": 1.3047630786895752,
      "learning_rate": 2.9638121390117633e-05,
      "loss": 1.3002,
      "step": 2440
    },
    {
      "epoch": 0.03791835945057071,
      "grad_norm": 1.1397956609725952,
      "learning_rate": 2.963657291510744e-05,
      "loss": 1.0562,
      "step": 2450
    },
    {
      "epoch": 0.03807312826465467,
      "grad_norm": 3.011918783187866,
      "learning_rate": 2.9635024440097244e-05,
      "loss": 1.173,
      "step": 2460
    },
    {
      "epoch": 0.03822789707873864,
      "grad_norm": 2.9248199462890625,
      "learning_rate": 2.963347596508705e-05,
      "loss": 1.2875,
      "step": 2470
    },
    {
      "epoch": 0.0383826658928226,
      "grad_norm": 1.6783809661865234,
      "learning_rate": 2.9631927490076855e-05,
      "loss": 0.9718,
      "step": 2480
    },
    {
      "epoch": 0.038537434706906556,
      "grad_norm": 1.7628271579742432,
      "learning_rate": 2.9630379015066662e-05,
      "loss": 1.2083,
      "step": 2490
    },
    {
      "epoch": 0.03869220352099052,
      "grad_norm": 3.388385534286499,
      "learning_rate": 2.9628830540056466e-05,
      "loss": 1.1634,
      "step": 2500
    },
    {
      "epoch": 0.03884697233507448,
      "grad_norm": 3.5901782512664795,
      "learning_rate": 2.9627282065046277e-05,
      "loss": 1.0578,
      "step": 2510
    },
    {
      "epoch": 0.03900174114915844,
      "grad_norm": 2.2991623878479004,
      "learning_rate": 2.962573359003608e-05,
      "loss": 1.0855,
      "step": 2520
    },
    {
      "epoch": 0.03915650996324241,
      "grad_norm": 1.3835512399673462,
      "learning_rate": 2.9624185115025887e-05,
      "loss": 1.127,
      "step": 2530
    },
    {
      "epoch": 0.03931127877732637,
      "grad_norm": 1.213487982749939,
      "learning_rate": 2.962263664001569e-05,
      "loss": 1.3089,
      "step": 2540
    },
    {
      "epoch": 0.03946604759141033,
      "grad_norm": 1.6073920726776123,
      "learning_rate": 2.9621088165005498e-05,
      "loss": 1.0633,
      "step": 2550
    },
    {
      "epoch": 0.039620816405494294,
      "grad_norm": 2.9046151638031006,
      "learning_rate": 2.9619539689995302e-05,
      "loss": 1.1842,
      "step": 2560
    },
    {
      "epoch": 0.039775585219578254,
      "grad_norm": 1.8551939725875854,
      "learning_rate": 2.961799121498511e-05,
      "loss": 1.1666,
      "step": 2570
    },
    {
      "epoch": 0.03993035403366222,
      "grad_norm": 2.5953993797302246,
      "learning_rate": 2.9616442739974916e-05,
      "loss": 1.3621,
      "step": 2580
    },
    {
      "epoch": 0.04008512284774618,
      "grad_norm": 2.5738344192504883,
      "learning_rate": 2.9614894264964723e-05,
      "loss": 1.2979,
      "step": 2590
    },
    {
      "epoch": 0.04023989166183014,
      "grad_norm": 2.4200398921966553,
      "learning_rate": 2.9613345789954527e-05,
      "loss": 1.12,
      "step": 2600
    },
    {
      "epoch": 0.040394660475914106,
      "grad_norm": 1.8157055377960205,
      "learning_rate": 2.9611797314944334e-05,
      "loss": 1.2248,
      "step": 2610
    },
    {
      "epoch": 0.040549429289998065,
      "grad_norm": 1.9972933530807495,
      "learning_rate": 2.9610248839934138e-05,
      "loss": 1.2789,
      "step": 2620
    },
    {
      "epoch": 0.040704198104082025,
      "grad_norm": 1.4697165489196777,
      "learning_rate": 2.9608700364923945e-05,
      "loss": 1.0288,
      "step": 2630
    },
    {
      "epoch": 0.04085896691816599,
      "grad_norm": 2.1347391605377197,
      "learning_rate": 2.960715188991375e-05,
      "loss": 1.1745,
      "step": 2640
    },
    {
      "epoch": 0.04101373573224995,
      "grad_norm": 1.334352731704712,
      "learning_rate": 2.960560341490356e-05,
      "loss": 1.196,
      "step": 2650
    },
    {
      "epoch": 0.04116850454633391,
      "grad_norm": 1.8528721332550049,
      "learning_rate": 2.9604054939893363e-05,
      "loss": 0.9661,
      "step": 2660
    },
    {
      "epoch": 0.04132327336041788,
      "grad_norm": 2.8667526245117188,
      "learning_rate": 2.9602506464883167e-05,
      "loss": 1.1904,
      "step": 2670
    },
    {
      "epoch": 0.041478042174501836,
      "grad_norm": 3.6767771244049072,
      "learning_rate": 2.9600957989872974e-05,
      "loss": 1.2387,
      "step": 2680
    },
    {
      "epoch": 0.0416328109885858,
      "grad_norm": 2.525360107421875,
      "learning_rate": 2.9599409514862778e-05,
      "loss": 1.2632,
      "step": 2690
    },
    {
      "epoch": 0.04178757980266976,
      "grad_norm": 1.7670559883117676,
      "learning_rate": 2.9597861039852585e-05,
      "loss": 1.237,
      "step": 2700
    },
    {
      "epoch": 0.04194234861675372,
      "grad_norm": 2.5036346912384033,
      "learning_rate": 2.959631256484239e-05,
      "loss": 1.1554,
      "step": 2710
    },
    {
      "epoch": 0.04209711743083769,
      "grad_norm": 3.229630947113037,
      "learning_rate": 2.95947640898322e-05,
      "loss": 1.0617,
      "step": 2720
    },
    {
      "epoch": 0.04225188624492165,
      "grad_norm": 2.5429177284240723,
      "learning_rate": 2.9593215614822003e-05,
      "loss": 1.2041,
      "step": 2730
    },
    {
      "epoch": 0.04240665505900561,
      "grad_norm": 1.5993785858154297,
      "learning_rate": 2.959166713981181e-05,
      "loss": 1.4035,
      "step": 2740
    },
    {
      "epoch": 0.042561423873089574,
      "grad_norm": 3.353445053100586,
      "learning_rate": 2.9590118664801614e-05,
      "loss": 1.1473,
      "step": 2750
    },
    {
      "epoch": 0.04271619268717353,
      "grad_norm": 2.253624439239502,
      "learning_rate": 2.958857018979142e-05,
      "loss": 1.3379,
      "step": 2760
    },
    {
      "epoch": 0.0428709615012575,
      "grad_norm": 1.670940637588501,
      "learning_rate": 2.9587021714781225e-05,
      "loss": 1.2977,
      "step": 2770
    },
    {
      "epoch": 0.04302573031534146,
      "grad_norm": 2.909695863723755,
      "learning_rate": 2.9585473239771035e-05,
      "loss": 1.0428,
      "step": 2780
    },
    {
      "epoch": 0.04318049912942542,
      "grad_norm": 1.4006681442260742,
      "learning_rate": 2.958392476476084e-05,
      "loss": 1.189,
      "step": 2790
    },
    {
      "epoch": 0.043335267943509385,
      "grad_norm": 3.082618474960327,
      "learning_rate": 2.9582376289750646e-05,
      "loss": 0.9917,
      "step": 2800
    },
    {
      "epoch": 0.043490036757593345,
      "grad_norm": 2.652440309524536,
      "learning_rate": 2.958082781474045e-05,
      "loss": 1.066,
      "step": 2810
    },
    {
      "epoch": 0.043644805571677305,
      "grad_norm": 1.8545591831207275,
      "learning_rate": 2.9579279339730257e-05,
      "loss": 1.1921,
      "step": 2820
    },
    {
      "epoch": 0.04379957438576127,
      "grad_norm": 1.8811413049697876,
      "learning_rate": 2.957773086472006e-05,
      "loss": 1.0559,
      "step": 2830
    },
    {
      "epoch": 0.04395434319984523,
      "grad_norm": 2.59084153175354,
      "learning_rate": 2.9576182389709868e-05,
      "loss": 1.2169,
      "step": 2840
    },
    {
      "epoch": 0.04410911201392919,
      "grad_norm": 3.5045816898345947,
      "learning_rate": 2.9574633914699675e-05,
      "loss": 1.1258,
      "step": 2850
    },
    {
      "epoch": 0.04426388082801316,
      "grad_norm": 1.5860002040863037,
      "learning_rate": 2.9573085439689482e-05,
      "loss": 1.2195,
      "step": 2860
    },
    {
      "epoch": 0.044418649642097116,
      "grad_norm": 1.6192907094955444,
      "learning_rate": 2.9571536964679286e-05,
      "loss": 1.1394,
      "step": 2870
    },
    {
      "epoch": 0.04457341845618108,
      "grad_norm": 2.8798511028289795,
      "learning_rate": 2.9569988489669093e-05,
      "loss": 1.0952,
      "step": 2880
    },
    {
      "epoch": 0.04472818727026504,
      "grad_norm": 1.5807839632034302,
      "learning_rate": 2.9568440014658897e-05,
      "loss": 1.0738,
      "step": 2890
    },
    {
      "epoch": 0.044882956084349,
      "grad_norm": 2.7557644844055176,
      "learning_rate": 2.9566891539648704e-05,
      "loss": 1.2453,
      "step": 2900
    },
    {
      "epoch": 0.04503772489843297,
      "grad_norm": 1.7429858446121216,
      "learning_rate": 2.9565343064638508e-05,
      "loss": 1.1224,
      "step": 2910
    },
    {
      "epoch": 0.04519249371251693,
      "grad_norm": 1.7688697576522827,
      "learning_rate": 2.9563794589628315e-05,
      "loss": 1.0499,
      "step": 2920
    },
    {
      "epoch": 0.04534726252660089,
      "grad_norm": 2.286728620529175,
      "learning_rate": 2.9562246114618122e-05,
      "loss": 1.2128,
      "step": 2930
    },
    {
      "epoch": 0.045502031340684854,
      "grad_norm": 2.154709577560425,
      "learning_rate": 2.9560697639607926e-05,
      "loss": 1.1586,
      "step": 2940
    },
    {
      "epoch": 0.04565680015476881,
      "grad_norm": 1.4421006441116333,
      "learning_rate": 2.9559149164597733e-05,
      "loss": 0.9983,
      "step": 2950
    },
    {
      "epoch": 0.04581156896885277,
      "grad_norm": 1.6281880140304565,
      "learning_rate": 2.9557600689587537e-05,
      "loss": 0.9564,
      "step": 2960
    },
    {
      "epoch": 0.04596633778293674,
      "grad_norm": 1.8711740970611572,
      "learning_rate": 2.9556052214577344e-05,
      "loss": 1.0938,
      "step": 2970
    },
    {
      "epoch": 0.0461211065970207,
      "grad_norm": 1.4654607772827148,
      "learning_rate": 2.9554503739567148e-05,
      "loss": 1.0645,
      "step": 2980
    },
    {
      "epoch": 0.046275875411104665,
      "grad_norm": 2.2766153812408447,
      "learning_rate": 2.9552955264556958e-05,
      "loss": 1.2473,
      "step": 2990
    },
    {
      "epoch": 0.046430644225188625,
      "grad_norm": 2.048323631286621,
      "learning_rate": 2.9551406789546762e-05,
      "loss": 1.0144,
      "step": 3000
    },
    {
      "epoch": 0.046585413039272584,
      "grad_norm": 1.9693630933761597,
      "learning_rate": 2.954985831453657e-05,
      "loss": 1.1445,
      "step": 3010
    },
    {
      "epoch": 0.04674018185335655,
      "grad_norm": 2.289520502090454,
      "learning_rate": 2.9548309839526373e-05,
      "loss": 1.2688,
      "step": 3020
    },
    {
      "epoch": 0.04689495066744051,
      "grad_norm": 1.8224459886550903,
      "learning_rate": 2.954676136451618e-05,
      "loss": 0.9921,
      "step": 3030
    },
    {
      "epoch": 0.04704971948152447,
      "grad_norm": 1.4176443815231323,
      "learning_rate": 2.9545212889505984e-05,
      "loss": 1.3746,
      "step": 3040
    },
    {
      "epoch": 0.047204488295608436,
      "grad_norm": 2.017106294631958,
      "learning_rate": 2.954366441449579e-05,
      "loss": 1.1263,
      "step": 3050
    },
    {
      "epoch": 0.047359257109692396,
      "grad_norm": 2.1546332836151123,
      "learning_rate": 2.9542115939485598e-05,
      "loss": 1.1667,
      "step": 3060
    },
    {
      "epoch": 0.04751402592377636,
      "grad_norm": 1.5831174850463867,
      "learning_rate": 2.9540567464475405e-05,
      "loss": 0.9795,
      "step": 3070
    },
    {
      "epoch": 0.04766879473786032,
      "grad_norm": 2.5589399337768555,
      "learning_rate": 2.953901898946521e-05,
      "loss": 0.9585,
      "step": 3080
    },
    {
      "epoch": 0.04782356355194428,
      "grad_norm": 3.2033607959747314,
      "learning_rate": 2.9537470514455016e-05,
      "loss": 1.1389,
      "step": 3090
    },
    {
      "epoch": 0.04797833236602825,
      "grad_norm": 2.290590286254883,
      "learning_rate": 2.953592203944482e-05,
      "loss": 1.2074,
      "step": 3100
    },
    {
      "epoch": 0.04813310118011221,
      "grad_norm": 1.3212521076202393,
      "learning_rate": 2.9534373564434627e-05,
      "loss": 0.9755,
      "step": 3110
    },
    {
      "epoch": 0.04828786999419617,
      "grad_norm": 4.475589752197266,
      "learning_rate": 2.953282508942443e-05,
      "loss": 1.0724,
      "step": 3120
    },
    {
      "epoch": 0.04844263880828013,
      "grad_norm": 1.7686928510665894,
      "learning_rate": 2.953127661441424e-05,
      "loss": 0.974,
      "step": 3130
    },
    {
      "epoch": 0.04859740762236409,
      "grad_norm": 1.9777404069900513,
      "learning_rate": 2.9529728139404045e-05,
      "loss": 0.9704,
      "step": 3140
    },
    {
      "epoch": 0.04875217643644805,
      "grad_norm": 1.2774908542633057,
      "learning_rate": 2.9528179664393852e-05,
      "loss": 0.9064,
      "step": 3150
    },
    {
      "epoch": 0.04890694525053202,
      "grad_norm": 1.4456955194473267,
      "learning_rate": 2.9526631189383656e-05,
      "loss": 1.0017,
      "step": 3160
    },
    {
      "epoch": 0.04906171406461598,
      "grad_norm": 1.74824059009552,
      "learning_rate": 2.952508271437346e-05,
      "loss": 1.3869,
      "step": 3170
    },
    {
      "epoch": 0.049216482878699945,
      "grad_norm": 1.8466466665267944,
      "learning_rate": 2.9523534239363267e-05,
      "loss": 1.2958,
      "step": 3180
    },
    {
      "epoch": 0.049371251692783905,
      "grad_norm": 2.9157092571258545,
      "learning_rate": 2.9521985764353074e-05,
      "loss": 1.3196,
      "step": 3190
    },
    {
      "epoch": 0.049526020506867864,
      "grad_norm": 1.2835018634796143,
      "learning_rate": 2.952043728934288e-05,
      "loss": 0.9895,
      "step": 3200
    },
    {
      "epoch": 0.04968078932095183,
      "grad_norm": 1.9476062059402466,
      "learning_rate": 2.9518888814332685e-05,
      "loss": 1.1237,
      "step": 3210
    },
    {
      "epoch": 0.04983555813503579,
      "grad_norm": 1.3542729616165161,
      "learning_rate": 2.9517340339322492e-05,
      "loss": 1.2702,
      "step": 3220
    },
    {
      "epoch": 0.04999032694911975,
      "grad_norm": 1.6401474475860596,
      "learning_rate": 2.9515791864312296e-05,
      "loss": 1.0937,
      "step": 3230
    },
    {
      "epoch": 0.050145095763203716,
      "grad_norm": 1.720253586769104,
      "learning_rate": 2.9514243389302103e-05,
      "loss": 1.0613,
      "step": 3240
    },
    {
      "epoch": 0.050299864577287676,
      "grad_norm": 1.485733985900879,
      "learning_rate": 2.9512694914291907e-05,
      "loss": 1.144,
      "step": 3250
    },
    {
      "epoch": 0.050454633391371635,
      "grad_norm": 1.7511796951293945,
      "learning_rate": 2.9511146439281717e-05,
      "loss": 0.9394,
      "step": 3260
    },
    {
      "epoch": 0.0506094022054556,
      "grad_norm": 1.856356143951416,
      "learning_rate": 2.950959796427152e-05,
      "loss": 1.2239,
      "step": 3270
    },
    {
      "epoch": 0.05076417101953956,
      "grad_norm": 1.4308961629867554,
      "learning_rate": 2.9508049489261328e-05,
      "loss": 1.1497,
      "step": 3280
    },
    {
      "epoch": 0.05091893983362353,
      "grad_norm": 3.298767328262329,
      "learning_rate": 2.950650101425113e-05,
      "loss": 1.0235,
      "step": 3290
    },
    {
      "epoch": 0.05107370864770749,
      "grad_norm": 3.5040407180786133,
      "learning_rate": 2.950495253924094e-05,
      "loss": 1.0648,
      "step": 3300
    },
    {
      "epoch": 0.05122847746179145,
      "grad_norm": 2.5166242122650146,
      "learning_rate": 2.9503404064230743e-05,
      "loss": 1.1456,
      "step": 3310
    },
    {
      "epoch": 0.05138324627587541,
      "grad_norm": 1.8473641872406006,
      "learning_rate": 2.950185558922055e-05,
      "loss": 1.1125,
      "step": 3320
    },
    {
      "epoch": 0.05153801508995937,
      "grad_norm": 2.1590633392333984,
      "learning_rate": 2.9500307114210357e-05,
      "loss": 1.0472,
      "step": 3330
    },
    {
      "epoch": 0.05169278390404333,
      "grad_norm": 1.301561951637268,
      "learning_rate": 2.9498758639200164e-05,
      "loss": 1.0639,
      "step": 3340
    },
    {
      "epoch": 0.0518475527181273,
      "grad_norm": 1.6715996265411377,
      "learning_rate": 2.9497210164189968e-05,
      "loss": 1.2054,
      "step": 3350
    },
    {
      "epoch": 0.05200232153221126,
      "grad_norm": 1.1755118370056152,
      "learning_rate": 2.9495661689179775e-05,
      "loss": 1.2687,
      "step": 3360
    },
    {
      "epoch": 0.052157090346295225,
      "grad_norm": 2.0137698650360107,
      "learning_rate": 2.949411321416958e-05,
      "loss": 1.0789,
      "step": 3370
    },
    {
      "epoch": 0.052311859160379184,
      "grad_norm": 1.284627914428711,
      "learning_rate": 2.9492564739159386e-05,
      "loss": 1.1945,
      "step": 3380
    },
    {
      "epoch": 0.052466627974463144,
      "grad_norm": 3.709775447845459,
      "learning_rate": 2.949101626414919e-05,
      "loss": 1.0091,
      "step": 3390
    },
    {
      "epoch": 0.05262139678854711,
      "grad_norm": 1.353500485420227,
      "learning_rate": 2.9489467789139e-05,
      "loss": 1.0999,
      "step": 3400
    },
    {
      "epoch": 0.05277616560263107,
      "grad_norm": 2.3116748332977295,
      "learning_rate": 2.9487919314128804e-05,
      "loss": 1.0999,
      "step": 3410
    },
    {
      "epoch": 0.05293093441671503,
      "grad_norm": 2.519282579421997,
      "learning_rate": 2.948637083911861e-05,
      "loss": 1.0218,
      "step": 3420
    },
    {
      "epoch": 0.053085703230798996,
      "grad_norm": 2.2396843433380127,
      "learning_rate": 2.9484822364108415e-05,
      "loss": 1.0633,
      "step": 3430
    },
    {
      "epoch": 0.053240472044882955,
      "grad_norm": 1.2944000959396362,
      "learning_rate": 2.948327388909822e-05,
      "loss": 1.0837,
      "step": 3440
    },
    {
      "epoch": 0.053395240858966915,
      "grad_norm": 2.100156545639038,
      "learning_rate": 2.9481725414088026e-05,
      "loss": 1.106,
      "step": 3450
    },
    {
      "epoch": 0.05355000967305088,
      "grad_norm": 3.0333566665649414,
      "learning_rate": 2.948017693907783e-05,
      "loss": 1.047,
      "step": 3460
    },
    {
      "epoch": 0.05370477848713484,
      "grad_norm": 1.6896113157272339,
      "learning_rate": 2.947862846406764e-05,
      "loss": 1.0448,
      "step": 3470
    },
    {
      "epoch": 0.05385954730121881,
      "grad_norm": 1.8554303646087646,
      "learning_rate": 2.9477079989057444e-05,
      "loss": 1.0669,
      "step": 3480
    },
    {
      "epoch": 0.05401431611530277,
      "grad_norm": 2.033413887023926,
      "learning_rate": 2.947553151404725e-05,
      "loss": 1.0187,
      "step": 3490
    },
    {
      "epoch": 0.05416908492938673,
      "grad_norm": 1.6124879121780396,
      "learning_rate": 2.9473983039037054e-05,
      "loss": 1.1442,
      "step": 3500
    },
    {
      "epoch": 0.05432385374347069,
      "grad_norm": 1.8032246828079224,
      "learning_rate": 2.947243456402686e-05,
      "loss": 1.1864,
      "step": 3510
    },
    {
      "epoch": 0.05447862255755465,
      "grad_norm": 1.5245459079742432,
      "learning_rate": 2.9470886089016665e-05,
      "loss": 1.0896,
      "step": 3520
    },
    {
      "epoch": 0.05463339137163861,
      "grad_norm": 3.3208045959472656,
      "learning_rate": 2.9469337614006473e-05,
      "loss": 1.1519,
      "step": 3530
    },
    {
      "epoch": 0.05478816018572258,
      "grad_norm": 2.943897247314453,
      "learning_rate": 2.946778913899628e-05,
      "loss": 1.2106,
      "step": 3540
    },
    {
      "epoch": 0.05494292899980654,
      "grad_norm": 3.1431117057800293,
      "learning_rate": 2.9466240663986087e-05,
      "loss": 1.1645,
      "step": 3550
    },
    {
      "epoch": 0.0550976978138905,
      "grad_norm": 2.8172717094421387,
      "learning_rate": 2.946469218897589e-05,
      "loss": 1.2793,
      "step": 3560
    },
    {
      "epoch": 0.055252466627974464,
      "grad_norm": 1.8037354946136475,
      "learning_rate": 2.9463143713965698e-05,
      "loss": 1.1502,
      "step": 3570
    },
    {
      "epoch": 0.055407235442058424,
      "grad_norm": 1.5297915935516357,
      "learning_rate": 2.94615952389555e-05,
      "loss": 1.1739,
      "step": 3580
    },
    {
      "epoch": 0.05556200425614239,
      "grad_norm": 1.5079388618469238,
      "learning_rate": 2.946004676394531e-05,
      "loss": 0.9669,
      "step": 3590
    },
    {
      "epoch": 0.05571677307022635,
      "grad_norm": 1.2887057065963745,
      "learning_rate": 2.9458498288935112e-05,
      "loss": 0.985,
      "step": 3600
    },
    {
      "epoch": 0.05587154188431031,
      "grad_norm": 2.9732134342193604,
      "learning_rate": 2.9456949813924923e-05,
      "loss": 1.1644,
      "step": 3610
    },
    {
      "epoch": 0.056026310698394276,
      "grad_norm": 3.518493890762329,
      "learning_rate": 2.9455401338914727e-05,
      "loss": 1.187,
      "step": 3620
    },
    {
      "epoch": 0.056181079512478235,
      "grad_norm": 2.523319721221924,
      "learning_rate": 2.9453852863904534e-05,
      "loss": 1.132,
      "step": 3630
    },
    {
      "epoch": 0.056335848326562195,
      "grad_norm": 1.5109153985977173,
      "learning_rate": 2.9452304388894338e-05,
      "loss": 1.2019,
      "step": 3640
    },
    {
      "epoch": 0.05649061714064616,
      "grad_norm": 2.6455554962158203,
      "learning_rate": 2.9450755913884145e-05,
      "loss": 1.2239,
      "step": 3650
    },
    {
      "epoch": 0.05664538595473012,
      "grad_norm": 1.6212742328643799,
      "learning_rate": 2.944920743887395e-05,
      "loss": 1.2294,
      "step": 3660
    },
    {
      "epoch": 0.05680015476881409,
      "grad_norm": 3.0636634826660156,
      "learning_rate": 2.9447658963863756e-05,
      "loss": 1.1885,
      "step": 3670
    },
    {
      "epoch": 0.05695492358289805,
      "grad_norm": 2.1488473415374756,
      "learning_rate": 2.9446110488853563e-05,
      "loss": 1.1613,
      "step": 3680
    },
    {
      "epoch": 0.057109692396982006,
      "grad_norm": 1.8019895553588867,
      "learning_rate": 2.9444562013843366e-05,
      "loss": 1.1923,
      "step": 3690
    },
    {
      "epoch": 0.05726446121106597,
      "grad_norm": 2.371140241622925,
      "learning_rate": 2.9443013538833174e-05,
      "loss": 1.0142,
      "step": 3700
    },
    {
      "epoch": 0.05741923002514993,
      "grad_norm": 1.9596505165100098,
      "learning_rate": 2.9441465063822977e-05,
      "loss": 1.0471,
      "step": 3710
    },
    {
      "epoch": 0.05757399883923389,
      "grad_norm": 2.1849725246429443,
      "learning_rate": 2.9439916588812784e-05,
      "loss": 1.0522,
      "step": 3720
    },
    {
      "epoch": 0.05772876765331786,
      "grad_norm": 3.247528553009033,
      "learning_rate": 2.9438368113802588e-05,
      "loss": 1.1469,
      "step": 3730
    },
    {
      "epoch": 0.05788353646740182,
      "grad_norm": 2.92311429977417,
      "learning_rate": 2.94368196387924e-05,
      "loss": 1.2611,
      "step": 3740
    },
    {
      "epoch": 0.05803830528148578,
      "grad_norm": 1.566774606704712,
      "learning_rate": 2.9435271163782202e-05,
      "loss": 1.2007,
      "step": 3750
    },
    {
      "epoch": 0.058193074095569744,
      "grad_norm": 2.3701977729797363,
      "learning_rate": 2.943372268877201e-05,
      "loss": 1.0605,
      "step": 3760
    },
    {
      "epoch": 0.058347842909653704,
      "grad_norm": 1.5031547546386719,
      "learning_rate": 2.9432174213761813e-05,
      "loss": 1.2263,
      "step": 3770
    },
    {
      "epoch": 0.05850261172373767,
      "grad_norm": 2.2402963638305664,
      "learning_rate": 2.943062573875162e-05,
      "loss": 1.2178,
      "step": 3780
    },
    {
      "epoch": 0.05865738053782163,
      "grad_norm": 3.0984950065612793,
      "learning_rate": 2.9429077263741424e-05,
      "loss": 1.3125,
      "step": 3790
    },
    {
      "epoch": 0.05881214935190559,
      "grad_norm": 2.1492536067962646,
      "learning_rate": 2.942752878873123e-05,
      "loss": 1.065,
      "step": 3800
    },
    {
      "epoch": 0.058966918165989556,
      "grad_norm": 1.6971583366394043,
      "learning_rate": 2.942598031372104e-05,
      "loss": 1.1785,
      "step": 3810
    },
    {
      "epoch": 0.059121686980073515,
      "grad_norm": NaN,
      "learning_rate": 2.9424431838710846e-05,
      "loss": 1.0621,
      "step": 3820
    },
    {
      "epoch": 0.059276455794157475,
      "grad_norm": 1.6408454179763794,
      "learning_rate": 2.9423038211201672e-05,
      "loss": 1.3593,
      "step": 3830
    },
    {
      "epoch": 0.05943122460824144,
      "grad_norm": 1.5902142524719238,
      "learning_rate": 2.9421489736191476e-05,
      "loss": 0.9852,
      "step": 3840
    },
    {
      "epoch": 0.0595859934223254,
      "grad_norm": 1.396099328994751,
      "learning_rate": 2.9419941261181283e-05,
      "loss": 0.9893,
      "step": 3850
    },
    {
      "epoch": 0.05974076223640936,
      "grad_norm": 1.617476463317871,
      "learning_rate": 2.9418392786171086e-05,
      "loss": 1.1227,
      "step": 3860
    },
    {
      "epoch": 0.05989553105049333,
      "grad_norm": 1.6853673458099365,
      "learning_rate": 2.941684431116089e-05,
      "loss": 1.0819,
      "step": 3870
    },
    {
      "epoch": 0.060050299864577286,
      "grad_norm": 1.748113751411438,
      "learning_rate": 2.9415295836150697e-05,
      "loss": 1.1072,
      "step": 3880
    },
    {
      "epoch": 0.06020506867866125,
      "grad_norm": 1.5326563119888306,
      "learning_rate": 2.9413747361140504e-05,
      "loss": 1.114,
      "step": 3890
    },
    {
      "epoch": 0.06035983749274521,
      "grad_norm": 1.2555674314498901,
      "learning_rate": 2.941219888613031e-05,
      "loss": 1.0331,
      "step": 3900
    },
    {
      "epoch": 0.06051460630682917,
      "grad_norm": 0.9108864068984985,
      "learning_rate": 2.9410650411120115e-05,
      "loss": 1.0995,
      "step": 3910
    },
    {
      "epoch": 0.06066937512091314,
      "grad_norm": 1.5015932321548462,
      "learning_rate": 2.9409101936109922e-05,
      "loss": 1.2158,
      "step": 3920
    },
    {
      "epoch": 0.0608241439349971,
      "grad_norm": 2.2533762454986572,
      "learning_rate": 2.9407553461099726e-05,
      "loss": 1.1706,
      "step": 3930
    },
    {
      "epoch": 0.06097891274908106,
      "grad_norm": 1.8847798109054565,
      "learning_rate": 2.9406004986089533e-05,
      "loss": 1.1973,
      "step": 3940
    },
    {
      "epoch": 0.061133681563165024,
      "grad_norm": 2.286972999572754,
      "learning_rate": 2.9404456511079337e-05,
      "loss": 1.1481,
      "step": 3950
    },
    {
      "epoch": 0.06128845037724898,
      "grad_norm": 1.4877842664718628,
      "learning_rate": 2.9402908036069148e-05,
      "loss": 1.0692,
      "step": 3960
    },
    {
      "epoch": 0.06144321919133295,
      "grad_norm": 1.95321524143219,
      "learning_rate": 2.940135956105895e-05,
      "loss": 1.1056,
      "step": 3970
    },
    {
      "epoch": 0.06159798800541691,
      "grad_norm": 1.937596082687378,
      "learning_rate": 2.939981108604876e-05,
      "loss": 1.1035,
      "step": 3980
    },
    {
      "epoch": 0.06175275681950087,
      "grad_norm": 1.9567004442214966,
      "learning_rate": 2.9398262611038562e-05,
      "loss": 1.1384,
      "step": 3990
    },
    {
      "epoch": 0.061907525633584835,
      "grad_norm": 1.19605553150177,
      "learning_rate": 2.939671413602837e-05,
      "loss": 1.1767,
      "step": 4000
    },
    {
      "epoch": 0.062062294447668795,
      "grad_norm": 2.0300683975219727,
      "learning_rate": 2.9395165661018173e-05,
      "loss": 1.0402,
      "step": 4010
    },
    {
      "epoch": 0.062217063261752754,
      "grad_norm": 1.859998106956482,
      "learning_rate": 2.939361718600798e-05,
      "loss": 0.9177,
      "step": 4020
    },
    {
      "epoch": 0.06237183207583672,
      "grad_norm": 2.4428021907806396,
      "learning_rate": 2.9392068710997787e-05,
      "loss": 1.2675,
      "step": 4030
    },
    {
      "epoch": 0.06252660088992068,
      "grad_norm": 1.9250768423080444,
      "learning_rate": 2.9390520235987595e-05,
      "loss": 1.0575,
      "step": 4040
    },
    {
      "epoch": 0.06268136970400465,
      "grad_norm": 2.1561474800109863,
      "learning_rate": 2.93889717609774e-05,
      "loss": 1.2301,
      "step": 4050
    },
    {
      "epoch": 0.0628361385180886,
      "grad_norm": 2.607454538345337,
      "learning_rate": 2.9387423285967205e-05,
      "loss": 1.1433,
      "step": 4060
    },
    {
      "epoch": 0.06299090733217257,
      "grad_norm": 2.008211374282837,
      "learning_rate": 2.938587481095701e-05,
      "loss": 1.1921,
      "step": 4070
    },
    {
      "epoch": 0.06314567614625653,
      "grad_norm": 1.7189784049987793,
      "learning_rate": 2.9384326335946816e-05,
      "loss": 1.2013,
      "step": 4080
    },
    {
      "epoch": 0.06330044496034049,
      "grad_norm": 2.231945514678955,
      "learning_rate": 2.938277786093662e-05,
      "loss": 0.9927,
      "step": 4090
    },
    {
      "epoch": 0.06345521377442445,
      "grad_norm": 3.0962138175964355,
      "learning_rate": 2.938122938592643e-05,
      "loss": 1.0461,
      "step": 4100
    },
    {
      "epoch": 0.06360998258850842,
      "grad_norm": 2.5151259899139404,
      "learning_rate": 2.9379680910916234e-05,
      "loss": 1.2125,
      "step": 4110
    },
    {
      "epoch": 0.06376475140259238,
      "grad_norm": 2.2160377502441406,
      "learning_rate": 2.9378132435906038e-05,
      "loss": 1.1527,
      "step": 4120
    },
    {
      "epoch": 0.06391952021667634,
      "grad_norm": 1.9605523347854614,
      "learning_rate": 2.9376583960895845e-05,
      "loss": 1.1425,
      "step": 4130
    },
    {
      "epoch": 0.0640742890307603,
      "grad_norm": 1.4990851879119873,
      "learning_rate": 2.937503548588565e-05,
      "loss": 1.1594,
      "step": 4140
    },
    {
      "epoch": 0.06422905784484427,
      "grad_norm": 2.64754581451416,
      "learning_rate": 2.9373487010875456e-05,
      "loss": 1.1619,
      "step": 4150
    },
    {
      "epoch": 0.06438382665892822,
      "grad_norm": 1.6340794563293457,
      "learning_rate": 2.937193853586526e-05,
      "loss": 1.1485,
      "step": 4160
    },
    {
      "epoch": 0.06453859547301219,
      "grad_norm": 4.119000434875488,
      "learning_rate": 2.937039006085507e-05,
      "loss": 1.2436,
      "step": 4170
    },
    {
      "epoch": 0.06469336428709616,
      "grad_norm": 1.659637451171875,
      "learning_rate": 2.9368841585844874e-05,
      "loss": 1.123,
      "step": 4180
    },
    {
      "epoch": 0.06484813310118011,
      "grad_norm": 2.7727370262145996,
      "learning_rate": 2.936729311083468e-05,
      "loss": 1.0591,
      "step": 4190
    },
    {
      "epoch": 0.06500290191526407,
      "grad_norm": 2.5526909828186035,
      "learning_rate": 2.9365744635824485e-05,
      "loss": 1.0811,
      "step": 4200
    },
    {
      "epoch": 0.06515767072934804,
      "grad_norm": 1.8160275220870972,
      "learning_rate": 2.9364196160814292e-05,
      "loss": 1.0465,
      "step": 4210
    },
    {
      "epoch": 0.065312439543432,
      "grad_norm": 1.9016785621643066,
      "learning_rate": 2.9362647685804096e-05,
      "loss": 1.316,
      "step": 4220
    },
    {
      "epoch": 0.06546720835751596,
      "grad_norm": 2.4447813034057617,
      "learning_rate": 2.9361099210793903e-05,
      "loss": 1.0913,
      "step": 4230
    },
    {
      "epoch": 0.06562197717159993,
      "grad_norm": 1.4684940576553345,
      "learning_rate": 2.935955073578371e-05,
      "loss": 1.1044,
      "step": 4240
    },
    {
      "epoch": 0.06577674598568388,
      "grad_norm": 2.587027072906494,
      "learning_rate": 2.9358002260773517e-05,
      "loss": 1.1318,
      "step": 4250
    },
    {
      "epoch": 0.06593151479976785,
      "grad_norm": 1.963407278060913,
      "learning_rate": 2.935645378576332e-05,
      "loss": 0.9567,
      "step": 4260
    },
    {
      "epoch": 0.06608628361385181,
      "grad_norm": 1.6438918113708496,
      "learning_rate": 2.9354905310753128e-05,
      "loss": 1.1241,
      "step": 4270
    },
    {
      "epoch": 0.06624105242793576,
      "grad_norm": 2.318181037902832,
      "learning_rate": 2.9353356835742932e-05,
      "loss": 1.1139,
      "step": 4280
    },
    {
      "epoch": 0.06639582124201973,
      "grad_norm": 1.0975620746612549,
      "learning_rate": 2.935180836073274e-05,
      "loss": 0.9837,
      "step": 4290
    },
    {
      "epoch": 0.0665505900561037,
      "grad_norm": 1.9251741170883179,
      "learning_rate": 2.9350259885722543e-05,
      "loss": 1.1826,
      "step": 4300
    },
    {
      "epoch": 0.06670535887018765,
      "grad_norm": 1.482645034790039,
      "learning_rate": 2.9348711410712353e-05,
      "loss": 0.9399,
      "step": 4310
    },
    {
      "epoch": 0.06686012768427162,
      "grad_norm": 2.0733842849731445,
      "learning_rate": 2.9347162935702157e-05,
      "loss": 1.2239,
      "step": 4320
    },
    {
      "epoch": 0.06701489649835558,
      "grad_norm": 1.3466322422027588,
      "learning_rate": 2.9345614460691964e-05,
      "loss": 1.0012,
      "step": 4330
    },
    {
      "epoch": 0.06716966531243955,
      "grad_norm": 1.2986944913864136,
      "learning_rate": 2.9344065985681768e-05,
      "loss": 1.1847,
      "step": 4340
    },
    {
      "epoch": 0.0673244341265235,
      "grad_norm": 1.9708209037780762,
      "learning_rate": 2.9342517510671575e-05,
      "loss": 1.1689,
      "step": 4350
    },
    {
      "epoch": 0.06747920294060747,
      "grad_norm": 1.3559688329696655,
      "learning_rate": 2.934096903566138e-05,
      "loss": 1.0948,
      "step": 4360
    },
    {
      "epoch": 0.06763397175469144,
      "grad_norm": 2.225487232208252,
      "learning_rate": 2.9339420560651186e-05,
      "loss": 1.3155,
      "step": 4370
    },
    {
      "epoch": 0.06778874056877539,
      "grad_norm": 2.93489670753479,
      "learning_rate": 2.9337872085640993e-05,
      "loss": 1.043,
      "step": 4380
    },
    {
      "epoch": 0.06794350938285935,
      "grad_norm": 2.1962623596191406,
      "learning_rate": 2.9336323610630797e-05,
      "loss": 1.1869,
      "step": 4390
    },
    {
      "epoch": 0.06809827819694332,
      "grad_norm": 2.1824300289154053,
      "learning_rate": 2.9334775135620604e-05,
      "loss": 1.2128,
      "step": 4400
    },
    {
      "epoch": 0.06825304701102727,
      "grad_norm": 1.2117735147476196,
      "learning_rate": 2.9333226660610408e-05,
      "loss": 1.0536,
      "step": 4410
    },
    {
      "epoch": 0.06840781582511124,
      "grad_norm": 2.299069881439209,
      "learning_rate": 2.9331678185600215e-05,
      "loss": 0.995,
      "step": 4420
    },
    {
      "epoch": 0.0685625846391952,
      "grad_norm": 1.994450330734253,
      "learning_rate": 2.933012971059002e-05,
      "loss": 1.1737,
      "step": 4430
    },
    {
      "epoch": 0.06871735345327916,
      "grad_norm": 1.9391608238220215,
      "learning_rate": 2.932858123557983e-05,
      "loss": 1.2764,
      "step": 4440
    },
    {
      "epoch": 0.06887212226736313,
      "grad_norm": 1.906241774559021,
      "learning_rate": 2.9327032760569633e-05,
      "loss": 1.2415,
      "step": 4450
    },
    {
      "epoch": 0.06902689108144709,
      "grad_norm": 1.8125745058059692,
      "learning_rate": 2.932548428555944e-05,
      "loss": 1.2891,
      "step": 4460
    },
    {
      "epoch": 0.06918165989553104,
      "grad_norm": 2.6087207794189453,
      "learning_rate": 2.9323935810549244e-05,
      "loss": 0.9606,
      "step": 4470
    },
    {
      "epoch": 0.06933642870961501,
      "grad_norm": 3.2637693881988525,
      "learning_rate": 2.932238733553905e-05,
      "loss": 1.152,
      "step": 4480
    },
    {
      "epoch": 0.06949119752369898,
      "grad_norm": 1.5565471649169922,
      "learning_rate": 2.9320838860528855e-05,
      "loss": 1.2094,
      "step": 4490
    },
    {
      "epoch": 0.06964596633778293,
      "grad_norm": 1.4787994623184204,
      "learning_rate": 2.9319290385518662e-05,
      "loss": 1.0208,
      "step": 4500
    },
    {
      "epoch": 0.0698007351518669,
      "grad_norm": 2.1838009357452393,
      "learning_rate": 2.931774191050847e-05,
      "loss": 1.0829,
      "step": 4510
    },
    {
      "epoch": 0.06995550396595086,
      "grad_norm": 2.0977158546447754,
      "learning_rate": 2.9316193435498276e-05,
      "loss": 1.1207,
      "step": 4520
    },
    {
      "epoch": 0.07011027278003483,
      "grad_norm": 2.064692258834839,
      "learning_rate": 2.931464496048808e-05,
      "loss": 1.0584,
      "step": 4530
    },
    {
      "epoch": 0.07026504159411878,
      "grad_norm": 1.500941514968872,
      "learning_rate": 2.9313096485477887e-05,
      "loss": 1.0726,
      "step": 4540
    },
    {
      "epoch": 0.07041981040820275,
      "grad_norm": 1.8933165073394775,
      "learning_rate": 2.931154801046769e-05,
      "loss": 1.1213,
      "step": 4550
    },
    {
      "epoch": 0.07057457922228672,
      "grad_norm": 2.353778123855591,
      "learning_rate": 2.9309999535457498e-05,
      "loss": 1.1995,
      "step": 4560
    },
    {
      "epoch": 0.07072934803637067,
      "grad_norm": 1.9725663661956787,
      "learning_rate": 2.9308451060447302e-05,
      "loss": 1.073,
      "step": 4570
    },
    {
      "epoch": 0.07088411685045463,
      "grad_norm": 2.0757699012756348,
      "learning_rate": 2.9306902585437112e-05,
      "loss": 1.112,
      "step": 4580
    },
    {
      "epoch": 0.0710388856645386,
      "grad_norm": 1.4542299509048462,
      "learning_rate": 2.9305354110426916e-05,
      "loss": 1.1226,
      "step": 4590
    },
    {
      "epoch": 0.07119365447862255,
      "grad_norm": 1.5266879796981812,
      "learning_rate": 2.9303805635416723e-05,
      "loss": 1.1096,
      "step": 4600
    },
    {
      "epoch": 0.07134842329270652,
      "grad_norm": 2.4880292415618896,
      "learning_rate": 2.9302257160406527e-05,
      "loss": 1.2079,
      "step": 4610
    },
    {
      "epoch": 0.07150319210679049,
      "grad_norm": 1.721664309501648,
      "learning_rate": 2.930070868539633e-05,
      "loss": 1.023,
      "step": 4620
    },
    {
      "epoch": 0.07165796092087444,
      "grad_norm": 1.6128771305084229,
      "learning_rate": 2.9299160210386138e-05,
      "loss": 1.1782,
      "step": 4630
    },
    {
      "epoch": 0.0718127297349584,
      "grad_norm": 2.235175371170044,
      "learning_rate": 2.929761173537594e-05,
      "loss": 1.1222,
      "step": 4640
    },
    {
      "epoch": 0.07196749854904237,
      "grad_norm": 1.5483996868133545,
      "learning_rate": 2.9296063260365752e-05,
      "loss": 1.0913,
      "step": 4650
    },
    {
      "epoch": 0.07212226736312632,
      "grad_norm": 2.409912586212158,
      "learning_rate": 2.9294514785355556e-05,
      "loss": 1.0538,
      "step": 4660
    },
    {
      "epoch": 0.07227703617721029,
      "grad_norm": 2.466461420059204,
      "learning_rate": 2.9292966310345363e-05,
      "loss": 1.1071,
      "step": 4670
    },
    {
      "epoch": 0.07243180499129426,
      "grad_norm": 1.172877311706543,
      "learning_rate": 2.9291417835335167e-05,
      "loss": 1.0781,
      "step": 4680
    },
    {
      "epoch": 0.07258657380537821,
      "grad_norm": 2.009305238723755,
      "learning_rate": 2.9289869360324974e-05,
      "loss": 1.1237,
      "step": 4690
    },
    {
      "epoch": 0.07274134261946218,
      "grad_norm": 1.715867280960083,
      "learning_rate": 2.9288320885314778e-05,
      "loss": 1.3415,
      "step": 4700
    },
    {
      "epoch": 0.07289611143354614,
      "grad_norm": 2.524472713470459,
      "learning_rate": 2.9286772410304585e-05,
      "loss": 1.1147,
      "step": 4710
    },
    {
      "epoch": 0.07305088024763011,
      "grad_norm": 2.0571913719177246,
      "learning_rate": 2.9285223935294392e-05,
      "loss": 1.2144,
      "step": 4720
    },
    {
      "epoch": 0.07320564906171406,
      "grad_norm": 0.9681835770606995,
      "learning_rate": 2.92836754602842e-05,
      "loss": 0.9567,
      "step": 4730
    },
    {
      "epoch": 0.07336041787579803,
      "grad_norm": 1.4937469959259033,
      "learning_rate": 2.9282126985274003e-05,
      "loss": 1.1339,
      "step": 4740
    },
    {
      "epoch": 0.073515186689882,
      "grad_norm": 1.448246955871582,
      "learning_rate": 2.928057851026381e-05,
      "loss": 1.2149,
      "step": 4750
    },
    {
      "epoch": 0.07366995550396595,
      "grad_norm": 1.7520002126693726,
      "learning_rate": 2.9279030035253614e-05,
      "loss": 1.0412,
      "step": 4760
    },
    {
      "epoch": 0.07382472431804991,
      "grad_norm": 1.9663467407226562,
      "learning_rate": 2.927748156024342e-05,
      "loss": 1.109,
      "step": 4770
    },
    {
      "epoch": 0.07397949313213388,
      "grad_norm": 1.8703759908676147,
      "learning_rate": 2.9275933085233228e-05,
      "loss": 1.1756,
      "step": 4780
    },
    {
      "epoch": 0.07413426194621783,
      "grad_norm": 1.3569488525390625,
      "learning_rate": 2.9274384610223035e-05,
      "loss": 1.0332,
      "step": 4790
    },
    {
      "epoch": 0.0742890307603018,
      "grad_norm": 1.4102082252502441,
      "learning_rate": 2.927283613521284e-05,
      "loss": 1.0942,
      "step": 4800
    },
    {
      "epoch": 0.07444379957438577,
      "grad_norm": 2.670626401901245,
      "learning_rate": 2.9271287660202646e-05,
      "loss": 1.138,
      "step": 4810
    },
    {
      "epoch": 0.07459856838846972,
      "grad_norm": 2.8598833084106445,
      "learning_rate": 2.926973918519245e-05,
      "loss": 1.0642,
      "step": 4820
    },
    {
      "epoch": 0.07475333720255369,
      "grad_norm": 2.2745845317840576,
      "learning_rate": 2.9268190710182257e-05,
      "loss": 1.265,
      "step": 4830
    },
    {
      "epoch": 0.07490810601663765,
      "grad_norm": 1.980042576789856,
      "learning_rate": 2.926664223517206e-05,
      "loss": 1.0586,
      "step": 4840
    },
    {
      "epoch": 0.0750628748307216,
      "grad_norm": 2.5893466472625732,
      "learning_rate": 2.926509376016187e-05,
      "loss": 1.1036,
      "step": 4850
    },
    {
      "epoch": 0.07521764364480557,
      "grad_norm": 1.3941025733947754,
      "learning_rate": 2.9263545285151675e-05,
      "loss": 0.9383,
      "step": 4860
    },
    {
      "epoch": 0.07537241245888954,
      "grad_norm": 2.461966037750244,
      "learning_rate": 2.926199681014148e-05,
      "loss": 1.1223,
      "step": 4870
    },
    {
      "epoch": 0.07552718127297349,
      "grad_norm": 1.8491976261138916,
      "learning_rate": 2.9260448335131286e-05,
      "loss": 1.1992,
      "step": 4880
    },
    {
      "epoch": 0.07568195008705746,
      "grad_norm": 1.6325284242630005,
      "learning_rate": 2.925889986012109e-05,
      "loss": 0.9322,
      "step": 4890
    },
    {
      "epoch": 0.07583671890114142,
      "grad_norm": 1.4052687883377075,
      "learning_rate": 2.9257351385110897e-05,
      "loss": 1.0556,
      "step": 4900
    },
    {
      "epoch": 0.07599148771522538,
      "grad_norm": 1.58479905128479,
      "learning_rate": 2.92558029101007e-05,
      "loss": 1.0349,
      "step": 4910
    },
    {
      "epoch": 0.07614625652930934,
      "grad_norm": 2.0476009845733643,
      "learning_rate": 2.925425443509051e-05,
      "loss": 1.1885,
      "step": 4920
    },
    {
      "epoch": 0.07630102534339331,
      "grad_norm": 3.1122920513153076,
      "learning_rate": 2.9252705960080315e-05,
      "loss": 1.2085,
      "step": 4930
    },
    {
      "epoch": 0.07645579415747727,
      "grad_norm": 2.3125805854797363,
      "learning_rate": 2.9251157485070122e-05,
      "loss": 1.1878,
      "step": 4940
    },
    {
      "epoch": 0.07661056297156123,
      "grad_norm": 3.666227340698242,
      "learning_rate": 2.9249609010059926e-05,
      "loss": 1.117,
      "step": 4950
    },
    {
      "epoch": 0.0767653317856452,
      "grad_norm": 2.1882152557373047,
      "learning_rate": 2.9248060535049733e-05,
      "loss": 1.2342,
      "step": 4960
    },
    {
      "epoch": 0.07692010059972916,
      "grad_norm": 1.7749065160751343,
      "learning_rate": 2.9246512060039537e-05,
      "loss": 1.0317,
      "step": 4970
    },
    {
      "epoch": 0.07707486941381311,
      "grad_norm": 1.5993552207946777,
      "learning_rate": 2.9244963585029344e-05,
      "loss": 1.2337,
      "step": 4980
    },
    {
      "epoch": 0.07722963822789708,
      "grad_norm": 1.5326201915740967,
      "learning_rate": 2.924341511001915e-05,
      "loss": 1.3066,
      "step": 4990
    },
    {
      "epoch": 0.07738440704198105,
      "grad_norm": 2.964322805404663,
      "learning_rate": 2.9241866635008958e-05,
      "loss": 1.326,
      "step": 5000
    },
    {
      "epoch": 0.077539175856065,
      "grad_norm": 2.9302594661712646,
      "learning_rate": 2.924031815999876e-05,
      "loss": 1.2187,
      "step": 5010
    },
    {
      "epoch": 0.07769394467014896,
      "grad_norm": 2.2964820861816406,
      "learning_rate": 2.923876968498857e-05,
      "loss": 1.181,
      "step": 5020
    },
    {
      "epoch": 0.07784871348423293,
      "grad_norm": 2.54598331451416,
      "learning_rate": 2.9237221209978373e-05,
      "loss": 1.1272,
      "step": 5030
    },
    {
      "epoch": 0.07800348229831688,
      "grad_norm": 1.4922354221343994,
      "learning_rate": 2.923567273496818e-05,
      "loss": 1.0613,
      "step": 5040
    },
    {
      "epoch": 0.07815825111240085,
      "grad_norm": 2.2907228469848633,
      "learning_rate": 2.9234124259957983e-05,
      "loss": 1.0457,
      "step": 5050
    },
    {
      "epoch": 0.07831301992648482,
      "grad_norm": 1.888440489768982,
      "learning_rate": 2.9232575784947794e-05,
      "loss": 1.0041,
      "step": 5060
    },
    {
      "epoch": 0.07846778874056877,
      "grad_norm": 2.349100351333618,
      "learning_rate": 2.9231027309937598e-05,
      "loss": 1.1947,
      "step": 5070
    },
    {
      "epoch": 0.07862255755465274,
      "grad_norm": 1.8621197938919067,
      "learning_rate": 2.9229478834927405e-05,
      "loss": 1.1194,
      "step": 5080
    },
    {
      "epoch": 0.0787773263687367,
      "grad_norm": 2.1758742332458496,
      "learning_rate": 2.922793035991721e-05,
      "loss": 1.2277,
      "step": 5090
    },
    {
      "epoch": 0.07893209518282066,
      "grad_norm": 1.820046067237854,
      "learning_rate": 2.9226381884907016e-05,
      "loss": 1.2757,
      "step": 5100
    },
    {
      "epoch": 0.07908686399690462,
      "grad_norm": 2.3288919925689697,
      "learning_rate": 2.922483340989682e-05,
      "loss": 1.1652,
      "step": 5110
    },
    {
      "epoch": 0.07924163281098859,
      "grad_norm": 1.946548342704773,
      "learning_rate": 2.9223284934886627e-05,
      "loss": 1.063,
      "step": 5120
    },
    {
      "epoch": 0.07939640162507255,
      "grad_norm": 1.5445823669433594,
      "learning_rate": 2.9221736459876434e-05,
      "loss": 1.1846,
      "step": 5130
    },
    {
      "epoch": 0.07955117043915651,
      "grad_norm": 1.5470713376998901,
      "learning_rate": 2.9220187984866238e-05,
      "loss": 1.0679,
      "step": 5140
    },
    {
      "epoch": 0.07970593925324047,
      "grad_norm": 1.6746796369552612,
      "learning_rate": 2.9218639509856045e-05,
      "loss": 0.9807,
      "step": 5150
    },
    {
      "epoch": 0.07986070806732444,
      "grad_norm": 2.247636318206787,
      "learning_rate": 2.921709103484585e-05,
      "loss": 0.8829,
      "step": 5160
    },
    {
      "epoch": 0.08001547688140839,
      "grad_norm": 1.3399652242660522,
      "learning_rate": 2.9215542559835656e-05,
      "loss": 1.0515,
      "step": 5170
    },
    {
      "epoch": 0.08017024569549236,
      "grad_norm": 1.716812014579773,
      "learning_rate": 2.921399408482546e-05,
      "loss": 1.245,
      "step": 5180
    },
    {
      "epoch": 0.08032501450957633,
      "grad_norm": 1.5134713649749756,
      "learning_rate": 2.9212445609815266e-05,
      "loss": 1.1561,
      "step": 5190
    },
    {
      "epoch": 0.08047978332366028,
      "grad_norm": 1.9127066135406494,
      "learning_rate": 2.9210897134805074e-05,
      "loss": 1.0795,
      "step": 5200
    },
    {
      "epoch": 0.08063455213774424,
      "grad_norm": 3.2676072120666504,
      "learning_rate": 2.920934865979488e-05,
      "loss": 1.2544,
      "step": 5210
    },
    {
      "epoch": 0.08078932095182821,
      "grad_norm": 3.7647545337677,
      "learning_rate": 2.9207800184784684e-05,
      "loss": 0.9898,
      "step": 5220
    },
    {
      "epoch": 0.08094408976591216,
      "grad_norm": 1.5040390491485596,
      "learning_rate": 2.920625170977449e-05,
      "loss": 0.929,
      "step": 5230
    },
    {
      "epoch": 0.08109885857999613,
      "grad_norm": 2.8471107482910156,
      "learning_rate": 2.9204703234764295e-05,
      "loss": 1.0528,
      "step": 5240
    },
    {
      "epoch": 0.0812536273940801,
      "grad_norm": 1.5360230207443237,
      "learning_rate": 2.9203154759754103e-05,
      "loss": 1.1078,
      "step": 5250
    },
    {
      "epoch": 0.08140839620816405,
      "grad_norm": 1.5852679014205933,
      "learning_rate": 2.920160628474391e-05,
      "loss": 1.155,
      "step": 5260
    },
    {
      "epoch": 0.08156316502224802,
      "grad_norm": 2.856823444366455,
      "learning_rate": 2.9200057809733717e-05,
      "loss": 1.1248,
      "step": 5270
    },
    {
      "epoch": 0.08171793383633198,
      "grad_norm": 1.7993539571762085,
      "learning_rate": 2.919850933472352e-05,
      "loss": 1.0414,
      "step": 5280
    },
    {
      "epoch": 0.08187270265041593,
      "grad_norm": 1.164103388786316,
      "learning_rate": 2.9196960859713328e-05,
      "loss": 1.0633,
      "step": 5290
    },
    {
      "epoch": 0.0820274714644999,
      "grad_norm": 1.8373115062713623,
      "learning_rate": 2.919541238470313e-05,
      "loss": 1.232,
      "step": 5300
    },
    {
      "epoch": 0.08218224027858387,
      "grad_norm": 2.428070068359375,
      "learning_rate": 2.919386390969294e-05,
      "loss": 1.1188,
      "step": 5310
    },
    {
      "epoch": 0.08233700909266782,
      "grad_norm": 1.9046283960342407,
      "learning_rate": 2.9192315434682742e-05,
      "loss": 1.0519,
      "step": 5320
    },
    {
      "epoch": 0.08249177790675179,
      "grad_norm": 1.784565806388855,
      "learning_rate": 2.9190766959672553e-05,
      "loss": 1.1606,
      "step": 5330
    },
    {
      "epoch": 0.08264654672083575,
      "grad_norm": 1.3760595321655273,
      "learning_rate": 2.9189218484662357e-05,
      "loss": 1.1658,
      "step": 5340
    },
    {
      "epoch": 0.08280131553491972,
      "grad_norm": 1.7720316648483276,
      "learning_rate": 2.9187670009652164e-05,
      "loss": 1.3307,
      "step": 5350
    },
    {
      "epoch": 0.08295608434900367,
      "grad_norm": 1.5476762056350708,
      "learning_rate": 2.9186121534641967e-05,
      "loss": 1.0551,
      "step": 5360
    },
    {
      "epoch": 0.08311085316308764,
      "grad_norm": 1.4243555068969727,
      "learning_rate": 2.9184573059631775e-05,
      "loss": 1.1647,
      "step": 5370
    },
    {
      "epoch": 0.0832656219771716,
      "grad_norm": 1.671976089477539,
      "learning_rate": 2.918302458462158e-05,
      "loss": 1.0369,
      "step": 5380
    },
    {
      "epoch": 0.08342039079125556,
      "grad_norm": 2.501915454864502,
      "learning_rate": 2.9181476109611382e-05,
      "loss": 1.0659,
      "step": 5390
    },
    {
      "epoch": 0.08357515960533952,
      "grad_norm": 1.5814276933670044,
      "learning_rate": 2.9179927634601193e-05,
      "loss": 1.1917,
      "step": 5400
    },
    {
      "epoch": 0.08372992841942349,
      "grad_norm": 1.9081121683120728,
      "learning_rate": 2.9178379159590996e-05,
      "loss": 1.0916,
      "step": 5410
    },
    {
      "epoch": 0.08388469723350744,
      "grad_norm": 2.5493106842041016,
      "learning_rate": 2.9176830684580804e-05,
      "loss": 1.3221,
      "step": 5420
    },
    {
      "epoch": 0.08403946604759141,
      "grad_norm": 1.5971928834915161,
      "learning_rate": 2.9175282209570607e-05,
      "loss": 0.9839,
      "step": 5430
    },
    {
      "epoch": 0.08419423486167538,
      "grad_norm": 2.3110828399658203,
      "learning_rate": 2.9173733734560414e-05,
      "loss": 1.256,
      "step": 5440
    },
    {
      "epoch": 0.08434900367575933,
      "grad_norm": 2.4529201984405518,
      "learning_rate": 2.9172185259550218e-05,
      "loss": 0.9412,
      "step": 5450
    },
    {
      "epoch": 0.0845037724898433,
      "grad_norm": 3.211940288543701,
      "learning_rate": 2.9170636784540025e-05,
      "loss": 1.1132,
      "step": 5460
    },
    {
      "epoch": 0.08465854130392726,
      "grad_norm": 2.049565553665161,
      "learning_rate": 2.9169088309529832e-05,
      "loss": 1.1061,
      "step": 5470
    },
    {
      "epoch": 0.08481331011801121,
      "grad_norm": 2.549644708633423,
      "learning_rate": 2.916753983451964e-05,
      "loss": 1.1738,
      "step": 5480
    },
    {
      "epoch": 0.08496807893209518,
      "grad_norm": 2.657163381576538,
      "learning_rate": 2.9165991359509443e-05,
      "loss": 1.1117,
      "step": 5490
    },
    {
      "epoch": 0.08512284774617915,
      "grad_norm": 1.902502179145813,
      "learning_rate": 2.916444288449925e-05,
      "loss": 1.0857,
      "step": 5500
    },
    {
      "epoch": 0.0852776165602631,
      "grad_norm": 2.6404688358306885,
      "learning_rate": 2.9162894409489054e-05,
      "loss": 1.0576,
      "step": 5510
    },
    {
      "epoch": 0.08543238537434707,
      "grad_norm": 2.2322885990142822,
      "learning_rate": 2.916134593447886e-05,
      "loss": 1.1415,
      "step": 5520
    },
    {
      "epoch": 0.08558715418843103,
      "grad_norm": 1.9101642370224,
      "learning_rate": 2.9159797459468665e-05,
      "loss": 1.2022,
      "step": 5530
    },
    {
      "epoch": 0.085741923002515,
      "grad_norm": 1.501656413078308,
      "learning_rate": 2.9158248984458476e-05,
      "loss": 1.0058,
      "step": 5540
    },
    {
      "epoch": 0.08589669181659895,
      "grad_norm": 1.5607781410217285,
      "learning_rate": 2.915670050944828e-05,
      "loss": 1.2232,
      "step": 5550
    },
    {
      "epoch": 0.08605146063068292,
      "grad_norm": 1.8821221590042114,
      "learning_rate": 2.9155152034438087e-05,
      "loss": 1.3321,
      "step": 5560
    },
    {
      "epoch": 0.08620622944476689,
      "grad_norm": 1.4908233880996704,
      "learning_rate": 2.915360355942789e-05,
      "loss": 1.0829,
      "step": 5570
    },
    {
      "epoch": 0.08636099825885084,
      "grad_norm": 1.9617215394973755,
      "learning_rate": 2.9152055084417697e-05,
      "loss": 1.1819,
      "step": 5580
    },
    {
      "epoch": 0.0865157670729348,
      "grad_norm": 1.9841034412384033,
      "learning_rate": 2.91505066094075e-05,
      "loss": 1.0067,
      "step": 5590
    },
    {
      "epoch": 0.08667053588701877,
      "grad_norm": 1.9138926267623901,
      "learning_rate": 2.914895813439731e-05,
      "loss": 1.0691,
      "step": 5600
    },
    {
      "epoch": 0.08682530470110272,
      "grad_norm": 2.4281136989593506,
      "learning_rate": 2.9147409659387115e-05,
      "loss": 1.096,
      "step": 5610
    },
    {
      "epoch": 0.08698007351518669,
      "grad_norm": 2.0495853424072266,
      "learning_rate": 2.9145861184376923e-05,
      "loss": 1.1246,
      "step": 5620
    },
    {
      "epoch": 0.08713484232927066,
      "grad_norm": 2.8465287685394287,
      "learning_rate": 2.9144312709366726e-05,
      "loss": 1.125,
      "step": 5630
    },
    {
      "epoch": 0.08728961114335461,
      "grad_norm": 1.6590065956115723,
      "learning_rate": 2.914276423435653e-05,
      "loss": 1.0491,
      "step": 5640
    },
    {
      "epoch": 0.08744437995743858,
      "grad_norm": 2.5145981311798096,
      "learning_rate": 2.9141215759346337e-05,
      "loss": 1.0128,
      "step": 5650
    },
    {
      "epoch": 0.08759914877152254,
      "grad_norm": 2.3046891689300537,
      "learning_rate": 2.913966728433614e-05,
      "loss": 1.1161,
      "step": 5660
    },
    {
      "epoch": 0.0877539175856065,
      "grad_norm": 1.6034505367279053,
      "learning_rate": 2.913811880932595e-05,
      "loss": 1.0298,
      "step": 5670
    },
    {
      "epoch": 0.08790868639969046,
      "grad_norm": 1.5722743272781372,
      "learning_rate": 2.9136570334315755e-05,
      "loss": 1.0965,
      "step": 5680
    },
    {
      "epoch": 0.08806345521377443,
      "grad_norm": 2.4558091163635254,
      "learning_rate": 2.9135021859305562e-05,
      "loss": 0.9875,
      "step": 5690
    },
    {
      "epoch": 0.08821822402785838,
      "grad_norm": 3.3222758769989014,
      "learning_rate": 2.9133473384295366e-05,
      "loss": 1.1803,
      "step": 5700
    },
    {
      "epoch": 0.08837299284194235,
      "grad_norm": 1.9011023044586182,
      "learning_rate": 2.9131924909285173e-05,
      "loss": 1.2409,
      "step": 5710
    },
    {
      "epoch": 0.08852776165602631,
      "grad_norm": 1.1847425699234009,
      "learning_rate": 2.9130376434274977e-05,
      "loss": 1.1065,
      "step": 5720
    },
    {
      "epoch": 0.08868253047011028,
      "grad_norm": 1.6367067098617554,
      "learning_rate": 2.9128827959264784e-05,
      "loss": 1.192,
      "step": 5730
    },
    {
      "epoch": 0.08883729928419423,
      "grad_norm": 2.3120028972625732,
      "learning_rate": 2.912727948425459e-05,
      "loss": 1.1501,
      "step": 5740
    },
    {
      "epoch": 0.0889920680982782,
      "grad_norm": 2.0114316940307617,
      "learning_rate": 2.91257310092444e-05,
      "loss": 1.165,
      "step": 5750
    },
    {
      "epoch": 0.08914683691236217,
      "grad_norm": 2.7159831523895264,
      "learning_rate": 2.9124182534234202e-05,
      "loss": 1.1377,
      "step": 5760
    },
    {
      "epoch": 0.08930160572644612,
      "grad_norm": 1.6500940322875977,
      "learning_rate": 2.912263405922401e-05,
      "loss": 1.1672,
      "step": 5770
    },
    {
      "epoch": 0.08945637454053008,
      "grad_norm": 2.4577090740203857,
      "learning_rate": 2.9121085584213813e-05,
      "loss": 1.0628,
      "step": 5780
    },
    {
      "epoch": 0.08961114335461405,
      "grad_norm": 2.771421194076538,
      "learning_rate": 2.911953710920362e-05,
      "loss": 0.9566,
      "step": 5790
    },
    {
      "epoch": 0.089765912168698,
      "grad_norm": 2.3545982837677,
      "learning_rate": 2.9117988634193424e-05,
      "loss": 1.0649,
      "step": 5800
    },
    {
      "epoch": 0.08992068098278197,
      "grad_norm": 2.162152051925659,
      "learning_rate": 2.9116440159183235e-05,
      "loss": 1.3106,
      "step": 5810
    },
    {
      "epoch": 0.09007544979686594,
      "grad_norm": 1.2010432481765747,
      "learning_rate": 2.9114891684173038e-05,
      "loss": 1.0035,
      "step": 5820
    },
    {
      "epoch": 0.09023021861094989,
      "grad_norm": 1.5583994388580322,
      "learning_rate": 2.9113343209162845e-05,
      "loss": 1.0768,
      "step": 5830
    },
    {
      "epoch": 0.09038498742503386,
      "grad_norm": 2.3556151390075684,
      "learning_rate": 2.911179473415265e-05,
      "loss": 1.0397,
      "step": 5840
    },
    {
      "epoch": 0.09053975623911782,
      "grad_norm": 2.1110293865203857,
      "learning_rate": 2.9110246259142456e-05,
      "loss": 1.1272,
      "step": 5850
    },
    {
      "epoch": 0.09069452505320177,
      "grad_norm": 2.528032064437866,
      "learning_rate": 2.910869778413226e-05,
      "loss": 1.2708,
      "step": 5860
    },
    {
      "epoch": 0.09084929386728574,
      "grad_norm": 2.4569175243377686,
      "learning_rate": 2.9107149309122067e-05,
      "loss": 0.9527,
      "step": 5870
    },
    {
      "epoch": 0.09100406268136971,
      "grad_norm": 3.133763074874878,
      "learning_rate": 2.9105600834111874e-05,
      "loss": 1.2467,
      "step": 5880
    },
    {
      "epoch": 0.09115883149545366,
      "grad_norm": 2.1498000621795654,
      "learning_rate": 2.9104052359101678e-05,
      "loss": 1.0936,
      "step": 5890
    },
    {
      "epoch": 0.09131360030953763,
      "grad_norm": 1.7177623510360718,
      "learning_rate": 2.9102503884091485e-05,
      "loss": 1.1529,
      "step": 5900
    },
    {
      "epoch": 0.09146836912362159,
      "grad_norm": 1.517892599105835,
      "learning_rate": 2.910095540908129e-05,
      "loss": 1.1583,
      "step": 5910
    },
    {
      "epoch": 0.09162313793770555,
      "grad_norm": 1.5663810968399048,
      "learning_rate": 2.9099406934071096e-05,
      "loss": 1.266,
      "step": 5920
    },
    {
      "epoch": 0.09177790675178951,
      "grad_norm": 2.4284873008728027,
      "learning_rate": 2.90978584590609e-05,
      "loss": 1.1686,
      "step": 5930
    },
    {
      "epoch": 0.09193267556587348,
      "grad_norm": 2.917198896408081,
      "learning_rate": 2.9096309984050707e-05,
      "loss": 1.1082,
      "step": 5940
    },
    {
      "epoch": 0.09208744437995744,
      "grad_norm": 2.442816972732544,
      "learning_rate": 2.9094761509040514e-05,
      "loss": 1.3049,
      "step": 5950
    },
    {
      "epoch": 0.0922422131940414,
      "grad_norm": 1.3376715183258057,
      "learning_rate": 2.909321303403032e-05,
      "loss": 1.1143,
      "step": 5960
    },
    {
      "epoch": 0.09239698200812536,
      "grad_norm": 1.8129057884216309,
      "learning_rate": 2.9091664559020125e-05,
      "loss": 0.882,
      "step": 5970
    },
    {
      "epoch": 0.09255175082220933,
      "grad_norm": 2.381422996520996,
      "learning_rate": 2.9090116084009932e-05,
      "loss": 1.1282,
      "step": 5980
    },
    {
      "epoch": 0.09270651963629328,
      "grad_norm": 1.8035935163497925,
      "learning_rate": 2.9088567608999736e-05,
      "loss": 1.1358,
      "step": 5990
    },
    {
      "epoch": 0.09286128845037725,
      "grad_norm": 2.0158607959747314,
      "learning_rate": 2.9087019133989543e-05,
      "loss": 1.0323,
      "step": 6000
    },
    {
      "epoch": 0.09301605726446122,
      "grad_norm": 1.4317234754562378,
      "learning_rate": 2.9085470658979347e-05,
      "loss": 1.0378,
      "step": 6010
    },
    {
      "epoch": 0.09317082607854517,
      "grad_norm": 2.8381245136260986,
      "learning_rate": 2.9083922183969157e-05,
      "loss": 0.8834,
      "step": 6020
    },
    {
      "epoch": 0.09332559489262914,
      "grad_norm": 1.1528879404067993,
      "learning_rate": 2.908237370895896e-05,
      "loss": 1.2186,
      "step": 6030
    },
    {
      "epoch": 0.0934803637067131,
      "grad_norm": 2.480767011642456,
      "learning_rate": 2.9080825233948768e-05,
      "loss": 1.0491,
      "step": 6040
    },
    {
      "epoch": 0.09363513252079705,
      "grad_norm": 2.5483741760253906,
      "learning_rate": 2.9079276758938572e-05,
      "loss": 1.032,
      "step": 6050
    },
    {
      "epoch": 0.09378990133488102,
      "grad_norm": 1.6598174571990967,
      "learning_rate": 2.907772828392838e-05,
      "loss": 0.9488,
      "step": 6060
    },
    {
      "epoch": 0.09394467014896499,
      "grad_norm": 1.4996130466461182,
      "learning_rate": 2.9076179808918183e-05,
      "loss": 0.9415,
      "step": 6070
    },
    {
      "epoch": 0.09409943896304894,
      "grad_norm": 1.7678815126419067,
      "learning_rate": 2.907463133390799e-05,
      "loss": 1.1247,
      "step": 6080
    },
    {
      "epoch": 0.0942542077771329,
      "grad_norm": 2.0095911026000977,
      "learning_rate": 2.9073082858897797e-05,
      "loss": 1.2105,
      "step": 6090
    },
    {
      "epoch": 0.09440897659121687,
      "grad_norm": 2.511319637298584,
      "learning_rate": 2.9071534383887604e-05,
      "loss": 0.9658,
      "step": 6100
    },
    {
      "epoch": 0.09456374540530083,
      "grad_norm": 1.9565880298614502,
      "learning_rate": 2.9069985908877408e-05,
      "loss": 1.1827,
      "step": 6110
    },
    {
      "epoch": 0.09471851421938479,
      "grad_norm": 1.7863949537277222,
      "learning_rate": 2.9068437433867215e-05,
      "loss": 1.0085,
      "step": 6120
    },
    {
      "epoch": 0.09487328303346876,
      "grad_norm": 1.4530483484268188,
      "learning_rate": 2.906688895885702e-05,
      "loss": 1.0558,
      "step": 6130
    },
    {
      "epoch": 0.09502805184755272,
      "grad_norm": 2.022047519683838,
      "learning_rate": 2.9065340483846823e-05,
      "loss": 1.0244,
      "step": 6140
    },
    {
      "epoch": 0.09518282066163668,
      "grad_norm": 1.4570647478103638,
      "learning_rate": 2.9063792008836633e-05,
      "loss": 1.027,
      "step": 6150
    },
    {
      "epoch": 0.09533758947572064,
      "grad_norm": 2.461503267288208,
      "learning_rate": 2.9062243533826437e-05,
      "loss": 1.0741,
      "step": 6160
    },
    {
      "epoch": 0.09549235828980461,
      "grad_norm": 1.440167784690857,
      "learning_rate": 2.9060695058816244e-05,
      "loss": 1.3298,
      "step": 6170
    },
    {
      "epoch": 0.09564712710388856,
      "grad_norm": 1.909550666809082,
      "learning_rate": 2.9059146583806048e-05,
      "loss": 1.0677,
      "step": 6180
    },
    {
      "epoch": 0.09580189591797253,
      "grad_norm": 1.259766697883606,
      "learning_rate": 2.9057598108795855e-05,
      "loss": 1.0111,
      "step": 6190
    },
    {
      "epoch": 0.0959566647320565,
      "grad_norm": 3.061880350112915,
      "learning_rate": 2.905604963378566e-05,
      "loss": 1.0319,
      "step": 6200
    },
    {
      "epoch": 0.09611143354614045,
      "grad_norm": 1.907743215560913,
      "learning_rate": 2.9054501158775466e-05,
      "loss": 1.2231,
      "step": 6210
    },
    {
      "epoch": 0.09626620236022441,
      "grad_norm": 2.4884026050567627,
      "learning_rate": 2.9052952683765273e-05,
      "loss": 1.0791,
      "step": 6220
    },
    {
      "epoch": 0.09642097117430838,
      "grad_norm": 1.5133588314056396,
      "learning_rate": 2.905140420875508e-05,
      "loss": 1.0659,
      "step": 6230
    },
    {
      "epoch": 0.09657573998839233,
      "grad_norm": 2.405930757522583,
      "learning_rate": 2.9049855733744884e-05,
      "loss": 1.1247,
      "step": 6240
    },
    {
      "epoch": 0.0967305088024763,
      "grad_norm": 1.6651345491409302,
      "learning_rate": 2.904830725873469e-05,
      "loss": 1.0233,
      "step": 6250
    },
    {
      "epoch": 0.09688527761656027,
      "grad_norm": 1.7644007205963135,
      "learning_rate": 2.9046758783724495e-05,
      "loss": 1.0326,
      "step": 6260
    },
    {
      "epoch": 0.09704004643064422,
      "grad_norm": 2.1152775287628174,
      "learning_rate": 2.9045210308714302e-05,
      "loss": 1.2629,
      "step": 6270
    },
    {
      "epoch": 0.09719481524472819,
      "grad_norm": 2.35136342048645,
      "learning_rate": 2.9043661833704106e-05,
      "loss": 1.2409,
      "step": 6280
    },
    {
      "epoch": 0.09734958405881215,
      "grad_norm": 2.2748827934265137,
      "learning_rate": 2.9042113358693916e-05,
      "loss": 1.3274,
      "step": 6290
    },
    {
      "epoch": 0.0975043528728961,
      "grad_norm": 2.285334587097168,
      "learning_rate": 2.904056488368372e-05,
      "loss": 1.0425,
      "step": 6300
    },
    {
      "epoch": 0.09765912168698007,
      "grad_norm": 1.8017337322235107,
      "learning_rate": 2.9039016408673527e-05,
      "loss": 1.1317,
      "step": 6310
    },
    {
      "epoch": 0.09781389050106404,
      "grad_norm": 1.3340421915054321,
      "learning_rate": 2.903746793366333e-05,
      "loss": 1.2335,
      "step": 6320
    },
    {
      "epoch": 0.097968659315148,
      "grad_norm": 1.6433569192886353,
      "learning_rate": 2.9035919458653138e-05,
      "loss": 1.3059,
      "step": 6330
    },
    {
      "epoch": 0.09812342812923196,
      "grad_norm": 2.5086307525634766,
      "learning_rate": 2.9034370983642942e-05,
      "loss": 0.9078,
      "step": 6340
    },
    {
      "epoch": 0.09827819694331592,
      "grad_norm": 2.262885570526123,
      "learning_rate": 2.903282250863275e-05,
      "loss": 1.0746,
      "step": 6350
    },
    {
      "epoch": 0.09843296575739989,
      "grad_norm": 1.8623764514923096,
      "learning_rate": 2.9031274033622556e-05,
      "loss": 1.0737,
      "step": 6360
    },
    {
      "epoch": 0.09858773457148384,
      "grad_norm": 2.085050106048584,
      "learning_rate": 2.9029725558612363e-05,
      "loss": 1.0229,
      "step": 6370
    },
    {
      "epoch": 0.09874250338556781,
      "grad_norm": 2.772764205932617,
      "learning_rate": 2.9028177083602167e-05,
      "loss": 1.0331,
      "step": 6380
    },
    {
      "epoch": 0.09889727219965178,
      "grad_norm": 1.8445076942443848,
      "learning_rate": 2.902662860859197e-05,
      "loss": 1.1264,
      "step": 6390
    },
    {
      "epoch": 0.09905204101373573,
      "grad_norm": 2.2543742656707764,
      "learning_rate": 2.9025080133581778e-05,
      "loss": 1.1621,
      "step": 6400
    },
    {
      "epoch": 0.0992068098278197,
      "grad_norm": 1.3302258253097534,
      "learning_rate": 2.902353165857158e-05,
      "loss": 1.1734,
      "step": 6410
    },
    {
      "epoch": 0.09936157864190366,
      "grad_norm": 1.8273590803146362,
      "learning_rate": 2.902198318356139e-05,
      "loss": 1.0329,
      "step": 6420
    },
    {
      "epoch": 0.09951634745598761,
      "grad_norm": 1.4294887781143188,
      "learning_rate": 2.9020434708551196e-05,
      "loss": 1.207,
      "step": 6430
    },
    {
      "epoch": 0.09967111627007158,
      "grad_norm": 2.7845139503479004,
      "learning_rate": 2.9018886233541003e-05,
      "loss": 0.8781,
      "step": 6440
    },
    {
      "epoch": 0.09982588508415555,
      "grad_norm": 2.641725778579712,
      "learning_rate": 2.9017337758530807e-05,
      "loss": 0.9864,
      "step": 6450
    },
    {
      "epoch": 0.0999806538982395,
      "grad_norm": 2.1515820026397705,
      "learning_rate": 2.9015789283520614e-05,
      "loss": 1.0799,
      "step": 6460
    },
    {
      "epoch": 0.10013542271232347,
      "grad_norm": 2.9650797843933105,
      "learning_rate": 2.9014240808510418e-05,
      "loss": 1.1416,
      "step": 6470
    },
    {
      "epoch": 0.10029019152640743,
      "grad_norm": 2.365084171295166,
      "learning_rate": 2.9012692333500225e-05,
      "loss": 1.1308,
      "step": 6480
    },
    {
      "epoch": 0.10044496034049138,
      "grad_norm": 1.446408987045288,
      "learning_rate": 2.901114385849003e-05,
      "loss": 1.1028,
      "step": 6490
    },
    {
      "epoch": 0.10059972915457535,
      "grad_norm": 1.6849273443222046,
      "learning_rate": 2.900959538347984e-05,
      "loss": 1.3035,
      "step": 6500
    },
    {
      "epoch": 0.10075449796865932,
      "grad_norm": 1.457894206047058,
      "learning_rate": 2.9008046908469643e-05,
      "loss": 1.2053,
      "step": 6510
    },
    {
      "epoch": 0.10090926678274327,
      "grad_norm": 2.292660713195801,
      "learning_rate": 2.900649843345945e-05,
      "loss": 1.0609,
      "step": 6520
    },
    {
      "epoch": 0.10106403559682724,
      "grad_norm": 1.4176268577575684,
      "learning_rate": 2.9004949958449254e-05,
      "loss": 1.1809,
      "step": 6530
    },
    {
      "epoch": 0.1012188044109112,
      "grad_norm": 1.4491262435913086,
      "learning_rate": 2.900340148343906e-05,
      "loss": 1.1093,
      "step": 6540
    },
    {
      "epoch": 0.10137357322499517,
      "grad_norm": 1.7853894233703613,
      "learning_rate": 2.9001853008428865e-05,
      "loss": 1.1262,
      "step": 6550
    },
    {
      "epoch": 0.10152834203907912,
      "grad_norm": 3.4262161254882812,
      "learning_rate": 2.9000304533418675e-05,
      "loss": 1.1492,
      "step": 6560
    },
    {
      "epoch": 0.10168311085316309,
      "grad_norm": 1.2750524282455444,
      "learning_rate": 2.899875605840848e-05,
      "loss": 1.0148,
      "step": 6570
    },
    {
      "epoch": 0.10183787966724706,
      "grad_norm": 1.457301139831543,
      "learning_rate": 2.8997207583398286e-05,
      "loss": 1.1403,
      "step": 6580
    },
    {
      "epoch": 0.10199264848133101,
      "grad_norm": 2.409376859664917,
      "learning_rate": 2.899565910838809e-05,
      "loss": 1.1861,
      "step": 6590
    },
    {
      "epoch": 0.10214741729541497,
      "grad_norm": 2.0559933185577393,
      "learning_rate": 2.8994110633377897e-05,
      "loss": 1.1829,
      "step": 6600
    },
    {
      "epoch": 0.10230218610949894,
      "grad_norm": 1.9440326690673828,
      "learning_rate": 2.89925621583677e-05,
      "loss": 1.0543,
      "step": 6610
    },
    {
      "epoch": 0.1024569549235829,
      "grad_norm": 1.2639714479446411,
      "learning_rate": 2.8991013683357508e-05,
      "loss": 1.183,
      "step": 6620
    },
    {
      "epoch": 0.10261172373766686,
      "grad_norm": 2.270766019821167,
      "learning_rate": 2.8989465208347315e-05,
      "loss": 1.014,
      "step": 6630
    },
    {
      "epoch": 0.10276649255175083,
      "grad_norm": 1.5404083728790283,
      "learning_rate": 2.8987916733337122e-05,
      "loss": 1.1722,
      "step": 6640
    },
    {
      "epoch": 0.10292126136583478,
      "grad_norm": 2.6616313457489014,
      "learning_rate": 2.8986368258326926e-05,
      "loss": 1.0534,
      "step": 6650
    },
    {
      "epoch": 0.10307603017991875,
      "grad_norm": 1.622918963432312,
      "learning_rate": 2.898481978331673e-05,
      "loss": 1.0909,
      "step": 6660
    },
    {
      "epoch": 0.10323079899400271,
      "grad_norm": 1.9175467491149902,
      "learning_rate": 2.8983271308306537e-05,
      "loss": 1.0223,
      "step": 6670
    },
    {
      "epoch": 0.10338556780808666,
      "grad_norm": 2.232508420944214,
      "learning_rate": 2.898172283329634e-05,
      "loss": 1.0815,
      "step": 6680
    },
    {
      "epoch": 0.10354033662217063,
      "grad_norm": 1.464032530784607,
      "learning_rate": 2.8980174358286148e-05,
      "loss": 1.1684,
      "step": 6690
    },
    {
      "epoch": 0.1036951054362546,
      "grad_norm": 1.5508699417114258,
      "learning_rate": 2.8978625883275955e-05,
      "loss": 1.1494,
      "step": 6700
    },
    {
      "epoch": 0.10384987425033855,
      "grad_norm": 1.9356374740600586,
      "learning_rate": 2.8977077408265762e-05,
      "loss": 1.2436,
      "step": 6710
    },
    {
      "epoch": 0.10400464306442252,
      "grad_norm": 2.0358023643493652,
      "learning_rate": 2.8975528933255566e-05,
      "loss": 1.1422,
      "step": 6720
    },
    {
      "epoch": 0.10415941187850648,
      "grad_norm": 1.9699786901474,
      "learning_rate": 2.8973980458245373e-05,
      "loss": 0.9816,
      "step": 6730
    },
    {
      "epoch": 0.10431418069259045,
      "grad_norm": 1.2409586906433105,
      "learning_rate": 2.8972431983235176e-05,
      "loss": 1.017,
      "step": 6740
    },
    {
      "epoch": 0.1044689495066744,
      "grad_norm": 2.004796266555786,
      "learning_rate": 2.8970883508224984e-05,
      "loss": 1.1805,
      "step": 6750
    },
    {
      "epoch": 0.10462371832075837,
      "grad_norm": 1.695522427558899,
      "learning_rate": 2.8969335033214787e-05,
      "loss": 1.1784,
      "step": 6760
    },
    {
      "epoch": 0.10477848713484234,
      "grad_norm": 1.4939781427383423,
      "learning_rate": 2.8967786558204598e-05,
      "loss": 1.147,
      "step": 6770
    },
    {
      "epoch": 0.10493325594892629,
      "grad_norm": 2.0072994232177734,
      "learning_rate": 2.89662380831944e-05,
      "loss": 1.1256,
      "step": 6780
    },
    {
      "epoch": 0.10508802476301025,
      "grad_norm": 1.6544567346572876,
      "learning_rate": 2.896468960818421e-05,
      "loss": 1.1411,
      "step": 6790
    },
    {
      "epoch": 0.10524279357709422,
      "grad_norm": 1.7691320180892944,
      "learning_rate": 2.8963141133174013e-05,
      "loss": 1.2813,
      "step": 6800
    },
    {
      "epoch": 0.10539756239117817,
      "grad_norm": 1.7342473268508911,
      "learning_rate": 2.896159265816382e-05,
      "loss": 1.0262,
      "step": 6810
    },
    {
      "epoch": 0.10555233120526214,
      "grad_norm": 2.3850173950195312,
      "learning_rate": 2.8960044183153623e-05,
      "loss": 1.0905,
      "step": 6820
    },
    {
      "epoch": 0.1057071000193461,
      "grad_norm": 2.5411341190338135,
      "learning_rate": 2.895849570814343e-05,
      "loss": 1.1037,
      "step": 6830
    },
    {
      "epoch": 0.10586186883343006,
      "grad_norm": 2.64091157913208,
      "learning_rate": 2.8956947233133238e-05,
      "loss": 1.2734,
      "step": 6840
    },
    {
      "epoch": 0.10601663764751403,
      "grad_norm": 1.8800703287124634,
      "learning_rate": 2.8955398758123045e-05,
      "loss": 1.1916,
      "step": 6850
    },
    {
      "epoch": 0.10617140646159799,
      "grad_norm": 1.9105939865112305,
      "learning_rate": 2.895385028311285e-05,
      "loss": 1.1618,
      "step": 6860
    },
    {
      "epoch": 0.10632617527568194,
      "grad_norm": 1.6334047317504883,
      "learning_rate": 2.8952301808102656e-05,
      "loss": 1.0581,
      "step": 6870
    },
    {
      "epoch": 0.10648094408976591,
      "grad_norm": 1.635768175125122,
      "learning_rate": 2.895075333309246e-05,
      "loss": 1.0623,
      "step": 6880
    },
    {
      "epoch": 0.10663571290384988,
      "grad_norm": 1.3215031623840332,
      "learning_rate": 2.8949204858082267e-05,
      "loss": 1.2163,
      "step": 6890
    },
    {
      "epoch": 0.10679048171793383,
      "grad_norm": 1.3676992654800415,
      "learning_rate": 2.894765638307207e-05,
      "loss": 1.0577,
      "step": 6900
    },
    {
      "epoch": 0.1069452505320178,
      "grad_norm": 2.603893995285034,
      "learning_rate": 2.8946107908061877e-05,
      "loss": 1.1562,
      "step": 6910
    },
    {
      "epoch": 0.10710001934610176,
      "grad_norm": 2.2664601802825928,
      "learning_rate": 2.8944559433051685e-05,
      "loss": 1.1714,
      "step": 6920
    },
    {
      "epoch": 0.10725478816018573,
      "grad_norm": 1.7702897787094116,
      "learning_rate": 2.894301095804149e-05,
      "loss": 1.105,
      "step": 6930
    },
    {
      "epoch": 0.10740955697426968,
      "grad_norm": 1.530448079109192,
      "learning_rate": 2.8941462483031296e-05,
      "loss": 1.0945,
      "step": 6940
    },
    {
      "epoch": 0.10756432578835365,
      "grad_norm": 1.2875641584396362,
      "learning_rate": 2.89399140080211e-05,
      "loss": 1.1268,
      "step": 6950
    },
    {
      "epoch": 0.10771909460243762,
      "grad_norm": 1.7916210889816284,
      "learning_rate": 2.8938365533010906e-05,
      "loss": 1.1796,
      "step": 6960
    },
    {
      "epoch": 0.10787386341652157,
      "grad_norm": 2.596496343612671,
      "learning_rate": 2.8936817058000714e-05,
      "loss": 1.1611,
      "step": 6970
    },
    {
      "epoch": 0.10802863223060553,
      "grad_norm": 1.7137988805770874,
      "learning_rate": 2.893526858299052e-05,
      "loss": 1.0142,
      "step": 6980
    },
    {
      "epoch": 0.1081834010446895,
      "grad_norm": 2.6839566230773926,
      "learning_rate": 2.8933720107980324e-05,
      "loss": 1.0311,
      "step": 6990
    },
    {
      "epoch": 0.10833816985877345,
      "grad_norm": 2.665621757507324,
      "learning_rate": 2.893217163297013e-05,
      "loss": 1.1604,
      "step": 7000
    },
    {
      "epoch": 0.10849293867285742,
      "grad_norm": 2.7318413257598877,
      "learning_rate": 2.8930623157959935e-05,
      "loss": 1.0382,
      "step": 7010
    },
    {
      "epoch": 0.10864770748694139,
      "grad_norm": 1.5860974788665771,
      "learning_rate": 2.8929074682949742e-05,
      "loss": 1.1566,
      "step": 7020
    },
    {
      "epoch": 0.10880247630102534,
      "grad_norm": 1.8170055150985718,
      "learning_rate": 2.8927526207939546e-05,
      "loss": 0.9528,
      "step": 7030
    },
    {
      "epoch": 0.1089572451151093,
      "grad_norm": 2.024660348892212,
      "learning_rate": 2.8925977732929357e-05,
      "loss": 1.0803,
      "step": 7040
    },
    {
      "epoch": 0.10911201392919327,
      "grad_norm": 1.5253819227218628,
      "learning_rate": 2.892442925791916e-05,
      "loss": 1.1166,
      "step": 7050
    },
    {
      "epoch": 0.10926678274327722,
      "grad_norm": 1.998036503791809,
      "learning_rate": 2.8922880782908968e-05,
      "loss": 1.0347,
      "step": 7060
    },
    {
      "epoch": 0.10942155155736119,
      "grad_norm": 3.0329277515411377,
      "learning_rate": 2.892133230789877e-05,
      "loss": 0.9883,
      "step": 7070
    },
    {
      "epoch": 0.10957632037144516,
      "grad_norm": 1.8709508180618286,
      "learning_rate": 2.891978383288858e-05,
      "loss": 1.1068,
      "step": 7080
    },
    {
      "epoch": 0.10973108918552911,
      "grad_norm": 2.22719407081604,
      "learning_rate": 2.8918235357878382e-05,
      "loss": 1.1402,
      "step": 7090
    },
    {
      "epoch": 0.10988585799961308,
      "grad_norm": 1.5408611297607422,
      "learning_rate": 2.891668688286819e-05,
      "loss": 1.0539,
      "step": 7100
    },
    {
      "epoch": 0.11004062681369704,
      "grad_norm": 1.8437656164169312,
      "learning_rate": 2.8915138407857997e-05,
      "loss": 0.9719,
      "step": 7110
    },
    {
      "epoch": 0.110195395627781,
      "grad_norm": 2.0855159759521484,
      "learning_rate": 2.8913589932847804e-05,
      "loss": 1.1012,
      "step": 7120
    },
    {
      "epoch": 0.11035016444186496,
      "grad_norm": 2.94421648979187,
      "learning_rate": 2.8912041457837607e-05,
      "loss": 1.2301,
      "step": 7130
    },
    {
      "epoch": 0.11050493325594893,
      "grad_norm": 1.7216180562973022,
      "learning_rate": 2.8910492982827415e-05,
      "loss": 1.0975,
      "step": 7140
    },
    {
      "epoch": 0.1106597020700329,
      "grad_norm": 1.5503828525543213,
      "learning_rate": 2.890894450781722e-05,
      "loss": 1.2115,
      "step": 7150
    },
    {
      "epoch": 0.11081447088411685,
      "grad_norm": 1.8923420906066895,
      "learning_rate": 2.8907396032807022e-05,
      "loss": 1.0375,
      "step": 7160
    },
    {
      "epoch": 0.11096923969820081,
      "grad_norm": 1.898105502128601,
      "learning_rate": 2.890584755779683e-05,
      "loss": 1.2294,
      "step": 7170
    },
    {
      "epoch": 0.11112400851228478,
      "grad_norm": 2.9035751819610596,
      "learning_rate": 2.8904299082786636e-05,
      "loss": 1.1076,
      "step": 7180
    },
    {
      "epoch": 0.11127877732636873,
      "grad_norm": 1.681166172027588,
      "learning_rate": 2.8902750607776443e-05,
      "loss": 0.9746,
      "step": 7190
    },
    {
      "epoch": 0.1114335461404527,
      "grad_norm": 1.9309786558151245,
      "learning_rate": 2.8901202132766247e-05,
      "loss": 1.1677,
      "step": 7200
    },
    {
      "epoch": 0.11158831495453667,
      "grad_norm": 1.7444239854812622,
      "learning_rate": 2.8899653657756054e-05,
      "loss": 1.0366,
      "step": 7210
    },
    {
      "epoch": 0.11174308376862062,
      "grad_norm": 2.0631041526794434,
      "learning_rate": 2.8898105182745858e-05,
      "loss": 1.0081,
      "step": 7220
    },
    {
      "epoch": 0.11189785258270459,
      "grad_norm": 2.4918668270111084,
      "learning_rate": 2.8896556707735665e-05,
      "loss": 1.1092,
      "step": 7230
    },
    {
      "epoch": 0.11205262139678855,
      "grad_norm": 2.2296478748321533,
      "learning_rate": 2.889500823272547e-05,
      "loss": 1.1319,
      "step": 7240
    },
    {
      "epoch": 0.1122073902108725,
      "grad_norm": 2.096331834793091,
      "learning_rate": 2.889345975771528e-05,
      "loss": 0.971,
      "step": 7250
    },
    {
      "epoch": 0.11236215902495647,
      "grad_norm": 2.2349815368652344,
      "learning_rate": 2.8891911282705083e-05,
      "loss": 1.2375,
      "step": 7260
    },
    {
      "epoch": 0.11251692783904044,
      "grad_norm": 2.236663341522217,
      "learning_rate": 2.889036280769489e-05,
      "loss": 1.019,
      "step": 7270
    },
    {
      "epoch": 0.11267169665312439,
      "grad_norm": 1.1461398601531982,
      "learning_rate": 2.8888814332684694e-05,
      "loss": 1.1565,
      "step": 7280
    },
    {
      "epoch": 0.11282646546720836,
      "grad_norm": 2.343815803527832,
      "learning_rate": 2.88872658576745e-05,
      "loss": 1.1468,
      "step": 7290
    },
    {
      "epoch": 0.11298123428129232,
      "grad_norm": 1.262494683265686,
      "learning_rate": 2.8885717382664305e-05,
      "loss": 1.1209,
      "step": 7300
    },
    {
      "epoch": 0.11313600309537628,
      "grad_norm": 2.434056043624878,
      "learning_rate": 2.8884168907654112e-05,
      "loss": 1.0188,
      "step": 7310
    },
    {
      "epoch": 0.11329077190946024,
      "grad_norm": 1.330946683883667,
      "learning_rate": 2.888262043264392e-05,
      "loss": 1.19,
      "step": 7320
    },
    {
      "epoch": 0.11344554072354421,
      "grad_norm": 2.1335248947143555,
      "learning_rate": 2.8881071957633726e-05,
      "loss": 1.1484,
      "step": 7330
    },
    {
      "epoch": 0.11360030953762817,
      "grad_norm": 1.5325443744659424,
      "learning_rate": 2.887952348262353e-05,
      "loss": 1.2285,
      "step": 7340
    },
    {
      "epoch": 0.11375507835171213,
      "grad_norm": 1.4534149169921875,
      "learning_rate": 2.8877975007613337e-05,
      "loss": 1.3605,
      "step": 7350
    },
    {
      "epoch": 0.1139098471657961,
      "grad_norm": 2.0990729331970215,
      "learning_rate": 2.887642653260314e-05,
      "loss": 1.0669,
      "step": 7360
    },
    {
      "epoch": 0.11406461597988006,
      "grad_norm": 2.335658550262451,
      "learning_rate": 2.8874878057592948e-05,
      "loss": 1.1032,
      "step": 7370
    },
    {
      "epoch": 0.11421938479396401,
      "grad_norm": 2.6641576290130615,
      "learning_rate": 2.8873329582582752e-05,
      "loss": 1.1052,
      "step": 7380
    },
    {
      "epoch": 0.11437415360804798,
      "grad_norm": 1.446523666381836,
      "learning_rate": 2.8871781107572563e-05,
      "loss": 1.1779,
      "step": 7390
    },
    {
      "epoch": 0.11452892242213195,
      "grad_norm": 1.441505789756775,
      "learning_rate": 2.8870232632562366e-05,
      "loss": 0.8835,
      "step": 7400
    },
    {
      "epoch": 0.1146836912362159,
      "grad_norm": 2.3683230876922607,
      "learning_rate": 2.886868415755217e-05,
      "loss": 1.174,
      "step": 7410
    },
    {
      "epoch": 0.11483846005029986,
      "grad_norm": 1.8510485887527466,
      "learning_rate": 2.8867135682541977e-05,
      "loss": 1.0448,
      "step": 7420
    },
    {
      "epoch": 0.11499322886438383,
      "grad_norm": 1.1149998903274536,
      "learning_rate": 2.886558720753178e-05,
      "loss": 0.9641,
      "step": 7430
    },
    {
      "epoch": 0.11514799767846778,
      "grad_norm": 1.667864203453064,
      "learning_rate": 2.8864038732521588e-05,
      "loss": 1.209,
      "step": 7440
    },
    {
      "epoch": 0.11530276649255175,
      "grad_norm": 2.582063674926758,
      "learning_rate": 2.8862490257511395e-05,
      "loss": 1.0919,
      "step": 7450
    },
    {
      "epoch": 0.11545753530663572,
      "grad_norm": 1.7772389650344849,
      "learning_rate": 2.8860941782501202e-05,
      "loss": 1.0701,
      "step": 7460
    },
    {
      "epoch": 0.11561230412071967,
      "grad_norm": 1.5788276195526123,
      "learning_rate": 2.8859393307491006e-05,
      "loss": 1.2278,
      "step": 7470
    },
    {
      "epoch": 0.11576707293480364,
      "grad_norm": 1.933605432510376,
      "learning_rate": 2.8857844832480813e-05,
      "loss": 1.0687,
      "step": 7480
    },
    {
      "epoch": 0.1159218417488876,
      "grad_norm": 1.4851287603378296,
      "learning_rate": 2.8856296357470617e-05,
      "loss": 1.1486,
      "step": 7490
    },
    {
      "epoch": 0.11607661056297155,
      "grad_norm": 1.7370312213897705,
      "learning_rate": 2.8854902729961443e-05,
      "loss": 1.2605,
      "step": 7500
    },
    {
      "epoch": 0.11623137937705552,
      "grad_norm": 2.4781746864318848,
      "learning_rate": 2.885335425495125e-05,
      "loss": 0.9777,
      "step": 7510
    },
    {
      "epoch": 0.11638614819113949,
      "grad_norm": 1.751362919807434,
      "learning_rate": 2.8851805779941054e-05,
      "loss": 1.0877,
      "step": 7520
    },
    {
      "epoch": 0.11654091700522344,
      "grad_norm": 2.5544886589050293,
      "learning_rate": 2.885025730493086e-05,
      "loss": 0.9496,
      "step": 7530
    },
    {
      "epoch": 0.11669568581930741,
      "grad_norm": 2.2016260623931885,
      "learning_rate": 2.8848708829920668e-05,
      "loss": 1.1869,
      "step": 7540
    },
    {
      "epoch": 0.11685045463339137,
      "grad_norm": 1.4976564645767212,
      "learning_rate": 2.8847160354910475e-05,
      "loss": 1.0755,
      "step": 7550
    },
    {
      "epoch": 0.11700522344747534,
      "grad_norm": 2.2199156284332275,
      "learning_rate": 2.884561187990028e-05,
      "loss": 1.2404,
      "step": 7560
    },
    {
      "epoch": 0.11715999226155929,
      "grad_norm": 2.7579591274261475,
      "learning_rate": 2.8844063404890086e-05,
      "loss": 0.9265,
      "step": 7570
    },
    {
      "epoch": 0.11731476107564326,
      "grad_norm": 1.4329724311828613,
      "learning_rate": 2.884251492987989e-05,
      "loss": 1.0169,
      "step": 7580
    },
    {
      "epoch": 0.11746952988972723,
      "grad_norm": 1.6678141355514526,
      "learning_rate": 2.8840966454869694e-05,
      "loss": 1.1807,
      "step": 7590
    },
    {
      "epoch": 0.11762429870381118,
      "grad_norm": 1.2777084112167358,
      "learning_rate": 2.88394179798595e-05,
      "loss": 1.0728,
      "step": 7600
    },
    {
      "epoch": 0.11777906751789514,
      "grad_norm": 1.7218424081802368,
      "learning_rate": 2.8837869504849308e-05,
      "loss": 1.1291,
      "step": 7610
    },
    {
      "epoch": 0.11793383633197911,
      "grad_norm": 1.8116657733917236,
      "learning_rate": 2.8836321029839115e-05,
      "loss": 1.1644,
      "step": 7620
    },
    {
      "epoch": 0.11808860514606306,
      "grad_norm": 1.5503705739974976,
      "learning_rate": 2.883477255482892e-05,
      "loss": 1.1826,
      "step": 7630
    },
    {
      "epoch": 0.11824337396014703,
      "grad_norm": 2.1249985694885254,
      "learning_rate": 2.8833224079818726e-05,
      "loss": 1.0631,
      "step": 7640
    },
    {
      "epoch": 0.118398142774231,
      "grad_norm": 1.3143913745880127,
      "learning_rate": 2.883167560480853e-05,
      "loss": 1.0598,
      "step": 7650
    },
    {
      "epoch": 0.11855291158831495,
      "grad_norm": 1.5848757028579712,
      "learning_rate": 2.8830127129798337e-05,
      "loss": 1.2234,
      "step": 7660
    },
    {
      "epoch": 0.11870768040239892,
      "grad_norm": 1.0281556844711304,
      "learning_rate": 2.8828578654788144e-05,
      "loss": 1.218,
      "step": 7670
    },
    {
      "epoch": 0.11886244921648288,
      "grad_norm": 1.7398582696914673,
      "learning_rate": 2.882703017977795e-05,
      "loss": 1.0793,
      "step": 7680
    },
    {
      "epoch": 0.11901721803056683,
      "grad_norm": 1.8406779766082764,
      "learning_rate": 2.8825481704767755e-05,
      "loss": 0.9963,
      "step": 7690
    },
    {
      "epoch": 0.1191719868446508,
      "grad_norm": 1.58372163772583,
      "learning_rate": 2.8823933229757562e-05,
      "loss": 1.1908,
      "step": 7700
    },
    {
      "epoch": 0.11932675565873477,
      "grad_norm": 2.1659250259399414,
      "learning_rate": 2.8822384754747366e-05,
      "loss": 1.1835,
      "step": 7710
    },
    {
      "epoch": 0.11948152447281872,
      "grad_norm": 1.7512598037719727,
      "learning_rate": 2.8820836279737173e-05,
      "loss": 1.1621,
      "step": 7720
    },
    {
      "epoch": 0.11963629328690269,
      "grad_norm": 1.1720881462097168,
      "learning_rate": 2.8819287804726977e-05,
      "loss": 1.0981,
      "step": 7730
    },
    {
      "epoch": 0.11979106210098665,
      "grad_norm": 1.671751618385315,
      "learning_rate": 2.8817739329716787e-05,
      "loss": 0.94,
      "step": 7740
    },
    {
      "epoch": 0.11994583091507062,
      "grad_norm": 1.6890497207641602,
      "learning_rate": 2.881619085470659e-05,
      "loss": 0.9487,
      "step": 7750
    },
    {
      "epoch": 0.12010059972915457,
      "grad_norm": 1.3145673274993896,
      "learning_rate": 2.8814642379696398e-05,
      "loss": 1.0691,
      "step": 7760
    },
    {
      "epoch": 0.12025536854323854,
      "grad_norm": 1.39010488986969,
      "learning_rate": 2.8813093904686202e-05,
      "loss": 1.1352,
      "step": 7770
    },
    {
      "epoch": 0.1204101373573225,
      "grad_norm": 1.9085057973861694,
      "learning_rate": 2.881154542967601e-05,
      "loss": 1.1699,
      "step": 7780
    },
    {
      "epoch": 0.12056490617140646,
      "grad_norm": 2.07291316986084,
      "learning_rate": 2.8809996954665813e-05,
      "loss": 0.8914,
      "step": 7790
    },
    {
      "epoch": 0.12071967498549042,
      "grad_norm": 1.3343981504440308,
      "learning_rate": 2.880844847965562e-05,
      "loss": 1.1745,
      "step": 7800
    },
    {
      "epoch": 0.12087444379957439,
      "grad_norm": 2.4286000728607178,
      "learning_rate": 2.8806900004645427e-05,
      "loss": 1.0794,
      "step": 7810
    },
    {
      "epoch": 0.12102921261365834,
      "grad_norm": 2.0735678672790527,
      "learning_rate": 2.8805351529635234e-05,
      "loss": 1.0923,
      "step": 7820
    },
    {
      "epoch": 0.12118398142774231,
      "grad_norm": 2.2084083557128906,
      "learning_rate": 2.8803803054625038e-05,
      "loss": 1.0783,
      "step": 7830
    },
    {
      "epoch": 0.12133875024182628,
      "grad_norm": 1.444717288017273,
      "learning_rate": 2.8802254579614842e-05,
      "loss": 0.9006,
      "step": 7840
    },
    {
      "epoch": 0.12149351905591023,
      "grad_norm": 1.0975955724716187,
      "learning_rate": 2.880070610460465e-05,
      "loss": 1.1126,
      "step": 7850
    },
    {
      "epoch": 0.1216482878699942,
      "grad_norm": 1.927897334098816,
      "learning_rate": 2.8799157629594453e-05,
      "loss": 1.1836,
      "step": 7860
    },
    {
      "epoch": 0.12180305668407816,
      "grad_norm": 1.99954092502594,
      "learning_rate": 2.879760915458426e-05,
      "loss": 1.0771,
      "step": 7870
    },
    {
      "epoch": 0.12195782549816211,
      "grad_norm": 1.908633828163147,
      "learning_rate": 2.8796060679574067e-05,
      "loss": 1.0475,
      "step": 7880
    },
    {
      "epoch": 0.12211259431224608,
      "grad_norm": 1.8895139694213867,
      "learning_rate": 2.8794512204563874e-05,
      "loss": 1.0186,
      "step": 7890
    },
    {
      "epoch": 0.12226736312633005,
      "grad_norm": 1.8973013162612915,
      "learning_rate": 2.8792963729553678e-05,
      "loss": 1.0262,
      "step": 7900
    },
    {
      "epoch": 0.122422131940414,
      "grad_norm": 1.9361954927444458,
      "learning_rate": 2.8791415254543485e-05,
      "loss": 1.0536,
      "step": 7910
    },
    {
      "epoch": 0.12257690075449797,
      "grad_norm": 1.7576699256896973,
      "learning_rate": 2.878986677953329e-05,
      "loss": 1.0265,
      "step": 7920
    },
    {
      "epoch": 0.12273166956858193,
      "grad_norm": 1.9989089965820312,
      "learning_rate": 2.8788318304523096e-05,
      "loss": 0.9673,
      "step": 7930
    },
    {
      "epoch": 0.1228864383826659,
      "grad_norm": 1.7839853763580322,
      "learning_rate": 2.87867698295129e-05,
      "loss": 0.9588,
      "step": 7940
    },
    {
      "epoch": 0.12304120719674985,
      "grad_norm": 1.715635061264038,
      "learning_rate": 2.878522135450271e-05,
      "loss": 1.1946,
      "step": 7950
    },
    {
      "epoch": 0.12319597601083382,
      "grad_norm": 1.2735425233840942,
      "learning_rate": 2.8783672879492514e-05,
      "loss": 1.061,
      "step": 7960
    },
    {
      "epoch": 0.12335074482491779,
      "grad_norm": 1.5010428428649902,
      "learning_rate": 2.878212440448232e-05,
      "loss": 1.2203,
      "step": 7970
    },
    {
      "epoch": 0.12350551363900174,
      "grad_norm": 2.229156732559204,
      "learning_rate": 2.8780575929472125e-05,
      "loss": 1.1407,
      "step": 7980
    },
    {
      "epoch": 0.1236602824530857,
      "grad_norm": 1.257461667060852,
      "learning_rate": 2.8779027454461932e-05,
      "loss": 1.0208,
      "step": 7990
    },
    {
      "epoch": 0.12381505126716967,
      "grad_norm": 1.2027138471603394,
      "learning_rate": 2.8777478979451736e-05,
      "loss": 0.9481,
      "step": 8000
    },
    {
      "epoch": 0.12396982008125362,
      "grad_norm": 1.9659490585327148,
      "learning_rate": 2.8775930504441543e-05,
      "loss": 1.1585,
      "step": 8010
    },
    {
      "epoch": 0.12412458889533759,
      "grad_norm": 1.9976495504379272,
      "learning_rate": 2.877438202943135e-05,
      "loss": 1.1864,
      "step": 8020
    },
    {
      "epoch": 0.12427935770942156,
      "grad_norm": 1.9018985033035278,
      "learning_rate": 2.8772833554421157e-05,
      "loss": 1.1758,
      "step": 8030
    },
    {
      "epoch": 0.12443412652350551,
      "grad_norm": 1.8554928302764893,
      "learning_rate": 2.877128507941096e-05,
      "loss": 1.0826,
      "step": 8040
    },
    {
      "epoch": 0.12458889533758948,
      "grad_norm": 2.3241758346557617,
      "learning_rate": 2.8769736604400768e-05,
      "loss": 1.1397,
      "step": 8050
    },
    {
      "epoch": 0.12474366415167344,
      "grad_norm": 1.5295218229293823,
      "learning_rate": 2.8768188129390572e-05,
      "loss": 1.1669,
      "step": 8060
    },
    {
      "epoch": 0.1248984329657574,
      "grad_norm": 1.7339640855789185,
      "learning_rate": 2.876663965438038e-05,
      "loss": 1.033,
      "step": 8070
    },
    {
      "epoch": 0.12505320177984136,
      "grad_norm": 2.6615242958068848,
      "learning_rate": 2.8765091179370186e-05,
      "loss": 1.1496,
      "step": 8080
    },
    {
      "epoch": 0.12520797059392533,
      "grad_norm": 1.500586748123169,
      "learning_rate": 2.876354270435999e-05,
      "loss": 1.1841,
      "step": 8090
    },
    {
      "epoch": 0.1253627394080093,
      "grad_norm": 1.1447476148605347,
      "learning_rate": 2.8761994229349797e-05,
      "loss": 1.1268,
      "step": 8100
    },
    {
      "epoch": 0.12551750822209326,
      "grad_norm": 1.3690401315689087,
      "learning_rate": 2.87604457543396e-05,
      "loss": 0.992,
      "step": 8110
    },
    {
      "epoch": 0.1256722770361772,
      "grad_norm": 1.942611575126648,
      "learning_rate": 2.8758897279329408e-05,
      "loss": 1.0659,
      "step": 8120
    },
    {
      "epoch": 0.12582704585026117,
      "grad_norm": 1.432557225227356,
      "learning_rate": 2.875734880431921e-05,
      "loss": 1.0041,
      "step": 8130
    },
    {
      "epoch": 0.12598181466434513,
      "grad_norm": 1.3813238143920898,
      "learning_rate": 2.875580032930902e-05,
      "loss": 1.0774,
      "step": 8140
    },
    {
      "epoch": 0.1261365834784291,
      "grad_norm": 2.0309860706329346,
      "learning_rate": 2.8754251854298826e-05,
      "loss": 1.1636,
      "step": 8150
    },
    {
      "epoch": 0.12629135229251306,
      "grad_norm": 1.7303977012634277,
      "learning_rate": 2.8752703379288633e-05,
      "loss": 1.1311,
      "step": 8160
    },
    {
      "epoch": 0.12644612110659703,
      "grad_norm": 1.7859554290771484,
      "learning_rate": 2.8751154904278437e-05,
      "loss": 1.0633,
      "step": 8170
    },
    {
      "epoch": 0.12660088992068097,
      "grad_norm": 2.9094655513763428,
      "learning_rate": 2.8749606429268244e-05,
      "loss": 1.1996,
      "step": 8180
    },
    {
      "epoch": 0.12675565873476494,
      "grad_norm": 2.5563178062438965,
      "learning_rate": 2.8748057954258048e-05,
      "loss": 1.0659,
      "step": 8190
    },
    {
      "epoch": 0.1269104275488489,
      "grad_norm": 1.4475303888320923,
      "learning_rate": 2.8746509479247855e-05,
      "loss": 1.1531,
      "step": 8200
    },
    {
      "epoch": 0.12706519636293287,
      "grad_norm": 1.7326278686523438,
      "learning_rate": 2.874496100423766e-05,
      "loss": 1.0408,
      "step": 8210
    },
    {
      "epoch": 0.12721996517701684,
      "grad_norm": 2.658756971359253,
      "learning_rate": 2.874341252922747e-05,
      "loss": 1.0438,
      "step": 8220
    },
    {
      "epoch": 0.1273747339911008,
      "grad_norm": 2.1248977184295654,
      "learning_rate": 2.8741864054217273e-05,
      "loss": 1.3629,
      "step": 8230
    },
    {
      "epoch": 0.12752950280518477,
      "grad_norm": 2.0412914752960205,
      "learning_rate": 2.874031557920708e-05,
      "loss": 1.1162,
      "step": 8240
    },
    {
      "epoch": 0.1276842716192687,
      "grad_norm": 1.9367917776107788,
      "learning_rate": 2.8738767104196884e-05,
      "loss": 0.9837,
      "step": 8250
    },
    {
      "epoch": 0.12783904043335267,
      "grad_norm": 2.097067356109619,
      "learning_rate": 2.873721862918669e-05,
      "loss": 1.199,
      "step": 8260
    },
    {
      "epoch": 0.12799380924743664,
      "grad_norm": 2.625140428543091,
      "learning_rate": 2.8735670154176495e-05,
      "loss": 0.977,
      "step": 8270
    },
    {
      "epoch": 0.1281485780615206,
      "grad_norm": 1.9578653573989868,
      "learning_rate": 2.87341216791663e-05,
      "loss": 1.1101,
      "step": 8280
    },
    {
      "epoch": 0.12830334687560457,
      "grad_norm": 1.2153948545455933,
      "learning_rate": 2.873257320415611e-05,
      "loss": 1.0772,
      "step": 8290
    },
    {
      "epoch": 0.12845811568968854,
      "grad_norm": 2.311596155166626,
      "learning_rate": 2.8731024729145916e-05,
      "loss": 1.0662,
      "step": 8300
    },
    {
      "epoch": 0.12861288450377248,
      "grad_norm": 2.601290464401245,
      "learning_rate": 2.872947625413572e-05,
      "loss": 0.9863,
      "step": 8310
    },
    {
      "epoch": 0.12876765331785645,
      "grad_norm": 2.030910015106201,
      "learning_rate": 2.8727927779125527e-05,
      "loss": 1.0521,
      "step": 8320
    },
    {
      "epoch": 0.1289224221319404,
      "grad_norm": 2.7403714656829834,
      "learning_rate": 2.872637930411533e-05,
      "loss": 1.1715,
      "step": 8330
    },
    {
      "epoch": 0.12907719094602438,
      "grad_norm": 2.156459331512451,
      "learning_rate": 2.8724830829105134e-05,
      "loss": 1.0619,
      "step": 8340
    },
    {
      "epoch": 0.12923195976010834,
      "grad_norm": 1.4616702795028687,
      "learning_rate": 2.872328235409494e-05,
      "loss": 1.009,
      "step": 8350
    },
    {
      "epoch": 0.1293867285741923,
      "grad_norm": 1.912855625152588,
      "learning_rate": 2.872173387908475e-05,
      "loss": 1.0314,
      "step": 8360
    },
    {
      "epoch": 0.12954149738827625,
      "grad_norm": 1.8189647197723389,
      "learning_rate": 2.8720185404074556e-05,
      "loss": 1.01,
      "step": 8370
    },
    {
      "epoch": 0.12969626620236022,
      "grad_norm": 2.0477867126464844,
      "learning_rate": 2.871863692906436e-05,
      "loss": 1.1662,
      "step": 8380
    },
    {
      "epoch": 0.12985103501644418,
      "grad_norm": 1.593125343322754,
      "learning_rate": 2.8717088454054167e-05,
      "loss": 1.1181,
      "step": 8390
    },
    {
      "epoch": 0.13000580383052815,
      "grad_norm": 1.703596830368042,
      "learning_rate": 2.871553997904397e-05,
      "loss": 1.0253,
      "step": 8400
    },
    {
      "epoch": 0.13016057264461212,
      "grad_norm": 2.435532331466675,
      "learning_rate": 2.8713991504033778e-05,
      "loss": 1.028,
      "step": 8410
    },
    {
      "epoch": 0.13031534145869608,
      "grad_norm": 2.7872791290283203,
      "learning_rate": 2.871244302902358e-05,
      "loss": 1.007,
      "step": 8420
    },
    {
      "epoch": 0.13047011027278002,
      "grad_norm": 1.615374207496643,
      "learning_rate": 2.8710894554013392e-05,
      "loss": 1.0863,
      "step": 8430
    },
    {
      "epoch": 0.130624879086864,
      "grad_norm": 2.3508362770080566,
      "learning_rate": 2.8709346079003196e-05,
      "loss": 1.1758,
      "step": 8440
    },
    {
      "epoch": 0.13077964790094795,
      "grad_norm": 2.092674732208252,
      "learning_rate": 2.8707797603993003e-05,
      "loss": 1.0354,
      "step": 8450
    },
    {
      "epoch": 0.13093441671503192,
      "grad_norm": 1.9595577716827393,
      "learning_rate": 2.8706249128982806e-05,
      "loss": 1.0236,
      "step": 8460
    },
    {
      "epoch": 0.1310891855291159,
      "grad_norm": 2.9842822551727295,
      "learning_rate": 2.8704700653972614e-05,
      "loss": 1.1366,
      "step": 8470
    },
    {
      "epoch": 0.13124395434319985,
      "grad_norm": 1.4374672174453735,
      "learning_rate": 2.8703152178962417e-05,
      "loss": 1.1633,
      "step": 8480
    },
    {
      "epoch": 0.13139872315728382,
      "grad_norm": 1.8629096746444702,
      "learning_rate": 2.8701603703952224e-05,
      "loss": 0.9243,
      "step": 8490
    },
    {
      "epoch": 0.13155349197136776,
      "grad_norm": 1.3471235036849976,
      "learning_rate": 2.870005522894203e-05,
      "loss": 1.032,
      "step": 8500
    },
    {
      "epoch": 0.13170826078545173,
      "grad_norm": 1.7558244466781616,
      "learning_rate": 2.869850675393184e-05,
      "loss": 1.2045,
      "step": 8510
    },
    {
      "epoch": 0.1318630295995357,
      "grad_norm": 2.205979108810425,
      "learning_rate": 2.8696958278921642e-05,
      "loss": 0.9592,
      "step": 8520
    },
    {
      "epoch": 0.13201779841361966,
      "grad_norm": 1.658558964729309,
      "learning_rate": 2.869540980391145e-05,
      "loss": 1.0727,
      "step": 8530
    },
    {
      "epoch": 0.13217256722770362,
      "grad_norm": 1.374664306640625,
      "learning_rate": 2.8693861328901253e-05,
      "loss": 1.001,
      "step": 8540
    },
    {
      "epoch": 0.1323273360417876,
      "grad_norm": 1.2275686264038086,
      "learning_rate": 2.869231285389106e-05,
      "loss": 1.0527,
      "step": 8550
    },
    {
      "epoch": 0.13248210485587153,
      "grad_norm": 2.2765069007873535,
      "learning_rate": 2.8690764378880868e-05,
      "loss": 1.0577,
      "step": 8560
    },
    {
      "epoch": 0.1326368736699555,
      "grad_norm": 1.7281564474105835,
      "learning_rate": 2.8689215903870675e-05,
      "loss": 1.0478,
      "step": 8570
    },
    {
      "epoch": 0.13279164248403946,
      "grad_norm": 1.636504054069519,
      "learning_rate": 2.868766742886048e-05,
      "loss": 1.0117,
      "step": 8580
    },
    {
      "epoch": 0.13294641129812343,
      "grad_norm": 2.0347225666046143,
      "learning_rate": 2.8686118953850286e-05,
      "loss": 1.0849,
      "step": 8590
    },
    {
      "epoch": 0.1331011801122074,
      "grad_norm": 2.916412830352783,
      "learning_rate": 2.868457047884009e-05,
      "loss": 1.0274,
      "step": 8600
    },
    {
      "epoch": 0.13325594892629136,
      "grad_norm": 2.066871166229248,
      "learning_rate": 2.8683022003829893e-05,
      "loss": 0.949,
      "step": 8610
    },
    {
      "epoch": 0.1334107177403753,
      "grad_norm": 1.4530764818191528,
      "learning_rate": 2.86814735288197e-05,
      "loss": 1.1933,
      "step": 8620
    },
    {
      "epoch": 0.13356548655445927,
      "grad_norm": 1.6394084692001343,
      "learning_rate": 2.8679925053809507e-05,
      "loss": 1.1268,
      "step": 8630
    },
    {
      "epoch": 0.13372025536854323,
      "grad_norm": 2.0434229373931885,
      "learning_rate": 2.8678376578799315e-05,
      "loss": 1.0111,
      "step": 8640
    },
    {
      "epoch": 0.1338750241826272,
      "grad_norm": 1.4085921049118042,
      "learning_rate": 2.867682810378912e-05,
      "loss": 1.2236,
      "step": 8650
    },
    {
      "epoch": 0.13402979299671117,
      "grad_norm": 1.4073604345321655,
      "learning_rate": 2.8675279628778925e-05,
      "loss": 1.2067,
      "step": 8660
    },
    {
      "epoch": 0.13418456181079513,
      "grad_norm": 2.3639934062957764,
      "learning_rate": 2.867373115376873e-05,
      "loss": 1.0688,
      "step": 8670
    },
    {
      "epoch": 0.1343393306248791,
      "grad_norm": 1.7746855020523071,
      "learning_rate": 2.8672182678758536e-05,
      "loss": 1.1272,
      "step": 8680
    },
    {
      "epoch": 0.13449409943896304,
      "grad_norm": 2.0148611068725586,
      "learning_rate": 2.867063420374834e-05,
      "loss": 1.2102,
      "step": 8690
    },
    {
      "epoch": 0.134648868253047,
      "grad_norm": 2.047128438949585,
      "learning_rate": 2.866908572873815e-05,
      "loss": 1.014,
      "step": 8700
    },
    {
      "epoch": 0.13480363706713097,
      "grad_norm": 1.556579351425171,
      "learning_rate": 2.8667537253727954e-05,
      "loss": 0.965,
      "step": 8710
    },
    {
      "epoch": 0.13495840588121494,
      "grad_norm": 2.3474128246307373,
      "learning_rate": 2.866598877871776e-05,
      "loss": 1.1434,
      "step": 8720
    },
    {
      "epoch": 0.1351131746952989,
      "grad_norm": 1.2172235250473022,
      "learning_rate": 2.8664440303707565e-05,
      "loss": 1.1699,
      "step": 8730
    },
    {
      "epoch": 0.13526794350938287,
      "grad_norm": 2.7069146633148193,
      "learning_rate": 2.8662891828697372e-05,
      "loss": 1.1188,
      "step": 8740
    },
    {
      "epoch": 0.1354227123234668,
      "grad_norm": 1.7246558666229248,
      "learning_rate": 2.8661343353687176e-05,
      "loss": 1.07,
      "step": 8750
    },
    {
      "epoch": 0.13557748113755078,
      "grad_norm": 2.1238744258880615,
      "learning_rate": 2.8659794878676983e-05,
      "loss": 1.1799,
      "step": 8760
    },
    {
      "epoch": 0.13573224995163474,
      "grad_norm": 1.4360383749008179,
      "learning_rate": 2.865824640366679e-05,
      "loss": 1.1868,
      "step": 8770
    },
    {
      "epoch": 0.1358870187657187,
      "grad_norm": 3.4515132904052734,
      "learning_rate": 2.8656697928656598e-05,
      "loss": 0.9579,
      "step": 8780
    },
    {
      "epoch": 0.13604178757980268,
      "grad_norm": 1.2140811681747437,
      "learning_rate": 2.86551494536464e-05,
      "loss": 1.145,
      "step": 8790
    },
    {
      "epoch": 0.13619655639388664,
      "grad_norm": 1.9062541723251343,
      "learning_rate": 2.865360097863621e-05,
      "loss": 1.1242,
      "step": 8800
    },
    {
      "epoch": 0.13635132520797058,
      "grad_norm": 1.8572478294372559,
      "learning_rate": 2.8652052503626012e-05,
      "loss": 1.2117,
      "step": 8810
    },
    {
      "epoch": 0.13650609402205455,
      "grad_norm": 1.3856287002563477,
      "learning_rate": 2.865050402861582e-05,
      "loss": 1.0003,
      "step": 8820
    },
    {
      "epoch": 0.1366608628361385,
      "grad_norm": 1.9364049434661865,
      "learning_rate": 2.8648955553605623e-05,
      "loss": 1.12,
      "step": 8830
    },
    {
      "epoch": 0.13681563165022248,
      "grad_norm": 1.81786048412323,
      "learning_rate": 2.8647407078595434e-05,
      "loss": 1.2195,
      "step": 8840
    },
    {
      "epoch": 0.13697040046430645,
      "grad_norm": 2.295640707015991,
      "learning_rate": 2.8645858603585237e-05,
      "loss": 1.1166,
      "step": 8850
    },
    {
      "epoch": 0.1371251692783904,
      "grad_norm": 2.099461078643799,
      "learning_rate": 2.864431012857504e-05,
      "loss": 1.226,
      "step": 8860
    },
    {
      "epoch": 0.13727993809247438,
      "grad_norm": 2.5422592163085938,
      "learning_rate": 2.8642761653564848e-05,
      "loss": 1.0051,
      "step": 8870
    },
    {
      "epoch": 0.13743470690655832,
      "grad_norm": 2.8520009517669678,
      "learning_rate": 2.8641213178554652e-05,
      "loss": 1.1066,
      "step": 8880
    },
    {
      "epoch": 0.13758947572064228,
      "grad_norm": 2.1997745037078857,
      "learning_rate": 2.863966470354446e-05,
      "loss": 1.1588,
      "step": 8890
    },
    {
      "epoch": 0.13774424453472625,
      "grad_norm": 1.9631227254867554,
      "learning_rate": 2.8638116228534263e-05,
      "loss": 0.9556,
      "step": 8900
    },
    {
      "epoch": 0.13789901334881022,
      "grad_norm": 1.4485102891921997,
      "learning_rate": 2.8636567753524073e-05,
      "loss": 1.26,
      "step": 8910
    },
    {
      "epoch": 0.13805378216289418,
      "grad_norm": 1.7511816024780273,
      "learning_rate": 2.8635019278513877e-05,
      "loss": 0.9118,
      "step": 8920
    },
    {
      "epoch": 0.13820855097697815,
      "grad_norm": 1.8443390130996704,
      "learning_rate": 2.8633470803503684e-05,
      "loss": 1.3129,
      "step": 8930
    },
    {
      "epoch": 0.1383633197910621,
      "grad_norm": 1.7550092935562134,
      "learning_rate": 2.8631922328493488e-05,
      "loss": 1.1161,
      "step": 8940
    },
    {
      "epoch": 0.13851808860514606,
      "grad_norm": 1.41813063621521,
      "learning_rate": 2.8630373853483295e-05,
      "loss": 0.9482,
      "step": 8950
    },
    {
      "epoch": 0.13867285741923002,
      "grad_norm": 2.440779447555542,
      "learning_rate": 2.86288253784731e-05,
      "loss": 1.0994,
      "step": 8960
    },
    {
      "epoch": 0.138827626233314,
      "grad_norm": 1.444765329360962,
      "learning_rate": 2.8627276903462906e-05,
      "loss": 0.9159,
      "step": 8970
    },
    {
      "epoch": 0.13898239504739796,
      "grad_norm": 1.2545278072357178,
      "learning_rate": 2.8625728428452713e-05,
      "loss": 0.9104,
      "step": 8980
    },
    {
      "epoch": 0.13913716386148192,
      "grad_norm": 1.255865216255188,
      "learning_rate": 2.862417995344252e-05,
      "loss": 1.0084,
      "step": 8990
    },
    {
      "epoch": 0.13929193267556586,
      "grad_norm": 2.17509388923645,
      "learning_rate": 2.8622631478432324e-05,
      "loss": 1.1583,
      "step": 9000
    },
    {
      "epoch": 0.13944670148964983,
      "grad_norm": 2.0373666286468506,
      "learning_rate": 2.862108300342213e-05,
      "loss": 1.1116,
      "step": 9010
    },
    {
      "epoch": 0.1396014703037338,
      "grad_norm": 2.452118396759033,
      "learning_rate": 2.8619534528411935e-05,
      "loss": 1.1956,
      "step": 9020
    },
    {
      "epoch": 0.13975623911781776,
      "grad_norm": 2.842556953430176,
      "learning_rate": 2.8617986053401742e-05,
      "loss": 1.1524,
      "step": 9030
    },
    {
      "epoch": 0.13991100793190173,
      "grad_norm": 1.6455371379852295,
      "learning_rate": 2.861643757839155e-05,
      "loss": 1.1938,
      "step": 9040
    },
    {
      "epoch": 0.1400657767459857,
      "grad_norm": 2.275615692138672,
      "learning_rate": 2.8614889103381356e-05,
      "loss": 1.1321,
      "step": 9050
    },
    {
      "epoch": 0.14022054556006966,
      "grad_norm": 2.019278049468994,
      "learning_rate": 2.861334062837116e-05,
      "loss": 1.0245,
      "step": 9060
    },
    {
      "epoch": 0.1403753143741536,
      "grad_norm": 1.874647855758667,
      "learning_rate": 2.8611792153360967e-05,
      "loss": 1.0084,
      "step": 9070
    },
    {
      "epoch": 0.14053008318823756,
      "grad_norm": 1.5513615608215332,
      "learning_rate": 2.861024367835077e-05,
      "loss": 1.1641,
      "step": 9080
    },
    {
      "epoch": 0.14068485200232153,
      "grad_norm": 2.8493361473083496,
      "learning_rate": 2.8608695203340578e-05,
      "loss": 0.976,
      "step": 9090
    },
    {
      "epoch": 0.1408396208164055,
      "grad_norm": 1.7527706623077393,
      "learning_rate": 2.8607146728330382e-05,
      "loss": 1.1569,
      "step": 9100
    },
    {
      "epoch": 0.14099438963048946,
      "grad_norm": 2.6952850818634033,
      "learning_rate": 2.860559825332019e-05,
      "loss": 1.1011,
      "step": 9110
    },
    {
      "epoch": 0.14114915844457343,
      "grad_norm": 1.8285812139511108,
      "learning_rate": 2.8604049778309996e-05,
      "loss": 0.9461,
      "step": 9120
    },
    {
      "epoch": 0.14130392725865737,
      "grad_norm": 2.28017258644104,
      "learning_rate": 2.86025013032998e-05,
      "loss": 1.0035,
      "step": 9130
    },
    {
      "epoch": 0.14145869607274134,
      "grad_norm": 1.7953569889068604,
      "learning_rate": 2.8600952828289607e-05,
      "loss": 1.2099,
      "step": 9140
    },
    {
      "epoch": 0.1416134648868253,
      "grad_norm": 1.1941624879837036,
      "learning_rate": 2.859940435327941e-05,
      "loss": 1.0129,
      "step": 9150
    },
    {
      "epoch": 0.14176823370090927,
      "grad_norm": 1.5082478523254395,
      "learning_rate": 2.8597855878269218e-05,
      "loss": 1.1377,
      "step": 9160
    },
    {
      "epoch": 0.14192300251499324,
      "grad_norm": 1.5553967952728271,
      "learning_rate": 2.8596307403259022e-05,
      "loss": 1.1742,
      "step": 9170
    },
    {
      "epoch": 0.1420777713290772,
      "grad_norm": 1.8781883716583252,
      "learning_rate": 2.8594758928248832e-05,
      "loss": 1.1369,
      "step": 9180
    },
    {
      "epoch": 0.14223254014316114,
      "grad_norm": 2.4634907245635986,
      "learning_rate": 2.8593210453238636e-05,
      "loss": 1.0386,
      "step": 9190
    },
    {
      "epoch": 0.1423873089572451,
      "grad_norm": 1.6629040241241455,
      "learning_rate": 2.8591661978228443e-05,
      "loss": 1.0925,
      "step": 9200
    },
    {
      "epoch": 0.14254207777132907,
      "grad_norm": 1.5423576831817627,
      "learning_rate": 2.8590113503218247e-05,
      "loss": 0.9487,
      "step": 9210
    },
    {
      "epoch": 0.14269684658541304,
      "grad_norm": 1.9195057153701782,
      "learning_rate": 2.8588565028208054e-05,
      "loss": 1.2066,
      "step": 9220
    },
    {
      "epoch": 0.142851615399497,
      "grad_norm": 1.400028944015503,
      "learning_rate": 2.8587016553197858e-05,
      "loss": 0.937,
      "step": 9230
    },
    {
      "epoch": 0.14300638421358097,
      "grad_norm": 1.8269777297973633,
      "learning_rate": 2.8585468078187665e-05,
      "loss": 1.0903,
      "step": 9240
    },
    {
      "epoch": 0.14316115302766494,
      "grad_norm": 1.8646748065948486,
      "learning_rate": 2.8583919603177472e-05,
      "loss": 1.1934,
      "step": 9250
    },
    {
      "epoch": 0.14331592184174888,
      "grad_norm": 2.4999277591705322,
      "learning_rate": 2.858237112816728e-05,
      "loss": 1.0324,
      "step": 9260
    },
    {
      "epoch": 0.14347069065583284,
      "grad_norm": 2.3246805667877197,
      "learning_rate": 2.8580822653157083e-05,
      "loss": 0.9848,
      "step": 9270
    },
    {
      "epoch": 0.1436254594699168,
      "grad_norm": 1.1455460786819458,
      "learning_rate": 2.857927417814689e-05,
      "loss": 1.1553,
      "step": 9280
    },
    {
      "epoch": 0.14378022828400078,
      "grad_norm": 1.7105311155319214,
      "learning_rate": 2.8577725703136694e-05,
      "loss": 1.0574,
      "step": 9290
    },
    {
      "epoch": 0.14393499709808474,
      "grad_norm": 1.296799659729004,
      "learning_rate": 2.85761772281265e-05,
      "loss": 1.2198,
      "step": 9300
    },
    {
      "epoch": 0.1440897659121687,
      "grad_norm": 1.8383216857910156,
      "learning_rate": 2.8574628753116305e-05,
      "loss": 1.2047,
      "step": 9310
    },
    {
      "epoch": 0.14424453472625265,
      "grad_norm": 2.093187093734741,
      "learning_rate": 2.8573080278106115e-05,
      "loss": 1.3691,
      "step": 9320
    },
    {
      "epoch": 0.14439930354033662,
      "grad_norm": 2.354421615600586,
      "learning_rate": 2.857153180309592e-05,
      "loss": 1.1454,
      "step": 9330
    },
    {
      "epoch": 0.14455407235442058,
      "grad_norm": 1.4475576877593994,
      "learning_rate": 2.8569983328085726e-05,
      "loss": 1.0459,
      "step": 9340
    },
    {
      "epoch": 0.14470884116850455,
      "grad_norm": 1.6817102432250977,
      "learning_rate": 2.856843485307553e-05,
      "loss": 1.23,
      "step": 9350
    },
    {
      "epoch": 0.14486360998258851,
      "grad_norm": 1.6186994314193726,
      "learning_rate": 2.8566886378065334e-05,
      "loss": 1.1968,
      "step": 9360
    },
    {
      "epoch": 0.14501837879667248,
      "grad_norm": 2.112467050552368,
      "learning_rate": 2.856533790305514e-05,
      "loss": 1.1155,
      "step": 9370
    },
    {
      "epoch": 0.14517314761075642,
      "grad_norm": 2.452130079269409,
      "learning_rate": 2.8563789428044945e-05,
      "loss": 1.0506,
      "step": 9380
    },
    {
      "epoch": 0.1453279164248404,
      "grad_norm": 1.4941742420196533,
      "learning_rate": 2.8562240953034755e-05,
      "loss": 1.1434,
      "step": 9390
    },
    {
      "epoch": 0.14548268523892435,
      "grad_norm": 1.2684884071350098,
      "learning_rate": 2.856069247802456e-05,
      "loss": 1.0322,
      "step": 9400
    },
    {
      "epoch": 0.14563745405300832,
      "grad_norm": 1.4657409191131592,
      "learning_rate": 2.8559144003014366e-05,
      "loss": 1.0707,
      "step": 9410
    },
    {
      "epoch": 0.14579222286709229,
      "grad_norm": 2.298736810684204,
      "learning_rate": 2.855759552800417e-05,
      "loss": 0.9426,
      "step": 9420
    },
    {
      "epoch": 0.14594699168117625,
      "grad_norm": 1.917335033416748,
      "learning_rate": 2.8556047052993977e-05,
      "loss": 1.1296,
      "step": 9430
    },
    {
      "epoch": 0.14610176049526022,
      "grad_norm": 1.4759018421173096,
      "learning_rate": 2.855449857798378e-05,
      "loss": 1.1015,
      "step": 9440
    },
    {
      "epoch": 0.14625652930934416,
      "grad_norm": 1.4918893575668335,
      "learning_rate": 2.855295010297359e-05,
      "loss": 1.1521,
      "step": 9450
    },
    {
      "epoch": 0.14641129812342812,
      "grad_norm": 2.2270348072052,
      "learning_rate": 2.8551401627963395e-05,
      "loss": 1.0422,
      "step": 9460
    },
    {
      "epoch": 0.1465660669375121,
      "grad_norm": 1.5643682479858398,
      "learning_rate": 2.8549853152953202e-05,
      "loss": 1.0861,
      "step": 9470
    },
    {
      "epoch": 0.14672083575159606,
      "grad_norm": 1.3988916873931885,
      "learning_rate": 2.8548304677943006e-05,
      "loss": 1.094,
      "step": 9480
    },
    {
      "epoch": 0.14687560456568002,
      "grad_norm": 1.674457311630249,
      "learning_rate": 2.8546756202932813e-05,
      "loss": 1.0197,
      "step": 9490
    },
    {
      "epoch": 0.147030373379764,
      "grad_norm": 1.648267388343811,
      "learning_rate": 2.8545207727922617e-05,
      "loss": 1.0728,
      "step": 9500
    },
    {
      "epoch": 0.14718514219384793,
      "grad_norm": 2.0231616497039795,
      "learning_rate": 2.8543659252912424e-05,
      "loss": 1.0939,
      "step": 9510
    },
    {
      "epoch": 0.1473399110079319,
      "grad_norm": 2.1676042079925537,
      "learning_rate": 2.854211077790223e-05,
      "loss": 1.0093,
      "step": 9520
    },
    {
      "epoch": 0.14749467982201586,
      "grad_norm": 1.878596305847168,
      "learning_rate": 2.8540562302892038e-05,
      "loss": 1.1821,
      "step": 9530
    },
    {
      "epoch": 0.14764944863609983,
      "grad_norm": 1.7449007034301758,
      "learning_rate": 2.8539013827881842e-05,
      "loss": 1.05,
      "step": 9540
    },
    {
      "epoch": 0.1478042174501838,
      "grad_norm": 1.6582980155944824,
      "learning_rate": 2.853746535287165e-05,
      "loss": 1.0611,
      "step": 9550
    },
    {
      "epoch": 0.14795898626426776,
      "grad_norm": 1.6180499792099,
      "learning_rate": 2.8535916877861453e-05,
      "loss": 1.0846,
      "step": 9560
    },
    {
      "epoch": 0.1481137550783517,
      "grad_norm": 1.7463505268096924,
      "learning_rate": 2.853436840285126e-05,
      "loss": 0.8894,
      "step": 9570
    },
    {
      "epoch": 0.14826852389243567,
      "grad_norm": 1.5851682424545288,
      "learning_rate": 2.8532819927841064e-05,
      "loss": 1.0687,
      "step": 9580
    },
    {
      "epoch": 0.14842329270651963,
      "grad_norm": 2.4214351177215576,
      "learning_rate": 2.8531271452830874e-05,
      "loss": 0.8936,
      "step": 9590
    },
    {
      "epoch": 0.1485780615206036,
      "grad_norm": 1.538893461227417,
      "learning_rate": 2.8529722977820678e-05,
      "loss": 1.2969,
      "step": 9600
    },
    {
      "epoch": 0.14873283033468757,
      "grad_norm": 3.123520851135254,
      "learning_rate": 2.8528174502810482e-05,
      "loss": 1.1308,
      "step": 9610
    },
    {
      "epoch": 0.14888759914877153,
      "grad_norm": 1.6664716005325317,
      "learning_rate": 2.852662602780029e-05,
      "loss": 0.9822,
      "step": 9620
    },
    {
      "epoch": 0.14904236796285547,
      "grad_norm": 1.7683991193771362,
      "learning_rate": 2.8525077552790093e-05,
      "loss": 1.0421,
      "step": 9630
    },
    {
      "epoch": 0.14919713677693944,
      "grad_norm": 1.2839224338531494,
      "learning_rate": 2.85235290777799e-05,
      "loss": 1.0794,
      "step": 9640
    },
    {
      "epoch": 0.1493519055910234,
      "grad_norm": 1.6863683462142944,
      "learning_rate": 2.8521980602769703e-05,
      "loss": 1.1981,
      "step": 9650
    },
    {
      "epoch": 0.14950667440510737,
      "grad_norm": 1.4862210750579834,
      "learning_rate": 2.8520432127759514e-05,
      "loss": 1.0809,
      "step": 9660
    },
    {
      "epoch": 0.14966144321919134,
      "grad_norm": 1.2703368663787842,
      "learning_rate": 2.8518883652749318e-05,
      "loss": 1.1538,
      "step": 9670
    },
    {
      "epoch": 0.1498162120332753,
      "grad_norm": 1.4211839437484741,
      "learning_rate": 2.8517335177739125e-05,
      "loss": 1.1495,
      "step": 9680
    },
    {
      "epoch": 0.14997098084735927,
      "grad_norm": 2.2440316677093506,
      "learning_rate": 2.851578670272893e-05,
      "loss": 1.0845,
      "step": 9690
    },
    {
      "epoch": 0.1501257496614432,
      "grad_norm": 3.106710195541382,
      "learning_rate": 2.8514238227718736e-05,
      "loss": 1.229,
      "step": 9700
    },
    {
      "epoch": 0.15028051847552718,
      "grad_norm": 2.3621933460235596,
      "learning_rate": 2.851268975270854e-05,
      "loss": 0.9067,
      "step": 9710
    },
    {
      "epoch": 0.15043528728961114,
      "grad_norm": 1.5393608808517456,
      "learning_rate": 2.8511141277698347e-05,
      "loss": 1.0677,
      "step": 9720
    },
    {
      "epoch": 0.1505900561036951,
      "grad_norm": 1.7765944004058838,
      "learning_rate": 2.8509592802688154e-05,
      "loss": 1.0537,
      "step": 9730
    },
    {
      "epoch": 0.15074482491777907,
      "grad_norm": 1.8201183080673218,
      "learning_rate": 2.850804432767796e-05,
      "loss": 1.019,
      "step": 9740
    },
    {
      "epoch": 0.15089959373186304,
      "grad_norm": 1.22330641746521,
      "learning_rate": 2.8506495852667765e-05,
      "loss": 1.1562,
      "step": 9750
    },
    {
      "epoch": 0.15105436254594698,
      "grad_norm": 1.5119415521621704,
      "learning_rate": 2.8504947377657572e-05,
      "loss": 0.9606,
      "step": 9760
    },
    {
      "epoch": 0.15120913136003095,
      "grad_norm": 3.8667376041412354,
      "learning_rate": 2.8503398902647376e-05,
      "loss": 1.1466,
      "step": 9770
    },
    {
      "epoch": 0.1513639001741149,
      "grad_norm": 2.493572473526001,
      "learning_rate": 2.8501850427637183e-05,
      "loss": 1.0921,
      "step": 9780
    },
    {
      "epoch": 0.15151866898819888,
      "grad_norm": 2.082581043243408,
      "learning_rate": 2.8500301952626986e-05,
      "loss": 1.2922,
      "step": 9790
    },
    {
      "epoch": 0.15167343780228285,
      "grad_norm": 1.3270782232284546,
      "learning_rate": 2.8498753477616797e-05,
      "loss": 1.0531,
      "step": 9800
    },
    {
      "epoch": 0.1518282066163668,
      "grad_norm": 1.4921636581420898,
      "learning_rate": 2.84972050026066e-05,
      "loss": 1.0419,
      "step": 9810
    },
    {
      "epoch": 0.15198297543045075,
      "grad_norm": 2.0699827671051025,
      "learning_rate": 2.8495656527596408e-05,
      "loss": 1.2015,
      "step": 9820
    },
    {
      "epoch": 0.15213774424453472,
      "grad_norm": 1.6538604497909546,
      "learning_rate": 2.849410805258621e-05,
      "loss": 0.9528,
      "step": 9830
    },
    {
      "epoch": 0.15229251305861868,
      "grad_norm": 1.2094299793243408,
      "learning_rate": 2.849255957757602e-05,
      "loss": 1.0161,
      "step": 9840
    },
    {
      "epoch": 0.15244728187270265,
      "grad_norm": 2.381805896759033,
      "learning_rate": 2.8491011102565823e-05,
      "loss": 1.149,
      "step": 9850
    },
    {
      "epoch": 0.15260205068678662,
      "grad_norm": 1.5488054752349854,
      "learning_rate": 2.848946262755563e-05,
      "loss": 1.0906,
      "step": 9860
    },
    {
      "epoch": 0.15275681950087058,
      "grad_norm": 2.1988954544067383,
      "learning_rate": 2.8487914152545437e-05,
      "loss": 1.2963,
      "step": 9870
    },
    {
      "epoch": 0.15291158831495455,
      "grad_norm": 1.4731007814407349,
      "learning_rate": 2.848636567753524e-05,
      "loss": 1.142,
      "step": 9880
    },
    {
      "epoch": 0.1530663571290385,
      "grad_norm": 2.683366298675537,
      "learning_rate": 2.8484817202525048e-05,
      "loss": 1.0915,
      "step": 9890
    },
    {
      "epoch": 0.15322112594312245,
      "grad_norm": 2.344869375228882,
      "learning_rate": 2.848326872751485e-05,
      "loss": 1.0951,
      "step": 9900
    },
    {
      "epoch": 0.15337589475720642,
      "grad_norm": 1.614522933959961,
      "learning_rate": 2.848172025250466e-05,
      "loss": 1.0844,
      "step": 9910
    },
    {
      "epoch": 0.1535306635712904,
      "grad_norm": 1.3989627361297607,
      "learning_rate": 2.8480171777494462e-05,
      "loss": 0.9378,
      "step": 9920
    },
    {
      "epoch": 0.15368543238537435,
      "grad_norm": 1.3762118816375732,
      "learning_rate": 2.8478623302484273e-05,
      "loss": 0.931,
      "step": 9930
    },
    {
      "epoch": 0.15384020119945832,
      "grad_norm": 2.165894031524658,
      "learning_rate": 2.8477074827474077e-05,
      "loss": 1.0468,
      "step": 9940
    },
    {
      "epoch": 0.15399497001354226,
      "grad_norm": 1.589181661605835,
      "learning_rate": 2.8475526352463884e-05,
      "loss": 1.0516,
      "step": 9950
    },
    {
      "epoch": 0.15414973882762623,
      "grad_norm": 2.3708112239837646,
      "learning_rate": 2.8473977877453688e-05,
      "loss": 1.2089,
      "step": 9960
    },
    {
      "epoch": 0.1543045076417102,
      "grad_norm": 1.817358374595642,
      "learning_rate": 2.8472429402443495e-05,
      "loss": 1.23,
      "step": 9970
    },
    {
      "epoch": 0.15445927645579416,
      "grad_norm": 1.5491533279418945,
      "learning_rate": 2.84708809274333e-05,
      "loss": 1.0781,
      "step": 9980
    },
    {
      "epoch": 0.15461404526987813,
      "grad_norm": 1.6458442211151123,
      "learning_rate": 2.8469332452423106e-05,
      "loss": 1.1114,
      "step": 9990
    },
    {
      "epoch": 0.1547688140839621,
      "grad_norm": 1.8010610342025757,
      "learning_rate": 2.8467783977412913e-05,
      "loss": 1.1406,
      "step": 10000
    },
    {
      "epoch": 0.15492358289804603,
      "grad_norm": 1.9795324802398682,
      "learning_rate": 2.846623550240272e-05,
      "loss": 1.012,
      "step": 10010
    },
    {
      "epoch": 0.15507835171213,
      "grad_norm": 1.2001575231552124,
      "learning_rate": 2.8464687027392524e-05,
      "loss": 1.1499,
      "step": 10020
    },
    {
      "epoch": 0.15523312052621396,
      "grad_norm": 1.905914306640625,
      "learning_rate": 2.846313855238233e-05,
      "loss": 1.0349,
      "step": 10030
    },
    {
      "epoch": 0.15538788934029793,
      "grad_norm": 1.8348580598831177,
      "learning_rate": 2.8461590077372134e-05,
      "loss": 1.219,
      "step": 10040
    },
    {
      "epoch": 0.1555426581543819,
      "grad_norm": 2.2350971698760986,
      "learning_rate": 2.846004160236194e-05,
      "loss": 1.2291,
      "step": 10050
    },
    {
      "epoch": 0.15569742696846586,
      "grad_norm": 1.4966002702713013,
      "learning_rate": 2.8458493127351745e-05,
      "loss": 1.0437,
      "step": 10060
    },
    {
      "epoch": 0.15585219578254983,
      "grad_norm": 1.6547635793685913,
      "learning_rate": 2.8456944652341556e-05,
      "loss": 1.1585,
      "step": 10070
    },
    {
      "epoch": 0.15600696459663377,
      "grad_norm": 1.5270897150039673,
      "learning_rate": 2.845539617733136e-05,
      "loss": 1.1109,
      "step": 10080
    },
    {
      "epoch": 0.15616173341071773,
      "grad_norm": 1.8618756532669067,
      "learning_rate": 2.8453847702321167e-05,
      "loss": 1.0132,
      "step": 10090
    },
    {
      "epoch": 0.1563165022248017,
      "grad_norm": 1.546927809715271,
      "learning_rate": 2.845229922731097e-05,
      "loss": 1.1964,
      "step": 10100
    },
    {
      "epoch": 0.15647127103888567,
      "grad_norm": 1.6325325965881348,
      "learning_rate": 2.8450750752300778e-05,
      "loss": 1.0568,
      "step": 10110
    },
    {
      "epoch": 0.15662603985296963,
      "grad_norm": 1.3743927478790283,
      "learning_rate": 2.844920227729058e-05,
      "loss": 0.9389,
      "step": 10120
    },
    {
      "epoch": 0.1567808086670536,
      "grad_norm": 3.00288462638855,
      "learning_rate": 2.8447653802280385e-05,
      "loss": 1.0311,
      "step": 10130
    },
    {
      "epoch": 0.15693557748113754,
      "grad_norm": 1.2875233888626099,
      "learning_rate": 2.8446105327270196e-05,
      "loss": 0.9436,
      "step": 10140
    },
    {
      "epoch": 0.1570903462952215,
      "grad_norm": 1.7721643447875977,
      "learning_rate": 2.844455685226e-05,
      "loss": 1.0895,
      "step": 10150
    },
    {
      "epoch": 0.15724511510930547,
      "grad_norm": 1.4087810516357422,
      "learning_rate": 2.8443008377249807e-05,
      "loss": 1.1112,
      "step": 10160
    },
    {
      "epoch": 0.15739988392338944,
      "grad_norm": 1.4010958671569824,
      "learning_rate": 2.844145990223961e-05,
      "loss": 1.0733,
      "step": 10170
    },
    {
      "epoch": 0.1575546527374734,
      "grad_norm": 1.3129485845565796,
      "learning_rate": 2.8439911427229417e-05,
      "loss": 1.1006,
      "step": 10180
    },
    {
      "epoch": 0.15770942155155737,
      "grad_norm": 1.8412630558013916,
      "learning_rate": 2.843836295221922e-05,
      "loss": 1.0007,
      "step": 10190
    },
    {
      "epoch": 0.1578641903656413,
      "grad_norm": 1.441992998123169,
      "learning_rate": 2.843681447720903e-05,
      "loss": 0.8721,
      "step": 10200
    },
    {
      "epoch": 0.15801895917972528,
      "grad_norm": 1.3451972007751465,
      "learning_rate": 2.8435266002198835e-05,
      "loss": 1.1347,
      "step": 10210
    },
    {
      "epoch": 0.15817372799380924,
      "grad_norm": 2.807870864868164,
      "learning_rate": 2.8433717527188643e-05,
      "loss": 1.0776,
      "step": 10220
    },
    {
      "epoch": 0.1583284968078932,
      "grad_norm": 2.1887924671173096,
      "learning_rate": 2.8432169052178446e-05,
      "loss": 1.0866,
      "step": 10230
    },
    {
      "epoch": 0.15848326562197718,
      "grad_norm": 1.1460353136062622,
      "learning_rate": 2.8430620577168254e-05,
      "loss": 0.8795,
      "step": 10240
    },
    {
      "epoch": 0.15863803443606114,
      "grad_norm": 1.846515417098999,
      "learning_rate": 2.8429072102158057e-05,
      "loss": 0.996,
      "step": 10250
    },
    {
      "epoch": 0.1587928032501451,
      "grad_norm": 2.4852473735809326,
      "learning_rate": 2.8427523627147864e-05,
      "loss": 0.9743,
      "step": 10260
    },
    {
      "epoch": 0.15894757206422905,
      "grad_norm": 1.9692877531051636,
      "learning_rate": 2.8425975152137668e-05,
      "loss": 1.0933,
      "step": 10270
    },
    {
      "epoch": 0.15910234087831301,
      "grad_norm": 1.3029553890228271,
      "learning_rate": 2.842442667712748e-05,
      "loss": 0.8987,
      "step": 10280
    },
    {
      "epoch": 0.15925710969239698,
      "grad_norm": 1.498579502105713,
      "learning_rate": 2.8422878202117282e-05,
      "loss": 1.1716,
      "step": 10290
    },
    {
      "epoch": 0.15941187850648095,
      "grad_norm": 1.6064414978027344,
      "learning_rate": 2.842132972710709e-05,
      "loss": 1.3577,
      "step": 10300
    },
    {
      "epoch": 0.1595666473205649,
      "grad_norm": 1.5633140802383423,
      "learning_rate": 2.8419781252096893e-05,
      "loss": 1.0115,
      "step": 10310
    },
    {
      "epoch": 0.15972141613464888,
      "grad_norm": 1.6841366291046143,
      "learning_rate": 2.84182327770867e-05,
      "loss": 1.0136,
      "step": 10320
    },
    {
      "epoch": 0.15987618494873282,
      "grad_norm": 2.3171424865722656,
      "learning_rate": 2.8416684302076504e-05,
      "loss": 1.1465,
      "step": 10330
    },
    {
      "epoch": 0.16003095376281679,
      "grad_norm": 1.76736581325531,
      "learning_rate": 2.8415135827066315e-05,
      "loss": 1.061,
      "step": 10340
    },
    {
      "epoch": 0.16018572257690075,
      "grad_norm": 2.1665103435516357,
      "learning_rate": 2.841358735205612e-05,
      "loss": 1.1801,
      "step": 10350
    },
    {
      "epoch": 0.16034049139098472,
      "grad_norm": 1.6778947114944458,
      "learning_rate": 2.8412038877045926e-05,
      "loss": 1.1165,
      "step": 10360
    },
    {
      "epoch": 0.16049526020506868,
      "grad_norm": 1.5886387825012207,
      "learning_rate": 2.841049040203573e-05,
      "loss": 1.122,
      "step": 10370
    },
    {
      "epoch": 0.16065002901915265,
      "grad_norm": 2.6221327781677246,
      "learning_rate": 2.8408941927025533e-05,
      "loss": 1.0478,
      "step": 10380
    },
    {
      "epoch": 0.1608047978332366,
      "grad_norm": 1.629150390625,
      "learning_rate": 2.840739345201534e-05,
      "loss": 1.2428,
      "step": 10390
    },
    {
      "epoch": 0.16095956664732056,
      "grad_norm": 1.1027860641479492,
      "learning_rate": 2.8405844977005144e-05,
      "loss": 1.0342,
      "step": 10400
    },
    {
      "epoch": 0.16111433546140452,
      "grad_norm": 1.3628431558609009,
      "learning_rate": 2.8404296501994955e-05,
      "loss": 1.2006,
      "step": 10410
    },
    {
      "epoch": 0.1612691042754885,
      "grad_norm": 1.4198713302612305,
      "learning_rate": 2.8402748026984758e-05,
      "loss": 1.0383,
      "step": 10420
    },
    {
      "epoch": 0.16142387308957246,
      "grad_norm": 1.9019873142242432,
      "learning_rate": 2.8401199551974565e-05,
      "loss": 1.1275,
      "step": 10430
    },
    {
      "epoch": 0.16157864190365642,
      "grad_norm": 1.9635496139526367,
      "learning_rate": 2.839965107696437e-05,
      "loss": 1.2348,
      "step": 10440
    },
    {
      "epoch": 0.1617334107177404,
      "grad_norm": 1.0426918268203735,
      "learning_rate": 2.8398102601954176e-05,
      "loss": 0.9688,
      "step": 10450
    },
    {
      "epoch": 0.16188817953182433,
      "grad_norm": 1.212470531463623,
      "learning_rate": 2.839655412694398e-05,
      "loss": 1.0665,
      "step": 10460
    },
    {
      "epoch": 0.1620429483459083,
      "grad_norm": 1.2638367414474487,
      "learning_rate": 2.8395005651933787e-05,
      "loss": 1.0458,
      "step": 10470
    },
    {
      "epoch": 0.16219771715999226,
      "grad_norm": 0.8834676742553711,
      "learning_rate": 2.8393457176923594e-05,
      "loss": 1.1189,
      "step": 10480
    },
    {
      "epoch": 0.16235248597407623,
      "grad_norm": 1.9342597723007202,
      "learning_rate": 2.83919087019134e-05,
      "loss": 1.1521,
      "step": 10490
    },
    {
      "epoch": 0.1625072547881602,
      "grad_norm": 1.5609638690948486,
      "learning_rate": 2.8390360226903205e-05,
      "loss": 1.2085,
      "step": 10500
    },
    {
      "epoch": 0.16266202360224416,
      "grad_norm": 2.7387876510620117,
      "learning_rate": 2.8388811751893012e-05,
      "loss": 1.1407,
      "step": 10510
    },
    {
      "epoch": 0.1628167924163281,
      "grad_norm": 2.664292097091675,
      "learning_rate": 2.8387263276882816e-05,
      "loss": 1.0063,
      "step": 10520
    },
    {
      "epoch": 0.16297156123041207,
      "grad_norm": 1.5526443719863892,
      "learning_rate": 2.8385714801872623e-05,
      "loss": 1.1977,
      "step": 10530
    },
    {
      "epoch": 0.16312633004449603,
      "grad_norm": 2.0438802242279053,
      "learning_rate": 2.8384166326862427e-05,
      "loss": 1.0514,
      "step": 10540
    },
    {
      "epoch": 0.16328109885858,
      "grad_norm": 1.9146322011947632,
      "learning_rate": 2.8382617851852238e-05,
      "loss": 1.202,
      "step": 10550
    },
    {
      "epoch": 0.16343586767266396,
      "grad_norm": 1.9162940979003906,
      "learning_rate": 2.838106937684204e-05,
      "loss": 1.0876,
      "step": 10560
    },
    {
      "epoch": 0.16359063648674793,
      "grad_norm": 2.1802163124084473,
      "learning_rate": 2.837952090183185e-05,
      "loss": 1.1681,
      "step": 10570
    },
    {
      "epoch": 0.16374540530083187,
      "grad_norm": 1.9779614210128784,
      "learning_rate": 2.8377972426821652e-05,
      "loss": 0.9714,
      "step": 10580
    },
    {
      "epoch": 0.16390017411491584,
      "grad_norm": 1.4519336223602295,
      "learning_rate": 2.837642395181146e-05,
      "loss": 1.0997,
      "step": 10590
    },
    {
      "epoch": 0.1640549429289998,
      "grad_norm": 1.623779296875,
      "learning_rate": 2.8374875476801263e-05,
      "loss": 1.0488,
      "step": 10600
    },
    {
      "epoch": 0.16420971174308377,
      "grad_norm": 1.0252654552459717,
      "learning_rate": 2.837332700179107e-05,
      "loss": 1.195,
      "step": 10610
    },
    {
      "epoch": 0.16436448055716774,
      "grad_norm": 1.6950123310089111,
      "learning_rate": 2.8371778526780877e-05,
      "loss": 0.9921,
      "step": 10620
    },
    {
      "epoch": 0.1645192493712517,
      "grad_norm": 1.416116714477539,
      "learning_rate": 2.837023005177068e-05,
      "loss": 1.0809,
      "step": 10630
    },
    {
      "epoch": 0.16467401818533564,
      "grad_norm": 2.6865360736846924,
      "learning_rate": 2.8368681576760488e-05,
      "loss": 0.979,
      "step": 10640
    },
    {
      "epoch": 0.1648287869994196,
      "grad_norm": 1.499394178390503,
      "learning_rate": 2.8367133101750292e-05,
      "loss": 0.9828,
      "step": 10650
    },
    {
      "epoch": 0.16498355581350357,
      "grad_norm": 1.6314308643341064,
      "learning_rate": 2.83655846267401e-05,
      "loss": 1.047,
      "step": 10660
    },
    {
      "epoch": 0.16513832462758754,
      "grad_norm": 1.6103931665420532,
      "learning_rate": 2.8364036151729903e-05,
      "loss": 1.1377,
      "step": 10670
    },
    {
      "epoch": 0.1652930934416715,
      "grad_norm": 1.7231050729751587,
      "learning_rate": 2.836248767671971e-05,
      "loss": 1.1324,
      "step": 10680
    },
    {
      "epoch": 0.16544786225575547,
      "grad_norm": 1.3138359785079956,
      "learning_rate": 2.8360939201709517e-05,
      "loss": 1.2351,
      "step": 10690
    },
    {
      "epoch": 0.16560263106983944,
      "grad_norm": 0.7644277811050415,
      "learning_rate": 2.8359390726699324e-05,
      "loss": 1.0199,
      "step": 10700
    },
    {
      "epoch": 0.16575739988392338,
      "grad_norm": 1.5547776222229004,
      "learning_rate": 2.8357842251689128e-05,
      "loss": 1.2565,
      "step": 10710
    },
    {
      "epoch": 0.16591216869800735,
      "grad_norm": 1.4475287199020386,
      "learning_rate": 2.8356293776678935e-05,
      "loss": 1.132,
      "step": 10720
    },
    {
      "epoch": 0.1660669375120913,
      "grad_norm": 1.9035907983779907,
      "learning_rate": 2.835474530166874e-05,
      "loss": 1.0928,
      "step": 10730
    },
    {
      "epoch": 0.16622170632617528,
      "grad_norm": 1.6774427890777588,
      "learning_rate": 2.8353196826658546e-05,
      "loss": 1.0258,
      "step": 10740
    },
    {
      "epoch": 0.16637647514025924,
      "grad_norm": 2.682370662689209,
      "learning_rate": 2.8351648351648353e-05,
      "loss": 0.9886,
      "step": 10750
    },
    {
      "epoch": 0.1665312439543432,
      "grad_norm": 1.6151043176651,
      "learning_rate": 2.835009987663816e-05,
      "loss": 1.1356,
      "step": 10760
    },
    {
      "epoch": 0.16668601276842715,
      "grad_norm": 2.031940460205078,
      "learning_rate": 2.8348551401627964e-05,
      "loss": 1.1836,
      "step": 10770
    },
    {
      "epoch": 0.16684078158251112,
      "grad_norm": 1.1155872344970703,
      "learning_rate": 2.834700292661777e-05,
      "loss": 1.1489,
      "step": 10780
    },
    {
      "epoch": 0.16699555039659508,
      "grad_norm": 1.6064461469650269,
      "learning_rate": 2.8345454451607575e-05,
      "loss": 1.1657,
      "step": 10790
    },
    {
      "epoch": 0.16715031921067905,
      "grad_norm": 2.313859701156616,
      "learning_rate": 2.8343905976597382e-05,
      "loss": 1.0683,
      "step": 10800
    },
    {
      "epoch": 0.16730508802476302,
      "grad_norm": 1.397437334060669,
      "learning_rate": 2.8342357501587186e-05,
      "loss": 1.1346,
      "step": 10810
    },
    {
      "epoch": 0.16745985683884698,
      "grad_norm": 2.5197393894195557,
      "learning_rate": 2.8340809026576996e-05,
      "loss": 1.208,
      "step": 10820
    },
    {
      "epoch": 0.16761462565293092,
      "grad_norm": 1.1049530506134033,
      "learning_rate": 2.83392605515668e-05,
      "loss": 0.9988,
      "step": 10830
    },
    {
      "epoch": 0.1677693944670149,
      "grad_norm": 1.7418053150177002,
      "learning_rate": 2.8337712076556607e-05,
      "loss": 1.0465,
      "step": 10840
    },
    {
      "epoch": 0.16792416328109885,
      "grad_norm": 1.51655113697052,
      "learning_rate": 2.833616360154641e-05,
      "loss": 0.9868,
      "step": 10850
    },
    {
      "epoch": 0.16807893209518282,
      "grad_norm": 1.7236039638519287,
      "learning_rate": 2.8334615126536218e-05,
      "loss": 1.0709,
      "step": 10860
    },
    {
      "epoch": 0.1682337009092668,
      "grad_norm": 1.2219083309173584,
      "learning_rate": 2.8333066651526022e-05,
      "loss": 1.0298,
      "step": 10870
    },
    {
      "epoch": 0.16838846972335075,
      "grad_norm": 2.230187177658081,
      "learning_rate": 2.8331518176515826e-05,
      "loss": 0.9926,
      "step": 10880
    },
    {
      "epoch": 0.16854323853743472,
      "grad_norm": 1.2821342945098877,
      "learning_rate": 2.8329969701505636e-05,
      "loss": 1.1383,
      "step": 10890
    },
    {
      "epoch": 0.16869800735151866,
      "grad_norm": 1.2966574430465698,
      "learning_rate": 2.832842122649544e-05,
      "loss": 1.1599,
      "step": 10900
    },
    {
      "epoch": 0.16885277616560262,
      "grad_norm": 1.4001226425170898,
      "learning_rate": 2.8326872751485247e-05,
      "loss": 1.0394,
      "step": 10910
    },
    {
      "epoch": 0.1690075449796866,
      "grad_norm": 0.9976192712783813,
      "learning_rate": 2.832532427647505e-05,
      "loss": 0.8865,
      "step": 10920
    },
    {
      "epoch": 0.16916231379377056,
      "grad_norm": 1.407413363456726,
      "learning_rate": 2.8323775801464858e-05,
      "loss": 1.0067,
      "step": 10930
    },
    {
      "epoch": 0.16931708260785452,
      "grad_norm": 2.4235873222351074,
      "learning_rate": 2.8322227326454662e-05,
      "loss": 1.0244,
      "step": 10940
    },
    {
      "epoch": 0.1694718514219385,
      "grad_norm": 1.1304757595062256,
      "learning_rate": 2.832067885144447e-05,
      "loss": 1.064,
      "step": 10950
    },
    {
      "epoch": 0.16962662023602243,
      "grad_norm": 2.067155361175537,
      "learning_rate": 2.8319130376434276e-05,
      "loss": 1.1227,
      "step": 10960
    },
    {
      "epoch": 0.1697813890501064,
      "grad_norm": 2.2301065921783447,
      "learning_rate": 2.8317581901424083e-05,
      "loss": 1.0654,
      "step": 10970
    },
    {
      "epoch": 0.16993615786419036,
      "grad_norm": 1.122080683708191,
      "learning_rate": 2.8316033426413887e-05,
      "loss": 1.2242,
      "step": 10980
    },
    {
      "epoch": 0.17009092667827433,
      "grad_norm": 1.3730593919754028,
      "learning_rate": 2.8314484951403694e-05,
      "loss": 1.2155,
      "step": 10990
    },
    {
      "epoch": 0.1702456954923583,
      "grad_norm": 1.4535610675811768,
      "learning_rate": 2.8312936476393498e-05,
      "loss": 1.0948,
      "step": 11000
    },
    {
      "epoch": 0.17040046430644226,
      "grad_norm": 1.4759711027145386,
      "learning_rate": 2.8311388001383305e-05,
      "loss": 1.0695,
      "step": 11010
    },
    {
      "epoch": 0.1705552331205262,
      "grad_norm": 1.286576271057129,
      "learning_rate": 2.830983952637311e-05,
      "loss": 1.321,
      "step": 11020
    },
    {
      "epoch": 0.17071000193461017,
      "grad_norm": 1.2845022678375244,
      "learning_rate": 2.830829105136292e-05,
      "loss": 1.1371,
      "step": 11030
    },
    {
      "epoch": 0.17086477074869413,
      "grad_norm": 1.3082046508789062,
      "learning_rate": 2.8306742576352723e-05,
      "loss": 1.056,
      "step": 11040
    },
    {
      "epoch": 0.1710195395627781,
      "grad_norm": 1.7236151695251465,
      "learning_rate": 2.830519410134253e-05,
      "loss": 0.9142,
      "step": 11050
    },
    {
      "epoch": 0.17117430837686207,
      "grad_norm": 1.6830083131790161,
      "learning_rate": 2.8303645626332334e-05,
      "loss": 1.1568,
      "step": 11060
    },
    {
      "epoch": 0.17132907719094603,
      "grad_norm": 2.2541728019714355,
      "learning_rate": 2.830209715132214e-05,
      "loss": 1.0415,
      "step": 11070
    },
    {
      "epoch": 0.17148384600503,
      "grad_norm": 1.4533780813217163,
      "learning_rate": 2.8300548676311945e-05,
      "loss": 1.1908,
      "step": 11080
    },
    {
      "epoch": 0.17163861481911394,
      "grad_norm": 2.305056571960449,
      "learning_rate": 2.8299000201301752e-05,
      "loss": 1.0374,
      "step": 11090
    },
    {
      "epoch": 0.1717933836331979,
      "grad_norm": 2.12591290473938,
      "learning_rate": 2.829745172629156e-05,
      "loss": 1.3518,
      "step": 11100
    },
    {
      "epoch": 0.17194815244728187,
      "grad_norm": 1.5386048555374146,
      "learning_rate": 2.8295903251281366e-05,
      "loss": 1.0559,
      "step": 11110
    },
    {
      "epoch": 0.17210292126136584,
      "grad_norm": 2.57483172416687,
      "learning_rate": 2.829435477627117e-05,
      "loss": 1.1043,
      "step": 11120
    },
    {
      "epoch": 0.1722576900754498,
      "grad_norm": 1.9309508800506592,
      "learning_rate": 2.8292806301260974e-05,
      "loss": 1.0271,
      "step": 11130
    },
    {
      "epoch": 0.17241245888953377,
      "grad_norm": 1.8326908349990845,
      "learning_rate": 2.829125782625078e-05,
      "loss": 1.0301,
      "step": 11140
    },
    {
      "epoch": 0.1725672277036177,
      "grad_norm": 1.2095022201538086,
      "learning_rate": 2.8289709351240585e-05,
      "loss": 1.2435,
      "step": 11150
    },
    {
      "epoch": 0.17272199651770168,
      "grad_norm": 1.9182130098342896,
      "learning_rate": 2.8288160876230392e-05,
      "loss": 1.2047,
      "step": 11160
    },
    {
      "epoch": 0.17287676533178564,
      "grad_norm": 1.2648119926452637,
      "learning_rate": 2.82866124012202e-05,
      "loss": 0.9453,
      "step": 11170
    },
    {
      "epoch": 0.1730315341458696,
      "grad_norm": 1.9780181646347046,
      "learning_rate": 2.8285063926210006e-05,
      "loss": 1.2218,
      "step": 11180
    },
    {
      "epoch": 0.17318630295995358,
      "grad_norm": 1.5855313539505005,
      "learning_rate": 2.828351545119981e-05,
      "loss": 1.1427,
      "step": 11190
    },
    {
      "epoch": 0.17334107177403754,
      "grad_norm": 1.3782267570495605,
      "learning_rate": 2.8281966976189617e-05,
      "loss": 1.2161,
      "step": 11200
    },
    {
      "epoch": 0.17349584058812148,
      "grad_norm": 1.719067931175232,
      "learning_rate": 2.828041850117942e-05,
      "loss": 0.985,
      "step": 11210
    },
    {
      "epoch": 0.17365060940220545,
      "grad_norm": 2.10798978805542,
      "learning_rate": 2.8278870026169228e-05,
      "loss": 1.1289,
      "step": 11220
    },
    {
      "epoch": 0.1738053782162894,
      "grad_norm": 1.9290802478790283,
      "learning_rate": 2.8277321551159035e-05,
      "loss": 1.1249,
      "step": 11230
    },
    {
      "epoch": 0.17396014703037338,
      "grad_norm": 1.233461856842041,
      "learning_rate": 2.8275773076148842e-05,
      "loss": 0.9999,
      "step": 11240
    },
    {
      "epoch": 0.17411491584445735,
      "grad_norm": 1.5877134799957275,
      "learning_rate": 2.8274224601138646e-05,
      "loss": 1.1097,
      "step": 11250
    },
    {
      "epoch": 0.1742696846585413,
      "grad_norm": 1.7053037881851196,
      "learning_rate": 2.8272676126128453e-05,
      "loss": 1.0928,
      "step": 11260
    },
    {
      "epoch": 0.17442445347262528,
      "grad_norm": 1.609436273574829,
      "learning_rate": 2.8271127651118257e-05,
      "loss": 1.0662,
      "step": 11270
    },
    {
      "epoch": 0.17457922228670922,
      "grad_norm": 1.632910132408142,
      "learning_rate": 2.8269579176108064e-05,
      "loss": 1.1323,
      "step": 11280
    },
    {
      "epoch": 0.17473399110079318,
      "grad_norm": 1.312565803527832,
      "learning_rate": 2.8268030701097868e-05,
      "loss": 0.9848,
      "step": 11290
    },
    {
      "epoch": 0.17488875991487715,
      "grad_norm": 1.2419794797897339,
      "learning_rate": 2.8266482226087678e-05,
      "loss": 1.0163,
      "step": 11300
    },
    {
      "epoch": 0.17504352872896112,
      "grad_norm": 1.3282777070999146,
      "learning_rate": 2.8264933751077482e-05,
      "loss": 1.2391,
      "step": 11310
    },
    {
      "epoch": 0.17519829754304508,
      "grad_norm": 1.2679630517959595,
      "learning_rate": 2.826338527606729e-05,
      "loss": 1.2017,
      "step": 11320
    },
    {
      "epoch": 0.17535306635712905,
      "grad_norm": 1.2877508401870728,
      "learning_rate": 2.8261836801057093e-05,
      "loss": 1.0958,
      "step": 11330
    },
    {
      "epoch": 0.175507835171213,
      "grad_norm": 1.589350938796997,
      "learning_rate": 2.82602883260469e-05,
      "loss": 1.0061,
      "step": 11340
    },
    {
      "epoch": 0.17566260398529696,
      "grad_norm": 1.8448538780212402,
      "learning_rate": 2.8258739851036704e-05,
      "loss": 1.1403,
      "step": 11350
    },
    {
      "epoch": 0.17581737279938092,
      "grad_norm": 2.4619140625,
      "learning_rate": 2.825719137602651e-05,
      "loss": 1.0686,
      "step": 11360
    },
    {
      "epoch": 0.1759721416134649,
      "grad_norm": 1.7953267097473145,
      "learning_rate": 2.8255642901016318e-05,
      "loss": 1.2783,
      "step": 11370
    },
    {
      "epoch": 0.17612691042754886,
      "grad_norm": 1.8163954019546509,
      "learning_rate": 2.825409442600612e-05,
      "loss": 0.9724,
      "step": 11380
    },
    {
      "epoch": 0.17628167924163282,
      "grad_norm": 1.3762259483337402,
      "learning_rate": 2.825254595099593e-05,
      "loss": 1.1026,
      "step": 11390
    },
    {
      "epoch": 0.17643644805571676,
      "grad_norm": 1.8027935028076172,
      "learning_rate": 2.8250997475985733e-05,
      "loss": 1.1145,
      "step": 11400
    },
    {
      "epoch": 0.17659121686980073,
      "grad_norm": 1.8021334409713745,
      "learning_rate": 2.824944900097554e-05,
      "loss": 1.0923,
      "step": 11410
    },
    {
      "epoch": 0.1767459856838847,
      "grad_norm": 1.2921473979949951,
      "learning_rate": 2.8247900525965343e-05,
      "loss": 1.0132,
      "step": 11420
    },
    {
      "epoch": 0.17690075449796866,
      "grad_norm": 2.554147720336914,
      "learning_rate": 2.824635205095515e-05,
      "loss": 1.1294,
      "step": 11430
    },
    {
      "epoch": 0.17705552331205263,
      "grad_norm": 1.4925426244735718,
      "learning_rate": 2.8244803575944958e-05,
      "loss": 1.179,
      "step": 11440
    },
    {
      "epoch": 0.1772102921261366,
      "grad_norm": 2.066527843475342,
      "learning_rate": 2.8243255100934765e-05,
      "loss": 1.1079,
      "step": 11450
    },
    {
      "epoch": 0.17736506094022056,
      "grad_norm": 1.7823798656463623,
      "learning_rate": 2.824170662592457e-05,
      "loss": 1.1966,
      "step": 11460
    },
    {
      "epoch": 0.1775198297543045,
      "grad_norm": 1.356339931488037,
      "learning_rate": 2.8240158150914376e-05,
      "loss": 1.0768,
      "step": 11470
    },
    {
      "epoch": 0.17767459856838846,
      "grad_norm": 1.2428909540176392,
      "learning_rate": 2.823860967590418e-05,
      "loss": 1.1192,
      "step": 11480
    },
    {
      "epoch": 0.17782936738247243,
      "grad_norm": 2.985454797744751,
      "learning_rate": 2.8237061200893987e-05,
      "loss": 1.045,
      "step": 11490
    },
    {
      "epoch": 0.1779841361965564,
      "grad_norm": 1.1228076219558716,
      "learning_rate": 2.8235667573384813e-05,
      "loss": 1.0266,
      "step": 11500
    },
    {
      "epoch": 0.17813890501064036,
      "grad_norm": 1.9792063236236572,
      "learning_rate": 2.8234119098374616e-05,
      "loss": 1.0513,
      "step": 11510
    },
    {
      "epoch": 0.17829367382472433,
      "grad_norm": 1.734466791152954,
      "learning_rate": 2.8232570623364427e-05,
      "loss": 0.912,
      "step": 11520
    },
    {
      "epoch": 0.17844844263880827,
      "grad_norm": 1.3339216709136963,
      "learning_rate": 2.823102214835423e-05,
      "loss": 1.0286,
      "step": 11530
    },
    {
      "epoch": 0.17860321145289224,
      "grad_norm": 3.025198221206665,
      "learning_rate": 2.8229473673344038e-05,
      "loss": 1.1219,
      "step": 11540
    },
    {
      "epoch": 0.1787579802669762,
      "grad_norm": 2.288832187652588,
      "learning_rate": 2.822792519833384e-05,
      "loss": 1.274,
      "step": 11550
    },
    {
      "epoch": 0.17891274908106017,
      "grad_norm": 1.7458739280700684,
      "learning_rate": 2.8226376723323645e-05,
      "loss": 1.178,
      "step": 11560
    },
    {
      "epoch": 0.17906751789514413,
      "grad_norm": 1.5365896224975586,
      "learning_rate": 2.8224828248313453e-05,
      "loss": 1.3187,
      "step": 11570
    },
    {
      "epoch": 0.1792222867092281,
      "grad_norm": 1.6571025848388672,
      "learning_rate": 2.8223279773303256e-05,
      "loss": 1.1672,
      "step": 11580
    },
    {
      "epoch": 0.17937705552331204,
      "grad_norm": 1.5586376190185547,
      "learning_rate": 2.8221731298293067e-05,
      "loss": 1.1272,
      "step": 11590
    },
    {
      "epoch": 0.179531824337396,
      "grad_norm": 1.790764331817627,
      "learning_rate": 2.822018282328287e-05,
      "loss": 0.9634,
      "step": 11600
    },
    {
      "epoch": 0.17968659315147997,
      "grad_norm": 1.371569037437439,
      "learning_rate": 2.8218634348272678e-05,
      "loss": 0.947,
      "step": 11610
    },
    {
      "epoch": 0.17984136196556394,
      "grad_norm": 1.3082692623138428,
      "learning_rate": 2.821708587326248e-05,
      "loss": 1.0374,
      "step": 11620
    },
    {
      "epoch": 0.1799961307796479,
      "grad_norm": 2.2822508811950684,
      "learning_rate": 2.821553739825229e-05,
      "loss": 1.0896,
      "step": 11630
    },
    {
      "epoch": 0.18015089959373187,
      "grad_norm": 1.2901378870010376,
      "learning_rate": 2.8213988923242092e-05,
      "loss": 1.1138,
      "step": 11640
    },
    {
      "epoch": 0.18030566840781584,
      "grad_norm": 1.2312607765197754,
      "learning_rate": 2.82124404482319e-05,
      "loss": 1.1323,
      "step": 11650
    },
    {
      "epoch": 0.18046043722189978,
      "grad_norm": 2.4948253631591797,
      "learning_rate": 2.8210891973221707e-05,
      "loss": 0.9379,
      "step": 11660
    },
    {
      "epoch": 0.18061520603598374,
      "grad_norm": 1.5193543434143066,
      "learning_rate": 2.8209343498211514e-05,
      "loss": 1.0723,
      "step": 11670
    },
    {
      "epoch": 0.1807699748500677,
      "grad_norm": 3.4755897521972656,
      "learning_rate": 2.8207795023201317e-05,
      "loss": 0.9658,
      "step": 11680
    },
    {
      "epoch": 0.18092474366415168,
      "grad_norm": 2.708482027053833,
      "learning_rate": 2.8206246548191125e-05,
      "loss": 0.9858,
      "step": 11690
    },
    {
      "epoch": 0.18107951247823564,
      "grad_norm": 1.2368396520614624,
      "learning_rate": 2.820469807318093e-05,
      "loss": 1.3041,
      "step": 11700
    },
    {
      "epoch": 0.1812342812923196,
      "grad_norm": 1.2364401817321777,
      "learning_rate": 2.8203149598170736e-05,
      "loss": 1.0667,
      "step": 11710
    },
    {
      "epoch": 0.18138905010640355,
      "grad_norm": 1.6801804304122925,
      "learning_rate": 2.820160112316054e-05,
      "loss": 1.27,
      "step": 11720
    },
    {
      "epoch": 0.18154381892048752,
      "grad_norm": 1.7600935697555542,
      "learning_rate": 2.820005264815035e-05,
      "loss": 0.8914,
      "step": 11730
    },
    {
      "epoch": 0.18169858773457148,
      "grad_norm": 1.3421945571899414,
      "learning_rate": 2.8198504173140154e-05,
      "loss": 1.1542,
      "step": 11740
    },
    {
      "epoch": 0.18185335654865545,
      "grad_norm": 1.3661577701568604,
      "learning_rate": 2.819695569812996e-05,
      "loss": 1.0873,
      "step": 11750
    },
    {
      "epoch": 0.18200812536273941,
      "grad_norm": 1.2845251560211182,
      "learning_rate": 2.8195407223119764e-05,
      "loss": 1.1645,
      "step": 11760
    },
    {
      "epoch": 0.18216289417682338,
      "grad_norm": 1.3911609649658203,
      "learning_rate": 2.819385874810957e-05,
      "loss": 1.071,
      "step": 11770
    },
    {
      "epoch": 0.18231766299090732,
      "grad_norm": 1.581002116203308,
      "learning_rate": 2.8192310273099375e-05,
      "loss": 1.194,
      "step": 11780
    },
    {
      "epoch": 0.1824724318049913,
      "grad_norm": 2.374675989151001,
      "learning_rate": 2.8190761798089182e-05,
      "loss": 1.2564,
      "step": 11790
    },
    {
      "epoch": 0.18262720061907525,
      "grad_norm": 2.084028482437134,
      "learning_rate": 2.818921332307899e-05,
      "loss": 1.2065,
      "step": 11800
    },
    {
      "epoch": 0.18278196943315922,
      "grad_norm": 1.40131676197052,
      "learning_rate": 2.8187664848068797e-05,
      "loss": 1.2386,
      "step": 11810
    },
    {
      "epoch": 0.18293673824724319,
      "grad_norm": 1.3170214891433716,
      "learning_rate": 2.81861163730586e-05,
      "loss": 1.3509,
      "step": 11820
    },
    {
      "epoch": 0.18309150706132715,
      "grad_norm": 1.4094594717025757,
      "learning_rate": 2.8184567898048404e-05,
      "loss": 0.9033,
      "step": 11830
    },
    {
      "epoch": 0.1832462758754111,
      "grad_norm": 1.4538313150405884,
      "learning_rate": 2.818301942303821e-05,
      "loss": 1.145,
      "step": 11840
    },
    {
      "epoch": 0.18340104468949506,
      "grad_norm": 1.6219233274459839,
      "learning_rate": 2.8181470948028015e-05,
      "loss": 1.1883,
      "step": 11850
    },
    {
      "epoch": 0.18355581350357902,
      "grad_norm": 1.6563667058944702,
      "learning_rate": 2.8179922473017826e-05,
      "loss": 1.0236,
      "step": 11860
    },
    {
      "epoch": 0.183710582317663,
      "grad_norm": 1.852408766746521,
      "learning_rate": 2.817837399800763e-05,
      "loss": 1.059,
      "step": 11870
    },
    {
      "epoch": 0.18386535113174696,
      "grad_norm": 1.9094151258468628,
      "learning_rate": 2.8176825522997437e-05,
      "loss": 1.2403,
      "step": 11880
    },
    {
      "epoch": 0.18402011994583092,
      "grad_norm": 2.593271493911743,
      "learning_rate": 2.817527704798724e-05,
      "loss": 1.1979,
      "step": 11890
    },
    {
      "epoch": 0.1841748887599149,
      "grad_norm": 1.707252860069275,
      "learning_rate": 2.8173728572977047e-05,
      "loss": 1.2279,
      "step": 11900
    },
    {
      "epoch": 0.18432965757399883,
      "grad_norm": 0.9829810261726379,
      "learning_rate": 2.817218009796685e-05,
      "loss": 0.9295,
      "step": 11910
    },
    {
      "epoch": 0.1844844263880828,
      "grad_norm": 1.5265346765518188,
      "learning_rate": 2.817063162295666e-05,
      "loss": 1.1444,
      "step": 11920
    },
    {
      "epoch": 0.18463919520216676,
      "grad_norm": 1.2089108228683472,
      "learning_rate": 2.8169083147946465e-05,
      "loss": 1.18,
      "step": 11930
    },
    {
      "epoch": 0.18479396401625073,
      "grad_norm": 2.526127576828003,
      "learning_rate": 2.8167534672936273e-05,
      "loss": 0.8942,
      "step": 11940
    },
    {
      "epoch": 0.1849487328303347,
      "grad_norm": 1.2637007236480713,
      "learning_rate": 2.8165986197926076e-05,
      "loss": 1.0823,
      "step": 11950
    },
    {
      "epoch": 0.18510350164441866,
      "grad_norm": 1.5741783380508423,
      "learning_rate": 2.8164437722915883e-05,
      "loss": 1.2618,
      "step": 11960
    },
    {
      "epoch": 0.1852582704585026,
      "grad_norm": 1.7909938097000122,
      "learning_rate": 2.8162889247905687e-05,
      "loss": 1.112,
      "step": 11970
    },
    {
      "epoch": 0.18541303927258657,
      "grad_norm": 1.6463501453399658,
      "learning_rate": 2.8161340772895494e-05,
      "loss": 0.9324,
      "step": 11980
    },
    {
      "epoch": 0.18556780808667053,
      "grad_norm": 2.3113789558410645,
      "learning_rate": 2.8159792297885298e-05,
      "loss": 1.1015,
      "step": 11990
    },
    {
      "epoch": 0.1857225769007545,
      "grad_norm": 2.3444502353668213,
      "learning_rate": 2.815824382287511e-05,
      "loss": 1.1223,
      "step": 12000
    },
    {
      "epoch": 0.18587734571483847,
      "grad_norm": 1.5802903175354004,
      "learning_rate": 2.8156695347864912e-05,
      "loss": 1.0588,
      "step": 12010
    },
    {
      "epoch": 0.18603211452892243,
      "grad_norm": 2.547473907470703,
      "learning_rate": 2.815514687285472e-05,
      "loss": 1.0326,
      "step": 12020
    },
    {
      "epoch": 0.18618688334300637,
      "grad_norm": 1.275804877281189,
      "learning_rate": 2.8153598397844523e-05,
      "loss": 1.144,
      "step": 12030
    },
    {
      "epoch": 0.18634165215709034,
      "grad_norm": 1.3065494298934937,
      "learning_rate": 2.815204992283433e-05,
      "loss": 1.0691,
      "step": 12040
    },
    {
      "epoch": 0.1864964209711743,
      "grad_norm": 1.4430302381515503,
      "learning_rate": 2.8150501447824134e-05,
      "loss": 1.0572,
      "step": 12050
    },
    {
      "epoch": 0.18665118978525827,
      "grad_norm": 1.7733519077301025,
      "learning_rate": 2.814895297281394e-05,
      "loss": 1.1682,
      "step": 12060
    },
    {
      "epoch": 0.18680595859934224,
      "grad_norm": 2.7599806785583496,
      "learning_rate": 2.814740449780375e-05,
      "loss": 1.3956,
      "step": 12070
    },
    {
      "epoch": 0.1869607274134262,
      "grad_norm": 1.6025358438491821,
      "learning_rate": 2.8145856022793552e-05,
      "loss": 1.0209,
      "step": 12080
    },
    {
      "epoch": 0.18711549622751017,
      "grad_norm": 1.6577931642532349,
      "learning_rate": 2.814430754778336e-05,
      "loss": 1.1219,
      "step": 12090
    },
    {
      "epoch": 0.1872702650415941,
      "grad_norm": 1.3883240222930908,
      "learning_rate": 2.8142759072773163e-05,
      "loss": 1.1294,
      "step": 12100
    },
    {
      "epoch": 0.18742503385567807,
      "grad_norm": 2.0886635780334473,
      "learning_rate": 2.814121059776297e-05,
      "loss": 1.0304,
      "step": 12110
    },
    {
      "epoch": 0.18757980266976204,
      "grad_norm": 3.5714526176452637,
      "learning_rate": 2.8139662122752774e-05,
      "loss": 1.1995,
      "step": 12120
    },
    {
      "epoch": 0.187734571483846,
      "grad_norm": 1.9713027477264404,
      "learning_rate": 2.813811364774258e-05,
      "loss": 1.0874,
      "step": 12130
    },
    {
      "epoch": 0.18788934029792997,
      "grad_norm": 1.0191117525100708,
      "learning_rate": 2.8136565172732388e-05,
      "loss": 1.069,
      "step": 12140
    },
    {
      "epoch": 0.18804410911201394,
      "grad_norm": 1.7678135633468628,
      "learning_rate": 2.8135016697722195e-05,
      "loss": 1.0984,
      "step": 12150
    },
    {
      "epoch": 0.18819887792609788,
      "grad_norm": 1.21905517578125,
      "learning_rate": 2.8133468222712e-05,
      "loss": 1.0861,
      "step": 12160
    },
    {
      "epoch": 0.18835364674018185,
      "grad_norm": 2.129065752029419,
      "learning_rate": 2.8131919747701806e-05,
      "loss": 1.1629,
      "step": 12170
    },
    {
      "epoch": 0.1885084155542658,
      "grad_norm": 1.6897854804992676,
      "learning_rate": 2.813037127269161e-05,
      "loss": 1.059,
      "step": 12180
    },
    {
      "epoch": 0.18866318436834978,
      "grad_norm": 1.6830371618270874,
      "learning_rate": 2.8128822797681417e-05,
      "loss": 1.1309,
      "step": 12190
    },
    {
      "epoch": 0.18881795318243375,
      "grad_norm": 2.059858560562134,
      "learning_rate": 2.812727432267122e-05,
      "loss": 1.14,
      "step": 12200
    },
    {
      "epoch": 0.1889727219965177,
      "grad_norm": 1.7055888175964355,
      "learning_rate": 2.812572584766103e-05,
      "loss": 1.0516,
      "step": 12210
    },
    {
      "epoch": 0.18912749081060165,
      "grad_norm": 2.5531489849090576,
      "learning_rate": 2.8124177372650835e-05,
      "loss": 1.174,
      "step": 12220
    },
    {
      "epoch": 0.18928225962468562,
      "grad_norm": 2.259439706802368,
      "learning_rate": 2.8122628897640642e-05,
      "loss": 1.2701,
      "step": 12230
    },
    {
      "epoch": 0.18943702843876958,
      "grad_norm": 1.869841456413269,
      "learning_rate": 2.8121080422630446e-05,
      "loss": 0.9136,
      "step": 12240
    },
    {
      "epoch": 0.18959179725285355,
      "grad_norm": 1.746473789215088,
      "learning_rate": 2.8119531947620253e-05,
      "loss": 1.0594,
      "step": 12250
    },
    {
      "epoch": 0.18974656606693752,
      "grad_norm": 1.9011139869689941,
      "learning_rate": 2.8117983472610057e-05,
      "loss": 1.1375,
      "step": 12260
    },
    {
      "epoch": 0.18990133488102148,
      "grad_norm": 1.9230971336364746,
      "learning_rate": 2.8116434997599864e-05,
      "loss": 1.036,
      "step": 12270
    },
    {
      "epoch": 0.19005610369510545,
      "grad_norm": 2.3788793087005615,
      "learning_rate": 2.811488652258967e-05,
      "loss": 0.9721,
      "step": 12280
    },
    {
      "epoch": 0.1902108725091894,
      "grad_norm": 1.9049286842346191,
      "learning_rate": 2.811333804757948e-05,
      "loss": 1.1688,
      "step": 12290
    },
    {
      "epoch": 0.19036564132327335,
      "grad_norm": 1.850377082824707,
      "learning_rate": 2.8111789572569282e-05,
      "loss": 1.0891,
      "step": 12300
    },
    {
      "epoch": 0.19052041013735732,
      "grad_norm": 1.638200283050537,
      "learning_rate": 2.811024109755909e-05,
      "loss": 1.0216,
      "step": 12310
    },
    {
      "epoch": 0.1906751789514413,
      "grad_norm": 1.6517668962478638,
      "learning_rate": 2.8108692622548893e-05,
      "loss": 0.9946,
      "step": 12320
    },
    {
      "epoch": 0.19082994776552525,
      "grad_norm": 1.6223082542419434,
      "learning_rate": 2.8107144147538697e-05,
      "loss": 1.1932,
      "step": 12330
    },
    {
      "epoch": 0.19098471657960922,
      "grad_norm": 2.2590911388397217,
      "learning_rate": 2.8105595672528507e-05,
      "loss": 1.0697,
      "step": 12340
    },
    {
      "epoch": 0.19113948539369316,
      "grad_norm": 1.288077712059021,
      "learning_rate": 2.810404719751831e-05,
      "loss": 1.213,
      "step": 12350
    },
    {
      "epoch": 0.19129425420777713,
      "grad_norm": 1.6860049962997437,
      "learning_rate": 2.8102498722508118e-05,
      "loss": 1.0492,
      "step": 12360
    },
    {
      "epoch": 0.1914490230218611,
      "grad_norm": 1.743356704711914,
      "learning_rate": 2.8100950247497922e-05,
      "loss": 1.108,
      "step": 12370
    },
    {
      "epoch": 0.19160379183594506,
      "grad_norm": 2.001217842102051,
      "learning_rate": 2.809940177248773e-05,
      "loss": 1.2405,
      "step": 12380
    },
    {
      "epoch": 0.19175856065002903,
      "grad_norm": 1.6657421588897705,
      "learning_rate": 2.8097853297477533e-05,
      "loss": 1.004,
      "step": 12390
    },
    {
      "epoch": 0.191913329464113,
      "grad_norm": 1.871703028678894,
      "learning_rate": 2.809630482246734e-05,
      "loss": 1.1288,
      "step": 12400
    },
    {
      "epoch": 0.19206809827819693,
      "grad_norm": 1.0558581352233887,
      "learning_rate": 2.8094756347457147e-05,
      "loss": 1.1304,
      "step": 12410
    },
    {
      "epoch": 0.1922228670922809,
      "grad_norm": 1.9354580640792847,
      "learning_rate": 2.8093207872446954e-05,
      "loss": 1.0501,
      "step": 12420
    },
    {
      "epoch": 0.19237763590636486,
      "grad_norm": 1.5362449884414673,
      "learning_rate": 2.8091659397436758e-05,
      "loss": 1.223,
      "step": 12430
    },
    {
      "epoch": 0.19253240472044883,
      "grad_norm": 2.5789003372192383,
      "learning_rate": 2.8090110922426565e-05,
      "loss": 1.0732,
      "step": 12440
    },
    {
      "epoch": 0.1926871735345328,
      "grad_norm": 1.4653114080429077,
      "learning_rate": 2.808856244741637e-05,
      "loss": 1.0176,
      "step": 12450
    },
    {
      "epoch": 0.19284194234861676,
      "grad_norm": 1.6167497634887695,
      "learning_rate": 2.8087013972406176e-05,
      "loss": 1.2181,
      "step": 12460
    },
    {
      "epoch": 0.19299671116270073,
      "grad_norm": 1.4218412637710571,
      "learning_rate": 2.808546549739598e-05,
      "loss": 1.0384,
      "step": 12470
    },
    {
      "epoch": 0.19315147997678467,
      "grad_norm": 1.9896087646484375,
      "learning_rate": 2.808391702238579e-05,
      "loss": 1.0602,
      "step": 12480
    },
    {
      "epoch": 0.19330624879086863,
      "grad_norm": 2.0600173473358154,
      "learning_rate": 2.8082368547375594e-05,
      "loss": 1.1833,
      "step": 12490
    },
    {
      "epoch": 0.1934610176049526,
      "grad_norm": 1.616204023361206,
      "learning_rate": 2.80808200723654e-05,
      "loss": 1.1184,
      "step": 12500
    },
    {
      "epoch": 0.19361578641903657,
      "grad_norm": 2.345411539077759,
      "learning_rate": 2.8079271597355205e-05,
      "loss": 0.9548,
      "step": 12510
    },
    {
      "epoch": 0.19377055523312053,
      "grad_norm": 1.4374374151229858,
      "learning_rate": 2.8077723122345012e-05,
      "loss": 1.1436,
      "step": 12520
    },
    {
      "epoch": 0.1939253240472045,
      "grad_norm": 1.588091492652893,
      "learning_rate": 2.8076174647334816e-05,
      "loss": 1.168,
      "step": 12530
    },
    {
      "epoch": 0.19408009286128844,
      "grad_norm": 1.2031408548355103,
      "learning_rate": 2.8074626172324623e-05,
      "loss": 1.0328,
      "step": 12540
    },
    {
      "epoch": 0.1942348616753724,
      "grad_norm": 1.452268123626709,
      "learning_rate": 2.807307769731443e-05,
      "loss": 1.1092,
      "step": 12550
    },
    {
      "epoch": 0.19438963048945637,
      "grad_norm": 2.2120494842529297,
      "learning_rate": 2.8071529222304237e-05,
      "loss": 1.2653,
      "step": 12560
    },
    {
      "epoch": 0.19454439930354034,
      "grad_norm": 2.014286994934082,
      "learning_rate": 2.806998074729404e-05,
      "loss": 1.2673,
      "step": 12570
    },
    {
      "epoch": 0.1946991681176243,
      "grad_norm": 1.648377537727356,
      "learning_rate": 2.8068432272283845e-05,
      "loss": 1.2624,
      "step": 12580
    },
    {
      "epoch": 0.19485393693170827,
      "grad_norm": 1.789332628250122,
      "learning_rate": 2.8066883797273652e-05,
      "loss": 1.0213,
      "step": 12590
    },
    {
      "epoch": 0.1950087057457922,
      "grad_norm": 1.5717424154281616,
      "learning_rate": 2.8065335322263456e-05,
      "loss": 1.1331,
      "step": 12600
    },
    {
      "epoch": 0.19516347455987618,
      "grad_norm": 1.7282929420471191,
      "learning_rate": 2.8063786847253263e-05,
      "loss": 1.1973,
      "step": 12610
    },
    {
      "epoch": 0.19531824337396014,
      "grad_norm": 1.0914660692214966,
      "learning_rate": 2.806223837224307e-05,
      "loss": 1.08,
      "step": 12620
    },
    {
      "epoch": 0.1954730121880441,
      "grad_norm": 2.67018723487854,
      "learning_rate": 2.8060689897232877e-05,
      "loss": 1.251,
      "step": 12630
    },
    {
      "epoch": 0.19562778100212808,
      "grad_norm": 2.096269130706787,
      "learning_rate": 2.805914142222268e-05,
      "loss": 1.0999,
      "step": 12640
    },
    {
      "epoch": 0.19578254981621204,
      "grad_norm": 2.236140012741089,
      "learning_rate": 2.8057592947212488e-05,
      "loss": 1.2641,
      "step": 12650
    },
    {
      "epoch": 0.195937318630296,
      "grad_norm": 1.319326639175415,
      "learning_rate": 2.8056044472202292e-05,
      "loss": 1.0763,
      "step": 12660
    },
    {
      "epoch": 0.19609208744437995,
      "grad_norm": 1.2218217849731445,
      "learning_rate": 2.80544959971921e-05,
      "loss": 1.2302,
      "step": 12670
    },
    {
      "epoch": 0.19624685625846391,
      "grad_norm": 2.2771403789520264,
      "learning_rate": 2.8052947522181903e-05,
      "loss": 1.1811,
      "step": 12680
    },
    {
      "epoch": 0.19640162507254788,
      "grad_norm": 1.4502772092819214,
      "learning_rate": 2.8051399047171713e-05,
      "loss": 1.0286,
      "step": 12690
    },
    {
      "epoch": 0.19655639388663185,
      "grad_norm": 2.0154333114624023,
      "learning_rate": 2.8049850572161517e-05,
      "loss": 1.0673,
      "step": 12700
    },
    {
      "epoch": 0.1967111627007158,
      "grad_norm": 2.943758249282837,
      "learning_rate": 2.8048302097151324e-05,
      "loss": 0.9988,
      "step": 12710
    },
    {
      "epoch": 0.19686593151479978,
      "grad_norm": 1.2209832668304443,
      "learning_rate": 2.8046753622141128e-05,
      "loss": 1.1441,
      "step": 12720
    },
    {
      "epoch": 0.19702070032888372,
      "grad_norm": 1.7729705572128296,
      "learning_rate": 2.8045205147130935e-05,
      "loss": 1.1765,
      "step": 12730
    },
    {
      "epoch": 0.19717546914296769,
      "grad_norm": 1.4961692094802856,
      "learning_rate": 2.804365667212074e-05,
      "loss": 0.9725,
      "step": 12740
    },
    {
      "epoch": 0.19733023795705165,
      "grad_norm": 1.7881501913070679,
      "learning_rate": 2.804210819711055e-05,
      "loss": 1.0377,
      "step": 12750
    },
    {
      "epoch": 0.19748500677113562,
      "grad_norm": 1.769911527633667,
      "learning_rate": 2.8040559722100353e-05,
      "loss": 1.1651,
      "step": 12760
    },
    {
      "epoch": 0.19763977558521958,
      "grad_norm": 1.390476107597351,
      "learning_rate": 2.803901124709016e-05,
      "loss": 1.0269,
      "step": 12770
    },
    {
      "epoch": 0.19779454439930355,
      "grad_norm": 1.2154748439788818,
      "learning_rate": 2.8037462772079964e-05,
      "loss": 0.8745,
      "step": 12780
    },
    {
      "epoch": 0.1979493132133875,
      "grad_norm": 1.6072750091552734,
      "learning_rate": 2.803591429706977e-05,
      "loss": 1.0695,
      "step": 12790
    },
    {
      "epoch": 0.19810408202747146,
      "grad_norm": 1.5623103380203247,
      "learning_rate": 2.8034365822059575e-05,
      "loss": 1.1258,
      "step": 12800
    },
    {
      "epoch": 0.19825885084155542,
      "grad_norm": 1.4910250902175903,
      "learning_rate": 2.8032817347049382e-05,
      "loss": 1.0398,
      "step": 12810
    },
    {
      "epoch": 0.1984136196556394,
      "grad_norm": 2.2352097034454346,
      "learning_rate": 2.803126887203919e-05,
      "loss": 1.0984,
      "step": 12820
    },
    {
      "epoch": 0.19856838846972336,
      "grad_norm": 1.1987687349319458,
      "learning_rate": 2.8029720397028993e-05,
      "loss": 1.0699,
      "step": 12830
    },
    {
      "epoch": 0.19872315728380732,
      "grad_norm": 1.7295528650283813,
      "learning_rate": 2.80281719220188e-05,
      "loss": 0.9563,
      "step": 12840
    },
    {
      "epoch": 0.19887792609789126,
      "grad_norm": 1.7861685752868652,
      "learning_rate": 2.8026623447008604e-05,
      "loss": 1.104,
      "step": 12850
    },
    {
      "epoch": 0.19903269491197523,
      "grad_norm": 2.0539591312408447,
      "learning_rate": 2.802507497199841e-05,
      "loss": 0.9448,
      "step": 12860
    },
    {
      "epoch": 0.1991874637260592,
      "grad_norm": 1.1447409391403198,
      "learning_rate": 2.8023526496988215e-05,
      "loss": 1.116,
      "step": 12870
    },
    {
      "epoch": 0.19934223254014316,
      "grad_norm": 1.3242058753967285,
      "learning_rate": 2.802197802197802e-05,
      "loss": 1.116,
      "step": 12880
    },
    {
      "epoch": 0.19949700135422713,
      "grad_norm": 1.7507445812225342,
      "learning_rate": 2.802042954696783e-05,
      "loss": 1.0956,
      "step": 12890
    },
    {
      "epoch": 0.1996517701683111,
      "grad_norm": 1.9121187925338745,
      "learning_rate": 2.8018881071957636e-05,
      "loss": 1.0111,
      "step": 12900
    },
    {
      "epoch": 0.19980653898239506,
      "grad_norm": 1.273703932762146,
      "learning_rate": 2.801733259694744e-05,
      "loss": 1.0497,
      "step": 12910
    },
    {
      "epoch": 0.199961307796479,
      "grad_norm": 1.2455233335494995,
      "learning_rate": 2.8015784121937247e-05,
      "loss": 0.9969,
      "step": 12920
    },
    {
      "epoch": 0.20011607661056297,
      "grad_norm": 1.9908006191253662,
      "learning_rate": 2.801423564692705e-05,
      "loss": 1.2136,
      "step": 12930
    },
    {
      "epoch": 0.20027084542464693,
      "grad_norm": 1.5442556142807007,
      "learning_rate": 2.8012687171916858e-05,
      "loss": 1.0107,
      "step": 12940
    },
    {
      "epoch": 0.2004256142387309,
      "grad_norm": 2.259075403213501,
      "learning_rate": 2.801113869690666e-05,
      "loss": 1.0012,
      "step": 12950
    },
    {
      "epoch": 0.20058038305281486,
      "grad_norm": 1.3101203441619873,
      "learning_rate": 2.8009590221896472e-05,
      "loss": 1.015,
      "step": 12960
    },
    {
      "epoch": 0.20073515186689883,
      "grad_norm": 2.13104510307312,
      "learning_rate": 2.8008041746886276e-05,
      "loss": 1.0135,
      "step": 12970
    },
    {
      "epoch": 0.20088992068098277,
      "grad_norm": 1.30000638961792,
      "learning_rate": 2.8006493271876083e-05,
      "loss": 0.9117,
      "step": 12980
    },
    {
      "epoch": 0.20104468949506674,
      "grad_norm": 1.6590462923049927,
      "learning_rate": 2.8004944796865887e-05,
      "loss": 1.0819,
      "step": 12990
    },
    {
      "epoch": 0.2011994583091507,
      "grad_norm": 1.428925633430481,
      "learning_rate": 2.8003396321855694e-05,
      "loss": 1.0214,
      "step": 13000
    },
    {
      "epoch": 0.20135422712323467,
      "grad_norm": 1.527072787284851,
      "learning_rate": 2.8001847846845498e-05,
      "loss": 1.0065,
      "step": 13010
    },
    {
      "epoch": 0.20150899593731864,
      "grad_norm": 1.3692035675048828,
      "learning_rate": 2.8000299371835305e-05,
      "loss": 1.0983,
      "step": 13020
    },
    {
      "epoch": 0.2016637647514026,
      "grad_norm": 2.6969385147094727,
      "learning_rate": 2.7998750896825112e-05,
      "loss": 1.1307,
      "step": 13030
    },
    {
      "epoch": 0.20181853356548654,
      "grad_norm": 1.5811653137207031,
      "learning_rate": 2.799720242181492e-05,
      "loss": 1.1532,
      "step": 13040
    },
    {
      "epoch": 0.2019733023795705,
      "grad_norm": 1.4071754217147827,
      "learning_rate": 2.7995653946804723e-05,
      "loss": 1.042,
      "step": 13050
    },
    {
      "epoch": 0.20212807119365447,
      "grad_norm": 2.3750243186950684,
      "learning_rate": 2.799410547179453e-05,
      "loss": 1.0889,
      "step": 13060
    },
    {
      "epoch": 0.20228284000773844,
      "grad_norm": 2.288919448852539,
      "learning_rate": 2.7992556996784334e-05,
      "loss": 1.0465,
      "step": 13070
    },
    {
      "epoch": 0.2024376088218224,
      "grad_norm": 2.3234975337982178,
      "learning_rate": 2.7991008521774137e-05,
      "loss": 1.24,
      "step": 13080
    },
    {
      "epoch": 0.20259237763590637,
      "grad_norm": 1.9641987085342407,
      "learning_rate": 2.7989460046763944e-05,
      "loss": 1.186,
      "step": 13090
    },
    {
      "epoch": 0.20274714644999034,
      "grad_norm": 2.078861713409424,
      "learning_rate": 2.798791157175375e-05,
      "loss": 0.9512,
      "step": 13100
    },
    {
      "epoch": 0.20290191526407428,
      "grad_norm": 2.0888848304748535,
      "learning_rate": 2.798636309674356e-05,
      "loss": 0.9702,
      "step": 13110
    },
    {
      "epoch": 0.20305668407815824,
      "grad_norm": 2.366462230682373,
      "learning_rate": 2.7984814621733363e-05,
      "loss": 1.1339,
      "step": 13120
    },
    {
      "epoch": 0.2032114528922422,
      "grad_norm": 1.3987489938735962,
      "learning_rate": 2.798326614672317e-05,
      "loss": 0.9919,
      "step": 13130
    },
    {
      "epoch": 0.20336622170632618,
      "grad_norm": 1.4168767929077148,
      "learning_rate": 2.7981717671712973e-05,
      "loss": 1.0978,
      "step": 13140
    },
    {
      "epoch": 0.20352099052041014,
      "grad_norm": 1.327309489250183,
      "learning_rate": 2.798016919670278e-05,
      "loss": 0.9723,
      "step": 13150
    },
    {
      "epoch": 0.2036757593344941,
      "grad_norm": 1.2168288230895996,
      "learning_rate": 2.7978620721692588e-05,
      "loss": 1.1234,
      "step": 13160
    },
    {
      "epoch": 0.20383052814857805,
      "grad_norm": 1.9583278894424438,
      "learning_rate": 2.7977072246682395e-05,
      "loss": 1.1854,
      "step": 13170
    },
    {
      "epoch": 0.20398529696266202,
      "grad_norm": 1.415589451789856,
      "learning_rate": 2.79755237716722e-05,
      "loss": 1.1458,
      "step": 13180
    },
    {
      "epoch": 0.20414006577674598,
      "grad_norm": 1.6017146110534668,
      "learning_rate": 2.7973975296662006e-05,
      "loss": 1.1054,
      "step": 13190
    },
    {
      "epoch": 0.20429483459082995,
      "grad_norm": 1.5421346426010132,
      "learning_rate": 2.797242682165181e-05,
      "loss": 1.3582,
      "step": 13200
    },
    {
      "epoch": 0.20444960340491392,
      "grad_norm": 1.3540472984313965,
      "learning_rate": 2.7970878346641617e-05,
      "loss": 0.9606,
      "step": 13210
    },
    {
      "epoch": 0.20460437221899788,
      "grad_norm": 2.075033187866211,
      "learning_rate": 2.796932987163142e-05,
      "loss": 1.0478,
      "step": 13220
    },
    {
      "epoch": 0.20475914103308182,
      "grad_norm": 2.1032791137695312,
      "learning_rate": 2.796778139662123e-05,
      "loss": 1.1364,
      "step": 13230
    },
    {
      "epoch": 0.2049139098471658,
      "grad_norm": 2.392943859100342,
      "learning_rate": 2.7966232921611035e-05,
      "loss": 1.0804,
      "step": 13240
    },
    {
      "epoch": 0.20506867866124975,
      "grad_norm": 1.7916702032089233,
      "learning_rate": 2.7964684446600842e-05,
      "loss": 0.9871,
      "step": 13250
    },
    {
      "epoch": 0.20522344747533372,
      "grad_norm": 1.8729867935180664,
      "learning_rate": 2.7963135971590646e-05,
      "loss": 1.0207,
      "step": 13260
    },
    {
      "epoch": 0.2053782162894177,
      "grad_norm": 1.4763662815093994,
      "learning_rate": 2.7961587496580453e-05,
      "loss": 1.0107,
      "step": 13270
    },
    {
      "epoch": 0.20553298510350165,
      "grad_norm": 1.8587793111801147,
      "learning_rate": 2.7960039021570256e-05,
      "loss": 1.1912,
      "step": 13280
    },
    {
      "epoch": 0.20568775391758562,
      "grad_norm": 1.6813665628433228,
      "learning_rate": 2.7958490546560064e-05,
      "loss": 1.1771,
      "step": 13290
    },
    {
      "epoch": 0.20584252273166956,
      "grad_norm": 1.6123336553573608,
      "learning_rate": 2.795694207154987e-05,
      "loss": 1.0963,
      "step": 13300
    },
    {
      "epoch": 0.20599729154575352,
      "grad_norm": 1.3717560768127441,
      "learning_rate": 2.7955393596539678e-05,
      "loss": 1.0791,
      "step": 13310
    },
    {
      "epoch": 0.2061520603598375,
      "grad_norm": 1.5079846382141113,
      "learning_rate": 2.795384512152948e-05,
      "loss": 1.1399,
      "step": 13320
    },
    {
      "epoch": 0.20630682917392146,
      "grad_norm": 2.2615373134613037,
      "learning_rate": 2.795229664651929e-05,
      "loss": 1.2489,
      "step": 13330
    },
    {
      "epoch": 0.20646159798800542,
      "grad_norm": 1.7138687372207642,
      "learning_rate": 2.7950748171509092e-05,
      "loss": 0.9647,
      "step": 13340
    },
    {
      "epoch": 0.2066163668020894,
      "grad_norm": 1.9696745872497559,
      "learning_rate": 2.7949199696498896e-05,
      "loss": 1.1551,
      "step": 13350
    },
    {
      "epoch": 0.20677113561617333,
      "grad_norm": 2.6852738857269287,
      "learning_rate": 2.7947651221488703e-05,
      "loss": 1.16,
      "step": 13360
    },
    {
      "epoch": 0.2069259044302573,
      "grad_norm": 1.7098276615142822,
      "learning_rate": 2.794610274647851e-05,
      "loss": 1.0206,
      "step": 13370
    },
    {
      "epoch": 0.20708067324434126,
      "grad_norm": 1.7420434951782227,
      "learning_rate": 2.7944554271468318e-05,
      "loss": 1.1853,
      "step": 13380
    },
    {
      "epoch": 0.20723544205842523,
      "grad_norm": 2.1386220455169678,
      "learning_rate": 2.794300579645812e-05,
      "loss": 1.0959,
      "step": 13390
    },
    {
      "epoch": 0.2073902108725092,
      "grad_norm": 1.3991037607192993,
      "learning_rate": 2.794145732144793e-05,
      "loss": 1.1853,
      "step": 13400
    },
    {
      "epoch": 0.20754497968659316,
      "grad_norm": 2.129127264022827,
      "learning_rate": 2.7939908846437732e-05,
      "loss": 1.1025,
      "step": 13410
    },
    {
      "epoch": 0.2076997485006771,
      "grad_norm": 1.5069408416748047,
      "learning_rate": 2.793836037142754e-05,
      "loss": 1.1201,
      "step": 13420
    },
    {
      "epoch": 0.20785451731476107,
      "grad_norm": 1.9381030797958374,
      "learning_rate": 2.7936811896417343e-05,
      "loss": 1.1432,
      "step": 13430
    },
    {
      "epoch": 0.20800928612884503,
      "grad_norm": 1.7231460809707642,
      "learning_rate": 2.7935263421407154e-05,
      "loss": 1.0866,
      "step": 13440
    },
    {
      "epoch": 0.208164054942929,
      "grad_norm": 1.7219828367233276,
      "learning_rate": 2.7933714946396957e-05,
      "loss": 0.9665,
      "step": 13450
    },
    {
      "epoch": 0.20831882375701297,
      "grad_norm": 1.4668636322021484,
      "learning_rate": 2.7932166471386765e-05,
      "loss": 0.9785,
      "step": 13460
    },
    {
      "epoch": 0.20847359257109693,
      "grad_norm": 1.7311396598815918,
      "learning_rate": 2.793061799637657e-05,
      "loss": 0.926,
      "step": 13470
    },
    {
      "epoch": 0.2086283613851809,
      "grad_norm": 1.3653454780578613,
      "learning_rate": 2.7929069521366375e-05,
      "loss": 1.0174,
      "step": 13480
    },
    {
      "epoch": 0.20878313019926484,
      "grad_norm": 1.7735776901245117,
      "learning_rate": 2.792752104635618e-05,
      "loss": 0.9929,
      "step": 13490
    },
    {
      "epoch": 0.2089378990133488,
      "grad_norm": 2.069418430328369,
      "learning_rate": 2.7926127418847005e-05,
      "loss": 1.0348,
      "step": 13500
    },
    {
      "epoch": 0.20909266782743277,
      "grad_norm": 1.8903087377548218,
      "learning_rate": 2.792457894383681e-05,
      "loss": 1.1628,
      "step": 13510
    },
    {
      "epoch": 0.20924743664151674,
      "grad_norm": 2.0837647914886475,
      "learning_rate": 2.792303046882662e-05,
      "loss": 1.053,
      "step": 13520
    },
    {
      "epoch": 0.2094022054556007,
      "grad_norm": 2.123971700668335,
      "learning_rate": 2.7921481993816423e-05,
      "loss": 1.072,
      "step": 13530
    },
    {
      "epoch": 0.20955697426968467,
      "grad_norm": 1.6617792844772339,
      "learning_rate": 2.791993351880623e-05,
      "loss": 1.2039,
      "step": 13540
    },
    {
      "epoch": 0.2097117430837686,
      "grad_norm": 1.363961935043335,
      "learning_rate": 2.7918385043796034e-05,
      "loss": 0.9955,
      "step": 13550
    },
    {
      "epoch": 0.20986651189785258,
      "grad_norm": 1.6897839307785034,
      "learning_rate": 2.791683656878584e-05,
      "loss": 1.1888,
      "step": 13560
    },
    {
      "epoch": 0.21002128071193654,
      "grad_norm": 2.0073275566101074,
      "learning_rate": 2.7915288093775645e-05,
      "loss": 1.014,
      "step": 13570
    },
    {
      "epoch": 0.2101760495260205,
      "grad_norm": 1.7952059507369995,
      "learning_rate": 2.7913739618765452e-05,
      "loss": 1.0328,
      "step": 13580
    },
    {
      "epoch": 0.21033081834010448,
      "grad_norm": 1.329088807106018,
      "learning_rate": 2.791219114375526e-05,
      "loss": 1.0138,
      "step": 13590
    },
    {
      "epoch": 0.21048558715418844,
      "grad_norm": 2.167464256286621,
      "learning_rate": 2.7910642668745067e-05,
      "loss": 1.1742,
      "step": 13600
    },
    {
      "epoch": 0.21064035596827238,
      "grad_norm": 1.4022071361541748,
      "learning_rate": 2.790909419373487e-05,
      "loss": 1.2606,
      "step": 13610
    },
    {
      "epoch": 0.21079512478235635,
      "grad_norm": 1.3623052835464478,
      "learning_rate": 2.7907545718724677e-05,
      "loss": 1.1393,
      "step": 13620
    },
    {
      "epoch": 0.2109498935964403,
      "grad_norm": 1.7839229106903076,
      "learning_rate": 2.790599724371448e-05,
      "loss": 0.9132,
      "step": 13630
    },
    {
      "epoch": 0.21110466241052428,
      "grad_norm": 2.0077786445617676,
      "learning_rate": 2.790444876870429e-05,
      "loss": 1.1536,
      "step": 13640
    },
    {
      "epoch": 0.21125943122460825,
      "grad_norm": 2.7656147480010986,
      "learning_rate": 2.7902900293694092e-05,
      "loss": 1.1353,
      "step": 13650
    },
    {
      "epoch": 0.2114142000386922,
      "grad_norm": 1.6446521282196045,
      "learning_rate": 2.7901351818683903e-05,
      "loss": 1.1889,
      "step": 13660
    },
    {
      "epoch": 0.21156896885277618,
      "grad_norm": 1.896126389503479,
      "learning_rate": 2.7899803343673706e-05,
      "loss": 0.992,
      "step": 13670
    },
    {
      "epoch": 0.21172373766686012,
      "grad_norm": 2.1451704502105713,
      "learning_rate": 2.7898254868663513e-05,
      "loss": 1.1466,
      "step": 13680
    },
    {
      "epoch": 0.21187850648094408,
      "grad_norm": 1.6157500743865967,
      "learning_rate": 2.7896706393653317e-05,
      "loss": 1.0719,
      "step": 13690
    },
    {
      "epoch": 0.21203327529502805,
      "grad_norm": 1.2391620874404907,
      "learning_rate": 2.7895157918643124e-05,
      "loss": 1.0638,
      "step": 13700
    },
    {
      "epoch": 0.21218804410911202,
      "grad_norm": 1.5184156894683838,
      "learning_rate": 2.7893609443632928e-05,
      "loss": 1.2191,
      "step": 13710
    },
    {
      "epoch": 0.21234281292319598,
      "grad_norm": 1.3937032222747803,
      "learning_rate": 2.7892060968622735e-05,
      "loss": 0.9202,
      "step": 13720
    },
    {
      "epoch": 0.21249758173727995,
      "grad_norm": 2.376173496246338,
      "learning_rate": 2.7890512493612542e-05,
      "loss": 1.0527,
      "step": 13730
    },
    {
      "epoch": 0.2126523505513639,
      "grad_norm": 1.3099262714385986,
      "learning_rate": 2.788896401860235e-05,
      "loss": 1.1208,
      "step": 13740
    },
    {
      "epoch": 0.21280711936544786,
      "grad_norm": 2.2129979133605957,
      "learning_rate": 2.7887415543592153e-05,
      "loss": 1.2572,
      "step": 13750
    },
    {
      "epoch": 0.21296188817953182,
      "grad_norm": 1.8813303709030151,
      "learning_rate": 2.788586706858196e-05,
      "loss": 1.1093,
      "step": 13760
    },
    {
      "epoch": 0.2131166569936158,
      "grad_norm": 1.507819652557373,
      "learning_rate": 2.7884318593571764e-05,
      "loss": 1.1415,
      "step": 13770
    },
    {
      "epoch": 0.21327142580769975,
      "grad_norm": 1.1220526695251465,
      "learning_rate": 2.7882770118561568e-05,
      "loss": 1.1336,
      "step": 13780
    },
    {
      "epoch": 0.21342619462178372,
      "grad_norm": 1.2868142127990723,
      "learning_rate": 2.7881221643551375e-05,
      "loss": 1.1092,
      "step": 13790
    },
    {
      "epoch": 0.21358096343586766,
      "grad_norm": 1.8503273725509644,
      "learning_rate": 2.7879673168541182e-05,
      "loss": 1.032,
      "step": 13800
    },
    {
      "epoch": 0.21373573224995163,
      "grad_norm": 1.366873025894165,
      "learning_rate": 2.787812469353099e-05,
      "loss": 0.9978,
      "step": 13810
    },
    {
      "epoch": 0.2138905010640356,
      "grad_norm": 1.6446114778518677,
      "learning_rate": 2.7876576218520793e-05,
      "loss": 1.1625,
      "step": 13820
    },
    {
      "epoch": 0.21404526987811956,
      "grad_norm": 1.6459531784057617,
      "learning_rate": 2.78750277435106e-05,
      "loss": 1.1739,
      "step": 13830
    },
    {
      "epoch": 0.21420003869220353,
      "grad_norm": 1.4624680280685425,
      "learning_rate": 2.7873479268500404e-05,
      "loss": 1.0643,
      "step": 13840
    },
    {
      "epoch": 0.2143548075062875,
      "grad_norm": 2.2416129112243652,
      "learning_rate": 2.787193079349021e-05,
      "loss": 1.2867,
      "step": 13850
    },
    {
      "epoch": 0.21450957632037146,
      "grad_norm": 1.8159527778625488,
      "learning_rate": 2.7870382318480018e-05,
      "loss": 1.0582,
      "step": 13860
    },
    {
      "epoch": 0.2146643451344554,
      "grad_norm": 1.7144713401794434,
      "learning_rate": 2.7868833843469825e-05,
      "loss": 1.2356,
      "step": 13870
    },
    {
      "epoch": 0.21481911394853936,
      "grad_norm": 2.208467721939087,
      "learning_rate": 2.786728536845963e-05,
      "loss": 1.0316,
      "step": 13880
    },
    {
      "epoch": 0.21497388276262333,
      "grad_norm": 1.389225721359253,
      "learning_rate": 2.7865736893449436e-05,
      "loss": 0.7988,
      "step": 13890
    },
    {
      "epoch": 0.2151286515767073,
      "grad_norm": 1.4753408432006836,
      "learning_rate": 2.786418841843924e-05,
      "loss": 1.2073,
      "step": 13900
    },
    {
      "epoch": 0.21528342039079126,
      "grad_norm": 1.573614239692688,
      "learning_rate": 2.7862639943429047e-05,
      "loss": 1.2091,
      "step": 13910
    },
    {
      "epoch": 0.21543818920487523,
      "grad_norm": 1.3391073942184448,
      "learning_rate": 2.786109146841885e-05,
      "loss": 1.0317,
      "step": 13920
    },
    {
      "epoch": 0.21559295801895917,
      "grad_norm": 1.6152868270874023,
      "learning_rate": 2.785954299340866e-05,
      "loss": 1.1514,
      "step": 13930
    },
    {
      "epoch": 0.21574772683304314,
      "grad_norm": 1.2209700345993042,
      "learning_rate": 2.7857994518398465e-05,
      "loss": 1.1175,
      "step": 13940
    },
    {
      "epoch": 0.2159024956471271,
      "grad_norm": 1.5513277053833008,
      "learning_rate": 2.7856446043388272e-05,
      "loss": 1.0037,
      "step": 13950
    },
    {
      "epoch": 0.21605726446121107,
      "grad_norm": 1.6328701972961426,
      "learning_rate": 2.7854897568378076e-05,
      "loss": 1.2655,
      "step": 13960
    },
    {
      "epoch": 0.21621203327529503,
      "grad_norm": 1.4401099681854248,
      "learning_rate": 2.7853349093367883e-05,
      "loss": 1.1167,
      "step": 13970
    },
    {
      "epoch": 0.216366802089379,
      "grad_norm": 1.948284387588501,
      "learning_rate": 2.7851800618357687e-05,
      "loss": 1.1461,
      "step": 13980
    },
    {
      "epoch": 0.21652157090346294,
      "grad_norm": 1.9585468769073486,
      "learning_rate": 2.7850252143347494e-05,
      "loss": 0.9754,
      "step": 13990
    },
    {
      "epoch": 0.2166763397175469,
      "grad_norm": 2.1168031692504883,
      "learning_rate": 2.78487036683373e-05,
      "loss": 1.094,
      "step": 14000
    },
    {
      "epoch": 0.21683110853163087,
      "grad_norm": 1.6157008409500122,
      "learning_rate": 2.784715519332711e-05,
      "loss": 0.9161,
      "step": 14010
    },
    {
      "epoch": 0.21698587734571484,
      "grad_norm": 1.6415693759918213,
      "learning_rate": 2.7845606718316912e-05,
      "loss": 1.0361,
      "step": 14020
    },
    {
      "epoch": 0.2171406461597988,
      "grad_norm": 2.084228277206421,
      "learning_rate": 2.7844058243306716e-05,
      "loss": 1.1514,
      "step": 14030
    },
    {
      "epoch": 0.21729541497388277,
      "grad_norm": 1.8151588439941406,
      "learning_rate": 2.7842509768296523e-05,
      "loss": 0.9741,
      "step": 14040
    },
    {
      "epoch": 0.2174501837879667,
      "grad_norm": 1.8816707134246826,
      "learning_rate": 2.7840961293286327e-05,
      "loss": 0.9253,
      "step": 14050
    },
    {
      "epoch": 0.21760495260205068,
      "grad_norm": 1.6513532400131226,
      "learning_rate": 2.7839412818276134e-05,
      "loss": 1.1936,
      "step": 14060
    },
    {
      "epoch": 0.21775972141613464,
      "grad_norm": 1.6333415508270264,
      "learning_rate": 2.783786434326594e-05,
      "loss": 1.0706,
      "step": 14070
    },
    {
      "epoch": 0.2179144902302186,
      "grad_norm": 2.5532143115997314,
      "learning_rate": 2.7836315868255748e-05,
      "loss": 1.2907,
      "step": 14080
    },
    {
      "epoch": 0.21806925904430258,
      "grad_norm": 2.326395034790039,
      "learning_rate": 2.7834767393245552e-05,
      "loss": 1.212,
      "step": 14090
    },
    {
      "epoch": 0.21822402785838654,
      "grad_norm": 1.5260429382324219,
      "learning_rate": 2.783321891823536e-05,
      "loss": 0.9454,
      "step": 14100
    },
    {
      "epoch": 0.2183787966724705,
      "grad_norm": 1.5286544561386108,
      "learning_rate": 2.7831670443225163e-05,
      "loss": 1.0651,
      "step": 14110
    },
    {
      "epoch": 0.21853356548655445,
      "grad_norm": 2.0259580612182617,
      "learning_rate": 2.783012196821497e-05,
      "loss": 1.1041,
      "step": 14120
    },
    {
      "epoch": 0.21868833430063842,
      "grad_norm": 1.9111838340759277,
      "learning_rate": 2.7828573493204774e-05,
      "loss": 1.2564,
      "step": 14130
    },
    {
      "epoch": 0.21884310311472238,
      "grad_norm": 1.675844669342041,
      "learning_rate": 2.7827025018194584e-05,
      "loss": 1.0642,
      "step": 14140
    },
    {
      "epoch": 0.21899787192880635,
      "grad_norm": 1.5743471384048462,
      "learning_rate": 2.7825476543184388e-05,
      "loss": 0.9908,
      "step": 14150
    },
    {
      "epoch": 0.21915264074289031,
      "grad_norm": 2.511361837387085,
      "learning_rate": 2.7823928068174195e-05,
      "loss": 1.1264,
      "step": 14160
    },
    {
      "epoch": 0.21930740955697428,
      "grad_norm": 0.9472100138664246,
      "learning_rate": 2.7822379593164e-05,
      "loss": 1.0819,
      "step": 14170
    },
    {
      "epoch": 0.21946217837105822,
      "grad_norm": 2.3344695568084717,
      "learning_rate": 2.7820831118153806e-05,
      "loss": 0.9708,
      "step": 14180
    },
    {
      "epoch": 0.2196169471851422,
      "grad_norm": 2.025437593460083,
      "learning_rate": 2.781928264314361e-05,
      "loss": 1.1987,
      "step": 14190
    },
    {
      "epoch": 0.21977171599922615,
      "grad_norm": 1.3736978769302368,
      "learning_rate": 2.7817734168133417e-05,
      "loss": 0.9959,
      "step": 14200
    },
    {
      "epoch": 0.21992648481331012,
      "grad_norm": 1.3949898481369019,
      "learning_rate": 2.7816185693123224e-05,
      "loss": 0.9347,
      "step": 14210
    },
    {
      "epoch": 0.22008125362739409,
      "grad_norm": 1.1465617418289185,
      "learning_rate": 2.781463721811303e-05,
      "loss": 1.0952,
      "step": 14220
    },
    {
      "epoch": 0.22023602244147805,
      "grad_norm": 1.4544259309768677,
      "learning_rate": 2.7813088743102835e-05,
      "loss": 1.0049,
      "step": 14230
    },
    {
      "epoch": 0.220390791255562,
      "grad_norm": 1.5907951593399048,
      "learning_rate": 2.7811540268092642e-05,
      "loss": 1.0126,
      "step": 14240
    },
    {
      "epoch": 0.22054556006964596,
      "grad_norm": 2.5754518508911133,
      "learning_rate": 2.7809991793082446e-05,
      "loss": 1.0885,
      "step": 14250
    },
    {
      "epoch": 0.22070032888372992,
      "grad_norm": 1.2598596811294556,
      "learning_rate": 2.7808443318072253e-05,
      "loss": 1.1655,
      "step": 14260
    },
    {
      "epoch": 0.2208550976978139,
      "grad_norm": 1.2632285356521606,
      "learning_rate": 2.7806894843062057e-05,
      "loss": 1.0609,
      "step": 14270
    },
    {
      "epoch": 0.22100986651189786,
      "grad_norm": 1.8657828569412231,
      "learning_rate": 2.7805346368051864e-05,
      "loss": 1.1159,
      "step": 14280
    },
    {
      "epoch": 0.22116463532598182,
      "grad_norm": 1.5220972299575806,
      "learning_rate": 2.780379789304167e-05,
      "loss": 1.1435,
      "step": 14290
    },
    {
      "epoch": 0.2213194041400658,
      "grad_norm": 1.454984426498413,
      "learning_rate": 2.7802249418031475e-05,
      "loss": 1.0998,
      "step": 14300
    },
    {
      "epoch": 0.22147417295414973,
      "grad_norm": 1.556791067123413,
      "learning_rate": 2.7800700943021282e-05,
      "loss": 1.0267,
      "step": 14310
    },
    {
      "epoch": 0.2216289417682337,
      "grad_norm": 1.683158278465271,
      "learning_rate": 2.7799152468011086e-05,
      "loss": 1.1375,
      "step": 14320
    },
    {
      "epoch": 0.22178371058231766,
      "grad_norm": 1.3229179382324219,
      "learning_rate": 2.7797603993000893e-05,
      "loss": 1.1349,
      "step": 14330
    },
    {
      "epoch": 0.22193847939640163,
      "grad_norm": 1.1614192724227905,
      "learning_rate": 2.77960555179907e-05,
      "loss": 1.0392,
      "step": 14340
    },
    {
      "epoch": 0.2220932482104856,
      "grad_norm": 2.1292104721069336,
      "learning_rate": 2.7794507042980507e-05,
      "loss": 1.1572,
      "step": 14350
    },
    {
      "epoch": 0.22224801702456956,
      "grad_norm": 1.524285078048706,
      "learning_rate": 2.779295856797031e-05,
      "loss": 1.203,
      "step": 14360
    },
    {
      "epoch": 0.2224027858386535,
      "grad_norm": 1.7401827573776245,
      "learning_rate": 2.7791410092960118e-05,
      "loss": 1.2381,
      "step": 14370
    },
    {
      "epoch": 0.22255755465273747,
      "grad_norm": 2.4421133995056152,
      "learning_rate": 2.7789861617949922e-05,
      "loss": 1.073,
      "step": 14380
    },
    {
      "epoch": 0.22271232346682143,
      "grad_norm": 1.3405534029006958,
      "learning_rate": 2.778831314293973e-05,
      "loss": 1.037,
      "step": 14390
    },
    {
      "epoch": 0.2228670922809054,
      "grad_norm": 1.8032031059265137,
      "learning_rate": 2.7786764667929533e-05,
      "loss": 1.0216,
      "step": 14400
    },
    {
      "epoch": 0.22302186109498937,
      "grad_norm": 1.7036185264587402,
      "learning_rate": 2.7785216192919343e-05,
      "loss": 1.1866,
      "step": 14410
    },
    {
      "epoch": 0.22317662990907333,
      "grad_norm": 2.089794158935547,
      "learning_rate": 2.7783667717909147e-05,
      "loss": 1.1099,
      "step": 14420
    },
    {
      "epoch": 0.22333139872315727,
      "grad_norm": 1.9862885475158691,
      "learning_rate": 2.7782119242898954e-05,
      "loss": 1.0159,
      "step": 14430
    },
    {
      "epoch": 0.22348616753724124,
      "grad_norm": 0.9704892039299011,
      "learning_rate": 2.7780570767888758e-05,
      "loss": 1.0969,
      "step": 14440
    },
    {
      "epoch": 0.2236409363513252,
      "grad_norm": 2.1023943424224854,
      "learning_rate": 2.7779022292878565e-05,
      "loss": 1.1959,
      "step": 14450
    },
    {
      "epoch": 0.22379570516540917,
      "grad_norm": 2.026160955429077,
      "learning_rate": 2.777747381786837e-05,
      "loss": 1.1216,
      "step": 14460
    },
    {
      "epoch": 0.22395047397949314,
      "grad_norm": 1.5780788660049438,
      "learning_rate": 2.7775925342858176e-05,
      "loss": 0.8887,
      "step": 14470
    },
    {
      "epoch": 0.2241052427935771,
      "grad_norm": 1.8246151208877563,
      "learning_rate": 2.7774376867847983e-05,
      "loss": 1.2436,
      "step": 14480
    },
    {
      "epoch": 0.22426001160766107,
      "grad_norm": 1.9321529865264893,
      "learning_rate": 2.777282839283779e-05,
      "loss": 1.0037,
      "step": 14490
    },
    {
      "epoch": 0.224414780421745,
      "grad_norm": 1.6654754877090454,
      "learning_rate": 2.7771279917827594e-05,
      "loss": 0.9754,
      "step": 14500
    },
    {
      "epoch": 0.22456954923582897,
      "grad_norm": 2.663764476776123,
      "learning_rate": 2.77697314428174e-05,
      "loss": 1.0431,
      "step": 14510
    },
    {
      "epoch": 0.22472431804991294,
      "grad_norm": 1.7199310064315796,
      "learning_rate": 2.7768182967807205e-05,
      "loss": 1.0459,
      "step": 14520
    },
    {
      "epoch": 0.2248790868639969,
      "grad_norm": 1.2743797302246094,
      "learning_rate": 2.776663449279701e-05,
      "loss": 0.9879,
      "step": 14530
    },
    {
      "epoch": 0.22503385567808087,
      "grad_norm": 1.6028732061386108,
      "learning_rate": 2.7765086017786816e-05,
      "loss": 1.095,
      "step": 14540
    },
    {
      "epoch": 0.22518862449216484,
      "grad_norm": 1.4069554805755615,
      "learning_rate": 2.7763537542776623e-05,
      "loss": 1.1802,
      "step": 14550
    },
    {
      "epoch": 0.22534339330624878,
      "grad_norm": 1.9724886417388916,
      "learning_rate": 2.776198906776643e-05,
      "loss": 1.0879,
      "step": 14560
    },
    {
      "epoch": 0.22549816212033275,
      "grad_norm": 1.5802030563354492,
      "learning_rate": 2.7760440592756234e-05,
      "loss": 1.0987,
      "step": 14570
    },
    {
      "epoch": 0.2256529309344167,
      "grad_norm": 1.4971604347229004,
      "learning_rate": 2.775889211774604e-05,
      "loss": 1.0533,
      "step": 14580
    },
    {
      "epoch": 0.22580769974850068,
      "grad_norm": 1.3521687984466553,
      "learning_rate": 2.7757343642735845e-05,
      "loss": 0.9799,
      "step": 14590
    },
    {
      "epoch": 0.22596246856258465,
      "grad_norm": 1.2211109399795532,
      "learning_rate": 2.775579516772565e-05,
      "loss": 1.1147,
      "step": 14600
    },
    {
      "epoch": 0.2261172373766686,
      "grad_norm": 1.728208303451538,
      "learning_rate": 2.7754246692715455e-05,
      "loss": 1.2231,
      "step": 14610
    },
    {
      "epoch": 0.22627200619075255,
      "grad_norm": 1.3999062776565552,
      "learning_rate": 2.7752698217705266e-05,
      "loss": 1.1641,
      "step": 14620
    },
    {
      "epoch": 0.22642677500483652,
      "grad_norm": 1.252467155456543,
      "learning_rate": 2.775114974269507e-05,
      "loss": 1.0406,
      "step": 14630
    },
    {
      "epoch": 0.22658154381892048,
      "grad_norm": 2.6405422687530518,
      "learning_rate": 2.7749601267684877e-05,
      "loss": 0.8675,
      "step": 14640
    },
    {
      "epoch": 0.22673631263300445,
      "grad_norm": 2.1863741874694824,
      "learning_rate": 2.774805279267468e-05,
      "loss": 1.0438,
      "step": 14650
    },
    {
      "epoch": 0.22689108144708842,
      "grad_norm": 1.455460548400879,
      "learning_rate": 2.7746504317664488e-05,
      "loss": 1.2387,
      "step": 14660
    },
    {
      "epoch": 0.22704585026117238,
      "grad_norm": 1.4871408939361572,
      "learning_rate": 2.774495584265429e-05,
      "loss": 0.8638,
      "step": 14670
    },
    {
      "epoch": 0.22720061907525635,
      "grad_norm": 1.367767095565796,
      "learning_rate": 2.77434073676441e-05,
      "loss": 1.1389,
      "step": 14680
    },
    {
      "epoch": 0.2273553878893403,
      "grad_norm": 1.5365817546844482,
      "learning_rate": 2.7741858892633906e-05,
      "loss": 1.0039,
      "step": 14690
    },
    {
      "epoch": 0.22751015670342425,
      "grad_norm": 1.0420907735824585,
      "learning_rate": 2.7740310417623713e-05,
      "loss": 1.1478,
      "step": 14700
    },
    {
      "epoch": 0.22766492551750822,
      "grad_norm": 1.8572195768356323,
      "learning_rate": 2.7738761942613517e-05,
      "loss": 1.1712,
      "step": 14710
    },
    {
      "epoch": 0.2278196943315922,
      "grad_norm": 2.3575055599212646,
      "learning_rate": 2.7737213467603324e-05,
      "loss": 1.1627,
      "step": 14720
    },
    {
      "epoch": 0.22797446314567615,
      "grad_norm": 1.8910605907440186,
      "learning_rate": 2.7735664992593128e-05,
      "loss": 1.1152,
      "step": 14730
    },
    {
      "epoch": 0.22812923195976012,
      "grad_norm": 1.220954179763794,
      "learning_rate": 2.7734116517582935e-05,
      "loss": 1.2151,
      "step": 14740
    },
    {
      "epoch": 0.22828400077384406,
      "grad_norm": 2.26505708694458,
      "learning_rate": 2.7732568042572742e-05,
      "loss": 1.1509,
      "step": 14750
    },
    {
      "epoch": 0.22843876958792803,
      "grad_norm": 1.1349798440933228,
      "learning_rate": 2.773101956756255e-05,
      "loss": 1.0679,
      "step": 14760
    },
    {
      "epoch": 0.228593538402012,
      "grad_norm": 1.3178107738494873,
      "learning_rate": 2.7729471092552353e-05,
      "loss": 1.076,
      "step": 14770
    },
    {
      "epoch": 0.22874830721609596,
      "grad_norm": 1.8627208471298218,
      "learning_rate": 2.7727922617542156e-05,
      "loss": 1.0607,
      "step": 14780
    },
    {
      "epoch": 0.22890307603017992,
      "grad_norm": 2.265063524246216,
      "learning_rate": 2.7726374142531964e-05,
      "loss": 1.0294,
      "step": 14790
    },
    {
      "epoch": 0.2290578448442639,
      "grad_norm": 1.3336241245269775,
      "learning_rate": 2.7724825667521767e-05,
      "loss": 1.2546,
      "step": 14800
    },
    {
      "epoch": 0.22921261365834783,
      "grad_norm": 1.5221065282821655,
      "learning_rate": 2.7723277192511574e-05,
      "loss": 1.101,
      "step": 14810
    },
    {
      "epoch": 0.2293673824724318,
      "grad_norm": 1.4440608024597168,
      "learning_rate": 2.772172871750138e-05,
      "loss": 1.1723,
      "step": 14820
    },
    {
      "epoch": 0.22952215128651576,
      "grad_norm": 1.7902288436889648,
      "learning_rate": 2.772018024249119e-05,
      "loss": 1.1634,
      "step": 14830
    },
    {
      "epoch": 0.22967692010059973,
      "grad_norm": 1.5845577716827393,
      "learning_rate": 2.7718631767480992e-05,
      "loss": 1.048,
      "step": 14840
    },
    {
      "epoch": 0.2298316889146837,
      "grad_norm": 2.798027276992798,
      "learning_rate": 2.77170832924708e-05,
      "loss": 1.2064,
      "step": 14850
    },
    {
      "epoch": 0.22998645772876766,
      "grad_norm": 1.799803614616394,
      "learning_rate": 2.7715534817460603e-05,
      "loss": 1.2864,
      "step": 14860
    },
    {
      "epoch": 0.23014122654285163,
      "grad_norm": 1.7014473676681519,
      "learning_rate": 2.771398634245041e-05,
      "loss": 1.1267,
      "step": 14870
    },
    {
      "epoch": 0.23029599535693557,
      "grad_norm": 1.7927007675170898,
      "learning_rate": 2.7712437867440214e-05,
      "loss": 1.2548,
      "step": 14880
    },
    {
      "epoch": 0.23045076417101953,
      "grad_norm": 1.263848900794983,
      "learning_rate": 2.7710889392430025e-05,
      "loss": 0.9695,
      "step": 14890
    },
    {
      "epoch": 0.2306055329851035,
      "grad_norm": 1.5650964975357056,
      "learning_rate": 2.770934091741983e-05,
      "loss": 1.0616,
      "step": 14900
    },
    {
      "epoch": 0.23076030179918747,
      "grad_norm": 1.3252190351486206,
      "learning_rate": 2.7707792442409636e-05,
      "loss": 1.1269,
      "step": 14910
    },
    {
      "epoch": 0.23091507061327143,
      "grad_norm": 1.4139330387115479,
      "learning_rate": 2.770624396739944e-05,
      "loss": 1.1215,
      "step": 14920
    },
    {
      "epoch": 0.2310698394273554,
      "grad_norm": 0.9449509978294373,
      "learning_rate": 2.7704695492389247e-05,
      "loss": 0.9522,
      "step": 14930
    },
    {
      "epoch": 0.23122460824143934,
      "grad_norm": 1.5409815311431885,
      "learning_rate": 2.770314701737905e-05,
      "loss": 0.9803,
      "step": 14940
    },
    {
      "epoch": 0.2313793770555233,
      "grad_norm": 1.537095069885254,
      "learning_rate": 2.7701598542368857e-05,
      "loss": 1.1431,
      "step": 14950
    },
    {
      "epoch": 0.23153414586960727,
      "grad_norm": 1.8384019136428833,
      "learning_rate": 2.7700050067358665e-05,
      "loss": 1.1028,
      "step": 14960
    },
    {
      "epoch": 0.23168891468369124,
      "grad_norm": 5.3425822257995605,
      "learning_rate": 2.7698501592348472e-05,
      "loss": 1.0846,
      "step": 14970
    },
    {
      "epoch": 0.2318436834977752,
      "grad_norm": 2.177217960357666,
      "learning_rate": 2.7696953117338276e-05,
      "loss": 0.9981,
      "step": 14980
    },
    {
      "epoch": 0.23199845231185917,
      "grad_norm": 2.0305230617523193,
      "learning_rate": 2.7695404642328083e-05,
      "loss": 1.0937,
      "step": 14990
    },
    {
      "epoch": 0.2321532211259431,
      "grad_norm": 1.516877293586731,
      "learning_rate": 2.7693856167317886e-05,
      "loss": 1.1155,
      "step": 15000
    },
    {
      "epoch": 0.23230798994002708,
      "grad_norm": 1.2076495885849,
      "learning_rate": 2.7692462539808712e-05,
      "loss": 1.0134,
      "step": 15010
    },
    {
      "epoch": 0.23246275875411104,
      "grad_norm": 1.6758874654769897,
      "learning_rate": 2.7690914064798516e-05,
      "loss": 1.0064,
      "step": 15020
    },
    {
      "epoch": 0.232617527568195,
      "grad_norm": 1.2747478485107422,
      "learning_rate": 2.7689365589788323e-05,
      "loss": 0.9734,
      "step": 15030
    },
    {
      "epoch": 0.23277229638227898,
      "grad_norm": 2.1271166801452637,
      "learning_rate": 2.768781711477813e-05,
      "loss": 1.1744,
      "step": 15040
    },
    {
      "epoch": 0.23292706519636294,
      "grad_norm": 1.6377110481262207,
      "learning_rate": 2.7686268639767938e-05,
      "loss": 1.1718,
      "step": 15050
    },
    {
      "epoch": 0.23308183401044688,
      "grad_norm": 1.7207725048065186,
      "learning_rate": 2.768472016475774e-05,
      "loss": 1.2296,
      "step": 15060
    },
    {
      "epoch": 0.23323660282453085,
      "grad_norm": 1.7813763618469238,
      "learning_rate": 2.768317168974755e-05,
      "loss": 1.1122,
      "step": 15070
    },
    {
      "epoch": 0.23339137163861481,
      "grad_norm": 1.5491331815719604,
      "learning_rate": 2.7681623214737352e-05,
      "loss": 0.8856,
      "step": 15080
    },
    {
      "epoch": 0.23354614045269878,
      "grad_norm": 1.4929453134536743,
      "learning_rate": 2.768007473972716e-05,
      "loss": 1.0313,
      "step": 15090
    },
    {
      "epoch": 0.23370090926678275,
      "grad_norm": 1.3506639003753662,
      "learning_rate": 2.7678526264716963e-05,
      "loss": 1.2574,
      "step": 15100
    },
    {
      "epoch": 0.2338556780808667,
      "grad_norm": 1.9409180879592896,
      "learning_rate": 2.7676977789706774e-05,
      "loss": 1.0684,
      "step": 15110
    },
    {
      "epoch": 0.23401044689495068,
      "grad_norm": 2.0879011154174805,
      "learning_rate": 2.7675429314696577e-05,
      "loss": 1.0732,
      "step": 15120
    },
    {
      "epoch": 0.23416521570903462,
      "grad_norm": 2.375054359436035,
      "learning_rate": 2.7673880839686385e-05,
      "loss": 1.0073,
      "step": 15130
    },
    {
      "epoch": 0.23431998452311859,
      "grad_norm": 1.8774735927581787,
      "learning_rate": 2.767233236467619e-05,
      "loss": 0.9814,
      "step": 15140
    },
    {
      "epoch": 0.23447475333720255,
      "grad_norm": 1.3576252460479736,
      "learning_rate": 2.7670783889665995e-05,
      "loss": 1.0006,
      "step": 15150
    },
    {
      "epoch": 0.23462952215128652,
      "grad_norm": 1.6161216497421265,
      "learning_rate": 2.76692354146558e-05,
      "loss": 1.2231,
      "step": 15160
    },
    {
      "epoch": 0.23478429096537048,
      "grad_norm": 1.7783151865005493,
      "learning_rate": 2.7667686939645606e-05,
      "loss": 1.4465,
      "step": 15170
    },
    {
      "epoch": 0.23493905977945445,
      "grad_norm": 1.3865163326263428,
      "learning_rate": 2.7666138464635414e-05,
      "loss": 1.2122,
      "step": 15180
    },
    {
      "epoch": 0.2350938285935384,
      "grad_norm": 1.3148170709609985,
      "learning_rate": 2.766458998962522e-05,
      "loss": 1.2394,
      "step": 15190
    },
    {
      "epoch": 0.23524859740762236,
      "grad_norm": 1.068686604499817,
      "learning_rate": 2.7663041514615024e-05,
      "loss": 1.1097,
      "step": 15200
    },
    {
      "epoch": 0.23540336622170632,
      "grad_norm": 1.6736969947814941,
      "learning_rate": 2.7661493039604828e-05,
      "loss": 0.995,
      "step": 15210
    },
    {
      "epoch": 0.2355581350357903,
      "grad_norm": 2.465513229370117,
      "learning_rate": 2.7659944564594635e-05,
      "loss": 0.959,
      "step": 15220
    },
    {
      "epoch": 0.23571290384987426,
      "grad_norm": 1.736894130706787,
      "learning_rate": 2.765839608958444e-05,
      "loss": 1.1637,
      "step": 15230
    },
    {
      "epoch": 0.23586767266395822,
      "grad_norm": 2.992215633392334,
      "learning_rate": 2.7656847614574246e-05,
      "loss": 1.0057,
      "step": 15240
    },
    {
      "epoch": 0.23602244147804216,
      "grad_norm": 1.2896133661270142,
      "learning_rate": 2.7655299139564053e-05,
      "loss": 1.138,
      "step": 15250
    },
    {
      "epoch": 0.23617721029212613,
      "grad_norm": 1.2459813356399536,
      "learning_rate": 2.765375066455386e-05,
      "loss": 1.0515,
      "step": 15260
    },
    {
      "epoch": 0.2363319791062101,
      "grad_norm": 1.2244927883148193,
      "learning_rate": 2.7652202189543664e-05,
      "loss": 1.0573,
      "step": 15270
    },
    {
      "epoch": 0.23648674792029406,
      "grad_norm": 1.6723313331604004,
      "learning_rate": 2.765065371453347e-05,
      "loss": 1.0851,
      "step": 15280
    },
    {
      "epoch": 0.23664151673437803,
      "grad_norm": 2.5584564208984375,
      "learning_rate": 2.7649105239523275e-05,
      "loss": 1.123,
      "step": 15290
    },
    {
      "epoch": 0.236796285548462,
      "grad_norm": 1.818457007408142,
      "learning_rate": 2.7647556764513082e-05,
      "loss": 1.1492,
      "step": 15300
    },
    {
      "epoch": 0.23695105436254596,
      "grad_norm": 1.5802847146987915,
      "learning_rate": 2.7646008289502886e-05,
      "loss": 1.1382,
      "step": 15310
    },
    {
      "epoch": 0.2371058231766299,
      "grad_norm": 2.7002642154693604,
      "learning_rate": 2.7644459814492697e-05,
      "loss": 1.2015,
      "step": 15320
    },
    {
      "epoch": 0.23726059199071386,
      "grad_norm": 1.2270859479904175,
      "learning_rate": 2.76429113394825e-05,
      "loss": 1.1191,
      "step": 15330
    },
    {
      "epoch": 0.23741536080479783,
      "grad_norm": 1.4128506183624268,
      "learning_rate": 2.7641362864472307e-05,
      "loss": 1.1401,
      "step": 15340
    },
    {
      "epoch": 0.2375701296188818,
      "grad_norm": 1.5257799625396729,
      "learning_rate": 2.763981438946211e-05,
      "loss": 1.0073,
      "step": 15350
    },
    {
      "epoch": 0.23772489843296576,
      "grad_norm": 1.8428863286972046,
      "learning_rate": 2.7638265914451918e-05,
      "loss": 1.0698,
      "step": 15360
    },
    {
      "epoch": 0.23787966724704973,
      "grad_norm": 2.3283064365386963,
      "learning_rate": 2.7636717439441722e-05,
      "loss": 1.008,
      "step": 15370
    },
    {
      "epoch": 0.23803443606113367,
      "grad_norm": 2.0321810245513916,
      "learning_rate": 2.763516896443153e-05,
      "loss": 1.2544,
      "step": 15380
    },
    {
      "epoch": 0.23818920487521764,
      "grad_norm": 1.5433921813964844,
      "learning_rate": 2.7633620489421336e-05,
      "loss": 1.2494,
      "step": 15390
    },
    {
      "epoch": 0.2383439736893016,
      "grad_norm": 1.198625087738037,
      "learning_rate": 2.7632072014411143e-05,
      "loss": 1.0344,
      "step": 15400
    },
    {
      "epoch": 0.23849874250338557,
      "grad_norm": 1.6593509912490845,
      "learning_rate": 2.7630523539400947e-05,
      "loss": 0.9271,
      "step": 15410
    },
    {
      "epoch": 0.23865351131746954,
      "grad_norm": 2.0034055709838867,
      "learning_rate": 2.7628975064390754e-05,
      "loss": 1.0324,
      "step": 15420
    },
    {
      "epoch": 0.2388082801315535,
      "grad_norm": 1.064416527748108,
      "learning_rate": 2.7627426589380558e-05,
      "loss": 1.0823,
      "step": 15430
    },
    {
      "epoch": 0.23896304894563744,
      "grad_norm": 1.2443574666976929,
      "learning_rate": 2.7625878114370365e-05,
      "loss": 0.9033,
      "step": 15440
    },
    {
      "epoch": 0.2391178177597214,
      "grad_norm": 1.803165316581726,
      "learning_rate": 2.7624329639360172e-05,
      "loss": 1.108,
      "step": 15450
    },
    {
      "epoch": 0.23927258657380537,
      "grad_norm": 1.817883849143982,
      "learning_rate": 2.762278116434998e-05,
      "loss": 1.2321,
      "step": 15460
    },
    {
      "epoch": 0.23942735538788934,
      "grad_norm": 1.5414206981658936,
      "learning_rate": 2.7621232689339783e-05,
      "loss": 1.0502,
      "step": 15470
    },
    {
      "epoch": 0.2395821242019733,
      "grad_norm": 1.2883198261260986,
      "learning_rate": 2.7619684214329587e-05,
      "loss": 1.1395,
      "step": 15480
    },
    {
      "epoch": 0.23973689301605727,
      "grad_norm": 1.6458760499954224,
      "learning_rate": 2.7618135739319394e-05,
      "loss": 1.0717,
      "step": 15490
    },
    {
      "epoch": 0.23989166183014124,
      "grad_norm": 1.6377922296524048,
      "learning_rate": 2.7616587264309198e-05,
      "loss": 1.3382,
      "step": 15500
    },
    {
      "epoch": 0.24004643064422518,
      "grad_norm": 2.3799142837524414,
      "learning_rate": 2.7615038789299005e-05,
      "loss": 0.9345,
      "step": 15510
    },
    {
      "epoch": 0.24020119945830914,
      "grad_norm": 1.3667609691619873,
      "learning_rate": 2.7613490314288812e-05,
      "loss": 0.93,
      "step": 15520
    },
    {
      "epoch": 0.2403559682723931,
      "grad_norm": 1.3525973558425903,
      "learning_rate": 2.761194183927862e-05,
      "loss": 1.0731,
      "step": 15530
    },
    {
      "epoch": 0.24051073708647708,
      "grad_norm": 2.1404616832733154,
      "learning_rate": 2.7610393364268423e-05,
      "loss": 1.0115,
      "step": 15540
    },
    {
      "epoch": 0.24066550590056104,
      "grad_norm": 2.3699278831481934,
      "learning_rate": 2.760884488925823e-05,
      "loss": 1.1743,
      "step": 15550
    },
    {
      "epoch": 0.240820274714645,
      "grad_norm": 1.0370712280273438,
      "learning_rate": 2.7607296414248034e-05,
      "loss": 0.9053,
      "step": 15560
    },
    {
      "epoch": 0.24097504352872895,
      "grad_norm": 1.467140793800354,
      "learning_rate": 2.760574793923784e-05,
      "loss": 1.1082,
      "step": 15570
    },
    {
      "epoch": 0.24112981234281292,
      "grad_norm": 1.7348554134368896,
      "learning_rate": 2.7604199464227645e-05,
      "loss": 0.9426,
      "step": 15580
    },
    {
      "epoch": 0.24128458115689688,
      "grad_norm": 1.467848539352417,
      "learning_rate": 2.7602650989217455e-05,
      "loss": 1.123,
      "step": 15590
    },
    {
      "epoch": 0.24143934997098085,
      "grad_norm": 1.6354013681411743,
      "learning_rate": 2.760110251420726e-05,
      "loss": 1.0674,
      "step": 15600
    },
    {
      "epoch": 0.24159411878506482,
      "grad_norm": 1.7873636484146118,
      "learning_rate": 2.7599554039197066e-05,
      "loss": 1.1041,
      "step": 15610
    },
    {
      "epoch": 0.24174888759914878,
      "grad_norm": 1.5784484148025513,
      "learning_rate": 2.759800556418687e-05,
      "loss": 1.1582,
      "step": 15620
    },
    {
      "epoch": 0.24190365641323272,
      "grad_norm": 1.2743968963623047,
      "learning_rate": 2.7596457089176677e-05,
      "loss": 1.1682,
      "step": 15630
    },
    {
      "epoch": 0.2420584252273167,
      "grad_norm": 1.3162792921066284,
      "learning_rate": 2.759490861416648e-05,
      "loss": 0.9916,
      "step": 15640
    },
    {
      "epoch": 0.24221319404140065,
      "grad_norm": 2.561138391494751,
      "learning_rate": 2.7593360139156288e-05,
      "loss": 1.1542,
      "step": 15650
    },
    {
      "epoch": 0.24236796285548462,
      "grad_norm": 1.3514004945755005,
      "learning_rate": 2.7591811664146095e-05,
      "loss": 1.0952,
      "step": 15660
    },
    {
      "epoch": 0.2425227316695686,
      "grad_norm": 1.718855381011963,
      "learning_rate": 2.7590263189135902e-05,
      "loss": 1.1705,
      "step": 15670
    },
    {
      "epoch": 0.24267750048365255,
      "grad_norm": 1.4722834825515747,
      "learning_rate": 2.7588714714125706e-05,
      "loss": 1.0176,
      "step": 15680
    },
    {
      "epoch": 0.24283226929773652,
      "grad_norm": 2.274226188659668,
      "learning_rate": 2.7587166239115513e-05,
      "loss": 1.0957,
      "step": 15690
    },
    {
      "epoch": 0.24298703811182046,
      "grad_norm": 1.7596707344055176,
      "learning_rate": 2.7585617764105317e-05,
      "loss": 0.8231,
      "step": 15700
    },
    {
      "epoch": 0.24314180692590442,
      "grad_norm": 1.4778858423233032,
      "learning_rate": 2.7584069289095124e-05,
      "loss": 1.0763,
      "step": 15710
    },
    {
      "epoch": 0.2432965757399884,
      "grad_norm": 1.295658826828003,
      "learning_rate": 2.7582520814084928e-05,
      "loss": 1.1525,
      "step": 15720
    },
    {
      "epoch": 0.24345134455407236,
      "grad_norm": 1.9763091802597046,
      "learning_rate": 2.7580972339074735e-05,
      "loss": 1.1892,
      "step": 15730
    },
    {
      "epoch": 0.24360611336815632,
      "grad_norm": 1.3934129476547241,
      "learning_rate": 2.7579423864064542e-05,
      "loss": 1.118,
      "step": 15740
    },
    {
      "epoch": 0.2437608821822403,
      "grad_norm": 2.0645711421966553,
      "learning_rate": 2.7577875389054346e-05,
      "loss": 1.1876,
      "step": 15750
    },
    {
      "epoch": 0.24391565099632423,
      "grad_norm": 1.1940820217132568,
      "learning_rate": 2.7576326914044153e-05,
      "loss": 1.0806,
      "step": 15760
    },
    {
      "epoch": 0.2440704198104082,
      "grad_norm": 1.1872479915618896,
      "learning_rate": 2.7574778439033957e-05,
      "loss": 1.1473,
      "step": 15770
    },
    {
      "epoch": 0.24422518862449216,
      "grad_norm": 2.1656904220581055,
      "learning_rate": 2.7573229964023764e-05,
      "loss": 1.1639,
      "step": 15780
    },
    {
      "epoch": 0.24437995743857613,
      "grad_norm": 1.3061984777450562,
      "learning_rate": 2.7571681489013568e-05,
      "loss": 1.028,
      "step": 15790
    },
    {
      "epoch": 0.2445347262526601,
      "grad_norm": 1.7344317436218262,
      "learning_rate": 2.7570133014003378e-05,
      "loss": 1.024,
      "step": 15800
    },
    {
      "epoch": 0.24468949506674406,
      "grad_norm": 1.7841464281082153,
      "learning_rate": 2.7568584538993182e-05,
      "loss": 0.9847,
      "step": 15810
    },
    {
      "epoch": 0.244844263880828,
      "grad_norm": 1.6481434106826782,
      "learning_rate": 2.756703606398299e-05,
      "loss": 1.025,
      "step": 15820
    },
    {
      "epoch": 0.24499903269491197,
      "grad_norm": 1.823013424873352,
      "learning_rate": 2.7565487588972793e-05,
      "loss": 1.1166,
      "step": 15830
    },
    {
      "epoch": 0.24515380150899593,
      "grad_norm": 1.284669041633606,
      "learning_rate": 2.75639391139626e-05,
      "loss": 1.0695,
      "step": 15840
    },
    {
      "epoch": 0.2453085703230799,
      "grad_norm": 2.4845917224884033,
      "learning_rate": 2.7562390638952404e-05,
      "loss": 1.1482,
      "step": 15850
    },
    {
      "epoch": 0.24546333913716387,
      "grad_norm": 1.59365975856781,
      "learning_rate": 2.7560842163942214e-05,
      "loss": 0.9991,
      "step": 15860
    },
    {
      "epoch": 0.24561810795124783,
      "grad_norm": 2.0024375915527344,
      "learning_rate": 2.7559293688932018e-05,
      "loss": 1.1782,
      "step": 15870
    },
    {
      "epoch": 0.2457728767653318,
      "grad_norm": 2.107393741607666,
      "learning_rate": 2.7557745213921825e-05,
      "loss": 1.1606,
      "step": 15880
    },
    {
      "epoch": 0.24592764557941574,
      "grad_norm": 2.1264994144439697,
      "learning_rate": 2.755619673891163e-05,
      "loss": 1.1381,
      "step": 15890
    },
    {
      "epoch": 0.2460824143934997,
      "grad_norm": 1.7477957010269165,
      "learning_rate": 2.7554648263901436e-05,
      "loss": 0.9928,
      "step": 15900
    },
    {
      "epoch": 0.24623718320758367,
      "grad_norm": 1.7902621030807495,
      "learning_rate": 2.755309978889124e-05,
      "loss": 1.2267,
      "step": 15910
    },
    {
      "epoch": 0.24639195202166764,
      "grad_norm": 1.6912506818771362,
      "learning_rate": 2.7551551313881047e-05,
      "loss": 1.1841,
      "step": 15920
    },
    {
      "epoch": 0.2465467208357516,
      "grad_norm": 1.5037099123001099,
      "learning_rate": 2.7550002838870854e-05,
      "loss": 1.0069,
      "step": 15930
    },
    {
      "epoch": 0.24670148964983557,
      "grad_norm": 1.370611310005188,
      "learning_rate": 2.754845436386066e-05,
      "loss": 1.1376,
      "step": 15940
    },
    {
      "epoch": 0.2468562584639195,
      "grad_norm": 1.7047990560531616,
      "learning_rate": 2.7546905888850465e-05,
      "loss": 1.1061,
      "step": 15950
    },
    {
      "epoch": 0.24701102727800348,
      "grad_norm": 1.719288945198059,
      "learning_rate": 2.7545357413840272e-05,
      "loss": 0.9615,
      "step": 15960
    },
    {
      "epoch": 0.24716579609208744,
      "grad_norm": 1.1432750225067139,
      "learning_rate": 2.7543808938830076e-05,
      "loss": 1.0464,
      "step": 15970
    },
    {
      "epoch": 0.2473205649061714,
      "grad_norm": 1.7194979190826416,
      "learning_rate": 2.754226046381988e-05,
      "loss": 1.0996,
      "step": 15980
    },
    {
      "epoch": 0.24747533372025537,
      "grad_norm": 2.1654841899871826,
      "learning_rate": 2.7540711988809687e-05,
      "loss": 1.0727,
      "step": 15990
    },
    {
      "epoch": 0.24763010253433934,
      "grad_norm": 2.270451784133911,
      "learning_rate": 2.7539163513799494e-05,
      "loss": 1.1077,
      "step": 16000
    },
    {
      "epoch": 0.24778487134842328,
      "grad_norm": 1.601089358329773,
      "learning_rate": 2.75376150387893e-05,
      "loss": 1.1341,
      "step": 16010
    },
    {
      "epoch": 0.24793964016250725,
      "grad_norm": 1.8647501468658447,
      "learning_rate": 2.7536066563779105e-05,
      "loss": 1.2452,
      "step": 16020
    },
    {
      "epoch": 0.2480944089765912,
      "grad_norm": 1.4416940212249756,
      "learning_rate": 2.7534518088768912e-05,
      "loss": 1.1657,
      "step": 16030
    },
    {
      "epoch": 0.24824917779067518,
      "grad_norm": 1.3652973175048828,
      "learning_rate": 2.7532969613758716e-05,
      "loss": 1.0635,
      "step": 16040
    },
    {
      "epoch": 0.24840394660475915,
      "grad_norm": 1.7742840051651,
      "learning_rate": 2.7531421138748523e-05,
      "loss": 1.1182,
      "step": 16050
    },
    {
      "epoch": 0.2485587154188431,
      "grad_norm": 2.044848680496216,
      "learning_rate": 2.7529872663738327e-05,
      "loss": 1.4199,
      "step": 16060
    },
    {
      "epoch": 0.24871348423292708,
      "grad_norm": 1.504411220550537,
      "learning_rate": 2.7528324188728137e-05,
      "loss": 1.044,
      "step": 16070
    },
    {
      "epoch": 0.24886825304701102,
      "grad_norm": 1.484535813331604,
      "learning_rate": 2.752677571371794e-05,
      "loss": 1.0059,
      "step": 16080
    },
    {
      "epoch": 0.24902302186109498,
      "grad_norm": 1.5156974792480469,
      "learning_rate": 2.7525227238707748e-05,
      "loss": 1.1505,
      "step": 16090
    },
    {
      "epoch": 0.24917779067517895,
      "grad_norm": 1.465732455253601,
      "learning_rate": 2.752367876369755e-05,
      "loss": 1.0483,
      "step": 16100
    },
    {
      "epoch": 0.24933255948926292,
      "grad_norm": 2.1559340953826904,
      "learning_rate": 2.752213028868736e-05,
      "loss": 1.0522,
      "step": 16110
    },
    {
      "epoch": 0.24948732830334688,
      "grad_norm": 1.0378490686416626,
      "learning_rate": 2.7520581813677163e-05,
      "loss": 1.0148,
      "step": 16120
    },
    {
      "epoch": 0.24964209711743085,
      "grad_norm": 1.67585027217865,
      "learning_rate": 2.751903333866697e-05,
      "loss": 1.1445,
      "step": 16130
    },
    {
      "epoch": 0.2497968659315148,
      "grad_norm": 1.9708985090255737,
      "learning_rate": 2.7517484863656777e-05,
      "loss": 0.9975,
      "step": 16140
    },
    {
      "epoch": 0.24995163474559876,
      "grad_norm": 1.6117416620254517,
      "learning_rate": 2.7515936388646584e-05,
      "loss": 1.0802,
      "step": 16150
    },
    {
      "epoch": 0.2501064035596827,
      "grad_norm": 2.2524094581604004,
      "learning_rate": 2.7514387913636388e-05,
      "loss": 0.9311,
      "step": 16160
    },
    {
      "epoch": 0.25026117237376666,
      "grad_norm": 1.2455792427062988,
      "learning_rate": 2.7512839438626195e-05,
      "loss": 0.9739,
      "step": 16170
    },
    {
      "epoch": 0.25041594118785065,
      "grad_norm": 1.734958291053772,
      "learning_rate": 2.7511290963616e-05,
      "loss": 1.1199,
      "step": 16180
    },
    {
      "epoch": 0.2505707100019346,
      "grad_norm": 1.2264316082000732,
      "learning_rate": 2.7509742488605806e-05,
      "loss": 1.0541,
      "step": 16190
    },
    {
      "epoch": 0.2507254788160186,
      "grad_norm": 2.168898582458496,
      "learning_rate": 2.750819401359561e-05,
      "loss": 1.3357,
      "step": 16200
    },
    {
      "epoch": 0.2508802476301025,
      "grad_norm": 1.3827265501022339,
      "learning_rate": 2.750664553858542e-05,
      "loss": 1.1477,
      "step": 16210
    },
    {
      "epoch": 0.2510350164441865,
      "grad_norm": 1.3726325035095215,
      "learning_rate": 2.7505097063575224e-05,
      "loss": 1.0517,
      "step": 16220
    },
    {
      "epoch": 0.25118978525827046,
      "grad_norm": 1.9496701955795288,
      "learning_rate": 2.7503548588565028e-05,
      "loss": 1.0253,
      "step": 16230
    },
    {
      "epoch": 0.2513445540723544,
      "grad_norm": 2.214597463607788,
      "learning_rate": 2.7502000113554835e-05,
      "loss": 1.1871,
      "step": 16240
    },
    {
      "epoch": 0.2514993228864384,
      "grad_norm": 1.6111360788345337,
      "learning_rate": 2.750045163854464e-05,
      "loss": 0.9815,
      "step": 16250
    },
    {
      "epoch": 0.25165409170052233,
      "grad_norm": 2.8219151496887207,
      "learning_rate": 2.7498903163534446e-05,
      "loss": 1.1728,
      "step": 16260
    },
    {
      "epoch": 0.2518088605146063,
      "grad_norm": 2.263997793197632,
      "learning_rate": 2.7497354688524253e-05,
      "loss": 0.8987,
      "step": 16270
    },
    {
      "epoch": 0.25196362932869026,
      "grad_norm": 1.9530611038208008,
      "learning_rate": 2.749580621351406e-05,
      "loss": 1.1449,
      "step": 16280
    },
    {
      "epoch": 0.25211839814277426,
      "grad_norm": 1.8631553649902344,
      "learning_rate": 2.7494257738503864e-05,
      "loss": 1.1073,
      "step": 16290
    },
    {
      "epoch": 0.2522731669568582,
      "grad_norm": 1.2010083198547363,
      "learning_rate": 2.749270926349367e-05,
      "loss": 1.0055,
      "step": 16300
    },
    {
      "epoch": 0.25242793577094214,
      "grad_norm": 1.6972317695617676,
      "learning_rate": 2.7491160788483475e-05,
      "loss": 1.0851,
      "step": 16310
    },
    {
      "epoch": 0.25258270458502613,
      "grad_norm": 1.7996611595153809,
      "learning_rate": 2.748961231347328e-05,
      "loss": 0.9584,
      "step": 16320
    },
    {
      "epoch": 0.25273747339911007,
      "grad_norm": 1.172345757484436,
      "learning_rate": 2.7488063838463085e-05,
      "loss": 1.0707,
      "step": 16330
    },
    {
      "epoch": 0.25289224221319406,
      "grad_norm": 1.4584074020385742,
      "learning_rate": 2.7486515363452896e-05,
      "loss": 1.0659,
      "step": 16340
    },
    {
      "epoch": 0.253047011027278,
      "grad_norm": 1.3317618370056152,
      "learning_rate": 2.74849668884427e-05,
      "loss": 1.0697,
      "step": 16350
    },
    {
      "epoch": 0.25320177984136194,
      "grad_norm": 2.3123388290405273,
      "learning_rate": 2.7483418413432507e-05,
      "loss": 1.0465,
      "step": 16360
    },
    {
      "epoch": 0.25335654865544593,
      "grad_norm": 1.9789972305297852,
      "learning_rate": 2.748186993842231e-05,
      "loss": 0.911,
      "step": 16370
    },
    {
      "epoch": 0.2535113174695299,
      "grad_norm": 1.126375436782837,
      "learning_rate": 2.7480321463412118e-05,
      "loss": 0.9811,
      "step": 16380
    },
    {
      "epoch": 0.25366608628361387,
      "grad_norm": 1.6239732503890991,
      "learning_rate": 2.747877298840192e-05,
      "loss": 1.166,
      "step": 16390
    },
    {
      "epoch": 0.2538208550976978,
      "grad_norm": 2.239980459213257,
      "learning_rate": 2.747722451339173e-05,
      "loss": 1.105,
      "step": 16400
    },
    {
      "epoch": 0.2539756239117818,
      "grad_norm": 1.5051298141479492,
      "learning_rate": 2.7475676038381536e-05,
      "loss": 1.108,
      "step": 16410
    },
    {
      "epoch": 0.25413039272586574,
      "grad_norm": 1.6276395320892334,
      "learning_rate": 2.7474127563371343e-05,
      "loss": 1.1727,
      "step": 16420
    },
    {
      "epoch": 0.2542851615399497,
      "grad_norm": 1.5407898426055908,
      "learning_rate": 2.7472579088361147e-05,
      "loss": 1.0492,
      "step": 16430
    },
    {
      "epoch": 0.25443993035403367,
      "grad_norm": 1.378339171409607,
      "learning_rate": 2.7471030613350954e-05,
      "loss": 1.1063,
      "step": 16440
    },
    {
      "epoch": 0.2545946991681176,
      "grad_norm": 1.3825420141220093,
      "learning_rate": 2.7469482138340758e-05,
      "loss": 1.1848,
      "step": 16450
    },
    {
      "epoch": 0.2547494679822016,
      "grad_norm": 1.337319016456604,
      "learning_rate": 2.7467933663330565e-05,
      "loss": 1.1583,
      "step": 16460
    },
    {
      "epoch": 0.25490423679628554,
      "grad_norm": 2.405298948287964,
      "learning_rate": 2.746638518832037e-05,
      "loss": 1.0575,
      "step": 16470
    },
    {
      "epoch": 0.25505900561036954,
      "grad_norm": 2.133666515350342,
      "learning_rate": 2.7464836713310176e-05,
      "loss": 1.0318,
      "step": 16480
    },
    {
      "epoch": 0.2552137744244535,
      "grad_norm": 1.5828031301498413,
      "learning_rate": 2.7463288238299983e-05,
      "loss": 1.1365,
      "step": 16490
    },
    {
      "epoch": 0.2553685432385374,
      "grad_norm": 1.2776118516921997,
      "learning_rate": 2.7461739763289786e-05,
      "loss": 0.9741,
      "step": 16500
    },
    {
      "epoch": 0.2555233120526214,
      "grad_norm": 1.475324273109436,
      "learning_rate": 2.7460191288279594e-05,
      "loss": 1.1168,
      "step": 16510
    },
    {
      "epoch": 0.25567808086670535,
      "grad_norm": 1.6857595443725586,
      "learning_rate": 2.7458642813269397e-05,
      "loss": 0.9718,
      "step": 16520
    },
    {
      "epoch": 0.25583284968078934,
      "grad_norm": 1.377193570137024,
      "learning_rate": 2.7457094338259204e-05,
      "loss": 1.0903,
      "step": 16530
    },
    {
      "epoch": 0.2559876184948733,
      "grad_norm": 1.147709846496582,
      "learning_rate": 2.7455545863249008e-05,
      "loss": 1.0822,
      "step": 16540
    },
    {
      "epoch": 0.2561423873089572,
      "grad_norm": 1.7308546304702759,
      "learning_rate": 2.745399738823882e-05,
      "loss": 1.244,
      "step": 16550
    },
    {
      "epoch": 0.2562971561230412,
      "grad_norm": 1.4293160438537598,
      "learning_rate": 2.7452448913228622e-05,
      "loss": 1.1229,
      "step": 16560
    },
    {
      "epoch": 0.25645192493712515,
      "grad_norm": 1.7785022258758545,
      "learning_rate": 2.745090043821843e-05,
      "loss": 1.1098,
      "step": 16570
    },
    {
      "epoch": 0.25660669375120915,
      "grad_norm": 1.4126771688461304,
      "learning_rate": 2.7449351963208233e-05,
      "loss": 1.2967,
      "step": 16580
    },
    {
      "epoch": 0.2567614625652931,
      "grad_norm": 1.5107223987579346,
      "learning_rate": 2.744780348819804e-05,
      "loss": 0.9354,
      "step": 16590
    },
    {
      "epoch": 0.2569162313793771,
      "grad_norm": 2.443847894668579,
      "learning_rate": 2.7446255013187844e-05,
      "loss": 1.0394,
      "step": 16600
    },
    {
      "epoch": 0.257071000193461,
      "grad_norm": 2.0979490280151367,
      "learning_rate": 2.744470653817765e-05,
      "loss": 1.0163,
      "step": 16610
    },
    {
      "epoch": 0.25722576900754496,
      "grad_norm": 1.3174625635147095,
      "learning_rate": 2.744315806316746e-05,
      "loss": 1.0649,
      "step": 16620
    },
    {
      "epoch": 0.25738053782162895,
      "grad_norm": 1.5020118951797485,
      "learning_rate": 2.7441609588157266e-05,
      "loss": 1.1701,
      "step": 16630
    },
    {
      "epoch": 0.2575353066357129,
      "grad_norm": 1.2729148864746094,
      "learning_rate": 2.744006111314707e-05,
      "loss": 1.0559,
      "step": 16640
    },
    {
      "epoch": 0.2576900754497969,
      "grad_norm": 2.1292848587036133,
      "learning_rate": 2.7438512638136877e-05,
      "loss": 1.145,
      "step": 16650
    },
    {
      "epoch": 0.2578448442638808,
      "grad_norm": 2.5758447647094727,
      "learning_rate": 2.743696416312668e-05,
      "loss": 1.1577,
      "step": 16660
    },
    {
      "epoch": 0.2579996130779648,
      "grad_norm": 2.524630546569824,
      "learning_rate": 2.7435415688116487e-05,
      "loss": 1.1882,
      "step": 16670
    },
    {
      "epoch": 0.25815438189204876,
      "grad_norm": 1.8365283012390137,
      "learning_rate": 2.743386721310629e-05,
      "loss": 1.1703,
      "step": 16680
    },
    {
      "epoch": 0.2583091507061327,
      "grad_norm": 1.8639578819274902,
      "learning_rate": 2.7432318738096102e-05,
      "loss": 1.0977,
      "step": 16690
    },
    {
      "epoch": 0.2584639195202167,
      "grad_norm": 1.4500956535339355,
      "learning_rate": 2.7430770263085905e-05,
      "loss": 1.0138,
      "step": 16700
    },
    {
      "epoch": 0.25861868833430063,
      "grad_norm": 1.5020229816436768,
      "learning_rate": 2.7429221788075713e-05,
      "loss": 1.1046,
      "step": 16710
    },
    {
      "epoch": 0.2587734571483846,
      "grad_norm": 1.645377278327942,
      "learning_rate": 2.7427673313065516e-05,
      "loss": 0.9201,
      "step": 16720
    },
    {
      "epoch": 0.25892822596246856,
      "grad_norm": 1.6934529542922974,
      "learning_rate": 2.742612483805532e-05,
      "loss": 1.0465,
      "step": 16730
    },
    {
      "epoch": 0.2590829947765525,
      "grad_norm": 1.676937222480774,
      "learning_rate": 2.7424576363045127e-05,
      "loss": 1.105,
      "step": 16740
    },
    {
      "epoch": 0.2592377635906365,
      "grad_norm": 1.3978818655014038,
      "learning_rate": 2.7423027888034934e-05,
      "loss": 1.0501,
      "step": 16750
    },
    {
      "epoch": 0.25939253240472043,
      "grad_norm": 3.1332411766052246,
      "learning_rate": 2.742147941302474e-05,
      "loss": 1.1057,
      "step": 16760
    },
    {
      "epoch": 0.2595473012188044,
      "grad_norm": 1.3938274383544922,
      "learning_rate": 2.7419930938014545e-05,
      "loss": 0.9382,
      "step": 16770
    },
    {
      "epoch": 0.25970207003288837,
      "grad_norm": 2.1572511196136475,
      "learning_rate": 2.7418382463004352e-05,
      "loss": 1.0669,
      "step": 16780
    },
    {
      "epoch": 0.25985683884697236,
      "grad_norm": 1.6062240600585938,
      "learning_rate": 2.7416833987994156e-05,
      "loss": 1.0925,
      "step": 16790
    },
    {
      "epoch": 0.2600116076610563,
      "grad_norm": 1.62932288646698,
      "learning_rate": 2.7415285512983963e-05,
      "loss": 1.2292,
      "step": 16800
    },
    {
      "epoch": 0.26016637647514024,
      "grad_norm": 2.054433822631836,
      "learning_rate": 2.7413737037973767e-05,
      "loss": 1.2191,
      "step": 16810
    },
    {
      "epoch": 0.26032114528922423,
      "grad_norm": 1.5206257104873657,
      "learning_rate": 2.7412188562963578e-05,
      "loss": 1.0922,
      "step": 16820
    },
    {
      "epoch": 0.26047591410330817,
      "grad_norm": 1.5016840696334839,
      "learning_rate": 2.741064008795338e-05,
      "loss": 1.0121,
      "step": 16830
    },
    {
      "epoch": 0.26063068291739216,
      "grad_norm": 1.482377052307129,
      "learning_rate": 2.740909161294319e-05,
      "loss": 0.9927,
      "step": 16840
    },
    {
      "epoch": 0.2607854517314761,
      "grad_norm": 1.617858648300171,
      "learning_rate": 2.7407543137932992e-05,
      "loss": 1.1479,
      "step": 16850
    },
    {
      "epoch": 0.26094022054556004,
      "grad_norm": 1.1564161777496338,
      "learning_rate": 2.74059946629228e-05,
      "loss": 1.0557,
      "step": 16860
    },
    {
      "epoch": 0.26109498935964404,
      "grad_norm": 1.3085687160491943,
      "learning_rate": 2.7404446187912603e-05,
      "loss": 0.9753,
      "step": 16870
    },
    {
      "epoch": 0.261249758173728,
      "grad_norm": 1.739370584487915,
      "learning_rate": 2.740289771290241e-05,
      "loss": 1.0551,
      "step": 16880
    },
    {
      "epoch": 0.26140452698781197,
      "grad_norm": 2.4084725379943848,
      "learning_rate": 2.7401349237892217e-05,
      "loss": 1.0925,
      "step": 16890
    },
    {
      "epoch": 0.2615592958018959,
      "grad_norm": 1.4899656772613525,
      "learning_rate": 2.7399800762882025e-05,
      "loss": 1.1022,
      "step": 16900
    },
    {
      "epoch": 0.2617140646159799,
      "grad_norm": 1.8198944330215454,
      "learning_rate": 2.7398252287871828e-05,
      "loss": 1.0498,
      "step": 16910
    },
    {
      "epoch": 0.26186883343006384,
      "grad_norm": 1.4889672994613647,
      "learning_rate": 2.7396703812861635e-05,
      "loss": 1.2702,
      "step": 16920
    },
    {
      "epoch": 0.2620236022441478,
      "grad_norm": 2.180232048034668,
      "learning_rate": 2.739515533785144e-05,
      "loss": 0.9817,
      "step": 16930
    },
    {
      "epoch": 0.2621783710582318,
      "grad_norm": 1.5879732370376587,
      "learning_rate": 2.7393606862841246e-05,
      "loss": 1.0248,
      "step": 16940
    },
    {
      "epoch": 0.2623331398723157,
      "grad_norm": 1.6637837886810303,
      "learning_rate": 2.739205838783105e-05,
      "loss": 1.0201,
      "step": 16950
    },
    {
      "epoch": 0.2624879086863997,
      "grad_norm": 1.899207353591919,
      "learning_rate": 2.739050991282086e-05,
      "loss": 1.1048,
      "step": 16960
    },
    {
      "epoch": 0.26264267750048365,
      "grad_norm": 1.1261564493179321,
      "learning_rate": 2.7388961437810664e-05,
      "loss": 1.0131,
      "step": 16970
    },
    {
      "epoch": 0.26279744631456764,
      "grad_norm": 1.3301540613174438,
      "learning_rate": 2.738741296280047e-05,
      "loss": 1.1033,
      "step": 16980
    },
    {
      "epoch": 0.2629522151286516,
      "grad_norm": 1.5057352781295776,
      "learning_rate": 2.7385864487790275e-05,
      "loss": 1.0757,
      "step": 16990
    },
    {
      "epoch": 0.2631069839427355,
      "grad_norm": 2.797825813293457,
      "learning_rate": 2.738431601278008e-05,
      "loss": 1.2533,
      "step": 17000
    },
    {
      "epoch": 0.2632617527568195,
      "grad_norm": 1.4817016124725342,
      "learning_rate": 2.7382767537769886e-05,
      "loss": 1.0764,
      "step": 17010
    },
    {
      "epoch": 0.26341652157090345,
      "grad_norm": 1.4563267230987549,
      "learning_rate": 2.738121906275969e-05,
      "loss": 1.0811,
      "step": 17020
    },
    {
      "epoch": 0.26357129038498744,
      "grad_norm": 1.4682546854019165,
      "learning_rate": 2.73796705877495e-05,
      "loss": 0.9054,
      "step": 17030
    },
    {
      "epoch": 0.2637260591990714,
      "grad_norm": 1.5350451469421387,
      "learning_rate": 2.7378122112739304e-05,
      "loss": 0.9956,
      "step": 17040
    },
    {
      "epoch": 0.2638808280131553,
      "grad_norm": 1.722608208656311,
      "learning_rate": 2.737657363772911e-05,
      "loss": 1.2078,
      "step": 17050
    },
    {
      "epoch": 0.2640355968272393,
      "grad_norm": 1.6617343425750732,
      "learning_rate": 2.7375025162718915e-05,
      "loss": 1.1384,
      "step": 17060
    },
    {
      "epoch": 0.26419036564132325,
      "grad_norm": 1.9666026830673218,
      "learning_rate": 2.7373476687708722e-05,
      "loss": 1.1105,
      "step": 17070
    },
    {
      "epoch": 0.26434513445540725,
      "grad_norm": 1.8014395236968994,
      "learning_rate": 2.7371928212698526e-05,
      "loss": 1.0791,
      "step": 17080
    },
    {
      "epoch": 0.2644999032694912,
      "grad_norm": 1.3661564588546753,
      "learning_rate": 2.7370379737688333e-05,
      "loss": 1.0717,
      "step": 17090
    },
    {
      "epoch": 0.2646546720835752,
      "grad_norm": 1.7349573373794556,
      "learning_rate": 2.736883126267814e-05,
      "loss": 1.087,
      "step": 17100
    },
    {
      "epoch": 0.2648094408976591,
      "grad_norm": 1.7122089862823486,
      "learning_rate": 2.7367282787667947e-05,
      "loss": 1.078,
      "step": 17110
    },
    {
      "epoch": 0.26496420971174306,
      "grad_norm": 2.2230498790740967,
      "learning_rate": 2.736573431265775e-05,
      "loss": 1.1222,
      "step": 17120
    },
    {
      "epoch": 0.26511897852582705,
      "grad_norm": 1.4260551929473877,
      "learning_rate": 2.7364185837647558e-05,
      "loss": 1.1831,
      "step": 17130
    },
    {
      "epoch": 0.265273747339911,
      "grad_norm": 1.9312398433685303,
      "learning_rate": 2.7362637362637362e-05,
      "loss": 0.9678,
      "step": 17140
    },
    {
      "epoch": 0.265428516153995,
      "grad_norm": 2.4529075622558594,
      "learning_rate": 2.736108888762717e-05,
      "loss": 1.1771,
      "step": 17150
    },
    {
      "epoch": 0.2655832849680789,
      "grad_norm": 1.9840750694274902,
      "learning_rate": 2.7359540412616976e-05,
      "loss": 1.2505,
      "step": 17160
    },
    {
      "epoch": 0.2657380537821629,
      "grad_norm": 1.5972685813903809,
      "learning_rate": 2.7357991937606783e-05,
      "loss": 1.0739,
      "step": 17170
    },
    {
      "epoch": 0.26589282259624686,
      "grad_norm": 1.6447874307632446,
      "learning_rate": 2.7356443462596587e-05,
      "loss": 1.0429,
      "step": 17180
    },
    {
      "epoch": 0.2660475914103308,
      "grad_norm": 1.3574550151824951,
      "learning_rate": 2.7354894987586394e-05,
      "loss": 1.0533,
      "step": 17190
    },
    {
      "epoch": 0.2662023602244148,
      "grad_norm": 1.607126235961914,
      "learning_rate": 2.7353346512576198e-05,
      "loss": 1.0343,
      "step": 17200
    },
    {
      "epoch": 0.26635712903849873,
      "grad_norm": 1.3943963050842285,
      "learning_rate": 2.7351798037566005e-05,
      "loss": 0.9016,
      "step": 17210
    },
    {
      "epoch": 0.2665118978525827,
      "grad_norm": 1.7359850406646729,
      "learning_rate": 2.735024956255581e-05,
      "loss": 1.3226,
      "step": 17220
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.017251968383789,
      "learning_rate": 2.734870108754562e-05,
      "loss": 1.1443,
      "step": 17230
    },
    {
      "epoch": 0.2668214354807506,
      "grad_norm": 1.8655868768692017,
      "learning_rate": 2.7347152612535423e-05,
      "loss": 1.0578,
      "step": 17240
    },
    {
      "epoch": 0.2669762042948346,
      "grad_norm": 1.9269391298294067,
      "learning_rate": 2.7345604137525227e-05,
      "loss": 1.3366,
      "step": 17250
    },
    {
      "epoch": 0.26713097310891853,
      "grad_norm": 1.2880319356918335,
      "learning_rate": 2.7344055662515034e-05,
      "loss": 0.9051,
      "step": 17260
    },
    {
      "epoch": 0.26728574192300253,
      "grad_norm": 1.9984830617904663,
      "learning_rate": 2.7342507187504838e-05,
      "loss": 1.105,
      "step": 17270
    },
    {
      "epoch": 0.26744051073708647,
      "grad_norm": 1.9861350059509277,
      "learning_rate": 2.7340958712494645e-05,
      "loss": 1.1227,
      "step": 17280
    },
    {
      "epoch": 0.26759527955117046,
      "grad_norm": 1.6776479482650757,
      "learning_rate": 2.733941023748445e-05,
      "loss": 0.9729,
      "step": 17290
    },
    {
      "epoch": 0.2677500483652544,
      "grad_norm": 1.3466612100601196,
      "learning_rate": 2.733786176247426e-05,
      "loss": 1.1486,
      "step": 17300
    },
    {
      "epoch": 0.26790481717933834,
      "grad_norm": 1.0163912773132324,
      "learning_rate": 2.7336313287464063e-05,
      "loss": 1.0659,
      "step": 17310
    },
    {
      "epoch": 0.26805958599342233,
      "grad_norm": 1.5266236066818237,
      "learning_rate": 2.733476481245387e-05,
      "loss": 1.0936,
      "step": 17320
    },
    {
      "epoch": 0.26821435480750627,
      "grad_norm": 1.2864408493041992,
      "learning_rate": 2.7333216337443674e-05,
      "loss": 1.0513,
      "step": 17330
    },
    {
      "epoch": 0.26836912362159027,
      "grad_norm": 1.9948492050170898,
      "learning_rate": 2.733166786243348e-05,
      "loss": 0.9371,
      "step": 17340
    },
    {
      "epoch": 0.2685238924356742,
      "grad_norm": 1.4833447933197021,
      "learning_rate": 2.7330119387423285e-05,
      "loss": 1.0785,
      "step": 17350
    },
    {
      "epoch": 0.2686786612497582,
      "grad_norm": 1.44096040725708,
      "learning_rate": 2.7328570912413092e-05,
      "loss": 1.2685,
      "step": 17360
    },
    {
      "epoch": 0.26883343006384214,
      "grad_norm": 1.3127222061157227,
      "learning_rate": 2.73270224374029e-05,
      "loss": 1.1156,
      "step": 17370
    },
    {
      "epoch": 0.2689881988779261,
      "grad_norm": 2.3258495330810547,
      "learning_rate": 2.7325473962392706e-05,
      "loss": 1.1096,
      "step": 17380
    },
    {
      "epoch": 0.26914296769201007,
      "grad_norm": 2.0070242881774902,
      "learning_rate": 2.732392548738251e-05,
      "loss": 1.1998,
      "step": 17390
    },
    {
      "epoch": 0.269297736506094,
      "grad_norm": 1.9885227680206299,
      "learning_rate": 2.7322377012372317e-05,
      "loss": 1.0437,
      "step": 17400
    },
    {
      "epoch": 0.269452505320178,
      "grad_norm": 1.2036075592041016,
      "learning_rate": 2.732082853736212e-05,
      "loss": 1.2721,
      "step": 17410
    },
    {
      "epoch": 0.26960727413426194,
      "grad_norm": 1.4080593585968018,
      "learning_rate": 2.7319280062351928e-05,
      "loss": 1.0067,
      "step": 17420
    },
    {
      "epoch": 0.2697620429483459,
      "grad_norm": 1.4822421073913574,
      "learning_rate": 2.7317731587341732e-05,
      "loss": 1.0839,
      "step": 17430
    },
    {
      "epoch": 0.2699168117624299,
      "grad_norm": 1.9128336906433105,
      "learning_rate": 2.7316183112331542e-05,
      "loss": 1.2521,
      "step": 17440
    },
    {
      "epoch": 0.2700715805765138,
      "grad_norm": 1.7409605979919434,
      "learning_rate": 2.7314634637321346e-05,
      "loss": 1.1682,
      "step": 17450
    },
    {
      "epoch": 0.2702263493905978,
      "grad_norm": 1.7751215696334839,
      "learning_rate": 2.7313086162311153e-05,
      "loss": 1.3074,
      "step": 17460
    },
    {
      "epoch": 0.27038111820468175,
      "grad_norm": 1.9816285371780396,
      "learning_rate": 2.7311537687300957e-05,
      "loss": 1.1362,
      "step": 17470
    },
    {
      "epoch": 0.27053588701876574,
      "grad_norm": 1.7525972127914429,
      "learning_rate": 2.7309989212290764e-05,
      "loss": 1.1823,
      "step": 17480
    },
    {
      "epoch": 0.2706906558328497,
      "grad_norm": 1.8537992238998413,
      "learning_rate": 2.7308440737280568e-05,
      "loss": 0.9991,
      "step": 17490
    },
    {
      "epoch": 0.2708454246469336,
      "grad_norm": 1.330287218093872,
      "learning_rate": 2.730689226227037e-05,
      "loss": 0.9931,
      "step": 17500
    },
    {
      "epoch": 0.2710001934610176,
      "grad_norm": 1.7571349143981934,
      "learning_rate": 2.7305343787260182e-05,
      "loss": 1.0521,
      "step": 17510
    },
    {
      "epoch": 0.27115496227510155,
      "grad_norm": 1.651155948638916,
      "learning_rate": 2.7303795312249986e-05,
      "loss": 0.9919,
      "step": 17520
    },
    {
      "epoch": 0.27130973108918555,
      "grad_norm": 1.784010887145996,
      "learning_rate": 2.7302246837239793e-05,
      "loss": 1.002,
      "step": 17530
    },
    {
      "epoch": 0.2714644999032695,
      "grad_norm": 1.5756529569625854,
      "learning_rate": 2.7300698362229597e-05,
      "loss": 1.0599,
      "step": 17540
    },
    {
      "epoch": 0.2716192687173535,
      "grad_norm": 1.7368967533111572,
      "learning_rate": 2.7299149887219404e-05,
      "loss": 1.0492,
      "step": 17550
    },
    {
      "epoch": 0.2717740375314374,
      "grad_norm": 2.0463013648986816,
      "learning_rate": 2.7297601412209208e-05,
      "loss": 1.0553,
      "step": 17560
    },
    {
      "epoch": 0.27192880634552136,
      "grad_norm": 1.5263609886169434,
      "learning_rate": 2.7296052937199015e-05,
      "loss": 1.1802,
      "step": 17570
    },
    {
      "epoch": 0.27208357515960535,
      "grad_norm": 1.4023420810699463,
      "learning_rate": 2.7294504462188822e-05,
      "loss": 1.0076,
      "step": 17580
    },
    {
      "epoch": 0.2722383439736893,
      "grad_norm": 1.2279281616210938,
      "learning_rate": 2.729295598717863e-05,
      "loss": 1.0467,
      "step": 17590
    },
    {
      "epoch": 0.2723931127877733,
      "grad_norm": 1.2572686672210693,
      "learning_rate": 2.7291407512168433e-05,
      "loss": 0.9896,
      "step": 17600
    },
    {
      "epoch": 0.2725478816018572,
      "grad_norm": 2.048882007598877,
      "learning_rate": 2.728985903715824e-05,
      "loss": 0.9296,
      "step": 17610
    },
    {
      "epoch": 0.27270265041594116,
      "grad_norm": 1.296461820602417,
      "learning_rate": 2.7288310562148044e-05,
      "loss": 1.2257,
      "step": 17620
    },
    {
      "epoch": 0.27285741923002516,
      "grad_norm": 2.3130695819854736,
      "learning_rate": 2.728676208713785e-05,
      "loss": 1.0933,
      "step": 17630
    },
    {
      "epoch": 0.2730121880441091,
      "grad_norm": 2.1397061347961426,
      "learning_rate": 2.7285213612127658e-05,
      "loss": 1.0431,
      "step": 17640
    },
    {
      "epoch": 0.2731669568581931,
      "grad_norm": 1.8215150833129883,
      "learning_rate": 2.7283665137117465e-05,
      "loss": 1.0899,
      "step": 17650
    },
    {
      "epoch": 0.273321725672277,
      "grad_norm": 2.5282113552093506,
      "learning_rate": 2.728211666210727e-05,
      "loss": 1.0859,
      "step": 17660
    },
    {
      "epoch": 0.273476494486361,
      "grad_norm": 1.8526257276535034,
      "learning_rate": 2.7280568187097076e-05,
      "loss": 1.0316,
      "step": 17670
    },
    {
      "epoch": 0.27363126330044496,
      "grad_norm": 1.4284672737121582,
      "learning_rate": 2.727901971208688e-05,
      "loss": 0.9993,
      "step": 17680
    },
    {
      "epoch": 0.2737860321145289,
      "grad_norm": 1.4271612167358398,
      "learning_rate": 2.7277471237076687e-05,
      "loss": 1.0765,
      "step": 17690
    },
    {
      "epoch": 0.2739408009286129,
      "grad_norm": 2.127568483352661,
      "learning_rate": 2.727592276206649e-05,
      "loss": 1.2154,
      "step": 17700
    },
    {
      "epoch": 0.27409556974269683,
      "grad_norm": 1.1888643503189087,
      "learning_rate": 2.72743742870563e-05,
      "loss": 0.9087,
      "step": 17710
    },
    {
      "epoch": 0.2742503385567808,
      "grad_norm": 1.900120735168457,
      "learning_rate": 2.7272825812046105e-05,
      "loss": 1.1043,
      "step": 17720
    },
    {
      "epoch": 0.27440510737086476,
      "grad_norm": 1.3662840127944946,
      "learning_rate": 2.7271277337035912e-05,
      "loss": 0.9598,
      "step": 17730
    },
    {
      "epoch": 0.27455987618494876,
      "grad_norm": 1.7182148694992065,
      "learning_rate": 2.7269728862025716e-05,
      "loss": 1.117,
      "step": 17740
    },
    {
      "epoch": 0.2747146449990327,
      "grad_norm": 1.8475443124771118,
      "learning_rate": 2.726818038701552e-05,
      "loss": 1.0914,
      "step": 17750
    },
    {
      "epoch": 0.27486941381311664,
      "grad_norm": 2.164027452468872,
      "learning_rate": 2.7266631912005327e-05,
      "loss": 1.1862,
      "step": 17760
    },
    {
      "epoch": 0.27502418262720063,
      "grad_norm": 1.545551061630249,
      "learning_rate": 2.726508343699513e-05,
      "loss": 0.9708,
      "step": 17770
    },
    {
      "epoch": 0.27517895144128457,
      "grad_norm": 1.5328800678253174,
      "learning_rate": 2.726353496198494e-05,
      "loss": 1.1336,
      "step": 17780
    },
    {
      "epoch": 0.27533372025536856,
      "grad_norm": 1.471199870109558,
      "learning_rate": 2.7261986486974745e-05,
      "loss": 1.1115,
      "step": 17790
    },
    {
      "epoch": 0.2754884890694525,
      "grad_norm": 1.2150652408599854,
      "learning_rate": 2.7260438011964552e-05,
      "loss": 1.1649,
      "step": 17800
    },
    {
      "epoch": 0.27564325788353644,
      "grad_norm": 1.6710097789764404,
      "learning_rate": 2.7258889536954356e-05,
      "loss": 1.0348,
      "step": 17810
    },
    {
      "epoch": 0.27579802669762044,
      "grad_norm": 2.1194334030151367,
      "learning_rate": 2.7257341061944163e-05,
      "loss": 1.0977,
      "step": 17820
    },
    {
      "epoch": 0.2759527955117044,
      "grad_norm": 1.7010942697525024,
      "learning_rate": 2.7255792586933966e-05,
      "loss": 1.2944,
      "step": 17830
    },
    {
      "epoch": 0.27610756432578837,
      "grad_norm": 1.6544197797775269,
      "learning_rate": 2.7254244111923774e-05,
      "loss": 1.14,
      "step": 17840
    },
    {
      "epoch": 0.2762623331398723,
      "grad_norm": 3.233388900756836,
      "learning_rate": 2.725269563691358e-05,
      "loss": 1.0562,
      "step": 17850
    },
    {
      "epoch": 0.2764171019539563,
      "grad_norm": 1.1343086957931519,
      "learning_rate": 2.7251147161903388e-05,
      "loss": 0.97,
      "step": 17860
    },
    {
      "epoch": 0.27657187076804024,
      "grad_norm": 1.7449853420257568,
      "learning_rate": 2.724959868689319e-05,
      "loss": 0.9085,
      "step": 17870
    },
    {
      "epoch": 0.2767266395821242,
      "grad_norm": 2.1803669929504395,
      "learning_rate": 2.7248050211883e-05,
      "loss": 0.9662,
      "step": 17880
    },
    {
      "epoch": 0.2768814083962082,
      "grad_norm": 1.5729424953460693,
      "learning_rate": 2.7246501736872803e-05,
      "loss": 1.1438,
      "step": 17890
    },
    {
      "epoch": 0.2770361772102921,
      "grad_norm": 1.5352096557617188,
      "learning_rate": 2.724495326186261e-05,
      "loss": 1.2221,
      "step": 17900
    },
    {
      "epoch": 0.2771909460243761,
      "grad_norm": 1.581079125404358,
      "learning_rate": 2.7243404786852413e-05,
      "loss": 0.974,
      "step": 17910
    },
    {
      "epoch": 0.27734571483846004,
      "grad_norm": 3.0752625465393066,
      "learning_rate": 2.7241856311842224e-05,
      "loss": 1.0209,
      "step": 17920
    },
    {
      "epoch": 0.27750048365254404,
      "grad_norm": 2.018890142440796,
      "learning_rate": 2.7240307836832028e-05,
      "loss": 0.8691,
      "step": 17930
    },
    {
      "epoch": 0.277655252466628,
      "grad_norm": 1.2929091453552246,
      "learning_rate": 2.7238759361821835e-05,
      "loss": 1.03,
      "step": 17940
    },
    {
      "epoch": 0.2778100212807119,
      "grad_norm": 1.8171734809875488,
      "learning_rate": 2.723721088681164e-05,
      "loss": 1.1464,
      "step": 17950
    },
    {
      "epoch": 0.2779647900947959,
      "grad_norm": 1.9142811298370361,
      "learning_rate": 2.7235662411801446e-05,
      "loss": 1.0669,
      "step": 17960
    },
    {
      "epoch": 0.27811955890887985,
      "grad_norm": 2.2643890380859375,
      "learning_rate": 2.723411393679125e-05,
      "loss": 1.114,
      "step": 17970
    },
    {
      "epoch": 0.27827432772296384,
      "grad_norm": 1.2580986022949219,
      "learning_rate": 2.7232565461781057e-05,
      "loss": 0.958,
      "step": 17980
    },
    {
      "epoch": 0.2784290965370478,
      "grad_norm": 2.0981931686401367,
      "learning_rate": 2.7231016986770864e-05,
      "loss": 1.1337,
      "step": 17990
    },
    {
      "epoch": 0.2785838653511317,
      "grad_norm": 2.0505189895629883,
      "learning_rate": 2.7229468511760668e-05,
      "loss": 1.1066,
      "step": 18000
    },
    {
      "epoch": 0.2787386341652157,
      "grad_norm": 2.305262327194214,
      "learning_rate": 2.7227920036750475e-05,
      "loss": 1.001,
      "step": 18010
    },
    {
      "epoch": 0.27889340297929965,
      "grad_norm": 1.0161430835723877,
      "learning_rate": 2.722637156174028e-05,
      "loss": 0.936,
      "step": 18020
    },
    {
      "epoch": 0.27904817179338365,
      "grad_norm": 1.8768694400787354,
      "learning_rate": 2.7224823086730086e-05,
      "loss": 1.3583,
      "step": 18030
    },
    {
      "epoch": 0.2792029406074676,
      "grad_norm": 2.1575260162353516,
      "learning_rate": 2.722327461171989e-05,
      "loss": 1.1264,
      "step": 18040
    },
    {
      "epoch": 0.2793577094215516,
      "grad_norm": 1.475950837135315,
      "learning_rate": 2.72217261367097e-05,
      "loss": 1.0698,
      "step": 18050
    },
    {
      "epoch": 0.2795124782356355,
      "grad_norm": 1.595073938369751,
      "learning_rate": 2.7220177661699504e-05,
      "loss": 1.2166,
      "step": 18060
    },
    {
      "epoch": 0.27966724704971946,
      "grad_norm": 1.2126743793487549,
      "learning_rate": 2.721862918668931e-05,
      "loss": 1.0683,
      "step": 18070
    },
    {
      "epoch": 0.27982201586380345,
      "grad_norm": 2.0055763721466064,
      "learning_rate": 2.7217080711679114e-05,
      "loss": 1.1666,
      "step": 18080
    },
    {
      "epoch": 0.2799767846778874,
      "grad_norm": 1.5590064525604248,
      "learning_rate": 2.721553223666892e-05,
      "loss": 1.2895,
      "step": 18090
    },
    {
      "epoch": 0.2801315534919714,
      "grad_norm": 2.054255485534668,
      "learning_rate": 2.7213983761658725e-05,
      "loss": 1.1635,
      "step": 18100
    },
    {
      "epoch": 0.2802863223060553,
      "grad_norm": 1.2987488508224487,
      "learning_rate": 2.7212435286648532e-05,
      "loss": 1.1255,
      "step": 18110
    },
    {
      "epoch": 0.2804410911201393,
      "grad_norm": 1.5353864431381226,
      "learning_rate": 2.721088681163834e-05,
      "loss": 1.1777,
      "step": 18120
    },
    {
      "epoch": 0.28059585993422326,
      "grad_norm": 1.741498589515686,
      "learning_rate": 2.7209338336628147e-05,
      "loss": 1.0473,
      "step": 18130
    },
    {
      "epoch": 0.2807506287483072,
      "grad_norm": 1.4227463006973267,
      "learning_rate": 2.720778986161795e-05,
      "loss": 1.1017,
      "step": 18140
    },
    {
      "epoch": 0.2809053975623912,
      "grad_norm": 1.4260083436965942,
      "learning_rate": 2.7206241386607758e-05,
      "loss": 1.044,
      "step": 18150
    },
    {
      "epoch": 0.28106016637647513,
      "grad_norm": 1.6872607469558716,
      "learning_rate": 2.720469291159756e-05,
      "loss": 1.0848,
      "step": 18160
    },
    {
      "epoch": 0.2812149351905591,
      "grad_norm": 1.4485619068145752,
      "learning_rate": 2.720314443658737e-05,
      "loss": 0.9703,
      "step": 18170
    },
    {
      "epoch": 0.28136970400464306,
      "grad_norm": 1.4342647790908813,
      "learning_rate": 2.7201595961577172e-05,
      "loss": 0.8912,
      "step": 18180
    },
    {
      "epoch": 0.281524472818727,
      "grad_norm": 1.9992740154266357,
      "learning_rate": 2.7200047486566983e-05,
      "loss": 1.1103,
      "step": 18190
    },
    {
      "epoch": 0.281679241632811,
      "grad_norm": 1.537948727607727,
      "learning_rate": 2.7198499011556787e-05,
      "loss": 1.0307,
      "step": 18200
    },
    {
      "epoch": 0.28183401044689493,
      "grad_norm": 1.724893569946289,
      "learning_rate": 2.7196950536546594e-05,
      "loss": 1.295,
      "step": 18210
    },
    {
      "epoch": 0.28198877926097893,
      "grad_norm": 1.947489619255066,
      "learning_rate": 2.7195402061536397e-05,
      "loss": 1.1545,
      "step": 18220
    },
    {
      "epoch": 0.28214354807506287,
      "grad_norm": 1.5098451375961304,
      "learning_rate": 2.7193853586526205e-05,
      "loss": 1.1691,
      "step": 18230
    },
    {
      "epoch": 0.28229831688914686,
      "grad_norm": 2.8365707397460938,
      "learning_rate": 2.719230511151601e-05,
      "loss": 1.1144,
      "step": 18240
    },
    {
      "epoch": 0.2824530857032308,
      "grad_norm": 1.4512097835540771,
      "learning_rate": 2.7190756636505812e-05,
      "loss": 1.0906,
      "step": 18250
    },
    {
      "epoch": 0.28260785451731474,
      "grad_norm": 1.280287742614746,
      "learning_rate": 2.7189208161495623e-05,
      "loss": 0.9846,
      "step": 18260
    },
    {
      "epoch": 0.28276262333139873,
      "grad_norm": 1.739955186843872,
      "learning_rate": 2.7187659686485426e-05,
      "loss": 1.0495,
      "step": 18270
    },
    {
      "epoch": 0.28291739214548267,
      "grad_norm": 1.127936840057373,
      "learning_rate": 2.7186111211475234e-05,
      "loss": 0.9451,
      "step": 18280
    },
    {
      "epoch": 0.28307216095956667,
      "grad_norm": 1.5301657915115356,
      "learning_rate": 2.7184562736465037e-05,
      "loss": 1.2484,
      "step": 18290
    },
    {
      "epoch": 0.2832269297736506,
      "grad_norm": 1.8867160081863403,
      "learning_rate": 2.7183014261454844e-05,
      "loss": 1.1186,
      "step": 18300
    },
    {
      "epoch": 0.2833816985877346,
      "grad_norm": 1.3836153745651245,
      "learning_rate": 2.7181465786444648e-05,
      "loss": 1.1897,
      "step": 18310
    },
    {
      "epoch": 0.28353646740181854,
      "grad_norm": 2.513819456100464,
      "learning_rate": 2.7179917311434455e-05,
      "loss": 1.1709,
      "step": 18320
    },
    {
      "epoch": 0.2836912362159025,
      "grad_norm": 2.1486313343048096,
      "learning_rate": 2.7178368836424262e-05,
      "loss": 1.2047,
      "step": 18330
    },
    {
      "epoch": 0.28384600502998647,
      "grad_norm": 1.9473950862884521,
      "learning_rate": 2.717682036141407e-05,
      "loss": 1.0229,
      "step": 18340
    },
    {
      "epoch": 0.2840007738440704,
      "grad_norm": 2.0639147758483887,
      "learning_rate": 2.7175271886403873e-05,
      "loss": 1.0486,
      "step": 18350
    },
    {
      "epoch": 0.2841555426581544,
      "grad_norm": 1.3894329071044922,
      "learning_rate": 2.717372341139368e-05,
      "loss": 1.0484,
      "step": 18360
    },
    {
      "epoch": 0.28431031147223834,
      "grad_norm": 1.210842251777649,
      "learning_rate": 2.7172174936383484e-05,
      "loss": 1.0298,
      "step": 18370
    },
    {
      "epoch": 0.2844650802863223,
      "grad_norm": 1.7797431945800781,
      "learning_rate": 2.717062646137329e-05,
      "loss": 0.9226,
      "step": 18380
    },
    {
      "epoch": 0.2846198491004063,
      "grad_norm": 1.77239191532135,
      "learning_rate": 2.7169077986363095e-05,
      "loss": 1.0582,
      "step": 18390
    },
    {
      "epoch": 0.2847746179144902,
      "grad_norm": 2.219900608062744,
      "learning_rate": 2.7167529511352906e-05,
      "loss": 1.1245,
      "step": 18400
    },
    {
      "epoch": 0.2849293867285742,
      "grad_norm": 2.1088266372680664,
      "learning_rate": 2.716598103634271e-05,
      "loss": 1.1453,
      "step": 18410
    },
    {
      "epoch": 0.28508415554265815,
      "grad_norm": 2.1494245529174805,
      "learning_rate": 2.7164432561332517e-05,
      "loss": 1.1138,
      "step": 18420
    },
    {
      "epoch": 0.28523892435674214,
      "grad_norm": 2.061250925064087,
      "learning_rate": 2.716288408632232e-05,
      "loss": 1.0365,
      "step": 18430
    },
    {
      "epoch": 0.2853936931708261,
      "grad_norm": 1.542162537574768,
      "learning_rate": 2.7161335611312127e-05,
      "loss": 1.2357,
      "step": 18440
    },
    {
      "epoch": 0.28554846198491,
      "grad_norm": 1.5248234272003174,
      "learning_rate": 2.715978713630193e-05,
      "loss": 1.1333,
      "step": 18450
    },
    {
      "epoch": 0.285703230798994,
      "grad_norm": 1.3948255777359009,
      "learning_rate": 2.7158238661291738e-05,
      "loss": 0.9781,
      "step": 18460
    },
    {
      "epoch": 0.28585799961307795,
      "grad_norm": 1.3554298877716064,
      "learning_rate": 2.7156690186281545e-05,
      "loss": 1.1994,
      "step": 18470
    },
    {
      "epoch": 0.28601276842716195,
      "grad_norm": 2.2799668312072754,
      "learning_rate": 2.7155141711271353e-05,
      "loss": 1.198,
      "step": 18480
    },
    {
      "epoch": 0.2861675372412459,
      "grad_norm": 2.1754579544067383,
      "learning_rate": 2.7153593236261156e-05,
      "loss": 1.261,
      "step": 18490
    },
    {
      "epoch": 0.2863223060553299,
      "grad_norm": 2.002286672592163,
      "learning_rate": 2.7152044761250963e-05,
      "loss": 1.1228,
      "step": 18500
    },
    {
      "epoch": 0.2864770748694138,
      "grad_norm": 1.715491533279419,
      "learning_rate": 2.7150496286240767e-05,
      "loss": 1.0778,
      "step": 18510
    },
    {
      "epoch": 0.28663184368349776,
      "grad_norm": 1.7372790575027466,
      "learning_rate": 2.714894781123057e-05,
      "loss": 1.1729,
      "step": 18520
    },
    {
      "epoch": 0.28678661249758175,
      "grad_norm": 1.2697845697402954,
      "learning_rate": 2.714739933622038e-05,
      "loss": 0.9654,
      "step": 18530
    },
    {
      "epoch": 0.2869413813116657,
      "grad_norm": 1.7622581720352173,
      "learning_rate": 2.7145850861210185e-05,
      "loss": 1.1234,
      "step": 18540
    },
    {
      "epoch": 0.2870961501257497,
      "grad_norm": 1.393110990524292,
      "learning_rate": 2.7144302386199992e-05,
      "loss": 1.1323,
      "step": 18550
    },
    {
      "epoch": 0.2872509189398336,
      "grad_norm": 1.2417365312576294,
      "learning_rate": 2.7142753911189796e-05,
      "loss": 1.125,
      "step": 18560
    },
    {
      "epoch": 0.28740568775391756,
      "grad_norm": 1.3686646223068237,
      "learning_rate": 2.7141205436179603e-05,
      "loss": 0.7723,
      "step": 18570
    },
    {
      "epoch": 0.28756045656800155,
      "grad_norm": 1.1442413330078125,
      "learning_rate": 2.7139656961169407e-05,
      "loss": 1.0606,
      "step": 18580
    },
    {
      "epoch": 0.2877152253820855,
      "grad_norm": 2.3321142196655273,
      "learning_rate": 2.7138108486159214e-05,
      "loss": 1.1734,
      "step": 18590
    },
    {
      "epoch": 0.2878699941961695,
      "grad_norm": 1.6687297821044922,
      "learning_rate": 2.713656001114902e-05,
      "loss": 1.1903,
      "step": 18600
    },
    {
      "epoch": 0.2880247630102534,
      "grad_norm": 1.8283787965774536,
      "learning_rate": 2.713501153613883e-05,
      "loss": 1.1576,
      "step": 18610
    },
    {
      "epoch": 0.2881795318243374,
      "grad_norm": 1.8551318645477295,
      "learning_rate": 2.7133463061128632e-05,
      "loss": 1.1983,
      "step": 18620
    },
    {
      "epoch": 0.28833430063842136,
      "grad_norm": 1.4433958530426025,
      "learning_rate": 2.713191458611844e-05,
      "loss": 0.8611,
      "step": 18630
    },
    {
      "epoch": 0.2884890694525053,
      "grad_norm": 1.2771142721176147,
      "learning_rate": 2.7130366111108243e-05,
      "loss": 1.1941,
      "step": 18640
    },
    {
      "epoch": 0.2886438382665893,
      "grad_norm": 2.064450740814209,
      "learning_rate": 2.712881763609805e-05,
      "loss": 1.0437,
      "step": 18650
    },
    {
      "epoch": 0.28879860708067323,
      "grad_norm": 1.2761262655258179,
      "learning_rate": 2.7127269161087854e-05,
      "loss": 0.9981,
      "step": 18660
    },
    {
      "epoch": 0.2889533758947572,
      "grad_norm": 1.9487491846084595,
      "learning_rate": 2.7125720686077664e-05,
      "loss": 0.8798,
      "step": 18670
    },
    {
      "epoch": 0.28910814470884116,
      "grad_norm": 1.426234245300293,
      "learning_rate": 2.7124172211067468e-05,
      "loss": 1.0459,
      "step": 18680
    },
    {
      "epoch": 0.28926291352292516,
      "grad_norm": 1.3597408533096313,
      "learning_rate": 2.7122623736057275e-05,
      "loss": 1.0348,
      "step": 18690
    },
    {
      "epoch": 0.2894176823370091,
      "grad_norm": 2.2442493438720703,
      "learning_rate": 2.712107526104708e-05,
      "loss": 0.9751,
      "step": 18700
    },
    {
      "epoch": 0.28957245115109304,
      "grad_norm": 1.392598271369934,
      "learning_rate": 2.7119526786036886e-05,
      "loss": 1.1008,
      "step": 18710
    },
    {
      "epoch": 0.28972721996517703,
      "grad_norm": 1.7813382148742676,
      "learning_rate": 2.711797831102669e-05,
      "loss": 1.1037,
      "step": 18720
    },
    {
      "epoch": 0.28988198877926097,
      "grad_norm": 1.5231903791427612,
      "learning_rate": 2.7116429836016497e-05,
      "loss": 1.147,
      "step": 18730
    },
    {
      "epoch": 0.29003675759334496,
      "grad_norm": 1.1373779773712158,
      "learning_rate": 2.7114881361006304e-05,
      "loss": 1.204,
      "step": 18740
    },
    {
      "epoch": 0.2901915264074289,
      "grad_norm": 1.6802847385406494,
      "learning_rate": 2.711333288599611e-05,
      "loss": 1.258,
      "step": 18750
    },
    {
      "epoch": 0.29034629522151284,
      "grad_norm": 1.8728398084640503,
      "learning_rate": 2.7111784410985915e-05,
      "loss": 1.1331,
      "step": 18760
    },
    {
      "epoch": 0.29050106403559683,
      "grad_norm": 1.341889500617981,
      "learning_rate": 2.711023593597572e-05,
      "loss": 1.0449,
      "step": 18770
    },
    {
      "epoch": 0.2906558328496808,
      "grad_norm": 1.5918289422988892,
      "learning_rate": 2.7108687460965526e-05,
      "loss": 1.0689,
      "step": 18780
    },
    {
      "epoch": 0.29081060166376477,
      "grad_norm": 1.4650812149047852,
      "learning_rate": 2.710713898595533e-05,
      "loss": 1.1481,
      "step": 18790
    },
    {
      "epoch": 0.2909653704778487,
      "grad_norm": 1.0901432037353516,
      "learning_rate": 2.7105590510945137e-05,
      "loss": 0.9931,
      "step": 18800
    },
    {
      "epoch": 0.2911201392919327,
      "grad_norm": 1.7306889295578003,
      "learning_rate": 2.7104042035934944e-05,
      "loss": 1.1729,
      "step": 18810
    },
    {
      "epoch": 0.29127490810601664,
      "grad_norm": 1.879055380821228,
      "learning_rate": 2.710249356092475e-05,
      "loss": 1.2056,
      "step": 18820
    },
    {
      "epoch": 0.2914296769201006,
      "grad_norm": 1.3119730949401855,
      "learning_rate": 2.7100945085914555e-05,
      "loss": 1.0516,
      "step": 18830
    },
    {
      "epoch": 0.29158444573418457,
      "grad_norm": 2.3971705436706543,
      "learning_rate": 2.7099396610904362e-05,
      "loss": 0.9679,
      "step": 18840
    },
    {
      "epoch": 0.2917392145482685,
      "grad_norm": 1.4706881046295166,
      "learning_rate": 2.7097848135894166e-05,
      "loss": 1.0859,
      "step": 18850
    },
    {
      "epoch": 0.2918939833623525,
      "grad_norm": 1.9054887294769287,
      "learning_rate": 2.7096299660883973e-05,
      "loss": 1.2255,
      "step": 18860
    },
    {
      "epoch": 0.29204875217643644,
      "grad_norm": 1.3817046880722046,
      "learning_rate": 2.7094751185873777e-05,
      "loss": 1.002,
      "step": 18870
    },
    {
      "epoch": 0.29220352099052044,
      "grad_norm": 1.304154634475708,
      "learning_rate": 2.7093202710863587e-05,
      "loss": 1.0775,
      "step": 18880
    },
    {
      "epoch": 0.2923582898046044,
      "grad_norm": 1.6989812850952148,
      "learning_rate": 2.709165423585339e-05,
      "loss": 1.2031,
      "step": 18890
    },
    {
      "epoch": 0.2925130586186883,
      "grad_norm": 1.7288267612457275,
      "learning_rate": 2.7090105760843198e-05,
      "loss": 1.243,
      "step": 18900
    },
    {
      "epoch": 0.2926678274327723,
      "grad_norm": 2.655515670776367,
      "learning_rate": 2.7088557285833002e-05,
      "loss": 1.1006,
      "step": 18910
    },
    {
      "epoch": 0.29282259624685625,
      "grad_norm": 1.8636538982391357,
      "learning_rate": 2.708700881082281e-05,
      "loss": 1.1252,
      "step": 18920
    },
    {
      "epoch": 0.29297736506094024,
      "grad_norm": 2.1119983196258545,
      "learning_rate": 2.7085460335812613e-05,
      "loss": 1.1186,
      "step": 18930
    },
    {
      "epoch": 0.2931321338750242,
      "grad_norm": 1.462847352027893,
      "learning_rate": 2.708391186080242e-05,
      "loss": 1.0093,
      "step": 18940
    },
    {
      "epoch": 0.2932869026891081,
      "grad_norm": 2.164384126663208,
      "learning_rate": 2.7082363385792227e-05,
      "loss": 1.0661,
      "step": 18950
    },
    {
      "epoch": 0.2934416715031921,
      "grad_norm": 1.8126388788223267,
      "learning_rate": 2.7080814910782034e-05,
      "loss": 1.129,
      "step": 18960
    },
    {
      "epoch": 0.29359644031727605,
      "grad_norm": 2.2248289585113525,
      "learning_rate": 2.7079266435771838e-05,
      "loss": 0.8999,
      "step": 18970
    },
    {
      "epoch": 0.29375120913136005,
      "grad_norm": 2.191032886505127,
      "learning_rate": 2.7077717960761645e-05,
      "loss": 1.1761,
      "step": 18980
    },
    {
      "epoch": 0.293905977945444,
      "grad_norm": 1.077736735343933,
      "learning_rate": 2.707616948575145e-05,
      "loss": 1.0368,
      "step": 18990
    },
    {
      "epoch": 0.294060746759528,
      "grad_norm": 1.3838014602661133,
      "learning_rate": 2.7074621010741256e-05,
      "loss": 0.9171,
      "step": 19000
    },
    {
      "epoch": 0.2942155155736119,
      "grad_norm": 1.8225756883621216,
      "learning_rate": 2.7073072535731063e-05,
      "loss": 1.1587,
      "step": 19010
    },
    {
      "epoch": 0.29437028438769586,
      "grad_norm": 1.2067104578018188,
      "learning_rate": 2.7071678908221886e-05,
      "loss": 1.0914,
      "step": 19020
    },
    {
      "epoch": 0.29452505320177985,
      "grad_norm": 1.2921582460403442,
      "learning_rate": 2.7070130433211693e-05,
      "loss": 1.045,
      "step": 19030
    },
    {
      "epoch": 0.2946798220158638,
      "grad_norm": 1.3527944087982178,
      "learning_rate": 2.70685819582015e-05,
      "loss": 1.0643,
      "step": 19040
    },
    {
      "epoch": 0.2948345908299478,
      "grad_norm": 1.1507123708724976,
      "learning_rate": 2.7067033483191304e-05,
      "loss": 0.9302,
      "step": 19050
    },
    {
      "epoch": 0.2949893596440317,
      "grad_norm": 2.072420597076416,
      "learning_rate": 2.706548500818111e-05,
      "loss": 0.952,
      "step": 19060
    },
    {
      "epoch": 0.29514412845811566,
      "grad_norm": 1.3948708772659302,
      "learning_rate": 2.7063936533170915e-05,
      "loss": 1.1381,
      "step": 19070
    },
    {
      "epoch": 0.29529889727219966,
      "grad_norm": 1.9842363595962524,
      "learning_rate": 2.7062388058160722e-05,
      "loss": 0.9103,
      "step": 19080
    },
    {
      "epoch": 0.2954536660862836,
      "grad_norm": 1.8748664855957031,
      "learning_rate": 2.7060839583150526e-05,
      "loss": 0.9987,
      "step": 19090
    },
    {
      "epoch": 0.2956084349003676,
      "grad_norm": 1.716774821281433,
      "learning_rate": 2.7059291108140336e-05,
      "loss": 1.1828,
      "step": 19100
    },
    {
      "epoch": 0.29576320371445153,
      "grad_norm": 1.7505521774291992,
      "learning_rate": 2.705774263313014e-05,
      "loss": 1.0425,
      "step": 19110
    },
    {
      "epoch": 0.2959179725285355,
      "grad_norm": 2.257901191711426,
      "learning_rate": 2.7056194158119947e-05,
      "loss": 1.0116,
      "step": 19120
    },
    {
      "epoch": 0.29607274134261946,
      "grad_norm": 1.3775795698165894,
      "learning_rate": 2.705464568310975e-05,
      "loss": 1.2051,
      "step": 19130
    },
    {
      "epoch": 0.2962275101567034,
      "grad_norm": 1.3282818794250488,
      "learning_rate": 2.7053097208099558e-05,
      "loss": 1.0899,
      "step": 19140
    },
    {
      "epoch": 0.2963822789707874,
      "grad_norm": 1.5813664197921753,
      "learning_rate": 2.7051548733089362e-05,
      "loss": 1.1825,
      "step": 19150
    },
    {
      "epoch": 0.29653704778487133,
      "grad_norm": 2.4901084899902344,
      "learning_rate": 2.705000025807917e-05,
      "loss": 1.2142,
      "step": 19160
    },
    {
      "epoch": 0.2966918165989553,
      "grad_norm": 2.243075370788574,
      "learning_rate": 2.7048451783068976e-05,
      "loss": 1.1297,
      "step": 19170
    },
    {
      "epoch": 0.29684658541303927,
      "grad_norm": 2.232811450958252,
      "learning_rate": 2.7046903308058783e-05,
      "loss": 1.1059,
      "step": 19180
    },
    {
      "epoch": 0.29700135422712326,
      "grad_norm": 1.5056347846984863,
      "learning_rate": 2.7045354833048587e-05,
      "loss": 1.17,
      "step": 19190
    },
    {
      "epoch": 0.2971561230412072,
      "grad_norm": 1.118897557258606,
      "learning_rate": 2.704380635803839e-05,
      "loss": 1.1115,
      "step": 19200
    },
    {
      "epoch": 0.29731089185529114,
      "grad_norm": 1.38021981716156,
      "learning_rate": 2.7042257883028198e-05,
      "loss": 1.182,
      "step": 19210
    },
    {
      "epoch": 0.29746566066937513,
      "grad_norm": 1.6431632041931152,
      "learning_rate": 2.7040709408018e-05,
      "loss": 1.1604,
      "step": 19220
    },
    {
      "epoch": 0.29762042948345907,
      "grad_norm": 1.4886012077331543,
      "learning_rate": 2.7039160933007812e-05,
      "loss": 0.9193,
      "step": 19230
    },
    {
      "epoch": 0.29777519829754306,
      "grad_norm": 1.287614345550537,
      "learning_rate": 2.7037612457997616e-05,
      "loss": 1.1792,
      "step": 19240
    },
    {
      "epoch": 0.297929967111627,
      "grad_norm": 1.7462167739868164,
      "learning_rate": 2.7036063982987423e-05,
      "loss": 1.1432,
      "step": 19250
    },
    {
      "epoch": 0.29808473592571094,
      "grad_norm": 2.6967897415161133,
      "learning_rate": 2.7034515507977227e-05,
      "loss": 1.0238,
      "step": 19260
    },
    {
      "epoch": 0.29823950473979494,
      "grad_norm": 1.0836138725280762,
      "learning_rate": 2.7032967032967034e-05,
      "loss": 0.994,
      "step": 19270
    },
    {
      "epoch": 0.2983942735538789,
      "grad_norm": 1.5863795280456543,
      "learning_rate": 2.7031418557956838e-05,
      "loss": 1.0043,
      "step": 19280
    },
    {
      "epoch": 0.29854904236796287,
      "grad_norm": 2.0655100345611572,
      "learning_rate": 2.7029870082946645e-05,
      "loss": 1.2778,
      "step": 19290
    },
    {
      "epoch": 0.2987038111820468,
      "grad_norm": 1.6206282377243042,
      "learning_rate": 2.7028321607936452e-05,
      "loss": 1.159,
      "step": 19300
    },
    {
      "epoch": 0.2988585799961308,
      "grad_norm": 2.3714399337768555,
      "learning_rate": 2.702677313292626e-05,
      "loss": 1.0017,
      "step": 19310
    },
    {
      "epoch": 0.29901334881021474,
      "grad_norm": 1.2000874280929565,
      "learning_rate": 2.7025224657916063e-05,
      "loss": 0.9634,
      "step": 19320
    },
    {
      "epoch": 0.2991681176242987,
      "grad_norm": 1.4106683731079102,
      "learning_rate": 2.702367618290587e-05,
      "loss": 0.9608,
      "step": 19330
    },
    {
      "epoch": 0.2993228864383827,
      "grad_norm": 1.1901217699050903,
      "learning_rate": 2.7022127707895674e-05,
      "loss": 0.8922,
      "step": 19340
    },
    {
      "epoch": 0.2994776552524666,
      "grad_norm": 1.4653996229171753,
      "learning_rate": 2.702057923288548e-05,
      "loss": 1.1521,
      "step": 19350
    },
    {
      "epoch": 0.2996324240665506,
      "grad_norm": 1.4617055654525757,
      "learning_rate": 2.7019030757875285e-05,
      "loss": 1.2794,
      "step": 19360
    },
    {
      "epoch": 0.29978719288063455,
      "grad_norm": 1.8112167119979858,
      "learning_rate": 2.7017482282865095e-05,
      "loss": 1.0333,
      "step": 19370
    },
    {
      "epoch": 0.29994196169471854,
      "grad_norm": 1.6626137495040894,
      "learning_rate": 2.70159338078549e-05,
      "loss": 1.0483,
      "step": 19380
    },
    {
      "epoch": 0.3000967305088025,
      "grad_norm": 1.7778993844985962,
      "learning_rate": 2.7014385332844706e-05,
      "loss": 1.1901,
      "step": 19390
    },
    {
      "epoch": 0.3002514993228864,
      "grad_norm": 1.4417580366134644,
      "learning_rate": 2.701283685783451e-05,
      "loss": 1.1113,
      "step": 19400
    },
    {
      "epoch": 0.3004062681369704,
      "grad_norm": 1.732012152671814,
      "learning_rate": 2.7011288382824317e-05,
      "loss": 1.1681,
      "step": 19410
    },
    {
      "epoch": 0.30056103695105435,
      "grad_norm": 1.3395726680755615,
      "learning_rate": 2.700973990781412e-05,
      "loss": 0.9766,
      "step": 19420
    },
    {
      "epoch": 0.30071580576513834,
      "grad_norm": 1.5317801237106323,
      "learning_rate": 2.7008191432803928e-05,
      "loss": 1.0344,
      "step": 19430
    },
    {
      "epoch": 0.3008705745792223,
      "grad_norm": 1.98899507522583,
      "learning_rate": 2.7006642957793735e-05,
      "loss": 1.1019,
      "step": 19440
    },
    {
      "epoch": 0.3010253433933062,
      "grad_norm": 1.275286078453064,
      "learning_rate": 2.700509448278354e-05,
      "loss": 0.9109,
      "step": 19450
    },
    {
      "epoch": 0.3011801122073902,
      "grad_norm": 1.687958836555481,
      "learning_rate": 2.7003546007773346e-05,
      "loss": 0.9867,
      "step": 19460
    },
    {
      "epoch": 0.30133488102147415,
      "grad_norm": 2.2098376750946045,
      "learning_rate": 2.700199753276315e-05,
      "loss": 1.1954,
      "step": 19470
    },
    {
      "epoch": 0.30148964983555815,
      "grad_norm": 1.342796802520752,
      "learning_rate": 2.7000449057752957e-05,
      "loss": 1.0158,
      "step": 19480
    },
    {
      "epoch": 0.3016444186496421,
      "grad_norm": 1.4859914779663086,
      "learning_rate": 2.699890058274276e-05,
      "loss": 1.1508,
      "step": 19490
    },
    {
      "epoch": 0.3017991874637261,
      "grad_norm": 1.8371003866195679,
      "learning_rate": 2.6997352107732568e-05,
      "loss": 1.1633,
      "step": 19500
    },
    {
      "epoch": 0.30195395627781,
      "grad_norm": 1.744929313659668,
      "learning_rate": 2.6995803632722375e-05,
      "loss": 1.0429,
      "step": 19510
    },
    {
      "epoch": 0.30210872509189396,
      "grad_norm": 2.3790416717529297,
      "learning_rate": 2.6994255157712182e-05,
      "loss": 1.0965,
      "step": 19520
    },
    {
      "epoch": 0.30226349390597795,
      "grad_norm": 1.5403902530670166,
      "learning_rate": 2.6992706682701986e-05,
      "loss": 1.0826,
      "step": 19530
    },
    {
      "epoch": 0.3024182627200619,
      "grad_norm": 1.4165688753128052,
      "learning_rate": 2.6991158207691793e-05,
      "loss": 1.1076,
      "step": 19540
    },
    {
      "epoch": 0.3025730315341459,
      "grad_norm": 1.5417020320892334,
      "learning_rate": 2.6989609732681596e-05,
      "loss": 1.1997,
      "step": 19550
    },
    {
      "epoch": 0.3027278003482298,
      "grad_norm": 1.7743319272994995,
      "learning_rate": 2.6988061257671404e-05,
      "loss": 1.1518,
      "step": 19560
    },
    {
      "epoch": 0.3028825691623138,
      "grad_norm": 1.633241891860962,
      "learning_rate": 2.6986512782661207e-05,
      "loss": 1.0791,
      "step": 19570
    },
    {
      "epoch": 0.30303733797639776,
      "grad_norm": 1.4069340229034424,
      "learning_rate": 2.6984964307651018e-05,
      "loss": 1.0123,
      "step": 19580
    },
    {
      "epoch": 0.3031921067904817,
      "grad_norm": 1.5109304189682007,
      "learning_rate": 2.698341583264082e-05,
      "loss": 1.1157,
      "step": 19590
    },
    {
      "epoch": 0.3033468756045657,
      "grad_norm": 2.0021488666534424,
      "learning_rate": 2.698186735763063e-05,
      "loss": 0.9837,
      "step": 19600
    },
    {
      "epoch": 0.30350164441864963,
      "grad_norm": 1.536558985710144,
      "learning_rate": 2.6980318882620433e-05,
      "loss": 0.8513,
      "step": 19610
    },
    {
      "epoch": 0.3036564132327336,
      "grad_norm": 1.8751262426376343,
      "learning_rate": 2.697877040761024e-05,
      "loss": 1.1007,
      "step": 19620
    },
    {
      "epoch": 0.30381118204681756,
      "grad_norm": 2.0609679222106934,
      "learning_rate": 2.6977221932600043e-05,
      "loss": 1.0685,
      "step": 19630
    },
    {
      "epoch": 0.3039659508609015,
      "grad_norm": 2.039942741394043,
      "learning_rate": 2.6975673457589854e-05,
      "loss": 1.1735,
      "step": 19640
    },
    {
      "epoch": 0.3041207196749855,
      "grad_norm": 1.863779902458191,
      "learning_rate": 2.6974124982579658e-05,
      "loss": 0.8696,
      "step": 19650
    },
    {
      "epoch": 0.30427548848906943,
      "grad_norm": 1.2125256061553955,
      "learning_rate": 2.6972576507569465e-05,
      "loss": 1.1326,
      "step": 19660
    },
    {
      "epoch": 0.30443025730315343,
      "grad_norm": 1.6371877193450928,
      "learning_rate": 2.697102803255927e-05,
      "loss": 1.1977,
      "step": 19670
    },
    {
      "epoch": 0.30458502611723737,
      "grad_norm": 1.955858588218689,
      "learning_rate": 2.6969479557549076e-05,
      "loss": 1.0907,
      "step": 19680
    },
    {
      "epoch": 0.30473979493132136,
      "grad_norm": 2.0817954540252686,
      "learning_rate": 2.696793108253888e-05,
      "loss": 1.1318,
      "step": 19690
    },
    {
      "epoch": 0.3048945637454053,
      "grad_norm": 1.9517017602920532,
      "learning_rate": 2.6966382607528683e-05,
      "loss": 1.0577,
      "step": 19700
    },
    {
      "epoch": 0.30504933255948924,
      "grad_norm": 1.3345980644226074,
      "learning_rate": 2.6964834132518494e-05,
      "loss": 1.1306,
      "step": 19710
    },
    {
      "epoch": 0.30520410137357323,
      "grad_norm": 1.7559207677841187,
      "learning_rate": 2.6963285657508297e-05,
      "loss": 0.9484,
      "step": 19720
    },
    {
      "epoch": 0.30535887018765717,
      "grad_norm": 1.86173415184021,
      "learning_rate": 2.6961737182498105e-05,
      "loss": 1.023,
      "step": 19730
    },
    {
      "epoch": 0.30551363900174117,
      "grad_norm": 2.469393014907837,
      "learning_rate": 2.696018870748791e-05,
      "loss": 1.1441,
      "step": 19740
    },
    {
      "epoch": 0.3056684078158251,
      "grad_norm": 1.1783373355865479,
      "learning_rate": 2.6958640232477716e-05,
      "loss": 1.045,
      "step": 19750
    },
    {
      "epoch": 0.3058231766299091,
      "grad_norm": 1.3460662364959717,
      "learning_rate": 2.695709175746752e-05,
      "loss": 1.2061,
      "step": 19760
    },
    {
      "epoch": 0.30597794544399304,
      "grad_norm": 1.670246958732605,
      "learning_rate": 2.6955543282457326e-05,
      "loss": 1.0649,
      "step": 19770
    },
    {
      "epoch": 0.306132714258077,
      "grad_norm": 1.503747820854187,
      "learning_rate": 2.6953994807447134e-05,
      "loss": 0.9692,
      "step": 19780
    },
    {
      "epoch": 0.30628748307216097,
      "grad_norm": 2.1834864616394043,
      "learning_rate": 2.695244633243694e-05,
      "loss": 1.0977,
      "step": 19790
    },
    {
      "epoch": 0.3064422518862449,
      "grad_norm": 1.53212308883667,
      "learning_rate": 2.6950897857426744e-05,
      "loss": 1.105,
      "step": 19800
    },
    {
      "epoch": 0.3065970207003289,
      "grad_norm": 1.587982177734375,
      "learning_rate": 2.694934938241655e-05,
      "loss": 0.8979,
      "step": 19810
    },
    {
      "epoch": 0.30675178951441284,
      "grad_norm": 2.141510486602783,
      "learning_rate": 2.6947800907406355e-05,
      "loss": 1.0741,
      "step": 19820
    },
    {
      "epoch": 0.3069065583284968,
      "grad_norm": 1.7760193347930908,
      "learning_rate": 2.6946252432396162e-05,
      "loss": 1.1563,
      "step": 19830
    },
    {
      "epoch": 0.3070613271425808,
      "grad_norm": 2.117983818054199,
      "learning_rate": 2.6944703957385966e-05,
      "loss": 1.1093,
      "step": 19840
    },
    {
      "epoch": 0.3072160959566647,
      "grad_norm": 1.813638687133789,
      "learning_rate": 2.6943155482375777e-05,
      "loss": 1.2203,
      "step": 19850
    },
    {
      "epoch": 0.3073708647707487,
      "grad_norm": 1.077937364578247,
      "learning_rate": 2.694160700736558e-05,
      "loss": 0.8705,
      "step": 19860
    },
    {
      "epoch": 0.30752563358483265,
      "grad_norm": 1.2679001092910767,
      "learning_rate": 2.6940058532355388e-05,
      "loss": 1.1624,
      "step": 19870
    },
    {
      "epoch": 0.30768040239891664,
      "grad_norm": 2.2698326110839844,
      "learning_rate": 2.693851005734519e-05,
      "loss": 1.0695,
      "step": 19880
    },
    {
      "epoch": 0.3078351712130006,
      "grad_norm": 2.280719518661499,
      "learning_rate": 2.6936961582335e-05,
      "loss": 1.1418,
      "step": 19890
    },
    {
      "epoch": 0.3079899400270845,
      "grad_norm": 1.5220955610275269,
      "learning_rate": 2.6935413107324802e-05,
      "loss": 1.0187,
      "step": 19900
    },
    {
      "epoch": 0.3081447088411685,
      "grad_norm": 1.553610920906067,
      "learning_rate": 2.693386463231461e-05,
      "loss": 1.0596,
      "step": 19910
    },
    {
      "epoch": 0.30829947765525245,
      "grad_norm": 2.0312588214874268,
      "learning_rate": 2.6932316157304417e-05,
      "loss": 1.0383,
      "step": 19920
    },
    {
      "epoch": 0.30845424646933645,
      "grad_norm": 1.190393328666687,
      "learning_rate": 2.6930767682294224e-05,
      "loss": 1.2151,
      "step": 19930
    },
    {
      "epoch": 0.3086090152834204,
      "grad_norm": 1.3798638582229614,
      "learning_rate": 2.6929219207284027e-05,
      "loss": 1.1581,
      "step": 19940
    },
    {
      "epoch": 0.3087637840975044,
      "grad_norm": 1.7527427673339844,
      "learning_rate": 2.692767073227383e-05,
      "loss": 1.1613,
      "step": 19950
    },
    {
      "epoch": 0.3089185529115883,
      "grad_norm": 1.6859244108200073,
      "learning_rate": 2.692612225726364e-05,
      "loss": 1.4532,
      "step": 19960
    },
    {
      "epoch": 0.30907332172567226,
      "grad_norm": 1.5055545568466187,
      "learning_rate": 2.6924573782253442e-05,
      "loss": 1.0192,
      "step": 19970
    },
    {
      "epoch": 0.30922809053975625,
      "grad_norm": 1.382636308670044,
      "learning_rate": 2.692302530724325e-05,
      "loss": 0.8954,
      "step": 19980
    },
    {
      "epoch": 0.3093828593538402,
      "grad_norm": 1.359354853630066,
      "learning_rate": 2.6921476832233056e-05,
      "loss": 1.0286,
      "step": 19990
    },
    {
      "epoch": 0.3095376281679242,
      "grad_norm": 1.6674026250839233,
      "learning_rate": 2.6919928357222863e-05,
      "loss": 1.2488,
      "step": 20000
    },
    {
      "epoch": 0.3096923969820081,
      "grad_norm": 1.2485570907592773,
      "learning_rate": 2.6918379882212667e-05,
      "loss": 1.1363,
      "step": 20010
    },
    {
      "epoch": 0.30984716579609206,
      "grad_norm": 1.6935228109359741,
      "learning_rate": 2.6916831407202474e-05,
      "loss": 1.087,
      "step": 20020
    },
    {
      "epoch": 0.31000193461017606,
      "grad_norm": 2.308695077896118,
      "learning_rate": 2.6915282932192278e-05,
      "loss": 1.0443,
      "step": 20030
    },
    {
      "epoch": 0.31015670342426,
      "grad_norm": 1.9323331117630005,
      "learning_rate": 2.6913734457182085e-05,
      "loss": 1.0591,
      "step": 20040
    },
    {
      "epoch": 0.310311472238344,
      "grad_norm": 1.8244214057922363,
      "learning_rate": 2.6912185982171892e-05,
      "loss": 1.1973,
      "step": 20050
    },
    {
      "epoch": 0.3104662410524279,
      "grad_norm": 1.1303645372390747,
      "learning_rate": 2.69106375071617e-05,
      "loss": 1.0258,
      "step": 20060
    },
    {
      "epoch": 0.3106210098665119,
      "grad_norm": 1.4696664810180664,
      "learning_rate": 2.6909089032151503e-05,
      "loss": 1.0851,
      "step": 20070
    },
    {
      "epoch": 0.31077577868059586,
      "grad_norm": 1.4341225624084473,
      "learning_rate": 2.690754055714131e-05,
      "loss": 1.0363,
      "step": 20080
    },
    {
      "epoch": 0.3109305474946798,
      "grad_norm": 2.062079429626465,
      "learning_rate": 2.6905992082131114e-05,
      "loss": 0.9477,
      "step": 20090
    },
    {
      "epoch": 0.3110853163087638,
      "grad_norm": 1.6738866567611694,
      "learning_rate": 2.690444360712092e-05,
      "loss": 0.9064,
      "step": 20100
    },
    {
      "epoch": 0.31124008512284773,
      "grad_norm": 2.481842041015625,
      "learning_rate": 2.6902895132110725e-05,
      "loss": 1.0871,
      "step": 20110
    },
    {
      "epoch": 0.3113948539369317,
      "grad_norm": 1.3779882192611694,
      "learning_rate": 2.6901346657100536e-05,
      "loss": 1.1732,
      "step": 20120
    },
    {
      "epoch": 0.31154962275101566,
      "grad_norm": 2.1153604984283447,
      "learning_rate": 2.689979818209034e-05,
      "loss": 1.1843,
      "step": 20130
    },
    {
      "epoch": 0.31170439156509966,
      "grad_norm": 1.647907018661499,
      "learning_rate": 2.6898249707080146e-05,
      "loss": 1.179,
      "step": 20140
    },
    {
      "epoch": 0.3118591603791836,
      "grad_norm": 1.7423123121261597,
      "learning_rate": 2.689670123206995e-05,
      "loss": 1.0689,
      "step": 20150
    },
    {
      "epoch": 0.31201392919326754,
      "grad_norm": 1.929485559463501,
      "learning_rate": 2.6895152757059757e-05,
      "loss": 0.9541,
      "step": 20160
    },
    {
      "epoch": 0.31216869800735153,
      "grad_norm": 1.858665108680725,
      "learning_rate": 2.689360428204956e-05,
      "loss": 0.9528,
      "step": 20170
    },
    {
      "epoch": 0.31232346682143547,
      "grad_norm": 1.7547751665115356,
      "learning_rate": 2.6892055807039368e-05,
      "loss": 1.2453,
      "step": 20180
    },
    {
      "epoch": 0.31247823563551946,
      "grad_norm": 2.4534659385681152,
      "learning_rate": 2.6890507332029175e-05,
      "loss": 1.0232,
      "step": 20190
    },
    {
      "epoch": 0.3126330044496034,
      "grad_norm": 2.185067892074585,
      "learning_rate": 2.688895885701898e-05,
      "loss": 0.9513,
      "step": 20200
    },
    {
      "epoch": 0.31278777326368734,
      "grad_norm": 2.061817169189453,
      "learning_rate": 2.6887410382008786e-05,
      "loss": 1.1628,
      "step": 20210
    },
    {
      "epoch": 0.31294254207777134,
      "grad_norm": 1.2718391418457031,
      "learning_rate": 2.688586190699859e-05,
      "loss": 1.1704,
      "step": 20220
    },
    {
      "epoch": 0.3130973108918553,
      "grad_norm": 1.4173369407653809,
      "learning_rate": 2.6884313431988397e-05,
      "loss": 1.2159,
      "step": 20230
    },
    {
      "epoch": 0.31325207970593927,
      "grad_norm": 1.7445861101150513,
      "learning_rate": 2.68827649569782e-05,
      "loss": 1.1662,
      "step": 20240
    },
    {
      "epoch": 0.3134068485200232,
      "grad_norm": 1.5450658798217773,
      "learning_rate": 2.6881216481968008e-05,
      "loss": 1.1207,
      "step": 20250
    },
    {
      "epoch": 0.3135616173341072,
      "grad_norm": 1.3271665573120117,
      "learning_rate": 2.6879668006957815e-05,
      "loss": 1.2349,
      "step": 20260
    },
    {
      "epoch": 0.31371638614819114,
      "grad_norm": 1.1991311311721802,
      "learning_rate": 2.6878119531947622e-05,
      "loss": 1.0393,
      "step": 20270
    },
    {
      "epoch": 0.3138711549622751,
      "grad_norm": 1.5106580257415771,
      "learning_rate": 2.6876571056937426e-05,
      "loss": 1.1467,
      "step": 20280
    },
    {
      "epoch": 0.3140259237763591,
      "grad_norm": 1.2267074584960938,
      "learning_rate": 2.6875022581927233e-05,
      "loss": 1.1908,
      "step": 20290
    },
    {
      "epoch": 0.314180692590443,
      "grad_norm": 1.190402865409851,
      "learning_rate": 2.6873474106917037e-05,
      "loss": 1.0326,
      "step": 20300
    },
    {
      "epoch": 0.314335461404527,
      "grad_norm": 1.237321138381958,
      "learning_rate": 2.6871925631906844e-05,
      "loss": 1.0137,
      "step": 20310
    },
    {
      "epoch": 0.31449023021861094,
      "grad_norm": 1.3263604640960693,
      "learning_rate": 2.6870377156896648e-05,
      "loss": 0.8983,
      "step": 20320
    },
    {
      "epoch": 0.31464499903269494,
      "grad_norm": 1.340389370918274,
      "learning_rate": 2.686882868188646e-05,
      "loss": 0.9247,
      "step": 20330
    },
    {
      "epoch": 0.3147997678467789,
      "grad_norm": 2.170947313308716,
      "learning_rate": 2.6867280206876262e-05,
      "loss": 1.0608,
      "step": 20340
    },
    {
      "epoch": 0.3149545366608628,
      "grad_norm": 1.4968007802963257,
      "learning_rate": 2.686573173186607e-05,
      "loss": 0.9518,
      "step": 20350
    },
    {
      "epoch": 0.3151093054749468,
      "grad_norm": 1.6289480924606323,
      "learning_rate": 2.6864183256855873e-05,
      "loss": 1.0938,
      "step": 20360
    },
    {
      "epoch": 0.31526407428903075,
      "grad_norm": 1.6798542737960815,
      "learning_rate": 2.686263478184568e-05,
      "loss": 1.0699,
      "step": 20370
    },
    {
      "epoch": 0.31541884310311474,
      "grad_norm": 2.0505082607269287,
      "learning_rate": 2.6861086306835484e-05,
      "loss": 1.0284,
      "step": 20380
    },
    {
      "epoch": 0.3155736119171987,
      "grad_norm": 1.3822453022003174,
      "learning_rate": 2.685953783182529e-05,
      "loss": 1.1119,
      "step": 20390
    },
    {
      "epoch": 0.3157283807312826,
      "grad_norm": 2.060422658920288,
      "learning_rate": 2.6857989356815098e-05,
      "loss": 1.0131,
      "step": 20400
    },
    {
      "epoch": 0.3158831495453666,
      "grad_norm": 2.046945571899414,
      "learning_rate": 2.6856440881804905e-05,
      "loss": 1.145,
      "step": 20410
    },
    {
      "epoch": 0.31603791835945055,
      "grad_norm": 1.5674846172332764,
      "learning_rate": 2.685489240679471e-05,
      "loss": 1.0469,
      "step": 20420
    },
    {
      "epoch": 0.31619268717353455,
      "grad_norm": 1.9626824855804443,
      "learning_rate": 2.6853343931784516e-05,
      "loss": 1.081,
      "step": 20430
    },
    {
      "epoch": 0.3163474559876185,
      "grad_norm": 1.4420139789581299,
      "learning_rate": 2.685179545677432e-05,
      "loss": 1.1818,
      "step": 20440
    },
    {
      "epoch": 0.3165022248017025,
      "grad_norm": 1.352448582649231,
      "learning_rate": 2.6850246981764127e-05,
      "loss": 1.0233,
      "step": 20450
    },
    {
      "epoch": 0.3166569936157864,
      "grad_norm": 2.2041280269622803,
      "learning_rate": 2.684869850675393e-05,
      "loss": 1.1584,
      "step": 20460
    },
    {
      "epoch": 0.31681176242987036,
      "grad_norm": 1.5031003952026367,
      "learning_rate": 2.6847150031743738e-05,
      "loss": 1.081,
      "step": 20470
    },
    {
      "epoch": 0.31696653124395435,
      "grad_norm": 1.6614305973052979,
      "learning_rate": 2.6845601556733545e-05,
      "loss": 0.9884,
      "step": 20480
    },
    {
      "epoch": 0.3171213000580383,
      "grad_norm": 2.0947153568267822,
      "learning_rate": 2.684405308172335e-05,
      "loss": 1.11,
      "step": 20490
    },
    {
      "epoch": 0.3172760688721223,
      "grad_norm": 1.4690701961517334,
      "learning_rate": 2.6842504606713156e-05,
      "loss": 1.148,
      "step": 20500
    },
    {
      "epoch": 0.3174308376862062,
      "grad_norm": 1.542096495628357,
      "learning_rate": 2.684095613170296e-05,
      "loss": 1.0586,
      "step": 20510
    },
    {
      "epoch": 0.3175856065002902,
      "grad_norm": 1.469414234161377,
      "learning_rate": 2.6839407656692767e-05,
      "loss": 1.0766,
      "step": 20520
    },
    {
      "epoch": 0.31774037531437416,
      "grad_norm": 1.5987128019332886,
      "learning_rate": 2.6837859181682574e-05,
      "loss": 1.117,
      "step": 20530
    },
    {
      "epoch": 0.3178951441284581,
      "grad_norm": 2.000028610229492,
      "learning_rate": 2.683631070667238e-05,
      "loss": 1.107,
      "step": 20540
    },
    {
      "epoch": 0.3180499129425421,
      "grad_norm": 1.5821973085403442,
      "learning_rate": 2.6834762231662185e-05,
      "loss": 1.0254,
      "step": 20550
    },
    {
      "epoch": 0.31820468175662603,
      "grad_norm": 1.5493130683898926,
      "learning_rate": 2.6833213756651992e-05,
      "loss": 1.1938,
      "step": 20560
    },
    {
      "epoch": 0.31835945057071,
      "grad_norm": 1.1792722940444946,
      "learning_rate": 2.6831665281641796e-05,
      "loss": 0.9978,
      "step": 20570
    },
    {
      "epoch": 0.31851421938479396,
      "grad_norm": 2.152329206466675,
      "learning_rate": 2.6830116806631603e-05,
      "loss": 1.0142,
      "step": 20580
    },
    {
      "epoch": 0.3186689881988779,
      "grad_norm": 1.791221022605896,
      "learning_rate": 2.6828568331621407e-05,
      "loss": 0.9795,
      "step": 20590
    },
    {
      "epoch": 0.3188237570129619,
      "grad_norm": 1.366859793663025,
      "learning_rate": 2.6827019856611217e-05,
      "loss": 1.3055,
      "step": 20600
    },
    {
      "epoch": 0.31897852582704583,
      "grad_norm": 1.7618639469146729,
      "learning_rate": 2.682547138160102e-05,
      "loss": 1.1818,
      "step": 20610
    },
    {
      "epoch": 0.3191332946411298,
      "grad_norm": 2.157133102416992,
      "learning_rate": 2.6823922906590828e-05,
      "loss": 1.0538,
      "step": 20620
    },
    {
      "epoch": 0.31928806345521377,
      "grad_norm": 1.060758113861084,
      "learning_rate": 2.6822374431580632e-05,
      "loss": 1.0255,
      "step": 20630
    },
    {
      "epoch": 0.31944283226929776,
      "grad_norm": 2.05975604057312,
      "learning_rate": 2.682082595657044e-05,
      "loss": 1.2682,
      "step": 20640
    },
    {
      "epoch": 0.3195976010833817,
      "grad_norm": 1.5888198614120483,
      "learning_rate": 2.6819277481560243e-05,
      "loss": 1.1098,
      "step": 20650
    },
    {
      "epoch": 0.31975236989746564,
      "grad_norm": 1.4975179433822632,
      "learning_rate": 2.681772900655005e-05,
      "loss": 0.8365,
      "step": 20660
    },
    {
      "epoch": 0.31990713871154963,
      "grad_norm": 1.5314663648605347,
      "learning_rate": 2.6816180531539857e-05,
      "loss": 0.9346,
      "step": 20670
    },
    {
      "epoch": 0.32006190752563357,
      "grad_norm": 1.3268204927444458,
      "learning_rate": 2.6814632056529664e-05,
      "loss": 0.9238,
      "step": 20680
    },
    {
      "epoch": 0.32021667633971757,
      "grad_norm": 1.6084020137786865,
      "learning_rate": 2.6813083581519468e-05,
      "loss": 1.3204,
      "step": 20690
    },
    {
      "epoch": 0.3203714451538015,
      "grad_norm": 1.3305290937423706,
      "learning_rate": 2.6811535106509275e-05,
      "loss": 0.9773,
      "step": 20700
    },
    {
      "epoch": 0.3205262139678855,
      "grad_norm": 1.1539745330810547,
      "learning_rate": 2.680998663149908e-05,
      "loss": 1.1069,
      "step": 20710
    },
    {
      "epoch": 0.32068098278196944,
      "grad_norm": 1.175180196762085,
      "learning_rate": 2.6808438156488883e-05,
      "loss": 1.3018,
      "step": 20720
    },
    {
      "epoch": 0.3208357515960534,
      "grad_norm": 1.2902103662490845,
      "learning_rate": 2.680688968147869e-05,
      "loss": 1.0156,
      "step": 20730
    },
    {
      "epoch": 0.32099052041013737,
      "grad_norm": 1.4965969324111938,
      "learning_rate": 2.6805341206468497e-05,
      "loss": 1.1116,
      "step": 20740
    },
    {
      "epoch": 0.3211452892242213,
      "grad_norm": 1.9667868614196777,
      "learning_rate": 2.6803792731458304e-05,
      "loss": 0.9594,
      "step": 20750
    },
    {
      "epoch": 0.3213000580383053,
      "grad_norm": 1.1412404775619507,
      "learning_rate": 2.6802244256448108e-05,
      "loss": 1.1607,
      "step": 20760
    },
    {
      "epoch": 0.32145482685238924,
      "grad_norm": 1.779164433479309,
      "learning_rate": 2.6800695781437915e-05,
      "loss": 1.1566,
      "step": 20770
    },
    {
      "epoch": 0.3216095956664732,
      "grad_norm": 1.3029221296310425,
      "learning_rate": 2.679914730642772e-05,
      "loss": 1.0535,
      "step": 20780
    },
    {
      "epoch": 0.3217643644805572,
      "grad_norm": 1.423142671585083,
      "learning_rate": 2.6797598831417526e-05,
      "loss": 1.0667,
      "step": 20790
    },
    {
      "epoch": 0.3219191332946411,
      "grad_norm": 2.000701665878296,
      "learning_rate": 2.679605035640733e-05,
      "loss": 0.9919,
      "step": 20800
    },
    {
      "epoch": 0.3220739021087251,
      "grad_norm": 2.1496851444244385,
      "learning_rate": 2.679450188139714e-05,
      "loss": 1.0066,
      "step": 20810
    },
    {
      "epoch": 0.32222867092280905,
      "grad_norm": 1.620505452156067,
      "learning_rate": 2.6792953406386944e-05,
      "loss": 1.0787,
      "step": 20820
    },
    {
      "epoch": 0.32238343973689304,
      "grad_norm": 1.482580304145813,
      "learning_rate": 2.679140493137675e-05,
      "loss": 1.0681,
      "step": 20830
    },
    {
      "epoch": 0.322538208550977,
      "grad_norm": 2.206879138946533,
      "learning_rate": 2.6789856456366555e-05,
      "loss": 1.0077,
      "step": 20840
    },
    {
      "epoch": 0.3226929773650609,
      "grad_norm": 1.4262895584106445,
      "learning_rate": 2.6788307981356362e-05,
      "loss": 1.1134,
      "step": 20850
    },
    {
      "epoch": 0.3228477461791449,
      "grad_norm": 1.640216588973999,
      "learning_rate": 2.6786759506346166e-05,
      "loss": 1.1171,
      "step": 20860
    },
    {
      "epoch": 0.32300251499322885,
      "grad_norm": 1.845422625541687,
      "learning_rate": 2.6785211031335973e-05,
      "loss": 1.0605,
      "step": 20870
    },
    {
      "epoch": 0.32315728380731285,
      "grad_norm": 2.7456376552581787,
      "learning_rate": 2.678366255632578e-05,
      "loss": 1.0366,
      "step": 20880
    },
    {
      "epoch": 0.3233120526213968,
      "grad_norm": 1.4383063316345215,
      "learning_rate": 2.6782114081315587e-05,
      "loss": 1.2096,
      "step": 20890
    },
    {
      "epoch": 0.3234668214354808,
      "grad_norm": 1.398642897605896,
      "learning_rate": 2.678056560630539e-05,
      "loss": 1.053,
      "step": 20900
    },
    {
      "epoch": 0.3236215902495647,
      "grad_norm": 1.2573097944259644,
      "learning_rate": 2.6779017131295198e-05,
      "loss": 1.0665,
      "step": 20910
    },
    {
      "epoch": 0.32377635906364866,
      "grad_norm": 1.3996731042861938,
      "learning_rate": 2.6777468656285e-05,
      "loss": 1.1221,
      "step": 20920
    },
    {
      "epoch": 0.32393112787773265,
      "grad_norm": 1.7407705783843994,
      "learning_rate": 2.677592018127481e-05,
      "loss": 0.9636,
      "step": 20930
    },
    {
      "epoch": 0.3240858966918166,
      "grad_norm": 1.8339471817016602,
      "learning_rate": 2.6774371706264616e-05,
      "loss": 1.0712,
      "step": 20940
    },
    {
      "epoch": 0.3242406655059006,
      "grad_norm": 1.2527915239334106,
      "learning_rate": 2.6772823231254423e-05,
      "loss": 1.0638,
      "step": 20950
    },
    {
      "epoch": 0.3243954343199845,
      "grad_norm": 1.7750275135040283,
      "learning_rate": 2.6771274756244227e-05,
      "loss": 1.0264,
      "step": 20960
    },
    {
      "epoch": 0.32455020313406846,
      "grad_norm": 1.3182077407836914,
      "learning_rate": 2.676972628123403e-05,
      "loss": 1.1265,
      "step": 20970
    },
    {
      "epoch": 0.32470497194815245,
      "grad_norm": 1.6311845779418945,
      "learning_rate": 2.6768177806223838e-05,
      "loss": 1.2325,
      "step": 20980
    },
    {
      "epoch": 0.3248597407622364,
      "grad_norm": 1.5010101795196533,
      "learning_rate": 2.676662933121364e-05,
      "loss": 1.1851,
      "step": 20990
    },
    {
      "epoch": 0.3250145095763204,
      "grad_norm": 2.122375965118408,
      "learning_rate": 2.676508085620345e-05,
      "loss": 0.934,
      "step": 21000
    },
    {
      "epoch": 0.3251692783904043,
      "grad_norm": 1.7906475067138672,
      "learning_rate": 2.6763532381193256e-05,
      "loss": 1.1301,
      "step": 21010
    },
    {
      "epoch": 0.3253240472044883,
      "grad_norm": 1.922392725944519,
      "learning_rate": 2.676213875368408e-05,
      "loss": 0.9723,
      "step": 21020
    },
    {
      "epoch": 0.32547881601857226,
      "grad_norm": 1.2542022466659546,
      "learning_rate": 2.676059027867389e-05,
      "loss": 0.9238,
      "step": 21030
    },
    {
      "epoch": 0.3256335848326562,
      "grad_norm": 1.5237953662872314,
      "learning_rate": 2.6759041803663693e-05,
      "loss": 1.0436,
      "step": 21040
    },
    {
      "epoch": 0.3257883536467402,
      "grad_norm": 1.2368137836456299,
      "learning_rate": 2.67574933286535e-05,
      "loss": 0.9731,
      "step": 21050
    },
    {
      "epoch": 0.32594312246082413,
      "grad_norm": 1.7686042785644531,
      "learning_rate": 2.6755944853643304e-05,
      "loss": 1.1179,
      "step": 21060
    },
    {
      "epoch": 0.3260978912749081,
      "grad_norm": 1.9747531414031982,
      "learning_rate": 2.675439637863311e-05,
      "loss": 0.9474,
      "step": 21070
    },
    {
      "epoch": 0.32625266008899206,
      "grad_norm": 1.8868563175201416,
      "learning_rate": 2.6752847903622915e-05,
      "loss": 1.0794,
      "step": 21080
    },
    {
      "epoch": 0.32640742890307606,
      "grad_norm": 1.2217655181884766,
      "learning_rate": 2.675129942861272e-05,
      "loss": 1.1336,
      "step": 21090
    },
    {
      "epoch": 0.32656219771716,
      "grad_norm": 1.4152613878250122,
      "learning_rate": 2.674975095360253e-05,
      "loss": 0.9018,
      "step": 21100
    },
    {
      "epoch": 0.32671696653124394,
      "grad_norm": 1.8576964139938354,
      "learning_rate": 2.6748202478592336e-05,
      "loss": 0.9414,
      "step": 21110
    },
    {
      "epoch": 0.32687173534532793,
      "grad_norm": 1.4486050605773926,
      "learning_rate": 2.674665400358214e-05,
      "loss": 0.921,
      "step": 21120
    },
    {
      "epoch": 0.32702650415941187,
      "grad_norm": 1.3781518936157227,
      "learning_rate": 2.6745105528571947e-05,
      "loss": 1.1342,
      "step": 21130
    },
    {
      "epoch": 0.32718127297349586,
      "grad_norm": 1.3810592889785767,
      "learning_rate": 2.674355705356175e-05,
      "loss": 1.018,
      "step": 21140
    },
    {
      "epoch": 0.3273360417875798,
      "grad_norm": 1.5128926038742065,
      "learning_rate": 2.6742008578551554e-05,
      "loss": 1.2523,
      "step": 21150
    },
    {
      "epoch": 0.32749081060166374,
      "grad_norm": 1.6657103300094604,
      "learning_rate": 2.6740460103541365e-05,
      "loss": 1.0707,
      "step": 21160
    },
    {
      "epoch": 0.32764557941574773,
      "grad_norm": 1.3672168254852295,
      "learning_rate": 2.673891162853117e-05,
      "loss": 1.0667,
      "step": 21170
    },
    {
      "epoch": 0.3278003482298317,
      "grad_norm": 1.4857254028320312,
      "learning_rate": 2.6737363153520976e-05,
      "loss": 0.9897,
      "step": 21180
    },
    {
      "epoch": 0.32795511704391567,
      "grad_norm": 1.4566600322723389,
      "learning_rate": 2.673581467851078e-05,
      "loss": 0.9839,
      "step": 21190
    },
    {
      "epoch": 0.3281098858579996,
      "grad_norm": 1.5983668565750122,
      "learning_rate": 2.6734266203500587e-05,
      "loss": 1.1296,
      "step": 21200
    },
    {
      "epoch": 0.3282646546720836,
      "grad_norm": 1.7950150966644287,
      "learning_rate": 2.673271772849039e-05,
      "loss": 1.1283,
      "step": 21210
    },
    {
      "epoch": 0.32841942348616754,
      "grad_norm": 1.503251552581787,
      "learning_rate": 2.6731169253480198e-05,
      "loss": 0.9128,
      "step": 21220
    },
    {
      "epoch": 0.3285741923002515,
      "grad_norm": 1.0282009840011597,
      "learning_rate": 2.6729620778470005e-05,
      "loss": 1.0147,
      "step": 21230
    },
    {
      "epoch": 0.32872896111433547,
      "grad_norm": 1.8442937135696411,
      "learning_rate": 2.6728072303459812e-05,
      "loss": 1.2547,
      "step": 21240
    },
    {
      "epoch": 0.3288837299284194,
      "grad_norm": 1.770778775215149,
      "learning_rate": 2.6726523828449616e-05,
      "loss": 1.2177,
      "step": 21250
    },
    {
      "epoch": 0.3290384987425034,
      "grad_norm": 1.5041532516479492,
      "learning_rate": 2.6724975353439423e-05,
      "loss": 1.1528,
      "step": 21260
    },
    {
      "epoch": 0.32919326755658734,
      "grad_norm": 1.7211312055587769,
      "learning_rate": 2.6723426878429226e-05,
      "loss": 0.9564,
      "step": 21270
    },
    {
      "epoch": 0.3293480363706713,
      "grad_norm": 1.662358045578003,
      "learning_rate": 2.6721878403419034e-05,
      "loss": 1.1435,
      "step": 21280
    },
    {
      "epoch": 0.3295028051847553,
      "grad_norm": 1.1799808740615845,
      "learning_rate": 2.6720329928408837e-05,
      "loss": 1.0094,
      "step": 21290
    },
    {
      "epoch": 0.3296575739988392,
      "grad_norm": 1.4128671884536743,
      "learning_rate": 2.6718781453398648e-05,
      "loss": 1.1152,
      "step": 21300
    },
    {
      "epoch": 0.3298123428129232,
      "grad_norm": 1.548275113105774,
      "learning_rate": 2.671723297838845e-05,
      "loss": 1.1008,
      "step": 21310
    },
    {
      "epoch": 0.32996711162700715,
      "grad_norm": 1.3889801502227783,
      "learning_rate": 2.671568450337826e-05,
      "loss": 1.1467,
      "step": 21320
    },
    {
      "epoch": 0.33012188044109114,
      "grad_norm": 1.558901071548462,
      "learning_rate": 2.6714136028368062e-05,
      "loss": 1.0446,
      "step": 21330
    },
    {
      "epoch": 0.3302766492551751,
      "grad_norm": 1.6274737119674683,
      "learning_rate": 2.671258755335787e-05,
      "loss": 1.1614,
      "step": 21340
    },
    {
      "epoch": 0.330431418069259,
      "grad_norm": 1.6059094667434692,
      "learning_rate": 2.6711039078347673e-05,
      "loss": 0.9825,
      "step": 21350
    },
    {
      "epoch": 0.330586186883343,
      "grad_norm": 1.8364284038543701,
      "learning_rate": 2.670949060333748e-05,
      "loss": 1.0044,
      "step": 21360
    },
    {
      "epoch": 0.33074095569742695,
      "grad_norm": 1.6194159984588623,
      "learning_rate": 2.6707942128327288e-05,
      "loss": 1.0284,
      "step": 21370
    },
    {
      "epoch": 0.33089572451151095,
      "grad_norm": 1.44126296043396,
      "learning_rate": 2.6706393653317095e-05,
      "loss": 1.1986,
      "step": 21380
    },
    {
      "epoch": 0.3310504933255949,
      "grad_norm": 1.642797827720642,
      "learning_rate": 2.67048451783069e-05,
      "loss": 1.065,
      "step": 21390
    },
    {
      "epoch": 0.3312052621396789,
      "grad_norm": 1.001357913017273,
      "learning_rate": 2.6703296703296702e-05,
      "loss": 1.0835,
      "step": 21400
    },
    {
      "epoch": 0.3313600309537628,
      "grad_norm": 1.6059529781341553,
      "learning_rate": 2.670174822828651e-05,
      "loss": 0.9405,
      "step": 21410
    },
    {
      "epoch": 0.33151479976784676,
      "grad_norm": 1.2317843437194824,
      "learning_rate": 2.6700199753276313e-05,
      "loss": 1.1897,
      "step": 21420
    },
    {
      "epoch": 0.33166956858193075,
      "grad_norm": 1.7477320432662964,
      "learning_rate": 2.669865127826612e-05,
      "loss": 1.2532,
      "step": 21430
    },
    {
      "epoch": 0.3318243373960147,
      "grad_norm": 2.096799612045288,
      "learning_rate": 2.6697102803255927e-05,
      "loss": 0.9405,
      "step": 21440
    },
    {
      "epoch": 0.3319791062100987,
      "grad_norm": 1.4004740715026855,
      "learning_rate": 2.6695554328245735e-05,
      "loss": 1.1207,
      "step": 21450
    },
    {
      "epoch": 0.3321338750241826,
      "grad_norm": 1.3250670433044434,
      "learning_rate": 2.669400585323554e-05,
      "loss": 1.0627,
      "step": 21460
    },
    {
      "epoch": 0.33228864383826656,
      "grad_norm": 1.5138860940933228,
      "learning_rate": 2.6692457378225345e-05,
      "loss": 0.9976,
      "step": 21470
    },
    {
      "epoch": 0.33244341265235056,
      "grad_norm": 2.3870275020599365,
      "learning_rate": 2.669090890321515e-05,
      "loss": 1.1371,
      "step": 21480
    },
    {
      "epoch": 0.3325981814664345,
      "grad_norm": 1.5357050895690918,
      "learning_rate": 2.6689360428204956e-05,
      "loss": 1.1563,
      "step": 21490
    },
    {
      "epoch": 0.3327529502805185,
      "grad_norm": 1.253638505935669,
      "learning_rate": 2.668781195319476e-05,
      "loss": 1.1316,
      "step": 21500
    },
    {
      "epoch": 0.33290771909460243,
      "grad_norm": 1.5318642854690552,
      "learning_rate": 2.668626347818457e-05,
      "loss": 0.9524,
      "step": 21510
    },
    {
      "epoch": 0.3330624879086864,
      "grad_norm": 2.0344839096069336,
      "learning_rate": 2.6684715003174374e-05,
      "loss": 1.1015,
      "step": 21520
    },
    {
      "epoch": 0.33321725672277036,
      "grad_norm": 1.3479734659194946,
      "learning_rate": 2.668316652816418e-05,
      "loss": 1.0999,
      "step": 21530
    },
    {
      "epoch": 0.3333720255368543,
      "grad_norm": 1.3172612190246582,
      "learning_rate": 2.6681618053153985e-05,
      "loss": 1.1205,
      "step": 21540
    },
    {
      "epoch": 0.3335267943509383,
      "grad_norm": 1.1662044525146484,
      "learning_rate": 2.6680069578143792e-05,
      "loss": 1.0692,
      "step": 21550
    },
    {
      "epoch": 0.33368156316502223,
      "grad_norm": 1.302056074142456,
      "learning_rate": 2.6678521103133596e-05,
      "loss": 1.0048,
      "step": 21560
    },
    {
      "epoch": 0.3338363319791062,
      "grad_norm": 1.2152113914489746,
      "learning_rate": 2.6676972628123403e-05,
      "loss": 1.07,
      "step": 21570
    },
    {
      "epoch": 0.33399110079319017,
      "grad_norm": 1.7305710315704346,
      "learning_rate": 2.667542415311321e-05,
      "loss": 1.0882,
      "step": 21580
    },
    {
      "epoch": 0.33414586960727416,
      "grad_norm": 2.108398914337158,
      "learning_rate": 2.6673875678103018e-05,
      "loss": 1.1217,
      "step": 21590
    },
    {
      "epoch": 0.3343006384213581,
      "grad_norm": 1.586751937866211,
      "learning_rate": 2.667232720309282e-05,
      "loss": 1.2464,
      "step": 21600
    },
    {
      "epoch": 0.33445540723544204,
      "grad_norm": 2.5664238929748535,
      "learning_rate": 2.667077872808263e-05,
      "loss": 1.0457,
      "step": 21610
    },
    {
      "epoch": 0.33461017604952603,
      "grad_norm": 1.6173174381256104,
      "learning_rate": 2.6669230253072432e-05,
      "loss": 1.0931,
      "step": 21620
    },
    {
      "epoch": 0.33476494486360997,
      "grad_norm": 1.6938515901565552,
      "learning_rate": 2.666768177806224e-05,
      "loss": 1.0221,
      "step": 21630
    },
    {
      "epoch": 0.33491971367769396,
      "grad_norm": 1.261258602142334,
      "learning_rate": 2.6666133303052047e-05,
      "loss": 1.0388,
      "step": 21640
    },
    {
      "epoch": 0.3350744824917779,
      "grad_norm": 1.8985235691070557,
      "learning_rate": 2.666458482804185e-05,
      "loss": 1.114,
      "step": 21650
    },
    {
      "epoch": 0.33522925130586184,
      "grad_norm": 1.6232082843780518,
      "learning_rate": 2.6663036353031657e-05,
      "loss": 1.0959,
      "step": 21660
    },
    {
      "epoch": 0.33538402011994584,
      "grad_norm": 1.530375361442566,
      "learning_rate": 2.666148787802146e-05,
      "loss": 0.9698,
      "step": 21670
    },
    {
      "epoch": 0.3355387889340298,
      "grad_norm": 1.7507715225219727,
      "learning_rate": 2.6659939403011268e-05,
      "loss": 1.1023,
      "step": 21680
    },
    {
      "epoch": 0.33569355774811377,
      "grad_norm": 1.4804964065551758,
      "learning_rate": 2.6658390928001072e-05,
      "loss": 1.1672,
      "step": 21690
    },
    {
      "epoch": 0.3358483265621977,
      "grad_norm": 1.8566850423812866,
      "learning_rate": 2.665684245299088e-05,
      "loss": 1.1377,
      "step": 21700
    },
    {
      "epoch": 0.3360030953762817,
      "grad_norm": 1.5965602397918701,
      "learning_rate": 2.6655293977980686e-05,
      "loss": 1.137,
      "step": 21710
    },
    {
      "epoch": 0.33615786419036564,
      "grad_norm": 1.3409518003463745,
      "learning_rate": 2.6653745502970493e-05,
      "loss": 1.0624,
      "step": 21720
    },
    {
      "epoch": 0.3363126330044496,
      "grad_norm": 1.4905622005462646,
      "learning_rate": 2.6652197027960297e-05,
      "loss": 0.9766,
      "step": 21730
    },
    {
      "epoch": 0.3364674018185336,
      "grad_norm": 1.6818311214447021,
      "learning_rate": 2.6650648552950104e-05,
      "loss": 1.1088,
      "step": 21740
    },
    {
      "epoch": 0.3366221706326175,
      "grad_norm": 2.015794038772583,
      "learning_rate": 2.6649100077939908e-05,
      "loss": 1.1816,
      "step": 21750
    },
    {
      "epoch": 0.3367769394467015,
      "grad_norm": 2.069941520690918,
      "learning_rate": 2.6647551602929715e-05,
      "loss": 1.0485,
      "step": 21760
    },
    {
      "epoch": 0.33693170826078545,
      "grad_norm": 1.424066185951233,
      "learning_rate": 2.664600312791952e-05,
      "loss": 1.0784,
      "step": 21770
    },
    {
      "epoch": 0.33708647707486944,
      "grad_norm": 1.1670809984207153,
      "learning_rate": 2.664445465290933e-05,
      "loss": 1.2396,
      "step": 21780
    },
    {
      "epoch": 0.3372412458889534,
      "grad_norm": 1.7774966955184937,
      "learning_rate": 2.6642906177899133e-05,
      "loss": 1.0853,
      "step": 21790
    },
    {
      "epoch": 0.3373960147030373,
      "grad_norm": 1.5507640838623047,
      "learning_rate": 2.664135770288894e-05,
      "loss": 1.0984,
      "step": 21800
    },
    {
      "epoch": 0.3375507835171213,
      "grad_norm": 1.6648077964782715,
      "learning_rate": 2.6639809227878744e-05,
      "loss": 0.9729,
      "step": 21810
    },
    {
      "epoch": 0.33770555233120525,
      "grad_norm": 1.951616644859314,
      "learning_rate": 2.663826075286855e-05,
      "loss": 0.9927,
      "step": 21820
    },
    {
      "epoch": 0.33786032114528924,
      "grad_norm": 2.1860501766204834,
      "learning_rate": 2.6636712277858355e-05,
      "loss": 1.1339,
      "step": 21830
    },
    {
      "epoch": 0.3380150899593732,
      "grad_norm": 0.9535675644874573,
      "learning_rate": 2.6635163802848162e-05,
      "loss": 1.0243,
      "step": 21840
    },
    {
      "epoch": 0.3381698587734571,
      "grad_norm": 1.3163546323776245,
      "learning_rate": 2.663361532783797e-05,
      "loss": 1.1351,
      "step": 21850
    },
    {
      "epoch": 0.3383246275875411,
      "grad_norm": 1.5322779417037964,
      "learning_rate": 2.6632066852827776e-05,
      "loss": 1.0363,
      "step": 21860
    },
    {
      "epoch": 0.33847939640162505,
      "grad_norm": 1.8030527830123901,
      "learning_rate": 2.663051837781758e-05,
      "loss": 1.16,
      "step": 21870
    },
    {
      "epoch": 0.33863416521570905,
      "grad_norm": 2.0990664958953857,
      "learning_rate": 2.6628969902807387e-05,
      "loss": 1.013,
      "step": 21880
    },
    {
      "epoch": 0.338788934029793,
      "grad_norm": 1.6314811706542969,
      "learning_rate": 2.662742142779719e-05,
      "loss": 1.156,
      "step": 21890
    },
    {
      "epoch": 0.338943702843877,
      "grad_norm": 1.3235982656478882,
      "learning_rate": 2.6625872952786995e-05,
      "loss": 1.101,
      "step": 21900
    },
    {
      "epoch": 0.3390984716579609,
      "grad_norm": 1.6864644289016724,
      "learning_rate": 2.6624324477776802e-05,
      "loss": 0.9636,
      "step": 21910
    },
    {
      "epoch": 0.33925324047204486,
      "grad_norm": 1.1934523582458496,
      "learning_rate": 2.662277600276661e-05,
      "loss": 1.1311,
      "step": 21920
    },
    {
      "epoch": 0.33940800928612885,
      "grad_norm": 2.0168678760528564,
      "learning_rate": 2.6621227527756416e-05,
      "loss": 0.946,
      "step": 21930
    },
    {
      "epoch": 0.3395627781002128,
      "grad_norm": 2.264779567718506,
      "learning_rate": 2.661967905274622e-05,
      "loss": 1.063,
      "step": 21940
    },
    {
      "epoch": 0.3397175469142968,
      "grad_norm": 1.5770916938781738,
      "learning_rate": 2.6618130577736027e-05,
      "loss": 1.3003,
      "step": 21950
    },
    {
      "epoch": 0.3398723157283807,
      "grad_norm": 1.448268175125122,
      "learning_rate": 2.661658210272583e-05,
      "loss": 1.0378,
      "step": 21960
    },
    {
      "epoch": 0.3400270845424647,
      "grad_norm": 1.3217917680740356,
      "learning_rate": 2.6615033627715638e-05,
      "loss": 1.2462,
      "step": 21970
    },
    {
      "epoch": 0.34018185335654866,
      "grad_norm": 1.1205519437789917,
      "learning_rate": 2.6613485152705442e-05,
      "loss": 1.117,
      "step": 21980
    },
    {
      "epoch": 0.3403366221706326,
      "grad_norm": 1.6627695560455322,
      "learning_rate": 2.6611936677695252e-05,
      "loss": 1.1776,
      "step": 21990
    },
    {
      "epoch": 0.3404913909847166,
      "grad_norm": 1.2486861944198608,
      "learning_rate": 2.6610388202685056e-05,
      "loss": 1.0812,
      "step": 22000
    },
    {
      "epoch": 0.34064615979880053,
      "grad_norm": 1.5096815824508667,
      "learning_rate": 2.6608839727674863e-05,
      "loss": 1.0619,
      "step": 22010
    },
    {
      "epoch": 0.3408009286128845,
      "grad_norm": 1.6538634300231934,
      "learning_rate": 2.6607291252664667e-05,
      "loss": 1.1176,
      "step": 22020
    },
    {
      "epoch": 0.34095569742696846,
      "grad_norm": 1.4896726608276367,
      "learning_rate": 2.6605742777654474e-05,
      "loss": 1.2093,
      "step": 22030
    },
    {
      "epoch": 0.3411104662410524,
      "grad_norm": 1.7092156410217285,
      "learning_rate": 2.6604194302644278e-05,
      "loss": 1.1502,
      "step": 22040
    },
    {
      "epoch": 0.3412652350551364,
      "grad_norm": 1.3349655866622925,
      "learning_rate": 2.660264582763409e-05,
      "loss": 1.0432,
      "step": 22050
    },
    {
      "epoch": 0.34142000386922033,
      "grad_norm": 1.6547718048095703,
      "learning_rate": 2.6601097352623892e-05,
      "loss": 1.1712,
      "step": 22060
    },
    {
      "epoch": 0.34157477268330433,
      "grad_norm": 1.223284125328064,
      "learning_rate": 2.65995488776137e-05,
      "loss": 1.0685,
      "step": 22070
    },
    {
      "epoch": 0.34172954149738827,
      "grad_norm": 1.4417870044708252,
      "learning_rate": 2.6598000402603503e-05,
      "loss": 1.1177,
      "step": 22080
    },
    {
      "epoch": 0.34188431031147226,
      "grad_norm": 1.3646191358566284,
      "learning_rate": 2.659645192759331e-05,
      "loss": 1.0161,
      "step": 22090
    },
    {
      "epoch": 0.3420390791255562,
      "grad_norm": 1.6508901119232178,
      "learning_rate": 2.6594903452583114e-05,
      "loss": 1.2571,
      "step": 22100
    },
    {
      "epoch": 0.34219384793964014,
      "grad_norm": 1.3354586362838745,
      "learning_rate": 2.659335497757292e-05,
      "loss": 1.0921,
      "step": 22110
    },
    {
      "epoch": 0.34234861675372413,
      "grad_norm": 2.144479274749756,
      "learning_rate": 2.6591806502562728e-05,
      "loss": 1.0717,
      "step": 22120
    },
    {
      "epoch": 0.34250338556780807,
      "grad_norm": 1.861055612564087,
      "learning_rate": 2.6590258027552535e-05,
      "loss": 1.1608,
      "step": 22130
    },
    {
      "epoch": 0.34265815438189207,
      "grad_norm": 2.2970571517944336,
      "learning_rate": 2.658870955254234e-05,
      "loss": 0.9134,
      "step": 22140
    },
    {
      "epoch": 0.342812923195976,
      "grad_norm": 1.3718445301055908,
      "learning_rate": 2.6587161077532146e-05,
      "loss": 1.0084,
      "step": 22150
    },
    {
      "epoch": 0.34296769201006,
      "grad_norm": 1.5410304069519043,
      "learning_rate": 2.658561260252195e-05,
      "loss": 1.0114,
      "step": 22160
    },
    {
      "epoch": 0.34312246082414394,
      "grad_norm": 1.3248506784439087,
      "learning_rate": 2.6584064127511754e-05,
      "loss": 0.9528,
      "step": 22170
    },
    {
      "epoch": 0.3432772296382279,
      "grad_norm": 1.901465892791748,
      "learning_rate": 2.658251565250156e-05,
      "loss": 1.1056,
      "step": 22180
    },
    {
      "epoch": 0.34343199845231187,
      "grad_norm": 1.630481481552124,
      "learning_rate": 2.6580967177491368e-05,
      "loss": 1.09,
      "step": 22190
    },
    {
      "epoch": 0.3435867672663958,
      "grad_norm": 2.1508960723876953,
      "learning_rate": 2.6579418702481175e-05,
      "loss": 1.0767,
      "step": 22200
    },
    {
      "epoch": 0.3437415360804798,
      "grad_norm": 1.3010540008544922,
      "learning_rate": 2.657787022747098e-05,
      "loss": 0.9762,
      "step": 22210
    },
    {
      "epoch": 0.34389630489456374,
      "grad_norm": 1.1559644937515259,
      "learning_rate": 2.6576321752460786e-05,
      "loss": 0.9518,
      "step": 22220
    },
    {
      "epoch": 0.3440510737086477,
      "grad_norm": 1.6116433143615723,
      "learning_rate": 2.657477327745059e-05,
      "loss": 1.2385,
      "step": 22230
    },
    {
      "epoch": 0.3442058425227317,
      "grad_norm": 1.5702364444732666,
      "learning_rate": 2.6573224802440397e-05,
      "loss": 1.226,
      "step": 22240
    },
    {
      "epoch": 0.3443606113368156,
      "grad_norm": 1.5447174310684204,
      "learning_rate": 2.65716763274302e-05,
      "loss": 1.0008,
      "step": 22250
    },
    {
      "epoch": 0.3445153801508996,
      "grad_norm": 0.9693048596382141,
      "learning_rate": 2.657012785242001e-05,
      "loss": 1.0584,
      "step": 22260
    },
    {
      "epoch": 0.34467014896498355,
      "grad_norm": 1.1778892278671265,
      "learning_rate": 2.6568579377409815e-05,
      "loss": 1.1419,
      "step": 22270
    },
    {
      "epoch": 0.34482491777906754,
      "grad_norm": 1.74542236328125,
      "learning_rate": 2.6567030902399622e-05,
      "loss": 1.1299,
      "step": 22280
    },
    {
      "epoch": 0.3449796865931515,
      "grad_norm": 1.1723414659500122,
      "learning_rate": 2.6565482427389426e-05,
      "loss": 1.0746,
      "step": 22290
    },
    {
      "epoch": 0.3451344554072354,
      "grad_norm": 1.313934087753296,
      "learning_rate": 2.6563933952379233e-05,
      "loss": 0.9569,
      "step": 22300
    },
    {
      "epoch": 0.3452892242213194,
      "grad_norm": 1.9614436626434326,
      "learning_rate": 2.6562385477369037e-05,
      "loss": 1.0148,
      "step": 22310
    },
    {
      "epoch": 0.34544399303540335,
      "grad_norm": 1.4102352857589722,
      "learning_rate": 2.6560837002358844e-05,
      "loss": 0.8784,
      "step": 22320
    },
    {
      "epoch": 0.34559876184948735,
      "grad_norm": 1.4246160984039307,
      "learning_rate": 2.655928852734865e-05,
      "loss": 1.1302,
      "step": 22330
    },
    {
      "epoch": 0.3457535306635713,
      "grad_norm": 1.4420822858810425,
      "learning_rate": 2.6557740052338458e-05,
      "loss": 1.1815,
      "step": 22340
    },
    {
      "epoch": 0.3459082994776553,
      "grad_norm": 2.2866456508636475,
      "learning_rate": 2.6556191577328262e-05,
      "loss": 1.0944,
      "step": 22350
    },
    {
      "epoch": 0.3460630682917392,
      "grad_norm": 1.586394190788269,
      "learning_rate": 2.655464310231807e-05,
      "loss": 1.0213,
      "step": 22360
    },
    {
      "epoch": 0.34621783710582316,
      "grad_norm": 1.6035511493682861,
      "learning_rate": 2.6553094627307873e-05,
      "loss": 1.1083,
      "step": 22370
    },
    {
      "epoch": 0.34637260591990715,
      "grad_norm": 1.9909429550170898,
      "learning_rate": 2.655154615229768e-05,
      "loss": 0.9618,
      "step": 22380
    },
    {
      "epoch": 0.3465273747339911,
      "grad_norm": 1.1923973560333252,
      "learning_rate": 2.6549997677287484e-05,
      "loss": 0.9527,
      "step": 22390
    },
    {
      "epoch": 0.3466821435480751,
      "grad_norm": 2.1466357707977295,
      "learning_rate": 2.6548449202277294e-05,
      "loss": 1.1511,
      "step": 22400
    },
    {
      "epoch": 0.346836912362159,
      "grad_norm": 1.4004660844802856,
      "learning_rate": 2.6546900727267098e-05,
      "loss": 1.1212,
      "step": 22410
    },
    {
      "epoch": 0.34699168117624296,
      "grad_norm": 1.1933794021606445,
      "learning_rate": 2.6545352252256902e-05,
      "loss": 1.0234,
      "step": 22420
    },
    {
      "epoch": 0.34714644999032696,
      "grad_norm": 1.1986769437789917,
      "learning_rate": 2.654380377724671e-05,
      "loss": 1.1567,
      "step": 22430
    },
    {
      "epoch": 0.3473012188044109,
      "grad_norm": 1.3997316360473633,
      "learning_rate": 2.6542255302236513e-05,
      "loss": 0.9353,
      "step": 22440
    },
    {
      "epoch": 0.3474559876184949,
      "grad_norm": 1.64432692527771,
      "learning_rate": 2.654070682722632e-05,
      "loss": 1.0963,
      "step": 22450
    },
    {
      "epoch": 0.3476107564325788,
      "grad_norm": 1.9180151224136353,
      "learning_rate": 2.6539158352216127e-05,
      "loss": 0.9742,
      "step": 22460
    },
    {
      "epoch": 0.3477655252466628,
      "grad_norm": 1.9074771404266357,
      "learning_rate": 2.6537609877205934e-05,
      "loss": 1.0099,
      "step": 22470
    },
    {
      "epoch": 0.34792029406074676,
      "grad_norm": 1.8246948719024658,
      "learning_rate": 2.6536061402195738e-05,
      "loss": 1.046,
      "step": 22480
    },
    {
      "epoch": 0.3480750628748307,
      "grad_norm": 1.6520105600357056,
      "learning_rate": 2.6534512927185545e-05,
      "loss": 0.8977,
      "step": 22490
    },
    {
      "epoch": 0.3482298316889147,
      "grad_norm": 1.4531619548797607,
      "learning_rate": 2.653296445217535e-05,
      "loss": 1.1283,
      "step": 22500
    },
    {
      "epoch": 0.34838460050299863,
      "grad_norm": 1.246857762336731,
      "learning_rate": 2.6531415977165156e-05,
      "loss": 1.2125,
      "step": 22510
    },
    {
      "epoch": 0.3485393693170826,
      "grad_norm": 1.6428102254867554,
      "learning_rate": 2.652986750215496e-05,
      "loss": 1.2207,
      "step": 22520
    },
    {
      "epoch": 0.34869413813116656,
      "grad_norm": 2.40435528755188,
      "learning_rate": 2.652831902714477e-05,
      "loss": 1.0595,
      "step": 22530
    },
    {
      "epoch": 0.34884890694525056,
      "grad_norm": 1.8982348442077637,
      "learning_rate": 2.6526770552134574e-05,
      "loss": 0.905,
      "step": 22540
    },
    {
      "epoch": 0.3490036757593345,
      "grad_norm": 1.4957630634307861,
      "learning_rate": 2.652522207712438e-05,
      "loss": 0.9632,
      "step": 22550
    },
    {
      "epoch": 0.34915844457341844,
      "grad_norm": 1.477603554725647,
      "learning_rate": 2.6523673602114185e-05,
      "loss": 0.991,
      "step": 22560
    },
    {
      "epoch": 0.34931321338750243,
      "grad_norm": 1.9306576251983643,
      "learning_rate": 2.6522125127103992e-05,
      "loss": 1.0813,
      "step": 22570
    },
    {
      "epoch": 0.34946798220158637,
      "grad_norm": 1.6149917840957642,
      "learning_rate": 2.6520576652093796e-05,
      "loss": 1.1476,
      "step": 22580
    },
    {
      "epoch": 0.34962275101567036,
      "grad_norm": 1.8514131307601929,
      "learning_rate": 2.6519028177083603e-05,
      "loss": 1.0788,
      "step": 22590
    },
    {
      "epoch": 0.3497775198297543,
      "grad_norm": 1.717805027961731,
      "learning_rate": 2.651747970207341e-05,
      "loss": 0.9611,
      "step": 22600
    },
    {
      "epoch": 0.34993228864383824,
      "grad_norm": 1.5929697751998901,
      "learning_rate": 2.6515931227063217e-05,
      "loss": 1.0022,
      "step": 22610
    },
    {
      "epoch": 0.35008705745792223,
      "grad_norm": 1.6117360591888428,
      "learning_rate": 2.651438275205302e-05,
      "loss": 1.0585,
      "step": 22620
    },
    {
      "epoch": 0.3502418262720062,
      "grad_norm": 1.483189344406128,
      "learning_rate": 2.6512834277042828e-05,
      "loss": 1.292,
      "step": 22630
    },
    {
      "epoch": 0.35039659508609017,
      "grad_norm": 1.433032512664795,
      "learning_rate": 2.651128580203263e-05,
      "loss": 1.0414,
      "step": 22640
    },
    {
      "epoch": 0.3505513639001741,
      "grad_norm": 1.649546504020691,
      "learning_rate": 2.650973732702244e-05,
      "loss": 1.1694,
      "step": 22650
    },
    {
      "epoch": 0.3507061327142581,
      "grad_norm": 1.959943175315857,
      "learning_rate": 2.6508188852012243e-05,
      "loss": 0.972,
      "step": 22660
    },
    {
      "epoch": 0.35086090152834204,
      "grad_norm": 2.3571159839630127,
      "learning_rate": 2.650664037700205e-05,
      "loss": 1.1596,
      "step": 22670
    },
    {
      "epoch": 0.351015670342426,
      "grad_norm": 1.9138081073760986,
      "learning_rate": 2.6505091901991857e-05,
      "loss": 1.0003,
      "step": 22680
    },
    {
      "epoch": 0.35117043915651,
      "grad_norm": 1.2838629484176636,
      "learning_rate": 2.650354342698166e-05,
      "loss": 1.0966,
      "step": 22690
    },
    {
      "epoch": 0.3513252079705939,
      "grad_norm": 1.6782079935073853,
      "learning_rate": 2.6501994951971468e-05,
      "loss": 1.0771,
      "step": 22700
    },
    {
      "epoch": 0.3514799767846779,
      "grad_norm": 1.572209358215332,
      "learning_rate": 2.650044647696127e-05,
      "loss": 1.2459,
      "step": 22710
    },
    {
      "epoch": 0.35163474559876184,
      "grad_norm": 1.6754125356674194,
      "learning_rate": 2.649889800195108e-05,
      "loss": 1.0133,
      "step": 22720
    },
    {
      "epoch": 0.35178951441284584,
      "grad_norm": 1.5786411762237549,
      "learning_rate": 2.6497349526940882e-05,
      "loss": 1.1459,
      "step": 22730
    },
    {
      "epoch": 0.3519442832269298,
      "grad_norm": 1.9504295587539673,
      "learning_rate": 2.6495801051930693e-05,
      "loss": 1.2012,
      "step": 22740
    },
    {
      "epoch": 0.3520990520410137,
      "grad_norm": 1.516215443611145,
      "learning_rate": 2.6494252576920497e-05,
      "loss": 1.0922,
      "step": 22750
    },
    {
      "epoch": 0.3522538208550977,
      "grad_norm": 1.9986722469329834,
      "learning_rate": 2.6492704101910304e-05,
      "loss": 1.298,
      "step": 22760
    },
    {
      "epoch": 0.35240858966918165,
      "grad_norm": 1.5913608074188232,
      "learning_rate": 2.6491155626900108e-05,
      "loss": 1.1343,
      "step": 22770
    },
    {
      "epoch": 0.35256335848326564,
      "grad_norm": 1.3766926527023315,
      "learning_rate": 2.6489607151889915e-05,
      "loss": 0.8459,
      "step": 22780
    },
    {
      "epoch": 0.3527181272973496,
      "grad_norm": 1.7418365478515625,
      "learning_rate": 2.648805867687972e-05,
      "loss": 1.1229,
      "step": 22790
    },
    {
      "epoch": 0.3528728961114335,
      "grad_norm": 1.633196234703064,
      "learning_rate": 2.6486510201869526e-05,
      "loss": 1.2256,
      "step": 22800
    },
    {
      "epoch": 0.3530276649255175,
      "grad_norm": 2.1408634185791016,
      "learning_rate": 2.6484961726859333e-05,
      "loss": 1.0908,
      "step": 22810
    },
    {
      "epoch": 0.35318243373960145,
      "grad_norm": 1.1596356630325317,
      "learning_rate": 2.648341325184914e-05,
      "loss": 1.0829,
      "step": 22820
    },
    {
      "epoch": 0.35333720255368545,
      "grad_norm": 1.571063756942749,
      "learning_rate": 2.6481864776838944e-05,
      "loss": 1.0244,
      "step": 22830
    },
    {
      "epoch": 0.3534919713677694,
      "grad_norm": 1.3540774583816528,
      "learning_rate": 2.648031630182875e-05,
      "loss": 1.1513,
      "step": 22840
    },
    {
      "epoch": 0.3536467401818534,
      "grad_norm": 1.9352473020553589,
      "learning_rate": 2.6478767826818554e-05,
      "loss": 0.9366,
      "step": 22850
    },
    {
      "epoch": 0.3538015089959373,
      "grad_norm": 1.9059748649597168,
      "learning_rate": 2.647721935180836e-05,
      "loss": 1.0092,
      "step": 22860
    },
    {
      "epoch": 0.35395627781002126,
      "grad_norm": 1.356061339378357,
      "learning_rate": 2.6475670876798165e-05,
      "loss": 1.1402,
      "step": 22870
    },
    {
      "epoch": 0.35411104662410525,
      "grad_norm": 1.5708210468292236,
      "learning_rate": 2.6474122401787976e-05,
      "loss": 0.9506,
      "step": 22880
    },
    {
      "epoch": 0.3542658154381892,
      "grad_norm": 1.3009514808654785,
      "learning_rate": 2.647257392677778e-05,
      "loss": 1.0023,
      "step": 22890
    },
    {
      "epoch": 0.3544205842522732,
      "grad_norm": 1.5768195390701294,
      "learning_rate": 2.6471025451767587e-05,
      "loss": 1.1277,
      "step": 22900
    },
    {
      "epoch": 0.3545753530663571,
      "grad_norm": 1.5493183135986328,
      "learning_rate": 2.646947697675739e-05,
      "loss": 1.0503,
      "step": 22910
    },
    {
      "epoch": 0.3547301218804411,
      "grad_norm": 1.5130099058151245,
      "learning_rate": 2.6467928501747194e-05,
      "loss": 1.0225,
      "step": 22920
    },
    {
      "epoch": 0.35488489069452506,
      "grad_norm": 1.53447425365448,
      "learning_rate": 2.6466380026737e-05,
      "loss": 1.1302,
      "step": 22930
    },
    {
      "epoch": 0.355039659508609,
      "grad_norm": 1.7725191116333008,
      "learning_rate": 2.646483155172681e-05,
      "loss": 0.941,
      "step": 22940
    },
    {
      "epoch": 0.355194428322693,
      "grad_norm": 1.5751463174819946,
      "learning_rate": 2.6463283076716616e-05,
      "loss": 1.0619,
      "step": 22950
    },
    {
      "epoch": 0.35534919713677693,
      "grad_norm": 1.3496832847595215,
      "learning_rate": 2.646173460170642e-05,
      "loss": 1.0579,
      "step": 22960
    },
    {
      "epoch": 0.3555039659508609,
      "grad_norm": 1.2314589023590088,
      "learning_rate": 2.6460186126696227e-05,
      "loss": 1.0308,
      "step": 22970
    },
    {
      "epoch": 0.35565873476494486,
      "grad_norm": 1.2248835563659668,
      "learning_rate": 2.645863765168603e-05,
      "loss": 1.1035,
      "step": 22980
    },
    {
      "epoch": 0.3558135035790288,
      "grad_norm": 1.6734046936035156,
      "learning_rate": 2.6457089176675837e-05,
      "loss": 1.1152,
      "step": 22990
    },
    {
      "epoch": 0.3559682723931128,
      "grad_norm": 1.5467935800552368,
      "learning_rate": 2.645554070166564e-05,
      "loss": 1.1494,
      "step": 23000
    },
    {
      "epoch": 0.35612304120719673,
      "grad_norm": 1.7529948949813843,
      "learning_rate": 2.6453992226655452e-05,
      "loss": 1.018,
      "step": 23010
    },
    {
      "epoch": 0.3562778100212807,
      "grad_norm": 2.91711688041687,
      "learning_rate": 2.6452598599146274e-05,
      "loss": 1.1685,
      "step": 23020
    },
    {
      "epoch": 0.35643257883536467,
      "grad_norm": 1.7074556350708008,
      "learning_rate": 2.645105012413608e-05,
      "loss": 0.9966,
      "step": 23030
    },
    {
      "epoch": 0.35658734764944866,
      "grad_norm": 1.78962242603302,
      "learning_rate": 2.644950164912589e-05,
      "loss": 1.0206,
      "step": 23040
    },
    {
      "epoch": 0.3567421164635326,
      "grad_norm": 1.1579644680023193,
      "learning_rate": 2.6447953174115692e-05,
      "loss": 1.0534,
      "step": 23050
    },
    {
      "epoch": 0.35689688527761654,
      "grad_norm": 1.2716728448867798,
      "learning_rate": 2.64464046991055e-05,
      "loss": 1.1474,
      "step": 23060
    },
    {
      "epoch": 0.35705165409170053,
      "grad_norm": 1.5225629806518555,
      "learning_rate": 2.6444856224095303e-05,
      "loss": 1.0937,
      "step": 23070
    },
    {
      "epoch": 0.35720642290578447,
      "grad_norm": 1.4463145732879639,
      "learning_rate": 2.644330774908511e-05,
      "loss": 1.0086,
      "step": 23080
    },
    {
      "epoch": 0.35736119171986847,
      "grad_norm": 1.5745656490325928,
      "learning_rate": 2.6441759274074914e-05,
      "loss": 1.1195,
      "step": 23090
    },
    {
      "epoch": 0.3575159605339524,
      "grad_norm": 1.328554391860962,
      "learning_rate": 2.644021079906472e-05,
      "loss": 1.0246,
      "step": 23100
    },
    {
      "epoch": 0.3576707293480364,
      "grad_norm": 1.8464807271957397,
      "learning_rate": 2.643866232405453e-05,
      "loss": 0.9534,
      "step": 23110
    },
    {
      "epoch": 0.35782549816212034,
      "grad_norm": 1.9616764783859253,
      "learning_rate": 2.6437113849044332e-05,
      "loss": 1.0182,
      "step": 23120
    },
    {
      "epoch": 0.3579802669762043,
      "grad_norm": 1.4212417602539062,
      "learning_rate": 2.643556537403414e-05,
      "loss": 1.1246,
      "step": 23130
    },
    {
      "epoch": 0.35813503579028827,
      "grad_norm": 1.5260872840881348,
      "learning_rate": 2.6434016899023943e-05,
      "loss": 1.1624,
      "step": 23140
    },
    {
      "epoch": 0.3582898046043722,
      "grad_norm": 1.262178659439087,
      "learning_rate": 2.643246842401375e-05,
      "loss": 1.1422,
      "step": 23150
    },
    {
      "epoch": 0.3584445734184562,
      "grad_norm": 1.6054108142852783,
      "learning_rate": 2.6430919949003557e-05,
      "loss": 1.0756,
      "step": 23160
    },
    {
      "epoch": 0.35859934223254014,
      "grad_norm": 1.4871885776519775,
      "learning_rate": 2.6429371473993365e-05,
      "loss": 1.0015,
      "step": 23170
    },
    {
      "epoch": 0.3587541110466241,
      "grad_norm": 1.6624383926391602,
      "learning_rate": 2.642782299898317e-05,
      "loss": 0.968,
      "step": 23180
    },
    {
      "epoch": 0.3589088798607081,
      "grad_norm": 1.6020249128341675,
      "learning_rate": 2.6426274523972975e-05,
      "loss": 1.1144,
      "step": 23190
    },
    {
      "epoch": 0.359063648674792,
      "grad_norm": 1.455424427986145,
      "learning_rate": 2.642472604896278e-05,
      "loss": 1.3375,
      "step": 23200
    },
    {
      "epoch": 0.359218417488876,
      "grad_norm": 1.3383506536483765,
      "learning_rate": 2.6423177573952586e-05,
      "loss": 1.1119,
      "step": 23210
    },
    {
      "epoch": 0.35937318630295995,
      "grad_norm": 2.089343309402466,
      "learning_rate": 2.642162909894239e-05,
      "loss": 1.0638,
      "step": 23220
    },
    {
      "epoch": 0.35952795511704394,
      "grad_norm": 1.1780613660812378,
      "learning_rate": 2.64200806239322e-05,
      "loss": 0.9405,
      "step": 23230
    },
    {
      "epoch": 0.3596827239311279,
      "grad_norm": 1.6840420961380005,
      "learning_rate": 2.6418532148922004e-05,
      "loss": 1.0912,
      "step": 23240
    },
    {
      "epoch": 0.3598374927452118,
      "grad_norm": 2.2196521759033203,
      "learning_rate": 2.641698367391181e-05,
      "loss": 0.9416,
      "step": 23250
    },
    {
      "epoch": 0.3599922615592958,
      "grad_norm": 1.454229474067688,
      "learning_rate": 2.6415435198901615e-05,
      "loss": 1.0869,
      "step": 23260
    },
    {
      "epoch": 0.36014703037337975,
      "grad_norm": 1.44429612159729,
      "learning_rate": 2.6413886723891422e-05,
      "loss": 0.9257,
      "step": 23270
    },
    {
      "epoch": 0.36030179918746374,
      "grad_norm": 1.584872841835022,
      "learning_rate": 2.6412338248881226e-05,
      "loss": 0.9244,
      "step": 23280
    },
    {
      "epoch": 0.3604565680015477,
      "grad_norm": 1.2085442543029785,
      "learning_rate": 2.6410789773871033e-05,
      "loss": 0.9879,
      "step": 23290
    },
    {
      "epoch": 0.3606113368156317,
      "grad_norm": 1.794058084487915,
      "learning_rate": 2.640924129886084e-05,
      "loss": 0.9559,
      "step": 23300
    },
    {
      "epoch": 0.3607661056297156,
      "grad_norm": 1.957780361175537,
      "learning_rate": 2.6407692823850648e-05,
      "loss": 1.0402,
      "step": 23310
    },
    {
      "epoch": 0.36092087444379956,
      "grad_norm": 1.788237452507019,
      "learning_rate": 2.640614434884045e-05,
      "loss": 1.0972,
      "step": 23320
    },
    {
      "epoch": 0.36107564325788355,
      "grad_norm": 1.7193686962127686,
      "learning_rate": 2.640459587383026e-05,
      "loss": 1.1626,
      "step": 23330
    },
    {
      "epoch": 0.3612304120719675,
      "grad_norm": 1.6724011898040771,
      "learning_rate": 2.6403047398820062e-05,
      "loss": 1.1057,
      "step": 23340
    },
    {
      "epoch": 0.3613851808860515,
      "grad_norm": 1.6614054441452026,
      "learning_rate": 2.6401498923809866e-05,
      "loss": 1.2822,
      "step": 23350
    },
    {
      "epoch": 0.3615399497001354,
      "grad_norm": 1.665220022201538,
      "learning_rate": 2.6399950448799673e-05,
      "loss": 1.1722,
      "step": 23360
    },
    {
      "epoch": 0.36169471851421936,
      "grad_norm": 1.378355622291565,
      "learning_rate": 2.639840197378948e-05,
      "loss": 1.2055,
      "step": 23370
    },
    {
      "epoch": 0.36184948732830335,
      "grad_norm": 2.3051412105560303,
      "learning_rate": 2.6396853498779287e-05,
      "loss": 1.0801,
      "step": 23380
    },
    {
      "epoch": 0.3620042561423873,
      "grad_norm": 1.4469255208969116,
      "learning_rate": 2.639530502376909e-05,
      "loss": 1.1103,
      "step": 23390
    },
    {
      "epoch": 0.3621590249564713,
      "grad_norm": 1.6299996376037598,
      "learning_rate": 2.6393756548758898e-05,
      "loss": 0.9822,
      "step": 23400
    },
    {
      "epoch": 0.3623137937705552,
      "grad_norm": 1.4202518463134766,
      "learning_rate": 2.6392208073748702e-05,
      "loss": 1.116,
      "step": 23410
    },
    {
      "epoch": 0.3624685625846392,
      "grad_norm": 1.8825966119766235,
      "learning_rate": 2.639065959873851e-05,
      "loss": 1.0286,
      "step": 23420
    },
    {
      "epoch": 0.36262333139872316,
      "grad_norm": 1.6388050317764282,
      "learning_rate": 2.6389111123728313e-05,
      "loss": 0.9813,
      "step": 23430
    },
    {
      "epoch": 0.3627781002128071,
      "grad_norm": 1.499076247215271,
      "learning_rate": 2.6387562648718123e-05,
      "loss": 1.1148,
      "step": 23440
    },
    {
      "epoch": 0.3629328690268911,
      "grad_norm": 2.0145888328552246,
      "learning_rate": 2.638616902120895e-05,
      "loss": 1.0346,
      "step": 23450
    },
    {
      "epoch": 0.36308763784097503,
      "grad_norm": 2.3027267456054688,
      "learning_rate": 2.6384620546198753e-05,
      "loss": 0.9932,
      "step": 23460
    },
    {
      "epoch": 0.363242406655059,
      "grad_norm": 1.1248115301132202,
      "learning_rate": 2.638307207118856e-05,
      "loss": 1.1204,
      "step": 23470
    },
    {
      "epoch": 0.36339717546914296,
      "grad_norm": 1.6194815635681152,
      "learning_rate": 2.6381523596178364e-05,
      "loss": 1.0569,
      "step": 23480
    },
    {
      "epoch": 0.3635519442832269,
      "grad_norm": 1.4338828325271606,
      "learning_rate": 2.637997512116817e-05,
      "loss": 1.0702,
      "step": 23490
    },
    {
      "epoch": 0.3637067130973109,
      "grad_norm": 1.3498085737228394,
      "learning_rate": 2.6378426646157975e-05,
      "loss": 1.0362,
      "step": 23500
    },
    {
      "epoch": 0.36386148191139484,
      "grad_norm": 2.2873032093048096,
      "learning_rate": 2.6376878171147782e-05,
      "loss": 1.052,
      "step": 23510
    },
    {
      "epoch": 0.36401625072547883,
      "grad_norm": 1.6092653274536133,
      "learning_rate": 2.637532969613759e-05,
      "loss": 0.9211,
      "step": 23520
    },
    {
      "epoch": 0.36417101953956277,
      "grad_norm": 1.9672446250915527,
      "learning_rate": 2.6373781221127393e-05,
      "loss": 1.0861,
      "step": 23530
    },
    {
      "epoch": 0.36432578835364676,
      "grad_norm": 1.5226891040802002,
      "learning_rate": 2.63722327461172e-05,
      "loss": 0.9949,
      "step": 23540
    },
    {
      "epoch": 0.3644805571677307,
      "grad_norm": 1.4377883672714233,
      "learning_rate": 2.6370684271107004e-05,
      "loss": 0.9931,
      "step": 23550
    },
    {
      "epoch": 0.36463532598181464,
      "grad_norm": 1.933754324913025,
      "learning_rate": 2.636913579609681e-05,
      "loss": 0.9784,
      "step": 23560
    },
    {
      "epoch": 0.36479009479589863,
      "grad_norm": 2.906297206878662,
      "learning_rate": 2.6367587321086615e-05,
      "loss": 1.0395,
      "step": 23570
    },
    {
      "epoch": 0.3649448636099826,
      "grad_norm": 1.9176597595214844,
      "learning_rate": 2.6366038846076422e-05,
      "loss": 1.0547,
      "step": 23580
    },
    {
      "epoch": 0.36509963242406657,
      "grad_norm": 0.9611604809761047,
      "learning_rate": 2.636449037106623e-05,
      "loss": 1.037,
      "step": 23590
    },
    {
      "epoch": 0.3652544012381505,
      "grad_norm": 1.5505340099334717,
      "learning_rate": 2.6362941896056036e-05,
      "loss": 1.1599,
      "step": 23600
    },
    {
      "epoch": 0.3654091700522345,
      "grad_norm": 1.7907825708389282,
      "learning_rate": 2.636139342104584e-05,
      "loss": 1.0039,
      "step": 23610
    },
    {
      "epoch": 0.36556393886631844,
      "grad_norm": 1.936079502105713,
      "learning_rate": 2.6359844946035647e-05,
      "loss": 1.201,
      "step": 23620
    },
    {
      "epoch": 0.3657187076804024,
      "grad_norm": 1.6222440004348755,
      "learning_rate": 2.635829647102545e-05,
      "loss": 0.9809,
      "step": 23630
    },
    {
      "epoch": 0.36587347649448637,
      "grad_norm": 1.6498831510543823,
      "learning_rate": 2.6356747996015258e-05,
      "loss": 1.0221,
      "step": 23640
    },
    {
      "epoch": 0.3660282453085703,
      "grad_norm": 1.7574492692947388,
      "learning_rate": 2.6355199521005062e-05,
      "loss": 1.1404,
      "step": 23650
    },
    {
      "epoch": 0.3661830141226543,
      "grad_norm": 1.7732752561569214,
      "learning_rate": 2.6353651045994872e-05,
      "loss": 0.9323,
      "step": 23660
    },
    {
      "epoch": 0.36633778293673824,
      "grad_norm": 1.7097258567810059,
      "learning_rate": 2.6352102570984676e-05,
      "loss": 1.1478,
      "step": 23670
    },
    {
      "epoch": 0.3664925517508222,
      "grad_norm": 1.7140814065933228,
      "learning_rate": 2.6350554095974483e-05,
      "loss": 1.1914,
      "step": 23680
    },
    {
      "epoch": 0.3666473205649062,
      "grad_norm": 1.58920156955719,
      "learning_rate": 2.6349005620964287e-05,
      "loss": 1.0824,
      "step": 23690
    },
    {
      "epoch": 0.3668020893789901,
      "grad_norm": 2.251730442047119,
      "learning_rate": 2.6347457145954094e-05,
      "loss": 0.9922,
      "step": 23700
    },
    {
      "epoch": 0.3669568581930741,
      "grad_norm": 1.4202475547790527,
      "learning_rate": 2.6345908670943898e-05,
      "loss": 0.9954,
      "step": 23710
    },
    {
      "epoch": 0.36711162700715805,
      "grad_norm": 1.7650221586227417,
      "learning_rate": 2.6344360195933705e-05,
      "loss": 1.2792,
      "step": 23720
    },
    {
      "epoch": 0.36726639582124204,
      "grad_norm": 2.0197975635528564,
      "learning_rate": 2.6342811720923512e-05,
      "loss": 1.0856,
      "step": 23730
    },
    {
      "epoch": 0.367421164635326,
      "grad_norm": 1.4362099170684814,
      "learning_rate": 2.634126324591332e-05,
      "loss": 1.2647,
      "step": 23740
    },
    {
      "epoch": 0.3675759334494099,
      "grad_norm": 1.1121165752410889,
      "learning_rate": 2.6339714770903123e-05,
      "loss": 1.2843,
      "step": 23750
    },
    {
      "epoch": 0.3677307022634939,
      "grad_norm": 1.5931675434112549,
      "learning_rate": 2.633816629589293e-05,
      "loss": 1.0825,
      "step": 23760
    },
    {
      "epoch": 0.36788547107757785,
      "grad_norm": 1.5243526697158813,
      "learning_rate": 2.6336617820882734e-05,
      "loss": 1.248,
      "step": 23770
    },
    {
      "epoch": 0.36804023989166185,
      "grad_norm": 1.3271671533584595,
      "learning_rate": 2.6335069345872538e-05,
      "loss": 1.1272,
      "step": 23780
    },
    {
      "epoch": 0.3681950087057458,
      "grad_norm": 1.457068920135498,
      "learning_rate": 2.6333520870862345e-05,
      "loss": 0.8871,
      "step": 23790
    },
    {
      "epoch": 0.3683497775198298,
      "grad_norm": 1.7000802755355835,
      "learning_rate": 2.6331972395852152e-05,
      "loss": 1.0814,
      "step": 23800
    },
    {
      "epoch": 0.3685045463339137,
      "grad_norm": 2.017066717147827,
      "learning_rate": 2.633042392084196e-05,
      "loss": 1.104,
      "step": 23810
    },
    {
      "epoch": 0.36865931514799766,
      "grad_norm": 1.6454312801361084,
      "learning_rate": 2.6328875445831763e-05,
      "loss": 1.2765,
      "step": 23820
    },
    {
      "epoch": 0.36881408396208165,
      "grad_norm": 2.4046413898468018,
      "learning_rate": 2.632732697082157e-05,
      "loss": 0.9729,
      "step": 23830
    },
    {
      "epoch": 0.3689688527761656,
      "grad_norm": 1.4570481777191162,
      "learning_rate": 2.6325778495811374e-05,
      "loss": 1.1019,
      "step": 23840
    },
    {
      "epoch": 0.3691236215902496,
      "grad_norm": 1.2986302375793457,
      "learning_rate": 2.632423002080118e-05,
      "loss": 1.2355,
      "step": 23850
    },
    {
      "epoch": 0.3692783904043335,
      "grad_norm": 1.421529769897461,
      "learning_rate": 2.6322681545790988e-05,
      "loss": 1.0924,
      "step": 23860
    },
    {
      "epoch": 0.36943315921841746,
      "grad_norm": 1.4996585845947266,
      "learning_rate": 2.6321133070780795e-05,
      "loss": 1.0299,
      "step": 23870
    },
    {
      "epoch": 0.36958792803250146,
      "grad_norm": 1.689900517463684,
      "learning_rate": 2.63195845957706e-05,
      "loss": 1.1211,
      "step": 23880
    },
    {
      "epoch": 0.3697426968465854,
      "grad_norm": 1.2730802297592163,
      "learning_rate": 2.6318036120760406e-05,
      "loss": 0.9093,
      "step": 23890
    },
    {
      "epoch": 0.3698974656606694,
      "grad_norm": 1.4238203763961792,
      "learning_rate": 2.631648764575021e-05,
      "loss": 1.0158,
      "step": 23900
    },
    {
      "epoch": 0.3700522344747533,
      "grad_norm": 1.4578275680541992,
      "learning_rate": 2.6314939170740017e-05,
      "loss": 0.9305,
      "step": 23910
    },
    {
      "epoch": 0.3702070032888373,
      "grad_norm": 1.5027929544448853,
      "learning_rate": 2.631339069572982e-05,
      "loss": 1.0881,
      "step": 23920
    },
    {
      "epoch": 0.37036177210292126,
      "grad_norm": 1.6242609024047852,
      "learning_rate": 2.631184222071963e-05,
      "loss": 1.1244,
      "step": 23930
    },
    {
      "epoch": 0.3705165409170052,
      "grad_norm": 1.716185450553894,
      "learning_rate": 2.6310293745709435e-05,
      "loss": 1.1406,
      "step": 23940
    },
    {
      "epoch": 0.3706713097310892,
      "grad_norm": 1.9175831079483032,
      "learning_rate": 2.6308745270699242e-05,
      "loss": 0.98,
      "step": 23950
    },
    {
      "epoch": 0.37082607854517313,
      "grad_norm": 2.2531747817993164,
      "learning_rate": 2.6307196795689046e-05,
      "loss": 1.0158,
      "step": 23960
    },
    {
      "epoch": 0.3709808473592571,
      "grad_norm": 2.043584108352661,
      "learning_rate": 2.6305648320678853e-05,
      "loss": 1.0854,
      "step": 23970
    },
    {
      "epoch": 0.37113561617334107,
      "grad_norm": 1.5796781778335571,
      "learning_rate": 2.6304099845668657e-05,
      "loss": 0.9867,
      "step": 23980
    },
    {
      "epoch": 0.37129038498742506,
      "grad_norm": 1.7190022468566895,
      "learning_rate": 2.6302551370658464e-05,
      "loss": 1.2328,
      "step": 23990
    },
    {
      "epoch": 0.371445153801509,
      "grad_norm": 1.8444188833236694,
      "learning_rate": 2.630100289564827e-05,
      "loss": 1.165,
      "step": 24000
    },
    {
      "epoch": 0.37159992261559294,
      "grad_norm": 1.8379623889923096,
      "learning_rate": 2.6299454420638078e-05,
      "loss": 0.9887,
      "step": 24010
    },
    {
      "epoch": 0.37175469142967693,
      "grad_norm": 1.9632747173309326,
      "learning_rate": 2.6297905945627882e-05,
      "loss": 1.1682,
      "step": 24020
    },
    {
      "epoch": 0.37190946024376087,
      "grad_norm": 1.5365866422653198,
      "learning_rate": 2.6296357470617686e-05,
      "loss": 0.9523,
      "step": 24030
    },
    {
      "epoch": 0.37206422905784486,
      "grad_norm": 1.9597419500350952,
      "learning_rate": 2.6294808995607493e-05,
      "loss": 1.1238,
      "step": 24040
    },
    {
      "epoch": 0.3722189978719288,
      "grad_norm": 1.6610301733016968,
      "learning_rate": 2.6293260520597297e-05,
      "loss": 1.0545,
      "step": 24050
    },
    {
      "epoch": 0.37237376668601274,
      "grad_norm": 1.3809833526611328,
      "learning_rate": 2.6291712045587104e-05,
      "loss": 1.0883,
      "step": 24060
    },
    {
      "epoch": 0.37252853550009674,
      "grad_norm": 1.7330846786499023,
      "learning_rate": 2.629016357057691e-05,
      "loss": 0.9365,
      "step": 24070
    },
    {
      "epoch": 0.3726833043141807,
      "grad_norm": 1.808562994003296,
      "learning_rate": 2.6288615095566718e-05,
      "loss": 0.9187,
      "step": 24080
    },
    {
      "epoch": 0.37283807312826467,
      "grad_norm": 1.1657761335372925,
      "learning_rate": 2.6287066620556522e-05,
      "loss": 1.1117,
      "step": 24090
    },
    {
      "epoch": 0.3729928419423486,
      "grad_norm": 1.5248676538467407,
      "learning_rate": 2.628551814554633e-05,
      "loss": 0.8766,
      "step": 24100
    },
    {
      "epoch": 0.3731476107564326,
      "grad_norm": 2.196570634841919,
      "learning_rate": 2.6283969670536133e-05,
      "loss": 1.1667,
      "step": 24110
    },
    {
      "epoch": 0.37330237957051654,
      "grad_norm": 1.9479280710220337,
      "learning_rate": 2.628242119552594e-05,
      "loss": 1.1465,
      "step": 24120
    },
    {
      "epoch": 0.3734571483846005,
      "grad_norm": 1.3516203165054321,
      "learning_rate": 2.6280872720515743e-05,
      "loss": 0.9845,
      "step": 24130
    },
    {
      "epoch": 0.3736119171986845,
      "grad_norm": 1.534452199935913,
      "learning_rate": 2.6279324245505554e-05,
      "loss": 1.2473,
      "step": 24140
    },
    {
      "epoch": 0.3737666860127684,
      "grad_norm": 1.6111141443252563,
      "learning_rate": 2.6277775770495358e-05,
      "loss": 1.062,
      "step": 24150
    },
    {
      "epoch": 0.3739214548268524,
      "grad_norm": 1.6618962287902832,
      "learning_rate": 2.6276227295485165e-05,
      "loss": 0.9921,
      "step": 24160
    },
    {
      "epoch": 0.37407622364093635,
      "grad_norm": 1.6167421340942383,
      "learning_rate": 2.627467882047497e-05,
      "loss": 1.0249,
      "step": 24170
    },
    {
      "epoch": 0.37423099245502034,
      "grad_norm": 1.4168833494186401,
      "learning_rate": 2.6273130345464776e-05,
      "loss": 1.0135,
      "step": 24180
    },
    {
      "epoch": 0.3743857612691043,
      "grad_norm": 1.2712270021438599,
      "learning_rate": 2.627158187045458e-05,
      "loss": 1.1659,
      "step": 24190
    },
    {
      "epoch": 0.3745405300831882,
      "grad_norm": 1.7343981266021729,
      "learning_rate": 2.6270033395444387e-05,
      "loss": 1.1176,
      "step": 24200
    },
    {
      "epoch": 0.3746952988972722,
      "grad_norm": 2.289579391479492,
      "learning_rate": 2.6268484920434194e-05,
      "loss": 1.0171,
      "step": 24210
    },
    {
      "epoch": 0.37485006771135615,
      "grad_norm": 1.7036876678466797,
      "learning_rate": 2.6266936445424e-05,
      "loss": 1.0623,
      "step": 24220
    },
    {
      "epoch": 0.37500483652544014,
      "grad_norm": 1.298606514930725,
      "learning_rate": 2.6265387970413805e-05,
      "loss": 0.9067,
      "step": 24230
    },
    {
      "epoch": 0.3751596053395241,
      "grad_norm": 1.7667529582977295,
      "learning_rate": 2.6263839495403612e-05,
      "loss": 1.0452,
      "step": 24240
    },
    {
      "epoch": 0.375314374153608,
      "grad_norm": 1.085861086845398,
      "learning_rate": 2.6262291020393416e-05,
      "loss": 0.9644,
      "step": 24250
    },
    {
      "epoch": 0.375469142967692,
      "grad_norm": 2.1358983516693115,
      "learning_rate": 2.6260742545383223e-05,
      "loss": 1.1124,
      "step": 24260
    },
    {
      "epoch": 0.37562391178177595,
      "grad_norm": 1.9181991815567017,
      "learning_rate": 2.625919407037303e-05,
      "loss": 1.1144,
      "step": 24270
    },
    {
      "epoch": 0.37577868059585995,
      "grad_norm": 2.295692205429077,
      "learning_rate": 2.6257645595362837e-05,
      "loss": 1.2892,
      "step": 24280
    },
    {
      "epoch": 0.3759334494099439,
      "grad_norm": 1.5726854801177979,
      "learning_rate": 2.625609712035264e-05,
      "loss": 1.1811,
      "step": 24290
    },
    {
      "epoch": 0.3760882182240279,
      "grad_norm": 1.4791786670684814,
      "learning_rate": 2.6254548645342445e-05,
      "loss": 1.1088,
      "step": 24300
    },
    {
      "epoch": 0.3762429870381118,
      "grad_norm": 1.6469734907150269,
      "learning_rate": 2.625300017033225e-05,
      "loss": 1.1247,
      "step": 24310
    },
    {
      "epoch": 0.37639775585219576,
      "grad_norm": 1.7686753273010254,
      "learning_rate": 2.6251451695322055e-05,
      "loss": 1.1588,
      "step": 24320
    },
    {
      "epoch": 0.37655252466627975,
      "grad_norm": 1.267554759979248,
      "learning_rate": 2.6249903220311863e-05,
      "loss": 1.0779,
      "step": 24330
    },
    {
      "epoch": 0.3767072934803637,
      "grad_norm": 1.7360533475875854,
      "learning_rate": 2.624835474530167e-05,
      "loss": 1.0139,
      "step": 24340
    },
    {
      "epoch": 0.3768620622944477,
      "grad_norm": 1.623100757598877,
      "learning_rate": 2.6246806270291477e-05,
      "loss": 0.9479,
      "step": 24350
    },
    {
      "epoch": 0.3770168311085316,
      "grad_norm": 1.3329410552978516,
      "learning_rate": 2.624525779528128e-05,
      "loss": 1.0918,
      "step": 24360
    },
    {
      "epoch": 0.3771715999226156,
      "grad_norm": 1.2761919498443604,
      "learning_rate": 2.6243709320271088e-05,
      "loss": 1.0743,
      "step": 24370
    },
    {
      "epoch": 0.37732636873669956,
      "grad_norm": 1.9313170909881592,
      "learning_rate": 2.624216084526089e-05,
      "loss": 1.2089,
      "step": 24380
    },
    {
      "epoch": 0.3774811375507835,
      "grad_norm": 1.3321805000305176,
      "learning_rate": 2.62406123702507e-05,
      "loss": 1.0722,
      "step": 24390
    },
    {
      "epoch": 0.3776359063648675,
      "grad_norm": 1.4263412952423096,
      "learning_rate": 2.6239063895240502e-05,
      "loss": 0.9702,
      "step": 24400
    },
    {
      "epoch": 0.37779067517895143,
      "grad_norm": 1.703368902206421,
      "learning_rate": 2.6237515420230313e-05,
      "loss": 1.0956,
      "step": 24410
    },
    {
      "epoch": 0.3779454439930354,
      "grad_norm": 1.8082997798919678,
      "learning_rate": 2.6235966945220117e-05,
      "loss": 1.1411,
      "step": 24420
    },
    {
      "epoch": 0.37810021280711936,
      "grad_norm": 1.6868945360183716,
      "learning_rate": 2.6234418470209924e-05,
      "loss": 1.1664,
      "step": 24430
    },
    {
      "epoch": 0.3782549816212033,
      "grad_norm": 2.1283023357391357,
      "learning_rate": 2.6232869995199728e-05,
      "loss": 1.0925,
      "step": 24440
    },
    {
      "epoch": 0.3784097504352873,
      "grad_norm": 1.3372429609298706,
      "learning_rate": 2.6231321520189535e-05,
      "loss": 1.0633,
      "step": 24450
    },
    {
      "epoch": 0.37856451924937123,
      "grad_norm": 1.2670360803604126,
      "learning_rate": 2.622977304517934e-05,
      "loss": 0.9835,
      "step": 24460
    },
    {
      "epoch": 0.37871928806345523,
      "grad_norm": 1.62405526638031,
      "learning_rate": 2.6228224570169146e-05,
      "loss": 1.1574,
      "step": 24470
    },
    {
      "epoch": 0.37887405687753917,
      "grad_norm": 1.7412269115447998,
      "learning_rate": 2.6226676095158953e-05,
      "loss": 1.0889,
      "step": 24480
    },
    {
      "epoch": 0.37902882569162316,
      "grad_norm": 1.817015528678894,
      "learning_rate": 2.622512762014876e-05,
      "loss": 0.842,
      "step": 24490
    },
    {
      "epoch": 0.3791835945057071,
      "grad_norm": 1.5650370121002197,
      "learning_rate": 2.6223579145138564e-05,
      "loss": 1.129,
      "step": 24500
    },
    {
      "epoch": 0.37933836331979104,
      "grad_norm": 1.5032368898391724,
      "learning_rate": 2.622203067012837e-05,
      "loss": 1.1662,
      "step": 24510
    },
    {
      "epoch": 0.37949313213387503,
      "grad_norm": 1.6705354452133179,
      "learning_rate": 2.6220482195118174e-05,
      "loss": 1.2392,
      "step": 24520
    },
    {
      "epoch": 0.37964790094795897,
      "grad_norm": 1.9321216344833374,
      "learning_rate": 2.621893372010798e-05,
      "loss": 0.9873,
      "step": 24530
    },
    {
      "epoch": 0.37980266976204297,
      "grad_norm": 1.4979783296585083,
      "learning_rate": 2.6217385245097785e-05,
      "loss": 1.1266,
      "step": 24540
    },
    {
      "epoch": 0.3799574385761269,
      "grad_norm": 1.5412777662277222,
      "learning_rate": 2.6215836770087593e-05,
      "loss": 1.0653,
      "step": 24550
    },
    {
      "epoch": 0.3801122073902109,
      "grad_norm": 1.4534735679626465,
      "learning_rate": 2.62142882950774e-05,
      "loss": 1.1777,
      "step": 24560
    },
    {
      "epoch": 0.38026697620429484,
      "grad_norm": 1.3171684741973877,
      "learning_rate": 2.6212739820067203e-05,
      "loss": 1.117,
      "step": 24570
    },
    {
      "epoch": 0.3804217450183788,
      "grad_norm": 1.1416479349136353,
      "learning_rate": 2.621119134505701e-05,
      "loss": 0.947,
      "step": 24580
    },
    {
      "epoch": 0.38057651383246277,
      "grad_norm": 1.317348837852478,
      "learning_rate": 2.6209642870046814e-05,
      "loss": 1.0068,
      "step": 24590
    },
    {
      "epoch": 0.3807312826465467,
      "grad_norm": 1.5671344995498657,
      "learning_rate": 2.620809439503662e-05,
      "loss": 1.1431,
      "step": 24600
    },
    {
      "epoch": 0.3808860514606307,
      "grad_norm": 1.6133800745010376,
      "learning_rate": 2.6206545920026425e-05,
      "loss": 1.2353,
      "step": 24610
    },
    {
      "epoch": 0.38104082027471464,
      "grad_norm": 1.132599949836731,
      "learning_rate": 2.6204997445016236e-05,
      "loss": 1.1392,
      "step": 24620
    },
    {
      "epoch": 0.3811955890887986,
      "grad_norm": 1.2838798761367798,
      "learning_rate": 2.620344897000604e-05,
      "loss": 0.9831,
      "step": 24630
    },
    {
      "epoch": 0.3813503579028826,
      "grad_norm": 1.5077334642410278,
      "learning_rate": 2.6201900494995847e-05,
      "loss": 1.108,
      "step": 24640
    },
    {
      "epoch": 0.3815051267169665,
      "grad_norm": 1.312437653541565,
      "learning_rate": 2.620035201998565e-05,
      "loss": 0.9661,
      "step": 24650
    },
    {
      "epoch": 0.3816598955310505,
      "grad_norm": 1.5771604776382446,
      "learning_rate": 2.6198803544975457e-05,
      "loss": 1.0693,
      "step": 24660
    },
    {
      "epoch": 0.38181466434513445,
      "grad_norm": 1.581270456314087,
      "learning_rate": 2.619725506996526e-05,
      "loss": 1.0405,
      "step": 24670
    },
    {
      "epoch": 0.38196943315921844,
      "grad_norm": 1.3028271198272705,
      "learning_rate": 2.619570659495507e-05,
      "loss": 0.8578,
      "step": 24680
    },
    {
      "epoch": 0.3821242019733024,
      "grad_norm": 1.576973795890808,
      "learning_rate": 2.6194158119944876e-05,
      "loss": 1.1781,
      "step": 24690
    },
    {
      "epoch": 0.3822789707873863,
      "grad_norm": 1.958072304725647,
      "learning_rate": 2.6192609644934683e-05,
      "loss": 1.1523,
      "step": 24700
    },
    {
      "epoch": 0.3824337396014703,
      "grad_norm": 1.5075732469558716,
      "learning_rate": 2.6191061169924486e-05,
      "loss": 0.9927,
      "step": 24710
    },
    {
      "epoch": 0.38258850841555425,
      "grad_norm": 1.4033299684524536,
      "learning_rate": 2.6189512694914294e-05,
      "loss": 1.0399,
      "step": 24720
    },
    {
      "epoch": 0.38274327722963825,
      "grad_norm": 1.5091794729232788,
      "learning_rate": 2.6187964219904097e-05,
      "loss": 0.8702,
      "step": 24730
    },
    {
      "epoch": 0.3828980460437222,
      "grad_norm": 1.3994516134262085,
      "learning_rate": 2.6186415744893904e-05,
      "loss": 1.0259,
      "step": 24740
    },
    {
      "epoch": 0.3830528148578062,
      "grad_norm": 1.0357353687286377,
      "learning_rate": 2.618486726988371e-05,
      "loss": 1.114,
      "step": 24750
    },
    {
      "epoch": 0.3832075836718901,
      "grad_norm": 1.995599389076233,
      "learning_rate": 2.618331879487352e-05,
      "loss": 1.0941,
      "step": 24760
    },
    {
      "epoch": 0.38336235248597406,
      "grad_norm": 1.4410998821258545,
      "learning_rate": 2.6181770319863322e-05,
      "loss": 1.0398,
      "step": 24770
    },
    {
      "epoch": 0.38351712130005805,
      "grad_norm": 1.1334282159805298,
      "learning_rate": 2.618022184485313e-05,
      "loss": 0.9757,
      "step": 24780
    },
    {
      "epoch": 0.383671890114142,
      "grad_norm": 1.5247571468353271,
      "learning_rate": 2.6178673369842933e-05,
      "loss": 1.01,
      "step": 24790
    },
    {
      "epoch": 0.383826658928226,
      "grad_norm": 2.5101892948150635,
      "learning_rate": 2.6177124894832737e-05,
      "loss": 1.1828,
      "step": 24800
    },
    {
      "epoch": 0.3839814277423099,
      "grad_norm": 1.5508865118026733,
      "learning_rate": 2.6175576419822544e-05,
      "loss": 1.0151,
      "step": 24810
    },
    {
      "epoch": 0.38413619655639386,
      "grad_norm": 1.5837366580963135,
      "learning_rate": 2.617402794481235e-05,
      "loss": 1.1393,
      "step": 24820
    },
    {
      "epoch": 0.38429096537047785,
      "grad_norm": 1.6539949178695679,
      "learning_rate": 2.617247946980216e-05,
      "loss": 0.91,
      "step": 24830
    },
    {
      "epoch": 0.3844457341845618,
      "grad_norm": 1.5727002620697021,
      "learning_rate": 2.6170930994791962e-05,
      "loss": 1.084,
      "step": 24840
    },
    {
      "epoch": 0.3846005029986458,
      "grad_norm": 1.6288039684295654,
      "learning_rate": 2.616938251978177e-05,
      "loss": 1.016,
      "step": 24850
    },
    {
      "epoch": 0.3847552718127297,
      "grad_norm": 1.4995290040969849,
      "learning_rate": 2.6167834044771573e-05,
      "loss": 1.1498,
      "step": 24860
    },
    {
      "epoch": 0.3849100406268137,
      "grad_norm": 1.5145782232284546,
      "learning_rate": 2.616628556976138e-05,
      "loss": 1.1377,
      "step": 24870
    },
    {
      "epoch": 0.38506480944089766,
      "grad_norm": 1.2989908456802368,
      "learning_rate": 2.6164737094751184e-05,
      "loss": 1.2514,
      "step": 24880
    },
    {
      "epoch": 0.3852195782549816,
      "grad_norm": 1.807626724243164,
      "learning_rate": 2.6163188619740995e-05,
      "loss": 1.065,
      "step": 24890
    },
    {
      "epoch": 0.3853743470690656,
      "grad_norm": 1.5902074575424194,
      "learning_rate": 2.61616401447308e-05,
      "loss": 1.0594,
      "step": 24900
    },
    {
      "epoch": 0.38552911588314953,
      "grad_norm": 1.347616195678711,
      "learning_rate": 2.6160091669720605e-05,
      "loss": 0.9936,
      "step": 24910
    },
    {
      "epoch": 0.3856838846972335,
      "grad_norm": 1.3346531391143799,
      "learning_rate": 2.615854319471041e-05,
      "loss": 1.0518,
      "step": 24920
    },
    {
      "epoch": 0.38583865351131746,
      "grad_norm": 1.7689692974090576,
      "learning_rate": 2.6156994719700216e-05,
      "loss": 1.0938,
      "step": 24930
    },
    {
      "epoch": 0.38599342232540146,
      "grad_norm": 1.921091079711914,
      "learning_rate": 2.615544624469002e-05,
      "loss": 1.1435,
      "step": 24940
    },
    {
      "epoch": 0.3861481911394854,
      "grad_norm": 1.862351655960083,
      "learning_rate": 2.6153897769679827e-05,
      "loss": 1.1427,
      "step": 24950
    },
    {
      "epoch": 0.38630295995356934,
      "grad_norm": 1.4403821229934692,
      "learning_rate": 2.6152349294669634e-05,
      "loss": 0.9768,
      "step": 24960
    },
    {
      "epoch": 0.38645772876765333,
      "grad_norm": 1.6805436611175537,
      "learning_rate": 2.615080081965944e-05,
      "loss": 1.1511,
      "step": 24970
    },
    {
      "epoch": 0.38661249758173727,
      "grad_norm": 1.5640733242034912,
      "learning_rate": 2.6149252344649245e-05,
      "loss": 1.0928,
      "step": 24980
    },
    {
      "epoch": 0.38676726639582126,
      "grad_norm": 1.5576380491256714,
      "learning_rate": 2.6147703869639052e-05,
      "loss": 1.1062,
      "step": 24990
    },
    {
      "epoch": 0.3869220352099052,
      "grad_norm": 1.8690241575241089,
      "learning_rate": 2.6146155394628856e-05,
      "loss": 1.1913,
      "step": 25000
    },
    {
      "epoch": 0.38707680402398914,
      "grad_norm": 1.1934504508972168,
      "learning_rate": 2.6144606919618663e-05,
      "loss": 1.1057,
      "step": 25010
    },
    {
      "epoch": 0.38723157283807313,
      "grad_norm": 1.3291879892349243,
      "learning_rate": 2.6143058444608467e-05,
      "loss": 0.8612,
      "step": 25020
    },
    {
      "epoch": 0.3873863416521571,
      "grad_norm": 1.550440788269043,
      "learning_rate": 2.6141509969598278e-05,
      "loss": 1.1003,
      "step": 25030
    },
    {
      "epoch": 0.38754111046624107,
      "grad_norm": 1.4941892623901367,
      "learning_rate": 2.613996149458808e-05,
      "loss": 1.0894,
      "step": 25040
    },
    {
      "epoch": 0.387695879280325,
      "grad_norm": 2.3217029571533203,
      "learning_rate": 2.6138413019577885e-05,
      "loss": 1.1244,
      "step": 25050
    },
    {
      "epoch": 0.387850648094409,
      "grad_norm": 1.7182576656341553,
      "learning_rate": 2.6136864544567692e-05,
      "loss": 0.9877,
      "step": 25060
    },
    {
      "epoch": 0.38800541690849294,
      "grad_norm": 1.0418473482131958,
      "learning_rate": 2.6135316069557496e-05,
      "loss": 0.9492,
      "step": 25070
    },
    {
      "epoch": 0.3881601857225769,
      "grad_norm": 1.5182199478149414,
      "learning_rate": 2.6133767594547303e-05,
      "loss": 1.1314,
      "step": 25080
    },
    {
      "epoch": 0.3883149545366609,
      "grad_norm": 1.0436533689498901,
      "learning_rate": 2.6132219119537107e-05,
      "loss": 1.0239,
      "step": 25090
    },
    {
      "epoch": 0.3884697233507448,
      "grad_norm": 2.1545228958129883,
      "learning_rate": 2.6130670644526917e-05,
      "loss": 1.1709,
      "step": 25100
    },
    {
      "epoch": 0.3886244921648288,
      "grad_norm": 1.187185287475586,
      "learning_rate": 2.612912216951672e-05,
      "loss": 1.2154,
      "step": 25110
    },
    {
      "epoch": 0.38877926097891274,
      "grad_norm": 1.685351848602295,
      "learning_rate": 2.6127573694506528e-05,
      "loss": 1.0361,
      "step": 25120
    },
    {
      "epoch": 0.38893402979299674,
      "grad_norm": 1.825118064880371,
      "learning_rate": 2.6126025219496332e-05,
      "loss": 1.0307,
      "step": 25130
    },
    {
      "epoch": 0.3890887986070807,
      "grad_norm": 1.1409690380096436,
      "learning_rate": 2.612447674448614e-05,
      "loss": 1.0899,
      "step": 25140
    },
    {
      "epoch": 0.3892435674211646,
      "grad_norm": 1.4915934801101685,
      "learning_rate": 2.6122928269475943e-05,
      "loss": 1.0104,
      "step": 25150
    },
    {
      "epoch": 0.3893983362352486,
      "grad_norm": 1.2173559665679932,
      "learning_rate": 2.6121379794465753e-05,
      "loss": 1.1677,
      "step": 25160
    },
    {
      "epoch": 0.38955310504933255,
      "grad_norm": 1.311771273612976,
      "learning_rate": 2.6119831319455557e-05,
      "loss": 1.0174,
      "step": 25170
    },
    {
      "epoch": 0.38970787386341654,
      "grad_norm": 1.5695096254348755,
      "learning_rate": 2.6118282844445364e-05,
      "loss": 1.1052,
      "step": 25180
    },
    {
      "epoch": 0.3898626426775005,
      "grad_norm": 1.8318806886672974,
      "learning_rate": 2.6116734369435168e-05,
      "loss": 1.1404,
      "step": 25190
    },
    {
      "epoch": 0.3900174114915844,
      "grad_norm": 1.590998649597168,
      "learning_rate": 2.6115185894424975e-05,
      "loss": 1.1779,
      "step": 25200
    },
    {
      "epoch": 0.3901721803056684,
      "grad_norm": 2.307090997695923,
      "learning_rate": 2.611363741941478e-05,
      "loss": 1.0319,
      "step": 25210
    },
    {
      "epoch": 0.39032694911975235,
      "grad_norm": 1.7176884412765503,
      "learning_rate": 2.6112088944404586e-05,
      "loss": 1.1963,
      "step": 25220
    },
    {
      "epoch": 0.39048171793383635,
      "grad_norm": 1.691015601158142,
      "learning_rate": 2.6110540469394393e-05,
      "loss": 1.0543,
      "step": 25230
    },
    {
      "epoch": 0.3906364867479203,
      "grad_norm": 1.436198115348816,
      "learning_rate": 2.61089919943842e-05,
      "loss": 1.2687,
      "step": 25240
    },
    {
      "epoch": 0.3907912555620043,
      "grad_norm": 1.6270514726638794,
      "learning_rate": 2.6107443519374004e-05,
      "loss": 1.2943,
      "step": 25250
    },
    {
      "epoch": 0.3909460243760882,
      "grad_norm": 1.5918920040130615,
      "learning_rate": 2.610589504436381e-05,
      "loss": 1.0743,
      "step": 25260
    },
    {
      "epoch": 0.39110079319017216,
      "grad_norm": 1.1106945276260376,
      "learning_rate": 2.6104346569353615e-05,
      "loss": 1.0286,
      "step": 25270
    },
    {
      "epoch": 0.39125556200425615,
      "grad_norm": 1.642367959022522,
      "learning_rate": 2.6102798094343422e-05,
      "loss": 1.1023,
      "step": 25280
    },
    {
      "epoch": 0.3914103308183401,
      "grad_norm": 1.327207326889038,
      "learning_rate": 2.6101249619333226e-05,
      "loss": 1.2051,
      "step": 25290
    },
    {
      "epoch": 0.3915650996324241,
      "grad_norm": 1.2378252744674683,
      "learning_rate": 2.6099701144323033e-05,
      "loss": 1.0707,
      "step": 25300
    },
    {
      "epoch": 0.391719868446508,
      "grad_norm": 1.1051528453826904,
      "learning_rate": 2.609815266931284e-05,
      "loss": 1.1022,
      "step": 25310
    },
    {
      "epoch": 0.391874637260592,
      "grad_norm": 1.5385171175003052,
      "learning_rate": 2.6096604194302644e-05,
      "loss": 1.1008,
      "step": 25320
    },
    {
      "epoch": 0.39202940607467596,
      "grad_norm": 1.2219569683074951,
      "learning_rate": 2.609505571929245e-05,
      "loss": 1.103,
      "step": 25330
    },
    {
      "epoch": 0.3921841748887599,
      "grad_norm": 2.051438570022583,
      "learning_rate": 2.6093507244282255e-05,
      "loss": 1.0841,
      "step": 25340
    },
    {
      "epoch": 0.3923389437028439,
      "grad_norm": 1.797597885131836,
      "learning_rate": 2.6091958769272062e-05,
      "loss": 1.118,
      "step": 25350
    },
    {
      "epoch": 0.39249371251692783,
      "grad_norm": 1.7095279693603516,
      "learning_rate": 2.6090410294261866e-05,
      "loss": 1.0404,
      "step": 25360
    },
    {
      "epoch": 0.3926484813310118,
      "grad_norm": 1.625144600868225,
      "learning_rate": 2.6088861819251676e-05,
      "loss": 0.9893,
      "step": 25370
    },
    {
      "epoch": 0.39280325014509576,
      "grad_norm": 1.122031569480896,
      "learning_rate": 2.608731334424148e-05,
      "loss": 1.025,
      "step": 25380
    },
    {
      "epoch": 0.3929580189591797,
      "grad_norm": 1.4194382429122925,
      "learning_rate": 2.6085764869231287e-05,
      "loss": 1.1251,
      "step": 25390
    },
    {
      "epoch": 0.3931127877732637,
      "grad_norm": 1.5564203262329102,
      "learning_rate": 2.608421639422109e-05,
      "loss": 1.1882,
      "step": 25400
    },
    {
      "epoch": 0.39326755658734763,
      "grad_norm": 1.1583774089813232,
      "learning_rate": 2.6082667919210898e-05,
      "loss": 0.9881,
      "step": 25410
    },
    {
      "epoch": 0.3934223254014316,
      "grad_norm": 1.4414728879928589,
      "learning_rate": 2.6081119444200702e-05,
      "loss": 1.0572,
      "step": 25420
    },
    {
      "epoch": 0.39357709421551557,
      "grad_norm": 1.4758975505828857,
      "learning_rate": 2.607957096919051e-05,
      "loss": 1.1559,
      "step": 25430
    },
    {
      "epoch": 0.39373186302959956,
      "grad_norm": 1.000476360321045,
      "learning_rate": 2.6078022494180316e-05,
      "loss": 0.9068,
      "step": 25440
    },
    {
      "epoch": 0.3938866318436835,
      "grad_norm": 1.6932958364486694,
      "learning_rate": 2.6076474019170123e-05,
      "loss": 1.2006,
      "step": 25450
    },
    {
      "epoch": 0.39404140065776744,
      "grad_norm": 1.6233174800872803,
      "learning_rate": 2.6074925544159927e-05,
      "loss": 1.2171,
      "step": 25460
    },
    {
      "epoch": 0.39419616947185143,
      "grad_norm": 1.9491162300109863,
      "learning_rate": 2.6073377069149734e-05,
      "loss": 0.8789,
      "step": 25470
    },
    {
      "epoch": 0.39435093828593537,
      "grad_norm": 1.5148872137069702,
      "learning_rate": 2.6071828594139538e-05,
      "loss": 1.1107,
      "step": 25480
    },
    {
      "epoch": 0.39450570710001936,
      "grad_norm": 1.5881900787353516,
      "learning_rate": 2.6070280119129345e-05,
      "loss": 1.1777,
      "step": 25490
    },
    {
      "epoch": 0.3946604759141033,
      "grad_norm": 1.4488894939422607,
      "learning_rate": 2.606873164411915e-05,
      "loss": 1.0375,
      "step": 25500
    },
    {
      "epoch": 0.3948152447281873,
      "grad_norm": 1.6651087999343872,
      "learning_rate": 2.606718316910896e-05,
      "loss": 1.0396,
      "step": 25510
    },
    {
      "epoch": 0.39497001354227124,
      "grad_norm": 1.888546347618103,
      "learning_rate": 2.6065634694098763e-05,
      "loss": 1.2173,
      "step": 25520
    },
    {
      "epoch": 0.3951247823563552,
      "grad_norm": 1.8541680574417114,
      "learning_rate": 2.606408621908857e-05,
      "loss": 1.0571,
      "step": 25530
    },
    {
      "epoch": 0.39527955117043917,
      "grad_norm": 1.710654854774475,
      "learning_rate": 2.6062537744078374e-05,
      "loss": 1.1964,
      "step": 25540
    },
    {
      "epoch": 0.3954343199845231,
      "grad_norm": 2.180529832839966,
      "learning_rate": 2.6060989269068178e-05,
      "loss": 1.0577,
      "step": 25550
    },
    {
      "epoch": 0.3955890887986071,
      "grad_norm": 1.3605964183807373,
      "learning_rate": 2.6059440794057985e-05,
      "loss": 1.0548,
      "step": 25560
    },
    {
      "epoch": 0.39574385761269104,
      "grad_norm": 1.5309396982192993,
      "learning_rate": 2.6057892319047792e-05,
      "loss": 0.9775,
      "step": 25570
    },
    {
      "epoch": 0.395898626426775,
      "grad_norm": 1.6535762548446655,
      "learning_rate": 2.60563438440376e-05,
      "loss": 1.1642,
      "step": 25580
    },
    {
      "epoch": 0.396053395240859,
      "grad_norm": 1.6581356525421143,
      "learning_rate": 2.6054795369027403e-05,
      "loss": 1.2107,
      "step": 25590
    },
    {
      "epoch": 0.3962081640549429,
      "grad_norm": 2.044377088546753,
      "learning_rate": 2.605324689401721e-05,
      "loss": 1.1762,
      "step": 25600
    },
    {
      "epoch": 0.3963629328690269,
      "grad_norm": 1.679196834564209,
      "learning_rate": 2.6051698419007014e-05,
      "loss": 1.1106,
      "step": 25610
    },
    {
      "epoch": 0.39651770168311085,
      "grad_norm": 1.5688806772232056,
      "learning_rate": 2.605014994399682e-05,
      "loss": 0.9788,
      "step": 25620
    },
    {
      "epoch": 0.39667247049719484,
      "grad_norm": 2.0844476222991943,
      "learning_rate": 2.6048601468986625e-05,
      "loss": 1.0772,
      "step": 25630
    },
    {
      "epoch": 0.3968272393112788,
      "grad_norm": 1.8310301303863525,
      "learning_rate": 2.6047052993976435e-05,
      "loss": 1.1085,
      "step": 25640
    },
    {
      "epoch": 0.3969820081253627,
      "grad_norm": 1.310166597366333,
      "learning_rate": 2.604550451896624e-05,
      "loss": 1.0581,
      "step": 25650
    },
    {
      "epoch": 0.3971367769394467,
      "grad_norm": 1.4370306730270386,
      "learning_rate": 2.6043956043956046e-05,
      "loss": 0.8098,
      "step": 25660
    },
    {
      "epoch": 0.39729154575353065,
      "grad_norm": 1.3128600120544434,
      "learning_rate": 2.604240756894585e-05,
      "loss": 1.0627,
      "step": 25670
    },
    {
      "epoch": 0.39744631456761464,
      "grad_norm": 1.934322476387024,
      "learning_rate": 2.6040859093935657e-05,
      "loss": 1.1346,
      "step": 25680
    },
    {
      "epoch": 0.3976010833816986,
      "grad_norm": 1.6259784698486328,
      "learning_rate": 2.603931061892546e-05,
      "loss": 1.0434,
      "step": 25690
    },
    {
      "epoch": 0.3977558521957825,
      "grad_norm": 1.41360342502594,
      "learning_rate": 2.6037762143915268e-05,
      "loss": 1.0816,
      "step": 25700
    },
    {
      "epoch": 0.3979106210098665,
      "grad_norm": 1.8983397483825684,
      "learning_rate": 2.6036213668905075e-05,
      "loss": 1.1112,
      "step": 25710
    },
    {
      "epoch": 0.39806538982395046,
      "grad_norm": 2.104783058166504,
      "learning_rate": 2.6034665193894882e-05,
      "loss": 1.0917,
      "step": 25720
    },
    {
      "epoch": 0.39822015863803445,
      "grad_norm": 1.2546125650405884,
      "learning_rate": 2.6033116718884686e-05,
      "loss": 0.927,
      "step": 25730
    },
    {
      "epoch": 0.3983749274521184,
      "grad_norm": 1.2647067308425903,
      "learning_rate": 2.6031568243874493e-05,
      "loss": 1.0276,
      "step": 25740
    },
    {
      "epoch": 0.3985296962662024,
      "grad_norm": 1.609771728515625,
      "learning_rate": 2.6030019768864297e-05,
      "loss": 0.9843,
      "step": 25750
    },
    {
      "epoch": 0.3986844650802863,
      "grad_norm": 1.403947353363037,
      "learning_rate": 2.6028471293854104e-05,
      "loss": 1.1465,
      "step": 25760
    },
    {
      "epoch": 0.39883923389437026,
      "grad_norm": 1.7783362865447998,
      "learning_rate": 2.6026922818843908e-05,
      "loss": 1.1052,
      "step": 25770
    },
    {
      "epoch": 0.39899400270845425,
      "grad_norm": 2.005491018295288,
      "learning_rate": 2.6025374343833718e-05,
      "loss": 1.2855,
      "step": 25780
    },
    {
      "epoch": 0.3991487715225382,
      "grad_norm": 2.146125078201294,
      "learning_rate": 2.6023825868823522e-05,
      "loss": 1.1977,
      "step": 25790
    },
    {
      "epoch": 0.3993035403366222,
      "grad_norm": 1.4310925006866455,
      "learning_rate": 2.602227739381333e-05,
      "loss": 1.0929,
      "step": 25800
    },
    {
      "epoch": 0.3994583091507061,
      "grad_norm": 1.8077201843261719,
      "learning_rate": 2.6020728918803133e-05,
      "loss": 1.328,
      "step": 25810
    },
    {
      "epoch": 0.3996130779647901,
      "grad_norm": 1.248849868774414,
      "learning_rate": 2.6019180443792936e-05,
      "loss": 1.1154,
      "step": 25820
    },
    {
      "epoch": 0.39976784677887406,
      "grad_norm": 1.9958196878433228,
      "learning_rate": 2.6017631968782744e-05,
      "loss": 0.9439,
      "step": 25830
    },
    {
      "epoch": 0.399922615592958,
      "grad_norm": 1.6184356212615967,
      "learning_rate": 2.6016083493772547e-05,
      "loss": 1.0465,
      "step": 25840
    },
    {
      "epoch": 0.400077384407042,
      "grad_norm": 2.137319326400757,
      "learning_rate": 2.6014535018762358e-05,
      "loss": 1.2909,
      "step": 25850
    },
    {
      "epoch": 0.40023215322112593,
      "grad_norm": 1.9133797883987427,
      "learning_rate": 2.601298654375216e-05,
      "loss": 1.0341,
      "step": 25860
    },
    {
      "epoch": 0.4003869220352099,
      "grad_norm": 1.2454510927200317,
      "learning_rate": 2.601143806874197e-05,
      "loss": 0.9921,
      "step": 25870
    },
    {
      "epoch": 0.40054169084929386,
      "grad_norm": 1.5321935415267944,
      "learning_rate": 2.6009889593731773e-05,
      "loss": 1.1913,
      "step": 25880
    },
    {
      "epoch": 0.4006964596633778,
      "grad_norm": 1.8305455446243286,
      "learning_rate": 2.600834111872158e-05,
      "loss": 1.2163,
      "step": 25890
    },
    {
      "epoch": 0.4008512284774618,
      "grad_norm": 2.260267496109009,
      "learning_rate": 2.6006792643711383e-05,
      "loss": 1.0617,
      "step": 25900
    },
    {
      "epoch": 0.40100599729154573,
      "grad_norm": 1.038253664970398,
      "learning_rate": 2.600524416870119e-05,
      "loss": 0.9547,
      "step": 25910
    },
    {
      "epoch": 0.40116076610562973,
      "grad_norm": 1.5011467933654785,
      "learning_rate": 2.6003695693690998e-05,
      "loss": 1.0382,
      "step": 25920
    },
    {
      "epoch": 0.40131553491971367,
      "grad_norm": 1.522440791130066,
      "learning_rate": 2.6002147218680805e-05,
      "loss": 1.0165,
      "step": 25930
    },
    {
      "epoch": 0.40147030373379766,
      "grad_norm": 1.6581144332885742,
      "learning_rate": 2.600059874367061e-05,
      "loss": 0.9709,
      "step": 25940
    },
    {
      "epoch": 0.4016250725478816,
      "grad_norm": 1.0662785768508911,
      "learning_rate": 2.5999050268660416e-05,
      "loss": 0.9537,
      "step": 25950
    },
    {
      "epoch": 0.40177984136196554,
      "grad_norm": 2.2880141735076904,
      "learning_rate": 2.599750179365022e-05,
      "loss": 1.212,
      "step": 25960
    },
    {
      "epoch": 0.40193461017604953,
      "grad_norm": 1.3675134181976318,
      "learning_rate": 2.5995953318640027e-05,
      "loss": 1.0866,
      "step": 25970
    },
    {
      "epoch": 0.4020893789901335,
      "grad_norm": 1.7787212133407593,
      "learning_rate": 2.599440484362983e-05,
      "loss": 1.1195,
      "step": 25980
    },
    {
      "epoch": 0.40224414780421747,
      "grad_norm": 1.452754259109497,
      "learning_rate": 2.599285636861964e-05,
      "loss": 0.9112,
      "step": 25990
    },
    {
      "epoch": 0.4023989166183014,
      "grad_norm": 1.4604421854019165,
      "learning_rate": 2.5991307893609445e-05,
      "loss": 1.1125,
      "step": 26000
    },
    {
      "epoch": 0.4025536854323854,
      "grad_norm": 1.5840932130813599,
      "learning_rate": 2.5989759418599252e-05,
      "loss": 1.0864,
      "step": 26010
    },
    {
      "epoch": 0.40270845424646934,
      "grad_norm": 2.514425039291382,
      "learning_rate": 2.5988210943589056e-05,
      "loss": 1.2661,
      "step": 26020
    },
    {
      "epoch": 0.4028632230605533,
      "grad_norm": 1.727420687675476,
      "learning_rate": 2.5986662468578863e-05,
      "loss": 1.0935,
      "step": 26030
    },
    {
      "epoch": 0.40301799187463727,
      "grad_norm": 1.4909616708755493,
      "learning_rate": 2.5985113993568666e-05,
      "loss": 0.9037,
      "step": 26040
    },
    {
      "epoch": 0.4031727606887212,
      "grad_norm": 1.4499282836914062,
      "learning_rate": 2.5983565518558477e-05,
      "loss": 1.0425,
      "step": 26050
    },
    {
      "epoch": 0.4033275295028052,
      "grad_norm": 1.2490590810775757,
      "learning_rate": 2.598201704354828e-05,
      "loss": 0.9688,
      "step": 26060
    },
    {
      "epoch": 0.40348229831688914,
      "grad_norm": 1.6342500448226929,
      "learning_rate": 2.5980468568538084e-05,
      "loss": 1.1951,
      "step": 26070
    },
    {
      "epoch": 0.4036370671309731,
      "grad_norm": 1.7295235395431519,
      "learning_rate": 2.597892009352789e-05,
      "loss": 0.962,
      "step": 26080
    },
    {
      "epoch": 0.4037918359450571,
      "grad_norm": 1.4953856468200684,
      "learning_rate": 2.5977371618517695e-05,
      "loss": 1.0361,
      "step": 26090
    },
    {
      "epoch": 0.403946604759141,
      "grad_norm": 1.4755687713623047,
      "learning_rate": 2.5975823143507503e-05,
      "loss": 1.0978,
      "step": 26100
    },
    {
      "epoch": 0.404101373573225,
      "grad_norm": 2.3773105144500732,
      "learning_rate": 2.5974274668497306e-05,
      "loss": 1.0571,
      "step": 26110
    },
    {
      "epoch": 0.40425614238730895,
      "grad_norm": 1.9207932949066162,
      "learning_rate": 2.5972726193487117e-05,
      "loss": 1.0607,
      "step": 26120
    },
    {
      "epoch": 0.40441091120139294,
      "grad_norm": 0.9047821164131165,
      "learning_rate": 2.597117771847692e-05,
      "loss": 1.0506,
      "step": 26130
    },
    {
      "epoch": 0.4045656800154769,
      "grad_norm": 1.5826369524002075,
      "learning_rate": 2.5969629243466728e-05,
      "loss": 0.9896,
      "step": 26140
    },
    {
      "epoch": 0.4047204488295608,
      "grad_norm": 1.6926885843276978,
      "learning_rate": 2.596808076845653e-05,
      "loss": 1.0507,
      "step": 26150
    },
    {
      "epoch": 0.4048752176436448,
      "grad_norm": 1.730080008506775,
      "learning_rate": 2.596653229344634e-05,
      "loss": 1.0822,
      "step": 26160
    },
    {
      "epoch": 0.40502998645772875,
      "grad_norm": 1.5349191427230835,
      "learning_rate": 2.5964983818436142e-05,
      "loss": 1.0394,
      "step": 26170
    },
    {
      "epoch": 0.40518475527181275,
      "grad_norm": 1.6817476749420166,
      "learning_rate": 2.596343534342595e-05,
      "loss": 0.9157,
      "step": 26180
    },
    {
      "epoch": 0.4053395240858967,
      "grad_norm": 1.6368318796157837,
      "learning_rate": 2.5961886868415757e-05,
      "loss": 1.05,
      "step": 26190
    },
    {
      "epoch": 0.4054942928999807,
      "grad_norm": 1.1212774515151978,
      "learning_rate": 2.5960338393405564e-05,
      "loss": 0.8941,
      "step": 26200
    },
    {
      "epoch": 0.4056490617140646,
      "grad_norm": 2.2346339225769043,
      "learning_rate": 2.5958789918395367e-05,
      "loss": 1.0256,
      "step": 26210
    },
    {
      "epoch": 0.40580383052814856,
      "grad_norm": 2.421356201171875,
      "learning_rate": 2.5957241443385175e-05,
      "loss": 0.9409,
      "step": 26220
    },
    {
      "epoch": 0.40595859934223255,
      "grad_norm": 1.4444042444229126,
      "learning_rate": 2.595569296837498e-05,
      "loss": 1.05,
      "step": 26230
    },
    {
      "epoch": 0.4061133681563165,
      "grad_norm": 2.147501230239868,
      "learning_rate": 2.5954144493364786e-05,
      "loss": 1.2745,
      "step": 26240
    },
    {
      "epoch": 0.4062681369704005,
      "grad_norm": 2.074653148651123,
      "learning_rate": 2.595259601835459e-05,
      "loss": 1.1003,
      "step": 26250
    },
    {
      "epoch": 0.4064229057844844,
      "grad_norm": 1.6007483005523682,
      "learning_rate": 2.59510475433444e-05,
      "loss": 1.2186,
      "step": 26260
    },
    {
      "epoch": 0.40657767459856836,
      "grad_norm": 1.2862366437911987,
      "learning_rate": 2.5949499068334204e-05,
      "loss": 1.2344,
      "step": 26270
    },
    {
      "epoch": 0.40673244341265236,
      "grad_norm": 1.3745648860931396,
      "learning_rate": 2.594795059332401e-05,
      "loss": 1.0807,
      "step": 26280
    },
    {
      "epoch": 0.4068872122267363,
      "grad_norm": 1.536907434463501,
      "learning_rate": 2.5946402118313814e-05,
      "loss": 1.154,
      "step": 26290
    },
    {
      "epoch": 0.4070419810408203,
      "grad_norm": 1.0797628164291382,
      "learning_rate": 2.594485364330362e-05,
      "loss": 0.9339,
      "step": 26300
    },
    {
      "epoch": 0.4071967498549042,
      "grad_norm": 1.128221035003662,
      "learning_rate": 2.5943305168293425e-05,
      "loss": 0.9382,
      "step": 26310
    },
    {
      "epoch": 0.4073515186689882,
      "grad_norm": 1.833880066871643,
      "learning_rate": 2.594175669328323e-05,
      "loss": 1.1067,
      "step": 26320
    },
    {
      "epoch": 0.40750628748307216,
      "grad_norm": 1.5472193956375122,
      "learning_rate": 2.594020821827304e-05,
      "loss": 1.1646,
      "step": 26330
    },
    {
      "epoch": 0.4076610562971561,
      "grad_norm": 3.631645917892456,
      "learning_rate": 2.5938659743262843e-05,
      "loss": 1.0941,
      "step": 26340
    },
    {
      "epoch": 0.4078158251112401,
      "grad_norm": 1.489848256111145,
      "learning_rate": 2.593711126825265e-05,
      "loss": 1.1816,
      "step": 26350
    },
    {
      "epoch": 0.40797059392532403,
      "grad_norm": 1.6132707595825195,
      "learning_rate": 2.5935562793242454e-05,
      "loss": 1.012,
      "step": 26360
    },
    {
      "epoch": 0.408125362739408,
      "grad_norm": 2.195735216140747,
      "learning_rate": 2.593401431823226e-05,
      "loss": 0.9939,
      "step": 26370
    },
    {
      "epoch": 0.40828013155349197,
      "grad_norm": 1.3691580295562744,
      "learning_rate": 2.5932465843222065e-05,
      "loss": 1.0411,
      "step": 26380
    },
    {
      "epoch": 0.40843490036757596,
      "grad_norm": 1.5172052383422852,
      "learning_rate": 2.5930917368211872e-05,
      "loss": 0.9125,
      "step": 26390
    },
    {
      "epoch": 0.4085896691816599,
      "grad_norm": 0.9733243584632874,
      "learning_rate": 2.592936889320168e-05,
      "loss": 1.0633,
      "step": 26400
    },
    {
      "epoch": 0.40874443799574384,
      "grad_norm": 1.3966224193572998,
      "learning_rate": 2.5927820418191487e-05,
      "loss": 1.1348,
      "step": 26410
    },
    {
      "epoch": 0.40889920680982783,
      "grad_norm": 1.0151258707046509,
      "learning_rate": 2.592627194318129e-05,
      "loss": 0.8492,
      "step": 26420
    },
    {
      "epoch": 0.40905397562391177,
      "grad_norm": 1.632598876953125,
      "learning_rate": 2.5924723468171097e-05,
      "loss": 1.1361,
      "step": 26430
    },
    {
      "epoch": 0.40920874443799576,
      "grad_norm": 2.3809726238250732,
      "learning_rate": 2.59231749931609e-05,
      "loss": 0.989,
      "step": 26440
    },
    {
      "epoch": 0.4093635132520797,
      "grad_norm": 1.8605883121490479,
      "learning_rate": 2.592162651815071e-05,
      "loss": 1.0293,
      "step": 26450
    },
    {
      "epoch": 0.40951828206616364,
      "grad_norm": 1.4314712285995483,
      "learning_rate": 2.5920078043140515e-05,
      "loss": 1.0112,
      "step": 26460
    },
    {
      "epoch": 0.40967305088024764,
      "grad_norm": 1.872936487197876,
      "learning_rate": 2.5918529568130323e-05,
      "loss": 1.0972,
      "step": 26470
    },
    {
      "epoch": 0.4098278196943316,
      "grad_norm": 1.559088945388794,
      "learning_rate": 2.5916981093120126e-05,
      "loss": 1.0903,
      "step": 26480
    },
    {
      "epoch": 0.40998258850841557,
      "grad_norm": 1.4213610887527466,
      "learning_rate": 2.5915432618109933e-05,
      "loss": 1.0568,
      "step": 26490
    },
    {
      "epoch": 0.4101373573224995,
      "grad_norm": 1.4812943935394287,
      "learning_rate": 2.5913884143099737e-05,
      "loss": 1.2717,
      "step": 26500
    },
    {
      "epoch": 0.4102921261365835,
      "grad_norm": 1.958959698677063,
      "learning_rate": 2.5912335668089544e-05,
      "loss": 1.0803,
      "step": 26510
    },
    {
      "epoch": 0.41044689495066744,
      "grad_norm": 2.1708362102508545,
      "learning_rate": 2.5910787193079348e-05,
      "loss": 1.2135,
      "step": 26520
    },
    {
      "epoch": 0.4106016637647514,
      "grad_norm": 2.225955009460449,
      "learning_rate": 2.590923871806916e-05,
      "loss": 1.2593,
      "step": 26530
    },
    {
      "epoch": 0.4107564325788354,
      "grad_norm": 1.3525854349136353,
      "learning_rate": 2.5907690243058962e-05,
      "loss": 1.0553,
      "step": 26540
    },
    {
      "epoch": 0.4109112013929193,
      "grad_norm": 1.4346389770507812,
      "learning_rate": 2.590614176804877e-05,
      "loss": 1.1557,
      "step": 26550
    },
    {
      "epoch": 0.4110659702070033,
      "grad_norm": 1.4831496477127075,
      "learning_rate": 2.5904593293038573e-05,
      "loss": 1.1704,
      "step": 26560
    },
    {
      "epoch": 0.41122073902108724,
      "grad_norm": 1.5837324857711792,
      "learning_rate": 2.5903044818028377e-05,
      "loss": 1.1213,
      "step": 26570
    },
    {
      "epoch": 0.41137550783517124,
      "grad_norm": 1.186462640762329,
      "learning_rate": 2.5901496343018184e-05,
      "loss": 1.1332,
      "step": 26580
    },
    {
      "epoch": 0.4115302766492552,
      "grad_norm": 1.391395926475525,
      "learning_rate": 2.5899947868007988e-05,
      "loss": 1.1843,
      "step": 26590
    },
    {
      "epoch": 0.4116850454633391,
      "grad_norm": 2.100217819213867,
      "learning_rate": 2.58983993929978e-05,
      "loss": 0.9977,
      "step": 26600
    },
    {
      "epoch": 0.4118398142774231,
      "grad_norm": 2.0925467014312744,
      "learning_rate": 2.5896850917987602e-05,
      "loss": 0.9846,
      "step": 26610
    },
    {
      "epoch": 0.41199458309150705,
      "grad_norm": 1.4909182786941528,
      "learning_rate": 2.589530244297741e-05,
      "loss": 1.0363,
      "step": 26620
    },
    {
      "epoch": 0.41214935190559104,
      "grad_norm": 1.5223628282546997,
      "learning_rate": 2.5893753967967213e-05,
      "loss": 1.0043,
      "step": 26630
    },
    {
      "epoch": 0.412304120719675,
      "grad_norm": 1.8416721820831299,
      "learning_rate": 2.589220549295702e-05,
      "loss": 1.1108,
      "step": 26640
    },
    {
      "epoch": 0.4124588895337589,
      "grad_norm": 1.7542086839675903,
      "learning_rate": 2.5890657017946824e-05,
      "loss": 1.0929,
      "step": 26650
    },
    {
      "epoch": 0.4126136583478429,
      "grad_norm": 1.979027509689331,
      "learning_rate": 2.588910854293663e-05,
      "loss": 1.0077,
      "step": 26660
    },
    {
      "epoch": 0.41276842716192685,
      "grad_norm": 1.497160792350769,
      "learning_rate": 2.5887560067926438e-05,
      "loss": 0.9379,
      "step": 26670
    },
    {
      "epoch": 0.41292319597601085,
      "grad_norm": 1.9024187326431274,
      "learning_rate": 2.5886011592916245e-05,
      "loss": 1.086,
      "step": 26680
    },
    {
      "epoch": 0.4130779647900948,
      "grad_norm": 1.4355700016021729,
      "learning_rate": 2.588446311790605e-05,
      "loss": 1.1474,
      "step": 26690
    },
    {
      "epoch": 0.4132327336041788,
      "grad_norm": 1.415163278579712,
      "learning_rate": 2.5882914642895856e-05,
      "loss": 1.2335,
      "step": 26700
    },
    {
      "epoch": 0.4133875024182627,
      "grad_norm": 1.0250834226608276,
      "learning_rate": 2.588136616788566e-05,
      "loss": 1.0559,
      "step": 26710
    },
    {
      "epoch": 0.41354227123234666,
      "grad_norm": 1.6922924518585205,
      "learning_rate": 2.5879817692875467e-05,
      "loss": 1.0537,
      "step": 26720
    },
    {
      "epoch": 0.41369704004643065,
      "grad_norm": 1.5530534982681274,
      "learning_rate": 2.587826921786527e-05,
      "loss": 1.1517,
      "step": 26730
    },
    {
      "epoch": 0.4138518088605146,
      "grad_norm": 1.173313021659851,
      "learning_rate": 2.587672074285508e-05,
      "loss": 1.1493,
      "step": 26740
    },
    {
      "epoch": 0.4140065776745986,
      "grad_norm": 1.9172298908233643,
      "learning_rate": 2.5875172267844885e-05,
      "loss": 1.1757,
      "step": 26750
    },
    {
      "epoch": 0.4141613464886825,
      "grad_norm": 2.037006378173828,
      "learning_rate": 2.5873623792834692e-05,
      "loss": 1.1346,
      "step": 26760
    },
    {
      "epoch": 0.4143161153027665,
      "grad_norm": 2.0638980865478516,
      "learning_rate": 2.5872075317824496e-05,
      "loss": 1.2156,
      "step": 26770
    },
    {
      "epoch": 0.41447088411685046,
      "grad_norm": 1.2923836708068848,
      "learning_rate": 2.5870526842814303e-05,
      "loss": 0.9815,
      "step": 26780
    },
    {
      "epoch": 0.4146256529309344,
      "grad_norm": 1.5366499423980713,
      "learning_rate": 2.5868978367804107e-05,
      "loss": 1.0301,
      "step": 26790
    },
    {
      "epoch": 0.4147804217450184,
      "grad_norm": 1.3542091846466064,
      "learning_rate": 2.5867429892793914e-05,
      "loss": 0.944,
      "step": 26800
    },
    {
      "epoch": 0.41493519055910233,
      "grad_norm": 1.3582571744918823,
      "learning_rate": 2.586588141778372e-05,
      "loss": 1.0243,
      "step": 26810
    },
    {
      "epoch": 0.4150899593731863,
      "grad_norm": 1.2587673664093018,
      "learning_rate": 2.5864332942773525e-05,
      "loss": 0.976,
      "step": 26820
    },
    {
      "epoch": 0.41524472818727026,
      "grad_norm": 0.9990625977516174,
      "learning_rate": 2.5862784467763332e-05,
      "loss": 1.2395,
      "step": 26830
    },
    {
      "epoch": 0.4153994970013542,
      "grad_norm": 1.3821717500686646,
      "learning_rate": 2.5861235992753136e-05,
      "loss": 1.1096,
      "step": 26840
    },
    {
      "epoch": 0.4155542658154382,
      "grad_norm": 1.9367623329162598,
      "learning_rate": 2.5859687517742943e-05,
      "loss": 1.0764,
      "step": 26850
    },
    {
      "epoch": 0.41570903462952213,
      "grad_norm": 1.8660964965820312,
      "learning_rate": 2.5858139042732747e-05,
      "loss": 1.04,
      "step": 26860
    },
    {
      "epoch": 0.41586380344360613,
      "grad_norm": 1.8755857944488525,
      "learning_rate": 2.5856590567722554e-05,
      "loss": 1.2272,
      "step": 26870
    },
    {
      "epoch": 0.41601857225769007,
      "grad_norm": 1.7821333408355713,
      "learning_rate": 2.585504209271236e-05,
      "loss": 1.0095,
      "step": 26880
    },
    {
      "epoch": 0.41617334107177406,
      "grad_norm": 1.5760408639907837,
      "learning_rate": 2.5853493617702168e-05,
      "loss": 1.1156,
      "step": 26890
    },
    {
      "epoch": 0.416328109885858,
      "grad_norm": 1.5505707263946533,
      "learning_rate": 2.5851945142691972e-05,
      "loss": 1.1292,
      "step": 26900
    },
    {
      "epoch": 0.41648287869994194,
      "grad_norm": 2.2410340309143066,
      "learning_rate": 2.585039666768178e-05,
      "loss": 1.0328,
      "step": 26910
    },
    {
      "epoch": 0.41663764751402593,
      "grad_norm": 1.820648193359375,
      "learning_rate": 2.5848848192671583e-05,
      "loss": 1.1776,
      "step": 26920
    },
    {
      "epoch": 0.41679241632810987,
      "grad_norm": 1.523835301399231,
      "learning_rate": 2.584729971766139e-05,
      "loss": 1.1072,
      "step": 26930
    },
    {
      "epoch": 0.41694718514219387,
      "grad_norm": 1.5906835794448853,
      "learning_rate": 2.5845751242651197e-05,
      "loss": 1.0477,
      "step": 26940
    },
    {
      "epoch": 0.4171019539562778,
      "grad_norm": 1.311569094657898,
      "learning_rate": 2.5844202767641004e-05,
      "loss": 1.0868,
      "step": 26950
    },
    {
      "epoch": 0.4172567227703618,
      "grad_norm": 2.943808078765869,
      "learning_rate": 2.5842654292630808e-05,
      "loss": 1.214,
      "step": 26960
    },
    {
      "epoch": 0.41741149158444574,
      "grad_norm": 1.3624614477157593,
      "learning_rate": 2.5841105817620615e-05,
      "loss": 0.9022,
      "step": 26970
    },
    {
      "epoch": 0.4175662603985297,
      "grad_norm": 1.4191001653671265,
      "learning_rate": 2.583955734261042e-05,
      "loss": 1.086,
      "step": 26980
    },
    {
      "epoch": 0.41772102921261367,
      "grad_norm": 1.5003820657730103,
      "learning_rate": 2.5838008867600226e-05,
      "loss": 1.0403,
      "step": 26990
    },
    {
      "epoch": 0.4178757980266976,
      "grad_norm": 1.9014462232589722,
      "learning_rate": 2.583646039259003e-05,
      "loss": 1.0819,
      "step": 27000
    },
    {
      "epoch": 0.4180305668407816,
      "grad_norm": 1.162868857383728,
      "learning_rate": 2.583491191757984e-05,
      "loss": 0.8733,
      "step": 27010
    },
    {
      "epoch": 0.41818533565486554,
      "grad_norm": 2.1383893489837646,
      "learning_rate": 2.5833363442569644e-05,
      "loss": 1.0808,
      "step": 27020
    },
    {
      "epoch": 0.4183401044689495,
      "grad_norm": 1.4909623861312866,
      "learning_rate": 2.583181496755945e-05,
      "loss": 0.9066,
      "step": 27030
    },
    {
      "epoch": 0.4184948732830335,
      "grad_norm": 1.8961536884307861,
      "learning_rate": 2.5830266492549255e-05,
      "loss": 1.0234,
      "step": 27040
    },
    {
      "epoch": 0.4186496420971174,
      "grad_norm": 1.093312382698059,
      "learning_rate": 2.5828718017539062e-05,
      "loss": 1.2331,
      "step": 27050
    },
    {
      "epoch": 0.4188044109112014,
      "grad_norm": 1.6406395435333252,
      "learning_rate": 2.5827169542528866e-05,
      "loss": 1.0438,
      "step": 27060
    },
    {
      "epoch": 0.41895917972528535,
      "grad_norm": 1.4589029550552368,
      "learning_rate": 2.582562106751867e-05,
      "loss": 0.9162,
      "step": 27070
    },
    {
      "epoch": 0.41911394853936934,
      "grad_norm": 1.4286201000213623,
      "learning_rate": 2.582407259250848e-05,
      "loss": 0.9954,
      "step": 27080
    },
    {
      "epoch": 0.4192687173534533,
      "grad_norm": 1.6688200235366821,
      "learning_rate": 2.5822524117498284e-05,
      "loss": 0.9615,
      "step": 27090
    },
    {
      "epoch": 0.4194234861675372,
      "grad_norm": 2.0914180278778076,
      "learning_rate": 2.582097564248809e-05,
      "loss": 0.9526,
      "step": 27100
    },
    {
      "epoch": 0.4195782549816212,
      "grad_norm": 1.6361356973648071,
      "learning_rate": 2.5819427167477895e-05,
      "loss": 1.1935,
      "step": 27110
    },
    {
      "epoch": 0.41973302379570515,
      "grad_norm": 1.1146129369735718,
      "learning_rate": 2.5817878692467702e-05,
      "loss": 1.0395,
      "step": 27120
    },
    {
      "epoch": 0.41988779260978915,
      "grad_norm": 1.3836535215377808,
      "learning_rate": 2.5816330217457506e-05,
      "loss": 1.265,
      "step": 27130
    },
    {
      "epoch": 0.4200425614238731,
      "grad_norm": 1.539126992225647,
      "learning_rate": 2.5814781742447313e-05,
      "loss": 1.0801,
      "step": 27140
    },
    {
      "epoch": 0.4201973302379571,
      "grad_norm": 1.1576967239379883,
      "learning_rate": 2.581323326743712e-05,
      "loss": 1.1044,
      "step": 27150
    },
    {
      "epoch": 0.420352099052041,
      "grad_norm": 1.3113657236099243,
      "learning_rate": 2.5811684792426927e-05,
      "loss": 1.056,
      "step": 27160
    },
    {
      "epoch": 0.42050686786612496,
      "grad_norm": 2.0541911125183105,
      "learning_rate": 2.581013631741673e-05,
      "loss": 0.9381,
      "step": 27170
    },
    {
      "epoch": 0.42066163668020895,
      "grad_norm": 1.7601468563079834,
      "learning_rate": 2.5808587842406538e-05,
      "loss": 1.156,
      "step": 27180
    },
    {
      "epoch": 0.4208164054942929,
      "grad_norm": 1.440040111541748,
      "learning_rate": 2.5807039367396342e-05,
      "loss": 1.1156,
      "step": 27190
    },
    {
      "epoch": 0.4209711743083769,
      "grad_norm": 1.712169885635376,
      "learning_rate": 2.580549089238615e-05,
      "loss": 1.2567,
      "step": 27200
    },
    {
      "epoch": 0.4211259431224608,
      "grad_norm": 1.237377405166626,
      "learning_rate": 2.5803942417375953e-05,
      "loss": 0.8172,
      "step": 27210
    },
    {
      "epoch": 0.42128071193654476,
      "grad_norm": 1.6504883766174316,
      "learning_rate": 2.5802393942365763e-05,
      "loss": 1.13,
      "step": 27220
    },
    {
      "epoch": 0.42143548075062875,
      "grad_norm": 2.082831859588623,
      "learning_rate": 2.5800845467355567e-05,
      "loss": 1.1326,
      "step": 27230
    },
    {
      "epoch": 0.4215902495647127,
      "grad_norm": 1.6342031955718994,
      "learning_rate": 2.5799296992345374e-05,
      "loss": 1.3038,
      "step": 27240
    },
    {
      "epoch": 0.4217450183787967,
      "grad_norm": 1.5489327907562256,
      "learning_rate": 2.5797748517335178e-05,
      "loss": 1.0629,
      "step": 27250
    },
    {
      "epoch": 0.4218997871928806,
      "grad_norm": 1.767181158065796,
      "learning_rate": 2.5796200042324985e-05,
      "loss": 1.1825,
      "step": 27260
    },
    {
      "epoch": 0.4220545560069646,
      "grad_norm": 1.4345849752426147,
      "learning_rate": 2.579465156731479e-05,
      "loss": 1.0286,
      "step": 27270
    },
    {
      "epoch": 0.42220932482104856,
      "grad_norm": 1.7030659914016724,
      "learning_rate": 2.5793103092304596e-05,
      "loss": 0.9081,
      "step": 27280
    },
    {
      "epoch": 0.4223640936351325,
      "grad_norm": 2.0576813220977783,
      "learning_rate": 2.5791554617294403e-05,
      "loss": 0.835,
      "step": 27290
    },
    {
      "epoch": 0.4225188624492165,
      "grad_norm": 1.8756239414215088,
      "learning_rate": 2.579000614228421e-05,
      "loss": 1.0504,
      "step": 27300
    },
    {
      "epoch": 0.42267363126330043,
      "grad_norm": 1.5016063451766968,
      "learning_rate": 2.5788457667274014e-05,
      "loss": 1.2066,
      "step": 27310
    },
    {
      "epoch": 0.4228284000773844,
      "grad_norm": 1.5497921705245972,
      "learning_rate": 2.578690919226382e-05,
      "loss": 1.0577,
      "step": 27320
    },
    {
      "epoch": 0.42298316889146836,
      "grad_norm": 1.1287307739257812,
      "learning_rate": 2.5785360717253625e-05,
      "loss": 0.9013,
      "step": 27330
    },
    {
      "epoch": 0.42313793770555236,
      "grad_norm": 1.4975141286849976,
      "learning_rate": 2.578381224224343e-05,
      "loss": 1.2849,
      "step": 27340
    },
    {
      "epoch": 0.4232927065196363,
      "grad_norm": 1.216766357421875,
      "learning_rate": 2.578226376723324e-05,
      "loss": 0.9315,
      "step": 27350
    },
    {
      "epoch": 0.42344747533372024,
      "grad_norm": 1.850786805152893,
      "learning_rate": 2.5780715292223043e-05,
      "loss": 1.0479,
      "step": 27360
    },
    {
      "epoch": 0.42360224414780423,
      "grad_norm": 1.6166784763336182,
      "learning_rate": 2.577916681721285e-05,
      "loss": 0.9646,
      "step": 27370
    },
    {
      "epoch": 0.42375701296188817,
      "grad_norm": 1.1442241668701172,
      "learning_rate": 2.5777618342202654e-05,
      "loss": 0.9962,
      "step": 27380
    },
    {
      "epoch": 0.42391178177597216,
      "grad_norm": 1.897868037223816,
      "learning_rate": 2.577606986719246e-05,
      "loss": 1.2497,
      "step": 27390
    },
    {
      "epoch": 0.4240665505900561,
      "grad_norm": 1.2592532634735107,
      "learning_rate": 2.5774521392182265e-05,
      "loss": 1.0266,
      "step": 27400
    },
    {
      "epoch": 0.42422131940414004,
      "grad_norm": 1.7809033393859863,
      "learning_rate": 2.577297291717207e-05,
      "loss": 1.1416,
      "step": 27410
    },
    {
      "epoch": 0.42437608821822403,
      "grad_norm": 2.0150146484375,
      "learning_rate": 2.577142444216188e-05,
      "loss": 1.0756,
      "step": 27420
    },
    {
      "epoch": 0.424530857032308,
      "grad_norm": 1.5845621824264526,
      "learning_rate": 2.5769875967151686e-05,
      "loss": 1.1015,
      "step": 27430
    },
    {
      "epoch": 0.42468562584639197,
      "grad_norm": 0.9936665892601013,
      "learning_rate": 2.576832749214149e-05,
      "loss": 0.991,
      "step": 27440
    },
    {
      "epoch": 0.4248403946604759,
      "grad_norm": NaN,
      "learning_rate": 2.5766779017131297e-05,
      "loss": 1.0411,
      "step": 27450
    },
    {
      "epoch": 0.4249951634745599,
      "grad_norm": 1.5385611057281494,
      "learning_rate": 2.5765385389622123e-05,
      "loss": 1.1651,
      "step": 27460
    },
    {
      "epoch": 0.42514993228864384,
      "grad_norm": 1.382619857788086,
      "learning_rate": 2.5763836914611927e-05,
      "loss": 1.0766,
      "step": 27470
    },
    {
      "epoch": 0.4253047011027278,
      "grad_norm": 1.3195151090621948,
      "learning_rate": 2.5762288439601734e-05,
      "loss": 1.1276,
      "step": 27480
    },
    {
      "epoch": 0.42545946991681177,
      "grad_norm": 1.4749351739883423,
      "learning_rate": 2.5760739964591538e-05,
      "loss": 0.9397,
      "step": 27490
    },
    {
      "epoch": 0.4256142387308957,
      "grad_norm": 1.1635452508926392,
      "learning_rate": 2.5759191489581345e-05,
      "loss": 1.0593,
      "step": 27500
    },
    {
      "epoch": 0.4257690075449797,
      "grad_norm": 1.310848355293274,
      "learning_rate": 2.5757643014571152e-05,
      "loss": 1.0283,
      "step": 27510
    },
    {
      "epoch": 0.42592377635906364,
      "grad_norm": 1.4026073217391968,
      "learning_rate": 2.5756094539560956e-05,
      "loss": 1.2194,
      "step": 27520
    },
    {
      "epoch": 0.42607854517314764,
      "grad_norm": 1.661438226699829,
      "learning_rate": 2.5754546064550763e-05,
      "loss": 1.1618,
      "step": 27530
    },
    {
      "epoch": 0.4262333139872316,
      "grad_norm": 1.504113793373108,
      "learning_rate": 2.5752997589540566e-05,
      "loss": 0.9879,
      "step": 27540
    },
    {
      "epoch": 0.4263880828013155,
      "grad_norm": 1.8853602409362793,
      "learning_rate": 2.5751449114530374e-05,
      "loss": 1.2741,
      "step": 27550
    },
    {
      "epoch": 0.4265428516153995,
      "grad_norm": 1.1620008945465088,
      "learning_rate": 2.5749900639520177e-05,
      "loss": 1.0956,
      "step": 27560
    },
    {
      "epoch": 0.42669762042948345,
      "grad_norm": 1.0000613927841187,
      "learning_rate": 2.5748352164509985e-05,
      "loss": 0.9542,
      "step": 27570
    },
    {
      "epoch": 0.42685238924356744,
      "grad_norm": 2.062711000442505,
      "learning_rate": 2.574680368949979e-05,
      "loss": 1.121,
      "step": 27580
    },
    {
      "epoch": 0.4270071580576514,
      "grad_norm": 1.2640740871429443,
      "learning_rate": 2.57452552144896e-05,
      "loss": 1.0461,
      "step": 27590
    },
    {
      "epoch": 0.4271619268717353,
      "grad_norm": 1.4477695226669312,
      "learning_rate": 2.5743706739479403e-05,
      "loss": 1.0838,
      "step": 27600
    },
    {
      "epoch": 0.4273166956858193,
      "grad_norm": 1.5808818340301514,
      "learning_rate": 2.574215826446921e-05,
      "loss": 0.9533,
      "step": 27610
    },
    {
      "epoch": 0.42747146449990325,
      "grad_norm": 1.4950953722000122,
      "learning_rate": 2.5740609789459013e-05,
      "loss": 1.1761,
      "step": 27620
    },
    {
      "epoch": 0.42762623331398725,
      "grad_norm": 1.0409117937088013,
      "learning_rate": 2.573906131444882e-05,
      "loss": 0.8963,
      "step": 27630
    },
    {
      "epoch": 0.4277810021280712,
      "grad_norm": 1.1516584157943726,
      "learning_rate": 2.5737512839438628e-05,
      "loss": 0.908,
      "step": 27640
    },
    {
      "epoch": 0.4279357709421552,
      "grad_norm": 1.5664137601852417,
      "learning_rate": 2.5735964364428435e-05,
      "loss": 0.9836,
      "step": 27650
    },
    {
      "epoch": 0.4280905397562391,
      "grad_norm": 1.1466704607009888,
      "learning_rate": 2.573441588941824e-05,
      "loss": 0.931,
      "step": 27660
    },
    {
      "epoch": 0.42824530857032306,
      "grad_norm": 1.9097238779067993,
      "learning_rate": 2.5732867414408046e-05,
      "loss": 1.0983,
      "step": 27670
    },
    {
      "epoch": 0.42840007738440705,
      "grad_norm": 1.3631819486618042,
      "learning_rate": 2.573131893939785e-05,
      "loss": 0.9994,
      "step": 27680
    },
    {
      "epoch": 0.428554846198491,
      "grad_norm": 1.0776405334472656,
      "learning_rate": 2.5729770464387657e-05,
      "loss": 1.2734,
      "step": 27690
    },
    {
      "epoch": 0.428709615012575,
      "grad_norm": 1.5187941789627075,
      "learning_rate": 2.572822198937746e-05,
      "loss": 0.9723,
      "step": 27700
    },
    {
      "epoch": 0.4288643838266589,
      "grad_norm": 1.7353529930114746,
      "learning_rate": 2.572667351436727e-05,
      "loss": 1.1866,
      "step": 27710
    },
    {
      "epoch": 0.4290191526407429,
      "grad_norm": 2.5215163230895996,
      "learning_rate": 2.5725125039357075e-05,
      "loss": 1.0454,
      "step": 27720
    },
    {
      "epoch": 0.42917392145482686,
      "grad_norm": 1.6816786527633667,
      "learning_rate": 2.5723576564346882e-05,
      "loss": 1.0777,
      "step": 27730
    },
    {
      "epoch": 0.4293286902689108,
      "grad_norm": 2.1797056198120117,
      "learning_rate": 2.5722028089336686e-05,
      "loss": 0.8532,
      "step": 27740
    },
    {
      "epoch": 0.4294834590829948,
      "grad_norm": 1.3639923334121704,
      "learning_rate": 2.5720479614326493e-05,
      "loss": 1.0208,
      "step": 27750
    },
    {
      "epoch": 0.42963822789707873,
      "grad_norm": 2.062683582305908,
      "learning_rate": 2.5718931139316296e-05,
      "loss": 1.0982,
      "step": 27760
    },
    {
      "epoch": 0.4297929967111627,
      "grad_norm": 1.7133268117904663,
      "learning_rate": 2.57173826643061e-05,
      "loss": 1.0793,
      "step": 27770
    },
    {
      "epoch": 0.42994776552524666,
      "grad_norm": 2.117330551147461,
      "learning_rate": 2.571583418929591e-05,
      "loss": 1.1294,
      "step": 27780
    },
    {
      "epoch": 0.4301025343393306,
      "grad_norm": 1.1348545551300049,
      "learning_rate": 2.5714285714285714e-05,
      "loss": 0.9639,
      "step": 27790
    },
    {
      "epoch": 0.4302573031534146,
      "grad_norm": 1.42001473903656,
      "learning_rate": 2.571273723927552e-05,
      "loss": 1.002,
      "step": 27800
    },
    {
      "epoch": 0.43041207196749853,
      "grad_norm": 1.0864050388336182,
      "learning_rate": 2.5711188764265325e-05,
      "loss": 1.1498,
      "step": 27810
    },
    {
      "epoch": 0.4305668407815825,
      "grad_norm": 1.7969417572021484,
      "learning_rate": 2.5709640289255132e-05,
      "loss": 1.1034,
      "step": 27820
    },
    {
      "epoch": 0.43072160959566647,
      "grad_norm": 1.4200514554977417,
      "learning_rate": 2.5708091814244936e-05,
      "loss": 1.0291,
      "step": 27830
    },
    {
      "epoch": 0.43087637840975046,
      "grad_norm": 1.4477312564849854,
      "learning_rate": 2.5706543339234743e-05,
      "loss": 1.1643,
      "step": 27840
    },
    {
      "epoch": 0.4310311472238344,
      "grad_norm": 1.6666944026947021,
      "learning_rate": 2.570499486422455e-05,
      "loss": 1.0053,
      "step": 27850
    },
    {
      "epoch": 0.43118591603791834,
      "grad_norm": 1.373800277709961,
      "learning_rate": 2.5703446389214358e-05,
      "loss": 1.0657,
      "step": 27860
    },
    {
      "epoch": 0.43134068485200233,
      "grad_norm": 1.6393426656723022,
      "learning_rate": 2.570189791420416e-05,
      "loss": 1.2215,
      "step": 27870
    },
    {
      "epoch": 0.43149545366608627,
      "grad_norm": 1.4292811155319214,
      "learning_rate": 2.570034943919397e-05,
      "loss": 1.0311,
      "step": 27880
    },
    {
      "epoch": 0.43165022248017026,
      "grad_norm": 1.2560473680496216,
      "learning_rate": 2.5698800964183772e-05,
      "loss": 1.0552,
      "step": 27890
    },
    {
      "epoch": 0.4318049912942542,
      "grad_norm": 1.408990740776062,
      "learning_rate": 2.569725248917358e-05,
      "loss": 1.1263,
      "step": 27900
    },
    {
      "epoch": 0.43195976010833814,
      "grad_norm": 1.571813702583313,
      "learning_rate": 2.5695704014163383e-05,
      "loss": 1.1124,
      "step": 27910
    },
    {
      "epoch": 0.43211452892242214,
      "grad_norm": 1.900834083557129,
      "learning_rate": 2.5694155539153194e-05,
      "loss": 1.1022,
      "step": 27920
    },
    {
      "epoch": 0.4322692977365061,
      "grad_norm": 1.607964038848877,
      "learning_rate": 2.5692607064142997e-05,
      "loss": 1.0434,
      "step": 27930
    },
    {
      "epoch": 0.43242406655059007,
      "grad_norm": 1.8003602027893066,
      "learning_rate": 2.5691058589132805e-05,
      "loss": 1.0406,
      "step": 27940
    },
    {
      "epoch": 0.432578835364674,
      "grad_norm": 2.051976203918457,
      "learning_rate": 2.568951011412261e-05,
      "loss": 0.9762,
      "step": 27950
    },
    {
      "epoch": 0.432733604178758,
      "grad_norm": 1.3383382558822632,
      "learning_rate": 2.5687961639112415e-05,
      "loss": 1.0586,
      "step": 27960
    },
    {
      "epoch": 0.43288837299284194,
      "grad_norm": 1.223507285118103,
      "learning_rate": 2.568641316410222e-05,
      "loss": 0.975,
      "step": 27970
    },
    {
      "epoch": 0.4330431418069259,
      "grad_norm": 1.5762114524841309,
      "learning_rate": 2.5684864689092026e-05,
      "loss": 1.082,
      "step": 27980
    },
    {
      "epoch": 0.4331979106210099,
      "grad_norm": 1.7629523277282715,
      "learning_rate": 2.5683316214081834e-05,
      "loss": 1.1351,
      "step": 27990
    },
    {
      "epoch": 0.4333526794350938,
      "grad_norm": 1.739243984222412,
      "learning_rate": 2.568176773907164e-05,
      "loss": 1.0381,
      "step": 28000
    },
    {
      "epoch": 0.4335074482491778,
      "grad_norm": 1.8453178405761719,
      "learning_rate": 2.5680219264061444e-05,
      "loss": 0.9992,
      "step": 28010
    },
    {
      "epoch": 0.43366221706326175,
      "grad_norm": 1.2788937091827393,
      "learning_rate": 2.5678670789051248e-05,
      "loss": 1.0847,
      "step": 28020
    },
    {
      "epoch": 0.43381698587734574,
      "grad_norm": 2.0313613414764404,
      "learning_rate": 2.5677122314041055e-05,
      "loss": 1.1141,
      "step": 28030
    },
    {
      "epoch": 0.4339717546914297,
      "grad_norm": 1.6751947402954102,
      "learning_rate": 2.567557383903086e-05,
      "loss": 1.043,
      "step": 28040
    },
    {
      "epoch": 0.4341265235055136,
      "grad_norm": 1.5991129875183105,
      "learning_rate": 2.567402536402067e-05,
      "loss": 1.2106,
      "step": 28050
    },
    {
      "epoch": 0.4342812923195976,
      "grad_norm": 2.065493583679199,
      "learning_rate": 2.5672476889010473e-05,
      "loss": 1.0495,
      "step": 28060
    },
    {
      "epoch": 0.43443606113368155,
      "grad_norm": 1.65269935131073,
      "learning_rate": 2.567092841400028e-05,
      "loss": 1.1358,
      "step": 28070
    },
    {
      "epoch": 0.43459082994776554,
      "grad_norm": 1.3389892578125,
      "learning_rate": 2.5669379938990084e-05,
      "loss": 1.0449,
      "step": 28080
    },
    {
      "epoch": 0.4347455987618495,
      "grad_norm": 2.0168297290802,
      "learning_rate": 2.566783146397989e-05,
      "loss": 1.1002,
      "step": 28090
    },
    {
      "epoch": 0.4349003675759334,
      "grad_norm": 1.2727471590042114,
      "learning_rate": 2.5666282988969695e-05,
      "loss": 1.0431,
      "step": 28100
    },
    {
      "epoch": 0.4350551363900174,
      "grad_norm": 1.6593761444091797,
      "learning_rate": 2.5664734513959502e-05,
      "loss": 1.0539,
      "step": 28110
    },
    {
      "epoch": 0.43520990520410135,
      "grad_norm": 1.0708740949630737,
      "learning_rate": 2.566318603894931e-05,
      "loss": 0.9783,
      "step": 28120
    },
    {
      "epoch": 0.43536467401818535,
      "grad_norm": 1.5788989067077637,
      "learning_rate": 2.5661637563939117e-05,
      "loss": 1.0334,
      "step": 28130
    },
    {
      "epoch": 0.4355194428322693,
      "grad_norm": 1.5462377071380615,
      "learning_rate": 2.566008908892892e-05,
      "loss": 1.0645,
      "step": 28140
    },
    {
      "epoch": 0.4356742116463533,
      "grad_norm": 1.0692381858825684,
      "learning_rate": 2.5658540613918727e-05,
      "loss": 0.9629,
      "step": 28150
    },
    {
      "epoch": 0.4358289804604372,
      "grad_norm": 1.7960690259933472,
      "learning_rate": 2.565699213890853e-05,
      "loss": 0.857,
      "step": 28160
    },
    {
      "epoch": 0.43598374927452116,
      "grad_norm": 1.4136403799057007,
      "learning_rate": 2.5655443663898338e-05,
      "loss": 1.088,
      "step": 28170
    },
    {
      "epoch": 0.43613851808860515,
      "grad_norm": 1.130550742149353,
      "learning_rate": 2.5653895188888142e-05,
      "loss": 1.1314,
      "step": 28180
    },
    {
      "epoch": 0.4362932869026891,
      "grad_norm": 2.0915544033050537,
      "learning_rate": 2.5652346713877953e-05,
      "loss": 1.3932,
      "step": 28190
    },
    {
      "epoch": 0.4364480557167731,
      "grad_norm": 1.3226619958877563,
      "learning_rate": 2.5650798238867756e-05,
      "loss": 1.0358,
      "step": 28200
    },
    {
      "epoch": 0.436602824530857,
      "grad_norm": 1.3454599380493164,
      "learning_rate": 2.5649249763857563e-05,
      "loss": 0.9933,
      "step": 28210
    },
    {
      "epoch": 0.436757593344941,
      "grad_norm": 1.552428126335144,
      "learning_rate": 2.5647701288847367e-05,
      "loss": 1.235,
      "step": 28220
    },
    {
      "epoch": 0.43691236215902496,
      "grad_norm": 1.161826491355896,
      "learning_rate": 2.5646152813837174e-05,
      "loss": 1.0213,
      "step": 28230
    },
    {
      "epoch": 0.4370671309731089,
      "grad_norm": 1.6164348125457764,
      "learning_rate": 2.5644604338826978e-05,
      "loss": 0.9934,
      "step": 28240
    },
    {
      "epoch": 0.4372218997871929,
      "grad_norm": 1.4811352491378784,
      "learning_rate": 2.5643055863816785e-05,
      "loss": 1.1663,
      "step": 28250
    },
    {
      "epoch": 0.43737666860127683,
      "grad_norm": 1.4194482564926147,
      "learning_rate": 2.5641507388806592e-05,
      "loss": 1.0161,
      "step": 28260
    },
    {
      "epoch": 0.4375314374153608,
      "grad_norm": 1.4496819972991943,
      "learning_rate": 2.5639958913796396e-05,
      "loss": 1.2118,
      "step": 28270
    },
    {
      "epoch": 0.43768620622944476,
      "grad_norm": 1.39484441280365,
      "learning_rate": 2.5638410438786203e-05,
      "loss": 0.9936,
      "step": 28280
    },
    {
      "epoch": 0.4378409750435287,
      "grad_norm": 1.3125345706939697,
      "learning_rate": 2.5636861963776007e-05,
      "loss": 1.1645,
      "step": 28290
    },
    {
      "epoch": 0.4379957438576127,
      "grad_norm": 1.1670022010803223,
      "learning_rate": 2.5635313488765814e-05,
      "loss": 1.0453,
      "step": 28300
    },
    {
      "epoch": 0.43815051267169663,
      "grad_norm": 1.7920212745666504,
      "learning_rate": 2.5633765013755618e-05,
      "loss": 1.0537,
      "step": 28310
    },
    {
      "epoch": 0.43830528148578063,
      "grad_norm": 1.5413501262664795,
      "learning_rate": 2.5632216538745425e-05,
      "loss": 0.9409,
      "step": 28320
    },
    {
      "epoch": 0.43846005029986457,
      "grad_norm": 0.9996784925460815,
      "learning_rate": 2.5630668063735232e-05,
      "loss": 0.9592,
      "step": 28330
    },
    {
      "epoch": 0.43861481911394856,
      "grad_norm": 1.5709606409072876,
      "learning_rate": 2.562911958872504e-05,
      "loss": 0.9907,
      "step": 28340
    },
    {
      "epoch": 0.4387695879280325,
      "grad_norm": 1.4117567539215088,
      "learning_rate": 2.5627571113714843e-05,
      "loss": 1.1115,
      "step": 28350
    },
    {
      "epoch": 0.43892435674211644,
      "grad_norm": 1.5523791313171387,
      "learning_rate": 2.562602263870465e-05,
      "loss": 0.9366,
      "step": 28360
    },
    {
      "epoch": 0.43907912555620043,
      "grad_norm": 1.7810920476913452,
      "learning_rate": 2.5624474163694454e-05,
      "loss": 1.1089,
      "step": 28370
    },
    {
      "epoch": 0.4392338943702844,
      "grad_norm": 1.5745689868927002,
      "learning_rate": 2.562292568868426e-05,
      "loss": 1.0994,
      "step": 28380
    },
    {
      "epoch": 0.43938866318436837,
      "grad_norm": 2.1674768924713135,
      "learning_rate": 2.5621377213674065e-05,
      "loss": 1.0293,
      "step": 28390
    },
    {
      "epoch": 0.4395434319984523,
      "grad_norm": 1.4624863862991333,
      "learning_rate": 2.5619828738663875e-05,
      "loss": 1.2239,
      "step": 28400
    },
    {
      "epoch": 0.4396982008125363,
      "grad_norm": 1.4512187242507935,
      "learning_rate": 2.561828026365368e-05,
      "loss": 1.2265,
      "step": 28410
    },
    {
      "epoch": 0.43985296962662024,
      "grad_norm": 1.1520849466323853,
      "learning_rate": 2.5616731788643486e-05,
      "loss": 1.0601,
      "step": 28420
    },
    {
      "epoch": 0.4400077384407042,
      "grad_norm": 1.5479254722595215,
      "learning_rate": 2.561518331363329e-05,
      "loss": 0.9231,
      "step": 28430
    },
    {
      "epoch": 0.44016250725478817,
      "grad_norm": 1.2225638628005981,
      "learning_rate": 2.5613634838623097e-05,
      "loss": 1.2539,
      "step": 28440
    },
    {
      "epoch": 0.4403172760688721,
      "grad_norm": 2.026557683944702,
      "learning_rate": 2.56120863636129e-05,
      "loss": 1.0802,
      "step": 28450
    },
    {
      "epoch": 0.4404720448829561,
      "grad_norm": 1.4895321130752563,
      "learning_rate": 2.5610537888602708e-05,
      "loss": 1.126,
      "step": 28460
    },
    {
      "epoch": 0.44062681369704004,
      "grad_norm": 1.2587772607803345,
      "learning_rate": 2.5608989413592515e-05,
      "loss": 0.9848,
      "step": 28470
    },
    {
      "epoch": 0.440781582511124,
      "grad_norm": 1.6271835565567017,
      "learning_rate": 2.5607440938582322e-05,
      "loss": 0.9808,
      "step": 28480
    },
    {
      "epoch": 0.440936351325208,
      "grad_norm": 1.2370789051055908,
      "learning_rate": 2.5605892463572126e-05,
      "loss": 1.1068,
      "step": 28490
    },
    {
      "epoch": 0.4410911201392919,
      "grad_norm": 1.343829870223999,
      "learning_rate": 2.5604343988561933e-05,
      "loss": 1.2713,
      "step": 28500
    },
    {
      "epoch": 0.4412458889533759,
      "grad_norm": 1.3569740056991577,
      "learning_rate": 2.5602795513551737e-05,
      "loss": 1.043,
      "step": 28510
    },
    {
      "epoch": 0.44140065776745985,
      "grad_norm": 1.6128953695297241,
      "learning_rate": 2.560124703854154e-05,
      "loss": 0.9952,
      "step": 28520
    },
    {
      "epoch": 0.44155542658154384,
      "grad_norm": 1.374167561531067,
      "learning_rate": 2.559969856353135e-05,
      "loss": 1.3105,
      "step": 28530
    },
    {
      "epoch": 0.4417101953956278,
      "grad_norm": 1.6171104907989502,
      "learning_rate": 2.5598150088521155e-05,
      "loss": 1.1296,
      "step": 28540
    },
    {
      "epoch": 0.4418649642097117,
      "grad_norm": 1.9217473268508911,
      "learning_rate": 2.5596601613510962e-05,
      "loss": 1.1172,
      "step": 28550
    },
    {
      "epoch": 0.4420197330237957,
      "grad_norm": 1.5503445863723755,
      "learning_rate": 2.5595053138500766e-05,
      "loss": 1.0217,
      "step": 28560
    },
    {
      "epoch": 0.44217450183787965,
      "grad_norm": 1.2247496843338013,
      "learning_rate": 2.5593504663490573e-05,
      "loss": 1.0435,
      "step": 28570
    },
    {
      "epoch": 0.44232927065196365,
      "grad_norm": 2.108018636703491,
      "learning_rate": 2.5591956188480377e-05,
      "loss": 1.0006,
      "step": 28580
    },
    {
      "epoch": 0.4424840394660476,
      "grad_norm": 1.9701075553894043,
      "learning_rate": 2.5590407713470184e-05,
      "loss": 1.0136,
      "step": 28590
    },
    {
      "epoch": 0.4426388082801316,
      "grad_norm": 1.5856777429580688,
      "learning_rate": 2.558885923845999e-05,
      "loss": 1.1137,
      "step": 28600
    },
    {
      "epoch": 0.4427935770942155,
      "grad_norm": 2.409618854522705,
      "learning_rate": 2.5587310763449798e-05,
      "loss": 1.0608,
      "step": 28610
    },
    {
      "epoch": 0.44294834590829946,
      "grad_norm": 1.6258151531219482,
      "learning_rate": 2.5585762288439602e-05,
      "loss": 1.2456,
      "step": 28620
    },
    {
      "epoch": 0.44310311472238345,
      "grad_norm": 1.2646868228912354,
      "learning_rate": 2.558421381342941e-05,
      "loss": 1.0898,
      "step": 28630
    },
    {
      "epoch": 0.4432578835364674,
      "grad_norm": 1.4028087854385376,
      "learning_rate": 2.5582665338419213e-05,
      "loss": 1.0844,
      "step": 28640
    },
    {
      "epoch": 0.4434126523505514,
      "grad_norm": 1.6015805006027222,
      "learning_rate": 2.558111686340902e-05,
      "loss": 0.9865,
      "step": 28650
    },
    {
      "epoch": 0.4435674211646353,
      "grad_norm": 1.503288984298706,
      "learning_rate": 2.5579568388398824e-05,
      "loss": 1.0689,
      "step": 28660
    },
    {
      "epoch": 0.44372218997871926,
      "grad_norm": 1.488264799118042,
      "learning_rate": 2.5578019913388634e-05,
      "loss": 1.0302,
      "step": 28670
    },
    {
      "epoch": 0.44387695879280326,
      "grad_norm": 1.5461708307266235,
      "learning_rate": 2.5576471438378438e-05,
      "loss": 1.0042,
      "step": 28680
    },
    {
      "epoch": 0.4440317276068872,
      "grad_norm": 1.5194294452667236,
      "learning_rate": 2.5574922963368245e-05,
      "loss": 1.1918,
      "step": 28690
    },
    {
      "epoch": 0.4441864964209712,
      "grad_norm": 1.374315857887268,
      "learning_rate": 2.557337448835805e-05,
      "loss": 0.9821,
      "step": 28700
    },
    {
      "epoch": 0.4443412652350551,
      "grad_norm": 1.8127000331878662,
      "learning_rate": 2.5571826013347856e-05,
      "loss": 0.9162,
      "step": 28710
    },
    {
      "epoch": 0.4444960340491391,
      "grad_norm": 1.4257543087005615,
      "learning_rate": 2.557027753833766e-05,
      "loss": 1.126,
      "step": 28720
    },
    {
      "epoch": 0.44465080286322306,
      "grad_norm": 1.7259926795959473,
      "learning_rate": 2.5568729063327467e-05,
      "loss": 1.0823,
      "step": 28730
    },
    {
      "epoch": 0.444805571677307,
      "grad_norm": 2.115638017654419,
      "learning_rate": 2.5567180588317274e-05,
      "loss": 1.0481,
      "step": 28740
    },
    {
      "epoch": 0.444960340491391,
      "grad_norm": 1.4192876815795898,
      "learning_rate": 2.556563211330708e-05,
      "loss": 1.001,
      "step": 28750
    },
    {
      "epoch": 0.44511510930547493,
      "grad_norm": 1.4609583616256714,
      "learning_rate": 2.5564083638296885e-05,
      "loss": 1.1608,
      "step": 28760
    },
    {
      "epoch": 0.4452698781195589,
      "grad_norm": 1.502788782119751,
      "learning_rate": 2.556253516328669e-05,
      "loss": 0.9854,
      "step": 28770
    },
    {
      "epoch": 0.44542464693364286,
      "grad_norm": 1.9754273891448975,
      "learning_rate": 2.5560986688276496e-05,
      "loss": 1.1606,
      "step": 28780
    },
    {
      "epoch": 0.44557941574772686,
      "grad_norm": 1.6158276796340942,
      "learning_rate": 2.55594382132663e-05,
      "loss": 0.9282,
      "step": 28790
    },
    {
      "epoch": 0.4457341845618108,
      "grad_norm": 1.6116173267364502,
      "learning_rate": 2.5557889738256107e-05,
      "loss": 1.1157,
      "step": 28800
    },
    {
      "epoch": 0.44588895337589474,
      "grad_norm": 1.8259472846984863,
      "learning_rate": 2.5556341263245914e-05,
      "loss": 1.0576,
      "step": 28810
    },
    {
      "epoch": 0.44604372218997873,
      "grad_norm": 1.2776744365692139,
      "learning_rate": 2.555479278823572e-05,
      "loss": 1.0184,
      "step": 28820
    },
    {
      "epoch": 0.44619849100406267,
      "grad_norm": 1.208901047706604,
      "learning_rate": 2.5553244313225525e-05,
      "loss": 0.8679,
      "step": 28830
    },
    {
      "epoch": 0.44635325981814666,
      "grad_norm": 1.4670487642288208,
      "learning_rate": 2.5551695838215332e-05,
      "loss": 1.0973,
      "step": 28840
    },
    {
      "epoch": 0.4465080286322306,
      "grad_norm": 1.3929505348205566,
      "learning_rate": 2.5550147363205136e-05,
      "loss": 1.2907,
      "step": 28850
    },
    {
      "epoch": 0.44666279744631454,
      "grad_norm": 1.1946210861206055,
      "learning_rate": 2.5548598888194943e-05,
      "loss": 1.0002,
      "step": 28860
    },
    {
      "epoch": 0.44681756626039854,
      "grad_norm": 1.227366328239441,
      "learning_rate": 2.5547050413184747e-05,
      "loss": 1.0236,
      "step": 28870
    },
    {
      "epoch": 0.4469723350744825,
      "grad_norm": 2.0041871070861816,
      "learning_rate": 2.5545501938174557e-05,
      "loss": 0.9976,
      "step": 28880
    },
    {
      "epoch": 0.44712710388856647,
      "grad_norm": 2.896242618560791,
      "learning_rate": 2.554395346316436e-05,
      "loss": 1.1128,
      "step": 28890
    },
    {
      "epoch": 0.4472818727026504,
      "grad_norm": 1.875746726989746,
      "learning_rate": 2.5542404988154168e-05,
      "loss": 1.1258,
      "step": 28900
    },
    {
      "epoch": 0.4474366415167344,
      "grad_norm": 1.8135005235671997,
      "learning_rate": 2.5540856513143972e-05,
      "loss": 1.0249,
      "step": 28910
    },
    {
      "epoch": 0.44759141033081834,
      "grad_norm": 1.5654147863388062,
      "learning_rate": 2.553930803813378e-05,
      "loss": 1.1274,
      "step": 28920
    },
    {
      "epoch": 0.4477461791449023,
      "grad_norm": 1.6072336435317993,
      "learning_rate": 2.5537759563123583e-05,
      "loss": 1.1106,
      "step": 28930
    },
    {
      "epoch": 0.4479009479589863,
      "grad_norm": 1.19992995262146,
      "learning_rate": 2.5536211088113393e-05,
      "loss": 1.0631,
      "step": 28940
    },
    {
      "epoch": 0.4480557167730702,
      "grad_norm": 1.7550060749053955,
      "learning_rate": 2.5534662613103197e-05,
      "loss": 1.0163,
      "step": 28950
    },
    {
      "epoch": 0.4482104855871542,
      "grad_norm": 1.5487309694290161,
      "learning_rate": 2.5533114138093004e-05,
      "loss": 1.0345,
      "step": 28960
    },
    {
      "epoch": 0.44836525440123814,
      "grad_norm": 1.2746978998184204,
      "learning_rate": 2.5531565663082808e-05,
      "loss": 0.8787,
      "step": 28970
    },
    {
      "epoch": 0.44852002321532214,
      "grad_norm": 1.7287424802780151,
      "learning_rate": 2.5530017188072615e-05,
      "loss": 1.084,
      "step": 28980
    },
    {
      "epoch": 0.4486747920294061,
      "grad_norm": 1.6303901672363281,
      "learning_rate": 2.552846871306242e-05,
      "loss": 1.1639,
      "step": 28990
    },
    {
      "epoch": 0.44882956084349,
      "grad_norm": 1.4023030996322632,
      "learning_rate": 2.5526920238052226e-05,
      "loss": 1.0124,
      "step": 29000
    },
    {
      "epoch": 0.448984329657574,
      "grad_norm": 1.3775670528411865,
      "learning_rate": 2.5525371763042033e-05,
      "loss": 1.1773,
      "step": 29010
    },
    {
      "epoch": 0.44913909847165795,
      "grad_norm": 2.1209771633148193,
      "learning_rate": 2.552382328803184e-05,
      "loss": 1.1919,
      "step": 29020
    },
    {
      "epoch": 0.44929386728574194,
      "grad_norm": 2.051520347595215,
      "learning_rate": 2.5522274813021644e-05,
      "loss": 1.1022,
      "step": 29030
    },
    {
      "epoch": 0.4494486360998259,
      "grad_norm": 1.3162251710891724,
      "learning_rate": 2.5520726338011448e-05,
      "loss": 1.1288,
      "step": 29040
    },
    {
      "epoch": 0.4496034049139098,
      "grad_norm": 1.0679124593734741,
      "learning_rate": 2.5519177863001255e-05,
      "loss": 0.9909,
      "step": 29050
    },
    {
      "epoch": 0.4497581737279938,
      "grad_norm": 1.7103872299194336,
      "learning_rate": 2.551762938799106e-05,
      "loss": 1.3298,
      "step": 29060
    },
    {
      "epoch": 0.44991294254207775,
      "grad_norm": 1.3390305042266846,
      "learning_rate": 2.5516080912980866e-05,
      "loss": 1.0625,
      "step": 29070
    },
    {
      "epoch": 0.45006771135616175,
      "grad_norm": 1.5726990699768066,
      "learning_rate": 2.5514532437970673e-05,
      "loss": 1.0403,
      "step": 29080
    },
    {
      "epoch": 0.4502224801702457,
      "grad_norm": 1.4179999828338623,
      "learning_rate": 2.551298396296048e-05,
      "loss": 1.0311,
      "step": 29090
    },
    {
      "epoch": 0.4503772489843297,
      "grad_norm": 1.1066854000091553,
      "learning_rate": 2.5511435487950284e-05,
      "loss": 1.0209,
      "step": 29100
    },
    {
      "epoch": 0.4505320177984136,
      "grad_norm": 1.4663115739822388,
      "learning_rate": 2.550988701294009e-05,
      "loss": 1.0201,
      "step": 29110
    },
    {
      "epoch": 0.45068678661249756,
      "grad_norm": 1.5743244886398315,
      "learning_rate": 2.5508338537929895e-05,
      "loss": 1.1604,
      "step": 29120
    },
    {
      "epoch": 0.45084155542658155,
      "grad_norm": 1.6828086376190186,
      "learning_rate": 2.55067900629197e-05,
      "loss": 1.0264,
      "step": 29130
    },
    {
      "epoch": 0.4509963242406655,
      "grad_norm": 1.4519639015197754,
      "learning_rate": 2.5505241587909505e-05,
      "loss": 1.1907,
      "step": 29140
    },
    {
      "epoch": 0.4511510930547495,
      "grad_norm": 1.4040186405181885,
      "learning_rate": 2.5503693112899316e-05,
      "loss": 0.9497,
      "step": 29150
    },
    {
      "epoch": 0.4513058618688334,
      "grad_norm": 1.3431117534637451,
      "learning_rate": 2.550214463788912e-05,
      "loss": 1.0165,
      "step": 29160
    },
    {
      "epoch": 0.4514606306829174,
      "grad_norm": 1.2217837572097778,
      "learning_rate": 2.5500596162878927e-05,
      "loss": 1.0162,
      "step": 29170
    },
    {
      "epoch": 0.45161539949700136,
      "grad_norm": 1.3579689264297485,
      "learning_rate": 2.549904768786873e-05,
      "loss": 1.1242,
      "step": 29180
    },
    {
      "epoch": 0.4517701683110853,
      "grad_norm": 1.731400489807129,
      "learning_rate": 2.5497499212858538e-05,
      "loss": 1.265,
      "step": 29190
    },
    {
      "epoch": 0.4519249371251693,
      "grad_norm": 1.3945235013961792,
      "learning_rate": 2.549595073784834e-05,
      "loss": 1.1219,
      "step": 29200
    },
    {
      "epoch": 0.45207970593925323,
      "grad_norm": 1.4474356174468994,
      "learning_rate": 2.549440226283815e-05,
      "loss": 1.3783,
      "step": 29210
    },
    {
      "epoch": 0.4522344747533372,
      "grad_norm": 1.5960696935653687,
      "learning_rate": 2.5492853787827956e-05,
      "loss": 0.99,
      "step": 29220
    },
    {
      "epoch": 0.45238924356742116,
      "grad_norm": 1.6059601306915283,
      "learning_rate": 2.5491305312817763e-05,
      "loss": 1.1254,
      "step": 29230
    },
    {
      "epoch": 0.4525440123815051,
      "grad_norm": 1.2770575284957886,
      "learning_rate": 2.5489756837807567e-05,
      "loss": 0.9733,
      "step": 29240
    },
    {
      "epoch": 0.4526987811955891,
      "grad_norm": 1.8612793684005737,
      "learning_rate": 2.5488208362797374e-05,
      "loss": 1.1188,
      "step": 29250
    },
    {
      "epoch": 0.45285355000967303,
      "grad_norm": 1.5707173347473145,
      "learning_rate": 2.5486659887787178e-05,
      "loss": 1.1256,
      "step": 29260
    },
    {
      "epoch": 0.45300831882375703,
      "grad_norm": 1.8739627599716187,
      "learning_rate": 2.5485111412776985e-05,
      "loss": 1.1257,
      "step": 29270
    },
    {
      "epoch": 0.45316308763784097,
      "grad_norm": 1.3532240390777588,
      "learning_rate": 2.548356293776679e-05,
      "loss": 1.0628,
      "step": 29280
    },
    {
      "epoch": 0.45331785645192496,
      "grad_norm": 1.5269402265548706,
      "learning_rate": 2.5482014462756596e-05,
      "loss": 1.0785,
      "step": 29290
    },
    {
      "epoch": 0.4534726252660089,
      "grad_norm": 1.4082497358322144,
      "learning_rate": 2.5480465987746403e-05,
      "loss": 1.0267,
      "step": 29300
    },
    {
      "epoch": 0.45362739408009284,
      "grad_norm": 1.3610624074935913,
      "learning_rate": 2.5478917512736206e-05,
      "loss": 1.2313,
      "step": 29310
    },
    {
      "epoch": 0.45378216289417683,
      "grad_norm": 1.6089553833007812,
      "learning_rate": 2.5477369037726014e-05,
      "loss": 0.9111,
      "step": 29320
    },
    {
      "epoch": 0.45393693170826077,
      "grad_norm": 1.392444133758545,
      "learning_rate": 2.5475820562715817e-05,
      "loss": 0.9973,
      "step": 29330
    },
    {
      "epoch": 0.45409170052234477,
      "grad_norm": 1.9109094142913818,
      "learning_rate": 2.5474272087705624e-05,
      "loss": 1.2247,
      "step": 29340
    },
    {
      "epoch": 0.4542464693364287,
      "grad_norm": 1.1704530715942383,
      "learning_rate": 2.547272361269543e-05,
      "loss": 0.7885,
      "step": 29350
    },
    {
      "epoch": 0.4544012381505127,
      "grad_norm": 1.3571473360061646,
      "learning_rate": 2.547117513768524e-05,
      "loss": 1.0744,
      "step": 29360
    },
    {
      "epoch": 0.45455600696459664,
      "grad_norm": 1.7613171339035034,
      "learning_rate": 2.5469626662675042e-05,
      "loss": 1.1325,
      "step": 29370
    },
    {
      "epoch": 0.4547107757786806,
      "grad_norm": 1.4284751415252686,
      "learning_rate": 2.546807818766485e-05,
      "loss": 1.0668,
      "step": 29380
    },
    {
      "epoch": 0.45486554459276457,
      "grad_norm": 1.4771965742111206,
      "learning_rate": 2.5466529712654653e-05,
      "loss": 1.1174,
      "step": 29390
    },
    {
      "epoch": 0.4550203134068485,
      "grad_norm": 1.1296746730804443,
      "learning_rate": 2.546498123764446e-05,
      "loss": 1.0775,
      "step": 29400
    },
    {
      "epoch": 0.4551750822209325,
      "grad_norm": 1.436919093132019,
      "learning_rate": 2.5463432762634264e-05,
      "loss": 0.9753,
      "step": 29410
    },
    {
      "epoch": 0.45532985103501644,
      "grad_norm": 1.370350956916809,
      "learning_rate": 2.5461884287624075e-05,
      "loss": 1.142,
      "step": 29420
    },
    {
      "epoch": 0.4554846198491004,
      "grad_norm": 1.527368426322937,
      "learning_rate": 2.546033581261388e-05,
      "loss": 1.1056,
      "step": 29430
    },
    {
      "epoch": 0.4556393886631844,
      "grad_norm": 1.9636212587356567,
      "learning_rate": 2.5458787337603686e-05,
      "loss": 0.9965,
      "step": 29440
    },
    {
      "epoch": 0.4557941574772683,
      "grad_norm": 1.6946755647659302,
      "learning_rate": 2.545723886259349e-05,
      "loss": 0.9539,
      "step": 29450
    },
    {
      "epoch": 0.4559489262913523,
      "grad_norm": 1.183977723121643,
      "learning_rate": 2.5455845235084316e-05,
      "loss": 0.9372,
      "step": 29460
    },
    {
      "epoch": 0.45610369510543625,
      "grad_norm": 1.8705332279205322,
      "learning_rate": 2.545429676007412e-05,
      "loss": 1.0721,
      "step": 29470
    },
    {
      "epoch": 0.45625846391952024,
      "grad_norm": 1.8230916261672974,
      "learning_rate": 2.5452748285063926e-05,
      "loss": 0.9047,
      "step": 29480
    },
    {
      "epoch": 0.4564132327336042,
      "grad_norm": 1.6434659957885742,
      "learning_rate": 2.545119981005373e-05,
      "loss": 1.1892,
      "step": 29490
    },
    {
      "epoch": 0.4565680015476881,
      "grad_norm": 1.374101996421814,
      "learning_rate": 2.5449651335043537e-05,
      "loss": 1.0396,
      "step": 29500
    },
    {
      "epoch": 0.4567227703617721,
      "grad_norm": 1.9564064741134644,
      "learning_rate": 2.5448102860033344e-05,
      "loss": 1.176,
      "step": 29510
    },
    {
      "epoch": 0.45687753917585605,
      "grad_norm": 2.05122971534729,
      "learning_rate": 2.544655438502315e-05,
      "loss": 1.1722,
      "step": 29520
    },
    {
      "epoch": 0.45703230798994005,
      "grad_norm": 1.757174015045166,
      "learning_rate": 2.5445005910012955e-05,
      "loss": 1.1637,
      "step": 29530
    },
    {
      "epoch": 0.457187076804024,
      "grad_norm": 1.6432440280914307,
      "learning_rate": 2.5443457435002762e-05,
      "loss": 1.0514,
      "step": 29540
    },
    {
      "epoch": 0.457341845618108,
      "grad_norm": 1.4964405298233032,
      "learning_rate": 2.5441908959992566e-05,
      "loss": 1.0987,
      "step": 29550
    },
    {
      "epoch": 0.4574966144321919,
      "grad_norm": 1.516733169555664,
      "learning_rate": 2.5440360484982373e-05,
      "loss": 1.1988,
      "step": 29560
    },
    {
      "epoch": 0.45765138324627586,
      "grad_norm": 1.2700343132019043,
      "learning_rate": 2.543881200997218e-05,
      "loss": 0.9599,
      "step": 29570
    },
    {
      "epoch": 0.45780615206035985,
      "grad_norm": 1.621973991394043,
      "learning_rate": 2.5437263534961988e-05,
      "loss": 1.061,
      "step": 29580
    },
    {
      "epoch": 0.4579609208744438,
      "grad_norm": 1.7877815961837769,
      "learning_rate": 2.543571505995179e-05,
      "loss": 1.1405,
      "step": 29590
    },
    {
      "epoch": 0.4581156896885278,
      "grad_norm": 2.0274133682250977,
      "learning_rate": 2.54341665849416e-05,
      "loss": 1.0244,
      "step": 29600
    },
    {
      "epoch": 0.4582704585026117,
      "grad_norm": 1.9725651741027832,
      "learning_rate": 2.5432618109931402e-05,
      "loss": 0.9965,
      "step": 29610
    },
    {
      "epoch": 0.45842522731669566,
      "grad_norm": 1.8412307500839233,
      "learning_rate": 2.543106963492121e-05,
      "loss": 1.0705,
      "step": 29620
    },
    {
      "epoch": 0.45857999613077965,
      "grad_norm": 1.2757079601287842,
      "learning_rate": 2.5429521159911013e-05,
      "loss": 0.9435,
      "step": 29630
    },
    {
      "epoch": 0.4587347649448636,
      "grad_norm": 1.2162307500839233,
      "learning_rate": 2.5427972684900824e-05,
      "loss": 0.9768,
      "step": 29640
    },
    {
      "epoch": 0.4588895337589476,
      "grad_norm": 1.940589189529419,
      "learning_rate": 2.5426424209890627e-05,
      "loss": 1.1667,
      "step": 29650
    },
    {
      "epoch": 0.4590443025730315,
      "grad_norm": 1.9914045333862305,
      "learning_rate": 2.5424875734880435e-05,
      "loss": 1.0641,
      "step": 29660
    },
    {
      "epoch": 0.4591990713871155,
      "grad_norm": 1.6015686988830566,
      "learning_rate": 2.542332725987024e-05,
      "loss": 1.0892,
      "step": 29670
    },
    {
      "epoch": 0.45935384020119946,
      "grad_norm": 1.2110422849655151,
      "learning_rate": 2.5421778784860045e-05,
      "loss": 1.0088,
      "step": 29680
    },
    {
      "epoch": 0.4595086090152834,
      "grad_norm": 1.9062578678131104,
      "learning_rate": 2.542023030984985e-05,
      "loss": 1.0258,
      "step": 29690
    },
    {
      "epoch": 0.4596633778293674,
      "grad_norm": 1.092572808265686,
      "learning_rate": 2.5418681834839656e-05,
      "loss": 0.9969,
      "step": 29700
    },
    {
      "epoch": 0.45981814664345133,
      "grad_norm": 1.4218499660491943,
      "learning_rate": 2.5417133359829463e-05,
      "loss": 1.086,
      "step": 29710
    },
    {
      "epoch": 0.4599729154575353,
      "grad_norm": 1.3974086046218872,
      "learning_rate": 2.5415584884819267e-05,
      "loss": 1.1404,
      "step": 29720
    },
    {
      "epoch": 0.46012768427161926,
      "grad_norm": 1.4940712451934814,
      "learning_rate": 2.5414036409809074e-05,
      "loss": 1.066,
      "step": 29730
    },
    {
      "epoch": 0.46028245308570326,
      "grad_norm": 1.4395031929016113,
      "learning_rate": 2.5412487934798878e-05,
      "loss": 1.14,
      "step": 29740
    },
    {
      "epoch": 0.4604372218997872,
      "grad_norm": 1.576731562614441,
      "learning_rate": 2.5410939459788685e-05,
      "loss": 1.0642,
      "step": 29750
    },
    {
      "epoch": 0.46059199071387114,
      "grad_norm": 1.8882790803909302,
      "learning_rate": 2.540939098477849e-05,
      "loss": 1.1488,
      "step": 29760
    },
    {
      "epoch": 0.46074675952795513,
      "grad_norm": 1.2550901174545288,
      "learning_rate": 2.5407842509768296e-05,
      "loss": 1.0075,
      "step": 29770
    },
    {
      "epoch": 0.46090152834203907,
      "grad_norm": 1.4020590782165527,
      "learning_rate": 2.5406294034758103e-05,
      "loss": 1.0916,
      "step": 29780
    },
    {
      "epoch": 0.46105629715612306,
      "grad_norm": 1.849653720855713,
      "learning_rate": 2.540474555974791e-05,
      "loss": 0.9366,
      "step": 29790
    },
    {
      "epoch": 0.461211065970207,
      "grad_norm": 1.3449701070785522,
      "learning_rate": 2.5403197084737714e-05,
      "loss": 0.968,
      "step": 29800
    },
    {
      "epoch": 0.46136583478429094,
      "grad_norm": 1.4179552793502808,
      "learning_rate": 2.540164860972752e-05,
      "loss": 1.1328,
      "step": 29810
    },
    {
      "epoch": 0.46152060359837493,
      "grad_norm": 1.95243239402771,
      "learning_rate": 2.5400100134717325e-05,
      "loss": 1.1531,
      "step": 29820
    },
    {
      "epoch": 0.4616753724124589,
      "grad_norm": 0.9457478523254395,
      "learning_rate": 2.5398551659707132e-05,
      "loss": 0.8843,
      "step": 29830
    },
    {
      "epoch": 0.46183014122654287,
      "grad_norm": 1.5180193185806274,
      "learning_rate": 2.5397003184696936e-05,
      "loss": 0.9711,
      "step": 29840
    },
    {
      "epoch": 0.4619849100406268,
      "grad_norm": 1.7534668445587158,
      "learning_rate": 2.5395454709686746e-05,
      "loss": 1.202,
      "step": 29850
    },
    {
      "epoch": 0.4621396788547108,
      "grad_norm": 1.524871587753296,
      "learning_rate": 2.539390623467655e-05,
      "loss": 1.1731,
      "step": 29860
    },
    {
      "epoch": 0.46229444766879474,
      "grad_norm": 1.7732435464859009,
      "learning_rate": 2.5392357759666357e-05,
      "loss": 0.9883,
      "step": 29870
    },
    {
      "epoch": 0.4624492164828787,
      "grad_norm": 2.065556764602661,
      "learning_rate": 2.539080928465616e-05,
      "loss": 0.9306,
      "step": 29880
    },
    {
      "epoch": 0.46260398529696267,
      "grad_norm": 1.6839911937713623,
      "learning_rate": 2.5389260809645968e-05,
      "loss": 1.1374,
      "step": 29890
    },
    {
      "epoch": 0.4627587541110466,
      "grad_norm": 1.215142011642456,
      "learning_rate": 2.5387712334635772e-05,
      "loss": 0.9399,
      "step": 29900
    },
    {
      "epoch": 0.4629135229251306,
      "grad_norm": 1.7212004661560059,
      "learning_rate": 2.538616385962558e-05,
      "loss": 0.9661,
      "step": 29910
    },
    {
      "epoch": 0.46306829173921454,
      "grad_norm": 1.3342379331588745,
      "learning_rate": 2.5384615384615386e-05,
      "loss": 1.0765,
      "step": 29920
    },
    {
      "epoch": 0.46322306055329854,
      "grad_norm": 1.4004251956939697,
      "learning_rate": 2.5383066909605193e-05,
      "loss": 0.9821,
      "step": 29930
    },
    {
      "epoch": 0.4633778293673825,
      "grad_norm": 1.591469168663025,
      "learning_rate": 2.5381518434594997e-05,
      "loss": 1.0583,
      "step": 29940
    },
    {
      "epoch": 0.4635325981814664,
      "grad_norm": 2.04683518409729,
      "learning_rate": 2.5379969959584804e-05,
      "loss": 1.1511,
      "step": 29950
    },
    {
      "epoch": 0.4636873669955504,
      "grad_norm": 1.9922395944595337,
      "learning_rate": 2.5378421484574608e-05,
      "loss": 1.0497,
      "step": 29960
    },
    {
      "epoch": 0.46384213580963435,
      "grad_norm": 1.5371628999710083,
      "learning_rate": 2.5376873009564412e-05,
      "loss": 0.9969,
      "step": 29970
    },
    {
      "epoch": 0.46399690462371834,
      "grad_norm": 1.3098119497299194,
      "learning_rate": 2.537532453455422e-05,
      "loss": 1.2013,
      "step": 29980
    },
    {
      "epoch": 0.4641516734378023,
      "grad_norm": 1.625432014465332,
      "learning_rate": 2.5373776059544026e-05,
      "loss": 0.9386,
      "step": 29990
    },
    {
      "epoch": 0.4643064422518862,
      "grad_norm": 1.1863043308258057,
      "learning_rate": 2.5372227584533833e-05,
      "loss": 0.9872,
      "step": 30000
    },
    {
      "epoch": 0.4644612110659702,
      "grad_norm": 1.3236948251724243,
      "learning_rate": 2.5370679109523637e-05,
      "loss": 1.0401,
      "step": 30010
    },
    {
      "epoch": 0.46461597988005415,
      "grad_norm": 2.2107746601104736,
      "learning_rate": 2.5369130634513444e-05,
      "loss": 1.1277,
      "step": 30020
    },
    {
      "epoch": 0.46477074869413815,
      "grad_norm": 1.2104355096817017,
      "learning_rate": 2.5367582159503248e-05,
      "loss": 0.9208,
      "step": 30030
    },
    {
      "epoch": 0.4649255175082221,
      "grad_norm": 1.402355670928955,
      "learning_rate": 2.5366033684493055e-05,
      "loss": 0.9562,
      "step": 30040
    },
    {
      "epoch": 0.4650802863223061,
      "grad_norm": 2.242739200592041,
      "learning_rate": 2.5364485209482862e-05,
      "loss": 1.1087,
      "step": 30050
    },
    {
      "epoch": 0.46523505513639,
      "grad_norm": 1.3772716522216797,
      "learning_rate": 2.536293673447267e-05,
      "loss": 1.0193,
      "step": 30060
    },
    {
      "epoch": 0.46538982395047396,
      "grad_norm": 1.2680028676986694,
      "learning_rate": 2.5361388259462473e-05,
      "loss": 0.9059,
      "step": 30070
    },
    {
      "epoch": 0.46554459276455795,
      "grad_norm": 1.3849416971206665,
      "learning_rate": 2.535983978445228e-05,
      "loss": 1.2776,
      "step": 30080
    },
    {
      "epoch": 0.4656993615786419,
      "grad_norm": 1.5658077001571655,
      "learning_rate": 2.5358291309442084e-05,
      "loss": 1.0947,
      "step": 30090
    },
    {
      "epoch": 0.4658541303927259,
      "grad_norm": 1.149204969406128,
      "learning_rate": 2.535674283443189e-05,
      "loss": 1.0539,
      "step": 30100
    },
    {
      "epoch": 0.4660088992068098,
      "grad_norm": 1.3525580167770386,
      "learning_rate": 2.5355194359421695e-05,
      "loss": 0.9198,
      "step": 30110
    },
    {
      "epoch": 0.46616366802089376,
      "grad_norm": 1.2738301753997803,
      "learning_rate": 2.5353645884411505e-05,
      "loss": 1.0827,
      "step": 30120
    },
    {
      "epoch": 0.46631843683497776,
      "grad_norm": 1.4300224781036377,
      "learning_rate": 2.535209740940131e-05,
      "loss": 0.9026,
      "step": 30130
    },
    {
      "epoch": 0.4664732056490617,
      "grad_norm": 1.9750224351882935,
      "learning_rate": 2.5350548934391116e-05,
      "loss": 1.1127,
      "step": 30140
    },
    {
      "epoch": 0.4666279744631457,
      "grad_norm": 1.3391201496124268,
      "learning_rate": 2.534915530688194e-05,
      "loss": 1.0913,
      "step": 30150
    },
    {
      "epoch": 0.46678274327722963,
      "grad_norm": 1.5397731065750122,
      "learning_rate": 2.5347606831871746e-05,
      "loss": 1.4515,
      "step": 30160
    },
    {
      "epoch": 0.4669375120913136,
      "grad_norm": 1.472068428993225,
      "learning_rate": 2.534605835686155e-05,
      "loss": 1.0322,
      "step": 30170
    },
    {
      "epoch": 0.46709228090539756,
      "grad_norm": 1.6650956869125366,
      "learning_rate": 2.5344509881851357e-05,
      "loss": 1.0475,
      "step": 30180
    },
    {
      "epoch": 0.4672470497194815,
      "grad_norm": 1.1187574863433838,
      "learning_rate": 2.534296140684116e-05,
      "loss": 0.98,
      "step": 30190
    },
    {
      "epoch": 0.4674018185335655,
      "grad_norm": 1.4138820171356201,
      "learning_rate": 2.5341412931830968e-05,
      "loss": 1.1649,
      "step": 30200
    },
    {
      "epoch": 0.46755658734764943,
      "grad_norm": 2.061894655227661,
      "learning_rate": 2.5339864456820775e-05,
      "loss": 1.0584,
      "step": 30210
    },
    {
      "epoch": 0.4677113561617334,
      "grad_norm": 1.4623520374298096,
      "learning_rate": 2.5338315981810582e-05,
      "loss": 1.1961,
      "step": 30220
    },
    {
      "epoch": 0.46786612497581737,
      "grad_norm": 1.5031917095184326,
      "learning_rate": 2.5336767506800386e-05,
      "loss": 1.0105,
      "step": 30230
    },
    {
      "epoch": 0.46802089378990136,
      "grad_norm": 1.2595012187957764,
      "learning_rate": 2.5335219031790193e-05,
      "loss": 1.2327,
      "step": 30240
    },
    {
      "epoch": 0.4681756626039853,
      "grad_norm": 1.656372308731079,
      "learning_rate": 2.5333670556779997e-05,
      "loss": 1.1992,
      "step": 30250
    },
    {
      "epoch": 0.46833043141806924,
      "grad_norm": 1.9673806428909302,
      "learning_rate": 2.5332122081769804e-05,
      "loss": 0.9541,
      "step": 30260
    },
    {
      "epoch": 0.46848520023215323,
      "grad_norm": 1.4691903591156006,
      "learning_rate": 2.533057360675961e-05,
      "loss": 1.248,
      "step": 30270
    },
    {
      "epoch": 0.46863996904623717,
      "grad_norm": 1.4193874597549438,
      "learning_rate": 2.5329025131749418e-05,
      "loss": 1.1034,
      "step": 30280
    },
    {
      "epoch": 0.46879473786032116,
      "grad_norm": 1.504746913909912,
      "learning_rate": 2.5327476656739222e-05,
      "loss": 0.8888,
      "step": 30290
    },
    {
      "epoch": 0.4689495066744051,
      "grad_norm": 2.155665397644043,
      "learning_rate": 2.532592818172903e-05,
      "loss": 1.1286,
      "step": 30300
    },
    {
      "epoch": 0.46910427548848904,
      "grad_norm": 1.5443916320800781,
      "learning_rate": 2.5324379706718833e-05,
      "loss": 1.1413,
      "step": 30310
    },
    {
      "epoch": 0.46925904430257304,
      "grad_norm": 1.9262282848358154,
      "learning_rate": 2.532283123170864e-05,
      "loss": 1.1805,
      "step": 30320
    },
    {
      "epoch": 0.469413813116657,
      "grad_norm": 1.5556457042694092,
      "learning_rate": 2.5321282756698444e-05,
      "loss": 1.1383,
      "step": 30330
    },
    {
      "epoch": 0.46956858193074097,
      "grad_norm": 1.6101123094558716,
      "learning_rate": 2.5319734281688254e-05,
      "loss": 1.141,
      "step": 30340
    },
    {
      "epoch": 0.4697233507448249,
      "grad_norm": 1.298789143562317,
      "learning_rate": 2.5318185806678058e-05,
      "loss": 1.0923,
      "step": 30350
    },
    {
      "epoch": 0.4698781195589089,
      "grad_norm": 1.112126111984253,
      "learning_rate": 2.5316637331667865e-05,
      "loss": 0.9548,
      "step": 30360
    },
    {
      "epoch": 0.47003288837299284,
      "grad_norm": 1.2535066604614258,
      "learning_rate": 2.531508885665767e-05,
      "loss": 1.4056,
      "step": 30370
    },
    {
      "epoch": 0.4701876571870768,
      "grad_norm": 2.546067476272583,
      "learning_rate": 2.5313540381647476e-05,
      "loss": 0.9131,
      "step": 30380
    },
    {
      "epoch": 0.4703424260011608,
      "grad_norm": 1.1251987218856812,
      "learning_rate": 2.531199190663728e-05,
      "loss": 1.1956,
      "step": 30390
    },
    {
      "epoch": 0.4704971948152447,
      "grad_norm": 1.8005785942077637,
      "learning_rate": 2.5310443431627084e-05,
      "loss": 1.1048,
      "step": 30400
    },
    {
      "epoch": 0.4706519636293287,
      "grad_norm": 1.2732819318771362,
      "learning_rate": 2.5308894956616894e-05,
      "loss": 1.1138,
      "step": 30410
    },
    {
      "epoch": 0.47080673244341265,
      "grad_norm": 1.5934468507766724,
      "learning_rate": 2.5307346481606698e-05,
      "loss": 1.252,
      "step": 30420
    },
    {
      "epoch": 0.47096150125749664,
      "grad_norm": 1.2958043813705444,
      "learning_rate": 2.5305798006596505e-05,
      "loss": 0.9976,
      "step": 30430
    },
    {
      "epoch": 0.4711162700715806,
      "grad_norm": 1.5619362592697144,
      "learning_rate": 2.530424953158631e-05,
      "loss": 0.9962,
      "step": 30440
    },
    {
      "epoch": 0.4712710388856645,
      "grad_norm": 1.9677211046218872,
      "learning_rate": 2.5302701056576116e-05,
      "loss": 1.0671,
      "step": 30450
    },
    {
      "epoch": 0.4714258076997485,
      "grad_norm": 1.3782541751861572,
      "learning_rate": 2.530115258156592e-05,
      "loss": 1.0488,
      "step": 30460
    },
    {
      "epoch": 0.47158057651383245,
      "grad_norm": 1.8138701915740967,
      "learning_rate": 2.5299604106555727e-05,
      "loss": 0.9091,
      "step": 30470
    },
    {
      "epoch": 0.47173534532791644,
      "grad_norm": 1.4844028949737549,
      "learning_rate": 2.5298055631545534e-05,
      "loss": 0.9641,
      "step": 30480
    },
    {
      "epoch": 0.4718901141420004,
      "grad_norm": 1.738242745399475,
      "learning_rate": 2.529650715653534e-05,
      "loss": 1.2042,
      "step": 30490
    },
    {
      "epoch": 0.4720448829560843,
      "grad_norm": 1.1748385429382324,
      "learning_rate": 2.5294958681525145e-05,
      "loss": 1.0816,
      "step": 30500
    },
    {
      "epoch": 0.4721996517701683,
      "grad_norm": 1.6345574855804443,
      "learning_rate": 2.5293410206514952e-05,
      "loss": 1.174,
      "step": 30510
    },
    {
      "epoch": 0.47235442058425225,
      "grad_norm": 1.9852795600891113,
      "learning_rate": 2.5291861731504756e-05,
      "loss": 1.1874,
      "step": 30520
    },
    {
      "epoch": 0.47250918939833625,
      "grad_norm": 1.3885900974273682,
      "learning_rate": 2.5290313256494563e-05,
      "loss": 0.9813,
      "step": 30530
    },
    {
      "epoch": 0.4726639582124202,
      "grad_norm": 1.5988985300064087,
      "learning_rate": 2.5288764781484367e-05,
      "loss": 1.1784,
      "step": 30540
    },
    {
      "epoch": 0.4728187270265042,
      "grad_norm": 1.2792004346847534,
      "learning_rate": 2.5287216306474177e-05,
      "loss": 1.1761,
      "step": 30550
    },
    {
      "epoch": 0.4729734958405881,
      "grad_norm": 1.459804654121399,
      "learning_rate": 2.528566783146398e-05,
      "loss": 0.9811,
      "step": 30560
    },
    {
      "epoch": 0.47312826465467206,
      "grad_norm": 1.2075752019882202,
      "learning_rate": 2.5284119356453788e-05,
      "loss": 0.9554,
      "step": 30570
    },
    {
      "epoch": 0.47328303346875605,
      "grad_norm": 1.3479485511779785,
      "learning_rate": 2.5282570881443592e-05,
      "loss": 1.0539,
      "step": 30580
    },
    {
      "epoch": 0.47343780228284,
      "grad_norm": 1.177005410194397,
      "learning_rate": 2.52810224064334e-05,
      "loss": 0.9792,
      "step": 30590
    },
    {
      "epoch": 0.473592571096924,
      "grad_norm": 2.0528597831726074,
      "learning_rate": 2.5279473931423203e-05,
      "loss": 1.035,
      "step": 30600
    },
    {
      "epoch": 0.4737473399110079,
      "grad_norm": 1.5980346202850342,
      "learning_rate": 2.527792545641301e-05,
      "loss": 1.0809,
      "step": 30610
    },
    {
      "epoch": 0.4739021087250919,
      "grad_norm": 1.2329497337341309,
      "learning_rate": 2.5276376981402817e-05,
      "loss": 0.9981,
      "step": 30620
    },
    {
      "epoch": 0.47405687753917586,
      "grad_norm": 1.5164575576782227,
      "learning_rate": 2.5274828506392624e-05,
      "loss": 1.2574,
      "step": 30630
    },
    {
      "epoch": 0.4742116463532598,
      "grad_norm": 1.5888842344284058,
      "learning_rate": 2.5273280031382428e-05,
      "loss": 1.1941,
      "step": 30640
    },
    {
      "epoch": 0.4743664151673438,
      "grad_norm": 1.6885157823562622,
      "learning_rate": 2.527173155637223e-05,
      "loss": 0.975,
      "step": 30650
    },
    {
      "epoch": 0.47452118398142773,
      "grad_norm": 1.4880906343460083,
      "learning_rate": 2.527018308136204e-05,
      "loss": 1.083,
      "step": 30660
    },
    {
      "epoch": 0.4746759527955117,
      "grad_norm": 1.2803477048873901,
      "learning_rate": 2.5268634606351842e-05,
      "loss": 1.0913,
      "step": 30670
    },
    {
      "epoch": 0.47483072160959566,
      "grad_norm": 1.8318899869918823,
      "learning_rate": 2.5267086131341653e-05,
      "loss": 1.2826,
      "step": 30680
    },
    {
      "epoch": 0.4749854904236796,
      "grad_norm": 1.1625053882598877,
      "learning_rate": 2.5265537656331457e-05,
      "loss": 1.0259,
      "step": 30690
    },
    {
      "epoch": 0.4751402592377636,
      "grad_norm": 1.6037358045578003,
      "learning_rate": 2.5263989181321264e-05,
      "loss": 1.0346,
      "step": 30700
    },
    {
      "epoch": 0.47529502805184753,
      "grad_norm": 1.6103090047836304,
      "learning_rate": 2.5262440706311068e-05,
      "loss": 0.908,
      "step": 30710
    },
    {
      "epoch": 0.47544979686593153,
      "grad_norm": 1.4153388738632202,
      "learning_rate": 2.5260892231300875e-05,
      "loss": 1.0753,
      "step": 30720
    },
    {
      "epoch": 0.47560456568001547,
      "grad_norm": 1.2989813089370728,
      "learning_rate": 2.525934375629068e-05,
      "loss": 1.1135,
      "step": 30730
    },
    {
      "epoch": 0.47575933449409946,
      "grad_norm": 1.200926661491394,
      "learning_rate": 2.5257795281280486e-05,
      "loss": 1.2885,
      "step": 30740
    },
    {
      "epoch": 0.4759141033081834,
      "grad_norm": 1.4577078819274902,
      "learning_rate": 2.5256246806270293e-05,
      "loss": 0.986,
      "step": 30750
    },
    {
      "epoch": 0.47606887212226734,
      "grad_norm": 1.2809911966323853,
      "learning_rate": 2.52546983312601e-05,
      "loss": 1.019,
      "step": 30760
    },
    {
      "epoch": 0.47622364093635133,
      "grad_norm": 1.6324076652526855,
      "learning_rate": 2.5253149856249904e-05,
      "loss": 1.1308,
      "step": 30770
    },
    {
      "epoch": 0.47637840975043527,
      "grad_norm": 1.3727705478668213,
      "learning_rate": 2.525160138123971e-05,
      "loss": 1.1037,
      "step": 30780
    },
    {
      "epoch": 0.47653317856451927,
      "grad_norm": 1.9482439756393433,
      "learning_rate": 2.5250052906229515e-05,
      "loss": 1.0563,
      "step": 30790
    },
    {
      "epoch": 0.4766879473786032,
      "grad_norm": 0.9411102533340454,
      "learning_rate": 2.524850443121932e-05,
      "loss": 0.9003,
      "step": 30800
    },
    {
      "epoch": 0.4768427161926872,
      "grad_norm": 1.656590223312378,
      "learning_rate": 2.5246955956209125e-05,
      "loss": 1.0994,
      "step": 30810
    },
    {
      "epoch": 0.47699748500677114,
      "grad_norm": 1.4939130544662476,
      "learning_rate": 2.5245407481198936e-05,
      "loss": 1.1251,
      "step": 30820
    },
    {
      "epoch": 0.4771522538208551,
      "grad_norm": 1.056065320968628,
      "learning_rate": 2.524385900618874e-05,
      "loss": 1.2436,
      "step": 30830
    },
    {
      "epoch": 0.47730702263493907,
      "grad_norm": 2.089169979095459,
      "learning_rate": 2.5242310531178547e-05,
      "loss": 1.2402,
      "step": 30840
    },
    {
      "epoch": 0.477461791449023,
      "grad_norm": 1.652069330215454,
      "learning_rate": 2.524076205616835e-05,
      "loss": 1.2094,
      "step": 30850
    },
    {
      "epoch": 0.477616560263107,
      "grad_norm": 1.1339739561080933,
      "learning_rate": 2.5239213581158158e-05,
      "loss": 1.0645,
      "step": 30860
    },
    {
      "epoch": 0.47777132907719094,
      "grad_norm": 1.6167265176773071,
      "learning_rate": 2.523766510614796e-05,
      "loss": 1.0516,
      "step": 30870
    },
    {
      "epoch": 0.4779260978912749,
      "grad_norm": 1.3542824983596802,
      "learning_rate": 2.523611663113777e-05,
      "loss": 1.062,
      "step": 30880
    },
    {
      "epoch": 0.4780808667053589,
      "grad_norm": 1.4315568208694458,
      "learning_rate": 2.5234568156127576e-05,
      "loss": 1.0822,
      "step": 30890
    },
    {
      "epoch": 0.4782356355194428,
      "grad_norm": 1.609354853630066,
      "learning_rate": 2.523301968111738e-05,
      "loss": 1.0182,
      "step": 30900
    },
    {
      "epoch": 0.4783904043335268,
      "grad_norm": 1.5447802543640137,
      "learning_rate": 2.5231471206107187e-05,
      "loss": 0.9785,
      "step": 30910
    },
    {
      "epoch": 0.47854517314761075,
      "grad_norm": 1.384263515472412,
      "learning_rate": 2.522992273109699e-05,
      "loss": 1.1398,
      "step": 30920
    },
    {
      "epoch": 0.47869994196169474,
      "grad_norm": 1.5161404609680176,
      "learning_rate": 2.5228374256086798e-05,
      "loss": 1.1828,
      "step": 30930
    },
    {
      "epoch": 0.4788547107757787,
      "grad_norm": 1.4024885892868042,
      "learning_rate": 2.52268257810766e-05,
      "loss": 1.1978,
      "step": 30940
    },
    {
      "epoch": 0.4790094795898626,
      "grad_norm": 1.3407596349716187,
      "learning_rate": 2.522527730606641e-05,
      "loss": 1.2341,
      "step": 30950
    },
    {
      "epoch": 0.4791642484039466,
      "grad_norm": 1.4032924175262451,
      "learning_rate": 2.5223728831056216e-05,
      "loss": 0.9636,
      "step": 30960
    },
    {
      "epoch": 0.47931901721803055,
      "grad_norm": 1.442360281944275,
      "learning_rate": 2.5222180356046023e-05,
      "loss": 1.2071,
      "step": 30970
    },
    {
      "epoch": 0.47947378603211455,
      "grad_norm": 1.6174614429473877,
      "learning_rate": 2.5220631881035826e-05,
      "loss": 1.1556,
      "step": 30980
    },
    {
      "epoch": 0.4796285548461985,
      "grad_norm": 1.4575527906417847,
      "learning_rate": 2.5219083406025634e-05,
      "loss": 0.907,
      "step": 30990
    },
    {
      "epoch": 0.4797833236602825,
      "grad_norm": 1.2342047691345215,
      "learning_rate": 2.5217534931015437e-05,
      "loss": 0.8917,
      "step": 31000
    },
    {
      "epoch": 0.4799380924743664,
      "grad_norm": 2.079252004623413,
      "learning_rate": 2.5215986456005244e-05,
      "loss": 1.1405,
      "step": 31010
    },
    {
      "epoch": 0.48009286128845036,
      "grad_norm": 1.3442176580429077,
      "learning_rate": 2.5214437980995048e-05,
      "loss": 1.1711,
      "step": 31020
    },
    {
      "epoch": 0.48024763010253435,
      "grad_norm": 1.3533828258514404,
      "learning_rate": 2.521288950598486e-05,
      "loss": 1.1848,
      "step": 31030
    },
    {
      "epoch": 0.4804023989166183,
      "grad_norm": 1.1491397619247437,
      "learning_rate": 2.5211341030974662e-05,
      "loss": 1.1455,
      "step": 31040
    },
    {
      "epoch": 0.4805571677307023,
      "grad_norm": 1.6934645175933838,
      "learning_rate": 2.520979255596447e-05,
      "loss": 1.1457,
      "step": 31050
    },
    {
      "epoch": 0.4807119365447862,
      "grad_norm": 2.23097562789917,
      "learning_rate": 2.5208244080954273e-05,
      "loss": 0.922,
      "step": 31060
    },
    {
      "epoch": 0.48086670535887016,
      "grad_norm": 1.6490488052368164,
      "learning_rate": 2.520669560594408e-05,
      "loss": 1.1823,
      "step": 31070
    },
    {
      "epoch": 0.48102147417295416,
      "grad_norm": 1.431868553161621,
      "learning_rate": 2.5205147130933884e-05,
      "loss": 1.1962,
      "step": 31080
    },
    {
      "epoch": 0.4811762429870381,
      "grad_norm": 1.8881444931030273,
      "learning_rate": 2.520359865592369e-05,
      "loss": 1.1354,
      "step": 31090
    },
    {
      "epoch": 0.4813310118011221,
      "grad_norm": 1.755264163017273,
      "learning_rate": 2.52020501809135e-05,
      "loss": 1.0181,
      "step": 31100
    },
    {
      "epoch": 0.481485780615206,
      "grad_norm": 1.629332423210144,
      "learning_rate": 2.5200501705903306e-05,
      "loss": 1.1352,
      "step": 31110
    },
    {
      "epoch": 0.48164054942929,
      "grad_norm": 1.4596525430679321,
      "learning_rate": 2.519895323089311e-05,
      "loss": 1.0035,
      "step": 31120
    },
    {
      "epoch": 0.48179531824337396,
      "grad_norm": 1.7502858638763428,
      "learning_rate": 2.5197404755882917e-05,
      "loss": 1.1749,
      "step": 31130
    },
    {
      "epoch": 0.4819500870574579,
      "grad_norm": 1.3829487562179565,
      "learning_rate": 2.519585628087272e-05,
      "loss": 1.2152,
      "step": 31140
    },
    {
      "epoch": 0.4821048558715419,
      "grad_norm": 1.3268581628799438,
      "learning_rate": 2.5194307805862527e-05,
      "loss": 1.1312,
      "step": 31150
    },
    {
      "epoch": 0.48225962468562583,
      "grad_norm": 1.6599578857421875,
      "learning_rate": 2.5192759330852335e-05,
      "loss": 1.2204,
      "step": 31160
    },
    {
      "epoch": 0.4824143934997098,
      "grad_norm": 2.8035242557525635,
      "learning_rate": 2.519121085584214e-05,
      "loss": 1.0294,
      "step": 31170
    },
    {
      "epoch": 0.48256916231379376,
      "grad_norm": 1.3728935718536377,
      "learning_rate": 2.5189662380831945e-05,
      "loss": 1.0929,
      "step": 31180
    },
    {
      "epoch": 0.48272393112787776,
      "grad_norm": 1.726706624031067,
      "learning_rate": 2.518811390582175e-05,
      "loss": 1.0755,
      "step": 31190
    },
    {
      "epoch": 0.4828786999419617,
      "grad_norm": 1.8727960586547852,
      "learning_rate": 2.5186565430811556e-05,
      "loss": 0.9831,
      "step": 31200
    },
    {
      "epoch": 0.48303346875604564,
      "grad_norm": 1.5896780490875244,
      "learning_rate": 2.518501695580136e-05,
      "loss": 1.0812,
      "step": 31210
    },
    {
      "epoch": 0.48318823757012963,
      "grad_norm": 1.5759656429290771,
      "learning_rate": 2.5183468480791167e-05,
      "loss": 1.1739,
      "step": 31220
    },
    {
      "epoch": 0.48334300638421357,
      "grad_norm": 1.8791447877883911,
      "learning_rate": 2.5181920005780974e-05,
      "loss": 1.2189,
      "step": 31230
    },
    {
      "epoch": 0.48349777519829756,
      "grad_norm": 1.460099220275879,
      "learning_rate": 2.518037153077078e-05,
      "loss": 1.0439,
      "step": 31240
    },
    {
      "epoch": 0.4836525440123815,
      "grad_norm": 1.5310571193695068,
      "learning_rate": 2.5178823055760585e-05,
      "loss": 1.0857,
      "step": 31250
    },
    {
      "epoch": 0.48380731282646544,
      "grad_norm": 1.8119182586669922,
      "learning_rate": 2.5177274580750392e-05,
      "loss": 1.1916,
      "step": 31260
    },
    {
      "epoch": 0.48396208164054944,
      "grad_norm": 2.0500779151916504,
      "learning_rate": 2.5175726105740196e-05,
      "loss": 1.1566,
      "step": 31270
    },
    {
      "epoch": 0.4841168504546334,
      "grad_norm": 1.2367138862609863,
      "learning_rate": 2.5174177630730003e-05,
      "loss": 0.9309,
      "step": 31280
    },
    {
      "epoch": 0.48427161926871737,
      "grad_norm": 1.0330636501312256,
      "learning_rate": 2.5172629155719807e-05,
      "loss": 0.8795,
      "step": 31290
    },
    {
      "epoch": 0.4844263880828013,
      "grad_norm": 1.7200987339019775,
      "learning_rate": 2.5171080680709618e-05,
      "loss": 1.231,
      "step": 31300
    },
    {
      "epoch": 0.4845811568968853,
      "grad_norm": 1.570439100265503,
      "learning_rate": 2.516953220569942e-05,
      "loss": 1.1297,
      "step": 31310
    },
    {
      "epoch": 0.48473592571096924,
      "grad_norm": 1.316422939300537,
      "learning_rate": 2.516798373068923e-05,
      "loss": 1.0068,
      "step": 31320
    },
    {
      "epoch": 0.4848906945250532,
      "grad_norm": 1.6750407218933105,
      "learning_rate": 2.5166435255679032e-05,
      "loss": 1.1987,
      "step": 31330
    },
    {
      "epoch": 0.4850454633391372,
      "grad_norm": 2.0190975666046143,
      "learning_rate": 2.516488678066884e-05,
      "loss": 1.037,
      "step": 31340
    },
    {
      "epoch": 0.4852002321532211,
      "grad_norm": 2.8587868213653564,
      "learning_rate": 2.5163338305658643e-05,
      "loss": 1.1848,
      "step": 31350
    },
    {
      "epoch": 0.4853550009673051,
      "grad_norm": 1.226364016532898,
      "learning_rate": 2.516178983064845e-05,
      "loss": 1.1258,
      "step": 31360
    },
    {
      "epoch": 0.48550976978138904,
      "grad_norm": 2.1569037437438965,
      "learning_rate": 2.5160241355638257e-05,
      "loss": 0.9598,
      "step": 31370
    },
    {
      "epoch": 0.48566453859547304,
      "grad_norm": 1.2985155582427979,
      "learning_rate": 2.5158692880628065e-05,
      "loss": 1.0364,
      "step": 31380
    },
    {
      "epoch": 0.485819307409557,
      "grad_norm": 1.4829208850860596,
      "learning_rate": 2.5157144405617868e-05,
      "loss": 1.1556,
      "step": 31390
    },
    {
      "epoch": 0.4859740762236409,
      "grad_norm": 1.78385329246521,
      "learning_rate": 2.5155595930607675e-05,
      "loss": 1.1004,
      "step": 31400
    },
    {
      "epoch": 0.4861288450377249,
      "grad_norm": 1.393676519393921,
      "learning_rate": 2.515404745559748e-05,
      "loss": 1.2859,
      "step": 31410
    },
    {
      "epoch": 0.48628361385180885,
      "grad_norm": 1.5610848665237427,
      "learning_rate": 2.5152498980587283e-05,
      "loss": 1.0362,
      "step": 31420
    },
    {
      "epoch": 0.48643838266589284,
      "grad_norm": 1.6643805503845215,
      "learning_rate": 2.515095050557709e-05,
      "loss": 1.1016,
      "step": 31430
    },
    {
      "epoch": 0.4865931514799768,
      "grad_norm": 1.4063873291015625,
      "learning_rate": 2.5149402030566897e-05,
      "loss": 1.0907,
      "step": 31440
    },
    {
      "epoch": 0.4867479202940607,
      "grad_norm": 1.2076866626739502,
      "learning_rate": 2.5147853555556704e-05,
      "loss": 0.9826,
      "step": 31450
    },
    {
      "epoch": 0.4869026891081447,
      "grad_norm": 1.3267637491226196,
      "learning_rate": 2.5146305080546508e-05,
      "loss": 1.0053,
      "step": 31460
    },
    {
      "epoch": 0.48705745792222865,
      "grad_norm": 1.620518684387207,
      "learning_rate": 2.5144756605536315e-05,
      "loss": 1.2197,
      "step": 31470
    },
    {
      "epoch": 0.48721222673631265,
      "grad_norm": 1.3725377321243286,
      "learning_rate": 2.514320813052612e-05,
      "loss": 0.9902,
      "step": 31480
    },
    {
      "epoch": 0.4873669955503966,
      "grad_norm": 1.7302697896957397,
      "learning_rate": 2.5141659655515926e-05,
      "loss": 1.1186,
      "step": 31490
    },
    {
      "epoch": 0.4875217643644806,
      "grad_norm": 1.160109519958496,
      "learning_rate": 2.514011118050573e-05,
      "loss": 0.9294,
      "step": 31500
    },
    {
      "epoch": 0.4876765331785645,
      "grad_norm": 1.825742483139038,
      "learning_rate": 2.513856270549554e-05,
      "loss": 1.1215,
      "step": 31510
    },
    {
      "epoch": 0.48783130199264846,
      "grad_norm": 1.5453007221221924,
      "learning_rate": 2.5137014230485344e-05,
      "loss": 1.0657,
      "step": 31520
    },
    {
      "epoch": 0.48798607080673245,
      "grad_norm": 1.6697286367416382,
      "learning_rate": 2.513546575547515e-05,
      "loss": 1.0179,
      "step": 31530
    },
    {
      "epoch": 0.4881408396208164,
      "grad_norm": 1.9340704679489136,
      "learning_rate": 2.5133917280464955e-05,
      "loss": 1.1066,
      "step": 31540
    },
    {
      "epoch": 0.4882956084349004,
      "grad_norm": 2.52213454246521,
      "learning_rate": 2.5132368805454762e-05,
      "loss": 1.0117,
      "step": 31550
    },
    {
      "epoch": 0.4884503772489843,
      "grad_norm": 1.3586692810058594,
      "learning_rate": 2.5130820330444566e-05,
      "loss": 1.0217,
      "step": 31560
    },
    {
      "epoch": 0.4886051460630683,
      "grad_norm": 1.7808271646499634,
      "learning_rate": 2.5129271855434373e-05,
      "loss": 1.0435,
      "step": 31570
    },
    {
      "epoch": 0.48875991487715226,
      "grad_norm": 1.4566435813903809,
      "learning_rate": 2.512772338042418e-05,
      "loss": 1.0427,
      "step": 31580
    },
    {
      "epoch": 0.4889146836912362,
      "grad_norm": 1.6324303150177002,
      "learning_rate": 2.5126174905413987e-05,
      "loss": 1.1101,
      "step": 31590
    },
    {
      "epoch": 0.4890694525053202,
      "grad_norm": 1.833430528640747,
      "learning_rate": 2.512462643040379e-05,
      "loss": 1.1219,
      "step": 31600
    },
    {
      "epoch": 0.48922422131940413,
      "grad_norm": 1.1300387382507324,
      "learning_rate": 2.5123077955393598e-05,
      "loss": 1.1368,
      "step": 31610
    },
    {
      "epoch": 0.4893789901334881,
      "grad_norm": 1.1723278760910034,
      "learning_rate": 2.5121529480383402e-05,
      "loss": 1.0316,
      "step": 31620
    },
    {
      "epoch": 0.48953375894757206,
      "grad_norm": 1.3361561298370361,
      "learning_rate": 2.511998100537321e-05,
      "loss": 1.0333,
      "step": 31630
    },
    {
      "epoch": 0.489688527761656,
      "grad_norm": 1.328884243965149,
      "learning_rate": 2.5118432530363016e-05,
      "loss": 0.9959,
      "step": 31640
    },
    {
      "epoch": 0.48984329657574,
      "grad_norm": 1.0169953107833862,
      "learning_rate": 2.5116884055352823e-05,
      "loss": 1.0459,
      "step": 31650
    },
    {
      "epoch": 0.48999806538982393,
      "grad_norm": 1.249287724494934,
      "learning_rate": 2.5115335580342627e-05,
      "loss": 1.1613,
      "step": 31660
    },
    {
      "epoch": 0.49015283420390793,
      "grad_norm": 1.8215081691741943,
      "learning_rate": 2.511378710533243e-05,
      "loss": 1.066,
      "step": 31670
    },
    {
      "epoch": 0.49030760301799187,
      "grad_norm": 1.5372509956359863,
      "learning_rate": 2.5112238630322238e-05,
      "loss": 1.0448,
      "step": 31680
    },
    {
      "epoch": 0.49046237183207586,
      "grad_norm": 1.423859715461731,
      "learning_rate": 2.5110690155312042e-05,
      "loss": 1.0425,
      "step": 31690
    },
    {
      "epoch": 0.4906171406461598,
      "grad_norm": 1.5010743141174316,
      "learning_rate": 2.510914168030185e-05,
      "loss": 1.0776,
      "step": 31700
    },
    {
      "epoch": 0.49077190946024374,
      "grad_norm": 1.3557908535003662,
      "learning_rate": 2.5107593205291656e-05,
      "loss": 1.1237,
      "step": 31710
    },
    {
      "epoch": 0.49092667827432773,
      "grad_norm": 1.518771767616272,
      "learning_rate": 2.5106044730281463e-05,
      "loss": 1.0521,
      "step": 31720
    },
    {
      "epoch": 0.49108144708841167,
      "grad_norm": 1.451013207435608,
      "learning_rate": 2.5104496255271267e-05,
      "loss": 1.0114,
      "step": 31730
    },
    {
      "epoch": 0.49123621590249567,
      "grad_norm": 1.7656041383743286,
      "learning_rate": 2.5102947780261074e-05,
      "loss": 0.9096,
      "step": 31740
    },
    {
      "epoch": 0.4913909847165796,
      "grad_norm": 1.3362082242965698,
      "learning_rate": 2.5101399305250878e-05,
      "loss": 0.9495,
      "step": 31750
    },
    {
      "epoch": 0.4915457535306636,
      "grad_norm": 1.108310341835022,
      "learning_rate": 2.5099850830240685e-05,
      "loss": 1.1828,
      "step": 31760
    },
    {
      "epoch": 0.49170052234474754,
      "grad_norm": 1.203635811805725,
      "learning_rate": 2.509830235523049e-05,
      "loss": 1.0199,
      "step": 31770
    },
    {
      "epoch": 0.4918552911588315,
      "grad_norm": 1.4815226793289185,
      "learning_rate": 2.50967538802203e-05,
      "loss": 0.9407,
      "step": 31780
    },
    {
      "epoch": 0.49201005997291547,
      "grad_norm": 1.2969292402267456,
      "learning_rate": 2.5095205405210103e-05,
      "loss": 0.9725,
      "step": 31790
    },
    {
      "epoch": 0.4921648287869994,
      "grad_norm": 2.419888973236084,
      "learning_rate": 2.509365693019991e-05,
      "loss": 1.1266,
      "step": 31800
    },
    {
      "epoch": 0.4923195976010834,
      "grad_norm": 1.3822016716003418,
      "learning_rate": 2.5092108455189714e-05,
      "loss": 1.2816,
      "step": 31810
    },
    {
      "epoch": 0.49247436641516734,
      "grad_norm": 1.743038535118103,
      "learning_rate": 2.509055998017952e-05,
      "loss": 1.1103,
      "step": 31820
    },
    {
      "epoch": 0.4926291352292513,
      "grad_norm": 1.8235397338867188,
      "learning_rate": 2.5089011505169325e-05,
      "loss": 1.1061,
      "step": 31830
    },
    {
      "epoch": 0.4927839040433353,
      "grad_norm": 1.3952702283859253,
      "learning_rate": 2.5087463030159132e-05,
      "loss": 1.0998,
      "step": 31840
    },
    {
      "epoch": 0.4929386728574192,
      "grad_norm": 2.0236146450042725,
      "learning_rate": 2.508591455514894e-05,
      "loss": 1.0264,
      "step": 31850
    },
    {
      "epoch": 0.4930934416715032,
      "grad_norm": 1.7778927087783813,
      "learning_rate": 2.5084366080138746e-05,
      "loss": 1.2014,
      "step": 31860
    },
    {
      "epoch": 0.49324821048558715,
      "grad_norm": 1.4382752180099487,
      "learning_rate": 2.508281760512855e-05,
      "loss": 1.1888,
      "step": 31870
    },
    {
      "epoch": 0.49340297929967114,
      "grad_norm": 1.643095850944519,
      "learning_rate": 2.5081269130118357e-05,
      "loss": 1.0401,
      "step": 31880
    },
    {
      "epoch": 0.4935577481137551,
      "grad_norm": 2.1020612716674805,
      "learning_rate": 2.507972065510816e-05,
      "loss": 1.1624,
      "step": 31890
    },
    {
      "epoch": 0.493712516927839,
      "grad_norm": 1.6282310485839844,
      "learning_rate": 2.5078172180097968e-05,
      "loss": 1.2007,
      "step": 31900
    },
    {
      "epoch": 0.493867285741923,
      "grad_norm": 1.5586962699890137,
      "learning_rate": 2.5076623705087772e-05,
      "loss": 1.1692,
      "step": 31910
    },
    {
      "epoch": 0.49402205455600695,
      "grad_norm": 1.3909920454025269,
      "learning_rate": 2.507507523007758e-05,
      "loss": 1.0735,
      "step": 31920
    },
    {
      "epoch": 0.49417682337009095,
      "grad_norm": 1.345874309539795,
      "learning_rate": 2.5073526755067386e-05,
      "loss": 1.0054,
      "step": 31930
    },
    {
      "epoch": 0.4943315921841749,
      "grad_norm": 1.2677462100982666,
      "learning_rate": 2.507197828005719e-05,
      "loss": 1.1731,
      "step": 31940
    },
    {
      "epoch": 0.4944863609982589,
      "grad_norm": 1.5741581916809082,
      "learning_rate": 2.5070429805046997e-05,
      "loss": 1.2209,
      "step": 31950
    },
    {
      "epoch": 0.4946411298123428,
      "grad_norm": 1.1278483867645264,
      "learning_rate": 2.50688813300368e-05,
      "loss": 1.2115,
      "step": 31960
    },
    {
      "epoch": 0.49479589862642676,
      "grad_norm": 1.225592851638794,
      "learning_rate": 2.5067332855026608e-05,
      "loss": 1.0709,
      "step": 31970
    },
    {
      "epoch": 0.49495066744051075,
      "grad_norm": 1.3383105993270874,
      "learning_rate": 2.5065784380016415e-05,
      "loss": 1.1027,
      "step": 31980
    },
    {
      "epoch": 0.4951054362545947,
      "grad_norm": 1.1628931760787964,
      "learning_rate": 2.5064235905006222e-05,
      "loss": 1.0262,
      "step": 31990
    },
    {
      "epoch": 0.4952602050686787,
      "grad_norm": 0.951918363571167,
      "learning_rate": 2.5062687429996026e-05,
      "loss": 1.2538,
      "step": 32000
    },
    {
      "epoch": 0.4954149738827626,
      "grad_norm": 1.5398329496383667,
      "learning_rate": 2.5061138954985833e-05,
      "loss": 0.999,
      "step": 32010
    },
    {
      "epoch": 0.49556974269684656,
      "grad_norm": 1.253078579902649,
      "learning_rate": 2.5059590479975637e-05,
      "loss": 1.1538,
      "step": 32020
    },
    {
      "epoch": 0.49572451151093055,
      "grad_norm": 1.499541997909546,
      "learning_rate": 2.5058042004965444e-05,
      "loss": 0.9574,
      "step": 32030
    },
    {
      "epoch": 0.4958792803250145,
      "grad_norm": 1.2678515911102295,
      "learning_rate": 2.5056493529955248e-05,
      "loss": 0.9415,
      "step": 32040
    },
    {
      "epoch": 0.4960340491390985,
      "grad_norm": 1.6117463111877441,
      "learning_rate": 2.5054945054945058e-05,
      "loss": 1.2322,
      "step": 32050
    },
    {
      "epoch": 0.4961888179531824,
      "grad_norm": 1.5390079021453857,
      "learning_rate": 2.5053396579934862e-05,
      "loss": 1.1982,
      "step": 32060
    },
    {
      "epoch": 0.4963435867672664,
      "grad_norm": 1.5580689907073975,
      "learning_rate": 2.505184810492467e-05,
      "loss": 0.9842,
      "step": 32070
    },
    {
      "epoch": 0.49649835558135036,
      "grad_norm": 2.1815879344940186,
      "learning_rate": 2.5050299629914473e-05,
      "loss": 1.223,
      "step": 32080
    },
    {
      "epoch": 0.4966531243954343,
      "grad_norm": 1.5087511539459229,
      "learning_rate": 2.504875115490428e-05,
      "loss": 0.9926,
      "step": 32090
    },
    {
      "epoch": 0.4968078932095183,
      "grad_norm": 1.8969693183898926,
      "learning_rate": 2.5047202679894084e-05,
      "loss": 0.993,
      "step": 32100
    },
    {
      "epoch": 0.49696266202360223,
      "grad_norm": 1.7296862602233887,
      "learning_rate": 2.504565420488389e-05,
      "loss": 0.9448,
      "step": 32110
    },
    {
      "epoch": 0.4971174308376862,
      "grad_norm": 1.7617251873016357,
      "learning_rate": 2.5044105729873698e-05,
      "loss": 1.186,
      "step": 32120
    },
    {
      "epoch": 0.49727219965177016,
      "grad_norm": 2.1852920055389404,
      "learning_rate": 2.5042557254863505e-05,
      "loss": 0.9953,
      "step": 32130
    },
    {
      "epoch": 0.49742696846585416,
      "grad_norm": 1.7933241128921509,
      "learning_rate": 2.504100877985331e-05,
      "loss": 0.9406,
      "step": 32140
    },
    {
      "epoch": 0.4975817372799381,
      "grad_norm": 1.64176344871521,
      "learning_rate": 2.5039460304843116e-05,
      "loss": 1.1316,
      "step": 32150
    },
    {
      "epoch": 0.49773650609402204,
      "grad_norm": 1.5399055480957031,
      "learning_rate": 2.503791182983292e-05,
      "loss": 1.1229,
      "step": 32160
    },
    {
      "epoch": 0.49789127490810603,
      "grad_norm": 1.2185449600219727,
      "learning_rate": 2.5036363354822723e-05,
      "loss": 1.2079,
      "step": 32170
    },
    {
      "epoch": 0.49804604372218997,
      "grad_norm": 1.6828714609146118,
      "learning_rate": 2.503481487981253e-05,
      "loss": 1.0947,
      "step": 32180
    },
    {
      "epoch": 0.49820081253627396,
      "grad_norm": 1.365743637084961,
      "learning_rate": 2.5033266404802338e-05,
      "loss": 0.9207,
      "step": 32190
    },
    {
      "epoch": 0.4983555813503579,
      "grad_norm": 1.7016288042068481,
      "learning_rate": 2.5031717929792145e-05,
      "loss": 0.9736,
      "step": 32200
    },
    {
      "epoch": 0.49851035016444184,
      "grad_norm": 1.3069044351577759,
      "learning_rate": 2.503016945478195e-05,
      "loss": 1.2266,
      "step": 32210
    },
    {
      "epoch": 0.49866511897852583,
      "grad_norm": 1.7811232805252075,
      "learning_rate": 2.5028620979771756e-05,
      "loss": 0.9816,
      "step": 32220
    },
    {
      "epoch": 0.4988198877926098,
      "grad_norm": 1.4162782430648804,
      "learning_rate": 2.502707250476156e-05,
      "loss": 1.0459,
      "step": 32230
    },
    {
      "epoch": 0.49897465660669377,
      "grad_norm": 1.3507838249206543,
      "learning_rate": 2.5025524029751367e-05,
      "loss": 1.1915,
      "step": 32240
    },
    {
      "epoch": 0.4991294254207777,
      "grad_norm": 1.9253721237182617,
      "learning_rate": 2.502397555474117e-05,
      "loss": 0.9303,
      "step": 32250
    },
    {
      "epoch": 0.4992841942348617,
      "grad_norm": 2.0180766582489014,
      "learning_rate": 2.502242707973098e-05,
      "loss": 1.1694,
      "step": 32260
    },
    {
      "epoch": 0.49943896304894564,
      "grad_norm": 1.9510092735290527,
      "learning_rate": 2.5020878604720785e-05,
      "loss": 1.0194,
      "step": 32270
    },
    {
      "epoch": 0.4995937318630296,
      "grad_norm": 1.4979382753372192,
      "learning_rate": 2.5019330129710592e-05,
      "loss": 1.1831,
      "step": 32280
    },
    {
      "epoch": 0.49974850067711357,
      "grad_norm": 1.581764578819275,
      "learning_rate": 2.5017781654700396e-05,
      "loss": 1.0659,
      "step": 32290
    },
    {
      "epoch": 0.4999032694911975,
      "grad_norm": 1.5013482570648193,
      "learning_rate": 2.5016233179690203e-05,
      "loss": 1.0998,
      "step": 32300
    },
    {
      "epoch": 0.5000580383052815,
      "grad_norm": 1.4185757637023926,
      "learning_rate": 2.5014684704680006e-05,
      "loss": 1.1184,
      "step": 32310
    },
    {
      "epoch": 0.5002128071193654,
      "grad_norm": 1.5551626682281494,
      "learning_rate": 2.5013136229669814e-05,
      "loss": 1.1344,
      "step": 32320
    },
    {
      "epoch": 0.5003675759334494,
      "grad_norm": 1.2155144214630127,
      "learning_rate": 2.501158775465962e-05,
      "loss": 0.9321,
      "step": 32330
    },
    {
      "epoch": 0.5005223447475333,
      "grad_norm": 1.1154412031173706,
      "learning_rate": 2.5010039279649428e-05,
      "loss": 1.0125,
      "step": 32340
    },
    {
      "epoch": 0.5006771135616174,
      "grad_norm": 1.736476182937622,
      "learning_rate": 2.500849080463923e-05,
      "loss": 1.0564,
      "step": 32350
    },
    {
      "epoch": 0.5008318823757013,
      "grad_norm": 1.5228601694107056,
      "learning_rate": 2.500694232962904e-05,
      "loss": 1.0716,
      "step": 32360
    },
    {
      "epoch": 0.5009866511897852,
      "grad_norm": 1.5721917152404785,
      "learning_rate": 2.5005393854618843e-05,
      "loss": 1.0909,
      "step": 32370
    },
    {
      "epoch": 0.5011414200038692,
      "grad_norm": 2.2437684535980225,
      "learning_rate": 2.500384537960865e-05,
      "loss": 1.248,
      "step": 32380
    },
    {
      "epoch": 0.5012961888179532,
      "grad_norm": 1.2743340730667114,
      "learning_rate": 2.5002296904598453e-05,
      "loss": 0.9843,
      "step": 32390
    },
    {
      "epoch": 0.5014509576320372,
      "grad_norm": 2.01016902923584,
      "learning_rate": 2.5000748429588264e-05,
      "loss": 1.1804,
      "step": 32400
    },
    {
      "epoch": 0.5016057264461211,
      "grad_norm": 1.4479840993881226,
      "learning_rate": 2.4999199954578068e-05,
      "loss": 1.1193,
      "step": 32410
    },
    {
      "epoch": 0.501760495260205,
      "grad_norm": 2.5680975914001465,
      "learning_rate": 2.499765147956787e-05,
      "loss": 1.1261,
      "step": 32420
    },
    {
      "epoch": 0.501915264074289,
      "grad_norm": 1.3462332487106323,
      "learning_rate": 2.499610300455768e-05,
      "loss": 1.1792,
      "step": 32430
    },
    {
      "epoch": 0.502070032888373,
      "grad_norm": 1.2530821561813354,
      "learning_rate": 2.4994554529547482e-05,
      "loss": 0.9866,
      "step": 32440
    },
    {
      "epoch": 0.502224801702457,
      "grad_norm": 1.4487359523773193,
      "learning_rate": 2.499300605453729e-05,
      "loss": 1.0703,
      "step": 32450
    },
    {
      "epoch": 0.5023795705165409,
      "grad_norm": 1.4595412015914917,
      "learning_rate": 2.4991457579527097e-05,
      "loss": 0.9042,
      "step": 32460
    },
    {
      "epoch": 0.5025343393306249,
      "grad_norm": 1.1942306756973267,
      "learning_rate": 2.4989909104516904e-05,
      "loss": 0.9164,
      "step": 32470
    },
    {
      "epoch": 0.5026891081447088,
      "grad_norm": 1.5164250135421753,
      "learning_rate": 2.4988360629506708e-05,
      "loss": 1.1916,
      "step": 32480
    },
    {
      "epoch": 0.5028438769587928,
      "grad_norm": 1.3265984058380127,
      "learning_rate": 2.4986812154496515e-05,
      "loss": 1.1252,
      "step": 32490
    },
    {
      "epoch": 0.5029986457728768,
      "grad_norm": 1.4098961353302002,
      "learning_rate": 2.498526367948632e-05,
      "loss": 1.1646,
      "step": 32500
    },
    {
      "epoch": 0.5031534145869607,
      "grad_norm": 1.9726250171661377,
      "learning_rate": 2.4983715204476126e-05,
      "loss": 0.9733,
      "step": 32510
    },
    {
      "epoch": 0.5033081834010447,
      "grad_norm": 1.7301135063171387,
      "learning_rate": 2.498216672946593e-05,
      "loss": 0.9568,
      "step": 32520
    },
    {
      "epoch": 0.5034629522151286,
      "grad_norm": 1.2209196090698242,
      "learning_rate": 2.498061825445574e-05,
      "loss": 0.9497,
      "step": 32530
    },
    {
      "epoch": 0.5036177210292127,
      "grad_norm": 1.4398638010025024,
      "learning_rate": 2.4979069779445544e-05,
      "loss": 1.0808,
      "step": 32540
    },
    {
      "epoch": 0.5037724898432966,
      "grad_norm": 1.4550732374191284,
      "learning_rate": 2.497752130443535e-05,
      "loss": 1.3312,
      "step": 32550
    },
    {
      "epoch": 0.5039272586573805,
      "grad_norm": 1.632226824760437,
      "learning_rate": 2.4975972829425154e-05,
      "loss": 1.0797,
      "step": 32560
    },
    {
      "epoch": 0.5040820274714645,
      "grad_norm": 1.3899842500686646,
      "learning_rate": 2.497442435441496e-05,
      "loss": 1.1975,
      "step": 32570
    },
    {
      "epoch": 0.5042367962855485,
      "grad_norm": 1.1162967681884766,
      "learning_rate": 2.4972875879404765e-05,
      "loss": 1.0304,
      "step": 32580
    },
    {
      "epoch": 0.5043915650996325,
      "grad_norm": 1.736984133720398,
      "learning_rate": 2.4971327404394572e-05,
      "loss": 0.9475,
      "step": 32590
    },
    {
      "epoch": 0.5045463339137164,
      "grad_norm": 1.6971324682235718,
      "learning_rate": 2.496977892938438e-05,
      "loss": 0.9603,
      "step": 32600
    },
    {
      "epoch": 0.5047011027278003,
      "grad_norm": 2.0724337100982666,
      "learning_rate": 2.4968230454374187e-05,
      "loss": 1.2671,
      "step": 32610
    },
    {
      "epoch": 0.5048558715418843,
      "grad_norm": 2.0050413608551025,
      "learning_rate": 2.496668197936399e-05,
      "loss": 1.2169,
      "step": 32620
    },
    {
      "epoch": 0.5050106403559683,
      "grad_norm": 1.6830973625183105,
      "learning_rate": 2.4965133504353798e-05,
      "loss": 1.0815,
      "step": 32630
    },
    {
      "epoch": 0.5051654091700523,
      "grad_norm": 1.7851903438568115,
      "learning_rate": 2.49635850293436e-05,
      "loss": 0.871,
      "step": 32640
    },
    {
      "epoch": 0.5053201779841362,
      "grad_norm": 1.6821088790893555,
      "learning_rate": 2.496203655433341e-05,
      "loss": 1.0178,
      "step": 32650
    },
    {
      "epoch": 0.5054749467982201,
      "grad_norm": 1.5496035814285278,
      "learning_rate": 2.4960488079323212e-05,
      "loss": 1.0066,
      "step": 32660
    },
    {
      "epoch": 0.5056297156123041,
      "grad_norm": 1.9228914976119995,
      "learning_rate": 2.4958939604313023e-05,
      "loss": 1.0271,
      "step": 32670
    },
    {
      "epoch": 0.5057844844263881,
      "grad_norm": 1.3285447359085083,
      "learning_rate": 2.4957391129302827e-05,
      "loss": 1.0797,
      "step": 32680
    },
    {
      "epoch": 0.5059392532404721,
      "grad_norm": 1.5100464820861816,
      "learning_rate": 2.495584265429263e-05,
      "loss": 1.0892,
      "step": 32690
    },
    {
      "epoch": 0.506094022054556,
      "grad_norm": 1.058948040008545,
      "learning_rate": 2.4954294179282437e-05,
      "loss": 1.2257,
      "step": 32700
    },
    {
      "epoch": 0.5062487908686399,
      "grad_norm": 2.097762107849121,
      "learning_rate": 2.495274570427224e-05,
      "loss": 1.0334,
      "step": 32710
    },
    {
      "epoch": 0.5064035596827239,
      "grad_norm": 1.895219326019287,
      "learning_rate": 2.495119722926205e-05,
      "loss": 1.2268,
      "step": 32720
    },
    {
      "epoch": 0.5065583284968079,
      "grad_norm": 1.9475547075271606,
      "learning_rate": 2.4949648754251852e-05,
      "loss": 1.0931,
      "step": 32730
    },
    {
      "epoch": 0.5067130973108919,
      "grad_norm": 2.2430572509765625,
      "learning_rate": 2.4948100279241663e-05,
      "loss": 1.0948,
      "step": 32740
    },
    {
      "epoch": 0.5068678661249758,
      "grad_norm": 1.3609901666641235,
      "learning_rate": 2.4946551804231466e-05,
      "loss": 1.2908,
      "step": 32750
    },
    {
      "epoch": 0.5070226349390597,
      "grad_norm": 1.5226516723632812,
      "learning_rate": 2.4945003329221274e-05,
      "loss": 1.1266,
      "step": 32760
    },
    {
      "epoch": 0.5071774037531438,
      "grad_norm": 1.6635743379592896,
      "learning_rate": 2.4943454854211077e-05,
      "loss": 1.2185,
      "step": 32770
    },
    {
      "epoch": 0.5073321725672277,
      "grad_norm": 2.070939302444458,
      "learning_rate": 2.4941906379200884e-05,
      "loss": 1.1977,
      "step": 32780
    },
    {
      "epoch": 0.5074869413813117,
      "grad_norm": 1.4797827005386353,
      "learning_rate": 2.4940357904190688e-05,
      "loss": 1.1312,
      "step": 32790
    },
    {
      "epoch": 0.5076417101953956,
      "grad_norm": 1.7222684621810913,
      "learning_rate": 2.4938809429180495e-05,
      "loss": 1.1571,
      "step": 32800
    },
    {
      "epoch": 0.5077964790094796,
      "grad_norm": 1.1914860010147095,
      "learning_rate": 2.4937260954170302e-05,
      "loss": 1.0469,
      "step": 32810
    },
    {
      "epoch": 0.5079512478235636,
      "grad_norm": 1.5412216186523438,
      "learning_rate": 2.493571247916011e-05,
      "loss": 1.2262,
      "step": 32820
    },
    {
      "epoch": 0.5081060166376475,
      "grad_norm": 1.5411334037780762,
      "learning_rate": 2.4934164004149913e-05,
      "loss": 1.137,
      "step": 32830
    },
    {
      "epoch": 0.5082607854517315,
      "grad_norm": 1.6098551750183105,
      "learning_rate": 2.493261552913972e-05,
      "loss": 1.0622,
      "step": 32840
    },
    {
      "epoch": 0.5084155542658154,
      "grad_norm": 1.3201748132705688,
      "learning_rate": 2.4931067054129524e-05,
      "loss": 0.9189,
      "step": 32850
    },
    {
      "epoch": 0.5085703230798994,
      "grad_norm": 2.375793933868408,
      "learning_rate": 2.492951857911933e-05,
      "loss": 1.1685,
      "step": 32860
    },
    {
      "epoch": 0.5087250918939834,
      "grad_norm": 1.4703900814056396,
      "learning_rate": 2.4927970104109135e-05,
      "loss": 1.0352,
      "step": 32870
    },
    {
      "epoch": 0.5088798607080673,
      "grad_norm": 1.2067502737045288,
      "learning_rate": 2.4926421629098946e-05,
      "loss": 1.108,
      "step": 32880
    },
    {
      "epoch": 0.5090346295221513,
      "grad_norm": 1.4105569124221802,
      "learning_rate": 2.492487315408875e-05,
      "loss": 1.0821,
      "step": 32890
    },
    {
      "epoch": 0.5091893983362352,
      "grad_norm": 2.075727701187134,
      "learning_rate": 2.4923324679078557e-05,
      "loss": 1.0027,
      "step": 32900
    },
    {
      "epoch": 0.5093441671503192,
      "grad_norm": 1.5120570659637451,
      "learning_rate": 2.492177620406836e-05,
      "loss": 1.0228,
      "step": 32910
    },
    {
      "epoch": 0.5094989359644032,
      "grad_norm": 1.2505297660827637,
      "learning_rate": 2.4920227729058167e-05,
      "loss": 1.0317,
      "step": 32920
    },
    {
      "epoch": 0.5096537047784871,
      "grad_norm": 1.3854955434799194,
      "learning_rate": 2.491867925404797e-05,
      "loss": 1.1466,
      "step": 32930
    },
    {
      "epoch": 0.5098084735925711,
      "grad_norm": 1.6566205024719238,
      "learning_rate": 2.4917130779037778e-05,
      "loss": 1.0717,
      "step": 32940
    },
    {
      "epoch": 0.509963242406655,
      "grad_norm": 1.2240160703659058,
      "learning_rate": 2.4915582304027585e-05,
      "loss": 1.0343,
      "step": 32950
    },
    {
      "epoch": 0.5101180112207391,
      "grad_norm": 1.7988851070404053,
      "learning_rate": 2.491403382901739e-05,
      "loss": 1.1294,
      "step": 32960
    },
    {
      "epoch": 0.510272780034823,
      "grad_norm": 1.6919180154800415,
      "learning_rate": 2.4912485354007196e-05,
      "loss": 1.0743,
      "step": 32970
    },
    {
      "epoch": 0.510427548848907,
      "grad_norm": 1.5865669250488281,
      "learning_rate": 2.4910936878997e-05,
      "loss": 0.8277,
      "step": 32980
    },
    {
      "epoch": 0.5105823176629909,
      "grad_norm": 1.7288074493408203,
      "learning_rate": 2.4909388403986807e-05,
      "loss": 1.0988,
      "step": 32990
    },
    {
      "epoch": 0.5107370864770748,
      "grad_norm": 1.3474721908569336,
      "learning_rate": 2.490783992897661e-05,
      "loss": 1.2324,
      "step": 33000
    },
    {
      "epoch": 0.5108918552911589,
      "grad_norm": 1.352034330368042,
      "learning_rate": 2.490629145396642e-05,
      "loss": 0.9196,
      "step": 33010
    },
    {
      "epoch": 0.5110466241052428,
      "grad_norm": 1.4199354648590088,
      "learning_rate": 2.4904742978956225e-05,
      "loss": 1.2673,
      "step": 33020
    },
    {
      "epoch": 0.5112013929193268,
      "grad_norm": 1.8403233289718628,
      "learning_rate": 2.4903194503946032e-05,
      "loss": 1.1401,
      "step": 33030
    },
    {
      "epoch": 0.5113561617334107,
      "grad_norm": 1.2966798543930054,
      "learning_rate": 2.4901646028935836e-05,
      "loss": 0.982,
      "step": 33040
    },
    {
      "epoch": 0.5115109305474946,
      "grad_norm": 1.6191054582595825,
      "learning_rate": 2.4900097553925643e-05,
      "loss": 0.9532,
      "step": 33050
    },
    {
      "epoch": 0.5116656993615787,
      "grad_norm": 1.770686149597168,
      "learning_rate": 2.4898549078915447e-05,
      "loss": 0.8877,
      "step": 33060
    },
    {
      "epoch": 0.5118204681756626,
      "grad_norm": 1.9920278787612915,
      "learning_rate": 2.4897000603905254e-05,
      "loss": 1.0653,
      "step": 33070
    },
    {
      "epoch": 0.5119752369897466,
      "grad_norm": 1.7348401546478271,
      "learning_rate": 2.489545212889506e-05,
      "loss": 1.0143,
      "step": 33080
    },
    {
      "epoch": 0.5121300058038305,
      "grad_norm": 1.466315746307373,
      "learning_rate": 2.489390365388487e-05,
      "loss": 1.0924,
      "step": 33090
    },
    {
      "epoch": 0.5122847746179144,
      "grad_norm": 1.5604121685028076,
      "learning_rate": 2.4892355178874672e-05,
      "loss": 1.0358,
      "step": 33100
    },
    {
      "epoch": 0.5124395434319985,
      "grad_norm": 1.5594128370285034,
      "learning_rate": 2.489080670386448e-05,
      "loss": 1.1414,
      "step": 33110
    },
    {
      "epoch": 0.5125943122460824,
      "grad_norm": 1.0582679510116577,
      "learning_rate": 2.4889258228854283e-05,
      "loss": 1.1915,
      "step": 33120
    },
    {
      "epoch": 0.5127490810601664,
      "grad_norm": 2.11921763420105,
      "learning_rate": 2.488770975384409e-05,
      "loss": 0.9867,
      "step": 33130
    },
    {
      "epoch": 0.5129038498742503,
      "grad_norm": 1.4881043434143066,
      "learning_rate": 2.4886161278833894e-05,
      "loss": 1.0917,
      "step": 33140
    },
    {
      "epoch": 0.5130586186883344,
      "grad_norm": 1.3307331800460815,
      "learning_rate": 2.4884612803823705e-05,
      "loss": 1.0612,
      "step": 33150
    },
    {
      "epoch": 0.5132133875024183,
      "grad_norm": 1.1505876779556274,
      "learning_rate": 2.4883064328813508e-05,
      "loss": 1.2102,
      "step": 33160
    },
    {
      "epoch": 0.5133681563165022,
      "grad_norm": 1.414427638053894,
      "learning_rate": 2.4881515853803315e-05,
      "loss": 1.0265,
      "step": 33170
    },
    {
      "epoch": 0.5135229251305862,
      "grad_norm": 2.1681947708129883,
      "learning_rate": 2.487996737879312e-05,
      "loss": 0.9944,
      "step": 33180
    },
    {
      "epoch": 0.5136776939446701,
      "grad_norm": 1.571521520614624,
      "learning_rate": 2.4878418903782923e-05,
      "loss": 1.1151,
      "step": 33190
    },
    {
      "epoch": 0.5138324627587542,
      "grad_norm": 2.2438650131225586,
      "learning_rate": 2.487687042877273e-05,
      "loss": 1.1227,
      "step": 33200
    },
    {
      "epoch": 0.5139872315728381,
      "grad_norm": 1.6209979057312012,
      "learning_rate": 2.4875321953762534e-05,
      "loss": 1.0624,
      "step": 33210
    },
    {
      "epoch": 0.514142000386922,
      "grad_norm": 2.0995569229125977,
      "learning_rate": 2.4873773478752344e-05,
      "loss": 0.9878,
      "step": 33220
    },
    {
      "epoch": 0.514296769201006,
      "grad_norm": 1.7477537393569946,
      "learning_rate": 2.4872225003742148e-05,
      "loss": 0.8577,
      "step": 33230
    },
    {
      "epoch": 0.5144515380150899,
      "grad_norm": 1.1875728368759155,
      "learning_rate": 2.4870676528731955e-05,
      "loss": 1.1077,
      "step": 33240
    },
    {
      "epoch": 0.514606306829174,
      "grad_norm": 1.6789768934249878,
      "learning_rate": 2.486912805372176e-05,
      "loss": 1.0223,
      "step": 33250
    },
    {
      "epoch": 0.5147610756432579,
      "grad_norm": 2.216966390609741,
      "learning_rate": 2.4867579578711566e-05,
      "loss": 0.8941,
      "step": 33260
    },
    {
      "epoch": 0.5149158444573418,
      "grad_norm": 1.3845518827438354,
      "learning_rate": 2.486603110370137e-05,
      "loss": 1.0762,
      "step": 33270
    },
    {
      "epoch": 0.5150706132714258,
      "grad_norm": 1.6095410585403442,
      "learning_rate": 2.4864482628691177e-05,
      "loss": 1.1235,
      "step": 33280
    },
    {
      "epoch": 0.5152253820855097,
      "grad_norm": 1.0919588804244995,
      "learning_rate": 2.4862934153680984e-05,
      "loss": 1.0111,
      "step": 33290
    },
    {
      "epoch": 0.5153801508995938,
      "grad_norm": 1.7557556629180908,
      "learning_rate": 2.486138567867079e-05,
      "loss": 1.0132,
      "step": 33300
    },
    {
      "epoch": 0.5155349197136777,
      "grad_norm": 1.7040258646011353,
      "learning_rate": 2.4859837203660595e-05,
      "loss": 1.0969,
      "step": 33310
    },
    {
      "epoch": 0.5156896885277616,
      "grad_norm": 1.404467225074768,
      "learning_rate": 2.4858288728650402e-05,
      "loss": 0.9965,
      "step": 33320
    },
    {
      "epoch": 0.5158444573418456,
      "grad_norm": 1.5758562088012695,
      "learning_rate": 2.4856740253640206e-05,
      "loss": 1.0211,
      "step": 33330
    },
    {
      "epoch": 0.5159992261559296,
      "grad_norm": 1.7118637561798096,
      "learning_rate": 2.4855191778630013e-05,
      "loss": 1.1204,
      "step": 33340
    },
    {
      "epoch": 0.5161539949700136,
      "grad_norm": 1.6547566652297974,
      "learning_rate": 2.485364330361982e-05,
      "loss": 0.9829,
      "step": 33350
    },
    {
      "epoch": 0.5163087637840975,
      "grad_norm": 1.4519178867340088,
      "learning_rate": 2.4852094828609627e-05,
      "loss": 1.0488,
      "step": 33360
    },
    {
      "epoch": 0.5164635325981815,
      "grad_norm": 1.4269157648086548,
      "learning_rate": 2.485054635359943e-05,
      "loss": 0.9398,
      "step": 33370
    },
    {
      "epoch": 0.5166183014122654,
      "grad_norm": 1.327457070350647,
      "learning_rate": 2.4848997878589238e-05,
      "loss": 1.1597,
      "step": 33380
    },
    {
      "epoch": 0.5167730702263494,
      "grad_norm": 1.3110852241516113,
      "learning_rate": 2.4847449403579042e-05,
      "loss": 0.843,
      "step": 33390
    },
    {
      "epoch": 0.5169278390404334,
      "grad_norm": 1.3735893964767456,
      "learning_rate": 2.484590092856885e-05,
      "loss": 1.1651,
      "step": 33400
    },
    {
      "epoch": 0.5170826078545173,
      "grad_norm": 1.8458884954452515,
      "learning_rate": 2.4844352453558653e-05,
      "loss": 0.9934,
      "step": 33410
    },
    {
      "epoch": 0.5172373766686013,
      "grad_norm": 1.5309017896652222,
      "learning_rate": 2.4842803978548463e-05,
      "loss": 1.1178,
      "step": 33420
    },
    {
      "epoch": 0.5173921454826852,
      "grad_norm": 1.3209208250045776,
      "learning_rate": 2.4841255503538267e-05,
      "loss": 1.0066,
      "step": 33430
    },
    {
      "epoch": 0.5175469142967692,
      "grad_norm": 1.9262927770614624,
      "learning_rate": 2.483970702852807e-05,
      "loss": 1.0444,
      "step": 33440
    },
    {
      "epoch": 0.5177016831108532,
      "grad_norm": 1.5943697690963745,
      "learning_rate": 2.4838158553517878e-05,
      "loss": 1.0291,
      "step": 33450
    },
    {
      "epoch": 0.5178564519249371,
      "grad_norm": 1.2459076642990112,
      "learning_rate": 2.4836610078507682e-05,
      "loss": 1.2832,
      "step": 33460
    },
    {
      "epoch": 0.5180112207390211,
      "grad_norm": 1.2780251502990723,
      "learning_rate": 2.483506160349749e-05,
      "loss": 1.0085,
      "step": 33470
    },
    {
      "epoch": 0.518165989553105,
      "grad_norm": 1.6460922956466675,
      "learning_rate": 2.4833513128487293e-05,
      "loss": 1.0081,
      "step": 33480
    },
    {
      "epoch": 0.518320758367189,
      "grad_norm": 1.3470536470413208,
      "learning_rate": 2.4831964653477103e-05,
      "loss": 1.0189,
      "step": 33490
    },
    {
      "epoch": 0.518475527181273,
      "grad_norm": 1.543990135192871,
      "learning_rate": 2.4830416178466907e-05,
      "loss": 1.0934,
      "step": 33500
    },
    {
      "epoch": 0.5186302959953569,
      "grad_norm": 1.3883464336395264,
      "learning_rate": 2.4828867703456714e-05,
      "loss": 1.1434,
      "step": 33510
    },
    {
      "epoch": 0.5187850648094409,
      "grad_norm": 1.2732131481170654,
      "learning_rate": 2.4827319228446518e-05,
      "loss": 0.9248,
      "step": 33520
    },
    {
      "epoch": 0.5189398336235248,
      "grad_norm": 1.3062279224395752,
      "learning_rate": 2.4825770753436325e-05,
      "loss": 0.9695,
      "step": 33530
    },
    {
      "epoch": 0.5190946024376089,
      "grad_norm": 1.8834179639816284,
      "learning_rate": 2.482422227842613e-05,
      "loss": 1.2124,
      "step": 33540
    },
    {
      "epoch": 0.5192493712516928,
      "grad_norm": 2.0482985973358154,
      "learning_rate": 2.4822673803415936e-05,
      "loss": 1.0231,
      "step": 33550
    },
    {
      "epoch": 0.5194041400657767,
      "grad_norm": 1.145775318145752,
      "learning_rate": 2.4821125328405743e-05,
      "loss": 1.0444,
      "step": 33560
    },
    {
      "epoch": 0.5195589088798607,
      "grad_norm": 1.3580646514892578,
      "learning_rate": 2.481957685339555e-05,
      "loss": 1.1518,
      "step": 33570
    },
    {
      "epoch": 0.5197136776939447,
      "grad_norm": 1.3176097869873047,
      "learning_rate": 2.4818028378385354e-05,
      "loss": 1.2684,
      "step": 33580
    },
    {
      "epoch": 0.5198684465080287,
      "grad_norm": 1.1595145463943481,
      "learning_rate": 2.481647990337516e-05,
      "loss": 0.9957,
      "step": 33590
    },
    {
      "epoch": 0.5200232153221126,
      "grad_norm": 1.6881572008132935,
      "learning_rate": 2.4814931428364965e-05,
      "loss": 1.2428,
      "step": 33600
    },
    {
      "epoch": 0.5201779841361965,
      "grad_norm": 1.6377819776535034,
      "learning_rate": 2.4813382953354772e-05,
      "loss": 1.0144,
      "step": 33610
    },
    {
      "epoch": 0.5203327529502805,
      "grad_norm": 1.3594855070114136,
      "learning_rate": 2.4811834478344576e-05,
      "loss": 1.0219,
      "step": 33620
    },
    {
      "epoch": 0.5204875217643645,
      "grad_norm": 2.096132278442383,
      "learning_rate": 2.4810286003334386e-05,
      "loss": 0.8806,
      "step": 33630
    },
    {
      "epoch": 0.5206422905784485,
      "grad_norm": 1.3302807807922363,
      "learning_rate": 2.480873752832419e-05,
      "loss": 1.1761,
      "step": 33640
    },
    {
      "epoch": 0.5207970593925324,
      "grad_norm": 1.920873761177063,
      "learning_rate": 2.4807189053313997e-05,
      "loss": 1.1144,
      "step": 33650
    },
    {
      "epoch": 0.5209518282066163,
      "grad_norm": 1.782774806022644,
      "learning_rate": 2.48056405783038e-05,
      "loss": 0.9835,
      "step": 33660
    },
    {
      "epoch": 0.5211065970207003,
      "grad_norm": 1.3725111484527588,
      "learning_rate": 2.4804092103293608e-05,
      "loss": 1.1545,
      "step": 33670
    },
    {
      "epoch": 0.5212613658347843,
      "grad_norm": 1.3078477382659912,
      "learning_rate": 2.4802543628283412e-05,
      "loss": 1.1133,
      "step": 33680
    },
    {
      "epoch": 0.5214161346488683,
      "grad_norm": 1.313854694366455,
      "learning_rate": 2.4800995153273215e-05,
      "loss": 0.9472,
      "step": 33690
    },
    {
      "epoch": 0.5215709034629522,
      "grad_norm": 1.3884379863739014,
      "learning_rate": 2.4799446678263026e-05,
      "loss": 0.8936,
      "step": 33700
    },
    {
      "epoch": 0.5217256722770361,
      "grad_norm": 2.3222432136535645,
      "learning_rate": 2.479789820325283e-05,
      "loss": 1.1756,
      "step": 33710
    },
    {
      "epoch": 0.5218804410911201,
      "grad_norm": 1.2151392698287964,
      "learning_rate": 2.4796349728242637e-05,
      "loss": 0.9135,
      "step": 33720
    },
    {
      "epoch": 0.5220352099052041,
      "grad_norm": 1.6500630378723145,
      "learning_rate": 2.479480125323244e-05,
      "loss": 1.0878,
      "step": 33730
    },
    {
      "epoch": 0.5221899787192881,
      "grad_norm": 1.5976507663726807,
      "learning_rate": 2.4793252778222248e-05,
      "loss": 1.185,
      "step": 33740
    },
    {
      "epoch": 0.522344747533372,
      "grad_norm": 1.9657524824142456,
      "learning_rate": 2.479170430321205e-05,
      "loss": 1.0608,
      "step": 33750
    },
    {
      "epoch": 0.522499516347456,
      "grad_norm": 1.7058757543563843,
      "learning_rate": 2.479015582820186e-05,
      "loss": 1.0382,
      "step": 33760
    },
    {
      "epoch": 0.52265428516154,
      "grad_norm": 1.5922549962997437,
      "learning_rate": 2.4788607353191666e-05,
      "loss": 1.0448,
      "step": 33770
    },
    {
      "epoch": 0.5228090539756239,
      "grad_norm": 1.5163850784301758,
      "learning_rate": 2.4787058878181473e-05,
      "loss": 1.1976,
      "step": 33780
    },
    {
      "epoch": 0.5229638227897079,
      "grad_norm": 1.366933822631836,
      "learning_rate": 2.4785510403171277e-05,
      "loss": 1.1113,
      "step": 33790
    },
    {
      "epoch": 0.5231185916037918,
      "grad_norm": 1.4132566452026367,
      "learning_rate": 2.4783961928161084e-05,
      "loss": 1.1038,
      "step": 33800
    },
    {
      "epoch": 0.5232733604178758,
      "grad_norm": 1.0596013069152832,
      "learning_rate": 2.4782413453150888e-05,
      "loss": 0.9692,
      "step": 33810
    },
    {
      "epoch": 0.5234281292319598,
      "grad_norm": 1.5230035781860352,
      "learning_rate": 2.4780864978140695e-05,
      "loss": 1.0461,
      "step": 33820
    },
    {
      "epoch": 0.5235828980460437,
      "grad_norm": 1.7250384092330933,
      "learning_rate": 2.4779316503130502e-05,
      "loss": 0.9268,
      "step": 33830
    },
    {
      "epoch": 0.5237376668601277,
      "grad_norm": 1.328040599822998,
      "learning_rate": 2.477776802812031e-05,
      "loss": 1.19,
      "step": 33840
    },
    {
      "epoch": 0.5238924356742116,
      "grad_norm": 1.8154494762420654,
      "learning_rate": 2.4776219553110113e-05,
      "loss": 0.9836,
      "step": 33850
    },
    {
      "epoch": 0.5240472044882956,
      "grad_norm": 1.3563792705535889,
      "learning_rate": 2.477467107809992e-05,
      "loss": 1.0702,
      "step": 33860
    },
    {
      "epoch": 0.5242019733023796,
      "grad_norm": 1.387587547302246,
      "learning_rate": 2.4773122603089724e-05,
      "loss": 0.8461,
      "step": 33870
    },
    {
      "epoch": 0.5243567421164635,
      "grad_norm": 1.1330641508102417,
      "learning_rate": 2.477157412807953e-05,
      "loss": 0.9428,
      "step": 33880
    },
    {
      "epoch": 0.5245115109305475,
      "grad_norm": 1.6228108406066895,
      "learning_rate": 2.4770025653069335e-05,
      "loss": 0.9405,
      "step": 33890
    },
    {
      "epoch": 0.5246662797446314,
      "grad_norm": 1.488457202911377,
      "learning_rate": 2.4768477178059145e-05,
      "loss": 0.8979,
      "step": 33900
    },
    {
      "epoch": 0.5248210485587154,
      "grad_norm": 1.4505964517593384,
      "learning_rate": 2.476692870304895e-05,
      "loss": 1.2124,
      "step": 33910
    },
    {
      "epoch": 0.5249758173727994,
      "grad_norm": 1.2012940645217896,
      "learning_rate": 2.4765380228038756e-05,
      "loss": 1.0337,
      "step": 33920
    },
    {
      "epoch": 0.5251305861868834,
      "grad_norm": 1.0306103229522705,
      "learning_rate": 2.476383175302856e-05,
      "loss": 1.0212,
      "step": 33930
    },
    {
      "epoch": 0.5252853550009673,
      "grad_norm": 3.1628477573394775,
      "learning_rate": 2.4762283278018363e-05,
      "loss": 1.2075,
      "step": 33940
    },
    {
      "epoch": 0.5254401238150512,
      "grad_norm": 1.2435616254806519,
      "learning_rate": 2.476073480300817e-05,
      "loss": 0.8459,
      "step": 33950
    },
    {
      "epoch": 0.5255948926291353,
      "grad_norm": 1.2616198062896729,
      "learning_rate": 2.4759186327997974e-05,
      "loss": 1.0597,
      "step": 33960
    },
    {
      "epoch": 0.5257496614432192,
      "grad_norm": 1.4250943660736084,
      "learning_rate": 2.4757637852987785e-05,
      "loss": 1.0587,
      "step": 33970
    },
    {
      "epoch": 0.5259044302573032,
      "grad_norm": 1.7027981281280518,
      "learning_rate": 2.475608937797759e-05,
      "loss": 1.0895,
      "step": 33980
    },
    {
      "epoch": 0.5260591990713871,
      "grad_norm": 2.122018337249756,
      "learning_rate": 2.4754540902967396e-05,
      "loss": 1.1942,
      "step": 33990
    },
    {
      "epoch": 0.526213967885471,
      "grad_norm": 1.2789483070373535,
      "learning_rate": 2.47529924279572e-05,
      "loss": 1.235,
      "step": 34000
    },
    {
      "epoch": 0.5263687366995551,
      "grad_norm": 1.6055365800857544,
      "learning_rate": 2.4751443952947007e-05,
      "loss": 0.9949,
      "step": 34010
    },
    {
      "epoch": 0.526523505513639,
      "grad_norm": 1.6471829414367676,
      "learning_rate": 2.474989547793681e-05,
      "loss": 1.0355,
      "step": 34020
    },
    {
      "epoch": 0.526678274327723,
      "grad_norm": 1.375406265258789,
      "learning_rate": 2.4748347002926618e-05,
      "loss": 1.0301,
      "step": 34030
    },
    {
      "epoch": 0.5268330431418069,
      "grad_norm": 1.9914060831069946,
      "learning_rate": 2.4746798527916425e-05,
      "loss": 0.9211,
      "step": 34040
    },
    {
      "epoch": 0.5269878119558908,
      "grad_norm": 1.8942917585372925,
      "learning_rate": 2.4745250052906232e-05,
      "loss": 0.9123,
      "step": 34050
    },
    {
      "epoch": 0.5271425807699749,
      "grad_norm": 1.2502628564834595,
      "learning_rate": 2.4743701577896036e-05,
      "loss": 0.9847,
      "step": 34060
    },
    {
      "epoch": 0.5272973495840588,
      "grad_norm": 1.8789238929748535,
      "learning_rate": 2.4742153102885843e-05,
      "loss": 1.2457,
      "step": 34070
    },
    {
      "epoch": 0.5274521183981428,
      "grad_norm": 1.604902744293213,
      "learning_rate": 2.4740604627875646e-05,
      "loss": 1.1274,
      "step": 34080
    },
    {
      "epoch": 0.5276068872122267,
      "grad_norm": 1.3457086086273193,
      "learning_rate": 2.4739056152865454e-05,
      "loss": 0.9885,
      "step": 34090
    },
    {
      "epoch": 0.5277616560263106,
      "grad_norm": 1.300384283065796,
      "learning_rate": 2.4737507677855257e-05,
      "loss": 1.1174,
      "step": 34100
    },
    {
      "epoch": 0.5279164248403947,
      "grad_norm": 1.3698610067367554,
      "learning_rate": 2.4735959202845068e-05,
      "loss": 1.0181,
      "step": 34110
    },
    {
      "epoch": 0.5280711936544786,
      "grad_norm": 1.3789358139038086,
      "learning_rate": 2.473441072783487e-05,
      "loss": 0.9737,
      "step": 34120
    },
    {
      "epoch": 0.5282259624685626,
      "grad_norm": 1.5257288217544556,
      "learning_rate": 2.473286225282468e-05,
      "loss": 1.131,
      "step": 34130
    },
    {
      "epoch": 0.5283807312826465,
      "grad_norm": 2.0837881565093994,
      "learning_rate": 2.4731313777814482e-05,
      "loss": 0.9948,
      "step": 34140
    },
    {
      "epoch": 0.5285355000967306,
      "grad_norm": 1.5370078086853027,
      "learning_rate": 2.472976530280429e-05,
      "loss": 1.014,
      "step": 34150
    },
    {
      "epoch": 0.5286902689108145,
      "grad_norm": 1.4997388124465942,
      "learning_rate": 2.4728371675295112e-05,
      "loss": 1.1536,
      "step": 34160
    },
    {
      "epoch": 0.5288450377248984,
      "grad_norm": 1.431713342666626,
      "learning_rate": 2.472682320028492e-05,
      "loss": 1.2313,
      "step": 34170
    },
    {
      "epoch": 0.5289998065389824,
      "grad_norm": 1.8549121618270874,
      "learning_rate": 2.4725274725274723e-05,
      "loss": 1.1157,
      "step": 34180
    },
    {
      "epoch": 0.5291545753530663,
      "grad_norm": 1.4770209789276123,
      "learning_rate": 2.4723726250264534e-05,
      "loss": 0.9694,
      "step": 34190
    },
    {
      "epoch": 0.5293093441671504,
      "grad_norm": 1.3434182405471802,
      "learning_rate": 2.4722177775254337e-05,
      "loss": 1.1355,
      "step": 34200
    },
    {
      "epoch": 0.5294641129812343,
      "grad_norm": 1.354243516921997,
      "learning_rate": 2.4720629300244145e-05,
      "loss": 0.8482,
      "step": 34210
    },
    {
      "epoch": 0.5296188817953182,
      "grad_norm": 1.3757976293563843,
      "learning_rate": 2.471908082523395e-05,
      "loss": 1.056,
      "step": 34220
    },
    {
      "epoch": 0.5297736506094022,
      "grad_norm": 1.9876694679260254,
      "learning_rate": 2.4717532350223756e-05,
      "loss": 1.0923,
      "step": 34230
    },
    {
      "epoch": 0.5299284194234861,
      "grad_norm": 1.6279901266098022,
      "learning_rate": 2.471598387521356e-05,
      "loss": 1.0615,
      "step": 34240
    },
    {
      "epoch": 0.5300831882375702,
      "grad_norm": 1.3827089071273804,
      "learning_rate": 2.4714435400203366e-05,
      "loss": 1.0804,
      "step": 34250
    },
    {
      "epoch": 0.5302379570516541,
      "grad_norm": 1.4699733257293701,
      "learning_rate": 2.4712886925193174e-05,
      "loss": 0.9914,
      "step": 34260
    },
    {
      "epoch": 0.530392725865738,
      "grad_norm": 1.4423484802246094,
      "learning_rate": 2.471133845018298e-05,
      "loss": 1.1888,
      "step": 34270
    },
    {
      "epoch": 0.530547494679822,
      "grad_norm": 1.7489078044891357,
      "learning_rate": 2.4709789975172784e-05,
      "loss": 1.0585,
      "step": 34280
    },
    {
      "epoch": 0.5307022634939059,
      "grad_norm": 1.4141021966934204,
      "learning_rate": 2.470824150016259e-05,
      "loss": 1.1242,
      "step": 34290
    },
    {
      "epoch": 0.53085703230799,
      "grad_norm": 1.5278172492980957,
      "learning_rate": 2.4706693025152395e-05,
      "loss": 1.0133,
      "step": 34300
    },
    {
      "epoch": 0.5310118011220739,
      "grad_norm": 1.1279422044754028,
      "learning_rate": 2.4705144550142202e-05,
      "loss": 1.003,
      "step": 34310
    },
    {
      "epoch": 0.5311665699361579,
      "grad_norm": 1.5237185955047607,
      "learning_rate": 2.4703596075132006e-05,
      "loss": 1.087,
      "step": 34320
    },
    {
      "epoch": 0.5313213387502418,
      "grad_norm": 1.4186683893203735,
      "learning_rate": 2.4702047600121817e-05,
      "loss": 1.0397,
      "step": 34330
    },
    {
      "epoch": 0.5314761075643258,
      "grad_norm": 2.1545939445495605,
      "learning_rate": 2.470049912511162e-05,
      "loss": 0.9454,
      "step": 34340
    },
    {
      "epoch": 0.5316308763784098,
      "grad_norm": 1.3111311197280884,
      "learning_rate": 2.4698950650101428e-05,
      "loss": 1.1484,
      "step": 34350
    },
    {
      "epoch": 0.5317856451924937,
      "grad_norm": 1.3605636358261108,
      "learning_rate": 2.469740217509123e-05,
      "loss": 1.2094,
      "step": 34360
    },
    {
      "epoch": 0.5319404140065777,
      "grad_norm": 1.5196011066436768,
      "learning_rate": 2.4695853700081035e-05,
      "loss": 1.1228,
      "step": 34370
    },
    {
      "epoch": 0.5320951828206616,
      "grad_norm": 1.2270883321762085,
      "learning_rate": 2.4694305225070842e-05,
      "loss": 0.8946,
      "step": 34380
    },
    {
      "epoch": 0.5322499516347456,
      "grad_norm": 1.4683942794799805,
      "learning_rate": 2.4692756750060646e-05,
      "loss": 1.1198,
      "step": 34390
    },
    {
      "epoch": 0.5324047204488296,
      "grad_norm": 1.3378872871398926,
      "learning_rate": 2.4691208275050457e-05,
      "loss": 0.9494,
      "step": 34400
    },
    {
      "epoch": 0.5325594892629135,
      "grad_norm": 1.1801701784133911,
      "learning_rate": 2.468965980004026e-05,
      "loss": 0.9611,
      "step": 34410
    },
    {
      "epoch": 0.5327142580769975,
      "grad_norm": 1.6046205759048462,
      "learning_rate": 2.4688111325030067e-05,
      "loss": 1.1705,
      "step": 34420
    },
    {
      "epoch": 0.5328690268910814,
      "grad_norm": 1.899935245513916,
      "learning_rate": 2.468656285001987e-05,
      "loss": 1.085,
      "step": 34430
    },
    {
      "epoch": 0.5330237957051654,
      "grad_norm": 1.4022985696792603,
      "learning_rate": 2.468501437500968e-05,
      "loss": 1.0698,
      "step": 34440
    },
    {
      "epoch": 0.5331785645192494,
      "grad_norm": 1.4843751192092896,
      "learning_rate": 2.4683465899999482e-05,
      "loss": 1.0796,
      "step": 34450
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.5319418907165527,
      "learning_rate": 2.4681917424989293e-05,
      "loss": 1.0793,
      "step": 34460
    },
    {
      "epoch": 0.5334881021474173,
      "grad_norm": 1.319376826286316,
      "learning_rate": 2.4680368949979096e-05,
      "loss": 0.9816,
      "step": 34470
    },
    {
      "epoch": 0.5336428709615012,
      "grad_norm": 1.0542312860488892,
      "learning_rate": 2.4678820474968904e-05,
      "loss": 0.9808,
      "step": 34480
    },
    {
      "epoch": 0.5337976397755853,
      "grad_norm": 1.2834179401397705,
      "learning_rate": 2.4677271999958707e-05,
      "loss": 1.2373,
      "step": 34490
    },
    {
      "epoch": 0.5339524085896692,
      "grad_norm": 1.8727768659591675,
      "learning_rate": 2.4675723524948514e-05,
      "loss": 0.9653,
      "step": 34500
    },
    {
      "epoch": 0.5341071774037531,
      "grad_norm": 1.4522110223770142,
      "learning_rate": 2.4674175049938318e-05,
      "loss": 1.1318,
      "step": 34510
    },
    {
      "epoch": 0.5342619462178371,
      "grad_norm": 1.192397952079773,
      "learning_rate": 2.4672626574928125e-05,
      "loss": 1.0635,
      "step": 34520
    },
    {
      "epoch": 0.5344167150319211,
      "grad_norm": 1.3659874200820923,
      "learning_rate": 2.4671078099917932e-05,
      "loss": 1.0249,
      "step": 34530
    },
    {
      "epoch": 0.5345714838460051,
      "grad_norm": 1.2511415481567383,
      "learning_rate": 2.466952962490774e-05,
      "loss": 1.2449,
      "step": 34540
    },
    {
      "epoch": 0.534726252660089,
      "grad_norm": 2.0014219284057617,
      "learning_rate": 2.4667981149897543e-05,
      "loss": 0.9449,
      "step": 34550
    },
    {
      "epoch": 0.5348810214741729,
      "grad_norm": 1.4203132390975952,
      "learning_rate": 2.466643267488735e-05,
      "loss": 0.8834,
      "step": 34560
    },
    {
      "epoch": 0.5350357902882569,
      "grad_norm": 1.1362996101379395,
      "learning_rate": 2.4664884199877154e-05,
      "loss": 1.1024,
      "step": 34570
    },
    {
      "epoch": 0.5351905591023409,
      "grad_norm": 1.3818408250808716,
      "learning_rate": 2.466333572486696e-05,
      "loss": 0.9542,
      "step": 34580
    },
    {
      "epoch": 0.5353453279164249,
      "grad_norm": 1.470423936843872,
      "learning_rate": 2.4661787249856765e-05,
      "loss": 1.0573,
      "step": 34590
    },
    {
      "epoch": 0.5355000967305088,
      "grad_norm": 1.7947620153427124,
      "learning_rate": 2.4660238774846576e-05,
      "loss": 1.0892,
      "step": 34600
    },
    {
      "epoch": 0.5356548655445927,
      "grad_norm": 1.201390027999878,
      "learning_rate": 2.465869029983638e-05,
      "loss": 1.1682,
      "step": 34610
    },
    {
      "epoch": 0.5358096343586767,
      "grad_norm": 1.1537888050079346,
      "learning_rate": 2.4657141824826187e-05,
      "loss": 1.0613,
      "step": 34620
    },
    {
      "epoch": 0.5359644031727607,
      "grad_norm": 2.202221632003784,
      "learning_rate": 2.465559334981599e-05,
      "loss": 1.0714,
      "step": 34630
    },
    {
      "epoch": 0.5361191719868447,
      "grad_norm": 1.2495908737182617,
      "learning_rate": 2.4654044874805794e-05,
      "loss": 1.2057,
      "step": 34640
    },
    {
      "epoch": 0.5362739408009286,
      "grad_norm": 1.4025936126708984,
      "learning_rate": 2.46524963997956e-05,
      "loss": 1.1187,
      "step": 34650
    },
    {
      "epoch": 0.5364287096150125,
      "grad_norm": 1.1610372066497803,
      "learning_rate": 2.4650947924785405e-05,
      "loss": 0.9974,
      "step": 34660
    },
    {
      "epoch": 0.5365834784290965,
      "grad_norm": 1.5155212879180908,
      "learning_rate": 2.4649399449775215e-05,
      "loss": 1.1223,
      "step": 34670
    },
    {
      "epoch": 0.5367382472431805,
      "grad_norm": 1.5670956373214722,
      "learning_rate": 2.464785097476502e-05,
      "loss": 0.956,
      "step": 34680
    },
    {
      "epoch": 0.5368930160572645,
      "grad_norm": 1.1383877992630005,
      "learning_rate": 2.4646302499754826e-05,
      "loss": 0.9337,
      "step": 34690
    },
    {
      "epoch": 0.5370477848713484,
      "grad_norm": 1.2997013330459595,
      "learning_rate": 2.464475402474463e-05,
      "loss": 1.0402,
      "step": 34700
    },
    {
      "epoch": 0.5372025536854323,
      "grad_norm": 1.0551347732543945,
      "learning_rate": 2.4643205549734437e-05,
      "loss": 1.039,
      "step": 34710
    },
    {
      "epoch": 0.5373573224995164,
      "grad_norm": 1.9086616039276123,
      "learning_rate": 2.464165707472424e-05,
      "loss": 1.1246,
      "step": 34720
    },
    {
      "epoch": 0.5375120913136003,
      "grad_norm": 1.4174617528915405,
      "learning_rate": 2.4640108599714048e-05,
      "loss": 0.9732,
      "step": 34730
    },
    {
      "epoch": 0.5376668601276843,
      "grad_norm": 1.5874874591827393,
      "learning_rate": 2.4638560124703855e-05,
      "loss": 1.1547,
      "step": 34740
    },
    {
      "epoch": 0.5378216289417682,
      "grad_norm": 1.2843676805496216,
      "learning_rate": 2.4637011649693662e-05,
      "loss": 0.9472,
      "step": 34750
    },
    {
      "epoch": 0.5379763977558522,
      "grad_norm": 1.8288921117782593,
      "learning_rate": 2.4635463174683466e-05,
      "loss": 0.9733,
      "step": 34760
    },
    {
      "epoch": 0.5381311665699362,
      "grad_norm": 1.6924492120742798,
      "learning_rate": 2.4633914699673273e-05,
      "loss": 1.0764,
      "step": 34770
    },
    {
      "epoch": 0.5382859353840201,
      "grad_norm": 1.3826439380645752,
      "learning_rate": 2.4632366224663077e-05,
      "loss": 1.0558,
      "step": 34780
    },
    {
      "epoch": 0.5384407041981041,
      "grad_norm": 1.5332776308059692,
      "learning_rate": 2.4630817749652884e-05,
      "loss": 1.1486,
      "step": 34790
    },
    {
      "epoch": 0.538595473012188,
      "grad_norm": 2.418821334838867,
      "learning_rate": 2.4629269274642688e-05,
      "loss": 1.3253,
      "step": 34800
    },
    {
      "epoch": 0.538750241826272,
      "grad_norm": 1.4519745111465454,
      "learning_rate": 2.46277207996325e-05,
      "loss": 1.1971,
      "step": 34810
    },
    {
      "epoch": 0.538905010640356,
      "grad_norm": 1.8480302095413208,
      "learning_rate": 2.4626172324622302e-05,
      "loss": 1.3452,
      "step": 34820
    },
    {
      "epoch": 0.53905977945444,
      "grad_norm": 1.5929288864135742,
      "learning_rate": 2.462462384961211e-05,
      "loss": 1.0226,
      "step": 34830
    },
    {
      "epoch": 0.5392145482685239,
      "grad_norm": 1.3119827508926392,
      "learning_rate": 2.4623075374601913e-05,
      "loss": 1.0531,
      "step": 34840
    },
    {
      "epoch": 0.5393693170826078,
      "grad_norm": 1.3835762739181519,
      "learning_rate": 2.462152689959172e-05,
      "loss": 1.0359,
      "step": 34850
    },
    {
      "epoch": 0.5395240858966918,
      "grad_norm": 1.370961308479309,
      "learning_rate": 2.4619978424581524e-05,
      "loss": 1.1581,
      "step": 34860
    },
    {
      "epoch": 0.5396788547107758,
      "grad_norm": 1.3126463890075684,
      "learning_rate": 2.461842994957133e-05,
      "loss": 1.2109,
      "step": 34870
    },
    {
      "epoch": 0.5398336235248598,
      "grad_norm": 1.83550226688385,
      "learning_rate": 2.4616881474561138e-05,
      "loss": 0.9438,
      "step": 34880
    },
    {
      "epoch": 0.5399883923389437,
      "grad_norm": 2.2094991207122803,
      "learning_rate": 2.4615332999550942e-05,
      "loss": 0.9803,
      "step": 34890
    },
    {
      "epoch": 0.5401431611530276,
      "grad_norm": 1.6298366785049438,
      "learning_rate": 2.461378452454075e-05,
      "loss": 0.9249,
      "step": 34900
    },
    {
      "epoch": 0.5402979299671117,
      "grad_norm": 1.503682017326355,
      "learning_rate": 2.4612236049530553e-05,
      "loss": 1.0696,
      "step": 34910
    },
    {
      "epoch": 0.5404526987811956,
      "grad_norm": 1.8237452507019043,
      "learning_rate": 2.461068757452036e-05,
      "loss": 1.0233,
      "step": 34920
    },
    {
      "epoch": 0.5406074675952796,
      "grad_norm": 1.5716447830200195,
      "learning_rate": 2.4609139099510164e-05,
      "loss": 1.0511,
      "step": 34930
    },
    {
      "epoch": 0.5407622364093635,
      "grad_norm": 2.2539210319519043,
      "learning_rate": 2.4607590624499974e-05,
      "loss": 1.1824,
      "step": 34940
    },
    {
      "epoch": 0.5409170052234474,
      "grad_norm": 1.482568383216858,
      "learning_rate": 2.4606042149489778e-05,
      "loss": 0.9314,
      "step": 34950
    },
    {
      "epoch": 0.5410717740375315,
      "grad_norm": 1.509982705116272,
      "learning_rate": 2.4604493674479585e-05,
      "loss": 0.9991,
      "step": 34960
    },
    {
      "epoch": 0.5412265428516154,
      "grad_norm": 1.616018533706665,
      "learning_rate": 2.460294519946939e-05,
      "loss": 0.935,
      "step": 34970
    },
    {
      "epoch": 0.5413813116656994,
      "grad_norm": 1.5537906885147095,
      "learning_rate": 2.4601396724459196e-05,
      "loss": 1.0489,
      "step": 34980
    },
    {
      "epoch": 0.5415360804797833,
      "grad_norm": 1.5839390754699707,
      "learning_rate": 2.4599848249449e-05,
      "loss": 0.9392,
      "step": 34990
    },
    {
      "epoch": 0.5416908492938672,
      "grad_norm": 1.4751063585281372,
      "learning_rate": 2.4598299774438807e-05,
      "loss": 1.0939,
      "step": 35000
    },
    {
      "epoch": 0.5418456181079513,
      "grad_norm": 1.5756739377975464,
      "learning_rate": 2.4596751299428614e-05,
      "loss": 1.0503,
      "step": 35010
    },
    {
      "epoch": 0.5420003869220352,
      "grad_norm": 1.5556553602218628,
      "learning_rate": 2.459520282441842e-05,
      "loss": 1.1889,
      "step": 35020
    },
    {
      "epoch": 0.5421551557361192,
      "grad_norm": 1.210072636604309,
      "learning_rate": 2.4593654349408225e-05,
      "loss": 1.0438,
      "step": 35030
    },
    {
      "epoch": 0.5423099245502031,
      "grad_norm": 1.5798345804214478,
      "learning_rate": 2.4592105874398032e-05,
      "loss": 1.0423,
      "step": 35040
    },
    {
      "epoch": 0.542464693364287,
      "grad_norm": 2.0434043407440186,
      "learning_rate": 2.4590557399387836e-05,
      "loss": 0.9863,
      "step": 35050
    },
    {
      "epoch": 0.5426194621783711,
      "grad_norm": 1.0548086166381836,
      "learning_rate": 2.4589008924377643e-05,
      "loss": 0.9164,
      "step": 35060
    },
    {
      "epoch": 0.542774230992455,
      "grad_norm": 2.122638463973999,
      "learning_rate": 2.4587460449367447e-05,
      "loss": 1.2005,
      "step": 35070
    },
    {
      "epoch": 0.542928999806539,
      "grad_norm": 1.9103072881698608,
      "learning_rate": 2.4585911974357257e-05,
      "loss": 1.07,
      "step": 35080
    },
    {
      "epoch": 0.5430837686206229,
      "grad_norm": 1.3176065683364868,
      "learning_rate": 2.458436349934706e-05,
      "loss": 1.053,
      "step": 35090
    },
    {
      "epoch": 0.543238537434707,
      "grad_norm": 1.313929796218872,
      "learning_rate": 2.4582815024336868e-05,
      "loss": 1.0858,
      "step": 35100
    },
    {
      "epoch": 0.5433933062487909,
      "grad_norm": 1.7395908832550049,
      "learning_rate": 2.4581266549326672e-05,
      "loss": 1.0664,
      "step": 35110
    },
    {
      "epoch": 0.5435480750628748,
      "grad_norm": 1.2607213258743286,
      "learning_rate": 2.457971807431648e-05,
      "loss": 1.2471,
      "step": 35120
    },
    {
      "epoch": 0.5437028438769588,
      "grad_norm": 1.0607556104660034,
      "learning_rate": 2.4578169599306283e-05,
      "loss": 1.0639,
      "step": 35130
    },
    {
      "epoch": 0.5438576126910427,
      "grad_norm": 1.3123680353164673,
      "learning_rate": 2.4576621124296087e-05,
      "loss": 0.9273,
      "step": 35140
    },
    {
      "epoch": 0.5440123815051268,
      "grad_norm": 1.5757724046707153,
      "learning_rate": 2.4575072649285897e-05,
      "loss": 1.18,
      "step": 35150
    },
    {
      "epoch": 0.5441671503192107,
      "grad_norm": 1.1608372926712036,
      "learning_rate": 2.45735241742757e-05,
      "loss": 1.0062,
      "step": 35160
    },
    {
      "epoch": 0.5443219191332946,
      "grad_norm": 1.9075392484664917,
      "learning_rate": 2.4571975699265508e-05,
      "loss": 1.142,
      "step": 35170
    },
    {
      "epoch": 0.5444766879473786,
      "grad_norm": 1.1346867084503174,
      "learning_rate": 2.4570427224255312e-05,
      "loss": 1.0474,
      "step": 35180
    },
    {
      "epoch": 0.5446314567614625,
      "grad_norm": 1.8056745529174805,
      "learning_rate": 2.456887874924512e-05,
      "loss": 1.0265,
      "step": 35190
    },
    {
      "epoch": 0.5447862255755466,
      "grad_norm": 1.195330262184143,
      "learning_rate": 2.4567330274234923e-05,
      "loss": 1.026,
      "step": 35200
    },
    {
      "epoch": 0.5449409943896305,
      "grad_norm": 1.3371191024780273,
      "learning_rate": 2.456578179922473e-05,
      "loss": 1.0926,
      "step": 35210
    },
    {
      "epoch": 0.5450957632037144,
      "grad_norm": 1.6140143871307373,
      "learning_rate": 2.4564233324214537e-05,
      "loss": 0.9525,
      "step": 35220
    },
    {
      "epoch": 0.5452505320177984,
      "grad_norm": 1.7150330543518066,
      "learning_rate": 2.4562684849204344e-05,
      "loss": 1.0605,
      "step": 35230
    },
    {
      "epoch": 0.5454053008318823,
      "grad_norm": 1.3279837369918823,
      "learning_rate": 2.4561136374194148e-05,
      "loss": 1.0995,
      "step": 35240
    },
    {
      "epoch": 0.5455600696459664,
      "grad_norm": 1.5181403160095215,
      "learning_rate": 2.4559587899183955e-05,
      "loss": 0.9889,
      "step": 35250
    },
    {
      "epoch": 0.5457148384600503,
      "grad_norm": 1.2335131168365479,
      "learning_rate": 2.455803942417376e-05,
      "loss": 0.9736,
      "step": 35260
    },
    {
      "epoch": 0.5458696072741342,
      "grad_norm": 1.4850579500198364,
      "learning_rate": 2.4556490949163566e-05,
      "loss": 1.0446,
      "step": 35270
    },
    {
      "epoch": 0.5460243760882182,
      "grad_norm": 1.231305480003357,
      "learning_rate": 2.455494247415337e-05,
      "loss": 1.0594,
      "step": 35280
    },
    {
      "epoch": 0.5461791449023022,
      "grad_norm": 1.4486302137374878,
      "learning_rate": 2.455339399914318e-05,
      "loss": 0.9528,
      "step": 35290
    },
    {
      "epoch": 0.5463339137163862,
      "grad_norm": 1.512794017791748,
      "learning_rate": 2.4551845524132984e-05,
      "loss": 1.0421,
      "step": 35300
    },
    {
      "epoch": 0.5464886825304701,
      "grad_norm": 1.3677431344985962,
      "learning_rate": 2.455029704912279e-05,
      "loss": 1.0844,
      "step": 35310
    },
    {
      "epoch": 0.546643451344554,
      "grad_norm": 1.496971845626831,
      "learning_rate": 2.4548748574112595e-05,
      "loss": 0.9669,
      "step": 35320
    },
    {
      "epoch": 0.546798220158638,
      "grad_norm": 1.3456757068634033,
      "learning_rate": 2.4547200099102402e-05,
      "loss": 1.0531,
      "step": 35330
    },
    {
      "epoch": 0.546952988972722,
      "grad_norm": 1.4208416938781738,
      "learning_rate": 2.4545651624092206e-05,
      "loss": 1.1313,
      "step": 35340
    },
    {
      "epoch": 0.547107757786806,
      "grad_norm": 1.5948132276535034,
      "learning_rate": 2.4544103149082016e-05,
      "loss": 1.053,
      "step": 35350
    },
    {
      "epoch": 0.5472625266008899,
      "grad_norm": 1.2117950916290283,
      "learning_rate": 2.454255467407182e-05,
      "loss": 1.0364,
      "step": 35360
    },
    {
      "epoch": 0.5474172954149739,
      "grad_norm": 1.8616389036178589,
      "learning_rate": 2.4541006199061627e-05,
      "loss": 1.2259,
      "step": 35370
    },
    {
      "epoch": 0.5475720642290578,
      "grad_norm": 1.2460788488388062,
      "learning_rate": 2.453945772405143e-05,
      "loss": 0.9762,
      "step": 35380
    },
    {
      "epoch": 0.5477268330431418,
      "grad_norm": 1.6672542095184326,
      "learning_rate": 2.4537909249041235e-05,
      "loss": 1.1448,
      "step": 35390
    },
    {
      "epoch": 0.5478816018572258,
      "grad_norm": 1.325959324836731,
      "learning_rate": 2.453636077403104e-05,
      "loss": 1.0895,
      "step": 35400
    },
    {
      "epoch": 0.5480363706713097,
      "grad_norm": 1.618101954460144,
      "learning_rate": 2.4534812299020845e-05,
      "loss": 0.9142,
      "step": 35410
    },
    {
      "epoch": 0.5481911394853937,
      "grad_norm": 1.0887070894241333,
      "learning_rate": 2.4533263824010656e-05,
      "loss": 1.1662,
      "step": 35420
    },
    {
      "epoch": 0.5483459082994776,
      "grad_norm": 1.533873438835144,
      "learning_rate": 2.453171534900046e-05,
      "loss": 1.1214,
      "step": 35430
    },
    {
      "epoch": 0.5485006771135617,
      "grad_norm": 1.8027199506759644,
      "learning_rate": 2.4530166873990267e-05,
      "loss": 1.0789,
      "step": 35440
    },
    {
      "epoch": 0.5486554459276456,
      "grad_norm": 1.5727359056472778,
      "learning_rate": 2.452861839898007e-05,
      "loss": 0.9389,
      "step": 35450
    },
    {
      "epoch": 0.5488102147417295,
      "grad_norm": 1.4899126291275024,
      "learning_rate": 2.4527069923969878e-05,
      "loss": 0.9738,
      "step": 35460
    },
    {
      "epoch": 0.5489649835558135,
      "grad_norm": 1.651759386062622,
      "learning_rate": 2.452552144895968e-05,
      "loss": 1.1811,
      "step": 35470
    },
    {
      "epoch": 0.5491197523698975,
      "grad_norm": 1.7775603532791138,
      "learning_rate": 2.452397297394949e-05,
      "loss": 1.3098,
      "step": 35480
    },
    {
      "epoch": 0.5492745211839815,
      "grad_norm": 1.5412541627883911,
      "learning_rate": 2.4522424498939296e-05,
      "loss": 1.0715,
      "step": 35490
    },
    {
      "epoch": 0.5494292899980654,
      "grad_norm": 1.494313359260559,
      "learning_rate": 2.4520876023929103e-05,
      "loss": 1.1176,
      "step": 35500
    },
    {
      "epoch": 0.5495840588121493,
      "grad_norm": 2.019921064376831,
      "learning_rate": 2.4519327548918907e-05,
      "loss": 1.0398,
      "step": 35510
    },
    {
      "epoch": 0.5497388276262333,
      "grad_norm": 1.3248382806777954,
      "learning_rate": 2.4517779073908714e-05,
      "loss": 1.1416,
      "step": 35520
    },
    {
      "epoch": 0.5498935964403173,
      "grad_norm": 1.7207229137420654,
      "learning_rate": 2.4516230598898518e-05,
      "loss": 0.9485,
      "step": 35530
    },
    {
      "epoch": 0.5500483652544013,
      "grad_norm": 1.9250760078430176,
      "learning_rate": 2.4514682123888325e-05,
      "loss": 1.1604,
      "step": 35540
    },
    {
      "epoch": 0.5502031340684852,
      "grad_norm": 1.9869413375854492,
      "learning_rate": 2.451313364887813e-05,
      "loss": 1.0724,
      "step": 35550
    },
    {
      "epoch": 0.5503579028825691,
      "grad_norm": 1.3641283512115479,
      "learning_rate": 2.451158517386794e-05,
      "loss": 1.2167,
      "step": 35560
    },
    {
      "epoch": 0.5505126716966531,
      "grad_norm": 1.0639511346817017,
      "learning_rate": 2.4510036698857743e-05,
      "loss": 1.2075,
      "step": 35570
    },
    {
      "epoch": 0.5506674405107371,
      "grad_norm": 1.8928834199905396,
      "learning_rate": 2.450848822384755e-05,
      "loss": 1.0442,
      "step": 35580
    },
    {
      "epoch": 0.5508222093248211,
      "grad_norm": 2.2194204330444336,
      "learning_rate": 2.4506939748837354e-05,
      "loss": 1.0156,
      "step": 35590
    },
    {
      "epoch": 0.550976978138905,
      "grad_norm": 1.9964959621429443,
      "learning_rate": 2.450539127382716e-05,
      "loss": 1.0974,
      "step": 35600
    },
    {
      "epoch": 0.5511317469529889,
      "grad_norm": 2.28109073638916,
      "learning_rate": 2.4503842798816964e-05,
      "loss": 1.1227,
      "step": 35610
    },
    {
      "epoch": 0.5512865157670729,
      "grad_norm": 1.5655008554458618,
      "learning_rate": 2.450229432380677e-05,
      "loss": 0.9732,
      "step": 35620
    },
    {
      "epoch": 0.5514412845811569,
      "grad_norm": 1.667275309562683,
      "learning_rate": 2.450074584879658e-05,
      "loss": 1.1315,
      "step": 35630
    },
    {
      "epoch": 0.5515960533952409,
      "grad_norm": 1.5763765573501587,
      "learning_rate": 2.4499197373786383e-05,
      "loss": 1.1305,
      "step": 35640
    },
    {
      "epoch": 0.5517508222093248,
      "grad_norm": 1.2590041160583496,
      "learning_rate": 2.449764889877619e-05,
      "loss": 1.2185,
      "step": 35650
    },
    {
      "epoch": 0.5519055910234087,
      "grad_norm": 1.548035740852356,
      "learning_rate": 2.4496100423765993e-05,
      "loss": 1.2819,
      "step": 35660
    },
    {
      "epoch": 0.5520603598374928,
      "grad_norm": 1.6079260110855103,
      "learning_rate": 2.44945519487558e-05,
      "loss": 1.166,
      "step": 35670
    },
    {
      "epoch": 0.5522151286515767,
      "grad_norm": 2.001438617706299,
      "learning_rate": 2.4493003473745604e-05,
      "loss": 1.1487,
      "step": 35680
    },
    {
      "epoch": 0.5523698974656607,
      "grad_norm": 1.870141625404358,
      "learning_rate": 2.449145499873541e-05,
      "loss": 1.2291,
      "step": 35690
    },
    {
      "epoch": 0.5525246662797446,
      "grad_norm": 1.8682125806808472,
      "learning_rate": 2.448990652372522e-05,
      "loss": 0.8716,
      "step": 35700
    },
    {
      "epoch": 0.5526794350938286,
      "grad_norm": 2.4986824989318848,
      "learning_rate": 2.4488358048715026e-05,
      "loss": 1.1367,
      "step": 35710
    },
    {
      "epoch": 0.5528342039079126,
      "grad_norm": 1.5586696863174438,
      "learning_rate": 2.448680957370483e-05,
      "loss": 1.069,
      "step": 35720
    },
    {
      "epoch": 0.5529889727219965,
      "grad_norm": 1.3127310276031494,
      "learning_rate": 2.4485261098694637e-05,
      "loss": 1.1086,
      "step": 35730
    },
    {
      "epoch": 0.5531437415360805,
      "grad_norm": 1.336028814315796,
      "learning_rate": 2.448371262368444e-05,
      "loss": 1.0321,
      "step": 35740
    },
    {
      "epoch": 0.5532985103501644,
      "grad_norm": 1.1866885423660278,
      "learning_rate": 2.4482164148674247e-05,
      "loss": 0.986,
      "step": 35750
    },
    {
      "epoch": 0.5534532791642484,
      "grad_norm": 1.0821670293807983,
      "learning_rate": 2.4480615673664055e-05,
      "loss": 1.1276,
      "step": 35760
    },
    {
      "epoch": 0.5536080479783324,
      "grad_norm": 2.2080488204956055,
      "learning_rate": 2.4479067198653862e-05,
      "loss": 1.0428,
      "step": 35770
    },
    {
      "epoch": 0.5537628167924163,
      "grad_norm": 1.6359868049621582,
      "learning_rate": 2.4477518723643666e-05,
      "loss": 1.1214,
      "step": 35780
    },
    {
      "epoch": 0.5539175856065003,
      "grad_norm": 1.2958589792251587,
      "learning_rate": 2.4475970248633473e-05,
      "loss": 1.0312,
      "step": 35790
    },
    {
      "epoch": 0.5540723544205842,
      "grad_norm": 1.3692846298217773,
      "learning_rate": 2.4474421773623276e-05,
      "loss": 1.1518,
      "step": 35800
    },
    {
      "epoch": 0.5542271232346682,
      "grad_norm": 1.3720908164978027,
      "learning_rate": 2.4472873298613084e-05,
      "loss": 1.0568,
      "step": 35810
    },
    {
      "epoch": 0.5543818920487522,
      "grad_norm": 1.203209400177002,
      "learning_rate": 2.4471324823602887e-05,
      "loss": 0.9324,
      "step": 35820
    },
    {
      "epoch": 0.5545366608628362,
      "grad_norm": 1.0128587484359741,
      "learning_rate": 2.4469776348592698e-05,
      "loss": 1.1847,
      "step": 35830
    },
    {
      "epoch": 0.5546914296769201,
      "grad_norm": 2.451925039291382,
      "learning_rate": 2.44682278735825e-05,
      "loss": 1.2331,
      "step": 35840
    },
    {
      "epoch": 0.554846198491004,
      "grad_norm": 1.988553524017334,
      "learning_rate": 2.446667939857231e-05,
      "loss": 0.9648,
      "step": 35850
    },
    {
      "epoch": 0.5550009673050881,
      "grad_norm": 1.9301981925964355,
      "learning_rate": 2.4465130923562112e-05,
      "loss": 1.1337,
      "step": 35860
    },
    {
      "epoch": 0.555155736119172,
      "grad_norm": 1.338273048400879,
      "learning_rate": 2.446358244855192e-05,
      "loss": 0.9819,
      "step": 35870
    },
    {
      "epoch": 0.555310504933256,
      "grad_norm": 1.5444868803024292,
      "learning_rate": 2.4462033973541723e-05,
      "loss": 1.1164,
      "step": 35880
    },
    {
      "epoch": 0.5554652737473399,
      "grad_norm": 1.305372953414917,
      "learning_rate": 2.4460485498531527e-05,
      "loss": 0.9178,
      "step": 35890
    },
    {
      "epoch": 0.5556200425614238,
      "grad_norm": 1.1468651294708252,
      "learning_rate": 2.4458937023521338e-05,
      "loss": 1.0805,
      "step": 35900
    },
    {
      "epoch": 0.5557748113755079,
      "grad_norm": 1.377235770225525,
      "learning_rate": 2.445738854851114e-05,
      "loss": 0.9989,
      "step": 35910
    },
    {
      "epoch": 0.5559295801895918,
      "grad_norm": 1.3268548250198364,
      "learning_rate": 2.445584007350095e-05,
      "loss": 1.2665,
      "step": 35920
    },
    {
      "epoch": 0.5560843490036758,
      "grad_norm": 1.4033046960830688,
      "learning_rate": 2.4454291598490752e-05,
      "loss": 1.1114,
      "step": 35930
    },
    {
      "epoch": 0.5562391178177597,
      "grad_norm": 2.5159502029418945,
      "learning_rate": 2.445274312348056e-05,
      "loss": 1.1193,
      "step": 35940
    },
    {
      "epoch": 0.5563938866318436,
      "grad_norm": 1.2891855239868164,
      "learning_rate": 2.4451194648470363e-05,
      "loss": 1.1918,
      "step": 35950
    },
    {
      "epoch": 0.5565486554459277,
      "grad_norm": 0.9493486881256104,
      "learning_rate": 2.444964617346017e-05,
      "loss": 0.9598,
      "step": 35960
    },
    {
      "epoch": 0.5567034242600116,
      "grad_norm": 1.2629591226577759,
      "learning_rate": 2.4448097698449977e-05,
      "loss": 0.8684,
      "step": 35970
    },
    {
      "epoch": 0.5568581930740956,
      "grad_norm": 1.2176371812820435,
      "learning_rate": 2.4446549223439785e-05,
      "loss": 1.2032,
      "step": 35980
    },
    {
      "epoch": 0.5570129618881795,
      "grad_norm": 1.6358274221420288,
      "learning_rate": 2.444500074842959e-05,
      "loss": 0.9847,
      "step": 35990
    },
    {
      "epoch": 0.5571677307022634,
      "grad_norm": 1.28655207157135,
      "learning_rate": 2.4443452273419395e-05,
      "loss": 1.1092,
      "step": 36000
    },
    {
      "epoch": 0.5573224995163475,
      "grad_norm": 1.5991255044937134,
      "learning_rate": 2.44419037984092e-05,
      "loss": 1.0965,
      "step": 36010
    },
    {
      "epoch": 0.5574772683304314,
      "grad_norm": 1.733272910118103,
      "learning_rate": 2.4440355323399006e-05,
      "loss": 1.2266,
      "step": 36020
    },
    {
      "epoch": 0.5576320371445154,
      "grad_norm": 2.450547456741333,
      "learning_rate": 2.443880684838881e-05,
      "loss": 1.1057,
      "step": 36030
    },
    {
      "epoch": 0.5577868059585993,
      "grad_norm": 1.7690551280975342,
      "learning_rate": 2.443725837337862e-05,
      "loss": 1.0903,
      "step": 36040
    },
    {
      "epoch": 0.5579415747726834,
      "grad_norm": 1.20427668094635,
      "learning_rate": 2.4435709898368424e-05,
      "loss": 1.0963,
      "step": 36050
    },
    {
      "epoch": 0.5580963435867673,
      "grad_norm": 1.6989010572433472,
      "learning_rate": 2.443416142335823e-05,
      "loss": 1.1996,
      "step": 36060
    },
    {
      "epoch": 0.5582511124008512,
      "grad_norm": 1.3935060501098633,
      "learning_rate": 2.4432612948348035e-05,
      "loss": 1.1994,
      "step": 36070
    },
    {
      "epoch": 0.5584058812149352,
      "grad_norm": 1.170540452003479,
      "learning_rate": 2.4431064473337842e-05,
      "loss": 0.9464,
      "step": 36080
    },
    {
      "epoch": 0.5585606500290191,
      "grad_norm": 1.587646245956421,
      "learning_rate": 2.4429515998327646e-05,
      "loss": 1.0671,
      "step": 36090
    },
    {
      "epoch": 0.5587154188431032,
      "grad_norm": 1.2218842506408691,
      "learning_rate": 2.4427967523317453e-05,
      "loss": 1.0835,
      "step": 36100
    },
    {
      "epoch": 0.5588701876571871,
      "grad_norm": 1.4266774654388428,
      "learning_rate": 2.442641904830726e-05,
      "loss": 1.2211,
      "step": 36110
    },
    {
      "epoch": 0.559024956471271,
      "grad_norm": 1.275641679763794,
      "learning_rate": 2.4424870573297068e-05,
      "loss": 1.0517,
      "step": 36120
    },
    {
      "epoch": 0.559179725285355,
      "grad_norm": 2.07979416847229,
      "learning_rate": 2.442332209828687e-05,
      "loss": 1.107,
      "step": 36130
    },
    {
      "epoch": 0.5593344940994389,
      "grad_norm": 1.2682627439498901,
      "learning_rate": 2.442177362327668e-05,
      "loss": 1.171,
      "step": 36140
    },
    {
      "epoch": 0.559489262913523,
      "grad_norm": 1.1574277877807617,
      "learning_rate": 2.4420225148266482e-05,
      "loss": 0.9646,
      "step": 36150
    },
    {
      "epoch": 0.5596440317276069,
      "grad_norm": 1.7688275575637817,
      "learning_rate": 2.441883152075731e-05,
      "loss": 0.967,
      "step": 36160
    },
    {
      "epoch": 0.5597988005416908,
      "grad_norm": 1.2546883821487427,
      "learning_rate": 2.4417283045747112e-05,
      "loss": 1.057,
      "step": 36170
    },
    {
      "epoch": 0.5599535693557748,
      "grad_norm": 1.6558215618133545,
      "learning_rate": 2.441573457073692e-05,
      "loss": 1.1685,
      "step": 36180
    },
    {
      "epoch": 0.5601083381698587,
      "grad_norm": 1.1959640979766846,
      "learning_rate": 2.4414186095726726e-05,
      "loss": 1.0705,
      "step": 36190
    },
    {
      "epoch": 0.5602631069839428,
      "grad_norm": 1.7618837356567383,
      "learning_rate": 2.4412637620716533e-05,
      "loss": 0.979,
      "step": 36200
    }
  ],
  "logging_steps": 10,
  "max_steps": 193839,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.4828312301797376e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
