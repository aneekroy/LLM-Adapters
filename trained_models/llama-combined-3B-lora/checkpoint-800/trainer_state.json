{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.012381505126716966,
  "eval_steps": 500,
  "global_step": 800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00015476881408396209,
      "grad_norm": 0.9293789863586426,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 2.9118,
      "step": 10
    },
    {
      "epoch": 0.00030953762816792417,
      "grad_norm": 1.3094398975372314,
      "learning_rate": 5.4e-06,
      "loss": 2.7035,
      "step": 20
    },
    {
      "epoch": 0.00046430644225188626,
      "grad_norm": 0.6917069554328918,
      "learning_rate": 8.400000000000001e-06,
      "loss": 2.6848,
      "step": 30
    },
    {
      "epoch": 0.0006190752563358483,
      "grad_norm": 0.6283435821533203,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 2.7382,
      "step": 40
    },
    {
      "epoch": 0.0007738440704198104,
      "grad_norm": 0.5893833041191101,
      "learning_rate": 1.44e-05,
      "loss": 2.5173,
      "step": 50
    },
    {
      "epoch": 0.0009286128845037725,
      "grad_norm": 0.9308087825775146,
      "learning_rate": 1.74e-05,
      "loss": 2.3983,
      "step": 60
    },
    {
      "epoch": 0.0010833816985877345,
      "grad_norm": 0.6720496416091919,
      "learning_rate": 2.04e-05,
      "loss": 2.3199,
      "step": 70
    },
    {
      "epoch": 0.0012381505126716967,
      "grad_norm": 0.705177366733551,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 2.5726,
      "step": 80
    },
    {
      "epoch": 0.0013929193267556588,
      "grad_norm": 0.5583527088165283,
      "learning_rate": 2.64e-05,
      "loss": 2.1276,
      "step": 90
    },
    {
      "epoch": 0.0015476881408396208,
      "grad_norm": 1.2375026941299438,
      "learning_rate": 2.94e-05,
      "loss": 2.1415,
      "step": 100
    },
    {
      "epoch": 0.001702456954923583,
      "grad_norm": 1.6871685981750488,
      "learning_rate": 2.9998761219991846e-05,
      "loss": 2.1947,
      "step": 110
    },
    {
      "epoch": 0.001857225769007545,
      "grad_norm": 1.183356761932373,
      "learning_rate": 2.9997212744981653e-05,
      "loss": 1.8332,
      "step": 120
    },
    {
      "epoch": 0.002011994583091507,
      "grad_norm": 0.9398878812789917,
      "learning_rate": 2.9995664269971457e-05,
      "loss": 1.845,
      "step": 130
    },
    {
      "epoch": 0.002166763397175469,
      "grad_norm": 2.0294010639190674,
      "learning_rate": 2.9994115794961264e-05,
      "loss": 1.5135,
      "step": 140
    },
    {
      "epoch": 0.0023215322112594312,
      "grad_norm": 2.056433916091919,
      "learning_rate": 2.9992567319951068e-05,
      "loss": 1.5815,
      "step": 150
    },
    {
      "epoch": 0.0024763010253433934,
      "grad_norm": 1.330277442932129,
      "learning_rate": 2.9991018844940875e-05,
      "loss": 1.4673,
      "step": 160
    },
    {
      "epoch": 0.0026310698394273555,
      "grad_norm": 1.145898699760437,
      "learning_rate": 2.9989470369930682e-05,
      "loss": 1.3815,
      "step": 170
    },
    {
      "epoch": 0.0027858386535113177,
      "grad_norm": 1.9492080211639404,
      "learning_rate": 2.998792189492049e-05,
      "loss": 1.4921,
      "step": 180
    },
    {
      "epoch": 0.0029406074675952794,
      "grad_norm": 1.4393186569213867,
      "learning_rate": 2.9986373419910293e-05,
      "loss": 1.4644,
      "step": 190
    },
    {
      "epoch": 0.0030953762816792415,
      "grad_norm": 1.5623074769973755,
      "learning_rate": 2.99848249449001e-05,
      "loss": 1.5076,
      "step": 200
    },
    {
      "epoch": 0.0032501450957632036,
      "grad_norm": 1.4472917318344116,
      "learning_rate": 2.9983276469889904e-05,
      "loss": 1.3171,
      "step": 210
    },
    {
      "epoch": 0.003404913909847166,
      "grad_norm": 1.786587119102478,
      "learning_rate": 2.9981727994879707e-05,
      "loss": 1.4731,
      "step": 220
    },
    {
      "epoch": 0.003559682723931128,
      "grad_norm": 3.0002281665802,
      "learning_rate": 2.9980179519869515e-05,
      "loss": 1.5146,
      "step": 230
    },
    {
      "epoch": 0.00371445153801509,
      "grad_norm": 1.6306660175323486,
      "learning_rate": 2.997863104485932e-05,
      "loss": 1.4057,
      "step": 240
    },
    {
      "epoch": 0.003869220352099052,
      "grad_norm": 1.2732411623001099,
      "learning_rate": 2.997708256984913e-05,
      "loss": 1.306,
      "step": 250
    },
    {
      "epoch": 0.004023989166183014,
      "grad_norm": 1.4062086343765259,
      "learning_rate": 2.9975534094838933e-05,
      "loss": 1.277,
      "step": 260
    },
    {
      "epoch": 0.004178757980266976,
      "grad_norm": 1.4736639261245728,
      "learning_rate": 2.997398561982874e-05,
      "loss": 1.3449,
      "step": 270
    },
    {
      "epoch": 0.004333526794350938,
      "grad_norm": 2.5323948860168457,
      "learning_rate": 2.9972437144818543e-05,
      "loss": 1.3219,
      "step": 280
    },
    {
      "epoch": 0.0044882956084349,
      "grad_norm": 2.2378861904144287,
      "learning_rate": 2.997088866980835e-05,
      "loss": 1.3303,
      "step": 290
    },
    {
      "epoch": 0.0046430644225188625,
      "grad_norm": 1.4951406717300415,
      "learning_rate": 2.9969340194798154e-05,
      "loss": 1.2705,
      "step": 300
    },
    {
      "epoch": 0.004797833236602825,
      "grad_norm": 1.464626431465149,
      "learning_rate": 2.9967791719787965e-05,
      "loss": 1.2563,
      "step": 310
    },
    {
      "epoch": 0.004952602050686787,
      "grad_norm": 1.792585849761963,
      "learning_rate": 2.996624324477777e-05,
      "loss": 1.3819,
      "step": 320
    },
    {
      "epoch": 0.005107370864770749,
      "grad_norm": 1.8863691091537476,
      "learning_rate": 2.9964694769767576e-05,
      "loss": 1.3528,
      "step": 330
    },
    {
      "epoch": 0.005262139678854711,
      "grad_norm": 2.424776554107666,
      "learning_rate": 2.996314629475738e-05,
      "loss": 1.3492,
      "step": 340
    },
    {
      "epoch": 0.005416908492938673,
      "grad_norm": 2.015796661376953,
      "learning_rate": 2.9961597819747187e-05,
      "loss": 1.5383,
      "step": 350
    },
    {
      "epoch": 0.005571677307022635,
      "grad_norm": 1.5766215324401855,
      "learning_rate": 2.996004934473699e-05,
      "loss": 1.2784,
      "step": 360
    },
    {
      "epoch": 0.005726446121106597,
      "grad_norm": 1.6540963649749756,
      "learning_rate": 2.99585008697268e-05,
      "loss": 1.3945,
      "step": 370
    },
    {
      "epoch": 0.005881214935190559,
      "grad_norm": 1.981877326965332,
      "learning_rate": 2.9956952394716605e-05,
      "loss": 1.3351,
      "step": 380
    },
    {
      "epoch": 0.006035983749274521,
      "grad_norm": 1.3894907236099243,
      "learning_rate": 2.9955403919706412e-05,
      "loss": 1.3847,
      "step": 390
    },
    {
      "epoch": 0.006190752563358483,
      "grad_norm": 1.6752378940582275,
      "learning_rate": 2.9953855444696216e-05,
      "loss": 1.1708,
      "step": 400
    },
    {
      "epoch": 0.006345521377442445,
      "grad_norm": 1.987499713897705,
      "learning_rate": 2.9952306969686023e-05,
      "loss": 1.3841,
      "step": 410
    },
    {
      "epoch": 0.006500290191526407,
      "grad_norm": 1.6152921915054321,
      "learning_rate": 2.9950758494675826e-05,
      "loss": 1.2437,
      "step": 420
    },
    {
      "epoch": 0.0066550590056103694,
      "grad_norm": 2.0003347396850586,
      "learning_rate": 2.9949210019665634e-05,
      "loss": 1.4563,
      "step": 430
    },
    {
      "epoch": 0.006809827819694332,
      "grad_norm": 2.3070998191833496,
      "learning_rate": 2.994766154465544e-05,
      "loss": 1.4079,
      "step": 440
    },
    {
      "epoch": 0.006964596633778294,
      "grad_norm": 1.3119601011276245,
      "learning_rate": 2.9946113069645248e-05,
      "loss": 1.2032,
      "step": 450
    },
    {
      "epoch": 0.007119365447862256,
      "grad_norm": 1.8440574407577515,
      "learning_rate": 2.994456459463505e-05,
      "loss": 1.4302,
      "step": 460
    },
    {
      "epoch": 0.007274134261946218,
      "grad_norm": 2.477747678756714,
      "learning_rate": 2.9943016119624855e-05,
      "loss": 1.2994,
      "step": 470
    },
    {
      "epoch": 0.00742890307603018,
      "grad_norm": 1.1642504930496216,
      "learning_rate": 2.9941467644614662e-05,
      "loss": 1.3172,
      "step": 480
    },
    {
      "epoch": 0.007583671890114142,
      "grad_norm": 2.7848310470581055,
      "learning_rate": 2.9939919169604466e-05,
      "loss": 1.3427,
      "step": 490
    },
    {
      "epoch": 0.007738440704198104,
      "grad_norm": 2.1389334201812744,
      "learning_rate": 2.9938370694594273e-05,
      "loss": 1.2077,
      "step": 500
    },
    {
      "epoch": 0.007893209518282067,
      "grad_norm": 2.3112361431121826,
      "learning_rate": 2.993682221958408e-05,
      "loss": 1.3995,
      "step": 510
    },
    {
      "epoch": 0.008047978332366028,
      "grad_norm": 2.213000535964966,
      "learning_rate": 2.9935273744573888e-05,
      "loss": 1.285,
      "step": 520
    },
    {
      "epoch": 0.00820274714644999,
      "grad_norm": 2.3975346088409424,
      "learning_rate": 2.993372526956369e-05,
      "loss": 1.301,
      "step": 530
    },
    {
      "epoch": 0.008357515960533952,
      "grad_norm": 1.3450312614440918,
      "learning_rate": 2.99321767945535e-05,
      "loss": 1.3222,
      "step": 540
    },
    {
      "epoch": 0.008512284774617915,
      "grad_norm": 1.6964539289474487,
      "learning_rate": 2.9930628319543302e-05,
      "loss": 1.2377,
      "step": 550
    },
    {
      "epoch": 0.008667053588701876,
      "grad_norm": 2.3644967079162598,
      "learning_rate": 2.992907984453311e-05,
      "loss": 1.3216,
      "step": 560
    },
    {
      "epoch": 0.00882182240278584,
      "grad_norm": 1.970975637435913,
      "learning_rate": 2.9927531369522913e-05,
      "loss": 1.3745,
      "step": 570
    },
    {
      "epoch": 0.0089765912168698,
      "grad_norm": 2.4449284076690674,
      "learning_rate": 2.9925982894512724e-05,
      "loss": 1.3581,
      "step": 580
    },
    {
      "epoch": 0.009131360030953764,
      "grad_norm": 2.391282081604004,
      "learning_rate": 2.9924434419502527e-05,
      "loss": 1.2694,
      "step": 590
    },
    {
      "epoch": 0.009286128845037725,
      "grad_norm": 3.136723041534424,
      "learning_rate": 2.9922885944492335e-05,
      "loss": 1.3576,
      "step": 600
    },
    {
      "epoch": 0.009440897659121686,
      "grad_norm": 3.100938558578491,
      "learning_rate": 2.992133746948214e-05,
      "loss": 1.3065,
      "step": 610
    },
    {
      "epoch": 0.00959566647320565,
      "grad_norm": 3.2854232788085938,
      "learning_rate": 2.9919788994471945e-05,
      "loss": 1.2749,
      "step": 620
    },
    {
      "epoch": 0.00975043528728961,
      "grad_norm": 1.2711650133132935,
      "learning_rate": 2.991824051946175e-05,
      "loss": 1.3297,
      "step": 630
    },
    {
      "epoch": 0.009905204101373574,
      "grad_norm": 1.758601188659668,
      "learning_rate": 2.9916692044451556e-05,
      "loss": 1.1486,
      "step": 640
    },
    {
      "epoch": 0.010059972915457535,
      "grad_norm": 3.1819708347320557,
      "learning_rate": 2.9915143569441364e-05,
      "loss": 1.2565,
      "step": 650
    },
    {
      "epoch": 0.010214741729541498,
      "grad_norm": 1.0954262018203735,
      "learning_rate": 2.991359509443117e-05,
      "loss": 1.1778,
      "step": 660
    },
    {
      "epoch": 0.010369510543625459,
      "grad_norm": 1.8710100650787354,
      "learning_rate": 2.9912046619420974e-05,
      "loss": 1.2119,
      "step": 670
    },
    {
      "epoch": 0.010524279357709422,
      "grad_norm": 2.4928297996520996,
      "learning_rate": 2.991049814441078e-05,
      "loss": 1.1229,
      "step": 680
    },
    {
      "epoch": 0.010679048171793383,
      "grad_norm": 2.735212802886963,
      "learning_rate": 2.9908949669400585e-05,
      "loss": 1.1905,
      "step": 690
    },
    {
      "epoch": 0.010833816985877346,
      "grad_norm": 3.066115379333496,
      "learning_rate": 2.9907401194390392e-05,
      "loss": 1.3761,
      "step": 700
    },
    {
      "epoch": 0.010988585799961308,
      "grad_norm": 2.8772833347320557,
      "learning_rate": 2.9905852719380196e-05,
      "loss": 1.1788,
      "step": 710
    },
    {
      "epoch": 0.01114335461404527,
      "grad_norm": 2.1440041065216064,
      "learning_rate": 2.9904304244370003e-05,
      "loss": 1.3344,
      "step": 720
    },
    {
      "epoch": 0.011298123428129232,
      "grad_norm": 5.085113048553467,
      "learning_rate": 2.990275576935981e-05,
      "loss": 1.3832,
      "step": 730
    },
    {
      "epoch": 0.011452892242213193,
      "grad_norm": 2.342639207839966,
      "learning_rate": 2.9901207294349614e-05,
      "loss": 1.2415,
      "step": 740
    },
    {
      "epoch": 0.011607661056297156,
      "grad_norm": 1.9859319925308228,
      "learning_rate": 2.989965881933942e-05,
      "loss": 1.3302,
      "step": 750
    },
    {
      "epoch": 0.011762429870381117,
      "grad_norm": 2.8693759441375732,
      "learning_rate": 2.9898110344329225e-05,
      "loss": 1.1293,
      "step": 760
    },
    {
      "epoch": 0.01191719868446508,
      "grad_norm": 2.0931291580200195,
      "learning_rate": 2.9896561869319032e-05,
      "loss": 1.2445,
      "step": 770
    },
    {
      "epoch": 0.012071967498549042,
      "grad_norm": 2.343360662460327,
      "learning_rate": 2.989501339430884e-05,
      "loss": 1.2867,
      "step": 780
    },
    {
      "epoch": 0.012226736312633005,
      "grad_norm": 1.6369969844818115,
      "learning_rate": 2.9893464919298647e-05,
      "loss": 1.2362,
      "step": 790
    },
    {
      "epoch": 0.012381505126716966,
      "grad_norm": 3.0782437324523926,
      "learning_rate": 2.989191644428845e-05,
      "loss": 1.2978,
      "step": 800
    }
  ],
  "logging_steps": 10,
  "max_steps": 193839,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7690203317665792.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
