{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.93339555970398,
  "eval_steps": 500,
  "global_step": 11000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0026668444562970863,
      "grad_norm": 1.6685614585876465,
      "learning_rate": 2.7e-06,
      "loss": 3.0703,
      "step": 10
    },
    {
      "epoch": 0.005333688912594173,
      "grad_norm": 1.75909423828125,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 3.1946,
      "step": 20
    },
    {
      "epoch": 0.00800053336889126,
      "grad_norm": 0.9162336587905884,
      "learning_rate": 8.7e-06,
      "loss": 2.9936,
      "step": 30
    },
    {
      "epoch": 0.010667377825188345,
      "grad_norm": 0.9862501621246338,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 3.0793,
      "step": 40
    },
    {
      "epoch": 0.013334222281485432,
      "grad_norm": 1.1814594268798828,
      "learning_rate": 1.47e-05,
      "loss": 3.1048,
      "step": 50
    },
    {
      "epoch": 0.01600106673778252,
      "grad_norm": 0.7411383986473083,
      "learning_rate": 1.77e-05,
      "loss": 2.9482,
      "step": 60
    },
    {
      "epoch": 0.018667911194079605,
      "grad_norm": 0.6314902901649475,
      "learning_rate": 2.07e-05,
      "loss": 2.9017,
      "step": 70
    },
    {
      "epoch": 0.02133475565037669,
      "grad_norm": 0.8828563690185547,
      "learning_rate": 2.37e-05,
      "loss": 2.7321,
      "step": 80
    },
    {
      "epoch": 0.02400160010667378,
      "grad_norm": 1.0619863271713257,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.5866,
      "step": 90
    },
    {
      "epoch": 0.026668444562970864,
      "grad_norm": 1.3451136350631714,
      "learning_rate": 2.97e-05,
      "loss": 2.3816,
      "step": 100
    },
    {
      "epoch": 0.029335289019267952,
      "grad_norm": 2.1078147888183594,
      "learning_rate": 2.9975784753363227e-05,
      "loss": 1.942,
      "step": 110
    },
    {
      "epoch": 0.03200213347556504,
      "grad_norm": 1.6735255718231201,
      "learning_rate": 2.9948878923766818e-05,
      "loss": 1.998,
      "step": 120
    },
    {
      "epoch": 0.034668977931862126,
      "grad_norm": 2.023973226547241,
      "learning_rate": 2.9921973094170404e-05,
      "loss": 1.7228,
      "step": 130
    },
    {
      "epoch": 0.03733582238815921,
      "grad_norm": 2.307169198989868,
      "learning_rate": 2.9895067264573995e-05,
      "loss": 1.6087,
      "step": 140
    },
    {
      "epoch": 0.040002666844456296,
      "grad_norm": 2.0462076663970947,
      "learning_rate": 2.9868161434977578e-05,
      "loss": 1.4846,
      "step": 150
    },
    {
      "epoch": 0.04266951130075338,
      "grad_norm": 1.2614914178848267,
      "learning_rate": 2.9841255605381165e-05,
      "loss": 1.5903,
      "step": 160
    },
    {
      "epoch": 0.04533635575705047,
      "grad_norm": 1.6010273694992065,
      "learning_rate": 2.9814349775784755e-05,
      "loss": 1.665,
      "step": 170
    },
    {
      "epoch": 0.04800320021334756,
      "grad_norm": 3.4474918842315674,
      "learning_rate": 2.978744394618834e-05,
      "loss": 1.4479,
      "step": 180
    },
    {
      "epoch": 0.05067004466964464,
      "grad_norm": 1.4517831802368164,
      "learning_rate": 2.976053811659193e-05,
      "loss": 1.4684,
      "step": 190
    },
    {
      "epoch": 0.05333688912594173,
      "grad_norm": 2.0061304569244385,
      "learning_rate": 2.973363228699552e-05,
      "loss": 1.5592,
      "step": 200
    },
    {
      "epoch": 0.05600373358223881,
      "grad_norm": 1.7196167707443237,
      "learning_rate": 2.9706726457399102e-05,
      "loss": 1.4278,
      "step": 210
    },
    {
      "epoch": 0.058670578038535905,
      "grad_norm": 1.4130134582519531,
      "learning_rate": 2.9679820627802692e-05,
      "loss": 1.4361,
      "step": 220
    },
    {
      "epoch": 0.06133742249483299,
      "grad_norm": 1.2973930835723877,
      "learning_rate": 2.965291479820628e-05,
      "loss": 1.3997,
      "step": 230
    },
    {
      "epoch": 0.06400426695113008,
      "grad_norm": 1.6527677774429321,
      "learning_rate": 2.9626008968609866e-05,
      "loss": 1.3031,
      "step": 240
    },
    {
      "epoch": 0.06667111140742717,
      "grad_norm": 1.717215895652771,
      "learning_rate": 2.9599103139013456e-05,
      "loss": 1.4065,
      "step": 250
    },
    {
      "epoch": 0.06933795586372425,
      "grad_norm": 1.5893999338150024,
      "learning_rate": 2.957219730941704e-05,
      "loss": 1.3365,
      "step": 260
    },
    {
      "epoch": 0.07200480032002134,
      "grad_norm": 2.3828392028808594,
      "learning_rate": 2.954529147982063e-05,
      "loss": 1.5452,
      "step": 270
    },
    {
      "epoch": 0.07467164477631842,
      "grad_norm": 2.468156099319458,
      "learning_rate": 2.9518385650224216e-05,
      "loss": 1.4707,
      "step": 280
    },
    {
      "epoch": 0.0773384892326155,
      "grad_norm": 3.317854881286621,
      "learning_rate": 2.9491479820627803e-05,
      "loss": 1.4477,
      "step": 290
    },
    {
      "epoch": 0.08000533368891259,
      "grad_norm": 1.7551945447921753,
      "learning_rate": 2.9464573991031393e-05,
      "loss": 1.4011,
      "step": 300
    },
    {
      "epoch": 0.08267217814520968,
      "grad_norm": 2.017289400100708,
      "learning_rate": 2.943766816143498e-05,
      "loss": 1.4285,
      "step": 310
    },
    {
      "epoch": 0.08533902260150676,
      "grad_norm": 2.6567046642303467,
      "learning_rate": 2.9410762331838563e-05,
      "loss": 1.3697,
      "step": 320
    },
    {
      "epoch": 0.08800586705780386,
      "grad_norm": 1.5890097618103027,
      "learning_rate": 2.9383856502242153e-05,
      "loss": 1.4326,
      "step": 330
    },
    {
      "epoch": 0.09067271151410095,
      "grad_norm": 1.6197402477264404,
      "learning_rate": 2.935695067264574e-05,
      "loss": 1.4133,
      "step": 340
    },
    {
      "epoch": 0.09333955597039803,
      "grad_norm": 1.6032702922821045,
      "learning_rate": 2.933004484304933e-05,
      "loss": 1.3929,
      "step": 350
    },
    {
      "epoch": 0.09600640042669512,
      "grad_norm": 1.4658114910125732,
      "learning_rate": 2.9303139013452917e-05,
      "loss": 1.3898,
      "step": 360
    },
    {
      "epoch": 0.0986732448829922,
      "grad_norm": 2.173036575317383,
      "learning_rate": 2.9276233183856504e-05,
      "loss": 1.3765,
      "step": 370
    },
    {
      "epoch": 0.10134008933928929,
      "grad_norm": 1.9849611520767212,
      "learning_rate": 2.924932735426009e-05,
      "loss": 1.3851,
      "step": 380
    },
    {
      "epoch": 0.10400693379558637,
      "grad_norm": 1.6532624959945679,
      "learning_rate": 2.9222421524663677e-05,
      "loss": 1.3015,
      "step": 390
    },
    {
      "epoch": 0.10667377825188346,
      "grad_norm": 2.217116117477417,
      "learning_rate": 2.9195515695067264e-05,
      "loss": 1.3141,
      "step": 400
    },
    {
      "epoch": 0.10934062270818054,
      "grad_norm": 1.6312416791915894,
      "learning_rate": 2.9168609865470854e-05,
      "loss": 1.4175,
      "step": 410
    },
    {
      "epoch": 0.11200746716447763,
      "grad_norm": 1.3584424257278442,
      "learning_rate": 2.914170403587444e-05,
      "loss": 1.3326,
      "step": 420
    },
    {
      "epoch": 0.11467431162077472,
      "grad_norm": 1.7362375259399414,
      "learning_rate": 2.9114798206278028e-05,
      "loss": 1.4541,
      "step": 430
    },
    {
      "epoch": 0.11734115607707181,
      "grad_norm": 1.8448734283447266,
      "learning_rate": 2.9087892376681614e-05,
      "loss": 1.4106,
      "step": 440
    },
    {
      "epoch": 0.1200080005333689,
      "grad_norm": 1.6765189170837402,
      "learning_rate": 2.90609865470852e-05,
      "loss": 1.2372,
      "step": 450
    },
    {
      "epoch": 0.12267484498966598,
      "grad_norm": 2.2655398845672607,
      "learning_rate": 2.903408071748879e-05,
      "loss": 1.3398,
      "step": 460
    },
    {
      "epoch": 0.12534168944596305,
      "grad_norm": 1.909807801246643,
      "learning_rate": 2.9007174887892378e-05,
      "loss": 1.4086,
      "step": 470
    },
    {
      "epoch": 0.12800853390226016,
      "grad_norm": 1.9463777542114258,
      "learning_rate": 2.8980269058295965e-05,
      "loss": 1.5066,
      "step": 480
    },
    {
      "epoch": 0.13067537835855725,
      "grad_norm": 1.964261770248413,
      "learning_rate": 2.895336322869955e-05,
      "loss": 1.3166,
      "step": 490
    },
    {
      "epoch": 0.13334222281485433,
      "grad_norm": 1.7773431539535522,
      "learning_rate": 2.8926457399103138e-05,
      "loss": 1.3052,
      "step": 500
    },
    {
      "epoch": 0.13600906727115142,
      "grad_norm": 1.5242547988891602,
      "learning_rate": 2.889955156950673e-05,
      "loss": 1.2171,
      "step": 510
    },
    {
      "epoch": 0.1386759117274485,
      "grad_norm": 1.3102420568466187,
      "learning_rate": 2.8872645739910315e-05,
      "loss": 1.3452,
      "step": 520
    },
    {
      "epoch": 0.1413427561837456,
      "grad_norm": 2.2279772758483887,
      "learning_rate": 2.8845739910313902e-05,
      "loss": 1.2918,
      "step": 530
    },
    {
      "epoch": 0.14400960064004267,
      "grad_norm": 1.5424811840057373,
      "learning_rate": 2.8818834080717492e-05,
      "loss": 1.41,
      "step": 540
    },
    {
      "epoch": 0.14667644509633976,
      "grad_norm": 2.1982147693634033,
      "learning_rate": 2.8791928251121075e-05,
      "loss": 1.3033,
      "step": 550
    },
    {
      "epoch": 0.14934328955263684,
      "grad_norm": 2.2307322025299072,
      "learning_rate": 2.8765022421524662e-05,
      "loss": 1.271,
      "step": 560
    },
    {
      "epoch": 0.15201013400893393,
      "grad_norm": 1.507632851600647,
      "learning_rate": 2.8738116591928252e-05,
      "loss": 1.358,
      "step": 570
    },
    {
      "epoch": 0.154676978465231,
      "grad_norm": 1.6611908674240112,
      "learning_rate": 2.871121076233184e-05,
      "loss": 1.3259,
      "step": 580
    },
    {
      "epoch": 0.1573438229215281,
      "grad_norm": 1.9038922786712646,
      "learning_rate": 2.868430493273543e-05,
      "loss": 1.2556,
      "step": 590
    },
    {
      "epoch": 0.16001066737782518,
      "grad_norm": 1.8414819240570068,
      "learning_rate": 2.8657399103139013e-05,
      "loss": 1.2848,
      "step": 600
    },
    {
      "epoch": 0.16267751183412227,
      "grad_norm": 2.178612232208252,
      "learning_rate": 2.86304932735426e-05,
      "loss": 1.3225,
      "step": 610
    },
    {
      "epoch": 0.16534435629041935,
      "grad_norm": 1.948810338973999,
      "learning_rate": 2.860358744394619e-05,
      "loss": 1.4483,
      "step": 620
    },
    {
      "epoch": 0.16801120074671644,
      "grad_norm": 2.4314217567443848,
      "learning_rate": 2.8576681614349776e-05,
      "loss": 1.4498,
      "step": 630
    },
    {
      "epoch": 0.17067804520301352,
      "grad_norm": 2.0171730518341064,
      "learning_rate": 2.8549775784753363e-05,
      "loss": 1.3765,
      "step": 640
    },
    {
      "epoch": 0.1733448896593106,
      "grad_norm": 1.8588758707046509,
      "learning_rate": 2.8522869955156953e-05,
      "loss": 1.4265,
      "step": 650
    },
    {
      "epoch": 0.17601173411560772,
      "grad_norm": 2.11650013923645,
      "learning_rate": 2.8495964125560537e-05,
      "loss": 1.2341,
      "step": 660
    },
    {
      "epoch": 0.1786785785719048,
      "grad_norm": 2.8748862743377686,
      "learning_rate": 2.8469058295964127e-05,
      "loss": 1.2721,
      "step": 670
    },
    {
      "epoch": 0.1813454230282019,
      "grad_norm": 1.6160929203033447,
      "learning_rate": 2.8442152466367713e-05,
      "loss": 1.3996,
      "step": 680
    },
    {
      "epoch": 0.18401226748449898,
      "grad_norm": 2.076932668685913,
      "learning_rate": 2.84152466367713e-05,
      "loss": 1.3071,
      "step": 690
    },
    {
      "epoch": 0.18667911194079606,
      "grad_norm": 2.2404298782348633,
      "learning_rate": 2.838834080717489e-05,
      "loss": 1.1886,
      "step": 700
    },
    {
      "epoch": 0.18934595639709315,
      "grad_norm": 2.80698299407959,
      "learning_rate": 2.8361434977578477e-05,
      "loss": 1.4131,
      "step": 710
    },
    {
      "epoch": 0.19201280085339023,
      "grad_norm": 1.8653714656829834,
      "learning_rate": 2.8334529147982064e-05,
      "loss": 1.3246,
      "step": 720
    },
    {
      "epoch": 0.19467964530968732,
      "grad_norm": 1.8084092140197754,
      "learning_rate": 2.830762331838565e-05,
      "loss": 1.2468,
      "step": 730
    },
    {
      "epoch": 0.1973464897659844,
      "grad_norm": 2.532078266143799,
      "learning_rate": 2.8280717488789237e-05,
      "loss": 1.4273,
      "step": 740
    },
    {
      "epoch": 0.20001333422228149,
      "grad_norm": 1.496025562286377,
      "learning_rate": 2.8253811659192828e-05,
      "loss": 1.4087,
      "step": 750
    },
    {
      "epoch": 0.20268017867857857,
      "grad_norm": 1.8233000040054321,
      "learning_rate": 2.8226905829596414e-05,
      "loss": 1.3216,
      "step": 760
    },
    {
      "epoch": 0.20534702313487566,
      "grad_norm": 1.891856074333191,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 1.2398,
      "step": 770
    },
    {
      "epoch": 0.20801386759117274,
      "grad_norm": 2.577177047729492,
      "learning_rate": 2.8173094170403588e-05,
      "loss": 1.3597,
      "step": 780
    },
    {
      "epoch": 0.21068071204746983,
      "grad_norm": 2.1303510665893555,
      "learning_rate": 2.8146188340807175e-05,
      "loss": 1.2349,
      "step": 790
    },
    {
      "epoch": 0.2133475565037669,
      "grad_norm": 1.6417649984359741,
      "learning_rate": 2.8119282511210765e-05,
      "loss": 1.3491,
      "step": 800
    },
    {
      "epoch": 0.216014400960064,
      "grad_norm": 1.560893177986145,
      "learning_rate": 2.809237668161435e-05,
      "loss": 1.2329,
      "step": 810
    },
    {
      "epoch": 0.21868124541636108,
      "grad_norm": 3.016528844833374,
      "learning_rate": 2.8065470852017938e-05,
      "loss": 1.5235,
      "step": 820
    },
    {
      "epoch": 0.22134808987265817,
      "grad_norm": 2.842646360397339,
      "learning_rate": 2.8038565022421525e-05,
      "loss": 1.3189,
      "step": 830
    },
    {
      "epoch": 0.22401493432895525,
      "grad_norm": 3.1945722103118896,
      "learning_rate": 2.8011659192825112e-05,
      "loss": 1.3075,
      "step": 840
    },
    {
      "epoch": 0.22668177878525236,
      "grad_norm": 2.3178014755249023,
      "learning_rate": 2.79847533632287e-05,
      "loss": 1.2275,
      "step": 850
    },
    {
      "epoch": 0.22934862324154945,
      "grad_norm": 2.142584800720215,
      "learning_rate": 2.795784753363229e-05,
      "loss": 1.2513,
      "step": 860
    },
    {
      "epoch": 0.23201546769784653,
      "grad_norm": 2.6185638904571533,
      "learning_rate": 2.7930941704035875e-05,
      "loss": 1.4123,
      "step": 870
    },
    {
      "epoch": 0.23468231215414362,
      "grad_norm": 2.6876418590545654,
      "learning_rate": 2.7904035874439466e-05,
      "loss": 1.3333,
      "step": 880
    },
    {
      "epoch": 0.2373491566104407,
      "grad_norm": 1.9923455715179443,
      "learning_rate": 2.787713004484305e-05,
      "loss": 1.4033,
      "step": 890
    },
    {
      "epoch": 0.2400160010667378,
      "grad_norm": 1.7962653636932373,
      "learning_rate": 2.7850224215246636e-05,
      "loss": 1.2506,
      "step": 900
    },
    {
      "epoch": 0.24268284552303487,
      "grad_norm": 2.041306495666504,
      "learning_rate": 2.7823318385650226e-05,
      "loss": 1.3062,
      "step": 910
    },
    {
      "epoch": 0.24534968997933196,
      "grad_norm": 3.118671178817749,
      "learning_rate": 2.7796412556053813e-05,
      "loss": 1.3464,
      "step": 920
    },
    {
      "epoch": 0.24801653443562904,
      "grad_norm": 2.578413486480713,
      "learning_rate": 2.77695067264574e-05,
      "loss": 1.2364,
      "step": 930
    },
    {
      "epoch": 0.2506833788919261,
      "grad_norm": 2.1572928428649902,
      "learning_rate": 2.7742600896860986e-05,
      "loss": 1.2399,
      "step": 940
    },
    {
      "epoch": 0.2533502233482232,
      "grad_norm": 2.089820384979248,
      "learning_rate": 2.7715695067264573e-05,
      "loss": 1.273,
      "step": 950
    },
    {
      "epoch": 0.2560170678045203,
      "grad_norm": 1.6284842491149902,
      "learning_rate": 2.7688789237668163e-05,
      "loss": 1.3225,
      "step": 960
    },
    {
      "epoch": 0.2586839122608174,
      "grad_norm": 1.4777785539627075,
      "learning_rate": 2.766188340807175e-05,
      "loss": 1.3914,
      "step": 970
    },
    {
      "epoch": 0.2613507567171145,
      "grad_norm": 2.6242685317993164,
      "learning_rate": 2.7634977578475337e-05,
      "loss": 1.3613,
      "step": 980
    },
    {
      "epoch": 0.26401760117341155,
      "grad_norm": 2.281165599822998,
      "learning_rate": 2.7608071748878927e-05,
      "loss": 1.2922,
      "step": 990
    },
    {
      "epoch": 0.26668444562970867,
      "grad_norm": 3.7758634090423584,
      "learning_rate": 2.758116591928251e-05,
      "loss": 1.2909,
      "step": 1000
    },
    {
      "epoch": 0.2693512900860057,
      "grad_norm": 2.031165361404419,
      "learning_rate": 2.75542600896861e-05,
      "loss": 1.4805,
      "step": 1010
    },
    {
      "epoch": 0.27201813454230284,
      "grad_norm": 2.9128165245056152,
      "learning_rate": 2.7527354260089687e-05,
      "loss": 1.2884,
      "step": 1020
    },
    {
      "epoch": 0.2746849789985999,
      "grad_norm": 2.7255282402038574,
      "learning_rate": 2.7500448430493274e-05,
      "loss": 1.2613,
      "step": 1030
    },
    {
      "epoch": 0.277351823454897,
      "grad_norm": 2.2282557487487793,
      "learning_rate": 2.7473542600896864e-05,
      "loss": 1.3771,
      "step": 1040
    },
    {
      "epoch": 0.28001866791119406,
      "grad_norm": 1.8403500318527222,
      "learning_rate": 2.744663677130045e-05,
      "loss": 1.41,
      "step": 1050
    },
    {
      "epoch": 0.2826855123674912,
      "grad_norm": 2.022338390350342,
      "learning_rate": 2.7419730941704034e-05,
      "loss": 1.1857,
      "step": 1060
    },
    {
      "epoch": 0.28535235682378823,
      "grad_norm": 4.464076519012451,
      "learning_rate": 2.7392825112107624e-05,
      "loss": 1.2255,
      "step": 1070
    },
    {
      "epoch": 0.28801920128008535,
      "grad_norm": 2.566352605819702,
      "learning_rate": 2.736591928251121e-05,
      "loss": 1.2866,
      "step": 1080
    },
    {
      "epoch": 0.2906860457363824,
      "grad_norm": 1.6350287199020386,
      "learning_rate": 2.73390134529148e-05,
      "loss": 1.2544,
      "step": 1090
    },
    {
      "epoch": 0.2933528901926795,
      "grad_norm": 9.435344696044922,
      "learning_rate": 2.7312107623318388e-05,
      "loss": 1.288,
      "step": 1100
    },
    {
      "epoch": 0.2960197346489766,
      "grad_norm": 3.1245434284210205,
      "learning_rate": 2.728520179372197e-05,
      "loss": 1.1556,
      "step": 1110
    },
    {
      "epoch": 0.2986865791052737,
      "grad_norm": 1.5910060405731201,
      "learning_rate": 2.725829596412556e-05,
      "loss": 1.3149,
      "step": 1120
    },
    {
      "epoch": 0.3013534235615708,
      "grad_norm": 2.3597092628479004,
      "learning_rate": 2.7231390134529148e-05,
      "loss": 1.3485,
      "step": 1130
    },
    {
      "epoch": 0.30402026801786786,
      "grad_norm": 1.849654197692871,
      "learning_rate": 2.7204484304932735e-05,
      "loss": 1.392,
      "step": 1140
    },
    {
      "epoch": 0.30668711247416497,
      "grad_norm": 2.0269947052001953,
      "learning_rate": 2.7177578475336325e-05,
      "loss": 1.3417,
      "step": 1150
    },
    {
      "epoch": 0.309353956930462,
      "grad_norm": 3.150095224380493,
      "learning_rate": 2.7150672645739912e-05,
      "loss": 1.224,
      "step": 1160
    },
    {
      "epoch": 0.31202080138675914,
      "grad_norm": 2.573857069015503,
      "learning_rate": 2.71237668161435e-05,
      "loss": 1.2729,
      "step": 1170
    },
    {
      "epoch": 0.3146876458430562,
      "grad_norm": 3.6318275928497314,
      "learning_rate": 2.7096860986547085e-05,
      "loss": 1.4329,
      "step": 1180
    },
    {
      "epoch": 0.3173544902993533,
      "grad_norm": 1.8921600580215454,
      "learning_rate": 2.7069955156950672e-05,
      "loss": 1.3135,
      "step": 1190
    },
    {
      "epoch": 0.32002133475565037,
      "grad_norm": 2.07594895362854,
      "learning_rate": 2.7043049327354262e-05,
      "loss": 1.2554,
      "step": 1200
    },
    {
      "epoch": 0.3226881792119475,
      "grad_norm": 2.505819320678711,
      "learning_rate": 2.701614349775785e-05,
      "loss": 1.3107,
      "step": 1210
    },
    {
      "epoch": 0.32535502366824454,
      "grad_norm": 1.5099217891693115,
      "learning_rate": 2.6989237668161436e-05,
      "loss": 1.2956,
      "step": 1220
    },
    {
      "epoch": 0.32802186812454165,
      "grad_norm": 3.6655144691467285,
      "learning_rate": 2.6962331838565022e-05,
      "loss": 1.4177,
      "step": 1230
    },
    {
      "epoch": 0.3306887125808387,
      "grad_norm": 1.5827890634536743,
      "learning_rate": 2.693542600896861e-05,
      "loss": 1.289,
      "step": 1240
    },
    {
      "epoch": 0.3333555570371358,
      "grad_norm": 2.1707096099853516,
      "learning_rate": 2.69085201793722e-05,
      "loss": 1.1544,
      "step": 1250
    },
    {
      "epoch": 0.3360224014934329,
      "grad_norm": 3.505753993988037,
      "learning_rate": 2.6881614349775786e-05,
      "loss": 1.276,
      "step": 1260
    },
    {
      "epoch": 0.33868924594973,
      "grad_norm": 2.0274643898010254,
      "learning_rate": 2.6854708520179373e-05,
      "loss": 1.2412,
      "step": 1270
    },
    {
      "epoch": 0.34135609040602705,
      "grad_norm": 3.220634937286377,
      "learning_rate": 2.682780269058296e-05,
      "loss": 1.2764,
      "step": 1280
    },
    {
      "epoch": 0.34402293486232416,
      "grad_norm": 2.362579584121704,
      "learning_rate": 2.6800896860986546e-05,
      "loss": 1.2292,
      "step": 1290
    },
    {
      "epoch": 0.3466897793186212,
      "grad_norm": 2.1471924781799316,
      "learning_rate": 2.6773991031390133e-05,
      "loss": 1.1549,
      "step": 1300
    },
    {
      "epoch": 0.34935662377491833,
      "grad_norm": 2.3979291915893555,
      "learning_rate": 2.6747085201793723e-05,
      "loss": 1.3843,
      "step": 1310
    },
    {
      "epoch": 0.35202346823121544,
      "grad_norm": 2.3563737869262695,
      "learning_rate": 2.672017937219731e-05,
      "loss": 1.3989,
      "step": 1320
    },
    {
      "epoch": 0.3546903126875125,
      "grad_norm": 2.1546592712402344,
      "learning_rate": 2.66932735426009e-05,
      "loss": 1.2103,
      "step": 1330
    },
    {
      "epoch": 0.3573571571438096,
      "grad_norm": 2.303328037261963,
      "learning_rate": 2.6666367713004484e-05,
      "loss": 1.4714,
      "step": 1340
    },
    {
      "epoch": 0.36002400160010667,
      "grad_norm": 2.7732841968536377,
      "learning_rate": 2.663946188340807e-05,
      "loss": 1.2452,
      "step": 1350
    },
    {
      "epoch": 0.3626908460564038,
      "grad_norm": 2.2367608547210693,
      "learning_rate": 2.661255605381166e-05,
      "loss": 1.3522,
      "step": 1360
    },
    {
      "epoch": 0.36535769051270084,
      "grad_norm": 1.7247159481048584,
      "learning_rate": 2.6585650224215247e-05,
      "loss": 1.2224,
      "step": 1370
    },
    {
      "epoch": 0.36802453496899795,
      "grad_norm": 2.4635889530181885,
      "learning_rate": 2.6558744394618837e-05,
      "loss": 1.1846,
      "step": 1380
    },
    {
      "epoch": 0.370691379425295,
      "grad_norm": 3.275744915008545,
      "learning_rate": 2.6531838565022424e-05,
      "loss": 1.1516,
      "step": 1390
    },
    {
      "epoch": 0.3733582238815921,
      "grad_norm": 4.614669322967529,
      "learning_rate": 2.6504932735426008e-05,
      "loss": 1.2831,
      "step": 1400
    },
    {
      "epoch": 0.3760250683378892,
      "grad_norm": 1.9025970697402954,
      "learning_rate": 2.6478026905829598e-05,
      "loss": 1.3957,
      "step": 1410
    },
    {
      "epoch": 0.3786919127941863,
      "grad_norm": 1.6171571016311646,
      "learning_rate": 2.6451121076233184e-05,
      "loss": 1.2839,
      "step": 1420
    },
    {
      "epoch": 0.38135875725048335,
      "grad_norm": 2.1646041870117188,
      "learning_rate": 2.642421524663677e-05,
      "loss": 1.2892,
      "step": 1430
    },
    {
      "epoch": 0.38402560170678046,
      "grad_norm": 1.9128364324569702,
      "learning_rate": 2.639730941704036e-05,
      "loss": 1.2142,
      "step": 1440
    },
    {
      "epoch": 0.3866924461630775,
      "grad_norm": 2.190577268600464,
      "learning_rate": 2.6370403587443945e-05,
      "loss": 1.2709,
      "step": 1450
    },
    {
      "epoch": 0.38935929061937463,
      "grad_norm": 2.555307388305664,
      "learning_rate": 2.6343497757847535e-05,
      "loss": 1.2864,
      "step": 1460
    },
    {
      "epoch": 0.3920261350756717,
      "grad_norm": 2.2512621879577637,
      "learning_rate": 2.631659192825112e-05,
      "loss": 1.2908,
      "step": 1470
    },
    {
      "epoch": 0.3946929795319688,
      "grad_norm": 3.3611953258514404,
      "learning_rate": 2.628968609865471e-05,
      "loss": 1.3703,
      "step": 1480
    },
    {
      "epoch": 0.39735982398826586,
      "grad_norm": 1.9521065950393677,
      "learning_rate": 2.62627802690583e-05,
      "loss": 1.2323,
      "step": 1490
    },
    {
      "epoch": 0.40002666844456297,
      "grad_norm": 2.6804709434509277,
      "learning_rate": 2.6235874439461885e-05,
      "loss": 1.3066,
      "step": 1500
    },
    {
      "epoch": 0.4026935129008601,
      "grad_norm": 2.400055170059204,
      "learning_rate": 2.620896860986547e-05,
      "loss": 1.2685,
      "step": 1510
    },
    {
      "epoch": 0.40536035735715714,
      "grad_norm": 1.8262016773223877,
      "learning_rate": 2.618206278026906e-05,
      "loss": 1.4284,
      "step": 1520
    },
    {
      "epoch": 0.40802720181345425,
      "grad_norm": 2.3458213806152344,
      "learning_rate": 2.6155156950672646e-05,
      "loss": 1.1782,
      "step": 1530
    },
    {
      "epoch": 0.4106940462697513,
      "grad_norm": 2.109158992767334,
      "learning_rate": 2.6128251121076236e-05,
      "loss": 1.2091,
      "step": 1540
    },
    {
      "epoch": 0.4133608907260484,
      "grad_norm": 2.6827926635742188,
      "learning_rate": 2.6101345291479823e-05,
      "loss": 1.3798,
      "step": 1550
    },
    {
      "epoch": 0.4160277351823455,
      "grad_norm": 1.9525014162063599,
      "learning_rate": 2.607443946188341e-05,
      "loss": 1.2358,
      "step": 1560
    },
    {
      "epoch": 0.4186945796386426,
      "grad_norm": 1.9214991331100464,
      "learning_rate": 2.6047533632286996e-05,
      "loss": 1.3293,
      "step": 1570
    },
    {
      "epoch": 0.42136142409493965,
      "grad_norm": 2.2883927822113037,
      "learning_rate": 2.6020627802690583e-05,
      "loss": 1.3411,
      "step": 1580
    },
    {
      "epoch": 0.42402826855123676,
      "grad_norm": 1.9126136302947998,
      "learning_rate": 2.599372197309417e-05,
      "loss": 1.1699,
      "step": 1590
    },
    {
      "epoch": 0.4266951130075338,
      "grad_norm": 1.9284217357635498,
      "learning_rate": 2.596681614349776e-05,
      "loss": 1.3081,
      "step": 1600
    },
    {
      "epoch": 0.42936195746383093,
      "grad_norm": 1.693718671798706,
      "learning_rate": 2.5939910313901346e-05,
      "loss": 1.2355,
      "step": 1610
    },
    {
      "epoch": 0.432028801920128,
      "grad_norm": 2.2576653957366943,
      "learning_rate": 2.5913004484304933e-05,
      "loss": 1.195,
      "step": 1620
    },
    {
      "epoch": 0.4346956463764251,
      "grad_norm": 2.835171937942505,
      "learning_rate": 2.588609865470852e-05,
      "loss": 1.3297,
      "step": 1630
    },
    {
      "epoch": 0.43736249083272216,
      "grad_norm": 2.3279776573181152,
      "learning_rate": 2.5859192825112107e-05,
      "loss": 1.456,
      "step": 1640
    },
    {
      "epoch": 0.4400293352890193,
      "grad_norm": 2.4354567527770996,
      "learning_rate": 2.5832286995515697e-05,
      "loss": 1.387,
      "step": 1650
    },
    {
      "epoch": 0.44269617974531633,
      "grad_norm": 1.960999608039856,
      "learning_rate": 2.5805381165919284e-05,
      "loss": 1.1788,
      "step": 1660
    },
    {
      "epoch": 0.44536302420161344,
      "grad_norm": 2.9568841457366943,
      "learning_rate": 2.577847533632287e-05,
      "loss": 1.3337,
      "step": 1670
    },
    {
      "epoch": 0.4480298686579105,
      "grad_norm": 2.457146406173706,
      "learning_rate": 2.5751569506726457e-05,
      "loss": 1.3545,
      "step": 1680
    },
    {
      "epoch": 0.4506967131142076,
      "grad_norm": 2.1011133193969727,
      "learning_rate": 2.5724663677130044e-05,
      "loss": 1.2476,
      "step": 1690
    },
    {
      "epoch": 0.4533635575705047,
      "grad_norm": 2.0458414554595947,
      "learning_rate": 2.5697757847533634e-05,
      "loss": 1.2581,
      "step": 1700
    },
    {
      "epoch": 0.4560304020268018,
      "grad_norm": 2.1311800479888916,
      "learning_rate": 2.567085201793722e-05,
      "loss": 1.2853,
      "step": 1710
    },
    {
      "epoch": 0.4586972464830989,
      "grad_norm": 2.0991063117980957,
      "learning_rate": 2.5643946188340808e-05,
      "loss": 1.2863,
      "step": 1720
    },
    {
      "epoch": 0.46136409093939595,
      "grad_norm": 3.67287278175354,
      "learning_rate": 2.5617040358744398e-05,
      "loss": 1.2441,
      "step": 1730
    },
    {
      "epoch": 0.46403093539569307,
      "grad_norm": 2.02945876121521,
      "learning_rate": 2.559013452914798e-05,
      "loss": 1.1735,
      "step": 1740
    },
    {
      "epoch": 0.4666977798519901,
      "grad_norm": 3.179671049118042,
      "learning_rate": 2.556322869955157e-05,
      "loss": 1.2899,
      "step": 1750
    },
    {
      "epoch": 0.46936462430828724,
      "grad_norm": 1.897187352180481,
      "learning_rate": 2.5536322869955158e-05,
      "loss": 1.427,
      "step": 1760
    },
    {
      "epoch": 0.4720314687645843,
      "grad_norm": 2.7926790714263916,
      "learning_rate": 2.5509417040358745e-05,
      "loss": 1.423,
      "step": 1770
    },
    {
      "epoch": 0.4746983132208814,
      "grad_norm": 2.6110453605651855,
      "learning_rate": 2.5482511210762335e-05,
      "loss": 1.2661,
      "step": 1780
    },
    {
      "epoch": 0.47736515767717846,
      "grad_norm": 2.1609623432159424,
      "learning_rate": 2.5455605381165918e-05,
      "loss": 1.2076,
      "step": 1790
    },
    {
      "epoch": 0.4800320021334756,
      "grad_norm": 1.8283882141113281,
      "learning_rate": 2.5428699551569505e-05,
      "loss": 1.3534,
      "step": 1800
    },
    {
      "epoch": 0.48269884658977263,
      "grad_norm": 2.4391801357269287,
      "learning_rate": 2.5401793721973095e-05,
      "loss": 1.3339,
      "step": 1810
    },
    {
      "epoch": 0.48536569104606975,
      "grad_norm": 2.5644161701202393,
      "learning_rate": 2.5374887892376682e-05,
      "loss": 1.2081,
      "step": 1820
    },
    {
      "epoch": 0.4880325355023668,
      "grad_norm": 2.6660521030426025,
      "learning_rate": 2.5347982062780272e-05,
      "loss": 1.1833,
      "step": 1830
    },
    {
      "epoch": 0.4906993799586639,
      "grad_norm": 2.321380376815796,
      "learning_rate": 2.532107623318386e-05,
      "loss": 1.2848,
      "step": 1840
    },
    {
      "epoch": 0.493366224414961,
      "grad_norm": 1.709873080253601,
      "learning_rate": 2.5294170403587442e-05,
      "loss": 1.3036,
      "step": 1850
    },
    {
      "epoch": 0.4960330688712581,
      "grad_norm": 2.534829616546631,
      "learning_rate": 2.5267264573991032e-05,
      "loss": 1.1071,
      "step": 1860
    },
    {
      "epoch": 0.49869991332755514,
      "grad_norm": 1.5781172513961792,
      "learning_rate": 2.524035874439462e-05,
      "loss": 1.2947,
      "step": 1870
    },
    {
      "epoch": 0.5013667577838522,
      "grad_norm": 2.0670392513275146,
      "learning_rate": 2.5213452914798206e-05,
      "loss": 1.2312,
      "step": 1880
    },
    {
      "epoch": 0.5040336022401494,
      "grad_norm": 2.2086429595947266,
      "learning_rate": 2.5186547085201796e-05,
      "loss": 1.1624,
      "step": 1890
    },
    {
      "epoch": 0.5067004466964464,
      "grad_norm": 2.997358798980713,
      "learning_rate": 2.5162331838565023e-05,
      "loss": 1.2702,
      "step": 1900
    },
    {
      "epoch": 0.5093672911527435,
      "grad_norm": 2.3558380603790283,
      "learning_rate": 2.5135426008968613e-05,
      "loss": 1.2938,
      "step": 1910
    },
    {
      "epoch": 0.5120341356090407,
      "grad_norm": 3.1746294498443604,
      "learning_rate": 2.5108520179372196e-05,
      "loss": 1.5036,
      "step": 1920
    },
    {
      "epoch": 0.5147009800653377,
      "grad_norm": 1.7981384992599487,
      "learning_rate": 2.5081614349775783e-05,
      "loss": 1.3014,
      "step": 1930
    },
    {
      "epoch": 0.5173678245216348,
      "grad_norm": 1.9545763731002808,
      "learning_rate": 2.5054708520179373e-05,
      "loss": 1.203,
      "step": 1940
    },
    {
      "epoch": 0.5200346689779318,
      "grad_norm": 2.5747509002685547,
      "learning_rate": 2.502780269058296e-05,
      "loss": 1.273,
      "step": 1950
    },
    {
      "epoch": 0.522701513434229,
      "grad_norm": 1.7839480638504028,
      "learning_rate": 2.500089686098655e-05,
      "loss": 1.2949,
      "step": 1960
    },
    {
      "epoch": 0.525368357890526,
      "grad_norm": 2.238955020904541,
      "learning_rate": 2.4973991031390137e-05,
      "loss": 1.2014,
      "step": 1970
    },
    {
      "epoch": 0.5280352023468231,
      "grad_norm": 2.3553390502929688,
      "learning_rate": 2.494708520179372e-05,
      "loss": 1.1365,
      "step": 1980
    },
    {
      "epoch": 0.5307020468031202,
      "grad_norm": 1.6561070680618286,
      "learning_rate": 2.492017937219731e-05,
      "loss": 1.3108,
      "step": 1990
    },
    {
      "epoch": 0.5333688912594173,
      "grad_norm": 1.8417413234710693,
      "learning_rate": 2.4893273542600897e-05,
      "loss": 1.3094,
      "step": 2000
    },
    {
      "epoch": 0.5360357357157144,
      "grad_norm": 2.6379446983337402,
      "learning_rate": 2.4866367713004484e-05,
      "loss": 1.2439,
      "step": 2010
    },
    {
      "epoch": 0.5387025801720114,
      "grad_norm": 2.175325632095337,
      "learning_rate": 2.4839461883408074e-05,
      "loss": 1.2364,
      "step": 2020
    },
    {
      "epoch": 0.5413694246283085,
      "grad_norm": 1.7018078565597534,
      "learning_rate": 2.481255605381166e-05,
      "loss": 1.2317,
      "step": 2030
    },
    {
      "epoch": 0.5440362690846057,
      "grad_norm": 2.5843873023986816,
      "learning_rate": 2.4785650224215248e-05,
      "loss": 1.1947,
      "step": 2040
    },
    {
      "epoch": 0.5467031135409027,
      "grad_norm": 2.1436927318573,
      "learning_rate": 2.4758744394618834e-05,
      "loss": 1.2846,
      "step": 2050
    },
    {
      "epoch": 0.5493699579971998,
      "grad_norm": 2.3880879878997803,
      "learning_rate": 2.473183856502242e-05,
      "loss": 1.1512,
      "step": 2060
    },
    {
      "epoch": 0.5520368024534968,
      "grad_norm": 3.1659021377563477,
      "learning_rate": 2.470493273542601e-05,
      "loss": 1.259,
      "step": 2070
    },
    {
      "epoch": 0.554703646909794,
      "grad_norm": 2.366670846939087,
      "learning_rate": 2.4678026905829598e-05,
      "loss": 1.2079,
      "step": 2080
    },
    {
      "epoch": 0.5573704913660911,
      "grad_norm": 2.7751195430755615,
      "learning_rate": 2.465112107623318e-05,
      "loss": 1.2797,
      "step": 2090
    },
    {
      "epoch": 0.5600373358223881,
      "grad_norm": 2.313422679901123,
      "learning_rate": 2.462421524663677e-05,
      "loss": 1.116,
      "step": 2100
    },
    {
      "epoch": 0.5627041802786853,
      "grad_norm": 1.8206138610839844,
      "learning_rate": 2.4597309417040358e-05,
      "loss": 1.2293,
      "step": 2110
    },
    {
      "epoch": 0.5653710247349824,
      "grad_norm": 2.392133951187134,
      "learning_rate": 2.457040358744395e-05,
      "loss": 1.3056,
      "step": 2120
    },
    {
      "epoch": 0.5680378691912794,
      "grad_norm": 2.3944966793060303,
      "learning_rate": 2.4543497757847535e-05,
      "loss": 1.3479,
      "step": 2130
    },
    {
      "epoch": 0.5707047136475765,
      "grad_norm": 2.0997602939605713,
      "learning_rate": 2.4516591928251122e-05,
      "loss": 1.1281,
      "step": 2140
    },
    {
      "epoch": 0.5733715581038736,
      "grad_norm": 2.446110248565674,
      "learning_rate": 2.448968609865471e-05,
      "loss": 1.4117,
      "step": 2150
    },
    {
      "epoch": 0.5760384025601707,
      "grad_norm": 3.0998613834381104,
      "learning_rate": 2.4462780269058295e-05,
      "loss": 1.2985,
      "step": 2160
    },
    {
      "epoch": 0.5787052470164677,
      "grad_norm": 2.0059802532196045,
      "learning_rate": 2.4435874439461882e-05,
      "loss": 1.1578,
      "step": 2170
    },
    {
      "epoch": 0.5813720914727648,
      "grad_norm": 1.581994891166687,
      "learning_rate": 2.4408968609865472e-05,
      "loss": 1.3089,
      "step": 2180
    },
    {
      "epoch": 0.584038935929062,
      "grad_norm": 2.769831657409668,
      "learning_rate": 2.438206278026906e-05,
      "loss": 1.2739,
      "step": 2190
    },
    {
      "epoch": 0.586705780385359,
      "grad_norm": 2.2474875450134277,
      "learning_rate": 2.435515695067265e-05,
      "loss": 1.3085,
      "step": 2200
    },
    {
      "epoch": 0.5893726248416561,
      "grad_norm": 2.7766475677490234,
      "learning_rate": 2.4328251121076233e-05,
      "loss": 1.1912,
      "step": 2210
    },
    {
      "epoch": 0.5920394692979531,
      "grad_norm": 2.0806617736816406,
      "learning_rate": 2.430134529147982e-05,
      "loss": 1.3092,
      "step": 2220
    },
    {
      "epoch": 0.5947063137542503,
      "grad_norm": 2.5305960178375244,
      "learning_rate": 2.427443946188341e-05,
      "loss": 1.2558,
      "step": 2230
    },
    {
      "epoch": 0.5973731582105474,
      "grad_norm": 2.8795580863952637,
      "learning_rate": 2.4247533632286996e-05,
      "loss": 1.2428,
      "step": 2240
    },
    {
      "epoch": 0.6000400026668444,
      "grad_norm": 3.843207836151123,
      "learning_rate": 2.4220627802690583e-05,
      "loss": 1.234,
      "step": 2250
    },
    {
      "epoch": 0.6027068471231416,
      "grad_norm": 2.75860857963562,
      "learning_rate": 2.419372197309417e-05,
      "loss": 1.2166,
      "step": 2260
    },
    {
      "epoch": 0.6053736915794387,
      "grad_norm": 2.2074406147003174,
      "learning_rate": 2.4166816143497757e-05,
      "loss": 1.0694,
      "step": 2270
    },
    {
      "epoch": 0.6080405360357357,
      "grad_norm": 3.072129249572754,
      "learning_rate": 2.4139910313901347e-05,
      "loss": 1.0594,
      "step": 2280
    },
    {
      "epoch": 0.6107073804920328,
      "grad_norm": 2.79135799407959,
      "learning_rate": 2.4113004484304934e-05,
      "loss": 1.1466,
      "step": 2290
    },
    {
      "epoch": 0.6133742249483299,
      "grad_norm": 1.770476222038269,
      "learning_rate": 2.408609865470852e-05,
      "loss": 1.2168,
      "step": 2300
    },
    {
      "epoch": 0.616041069404627,
      "grad_norm": 1.9092341661453247,
      "learning_rate": 2.405919282511211e-05,
      "loss": 1.1798,
      "step": 2310
    },
    {
      "epoch": 0.618707913860924,
      "grad_norm": 2.796212673187256,
      "learning_rate": 2.4032286995515694e-05,
      "loss": 1.2433,
      "step": 2320
    },
    {
      "epoch": 0.6213747583172211,
      "grad_norm": 1.9433553218841553,
      "learning_rate": 2.4005381165919284e-05,
      "loss": 1.0904,
      "step": 2330
    },
    {
      "epoch": 0.6240416027735183,
      "grad_norm": 2.0869102478027344,
      "learning_rate": 2.397847533632287e-05,
      "loss": 1.1898,
      "step": 2340
    },
    {
      "epoch": 0.6267084472298153,
      "grad_norm": 2.2314586639404297,
      "learning_rate": 2.3951569506726457e-05,
      "loss": 1.192,
      "step": 2350
    },
    {
      "epoch": 0.6293752916861124,
      "grad_norm": 2.724501132965088,
      "learning_rate": 2.3924663677130048e-05,
      "loss": 1.2526,
      "step": 2360
    },
    {
      "epoch": 0.6320421361424094,
      "grad_norm": 1.9226382970809937,
      "learning_rate": 2.3897757847533634e-05,
      "loss": 1.1261,
      "step": 2370
    },
    {
      "epoch": 0.6347089805987066,
      "grad_norm": 2.2586922645568848,
      "learning_rate": 2.3870852017937218e-05,
      "loss": 1.135,
      "step": 2380
    },
    {
      "epoch": 0.6373758250550037,
      "grad_norm": 2.2069034576416016,
      "learning_rate": 2.3843946188340808e-05,
      "loss": 1.3226,
      "step": 2390
    },
    {
      "epoch": 0.6400426695113007,
      "grad_norm": 3.012927293777466,
      "learning_rate": 2.3817040358744395e-05,
      "loss": 1.1156,
      "step": 2400
    },
    {
      "epoch": 0.6427095139675978,
      "grad_norm": 2.273000955581665,
      "learning_rate": 2.3790134529147985e-05,
      "loss": 1.3411,
      "step": 2410
    },
    {
      "epoch": 0.645376358423895,
      "grad_norm": 2.2169415950775146,
      "learning_rate": 2.376322869955157e-05,
      "loss": 1.2033,
      "step": 2420
    },
    {
      "epoch": 0.648043202880192,
      "grad_norm": 2.386443853378296,
      "learning_rate": 2.3736322869955155e-05,
      "loss": 1.2576,
      "step": 2430
    },
    {
      "epoch": 0.6507100473364891,
      "grad_norm": 1.6831440925598145,
      "learning_rate": 2.3709417040358745e-05,
      "loss": 1.2793,
      "step": 2440
    },
    {
      "epoch": 0.6533768917927862,
      "grad_norm": 2.519660234451294,
      "learning_rate": 2.3682511210762332e-05,
      "loss": 1.0963,
      "step": 2450
    },
    {
      "epoch": 0.6560437362490833,
      "grad_norm": 2.2155473232269287,
      "learning_rate": 2.365560538116592e-05,
      "loss": 1.2719,
      "step": 2460
    },
    {
      "epoch": 0.6587105807053804,
      "grad_norm": 3.1347506046295166,
      "learning_rate": 2.362869955156951e-05,
      "loss": 1.0424,
      "step": 2470
    },
    {
      "epoch": 0.6613774251616774,
      "grad_norm": 2.327798843383789,
      "learning_rate": 2.3601793721973095e-05,
      "loss": 1.1616,
      "step": 2480
    },
    {
      "epoch": 0.6640442696179746,
      "grad_norm": 1.9347264766693115,
      "learning_rate": 2.3574887892376682e-05,
      "loss": 1.105,
      "step": 2490
    },
    {
      "epoch": 0.6667111140742716,
      "grad_norm": 2.378361940383911,
      "learning_rate": 2.354798206278027e-05,
      "loss": 1.1892,
      "step": 2500
    },
    {
      "epoch": 0.6693779585305687,
      "grad_norm": 2.0585262775421143,
      "learning_rate": 2.3521076233183856e-05,
      "loss": 1.2872,
      "step": 2510
    },
    {
      "epoch": 0.6720448029868658,
      "grad_norm": 1.93104887008667,
      "learning_rate": 2.3494170403587446e-05,
      "loss": 1.3018,
      "step": 2520
    },
    {
      "epoch": 0.6747116474431629,
      "grad_norm": 2.2971420288085938,
      "learning_rate": 2.3467264573991033e-05,
      "loss": 1.2994,
      "step": 2530
    },
    {
      "epoch": 0.67737849189946,
      "grad_norm": 2.2501637935638428,
      "learning_rate": 2.344035874439462e-05,
      "loss": 1.2031,
      "step": 2540
    },
    {
      "epoch": 0.680045336355757,
      "grad_norm": 3.0661215782165527,
      "learning_rate": 2.3413452914798206e-05,
      "loss": 1.2059,
      "step": 2550
    },
    {
      "epoch": 0.6827121808120541,
      "grad_norm": 3.5150504112243652,
      "learning_rate": 2.3386547085201793e-05,
      "loss": 1.1532,
      "step": 2560
    },
    {
      "epoch": 0.6853790252683513,
      "grad_norm": 2.18200945854187,
      "learning_rate": 2.3359641255605383e-05,
      "loss": 1.1276,
      "step": 2570
    },
    {
      "epoch": 0.6880458697246483,
      "grad_norm": 1.9129164218902588,
      "learning_rate": 2.333273542600897e-05,
      "loss": 1.19,
      "step": 2580
    },
    {
      "epoch": 0.6907127141809454,
      "grad_norm": 2.0710813999176025,
      "learning_rate": 2.3305829596412557e-05,
      "loss": 1.2022,
      "step": 2590
    },
    {
      "epoch": 0.6933795586372424,
      "grad_norm": 2.270831346511841,
      "learning_rate": 2.3278923766816143e-05,
      "loss": 1.2545,
      "step": 2600
    },
    {
      "epoch": 0.6960464030935396,
      "grad_norm": 2.8375842571258545,
      "learning_rate": 2.325201793721973e-05,
      "loss": 1.0716,
      "step": 2610
    },
    {
      "epoch": 0.6987132475498367,
      "grad_norm": 2.183072805404663,
      "learning_rate": 2.322511210762332e-05,
      "loss": 1.2053,
      "step": 2620
    },
    {
      "epoch": 0.7013800920061337,
      "grad_norm": 2.8211779594421387,
      "learning_rate": 2.3198206278026907e-05,
      "loss": 1.1499,
      "step": 2630
    },
    {
      "epoch": 0.7040469364624309,
      "grad_norm": 1.7545716762542725,
      "learning_rate": 2.3171300448430494e-05,
      "loss": 1.0868,
      "step": 2640
    },
    {
      "epoch": 0.7067137809187279,
      "grad_norm": 2.6505346298217773,
      "learning_rate": 2.3144394618834084e-05,
      "loss": 1.1313,
      "step": 2650
    },
    {
      "epoch": 0.709380625375025,
      "grad_norm": 2.4273786544799805,
      "learning_rate": 2.3117488789237667e-05,
      "loss": 1.292,
      "step": 2660
    },
    {
      "epoch": 0.712047469831322,
      "grad_norm": 1.9608126878738403,
      "learning_rate": 2.3090582959641254e-05,
      "loss": 1.2358,
      "step": 2670
    },
    {
      "epoch": 0.7147143142876192,
      "grad_norm": 2.4075582027435303,
      "learning_rate": 2.3063677130044844e-05,
      "loss": 1.1375,
      "step": 2680
    },
    {
      "epoch": 0.7173811587439163,
      "grad_norm": 2.6369564533233643,
      "learning_rate": 2.303677130044843e-05,
      "loss": 1.3461,
      "step": 2690
    },
    {
      "epoch": 0.7200480032002133,
      "grad_norm": 3.5202200412750244,
      "learning_rate": 2.300986547085202e-05,
      "loss": 1.071,
      "step": 2700
    },
    {
      "epoch": 0.7227148476565104,
      "grad_norm": 2.002875566482544,
      "learning_rate": 2.2982959641255608e-05,
      "loss": 1.3694,
      "step": 2710
    },
    {
      "epoch": 0.7253816921128076,
      "grad_norm": 2.766411781311035,
      "learning_rate": 2.295605381165919e-05,
      "loss": 1.1373,
      "step": 2720
    },
    {
      "epoch": 0.7280485365691046,
      "grad_norm": 2.476249933242798,
      "learning_rate": 2.292914798206278e-05,
      "loss": 1.182,
      "step": 2730
    },
    {
      "epoch": 0.7307153810254017,
      "grad_norm": 2.207679033279419,
      "learning_rate": 2.2902242152466368e-05,
      "loss": 1.1686,
      "step": 2740
    },
    {
      "epoch": 0.7333822254816987,
      "grad_norm": 1.6975233554840088,
      "learning_rate": 2.2875336322869955e-05,
      "loss": 1.2423,
      "step": 2750
    },
    {
      "epoch": 0.7360490699379959,
      "grad_norm": 2.86650013923645,
      "learning_rate": 2.2848430493273545e-05,
      "loss": 1.206,
      "step": 2760
    },
    {
      "epoch": 0.738715914394293,
      "grad_norm": 2.139098644256592,
      "learning_rate": 2.282152466367713e-05,
      "loss": 1.2673,
      "step": 2770
    },
    {
      "epoch": 0.74138275885059,
      "grad_norm": 2.4810869693756104,
      "learning_rate": 2.279461883408072e-05,
      "loss": 1.1246,
      "step": 2780
    },
    {
      "epoch": 0.7440496033068871,
      "grad_norm": 2.5989346504211426,
      "learning_rate": 2.2767713004484305e-05,
      "loss": 1.2761,
      "step": 2790
    },
    {
      "epoch": 0.7467164477631842,
      "grad_norm": 2.5899415016174316,
      "learning_rate": 2.2740807174887892e-05,
      "loss": 1.1793,
      "step": 2800
    },
    {
      "epoch": 0.7493832922194813,
      "grad_norm": 2.051947832107544,
      "learning_rate": 2.2713901345291482e-05,
      "loss": 1.287,
      "step": 2810
    },
    {
      "epoch": 0.7520501366757784,
      "grad_norm": 2.5028858184814453,
      "learning_rate": 2.268699551569507e-05,
      "loss": 1.2253,
      "step": 2820
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 1.8893635272979736,
      "learning_rate": 2.2660089686098652e-05,
      "loss": 1.2198,
      "step": 2830
    },
    {
      "epoch": 0.7573838255883726,
      "grad_norm": 2.309460163116455,
      "learning_rate": 2.2633183856502243e-05,
      "loss": 1.0866,
      "step": 2840
    },
    {
      "epoch": 0.7600506700446696,
      "grad_norm": 2.8877756595611572,
      "learning_rate": 2.260627802690583e-05,
      "loss": 1.0463,
      "step": 2850
    },
    {
      "epoch": 0.7627175145009667,
      "grad_norm": 2.0696988105773926,
      "learning_rate": 2.257937219730942e-05,
      "loss": 1.3013,
      "step": 2860
    },
    {
      "epoch": 0.7653843589572639,
      "grad_norm": 2.0616371631622314,
      "learning_rate": 2.2552466367713006e-05,
      "loss": 1.1992,
      "step": 2870
    },
    {
      "epoch": 0.7680512034135609,
      "grad_norm": 2.2981927394866943,
      "learning_rate": 2.2525560538116593e-05,
      "loss": 1.2034,
      "step": 2880
    },
    {
      "epoch": 0.770718047869858,
      "grad_norm": 2.5992932319641113,
      "learning_rate": 2.249865470852018e-05,
      "loss": 1.2134,
      "step": 2890
    },
    {
      "epoch": 0.773384892326155,
      "grad_norm": 2.4630191326141357,
      "learning_rate": 2.2471748878923766e-05,
      "loss": 1.1917,
      "step": 2900
    },
    {
      "epoch": 0.7760517367824522,
      "grad_norm": 2.682920217514038,
      "learning_rate": 2.2444843049327353e-05,
      "loss": 1.0966,
      "step": 2910
    },
    {
      "epoch": 0.7787185812387493,
      "grad_norm": 2.150782823562622,
      "learning_rate": 2.2417937219730943e-05,
      "loss": 1.0825,
      "step": 2920
    },
    {
      "epoch": 0.7813854256950463,
      "grad_norm": 3.1977906227111816,
      "learning_rate": 2.239103139013453e-05,
      "loss": 1.1848,
      "step": 2930
    },
    {
      "epoch": 0.7840522701513434,
      "grad_norm": 2.235153913497925,
      "learning_rate": 2.2364125560538117e-05,
      "loss": 1.0958,
      "step": 2940
    },
    {
      "epoch": 0.7867191146076405,
      "grad_norm": 2.2132363319396973,
      "learning_rate": 2.2337219730941704e-05,
      "loss": 1.2435,
      "step": 2950
    },
    {
      "epoch": 0.7893859590639376,
      "grad_norm": 2.569749593734741,
      "learning_rate": 2.231031390134529e-05,
      "loss": 1.0627,
      "step": 2960
    },
    {
      "epoch": 0.7920528035202347,
      "grad_norm": 3.201526165008545,
      "learning_rate": 2.228340807174888e-05,
      "loss": 1.0756,
      "step": 2970
    },
    {
      "epoch": 0.7947196479765317,
      "grad_norm": 2.4963481426239014,
      "learning_rate": 2.2256502242152467e-05,
      "loss": 1.2172,
      "step": 2980
    },
    {
      "epoch": 0.7973864924328289,
      "grad_norm": 2.3950722217559814,
      "learning_rate": 2.2229596412556054e-05,
      "loss": 1.132,
      "step": 2990
    },
    {
      "epoch": 0.8000533368891259,
      "grad_norm": 2.4691162109375,
      "learning_rate": 2.220269058295964e-05,
      "loss": 1.1064,
      "step": 3000
    },
    {
      "epoch": 0.802720181345423,
      "grad_norm": 1.8353968858718872,
      "learning_rate": 2.2175784753363228e-05,
      "loss": 1.182,
      "step": 3010
    },
    {
      "epoch": 0.8053870258017202,
      "grad_norm": 2.08754825592041,
      "learning_rate": 2.2148878923766818e-05,
      "loss": 1.2671,
      "step": 3020
    },
    {
      "epoch": 0.8080538702580172,
      "grad_norm": 2.150068998336792,
      "learning_rate": 2.2121973094170405e-05,
      "loss": 1.2829,
      "step": 3030
    },
    {
      "epoch": 0.8107207147143143,
      "grad_norm": 2.22200345993042,
      "learning_rate": 2.209506726457399e-05,
      "loss": 1.2167,
      "step": 3040
    },
    {
      "epoch": 0.8133875591706113,
      "grad_norm": 1.771251916885376,
      "learning_rate": 2.206816143497758e-05,
      "loss": 1.18,
      "step": 3050
    },
    {
      "epoch": 0.8160544036269085,
      "grad_norm": 3.7721688747406006,
      "learning_rate": 2.2041255605381165e-05,
      "loss": 1.1813,
      "step": 3060
    },
    {
      "epoch": 0.8187212480832056,
      "grad_norm": 2.0768182277679443,
      "learning_rate": 2.2014349775784755e-05,
      "loss": 1.3841,
      "step": 3070
    },
    {
      "epoch": 0.8213880925395026,
      "grad_norm": 3.515990734100342,
      "learning_rate": 2.1987443946188342e-05,
      "loss": 1.3338,
      "step": 3080
    },
    {
      "epoch": 0.8240549369957997,
      "grad_norm": 2.4968130588531494,
      "learning_rate": 2.196053811659193e-05,
      "loss": 1.1293,
      "step": 3090
    },
    {
      "epoch": 0.8267217814520968,
      "grad_norm": 2.3384125232696533,
      "learning_rate": 2.193363228699552e-05,
      "loss": 1.1439,
      "step": 3100
    },
    {
      "epoch": 0.8293886259083939,
      "grad_norm": 2.4026317596435547,
      "learning_rate": 2.1906726457399102e-05,
      "loss": 1.1717,
      "step": 3110
    },
    {
      "epoch": 0.832055470364691,
      "grad_norm": 1.8888660669326782,
      "learning_rate": 2.187982062780269e-05,
      "loss": 1.1352,
      "step": 3120
    },
    {
      "epoch": 0.834722314820988,
      "grad_norm": 2.6833932399749756,
      "learning_rate": 2.185291479820628e-05,
      "loss": 1.1434,
      "step": 3130
    },
    {
      "epoch": 0.8373891592772852,
      "grad_norm": 2.285897731781006,
      "learning_rate": 2.1826008968609866e-05,
      "loss": 1.1207,
      "step": 3140
    },
    {
      "epoch": 0.8400560037335822,
      "grad_norm": 1.9166324138641357,
      "learning_rate": 2.1799103139013456e-05,
      "loss": 1.0893,
      "step": 3150
    },
    {
      "epoch": 0.8427228481898793,
      "grad_norm": 2.451404571533203,
      "learning_rate": 2.1772197309417043e-05,
      "loss": 1.1247,
      "step": 3160
    },
    {
      "epoch": 0.8453896926461764,
      "grad_norm": 2.5908150672912598,
      "learning_rate": 2.1745291479820626e-05,
      "loss": 1.4156,
      "step": 3170
    },
    {
      "epoch": 0.8480565371024735,
      "grad_norm": 3.507526397705078,
      "learning_rate": 2.1718385650224216e-05,
      "loss": 1.1603,
      "step": 3180
    },
    {
      "epoch": 0.8507233815587706,
      "grad_norm": 3.873366355895996,
      "learning_rate": 2.1691479820627803e-05,
      "loss": 1.1306,
      "step": 3190
    },
    {
      "epoch": 0.8533902260150676,
      "grad_norm": 2.4321272373199463,
      "learning_rate": 2.166457399103139e-05,
      "loss": 1.3356,
      "step": 3200
    },
    {
      "epoch": 0.8560570704713648,
      "grad_norm": 2.771697521209717,
      "learning_rate": 2.164035874439462e-05,
      "loss": 1.2254,
      "step": 3210
    },
    {
      "epoch": 0.8587239149276619,
      "grad_norm": 2.103986978530884,
      "learning_rate": 2.1613452914798206e-05,
      "loss": 1.2211,
      "step": 3220
    },
    {
      "epoch": 0.8613907593839589,
      "grad_norm": 6.447595119476318,
      "learning_rate": 2.1586547085201797e-05,
      "loss": 1.2063,
      "step": 3230
    },
    {
      "epoch": 0.864057603840256,
      "grad_norm": 2.64623761177063,
      "learning_rate": 2.155964125560538e-05,
      "loss": 1.134,
      "step": 3240
    },
    {
      "epoch": 0.8667244482965532,
      "grad_norm": 1.9658862352371216,
      "learning_rate": 2.1532735426008967e-05,
      "loss": 1.2527,
      "step": 3250
    },
    {
      "epoch": 0.8693912927528502,
      "grad_norm": 2.211322546005249,
      "learning_rate": 2.1505829596412557e-05,
      "loss": 1.2588,
      "step": 3260
    },
    {
      "epoch": 0.8720581372091473,
      "grad_norm": 3.4889252185821533,
      "learning_rate": 2.1478923766816144e-05,
      "loss": 1.1983,
      "step": 3270
    },
    {
      "epoch": 0.8747249816654443,
      "grad_norm": 2.73410701751709,
      "learning_rate": 2.1452017937219734e-05,
      "loss": 1.359,
      "step": 3280
    },
    {
      "epoch": 0.8773918261217415,
      "grad_norm": 1.9970606565475464,
      "learning_rate": 2.142511210762332e-05,
      "loss": 1.118,
      "step": 3290
    },
    {
      "epoch": 0.8800586705780385,
      "grad_norm": 2.6107916831970215,
      "learning_rate": 2.1398206278026904e-05,
      "loss": 1.2607,
      "step": 3300
    },
    {
      "epoch": 0.8827255150343356,
      "grad_norm": 2.39473295211792,
      "learning_rate": 2.1371300448430494e-05,
      "loss": 1.2024,
      "step": 3310
    },
    {
      "epoch": 0.8853923594906327,
      "grad_norm": 2.2371063232421875,
      "learning_rate": 2.134439461883408e-05,
      "loss": 1.2423,
      "step": 3320
    },
    {
      "epoch": 0.8880592039469298,
      "grad_norm": 1.772980809211731,
      "learning_rate": 2.1317488789237668e-05,
      "loss": 1.2201,
      "step": 3330
    },
    {
      "epoch": 0.8907260484032269,
      "grad_norm": 2.564115524291992,
      "learning_rate": 2.1290582959641258e-05,
      "loss": 1.0819,
      "step": 3340
    },
    {
      "epoch": 0.893392892859524,
      "grad_norm": 2.67147159576416,
      "learning_rate": 2.1263677130044845e-05,
      "loss": 1.2753,
      "step": 3350
    },
    {
      "epoch": 0.896059737315821,
      "grad_norm": 2.153294801712036,
      "learning_rate": 2.123677130044843e-05,
      "loss": 1.3068,
      "step": 3360
    },
    {
      "epoch": 0.8987265817721182,
      "grad_norm": 2.231006622314453,
      "learning_rate": 2.1209865470852018e-05,
      "loss": 1.2076,
      "step": 3370
    },
    {
      "epoch": 0.9013934262284152,
      "grad_norm": 4.425205230712891,
      "learning_rate": 2.1182959641255605e-05,
      "loss": 1.1194,
      "step": 3380
    },
    {
      "epoch": 0.9040602706847123,
      "grad_norm": 1.9285346269607544,
      "learning_rate": 2.1156053811659195e-05,
      "loss": 1.1156,
      "step": 3390
    },
    {
      "epoch": 0.9067271151410095,
      "grad_norm": 2.7564713954925537,
      "learning_rate": 2.1129147982062782e-05,
      "loss": 1.0781,
      "step": 3400
    },
    {
      "epoch": 0.9093939595973065,
      "grad_norm": 3.9973011016845703,
      "learning_rate": 2.1102242152466365e-05,
      "loss": 1.1669,
      "step": 3410
    },
    {
      "epoch": 0.9120608040536036,
      "grad_norm": 2.6074509620666504,
      "learning_rate": 2.1075336322869955e-05,
      "loss": 1.2189,
      "step": 3420
    },
    {
      "epoch": 0.9147276485099006,
      "grad_norm": 2.3985774517059326,
      "learning_rate": 2.1048430493273542e-05,
      "loss": 1.1812,
      "step": 3430
    },
    {
      "epoch": 0.9173944929661978,
      "grad_norm": 2.466498851776123,
      "learning_rate": 2.1021524663677132e-05,
      "loss": 1.1516,
      "step": 3440
    },
    {
      "epoch": 0.9200613374224949,
      "grad_norm": 1.742305040359497,
      "learning_rate": 2.099461883408072e-05,
      "loss": 1.1417,
      "step": 3450
    },
    {
      "epoch": 0.9227281818787919,
      "grad_norm": 2.045832872390747,
      "learning_rate": 2.0967713004484306e-05,
      "loss": 1.2099,
      "step": 3460
    },
    {
      "epoch": 0.925395026335089,
      "grad_norm": 1.848462700843811,
      "learning_rate": 2.0940807174887892e-05,
      "loss": 1.2541,
      "step": 3470
    },
    {
      "epoch": 0.9280618707913861,
      "grad_norm": 1.7846660614013672,
      "learning_rate": 2.091390134529148e-05,
      "loss": 1.1908,
      "step": 3480
    },
    {
      "epoch": 0.9307287152476832,
      "grad_norm": 2.473750591278076,
      "learning_rate": 2.0886995515695066e-05,
      "loss": 1.1763,
      "step": 3490
    },
    {
      "epoch": 0.9333955597039802,
      "grad_norm": 2.4934513568878174,
      "learning_rate": 2.0860089686098656e-05,
      "loss": 1.1644,
      "step": 3500
    },
    {
      "epoch": 0.9360624041602773,
      "grad_norm": 2.3391685485839844,
      "learning_rate": 2.0833183856502243e-05,
      "loss": 1.2372,
      "step": 3510
    },
    {
      "epoch": 0.9387292486165745,
      "grad_norm": 2.3032991886138916,
      "learning_rate": 2.0806278026905833e-05,
      "loss": 1.1256,
      "step": 3520
    },
    {
      "epoch": 0.9413960930728715,
      "grad_norm": 3.305898427963257,
      "learning_rate": 2.0779372197309416e-05,
      "loss": 1.2354,
      "step": 3530
    },
    {
      "epoch": 0.9440629375291686,
      "grad_norm": 2.250060796737671,
      "learning_rate": 2.0752466367713003e-05,
      "loss": 1.2511,
      "step": 3540
    },
    {
      "epoch": 0.9467297819854656,
      "grad_norm": 2.945944309234619,
      "learning_rate": 2.0725560538116593e-05,
      "loss": 1.1581,
      "step": 3550
    },
    {
      "epoch": 0.9493966264417628,
      "grad_norm": 2.04836368560791,
      "learning_rate": 2.069865470852018e-05,
      "loss": 1.1368,
      "step": 3560
    },
    {
      "epoch": 0.9520634708980599,
      "grad_norm": 1.9141002893447876,
      "learning_rate": 2.067174887892377e-05,
      "loss": 1.2914,
      "step": 3570
    },
    {
      "epoch": 0.9547303153543569,
      "grad_norm": 1.8516285419464111,
      "learning_rate": 2.0644843049327354e-05,
      "loss": 1.1837,
      "step": 3580
    },
    {
      "epoch": 0.9573971598106541,
      "grad_norm": 2.950798988342285,
      "learning_rate": 2.061793721973094e-05,
      "loss": 1.2303,
      "step": 3590
    },
    {
      "epoch": 0.9600640042669512,
      "grad_norm": 2.0678789615631104,
      "learning_rate": 2.059103139013453e-05,
      "loss": 1.2342,
      "step": 3600
    },
    {
      "epoch": 0.9627308487232482,
      "grad_norm": 2.1614248752593994,
      "learning_rate": 2.0564125560538117e-05,
      "loss": 1.2385,
      "step": 3610
    },
    {
      "epoch": 0.9653976931795453,
      "grad_norm": 2.261038303375244,
      "learning_rate": 2.0537219730941704e-05,
      "loss": 1.1412,
      "step": 3620
    },
    {
      "epoch": 0.9680645376358424,
      "grad_norm": 3.588989019393921,
      "learning_rate": 2.0510313901345294e-05,
      "loss": 1.2087,
      "step": 3630
    },
    {
      "epoch": 0.9707313820921395,
      "grad_norm": 2.403611660003662,
      "learning_rate": 2.0483408071748877e-05,
      "loss": 1.1641,
      "step": 3640
    },
    {
      "epoch": 0.9733982265484366,
      "grad_norm": 2.149376392364502,
      "learning_rate": 2.0456502242152468e-05,
      "loss": 1.1621,
      "step": 3650
    },
    {
      "epoch": 0.9760650710047336,
      "grad_norm": 2.1121325492858887,
      "learning_rate": 2.0429596412556054e-05,
      "loss": 1.0567,
      "step": 3660
    },
    {
      "epoch": 0.9787319154610308,
      "grad_norm": 1.8842791318893433,
      "learning_rate": 2.040269058295964e-05,
      "loss": 1.3189,
      "step": 3670
    },
    {
      "epoch": 0.9813987599173278,
      "grad_norm": 2.5044076442718506,
      "learning_rate": 2.037578475336323e-05,
      "loss": 1.1641,
      "step": 3680
    },
    {
      "epoch": 0.9840656043736249,
      "grad_norm": 3.009636878967285,
      "learning_rate": 2.0348878923766818e-05,
      "loss": 1.0493,
      "step": 3690
    },
    {
      "epoch": 0.986732448829922,
      "grad_norm": 2.0071330070495605,
      "learning_rate": 2.03219730941704e-05,
      "loss": 1.0873,
      "step": 3700
    },
    {
      "epoch": 0.9893992932862191,
      "grad_norm": 2.699528694152832,
      "learning_rate": 2.029506726457399e-05,
      "loss": 1.1864,
      "step": 3710
    },
    {
      "epoch": 0.9920661377425162,
      "grad_norm": 2.3993093967437744,
      "learning_rate": 2.026816143497758e-05,
      "loss": 1.1381,
      "step": 3720
    },
    {
      "epoch": 0.9947329821988132,
      "grad_norm": 2.6187245845794678,
      "learning_rate": 2.024125560538117e-05,
      "loss": 1.2314,
      "step": 3730
    },
    {
      "epoch": 0.9973998266551103,
      "grad_norm": 1.6737470626831055,
      "learning_rate": 2.0214349775784755e-05,
      "loss": 1.2429,
      "step": 3740
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.987114906311035,
      "learning_rate": 2.018744394618834e-05,
      "loss": 1.0789,
      "step": 3750
    },
    {
      "epoch": 1.0026668444562972,
      "grad_norm": 2.481090784072876,
      "learning_rate": 2.016053811659193e-05,
      "loss": 1.3121,
      "step": 3760
    },
    {
      "epoch": 1.0053336889125941,
      "grad_norm": 2.1978139877319336,
      "learning_rate": 2.0133632286995516e-05,
      "loss": 1.272,
      "step": 3770
    },
    {
      "epoch": 1.0080005333688913,
      "grad_norm": 1.9391626119613647,
      "learning_rate": 2.0106726457399102e-05,
      "loss": 1.3637,
      "step": 3780
    },
    {
      "epoch": 1.0106673778251885,
      "grad_norm": 1.8704237937927246,
      "learning_rate": 2.0079820627802692e-05,
      "loss": 1.0709,
      "step": 3790
    },
    {
      "epoch": 1.0133342222814854,
      "grad_norm": 2.573998212814331,
      "learning_rate": 2.005291479820628e-05,
      "loss": 1.0555,
      "step": 3800
    },
    {
      "epoch": 1.0160010667377826,
      "grad_norm": 2.4954493045806885,
      "learning_rate": 2.0026008968609866e-05,
      "loss": 1.1321,
      "step": 3810
    },
    {
      "epoch": 1.0186679111940795,
      "grad_norm": 2.088256359100342,
      "learning_rate": 1.9999103139013453e-05,
      "loss": 1.0557,
      "step": 3820
    },
    {
      "epoch": 1.0213347556503767,
      "grad_norm": 2.8275115489959717,
      "learning_rate": 1.997219730941704e-05,
      "loss": 1.1925,
      "step": 3830
    },
    {
      "epoch": 1.0240016001066738,
      "grad_norm": 2.6180641651153564,
      "learning_rate": 1.994529147982063e-05,
      "loss": 1.2618,
      "step": 3840
    },
    {
      "epoch": 1.0266684445629708,
      "grad_norm": 2.70778489112854,
      "learning_rate": 1.9918385650224216e-05,
      "loss": 1.1087,
      "step": 3850
    },
    {
      "epoch": 1.029335289019268,
      "grad_norm": 2.2991480827331543,
      "learning_rate": 1.9891479820627803e-05,
      "loss": 1.0884,
      "step": 3860
    },
    {
      "epoch": 1.0320021334755651,
      "grad_norm": 2.1432321071624756,
      "learning_rate": 1.986457399103139e-05,
      "loss": 1.1358,
      "step": 3870
    },
    {
      "epoch": 1.034668977931862,
      "grad_norm": 2.3883848190307617,
      "learning_rate": 1.9837668161434977e-05,
      "loss": 1.2093,
      "step": 3880
    },
    {
      "epoch": 1.0373358223881592,
      "grad_norm": 2.701066017150879,
      "learning_rate": 1.9810762331838567e-05,
      "loss": 1.0358,
      "step": 3890
    },
    {
      "epoch": 1.0400026668444562,
      "grad_norm": 3.454329252243042,
      "learning_rate": 1.9783856502242154e-05,
      "loss": 1.3206,
      "step": 3900
    },
    {
      "epoch": 1.0426695113007534,
      "grad_norm": 2.174165964126587,
      "learning_rate": 1.975695067264574e-05,
      "loss": 1.16,
      "step": 3910
    },
    {
      "epoch": 1.0453363557570505,
      "grad_norm": 2.6699745655059814,
      "learning_rate": 1.9730044843049327e-05,
      "loss": 1.1629,
      "step": 3920
    },
    {
      "epoch": 1.0480032002133475,
      "grad_norm": 2.0937728881835938,
      "learning_rate": 1.9703139013452914e-05,
      "loss": 1.1427,
      "step": 3930
    },
    {
      "epoch": 1.0506700446696446,
      "grad_norm": 2.0537381172180176,
      "learning_rate": 1.9676233183856504e-05,
      "loss": 1.1345,
      "step": 3940
    },
    {
      "epoch": 1.0533368891259418,
      "grad_norm": 2.273721694946289,
      "learning_rate": 1.964932735426009e-05,
      "loss": 1.1559,
      "step": 3950
    },
    {
      "epoch": 1.0560037335822388,
      "grad_norm": 2.050394296646118,
      "learning_rate": 1.9622421524663678e-05,
      "loss": 1.1482,
      "step": 3960
    },
    {
      "epoch": 1.058670578038536,
      "grad_norm": 2.8369905948638916,
      "learning_rate": 1.9595515695067268e-05,
      "loss": 1.2111,
      "step": 3970
    },
    {
      "epoch": 1.061337422494833,
      "grad_norm": 3.0604805946350098,
      "learning_rate": 1.956860986547085e-05,
      "loss": 1.1339,
      "step": 3980
    },
    {
      "epoch": 1.06400426695113,
      "grad_norm": 2.4830455780029297,
      "learning_rate": 1.9541704035874438e-05,
      "loss": 1.3076,
      "step": 3990
    },
    {
      "epoch": 1.0666711114074272,
      "grad_norm": 2.3848178386688232,
      "learning_rate": 1.9514798206278028e-05,
      "loss": 1.283,
      "step": 4000
    },
    {
      "epoch": 1.0693379558637242,
      "grad_norm": 2.1078124046325684,
      "learning_rate": 1.9487892376681615e-05,
      "loss": 1.1746,
      "step": 4010
    },
    {
      "epoch": 1.0720048003200213,
      "grad_norm": 2.1745150089263916,
      "learning_rate": 1.9460986547085205e-05,
      "loss": 1.163,
      "step": 4020
    },
    {
      "epoch": 1.0746716447763185,
      "grad_norm": 2.4476401805877686,
      "learning_rate": 1.943408071748879e-05,
      "loss": 1.2485,
      "step": 4030
    },
    {
      "epoch": 1.0773384892326154,
      "grad_norm": 2.379032611846924,
      "learning_rate": 1.9407174887892375e-05,
      "loss": 1.216,
      "step": 4040
    },
    {
      "epoch": 1.0800053336889126,
      "grad_norm": 2.4948806762695312,
      "learning_rate": 1.9380269058295965e-05,
      "loss": 1.2317,
      "step": 4050
    },
    {
      "epoch": 1.0826721781452098,
      "grad_norm": 2.4276435375213623,
      "learning_rate": 1.9353363228699552e-05,
      "loss": 1.1762,
      "step": 4060
    },
    {
      "epoch": 1.0853390226015067,
      "grad_norm": 2.594517230987549,
      "learning_rate": 1.932645739910314e-05,
      "loss": 1.1433,
      "step": 4070
    },
    {
      "epoch": 1.0880058670578039,
      "grad_norm": 2.2830727100372314,
      "learning_rate": 1.929955156950673e-05,
      "loss": 1.1467,
      "step": 4080
    },
    {
      "epoch": 1.090672711514101,
      "grad_norm": 2.6040914058685303,
      "learning_rate": 1.9272645739910312e-05,
      "loss": 1.2063,
      "step": 4090
    },
    {
      "epoch": 1.093339555970398,
      "grad_norm": 3.1025116443634033,
      "learning_rate": 1.9245739910313902e-05,
      "loss": 1.1727,
      "step": 4100
    },
    {
      "epoch": 1.0960064004266952,
      "grad_norm": 1.8737696409225464,
      "learning_rate": 1.921883408071749e-05,
      "loss": 1.0838,
      "step": 4110
    },
    {
      "epoch": 1.0986732448829921,
      "grad_norm": 1.9415315389633179,
      "learning_rate": 1.9191928251121076e-05,
      "loss": 1.2442,
      "step": 4120
    },
    {
      "epoch": 1.1013400893392893,
      "grad_norm": 1.911879301071167,
      "learning_rate": 1.9165022421524666e-05,
      "loss": 1.1178,
      "step": 4130
    },
    {
      "epoch": 1.1040069337955865,
      "grad_norm": 1.9975768327713013,
      "learning_rate": 1.9138116591928253e-05,
      "loss": 1.1855,
      "step": 4140
    },
    {
      "epoch": 1.1066737782518834,
      "grad_norm": 2.4057717323303223,
      "learning_rate": 1.9111210762331836e-05,
      "loss": 1.1178,
      "step": 4150
    },
    {
      "epoch": 1.1093406227081806,
      "grad_norm": 2.7404119968414307,
      "learning_rate": 1.9084304932735426e-05,
      "loss": 1.0684,
      "step": 4160
    },
    {
      "epoch": 1.1120074671644775,
      "grad_norm": 2.1881070137023926,
      "learning_rate": 1.9057399103139013e-05,
      "loss": 1.1376,
      "step": 4170
    },
    {
      "epoch": 1.1146743116207747,
      "grad_norm": 2.2557482719421387,
      "learning_rate": 1.9030493273542603e-05,
      "loss": 1.1582,
      "step": 4180
    },
    {
      "epoch": 1.1173411560770719,
      "grad_norm": 2.4599993228912354,
      "learning_rate": 1.900358744394619e-05,
      "loss": 1.0681,
      "step": 4190
    },
    {
      "epoch": 1.1200080005333688,
      "grad_norm": 2.091517925262451,
      "learning_rate": 1.8976681614349777e-05,
      "loss": 1.3336,
      "step": 4200
    },
    {
      "epoch": 1.122674844989666,
      "grad_norm": 1.8491113185882568,
      "learning_rate": 1.8949775784753363e-05,
      "loss": 1.3268,
      "step": 4210
    },
    {
      "epoch": 1.1253416894459631,
      "grad_norm": 2.6124935150146484,
      "learning_rate": 1.892286995515695e-05,
      "loss": 1.3105,
      "step": 4220
    },
    {
      "epoch": 1.12800853390226,
      "grad_norm": 2.4829163551330566,
      "learning_rate": 1.8895964125560537e-05,
      "loss": 1.1575,
      "step": 4230
    },
    {
      "epoch": 1.1306753783585572,
      "grad_norm": 3.720559597015381,
      "learning_rate": 1.8869058295964127e-05,
      "loss": 1.1929,
      "step": 4240
    },
    {
      "epoch": 1.1333422228148544,
      "grad_norm": 2.7702577114105225,
      "learning_rate": 1.8842152466367714e-05,
      "loss": 1.3226,
      "step": 4250
    },
    {
      "epoch": 1.1360090672711514,
      "grad_norm": 2.276538372039795,
      "learning_rate": 1.88152466367713e-05,
      "loss": 1.1737,
      "step": 4260
    },
    {
      "epoch": 1.1386759117274485,
      "grad_norm": 1.919238805770874,
      "learning_rate": 1.8788340807174887e-05,
      "loss": 1.1637,
      "step": 4270
    },
    {
      "epoch": 1.1413427561837457,
      "grad_norm": 2.2709460258483887,
      "learning_rate": 1.8761434977578474e-05,
      "loss": 1.1929,
      "step": 4280
    },
    {
      "epoch": 1.1440096006400426,
      "grad_norm": 2.4236199855804443,
      "learning_rate": 1.8734529147982064e-05,
      "loss": 1.2604,
      "step": 4290
    },
    {
      "epoch": 1.1466764450963398,
      "grad_norm": 2.370499610900879,
      "learning_rate": 1.870762331838565e-05,
      "loss": 1.1549,
      "step": 4300
    },
    {
      "epoch": 1.1493432895526368,
      "grad_norm": 2.7951772212982178,
      "learning_rate": 1.868071748878924e-05,
      "loss": 1.0817,
      "step": 4310
    },
    {
      "epoch": 1.152010134008934,
      "grad_norm": 2.348418951034546,
      "learning_rate": 1.8653811659192825e-05,
      "loss": 1.098,
      "step": 4320
    },
    {
      "epoch": 1.154676978465231,
      "grad_norm": 2.0700597763061523,
      "learning_rate": 1.862690582959641e-05,
      "loss": 1.1362,
      "step": 4330
    },
    {
      "epoch": 1.157343822921528,
      "grad_norm": 2.465452194213867,
      "learning_rate": 1.86e-05,
      "loss": 1.1305,
      "step": 4340
    },
    {
      "epoch": 1.1600106673778252,
      "grad_norm": 2.9038169384002686,
      "learning_rate": 1.8573094170403588e-05,
      "loss": 1.3822,
      "step": 4350
    },
    {
      "epoch": 1.1626775118341222,
      "grad_norm": 2.626603841781616,
      "learning_rate": 1.8546188340807175e-05,
      "loss": 1.2462,
      "step": 4360
    },
    {
      "epoch": 1.1653443562904193,
      "grad_norm": 2.097757339477539,
      "learning_rate": 1.8519282511210765e-05,
      "loss": 1.1452,
      "step": 4370
    },
    {
      "epoch": 1.1680112007467165,
      "grad_norm": 2.1979453563690186,
      "learning_rate": 1.849237668161435e-05,
      "loss": 1.1026,
      "step": 4380
    },
    {
      "epoch": 1.1706780452030134,
      "grad_norm": 3.035209894180298,
      "learning_rate": 1.846547085201794e-05,
      "loss": 1.0816,
      "step": 4390
    },
    {
      "epoch": 1.1733448896593106,
      "grad_norm": 1.8711541891098022,
      "learning_rate": 1.8438565022421525e-05,
      "loss": 1.4504,
      "step": 4400
    },
    {
      "epoch": 1.1760117341156078,
      "grad_norm": 3.7237675189971924,
      "learning_rate": 1.8411659192825112e-05,
      "loss": 1.1567,
      "step": 4410
    },
    {
      "epoch": 1.1786785785719047,
      "grad_norm": 2.2570459842681885,
      "learning_rate": 1.8384753363228702e-05,
      "loss": 1.1485,
      "step": 4420
    },
    {
      "epoch": 1.181345423028202,
      "grad_norm": 2.167645215988159,
      "learning_rate": 1.8357847533632286e-05,
      "loss": 1.1296,
      "step": 4430
    },
    {
      "epoch": 1.184012267484499,
      "grad_norm": 3.023794174194336,
      "learning_rate": 1.8330941704035872e-05,
      "loss": 1.0817,
      "step": 4440
    },
    {
      "epoch": 1.186679111940796,
      "grad_norm": 2.3610148429870605,
      "learning_rate": 1.8304035874439463e-05,
      "loss": 1.083,
      "step": 4450
    },
    {
      "epoch": 1.1893459563970932,
      "grad_norm": 1.9984394311904907,
      "learning_rate": 1.827713004484305e-05,
      "loss": 1.1559,
      "step": 4460
    },
    {
      "epoch": 1.1920128008533903,
      "grad_norm": 1.8333077430725098,
      "learning_rate": 1.825022421524664e-05,
      "loss": 1.2321,
      "step": 4470
    },
    {
      "epoch": 1.1946796453096873,
      "grad_norm": 2.392350912094116,
      "learning_rate": 1.8223318385650226e-05,
      "loss": 1.1769,
      "step": 4480
    },
    {
      "epoch": 1.1973464897659845,
      "grad_norm": 3.282356023788452,
      "learning_rate": 1.819641255605381e-05,
      "loss": 1.1471,
      "step": 4490
    },
    {
      "epoch": 1.2000133342222814,
      "grad_norm": 2.7321832180023193,
      "learning_rate": 1.81695067264574e-05,
      "loss": 1.1606,
      "step": 4500
    },
    {
      "epoch": 1.2026801786785786,
      "grad_norm": 2.3241310119628906,
      "learning_rate": 1.8142600896860987e-05,
      "loss": 1.2519,
      "step": 4510
    },
    {
      "epoch": 1.2053470231348757,
      "grad_norm": 2.9281105995178223,
      "learning_rate": 1.8115695067264573e-05,
      "loss": 1.0552,
      "step": 4520
    },
    {
      "epoch": 1.2080138675911727,
      "grad_norm": 2.3664398193359375,
      "learning_rate": 1.8088789237668163e-05,
      "loss": 1.3421,
      "step": 4530
    },
    {
      "epoch": 1.2106807120474699,
      "grad_norm": 2.286379098892212,
      "learning_rate": 1.806188340807175e-05,
      "loss": 1.0361,
      "step": 4540
    },
    {
      "epoch": 1.2133475565037668,
      "grad_norm": 3.9921016693115234,
      "learning_rate": 1.8034977578475337e-05,
      "loss": 1.1657,
      "step": 4550
    },
    {
      "epoch": 1.216014400960064,
      "grad_norm": 3.668769121170044,
      "learning_rate": 1.8008071748878924e-05,
      "loss": 1.1737,
      "step": 4560
    },
    {
      "epoch": 1.2186812454163611,
      "grad_norm": 1.9232850074768066,
      "learning_rate": 1.798116591928251e-05,
      "loss": 1.2765,
      "step": 4570
    },
    {
      "epoch": 1.221348089872658,
      "grad_norm": 2.0264973640441895,
      "learning_rate": 1.79542600896861e-05,
      "loss": 1.1743,
      "step": 4580
    },
    {
      "epoch": 1.2240149343289553,
      "grad_norm": 2.318103075027466,
      "learning_rate": 1.7927354260089687e-05,
      "loss": 1.1465,
      "step": 4590
    },
    {
      "epoch": 1.2266817787852524,
      "grad_norm": 3.007091999053955,
      "learning_rate": 1.7900448430493274e-05,
      "loss": 1.2484,
      "step": 4600
    },
    {
      "epoch": 1.2293486232415494,
      "grad_norm": 2.3560025691986084,
      "learning_rate": 1.787354260089686e-05,
      "loss": 1.1166,
      "step": 4610
    },
    {
      "epoch": 1.2320154676978465,
      "grad_norm": 2.402570962905884,
      "learning_rate": 1.7846636771300448e-05,
      "loss": 1.1451,
      "step": 4620
    },
    {
      "epoch": 1.2346823121541437,
      "grad_norm": 4.209014415740967,
      "learning_rate": 1.7819730941704038e-05,
      "loss": 1.1592,
      "step": 4630
    },
    {
      "epoch": 1.2373491566104406,
      "grad_norm": 1.9661760330200195,
      "learning_rate": 1.7792825112107625e-05,
      "loss": 1.1813,
      "step": 4640
    },
    {
      "epoch": 1.2400160010667378,
      "grad_norm": 2.026577949523926,
      "learning_rate": 1.776591928251121e-05,
      "loss": 1.3046,
      "step": 4650
    },
    {
      "epoch": 1.242682845523035,
      "grad_norm": 1.8965933322906494,
      "learning_rate": 1.7739013452914798e-05,
      "loss": 1.0418,
      "step": 4660
    },
    {
      "epoch": 1.245349689979332,
      "grad_norm": 2.381683111190796,
      "learning_rate": 1.7712107623318385e-05,
      "loss": 1.0983,
      "step": 4670
    },
    {
      "epoch": 1.248016534435629,
      "grad_norm": 2.3452906608581543,
      "learning_rate": 1.7685201793721975e-05,
      "loss": 1.2159,
      "step": 4680
    },
    {
      "epoch": 1.250683378891926,
      "grad_norm": 2.9796907901763916,
      "learning_rate": 1.7658295964125562e-05,
      "loss": 1.1959,
      "step": 4690
    },
    {
      "epoch": 1.2533502233482232,
      "grad_norm": 1.8476462364196777,
      "learning_rate": 1.763139013452915e-05,
      "loss": 1.18,
      "step": 4700
    },
    {
      "epoch": 1.2560170678045204,
      "grad_norm": 2.892239570617676,
      "learning_rate": 1.760448430493274e-05,
      "loss": 1.1585,
      "step": 4710
    },
    {
      "epoch": 1.2586839122608173,
      "grad_norm": 2.1354570388793945,
      "learning_rate": 1.7577578475336322e-05,
      "loss": 1.2916,
      "step": 4720
    },
    {
      "epoch": 1.2613507567171145,
      "grad_norm": 1.7925766706466675,
      "learning_rate": 1.755067264573991e-05,
      "loss": 1.1667,
      "step": 4730
    },
    {
      "epoch": 1.2640176011734114,
      "grad_norm": 1.9148218631744385,
      "learning_rate": 1.75237668161435e-05,
      "loss": 1.1928,
      "step": 4740
    },
    {
      "epoch": 1.2666844456297086,
      "grad_norm": 3.2481794357299805,
      "learning_rate": 1.7496860986547086e-05,
      "loss": 1.0682,
      "step": 4750
    },
    {
      "epoch": 1.2693512900860058,
      "grad_norm": 2.5535054206848145,
      "learning_rate": 1.7469955156950676e-05,
      "loss": 1.1058,
      "step": 4760
    },
    {
      "epoch": 1.2720181345423027,
      "grad_norm": 2.781358480453491,
      "learning_rate": 1.744304932735426e-05,
      "loss": 1.256,
      "step": 4770
    },
    {
      "epoch": 1.2746849789986,
      "grad_norm": 1.8176400661468506,
      "learning_rate": 1.7416143497757846e-05,
      "loss": 1.1548,
      "step": 4780
    },
    {
      "epoch": 1.277351823454897,
      "grad_norm": 2.4071216583251953,
      "learning_rate": 1.7389237668161436e-05,
      "loss": 1.1495,
      "step": 4790
    },
    {
      "epoch": 1.280018667911194,
      "grad_norm": 2.6304144859313965,
      "learning_rate": 1.7362331838565023e-05,
      "loss": 1.1087,
      "step": 4800
    },
    {
      "epoch": 1.2826855123674912,
      "grad_norm": 2.4105000495910645,
      "learning_rate": 1.733542600896861e-05,
      "loss": 1.2582,
      "step": 4810
    },
    {
      "epoch": 1.2853523568237883,
      "grad_norm": 2.7436795234680176,
      "learning_rate": 1.73085201793722e-05,
      "loss": 1.2502,
      "step": 4820
    },
    {
      "epoch": 1.2880192012800853,
      "grad_norm": 3.317911386489868,
      "learning_rate": 1.7281614349775783e-05,
      "loss": 1.158,
      "step": 4830
    },
    {
      "epoch": 1.2906860457363825,
      "grad_norm": 1.9725394248962402,
      "learning_rate": 1.7254708520179373e-05,
      "loss": 1.1275,
      "step": 4840
    },
    {
      "epoch": 1.2933528901926796,
      "grad_norm": 2.599125385284424,
      "learning_rate": 1.722780269058296e-05,
      "loss": 1.1159,
      "step": 4850
    },
    {
      "epoch": 1.2960197346489766,
      "grad_norm": 2.1548855304718018,
      "learning_rate": 1.7200896860986547e-05,
      "loss": 1.043,
      "step": 4860
    },
    {
      "epoch": 1.2986865791052737,
      "grad_norm": 2.9532954692840576,
      "learning_rate": 1.7173991031390137e-05,
      "loss": 1.2517,
      "step": 4870
    },
    {
      "epoch": 1.301353423561571,
      "grad_norm": 2.8674442768096924,
      "learning_rate": 1.714708520179372e-05,
      "loss": 1.1605,
      "step": 4880
    },
    {
      "epoch": 1.3040202680178679,
      "grad_norm": 2.4866433143615723,
      "learning_rate": 1.7120179372197307e-05,
      "loss": 1.2411,
      "step": 4890
    },
    {
      "epoch": 1.306687112474165,
      "grad_norm": 2.3182225227355957,
      "learning_rate": 1.7093273542600897e-05,
      "loss": 1.1303,
      "step": 4900
    },
    {
      "epoch": 1.309353956930462,
      "grad_norm": 3.051619291305542,
      "learning_rate": 1.7066367713004484e-05,
      "loss": 1.0309,
      "step": 4910
    },
    {
      "epoch": 1.3120208013867591,
      "grad_norm": 1.712225317955017,
      "learning_rate": 1.7039461883408074e-05,
      "loss": 1.2521,
      "step": 4920
    },
    {
      "epoch": 1.314687645843056,
      "grad_norm": 2.0570051670074463,
      "learning_rate": 1.701255605381166e-05,
      "loss": 1.0976,
      "step": 4930
    },
    {
      "epoch": 1.3173544902993533,
      "grad_norm": 2.867283582687378,
      "learning_rate": 1.6985650224215244e-05,
      "loss": 1.117,
      "step": 4940
    },
    {
      "epoch": 1.3200213347556504,
      "grad_norm": 3.2405142784118652,
      "learning_rate": 1.6958744394618834e-05,
      "loss": 1.2645,
      "step": 4950
    },
    {
      "epoch": 1.3226881792119474,
      "grad_norm": 2.2970638275146484,
      "learning_rate": 1.693183856502242e-05,
      "loss": 1.1207,
      "step": 4960
    },
    {
      "epoch": 1.3253550236682445,
      "grad_norm": 2.160860776901245,
      "learning_rate": 1.690493273542601e-05,
      "loss": 1.3142,
      "step": 4970
    },
    {
      "epoch": 1.3280218681245417,
      "grad_norm": 2.624922752380371,
      "learning_rate": 1.6878026905829598e-05,
      "loss": 1.2868,
      "step": 4980
    },
    {
      "epoch": 1.3306887125808387,
      "grad_norm": 2.397683620452881,
      "learning_rate": 1.6851121076233185e-05,
      "loss": 1.1531,
      "step": 4990
    },
    {
      "epoch": 1.3333555570371358,
      "grad_norm": 2.9251649379730225,
      "learning_rate": 1.682421524663677e-05,
      "loss": 1.2464,
      "step": 5000
    },
    {
      "epoch": 1.336022401493433,
      "grad_norm": 2.851853132247925,
      "learning_rate": 1.679730941704036e-05,
      "loss": 1.2065,
      "step": 5010
    },
    {
      "epoch": 1.33868924594973,
      "grad_norm": 2.6597630977630615,
      "learning_rate": 1.6770403587443945e-05,
      "loss": 1.3203,
      "step": 5020
    },
    {
      "epoch": 1.341356090406027,
      "grad_norm": 2.214336395263672,
      "learning_rate": 1.6743497757847535e-05,
      "loss": 1.1634,
      "step": 5030
    },
    {
      "epoch": 1.3440229348623243,
      "grad_norm": 2.160017728805542,
      "learning_rate": 1.6716591928251122e-05,
      "loss": 1.2187,
      "step": 5040
    },
    {
      "epoch": 1.3466897793186212,
      "grad_norm": 1.9574607610702515,
      "learning_rate": 1.668968609865471e-05,
      "loss": 1.0627,
      "step": 5050
    },
    {
      "epoch": 1.3493566237749184,
      "grad_norm": 1.9633253812789917,
      "learning_rate": 1.6662780269058296e-05,
      "loss": 1.302,
      "step": 5060
    },
    {
      "epoch": 1.3520234682312156,
      "grad_norm": 2.082130193710327,
      "learning_rate": 1.6635874439461882e-05,
      "loss": 1.1406,
      "step": 5070
    },
    {
      "epoch": 1.3546903126875125,
      "grad_norm": 2.415416955947876,
      "learning_rate": 1.6608968609865473e-05,
      "loss": 1.0457,
      "step": 5080
    },
    {
      "epoch": 1.3573571571438097,
      "grad_norm": 2.4432616233825684,
      "learning_rate": 1.658206278026906e-05,
      "loss": 1.0674,
      "step": 5090
    },
    {
      "epoch": 1.3600240016001066,
      "grad_norm": 2.1200318336486816,
      "learning_rate": 1.6555156950672646e-05,
      "loss": 1.0715,
      "step": 5100
    },
    {
      "epoch": 1.3626908460564038,
      "grad_norm": 2.476288080215454,
      "learning_rate": 1.6528251121076233e-05,
      "loss": 1.2172,
      "step": 5110
    },
    {
      "epoch": 1.3653576905127007,
      "grad_norm": 1.8441523313522339,
      "learning_rate": 1.650134529147982e-05,
      "loss": 1.1498,
      "step": 5120
    },
    {
      "epoch": 1.368024534968998,
      "grad_norm": 2.2884621620178223,
      "learning_rate": 1.647443946188341e-05,
      "loss": 1.0821,
      "step": 5130
    },
    {
      "epoch": 1.370691379425295,
      "grad_norm": 2.274109125137329,
      "learning_rate": 1.6447533632286996e-05,
      "loss": 1.2143,
      "step": 5140
    },
    {
      "epoch": 1.373358223881592,
      "grad_norm": 3.6325764656066895,
      "learning_rate": 1.6420627802690583e-05,
      "loss": 1.0643,
      "step": 5150
    },
    {
      "epoch": 1.3760250683378892,
      "grad_norm": 2.324908494949341,
      "learning_rate": 1.6393721973094173e-05,
      "loss": 1.2448,
      "step": 5160
    },
    {
      "epoch": 1.3786919127941863,
      "grad_norm": 1.8889375925064087,
      "learning_rate": 1.6366816143497757e-05,
      "loss": 1.1983,
      "step": 5170
    },
    {
      "epoch": 1.3813587572504833,
      "grad_norm": 3.075096607208252,
      "learning_rate": 1.6339910313901344e-05,
      "loss": 1.1948,
      "step": 5180
    },
    {
      "epoch": 1.3840256017067805,
      "grad_norm": 2.2498390674591064,
      "learning_rate": 1.6313004484304934e-05,
      "loss": 1.1795,
      "step": 5190
    },
    {
      "epoch": 1.3866924461630776,
      "grad_norm": 2.5819852352142334,
      "learning_rate": 1.628609865470852e-05,
      "loss": 1.1019,
      "step": 5200
    },
    {
      "epoch": 1.3893592906193746,
      "grad_norm": 3.3408515453338623,
      "learning_rate": 1.625919282511211e-05,
      "loss": 1.2244,
      "step": 5210
    },
    {
      "epoch": 1.3920261350756717,
      "grad_norm": 2.2469043731689453,
      "learning_rate": 1.6232286995515694e-05,
      "loss": 1.1898,
      "step": 5220
    },
    {
      "epoch": 1.394692979531969,
      "grad_norm": 2.041891098022461,
      "learning_rate": 1.620538116591928e-05,
      "loss": 1.2396,
      "step": 5230
    },
    {
      "epoch": 1.3973598239882659,
      "grad_norm": 1.9165276288986206,
      "learning_rate": 1.617847533632287e-05,
      "loss": 1.0609,
      "step": 5240
    },
    {
      "epoch": 1.400026668444563,
      "grad_norm": 1.8958922624588013,
      "learning_rate": 1.6151569506726458e-05,
      "loss": 1.2189,
      "step": 5250
    },
    {
      "epoch": 1.4026935129008602,
      "grad_norm": 2.1332907676696777,
      "learning_rate": 1.6124663677130044e-05,
      "loss": 1.1312,
      "step": 5260
    },
    {
      "epoch": 1.4053603573571571,
      "grad_norm": 2.510542392730713,
      "learning_rate": 1.6097757847533635e-05,
      "loss": 1.0531,
      "step": 5270
    },
    {
      "epoch": 1.4080272018134543,
      "grad_norm": 2.763942241668701,
      "learning_rate": 1.6070852017937218e-05,
      "loss": 1.158,
      "step": 5280
    },
    {
      "epoch": 1.4106940462697513,
      "grad_norm": 2.964454412460327,
      "learning_rate": 1.6043946188340808e-05,
      "loss": 1.1485,
      "step": 5290
    },
    {
      "epoch": 1.4133608907260484,
      "grad_norm": 2.1800498962402344,
      "learning_rate": 1.6017040358744395e-05,
      "loss": 1.129,
      "step": 5300
    },
    {
      "epoch": 1.4160277351823454,
      "grad_norm": 2.3233580589294434,
      "learning_rate": 1.599013452914798e-05,
      "loss": 1.1058,
      "step": 5310
    },
    {
      "epoch": 1.4186945796386425,
      "grad_norm": 2.127157688140869,
      "learning_rate": 1.596322869955157e-05,
      "loss": 1.213,
      "step": 5320
    },
    {
      "epoch": 1.4213614240949397,
      "grad_norm": 2.05080246925354,
      "learning_rate": 1.593632286995516e-05,
      "loss": 1.1192,
      "step": 5330
    },
    {
      "epoch": 1.4240282685512367,
      "grad_norm": 2.6067066192626953,
      "learning_rate": 1.5909417040358745e-05,
      "loss": 1.1705,
      "step": 5340
    },
    {
      "epoch": 1.4266951130075338,
      "grad_norm": 2.8422415256500244,
      "learning_rate": 1.5882511210762332e-05,
      "loss": 1.0944,
      "step": 5350
    },
    {
      "epoch": 1.429361957463831,
      "grad_norm": 1.8185869455337524,
      "learning_rate": 1.585560538116592e-05,
      "loss": 1.1318,
      "step": 5360
    },
    {
      "epoch": 1.432028801920128,
      "grad_norm": 2.3036515712738037,
      "learning_rate": 1.582869955156951e-05,
      "loss": 1.1647,
      "step": 5370
    },
    {
      "epoch": 1.434695646376425,
      "grad_norm": 1.8093318939208984,
      "learning_rate": 1.5801793721973096e-05,
      "loss": 1.0515,
      "step": 5380
    },
    {
      "epoch": 1.4373624908327223,
      "grad_norm": 4.665255069732666,
      "learning_rate": 1.577488789237668e-05,
      "loss": 1.1972,
      "step": 5390
    },
    {
      "epoch": 1.4400293352890192,
      "grad_norm": 2.2865583896636963,
      "learning_rate": 1.574798206278027e-05,
      "loss": 1.1858,
      "step": 5400
    },
    {
      "epoch": 1.4426961797453164,
      "grad_norm": 2.6306910514831543,
      "learning_rate": 1.5721076233183856e-05,
      "loss": 1.1285,
      "step": 5410
    },
    {
      "epoch": 1.4453630242016136,
      "grad_norm": 2.0709314346313477,
      "learning_rate": 1.5694170403587446e-05,
      "loss": 1.2947,
      "step": 5420
    },
    {
      "epoch": 1.4480298686579105,
      "grad_norm": 2.32745361328125,
      "learning_rate": 1.5667264573991033e-05,
      "loss": 1.1756,
      "step": 5430
    },
    {
      "epoch": 1.4506967131142077,
      "grad_norm": 3.0696969032287598,
      "learning_rate": 1.564035874439462e-05,
      "loss": 1.1345,
      "step": 5440
    },
    {
      "epoch": 1.4533635575705048,
      "grad_norm": 2.728586435317993,
      "learning_rate": 1.5613452914798206e-05,
      "loss": 1.1626,
      "step": 5450
    },
    {
      "epoch": 1.4560304020268018,
      "grad_norm": 2.1566920280456543,
      "learning_rate": 1.5586547085201793e-05,
      "loss": 1.1967,
      "step": 5460
    },
    {
      "epoch": 1.458697246483099,
      "grad_norm": 1.941405177116394,
      "learning_rate": 1.555964125560538e-05,
      "loss": 1.1383,
      "step": 5470
    },
    {
      "epoch": 1.461364090939396,
      "grad_norm": 2.7545859813690186,
      "learning_rate": 1.553273542600897e-05,
      "loss": 1.1098,
      "step": 5480
    },
    {
      "epoch": 1.464030935395693,
      "grad_norm": 2.239535093307495,
      "learning_rate": 1.5505829596412557e-05,
      "loss": 1.2695,
      "step": 5490
    },
    {
      "epoch": 1.46669777985199,
      "grad_norm": 3.7362735271453857,
      "learning_rate": 1.5478923766816147e-05,
      "loss": 1.1012,
      "step": 5500
    },
    {
      "epoch": 1.4693646243082872,
      "grad_norm": 2.574136734008789,
      "learning_rate": 1.545201793721973e-05,
      "loss": 1.2422,
      "step": 5510
    },
    {
      "epoch": 1.4720314687645843,
      "grad_norm": 3.4516358375549316,
      "learning_rate": 1.5425112107623317e-05,
      "loss": 1.0961,
      "step": 5520
    },
    {
      "epoch": 1.4746983132208813,
      "grad_norm": 2.476346254348755,
      "learning_rate": 1.5398206278026907e-05,
      "loss": 1.2082,
      "step": 5530
    },
    {
      "epoch": 1.4773651576771785,
      "grad_norm": 4.141952991485596,
      "learning_rate": 1.5371300448430494e-05,
      "loss": 1.2647,
      "step": 5540
    },
    {
      "epoch": 1.4800320021334756,
      "grad_norm": 2.8018765449523926,
      "learning_rate": 1.534439461883408e-05,
      "loss": 1.2663,
      "step": 5550
    },
    {
      "epoch": 1.4826988465897726,
      "grad_norm": 2.418438196182251,
      "learning_rate": 1.5317488789237667e-05,
      "loss": 1.2241,
      "step": 5560
    },
    {
      "epoch": 1.4853656910460697,
      "grad_norm": 2.143496513366699,
      "learning_rate": 1.5290582959641254e-05,
      "loss": 1.2724,
      "step": 5570
    },
    {
      "epoch": 1.488032535502367,
      "grad_norm": 3.0153651237487793,
      "learning_rate": 1.5263677130044844e-05,
      "loss": 1.1481,
      "step": 5580
    },
    {
      "epoch": 1.4906993799586639,
      "grad_norm": 2.1837146282196045,
      "learning_rate": 1.5236771300448431e-05,
      "loss": 1.0999,
      "step": 5590
    },
    {
      "epoch": 1.493366224414961,
      "grad_norm": 2.0661509037017822,
      "learning_rate": 1.520986547085202e-05,
      "loss": 1.205,
      "step": 5600
    },
    {
      "epoch": 1.4960330688712582,
      "grad_norm": 2.452425718307495,
      "learning_rate": 1.5182959641255606e-05,
      "loss": 1.0129,
      "step": 5610
    },
    {
      "epoch": 1.4986999133275551,
      "grad_norm": 3.2264716625213623,
      "learning_rate": 1.5156053811659191e-05,
      "loss": 1.3161,
      "step": 5620
    },
    {
      "epoch": 1.501366757783852,
      "grad_norm": 3.2912325859069824,
      "learning_rate": 1.512914798206278e-05,
      "loss": 1.2652,
      "step": 5630
    },
    {
      "epoch": 1.5040336022401495,
      "grad_norm": 1.728502631187439,
      "learning_rate": 1.5102242152466368e-05,
      "loss": 1.171,
      "step": 5640
    },
    {
      "epoch": 1.5067004466964464,
      "grad_norm": 5.156145095825195,
      "learning_rate": 1.5075336322869955e-05,
      "loss": 1.1117,
      "step": 5650
    },
    {
      "epoch": 1.5093672911527434,
      "grad_norm": 2.436605453491211,
      "learning_rate": 1.5048430493273544e-05,
      "loss": 0.9827,
      "step": 5660
    },
    {
      "epoch": 1.5120341356090408,
      "grad_norm": 3.2194013595581055,
      "learning_rate": 1.5021524663677132e-05,
      "loss": 1.2478,
      "step": 5670
    },
    {
      "epoch": 1.5147009800653377,
      "grad_norm": 2.0658719539642334,
      "learning_rate": 1.4994618834080719e-05,
      "loss": 1.1647,
      "step": 5680
    },
    {
      "epoch": 1.5173678245216347,
      "grad_norm": 2.671656847000122,
      "learning_rate": 1.4967713004484306e-05,
      "loss": 1.0898,
      "step": 5690
    },
    {
      "epoch": 1.5200346689779318,
      "grad_norm": 2.4567580223083496,
      "learning_rate": 1.4940807174887892e-05,
      "loss": 1.1383,
      "step": 5700
    },
    {
      "epoch": 1.522701513434229,
      "grad_norm": 3.268338680267334,
      "learning_rate": 1.491390134529148e-05,
      "loss": 1.1387,
      "step": 5710
    },
    {
      "epoch": 1.525368357890526,
      "grad_norm": 2.047494888305664,
      "learning_rate": 1.4886995515695067e-05,
      "loss": 1.3186,
      "step": 5720
    },
    {
      "epoch": 1.528035202346823,
      "grad_norm": 1.8019185066223145,
      "learning_rate": 1.4860089686098656e-05,
      "loss": 1.1753,
      "step": 5730
    },
    {
      "epoch": 1.5307020468031203,
      "grad_norm": 2.4992752075195312,
      "learning_rate": 1.4833183856502243e-05,
      "loss": 1.1013,
      "step": 5740
    },
    {
      "epoch": 1.5333688912594172,
      "grad_norm": 3.2141458988189697,
      "learning_rate": 1.480627802690583e-05,
      "loss": 1.2279,
      "step": 5750
    },
    {
      "epoch": 1.5360357357157144,
      "grad_norm": 2.979062557220459,
      "learning_rate": 1.4779372197309418e-05,
      "loss": 1.1379,
      "step": 5760
    },
    {
      "epoch": 1.5387025801720116,
      "grad_norm": 2.2841649055480957,
      "learning_rate": 1.4752466367713006e-05,
      "loss": 1.1406,
      "step": 5770
    },
    {
      "epoch": 1.5413694246283085,
      "grad_norm": 1.9276937246322632,
      "learning_rate": 1.4725560538116591e-05,
      "loss": 1.2065,
      "step": 5780
    },
    {
      "epoch": 1.5440362690846057,
      "grad_norm": 2.240072727203369,
      "learning_rate": 1.469865470852018e-05,
      "loss": 1.1956,
      "step": 5790
    },
    {
      "epoch": 1.5467031135409028,
      "grad_norm": 3.137502908706665,
      "learning_rate": 1.4671748878923767e-05,
      "loss": 1.153,
      "step": 5800
    },
    {
      "epoch": 1.5493699579971998,
      "grad_norm": 1.7466305494308472,
      "learning_rate": 1.4644843049327355e-05,
      "loss": 1.0722,
      "step": 5810
    },
    {
      "epoch": 1.5520368024534967,
      "grad_norm": 1.9225404262542725,
      "learning_rate": 1.4617937219730942e-05,
      "loss": 1.0966,
      "step": 5820
    },
    {
      "epoch": 1.5547036469097941,
      "grad_norm": 2.281147003173828,
      "learning_rate": 1.4591031390134529e-05,
      "loss": 1.2676,
      "step": 5830
    },
    {
      "epoch": 1.557370491366091,
      "grad_norm": 2.6972827911376953,
      "learning_rate": 1.4564125560538117e-05,
      "loss": 1.1095,
      "step": 5840
    },
    {
      "epoch": 1.560037335822388,
      "grad_norm": 2.1563074588775635,
      "learning_rate": 1.4537219730941706e-05,
      "loss": 1.1991,
      "step": 5850
    },
    {
      "epoch": 1.5627041802786854,
      "grad_norm": 2.3886215686798096,
      "learning_rate": 1.451031390134529e-05,
      "loss": 1.1021,
      "step": 5860
    },
    {
      "epoch": 1.5653710247349824,
      "grad_norm": 2.428812265396118,
      "learning_rate": 1.4483408071748879e-05,
      "loss": 1.2178,
      "step": 5870
    },
    {
      "epoch": 1.5680378691912793,
      "grad_norm": 2.813936710357666,
      "learning_rate": 1.4456502242152467e-05,
      "loss": 1.2208,
      "step": 5880
    },
    {
      "epoch": 1.5707047136475765,
      "grad_norm": 2.813610076904297,
      "learning_rate": 1.4429596412556054e-05,
      "loss": 1.0346,
      "step": 5890
    },
    {
      "epoch": 1.5733715581038736,
      "grad_norm": 2.1266520023345947,
      "learning_rate": 1.4402690582959641e-05,
      "loss": 1.2941,
      "step": 5900
    },
    {
      "epoch": 1.5760384025601706,
      "grad_norm": 2.292771100997925,
      "learning_rate": 1.437578475336323e-05,
      "loss": 1.281,
      "step": 5910
    },
    {
      "epoch": 1.5787052470164677,
      "grad_norm": 1.9412497282028198,
      "learning_rate": 1.4348878923766816e-05,
      "loss": 1.1277,
      "step": 5920
    },
    {
      "epoch": 1.581372091472765,
      "grad_norm": 2.4238247871398926,
      "learning_rate": 1.4321973094170405e-05,
      "loss": 1.1129,
      "step": 5930
    },
    {
      "epoch": 1.5840389359290619,
      "grad_norm": 1.8521199226379395,
      "learning_rate": 1.4295067264573991e-05,
      "loss": 1.0797,
      "step": 5940
    },
    {
      "epoch": 1.586705780385359,
      "grad_norm": 2.484172821044922,
      "learning_rate": 1.4268161434977578e-05,
      "loss": 1.1221,
      "step": 5950
    },
    {
      "epoch": 1.5893726248416562,
      "grad_norm": 2.531179666519165,
      "learning_rate": 1.4241255605381167e-05,
      "loss": 1.2103,
      "step": 5960
    },
    {
      "epoch": 1.5920394692979531,
      "grad_norm": 2.5952088832855225,
      "learning_rate": 1.4214349775784753e-05,
      "loss": 1.0974,
      "step": 5970
    },
    {
      "epoch": 1.5947063137542503,
      "grad_norm": 2.615478515625,
      "learning_rate": 1.418744394618834e-05,
      "loss": 1.113,
      "step": 5980
    },
    {
      "epoch": 1.5973731582105475,
      "grad_norm": 2.026104211807251,
      "learning_rate": 1.4160538116591929e-05,
      "loss": 1.1945,
      "step": 5990
    },
    {
      "epoch": 1.6000400026668444,
      "grad_norm": 2.630321502685547,
      "learning_rate": 1.4133632286995515e-05,
      "loss": 1.1699,
      "step": 6000
    },
    {
      "epoch": 1.6027068471231416,
      "grad_norm": 1.9031314849853516,
      "learning_rate": 1.4106726457399104e-05,
      "loss": 1.1682,
      "step": 6010
    },
    {
      "epoch": 1.6053736915794388,
      "grad_norm": 1.6618742942810059,
      "learning_rate": 1.407982062780269e-05,
      "loss": 1.0269,
      "step": 6020
    },
    {
      "epoch": 1.6080405360357357,
      "grad_norm": 2.0288705825805664,
      "learning_rate": 1.4052914798206277e-05,
      "loss": 1.1091,
      "step": 6030
    },
    {
      "epoch": 1.6107073804920327,
      "grad_norm": 2.2729361057281494,
      "learning_rate": 1.4026008968609866e-05,
      "loss": 1.1608,
      "step": 6040
    },
    {
      "epoch": 1.61337422494833,
      "grad_norm": 1.9429773092269897,
      "learning_rate": 1.3999103139013454e-05,
      "loss": 1.1662,
      "step": 6050
    },
    {
      "epoch": 1.616041069404627,
      "grad_norm": 2.9855904579162598,
      "learning_rate": 1.3972197309417041e-05,
      "loss": 1.0922,
      "step": 6060
    },
    {
      "epoch": 1.618707913860924,
      "grad_norm": 2.2699406147003174,
      "learning_rate": 1.3945291479820628e-05,
      "loss": 1.1609,
      "step": 6070
    },
    {
      "epoch": 1.621374758317221,
      "grad_norm": 2.9295899868011475,
      "learning_rate": 1.3918385650224216e-05,
      "loss": 1.1218,
      "step": 6080
    },
    {
      "epoch": 1.6240416027735183,
      "grad_norm": 2.4945335388183594,
      "learning_rate": 1.3891479820627803e-05,
      "loss": 1.144,
      "step": 6090
    },
    {
      "epoch": 1.6267084472298152,
      "grad_norm": 3.178637981414795,
      "learning_rate": 1.3864573991031391e-05,
      "loss": 1.2446,
      "step": 6100
    },
    {
      "epoch": 1.6293752916861124,
      "grad_norm": 2.0824549198150635,
      "learning_rate": 1.3837668161434978e-05,
      "loss": 1.286,
      "step": 6110
    },
    {
      "epoch": 1.6320421361424096,
      "grad_norm": 2.421947479248047,
      "learning_rate": 1.3810762331838565e-05,
      "loss": 1.1407,
      "step": 6120
    },
    {
      "epoch": 1.6347089805987065,
      "grad_norm": 3.6295065879821777,
      "learning_rate": 1.3783856502242153e-05,
      "loss": 1.1473,
      "step": 6130
    },
    {
      "epoch": 1.6373758250550037,
      "grad_norm": 2.190671920776367,
      "learning_rate": 1.375695067264574e-05,
      "loss": 1.2738,
      "step": 6140
    },
    {
      "epoch": 1.6400426695113008,
      "grad_norm": 3.1959543228149414,
      "learning_rate": 1.3730044843049327e-05,
      "loss": 1.1561,
      "step": 6150
    },
    {
      "epoch": 1.6427095139675978,
      "grad_norm": 1.7806189060211182,
      "learning_rate": 1.3703139013452915e-05,
      "loss": 1.248,
      "step": 6160
    },
    {
      "epoch": 1.645376358423895,
      "grad_norm": 3.3646769523620605,
      "learning_rate": 1.3676233183856502e-05,
      "loss": 1.1512,
      "step": 6170
    },
    {
      "epoch": 1.6480432028801921,
      "grad_norm": 2.2545952796936035,
      "learning_rate": 1.364932735426009e-05,
      "loss": 1.045,
      "step": 6180
    },
    {
      "epoch": 1.650710047336489,
      "grad_norm": 3.2891266345977783,
      "learning_rate": 1.3622421524663677e-05,
      "loss": 1.1733,
      "step": 6190
    },
    {
      "epoch": 1.6533768917927862,
      "grad_norm": 2.662370204925537,
      "learning_rate": 1.3595515695067264e-05,
      "loss": 1.2827,
      "step": 6200
    },
    {
      "epoch": 1.6560437362490834,
      "grad_norm": 2.7713584899902344,
      "learning_rate": 1.3568609865470853e-05,
      "loss": 1.1879,
      "step": 6210
    },
    {
      "epoch": 1.6587105807053804,
      "grad_norm": 1.894743800163269,
      "learning_rate": 1.3541704035874441e-05,
      "loss": 1.1109,
      "step": 6220
    },
    {
      "epoch": 1.6613774251616773,
      "grad_norm": 2.940246343612671,
      "learning_rate": 1.3514798206278026e-05,
      "loss": 1.1994,
      "step": 6230
    },
    {
      "epoch": 1.6640442696179747,
      "grad_norm": 2.221737861633301,
      "learning_rate": 1.3487892376681615e-05,
      "loss": 1.0984,
      "step": 6240
    },
    {
      "epoch": 1.6667111140742716,
      "grad_norm": 2.537710428237915,
      "learning_rate": 1.3460986547085203e-05,
      "loss": 1.1366,
      "step": 6250
    },
    {
      "epoch": 1.6693779585305686,
      "grad_norm": 2.1644229888916016,
      "learning_rate": 1.343408071748879e-05,
      "loss": 1.2407,
      "step": 6260
    },
    {
      "epoch": 1.6720448029868658,
      "grad_norm": 1.883833885192871,
      "learning_rate": 1.3407174887892377e-05,
      "loss": 1.317,
      "step": 6270
    },
    {
      "epoch": 1.674711647443163,
      "grad_norm": 2.350292682647705,
      "learning_rate": 1.3380269058295965e-05,
      "loss": 1.2148,
      "step": 6280
    },
    {
      "epoch": 1.6773784918994599,
      "grad_norm": 3.2703380584716797,
      "learning_rate": 1.3353363228699552e-05,
      "loss": 1.1633,
      "step": 6290
    },
    {
      "epoch": 1.680045336355757,
      "grad_norm": 2.7042572498321533,
      "learning_rate": 1.332645739910314e-05,
      "loss": 1.2686,
      "step": 6300
    },
    {
      "epoch": 1.6827121808120542,
      "grad_norm": 3.451007843017578,
      "learning_rate": 1.3299551569506725e-05,
      "loss": 1.379,
      "step": 6310
    },
    {
      "epoch": 1.6853790252683511,
      "grad_norm": 1.999739408493042,
      "learning_rate": 1.3272645739910314e-05,
      "loss": 1.3675,
      "step": 6320
    },
    {
      "epoch": 1.6880458697246483,
      "grad_norm": 3.0882928371429443,
      "learning_rate": 1.3245739910313902e-05,
      "loss": 1.0675,
      "step": 6330
    },
    {
      "epoch": 1.6907127141809455,
      "grad_norm": 2.578094482421875,
      "learning_rate": 1.3218834080717489e-05,
      "loss": 1.2054,
      "step": 6340
    },
    {
      "epoch": 1.6933795586372424,
      "grad_norm": 1.8365637063980103,
      "learning_rate": 1.3191928251121076e-05,
      "loss": 1.0925,
      "step": 6350
    },
    {
      "epoch": 1.6960464030935396,
      "grad_norm": 2.8833391666412354,
      "learning_rate": 1.3165022421524664e-05,
      "loss": 1.1624,
      "step": 6360
    },
    {
      "epoch": 1.6987132475498368,
      "grad_norm": 1.9945354461669922,
      "learning_rate": 1.3138116591928251e-05,
      "loss": 1.2255,
      "step": 6370
    },
    {
      "epoch": 1.7013800920061337,
      "grad_norm": 3.039734363555908,
      "learning_rate": 1.311121076233184e-05,
      "loss": 1.2551,
      "step": 6380
    },
    {
      "epoch": 1.7040469364624309,
      "grad_norm": 1.6366437673568726,
      "learning_rate": 1.3084304932735426e-05,
      "loss": 1.1156,
      "step": 6390
    },
    {
      "epoch": 1.706713780918728,
      "grad_norm": 1.9919792413711548,
      "learning_rate": 1.3057399103139013e-05,
      "loss": 1.2113,
      "step": 6400
    },
    {
      "epoch": 1.709380625375025,
      "grad_norm": 2.4734976291656494,
      "learning_rate": 1.3030493273542601e-05,
      "loss": 1.2947,
      "step": 6410
    },
    {
      "epoch": 1.712047469831322,
      "grad_norm": 3.138292074203491,
      "learning_rate": 1.300358744394619e-05,
      "loss": 1.1089,
      "step": 6420
    },
    {
      "epoch": 1.7147143142876193,
      "grad_norm": 2.1562798023223877,
      "learning_rate": 1.2976681614349777e-05,
      "loss": 1.1353,
      "step": 6430
    },
    {
      "epoch": 1.7173811587439163,
      "grad_norm": 2.3215997219085693,
      "learning_rate": 1.2949775784753363e-05,
      "loss": 1.1958,
      "step": 6440
    },
    {
      "epoch": 1.7200480032002132,
      "grad_norm": 2.0536720752716064,
      "learning_rate": 1.2922869955156952e-05,
      "loss": 1.2599,
      "step": 6450
    },
    {
      "epoch": 1.7227148476565104,
      "grad_norm": 2.107321262359619,
      "learning_rate": 1.2895964125560539e-05,
      "loss": 1.1942,
      "step": 6460
    },
    {
      "epoch": 1.7253816921128076,
      "grad_norm": 2.2173125743865967,
      "learning_rate": 1.2869058295964127e-05,
      "loss": 1.2352,
      "step": 6470
    },
    {
      "epoch": 1.7280485365691045,
      "grad_norm": 3.411552667617798,
      "learning_rate": 1.2842152466367712e-05,
      "loss": 1.1307,
      "step": 6480
    },
    {
      "epoch": 1.7307153810254017,
      "grad_norm": 2.610323905944824,
      "learning_rate": 1.28152466367713e-05,
      "loss": 1.1663,
      "step": 6490
    },
    {
      "epoch": 1.7333822254816988,
      "grad_norm": 2.6383354663848877,
      "learning_rate": 1.2788340807174889e-05,
      "loss": 1.2507,
      "step": 6500
    },
    {
      "epoch": 1.7360490699379958,
      "grad_norm": 2.767843008041382,
      "learning_rate": 1.2761434977578476e-05,
      "loss": 1.2059,
      "step": 6510
    },
    {
      "epoch": 1.738715914394293,
      "grad_norm": 2.2487382888793945,
      "learning_rate": 1.2734529147982062e-05,
      "loss": 1.158,
      "step": 6520
    },
    {
      "epoch": 1.7413827588505901,
      "grad_norm": 2.519467830657959,
      "learning_rate": 1.2707623318385651e-05,
      "loss": 1.1219,
      "step": 6530
    },
    {
      "epoch": 1.744049603306887,
      "grad_norm": 2.164271354675293,
      "learning_rate": 1.2680717488789238e-05,
      "loss": 1.2534,
      "step": 6540
    },
    {
      "epoch": 1.7467164477631842,
      "grad_norm": 3.025130271911621,
      "learning_rate": 1.2653811659192826e-05,
      "loss": 1.1787,
      "step": 6550
    },
    {
      "epoch": 1.7493832922194814,
      "grad_norm": 2.591233730316162,
      "learning_rate": 1.2626905829596413e-05,
      "loss": 1.2279,
      "step": 6560
    },
    {
      "epoch": 1.7520501366757784,
      "grad_norm": 1.936845064163208,
      "learning_rate": 1.26e-05,
      "loss": 1.1343,
      "step": 6570
    },
    {
      "epoch": 1.7547169811320755,
      "grad_norm": 2.3540842533111572,
      "learning_rate": 1.2573094170403588e-05,
      "loss": 1.0638,
      "step": 6580
    },
    {
      "epoch": 1.7573838255883727,
      "grad_norm": 2.2675209045410156,
      "learning_rate": 1.2546188340807177e-05,
      "loss": 1.3056,
      "step": 6590
    },
    {
      "epoch": 1.7600506700446696,
      "grad_norm": 2.35062313079834,
      "learning_rate": 1.2519282511210762e-05,
      "loss": 1.1693,
      "step": 6600
    },
    {
      "epoch": 1.7627175145009666,
      "grad_norm": 1.7555137872695923,
      "learning_rate": 1.249237668161435e-05,
      "loss": 1.1381,
      "step": 6610
    },
    {
      "epoch": 1.765384358957264,
      "grad_norm": 2.0645508766174316,
      "learning_rate": 1.2465470852017939e-05,
      "loss": 1.2541,
      "step": 6620
    },
    {
      "epoch": 1.768051203413561,
      "grad_norm": 3.289048671722412,
      "learning_rate": 1.2438565022421525e-05,
      "loss": 1.1076,
      "step": 6630
    },
    {
      "epoch": 1.7707180478698579,
      "grad_norm": 2.4154961109161377,
      "learning_rate": 1.2411659192825112e-05,
      "loss": 1.2093,
      "step": 6640
    },
    {
      "epoch": 1.773384892326155,
      "grad_norm": 1.9730027914047241,
      "learning_rate": 1.2384753363228699e-05,
      "loss": 1.106,
      "step": 6650
    },
    {
      "epoch": 1.7760517367824522,
      "grad_norm": 2.2451305389404297,
      "learning_rate": 1.2357847533632287e-05,
      "loss": 1.1092,
      "step": 6660
    },
    {
      "epoch": 1.7787185812387492,
      "grad_norm": 2.067678451538086,
      "learning_rate": 1.2330941704035876e-05,
      "loss": 1.2318,
      "step": 6670
    },
    {
      "epoch": 1.7813854256950463,
      "grad_norm": 2.321044445037842,
      "learning_rate": 1.230403587443946e-05,
      "loss": 1.1945,
      "step": 6680
    },
    {
      "epoch": 1.7840522701513435,
      "grad_norm": 2.3797590732574463,
      "learning_rate": 1.227713004484305e-05,
      "loss": 1.1623,
      "step": 6690
    },
    {
      "epoch": 1.7867191146076404,
      "grad_norm": 2.770280361175537,
      "learning_rate": 1.2250224215246638e-05,
      "loss": 1.1159,
      "step": 6700
    },
    {
      "epoch": 1.7893859590639376,
      "grad_norm": 2.6767306327819824,
      "learning_rate": 1.2223318385650224e-05,
      "loss": 1.2056,
      "step": 6710
    },
    {
      "epoch": 1.7920528035202348,
      "grad_norm": 2.83278751373291,
      "learning_rate": 1.2196412556053811e-05,
      "loss": 1.1944,
      "step": 6720
    },
    {
      "epoch": 1.7947196479765317,
      "grad_norm": 2.9178059101104736,
      "learning_rate": 1.21695067264574e-05,
      "loss": 1.2324,
      "step": 6730
    },
    {
      "epoch": 1.7973864924328289,
      "grad_norm": 2.850659132003784,
      "learning_rate": 1.2142600896860986e-05,
      "loss": 1.1383,
      "step": 6740
    },
    {
      "epoch": 1.800053336889126,
      "grad_norm": 2.8181493282318115,
      "learning_rate": 1.2115695067264575e-05,
      "loss": 1.0914,
      "step": 6750
    },
    {
      "epoch": 1.802720181345423,
      "grad_norm": 2.3237102031707764,
      "learning_rate": 1.2088789237668162e-05,
      "loss": 1.2775,
      "step": 6760
    },
    {
      "epoch": 1.8053870258017202,
      "grad_norm": 2.423560619354248,
      "learning_rate": 1.2061883408071748e-05,
      "loss": 1.1252,
      "step": 6770
    },
    {
      "epoch": 1.8080538702580173,
      "grad_norm": 3.191197395324707,
      "learning_rate": 1.2034977578475337e-05,
      "loss": 1.2303,
      "step": 6780
    },
    {
      "epoch": 1.8107207147143143,
      "grad_norm": 2.0233941078186035,
      "learning_rate": 1.2008071748878925e-05,
      "loss": 1.0354,
      "step": 6790
    },
    {
      "epoch": 1.8133875591706112,
      "grad_norm": 2.6622846126556396,
      "learning_rate": 1.1981165919282512e-05,
      "loss": 1.1763,
      "step": 6800
    },
    {
      "epoch": 1.8160544036269086,
      "grad_norm": 3.1484713554382324,
      "learning_rate": 1.1954260089686099e-05,
      "loss": 1.178,
      "step": 6810
    },
    {
      "epoch": 1.8187212480832056,
      "grad_norm": 1.6869791746139526,
      "learning_rate": 1.1927354260089686e-05,
      "loss": 1.114,
      "step": 6820
    },
    {
      "epoch": 1.8213880925395025,
      "grad_norm": 1.997488260269165,
      "learning_rate": 1.1900448430493274e-05,
      "loss": 1.1155,
      "step": 6830
    },
    {
      "epoch": 1.8240549369957997,
      "grad_norm": 2.2586073875427246,
      "learning_rate": 1.1873542600896862e-05,
      "loss": 1.0113,
      "step": 6840
    },
    {
      "epoch": 1.8267217814520968,
      "grad_norm": 2.615051507949829,
      "learning_rate": 1.1846636771300448e-05,
      "loss": 1.0823,
      "step": 6850
    },
    {
      "epoch": 1.8293886259083938,
      "grad_norm": 2.134033441543579,
      "learning_rate": 1.1819730941704036e-05,
      "loss": 0.982,
      "step": 6860
    },
    {
      "epoch": 1.832055470364691,
      "grad_norm": 2.6569759845733643,
      "learning_rate": 1.1792825112107624e-05,
      "loss": 1.02,
      "step": 6870
    },
    {
      "epoch": 1.8347223148209881,
      "grad_norm": 2.5059654712677,
      "learning_rate": 1.1765919282511211e-05,
      "loss": 1.1685,
      "step": 6880
    },
    {
      "epoch": 1.837389159277285,
      "grad_norm": 2.6173665523529053,
      "learning_rate": 1.1739013452914798e-05,
      "loss": 1.1853,
      "step": 6890
    },
    {
      "epoch": 1.8400560037335822,
      "grad_norm": 2.64762020111084,
      "learning_rate": 1.1712107623318386e-05,
      "loss": 1.2254,
      "step": 6900
    },
    {
      "epoch": 1.8427228481898794,
      "grad_norm": 2.4232282638549805,
      "learning_rate": 1.1685201793721973e-05,
      "loss": 1.2885,
      "step": 6910
    },
    {
      "epoch": 1.8453896926461764,
      "grad_norm": 2.5930063724517822,
      "learning_rate": 1.1658295964125562e-05,
      "loss": 1.1052,
      "step": 6920
    },
    {
      "epoch": 1.8480565371024735,
      "grad_norm": 3.2010836601257324,
      "learning_rate": 1.1631390134529148e-05,
      "loss": 1.2827,
      "step": 6930
    },
    {
      "epoch": 1.8507233815587707,
      "grad_norm": 2.87314772605896,
      "learning_rate": 1.1604484304932735e-05,
      "loss": 1.2189,
      "step": 6940
    },
    {
      "epoch": 1.8533902260150676,
      "grad_norm": 2.3537702560424805,
      "learning_rate": 1.1577578475336324e-05,
      "loss": 1.0602,
      "step": 6950
    },
    {
      "epoch": 1.8560570704713648,
      "grad_norm": 3.371316432952881,
      "learning_rate": 1.155067264573991e-05,
      "loss": 1.1121,
      "step": 6960
    },
    {
      "epoch": 1.858723914927662,
      "grad_norm": 1.8325773477554321,
      "learning_rate": 1.1523766816143497e-05,
      "loss": 1.226,
      "step": 6970
    },
    {
      "epoch": 1.861390759383959,
      "grad_norm": 1.6058294773101807,
      "learning_rate": 1.1496860986547086e-05,
      "loss": 1.151,
      "step": 6980
    },
    {
      "epoch": 1.8640576038402559,
      "grad_norm": 3.3894364833831787,
      "learning_rate": 1.1469955156950672e-05,
      "loss": 1.03,
      "step": 6990
    },
    {
      "epoch": 1.8667244482965533,
      "grad_norm": 2.2641775608062744,
      "learning_rate": 1.144304932735426e-05,
      "loss": 1.1685,
      "step": 7000
    },
    {
      "epoch": 1.8693912927528502,
      "grad_norm": 3.9794254302978516,
      "learning_rate": 1.1416143497757848e-05,
      "loss": 1.1294,
      "step": 7010
    },
    {
      "epoch": 1.8720581372091472,
      "grad_norm": 2.9422249794006348,
      "learning_rate": 1.1389237668161434e-05,
      "loss": 1.0984,
      "step": 7020
    },
    {
      "epoch": 1.8747249816654443,
      "grad_norm": 2.660824775695801,
      "learning_rate": 1.1362331838565023e-05,
      "loss": 1.2121,
      "step": 7030
    },
    {
      "epoch": 1.8773918261217415,
      "grad_norm": 2.287705898284912,
      "learning_rate": 1.1335426008968611e-05,
      "loss": 1.1526,
      "step": 7040
    },
    {
      "epoch": 1.8800586705780384,
      "grad_norm": 2.5168027877807617,
      "learning_rate": 1.1308520179372196e-05,
      "loss": 1.0846,
      "step": 7050
    },
    {
      "epoch": 1.8827255150343356,
      "grad_norm": 2.2929840087890625,
      "learning_rate": 1.1281614349775785e-05,
      "loss": 1.0741,
      "step": 7060
    },
    {
      "epoch": 1.8853923594906328,
      "grad_norm": 2.925785541534424,
      "learning_rate": 1.1254708520179373e-05,
      "loss": 1.0171,
      "step": 7070
    },
    {
      "epoch": 1.8880592039469297,
      "grad_norm": 2.6447837352752686,
      "learning_rate": 1.122780269058296e-05,
      "loss": 1.2314,
      "step": 7080
    },
    {
      "epoch": 1.8907260484032269,
      "grad_norm": 2.219818353652954,
      "learning_rate": 1.1200896860986547e-05,
      "loss": 1.1789,
      "step": 7090
    },
    {
      "epoch": 1.893392892859524,
      "grad_norm": 2.87274169921875,
      "learning_rate": 1.1173991031390135e-05,
      "loss": 1.0993,
      "step": 7100
    },
    {
      "epoch": 1.896059737315821,
      "grad_norm": 2.902486801147461,
      "learning_rate": 1.1147085201793722e-05,
      "loss": 1.1008,
      "step": 7110
    },
    {
      "epoch": 1.8987265817721182,
      "grad_norm": 2.0759193897247314,
      "learning_rate": 1.112017937219731e-05,
      "loss": 1.0407,
      "step": 7120
    },
    {
      "epoch": 1.9013934262284153,
      "grad_norm": 1.876494288444519,
      "learning_rate": 1.1093273542600897e-05,
      "loss": 1.2124,
      "step": 7130
    },
    {
      "epoch": 1.9040602706847123,
      "grad_norm": 1.6741939783096313,
      "learning_rate": 1.1066367713004484e-05,
      "loss": 0.9866,
      "step": 7140
    },
    {
      "epoch": 1.9067271151410095,
      "grad_norm": 2.244515895843506,
      "learning_rate": 1.1039461883408072e-05,
      "loss": 1.0576,
      "step": 7150
    },
    {
      "epoch": 1.9093939595973066,
      "grad_norm": 2.3204283714294434,
      "learning_rate": 1.1012556053811659e-05,
      "loss": 1.1093,
      "step": 7160
    },
    {
      "epoch": 1.9120608040536036,
      "grad_norm": 2.752711534500122,
      "learning_rate": 1.0985650224215248e-05,
      "loss": 1.1207,
      "step": 7170
    },
    {
      "epoch": 1.9147276485099005,
      "grad_norm": 3.735346555709839,
      "learning_rate": 1.0958744394618834e-05,
      "loss": 1.1144,
      "step": 7180
    },
    {
      "epoch": 1.917394492966198,
      "grad_norm": 2.3041555881500244,
      "learning_rate": 1.0931838565022421e-05,
      "loss": 1.2123,
      "step": 7190
    },
    {
      "epoch": 1.9200613374224949,
      "grad_norm": 2.5920653343200684,
      "learning_rate": 1.090493273542601e-05,
      "loss": 1.1447,
      "step": 7200
    },
    {
      "epoch": 1.9227281818787918,
      "grad_norm": 1.6400330066680908,
      "learning_rate": 1.0878026905829598e-05,
      "loss": 1.2106,
      "step": 7210
    },
    {
      "epoch": 1.925395026335089,
      "grad_norm": 4.050090312957764,
      "learning_rate": 1.0851121076233183e-05,
      "loss": 1.1565,
      "step": 7220
    },
    {
      "epoch": 1.9280618707913861,
      "grad_norm": 1.7844666242599487,
      "learning_rate": 1.0824215246636772e-05,
      "loss": 1.2002,
      "step": 7230
    },
    {
      "epoch": 1.930728715247683,
      "grad_norm": 2.857792615890503,
      "learning_rate": 1.079730941704036e-05,
      "loss": 1.1111,
      "step": 7240
    },
    {
      "epoch": 1.9333955597039802,
      "grad_norm": 2.1742465496063232,
      "learning_rate": 1.0770403587443947e-05,
      "loss": 1.207,
      "step": 7250
    },
    {
      "epoch": 1.9360624041602774,
      "grad_norm": 2.5287282466888428,
      "learning_rate": 1.0743497757847533e-05,
      "loss": 1.1068,
      "step": 7260
    },
    {
      "epoch": 1.9387292486165744,
      "grad_norm": 1.876388430595398,
      "learning_rate": 1.0716591928251122e-05,
      "loss": 1.2405,
      "step": 7270
    },
    {
      "epoch": 1.9413960930728715,
      "grad_norm": 1.6285456418991089,
      "learning_rate": 1.0689686098654709e-05,
      "loss": 1.1872,
      "step": 7280
    },
    {
      "epoch": 1.9440629375291687,
      "grad_norm": 3.1044259071350098,
      "learning_rate": 1.0662780269058297e-05,
      "loss": 1.2071,
      "step": 7290
    },
    {
      "epoch": 1.9467297819854656,
      "grad_norm": 2.4047787189483643,
      "learning_rate": 1.0635874439461882e-05,
      "loss": 1.2112,
      "step": 7300
    },
    {
      "epoch": 1.9493966264417628,
      "grad_norm": 2.3706109523773193,
      "learning_rate": 1.060896860986547e-05,
      "loss": 1.0798,
      "step": 7310
    },
    {
      "epoch": 1.95206347089806,
      "grad_norm": 1.9373400211334229,
      "learning_rate": 1.0582062780269059e-05,
      "loss": 1.0954,
      "step": 7320
    },
    {
      "epoch": 1.954730315354357,
      "grad_norm": 2.049234390258789,
      "learning_rate": 1.0555156950672646e-05,
      "loss": 1.1461,
      "step": 7330
    },
    {
      "epoch": 1.957397159810654,
      "grad_norm": 2.3846514225006104,
      "learning_rate": 1.0528251121076233e-05,
      "loss": 1.1104,
      "step": 7340
    },
    {
      "epoch": 1.9600640042669513,
      "grad_norm": 1.9330732822418213,
      "learning_rate": 1.0501345291479821e-05,
      "loss": 1.1389,
      "step": 7350
    },
    {
      "epoch": 1.9627308487232482,
      "grad_norm": 2.6054892539978027,
      "learning_rate": 1.0474439461883408e-05,
      "loss": 1.0698,
      "step": 7360
    },
    {
      "epoch": 1.9653976931795452,
      "grad_norm": 2.475370407104492,
      "learning_rate": 1.0447533632286996e-05,
      "loss": 1.1579,
      "step": 7370
    },
    {
      "epoch": 1.9680645376358425,
      "grad_norm": 2.353550910949707,
      "learning_rate": 1.0420627802690583e-05,
      "loss": 1.1788,
      "step": 7380
    },
    {
      "epoch": 1.9707313820921395,
      "grad_norm": 2.0077028274536133,
      "learning_rate": 1.039372197309417e-05,
      "loss": 1.2447,
      "step": 7390
    },
    {
      "epoch": 1.9733982265484364,
      "grad_norm": 3.001216411590576,
      "learning_rate": 1.0366816143497758e-05,
      "loss": 1.2087,
      "step": 7400
    },
    {
      "epoch": 1.9760650710047336,
      "grad_norm": 2.4420034885406494,
      "learning_rate": 1.0339910313901347e-05,
      "loss": 1.1598,
      "step": 7410
    },
    {
      "epoch": 1.9787319154610308,
      "grad_norm": 2.5477163791656494,
      "learning_rate": 1.0313004484304932e-05,
      "loss": 1.2162,
      "step": 7420
    },
    {
      "epoch": 1.9813987599173277,
      "grad_norm": 2.194554328918457,
      "learning_rate": 1.028609865470852e-05,
      "loss": 1.4089,
      "step": 7430
    },
    {
      "epoch": 1.984065604373625,
      "grad_norm": 2.396216630935669,
      "learning_rate": 1.0259192825112109e-05,
      "loss": 1.2758,
      "step": 7440
    },
    {
      "epoch": 1.986732448829922,
      "grad_norm": 2.109013795852661,
      "learning_rate": 1.0232286995515695e-05,
      "loss": 1.1008,
      "step": 7450
    },
    {
      "epoch": 1.989399293286219,
      "grad_norm": 1.961639642715454,
      "learning_rate": 1.0205381165919282e-05,
      "loss": 1.2378,
      "step": 7460
    },
    {
      "epoch": 1.9920661377425162,
      "grad_norm": 2.129453659057617,
      "learning_rate": 1.0178475336322869e-05,
      "loss": 1.2021,
      "step": 7470
    },
    {
      "epoch": 1.9947329821988133,
      "grad_norm": 2.567910671234131,
      "learning_rate": 1.0151569506726457e-05,
      "loss": 1.0237,
      "step": 7480
    },
    {
      "epoch": 1.9973998266551103,
      "grad_norm": 2.0437896251678467,
      "learning_rate": 1.0124663677130046e-05,
      "loss": 1.1438,
      "step": 7490
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.9280083179473877,
      "learning_rate": 1.0097757847533633e-05,
      "loss": 1.109,
      "step": 7500
    },
    {
      "epoch": 2.002666844456297,
      "grad_norm": 1.854635238647461,
      "learning_rate": 1.007085201793722e-05,
      "loss": 1.1951,
      "step": 7510
    },
    {
      "epoch": 2.0053336889125943,
      "grad_norm": 3.4475150108337402,
      "learning_rate": 1.0043946188340808e-05,
      "loss": 1.1052,
      "step": 7520
    },
    {
      "epoch": 2.0080005333688913,
      "grad_norm": 3.0984294414520264,
      "learning_rate": 1.0017040358744395e-05,
      "loss": 1.0928,
      "step": 7530
    },
    {
      "epoch": 2.0106673778251882,
      "grad_norm": 1.951041340827942,
      "learning_rate": 9.990134529147983e-06,
      "loss": 1.1763,
      "step": 7540
    },
    {
      "epoch": 2.0133342222814856,
      "grad_norm": 2.8757987022399902,
      "learning_rate": 9.96322869955157e-06,
      "loss": 1.1251,
      "step": 7550
    },
    {
      "epoch": 2.0160010667377826,
      "grad_norm": 2.312044620513916,
      "learning_rate": 9.936322869955157e-06,
      "loss": 1.2631,
      "step": 7560
    },
    {
      "epoch": 2.0186679111940795,
      "grad_norm": 1.9821974039077759,
      "learning_rate": 9.909417040358745e-06,
      "loss": 1.0938,
      "step": 7570
    },
    {
      "epoch": 2.021334755650377,
      "grad_norm": 2.846892833709717,
      "learning_rate": 9.882511210762334e-06,
      "loss": 1.1104,
      "step": 7580
    },
    {
      "epoch": 2.024001600106674,
      "grad_norm": 2.0681111812591553,
      "learning_rate": 9.855605381165919e-06,
      "loss": 1.2627,
      "step": 7590
    },
    {
      "epoch": 2.026668444562971,
      "grad_norm": 2.0430076122283936,
      "learning_rate": 9.828699551569507e-06,
      "loss": 1.0632,
      "step": 7600
    },
    {
      "epoch": 2.0293352890192677,
      "grad_norm": 1.903747320175171,
      "learning_rate": 9.801793721973095e-06,
      "loss": 1.3086,
      "step": 7610
    },
    {
      "epoch": 2.032002133475565,
      "grad_norm": 8.017431259155273,
      "learning_rate": 9.774887892376682e-06,
      "loss": 1.1163,
      "step": 7620
    },
    {
      "epoch": 2.034668977931862,
      "grad_norm": 2.5062904357910156,
      "learning_rate": 9.747982062780269e-06,
      "loss": 1.1866,
      "step": 7630
    },
    {
      "epoch": 2.037335822388159,
      "grad_norm": 2.3792612552642822,
      "learning_rate": 9.721076233183856e-06,
      "loss": 1.1165,
      "step": 7640
    },
    {
      "epoch": 2.0400026668444564,
      "grad_norm": 2.1153976917266846,
      "learning_rate": 9.694170403587444e-06,
      "loss": 1.019,
      "step": 7650
    },
    {
      "epoch": 2.0426695113007534,
      "grad_norm": 2.8526837825775146,
      "learning_rate": 9.667264573991033e-06,
      "loss": 1.1083,
      "step": 7660
    },
    {
      "epoch": 2.0453363557570503,
      "grad_norm": 2.606837511062622,
      "learning_rate": 9.640358744394618e-06,
      "loss": 1.0898,
      "step": 7670
    },
    {
      "epoch": 2.0480032002133477,
      "grad_norm": 1.6864858865737915,
      "learning_rate": 9.613452914798206e-06,
      "loss": 1.2018,
      "step": 7680
    },
    {
      "epoch": 2.0506700446696446,
      "grad_norm": 2.2076704502105713,
      "learning_rate": 9.586547085201795e-06,
      "loss": 1.3077,
      "step": 7690
    },
    {
      "epoch": 2.0533368891259416,
      "grad_norm": 2.4590461254119873,
      "learning_rate": 9.559641255605381e-06,
      "loss": 1.1775,
      "step": 7700
    },
    {
      "epoch": 2.056003733582239,
      "grad_norm": 2.920567035675049,
      "learning_rate": 9.532735426008968e-06,
      "loss": 1.2286,
      "step": 7710
    },
    {
      "epoch": 2.058670578038536,
      "grad_norm": 2.416973829269409,
      "learning_rate": 9.505829596412557e-06,
      "loss": 1.1805,
      "step": 7720
    },
    {
      "epoch": 2.061337422494833,
      "grad_norm": 2.6110165119171143,
      "learning_rate": 9.478923766816143e-06,
      "loss": 1.3043,
      "step": 7730
    },
    {
      "epoch": 2.0640042669511303,
      "grad_norm": 2.3243823051452637,
      "learning_rate": 9.452017937219732e-06,
      "loss": 1.1678,
      "step": 7740
    },
    {
      "epoch": 2.066671111407427,
      "grad_norm": 2.3540170192718506,
      "learning_rate": 9.425112107623319e-06,
      "loss": 1.3224,
      "step": 7750
    },
    {
      "epoch": 2.069337955863724,
      "grad_norm": 1.966426134109497,
      "learning_rate": 9.398206278026905e-06,
      "loss": 1.1562,
      "step": 7760
    },
    {
      "epoch": 2.0720048003200215,
      "grad_norm": 2.6004350185394287,
      "learning_rate": 9.371300448430494e-06,
      "loss": 1.2121,
      "step": 7770
    },
    {
      "epoch": 2.0746716447763185,
      "grad_norm": 2.2808828353881836,
      "learning_rate": 9.344394618834082e-06,
      "loss": 1.2352,
      "step": 7780
    },
    {
      "epoch": 2.0773384892326154,
      "grad_norm": 2.045668363571167,
      "learning_rate": 9.317488789237667e-06,
      "loss": 1.1882,
      "step": 7790
    },
    {
      "epoch": 2.0800053336889124,
      "grad_norm": 2.2499637603759766,
      "learning_rate": 9.290582959641256e-06,
      "loss": 1.1287,
      "step": 7800
    },
    {
      "epoch": 2.0826721781452098,
      "grad_norm": 3.218184232711792,
      "learning_rate": 9.263677130044843e-06,
      "loss": 1.1343,
      "step": 7810
    },
    {
      "epoch": 2.0853390226015067,
      "grad_norm": 3.017970561981201,
      "learning_rate": 9.236771300448431e-06,
      "loss": 1.2379,
      "step": 7820
    },
    {
      "epoch": 2.0880058670578037,
      "grad_norm": 2.0029489994049072,
      "learning_rate": 9.209865470852018e-06,
      "loss": 1.2331,
      "step": 7830
    },
    {
      "epoch": 2.090672711514101,
      "grad_norm": 3.0580081939697266,
      "learning_rate": 9.182959641255605e-06,
      "loss": 1.1652,
      "step": 7840
    },
    {
      "epoch": 2.093339555970398,
      "grad_norm": 2.9702842235565186,
      "learning_rate": 9.156053811659193e-06,
      "loss": 1.1082,
      "step": 7850
    },
    {
      "epoch": 2.096006400426695,
      "grad_norm": 3.0734660625457764,
      "learning_rate": 9.129147982062781e-06,
      "loss": 1.179,
      "step": 7860
    },
    {
      "epoch": 2.0986732448829923,
      "grad_norm": 2.2450060844421387,
      "learning_rate": 9.102242152466368e-06,
      "loss": 1.0593,
      "step": 7870
    },
    {
      "epoch": 2.1013400893392893,
      "grad_norm": 2.3061139583587646,
      "learning_rate": 9.075336322869955e-06,
      "loss": 1.1539,
      "step": 7880
    },
    {
      "epoch": 2.1040069337955862,
      "grad_norm": 2.5877678394317627,
      "learning_rate": 9.048430493273543e-06,
      "loss": 0.9944,
      "step": 7890
    },
    {
      "epoch": 2.1066737782518836,
      "grad_norm": 2.418250799179077,
      "learning_rate": 9.02152466367713e-06,
      "loss": 1.2633,
      "step": 7900
    },
    {
      "epoch": 2.1093406227081806,
      "grad_norm": 2.3623311519622803,
      "learning_rate": 8.994618834080719e-06,
      "loss": 1.2349,
      "step": 7910
    },
    {
      "epoch": 2.1120074671644775,
      "grad_norm": 2.0045738220214844,
      "learning_rate": 8.967713004484305e-06,
      "loss": 1.113,
      "step": 7920
    },
    {
      "epoch": 2.114674311620775,
      "grad_norm": 3.144477605819702,
      "learning_rate": 8.940807174887892e-06,
      "loss": 1.2132,
      "step": 7930
    },
    {
      "epoch": 2.117341156077072,
      "grad_norm": 2.157504081726074,
      "learning_rate": 8.91390134529148e-06,
      "loss": 1.2672,
      "step": 7940
    },
    {
      "epoch": 2.120008000533369,
      "grad_norm": 2.5193464756011963,
      "learning_rate": 8.886995515695069e-06,
      "loss": 1.0318,
      "step": 7950
    },
    {
      "epoch": 2.122674844989666,
      "grad_norm": 3.053270101547241,
      "learning_rate": 8.860089686098654e-06,
      "loss": 1.142,
      "step": 7960
    },
    {
      "epoch": 2.125341689445963,
      "grad_norm": 2.8622004985809326,
      "learning_rate": 8.833183856502243e-06,
      "loss": 1.1906,
      "step": 7970
    },
    {
      "epoch": 2.12800853390226,
      "grad_norm": 2.3995018005371094,
      "learning_rate": 8.80627802690583e-06,
      "loss": 1.2036,
      "step": 7980
    },
    {
      "epoch": 2.130675378358557,
      "grad_norm": 2.3842074871063232,
      "learning_rate": 8.779372197309418e-06,
      "loss": 1.2129,
      "step": 7990
    },
    {
      "epoch": 2.1333422228148544,
      "grad_norm": 2.146064043045044,
      "learning_rate": 8.752466367713005e-06,
      "loss": 1.1089,
      "step": 8000
    },
    {
      "epoch": 2.1360090672711514,
      "grad_norm": 2.374755859375,
      "learning_rate": 8.725560538116591e-06,
      "loss": 1.1924,
      "step": 8010
    },
    {
      "epoch": 2.1386759117274483,
      "grad_norm": 2.3555219173431396,
      "learning_rate": 8.69865470852018e-06,
      "loss": 1.2267,
      "step": 8020
    },
    {
      "epoch": 2.1413427561837457,
      "grad_norm": 1.998429775238037,
      "learning_rate": 8.671748878923768e-06,
      "loss": 1.2441,
      "step": 8030
    },
    {
      "epoch": 2.1440096006400426,
      "grad_norm": 2.525801420211792,
      "learning_rate": 8.644843049327353e-06,
      "loss": 1.0422,
      "step": 8040
    },
    {
      "epoch": 2.1466764450963396,
      "grad_norm": 2.8186304569244385,
      "learning_rate": 8.617937219730942e-06,
      "loss": 1.2263,
      "step": 8050
    },
    {
      "epoch": 2.149343289552637,
      "grad_norm": 2.896294116973877,
      "learning_rate": 8.59103139013453e-06,
      "loss": 1.3258,
      "step": 8060
    },
    {
      "epoch": 2.152010134008934,
      "grad_norm": 2.2222886085510254,
      "learning_rate": 8.564125560538117e-06,
      "loss": 1.2387,
      "step": 8070
    },
    {
      "epoch": 2.154676978465231,
      "grad_norm": 2.703888177871704,
      "learning_rate": 8.537219730941704e-06,
      "loss": 1.0613,
      "step": 8080
    },
    {
      "epoch": 2.1573438229215283,
      "grad_norm": 2.3517608642578125,
      "learning_rate": 8.510313901345292e-06,
      "loss": 1.182,
      "step": 8090
    },
    {
      "epoch": 2.160010667377825,
      "grad_norm": 2.4977362155914307,
      "learning_rate": 8.483408071748879e-06,
      "loss": 1.1325,
      "step": 8100
    },
    {
      "epoch": 2.162677511834122,
      "grad_norm": 2.073012113571167,
      "learning_rate": 8.456502242152467e-06,
      "loss": 1.1668,
      "step": 8110
    },
    {
      "epoch": 2.1653443562904195,
      "grad_norm": 2.6880147457122803,
      "learning_rate": 8.429596412556054e-06,
      "loss": 1.0397,
      "step": 8120
    },
    {
      "epoch": 2.1680112007467165,
      "grad_norm": 3.465757131576538,
      "learning_rate": 8.402690582959641e-06,
      "loss": 1.2953,
      "step": 8130
    },
    {
      "epoch": 2.1706780452030134,
      "grad_norm": 3.007965326309204,
      "learning_rate": 8.37578475336323e-06,
      "loss": 1.1937,
      "step": 8140
    },
    {
      "epoch": 2.173344889659311,
      "grad_norm": 2.3289191722869873,
      "learning_rate": 8.348878923766816e-06,
      "loss": 1.0808,
      "step": 8150
    },
    {
      "epoch": 2.1760117341156078,
      "grad_norm": 1.904119610786438,
      "learning_rate": 8.321973094170403e-06,
      "loss": 0.9786,
      "step": 8160
    },
    {
      "epoch": 2.1786785785719047,
      "grad_norm": 2.753157377243042,
      "learning_rate": 8.295067264573991e-06,
      "loss": 1.1869,
      "step": 8170
    },
    {
      "epoch": 2.181345423028202,
      "grad_norm": 2.48962664604187,
      "learning_rate": 8.268161434977578e-06,
      "loss": 1.1701,
      "step": 8180
    },
    {
      "epoch": 2.184012267484499,
      "grad_norm": 2.696532964706421,
      "learning_rate": 8.241255605381166e-06,
      "loss": 0.9884,
      "step": 8190
    },
    {
      "epoch": 2.186679111940796,
      "grad_norm": 2.049814462661743,
      "learning_rate": 8.214349775784753e-06,
      "loss": 1.0305,
      "step": 8200
    },
    {
      "epoch": 2.189345956397093,
      "grad_norm": 2.1452834606170654,
      "learning_rate": 8.18744394618834e-06,
      "loss": 1.0981,
      "step": 8210
    },
    {
      "epoch": 2.1920128008533903,
      "grad_norm": 2.511244058609009,
      "learning_rate": 8.160538116591928e-06,
      "loss": 1.1569,
      "step": 8220
    },
    {
      "epoch": 2.1946796453096873,
      "grad_norm": 2.2476696968078613,
      "learning_rate": 8.133632286995517e-06,
      "loss": 1.1225,
      "step": 8230
    },
    {
      "epoch": 2.1973464897659842,
      "grad_norm": 3.5722341537475586,
      "learning_rate": 8.106726457399104e-06,
      "loss": 1.2066,
      "step": 8240
    },
    {
      "epoch": 2.2000133342222816,
      "grad_norm": 2.6618690490722656,
      "learning_rate": 8.07982062780269e-06,
      "loss": 1.0895,
      "step": 8250
    },
    {
      "epoch": 2.2026801786785786,
      "grad_norm": 2.6855499744415283,
      "learning_rate": 8.052914798206279e-06,
      "loss": 1.1141,
      "step": 8260
    },
    {
      "epoch": 2.2053470231348755,
      "grad_norm": 3.1426339149475098,
      "learning_rate": 8.026008968609866e-06,
      "loss": 1.2253,
      "step": 8270
    },
    {
      "epoch": 2.208013867591173,
      "grad_norm": 4.206092834472656,
      "learning_rate": 7.999103139013454e-06,
      "loss": 1.1106,
      "step": 8280
    },
    {
      "epoch": 2.21068071204747,
      "grad_norm": 2.5287656784057617,
      "learning_rate": 7.972197309417041e-06,
      "loss": 1.1195,
      "step": 8290
    },
    {
      "epoch": 2.213347556503767,
      "grad_norm": 2.139111280441284,
      "learning_rate": 7.945291479820628e-06,
      "loss": 1.0772,
      "step": 8300
    },
    {
      "epoch": 2.216014400960064,
      "grad_norm": 2.212238073348999,
      "learning_rate": 7.918385650224216e-06,
      "loss": 1.159,
      "step": 8310
    },
    {
      "epoch": 2.218681245416361,
      "grad_norm": 2.2791335582733154,
      "learning_rate": 7.891479820627803e-06,
      "loss": 1.1448,
      "step": 8320
    },
    {
      "epoch": 2.221348089872658,
      "grad_norm": 2.5001091957092285,
      "learning_rate": 7.86457399103139e-06,
      "loss": 1.127,
      "step": 8330
    },
    {
      "epoch": 2.224014934328955,
      "grad_norm": 2.1835315227508545,
      "learning_rate": 7.837668161434978e-06,
      "loss": 1.1699,
      "step": 8340
    },
    {
      "epoch": 2.2266817787852524,
      "grad_norm": 2.7376654148101807,
      "learning_rate": 7.810762331838565e-06,
      "loss": 1.1086,
      "step": 8350
    },
    {
      "epoch": 2.2293486232415494,
      "grad_norm": 2.090482473373413,
      "learning_rate": 7.783856502242153e-06,
      "loss": 1.1437,
      "step": 8360
    },
    {
      "epoch": 2.2320154676978463,
      "grad_norm": 2.2625319957733154,
      "learning_rate": 7.75695067264574e-06,
      "loss": 1.1259,
      "step": 8370
    },
    {
      "epoch": 2.2346823121541437,
      "grad_norm": 1.8171366453170776,
      "learning_rate": 7.730044843049327e-06,
      "loss": 1.1764,
      "step": 8380
    },
    {
      "epoch": 2.2373491566104406,
      "grad_norm": 3.363417148590088,
      "learning_rate": 7.703139013452915e-06,
      "loss": 1.2192,
      "step": 8390
    },
    {
      "epoch": 2.2400160010667376,
      "grad_norm": 1.7728310823440552,
      "learning_rate": 7.676233183856504e-06,
      "loss": 1.2239,
      "step": 8400
    },
    {
      "epoch": 2.242682845523035,
      "grad_norm": 1.7122105360031128,
      "learning_rate": 7.649327354260089e-06,
      "loss": 1.2363,
      "step": 8410
    },
    {
      "epoch": 2.245349689979332,
      "grad_norm": 2.1742093563079834,
      "learning_rate": 7.622421524663677e-06,
      "loss": 1.1204,
      "step": 8420
    },
    {
      "epoch": 2.248016534435629,
      "grad_norm": 1.9817173480987549,
      "learning_rate": 7.595515695067266e-06,
      "loss": 1.1716,
      "step": 8430
    },
    {
      "epoch": 2.2506833788919263,
      "grad_norm": 1.895214557647705,
      "learning_rate": 7.568609865470852e-06,
      "loss": 1.1051,
      "step": 8440
    },
    {
      "epoch": 2.253350223348223,
      "grad_norm": 2.274571418762207,
      "learning_rate": 7.54170403587444e-06,
      "loss": 1.2779,
      "step": 8450
    },
    {
      "epoch": 2.25601706780452,
      "grad_norm": 2.032921075820923,
      "learning_rate": 7.514798206278026e-06,
      "loss": 1.1494,
      "step": 8460
    },
    {
      "epoch": 2.2586839122608176,
      "grad_norm": 2.161600112915039,
      "learning_rate": 7.487892376681614e-06,
      "loss": 1.2223,
      "step": 8470
    },
    {
      "epoch": 2.2613507567171145,
      "grad_norm": 2.423703670501709,
      "learning_rate": 7.460986547085202e-06,
      "loss": 1.0391,
      "step": 8480
    },
    {
      "epoch": 2.2640176011734114,
      "grad_norm": 3.3564398288726807,
      "learning_rate": 7.43408071748879e-06,
      "loss": 1.1667,
      "step": 8490
    },
    {
      "epoch": 2.266684445629709,
      "grad_norm": 2.23317813873291,
      "learning_rate": 7.407174887892376e-06,
      "loss": 1.1111,
      "step": 8500
    },
    {
      "epoch": 2.269351290086006,
      "grad_norm": 1.9189133644104004,
      "learning_rate": 7.380269058295964e-06,
      "loss": 1.0711,
      "step": 8510
    },
    {
      "epoch": 2.2720181345423027,
      "grad_norm": 2.5071001052856445,
      "learning_rate": 7.353363228699552e-06,
      "loss": 1.1234,
      "step": 8520
    },
    {
      "epoch": 2.2746849789986,
      "grad_norm": 2.0139384269714355,
      "learning_rate": 7.326457399103139e-06,
      "loss": 1.0393,
      "step": 8530
    },
    {
      "epoch": 2.277351823454897,
      "grad_norm": 2.3229012489318848,
      "learning_rate": 7.299551569506726e-06,
      "loss": 1.0307,
      "step": 8540
    },
    {
      "epoch": 2.280018667911194,
      "grad_norm": 2.3346807956695557,
      "learning_rate": 7.272645739910314e-06,
      "loss": 1.2149,
      "step": 8550
    },
    {
      "epoch": 2.2826855123674914,
      "grad_norm": 2.41251802444458,
      "learning_rate": 7.245739910313902e-06,
      "loss": 1.185,
      "step": 8560
    },
    {
      "epoch": 2.2853523568237883,
      "grad_norm": 2.817836046218872,
      "learning_rate": 7.218834080717489e-06,
      "loss": 1.3623,
      "step": 8570
    },
    {
      "epoch": 2.2880192012800853,
      "grad_norm": 2.412574529647827,
      "learning_rate": 7.191928251121077e-06,
      "loss": 1.1694,
      "step": 8580
    },
    {
      "epoch": 2.2906860457363822,
      "grad_norm": 2.032541275024414,
      "learning_rate": 7.165022421524664e-06,
      "loss": 1.2535,
      "step": 8590
    },
    {
      "epoch": 2.2933528901926796,
      "grad_norm": 2.4110469818115234,
      "learning_rate": 7.138116591928252e-06,
      "loss": 1.2556,
      "step": 8600
    },
    {
      "epoch": 2.2960197346489766,
      "grad_norm": 2.4891412258148193,
      "learning_rate": 7.111210762331838e-06,
      "loss": 1.1729,
      "step": 8610
    },
    {
      "epoch": 2.2986865791052735,
      "grad_norm": 2.2748119831085205,
      "learning_rate": 7.084304932735427e-06,
      "loss": 0.9429,
      "step": 8620
    },
    {
      "epoch": 2.301353423561571,
      "grad_norm": 2.3786282539367676,
      "learning_rate": 7.0573991031390136e-06,
      "loss": 1.1244,
      "step": 8630
    },
    {
      "epoch": 2.304020268017868,
      "grad_norm": 3.463264226913452,
      "learning_rate": 7.030493273542601e-06,
      "loss": 1.0892,
      "step": 8640
    },
    {
      "epoch": 2.306687112474165,
      "grad_norm": 2.183293581008911,
      "learning_rate": 7.003587443946189e-06,
      "loss": 1.213,
      "step": 8650
    },
    {
      "epoch": 2.309353956930462,
      "grad_norm": 2.038132667541504,
      "learning_rate": 6.976681614349776e-06,
      "loss": 1.2848,
      "step": 8660
    },
    {
      "epoch": 2.312020801386759,
      "grad_norm": 2.2677977085113525,
      "learning_rate": 6.949775784753363e-06,
      "loss": 1.2969,
      "step": 8670
    },
    {
      "epoch": 2.314687645843056,
      "grad_norm": 2.880384683609009,
      "learning_rate": 6.922869955156951e-06,
      "loss": 1.0707,
      "step": 8680
    },
    {
      "epoch": 2.3173544902993535,
      "grad_norm": 3.0431149005889893,
      "learning_rate": 6.895964125560538e-06,
      "loss": 1.1293,
      "step": 8690
    },
    {
      "epoch": 2.3200213347556504,
      "grad_norm": 2.497020959854126,
      "learning_rate": 6.869058295964126e-06,
      "loss": 1.1332,
      "step": 8700
    },
    {
      "epoch": 2.3226881792119474,
      "grad_norm": 2.6143648624420166,
      "learning_rate": 6.842152466367713e-06,
      "loss": 1.2704,
      "step": 8710
    },
    {
      "epoch": 2.3253550236682443,
      "grad_norm": 2.679349899291992,
      "learning_rate": 6.815246636771301e-06,
      "loss": 1.1982,
      "step": 8720
    },
    {
      "epoch": 2.3280218681245417,
      "grad_norm": 4.07633638381958,
      "learning_rate": 6.788340807174888e-06,
      "loss": 0.9877,
      "step": 8730
    },
    {
      "epoch": 2.3306887125808387,
      "grad_norm": 2.664055824279785,
      "learning_rate": 6.7614349775784755e-06,
      "loss": 1.0986,
      "step": 8740
    },
    {
      "epoch": 2.3333555570371356,
      "grad_norm": 2.4971764087677,
      "learning_rate": 6.734529147982062e-06,
      "loss": 1.2099,
      "step": 8750
    },
    {
      "epoch": 2.336022401493433,
      "grad_norm": 2.0010242462158203,
      "learning_rate": 6.707623318385651e-06,
      "loss": 1.2005,
      "step": 8760
    },
    {
      "epoch": 2.33868924594973,
      "grad_norm": 1.8989347219467163,
      "learning_rate": 6.6807174887892375e-06,
      "loss": 1.1755,
      "step": 8770
    },
    {
      "epoch": 2.341356090406027,
      "grad_norm": 2.014906883239746,
      "learning_rate": 6.653811659192825e-06,
      "loss": 1.1593,
      "step": 8780
    },
    {
      "epoch": 2.3440229348623243,
      "grad_norm": 3.368619203567505,
      "learning_rate": 6.626905829596413e-06,
      "loss": 1.1004,
      "step": 8790
    },
    {
      "epoch": 2.346689779318621,
      "grad_norm": 2.5170955657958984,
      "learning_rate": 6.6e-06,
      "loss": 1.1703,
      "step": 8800
    },
    {
      "epoch": 2.349356623774918,
      "grad_norm": 2.572418689727783,
      "learning_rate": 6.573094170403587e-06,
      "loss": 1.0479,
      "step": 8810
    },
    {
      "epoch": 2.3520234682312156,
      "grad_norm": 2.613215446472168,
      "learning_rate": 6.5461883408071755e-06,
      "loss": 1.1777,
      "step": 8820
    },
    {
      "epoch": 2.3546903126875125,
      "grad_norm": 4.148019313812256,
      "learning_rate": 6.519282511210762e-06,
      "loss": 1.1925,
      "step": 8830
    },
    {
      "epoch": 2.3573571571438094,
      "grad_norm": 2.333838701248169,
      "learning_rate": 6.49237668161435e-06,
      "loss": 1.2437,
      "step": 8840
    },
    {
      "epoch": 2.360024001600107,
      "grad_norm": 2.0159521102905273,
      "learning_rate": 6.465470852017937e-06,
      "loss": 1.1151,
      "step": 8850
    },
    {
      "epoch": 2.362690846056404,
      "grad_norm": 2.8458077907562256,
      "learning_rate": 6.438565022421525e-06,
      "loss": 1.1503,
      "step": 8860
    },
    {
      "epoch": 2.3653576905127007,
      "grad_norm": 2.3599131107330322,
      "learning_rate": 6.411659192825112e-06,
      "loss": 1.0497,
      "step": 8870
    },
    {
      "epoch": 2.368024534968998,
      "grad_norm": 1.8708432912826538,
      "learning_rate": 6.3847533632286995e-06,
      "loss": 1.2124,
      "step": 8880
    },
    {
      "epoch": 2.370691379425295,
      "grad_norm": 2.015333414077759,
      "learning_rate": 6.357847533632287e-06,
      "loss": 1.3443,
      "step": 8890
    },
    {
      "epoch": 2.373358223881592,
      "grad_norm": 3.4624998569488525,
      "learning_rate": 6.330941704035875e-06,
      "loss": 1.2324,
      "step": 8900
    },
    {
      "epoch": 2.3760250683378894,
      "grad_norm": 2.514504909515381,
      "learning_rate": 6.3040358744394615e-06,
      "loss": 1.1639,
      "step": 8910
    },
    {
      "epoch": 2.3786919127941863,
      "grad_norm": 2.693432331085205,
      "learning_rate": 6.277130044843049e-06,
      "loss": 1.0681,
      "step": 8920
    },
    {
      "epoch": 2.3813587572504833,
      "grad_norm": 2.284395933151245,
      "learning_rate": 6.2502242152466375e-06,
      "loss": 1.2346,
      "step": 8930
    },
    {
      "epoch": 2.3840256017067807,
      "grad_norm": 1.9718244075775146,
      "learning_rate": 6.223318385650224e-06,
      "loss": 1.1555,
      "step": 8940
    },
    {
      "epoch": 2.3866924461630776,
      "grad_norm": 3.106039524078369,
      "learning_rate": 6.196412556053812e-06,
      "loss": 1.1978,
      "step": 8950
    },
    {
      "epoch": 2.3893592906193746,
      "grad_norm": 2.3779311180114746,
      "learning_rate": 6.1695067264573995e-06,
      "loss": 1.1197,
      "step": 8960
    },
    {
      "epoch": 2.3920261350756715,
      "grad_norm": 2.236605644226074,
      "learning_rate": 6.142600896860987e-06,
      "loss": 1.0543,
      "step": 8970
    },
    {
      "epoch": 2.394692979531969,
      "grad_norm": 3.669417142868042,
      "learning_rate": 6.115695067264574e-06,
      "loss": 1.0398,
      "step": 8980
    },
    {
      "epoch": 2.397359823988266,
      "grad_norm": 2.6063666343688965,
      "learning_rate": 6.088789237668162e-06,
      "loss": 1.0637,
      "step": 8990
    },
    {
      "epoch": 2.400026668444563,
      "grad_norm": 2.0107932090759277,
      "learning_rate": 6.061883408071749e-06,
      "loss": 1.0678,
      "step": 9000
    },
    {
      "epoch": 2.40269351290086,
      "grad_norm": 3.273162841796875,
      "learning_rate": 6.034977578475337e-06,
      "loss": 1.1814,
      "step": 9010
    },
    {
      "epoch": 2.405360357357157,
      "grad_norm": 2.0858445167541504,
      "learning_rate": 6.0080717488789234e-06,
      "loss": 1.1539,
      "step": 9020
    },
    {
      "epoch": 2.408027201813454,
      "grad_norm": 2.773350477218628,
      "learning_rate": 5.981165919282512e-06,
      "loss": 1.2334,
      "step": 9030
    },
    {
      "epoch": 2.4106940462697515,
      "grad_norm": 2.6674351692199707,
      "learning_rate": 5.954260089686099e-06,
      "loss": 1.1607,
      "step": 9040
    },
    {
      "epoch": 2.4133608907260484,
      "grad_norm": 2.78692889213562,
      "learning_rate": 5.927354260089686e-06,
      "loss": 1.2209,
      "step": 9050
    },
    {
      "epoch": 2.4160277351823454,
      "grad_norm": 2.7983591556549072,
      "learning_rate": 5.900448430493274e-06,
      "loss": 1.2127,
      "step": 9060
    },
    {
      "epoch": 2.4186945796386428,
      "grad_norm": 2.1028828620910645,
      "learning_rate": 5.8735426008968615e-06,
      "loss": 1.1117,
      "step": 9070
    },
    {
      "epoch": 2.4213614240949397,
      "grad_norm": 2.246392011642456,
      "learning_rate": 5.846636771300448e-06,
      "loss": 1.0876,
      "step": 9080
    },
    {
      "epoch": 2.4240282685512367,
      "grad_norm": 3.666944980621338,
      "learning_rate": 5.819730941704036e-06,
      "loss": 1.1323,
      "step": 9090
    },
    {
      "epoch": 2.4266951130075336,
      "grad_norm": 2.5798001289367676,
      "learning_rate": 5.7928251121076235e-06,
      "loss": 1.1453,
      "step": 9100
    },
    {
      "epoch": 2.429361957463831,
      "grad_norm": 2.6931824684143066,
      "learning_rate": 5.765919282511211e-06,
      "loss": 1.2219,
      "step": 9110
    },
    {
      "epoch": 2.432028801920128,
      "grad_norm": 2.6856982707977295,
      "learning_rate": 5.739013452914798e-06,
      "loss": 1.2037,
      "step": 9120
    },
    {
      "epoch": 2.434695646376425,
      "grad_norm": 2.221259832382202,
      "learning_rate": 5.712107623318386e-06,
      "loss": 1.1897,
      "step": 9130
    },
    {
      "epoch": 2.4373624908327223,
      "grad_norm": 3.0163826942443848,
      "learning_rate": 5.685201793721973e-06,
      "loss": 1.295,
      "step": 9140
    },
    {
      "epoch": 2.440029335289019,
      "grad_norm": 4.741863250732422,
      "learning_rate": 5.658295964125561e-06,
      "loss": 1.1668,
      "step": 9150
    },
    {
      "epoch": 2.442696179745316,
      "grad_norm": 1.9317867755889893,
      "learning_rate": 5.631390134529148e-06,
      "loss": 1.1323,
      "step": 9160
    },
    {
      "epoch": 2.4453630242016136,
      "grad_norm": 1.9964520931243896,
      "learning_rate": 5.604484304932736e-06,
      "loss": 1.2111,
      "step": 9170
    },
    {
      "epoch": 2.4480298686579105,
      "grad_norm": 2.9318885803222656,
      "learning_rate": 5.577578475336323e-06,
      "loss": 1.2943,
      "step": 9180
    },
    {
      "epoch": 2.4506967131142074,
      "grad_norm": 1.9190882444381714,
      "learning_rate": 5.55067264573991e-06,
      "loss": 1.0134,
      "step": 9190
    },
    {
      "epoch": 2.453363557570505,
      "grad_norm": 2.3777503967285156,
      "learning_rate": 5.523766816143498e-06,
      "loss": 1.0696,
      "step": 9200
    },
    {
      "epoch": 2.456030402026802,
      "grad_norm": 2.4593398571014404,
      "learning_rate": 5.4968609865470854e-06,
      "loss": 1.2548,
      "step": 9210
    },
    {
      "epoch": 2.4586972464830987,
      "grad_norm": 2.859158515930176,
      "learning_rate": 5.469955156950672e-06,
      "loss": 1.1929,
      "step": 9220
    },
    {
      "epoch": 2.461364090939396,
      "grad_norm": 2.3845276832580566,
      "learning_rate": 5.443049327354261e-06,
      "loss": 1.2137,
      "step": 9230
    },
    {
      "epoch": 2.464030935395693,
      "grad_norm": 2.2946619987487793,
      "learning_rate": 5.416143497757847e-06,
      "loss": 1.0712,
      "step": 9240
    },
    {
      "epoch": 2.46669777985199,
      "grad_norm": 2.7225112915039062,
      "learning_rate": 5.389237668161435e-06,
      "loss": 1.2021,
      "step": 9250
    },
    {
      "epoch": 2.4693646243082874,
      "grad_norm": 2.760418176651001,
      "learning_rate": 5.362331838565022e-06,
      "loss": 1.1726,
      "step": 9260
    },
    {
      "epoch": 2.4720314687645843,
      "grad_norm": 2.983412265777588,
      "learning_rate": 5.33542600896861e-06,
      "loss": 1.0651,
      "step": 9270
    },
    {
      "epoch": 2.4746983132208813,
      "grad_norm": 2.3733723163604736,
      "learning_rate": 5.308520179372198e-06,
      "loss": 1.1664,
      "step": 9280
    },
    {
      "epoch": 2.4773651576771787,
      "grad_norm": 1.9041520357131958,
      "learning_rate": 5.281614349775785e-06,
      "loss": 1.0423,
      "step": 9290
    },
    {
      "epoch": 2.4800320021334756,
      "grad_norm": 2.90535831451416,
      "learning_rate": 5.254708520179373e-06,
      "loss": 1.123,
      "step": 9300
    },
    {
      "epoch": 2.4826988465897726,
      "grad_norm": 2.0705301761627197,
      "learning_rate": 5.22780269058296e-06,
      "loss": 1.153,
      "step": 9310
    },
    {
      "epoch": 2.48536569104607,
      "grad_norm": 2.5990209579467773,
      "learning_rate": 5.200896860986547e-06,
      "loss": 1.2295,
      "step": 9320
    },
    {
      "epoch": 2.488032535502367,
      "grad_norm": 2.4878971576690674,
      "learning_rate": 5.173991031390135e-06,
      "loss": 1.1831,
      "step": 9330
    },
    {
      "epoch": 2.490699379958664,
      "grad_norm": 1.9429140090942383,
      "learning_rate": 5.147085201793723e-06,
      "loss": 1.2002,
      "step": 9340
    },
    {
      "epoch": 2.493366224414961,
      "grad_norm": 2.2279233932495117,
      "learning_rate": 5.120179372197309e-06,
      "loss": 1.1917,
      "step": 9350
    },
    {
      "epoch": 2.496033068871258,
      "grad_norm": 2.496467113494873,
      "learning_rate": 5.093273542600897e-06,
      "loss": 1.1958,
      "step": 9360
    },
    {
      "epoch": 2.498699913327555,
      "grad_norm": 2.6123437881469727,
      "learning_rate": 5.066367713004485e-06,
      "loss": 1.1701,
      "step": 9370
    },
    {
      "epoch": 2.501366757783852,
      "grad_norm": 2.8855044841766357,
      "learning_rate": 5.039461883408072e-06,
      "loss": 1.1586,
      "step": 9380
    },
    {
      "epoch": 2.5040336022401495,
      "grad_norm": 2.2333216667175293,
      "learning_rate": 5.012556053811659e-06,
      "loss": 1.0328,
      "step": 9390
    },
    {
      "epoch": 2.5067004466964464,
      "grad_norm": 2.796840190887451,
      "learning_rate": 4.985650224215247e-06,
      "loss": 1.1146,
      "step": 9400
    },
    {
      "epoch": 2.5093672911527434,
      "grad_norm": 2.540682077407837,
      "learning_rate": 4.958744394618834e-06,
      "loss": 0.9975,
      "step": 9410
    },
    {
      "epoch": 2.5120341356090408,
      "grad_norm": 3.1056432723999023,
      "learning_rate": 4.931838565022422e-06,
      "loss": 1.1899,
      "step": 9420
    },
    {
      "epoch": 2.5147009800653377,
      "grad_norm": 2.0309202671051025,
      "learning_rate": 4.9049327354260085e-06,
      "loss": 1.2123,
      "step": 9430
    },
    {
      "epoch": 2.5173678245216347,
      "grad_norm": 2.257272243499756,
      "learning_rate": 4.878026905829597e-06,
      "loss": 1.1029,
      "step": 9440
    },
    {
      "epoch": 2.5200346689779316,
      "grad_norm": 3.126619815826416,
      "learning_rate": 4.851121076233184e-06,
      "loss": 1.2393,
      "step": 9450
    },
    {
      "epoch": 2.522701513434229,
      "grad_norm": 2.7023327350616455,
      "learning_rate": 4.824215246636771e-06,
      "loss": 1.1784,
      "step": 9460
    },
    {
      "epoch": 2.525368357890526,
      "grad_norm": 3.401277780532837,
      "learning_rate": 4.797309417040359e-06,
      "loss": 1.126,
      "step": 9470
    },
    {
      "epoch": 2.528035202346823,
      "grad_norm": 2.8018720149993896,
      "learning_rate": 4.7704035874439466e-06,
      "loss": 1.0721,
      "step": 9480
    },
    {
      "epoch": 2.5307020468031203,
      "grad_norm": 2.5848569869995117,
      "learning_rate": 4.743497757847533e-06,
      "loss": 1.1466,
      "step": 9490
    },
    {
      "epoch": 2.533368891259417,
      "grad_norm": 3.32004976272583,
      "learning_rate": 4.716591928251121e-06,
      "loss": 1.2988,
      "step": 9500
    },
    {
      "epoch": 2.536035735715714,
      "grad_norm": 2.287968635559082,
      "learning_rate": 4.6896860986547085e-06,
      "loss": 0.9648,
      "step": 9510
    },
    {
      "epoch": 2.5387025801720116,
      "grad_norm": 2.242832899093628,
      "learning_rate": 4.662780269058296e-06,
      "loss": 1.0998,
      "step": 9520
    },
    {
      "epoch": 2.5413694246283085,
      "grad_norm": 3.228414535522461,
      "learning_rate": 4.635874439461883e-06,
      "loss": 1.3171,
      "step": 9530
    },
    {
      "epoch": 2.5440362690846055,
      "grad_norm": 2.56839919090271,
      "learning_rate": 4.608968609865471e-06,
      "loss": 1.2023,
      "step": 9540
    },
    {
      "epoch": 2.546703113540903,
      "grad_norm": 2.1543705463409424,
      "learning_rate": 4.582062780269058e-06,
      "loss": 1.1553,
      "step": 9550
    },
    {
      "epoch": 2.5493699579972,
      "grad_norm": 1.799282193183899,
      "learning_rate": 4.555156950672646e-06,
      "loss": 1.0841,
      "step": 9560
    },
    {
      "epoch": 2.5520368024534967,
      "grad_norm": 2.798765182495117,
      "learning_rate": 4.528251121076233e-06,
      "loss": 1.1912,
      "step": 9570
    },
    {
      "epoch": 2.554703646909794,
      "grad_norm": 2.1365115642547607,
      "learning_rate": 4.501345291479821e-06,
      "loss": 1.1424,
      "step": 9580
    },
    {
      "epoch": 2.557370491366091,
      "grad_norm": 1.8597657680511475,
      "learning_rate": 4.474439461883408e-06,
      "loss": 1.2463,
      "step": 9590
    },
    {
      "epoch": 2.560037335822388,
      "grad_norm": 2.375415086746216,
      "learning_rate": 4.447533632286995e-06,
      "loss": 1.1065,
      "step": 9600
    },
    {
      "epoch": 2.5627041802786854,
      "grad_norm": 2.3330812454223633,
      "learning_rate": 4.420627802690583e-06,
      "loss": 1.006,
      "step": 9610
    },
    {
      "epoch": 2.5653710247349824,
      "grad_norm": 2.6000773906707764,
      "learning_rate": 4.3937219730941705e-06,
      "loss": 1.0552,
      "step": 9620
    },
    {
      "epoch": 2.5680378691912793,
      "grad_norm": 2.86214017868042,
      "learning_rate": 4.366816143497757e-06,
      "loss": 1.1183,
      "step": 9630
    },
    {
      "epoch": 2.5707047136475767,
      "grad_norm": 2.884385108947754,
      "learning_rate": 4.339910313901346e-06,
      "loss": 1.0999,
      "step": 9640
    },
    {
      "epoch": 2.5733715581038736,
      "grad_norm": 2.5461575984954834,
      "learning_rate": 4.313004484304933e-06,
      "loss": 1.0753,
      "step": 9650
    },
    {
      "epoch": 2.5760384025601706,
      "grad_norm": 2.096242666244507,
      "learning_rate": 4.28609865470852e-06,
      "loss": 1.2012,
      "step": 9660
    },
    {
      "epoch": 2.578705247016468,
      "grad_norm": 3.3709301948547363,
      "learning_rate": 4.259192825112108e-06,
      "loss": 1.152,
      "step": 9670
    },
    {
      "epoch": 2.581372091472765,
      "grad_norm": 2.9414820671081543,
      "learning_rate": 4.232286995515695e-06,
      "loss": 1.1106,
      "step": 9680
    },
    {
      "epoch": 2.584038935929062,
      "grad_norm": 2.1949994564056396,
      "learning_rate": 4.205381165919283e-06,
      "loss": 1.207,
      "step": 9690
    },
    {
      "epoch": 2.5867057803853593,
      "grad_norm": 2.2970261573791504,
      "learning_rate": 4.17847533632287e-06,
      "loss": 1.1321,
      "step": 9700
    },
    {
      "epoch": 2.589372624841656,
      "grad_norm": 2.072565793991089,
      "learning_rate": 4.151569506726458e-06,
      "loss": 1.318,
      "step": 9710
    },
    {
      "epoch": 2.592039469297953,
      "grad_norm": 3.0179076194763184,
      "learning_rate": 4.124663677130045e-06,
      "loss": 1.1683,
      "step": 9720
    },
    {
      "epoch": 2.5947063137542505,
      "grad_norm": 3.4961092472076416,
      "learning_rate": 4.0977578475336325e-06,
      "loss": 1.1181,
      "step": 9730
    },
    {
      "epoch": 2.5973731582105475,
      "grad_norm": 2.4178905487060547,
      "learning_rate": 4.07085201793722e-06,
      "loss": 1.2672,
      "step": 9740
    },
    {
      "epoch": 2.6000400026668444,
      "grad_norm": 2.4597389698028564,
      "learning_rate": 4.043946188340808e-06,
      "loss": 1.2022,
      "step": 9750
    },
    {
      "epoch": 2.602706847123142,
      "grad_norm": 2.2649662494659424,
      "learning_rate": 4.0170403587443945e-06,
      "loss": 1.1449,
      "step": 9760
    },
    {
      "epoch": 2.6053736915794388,
      "grad_norm": 2.6895334720611572,
      "learning_rate": 3.990134529147982e-06,
      "loss": 1.0355,
      "step": 9770
    },
    {
      "epoch": 2.6080405360357357,
      "grad_norm": 2.3963570594787598,
      "learning_rate": 3.96322869955157e-06,
      "loss": 1.0886,
      "step": 9780
    },
    {
      "epoch": 2.6107073804920327,
      "grad_norm": 2.7981741428375244,
      "learning_rate": 3.936322869955157e-06,
      "loss": 1.0917,
      "step": 9790
    },
    {
      "epoch": 2.61337422494833,
      "grad_norm": 2.3190295696258545,
      "learning_rate": 3.909417040358744e-06,
      "loss": 1.1662,
      "step": 9800
    },
    {
      "epoch": 2.616041069404627,
      "grad_norm": 3.544722557067871,
      "learning_rate": 3.8825112107623325e-06,
      "loss": 1.1648,
      "step": 9810
    },
    {
      "epoch": 2.618707913860924,
      "grad_norm": 3.3048272132873535,
      "learning_rate": 3.855605381165919e-06,
      "loss": 1.2353,
      "step": 9820
    },
    {
      "epoch": 2.621374758317221,
      "grad_norm": 2.3259522914886475,
      "learning_rate": 3.828699551569507e-06,
      "loss": 1.2844,
      "step": 9830
    },
    {
      "epoch": 2.6240416027735183,
      "grad_norm": 2.136876106262207,
      "learning_rate": 3.801793721973094e-06,
      "loss": 1.1306,
      "step": 9840
    },
    {
      "epoch": 2.6267084472298152,
      "grad_norm": 2.8800501823425293,
      "learning_rate": 3.774887892376682e-06,
      "loss": 1.12,
      "step": 9850
    },
    {
      "epoch": 2.629375291686112,
      "grad_norm": 2.797243356704712,
      "learning_rate": 3.7479820627802693e-06,
      "loss": 1.1806,
      "step": 9860
    },
    {
      "epoch": 2.6320421361424096,
      "grad_norm": 2.8868308067321777,
      "learning_rate": 3.721076233183857e-06,
      "loss": 1.2079,
      "step": 9870
    },
    {
      "epoch": 2.6347089805987065,
      "grad_norm": 2.2100460529327393,
      "learning_rate": 3.694170403587444e-06,
      "loss": 1.1228,
      "step": 9880
    },
    {
      "epoch": 2.6373758250550035,
      "grad_norm": 3.0815699100494385,
      "learning_rate": 3.6672645739910317e-06,
      "loss": 1.0703,
      "step": 9890
    },
    {
      "epoch": 2.640042669511301,
      "grad_norm": 2.243656635284424,
      "learning_rate": 3.640358744394619e-06,
      "loss": 1.0577,
      "step": 9900
    },
    {
      "epoch": 2.642709513967598,
      "grad_norm": 3.2441365718841553,
      "learning_rate": 3.6134529147982065e-06,
      "loss": 1.1968,
      "step": 9910
    },
    {
      "epoch": 2.6453763584238947,
      "grad_norm": 2.8218727111816406,
      "learning_rate": 3.586547085201794e-06,
      "loss": 1.2717,
      "step": 9920
    },
    {
      "epoch": 2.648043202880192,
      "grad_norm": 4.026326656341553,
      "learning_rate": 3.5596412556053812e-06,
      "loss": 0.9988,
      "step": 9930
    },
    {
      "epoch": 2.650710047336489,
      "grad_norm": 2.860651731491089,
      "learning_rate": 3.532735426008969e-06,
      "loss": 1.1482,
      "step": 9940
    },
    {
      "epoch": 2.653376891792786,
      "grad_norm": 2.3984596729278564,
      "learning_rate": 3.505829596412556e-06,
      "loss": 1.172,
      "step": 9950
    },
    {
      "epoch": 2.6560437362490834,
      "grad_norm": 2.4592912197113037,
      "learning_rate": 3.4789237668161436e-06,
      "loss": 1.1316,
      "step": 9960
    },
    {
      "epoch": 2.6587105807053804,
      "grad_norm": 1.885079026222229,
      "learning_rate": 3.4520179372197312e-06,
      "loss": 1.0691,
      "step": 9970
    },
    {
      "epoch": 2.6613774251616773,
      "grad_norm": 3.0545382499694824,
      "learning_rate": 3.4251121076233184e-06,
      "loss": 1.0794,
      "step": 9980
    },
    {
      "epoch": 2.6640442696179747,
      "grad_norm": 2.4165658950805664,
      "learning_rate": 3.398206278026906e-06,
      "loss": 1.2309,
      "step": 9990
    },
    {
      "epoch": 2.6667111140742716,
      "grad_norm": 3.030433416366577,
      "learning_rate": 3.3713004484304932e-06,
      "loss": 1.1667,
      "step": 10000
    },
    {
      "epoch": 2.6693779585305686,
      "grad_norm": 3.4747982025146484,
      "learning_rate": 3.344394618834081e-06,
      "loss": 1.1358,
      "step": 10010
    },
    {
      "epoch": 2.672044802986866,
      "grad_norm": 3.600294351577759,
      "learning_rate": 3.317488789237668e-06,
      "loss": 1.228,
      "step": 10020
    },
    {
      "epoch": 2.674711647443163,
      "grad_norm": 2.3750839233398438,
      "learning_rate": 3.2905829596412556e-06,
      "loss": 1.1467,
      "step": 10030
    },
    {
      "epoch": 2.67737849189946,
      "grad_norm": 2.913351535797119,
      "learning_rate": 3.2636771300448432e-06,
      "loss": 1.0753,
      "step": 10040
    },
    {
      "epoch": 2.6800453363557573,
      "grad_norm": 2.9002740383148193,
      "learning_rate": 3.2367713004484304e-06,
      "loss": 1.1782,
      "step": 10050
    },
    {
      "epoch": 2.682712180812054,
      "grad_norm": 2.5107016563415527,
      "learning_rate": 3.209865470852018e-06,
      "loss": 1.1212,
      "step": 10060
    },
    {
      "epoch": 2.685379025268351,
      "grad_norm": 2.5370240211486816,
      "learning_rate": 3.182959641255605e-06,
      "loss": 1.3185,
      "step": 10070
    },
    {
      "epoch": 2.6880458697246485,
      "grad_norm": 2.4518611431121826,
      "learning_rate": 3.156053811659193e-06,
      "loss": 1.1719,
      "step": 10080
    },
    {
      "epoch": 2.6907127141809455,
      "grad_norm": 2.289201021194458,
      "learning_rate": 3.1291479820627804e-06,
      "loss": 1.0587,
      "step": 10090
    },
    {
      "epoch": 2.6933795586372424,
      "grad_norm": 3.0007524490356445,
      "learning_rate": 3.1022421524663676e-06,
      "loss": 1.2369,
      "step": 10100
    },
    {
      "epoch": 2.69604640309354,
      "grad_norm": 1.9802520275115967,
      "learning_rate": 3.075336322869955e-06,
      "loss": 1.2441,
      "step": 10110
    },
    {
      "epoch": 2.6987132475498368,
      "grad_norm": 2.1836342811584473,
      "learning_rate": 3.0484304932735424e-06,
      "loss": 1.1291,
      "step": 10120
    },
    {
      "epoch": 2.7013800920061337,
      "grad_norm": 4.4299702644348145,
      "learning_rate": 3.02152466367713e-06,
      "loss": 1.1605,
      "step": 10130
    },
    {
      "epoch": 2.704046936462431,
      "grad_norm": 2.756664752960205,
      "learning_rate": 2.9946188340807176e-06,
      "loss": 1.2843,
      "step": 10140
    },
    {
      "epoch": 2.706713780918728,
      "grad_norm": 2.7023935317993164,
      "learning_rate": 2.9677130044843048e-06,
      "loss": 1.1761,
      "step": 10150
    },
    {
      "epoch": 2.709380625375025,
      "grad_norm": 2.4927661418914795,
      "learning_rate": 2.9408071748878924e-06,
      "loss": 1.1527,
      "step": 10160
    },
    {
      "epoch": 2.712047469831322,
      "grad_norm": 1.7020825147628784,
      "learning_rate": 2.9139013452914796e-06,
      "loss": 1.1883,
      "step": 10170
    },
    {
      "epoch": 2.7147143142876193,
      "grad_norm": 2.5925612449645996,
      "learning_rate": 2.8869955156950676e-06,
      "loss": 1.1093,
      "step": 10180
    },
    {
      "epoch": 2.7173811587439163,
      "grad_norm": 2.9127655029296875,
      "learning_rate": 2.8600896860986548e-06,
      "loss": 1.0828,
      "step": 10190
    },
    {
      "epoch": 2.7200480032002132,
      "grad_norm": 2.052640914916992,
      "learning_rate": 2.8331838565022424e-06,
      "loss": 1.1814,
      "step": 10200
    },
    {
      "epoch": 2.72271484765651,
      "grad_norm": 3.128626585006714,
      "learning_rate": 2.80627802690583e-06,
      "loss": 1.234,
      "step": 10210
    },
    {
      "epoch": 2.7253816921128076,
      "grad_norm": 2.691026210784912,
      "learning_rate": 2.779372197309417e-06,
      "loss": 0.9556,
      "step": 10220
    },
    {
      "epoch": 2.7280485365691045,
      "grad_norm": 3.0415518283843994,
      "learning_rate": 2.7524663677130048e-06,
      "loss": 1.1861,
      "step": 10230
    },
    {
      "epoch": 2.7307153810254015,
      "grad_norm": 2.5313634872436523,
      "learning_rate": 2.725560538116592e-06,
      "loss": 1.2342,
      "step": 10240
    },
    {
      "epoch": 2.733382225481699,
      "grad_norm": 2.8844213485717773,
      "learning_rate": 2.6986547085201796e-06,
      "loss": 1.1419,
      "step": 10250
    },
    {
      "epoch": 2.736049069937996,
      "grad_norm": 3.251530647277832,
      "learning_rate": 2.671748878923767e-06,
      "loss": 1.3261,
      "step": 10260
    },
    {
      "epoch": 2.7387159143942927,
      "grad_norm": 3.1987807750701904,
      "learning_rate": 2.6448430493273544e-06,
      "loss": 1.0488,
      "step": 10270
    },
    {
      "epoch": 2.74138275885059,
      "grad_norm": 2.5016558170318604,
      "learning_rate": 2.617937219730942e-06,
      "loss": 1.1626,
      "step": 10280
    },
    {
      "epoch": 2.744049603306887,
      "grad_norm": 2.4074039459228516,
      "learning_rate": 2.591031390134529e-06,
      "loss": 1.0887,
      "step": 10290
    },
    {
      "epoch": 2.746716447763184,
      "grad_norm": 3.0816638469696045,
      "learning_rate": 2.5641255605381168e-06,
      "loss": 1.1298,
      "step": 10300
    },
    {
      "epoch": 2.7493832922194814,
      "grad_norm": 2.548037528991699,
      "learning_rate": 2.537219730941704e-06,
      "loss": 1.0419,
      "step": 10310
    },
    {
      "epoch": 2.7520501366757784,
      "grad_norm": 2.269468069076538,
      "learning_rate": 2.5103139013452916e-06,
      "loss": 1.1542,
      "step": 10320
    },
    {
      "epoch": 2.7547169811320753,
      "grad_norm": 2.940842390060425,
      "learning_rate": 2.483408071748879e-06,
      "loss": 1.1439,
      "step": 10330
    },
    {
      "epoch": 2.7573838255883727,
      "grad_norm": 2.181520462036133,
      "learning_rate": 2.4565022421524663e-06,
      "loss": 1.2569,
      "step": 10340
    },
    {
      "epoch": 2.7600506700446696,
      "grad_norm": 2.0268654823303223,
      "learning_rate": 2.429596412556054e-06,
      "loss": 1.1011,
      "step": 10350
    },
    {
      "epoch": 2.7627175145009666,
      "grad_norm": 2.5569818019866943,
      "learning_rate": 2.402690582959641e-06,
      "loss": 1.0515,
      "step": 10360
    },
    {
      "epoch": 2.765384358957264,
      "grad_norm": 3.049852132797241,
      "learning_rate": 2.3757847533632287e-06,
      "loss": 1.1155,
      "step": 10370
    },
    {
      "epoch": 2.768051203413561,
      "grad_norm": 2.8757314682006836,
      "learning_rate": 2.3488789237668163e-06,
      "loss": 1.2136,
      "step": 10380
    },
    {
      "epoch": 2.770718047869858,
      "grad_norm": 3.423346996307373,
      "learning_rate": 2.3219730941704035e-06,
      "loss": 1.3263,
      "step": 10390
    },
    {
      "epoch": 2.7733848923261553,
      "grad_norm": 2.4912967681884766,
      "learning_rate": 2.295067264573991e-06,
      "loss": 1.1223,
      "step": 10400
    },
    {
      "epoch": 2.776051736782452,
      "grad_norm": 2.3035285472869873,
      "learning_rate": 2.2681614349775783e-06,
      "loss": 1.2059,
      "step": 10410
    },
    {
      "epoch": 2.778718581238749,
      "grad_norm": 2.172085762023926,
      "learning_rate": 2.241255605381166e-06,
      "loss": 1.1715,
      "step": 10420
    },
    {
      "epoch": 2.7813854256950465,
      "grad_norm": 2.6155855655670166,
      "learning_rate": 2.2143497757847535e-06,
      "loss": 1.1561,
      "step": 10430
    },
    {
      "epoch": 2.7840522701513435,
      "grad_norm": 2.925062417984009,
      "learning_rate": 2.1874439461883407e-06,
      "loss": 1.198,
      "step": 10440
    },
    {
      "epoch": 2.7867191146076404,
      "grad_norm": 3.899247407913208,
      "learning_rate": 2.1605381165919283e-06,
      "loss": 1.2692,
      "step": 10450
    },
    {
      "epoch": 2.789385959063938,
      "grad_norm": 3.100860595703125,
      "learning_rate": 2.1336322869955155e-06,
      "loss": 1.0988,
      "step": 10460
    },
    {
      "epoch": 2.7920528035202348,
      "grad_norm": 2.9115264415740967,
      "learning_rate": 2.106726457399103e-06,
      "loss": 1.2223,
      "step": 10470
    },
    {
      "epoch": 2.7947196479765317,
      "grad_norm": 4.322556972503662,
      "learning_rate": 2.0798206278026903e-06,
      "loss": 1.0905,
      "step": 10480
    },
    {
      "epoch": 2.797386492432829,
      "grad_norm": 3.3862032890319824,
      "learning_rate": 2.052914798206278e-06,
      "loss": 1.1523,
      "step": 10490
    },
    {
      "epoch": 2.800053336889126,
      "grad_norm": 3.4435458183288574,
      "learning_rate": 2.0260089686098655e-06,
      "loss": 1.0938,
      "step": 10500
    },
    {
      "epoch": 2.802720181345423,
      "grad_norm": 2.155778408050537,
      "learning_rate": 1.9991031390134527e-06,
      "loss": 1.1981,
      "step": 10510
    },
    {
      "epoch": 2.8053870258017204,
      "grad_norm": 2.962864637374878,
      "learning_rate": 1.9721973094170403e-06,
      "loss": 1.0884,
      "step": 10520
    },
    {
      "epoch": 2.8080538702580173,
      "grad_norm": 2.931657075881958,
      "learning_rate": 1.9452914798206275e-06,
      "loss": 1.1795,
      "step": 10530
    },
    {
      "epoch": 2.8107207147143143,
      "grad_norm": 3.44854474067688,
      "learning_rate": 1.9183856502242155e-06,
      "loss": 1.2484,
      "step": 10540
    },
    {
      "epoch": 2.8133875591706112,
      "grad_norm": 3.0805468559265137,
      "learning_rate": 1.891479820627803e-06,
      "loss": 1.1629,
      "step": 10550
    },
    {
      "epoch": 2.8160544036269086,
      "grad_norm": 2.999842643737793,
      "learning_rate": 1.86457399103139e-06,
      "loss": 1.1106,
      "step": 10560
    },
    {
      "epoch": 2.8187212480832056,
      "grad_norm": 2.4072210788726807,
      "learning_rate": 1.8376681614349775e-06,
      "loss": 1.1934,
      "step": 10570
    },
    {
      "epoch": 2.8213880925395025,
      "grad_norm": 1.7679049968719482,
      "learning_rate": 1.810762331838565e-06,
      "loss": 1.056,
      "step": 10580
    },
    {
      "epoch": 2.8240549369957995,
      "grad_norm": 2.886883497238159,
      "learning_rate": 1.7838565022421525e-06,
      "loss": 1.2163,
      "step": 10590
    },
    {
      "epoch": 2.826721781452097,
      "grad_norm": 3.0574898719787598,
      "learning_rate": 1.7569506726457399e-06,
      "loss": 1.1369,
      "step": 10600
    },
    {
      "epoch": 2.829388625908394,
      "grad_norm": 2.3859548568725586,
      "learning_rate": 1.7300448430493273e-06,
      "loss": 1.1547,
      "step": 10610
    },
    {
      "epoch": 2.8320554703646907,
      "grad_norm": 2.099215269088745,
      "learning_rate": 1.7031390134529147e-06,
      "loss": 1.1174,
      "step": 10620
    },
    {
      "epoch": 2.834722314820988,
      "grad_norm": 3.5409469604492188,
      "learning_rate": 1.6762331838565025e-06,
      "loss": 1.1076,
      "step": 10630
    },
    {
      "epoch": 2.837389159277285,
      "grad_norm": 2.5342471599578857,
      "learning_rate": 1.6493273542600899e-06,
      "loss": 1.1189,
      "step": 10640
    },
    {
      "epoch": 2.840056003733582,
      "grad_norm": 1.9513754844665527,
      "learning_rate": 1.6224215246636773e-06,
      "loss": 1.1418,
      "step": 10650
    },
    {
      "epoch": 2.8427228481898794,
      "grad_norm": 3.1357197761535645,
      "learning_rate": 1.5955156950672647e-06,
      "loss": 1.1622,
      "step": 10660
    },
    {
      "epoch": 2.8453896926461764,
      "grad_norm": 2.7394142150878906,
      "learning_rate": 1.568609865470852e-06,
      "loss": 1.1981,
      "step": 10670
    },
    {
      "epoch": 2.8480565371024733,
      "grad_norm": 2.4692745208740234,
      "learning_rate": 1.5417040358744395e-06,
      "loss": 1.0772,
      "step": 10680
    },
    {
      "epoch": 2.8507233815587707,
      "grad_norm": 6.225238800048828,
      "learning_rate": 1.514798206278027e-06,
      "loss": 1.0357,
      "step": 10690
    },
    {
      "epoch": 2.8533902260150676,
      "grad_norm": 3.2283992767333984,
      "learning_rate": 1.4878923766816145e-06,
      "loss": 1.1843,
      "step": 10700
    },
    {
      "epoch": 2.8560570704713646,
      "grad_norm": 1.9967641830444336,
      "learning_rate": 1.4609865470852019e-06,
      "loss": 1.2578,
      "step": 10710
    },
    {
      "epoch": 2.858723914927662,
      "grad_norm": 2.1260170936584473,
      "learning_rate": 1.4340807174887893e-06,
      "loss": 1.2262,
      "step": 10720
    },
    {
      "epoch": 2.861390759383959,
      "grad_norm": 2.1185226440429688,
      "learning_rate": 1.4071748878923766e-06,
      "loss": 1.1417,
      "step": 10730
    },
    {
      "epoch": 2.864057603840256,
      "grad_norm": 2.5325498580932617,
      "learning_rate": 1.380269058295964e-06,
      "loss": 1.1026,
      "step": 10740
    },
    {
      "epoch": 2.8667244482965533,
      "grad_norm": 2.433711051940918,
      "learning_rate": 1.3533632286995516e-06,
      "loss": 1.097,
      "step": 10750
    },
    {
      "epoch": 2.86939129275285,
      "grad_norm": 3.039743423461914,
      "learning_rate": 1.326457399103139e-06,
      "loss": 1.2178,
      "step": 10760
    },
    {
      "epoch": 2.872058137209147,
      "grad_norm": 1.946292519569397,
      "learning_rate": 1.2995515695067264e-06,
      "loss": 1.0645,
      "step": 10770
    },
    {
      "epoch": 2.8747249816654445,
      "grad_norm": 2.337750196456909,
      "learning_rate": 1.2726457399103138e-06,
      "loss": 1.0492,
      "step": 10780
    },
    {
      "epoch": 2.8773918261217415,
      "grad_norm": 2.5311994552612305,
      "learning_rate": 1.2457399103139012e-06,
      "loss": 1.2707,
      "step": 10790
    },
    {
      "epoch": 2.8800586705780384,
      "grad_norm": 2.010145425796509,
      "learning_rate": 1.2188340807174886e-06,
      "loss": 1.2817,
      "step": 10800
    },
    {
      "epoch": 2.882725515034336,
      "grad_norm": 2.2405753135681152,
      "learning_rate": 1.1919282511210764e-06,
      "loss": 1.1837,
      "step": 10810
    },
    {
      "epoch": 2.8853923594906328,
      "grad_norm": 3.7367160320281982,
      "learning_rate": 1.1650224215246638e-06,
      "loss": 1.1121,
      "step": 10820
    },
    {
      "epoch": 2.8880592039469297,
      "grad_norm": 2.4815337657928467,
      "learning_rate": 1.1381165919282512e-06,
      "loss": 1.0976,
      "step": 10830
    },
    {
      "epoch": 2.890726048403227,
      "grad_norm": 2.6562983989715576,
      "learning_rate": 1.1112107623318386e-06,
      "loss": 1.021,
      "step": 10840
    },
    {
      "epoch": 2.893392892859524,
      "grad_norm": 2.2547097206115723,
      "learning_rate": 1.084304932735426e-06,
      "loss": 1.062,
      "step": 10850
    },
    {
      "epoch": 2.896059737315821,
      "grad_norm": 2.68965482711792,
      "learning_rate": 1.0573991031390134e-06,
      "loss": 1.1177,
      "step": 10860
    },
    {
      "epoch": 2.8987265817721184,
      "grad_norm": 2.6160802841186523,
      "learning_rate": 1.030493273542601e-06,
      "loss": 1.2177,
      "step": 10870
    },
    {
      "epoch": 2.9013934262284153,
      "grad_norm": 1.8172169923782349,
      "learning_rate": 1.0035874439461884e-06,
      "loss": 1.0726,
      "step": 10880
    },
    {
      "epoch": 2.9040602706847123,
      "grad_norm": 2.5130908489227295,
      "learning_rate": 9.766816143497758e-07,
      "loss": 1.0862,
      "step": 10890
    },
    {
      "epoch": 2.9067271151410097,
      "grad_norm": 2.7682321071624756,
      "learning_rate": 9.497757847533632e-07,
      "loss": 1.0744,
      "step": 10900
    },
    {
      "epoch": 2.9093939595973066,
      "grad_norm": 2.79093074798584,
      "learning_rate": 9.228699551569507e-07,
      "loss": 1.1213,
      "step": 10910
    },
    {
      "epoch": 2.9120608040536036,
      "grad_norm": 2.5322628021240234,
      "learning_rate": 8.959641255605381e-07,
      "loss": 1.1778,
      "step": 10920
    },
    {
      "epoch": 2.9147276485099005,
      "grad_norm": 2.6522068977355957,
      "learning_rate": 8.690582959641255e-07,
      "loss": 1.1789,
      "step": 10930
    },
    {
      "epoch": 2.917394492966198,
      "grad_norm": 2.3746213912963867,
      "learning_rate": 8.421524663677131e-07,
      "loss": 1.0653,
      "step": 10940
    },
    {
      "epoch": 2.920061337422495,
      "grad_norm": 2.1808292865753174,
      "learning_rate": 8.152466367713005e-07,
      "loss": 1.1881,
      "step": 10950
    },
    {
      "epoch": 2.922728181878792,
      "grad_norm": 2.2905173301696777,
      "learning_rate": 7.883408071748879e-07,
      "loss": 1.1477,
      "step": 10960
    },
    {
      "epoch": 2.9253950263350887,
      "grad_norm": 3.6060049533843994,
      "learning_rate": 7.614349775784754e-07,
      "loss": 1.1245,
      "step": 10970
    },
    {
      "epoch": 2.928061870791386,
      "grad_norm": 2.9563422203063965,
      "learning_rate": 7.345291479820628e-07,
      "loss": 1.207,
      "step": 10980
    },
    {
      "epoch": 2.930728715247683,
      "grad_norm": 3.662240505218506,
      "learning_rate": 7.076233183856502e-07,
      "loss": 1.2703,
      "step": 10990
    },
    {
      "epoch": 2.93339555970398,
      "grad_norm": 2.891148805618286,
      "learning_rate": 6.807174887892377e-07,
      "loss": 1.0839,
      "step": 11000
    }
  ],
  "logging_steps": 10,
  "max_steps": 11250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.515676072941978e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
