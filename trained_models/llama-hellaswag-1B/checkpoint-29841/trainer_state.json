{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 29841,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001005378776454029,
      "grad_norm": 0.8593980073928833,
      "learning_rate": 2.7e-06,
      "loss": 3.1642,
      "step": 10
    },
    {
      "epoch": 0.002010757552908058,
      "grad_norm": 0.7727442979812622,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 3.1501,
      "step": 20
    },
    {
      "epoch": 0.003016136329362087,
      "grad_norm": 0.6269122362136841,
      "learning_rate": 8.7e-06,
      "loss": 3.1639,
      "step": 30
    },
    {
      "epoch": 0.004021515105816116,
      "grad_norm": 0.9767593145370483,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 3.1525,
      "step": 40
    },
    {
      "epoch": 0.005026893882270145,
      "grad_norm": 0.8241017460823059,
      "learning_rate": 1.47e-05,
      "loss": 3.0079,
      "step": 50
    },
    {
      "epoch": 0.006032272658724174,
      "grad_norm": 0.776248037815094,
      "learning_rate": 1.77e-05,
      "loss": 3.08,
      "step": 60
    },
    {
      "epoch": 0.007037651435178203,
      "grad_norm": 1.1589140892028809,
      "learning_rate": 2.07e-05,
      "loss": 2.9999,
      "step": 70
    },
    {
      "epoch": 0.008043030211632232,
      "grad_norm": 0.8926935791969299,
      "learning_rate": 2.37e-05,
      "loss": 2.9778,
      "step": 80
    },
    {
      "epoch": 0.009048408988086261,
      "grad_norm": 1.0418747663497925,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.8463,
      "step": 90
    },
    {
      "epoch": 0.01005378776454029,
      "grad_norm": 1.1681439876556396,
      "learning_rate": 2.97e-05,
      "loss": 2.7174,
      "step": 100
    },
    {
      "epoch": 0.01105916654099432,
      "grad_norm": 1.2794231176376343,
      "learning_rate": 2.999092162334824e-05,
      "loss": 2.6609,
      "step": 110
    },
    {
      "epoch": 0.012064545317448348,
      "grad_norm": 1.8364795446395874,
      "learning_rate": 2.9980834538179617e-05,
      "loss": 2.54,
      "step": 120
    },
    {
      "epoch": 0.013069924093902377,
      "grad_norm": 2.260150671005249,
      "learning_rate": 2.9970747453010997e-05,
      "loss": 2.4438,
      "step": 130
    },
    {
      "epoch": 0.014075302870356406,
      "grad_norm": 2.1845273971557617,
      "learning_rate": 2.9960660367842373e-05,
      "loss": 2.2981,
      "step": 140
    },
    {
      "epoch": 0.015080681646810435,
      "grad_norm": 1.7204320430755615,
      "learning_rate": 2.9950573282673753e-05,
      "loss": 2.2776,
      "step": 150
    },
    {
      "epoch": 0.016086060423264464,
      "grad_norm": 2.0795083045959473,
      "learning_rate": 2.9940486197505126e-05,
      "loss": 2.1237,
      "step": 160
    },
    {
      "epoch": 0.017091439199718495,
      "grad_norm": 1.9029721021652222,
      "learning_rate": 2.9930399112336506e-05,
      "loss": 2.1247,
      "step": 170
    },
    {
      "epoch": 0.018096817976172522,
      "grad_norm": 1.551301121711731,
      "learning_rate": 2.9920312027167882e-05,
      "loss": 2.0542,
      "step": 180
    },
    {
      "epoch": 0.019102196752626553,
      "grad_norm": 2.1221811771392822,
      "learning_rate": 2.9910224941999262e-05,
      "loss": 2.091,
      "step": 190
    },
    {
      "epoch": 0.02010757552908058,
      "grad_norm": 2.2380731105804443,
      "learning_rate": 2.9900137856830638e-05,
      "loss": 1.9894,
      "step": 200
    },
    {
      "epoch": 0.02111295430553461,
      "grad_norm": 1.5181841850280762,
      "learning_rate": 2.9890050771662018e-05,
      "loss": 2.0789,
      "step": 210
    },
    {
      "epoch": 0.02211833308198864,
      "grad_norm": 1.7441436052322388,
      "learning_rate": 2.987996368649339e-05,
      "loss": 2.0543,
      "step": 220
    },
    {
      "epoch": 0.02312371185844267,
      "grad_norm": 1.740822672843933,
      "learning_rate": 2.986987660132477e-05,
      "loss": 1.9761,
      "step": 230
    },
    {
      "epoch": 0.024129090634896697,
      "grad_norm": 2.070430040359497,
      "learning_rate": 2.985978951615615e-05,
      "loss": 1.9652,
      "step": 240
    },
    {
      "epoch": 0.025134469411350727,
      "grad_norm": 6.052666187286377,
      "learning_rate": 2.9849702430987526e-05,
      "loss": 2.0564,
      "step": 250
    },
    {
      "epoch": 0.026139848187804755,
      "grad_norm": 2.084456443786621,
      "learning_rate": 2.9839615345818906e-05,
      "loss": 2.0077,
      "step": 260
    },
    {
      "epoch": 0.027145226964258785,
      "grad_norm": 1.9908347129821777,
      "learning_rate": 2.9829528260650282e-05,
      "loss": 1.9719,
      "step": 270
    },
    {
      "epoch": 0.028150605740712813,
      "grad_norm": 1.6231478452682495,
      "learning_rate": 2.981944117548166e-05,
      "loss": 1.9569,
      "step": 280
    },
    {
      "epoch": 0.029155984517166843,
      "grad_norm": 1.6709409952163696,
      "learning_rate": 2.9809354090313035e-05,
      "loss": 1.9276,
      "step": 290
    },
    {
      "epoch": 0.03016136329362087,
      "grad_norm": 2.623640537261963,
      "learning_rate": 2.9799267005144415e-05,
      "loss": 1.9902,
      "step": 300
    },
    {
      "epoch": 0.0311667420700749,
      "grad_norm": 2.0517513751983643,
      "learning_rate": 2.978917991997579e-05,
      "loss": 1.9831,
      "step": 310
    },
    {
      "epoch": 0.03217212084652893,
      "grad_norm": 1.8148112297058105,
      "learning_rate": 2.977909283480717e-05,
      "loss": 1.9238,
      "step": 320
    },
    {
      "epoch": 0.033177499622982956,
      "grad_norm": 1.5474315881729126,
      "learning_rate": 2.9769005749638547e-05,
      "loss": 1.8883,
      "step": 330
    },
    {
      "epoch": 0.03418287839943699,
      "grad_norm": 4.040038108825684,
      "learning_rate": 2.9758918664469923e-05,
      "loss": 1.9588,
      "step": 340
    },
    {
      "epoch": 0.03518825717589102,
      "grad_norm": 2.3740382194519043,
      "learning_rate": 2.97488315793013e-05,
      "loss": 1.8979,
      "step": 350
    },
    {
      "epoch": 0.036193635952345045,
      "grad_norm": 4.3878350257873535,
      "learning_rate": 2.973874449413268e-05,
      "loss": 1.8878,
      "step": 360
    },
    {
      "epoch": 0.03719901472879907,
      "grad_norm": 2.0777432918548584,
      "learning_rate": 2.9728657408964056e-05,
      "loss": 1.8937,
      "step": 370
    },
    {
      "epoch": 0.038204393505253106,
      "grad_norm": 2.0318732261657715,
      "learning_rate": 2.9718570323795435e-05,
      "loss": 1.9131,
      "step": 380
    },
    {
      "epoch": 0.039209772281707134,
      "grad_norm": 1.5625004768371582,
      "learning_rate": 2.9708483238626812e-05,
      "loss": 1.9574,
      "step": 390
    },
    {
      "epoch": 0.04021515105816116,
      "grad_norm": 1.7622026205062866,
      "learning_rate": 2.9698396153458188e-05,
      "loss": 1.9284,
      "step": 400
    },
    {
      "epoch": 0.04122052983461519,
      "grad_norm": 1.9412175416946411,
      "learning_rate": 2.9688309068289568e-05,
      "loss": 1.8512,
      "step": 410
    },
    {
      "epoch": 0.04222590861106922,
      "grad_norm": 2.27590012550354,
      "learning_rate": 2.9678221983120944e-05,
      "loss": 1.8684,
      "step": 420
    },
    {
      "epoch": 0.04323128738752325,
      "grad_norm": 2.1942083835601807,
      "learning_rate": 2.9668134897952324e-05,
      "loss": 1.9106,
      "step": 430
    },
    {
      "epoch": 0.04423666616397728,
      "grad_norm": 2.364565849304199,
      "learning_rate": 2.96580478127837e-05,
      "loss": 1.8786,
      "step": 440
    },
    {
      "epoch": 0.045242044940431304,
      "grad_norm": 2.299863576889038,
      "learning_rate": 2.964796072761508e-05,
      "loss": 1.9216,
      "step": 450
    },
    {
      "epoch": 0.04624742371688534,
      "grad_norm": 2.717315673828125,
      "learning_rate": 2.9637873642446453e-05,
      "loss": 2.0376,
      "step": 460
    },
    {
      "epoch": 0.047252802493339366,
      "grad_norm": 2.9010121822357178,
      "learning_rate": 2.9627786557277832e-05,
      "loss": 1.8875,
      "step": 470
    },
    {
      "epoch": 0.04825818126979339,
      "grad_norm": 3.459165096282959,
      "learning_rate": 2.961769947210921e-05,
      "loss": 1.8775,
      "step": 480
    },
    {
      "epoch": 0.04926356004624742,
      "grad_norm": 2.1607155799865723,
      "learning_rate": 2.960761238694059e-05,
      "loss": 1.8438,
      "step": 490
    },
    {
      "epoch": 0.050268938822701455,
      "grad_norm": 2.374803304672241,
      "learning_rate": 2.9597525301771965e-05,
      "loss": 1.9616,
      "step": 500
    },
    {
      "epoch": 0.05127431759915548,
      "grad_norm": 3.351806640625,
      "learning_rate": 2.9587438216603345e-05,
      "loss": 1.9196,
      "step": 510
    },
    {
      "epoch": 0.05227969637560951,
      "grad_norm": 3.412476062774658,
      "learning_rate": 2.9577351131434717e-05,
      "loss": 1.9511,
      "step": 520
    },
    {
      "epoch": 0.05328507515206354,
      "grad_norm": 2.278491497039795,
      "learning_rate": 2.9567264046266097e-05,
      "loss": 1.8831,
      "step": 530
    },
    {
      "epoch": 0.05429045392851757,
      "grad_norm": 1.9114049673080444,
      "learning_rate": 2.9557176961097473e-05,
      "loss": 1.8968,
      "step": 540
    },
    {
      "epoch": 0.0552958327049716,
      "grad_norm": 2.1632630825042725,
      "learning_rate": 2.9547089875928853e-05,
      "loss": 1.8829,
      "step": 550
    },
    {
      "epoch": 0.056301211481425625,
      "grad_norm": 2.003317356109619,
      "learning_rate": 2.9537002790760233e-05,
      "loss": 1.8392,
      "step": 560
    },
    {
      "epoch": 0.05730659025787966,
      "grad_norm": 2.0191147327423096,
      "learning_rate": 2.952691570559161e-05,
      "loss": 1.8904,
      "step": 570
    },
    {
      "epoch": 0.05831196903433369,
      "grad_norm": 1.8793458938598633,
      "learning_rate": 2.9516828620422985e-05,
      "loss": 1.7876,
      "step": 580
    },
    {
      "epoch": 0.059317347810787714,
      "grad_norm": 1.7716037034988403,
      "learning_rate": 2.9506741535254362e-05,
      "loss": 1.8977,
      "step": 590
    },
    {
      "epoch": 0.06032272658724174,
      "grad_norm": 2.1119182109832764,
      "learning_rate": 2.949665445008574e-05,
      "loss": 2.0306,
      "step": 600
    },
    {
      "epoch": 0.061328105363695776,
      "grad_norm": 2.3468878269195557,
      "learning_rate": 2.9486567364917118e-05,
      "loss": 1.8606,
      "step": 610
    },
    {
      "epoch": 0.0623334841401498,
      "grad_norm": 2.4917094707489014,
      "learning_rate": 2.9476480279748498e-05,
      "loss": 1.9162,
      "step": 620
    },
    {
      "epoch": 0.06333886291660383,
      "grad_norm": 2.21484112739563,
      "learning_rate": 2.9466393194579874e-05,
      "loss": 1.8228,
      "step": 630
    },
    {
      "epoch": 0.06434424169305786,
      "grad_norm": 1.9754680395126343,
      "learning_rate": 2.945630610941125e-05,
      "loss": 1.9436,
      "step": 640
    },
    {
      "epoch": 0.06534962046951188,
      "grad_norm": 3.286773204803467,
      "learning_rate": 2.9446219024242626e-05,
      "loss": 1.8704,
      "step": 650
    },
    {
      "epoch": 0.06635499924596591,
      "grad_norm": 2.6736907958984375,
      "learning_rate": 2.9436131939074006e-05,
      "loss": 1.8686,
      "step": 660
    },
    {
      "epoch": 0.06736037802241995,
      "grad_norm": 2.365555763244629,
      "learning_rate": 2.9426044853905382e-05,
      "loss": 1.8143,
      "step": 670
    },
    {
      "epoch": 0.06836575679887398,
      "grad_norm": 2.2626407146453857,
      "learning_rate": 2.9415957768736762e-05,
      "loss": 1.9594,
      "step": 680
    },
    {
      "epoch": 0.06937113557532801,
      "grad_norm": 1.8847929239273071,
      "learning_rate": 2.940587068356814e-05,
      "loss": 1.9475,
      "step": 690
    },
    {
      "epoch": 0.07037651435178204,
      "grad_norm": 2.748100996017456,
      "learning_rate": 2.9395783598399515e-05,
      "loss": 1.8207,
      "step": 700
    },
    {
      "epoch": 0.07138189312823606,
      "grad_norm": 2.5311920642852783,
      "learning_rate": 2.9385696513230894e-05,
      "loss": 1.8115,
      "step": 710
    },
    {
      "epoch": 0.07238727190469009,
      "grad_norm": 2.2166876792907715,
      "learning_rate": 2.937560942806227e-05,
      "loss": 1.8946,
      "step": 720
    },
    {
      "epoch": 0.07339265068114412,
      "grad_norm": 2.5593111515045166,
      "learning_rate": 2.936552234289365e-05,
      "loss": 1.8144,
      "step": 730
    },
    {
      "epoch": 0.07439802945759814,
      "grad_norm": 2.5342233180999756,
      "learning_rate": 2.9355435257725027e-05,
      "loss": 1.8937,
      "step": 740
    },
    {
      "epoch": 0.07540340823405219,
      "grad_norm": 1.690926194190979,
      "learning_rate": 2.9345348172556407e-05,
      "loss": 1.8166,
      "step": 750
    },
    {
      "epoch": 0.07640878701050621,
      "grad_norm": 2.2186062335968018,
      "learning_rate": 2.9335261087387783e-05,
      "loss": 1.845,
      "step": 760
    },
    {
      "epoch": 0.07741416578696024,
      "grad_norm": 3.037296772003174,
      "learning_rate": 2.932517400221916e-05,
      "loss": 1.8356,
      "step": 770
    },
    {
      "epoch": 0.07841954456341427,
      "grad_norm": 2.897390127182007,
      "learning_rate": 2.9315086917050535e-05,
      "loss": 1.8043,
      "step": 780
    },
    {
      "epoch": 0.0794249233398683,
      "grad_norm": 2.1524951457977295,
      "learning_rate": 2.9304999831881915e-05,
      "loss": 1.8512,
      "step": 790
    },
    {
      "epoch": 0.08043030211632232,
      "grad_norm": 2.2488296031951904,
      "learning_rate": 2.929491274671329e-05,
      "loss": 1.8327,
      "step": 800
    },
    {
      "epoch": 0.08143568089277635,
      "grad_norm": 2.28145432472229,
      "learning_rate": 2.928482566154467e-05,
      "loss": 1.9092,
      "step": 810
    },
    {
      "epoch": 0.08244105966923038,
      "grad_norm": 1.9267644882202148,
      "learning_rate": 2.9274738576376047e-05,
      "loss": 1.8301,
      "step": 820
    },
    {
      "epoch": 0.08344643844568442,
      "grad_norm": 2.260857105255127,
      "learning_rate": 2.9264651491207424e-05,
      "loss": 1.804,
      "step": 830
    },
    {
      "epoch": 0.08445181722213844,
      "grad_norm": 2.8524410724639893,
      "learning_rate": 2.92545644060388e-05,
      "loss": 1.8006,
      "step": 840
    },
    {
      "epoch": 0.08545719599859247,
      "grad_norm": 1.953039526939392,
      "learning_rate": 2.924447732087018e-05,
      "loss": 1.9006,
      "step": 850
    },
    {
      "epoch": 0.0864625747750465,
      "grad_norm": 2.4916329383850098,
      "learning_rate": 2.923439023570156e-05,
      "loss": 1.7838,
      "step": 860
    },
    {
      "epoch": 0.08746795355150053,
      "grad_norm": 2.1277213096618652,
      "learning_rate": 2.9224303150532936e-05,
      "loss": 1.8433,
      "step": 870
    },
    {
      "epoch": 0.08847333232795455,
      "grad_norm": 1.979656457901001,
      "learning_rate": 2.9214216065364316e-05,
      "loss": 1.9034,
      "step": 880
    },
    {
      "epoch": 0.08947871110440858,
      "grad_norm": 2.2034006118774414,
      "learning_rate": 2.920412898019569e-05,
      "loss": 1.8532,
      "step": 890
    },
    {
      "epoch": 0.09048408988086261,
      "grad_norm": 2.1270463466644287,
      "learning_rate": 2.9194041895027068e-05,
      "loss": 1.883,
      "step": 900
    },
    {
      "epoch": 0.09148946865731665,
      "grad_norm": 2.72247576713562,
      "learning_rate": 2.9183954809858444e-05,
      "loss": 1.8433,
      "step": 910
    },
    {
      "epoch": 0.09249484743377068,
      "grad_norm": 2.994781970977783,
      "learning_rate": 2.9173867724689824e-05,
      "loss": 1.8111,
      "step": 920
    },
    {
      "epoch": 0.0935002262102247,
      "grad_norm": 2.503239154815674,
      "learning_rate": 2.91637806395212e-05,
      "loss": 1.7924,
      "step": 930
    },
    {
      "epoch": 0.09450560498667873,
      "grad_norm": 2.7220842838287354,
      "learning_rate": 2.915369355435258e-05,
      "loss": 1.7851,
      "step": 940
    },
    {
      "epoch": 0.09551098376313276,
      "grad_norm": 2.1322154998779297,
      "learning_rate": 2.9143606469183953e-05,
      "loss": 1.8162,
      "step": 950
    },
    {
      "epoch": 0.09651636253958679,
      "grad_norm": 1.8680012226104736,
      "learning_rate": 2.9133519384015333e-05,
      "loss": 1.8474,
      "step": 960
    },
    {
      "epoch": 0.09752174131604081,
      "grad_norm": 2.2748868465423584,
      "learning_rate": 2.912343229884671e-05,
      "loss": 1.8492,
      "step": 970
    },
    {
      "epoch": 0.09852712009249484,
      "grad_norm": 2.70405912399292,
      "learning_rate": 2.911334521367809e-05,
      "loss": 1.8081,
      "step": 980
    },
    {
      "epoch": 0.09953249886894888,
      "grad_norm": 2.4465715885162354,
      "learning_rate": 2.9103258128509465e-05,
      "loss": 1.8874,
      "step": 990
    },
    {
      "epoch": 0.10053787764540291,
      "grad_norm": 1.9776325225830078,
      "learning_rate": 2.9093171043340845e-05,
      "loss": 1.8274,
      "step": 1000
    },
    {
      "epoch": 0.10154325642185694,
      "grad_norm": 2.844968318939209,
      "learning_rate": 2.908308395817222e-05,
      "loss": 1.8483,
      "step": 1010
    },
    {
      "epoch": 0.10254863519831096,
      "grad_norm": 3.142763137817383,
      "learning_rate": 2.9072996873003597e-05,
      "loss": 1.8754,
      "step": 1020
    },
    {
      "epoch": 0.10355401397476499,
      "grad_norm": 2.305720329284668,
      "learning_rate": 2.9062909787834977e-05,
      "loss": 1.7733,
      "step": 1030
    },
    {
      "epoch": 0.10455939275121902,
      "grad_norm": 2.5930662155151367,
      "learning_rate": 2.9052822702666353e-05,
      "loss": 1.8062,
      "step": 1040
    },
    {
      "epoch": 0.10556477152767305,
      "grad_norm": 2.0751543045043945,
      "learning_rate": 2.9042735617497733e-05,
      "loss": 1.8051,
      "step": 1050
    },
    {
      "epoch": 0.10657015030412707,
      "grad_norm": 3.1218643188476562,
      "learning_rate": 2.903264853232911e-05,
      "loss": 1.8279,
      "step": 1060
    },
    {
      "epoch": 0.10757552908058111,
      "grad_norm": 1.8414572477340698,
      "learning_rate": 2.9022561447160486e-05,
      "loss": 1.8473,
      "step": 1070
    },
    {
      "epoch": 0.10858090785703514,
      "grad_norm": 2.2607836723327637,
      "learning_rate": 2.9012474361991862e-05,
      "loss": 1.858,
      "step": 1080
    },
    {
      "epoch": 0.10958628663348917,
      "grad_norm": 1.7849160432815552,
      "learning_rate": 2.9002387276823242e-05,
      "loss": 1.7747,
      "step": 1090
    },
    {
      "epoch": 0.1105916654099432,
      "grad_norm": 2.1496264934539795,
      "learning_rate": 2.8992300191654618e-05,
      "loss": 1.8745,
      "step": 1100
    },
    {
      "epoch": 0.11159704418639722,
      "grad_norm": 2.4677512645721436,
      "learning_rate": 2.8982213106485998e-05,
      "loss": 1.8014,
      "step": 1110
    },
    {
      "epoch": 0.11260242296285125,
      "grad_norm": 2.7962918281555176,
      "learning_rate": 2.8972126021317374e-05,
      "loss": 1.8852,
      "step": 1120
    },
    {
      "epoch": 0.11360780173930528,
      "grad_norm": 2.3797335624694824,
      "learning_rate": 2.896203893614875e-05,
      "loss": 1.8571,
      "step": 1130
    },
    {
      "epoch": 0.11461318051575932,
      "grad_norm": 2.134577989578247,
      "learning_rate": 2.8951951850980127e-05,
      "loss": 1.7965,
      "step": 1140
    },
    {
      "epoch": 0.11561855929221335,
      "grad_norm": 2.070488214492798,
      "learning_rate": 2.8941864765811507e-05,
      "loss": 1.8247,
      "step": 1150
    },
    {
      "epoch": 0.11662393806866737,
      "grad_norm": 2.7699620723724365,
      "learning_rate": 2.8931777680642886e-05,
      "loss": 1.8107,
      "step": 1160
    },
    {
      "epoch": 0.1176293168451214,
      "grad_norm": 2.6800549030303955,
      "learning_rate": 2.8921690595474263e-05,
      "loss": 1.8338,
      "step": 1170
    },
    {
      "epoch": 0.11863469562157543,
      "grad_norm": 1.8205945491790771,
      "learning_rate": 2.8911603510305642e-05,
      "loss": 1.8288,
      "step": 1180
    },
    {
      "epoch": 0.11964007439802946,
      "grad_norm": 1.8642176389694214,
      "learning_rate": 2.8901516425137015e-05,
      "loss": 1.8522,
      "step": 1190
    },
    {
      "epoch": 0.12064545317448348,
      "grad_norm": 1.7910958528518677,
      "learning_rate": 2.8891429339968395e-05,
      "loss": 1.8266,
      "step": 1200
    },
    {
      "epoch": 0.12165083195093751,
      "grad_norm": 2.302273988723755,
      "learning_rate": 2.888134225479977e-05,
      "loss": 1.827,
      "step": 1210
    },
    {
      "epoch": 0.12265621072739155,
      "grad_norm": 2.145380973815918,
      "learning_rate": 2.887125516963115e-05,
      "loss": 1.9057,
      "step": 1220
    },
    {
      "epoch": 0.12366158950384558,
      "grad_norm": 2.013181686401367,
      "learning_rate": 2.8861168084462527e-05,
      "loss": 1.8057,
      "step": 1230
    },
    {
      "epoch": 0.1246669682802996,
      "grad_norm": 3.1762659549713135,
      "learning_rate": 2.8851080999293907e-05,
      "loss": 1.8095,
      "step": 1240
    },
    {
      "epoch": 0.12567234705675362,
      "grad_norm": 2.5776031017303467,
      "learning_rate": 2.884099391412528e-05,
      "loss": 1.9681,
      "step": 1250
    },
    {
      "epoch": 0.12667772583320766,
      "grad_norm": 2.235424757003784,
      "learning_rate": 2.883090682895666e-05,
      "loss": 1.7861,
      "step": 1260
    },
    {
      "epoch": 0.1276831046096617,
      "grad_norm": 3.336393117904663,
      "learning_rate": 2.8820819743788036e-05,
      "loss": 1.8901,
      "step": 1270
    },
    {
      "epoch": 0.12868848338611572,
      "grad_norm": 2.0966711044311523,
      "learning_rate": 2.8810732658619416e-05,
      "loss": 1.8538,
      "step": 1280
    },
    {
      "epoch": 0.12969386216256976,
      "grad_norm": 2.071383237838745,
      "learning_rate": 2.8800645573450792e-05,
      "loss": 1.8577,
      "step": 1290
    },
    {
      "epoch": 0.13069924093902377,
      "grad_norm": 2.2588064670562744,
      "learning_rate": 2.879055848828217e-05,
      "loss": 1.8274,
      "step": 1300
    },
    {
      "epoch": 0.1317046197154778,
      "grad_norm": 2.2016701698303223,
      "learning_rate": 2.8780471403113548e-05,
      "loss": 1.8449,
      "step": 1310
    },
    {
      "epoch": 0.13270999849193182,
      "grad_norm": 2.03275990486145,
      "learning_rate": 2.8770384317944924e-05,
      "loss": 1.8099,
      "step": 1320
    },
    {
      "epoch": 0.13371537726838587,
      "grad_norm": 1.9920220375061035,
      "learning_rate": 2.8760297232776304e-05,
      "loss": 1.9358,
      "step": 1330
    },
    {
      "epoch": 0.1347207560448399,
      "grad_norm": 2.284839391708374,
      "learning_rate": 2.875021014760768e-05,
      "loss": 1.8636,
      "step": 1340
    },
    {
      "epoch": 0.13572613482129392,
      "grad_norm": 2.889026403427124,
      "learning_rate": 2.874012306243906e-05,
      "loss": 1.7991,
      "step": 1350
    },
    {
      "epoch": 0.13673151359774796,
      "grad_norm": 1.9531159400939941,
      "learning_rate": 2.8730035977270436e-05,
      "loss": 1.8616,
      "step": 1360
    },
    {
      "epoch": 0.13773689237420197,
      "grad_norm": 2.189138174057007,
      "learning_rate": 2.8719948892101813e-05,
      "loss": 1.7269,
      "step": 1370
    },
    {
      "epoch": 0.13874227115065602,
      "grad_norm": 2.032785654067993,
      "learning_rate": 2.870986180693319e-05,
      "loss": 1.8512,
      "step": 1380
    },
    {
      "epoch": 0.13974764992711003,
      "grad_norm": 2.1684741973876953,
      "learning_rate": 2.869977472176457e-05,
      "loss": 1.8616,
      "step": 1390
    },
    {
      "epoch": 0.14075302870356407,
      "grad_norm": 2.296365976333618,
      "learning_rate": 2.8689687636595945e-05,
      "loss": 1.8679,
      "step": 1400
    },
    {
      "epoch": 0.14175840748001808,
      "grad_norm": 2.555476665496826,
      "learning_rate": 2.8679600551427325e-05,
      "loss": 1.8706,
      "step": 1410
    },
    {
      "epoch": 0.14276378625647212,
      "grad_norm": 1.929545521736145,
      "learning_rate": 2.86695134662587e-05,
      "loss": 1.8766,
      "step": 1420
    },
    {
      "epoch": 0.14376916503292617,
      "grad_norm": 2.172813892364502,
      "learning_rate": 2.8659426381090077e-05,
      "loss": 1.8122,
      "step": 1430
    },
    {
      "epoch": 0.14477454380938018,
      "grad_norm": 6.706202983856201,
      "learning_rate": 2.8649339295921453e-05,
      "loss": 1.8002,
      "step": 1440
    },
    {
      "epoch": 0.14577992258583422,
      "grad_norm": 2.3283698558807373,
      "learning_rate": 2.8639252210752833e-05,
      "loss": 1.7954,
      "step": 1450
    },
    {
      "epoch": 0.14678530136228823,
      "grad_norm": 2.6487133502960205,
      "learning_rate": 2.8629165125584213e-05,
      "loss": 1.7322,
      "step": 1460
    },
    {
      "epoch": 0.14779068013874228,
      "grad_norm": 1.8622608184814453,
      "learning_rate": 2.861907804041559e-05,
      "loss": 1.8443,
      "step": 1470
    },
    {
      "epoch": 0.1487960589151963,
      "grad_norm": 1.9039045572280884,
      "learning_rate": 2.860899095524697e-05,
      "loss": 1.8061,
      "step": 1480
    },
    {
      "epoch": 0.14980143769165033,
      "grad_norm": 2.1612491607666016,
      "learning_rate": 2.8598903870078342e-05,
      "loss": 1.8399,
      "step": 1490
    },
    {
      "epoch": 0.15080681646810437,
      "grad_norm": 2.16634202003479,
      "learning_rate": 2.858881678490972e-05,
      "loss": 1.8365,
      "step": 1500
    },
    {
      "epoch": 0.15181219524455838,
      "grad_norm": 2.641552686691284,
      "learning_rate": 2.8578729699741098e-05,
      "loss": 1.7769,
      "step": 1510
    },
    {
      "epoch": 0.15281757402101243,
      "grad_norm": 2.195915699005127,
      "learning_rate": 2.8568642614572478e-05,
      "loss": 1.7467,
      "step": 1520
    },
    {
      "epoch": 0.15382295279746644,
      "grad_norm": 1.863767147064209,
      "learning_rate": 2.8558555529403854e-05,
      "loss": 1.7669,
      "step": 1530
    },
    {
      "epoch": 0.15482833157392048,
      "grad_norm": 2.8799283504486084,
      "learning_rate": 2.8548468444235234e-05,
      "loss": 1.838,
      "step": 1540
    },
    {
      "epoch": 0.1558337103503745,
      "grad_norm": 2.6208298206329346,
      "learning_rate": 2.8538381359066606e-05,
      "loss": 1.8537,
      "step": 1550
    },
    {
      "epoch": 0.15683908912682853,
      "grad_norm": 2.775254487991333,
      "learning_rate": 2.8528294273897986e-05,
      "loss": 1.8263,
      "step": 1560
    },
    {
      "epoch": 0.15784446790328255,
      "grad_norm": 1.8405351638793945,
      "learning_rate": 2.8518207188729362e-05,
      "loss": 1.8841,
      "step": 1570
    },
    {
      "epoch": 0.1588498466797366,
      "grad_norm": 2.3658547401428223,
      "learning_rate": 2.8508120103560742e-05,
      "loss": 1.8198,
      "step": 1580
    },
    {
      "epoch": 0.15985522545619063,
      "grad_norm": 1.9417760372161865,
      "learning_rate": 2.849803301839212e-05,
      "loss": 1.8322,
      "step": 1590
    },
    {
      "epoch": 0.16086060423264464,
      "grad_norm": 2.1524271965026855,
      "learning_rate": 2.8487945933223498e-05,
      "loss": 1.8612,
      "step": 1600
    },
    {
      "epoch": 0.16186598300909869,
      "grad_norm": 2.0843303203582764,
      "learning_rate": 2.8477858848054875e-05,
      "loss": 1.8782,
      "step": 1610
    },
    {
      "epoch": 0.1628713617855527,
      "grad_norm": 1.9990047216415405,
      "learning_rate": 2.846777176288625e-05,
      "loss": 1.8494,
      "step": 1620
    },
    {
      "epoch": 0.16387674056200674,
      "grad_norm": 2.5698060989379883,
      "learning_rate": 2.845768467771763e-05,
      "loss": 1.8912,
      "step": 1630
    },
    {
      "epoch": 0.16488211933846075,
      "grad_norm": 2.1996731758117676,
      "learning_rate": 2.8447597592549007e-05,
      "loss": 1.8655,
      "step": 1640
    },
    {
      "epoch": 0.1658874981149148,
      "grad_norm": 2.4499704837799072,
      "learning_rate": 2.8437510507380387e-05,
      "loss": 1.8488,
      "step": 1650
    },
    {
      "epoch": 0.16689287689136884,
      "grad_norm": 1.7854351997375488,
      "learning_rate": 2.8427423422211763e-05,
      "loss": 1.8532,
      "step": 1660
    },
    {
      "epoch": 0.16789825566782285,
      "grad_norm": 2.4342939853668213,
      "learning_rate": 2.841733633704314e-05,
      "loss": 1.8398,
      "step": 1670
    },
    {
      "epoch": 0.1689036344442769,
      "grad_norm": 2.013589382171631,
      "learning_rate": 2.8407249251874515e-05,
      "loss": 1.7881,
      "step": 1680
    },
    {
      "epoch": 0.1699090132207309,
      "grad_norm": 2.638862133026123,
      "learning_rate": 2.8397162166705895e-05,
      "loss": 1.815,
      "step": 1690
    },
    {
      "epoch": 0.17091439199718494,
      "grad_norm": 3.0801005363464355,
      "learning_rate": 2.838707508153727e-05,
      "loss": 1.7707,
      "step": 1700
    },
    {
      "epoch": 0.17191977077363896,
      "grad_norm": 2.2871696949005127,
      "learning_rate": 2.837698799636865e-05,
      "loss": 1.8578,
      "step": 1710
    },
    {
      "epoch": 0.172925149550093,
      "grad_norm": 1.6369832754135132,
      "learning_rate": 2.8366900911200028e-05,
      "loss": 1.7749,
      "step": 1720
    },
    {
      "epoch": 0.17393052832654704,
      "grad_norm": 2.5209386348724365,
      "learning_rate": 2.8356813826031404e-05,
      "loss": 1.8278,
      "step": 1730
    },
    {
      "epoch": 0.17493590710300105,
      "grad_norm": 2.2397496700286865,
      "learning_rate": 2.834672674086278e-05,
      "loss": 1.8459,
      "step": 1740
    },
    {
      "epoch": 0.1759412858794551,
      "grad_norm": 2.199536085128784,
      "learning_rate": 2.833663965569416e-05,
      "loss": 1.7843,
      "step": 1750
    },
    {
      "epoch": 0.1769466646559091,
      "grad_norm": 2.467102527618408,
      "learning_rate": 2.832655257052554e-05,
      "loss": 1.8121,
      "step": 1760
    },
    {
      "epoch": 0.17795204343236315,
      "grad_norm": 2.153637170791626,
      "learning_rate": 2.8316465485356916e-05,
      "loss": 1.7702,
      "step": 1770
    },
    {
      "epoch": 0.17895742220881716,
      "grad_norm": 1.7340459823608398,
      "learning_rate": 2.8306378400188296e-05,
      "loss": 1.7808,
      "step": 1780
    },
    {
      "epoch": 0.1799628009852712,
      "grad_norm": 2.1596405506134033,
      "learning_rate": 2.829629131501967e-05,
      "loss": 1.82,
      "step": 1790
    },
    {
      "epoch": 0.18096817976172522,
      "grad_norm": 3.1212737560272217,
      "learning_rate": 2.8286204229851048e-05,
      "loss": 1.9079,
      "step": 1800
    },
    {
      "epoch": 0.18197355853817926,
      "grad_norm": 2.5742690563201904,
      "learning_rate": 2.8276117144682425e-05,
      "loss": 1.832,
      "step": 1810
    },
    {
      "epoch": 0.1829789373146333,
      "grad_norm": 2.542733669281006,
      "learning_rate": 2.8266030059513804e-05,
      "loss": 1.87,
      "step": 1820
    },
    {
      "epoch": 0.1839843160910873,
      "grad_norm": 2.1242477893829346,
      "learning_rate": 2.825594297434518e-05,
      "loss": 1.7682,
      "step": 1830
    },
    {
      "epoch": 0.18498969486754135,
      "grad_norm": 3.593458414077759,
      "learning_rate": 2.824585588917656e-05,
      "loss": 1.7609,
      "step": 1840
    },
    {
      "epoch": 0.18599507364399537,
      "grad_norm": 2.171473503112793,
      "learning_rate": 2.8235768804007933e-05,
      "loss": 1.8545,
      "step": 1850
    },
    {
      "epoch": 0.1870004524204494,
      "grad_norm": 1.751547932624817,
      "learning_rate": 2.8225681718839313e-05,
      "loss": 1.7874,
      "step": 1860
    },
    {
      "epoch": 0.18800583119690342,
      "grad_norm": 2.0235862731933594,
      "learning_rate": 2.821559463367069e-05,
      "loss": 1.7738,
      "step": 1870
    },
    {
      "epoch": 0.18901120997335746,
      "grad_norm": 2.4262940883636475,
      "learning_rate": 2.820550754850207e-05,
      "loss": 1.8775,
      "step": 1880
    },
    {
      "epoch": 0.1900165887498115,
      "grad_norm": 1.6165345907211304,
      "learning_rate": 2.8195420463333445e-05,
      "loss": 1.7091,
      "step": 1890
    },
    {
      "epoch": 0.19102196752626552,
      "grad_norm": 1.8040447235107422,
      "learning_rate": 2.8185333378164825e-05,
      "loss": 1.8041,
      "step": 1900
    },
    {
      "epoch": 0.19202734630271956,
      "grad_norm": 2.283010721206665,
      "learning_rate": 2.81752462929962e-05,
      "loss": 1.8022,
      "step": 1910
    },
    {
      "epoch": 0.19303272507917357,
      "grad_norm": 2.1089184284210205,
      "learning_rate": 2.8165159207827578e-05,
      "loss": 1.8254,
      "step": 1920
    },
    {
      "epoch": 0.1940381038556276,
      "grad_norm": 2.5875749588012695,
      "learning_rate": 2.8155072122658957e-05,
      "loss": 1.7922,
      "step": 1930
    },
    {
      "epoch": 0.19504348263208163,
      "grad_norm": 2.111345052719116,
      "learning_rate": 2.8144985037490334e-05,
      "loss": 1.8554,
      "step": 1940
    },
    {
      "epoch": 0.19604886140853567,
      "grad_norm": 1.8971444368362427,
      "learning_rate": 2.8134897952321713e-05,
      "loss": 1.7825,
      "step": 1950
    },
    {
      "epoch": 0.19705424018498968,
      "grad_norm": 2.3121936321258545,
      "learning_rate": 2.812481086715309e-05,
      "loss": 1.8644,
      "step": 1960
    },
    {
      "epoch": 0.19805961896144372,
      "grad_norm": 2.301577568054199,
      "learning_rate": 2.811472378198447e-05,
      "loss": 1.8222,
      "step": 1970
    },
    {
      "epoch": 0.19906499773789776,
      "grad_norm": 1.8421542644500732,
      "learning_rate": 2.8104636696815842e-05,
      "loss": 1.8959,
      "step": 1980
    },
    {
      "epoch": 0.20007037651435178,
      "grad_norm": 1.7672758102416992,
      "learning_rate": 2.8094549611647222e-05,
      "loss": 1.8976,
      "step": 1990
    },
    {
      "epoch": 0.20107575529080582,
      "grad_norm": 1.9901609420776367,
      "learning_rate": 2.8084462526478598e-05,
      "loss": 1.8556,
      "step": 2000
    },
    {
      "epoch": 0.20208113406725983,
      "grad_norm": 2.542177677154541,
      "learning_rate": 2.8074375441309978e-05,
      "loss": 1.7682,
      "step": 2010
    },
    {
      "epoch": 0.20308651284371387,
      "grad_norm": 2.259472370147705,
      "learning_rate": 2.8064288356141354e-05,
      "loss": 1.8213,
      "step": 2020
    },
    {
      "epoch": 0.2040918916201679,
      "grad_norm": 3.867034673690796,
      "learning_rate": 2.8054201270972734e-05,
      "loss": 1.8118,
      "step": 2030
    },
    {
      "epoch": 0.20509727039662193,
      "grad_norm": 1.882993459701538,
      "learning_rate": 2.8044114185804107e-05,
      "loss": 1.8342,
      "step": 2040
    },
    {
      "epoch": 0.20610264917307597,
      "grad_norm": 2.183657646179199,
      "learning_rate": 2.8034027100635487e-05,
      "loss": 1.9051,
      "step": 2050
    },
    {
      "epoch": 0.20710802794952998,
      "grad_norm": 1.8984555006027222,
      "learning_rate": 2.8023940015466866e-05,
      "loss": 1.7792,
      "step": 2060
    },
    {
      "epoch": 0.20811340672598402,
      "grad_norm": 2.372002363204956,
      "learning_rate": 2.8013852930298243e-05,
      "loss": 1.8908,
      "step": 2070
    },
    {
      "epoch": 0.20911878550243804,
      "grad_norm": 1.9588096141815186,
      "learning_rate": 2.8003765845129622e-05,
      "loss": 1.8427,
      "step": 2080
    },
    {
      "epoch": 0.21012416427889208,
      "grad_norm": 2.0126209259033203,
      "learning_rate": 2.7993678759961e-05,
      "loss": 1.8293,
      "step": 2090
    },
    {
      "epoch": 0.2111295430553461,
      "grad_norm": 2.132990598678589,
      "learning_rate": 2.7983591674792375e-05,
      "loss": 1.7523,
      "step": 2100
    },
    {
      "epoch": 0.21213492183180013,
      "grad_norm": 2.0758330821990967,
      "learning_rate": 2.797350458962375e-05,
      "loss": 1.8014,
      "step": 2110
    },
    {
      "epoch": 0.21314030060825415,
      "grad_norm": 1.9420034885406494,
      "learning_rate": 2.796341750445513e-05,
      "loss": 1.7929,
      "step": 2120
    },
    {
      "epoch": 0.2141456793847082,
      "grad_norm": 1.8435492515563965,
      "learning_rate": 2.7953330419286507e-05,
      "loss": 1.8152,
      "step": 2130
    },
    {
      "epoch": 0.21515105816116223,
      "grad_norm": 1.7371821403503418,
      "learning_rate": 2.7943243334117887e-05,
      "loss": 1.7324,
      "step": 2140
    },
    {
      "epoch": 0.21615643693761624,
      "grad_norm": 2.2306416034698486,
      "learning_rate": 2.7933156248949263e-05,
      "loss": 1.8794,
      "step": 2150
    },
    {
      "epoch": 0.21716181571407028,
      "grad_norm": 2.1510987281799316,
      "learning_rate": 2.792306916378064e-05,
      "loss": 1.9076,
      "step": 2160
    },
    {
      "epoch": 0.2181671944905243,
      "grad_norm": 2.964878559112549,
      "learning_rate": 2.7912982078612016e-05,
      "loss": 1.8468,
      "step": 2170
    },
    {
      "epoch": 0.21917257326697834,
      "grad_norm": 1.8866726160049438,
      "learning_rate": 2.7902894993443396e-05,
      "loss": 1.7856,
      "step": 2180
    },
    {
      "epoch": 0.22017795204343235,
      "grad_norm": 2.100548505783081,
      "learning_rate": 2.7892807908274772e-05,
      "loss": 1.8553,
      "step": 2190
    },
    {
      "epoch": 0.2211833308198864,
      "grad_norm": 2.2865397930145264,
      "learning_rate": 2.788272082310615e-05,
      "loss": 1.7626,
      "step": 2200
    },
    {
      "epoch": 0.22218870959634043,
      "grad_norm": 2.1279006004333496,
      "learning_rate": 2.7872633737937528e-05,
      "loss": 1.838,
      "step": 2210
    },
    {
      "epoch": 0.22319408837279445,
      "grad_norm": 2.7419731616973877,
      "learning_rate": 2.7862546652768904e-05,
      "loss": 1.8225,
      "step": 2220
    },
    {
      "epoch": 0.2241994671492485,
      "grad_norm": 1.8406141996383667,
      "learning_rate": 2.7852459567600284e-05,
      "loss": 1.872,
      "step": 2230
    },
    {
      "epoch": 0.2252048459257025,
      "grad_norm": 1.9261785745620728,
      "learning_rate": 2.784237248243166e-05,
      "loss": 1.7821,
      "step": 2240
    },
    {
      "epoch": 0.22621022470215654,
      "grad_norm": 2.0058579444885254,
      "learning_rate": 2.783228539726304e-05,
      "loss": 1.8295,
      "step": 2250
    },
    {
      "epoch": 0.22721560347861056,
      "grad_norm": 2.1806302070617676,
      "learning_rate": 2.7822198312094416e-05,
      "loss": 1.8066,
      "step": 2260
    },
    {
      "epoch": 0.2282209822550646,
      "grad_norm": 2.7200429439544678,
      "learning_rate": 2.7812111226925796e-05,
      "loss": 1.7199,
      "step": 2270
    },
    {
      "epoch": 0.22922636103151864,
      "grad_norm": 1.936362385749817,
      "learning_rate": 2.780202414175717e-05,
      "loss": 1.7751,
      "step": 2280
    },
    {
      "epoch": 0.23023173980797265,
      "grad_norm": 3.208326578140259,
      "learning_rate": 2.779193705658855e-05,
      "loss": 1.9119,
      "step": 2290
    },
    {
      "epoch": 0.2312371185844267,
      "grad_norm": 2.0853734016418457,
      "learning_rate": 2.7782858679936786e-05,
      "loss": 1.8208,
      "step": 2300
    },
    {
      "epoch": 0.2322424973608807,
      "grad_norm": 2.1797423362731934,
      "learning_rate": 2.7772771594768165e-05,
      "loss": 1.8046,
      "step": 2310
    },
    {
      "epoch": 0.23324787613733475,
      "grad_norm": 2.671808958053589,
      "learning_rate": 2.776268450959954e-05,
      "loss": 1.8502,
      "step": 2320
    },
    {
      "epoch": 0.23425325491378876,
      "grad_norm": 1.8839994668960571,
      "learning_rate": 2.775259742443092e-05,
      "loss": 1.8251,
      "step": 2330
    },
    {
      "epoch": 0.2352586336902428,
      "grad_norm": 2.0273966789245605,
      "learning_rate": 2.7742510339262298e-05,
      "loss": 1.8446,
      "step": 2340
    },
    {
      "epoch": 0.23626401246669682,
      "grad_norm": 1.612668514251709,
      "learning_rate": 2.7732423254093677e-05,
      "loss": 1.8009,
      "step": 2350
    },
    {
      "epoch": 0.23726939124315086,
      "grad_norm": 2.0332517623901367,
      "learning_rate": 2.7722336168925054e-05,
      "loss": 1.8606,
      "step": 2360
    },
    {
      "epoch": 0.2382747700196049,
      "grad_norm": 1.8239506483078003,
      "learning_rate": 2.771224908375643e-05,
      "loss": 1.8333,
      "step": 2370
    },
    {
      "epoch": 0.2392801487960589,
      "grad_norm": 2.483429431915283,
      "learning_rate": 2.770216199858781e-05,
      "loss": 1.7935,
      "step": 2380
    },
    {
      "epoch": 0.24028552757251295,
      "grad_norm": 1.782593011856079,
      "learning_rate": 2.7692074913419186e-05,
      "loss": 1.9023,
      "step": 2390
    },
    {
      "epoch": 0.24129090634896697,
      "grad_norm": 1.8824098110198975,
      "learning_rate": 2.7681987828250566e-05,
      "loss": 1.7864,
      "step": 2400
    },
    {
      "epoch": 0.242296285125421,
      "grad_norm": 1.8834069967269897,
      "learning_rate": 2.7671900743081942e-05,
      "loss": 1.7939,
      "step": 2410
    },
    {
      "epoch": 0.24330166390187502,
      "grad_norm": 1.946003794670105,
      "learning_rate": 2.7661813657913318e-05,
      "loss": 1.7634,
      "step": 2420
    },
    {
      "epoch": 0.24430704267832906,
      "grad_norm": 2.1905298233032227,
      "learning_rate": 2.7651726572744695e-05,
      "loss": 1.7909,
      "step": 2430
    },
    {
      "epoch": 0.2453124214547831,
      "grad_norm": 2.316681385040283,
      "learning_rate": 2.7641639487576074e-05,
      "loss": 1.7688,
      "step": 2440
    },
    {
      "epoch": 0.24631780023123712,
      "grad_norm": 2.325317144393921,
      "learning_rate": 2.763155240240745e-05,
      "loss": 1.8191,
      "step": 2450
    },
    {
      "epoch": 0.24732317900769116,
      "grad_norm": 2.470363140106201,
      "learning_rate": 2.762146531723883e-05,
      "loss": 1.8477,
      "step": 2460
    },
    {
      "epoch": 0.24832855778414517,
      "grad_norm": 2.134751558303833,
      "learning_rate": 2.7611378232070207e-05,
      "loss": 1.866,
      "step": 2470
    },
    {
      "epoch": 0.2493339365605992,
      "grad_norm": 2.691272497177124,
      "learning_rate": 2.7601291146901583e-05,
      "loss": 1.782,
      "step": 2480
    },
    {
      "epoch": 0.2503393153370532,
      "grad_norm": 2.1015539169311523,
      "learning_rate": 2.759120406173296e-05,
      "loss": 1.8479,
      "step": 2490
    },
    {
      "epoch": 0.25134469411350724,
      "grad_norm": 2.093693733215332,
      "learning_rate": 2.758111697656434e-05,
      "loss": 1.7728,
      "step": 2500
    },
    {
      "epoch": 0.2523500728899613,
      "grad_norm": 2.119145393371582,
      "learning_rate": 2.757102989139572e-05,
      "loss": 1.8346,
      "step": 2510
    },
    {
      "epoch": 0.2533554516664153,
      "grad_norm": 2.3524270057678223,
      "learning_rate": 2.7560942806227095e-05,
      "loss": 1.7836,
      "step": 2520
    },
    {
      "epoch": 0.25436083044286933,
      "grad_norm": 2.840432643890381,
      "learning_rate": 2.7550855721058475e-05,
      "loss": 1.8169,
      "step": 2530
    },
    {
      "epoch": 0.2553662092193234,
      "grad_norm": 1.8921335935592651,
      "learning_rate": 2.754076863588985e-05,
      "loss": 1.8753,
      "step": 2540
    },
    {
      "epoch": 0.2563715879957774,
      "grad_norm": 2.2152392864227295,
      "learning_rate": 2.7530681550721227e-05,
      "loss": 1.8025,
      "step": 2550
    },
    {
      "epoch": 0.25737696677223143,
      "grad_norm": 2.321728229522705,
      "learning_rate": 2.7520594465552604e-05,
      "loss": 1.8641,
      "step": 2560
    },
    {
      "epoch": 0.25838234554868544,
      "grad_norm": 1.6628105640411377,
      "learning_rate": 2.7510507380383983e-05,
      "loss": 1.823,
      "step": 2570
    },
    {
      "epoch": 0.2593877243251395,
      "grad_norm": 2.5622506141662598,
      "learning_rate": 2.750042029521536e-05,
      "loss": 1.8394,
      "step": 2580
    },
    {
      "epoch": 0.2603931031015935,
      "grad_norm": 1.8316714763641357,
      "learning_rate": 2.749033321004674e-05,
      "loss": 1.7505,
      "step": 2590
    },
    {
      "epoch": 0.26139848187804754,
      "grad_norm": 1.9419533014297485,
      "learning_rate": 2.7480246124878116e-05,
      "loss": 1.827,
      "step": 2600
    },
    {
      "epoch": 0.2624038606545016,
      "grad_norm": 2.2372519969940186,
      "learning_rate": 2.7470159039709492e-05,
      "loss": 1.765,
      "step": 2610
    },
    {
      "epoch": 0.2634092394309556,
      "grad_norm": 2.6580440998077393,
      "learning_rate": 2.7460071954540868e-05,
      "loss": 1.796,
      "step": 2620
    },
    {
      "epoch": 0.26441461820740964,
      "grad_norm": 3.718700647354126,
      "learning_rate": 2.7449984869372248e-05,
      "loss": 1.8341,
      "step": 2630
    },
    {
      "epoch": 0.26541999698386365,
      "grad_norm": 2.2585928440093994,
      "learning_rate": 2.7439897784203624e-05,
      "loss": 1.7592,
      "step": 2640
    },
    {
      "epoch": 0.2664253757603177,
      "grad_norm": 1.9512932300567627,
      "learning_rate": 2.7429810699035004e-05,
      "loss": 1.8,
      "step": 2650
    },
    {
      "epoch": 0.26743075453677173,
      "grad_norm": 2.40708589553833,
      "learning_rate": 2.7419723613866384e-05,
      "loss": 1.877,
      "step": 2660
    },
    {
      "epoch": 0.26843613331322574,
      "grad_norm": 1.854235053062439,
      "learning_rate": 2.7409636528697757e-05,
      "loss": 1.8579,
      "step": 2670
    },
    {
      "epoch": 0.2694415120896798,
      "grad_norm": 2.4979851245880127,
      "learning_rate": 2.7399549443529136e-05,
      "loss": 1.8003,
      "step": 2680
    },
    {
      "epoch": 0.2704468908661338,
      "grad_norm": 2.340397834777832,
      "learning_rate": 2.7389462358360513e-05,
      "loss": 1.8173,
      "step": 2690
    },
    {
      "epoch": 0.27145226964258784,
      "grad_norm": 2.1506855487823486,
      "learning_rate": 2.7379375273191892e-05,
      "loss": 1.817,
      "step": 2700
    },
    {
      "epoch": 0.27245764841904185,
      "grad_norm": 1.654011607170105,
      "learning_rate": 2.736928818802327e-05,
      "loss": 1.8232,
      "step": 2710
    },
    {
      "epoch": 0.2734630271954959,
      "grad_norm": 2.094085216522217,
      "learning_rate": 2.735920110285465e-05,
      "loss": 1.8371,
      "step": 2720
    },
    {
      "epoch": 0.27446840597194994,
      "grad_norm": 2.7191920280456543,
      "learning_rate": 2.734911401768602e-05,
      "loss": 1.8209,
      "step": 2730
    },
    {
      "epoch": 0.27547378474840395,
      "grad_norm": 2.127180576324463,
      "learning_rate": 2.73390269325174e-05,
      "loss": 1.8269,
      "step": 2740
    },
    {
      "epoch": 0.27647916352485796,
      "grad_norm": 2.1531589031219482,
      "learning_rate": 2.7328939847348777e-05,
      "loss": 1.831,
      "step": 2750
    },
    {
      "epoch": 0.27748454230131203,
      "grad_norm": 2.552278995513916,
      "learning_rate": 2.7318852762180157e-05,
      "loss": 1.7918,
      "step": 2760
    },
    {
      "epoch": 0.27848992107776604,
      "grad_norm": 1.6737149953842163,
      "learning_rate": 2.7308765677011533e-05,
      "loss": 1.856,
      "step": 2770
    },
    {
      "epoch": 0.27949529985422006,
      "grad_norm": 2.55788254737854,
      "learning_rate": 2.7298678591842913e-05,
      "loss": 1.8123,
      "step": 2780
    },
    {
      "epoch": 0.2805006786306741,
      "grad_norm": 2.406360149383545,
      "learning_rate": 2.7288591506674286e-05,
      "loss": 1.8229,
      "step": 2790
    },
    {
      "epoch": 0.28150605740712814,
      "grad_norm": 1.9067655801773071,
      "learning_rate": 2.7278504421505666e-05,
      "loss": 1.8424,
      "step": 2800
    },
    {
      "epoch": 0.28251143618358215,
      "grad_norm": 2.0312538146972656,
      "learning_rate": 2.7268417336337045e-05,
      "loss": 1.8322,
      "step": 2810
    },
    {
      "epoch": 0.28351681496003617,
      "grad_norm": 1.905033826828003,
      "learning_rate": 2.725833025116842e-05,
      "loss": 1.8628,
      "step": 2820
    },
    {
      "epoch": 0.28452219373649024,
      "grad_norm": 1.6047601699829102,
      "learning_rate": 2.72482431659998e-05,
      "loss": 1.8655,
      "step": 2830
    },
    {
      "epoch": 0.28552757251294425,
      "grad_norm": 2.5488176345825195,
      "learning_rate": 2.7238156080831178e-05,
      "loss": 1.8023,
      "step": 2840
    },
    {
      "epoch": 0.28653295128939826,
      "grad_norm": 1.6810240745544434,
      "learning_rate": 2.7228068995662554e-05,
      "loss": 1.8201,
      "step": 2850
    },
    {
      "epoch": 0.28753833006585233,
      "grad_norm": 2.2530176639556885,
      "learning_rate": 2.721798191049393e-05,
      "loss": 1.8081,
      "step": 2860
    },
    {
      "epoch": 0.28854370884230635,
      "grad_norm": 2.209265947341919,
      "learning_rate": 2.720789482532531e-05,
      "loss": 1.7978,
      "step": 2870
    },
    {
      "epoch": 0.28954908761876036,
      "grad_norm": 2.5048909187316895,
      "learning_rate": 2.7197807740156686e-05,
      "loss": 1.7771,
      "step": 2880
    },
    {
      "epoch": 0.2905544663952144,
      "grad_norm": 2.3620221614837646,
      "learning_rate": 2.7187720654988066e-05,
      "loss": 1.8019,
      "step": 2890
    },
    {
      "epoch": 0.29155984517166844,
      "grad_norm": 3.0834097862243652,
      "learning_rate": 2.7177633569819442e-05,
      "loss": 1.7503,
      "step": 2900
    },
    {
      "epoch": 0.29256522394812245,
      "grad_norm": 2.194845199584961,
      "learning_rate": 2.716754648465082e-05,
      "loss": 1.8458,
      "step": 2910
    },
    {
      "epoch": 0.29357060272457647,
      "grad_norm": 2.086897373199463,
      "learning_rate": 2.7157459399482195e-05,
      "loss": 1.8186,
      "step": 2920
    },
    {
      "epoch": 0.29457598150103054,
      "grad_norm": 1.989654779434204,
      "learning_rate": 2.7147372314313575e-05,
      "loss": 1.8297,
      "step": 2930
    },
    {
      "epoch": 0.29558136027748455,
      "grad_norm": 1.9423691034317017,
      "learning_rate": 2.713728522914495e-05,
      "loss": 1.767,
      "step": 2940
    },
    {
      "epoch": 0.29658673905393856,
      "grad_norm": 2.0986783504486084,
      "learning_rate": 2.712719814397633e-05,
      "loss": 1.8214,
      "step": 2950
    },
    {
      "epoch": 0.2975921178303926,
      "grad_norm": 2.115767478942871,
      "learning_rate": 2.711711105880771e-05,
      "loss": 1.8671,
      "step": 2960
    },
    {
      "epoch": 0.29859749660684665,
      "grad_norm": 1.8224531412124634,
      "learning_rate": 2.7107023973639083e-05,
      "loss": 1.8002,
      "step": 2970
    },
    {
      "epoch": 0.29960287538330066,
      "grad_norm": 2.162356376647949,
      "learning_rate": 2.7096936888470463e-05,
      "loss": 1.8507,
      "step": 2980
    },
    {
      "epoch": 0.3006082541597547,
      "grad_norm": 1.9778928756713867,
      "learning_rate": 2.708684980330184e-05,
      "loss": 1.8603,
      "step": 2990
    },
    {
      "epoch": 0.30161363293620874,
      "grad_norm": 1.8006619215011597,
      "learning_rate": 2.707676271813322e-05,
      "loss": 1.8434,
      "step": 3000
    },
    {
      "epoch": 0.30261901171266276,
      "grad_norm": 1.9598006010055542,
      "learning_rate": 2.7066675632964595e-05,
      "loss": 1.8271,
      "step": 3010
    },
    {
      "epoch": 0.30362439048911677,
      "grad_norm": 2.0908610820770264,
      "learning_rate": 2.7056588547795975e-05,
      "loss": 1.8269,
      "step": 3020
    },
    {
      "epoch": 0.3046297692655708,
      "grad_norm": 1.8623578548431396,
      "learning_rate": 2.7046501462627348e-05,
      "loss": 1.8383,
      "step": 3030
    },
    {
      "epoch": 0.30563514804202485,
      "grad_norm": 1.9606738090515137,
      "learning_rate": 2.7036414377458728e-05,
      "loss": 1.7643,
      "step": 3040
    },
    {
      "epoch": 0.30664052681847886,
      "grad_norm": 2.9370970726013184,
      "learning_rate": 2.7026327292290104e-05,
      "loss": 1.7744,
      "step": 3050
    },
    {
      "epoch": 0.3076459055949329,
      "grad_norm": 1.911322832107544,
      "learning_rate": 2.7016240207121484e-05,
      "loss": 1.7803,
      "step": 3060
    },
    {
      "epoch": 0.30865128437138695,
      "grad_norm": 2.3739078044891357,
      "learning_rate": 2.700615312195286e-05,
      "loss": 1.775,
      "step": 3070
    },
    {
      "epoch": 0.30965666314784096,
      "grad_norm": 2.181790828704834,
      "learning_rate": 2.699606603678424e-05,
      "loss": 1.7953,
      "step": 3080
    },
    {
      "epoch": 0.310662041924295,
      "grad_norm": 1.8684265613555908,
      "learning_rate": 2.6985978951615613e-05,
      "loss": 1.7582,
      "step": 3090
    },
    {
      "epoch": 0.311667420700749,
      "grad_norm": 2.1321563720703125,
      "learning_rate": 2.6975891866446992e-05,
      "loss": 1.7391,
      "step": 3100
    },
    {
      "epoch": 0.31267279947720306,
      "grad_norm": 1.981597661972046,
      "learning_rate": 2.6965804781278372e-05,
      "loss": 1.7745,
      "step": 3110
    },
    {
      "epoch": 0.31367817825365707,
      "grad_norm": 2.2479817867279053,
      "learning_rate": 2.6955717696109748e-05,
      "loss": 1.8645,
      "step": 3120
    },
    {
      "epoch": 0.3146835570301111,
      "grad_norm": 1.8371798992156982,
      "learning_rate": 2.6945630610941128e-05,
      "loss": 1.8081,
      "step": 3130
    },
    {
      "epoch": 0.3156889358065651,
      "grad_norm": 2.1084067821502686,
      "learning_rate": 2.6935543525772504e-05,
      "loss": 1.8212,
      "step": 3140
    },
    {
      "epoch": 0.31669431458301917,
      "grad_norm": 1.856185793876648,
      "learning_rate": 2.692545644060388e-05,
      "loss": 1.8676,
      "step": 3150
    },
    {
      "epoch": 0.3176996933594732,
      "grad_norm": 1.9484554529190063,
      "learning_rate": 2.6915369355435257e-05,
      "loss": 1.8892,
      "step": 3160
    },
    {
      "epoch": 0.3187050721359272,
      "grad_norm": 2.440072536468506,
      "learning_rate": 2.6905282270266637e-05,
      "loss": 1.7963,
      "step": 3170
    },
    {
      "epoch": 0.31971045091238126,
      "grad_norm": 2.7510483264923096,
      "learning_rate": 2.6895195185098013e-05,
      "loss": 1.7218,
      "step": 3180
    },
    {
      "epoch": 0.3207158296888353,
      "grad_norm": 2.461311101913452,
      "learning_rate": 2.6885108099929393e-05,
      "loss": 1.8582,
      "step": 3190
    },
    {
      "epoch": 0.3217212084652893,
      "grad_norm": 2.3242104053497314,
      "learning_rate": 2.687502101476077e-05,
      "loss": 1.8464,
      "step": 3200
    },
    {
      "epoch": 0.3227265872417433,
      "grad_norm": 1.6869909763336182,
      "learning_rate": 2.6864933929592145e-05,
      "loss": 1.8448,
      "step": 3210
    },
    {
      "epoch": 0.32373196601819737,
      "grad_norm": 2.255139112472534,
      "learning_rate": 2.685484684442352e-05,
      "loss": 1.7659,
      "step": 3220
    },
    {
      "epoch": 0.3247373447946514,
      "grad_norm": 2.2987890243530273,
      "learning_rate": 2.68447597592549e-05,
      "loss": 1.905,
      "step": 3230
    },
    {
      "epoch": 0.3257427235711054,
      "grad_norm": 2.5725808143615723,
      "learning_rate": 2.6834672674086278e-05,
      "loss": 1.8619,
      "step": 3240
    },
    {
      "epoch": 0.32674810234755947,
      "grad_norm": 2.421412229537964,
      "learning_rate": 2.6824585588917657e-05,
      "loss": 1.7866,
      "step": 3250
    },
    {
      "epoch": 0.3277534811240135,
      "grad_norm": 3.6843008995056152,
      "learning_rate": 2.6814498503749037e-05,
      "loss": 1.7885,
      "step": 3260
    },
    {
      "epoch": 0.3287588599004675,
      "grad_norm": 1.8619718551635742,
      "learning_rate": 2.680441141858041e-05,
      "loss": 1.8089,
      "step": 3270
    },
    {
      "epoch": 0.3297642386769215,
      "grad_norm": 2.706418514251709,
      "learning_rate": 2.679432433341179e-05,
      "loss": 1.7725,
      "step": 3280
    },
    {
      "epoch": 0.3307696174533756,
      "grad_norm": 1.987703561782837,
      "learning_rate": 2.6784237248243166e-05,
      "loss": 1.7796,
      "step": 3290
    },
    {
      "epoch": 0.3317749962298296,
      "grad_norm": 2.018051862716675,
      "learning_rate": 2.6774150163074546e-05,
      "loss": 1.7845,
      "step": 3300
    },
    {
      "epoch": 0.3327803750062836,
      "grad_norm": 2.8045053482055664,
      "learning_rate": 2.6764063077905922e-05,
      "loss": 1.8073,
      "step": 3310
    },
    {
      "epoch": 0.33378575378273767,
      "grad_norm": 2.446622610092163,
      "learning_rate": 2.67539759927373e-05,
      "loss": 1.8226,
      "step": 3320
    },
    {
      "epoch": 0.3347911325591917,
      "grad_norm": 2.0071327686309814,
      "learning_rate": 2.6743888907568675e-05,
      "loss": 1.7946,
      "step": 3330
    },
    {
      "epoch": 0.3357965113356457,
      "grad_norm": 1.7759690284729004,
      "learning_rate": 2.6733801822400054e-05,
      "loss": 1.9136,
      "step": 3340
    },
    {
      "epoch": 0.3368018901120997,
      "grad_norm": 1.979271650314331,
      "learning_rate": 2.672371473723143e-05,
      "loss": 1.8134,
      "step": 3350
    },
    {
      "epoch": 0.3378072688885538,
      "grad_norm": 2.302701473236084,
      "learning_rate": 2.671362765206281e-05,
      "loss": 1.7825,
      "step": 3360
    },
    {
      "epoch": 0.3388126476650078,
      "grad_norm": 2.347738742828369,
      "learning_rate": 2.6703540566894187e-05,
      "loss": 1.779,
      "step": 3370
    },
    {
      "epoch": 0.3398180264414618,
      "grad_norm": 2.0868284702301025,
      "learning_rate": 2.6693453481725566e-05,
      "loss": 1.7593,
      "step": 3380
    },
    {
      "epoch": 0.3408234052179159,
      "grad_norm": 2.038438558578491,
      "learning_rate": 2.668336639655694e-05,
      "loss": 1.8487,
      "step": 3390
    },
    {
      "epoch": 0.3418287839943699,
      "grad_norm": 2.0280890464782715,
      "learning_rate": 2.667327931138832e-05,
      "loss": 1.8322,
      "step": 3400
    },
    {
      "epoch": 0.3428341627708239,
      "grad_norm": 1.7322195768356323,
      "learning_rate": 2.66631922262197e-05,
      "loss": 1.8569,
      "step": 3410
    },
    {
      "epoch": 0.3438395415472779,
      "grad_norm": 2.5118565559387207,
      "learning_rate": 2.6653105141051075e-05,
      "loss": 1.7706,
      "step": 3420
    },
    {
      "epoch": 0.344844920323732,
      "grad_norm": 2.692594051361084,
      "learning_rate": 2.6643018055882455e-05,
      "loss": 1.8323,
      "step": 3430
    },
    {
      "epoch": 0.345850299100186,
      "grad_norm": 1.8609225749969482,
      "learning_rate": 2.663293097071383e-05,
      "loss": 1.8278,
      "step": 3440
    },
    {
      "epoch": 0.34685567787664,
      "grad_norm": 2.353097915649414,
      "learning_rate": 2.6622843885545207e-05,
      "loss": 1.8042,
      "step": 3450
    },
    {
      "epoch": 0.3478610566530941,
      "grad_norm": 2.164731979370117,
      "learning_rate": 2.6612756800376584e-05,
      "loss": 1.783,
      "step": 3460
    },
    {
      "epoch": 0.3488664354295481,
      "grad_norm": 2.487304449081421,
      "learning_rate": 2.6602669715207963e-05,
      "loss": 1.9465,
      "step": 3470
    },
    {
      "epoch": 0.3498718142060021,
      "grad_norm": 2.667672872543335,
      "learning_rate": 2.659258263003934e-05,
      "loss": 1.8911,
      "step": 3480
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 2.648203134536743,
      "learning_rate": 2.658249554487072e-05,
      "loss": 1.7344,
      "step": 3490
    },
    {
      "epoch": 0.3518825717589102,
      "grad_norm": 2.3276240825653076,
      "learning_rate": 2.6572408459702096e-05,
      "loss": 1.7221,
      "step": 3500
    },
    {
      "epoch": 0.3528879505353642,
      "grad_norm": 1.8880447149276733,
      "learning_rate": 2.6562321374533472e-05,
      "loss": 1.856,
      "step": 3510
    },
    {
      "epoch": 0.3538933293118182,
      "grad_norm": 2.039973735809326,
      "learning_rate": 2.6552234289364848e-05,
      "loss": 1.792,
      "step": 3520
    },
    {
      "epoch": 0.35489870808827223,
      "grad_norm": 2.265648603439331,
      "learning_rate": 2.6542147204196228e-05,
      "loss": 1.8185,
      "step": 3530
    },
    {
      "epoch": 0.3559040868647263,
      "grad_norm": 2.38858962059021,
      "learning_rate": 2.6532060119027604e-05,
      "loss": 1.7685,
      "step": 3540
    },
    {
      "epoch": 0.3569094656411803,
      "grad_norm": 1.7691162824630737,
      "learning_rate": 2.6521973033858984e-05,
      "loss": 1.7916,
      "step": 3550
    },
    {
      "epoch": 0.3579148444176343,
      "grad_norm": 1.9352508783340454,
      "learning_rate": 2.651188594869036e-05,
      "loss": 1.8158,
      "step": 3560
    },
    {
      "epoch": 0.3589202231940884,
      "grad_norm": 1.8911478519439697,
      "learning_rate": 2.6501798863521737e-05,
      "loss": 1.7963,
      "step": 3570
    },
    {
      "epoch": 0.3599256019705424,
      "grad_norm": 1.7735072374343872,
      "learning_rate": 2.6491711778353116e-05,
      "loss": 1.8354,
      "step": 3580
    },
    {
      "epoch": 0.3609309807469964,
      "grad_norm": 1.98685622215271,
      "learning_rate": 2.6481624693184493e-05,
      "loss": 1.8223,
      "step": 3590
    },
    {
      "epoch": 0.36193635952345043,
      "grad_norm": 2.3216447830200195,
      "learning_rate": 2.6471537608015872e-05,
      "loss": 1.8631,
      "step": 3600
    },
    {
      "epoch": 0.3629417382999045,
      "grad_norm": 1.9348559379577637,
      "learning_rate": 2.646145052284725e-05,
      "loss": 1.7669,
      "step": 3610
    },
    {
      "epoch": 0.3639471170763585,
      "grad_norm": 2.4479289054870605,
      "learning_rate": 2.645136343767863e-05,
      "loss": 1.8515,
      "step": 3620
    },
    {
      "epoch": 0.36495249585281253,
      "grad_norm": 1.8435964584350586,
      "learning_rate": 2.644127635251e-05,
      "loss": 1.8041,
      "step": 3630
    },
    {
      "epoch": 0.3659578746292666,
      "grad_norm": 2.275505781173706,
      "learning_rate": 2.643118926734138e-05,
      "loss": 1.8029,
      "step": 3640
    },
    {
      "epoch": 0.3669632534057206,
      "grad_norm": 2.1936898231506348,
      "learning_rate": 2.6421102182172757e-05,
      "loss": 1.8204,
      "step": 3650
    },
    {
      "epoch": 0.3679686321821746,
      "grad_norm": 1.5536001920700073,
      "learning_rate": 2.6411015097004137e-05,
      "loss": 1.7981,
      "step": 3660
    },
    {
      "epoch": 0.36897401095862864,
      "grad_norm": 2.492550849914551,
      "learning_rate": 2.6400928011835513e-05,
      "loss": 1.7524,
      "step": 3670
    },
    {
      "epoch": 0.3699793897350827,
      "grad_norm": 1.8203644752502441,
      "learning_rate": 2.6390840926666893e-05,
      "loss": 1.8077,
      "step": 3680
    },
    {
      "epoch": 0.3709847685115367,
      "grad_norm": 2.4281044006347656,
      "learning_rate": 2.6380753841498266e-05,
      "loss": 1.8948,
      "step": 3690
    },
    {
      "epoch": 0.37199014728799074,
      "grad_norm": 1.9669685363769531,
      "learning_rate": 2.6370666756329646e-05,
      "loss": 1.7895,
      "step": 3700
    },
    {
      "epoch": 0.3729955260644448,
      "grad_norm": 1.9671714305877686,
      "learning_rate": 2.6360579671161022e-05,
      "loss": 1.8249,
      "step": 3710
    },
    {
      "epoch": 0.3740009048408988,
      "grad_norm": 2.278188943862915,
      "learning_rate": 2.63504925859924e-05,
      "loss": 1.7256,
      "step": 3720
    },
    {
      "epoch": 0.37500628361735283,
      "grad_norm": 1.9706419706344604,
      "learning_rate": 2.634040550082378e-05,
      "loss": 1.8299,
      "step": 3730
    },
    {
      "epoch": 0.37601166239380684,
      "grad_norm": 2.4804911613464355,
      "learning_rate": 2.6330318415655158e-05,
      "loss": 1.7764,
      "step": 3740
    },
    {
      "epoch": 0.3770170411702609,
      "grad_norm": 2.5430338382720947,
      "learning_rate": 2.6320231330486537e-05,
      "loss": 1.8029,
      "step": 3750
    },
    {
      "epoch": 0.3780224199467149,
      "grad_norm": 2.2601587772369385,
      "learning_rate": 2.631014424531791e-05,
      "loss": 1.8155,
      "step": 3760
    },
    {
      "epoch": 0.37902779872316894,
      "grad_norm": 1.5643240213394165,
      "learning_rate": 2.630005716014929e-05,
      "loss": 1.8429,
      "step": 3770
    },
    {
      "epoch": 0.380033177499623,
      "grad_norm": 2.6003823280334473,
      "learning_rate": 2.6289970074980666e-05,
      "loss": 1.7563,
      "step": 3780
    },
    {
      "epoch": 0.381038556276077,
      "grad_norm": 1.8518863916397095,
      "learning_rate": 2.6279882989812046e-05,
      "loss": 1.7202,
      "step": 3790
    },
    {
      "epoch": 0.38204393505253104,
      "grad_norm": 2.789726972579956,
      "learning_rate": 2.6269795904643422e-05,
      "loss": 1.7954,
      "step": 3800
    },
    {
      "epoch": 0.38304931382898505,
      "grad_norm": 2.7955803871154785,
      "learning_rate": 2.6259708819474802e-05,
      "loss": 1.777,
      "step": 3810
    },
    {
      "epoch": 0.3840546926054391,
      "grad_norm": 1.8206437826156616,
      "learning_rate": 2.6249621734306175e-05,
      "loss": 1.7534,
      "step": 3820
    },
    {
      "epoch": 0.38506007138189313,
      "grad_norm": 1.6253496408462524,
      "learning_rate": 2.6239534649137555e-05,
      "loss": 1.7554,
      "step": 3830
    },
    {
      "epoch": 0.38606545015834715,
      "grad_norm": 2.036987543106079,
      "learning_rate": 2.622944756396893e-05,
      "loss": 1.7485,
      "step": 3840
    },
    {
      "epoch": 0.38707082893480116,
      "grad_norm": 2.1231770515441895,
      "learning_rate": 2.621936047880031e-05,
      "loss": 1.8066,
      "step": 3850
    },
    {
      "epoch": 0.3880762077112552,
      "grad_norm": 2.497812271118164,
      "learning_rate": 2.6209273393631687e-05,
      "loss": 1.8519,
      "step": 3860
    },
    {
      "epoch": 0.38908158648770924,
      "grad_norm": 1.8506860733032227,
      "learning_rate": 2.6199186308463067e-05,
      "loss": 1.7936,
      "step": 3870
    },
    {
      "epoch": 0.39008696526416325,
      "grad_norm": 1.9245744943618774,
      "learning_rate": 2.6189099223294443e-05,
      "loss": 1.8166,
      "step": 3880
    },
    {
      "epoch": 0.3910923440406173,
      "grad_norm": 2.7221248149871826,
      "learning_rate": 2.617901213812582e-05,
      "loss": 1.8305,
      "step": 3890
    },
    {
      "epoch": 0.39209772281707134,
      "grad_norm": 1.8132137060165405,
      "learning_rate": 2.61689250529572e-05,
      "loss": 1.7719,
      "step": 3900
    },
    {
      "epoch": 0.39310310159352535,
      "grad_norm": 2.5205440521240234,
      "learning_rate": 2.6158837967788575e-05,
      "loss": 1.8831,
      "step": 3910
    },
    {
      "epoch": 0.39410848036997936,
      "grad_norm": 1.866208791732788,
      "learning_rate": 2.6148750882619955e-05,
      "loss": 1.7473,
      "step": 3920
    },
    {
      "epoch": 0.39511385914643343,
      "grad_norm": 1.6519242525100708,
      "learning_rate": 2.613866379745133e-05,
      "loss": 1.7595,
      "step": 3930
    },
    {
      "epoch": 0.39611923792288745,
      "grad_norm": 2.179610252380371,
      "learning_rate": 2.6128576712282708e-05,
      "loss": 1.7559,
      "step": 3940
    },
    {
      "epoch": 0.39712461669934146,
      "grad_norm": 1.6487845182418823,
      "learning_rate": 2.6118489627114084e-05,
      "loss": 1.8797,
      "step": 3950
    },
    {
      "epoch": 0.39812999547579553,
      "grad_norm": 2.1952743530273438,
      "learning_rate": 2.6108402541945464e-05,
      "loss": 1.8095,
      "step": 3960
    },
    {
      "epoch": 0.39913537425224954,
      "grad_norm": 2.3135986328125,
      "learning_rate": 2.609831545677684e-05,
      "loss": 1.8216,
      "step": 3970
    },
    {
      "epoch": 0.40014075302870356,
      "grad_norm": 1.9180313348770142,
      "learning_rate": 2.608822837160822e-05,
      "loss": 1.7567,
      "step": 3980
    },
    {
      "epoch": 0.40114613180515757,
      "grad_norm": 1.7413946390151978,
      "learning_rate": 2.6078141286439596e-05,
      "loss": 1.8036,
      "step": 3990
    },
    {
      "epoch": 0.40215151058161164,
      "grad_norm": 2.772919178009033,
      "learning_rate": 2.6068054201270972e-05,
      "loss": 1.7815,
      "step": 4000
    },
    {
      "epoch": 0.40315688935806565,
      "grad_norm": 2.1403396129608154,
      "learning_rate": 2.605796711610235e-05,
      "loss": 1.7615,
      "step": 4010
    },
    {
      "epoch": 0.40416226813451966,
      "grad_norm": 1.8654109239578247,
      "learning_rate": 2.604788003093373e-05,
      "loss": 1.8241,
      "step": 4020
    },
    {
      "epoch": 0.40516764691097373,
      "grad_norm": 3.2565507888793945,
      "learning_rate": 2.6037792945765108e-05,
      "loss": 1.8312,
      "step": 4030
    },
    {
      "epoch": 0.40617302568742775,
      "grad_norm": 2.1359810829162598,
      "learning_rate": 2.6027705860596484e-05,
      "loss": 1.8161,
      "step": 4040
    },
    {
      "epoch": 0.40717840446388176,
      "grad_norm": 1.9486271142959595,
      "learning_rate": 2.6017618775427864e-05,
      "loss": 1.8009,
      "step": 4050
    },
    {
      "epoch": 0.4081837832403358,
      "grad_norm": 2.4658100605010986,
      "learning_rate": 2.6007531690259237e-05,
      "loss": 1.7999,
      "step": 4060
    },
    {
      "epoch": 0.40918916201678984,
      "grad_norm": 1.7690739631652832,
      "learning_rate": 2.5997444605090617e-05,
      "loss": 1.7826,
      "step": 4070
    },
    {
      "epoch": 0.41019454079324386,
      "grad_norm": 2.1308233737945557,
      "learning_rate": 2.5987357519921993e-05,
      "loss": 1.747,
      "step": 4080
    },
    {
      "epoch": 0.41119991956969787,
      "grad_norm": 1.9670661687850952,
      "learning_rate": 2.5977270434753373e-05,
      "loss": 1.809,
      "step": 4090
    },
    {
      "epoch": 0.41220529834615194,
      "grad_norm": 2.149540424346924,
      "learning_rate": 2.596718334958475e-05,
      "loss": 1.8124,
      "step": 4100
    },
    {
      "epoch": 0.41321067712260595,
      "grad_norm": 2.160900831222534,
      "learning_rate": 2.595709626441613e-05,
      "loss": 1.8424,
      "step": 4110
    },
    {
      "epoch": 0.41421605589905997,
      "grad_norm": 2.015589475631714,
      "learning_rate": 2.59470091792475e-05,
      "loss": 1.8111,
      "step": 4120
    },
    {
      "epoch": 0.415221434675514,
      "grad_norm": 2.629584789276123,
      "learning_rate": 2.593692209407888e-05,
      "loss": 1.7442,
      "step": 4130
    },
    {
      "epoch": 0.41622681345196805,
      "grad_norm": 1.8644675016403198,
      "learning_rate": 2.5926835008910258e-05,
      "loss": 1.7876,
      "step": 4140
    },
    {
      "epoch": 0.41723219222842206,
      "grad_norm": 2.5285849571228027,
      "learning_rate": 2.5916747923741637e-05,
      "loss": 1.8036,
      "step": 4150
    },
    {
      "epoch": 0.4182375710048761,
      "grad_norm": 2.0149660110473633,
      "learning_rate": 2.5906660838573014e-05,
      "loss": 1.8109,
      "step": 4160
    },
    {
      "epoch": 0.41924294978133014,
      "grad_norm": 1.7591402530670166,
      "learning_rate": 2.5896573753404393e-05,
      "loss": 1.7208,
      "step": 4170
    },
    {
      "epoch": 0.42024832855778416,
      "grad_norm": 1.5663723945617676,
      "learning_rate": 2.588648666823577e-05,
      "loss": 1.8141,
      "step": 4180
    },
    {
      "epoch": 0.42125370733423817,
      "grad_norm": 3.268857002258301,
      "learning_rate": 2.5876399583067146e-05,
      "loss": 1.8793,
      "step": 4190
    },
    {
      "epoch": 0.4222590861106922,
      "grad_norm": 1.8955813646316528,
      "learning_rate": 2.5866312497898526e-05,
      "loss": 1.8154,
      "step": 4200
    },
    {
      "epoch": 0.42326446488714625,
      "grad_norm": 2.1405622959136963,
      "learning_rate": 2.5856225412729902e-05,
      "loss": 1.8131,
      "step": 4210
    },
    {
      "epoch": 0.42426984366360027,
      "grad_norm": 1.6529194116592407,
      "learning_rate": 2.5846138327561282e-05,
      "loss": 1.7771,
      "step": 4220
    },
    {
      "epoch": 0.4252752224400543,
      "grad_norm": 2.059661865234375,
      "learning_rate": 2.5836051242392658e-05,
      "loss": 1.8468,
      "step": 4230
    },
    {
      "epoch": 0.4262806012165083,
      "grad_norm": 2.150315523147583,
      "learning_rate": 2.5825964157224034e-05,
      "loss": 1.8311,
      "step": 4240
    },
    {
      "epoch": 0.42728597999296236,
      "grad_norm": 2.2456893920898438,
      "learning_rate": 2.581587707205541e-05,
      "loss": 1.7867,
      "step": 4250
    },
    {
      "epoch": 0.4282913587694164,
      "grad_norm": 2.0505287647247314,
      "learning_rate": 2.580578998688679e-05,
      "loss": 1.758,
      "step": 4260
    },
    {
      "epoch": 0.4292967375458704,
      "grad_norm": 1.8130309581756592,
      "learning_rate": 2.5795702901718167e-05,
      "loss": 1.8418,
      "step": 4270
    },
    {
      "epoch": 0.43030211632232446,
      "grad_norm": 1.9560483694076538,
      "learning_rate": 2.5785615816549546e-05,
      "loss": 1.8231,
      "step": 4280
    },
    {
      "epoch": 0.43130749509877847,
      "grad_norm": 2.3552498817443848,
      "learning_rate": 2.5775528731380923e-05,
      "loss": 1.792,
      "step": 4290
    },
    {
      "epoch": 0.4323128738752325,
      "grad_norm": 2.280635118484497,
      "learning_rate": 2.57654416462123e-05,
      "loss": 1.7554,
      "step": 4300
    },
    {
      "epoch": 0.4333182526516865,
      "grad_norm": 3.216885566711426,
      "learning_rate": 2.5755354561043675e-05,
      "loss": 1.8529,
      "step": 4310
    },
    {
      "epoch": 0.43432363142814057,
      "grad_norm": 1.6759928464889526,
      "learning_rate": 2.5745267475875055e-05,
      "loss": 1.8274,
      "step": 4320
    },
    {
      "epoch": 0.4353290102045946,
      "grad_norm": 2.711273670196533,
      "learning_rate": 2.5735180390706435e-05,
      "loss": 1.7379,
      "step": 4330
    },
    {
      "epoch": 0.4363343889810486,
      "grad_norm": 2.470215320587158,
      "learning_rate": 2.572509330553781e-05,
      "loss": 1.8172,
      "step": 4340
    },
    {
      "epoch": 0.43733976775750266,
      "grad_norm": 1.8786088228225708,
      "learning_rate": 2.571500622036919e-05,
      "loss": 1.7316,
      "step": 4350
    },
    {
      "epoch": 0.4383451465339567,
      "grad_norm": 2.024757146835327,
      "learning_rate": 2.5704919135200564e-05,
      "loss": 1.7865,
      "step": 4360
    },
    {
      "epoch": 0.4393505253104107,
      "grad_norm": 1.906266212463379,
      "learning_rate": 2.5694832050031943e-05,
      "loss": 1.8433,
      "step": 4370
    },
    {
      "epoch": 0.4403559040868647,
      "grad_norm": 2.2020676136016846,
      "learning_rate": 2.568474496486332e-05,
      "loss": 1.7944,
      "step": 4380
    },
    {
      "epoch": 0.44136128286331877,
      "grad_norm": 1.9005192518234253,
      "learning_rate": 2.56746578796947e-05,
      "loss": 1.8277,
      "step": 4390
    },
    {
      "epoch": 0.4423666616397728,
      "grad_norm": 2.0341763496398926,
      "learning_rate": 2.5664570794526076e-05,
      "loss": 1.7979,
      "step": 4400
    },
    {
      "epoch": 0.4433720404162268,
      "grad_norm": 2.7280631065368652,
      "learning_rate": 2.5654483709357455e-05,
      "loss": 1.8877,
      "step": 4410
    },
    {
      "epoch": 0.44437741919268087,
      "grad_norm": 1.7945961952209473,
      "learning_rate": 2.5644396624188828e-05,
      "loss": 1.7462,
      "step": 4420
    },
    {
      "epoch": 0.4453827979691349,
      "grad_norm": 2.730475664138794,
      "learning_rate": 2.5634309539020208e-05,
      "loss": 1.7463,
      "step": 4430
    },
    {
      "epoch": 0.4463881767455889,
      "grad_norm": 2.0043954849243164,
      "learning_rate": 2.5624222453851584e-05,
      "loss": 1.7378,
      "step": 4440
    },
    {
      "epoch": 0.4473935555220429,
      "grad_norm": 2.607084274291992,
      "learning_rate": 2.5614135368682964e-05,
      "loss": 1.7416,
      "step": 4450
    },
    {
      "epoch": 0.448398934298497,
      "grad_norm": 2.2599101066589355,
      "learning_rate": 2.560404828351434e-05,
      "loss": 1.8094,
      "step": 4460
    },
    {
      "epoch": 0.449404313074951,
      "grad_norm": 1.8024104833602905,
      "learning_rate": 2.559396119834572e-05,
      "loss": 1.7485,
      "step": 4470
    },
    {
      "epoch": 0.450409691851405,
      "grad_norm": 1.6476967334747314,
      "learning_rate": 2.5583874113177096e-05,
      "loss": 1.7845,
      "step": 4480
    },
    {
      "epoch": 0.45141507062785907,
      "grad_norm": 2.2360050678253174,
      "learning_rate": 2.5573787028008473e-05,
      "loss": 1.7648,
      "step": 4490
    },
    {
      "epoch": 0.4524204494043131,
      "grad_norm": 1.9028418064117432,
      "learning_rate": 2.5563699942839852e-05,
      "loss": 1.8112,
      "step": 4500
    },
    {
      "epoch": 0.4534258281807671,
      "grad_norm": 1.8325183391571045,
      "learning_rate": 2.555361285767123e-05,
      "loss": 1.8602,
      "step": 4510
    },
    {
      "epoch": 0.4544312069572211,
      "grad_norm": 2.333076000213623,
      "learning_rate": 2.554352577250261e-05,
      "loss": 1.8488,
      "step": 4520
    },
    {
      "epoch": 0.4554365857336752,
      "grad_norm": 2.0988309383392334,
      "learning_rate": 2.5533438687333985e-05,
      "loss": 1.7138,
      "step": 4530
    },
    {
      "epoch": 0.4564419645101292,
      "grad_norm": 1.9828641414642334,
      "learning_rate": 2.552335160216536e-05,
      "loss": 1.7898,
      "step": 4540
    },
    {
      "epoch": 0.4574473432865832,
      "grad_norm": 1.8305360078811646,
      "learning_rate": 2.5513264516996737e-05,
      "loss": 1.8092,
      "step": 4550
    },
    {
      "epoch": 0.4584527220630373,
      "grad_norm": 3.1962978839874268,
      "learning_rate": 2.5503177431828117e-05,
      "loss": 1.8312,
      "step": 4560
    },
    {
      "epoch": 0.4594581008394913,
      "grad_norm": 2.2224481105804443,
      "learning_rate": 2.5493090346659493e-05,
      "loss": 1.7816,
      "step": 4570
    },
    {
      "epoch": 0.4604634796159453,
      "grad_norm": 2.334615707397461,
      "learning_rate": 2.5483003261490873e-05,
      "loss": 1.7668,
      "step": 4580
    },
    {
      "epoch": 0.4614688583923993,
      "grad_norm": 2.1750199794769287,
      "learning_rate": 2.547291617632225e-05,
      "loss": 1.7296,
      "step": 4590
    },
    {
      "epoch": 0.4624742371688534,
      "grad_norm": 2.0215611457824707,
      "learning_rate": 2.5462829091153626e-05,
      "loss": 1.7556,
      "step": 4600
    },
    {
      "epoch": 0.4634796159453074,
      "grad_norm": 2.047295093536377,
      "learning_rate": 2.5452742005985002e-05,
      "loss": 1.8436,
      "step": 4610
    },
    {
      "epoch": 0.4644849947217614,
      "grad_norm": 1.7340567111968994,
      "learning_rate": 2.544265492081638e-05,
      "loss": 1.7865,
      "step": 4620
    },
    {
      "epoch": 0.4654903734982154,
      "grad_norm": 1.9171338081359863,
      "learning_rate": 2.543256783564776e-05,
      "loss": 1.7804,
      "step": 4630
    },
    {
      "epoch": 0.4664957522746695,
      "grad_norm": 2.595794916152954,
      "learning_rate": 2.5422480750479138e-05,
      "loss": 1.7782,
      "step": 4640
    },
    {
      "epoch": 0.4675011310511235,
      "grad_norm": 2.4596145153045654,
      "learning_rate": 2.5412393665310517e-05,
      "loss": 1.787,
      "step": 4650
    },
    {
      "epoch": 0.4685065098275775,
      "grad_norm": 1.778241753578186,
      "learning_rate": 2.540230658014189e-05,
      "loss": 1.7513,
      "step": 4660
    },
    {
      "epoch": 0.4695118886040316,
      "grad_norm": 2.457470178604126,
      "learning_rate": 2.539221949497327e-05,
      "loss": 1.7739,
      "step": 4670
    },
    {
      "epoch": 0.4705172673804856,
      "grad_norm": 1.8046653270721436,
      "learning_rate": 2.5382132409804646e-05,
      "loss": 1.833,
      "step": 4680
    },
    {
      "epoch": 0.4715226461569396,
      "grad_norm": 1.9610296487808228,
      "learning_rate": 2.5372045324636026e-05,
      "loss": 1.7602,
      "step": 4690
    },
    {
      "epoch": 0.47252802493339363,
      "grad_norm": 2.4345991611480713,
      "learning_rate": 2.5361958239467402e-05,
      "loss": 1.7619,
      "step": 4700
    },
    {
      "epoch": 0.4735334037098477,
      "grad_norm": 1.8692878484725952,
      "learning_rate": 2.5351871154298782e-05,
      "loss": 1.7512,
      "step": 4710
    },
    {
      "epoch": 0.4745387824863017,
      "grad_norm": 2.1361348628997803,
      "learning_rate": 2.5341784069130155e-05,
      "loss": 1.8177,
      "step": 4720
    },
    {
      "epoch": 0.4755441612627557,
      "grad_norm": 2.7320024967193604,
      "learning_rate": 2.5331696983961535e-05,
      "loss": 1.8463,
      "step": 4730
    },
    {
      "epoch": 0.4765495400392098,
      "grad_norm": 1.9874616861343384,
      "learning_rate": 2.532160989879291e-05,
      "loss": 1.8057,
      "step": 4740
    },
    {
      "epoch": 0.4775549188156638,
      "grad_norm": 2.2328124046325684,
      "learning_rate": 2.531152281362429e-05,
      "loss": 1.9071,
      "step": 4750
    },
    {
      "epoch": 0.4785602975921178,
      "grad_norm": 2.2151424884796143,
      "learning_rate": 2.5301435728455667e-05,
      "loss": 1.7454,
      "step": 4760
    },
    {
      "epoch": 0.47956567636857184,
      "grad_norm": 1.9932470321655273,
      "learning_rate": 2.5291348643287047e-05,
      "loss": 1.7951,
      "step": 4770
    },
    {
      "epoch": 0.4805710551450259,
      "grad_norm": 1.6455179452896118,
      "learning_rate": 2.5281261558118423e-05,
      "loss": 1.8186,
      "step": 4780
    },
    {
      "epoch": 0.4815764339214799,
      "grad_norm": 1.7298345565795898,
      "learning_rate": 2.52711744729498e-05,
      "loss": 1.7608,
      "step": 4790
    },
    {
      "epoch": 0.48258181269793393,
      "grad_norm": 1.6156727075576782,
      "learning_rate": 2.526108738778118e-05,
      "loss": 1.7221,
      "step": 4800
    },
    {
      "epoch": 0.483587191474388,
      "grad_norm": 2.4215011596679688,
      "learning_rate": 2.5251000302612555e-05,
      "loss": 1.771,
      "step": 4810
    },
    {
      "epoch": 0.484592570250842,
      "grad_norm": 1.9653418064117432,
      "learning_rate": 2.5240913217443935e-05,
      "loss": 1.8016,
      "step": 4820
    },
    {
      "epoch": 0.485597949027296,
      "grad_norm": 2.0818874835968018,
      "learning_rate": 2.523082613227531e-05,
      "loss": 1.8884,
      "step": 4830
    },
    {
      "epoch": 0.48660332780375004,
      "grad_norm": 1.9695326089859009,
      "learning_rate": 2.5220739047106688e-05,
      "loss": 1.7992,
      "step": 4840
    },
    {
      "epoch": 0.4876087065802041,
      "grad_norm": 2.088113784790039,
      "learning_rate": 2.5210651961938064e-05,
      "loss": 1.8686,
      "step": 4850
    },
    {
      "epoch": 0.4886140853566581,
      "grad_norm": 1.8735692501068115,
      "learning_rate": 2.5200564876769444e-05,
      "loss": 1.7038,
      "step": 4860
    },
    {
      "epoch": 0.48961946413311214,
      "grad_norm": 1.771715760231018,
      "learning_rate": 2.519047779160082e-05,
      "loss": 1.7142,
      "step": 4870
    },
    {
      "epoch": 0.4906248429095662,
      "grad_norm": 1.8058394193649292,
      "learning_rate": 2.51803907064322e-05,
      "loss": 1.7731,
      "step": 4880
    },
    {
      "epoch": 0.4916302216860202,
      "grad_norm": 2.3969569206237793,
      "learning_rate": 2.5170303621263576e-05,
      "loss": 1.8265,
      "step": 4890
    },
    {
      "epoch": 0.49263560046247423,
      "grad_norm": 2.630741596221924,
      "learning_rate": 2.5160216536094952e-05,
      "loss": 1.7789,
      "step": 4900
    },
    {
      "epoch": 0.49364097923892825,
      "grad_norm": 2.021695613861084,
      "learning_rate": 2.515012945092633e-05,
      "loss": 1.8057,
      "step": 4910
    },
    {
      "epoch": 0.4946463580153823,
      "grad_norm": 2.6572370529174805,
      "learning_rate": 2.514004236575771e-05,
      "loss": 1.8128,
      "step": 4920
    },
    {
      "epoch": 0.49565173679183633,
      "grad_norm": 1.9186826944351196,
      "learning_rate": 2.5129955280589088e-05,
      "loss": 1.8144,
      "step": 4930
    },
    {
      "epoch": 0.49665711556829034,
      "grad_norm": 2.045252799987793,
      "learning_rate": 2.5119868195420464e-05,
      "loss": 1.7915,
      "step": 4940
    },
    {
      "epoch": 0.49766249434474435,
      "grad_norm": 2.0675809383392334,
      "learning_rate": 2.5109781110251844e-05,
      "loss": 1.7936,
      "step": 4950
    },
    {
      "epoch": 0.4986678731211984,
      "grad_norm": 2.0321860313415527,
      "learning_rate": 2.509969402508322e-05,
      "loss": 1.8015,
      "step": 4960
    },
    {
      "epoch": 0.49967325189765244,
      "grad_norm": 2.4404191970825195,
      "learning_rate": 2.5089606939914597e-05,
      "loss": 1.758,
      "step": 4970
    },
    {
      "epoch": 0.5006786306741065,
      "grad_norm": 1.709632396697998,
      "learning_rate": 2.5079519854745973e-05,
      "loss": 1.7747,
      "step": 4980
    },
    {
      "epoch": 0.5016840094505605,
      "grad_norm": 2.656087636947632,
      "learning_rate": 2.5069432769577353e-05,
      "loss": 1.7269,
      "step": 4990
    },
    {
      "epoch": 0.5026893882270145,
      "grad_norm": 1.9943656921386719,
      "learning_rate": 2.505934568440873e-05,
      "loss": 1.8247,
      "step": 5000
    },
    {
      "epoch": 0.5036947670034686,
      "grad_norm": 1.898701548576355,
      "learning_rate": 2.504925859924011e-05,
      "loss": 1.8228,
      "step": 5010
    },
    {
      "epoch": 0.5047001457799226,
      "grad_norm": 2.172477960586548,
      "learning_rate": 2.5039171514071485e-05,
      "loss": 1.9061,
      "step": 5020
    },
    {
      "epoch": 0.5057055245563766,
      "grad_norm": 2.0516350269317627,
      "learning_rate": 2.502908442890286e-05,
      "loss": 1.6972,
      "step": 5030
    },
    {
      "epoch": 0.5067109033328306,
      "grad_norm": 2.4145936965942383,
      "learning_rate": 2.5018997343734238e-05,
      "loss": 1.7101,
      "step": 5040
    },
    {
      "epoch": 0.5077162821092847,
      "grad_norm": 1.4637359380722046,
      "learning_rate": 2.5008910258565617e-05,
      "loss": 1.7913,
      "step": 5050
    },
    {
      "epoch": 0.5087216608857387,
      "grad_norm": 3.056136131286621,
      "learning_rate": 2.4998823173396994e-05,
      "loss": 1.8194,
      "step": 5060
    },
    {
      "epoch": 0.5097270396621927,
      "grad_norm": 1.614419937133789,
      "learning_rate": 2.4988736088228373e-05,
      "loss": 1.7892,
      "step": 5070
    },
    {
      "epoch": 0.5107324184386468,
      "grad_norm": 2.004408359527588,
      "learning_rate": 2.4978649003059753e-05,
      "loss": 1.704,
      "step": 5080
    },
    {
      "epoch": 0.5117377972151008,
      "grad_norm": 1.8633314371109009,
      "learning_rate": 2.4968561917891126e-05,
      "loss": 1.7087,
      "step": 5090
    },
    {
      "epoch": 0.5127431759915548,
      "grad_norm": 1.8347582817077637,
      "learning_rate": 2.4958474832722506e-05,
      "loss": 1.7763,
      "step": 5100
    },
    {
      "epoch": 0.5137485547680088,
      "grad_norm": 2.1423768997192383,
      "learning_rate": 2.4948387747553882e-05,
      "loss": 1.867,
      "step": 5110
    },
    {
      "epoch": 0.5147539335444629,
      "grad_norm": 2.856485605239868,
      "learning_rate": 2.4938300662385262e-05,
      "loss": 1.8323,
      "step": 5120
    },
    {
      "epoch": 0.5157593123209169,
      "grad_norm": 2.1879971027374268,
      "learning_rate": 2.4928213577216638e-05,
      "loss": 1.7341,
      "step": 5130
    },
    {
      "epoch": 0.5167646910973709,
      "grad_norm": 1.883408546447754,
      "learning_rate": 2.4918126492048018e-05,
      "loss": 1.7908,
      "step": 5140
    },
    {
      "epoch": 0.517770069873825,
      "grad_norm": 2.471245765686035,
      "learning_rate": 2.490803940687939e-05,
      "loss": 1.8549,
      "step": 5150
    },
    {
      "epoch": 0.518775448650279,
      "grad_norm": 1.7101787328720093,
      "learning_rate": 2.489795232171077e-05,
      "loss": 1.8487,
      "step": 5160
    },
    {
      "epoch": 0.519780827426733,
      "grad_norm": 1.8577431440353394,
      "learning_rate": 2.4887865236542147e-05,
      "loss": 1.8316,
      "step": 5170
    },
    {
      "epoch": 0.520786206203187,
      "grad_norm": 1.8871830701828003,
      "learning_rate": 2.4877778151373526e-05,
      "loss": 1.8526,
      "step": 5180
    },
    {
      "epoch": 0.5217915849796411,
      "grad_norm": 1.9712334871292114,
      "learning_rate": 2.4867691066204903e-05,
      "loss": 1.806,
      "step": 5190
    },
    {
      "epoch": 0.5227969637560951,
      "grad_norm": 2.379260778427124,
      "learning_rate": 2.4857603981036282e-05,
      "loss": 1.7519,
      "step": 5200
    },
    {
      "epoch": 0.5238023425325491,
      "grad_norm": 2.195338010787964,
      "learning_rate": 2.4847516895867655e-05,
      "loss": 1.767,
      "step": 5210
    },
    {
      "epoch": 0.5248077213090032,
      "grad_norm": 1.6985599994659424,
      "learning_rate": 2.4837429810699035e-05,
      "loss": 1.8288,
      "step": 5220
    },
    {
      "epoch": 0.5258131000854572,
      "grad_norm": 1.8047304153442383,
      "learning_rate": 2.4827342725530415e-05,
      "loss": 1.7528,
      "step": 5230
    },
    {
      "epoch": 0.5268184788619112,
      "grad_norm": 1.5850627422332764,
      "learning_rate": 2.481725564036179e-05,
      "loss": 1.7789,
      "step": 5240
    },
    {
      "epoch": 0.5278238576383653,
      "grad_norm": 1.8973159790039062,
      "learning_rate": 2.480716855519317e-05,
      "loss": 1.8277,
      "step": 5250
    },
    {
      "epoch": 0.5288292364148193,
      "grad_norm": 2.946967840194702,
      "learning_rate": 2.4797081470024547e-05,
      "loss": 1.6959,
      "step": 5260
    },
    {
      "epoch": 0.5298346151912733,
      "grad_norm": 1.782736897468567,
      "learning_rate": 2.4786994384855923e-05,
      "loss": 1.8266,
      "step": 5270
    },
    {
      "epoch": 0.5308399939677273,
      "grad_norm": 1.9257452487945557,
      "learning_rate": 2.47769072996873e-05,
      "loss": 1.84,
      "step": 5280
    },
    {
      "epoch": 0.5318453727441814,
      "grad_norm": 2.3210785388946533,
      "learning_rate": 2.476682021451868e-05,
      "loss": 1.7781,
      "step": 5290
    },
    {
      "epoch": 0.5328507515206354,
      "grad_norm": 1.592289924621582,
      "learning_rate": 2.4756733129350056e-05,
      "loss": 1.7268,
      "step": 5300
    },
    {
      "epoch": 0.5338561302970894,
      "grad_norm": 2.1801085472106934,
      "learning_rate": 2.4746646044181435e-05,
      "loss": 1.6683,
      "step": 5310
    },
    {
      "epoch": 0.5348615090735435,
      "grad_norm": 2.4116153717041016,
      "learning_rate": 2.4736558959012812e-05,
      "loss": 1.7982,
      "step": 5320
    },
    {
      "epoch": 0.5358668878499975,
      "grad_norm": 2.40433931350708,
      "learning_rate": 2.4726471873844188e-05,
      "loss": 1.7909,
      "step": 5330
    },
    {
      "epoch": 0.5368722666264515,
      "grad_norm": 1.525398850440979,
      "learning_rate": 2.4716384788675564e-05,
      "loss": 1.8018,
      "step": 5340
    },
    {
      "epoch": 0.5378776454029055,
      "grad_norm": 1.7495379447937012,
      "learning_rate": 2.4706297703506944e-05,
      "loss": 1.8345,
      "step": 5350
    },
    {
      "epoch": 0.5388830241793596,
      "grad_norm": 1.933394432067871,
      "learning_rate": 2.469621061833832e-05,
      "loss": 1.8355,
      "step": 5360
    },
    {
      "epoch": 0.5398884029558136,
      "grad_norm": 1.9235903024673462,
      "learning_rate": 2.46861235331697e-05,
      "loss": 1.7814,
      "step": 5370
    },
    {
      "epoch": 0.5408937817322677,
      "grad_norm": 2.145585298538208,
      "learning_rate": 2.4676036448001076e-05,
      "loss": 1.795,
      "step": 5380
    },
    {
      "epoch": 0.5418991605087217,
      "grad_norm": 1.8396384716033936,
      "learning_rate": 2.4665949362832453e-05,
      "loss": 1.8004,
      "step": 5390
    },
    {
      "epoch": 0.5429045392851757,
      "grad_norm": 2.2500555515289307,
      "learning_rate": 2.4655862277663832e-05,
      "loss": 1.8032,
      "step": 5400
    },
    {
      "epoch": 0.5439099180616297,
      "grad_norm": 1.9510670900344849,
      "learning_rate": 2.464577519249521e-05,
      "loss": 1.8354,
      "step": 5410
    },
    {
      "epoch": 0.5449152968380837,
      "grad_norm": 2.0376644134521484,
      "learning_rate": 2.463568810732659e-05,
      "loss": 1.7549,
      "step": 5420
    },
    {
      "epoch": 0.5459206756145378,
      "grad_norm": 2.4860551357269287,
      "learning_rate": 2.4625601022157965e-05,
      "loss": 1.7955,
      "step": 5430
    },
    {
      "epoch": 0.5469260543909918,
      "grad_norm": 2.2334437370300293,
      "learning_rate": 2.4615513936989344e-05,
      "loss": 1.8102,
      "step": 5440
    },
    {
      "epoch": 0.5479314331674459,
      "grad_norm": 1.9376113414764404,
      "learning_rate": 2.4605426851820717e-05,
      "loss": 1.743,
      "step": 5450
    },
    {
      "epoch": 0.5489368119438999,
      "grad_norm": 2.0506985187530518,
      "learning_rate": 2.4595339766652097e-05,
      "loss": 1.7721,
      "step": 5460
    },
    {
      "epoch": 0.5499421907203539,
      "grad_norm": 2.188023567199707,
      "learning_rate": 2.4585252681483473e-05,
      "loss": 1.8846,
      "step": 5470
    },
    {
      "epoch": 0.5509475694968079,
      "grad_norm": 2.7551300525665283,
      "learning_rate": 2.4575165596314853e-05,
      "loss": 1.752,
      "step": 5480
    },
    {
      "epoch": 0.5519529482732619,
      "grad_norm": 2.1103477478027344,
      "learning_rate": 2.456507851114623e-05,
      "loss": 1.8392,
      "step": 5490
    },
    {
      "epoch": 0.5529583270497159,
      "grad_norm": 1.815843105316162,
      "learning_rate": 2.455499142597761e-05,
      "loss": 1.7655,
      "step": 5500
    },
    {
      "epoch": 0.55396370582617,
      "grad_norm": 1.683628797531128,
      "learning_rate": 2.4544904340808982e-05,
      "loss": 1.7056,
      "step": 5510
    },
    {
      "epoch": 0.5549690846026241,
      "grad_norm": 1.6609086990356445,
      "learning_rate": 2.4534817255640362e-05,
      "loss": 1.7605,
      "step": 5520
    },
    {
      "epoch": 0.5559744633790781,
      "grad_norm": 2.1913135051727295,
      "learning_rate": 2.4524730170471738e-05,
      "loss": 1.7951,
      "step": 5530
    },
    {
      "epoch": 0.5569798421555321,
      "grad_norm": 1.9238057136535645,
      "learning_rate": 2.4514643085303118e-05,
      "loss": 1.8147,
      "step": 5540
    },
    {
      "epoch": 0.5579852209319861,
      "grad_norm": 1.7583348751068115,
      "learning_rate": 2.4504556000134497e-05,
      "loss": 1.8096,
      "step": 5550
    },
    {
      "epoch": 0.5589905997084401,
      "grad_norm": 2.0638391971588135,
      "learning_rate": 2.4494468914965874e-05,
      "loss": 1.7777,
      "step": 5560
    },
    {
      "epoch": 0.5599959784848941,
      "grad_norm": 1.7908908128738403,
      "learning_rate": 2.448438182979725e-05,
      "loss": 1.7783,
      "step": 5570
    },
    {
      "epoch": 0.5610013572613483,
      "grad_norm": 2.2296760082244873,
      "learning_rate": 2.4474294744628626e-05,
      "loss": 1.8124,
      "step": 5580
    },
    {
      "epoch": 0.5620067360378023,
      "grad_norm": 2.154510974884033,
      "learning_rate": 2.4464207659460006e-05,
      "loss": 1.7018,
      "step": 5590
    },
    {
      "epoch": 0.5630121148142563,
      "grad_norm": 2.583306074142456,
      "learning_rate": 2.4454120574291382e-05,
      "loss": 1.7154,
      "step": 5600
    },
    {
      "epoch": 0.5640174935907103,
      "grad_norm": 2.1263961791992188,
      "learning_rate": 2.4444033489122762e-05,
      "loss": 1.762,
      "step": 5610
    },
    {
      "epoch": 0.5650228723671643,
      "grad_norm": 2.654752016067505,
      "learning_rate": 2.443394640395414e-05,
      "loss": 1.7664,
      "step": 5620
    },
    {
      "epoch": 0.5660282511436183,
      "grad_norm": 2.458460569381714,
      "learning_rate": 2.4423859318785515e-05,
      "loss": 1.7992,
      "step": 5630
    },
    {
      "epoch": 0.5670336299200723,
      "grad_norm": 2.2472617626190186,
      "learning_rate": 2.441377223361689e-05,
      "loss": 1.8552,
      "step": 5640
    },
    {
      "epoch": 0.5680390086965265,
      "grad_norm": 2.340620279312134,
      "learning_rate": 2.440368514844827e-05,
      "loss": 1.7783,
      "step": 5650
    },
    {
      "epoch": 0.5690443874729805,
      "grad_norm": 1.6860365867614746,
      "learning_rate": 2.4393598063279647e-05,
      "loss": 1.8626,
      "step": 5660
    },
    {
      "epoch": 0.5700497662494345,
      "grad_norm": 1.996596336364746,
      "learning_rate": 2.4383510978111027e-05,
      "loss": 1.7993,
      "step": 5670
    },
    {
      "epoch": 0.5710551450258885,
      "grad_norm": 1.5993446111679077,
      "learning_rate": 2.4373423892942403e-05,
      "loss": 1.8123,
      "step": 5680
    },
    {
      "epoch": 0.5720605238023425,
      "grad_norm": 1.8652987480163574,
      "learning_rate": 2.436333680777378e-05,
      "loss": 1.7723,
      "step": 5690
    },
    {
      "epoch": 0.5730659025787965,
      "grad_norm": 2.3031160831451416,
      "learning_rate": 2.435324972260516e-05,
      "loss": 1.8165,
      "step": 5700
    },
    {
      "epoch": 0.5740712813552505,
      "grad_norm": 2.188657283782959,
      "learning_rate": 2.4343162637436535e-05,
      "loss": 1.8173,
      "step": 5710
    },
    {
      "epoch": 0.5750766601317047,
      "grad_norm": 2.7153868675231934,
      "learning_rate": 2.4333075552267915e-05,
      "loss": 1.6951,
      "step": 5720
    },
    {
      "epoch": 0.5760820389081587,
      "grad_norm": 2.3686158657073975,
      "learning_rate": 2.432298846709929e-05,
      "loss": 1.8161,
      "step": 5730
    },
    {
      "epoch": 0.5770874176846127,
      "grad_norm": 2.0677006244659424,
      "learning_rate": 2.431290138193067e-05,
      "loss": 1.7861,
      "step": 5740
    },
    {
      "epoch": 0.5780927964610667,
      "grad_norm": 2.6268417835235596,
      "learning_rate": 2.4302814296762044e-05,
      "loss": 1.6988,
      "step": 5750
    },
    {
      "epoch": 0.5790981752375207,
      "grad_norm": 2.2326853275299072,
      "learning_rate": 2.4292727211593424e-05,
      "loss": 1.7762,
      "step": 5760
    },
    {
      "epoch": 0.5801035540139747,
      "grad_norm": 2.273958444595337,
      "learning_rate": 2.42826401264248e-05,
      "loss": 1.7856,
      "step": 5770
    },
    {
      "epoch": 0.5811089327904287,
      "grad_norm": 2.2681150436401367,
      "learning_rate": 2.427255304125618e-05,
      "loss": 1.8319,
      "step": 5780
    },
    {
      "epoch": 0.5821143115668829,
      "grad_norm": 1.7629806995391846,
      "learning_rate": 2.4262465956087556e-05,
      "loss": 1.8225,
      "step": 5790
    },
    {
      "epoch": 0.5831196903433369,
      "grad_norm": 2.5060231685638428,
      "learning_rate": 2.4252378870918936e-05,
      "loss": 1.8128,
      "step": 5800
    },
    {
      "epoch": 0.5841250691197909,
      "grad_norm": 2.133730888366699,
      "learning_rate": 2.424229178575031e-05,
      "loss": 1.8253,
      "step": 5810
    },
    {
      "epoch": 0.5851304478962449,
      "grad_norm": 2.4212160110473633,
      "learning_rate": 2.423220470058169e-05,
      "loss": 1.8141,
      "step": 5820
    },
    {
      "epoch": 0.5861358266726989,
      "grad_norm": 1.6411464214324951,
      "learning_rate": 2.4222117615413065e-05,
      "loss": 1.8592,
      "step": 5830
    },
    {
      "epoch": 0.5871412054491529,
      "grad_norm": 1.8901036977767944,
      "learning_rate": 2.4212030530244444e-05,
      "loss": 1.7697,
      "step": 5840
    },
    {
      "epoch": 0.588146584225607,
      "grad_norm": 1.94386625289917,
      "learning_rate": 2.4201943445075824e-05,
      "loss": 1.8203,
      "step": 5850
    },
    {
      "epoch": 0.5891519630020611,
      "grad_norm": 2.470551013946533,
      "learning_rate": 2.41918563599072e-05,
      "loss": 1.8434,
      "step": 5860
    },
    {
      "epoch": 0.5901573417785151,
      "grad_norm": 2.0020172595977783,
      "learning_rate": 2.4181769274738577e-05,
      "loss": 1.7736,
      "step": 5870
    },
    {
      "epoch": 0.5911627205549691,
      "grad_norm": 1.9781134128570557,
      "learning_rate": 2.4171682189569953e-05,
      "loss": 1.7242,
      "step": 5880
    },
    {
      "epoch": 0.5921680993314231,
      "grad_norm": 2.038026809692383,
      "learning_rate": 2.4161595104401333e-05,
      "loss": 1.7945,
      "step": 5890
    },
    {
      "epoch": 0.5931734781078771,
      "grad_norm": 2.0951054096221924,
      "learning_rate": 2.415150801923271e-05,
      "loss": 1.7708,
      "step": 5900
    },
    {
      "epoch": 0.5941788568843311,
      "grad_norm": 2.694246530532837,
      "learning_rate": 2.414142093406409e-05,
      "loss": 1.8303,
      "step": 5910
    },
    {
      "epoch": 0.5951842356607852,
      "grad_norm": 2.304396152496338,
      "learning_rate": 2.4131333848895465e-05,
      "loss": 1.7597,
      "step": 5920
    },
    {
      "epoch": 0.5961896144372393,
      "grad_norm": 2.2375826835632324,
      "learning_rate": 2.412124676372684e-05,
      "loss": 1.812,
      "step": 5930
    },
    {
      "epoch": 0.5971949932136933,
      "grad_norm": 1.9827953577041626,
      "learning_rate": 2.4111159678558218e-05,
      "loss": 1.7858,
      "step": 5940
    },
    {
      "epoch": 0.5982003719901473,
      "grad_norm": 2.240741491317749,
      "learning_rate": 2.4101072593389597e-05,
      "loss": 1.8054,
      "step": 5950
    },
    {
      "epoch": 0.5992057507666013,
      "grad_norm": 1.8699625730514526,
      "learning_rate": 2.4090985508220974e-05,
      "loss": 1.8529,
      "step": 5960
    },
    {
      "epoch": 0.6002111295430553,
      "grad_norm": 2.146394968032837,
      "learning_rate": 2.4080898423052353e-05,
      "loss": 1.6623,
      "step": 5970
    },
    {
      "epoch": 0.6012165083195093,
      "grad_norm": 1.9641258716583252,
      "learning_rate": 2.407081133788373e-05,
      "loss": 1.7409,
      "step": 5980
    },
    {
      "epoch": 0.6022218870959634,
      "grad_norm": 1.8507684469223022,
      "learning_rate": 2.4060724252715106e-05,
      "loss": 1.8469,
      "step": 5990
    },
    {
      "epoch": 0.6032272658724175,
      "grad_norm": 2.446256637573242,
      "learning_rate": 2.4050637167546486e-05,
      "loss": 1.7324,
      "step": 6000
    },
    {
      "epoch": 0.6042326446488715,
      "grad_norm": 2.2139108180999756,
      "learning_rate": 2.4040550082377862e-05,
      "loss": 1.8039,
      "step": 6010
    },
    {
      "epoch": 0.6052380234253255,
      "grad_norm": 2.9955058097839355,
      "learning_rate": 2.4030462997209242e-05,
      "loss": 1.768,
      "step": 6020
    },
    {
      "epoch": 0.6062434022017795,
      "grad_norm": 1.8445338010787964,
      "learning_rate": 2.4020375912040618e-05,
      "loss": 1.7473,
      "step": 6030
    },
    {
      "epoch": 0.6072487809782335,
      "grad_norm": 2.224822759628296,
      "learning_rate": 2.4010288826871998e-05,
      "loss": 1.7832,
      "step": 6040
    },
    {
      "epoch": 0.6082541597546876,
      "grad_norm": 2.042785167694092,
      "learning_rate": 2.400020174170337e-05,
      "loss": 1.8247,
      "step": 6050
    },
    {
      "epoch": 0.6092595385311416,
      "grad_norm": 1.9729492664337158,
      "learning_rate": 2.399011465653475e-05,
      "loss": 1.7579,
      "step": 6060
    },
    {
      "epoch": 0.6102649173075957,
      "grad_norm": 1.820202112197876,
      "learning_rate": 2.3980027571366127e-05,
      "loss": 1.8499,
      "step": 6070
    },
    {
      "epoch": 0.6112702960840497,
      "grad_norm": 2.597644805908203,
      "learning_rate": 2.3969940486197506e-05,
      "loss": 1.7661,
      "step": 6080
    },
    {
      "epoch": 0.6122756748605037,
      "grad_norm": 2.6580841541290283,
      "learning_rate": 2.3959853401028883e-05,
      "loss": 1.7672,
      "step": 6090
    },
    {
      "epoch": 0.6132810536369577,
      "grad_norm": 2.403280735015869,
      "learning_rate": 2.3949766315860262e-05,
      "loss": 1.789,
      "step": 6100
    },
    {
      "epoch": 0.6142864324134117,
      "grad_norm": 2.703594923019409,
      "learning_rate": 2.3939679230691635e-05,
      "loss": 1.8252,
      "step": 6110
    },
    {
      "epoch": 0.6152918111898658,
      "grad_norm": 2.160527229309082,
      "learning_rate": 2.3929592145523015e-05,
      "loss": 1.7488,
      "step": 6120
    },
    {
      "epoch": 0.6162971899663198,
      "grad_norm": 1.9663132429122925,
      "learning_rate": 2.391950506035439e-05,
      "loss": 1.8248,
      "step": 6130
    },
    {
      "epoch": 0.6173025687427739,
      "grad_norm": 1.9974398612976074,
      "learning_rate": 2.390941797518577e-05,
      "loss": 1.7684,
      "step": 6140
    },
    {
      "epoch": 0.6183079475192279,
      "grad_norm": 2.0967791080474854,
      "learning_rate": 2.389933089001715e-05,
      "loss": 1.7847,
      "step": 6150
    },
    {
      "epoch": 0.6193133262956819,
      "grad_norm": 1.7087838649749756,
      "learning_rate": 2.3890252513365388e-05,
      "loss": 1.6988,
      "step": 6160
    },
    {
      "epoch": 0.6203187050721359,
      "grad_norm": 2.1489715576171875,
      "learning_rate": 2.3880165428196767e-05,
      "loss": 1.877,
      "step": 6170
    },
    {
      "epoch": 0.62132408384859,
      "grad_norm": 2.009152412414551,
      "learning_rate": 2.3870078343028144e-05,
      "loss": 1.7803,
      "step": 6180
    },
    {
      "epoch": 0.622329462625044,
      "grad_norm": 2.118980646133423,
      "learning_rate": 2.3859991257859523e-05,
      "loss": 1.8796,
      "step": 6190
    },
    {
      "epoch": 0.623334841401498,
      "grad_norm": 1.7445518970489502,
      "learning_rate": 2.3849904172690896e-05,
      "loss": 1.803,
      "step": 6200
    },
    {
      "epoch": 0.6243402201779521,
      "grad_norm": 2.272538661956787,
      "learning_rate": 2.3839817087522276e-05,
      "loss": 1.7533,
      "step": 6210
    },
    {
      "epoch": 0.6253455989544061,
      "grad_norm": 2.0337488651275635,
      "learning_rate": 2.3829730002353652e-05,
      "loss": 1.832,
      "step": 6220
    },
    {
      "epoch": 0.6263509777308601,
      "grad_norm": 1.959201455116272,
      "learning_rate": 2.3819642917185032e-05,
      "loss": 1.8244,
      "step": 6230
    },
    {
      "epoch": 0.6273563565073141,
      "grad_norm": 2.010110855102539,
      "learning_rate": 2.380955583201641e-05,
      "loss": 1.7334,
      "step": 6240
    },
    {
      "epoch": 0.6283617352837682,
      "grad_norm": 2.0814497470855713,
      "learning_rate": 2.3799468746847788e-05,
      "loss": 1.7687,
      "step": 6250
    },
    {
      "epoch": 0.6293671140602222,
      "grad_norm": 1.7320994138717651,
      "learning_rate": 2.378938166167916e-05,
      "loss": 1.8075,
      "step": 6260
    },
    {
      "epoch": 0.6303724928366762,
      "grad_norm": 1.8533952236175537,
      "learning_rate": 2.377929457651054e-05,
      "loss": 1.7112,
      "step": 6270
    },
    {
      "epoch": 0.6313778716131302,
      "grad_norm": 2.924834966659546,
      "learning_rate": 2.376920749134192e-05,
      "loss": 1.7559,
      "step": 6280
    },
    {
      "epoch": 0.6323832503895843,
      "grad_norm": 2.139348268508911,
      "learning_rate": 2.3759120406173297e-05,
      "loss": 1.7922,
      "step": 6290
    },
    {
      "epoch": 0.6333886291660383,
      "grad_norm": 1.67105233669281,
      "learning_rate": 2.3749033321004676e-05,
      "loss": 1.7871,
      "step": 6300
    },
    {
      "epoch": 0.6343940079424923,
      "grad_norm": 1.8206884860992432,
      "learning_rate": 2.3738946235836053e-05,
      "loss": 1.833,
      "step": 6310
    },
    {
      "epoch": 0.6353993867189464,
      "grad_norm": 2.1666080951690674,
      "learning_rate": 2.372885915066743e-05,
      "loss": 1.737,
      "step": 6320
    },
    {
      "epoch": 0.6364047654954004,
      "grad_norm": 2.225726842880249,
      "learning_rate": 2.3718772065498805e-05,
      "loss": 1.809,
      "step": 6330
    },
    {
      "epoch": 0.6374101442718544,
      "grad_norm": 1.667618989944458,
      "learning_rate": 2.3708684980330185e-05,
      "loss": 1.8553,
      "step": 6340
    },
    {
      "epoch": 0.6384155230483084,
      "grad_norm": 1.9733690023422241,
      "learning_rate": 2.369859789516156e-05,
      "loss": 1.7508,
      "step": 6350
    },
    {
      "epoch": 0.6394209018247625,
      "grad_norm": 1.9975405931472778,
      "learning_rate": 2.368851080999294e-05,
      "loss": 1.7057,
      "step": 6360
    },
    {
      "epoch": 0.6404262806012165,
      "grad_norm": 1.9630922079086304,
      "learning_rate": 2.3678423724824317e-05,
      "loss": 1.7406,
      "step": 6370
    },
    {
      "epoch": 0.6414316593776705,
      "grad_norm": 2.028726100921631,
      "learning_rate": 2.3668336639655694e-05,
      "loss": 1.8109,
      "step": 6380
    },
    {
      "epoch": 0.6424370381541246,
      "grad_norm": 2.2604622840881348,
      "learning_rate": 2.365824955448707e-05,
      "loss": 1.759,
      "step": 6390
    },
    {
      "epoch": 0.6434424169305786,
      "grad_norm": 1.8189797401428223,
      "learning_rate": 2.364816246931845e-05,
      "loss": 1.7169,
      "step": 6400
    },
    {
      "epoch": 0.6444477957070326,
      "grad_norm": 2.4418535232543945,
      "learning_rate": 2.3638075384149826e-05,
      "loss": 1.8059,
      "step": 6410
    },
    {
      "epoch": 0.6454531744834866,
      "grad_norm": 2.016489028930664,
      "learning_rate": 2.3627988298981206e-05,
      "loss": 1.7308,
      "step": 6420
    },
    {
      "epoch": 0.6464585532599407,
      "grad_norm": 2.3124101161956787,
      "learning_rate": 2.3617901213812586e-05,
      "loss": 1.907,
      "step": 6430
    },
    {
      "epoch": 0.6474639320363947,
      "grad_norm": 2.118415594100952,
      "learning_rate": 2.360781412864396e-05,
      "loss": 1.7108,
      "step": 6440
    },
    {
      "epoch": 0.6484693108128488,
      "grad_norm": 2.0116829872131348,
      "learning_rate": 2.3597727043475338e-05,
      "loss": 1.8437,
      "step": 6450
    },
    {
      "epoch": 0.6494746895893028,
      "grad_norm": 1.917880654335022,
      "learning_rate": 2.3587639958306714e-05,
      "loss": 1.7332,
      "step": 6460
    },
    {
      "epoch": 0.6504800683657568,
      "grad_norm": 2.3615901470184326,
      "learning_rate": 2.3577552873138094e-05,
      "loss": 1.796,
      "step": 6470
    },
    {
      "epoch": 0.6514854471422108,
      "grad_norm": 2.408646583557129,
      "learning_rate": 2.356746578796947e-05,
      "loss": 1.7572,
      "step": 6480
    },
    {
      "epoch": 0.6524908259186648,
      "grad_norm": 3.4658589363098145,
      "learning_rate": 2.355737870280085e-05,
      "loss": 1.6767,
      "step": 6490
    },
    {
      "epoch": 0.6534962046951189,
      "grad_norm": 1.6915626525878906,
      "learning_rate": 2.3547291617632223e-05,
      "loss": 1.786,
      "step": 6500
    },
    {
      "epoch": 0.654501583471573,
      "grad_norm": 2.056398630142212,
      "learning_rate": 2.3537204532463603e-05,
      "loss": 1.791,
      "step": 6510
    },
    {
      "epoch": 0.655506962248027,
      "grad_norm": 1.8986402750015259,
      "learning_rate": 2.352711744729498e-05,
      "loss": 1.7974,
      "step": 6520
    },
    {
      "epoch": 0.656512341024481,
      "grad_norm": 2.229012966156006,
      "learning_rate": 2.351703036212636e-05,
      "loss": 1.8226,
      "step": 6530
    },
    {
      "epoch": 0.657517719800935,
      "grad_norm": 3.2003185749053955,
      "learning_rate": 2.3506943276957735e-05,
      "loss": 1.7728,
      "step": 6540
    },
    {
      "epoch": 0.658523098577389,
      "grad_norm": 2.7801671028137207,
      "learning_rate": 2.3496856191789115e-05,
      "loss": 1.7025,
      "step": 6550
    },
    {
      "epoch": 0.659528477353843,
      "grad_norm": 1.9822614192962646,
      "learning_rate": 2.3486769106620488e-05,
      "loss": 1.6817,
      "step": 6560
    },
    {
      "epoch": 0.6605338561302971,
      "grad_norm": 1.8074195384979248,
      "learning_rate": 2.3476682021451867e-05,
      "loss": 1.8027,
      "step": 6570
    },
    {
      "epoch": 0.6615392349067512,
      "grad_norm": 1.6735167503356934,
      "learning_rate": 2.3466594936283247e-05,
      "loss": 1.6986,
      "step": 6580
    },
    {
      "epoch": 0.6625446136832052,
      "grad_norm": 1.8053439855575562,
      "learning_rate": 2.3456507851114623e-05,
      "loss": 1.7772,
      "step": 6590
    },
    {
      "epoch": 0.6635499924596592,
      "grad_norm": 1.7847577333450317,
      "learning_rate": 2.3446420765946003e-05,
      "loss": 1.7566,
      "step": 6600
    },
    {
      "epoch": 0.6645553712361132,
      "grad_norm": 1.6490195989608765,
      "learning_rate": 2.343633368077738e-05,
      "loss": 1.7182,
      "step": 6610
    },
    {
      "epoch": 0.6655607500125672,
      "grad_norm": 2.0000827312469482,
      "learning_rate": 2.3426246595608756e-05,
      "loss": 1.7383,
      "step": 6620
    },
    {
      "epoch": 0.6665661287890212,
      "grad_norm": 2.0378661155700684,
      "learning_rate": 2.3416159510440132e-05,
      "loss": 1.775,
      "step": 6630
    },
    {
      "epoch": 0.6675715075654753,
      "grad_norm": 2.332536220550537,
      "learning_rate": 2.3406072425271512e-05,
      "loss": 1.8375,
      "step": 6640
    },
    {
      "epoch": 0.6685768863419294,
      "grad_norm": 1.9683008193969727,
      "learning_rate": 2.3395985340102888e-05,
      "loss": 1.8551,
      "step": 6650
    },
    {
      "epoch": 0.6695822651183834,
      "grad_norm": 2.1213345527648926,
      "learning_rate": 2.3385898254934268e-05,
      "loss": 1.7334,
      "step": 6660
    },
    {
      "epoch": 0.6705876438948374,
      "grad_norm": 2.1274425983428955,
      "learning_rate": 2.3375811169765644e-05,
      "loss": 1.8349,
      "step": 6670
    },
    {
      "epoch": 0.6715930226712914,
      "grad_norm": 2.484241247177124,
      "learning_rate": 2.336572408459702e-05,
      "loss": 1.7771,
      "step": 6680
    },
    {
      "epoch": 0.6725984014477454,
      "grad_norm": 1.9488589763641357,
      "learning_rate": 2.3355636999428397e-05,
      "loss": 1.7675,
      "step": 6690
    },
    {
      "epoch": 0.6736037802241994,
      "grad_norm": 2.101308822631836,
      "learning_rate": 2.3345549914259776e-05,
      "loss": 1.799,
      "step": 6700
    },
    {
      "epoch": 0.6746091590006535,
      "grad_norm": 2.4271159172058105,
      "learning_rate": 2.3335462829091153e-05,
      "loss": 1.7738,
      "step": 6710
    },
    {
      "epoch": 0.6756145377771076,
      "grad_norm": 1.7773370742797852,
      "learning_rate": 2.3325375743922532e-05,
      "loss": 1.8071,
      "step": 6720
    },
    {
      "epoch": 0.6766199165535616,
      "grad_norm": 1.7462083101272583,
      "learning_rate": 2.331528865875391e-05,
      "loss": 1.8381,
      "step": 6730
    },
    {
      "epoch": 0.6776252953300156,
      "grad_norm": 2.257302761077881,
      "learning_rate": 2.3305201573585285e-05,
      "loss": 1.7901,
      "step": 6740
    },
    {
      "epoch": 0.6786306741064696,
      "grad_norm": 2.2921054363250732,
      "learning_rate": 2.3295114488416665e-05,
      "loss": 1.8489,
      "step": 6750
    },
    {
      "epoch": 0.6796360528829236,
      "grad_norm": 1.9656733274459839,
      "learning_rate": 2.328502740324804e-05,
      "loss": 1.7891,
      "step": 6760
    },
    {
      "epoch": 0.6806414316593776,
      "grad_norm": 1.8573248386383057,
      "learning_rate": 2.327494031807942e-05,
      "loss": 1.7242,
      "step": 6770
    },
    {
      "epoch": 0.6816468104358318,
      "grad_norm": 2.094137668609619,
      "learning_rate": 2.3264853232910797e-05,
      "loss": 1.7438,
      "step": 6780
    },
    {
      "epoch": 0.6826521892122858,
      "grad_norm": 2.1785035133361816,
      "learning_rate": 2.3254766147742177e-05,
      "loss": 1.866,
      "step": 6790
    },
    {
      "epoch": 0.6836575679887398,
      "grad_norm": 2.4962892532348633,
      "learning_rate": 2.3244679062573553e-05,
      "loss": 1.7067,
      "step": 6800
    },
    {
      "epoch": 0.6846629467651938,
      "grad_norm": 2.0701189041137695,
      "learning_rate": 2.323459197740493e-05,
      "loss": 1.7891,
      "step": 6810
    },
    {
      "epoch": 0.6856683255416478,
      "grad_norm": 1.7505371570587158,
      "learning_rate": 2.3224504892236306e-05,
      "loss": 1.7366,
      "step": 6820
    },
    {
      "epoch": 0.6866737043181018,
      "grad_norm": 2.1703810691833496,
      "learning_rate": 2.3214417807067685e-05,
      "loss": 1.811,
      "step": 6830
    },
    {
      "epoch": 0.6876790830945558,
      "grad_norm": 2.3071067333221436,
      "learning_rate": 2.3204330721899062e-05,
      "loss": 1.7873,
      "step": 6840
    },
    {
      "epoch": 0.68868446187101,
      "grad_norm": 1.884243130683899,
      "learning_rate": 2.319424363673044e-05,
      "loss": 1.7307,
      "step": 6850
    },
    {
      "epoch": 0.689689840647464,
      "grad_norm": 2.1546339988708496,
      "learning_rate": 2.3184156551561818e-05,
      "loss": 1.7266,
      "step": 6860
    },
    {
      "epoch": 0.690695219423918,
      "grad_norm": 3.6220362186431885,
      "learning_rate": 2.3174069466393194e-05,
      "loss": 1.7789,
      "step": 6870
    },
    {
      "epoch": 0.691700598200372,
      "grad_norm": 1.872755527496338,
      "learning_rate": 2.316398238122457e-05,
      "loss": 1.7654,
      "step": 6880
    },
    {
      "epoch": 0.692705976976826,
      "grad_norm": 2.2596726417541504,
      "learning_rate": 2.315389529605595e-05,
      "loss": 1.8261,
      "step": 6890
    },
    {
      "epoch": 0.69371135575328,
      "grad_norm": 1.99137282371521,
      "learning_rate": 2.314380821088733e-05,
      "loss": 1.7467,
      "step": 6900
    },
    {
      "epoch": 0.694716734529734,
      "grad_norm": 1.8311418294906616,
      "learning_rate": 2.3133721125718706e-05,
      "loss": 1.7283,
      "step": 6910
    },
    {
      "epoch": 0.6957221133061882,
      "grad_norm": 1.7923476696014404,
      "learning_rate": 2.3123634040550086e-05,
      "loss": 1.8176,
      "step": 6920
    },
    {
      "epoch": 0.6967274920826422,
      "grad_norm": 2.0262374877929688,
      "learning_rate": 2.311354695538146e-05,
      "loss": 1.7344,
      "step": 6930
    },
    {
      "epoch": 0.6977328708590962,
      "grad_norm": 1.9817529916763306,
      "learning_rate": 2.310345987021284e-05,
      "loss": 1.7656,
      "step": 6940
    },
    {
      "epoch": 0.6987382496355502,
      "grad_norm": 1.9093989133834839,
      "learning_rate": 2.3093372785044215e-05,
      "loss": 1.7233,
      "step": 6950
    },
    {
      "epoch": 0.6997436284120042,
      "grad_norm": 2.5125017166137695,
      "learning_rate": 2.3083285699875595e-05,
      "loss": 1.8063,
      "step": 6960
    },
    {
      "epoch": 0.7007490071884582,
      "grad_norm": 1.7598358392715454,
      "learning_rate": 2.307319861470697e-05,
      "loss": 1.7674,
      "step": 6970
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 2.6373560428619385,
      "learning_rate": 2.306311152953835e-05,
      "loss": 1.7763,
      "step": 6980
    },
    {
      "epoch": 0.7027597647413663,
      "grad_norm": 1.9823962450027466,
      "learning_rate": 2.3053024444369723e-05,
      "loss": 1.8116,
      "step": 6990
    },
    {
      "epoch": 0.7037651435178204,
      "grad_norm": 1.8470056056976318,
      "learning_rate": 2.3042937359201103e-05,
      "loss": 1.8105,
      "step": 7000
    },
    {
      "epoch": 0.7047705222942744,
      "grad_norm": 1.9913804531097412,
      "learning_rate": 2.303285027403248e-05,
      "loss": 1.8541,
      "step": 7010
    },
    {
      "epoch": 0.7057759010707284,
      "grad_norm": 2.3253800868988037,
      "learning_rate": 2.302276318886386e-05,
      "loss": 1.7801,
      "step": 7020
    },
    {
      "epoch": 0.7067812798471824,
      "grad_norm": 1.936263918876648,
      "learning_rate": 2.3012676103695235e-05,
      "loss": 1.8455,
      "step": 7030
    },
    {
      "epoch": 0.7077866586236364,
      "grad_norm": 2.1693639755249023,
      "learning_rate": 2.3002589018526615e-05,
      "loss": 1.7294,
      "step": 7040
    },
    {
      "epoch": 0.7087920374000904,
      "grad_norm": 1.7817100286483765,
      "learning_rate": 2.299250193335799e-05,
      "loss": 1.9233,
      "step": 7050
    },
    {
      "epoch": 0.7097974161765445,
      "grad_norm": 2.141718626022339,
      "learning_rate": 2.2982414848189368e-05,
      "loss": 1.7576,
      "step": 7060
    },
    {
      "epoch": 0.7108027949529986,
      "grad_norm": 2.3377017974853516,
      "learning_rate": 2.2972327763020748e-05,
      "loss": 1.8047,
      "step": 7070
    },
    {
      "epoch": 0.7118081737294526,
      "grad_norm": 1.964491605758667,
      "learning_rate": 2.2962240677852124e-05,
      "loss": 1.8716,
      "step": 7080
    },
    {
      "epoch": 0.7128135525059066,
      "grad_norm": 1.9418680667877197,
      "learning_rate": 2.2952153592683504e-05,
      "loss": 1.8473,
      "step": 7090
    },
    {
      "epoch": 0.7138189312823606,
      "grad_norm": 2.9053421020507812,
      "learning_rate": 2.294206650751488e-05,
      "loss": 1.8584,
      "step": 7100
    },
    {
      "epoch": 0.7148243100588146,
      "grad_norm": 2.077238082885742,
      "learning_rate": 2.2931979422346256e-05,
      "loss": 1.7409,
      "step": 7110
    },
    {
      "epoch": 0.7158296888352687,
      "grad_norm": 2.7031898498535156,
      "learning_rate": 2.2921892337177632e-05,
      "loss": 1.7959,
      "step": 7120
    },
    {
      "epoch": 0.7168350676117227,
      "grad_norm": 2.139516592025757,
      "learning_rate": 2.2911805252009012e-05,
      "loss": 1.7649,
      "step": 7130
    },
    {
      "epoch": 0.7178404463881768,
      "grad_norm": 2.6283910274505615,
      "learning_rate": 2.290171816684039e-05,
      "loss": 1.7531,
      "step": 7140
    },
    {
      "epoch": 0.7188458251646308,
      "grad_norm": 1.5975106954574585,
      "learning_rate": 2.2891631081671768e-05,
      "loss": 1.7277,
      "step": 7150
    },
    {
      "epoch": 0.7198512039410848,
      "grad_norm": 1.9975837469100952,
      "learning_rate": 2.2881543996503144e-05,
      "loss": 1.8442,
      "step": 7160
    },
    {
      "epoch": 0.7208565827175388,
      "grad_norm": 2.369886636734009,
      "learning_rate": 2.287145691133452e-05,
      "loss": 1.7286,
      "step": 7170
    },
    {
      "epoch": 0.7218619614939928,
      "grad_norm": 2.359001398086548,
      "learning_rate": 2.2861369826165897e-05,
      "loss": 1.6828,
      "step": 7180
    },
    {
      "epoch": 0.7228673402704469,
      "grad_norm": 2.534900665283203,
      "learning_rate": 2.2851282740997277e-05,
      "loss": 1.7673,
      "step": 7190
    },
    {
      "epoch": 0.7238727190469009,
      "grad_norm": 1.7771201133728027,
      "learning_rate": 2.2841195655828657e-05,
      "loss": 1.8299,
      "step": 7200
    },
    {
      "epoch": 0.724878097823355,
      "grad_norm": 2.0153071880340576,
      "learning_rate": 2.2831108570660033e-05,
      "loss": 1.7261,
      "step": 7210
    },
    {
      "epoch": 0.725883476599809,
      "grad_norm": 2.4059855937957764,
      "learning_rate": 2.2821021485491413e-05,
      "loss": 1.8595,
      "step": 7220
    },
    {
      "epoch": 0.726888855376263,
      "grad_norm": 2.4209742546081543,
      "learning_rate": 2.2810934400322785e-05,
      "loss": 1.6936,
      "step": 7230
    },
    {
      "epoch": 0.727894234152717,
      "grad_norm": 2.3213136196136475,
      "learning_rate": 2.2800847315154165e-05,
      "loss": 1.7873,
      "step": 7240
    },
    {
      "epoch": 0.728899612929171,
      "grad_norm": 2.9699337482452393,
      "learning_rate": 2.279076022998554e-05,
      "loss": 1.7292,
      "step": 7250
    },
    {
      "epoch": 0.7299049917056251,
      "grad_norm": 2.36449933052063,
      "learning_rate": 2.278067314481692e-05,
      "loss": 1.7574,
      "step": 7260
    },
    {
      "epoch": 0.7309103704820791,
      "grad_norm": 2.10831880569458,
      "learning_rate": 2.2770586059648297e-05,
      "loss": 1.8289,
      "step": 7270
    },
    {
      "epoch": 0.7319157492585332,
      "grad_norm": 2.1350257396698,
      "learning_rate": 2.2760498974479677e-05,
      "loss": 1.8698,
      "step": 7280
    },
    {
      "epoch": 0.7329211280349872,
      "grad_norm": 2.041910171508789,
      "learning_rate": 2.275041188931105e-05,
      "loss": 1.783,
      "step": 7290
    },
    {
      "epoch": 0.7339265068114412,
      "grad_norm": 2.5774781703948975,
      "learning_rate": 2.274032480414243e-05,
      "loss": 1.8023,
      "step": 7300
    },
    {
      "epoch": 0.7349318855878952,
      "grad_norm": 2.473006248474121,
      "learning_rate": 2.2730237718973806e-05,
      "loss": 1.7827,
      "step": 7310
    },
    {
      "epoch": 0.7359372643643493,
      "grad_norm": 1.8211698532104492,
      "learning_rate": 2.2720150633805186e-05,
      "loss": 1.7895,
      "step": 7320
    },
    {
      "epoch": 0.7369426431408033,
      "grad_norm": 2.1850271224975586,
      "learning_rate": 2.2710063548636562e-05,
      "loss": 1.7781,
      "step": 7330
    },
    {
      "epoch": 0.7379480219172573,
      "grad_norm": 1.79209566116333,
      "learning_rate": 2.2699976463467942e-05,
      "loss": 1.8294,
      "step": 7340
    },
    {
      "epoch": 0.7389534006937114,
      "grad_norm": 2.122410297393799,
      "learning_rate": 2.2689889378299318e-05,
      "loss": 1.7494,
      "step": 7350
    },
    {
      "epoch": 0.7399587794701654,
      "grad_norm": 2.392359972000122,
      "learning_rate": 2.2679802293130694e-05,
      "loss": 1.8229,
      "step": 7360
    },
    {
      "epoch": 0.7409641582466194,
      "grad_norm": 1.683607578277588,
      "learning_rate": 2.2669715207962074e-05,
      "loss": 1.8449,
      "step": 7370
    },
    {
      "epoch": 0.7419695370230734,
      "grad_norm": 2.6643166542053223,
      "learning_rate": 2.265962812279345e-05,
      "loss": 1.7674,
      "step": 7380
    },
    {
      "epoch": 0.7429749157995275,
      "grad_norm": 1.9299476146697998,
      "learning_rate": 2.264954103762483e-05,
      "loss": 1.8178,
      "step": 7390
    },
    {
      "epoch": 0.7439802945759815,
      "grad_norm": 2.7414326667785645,
      "learning_rate": 2.2639453952456207e-05,
      "loss": 1.8532,
      "step": 7400
    },
    {
      "epoch": 0.7449856733524355,
      "grad_norm": 1.6377787590026855,
      "learning_rate": 2.2629366867287583e-05,
      "loss": 1.8055,
      "step": 7410
    },
    {
      "epoch": 0.7459910521288896,
      "grad_norm": 2.3908729553222656,
      "learning_rate": 2.261927978211896e-05,
      "loss": 1.8084,
      "step": 7420
    },
    {
      "epoch": 0.7469964309053436,
      "grad_norm": 1.8509001731872559,
      "learning_rate": 2.260919269695034e-05,
      "loss": 1.7103,
      "step": 7430
    },
    {
      "epoch": 0.7480018096817976,
      "grad_norm": 2.2020153999328613,
      "learning_rate": 2.2599105611781715e-05,
      "loss": 1.8183,
      "step": 7440
    },
    {
      "epoch": 0.7490071884582516,
      "grad_norm": 1.5407129526138306,
      "learning_rate": 2.2589018526613095e-05,
      "loss": 1.7684,
      "step": 7450
    },
    {
      "epoch": 0.7500125672347057,
      "grad_norm": 1.9171984195709229,
      "learning_rate": 2.257893144144447e-05,
      "loss": 1.6889,
      "step": 7460
    },
    {
      "epoch": 0.7510179460111597,
      "grad_norm": 2.2408084869384766,
      "learning_rate": 2.2568844356275847e-05,
      "loss": 1.7492,
      "step": 7470
    },
    {
      "epoch": 0.7520233247876137,
      "grad_norm": 2.021238088607788,
      "learning_rate": 2.2558757271107224e-05,
      "loss": 1.7928,
      "step": 7480
    },
    {
      "epoch": 0.7530287035640678,
      "grad_norm": 2.6396889686584473,
      "learning_rate": 2.2548670185938604e-05,
      "loss": 1.8385,
      "step": 7490
    },
    {
      "epoch": 0.7540340823405218,
      "grad_norm": 1.8855390548706055,
      "learning_rate": 2.2538583100769983e-05,
      "loss": 1.8173,
      "step": 7500
    },
    {
      "epoch": 0.7550394611169758,
      "grad_norm": 1.8387075662612915,
      "learning_rate": 2.252849601560136e-05,
      "loss": 1.7248,
      "step": 7510
    },
    {
      "epoch": 0.7560448398934299,
      "grad_norm": 2.1256086826324463,
      "learning_rate": 2.251840893043274e-05,
      "loss": 1.7764,
      "step": 7520
    },
    {
      "epoch": 0.7570502186698839,
      "grad_norm": 1.8212542533874512,
      "learning_rate": 2.2508321845264112e-05,
      "loss": 1.6878,
      "step": 7530
    },
    {
      "epoch": 0.7580555974463379,
      "grad_norm": 1.6281416416168213,
      "learning_rate": 2.2498234760095492e-05,
      "loss": 1.8227,
      "step": 7540
    },
    {
      "epoch": 0.7590609762227919,
      "grad_norm": 1.7932212352752686,
      "learning_rate": 2.2488147674926868e-05,
      "loss": 1.8638,
      "step": 7550
    },
    {
      "epoch": 0.760066354999246,
      "grad_norm": 2.2747082710266113,
      "learning_rate": 2.2478060589758248e-05,
      "loss": 1.824,
      "step": 7560
    },
    {
      "epoch": 0.7610717337757,
      "grad_norm": 2.2357594966888428,
      "learning_rate": 2.2467973504589624e-05,
      "loss": 1.7757,
      "step": 7570
    },
    {
      "epoch": 0.762077112552154,
      "grad_norm": 2.295140027999878,
      "learning_rate": 2.2457886419421004e-05,
      "loss": 1.8035,
      "step": 7580
    },
    {
      "epoch": 0.7630824913286081,
      "grad_norm": 1.9966272115707397,
      "learning_rate": 2.2447799334252377e-05,
      "loss": 1.8415,
      "step": 7590
    },
    {
      "epoch": 0.7640878701050621,
      "grad_norm": 1.935242772102356,
      "learning_rate": 2.2437712249083757e-05,
      "loss": 1.8151,
      "step": 7600
    },
    {
      "epoch": 0.7650932488815161,
      "grad_norm": 2.4488253593444824,
      "learning_rate": 2.2427625163915133e-05,
      "loss": 1.6966,
      "step": 7610
    },
    {
      "epoch": 0.7660986276579701,
      "grad_norm": 1.7067432403564453,
      "learning_rate": 2.2417538078746513e-05,
      "loss": 1.7922,
      "step": 7620
    },
    {
      "epoch": 0.7671040064344242,
      "grad_norm": 2.0694286823272705,
      "learning_rate": 2.240745099357789e-05,
      "loss": 1.7776,
      "step": 7630
    },
    {
      "epoch": 0.7681093852108782,
      "grad_norm": 2.483116626739502,
      "learning_rate": 2.239736390840927e-05,
      "loss": 1.7255,
      "step": 7640
    },
    {
      "epoch": 0.7691147639873323,
      "grad_norm": 2.272826671600342,
      "learning_rate": 2.2387276823240645e-05,
      "loss": 1.777,
      "step": 7650
    },
    {
      "epoch": 0.7701201427637863,
      "grad_norm": 2.1235733032226562,
      "learning_rate": 2.237718973807202e-05,
      "loss": 1.7408,
      "step": 7660
    },
    {
      "epoch": 0.7711255215402403,
      "grad_norm": 2.1313509941101074,
      "learning_rate": 2.23671026529034e-05,
      "loss": 1.8355,
      "step": 7670
    },
    {
      "epoch": 0.7721309003166943,
      "grad_norm": 1.4790133237838745,
      "learning_rate": 2.2357015567734777e-05,
      "loss": 1.7389,
      "step": 7680
    },
    {
      "epoch": 0.7731362790931483,
      "grad_norm": 2.0010793209075928,
      "learning_rate": 2.2346928482566157e-05,
      "loss": 1.7742,
      "step": 7690
    },
    {
      "epoch": 0.7741416578696023,
      "grad_norm": 2.2865617275238037,
      "learning_rate": 2.2336841397397533e-05,
      "loss": 1.8175,
      "step": 7700
    },
    {
      "epoch": 0.7751470366460564,
      "grad_norm": 2.0831851959228516,
      "learning_rate": 2.232675431222891e-05,
      "loss": 1.8066,
      "step": 7710
    },
    {
      "epoch": 0.7761524154225105,
      "grad_norm": 1.7818620204925537,
      "learning_rate": 2.2316667227060286e-05,
      "loss": 1.8178,
      "step": 7720
    },
    {
      "epoch": 0.7771577941989645,
      "grad_norm": 3.4736742973327637,
      "learning_rate": 2.2306580141891666e-05,
      "loss": 1.8375,
      "step": 7730
    },
    {
      "epoch": 0.7781631729754185,
      "grad_norm": 1.7462406158447266,
      "learning_rate": 2.2296493056723042e-05,
      "loss": 1.827,
      "step": 7740
    },
    {
      "epoch": 0.7791685517518725,
      "grad_norm": 2.0316364765167236,
      "learning_rate": 2.228640597155442e-05,
      "loss": 1.7821,
      "step": 7750
    },
    {
      "epoch": 0.7801739305283265,
      "grad_norm": 2.3711791038513184,
      "learning_rate": 2.2276318886385798e-05,
      "loss": 1.6471,
      "step": 7760
    },
    {
      "epoch": 0.7811793093047805,
      "grad_norm": 2.2359275817871094,
      "learning_rate": 2.2266231801217174e-05,
      "loss": 1.8308,
      "step": 7770
    },
    {
      "epoch": 0.7821846880812346,
      "grad_norm": 2.6441245079040527,
      "learning_rate": 2.225614471604855e-05,
      "loss": 1.8062,
      "step": 7780
    },
    {
      "epoch": 0.7831900668576887,
      "grad_norm": 1.8159921169281006,
      "learning_rate": 2.224605763087993e-05,
      "loss": 1.741,
      "step": 7790
    },
    {
      "epoch": 0.7841954456341427,
      "grad_norm": 1.811444640159607,
      "learning_rate": 2.223597054571131e-05,
      "loss": 1.7591,
      "step": 7800
    },
    {
      "epoch": 0.7852008244105967,
      "grad_norm": 1.7162551879882812,
      "learning_rate": 2.2225883460542686e-05,
      "loss": 1.7973,
      "step": 7810
    },
    {
      "epoch": 0.7862062031870507,
      "grad_norm": 2.463998794555664,
      "learning_rate": 2.2215796375374066e-05,
      "loss": 1.786,
      "step": 7820
    },
    {
      "epoch": 0.7872115819635047,
      "grad_norm": 1.8297958374023438,
      "learning_rate": 2.220570929020544e-05,
      "loss": 1.8402,
      "step": 7830
    },
    {
      "epoch": 0.7882169607399587,
      "grad_norm": 1.7715269327163696,
      "learning_rate": 2.219562220503682e-05,
      "loss": 1.8305,
      "step": 7840
    },
    {
      "epoch": 0.7892223395164129,
      "grad_norm": 2.0252299308776855,
      "learning_rate": 2.2185535119868195e-05,
      "loss": 1.7974,
      "step": 7850
    },
    {
      "epoch": 0.7902277182928669,
      "grad_norm": 2.4918878078460693,
      "learning_rate": 2.2175448034699575e-05,
      "loss": 1.8285,
      "step": 7860
    },
    {
      "epoch": 0.7912330970693209,
      "grad_norm": 2.0587222576141357,
      "learning_rate": 2.216536094953095e-05,
      "loss": 1.8071,
      "step": 7870
    },
    {
      "epoch": 0.7922384758457749,
      "grad_norm": 1.8673049211502075,
      "learning_rate": 2.215527386436233e-05,
      "loss": 1.8026,
      "step": 7880
    },
    {
      "epoch": 0.7932438546222289,
      "grad_norm": 1.6922675371170044,
      "learning_rate": 2.2145186779193703e-05,
      "loss": 1.7968,
      "step": 7890
    },
    {
      "epoch": 0.7942492333986829,
      "grad_norm": 1.7977571487426758,
      "learning_rate": 2.2135099694025083e-05,
      "loss": 1.7419,
      "step": 7900
    },
    {
      "epoch": 0.7952546121751369,
      "grad_norm": 1.9358292818069458,
      "learning_rate": 2.212501260885646e-05,
      "loss": 1.7661,
      "step": 7910
    },
    {
      "epoch": 0.7962599909515911,
      "grad_norm": 2.0640523433685303,
      "learning_rate": 2.211492552368784e-05,
      "loss": 1.7143,
      "step": 7920
    },
    {
      "epoch": 0.7972653697280451,
      "grad_norm": 2.2540183067321777,
      "learning_rate": 2.2104838438519216e-05,
      "loss": 1.8188,
      "step": 7930
    },
    {
      "epoch": 0.7982707485044991,
      "grad_norm": 2.039574384689331,
      "learning_rate": 2.2094751353350595e-05,
      "loss": 1.7617,
      "step": 7940
    },
    {
      "epoch": 0.7992761272809531,
      "grad_norm": 2.8066864013671875,
      "learning_rate": 2.208466426818197e-05,
      "loss": 1.7851,
      "step": 7950
    },
    {
      "epoch": 0.8002815060574071,
      "grad_norm": 2.387065887451172,
      "learning_rate": 2.2074577183013348e-05,
      "loss": 1.7972,
      "step": 7960
    },
    {
      "epoch": 0.8012868848338611,
      "grad_norm": 1.8969460725784302,
      "learning_rate": 2.2064490097844728e-05,
      "loss": 1.8108,
      "step": 7970
    },
    {
      "epoch": 0.8022922636103151,
      "grad_norm": 2.409682273864746,
      "learning_rate": 2.2054403012676104e-05,
      "loss": 1.7821,
      "step": 7980
    },
    {
      "epoch": 0.8032976423867693,
      "grad_norm": 2.163128137588501,
      "learning_rate": 2.2044315927507484e-05,
      "loss": 1.7983,
      "step": 7990
    },
    {
      "epoch": 0.8043030211632233,
      "grad_norm": 2.272164821624756,
      "learning_rate": 2.203422884233886e-05,
      "loss": 1.7689,
      "step": 8000
    },
    {
      "epoch": 0.8053083999396773,
      "grad_norm": 2.21642804145813,
      "learning_rate": 2.202414175717024e-05,
      "loss": 1.6926,
      "step": 8010
    },
    {
      "epoch": 0.8063137787161313,
      "grad_norm": 2.1230580806732178,
      "learning_rate": 2.2014054672001613e-05,
      "loss": 1.7733,
      "step": 8020
    },
    {
      "epoch": 0.8073191574925853,
      "grad_norm": 1.541330337524414,
      "learning_rate": 2.2003967586832992e-05,
      "loss": 1.764,
      "step": 8030
    },
    {
      "epoch": 0.8083245362690393,
      "grad_norm": 2.5779192447662354,
      "learning_rate": 2.199388050166437e-05,
      "loss": 1.8012,
      "step": 8040
    },
    {
      "epoch": 0.8093299150454933,
      "grad_norm": 2.0603585243225098,
      "learning_rate": 2.1983793416495748e-05,
      "loss": 1.76,
      "step": 8050
    },
    {
      "epoch": 0.8103352938219475,
      "grad_norm": 1.8545262813568115,
      "learning_rate": 2.1973706331327125e-05,
      "loss": 1.798,
      "step": 8060
    },
    {
      "epoch": 0.8113406725984015,
      "grad_norm": 2.224668502807617,
      "learning_rate": 2.1963619246158504e-05,
      "loss": 1.8213,
      "step": 8070
    },
    {
      "epoch": 0.8123460513748555,
      "grad_norm": 2.2623488903045654,
      "learning_rate": 2.1953532160989877e-05,
      "loss": 1.9101,
      "step": 8080
    },
    {
      "epoch": 0.8133514301513095,
      "grad_norm": 2.1918094158172607,
      "learning_rate": 2.1943445075821257e-05,
      "loss": 1.8185,
      "step": 8090
    },
    {
      "epoch": 0.8143568089277635,
      "grad_norm": 2.3089377880096436,
      "learning_rate": 2.1933357990652637e-05,
      "loss": 1.8078,
      "step": 8100
    },
    {
      "epoch": 0.8153621877042175,
      "grad_norm": 2.0946269035339355,
      "learning_rate": 2.1923270905484013e-05,
      "loss": 1.8302,
      "step": 8110
    },
    {
      "epoch": 0.8163675664806715,
      "grad_norm": 1.8548182249069214,
      "learning_rate": 2.1913183820315393e-05,
      "loss": 1.7863,
      "step": 8120
    },
    {
      "epoch": 0.8173729452571257,
      "grad_norm": 2.3458447456359863,
      "learning_rate": 2.190309673514677e-05,
      "loss": 1.7643,
      "step": 8130
    },
    {
      "epoch": 0.8183783240335797,
      "grad_norm": 1.8127909898757935,
      "learning_rate": 2.1893009649978145e-05,
      "loss": 1.741,
      "step": 8140
    },
    {
      "epoch": 0.8193837028100337,
      "grad_norm": 2.102755069732666,
      "learning_rate": 2.188292256480952e-05,
      "loss": 1.7804,
      "step": 8150
    },
    {
      "epoch": 0.8203890815864877,
      "grad_norm": 2.7235777378082275,
      "learning_rate": 2.18728354796409e-05,
      "loss": 1.7643,
      "step": 8160
    },
    {
      "epoch": 0.8213944603629417,
      "grad_norm": 1.7771116495132446,
      "learning_rate": 2.1862748394472278e-05,
      "loss": 1.7549,
      "step": 8170
    },
    {
      "epoch": 0.8223998391393957,
      "grad_norm": 1.568450689315796,
      "learning_rate": 2.1852661309303657e-05,
      "loss": 1.8105,
      "step": 8180
    },
    {
      "epoch": 0.8234052179158498,
      "grad_norm": 1.9557510614395142,
      "learning_rate": 2.1842574224135034e-05,
      "loss": 1.7679,
      "step": 8190
    },
    {
      "epoch": 0.8244105966923039,
      "grad_norm": 2.1129000186920166,
      "learning_rate": 2.183248713896641e-05,
      "loss": 1.8048,
      "step": 8200
    },
    {
      "epoch": 0.8254159754687579,
      "grad_norm": 1.8164691925048828,
      "learning_rate": 2.1822400053797786e-05,
      "loss": 1.741,
      "step": 8210
    },
    {
      "epoch": 0.8264213542452119,
      "grad_norm": 2.3034167289733887,
      "learning_rate": 2.1812312968629166e-05,
      "loss": 1.8705,
      "step": 8220
    },
    {
      "epoch": 0.8274267330216659,
      "grad_norm": 2.2164547443389893,
      "learning_rate": 2.1802225883460542e-05,
      "loss": 1.7771,
      "step": 8230
    },
    {
      "epoch": 0.8284321117981199,
      "grad_norm": 1.9705332517623901,
      "learning_rate": 2.1792138798291922e-05,
      "loss": 1.7443,
      "step": 8240
    },
    {
      "epoch": 0.8294374905745739,
      "grad_norm": 1.682462215423584,
      "learning_rate": 2.17820517131233e-05,
      "loss": 1.7832,
      "step": 8250
    },
    {
      "epoch": 0.830442869351028,
      "grad_norm": 1.984839916229248,
      "learning_rate": 2.1771964627954675e-05,
      "loss": 1.8235,
      "step": 8260
    },
    {
      "epoch": 0.8314482481274821,
      "grad_norm": 2.4266316890716553,
      "learning_rate": 2.1761877542786054e-05,
      "loss": 1.8456,
      "step": 8270
    },
    {
      "epoch": 0.8324536269039361,
      "grad_norm": 2.4326045513153076,
      "learning_rate": 2.175179045761743e-05,
      "loss": 1.7716,
      "step": 8280
    },
    {
      "epoch": 0.8334590056803901,
      "grad_norm": 1.9368019104003906,
      "learning_rate": 2.174170337244881e-05,
      "loss": 1.7347,
      "step": 8290
    },
    {
      "epoch": 0.8344643844568441,
      "grad_norm": 2.4551384449005127,
      "learning_rate": 2.1731616287280187e-05,
      "loss": 1.7414,
      "step": 8300
    },
    {
      "epoch": 0.8354697632332981,
      "grad_norm": 1.848258137702942,
      "learning_rate": 2.1721529202111566e-05,
      "loss": 1.7357,
      "step": 8310
    },
    {
      "epoch": 0.8364751420097521,
      "grad_norm": 1.8359856605529785,
      "learning_rate": 2.171144211694294e-05,
      "loss": 1.8171,
      "step": 8320
    },
    {
      "epoch": 0.8374805207862062,
      "grad_norm": 2.3056187629699707,
      "learning_rate": 2.170135503177432e-05,
      "loss": 1.8139,
      "step": 8330
    },
    {
      "epoch": 0.8384858995626603,
      "grad_norm": 1.910136103630066,
      "learning_rate": 2.1691267946605695e-05,
      "loss": 1.7658,
      "step": 8340
    },
    {
      "epoch": 0.8394912783391143,
      "grad_norm": Infinity,
      "learning_rate": 2.1681180861437075e-05,
      "loss": 1.782,
      "step": 8350
    },
    {
      "epoch": 0.8404966571155683,
      "grad_norm": 1.922467827796936,
      "learning_rate": 2.1672102484785312e-05,
      "loss": 1.8125,
      "step": 8360
    },
    {
      "epoch": 0.8415020358920223,
      "grad_norm": 2.0909416675567627,
      "learning_rate": 2.166201539961669e-05,
      "loss": 1.8317,
      "step": 8370
    },
    {
      "epoch": 0.8425074146684763,
      "grad_norm": 2.0018014907836914,
      "learning_rate": 2.1651928314448068e-05,
      "loss": 1.8466,
      "step": 8380
    },
    {
      "epoch": 0.8435127934449304,
      "grad_norm": 2.518627405166626,
      "learning_rate": 2.1641841229279448e-05,
      "loss": 1.7363,
      "step": 8390
    },
    {
      "epoch": 0.8445181722213844,
      "grad_norm": 2.026291608810425,
      "learning_rate": 2.1631754144110824e-05,
      "loss": 1.8045,
      "step": 8400
    },
    {
      "epoch": 0.8455235509978385,
      "grad_norm": 1.968992829322815,
      "learning_rate": 2.16216670589422e-05,
      "loss": 1.8332,
      "step": 8410
    },
    {
      "epoch": 0.8465289297742925,
      "grad_norm": 1.8492308855056763,
      "learning_rate": 2.161157997377358e-05,
      "loss": 1.8185,
      "step": 8420
    },
    {
      "epoch": 0.8475343085507465,
      "grad_norm": 1.9003900289535522,
      "learning_rate": 2.1601492888604956e-05,
      "loss": 1.8172,
      "step": 8430
    },
    {
      "epoch": 0.8485396873272005,
      "grad_norm": 2.230426549911499,
      "learning_rate": 2.1591405803436336e-05,
      "loss": 1.7261,
      "step": 8440
    },
    {
      "epoch": 0.8495450661036545,
      "grad_norm": 1.7145735025405884,
      "learning_rate": 2.1581318718267712e-05,
      "loss": 1.8313,
      "step": 8450
    },
    {
      "epoch": 0.8505504448801086,
      "grad_norm": 2.1118900775909424,
      "learning_rate": 2.157123163309909e-05,
      "loss": 1.6391,
      "step": 8460
    },
    {
      "epoch": 0.8515558236565626,
      "grad_norm": 2.2875375747680664,
      "learning_rate": 2.1561144547930465e-05,
      "loss": 1.7966,
      "step": 8470
    },
    {
      "epoch": 0.8525612024330166,
      "grad_norm": 2.834780216217041,
      "learning_rate": 2.1551057462761845e-05,
      "loss": 1.8277,
      "step": 8480
    },
    {
      "epoch": 0.8535665812094707,
      "grad_norm": 1.7816762924194336,
      "learning_rate": 2.154097037759322e-05,
      "loss": 1.7265,
      "step": 8490
    },
    {
      "epoch": 0.8545719599859247,
      "grad_norm": 1.92367684841156,
      "learning_rate": 2.15308832924246e-05,
      "loss": 1.737,
      "step": 8500
    },
    {
      "epoch": 0.8555773387623787,
      "grad_norm": 1.7316503524780273,
      "learning_rate": 2.1520796207255977e-05,
      "loss": 1.708,
      "step": 8510
    },
    {
      "epoch": 0.8565827175388327,
      "grad_norm": 1.932417869567871,
      "learning_rate": 2.1510709122087353e-05,
      "loss": 1.7676,
      "step": 8520
    },
    {
      "epoch": 0.8575880963152868,
      "grad_norm": 2.147510290145874,
      "learning_rate": 2.150062203691873e-05,
      "loss": 1.8137,
      "step": 8530
    },
    {
      "epoch": 0.8585934750917408,
      "grad_norm": 2.126171588897705,
      "learning_rate": 2.149053495175011e-05,
      "loss": 1.7568,
      "step": 8540
    },
    {
      "epoch": 0.8595988538681948,
      "grad_norm": 2.5105316638946533,
      "learning_rate": 2.148044786658149e-05,
      "loss": 1.8006,
      "step": 8550
    },
    {
      "epoch": 0.8606042326446489,
      "grad_norm": 2.3384439945220947,
      "learning_rate": 2.1470360781412865e-05,
      "loss": 1.7811,
      "step": 8560
    },
    {
      "epoch": 0.8616096114211029,
      "grad_norm": 2.4491491317749023,
      "learning_rate": 2.1460273696244245e-05,
      "loss": 1.7917,
      "step": 8570
    },
    {
      "epoch": 0.8626149901975569,
      "grad_norm": 2.5009539127349854,
      "learning_rate": 2.1450186611075618e-05,
      "loss": 1.6954,
      "step": 8580
    },
    {
      "epoch": 0.863620368974011,
      "grad_norm": 2.7141566276550293,
      "learning_rate": 2.1440099525906998e-05,
      "loss": 1.725,
      "step": 8590
    },
    {
      "epoch": 0.864625747750465,
      "grad_norm": 1.7769461870193481,
      "learning_rate": 2.1430012440738374e-05,
      "loss": 1.7097,
      "step": 8600
    },
    {
      "epoch": 0.865631126526919,
      "grad_norm": 1.9012006521224976,
      "learning_rate": 2.1419925355569754e-05,
      "loss": 1.8169,
      "step": 8610
    },
    {
      "epoch": 0.866636505303373,
      "grad_norm": 2.3180439472198486,
      "learning_rate": 2.140983827040113e-05,
      "loss": 1.7794,
      "step": 8620
    },
    {
      "epoch": 0.8676418840798271,
      "grad_norm": 2.337158203125,
      "learning_rate": 2.139975118523251e-05,
      "loss": 1.8259,
      "step": 8630
    },
    {
      "epoch": 0.8686472628562811,
      "grad_norm": 2.027325391769409,
      "learning_rate": 2.1389664100063886e-05,
      "loss": 1.8164,
      "step": 8640
    },
    {
      "epoch": 0.8696526416327351,
      "grad_norm": 2.026310682296753,
      "learning_rate": 2.1379577014895262e-05,
      "loss": 1.8047,
      "step": 8650
    },
    {
      "epoch": 0.8706580204091892,
      "grad_norm": 1.7425041198730469,
      "learning_rate": 2.136948992972664e-05,
      "loss": 1.769,
      "step": 8660
    },
    {
      "epoch": 0.8716633991856432,
      "grad_norm": 1.997987151145935,
      "learning_rate": 2.1359402844558018e-05,
      "loss": 1.8494,
      "step": 8670
    },
    {
      "epoch": 0.8726687779620972,
      "grad_norm": 1.8995987176895142,
      "learning_rate": 2.1349315759389395e-05,
      "loss": 1.8624,
      "step": 8680
    },
    {
      "epoch": 0.8736741567385512,
      "grad_norm": 2.3324310779571533,
      "learning_rate": 2.1339228674220774e-05,
      "loss": 1.8679,
      "step": 8690
    },
    {
      "epoch": 0.8746795355150053,
      "grad_norm": 2.0974843502044678,
      "learning_rate": 2.1329141589052154e-05,
      "loss": 1.8309,
      "step": 8700
    },
    {
      "epoch": 0.8756849142914593,
      "grad_norm": 2.0020923614501953,
      "learning_rate": 2.1319054503883527e-05,
      "loss": 1.6522,
      "step": 8710
    },
    {
      "epoch": 0.8766902930679134,
      "grad_norm": 1.8179434537887573,
      "learning_rate": 2.1308967418714907e-05,
      "loss": 1.7572,
      "step": 8720
    },
    {
      "epoch": 0.8776956718443674,
      "grad_norm": 1.6030476093292236,
      "learning_rate": 2.1298880333546283e-05,
      "loss": 1.7587,
      "step": 8730
    },
    {
      "epoch": 0.8787010506208214,
      "grad_norm": 2.1599183082580566,
      "learning_rate": 2.1288793248377663e-05,
      "loss": 1.7098,
      "step": 8740
    },
    {
      "epoch": 0.8797064293972754,
      "grad_norm": 2.0470290184020996,
      "learning_rate": 2.127870616320904e-05,
      "loss": 1.7549,
      "step": 8750
    },
    {
      "epoch": 0.8807118081737294,
      "grad_norm": 2.5889012813568115,
      "learning_rate": 2.126861907804042e-05,
      "loss": 1.7504,
      "step": 8760
    },
    {
      "epoch": 0.8817171869501835,
      "grad_norm": 2.1009600162506104,
      "learning_rate": 2.125853199287179e-05,
      "loss": 1.815,
      "step": 8770
    },
    {
      "epoch": 0.8827225657266375,
      "grad_norm": 1.910124659538269,
      "learning_rate": 2.124844490770317e-05,
      "loss": 1.8403,
      "step": 8780
    },
    {
      "epoch": 0.8837279445030916,
      "grad_norm": 2.3832685947418213,
      "learning_rate": 2.1238357822534548e-05,
      "loss": 1.7854,
      "step": 8790
    },
    {
      "epoch": 0.8847333232795456,
      "grad_norm": 1.746416449546814,
      "learning_rate": 2.1228270737365927e-05,
      "loss": 1.7629,
      "step": 8800
    },
    {
      "epoch": 0.8857387020559996,
      "grad_norm": 2.2947707176208496,
      "learning_rate": 2.1218183652197304e-05,
      "loss": 1.6966,
      "step": 8810
    },
    {
      "epoch": 0.8867440808324536,
      "grad_norm": 1.8612879514694214,
      "learning_rate": 2.1208096567028683e-05,
      "loss": 1.7591,
      "step": 8820
    },
    {
      "epoch": 0.8877494596089076,
      "grad_norm": 1.8318520784378052,
      "learning_rate": 2.1198009481860056e-05,
      "loss": 1.7524,
      "step": 8830
    },
    {
      "epoch": 0.8887548383853617,
      "grad_norm": 2.3895294666290283,
      "learning_rate": 2.1187922396691436e-05,
      "loss": 1.7524,
      "step": 8840
    },
    {
      "epoch": 0.8897602171618157,
      "grad_norm": 2.0388071537017822,
      "learning_rate": 2.1177835311522816e-05,
      "loss": 1.7524,
      "step": 8850
    },
    {
      "epoch": 0.8907655959382698,
      "grad_norm": 2.060826539993286,
      "learning_rate": 2.1167748226354192e-05,
      "loss": 1.7984,
      "step": 8860
    },
    {
      "epoch": 0.8917709747147238,
      "grad_norm": 1.8864622116088867,
      "learning_rate": 2.115766114118557e-05,
      "loss": 1.789,
      "step": 8870
    },
    {
      "epoch": 0.8927763534911778,
      "grad_norm": 1.958487629890442,
      "learning_rate": 2.1147574056016948e-05,
      "loss": 1.7971,
      "step": 8880
    },
    {
      "epoch": 0.8937817322676318,
      "grad_norm": 1.3939690589904785,
      "learning_rate": 2.1137486970848324e-05,
      "loss": 1.7521,
      "step": 8890
    },
    {
      "epoch": 0.8947871110440858,
      "grad_norm": 6.732748508453369,
      "learning_rate": 2.11273998856797e-05,
      "loss": 1.7891,
      "step": 8900
    },
    {
      "epoch": 0.8957924898205399,
      "grad_norm": 1.7290122509002686,
      "learning_rate": 2.111731280051108e-05,
      "loss": 1.8421,
      "step": 8910
    },
    {
      "epoch": 0.896797868596994,
      "grad_norm": 1.8489090204238892,
      "learning_rate": 2.1107225715342457e-05,
      "loss": 1.8739,
      "step": 8920
    },
    {
      "epoch": 0.897803247373448,
      "grad_norm": 1.9761123657226562,
      "learning_rate": 2.1097138630173836e-05,
      "loss": 1.752,
      "step": 8930
    },
    {
      "epoch": 0.898808626149902,
      "grad_norm": 2.8848936557769775,
      "learning_rate": 2.1087051545005213e-05,
      "loss": 1.8514,
      "step": 8940
    },
    {
      "epoch": 0.899814004926356,
      "grad_norm": 1.7779417037963867,
      "learning_rate": 2.107696445983659e-05,
      "loss": 1.84,
      "step": 8950
    },
    {
      "epoch": 0.90081938370281,
      "grad_norm": 2.1673333644866943,
      "learning_rate": 2.1066877374667965e-05,
      "loss": 1.8623,
      "step": 8960
    },
    {
      "epoch": 0.901824762479264,
      "grad_norm": 1.682174801826477,
      "learning_rate": 2.1056790289499345e-05,
      "loss": 1.7365,
      "step": 8970
    },
    {
      "epoch": 0.9028301412557181,
      "grad_norm": 2.15322208404541,
      "learning_rate": 2.104670320433072e-05,
      "loss": 1.8158,
      "step": 8980
    },
    {
      "epoch": 0.9038355200321722,
      "grad_norm": 1.5466737747192383,
      "learning_rate": 2.10366161191621e-05,
      "loss": 1.769,
      "step": 8990
    },
    {
      "epoch": 0.9048408988086262,
      "grad_norm": 1.8901723623275757,
      "learning_rate": 2.102652903399348e-05,
      "loss": 1.7851,
      "step": 9000
    },
    {
      "epoch": 0.9058462775850802,
      "grad_norm": 1.724256992340088,
      "learning_rate": 2.1016441948824854e-05,
      "loss": 1.7398,
      "step": 9010
    },
    {
      "epoch": 0.9068516563615342,
      "grad_norm": 2.211237907409668,
      "learning_rate": 2.1006354863656233e-05,
      "loss": 1.7251,
      "step": 9020
    },
    {
      "epoch": 0.9078570351379882,
      "grad_norm": 1.6436949968338013,
      "learning_rate": 2.099626777848761e-05,
      "loss": 1.8451,
      "step": 9030
    },
    {
      "epoch": 0.9088624139144422,
      "grad_norm": 2.1668946743011475,
      "learning_rate": 2.098618069331899e-05,
      "loss": 1.8185,
      "step": 9040
    },
    {
      "epoch": 0.9098677926908963,
      "grad_norm": 2.531822443008423,
      "learning_rate": 2.0976093608150366e-05,
      "loss": 1.8181,
      "step": 9050
    },
    {
      "epoch": 0.9108731714673504,
      "grad_norm": 2.232862710952759,
      "learning_rate": 2.0966006522981745e-05,
      "loss": 1.7648,
      "step": 9060
    },
    {
      "epoch": 0.9118785502438044,
      "grad_norm": 2.238602638244629,
      "learning_rate": 2.0955919437813118e-05,
      "loss": 1.8547,
      "step": 9070
    },
    {
      "epoch": 0.9128839290202584,
      "grad_norm": 1.8574961423873901,
      "learning_rate": 2.0945832352644498e-05,
      "loss": 1.7482,
      "step": 9080
    },
    {
      "epoch": 0.9138893077967124,
      "grad_norm": 2.0424551963806152,
      "learning_rate": 2.0935745267475874e-05,
      "loss": 1.7102,
      "step": 9090
    },
    {
      "epoch": 0.9148946865731664,
      "grad_norm": 1.8959465026855469,
      "learning_rate": 2.0925658182307254e-05,
      "loss": 1.746,
      "step": 9100
    },
    {
      "epoch": 0.9159000653496204,
      "grad_norm": 1.9752827882766724,
      "learning_rate": 2.091557109713863e-05,
      "loss": 1.7955,
      "step": 9110
    },
    {
      "epoch": 0.9169054441260746,
      "grad_norm": 1.8128207921981812,
      "learning_rate": 2.090548401197001e-05,
      "loss": 1.7483,
      "step": 9120
    },
    {
      "epoch": 0.9179108229025286,
      "grad_norm": 1.8435413837432861,
      "learning_rate": 2.0895396926801383e-05,
      "loss": 1.7647,
      "step": 9130
    },
    {
      "epoch": 0.9189162016789826,
      "grad_norm": 2.1437768936157227,
      "learning_rate": 2.0885309841632763e-05,
      "loss": 1.8704,
      "step": 9140
    },
    {
      "epoch": 0.9199215804554366,
      "grad_norm": 2.3206920623779297,
      "learning_rate": 2.0875222756464142e-05,
      "loss": 1.7356,
      "step": 9150
    },
    {
      "epoch": 0.9209269592318906,
      "grad_norm": 1.8204470872879028,
      "learning_rate": 2.086513567129552e-05,
      "loss": 1.7964,
      "step": 9160
    },
    {
      "epoch": 0.9219323380083446,
      "grad_norm": 1.658715009689331,
      "learning_rate": 2.08550485861269e-05,
      "loss": 1.8408,
      "step": 9170
    },
    {
      "epoch": 0.9229377167847986,
      "grad_norm": 2.187411308288574,
      "learning_rate": 2.0844961500958275e-05,
      "loss": 1.7252,
      "step": 9180
    },
    {
      "epoch": 0.9239430955612526,
      "grad_norm": 1.9073516130447388,
      "learning_rate": 2.083487441578965e-05,
      "loss": 1.7748,
      "step": 9190
    },
    {
      "epoch": 0.9249484743377068,
      "grad_norm": 1.9866291284561157,
      "learning_rate": 2.0824787330621027e-05,
      "loss": 1.7132,
      "step": 9200
    },
    {
      "epoch": 0.9259538531141608,
      "grad_norm": 1.9887990951538086,
      "learning_rate": 2.0814700245452407e-05,
      "loss": 1.7898,
      "step": 9210
    },
    {
      "epoch": 0.9269592318906148,
      "grad_norm": 1.6376900672912598,
      "learning_rate": 2.0804613160283783e-05,
      "loss": 1.7433,
      "step": 9220
    },
    {
      "epoch": 0.9279646106670688,
      "grad_norm": 1.6095796823501587,
      "learning_rate": 2.0794526075115163e-05,
      "loss": 1.7264,
      "step": 9230
    },
    {
      "epoch": 0.9289699894435228,
      "grad_norm": 2.2761597633361816,
      "learning_rate": 2.078443898994654e-05,
      "loss": 1.7683,
      "step": 9240
    },
    {
      "epoch": 0.9299753682199768,
      "grad_norm": 1.8763993978500366,
      "learning_rate": 2.0774351904777916e-05,
      "loss": 1.7307,
      "step": 9250
    },
    {
      "epoch": 0.9309807469964309,
      "grad_norm": 2.359318971633911,
      "learning_rate": 2.0764264819609292e-05,
      "loss": 1.7884,
      "step": 9260
    },
    {
      "epoch": 0.931986125772885,
      "grad_norm": 1.7732194662094116,
      "learning_rate": 2.075417773444067e-05,
      "loss": 1.8072,
      "step": 9270
    },
    {
      "epoch": 0.932991504549339,
      "grad_norm": 1.9646364450454712,
      "learning_rate": 2.0744090649272048e-05,
      "loss": 1.7925,
      "step": 9280
    },
    {
      "epoch": 0.933996883325793,
      "grad_norm": 1.6059165000915527,
      "learning_rate": 2.0734003564103428e-05,
      "loss": 1.7584,
      "step": 9290
    },
    {
      "epoch": 0.935002262102247,
      "grad_norm": 1.6454366445541382,
      "learning_rate": 2.0723916478934807e-05,
      "loss": 1.8217,
      "step": 9300
    },
    {
      "epoch": 0.936007640878701,
      "grad_norm": 2.0838687419891357,
      "learning_rate": 2.071382939376618e-05,
      "loss": 1.8079,
      "step": 9310
    },
    {
      "epoch": 0.937013019655155,
      "grad_norm": 2.2227914333343506,
      "learning_rate": 2.070374230859756e-05,
      "loss": 1.7862,
      "step": 9320
    },
    {
      "epoch": 0.9380183984316091,
      "grad_norm": 1.91953706741333,
      "learning_rate": 2.0693655223428936e-05,
      "loss": 1.7841,
      "step": 9330
    },
    {
      "epoch": 0.9390237772080632,
      "grad_norm": 1.851086974143982,
      "learning_rate": 2.0683568138260316e-05,
      "loss": 1.7836,
      "step": 9340
    },
    {
      "epoch": 0.9400291559845172,
      "grad_norm": 2.372849702835083,
      "learning_rate": 2.0673481053091692e-05,
      "loss": 1.7291,
      "step": 9350
    },
    {
      "epoch": 0.9410345347609712,
      "grad_norm": 2.6784462928771973,
      "learning_rate": 2.0663393967923072e-05,
      "loss": 1.8106,
      "step": 9360
    },
    {
      "epoch": 0.9420399135374252,
      "grad_norm": 1.977316975593567,
      "learning_rate": 2.0653306882754445e-05,
      "loss": 1.7994,
      "step": 9370
    },
    {
      "epoch": 0.9430452923138792,
      "grad_norm": 1.6592421531677246,
      "learning_rate": 2.0643219797585825e-05,
      "loss": 1.7774,
      "step": 9380
    },
    {
      "epoch": 0.9440506710903332,
      "grad_norm": 1.9252445697784424,
      "learning_rate": 2.06331327124172e-05,
      "loss": 1.7245,
      "step": 9390
    },
    {
      "epoch": 0.9450560498667873,
      "grad_norm": 1.665116310119629,
      "learning_rate": 2.062304562724858e-05,
      "loss": 1.7359,
      "step": 9400
    },
    {
      "epoch": 0.9460614286432414,
      "grad_norm": 1.7670402526855469,
      "learning_rate": 2.0612958542079957e-05,
      "loss": 1.8103,
      "step": 9410
    },
    {
      "epoch": 0.9470668074196954,
      "grad_norm": 1.5058156251907349,
      "learning_rate": 2.0602871456911337e-05,
      "loss": 1.85,
      "step": 9420
    },
    {
      "epoch": 0.9480721861961494,
      "grad_norm": 2.1254146099090576,
      "learning_rate": 2.059278437174271e-05,
      "loss": 1.8163,
      "step": 9430
    },
    {
      "epoch": 0.9490775649726034,
      "grad_norm": 1.8860924243927002,
      "learning_rate": 2.058269728657409e-05,
      "loss": 1.7509,
      "step": 9440
    },
    {
      "epoch": 0.9500829437490574,
      "grad_norm": 1.708926796913147,
      "learning_rate": 2.057261020140547e-05,
      "loss": 1.8007,
      "step": 9450
    },
    {
      "epoch": 0.9510883225255115,
      "grad_norm": 1.7393466234207153,
      "learning_rate": 2.0562523116236845e-05,
      "loss": 1.7801,
      "step": 9460
    },
    {
      "epoch": 0.9520937013019655,
      "grad_norm": 2.0479209423065186,
      "learning_rate": 2.0552436031068225e-05,
      "loss": 1.7952,
      "step": 9470
    },
    {
      "epoch": 0.9530990800784196,
      "grad_norm": 2.270419120788574,
      "learning_rate": 2.05423489458996e-05,
      "loss": 1.7496,
      "step": 9480
    },
    {
      "epoch": 0.9541044588548736,
      "grad_norm": 1.9295670986175537,
      "learning_rate": 2.0532261860730978e-05,
      "loss": 1.7209,
      "step": 9490
    },
    {
      "epoch": 0.9551098376313276,
      "grad_norm": 1.9858977794647217,
      "learning_rate": 2.0522174775562354e-05,
      "loss": 1.7607,
      "step": 9500
    },
    {
      "epoch": 0.9561152164077816,
      "grad_norm": 2.0979199409484863,
      "learning_rate": 2.0512087690393734e-05,
      "loss": 1.7864,
      "step": 9510
    },
    {
      "epoch": 0.9571205951842356,
      "grad_norm": 1.597733497619629,
      "learning_rate": 2.050200060522511e-05,
      "loss": 1.7081,
      "step": 9520
    },
    {
      "epoch": 0.9581259739606897,
      "grad_norm": 2.2058820724487305,
      "learning_rate": 2.049191352005649e-05,
      "loss": 1.7883,
      "step": 9530
    },
    {
      "epoch": 0.9591313527371437,
      "grad_norm": 2.3590152263641357,
      "learning_rate": 2.0481826434887866e-05,
      "loss": 1.7194,
      "step": 9540
    },
    {
      "epoch": 0.9601367315135978,
      "grad_norm": 1.6255910396575928,
      "learning_rate": 2.0471739349719242e-05,
      "loss": 1.8018,
      "step": 9550
    },
    {
      "epoch": 0.9611421102900518,
      "grad_norm": 2.865734815597534,
      "learning_rate": 2.046165226455062e-05,
      "loss": 1.7605,
      "step": 9560
    },
    {
      "epoch": 0.9621474890665058,
      "grad_norm": 2.467881917953491,
      "learning_rate": 2.0451565179381998e-05,
      "loss": 1.8198,
      "step": 9570
    },
    {
      "epoch": 0.9631528678429598,
      "grad_norm": 2.621995449066162,
      "learning_rate": 2.0441478094213375e-05,
      "loss": 1.7757,
      "step": 9580
    },
    {
      "epoch": 0.9641582466194139,
      "grad_norm": 1.5932790040969849,
      "learning_rate": 2.0431391009044754e-05,
      "loss": 1.8112,
      "step": 9590
    },
    {
      "epoch": 0.9651636253958679,
      "grad_norm": 1.7447093725204468,
      "learning_rate": 2.0421303923876134e-05,
      "loss": 1.7706,
      "step": 9600
    },
    {
      "epoch": 0.9661690041723219,
      "grad_norm": 2.4563746452331543,
      "learning_rate": 2.0411216838707507e-05,
      "loss": 1.7789,
      "step": 9610
    },
    {
      "epoch": 0.967174382948776,
      "grad_norm": 1.7063329219818115,
      "learning_rate": 2.0401129753538887e-05,
      "loss": 1.7364,
      "step": 9620
    },
    {
      "epoch": 0.96817976172523,
      "grad_norm": 1.8371798992156982,
      "learning_rate": 2.0391042668370263e-05,
      "loss": 1.7712,
      "step": 9630
    },
    {
      "epoch": 0.969185140501684,
      "grad_norm": 2.9026198387145996,
      "learning_rate": 2.0380955583201643e-05,
      "loss": 1.6839,
      "step": 9640
    },
    {
      "epoch": 0.970190519278138,
      "grad_norm": 1.953813910484314,
      "learning_rate": 2.037086849803302e-05,
      "loss": 1.7726,
      "step": 9650
    },
    {
      "epoch": 0.971195898054592,
      "grad_norm": 2.0490169525146484,
      "learning_rate": 2.03607814128644e-05,
      "loss": 1.8015,
      "step": 9660
    },
    {
      "epoch": 0.9722012768310461,
      "grad_norm": 1.7698231935501099,
      "learning_rate": 2.035069432769577e-05,
      "loss": 1.738,
      "step": 9670
    },
    {
      "epoch": 0.9732066556075001,
      "grad_norm": 1.919813871383667,
      "learning_rate": 2.034060724252715e-05,
      "loss": 1.7568,
      "step": 9680
    },
    {
      "epoch": 0.9742120343839542,
      "grad_norm": 2.1119384765625,
      "learning_rate": 2.0330520157358528e-05,
      "loss": 1.804,
      "step": 9690
    },
    {
      "epoch": 0.9752174131604082,
      "grad_norm": 1.8330841064453125,
      "learning_rate": 2.0320433072189907e-05,
      "loss": 1.6783,
      "step": 9700
    },
    {
      "epoch": 0.9762227919368622,
      "grad_norm": 1.9874844551086426,
      "learning_rate": 2.0310345987021284e-05,
      "loss": 1.806,
      "step": 9710
    },
    {
      "epoch": 0.9772281707133162,
      "grad_norm": 2.033102512359619,
      "learning_rate": 2.0300258901852663e-05,
      "loss": 1.7301,
      "step": 9720
    },
    {
      "epoch": 0.9782335494897703,
      "grad_norm": 1.9547579288482666,
      "learning_rate": 2.0290171816684036e-05,
      "loss": 1.7969,
      "step": 9730
    },
    {
      "epoch": 0.9792389282662243,
      "grad_norm": 1.9731175899505615,
      "learning_rate": 2.0280084731515416e-05,
      "loss": 1.7919,
      "step": 9740
    },
    {
      "epoch": 0.9802443070426783,
      "grad_norm": 1.686333179473877,
      "learning_rate": 2.0269997646346796e-05,
      "loss": 1.661,
      "step": 9750
    },
    {
      "epoch": 0.9812496858191324,
      "grad_norm": 1.9883028268814087,
      "learning_rate": 2.0259910561178172e-05,
      "loss": 1.7925,
      "step": 9760
    },
    {
      "epoch": 0.9822550645955864,
      "grad_norm": 3.0431289672851562,
      "learning_rate": 2.024982347600955e-05,
      "loss": 1.6427,
      "step": 9770
    },
    {
      "epoch": 0.9832604433720404,
      "grad_norm": 1.4529836177825928,
      "learning_rate": 2.0239736390840928e-05,
      "loss": 1.7014,
      "step": 9780
    },
    {
      "epoch": 0.9842658221484945,
      "grad_norm": 2.3999407291412354,
      "learning_rate": 2.0229649305672304e-05,
      "loss": 1.8093,
      "step": 9790
    },
    {
      "epoch": 0.9852712009249485,
      "grad_norm": 2.073756456375122,
      "learning_rate": 2.021956222050368e-05,
      "loss": 1.7783,
      "step": 9800
    },
    {
      "epoch": 0.9862765797014025,
      "grad_norm": 2.348554849624634,
      "learning_rate": 2.020947513533506e-05,
      "loss": 1.9121,
      "step": 9810
    },
    {
      "epoch": 0.9872819584778565,
      "grad_norm": 1.574609637260437,
      "learning_rate": 2.0199388050166437e-05,
      "loss": 1.7843,
      "step": 9820
    },
    {
      "epoch": 0.9882873372543106,
      "grad_norm": 1.7709240913391113,
      "learning_rate": 2.0189300964997816e-05,
      "loss": 1.7743,
      "step": 9830
    },
    {
      "epoch": 0.9892927160307646,
      "grad_norm": 2.409536123275757,
      "learning_rate": 2.0179213879829193e-05,
      "loss": 1.7531,
      "step": 9840
    },
    {
      "epoch": 0.9902980948072186,
      "grad_norm": 1.986054539680481,
      "learning_rate": 2.0169126794660572e-05,
      "loss": 1.7732,
      "step": 9850
    },
    {
      "epoch": 0.9913034735836727,
      "grad_norm": 1.7646455764770508,
      "learning_rate": 2.0159039709491945e-05,
      "loss": 1.7791,
      "step": 9860
    },
    {
      "epoch": 0.9923088523601267,
      "grad_norm": 2.230835437774658,
      "learning_rate": 2.0148952624323325e-05,
      "loss": 1.8592,
      "step": 9870
    },
    {
      "epoch": 0.9933142311365807,
      "grad_norm": 1.838422179222107,
      "learning_rate": 2.01388655391547e-05,
      "loss": 1.7328,
      "step": 9880
    },
    {
      "epoch": 0.9943196099130347,
      "grad_norm": 1.8750481605529785,
      "learning_rate": 2.012877845398608e-05,
      "loss": 1.7613,
      "step": 9890
    },
    {
      "epoch": 0.9953249886894887,
      "grad_norm": 1.6660898923873901,
      "learning_rate": 2.0118691368817457e-05,
      "loss": 1.7348,
      "step": 9900
    },
    {
      "epoch": 0.9963303674659428,
      "grad_norm": 2.315661668777466,
      "learning_rate": 2.0108604283648837e-05,
      "loss": 1.7673,
      "step": 9910
    },
    {
      "epoch": 0.9973357462423968,
      "grad_norm": 1.6116288900375366,
      "learning_rate": 2.0098517198480213e-05,
      "loss": 1.7172,
      "step": 9920
    },
    {
      "epoch": 0.9983411250188509,
      "grad_norm": 1.8703187704086304,
      "learning_rate": 2.008843011331159e-05,
      "loss": 1.749,
      "step": 9930
    },
    {
      "epoch": 0.9993465037953049,
      "grad_norm": 2.110293388366699,
      "learning_rate": 2.007834302814297e-05,
      "loss": 1.8153,
      "step": 9940
    },
    {
      "epoch": 1.0003016136329361,
      "grad_norm": 1.8138128519058228,
      "learning_rate": 2.0068255942974346e-05,
      "loss": 1.8346,
      "step": 9950
    },
    {
      "epoch": 1.0013069924093903,
      "grad_norm": 2.0905399322509766,
      "learning_rate": 2.0058168857805725e-05,
      "loss": 1.6787,
      "step": 9960
    },
    {
      "epoch": 1.0023123711858444,
      "grad_norm": 1.9838850498199463,
      "learning_rate": 2.00480817726371e-05,
      "loss": 1.7918,
      "step": 9970
    },
    {
      "epoch": 1.0033177499622983,
      "grad_norm": 1.8685489892959595,
      "learning_rate": 2.0037994687468478e-05,
      "loss": 1.6746,
      "step": 9980
    },
    {
      "epoch": 1.0043231287387524,
      "grad_norm": 1.913598895072937,
      "learning_rate": 2.0027907602299854e-05,
      "loss": 1.8154,
      "step": 9990
    },
    {
      "epoch": 1.0053285075152063,
      "grad_norm": 2.5022480487823486,
      "learning_rate": 2.0017820517131234e-05,
      "loss": 1.7807,
      "step": 10000
    },
    {
      "epoch": 1.0063338862916604,
      "grad_norm": 2.3962152004241943,
      "learning_rate": 2.000773343196261e-05,
      "loss": 1.8947,
      "step": 10010
    },
    {
      "epoch": 1.0073392650681143,
      "grad_norm": 2.444998025894165,
      "learning_rate": 1.999764634679399e-05,
      "loss": 1.8901,
      "step": 10020
    },
    {
      "epoch": 1.0083446438445685,
      "grad_norm": 2.1710784435272217,
      "learning_rate": 1.9987559261625366e-05,
      "loss": 1.7497,
      "step": 10030
    },
    {
      "epoch": 1.0093500226210226,
      "grad_norm": 2.0531716346740723,
      "learning_rate": 1.9977472176456743e-05,
      "loss": 1.8123,
      "step": 10040
    },
    {
      "epoch": 1.0103554013974765,
      "grad_norm": 1.9851375818252563,
      "learning_rate": 1.996738509128812e-05,
      "loss": 1.7632,
      "step": 10050
    },
    {
      "epoch": 1.0113607801739306,
      "grad_norm": 1.9685677289962769,
      "learning_rate": 1.99572980061195e-05,
      "loss": 1.7469,
      "step": 10060
    },
    {
      "epoch": 1.0123661589503845,
      "grad_norm": 2.0305020809173584,
      "learning_rate": 1.994721092095088e-05,
      "loss": 1.7886,
      "step": 10070
    },
    {
      "epoch": 1.0133715377268386,
      "grad_norm": 2.536630630493164,
      "learning_rate": 1.9937123835782255e-05,
      "loss": 1.8265,
      "step": 10080
    },
    {
      "epoch": 1.0143769165032925,
      "grad_norm": 2.7977898120880127,
      "learning_rate": 1.9927036750613634e-05,
      "loss": 1.6718,
      "step": 10090
    },
    {
      "epoch": 1.0153822952797467,
      "grad_norm": 2.043001174926758,
      "learning_rate": 1.9916949665445007e-05,
      "loss": 1.753,
      "step": 10100
    },
    {
      "epoch": 1.0163876740562006,
      "grad_norm": 2.351144552230835,
      "learning_rate": 1.9906862580276387e-05,
      "loss": 1.7856,
      "step": 10110
    },
    {
      "epoch": 1.0173930528326547,
      "grad_norm": 1.913190484046936,
      "learning_rate": 1.9896775495107763e-05,
      "loss": 1.8081,
      "step": 10120
    },
    {
      "epoch": 1.0183984316091088,
      "grad_norm": 1.8865735530853271,
      "learning_rate": 1.9886688409939143e-05,
      "loss": 1.6835,
      "step": 10130
    },
    {
      "epoch": 1.0194038103855627,
      "grad_norm": 1.9312090873718262,
      "learning_rate": 1.987660132477052e-05,
      "loss": 1.7712,
      "step": 10140
    },
    {
      "epoch": 1.0204091891620168,
      "grad_norm": 1.825025200843811,
      "learning_rate": 1.98665142396019e-05,
      "loss": 1.7428,
      "step": 10150
    },
    {
      "epoch": 1.0214145679384707,
      "grad_norm": 2.7858569622039795,
      "learning_rate": 1.9856427154433272e-05,
      "loss": 1.7512,
      "step": 10160
    },
    {
      "epoch": 1.0224199467149249,
      "grad_norm": 2.148198127746582,
      "learning_rate": 1.984634006926465e-05,
      "loss": 1.6955,
      "step": 10170
    },
    {
      "epoch": 1.0234253254913788,
      "grad_norm": 1.5760751962661743,
      "learning_rate": 1.9836252984096028e-05,
      "loss": 1.8073,
      "step": 10180
    },
    {
      "epoch": 1.024430704267833,
      "grad_norm": 1.666834831237793,
      "learning_rate": 1.9826165898927408e-05,
      "loss": 1.7512,
      "step": 10190
    },
    {
      "epoch": 1.025436083044287,
      "grad_norm": 1.756670355796814,
      "learning_rate": 1.9816078813758784e-05,
      "loss": 1.7546,
      "step": 10200
    },
    {
      "epoch": 1.026441461820741,
      "grad_norm": 2.2901954650878906,
      "learning_rate": 1.9805991728590164e-05,
      "loss": 1.7135,
      "step": 10210
    },
    {
      "epoch": 1.027446840597195,
      "grad_norm": 2.0707738399505615,
      "learning_rate": 1.979590464342154e-05,
      "loss": 1.7459,
      "step": 10220
    },
    {
      "epoch": 1.028452219373649,
      "grad_norm": 2.4580116271972656,
      "learning_rate": 1.9785817558252916e-05,
      "loss": 1.7703,
      "step": 10230
    },
    {
      "epoch": 1.029457598150103,
      "grad_norm": 1.9248261451721191,
      "learning_rate": 1.9775730473084296e-05,
      "loss": 1.7418,
      "step": 10240
    },
    {
      "epoch": 1.030462976926557,
      "grad_norm": 1.9339995384216309,
      "learning_rate": 1.9765643387915672e-05,
      "loss": 1.7493,
      "step": 10250
    },
    {
      "epoch": 1.031468355703011,
      "grad_norm": 2.6393046379089355,
      "learning_rate": 1.9755556302747052e-05,
      "loss": 1.8442,
      "step": 10260
    },
    {
      "epoch": 1.0324737344794652,
      "grad_norm": 1.758182168006897,
      "learning_rate": 1.974546921757843e-05,
      "loss": 1.7738,
      "step": 10270
    },
    {
      "epoch": 1.0334791132559191,
      "grad_norm": 1.9610856771469116,
      "learning_rate": 1.9735382132409805e-05,
      "loss": 1.7635,
      "step": 10280
    },
    {
      "epoch": 1.0344844920323732,
      "grad_norm": 2.015186309814453,
      "learning_rate": 1.972529504724118e-05,
      "loss": 1.8407,
      "step": 10290
    },
    {
      "epoch": 1.0354898708088272,
      "grad_norm": 2.467879056930542,
      "learning_rate": 1.971520796207256e-05,
      "loss": 1.8019,
      "step": 10300
    },
    {
      "epoch": 1.0364952495852813,
      "grad_norm": 1.5778316259384155,
      "learning_rate": 1.9705120876903937e-05,
      "loss": 1.8022,
      "step": 10310
    },
    {
      "epoch": 1.0375006283617352,
      "grad_norm": 2.1789660453796387,
      "learning_rate": 1.9695033791735317e-05,
      "loss": 1.7739,
      "step": 10320
    },
    {
      "epoch": 1.0385060071381893,
      "grad_norm": 2.8944900035858154,
      "learning_rate": 1.9684946706566693e-05,
      "loss": 1.7372,
      "step": 10330
    },
    {
      "epoch": 1.0395113859146434,
      "grad_norm": 1.9431335926055908,
      "learning_rate": 1.967485962139807e-05,
      "loss": 1.8373,
      "step": 10340
    },
    {
      "epoch": 1.0405167646910973,
      "grad_norm": 1.8471698760986328,
      "learning_rate": 1.9664772536229446e-05,
      "loss": 1.7034,
      "step": 10350
    },
    {
      "epoch": 1.0415221434675515,
      "grad_norm": 2.1729233264923096,
      "learning_rate": 1.9654685451060825e-05,
      "loss": 1.746,
      "step": 10360
    },
    {
      "epoch": 1.0425275222440054,
      "grad_norm": 1.9557468891143799,
      "learning_rate": 1.9644598365892205e-05,
      "loss": 1.7977,
      "step": 10370
    },
    {
      "epoch": 1.0435329010204595,
      "grad_norm": 1.6793086528778076,
      "learning_rate": 1.963451128072358e-05,
      "loss": 1.7639,
      "step": 10380
    },
    {
      "epoch": 1.0445382797969134,
      "grad_norm": 2.1470601558685303,
      "learning_rate": 1.962442419555496e-05,
      "loss": 1.8216,
      "step": 10390
    },
    {
      "epoch": 1.0455436585733675,
      "grad_norm": 2.0569264888763428,
      "learning_rate": 1.9614337110386334e-05,
      "loss": 1.809,
      "step": 10400
    },
    {
      "epoch": 1.0465490373498216,
      "grad_norm": 2.19020676612854,
      "learning_rate": 1.9604250025217714e-05,
      "loss": 1.6997,
      "step": 10410
    },
    {
      "epoch": 1.0475544161262755,
      "grad_norm": 1.7849112749099731,
      "learning_rate": 1.959416294004909e-05,
      "loss": 1.7485,
      "step": 10420
    },
    {
      "epoch": 1.0485597949027297,
      "grad_norm": 2.6378424167633057,
      "learning_rate": 1.958407585488047e-05,
      "loss": 1.727,
      "step": 10430
    },
    {
      "epoch": 1.0495651736791836,
      "grad_norm": 1.6443818807601929,
      "learning_rate": 1.9573988769711846e-05,
      "loss": 1.7553,
      "step": 10440
    },
    {
      "epoch": 1.0505705524556377,
      "grad_norm": 2.107529401779175,
      "learning_rate": 1.9563901684543226e-05,
      "loss": 1.7227,
      "step": 10450
    },
    {
      "epoch": 1.0515759312320916,
      "grad_norm": 1.7830218076705933,
      "learning_rate": 1.95538145993746e-05,
      "loss": 1.7997,
      "step": 10460
    },
    {
      "epoch": 1.0525813100085457,
      "grad_norm": 1.7503085136413574,
      "learning_rate": 1.954372751420598e-05,
      "loss": 1.8437,
      "step": 10470
    },
    {
      "epoch": 1.0535866887849998,
      "grad_norm": 1.5265538692474365,
      "learning_rate": 1.9533640429037355e-05,
      "loss": 1.7781,
      "step": 10480
    },
    {
      "epoch": 1.0545920675614537,
      "grad_norm": 2.6806483268737793,
      "learning_rate": 1.9523553343868734e-05,
      "loss": 1.8058,
      "step": 10490
    },
    {
      "epoch": 1.0555974463379079,
      "grad_norm": 1.7568678855895996,
      "learning_rate": 1.951346625870011e-05,
      "loss": 1.7625,
      "step": 10500
    },
    {
      "epoch": 1.0566028251143618,
      "grad_norm": 1.9166064262390137,
      "learning_rate": 1.950438788204835e-05,
      "loss": 1.72,
      "step": 10510
    },
    {
      "epoch": 1.057608203890816,
      "grad_norm": 3.065722703933716,
      "learning_rate": 1.949430079687973e-05,
      "loss": 1.8575,
      "step": 10520
    },
    {
      "epoch": 1.0586135826672698,
      "grad_norm": 1.9492340087890625,
      "learning_rate": 1.9484213711711107e-05,
      "loss": 1.7585,
      "step": 10530
    },
    {
      "epoch": 1.059618961443724,
      "grad_norm": 1.934385895729065,
      "learning_rate": 1.9474126626542487e-05,
      "loss": 1.7538,
      "step": 10540
    },
    {
      "epoch": 1.060624340220178,
      "grad_norm": 1.957826018333435,
      "learning_rate": 1.946403954137386e-05,
      "loss": 1.8264,
      "step": 10550
    },
    {
      "epoch": 1.061629718996632,
      "grad_norm": 1.805713415145874,
      "learning_rate": 1.945395245620524e-05,
      "loss": 1.7175,
      "step": 10560
    },
    {
      "epoch": 1.062635097773086,
      "grad_norm": 1.8186168670654297,
      "learning_rate": 1.9443865371036616e-05,
      "loss": 1.7984,
      "step": 10570
    },
    {
      "epoch": 1.06364047654954,
      "grad_norm": 2.416057586669922,
      "learning_rate": 1.9433778285867995e-05,
      "loss": 1.7371,
      "step": 10580
    },
    {
      "epoch": 1.064645855325994,
      "grad_norm": 2.3185014724731445,
      "learning_rate": 1.9423691200699372e-05,
      "loss": 1.7326,
      "step": 10590
    },
    {
      "epoch": 1.065651234102448,
      "grad_norm": 2.1519007682800293,
      "learning_rate": 1.941360411553075e-05,
      "loss": 1.7864,
      "step": 10600
    },
    {
      "epoch": 1.0666566128789021,
      "grad_norm": 1.9810978174209595,
      "learning_rate": 1.9403517030362124e-05,
      "loss": 1.7539,
      "step": 10610
    },
    {
      "epoch": 1.0676619916553562,
      "grad_norm": 1.7780609130859375,
      "learning_rate": 1.9393429945193504e-05,
      "loss": 1.823,
      "step": 10620
    },
    {
      "epoch": 1.0686673704318101,
      "grad_norm": 1.6915122270584106,
      "learning_rate": 1.938334286002488e-05,
      "loss": 1.767,
      "step": 10630
    },
    {
      "epoch": 1.0696727492082643,
      "grad_norm": 1.8858892917633057,
      "learning_rate": 1.937325577485626e-05,
      "loss": 1.7665,
      "step": 10640
    },
    {
      "epoch": 1.0706781279847182,
      "grad_norm": 1.8450818061828613,
      "learning_rate": 1.936316868968764e-05,
      "loss": 1.6654,
      "step": 10650
    },
    {
      "epoch": 1.0716835067611723,
      "grad_norm": 2.3824455738067627,
      "learning_rate": 1.9353081604519016e-05,
      "loss": 1.7284,
      "step": 10660
    },
    {
      "epoch": 1.0726888855376262,
      "grad_norm": 2.376997947692871,
      "learning_rate": 1.9342994519350392e-05,
      "loss": 1.7495,
      "step": 10670
    },
    {
      "epoch": 1.0736942643140803,
      "grad_norm": 2.0184383392333984,
      "learning_rate": 1.933290743418177e-05,
      "loss": 1.7712,
      "step": 10680
    },
    {
      "epoch": 1.0746996430905345,
      "grad_norm": 1.6243488788604736,
      "learning_rate": 1.932282034901315e-05,
      "loss": 1.7474,
      "step": 10690
    },
    {
      "epoch": 1.0757050218669884,
      "grad_norm": 1.7324692010879517,
      "learning_rate": 1.9312733263844525e-05,
      "loss": 1.8002,
      "step": 10700
    },
    {
      "epoch": 1.0767104006434425,
      "grad_norm": 1.5972704887390137,
      "learning_rate": 1.9302646178675904e-05,
      "loss": 1.8101,
      "step": 10710
    },
    {
      "epoch": 1.0777157794198964,
      "grad_norm": 2.2762527465820312,
      "learning_rate": 1.929255909350728e-05,
      "loss": 1.7728,
      "step": 10720
    },
    {
      "epoch": 1.0787211581963505,
      "grad_norm": 1.706919550895691,
      "learning_rate": 1.9282472008338657e-05,
      "loss": 1.8194,
      "step": 10730
    },
    {
      "epoch": 1.0797265369728044,
      "grad_norm": 2.088440179824829,
      "learning_rate": 1.9272384923170033e-05,
      "loss": 1.811,
      "step": 10740
    },
    {
      "epoch": 1.0807319157492585,
      "grad_norm": 1.9262089729309082,
      "learning_rate": 1.9262297838001413e-05,
      "loss": 1.9388,
      "step": 10750
    },
    {
      "epoch": 1.0817372945257127,
      "grad_norm": 2.2428507804870605,
      "learning_rate": 1.925221075283279e-05,
      "loss": 1.7568,
      "step": 10760
    },
    {
      "epoch": 1.0827426733021666,
      "grad_norm": 1.6283035278320312,
      "learning_rate": 1.924212366766417e-05,
      "loss": 1.8479,
      "step": 10770
    },
    {
      "epoch": 1.0837480520786207,
      "grad_norm": 2.0133960247039795,
      "learning_rate": 1.9232036582495545e-05,
      "loss": 1.8709,
      "step": 10780
    },
    {
      "epoch": 1.0847534308550746,
      "grad_norm": 1.8935763835906982,
      "learning_rate": 1.9221949497326922e-05,
      "loss": 1.7054,
      "step": 10790
    },
    {
      "epoch": 1.0857588096315287,
      "grad_norm": 1.9190335273742676,
      "learning_rate": 1.92118624121583e-05,
      "loss": 1.6916,
      "step": 10800
    },
    {
      "epoch": 1.0867641884079826,
      "grad_norm": 2.303901433944702,
      "learning_rate": 1.9201775326989678e-05,
      "loss": 1.7016,
      "step": 10810
    },
    {
      "epoch": 1.0877695671844367,
      "grad_norm": 1.9824782609939575,
      "learning_rate": 1.9191688241821057e-05,
      "loss": 1.7847,
      "step": 10820
    },
    {
      "epoch": 1.0887749459608909,
      "grad_norm": 1.7683191299438477,
      "learning_rate": 1.9181601156652434e-05,
      "loss": 1.7905,
      "step": 10830
    },
    {
      "epoch": 1.0897803247373448,
      "grad_norm": 1.7073415517807007,
      "learning_rate": 1.9171514071483813e-05,
      "loss": 1.8039,
      "step": 10840
    },
    {
      "epoch": 1.0907857035137989,
      "grad_norm": 1.3844493627548218,
      "learning_rate": 1.9161426986315186e-05,
      "loss": 1.7215,
      "step": 10850
    },
    {
      "epoch": 1.0917910822902528,
      "grad_norm": 2.218444347381592,
      "learning_rate": 1.9151339901146566e-05,
      "loss": 1.7051,
      "step": 10860
    },
    {
      "epoch": 1.092796461066707,
      "grad_norm": 1.793212652206421,
      "learning_rate": 1.9141252815977942e-05,
      "loss": 1.7836,
      "step": 10870
    },
    {
      "epoch": 1.0938018398431608,
      "grad_norm": 1.846561312675476,
      "learning_rate": 1.9131165730809322e-05,
      "loss": 1.7963,
      "step": 10880
    },
    {
      "epoch": 1.094807218619615,
      "grad_norm": 2.06215763092041,
      "learning_rate": 1.91210786456407e-05,
      "loss": 1.7686,
      "step": 10890
    },
    {
      "epoch": 1.095812597396069,
      "grad_norm": 1.97019624710083,
      "learning_rate": 1.9110991560472078e-05,
      "loss": 1.8354,
      "step": 10900
    },
    {
      "epoch": 1.096817976172523,
      "grad_norm": 2.131145715713501,
      "learning_rate": 1.910090447530345e-05,
      "loss": 1.7666,
      "step": 10910
    },
    {
      "epoch": 1.097823354948977,
      "grad_norm": 2.4054315090179443,
      "learning_rate": 1.909081739013483e-05,
      "loss": 1.7459,
      "step": 10920
    },
    {
      "epoch": 1.098828733725431,
      "grad_norm": 2.2307612895965576,
      "learning_rate": 1.9080730304966207e-05,
      "loss": 1.7892,
      "step": 10930
    },
    {
      "epoch": 1.0998341125018851,
      "grad_norm": 2.2819645404815674,
      "learning_rate": 1.9070643219797587e-05,
      "loss": 1.8,
      "step": 10940
    },
    {
      "epoch": 1.100839491278339,
      "grad_norm": 1.8942111730575562,
      "learning_rate": 1.9060556134628963e-05,
      "loss": 1.8131,
      "step": 10950
    },
    {
      "epoch": 1.1018448700547931,
      "grad_norm": 2.504563570022583,
      "learning_rate": 1.9050469049460343e-05,
      "loss": 1.7886,
      "step": 10960
    },
    {
      "epoch": 1.1028502488312473,
      "grad_norm": 1.9950553178787231,
      "learning_rate": 1.904038196429172e-05,
      "loss": 1.7214,
      "step": 10970
    },
    {
      "epoch": 1.1038556276077012,
      "grad_norm": 2.184746265411377,
      "learning_rate": 1.9030294879123095e-05,
      "loss": 1.7993,
      "step": 10980
    },
    {
      "epoch": 1.1048610063841553,
      "grad_norm": 2.1018898487091064,
      "learning_rate": 1.9020207793954475e-05,
      "loss": 1.7194,
      "step": 10990
    },
    {
      "epoch": 1.1058663851606092,
      "grad_norm": 1.8123806715011597,
      "learning_rate": 1.901012070878585e-05,
      "loss": 1.7137,
      "step": 11000
    },
    {
      "epoch": 1.1068717639370633,
      "grad_norm": 1.9453365802764893,
      "learning_rate": 1.900003362361723e-05,
      "loss": 1.7324,
      "step": 11010
    },
    {
      "epoch": 1.1078771427135172,
      "grad_norm": 2.7537996768951416,
      "learning_rate": 1.8989946538448607e-05,
      "loss": 1.8491,
      "step": 11020
    },
    {
      "epoch": 1.1088825214899714,
      "grad_norm": 1.9428215026855469,
      "learning_rate": 1.8979859453279984e-05,
      "loss": 1.7762,
      "step": 11030
    },
    {
      "epoch": 1.1098879002664255,
      "grad_norm": 2.7131712436676025,
      "learning_rate": 1.896977236811136e-05,
      "loss": 1.7935,
      "step": 11040
    },
    {
      "epoch": 1.1108932790428794,
      "grad_norm": 3.2356600761413574,
      "learning_rate": 1.895968528294274e-05,
      "loss": 1.6473,
      "step": 11050
    },
    {
      "epoch": 1.1118986578193335,
      "grad_norm": 1.9848828315734863,
      "learning_rate": 1.8949598197774116e-05,
      "loss": 1.7881,
      "step": 11060
    },
    {
      "epoch": 1.1129040365957874,
      "grad_norm": 1.852797269821167,
      "learning_rate": 1.8939511112605496e-05,
      "loss": 1.7916,
      "step": 11070
    },
    {
      "epoch": 1.1139094153722415,
      "grad_norm": 1.7499481439590454,
      "learning_rate": 1.8929424027436872e-05,
      "loss": 1.7887,
      "step": 11080
    },
    {
      "epoch": 1.1149147941486954,
      "grad_norm": 1.6951160430908203,
      "learning_rate": 1.891933694226825e-05,
      "loss": 1.8036,
      "step": 11090
    },
    {
      "epoch": 1.1159201729251496,
      "grad_norm": 1.965260624885559,
      "learning_rate": 1.8909249857099628e-05,
      "loss": 1.835,
      "step": 11100
    },
    {
      "epoch": 1.1169255517016037,
      "grad_norm": 1.9935187101364136,
      "learning_rate": 1.8899162771931004e-05,
      "loss": 1.8569,
      "step": 11110
    },
    {
      "epoch": 1.1179309304780576,
      "grad_norm": 1.8857697248458862,
      "learning_rate": 1.8889075686762384e-05,
      "loss": 1.7696,
      "step": 11120
    },
    {
      "epoch": 1.1189363092545117,
      "grad_norm": 2.08913254737854,
      "learning_rate": 1.887898860159376e-05,
      "loss": 1.8595,
      "step": 11130
    },
    {
      "epoch": 1.1199416880309656,
      "grad_norm": 2.380570888519287,
      "learning_rate": 1.886890151642514e-05,
      "loss": 1.7354,
      "step": 11140
    },
    {
      "epoch": 1.1209470668074197,
      "grad_norm": 2.006561756134033,
      "learning_rate": 1.8858814431256513e-05,
      "loss": 1.7719,
      "step": 11150
    },
    {
      "epoch": 1.1219524455838736,
      "grad_norm": 1.6705485582351685,
      "learning_rate": 1.8848727346087893e-05,
      "loss": 1.7027,
      "step": 11160
    },
    {
      "epoch": 1.1229578243603278,
      "grad_norm": 2.718033790588379,
      "learning_rate": 1.883864026091927e-05,
      "loss": 1.8794,
      "step": 11170
    },
    {
      "epoch": 1.1239632031367819,
      "grad_norm": 2.0106916427612305,
      "learning_rate": 1.882855317575065e-05,
      "loss": 1.8137,
      "step": 11180
    },
    {
      "epoch": 1.1249685819132358,
      "grad_norm": 1.802944302558899,
      "learning_rate": 1.8818466090582025e-05,
      "loss": 1.6678,
      "step": 11190
    },
    {
      "epoch": 1.12597396068969,
      "grad_norm": 2.9569756984710693,
      "learning_rate": 1.8808379005413405e-05,
      "loss": 1.7773,
      "step": 11200
    },
    {
      "epoch": 1.1269793394661438,
      "grad_norm": 2.0633981227874756,
      "learning_rate": 1.8798291920244778e-05,
      "loss": 1.871,
      "step": 11210
    },
    {
      "epoch": 1.127984718242598,
      "grad_norm": 2.3063971996307373,
      "learning_rate": 1.8788204835076157e-05,
      "loss": 1.8327,
      "step": 11220
    },
    {
      "epoch": 1.1289900970190518,
      "grad_norm": 1.7786380052566528,
      "learning_rate": 1.8778117749907534e-05,
      "loss": 1.8561,
      "step": 11230
    },
    {
      "epoch": 1.129995475795506,
      "grad_norm": 1.6282789707183838,
      "learning_rate": 1.8768030664738913e-05,
      "loss": 1.7799,
      "step": 11240
    },
    {
      "epoch": 1.13100085457196,
      "grad_norm": 1.8906922340393066,
      "learning_rate": 1.875794357957029e-05,
      "loss": 1.7845,
      "step": 11250
    },
    {
      "epoch": 1.132006233348414,
      "grad_norm": 1.5918899774551392,
      "learning_rate": 1.874785649440167e-05,
      "loss": 1.8441,
      "step": 11260
    },
    {
      "epoch": 1.1330116121248681,
      "grad_norm": 1.8513067960739136,
      "learning_rate": 1.8737769409233046e-05,
      "loss": 1.7676,
      "step": 11270
    },
    {
      "epoch": 1.134016990901322,
      "grad_norm": 2.1362125873565674,
      "learning_rate": 1.8727682324064422e-05,
      "loss": 1.8211,
      "step": 11280
    },
    {
      "epoch": 1.1350223696777761,
      "grad_norm": 1.9844579696655273,
      "learning_rate": 1.8717595238895802e-05,
      "loss": 1.8076,
      "step": 11290
    },
    {
      "epoch": 1.13602774845423,
      "grad_norm": 1.9151716232299805,
      "learning_rate": 1.8707508153727178e-05,
      "loss": 1.7074,
      "step": 11300
    },
    {
      "epoch": 1.1370331272306842,
      "grad_norm": 2.2914817333221436,
      "learning_rate": 1.8697421068558558e-05,
      "loss": 1.6842,
      "step": 11310
    },
    {
      "epoch": 1.1380385060071383,
      "grad_norm": 1.814072608947754,
      "learning_rate": 1.8687333983389934e-05,
      "loss": 1.7606,
      "step": 11320
    },
    {
      "epoch": 1.1390438847835922,
      "grad_norm": 1.735349416732788,
      "learning_rate": 1.867724689822131e-05,
      "loss": 1.8289,
      "step": 11330
    },
    {
      "epoch": 1.1400492635600463,
      "grad_norm": 1.8880418539047241,
      "learning_rate": 1.8667159813052687e-05,
      "loss": 1.8253,
      "step": 11340
    },
    {
      "epoch": 1.1410546423365002,
      "grad_norm": 1.7658275365829468,
      "learning_rate": 1.8657072727884066e-05,
      "loss": 1.8478,
      "step": 11350
    },
    {
      "epoch": 1.1420600211129543,
      "grad_norm": 2.405952215194702,
      "learning_rate": 1.8646985642715443e-05,
      "loss": 1.7269,
      "step": 11360
    },
    {
      "epoch": 1.1430653998894083,
      "grad_norm": 1.9757277965545654,
      "learning_rate": 1.8636898557546822e-05,
      "loss": 1.8264,
      "step": 11370
    },
    {
      "epoch": 1.1440707786658624,
      "grad_norm": 1.8888168334960938,
      "learning_rate": 1.86268114723782e-05,
      "loss": 1.7692,
      "step": 11380
    },
    {
      "epoch": 1.1450761574423165,
      "grad_norm": 2.3920631408691406,
      "learning_rate": 1.8616724387209575e-05,
      "loss": 1.8007,
      "step": 11390
    },
    {
      "epoch": 1.1460815362187704,
      "grad_norm": 2.0095536708831787,
      "learning_rate": 1.860663730204095e-05,
      "loss": 1.7607,
      "step": 11400
    },
    {
      "epoch": 1.1470869149952245,
      "grad_norm": 2.0883514881134033,
      "learning_rate": 1.859655021687233e-05,
      "loss": 1.8178,
      "step": 11410
    },
    {
      "epoch": 1.1480922937716784,
      "grad_norm": 2.785879611968994,
      "learning_rate": 1.858646313170371e-05,
      "loss": 1.7969,
      "step": 11420
    },
    {
      "epoch": 1.1490976725481326,
      "grad_norm": 1.8849443197250366,
      "learning_rate": 1.8576376046535087e-05,
      "loss": 1.7723,
      "step": 11430
    },
    {
      "epoch": 1.1501030513245865,
      "grad_norm": 1.9520193338394165,
      "learning_rate": 1.8566288961366467e-05,
      "loss": 1.7554,
      "step": 11440
    },
    {
      "epoch": 1.1511084301010406,
      "grad_norm": 2.407339334487915,
      "learning_rate": 1.855620187619784e-05,
      "loss": 1.7793,
      "step": 11450
    },
    {
      "epoch": 1.1521138088774947,
      "grad_norm": 5.232269287109375,
      "learning_rate": 1.854611479102922e-05,
      "loss": 1.8029,
      "step": 11460
    },
    {
      "epoch": 1.1531191876539486,
      "grad_norm": 1.9026226997375488,
      "learning_rate": 1.8536027705860596e-05,
      "loss": 1.7914,
      "step": 11470
    },
    {
      "epoch": 1.1541245664304027,
      "grad_norm": 1.4837766885757446,
      "learning_rate": 1.8525940620691975e-05,
      "loss": 1.736,
      "step": 11480
    },
    {
      "epoch": 1.1551299452068566,
      "grad_norm": 1.5414421558380127,
      "learning_rate": 1.8515853535523352e-05,
      "loss": 1.7968,
      "step": 11490
    },
    {
      "epoch": 1.1561353239833108,
      "grad_norm": 2.256197452545166,
      "learning_rate": 1.850576645035473e-05,
      "loss": 1.7855,
      "step": 11500
    },
    {
      "epoch": 1.1571407027597647,
      "grad_norm": 2.079179525375366,
      "learning_rate": 1.8495679365186104e-05,
      "loss": 1.8252,
      "step": 11510
    },
    {
      "epoch": 1.1581460815362188,
      "grad_norm": 1.9878273010253906,
      "learning_rate": 1.8485592280017484e-05,
      "loss": 1.8075,
      "step": 11520
    },
    {
      "epoch": 1.159151460312673,
      "grad_norm": 1.9457533359527588,
      "learning_rate": 1.847550519484886e-05,
      "loss": 1.7207,
      "step": 11530
    },
    {
      "epoch": 1.1601568390891268,
      "grad_norm": 1.9276968240737915,
      "learning_rate": 1.846541810968024e-05,
      "loss": 1.7774,
      "step": 11540
    },
    {
      "epoch": 1.161162217865581,
      "grad_norm": 2.0397238731384277,
      "learning_rate": 1.8455331024511616e-05,
      "loss": 1.7938,
      "step": 11550
    },
    {
      "epoch": 1.1621675966420348,
      "grad_norm": 2.134392738342285,
      "learning_rate": 1.8445243939342996e-05,
      "loss": 1.8291,
      "step": 11560
    },
    {
      "epoch": 1.163172975418489,
      "grad_norm": 2.2821261882781982,
      "learning_rate": 1.8435156854174372e-05,
      "loss": 1.7691,
      "step": 11570
    },
    {
      "epoch": 1.1641783541949429,
      "grad_norm": 2.470386505126953,
      "learning_rate": 1.842506976900575e-05,
      "loss": 1.773,
      "step": 11580
    },
    {
      "epoch": 1.165183732971397,
      "grad_norm": 1.9957983493804932,
      "learning_rate": 1.8415991392353992e-05,
      "loss": 1.7563,
      "step": 11590
    },
    {
      "epoch": 1.1661891117478511,
      "grad_norm": 1.6456708908081055,
      "learning_rate": 1.8405904307185365e-05,
      "loss": 1.7869,
      "step": 11600
    },
    {
      "epoch": 1.167194490524305,
      "grad_norm": 1.7936041355133057,
      "learning_rate": 1.8395817222016745e-05,
      "loss": 1.7084,
      "step": 11610
    },
    {
      "epoch": 1.1681998693007591,
      "grad_norm": 2.2933244705200195,
      "learning_rate": 1.838573013684812e-05,
      "loss": 1.7007,
      "step": 11620
    },
    {
      "epoch": 1.169205248077213,
      "grad_norm": 1.8526631593704224,
      "learning_rate": 1.83756430516795e-05,
      "loss": 1.7997,
      "step": 11630
    },
    {
      "epoch": 1.1702106268536672,
      "grad_norm": 1.8173061609268188,
      "learning_rate": 1.8365555966510877e-05,
      "loss": 1.8481,
      "step": 11640
    },
    {
      "epoch": 1.171216005630121,
      "grad_norm": 1.8964588642120361,
      "learning_rate": 1.8355468881342257e-05,
      "loss": 1.7611,
      "step": 11650
    },
    {
      "epoch": 1.1722213844065752,
      "grad_norm": 2.1674623489379883,
      "learning_rate": 1.834538179617363e-05,
      "loss": 1.8201,
      "step": 11660
    },
    {
      "epoch": 1.1732267631830293,
      "grad_norm": 1.8481422662734985,
      "learning_rate": 1.833529471100501e-05,
      "loss": 1.78,
      "step": 11670
    },
    {
      "epoch": 1.1742321419594832,
      "grad_norm": 2.427155017852783,
      "learning_rate": 1.8325207625836386e-05,
      "loss": 1.7943,
      "step": 11680
    },
    {
      "epoch": 1.1752375207359373,
      "grad_norm": 1.9865113496780396,
      "learning_rate": 1.8315120540667766e-05,
      "loss": 1.7785,
      "step": 11690
    },
    {
      "epoch": 1.1762428995123912,
      "grad_norm": 1.6601574420928955,
      "learning_rate": 1.8305033455499145e-05,
      "loss": 1.7141,
      "step": 11700
    },
    {
      "epoch": 1.1772482782888454,
      "grad_norm": 1.6216691732406616,
      "learning_rate": 1.8294946370330522e-05,
      "loss": 1.7581,
      "step": 11710
    },
    {
      "epoch": 1.1782536570652993,
      "grad_norm": 1.7666770219802856,
      "learning_rate": 1.8284859285161898e-05,
      "loss": 1.7691,
      "step": 11720
    },
    {
      "epoch": 1.1792590358417534,
      "grad_norm": 3.0713729858398438,
      "learning_rate": 1.8274772199993274e-05,
      "loss": 1.72,
      "step": 11730
    },
    {
      "epoch": 1.1802644146182075,
      "grad_norm": 2.0465800762176514,
      "learning_rate": 1.8264685114824654e-05,
      "loss": 1.761,
      "step": 11740
    },
    {
      "epoch": 1.1812697933946614,
      "grad_norm": 2.103797674179077,
      "learning_rate": 1.825459802965603e-05,
      "loss": 1.7092,
      "step": 11750
    },
    {
      "epoch": 1.1822751721711156,
      "grad_norm": 2.0067989826202393,
      "learning_rate": 1.824451094448741e-05,
      "loss": 1.8292,
      "step": 11760
    },
    {
      "epoch": 1.1832805509475695,
      "grad_norm": 1.686241626739502,
      "learning_rate": 1.8234423859318786e-05,
      "loss": 1.7607,
      "step": 11770
    },
    {
      "epoch": 1.1842859297240236,
      "grad_norm": 1.7862775325775146,
      "learning_rate": 1.8224336774150163e-05,
      "loss": 1.8013,
      "step": 11780
    },
    {
      "epoch": 1.1852913085004775,
      "grad_norm": 2.217435121536255,
      "learning_rate": 1.821424968898154e-05,
      "loss": 1.7841,
      "step": 11790
    },
    {
      "epoch": 1.1862966872769316,
      "grad_norm": 1.890092372894287,
      "learning_rate": 1.820416260381292e-05,
      "loss": 1.8003,
      "step": 11800
    },
    {
      "epoch": 1.1873020660533857,
      "grad_norm": 2.1775238513946533,
      "learning_rate": 1.8194075518644295e-05,
      "loss": 1.7309,
      "step": 11810
    },
    {
      "epoch": 1.1883074448298396,
      "grad_norm": 1.8621991872787476,
      "learning_rate": 1.8183988433475675e-05,
      "loss": 1.7673,
      "step": 11820
    },
    {
      "epoch": 1.1893128236062938,
      "grad_norm": 1.9105322360992432,
      "learning_rate": 1.817390134830705e-05,
      "loss": 1.7949,
      "step": 11830
    },
    {
      "epoch": 1.1903182023827477,
      "grad_norm": 1.8088446855545044,
      "learning_rate": 1.8163814263138427e-05,
      "loss": 1.7981,
      "step": 11840
    },
    {
      "epoch": 1.1913235811592018,
      "grad_norm": 1.7490051984786987,
      "learning_rate": 1.8153727177969807e-05,
      "loss": 1.7804,
      "step": 11850
    },
    {
      "epoch": 1.1923289599356557,
      "grad_norm": 1.871730089187622,
      "learning_rate": 1.8143640092801183e-05,
      "loss": 1.8145,
      "step": 11860
    },
    {
      "epoch": 1.1933343387121098,
      "grad_norm": 2.2797365188598633,
      "learning_rate": 1.8133553007632563e-05,
      "loss": 1.7579,
      "step": 11870
    },
    {
      "epoch": 1.194339717488564,
      "grad_norm": 2.564885139465332,
      "learning_rate": 1.812346592246394e-05,
      "loss": 1.8227,
      "step": 11880
    },
    {
      "epoch": 1.1953450962650178,
      "grad_norm": 2.2979538440704346,
      "learning_rate": 1.811337883729532e-05,
      "loss": 1.6859,
      "step": 11890
    },
    {
      "epoch": 1.196350475041472,
      "grad_norm": 2.2159180641174316,
      "learning_rate": 1.8103291752126692e-05,
      "loss": 1.7643,
      "step": 11900
    },
    {
      "epoch": 1.1973558538179259,
      "grad_norm": 1.9138416051864624,
      "learning_rate": 1.8093204666958072e-05,
      "loss": 1.8436,
      "step": 11910
    },
    {
      "epoch": 1.19836123259438,
      "grad_norm": 1.8679269552230835,
      "learning_rate": 1.8083117581789448e-05,
      "loss": 1.7628,
      "step": 11920
    },
    {
      "epoch": 1.199366611370834,
      "grad_norm": 1.774368405342102,
      "learning_rate": 1.8073030496620828e-05,
      "loss": 1.6947,
      "step": 11930
    },
    {
      "epoch": 1.200371990147288,
      "grad_norm": 1.7695807218551636,
      "learning_rate": 1.8062943411452204e-05,
      "loss": 1.8718,
      "step": 11940
    },
    {
      "epoch": 1.2013773689237421,
      "grad_norm": 2.1067473888397217,
      "learning_rate": 1.8052856326283584e-05,
      "loss": 1.7887,
      "step": 11950
    },
    {
      "epoch": 1.202382747700196,
      "grad_norm": 2.2618227005004883,
      "learning_rate": 1.8042769241114957e-05,
      "loss": 1.8078,
      "step": 11960
    },
    {
      "epoch": 1.2033881264766502,
      "grad_norm": 2.024545907974243,
      "learning_rate": 1.8032682155946336e-05,
      "loss": 1.7293,
      "step": 11970
    },
    {
      "epoch": 1.204393505253104,
      "grad_norm": 1.899287462234497,
      "learning_rate": 1.8022595070777713e-05,
      "loss": 1.8109,
      "step": 11980
    },
    {
      "epoch": 1.2053988840295582,
      "grad_norm": 1.7193280458450317,
      "learning_rate": 1.8012507985609092e-05,
      "loss": 1.7558,
      "step": 11990
    },
    {
      "epoch": 1.206404262806012,
      "grad_norm": 2.061847686767578,
      "learning_rate": 1.8002420900440472e-05,
      "loss": 1.6847,
      "step": 12000
    },
    {
      "epoch": 1.2074096415824662,
      "grad_norm": 2.7412941455841064,
      "learning_rate": 1.799233381527185e-05,
      "loss": 1.8008,
      "step": 12010
    },
    {
      "epoch": 1.2084150203589203,
      "grad_norm": 1.9687312841415405,
      "learning_rate": 1.7982246730103225e-05,
      "loss": 1.7082,
      "step": 12020
    },
    {
      "epoch": 1.2094203991353742,
      "grad_norm": 2.025036334991455,
      "learning_rate": 1.79721596449346e-05,
      "loss": 1.7726,
      "step": 12030
    },
    {
      "epoch": 1.2104257779118284,
      "grad_norm": 2.0286529064178467,
      "learning_rate": 1.796207255976598e-05,
      "loss": 1.7479,
      "step": 12040
    },
    {
      "epoch": 1.2114311566882823,
      "grad_norm": 2.585242986679077,
      "learning_rate": 1.7951985474597357e-05,
      "loss": 1.7296,
      "step": 12050
    },
    {
      "epoch": 1.2124365354647364,
      "grad_norm": 2.687377452850342,
      "learning_rate": 1.7941898389428737e-05,
      "loss": 1.7808,
      "step": 12060
    },
    {
      "epoch": 1.2134419142411903,
      "grad_norm": 1.7183796167373657,
      "learning_rate": 1.7931811304260113e-05,
      "loss": 1.8071,
      "step": 12070
    },
    {
      "epoch": 1.2144472930176444,
      "grad_norm": 1.9161341190338135,
      "learning_rate": 1.792172421909149e-05,
      "loss": 1.7773,
      "step": 12080
    },
    {
      "epoch": 1.2154526717940985,
      "grad_norm": 1.6843597888946533,
      "learning_rate": 1.7911637133922866e-05,
      "loss": 1.8653,
      "step": 12090
    },
    {
      "epoch": 1.2164580505705525,
      "grad_norm": 1.7068063020706177,
      "learning_rate": 1.7901550048754245e-05,
      "loss": 1.7629,
      "step": 12100
    },
    {
      "epoch": 1.2174634293470066,
      "grad_norm": 1.8675134181976318,
      "learning_rate": 1.7891462963585622e-05,
      "loss": 1.7674,
      "step": 12110
    },
    {
      "epoch": 1.2184688081234605,
      "grad_norm": 2.292506456375122,
      "learning_rate": 1.7881375878417e-05,
      "loss": 1.7666,
      "step": 12120
    },
    {
      "epoch": 1.2194741868999146,
      "grad_norm": 1.9729139804840088,
      "learning_rate": 1.7871288793248378e-05,
      "loss": 1.8436,
      "step": 12130
    },
    {
      "epoch": 1.2204795656763685,
      "grad_norm": 2.9137356281280518,
      "learning_rate": 1.7861201708079754e-05,
      "loss": 1.8054,
      "step": 12140
    },
    {
      "epoch": 1.2214849444528226,
      "grad_norm": 1.7277963161468506,
      "learning_rate": 1.7851114622911134e-05,
      "loss": 1.7748,
      "step": 12150
    },
    {
      "epoch": 1.2224903232292768,
      "grad_norm": 1.8792804479599,
      "learning_rate": 1.784102753774251e-05,
      "loss": 1.7781,
      "step": 12160
    },
    {
      "epoch": 1.2234957020057307,
      "grad_norm": 2.0401198863983154,
      "learning_rate": 1.783094045257389e-05,
      "loss": 1.7161,
      "step": 12170
    },
    {
      "epoch": 1.2245010807821848,
      "grad_norm": 2.182201623916626,
      "learning_rate": 1.7820853367405266e-05,
      "loss": 1.7236,
      "step": 12180
    },
    {
      "epoch": 1.2255064595586387,
      "grad_norm": 2.214881658554077,
      "learning_rate": 1.7810766282236646e-05,
      "loss": 1.7784,
      "step": 12190
    },
    {
      "epoch": 1.2265118383350928,
      "grad_norm": 2.242917537689209,
      "learning_rate": 1.780067919706802e-05,
      "loss": 1.7555,
      "step": 12200
    },
    {
      "epoch": 1.2275172171115467,
      "grad_norm": 2.1551880836486816,
      "learning_rate": 1.77905921118994e-05,
      "loss": 1.8786,
      "step": 12210
    },
    {
      "epoch": 1.2285225958880008,
      "grad_norm": 1.9804718494415283,
      "learning_rate": 1.7780505026730775e-05,
      "loss": 1.752,
      "step": 12220
    },
    {
      "epoch": 1.2295279746644547,
      "grad_norm": 1.910097360610962,
      "learning_rate": 1.7770417941562154e-05,
      "loss": 1.736,
      "step": 12230
    },
    {
      "epoch": 1.2305333534409089,
      "grad_norm": 2.02365779876709,
      "learning_rate": 1.776033085639353e-05,
      "loss": 1.705,
      "step": 12240
    },
    {
      "epoch": 1.231538732217363,
      "grad_norm": 1.8965270519256592,
      "learning_rate": 1.775024377122491e-05,
      "loss": 1.6707,
      "step": 12250
    },
    {
      "epoch": 1.2325441109938169,
      "grad_norm": 1.706744909286499,
      "learning_rate": 1.7740156686056287e-05,
      "loss": 1.8138,
      "step": 12260
    },
    {
      "epoch": 1.233549489770271,
      "grad_norm": 2.512871265411377,
      "learning_rate": 1.7730069600887663e-05,
      "loss": 1.8455,
      "step": 12270
    },
    {
      "epoch": 1.234554868546725,
      "grad_norm": 2.0733048915863037,
      "learning_rate": 1.771998251571904e-05,
      "loss": 1.8106,
      "step": 12280
    },
    {
      "epoch": 1.235560247323179,
      "grad_norm": 1.7783637046813965,
      "learning_rate": 1.770989543055042e-05,
      "loss": 1.8321,
      "step": 12290
    },
    {
      "epoch": 1.236565626099633,
      "grad_norm": 1.9682586193084717,
      "learning_rate": 1.7699808345381795e-05,
      "loss": 1.7571,
      "step": 12300
    },
    {
      "epoch": 1.237571004876087,
      "grad_norm": 2.4332962036132812,
      "learning_rate": 1.7689721260213175e-05,
      "loss": 1.814,
      "step": 12310
    },
    {
      "epoch": 1.2385763836525412,
      "grad_norm": 1.6339969635009766,
      "learning_rate": 1.7679634175044555e-05,
      "loss": 1.7714,
      "step": 12320
    },
    {
      "epoch": 1.239581762428995,
      "grad_norm": 2.133331537246704,
      "learning_rate": 1.7669547089875928e-05,
      "loss": 1.8019,
      "step": 12330
    },
    {
      "epoch": 1.2405871412054492,
      "grad_norm": 2.3981616497039795,
      "learning_rate": 1.7659460004707307e-05,
      "loss": 1.7691,
      "step": 12340
    },
    {
      "epoch": 1.2415925199819031,
      "grad_norm": 2.4957807064056396,
      "learning_rate": 1.7649372919538684e-05,
      "loss": 1.7178,
      "step": 12350
    },
    {
      "epoch": 1.2425978987583572,
      "grad_norm": 1.7536324262619019,
      "learning_rate": 1.7639285834370064e-05,
      "loss": 1.8072,
      "step": 12360
    },
    {
      "epoch": 1.2436032775348111,
      "grad_norm": 1.9848706722259521,
      "learning_rate": 1.762919874920144e-05,
      "loss": 1.7868,
      "step": 12370
    },
    {
      "epoch": 1.2446086563112653,
      "grad_norm": 3.2589945793151855,
      "learning_rate": 1.761911166403282e-05,
      "loss": 1.8001,
      "step": 12380
    },
    {
      "epoch": 1.2456140350877192,
      "grad_norm": 1.7827739715576172,
      "learning_rate": 1.7609024578864192e-05,
      "loss": 1.7528,
      "step": 12390
    },
    {
      "epoch": 1.2466194138641733,
      "grad_norm": 1.6329350471496582,
      "learning_rate": 1.7598937493695572e-05,
      "loss": 1.7829,
      "step": 12400
    },
    {
      "epoch": 1.2476247926406274,
      "grad_norm": 2.116990089416504,
      "learning_rate": 1.758885040852695e-05,
      "loss": 1.9294,
      "step": 12410
    },
    {
      "epoch": 1.2486301714170813,
      "grad_norm": 2.147188186645508,
      "learning_rate": 1.7578763323358328e-05,
      "loss": 1.77,
      "step": 12420
    },
    {
      "epoch": 1.2496355501935354,
      "grad_norm": 1.9604830741882324,
      "learning_rate": 1.7568676238189704e-05,
      "loss": 1.7375,
      "step": 12430
    },
    {
      "epoch": 1.2506409289699896,
      "grad_norm": 1.9305188655853271,
      "learning_rate": 1.7558589153021084e-05,
      "loss": 1.738,
      "step": 12440
    },
    {
      "epoch": 1.2516463077464435,
      "grad_norm": 1.8500659465789795,
      "learning_rate": 1.754850206785246e-05,
      "loss": 1.7487,
      "step": 12450
    },
    {
      "epoch": 1.2526516865228974,
      "grad_norm": 2.394458055496216,
      "learning_rate": 1.7538414982683837e-05,
      "loss": 1.7263,
      "step": 12460
    },
    {
      "epoch": 1.2536570652993515,
      "grad_norm": 1.9154092073440552,
      "learning_rate": 1.7528327897515217e-05,
      "loss": 1.849,
      "step": 12470
    },
    {
      "epoch": 1.2546624440758056,
      "grad_norm": 2.067690849304199,
      "learning_rate": 1.7518240812346593e-05,
      "loss": 1.7702,
      "step": 12480
    },
    {
      "epoch": 1.2556678228522595,
      "grad_norm": 2.2334487438201904,
      "learning_rate": 1.7508153727177973e-05,
      "loss": 1.8464,
      "step": 12490
    },
    {
      "epoch": 1.2566732016287137,
      "grad_norm": 2.4130091667175293,
      "learning_rate": 1.749806664200935e-05,
      "loss": 1.7969,
      "step": 12500
    },
    {
      "epoch": 1.2576785804051678,
      "grad_norm": 1.8964157104492188,
      "learning_rate": 1.7487979556840725e-05,
      "loss": 1.7291,
      "step": 12510
    },
    {
      "epoch": 1.2586839591816217,
      "grad_norm": 1.8466877937316895,
      "learning_rate": 1.74778924716721e-05,
      "loss": 1.8033,
      "step": 12520
    },
    {
      "epoch": 1.2596893379580756,
      "grad_norm": 2.427265167236328,
      "learning_rate": 1.746780538650348e-05,
      "loss": 1.7279,
      "step": 12530
    },
    {
      "epoch": 1.2606947167345297,
      "grad_norm": 2.567150354385376,
      "learning_rate": 1.7457718301334857e-05,
      "loss": 1.5774,
      "step": 12540
    },
    {
      "epoch": 1.2617000955109838,
      "grad_norm": 2.3976564407348633,
      "learning_rate": 1.7447631216166237e-05,
      "loss": 1.7434,
      "step": 12550
    },
    {
      "epoch": 1.2627054742874377,
      "grad_norm": 1.7685269117355347,
      "learning_rate": 1.7437544130997613e-05,
      "loss": 1.7848,
      "step": 12560
    },
    {
      "epoch": 1.2637108530638919,
      "grad_norm": 2.8956263065338135,
      "learning_rate": 1.742745704582899e-05,
      "loss": 1.7753,
      "step": 12570
    },
    {
      "epoch": 1.264716231840346,
      "grad_norm": 1.7981257438659668,
      "learning_rate": 1.7417369960660366e-05,
      "loss": 1.7605,
      "step": 12580
    },
    {
      "epoch": 1.2657216106167999,
      "grad_norm": 1.7924836874008179,
      "learning_rate": 1.7407282875491746e-05,
      "loss": 1.8343,
      "step": 12590
    },
    {
      "epoch": 1.2667269893932538,
      "grad_norm": 1.6159489154815674,
      "learning_rate": 1.7397195790323122e-05,
      "loss": 1.7813,
      "step": 12600
    },
    {
      "epoch": 1.267732368169708,
      "grad_norm": 3.7631144523620605,
      "learning_rate": 1.7387108705154502e-05,
      "loss": 1.8539,
      "step": 12610
    },
    {
      "epoch": 1.268737746946162,
      "grad_norm": 1.448179006576538,
      "learning_rate": 1.737702161998588e-05,
      "loss": 1.735,
      "step": 12620
    },
    {
      "epoch": 1.269743125722616,
      "grad_norm": 1.8125683069229126,
      "learning_rate": 1.7366934534817254e-05,
      "loss": 1.7395,
      "step": 12630
    },
    {
      "epoch": 1.27074850449907,
      "grad_norm": 2.769605875015259,
      "learning_rate": 1.7356847449648634e-05,
      "loss": 1.6898,
      "step": 12640
    },
    {
      "epoch": 1.2717538832755242,
      "grad_norm": 1.7347735166549683,
      "learning_rate": 1.734676036448001e-05,
      "loss": 1.7495,
      "step": 12650
    },
    {
      "epoch": 1.272759262051978,
      "grad_norm": 2.39260196685791,
      "learning_rate": 1.733667327931139e-05,
      "loss": 1.7575,
      "step": 12660
    },
    {
      "epoch": 1.273764640828432,
      "grad_norm": 2.6935250759124756,
      "learning_rate": 1.7326586194142767e-05,
      "loss": 1.7713,
      "step": 12670
    },
    {
      "epoch": 1.2747700196048861,
      "grad_norm": 1.8871568441390991,
      "learning_rate": 1.7316499108974146e-05,
      "loss": 1.7819,
      "step": 12680
    },
    {
      "epoch": 1.2757753983813402,
      "grad_norm": 2.2750916481018066,
      "learning_rate": 1.730641202380552e-05,
      "loss": 1.7498,
      "step": 12690
    },
    {
      "epoch": 1.2767807771577941,
      "grad_norm": 2.454106092453003,
      "learning_rate": 1.72963249386369e-05,
      "loss": 1.7271,
      "step": 12700
    },
    {
      "epoch": 1.2777861559342483,
      "grad_norm": 3.2810895442962646,
      "learning_rate": 1.7286237853468275e-05,
      "loss": 1.7213,
      "step": 12710
    },
    {
      "epoch": 1.2787915347107024,
      "grad_norm": 1.8181229829788208,
      "learning_rate": 1.7276150768299655e-05,
      "loss": 1.7527,
      "step": 12720
    },
    {
      "epoch": 1.2797969134871563,
      "grad_norm": 2.1770761013031006,
      "learning_rate": 1.726606368313103e-05,
      "loss": 1.7963,
      "step": 12730
    },
    {
      "epoch": 1.2808022922636102,
      "grad_norm": 1.8674870729446411,
      "learning_rate": 1.725597659796241e-05,
      "loss": 1.831,
      "step": 12740
    },
    {
      "epoch": 1.2818076710400643,
      "grad_norm": 2.788835287094116,
      "learning_rate": 1.7245889512793784e-05,
      "loss": 1.7946,
      "step": 12750
    },
    {
      "epoch": 1.2828130498165184,
      "grad_norm": 2.524226427078247,
      "learning_rate": 1.7235802427625163e-05,
      "loss": 1.7396,
      "step": 12760
    },
    {
      "epoch": 1.2838184285929723,
      "grad_norm": 1.8444156646728516,
      "learning_rate": 1.7225715342456543e-05,
      "loss": 1.7362,
      "step": 12770
    },
    {
      "epoch": 1.2848238073694265,
      "grad_norm": 1.9310203790664673,
      "learning_rate": 1.721562825728792e-05,
      "loss": 1.8461,
      "step": 12780
    },
    {
      "epoch": 1.2858291861458806,
      "grad_norm": 2.246073007583618,
      "learning_rate": 1.72055411721193e-05,
      "loss": 1.9064,
      "step": 12790
    },
    {
      "epoch": 1.2868345649223345,
      "grad_norm": 2.284271478652954,
      "learning_rate": 1.7195454086950676e-05,
      "loss": 1.6819,
      "step": 12800
    },
    {
      "epoch": 1.2878399436987884,
      "grad_norm": 1.97719407081604,
      "learning_rate": 1.7185367001782052e-05,
      "loss": 1.7533,
      "step": 12810
    },
    {
      "epoch": 1.2888453224752425,
      "grad_norm": 2.3214404582977295,
      "learning_rate": 1.7175279916613428e-05,
      "loss": 1.7614,
      "step": 12820
    },
    {
      "epoch": 1.2898507012516967,
      "grad_norm": 1.941901445388794,
      "learning_rate": 1.7165192831444808e-05,
      "loss": 1.7751,
      "step": 12830
    },
    {
      "epoch": 1.2908560800281506,
      "grad_norm": 2.199368476867676,
      "learning_rate": 1.7155105746276184e-05,
      "loss": 1.84,
      "step": 12840
    },
    {
      "epoch": 1.2918614588046047,
      "grad_norm": 1.638691782951355,
      "learning_rate": 1.7145018661107564e-05,
      "loss": 1.905,
      "step": 12850
    },
    {
      "epoch": 1.2928668375810586,
      "grad_norm": 1.9020211696624756,
      "learning_rate": 1.713493157593894e-05,
      "loss": 1.8055,
      "step": 12860
    },
    {
      "epoch": 1.2938722163575127,
      "grad_norm": 1.9500077962875366,
      "learning_rate": 1.7124844490770316e-05,
      "loss": 1.7703,
      "step": 12870
    },
    {
      "epoch": 1.2948775951339666,
      "grad_norm": 1.7006750106811523,
      "learning_rate": 1.7114757405601693e-05,
      "loss": 1.7923,
      "step": 12880
    },
    {
      "epoch": 1.2958829739104207,
      "grad_norm": 3.1255900859832764,
      "learning_rate": 1.7104670320433073e-05,
      "loss": 1.7585,
      "step": 12890
    },
    {
      "epoch": 1.2968883526868749,
      "grad_norm": 1.6729531288146973,
      "learning_rate": 1.709458323526445e-05,
      "loss": 1.763,
      "step": 12900
    },
    {
      "epoch": 1.2978937314633288,
      "grad_norm": 1.6611404418945312,
      "learning_rate": 1.708449615009583e-05,
      "loss": 1.6809,
      "step": 12910
    },
    {
      "epoch": 1.2988991102397829,
      "grad_norm": 2.49489426612854,
      "learning_rate": 1.7074409064927208e-05,
      "loss": 1.7717,
      "step": 12920
    },
    {
      "epoch": 1.2999044890162368,
      "grad_norm": 2.3814685344696045,
      "learning_rate": 1.706432197975858e-05,
      "loss": 1.7733,
      "step": 12930
    },
    {
      "epoch": 1.300909867792691,
      "grad_norm": 2.361279249191284,
      "learning_rate": 1.705423489458996e-05,
      "loss": 1.7512,
      "step": 12940
    },
    {
      "epoch": 1.3019152465691448,
      "grad_norm": 1.729151964187622,
      "learning_rate": 1.7044147809421337e-05,
      "loss": 1.7937,
      "step": 12950
    },
    {
      "epoch": 1.302920625345599,
      "grad_norm": 1.6767276525497437,
      "learning_rate": 1.7034060724252717e-05,
      "loss": 1.8614,
      "step": 12960
    },
    {
      "epoch": 1.303926004122053,
      "grad_norm": 1.601131796836853,
      "learning_rate": 1.7023973639084093e-05,
      "loss": 1.7874,
      "step": 12970
    },
    {
      "epoch": 1.304931382898507,
      "grad_norm": 2.470207929611206,
      "learning_rate": 1.7013886553915473e-05,
      "loss": 1.8261,
      "step": 12980
    },
    {
      "epoch": 1.305936761674961,
      "grad_norm": 2.5722506046295166,
      "learning_rate": 1.7003799468746846e-05,
      "loss": 1.6884,
      "step": 12990
    },
    {
      "epoch": 1.306942140451415,
      "grad_norm": 2.3954951763153076,
      "learning_rate": 1.6993712383578226e-05,
      "loss": 1.8164,
      "step": 13000
    },
    {
      "epoch": 1.3079475192278691,
      "grad_norm": 1.681091547012329,
      "learning_rate": 1.6983625298409602e-05,
      "loss": 1.8822,
      "step": 13010
    },
    {
      "epoch": 1.308952898004323,
      "grad_norm": 1.7512004375457764,
      "learning_rate": 1.697353821324098e-05,
      "loss": 1.848,
      "step": 13020
    },
    {
      "epoch": 1.3099582767807771,
      "grad_norm": 1.7265936136245728,
      "learning_rate": 1.6963451128072358e-05,
      "loss": 1.7697,
      "step": 13030
    },
    {
      "epoch": 1.3109636555572313,
      "grad_norm": 1.8372888565063477,
      "learning_rate": 1.6953364042903738e-05,
      "loss": 1.7904,
      "step": 13040
    },
    {
      "epoch": 1.3119690343336852,
      "grad_norm": 2.9743707180023193,
      "learning_rate": 1.694327695773511e-05,
      "loss": 1.7537,
      "step": 13050
    },
    {
      "epoch": 1.3129744131101393,
      "grad_norm": 1.7151590585708618,
      "learning_rate": 1.693318987256649e-05,
      "loss": 1.7783,
      "step": 13060
    },
    {
      "epoch": 1.3139797918865932,
      "grad_norm": 2.1187679767608643,
      "learning_rate": 1.692310278739787e-05,
      "loss": 1.7839,
      "step": 13070
    },
    {
      "epoch": 1.3149851706630473,
      "grad_norm": 1.6424676179885864,
      "learning_rate": 1.6913015702229246e-05,
      "loss": 1.8034,
      "step": 13080
    },
    {
      "epoch": 1.3159905494395012,
      "grad_norm": 2.048454761505127,
      "learning_rate": 1.6902928617060626e-05,
      "loss": 1.7968,
      "step": 13090
    },
    {
      "epoch": 1.3169959282159553,
      "grad_norm": 1.453177571296692,
      "learning_rate": 1.6892841531892002e-05,
      "loss": 1.7428,
      "step": 13100
    },
    {
      "epoch": 1.3180013069924095,
      "grad_norm": 3.531827688217163,
      "learning_rate": 1.688275444672338e-05,
      "loss": 1.7314,
      "step": 13110
    },
    {
      "epoch": 1.3190066857688634,
      "grad_norm": 2.7202560901641846,
      "learning_rate": 1.6872667361554755e-05,
      "loss": 1.7888,
      "step": 13120
    },
    {
      "epoch": 1.3200120645453175,
      "grad_norm": 1.941554307937622,
      "learning_rate": 1.6862580276386135e-05,
      "loss": 1.7962,
      "step": 13130
    },
    {
      "epoch": 1.3210174433217714,
      "grad_norm": 1.822432041168213,
      "learning_rate": 1.685249319121751e-05,
      "loss": 1.7197,
      "step": 13140
    },
    {
      "epoch": 1.3220228220982255,
      "grad_norm": 2.5679852962493896,
      "learning_rate": 1.684240610604889e-05,
      "loss": 1.8389,
      "step": 13150
    },
    {
      "epoch": 1.3230282008746794,
      "grad_norm": 1.7575286626815796,
      "learning_rate": 1.6832319020880267e-05,
      "loss": 1.7814,
      "step": 13160
    },
    {
      "epoch": 1.3240335796511336,
      "grad_norm": 1.8082787990570068,
      "learning_rate": 1.6822231935711643e-05,
      "loss": 1.8036,
      "step": 13170
    },
    {
      "epoch": 1.3250389584275877,
      "grad_norm": 2.034705400466919,
      "learning_rate": 1.681214485054302e-05,
      "loss": 1.6383,
      "step": 13180
    },
    {
      "epoch": 1.3260443372040416,
      "grad_norm": 2.444930076599121,
      "learning_rate": 1.68020577653744e-05,
      "loss": 1.715,
      "step": 13190
    },
    {
      "epoch": 1.3270497159804957,
      "grad_norm": 2.144474744796753,
      "learning_rate": 1.6791970680205775e-05,
      "loss": 1.732,
      "step": 13200
    },
    {
      "epoch": 1.3280550947569496,
      "grad_norm": 1.677016258239746,
      "learning_rate": 1.6781883595037155e-05,
      "loss": 1.7763,
      "step": 13210
    },
    {
      "epoch": 1.3290604735334037,
      "grad_norm": 1.8733526468276978,
      "learning_rate": 1.6771796509868535e-05,
      "loss": 1.7469,
      "step": 13220
    },
    {
      "epoch": 1.3300658523098576,
      "grad_norm": 2.0379252433776855,
      "learning_rate": 1.6761709424699908e-05,
      "loss": 1.8286,
      "step": 13230
    },
    {
      "epoch": 1.3310712310863118,
      "grad_norm": 1.662588357925415,
      "learning_rate": 1.6751622339531288e-05,
      "loss": 1.8263,
      "step": 13240
    },
    {
      "epoch": 1.3320766098627659,
      "grad_norm": 1.8915799856185913,
      "learning_rate": 1.6741535254362664e-05,
      "loss": 1.6682,
      "step": 13250
    },
    {
      "epoch": 1.3330819886392198,
      "grad_norm": 1.8784099817276,
      "learning_rate": 1.6731448169194044e-05,
      "loss": 1.7913,
      "step": 13260
    },
    {
      "epoch": 1.334087367415674,
      "grad_norm": 1.771382212638855,
      "learning_rate": 1.672136108402542e-05,
      "loss": 1.8541,
      "step": 13270
    },
    {
      "epoch": 1.3350927461921278,
      "grad_norm": 2.6363959312438965,
      "learning_rate": 1.67112739988568e-05,
      "loss": 1.7104,
      "step": 13280
    },
    {
      "epoch": 1.336098124968582,
      "grad_norm": 2.4184813499450684,
      "learning_rate": 1.6701186913688172e-05,
      "loss": 1.73,
      "step": 13290
    },
    {
      "epoch": 1.3371035037450358,
      "grad_norm": 1.9304403066635132,
      "learning_rate": 1.6691099828519552e-05,
      "loss": 1.7782,
      "step": 13300
    },
    {
      "epoch": 1.33810888252149,
      "grad_norm": 2.7700095176696777,
      "learning_rate": 1.668101274335093e-05,
      "loss": 1.7532,
      "step": 13310
    },
    {
      "epoch": 1.339114261297944,
      "grad_norm": 2.1474268436431885,
      "learning_rate": 1.6670925658182308e-05,
      "loss": 1.7745,
      "step": 13320
    },
    {
      "epoch": 1.340119640074398,
      "grad_norm": 1.8884576559066772,
      "learning_rate": 1.6660838573013685e-05,
      "loss": 1.7862,
      "step": 13330
    },
    {
      "epoch": 1.3411250188508521,
      "grad_norm": 2.46039080619812,
      "learning_rate": 1.6650751487845064e-05,
      "loss": 1.8014,
      "step": 13340
    },
    {
      "epoch": 1.342130397627306,
      "grad_norm": 1.63520085811615,
      "learning_rate": 1.6640664402676437e-05,
      "loss": 1.7159,
      "step": 13350
    },
    {
      "epoch": 1.3431357764037601,
      "grad_norm": 2.1398706436157227,
      "learning_rate": 1.6630577317507817e-05,
      "loss": 1.6426,
      "step": 13360
    },
    {
      "epoch": 1.344141155180214,
      "grad_norm": 2.574270725250244,
      "learning_rate": 1.6620490232339197e-05,
      "loss": 1.7837,
      "step": 13370
    },
    {
      "epoch": 1.3451465339566682,
      "grad_norm": 1.7190754413604736,
      "learning_rate": 1.6610403147170573e-05,
      "loss": 1.8225,
      "step": 13380
    },
    {
      "epoch": 1.3461519127331223,
      "grad_norm": 2.810755491256714,
      "learning_rate": 1.6600316062001953e-05,
      "loss": 1.7548,
      "step": 13390
    },
    {
      "epoch": 1.3471572915095762,
      "grad_norm": 2.656461715698242,
      "learning_rate": 1.659022897683333e-05,
      "loss": 1.7402,
      "step": 13400
    },
    {
      "epoch": 1.3481626702860303,
      "grad_norm": 2.4550628662109375,
      "learning_rate": 1.6580141891664705e-05,
      "loss": 1.6987,
      "step": 13410
    },
    {
      "epoch": 1.3491680490624842,
      "grad_norm": 1.8989248275756836,
      "learning_rate": 1.657005480649608e-05,
      "loss": 1.7899,
      "step": 13420
    },
    {
      "epoch": 1.3501734278389383,
      "grad_norm": 2.2139947414398193,
      "learning_rate": 1.655996772132746e-05,
      "loss": 1.6878,
      "step": 13430
    },
    {
      "epoch": 1.3511788066153922,
      "grad_norm": 2.4914603233337402,
      "learning_rate": 1.6549880636158838e-05,
      "loss": 1.7263,
      "step": 13440
    },
    {
      "epoch": 1.3521841853918464,
      "grad_norm": 2.3079912662506104,
      "learning_rate": 1.6539793550990217e-05,
      "loss": 1.7019,
      "step": 13450
    },
    {
      "epoch": 1.3531895641683005,
      "grad_norm": 1.7794021368026733,
      "learning_rate": 1.6529706465821594e-05,
      "loss": 1.7462,
      "step": 13460
    },
    {
      "epoch": 1.3541949429447544,
      "grad_norm": 1.889469027519226,
      "learning_rate": 1.6519619380652973e-05,
      "loss": 1.7904,
      "step": 13470
    },
    {
      "epoch": 1.3552003217212085,
      "grad_norm": 1.9367027282714844,
      "learning_rate": 1.6509532295484346e-05,
      "loss": 1.6872,
      "step": 13480
    },
    {
      "epoch": 1.3562057004976624,
      "grad_norm": 1.992613673210144,
      "learning_rate": 1.6499445210315726e-05,
      "loss": 1.7301,
      "step": 13490
    },
    {
      "epoch": 1.3572110792741165,
      "grad_norm": 2.1562299728393555,
      "learning_rate": 1.6489358125147102e-05,
      "loss": 1.6962,
      "step": 13500
    },
    {
      "epoch": 1.3582164580505705,
      "grad_norm": 1.8151679039001465,
      "learning_rate": 1.6479271039978482e-05,
      "loss": 1.7503,
      "step": 13510
    },
    {
      "epoch": 1.3592218368270246,
      "grad_norm": 1.7634201049804688,
      "learning_rate": 1.646918395480986e-05,
      "loss": 1.849,
      "step": 13520
    },
    {
      "epoch": 1.3602272156034787,
      "grad_norm": 1.6283093690872192,
      "learning_rate": 1.6459096869641238e-05,
      "loss": 1.7374,
      "step": 13530
    },
    {
      "epoch": 1.3612325943799326,
      "grad_norm": 1.7758828401565552,
      "learning_rate": 1.6449009784472614e-05,
      "loss": 1.7514,
      "step": 13540
    },
    {
      "epoch": 1.3622379731563867,
      "grad_norm": 1.816838026046753,
      "learning_rate": 1.643892269930399e-05,
      "loss": 1.8014,
      "step": 13550
    },
    {
      "epoch": 1.3632433519328406,
      "grad_norm": 1.6517693996429443,
      "learning_rate": 1.642883561413537e-05,
      "loss": 1.7879,
      "step": 13560
    },
    {
      "epoch": 1.3642487307092948,
      "grad_norm": 2.0777838230133057,
      "learning_rate": 1.6418748528966747e-05,
      "loss": 1.7787,
      "step": 13570
    },
    {
      "epoch": 1.3652541094857487,
      "grad_norm": 2.0645840167999268,
      "learning_rate": 1.6408661443798126e-05,
      "loss": 1.7818,
      "step": 13580
    },
    {
      "epoch": 1.3662594882622028,
      "grad_norm": 1.9636398553848267,
      "learning_rate": 1.6398574358629503e-05,
      "loss": 1.7977,
      "step": 13590
    },
    {
      "epoch": 1.367264867038657,
      "grad_norm": 2.3629579544067383,
      "learning_rate": 1.638848727346088e-05,
      "loss": 1.7944,
      "step": 13600
    },
    {
      "epoch": 1.3682702458151108,
      "grad_norm": 1.7720179557800293,
      "learning_rate": 1.6378400188292255e-05,
      "loss": 1.7844,
      "step": 13610
    },
    {
      "epoch": 1.369275624591565,
      "grad_norm": 1.8772474527359009,
      "learning_rate": 1.6368313103123635e-05,
      "loss": 1.8051,
      "step": 13620
    },
    {
      "epoch": 1.3702810033680188,
      "grad_norm": 1.7818100452423096,
      "learning_rate": 1.635822601795501e-05,
      "loss": 1.7894,
      "step": 13630
    },
    {
      "epoch": 1.371286382144473,
      "grad_norm": 2.0683209896087646,
      "learning_rate": 1.634813893278639e-05,
      "loss": 1.7652,
      "step": 13640
    },
    {
      "epoch": 1.3722917609209269,
      "grad_norm": 1.750223159790039,
      "learning_rate": 1.6338051847617767e-05,
      "loss": 1.7518,
      "step": 13650
    },
    {
      "epoch": 1.373297139697381,
      "grad_norm": 1.641158938407898,
      "learning_rate": 1.6327964762449144e-05,
      "loss": 1.8071,
      "step": 13660
    },
    {
      "epoch": 1.374302518473835,
      "grad_norm": 1.8381410837173462,
      "learning_rate": 1.6317877677280523e-05,
      "loss": 1.755,
      "step": 13670
    },
    {
      "epoch": 1.375307897250289,
      "grad_norm": 2.0899646282196045,
      "learning_rate": 1.63077905921119e-05,
      "loss": 1.7595,
      "step": 13680
    },
    {
      "epoch": 1.3763132760267431,
      "grad_norm": 2.075211524963379,
      "learning_rate": 1.629770350694328e-05,
      "loss": 1.7948,
      "step": 13690
    },
    {
      "epoch": 1.377318654803197,
      "grad_norm": 2.3894240856170654,
      "learning_rate": 1.6287616421774656e-05,
      "loss": 1.7382,
      "step": 13700
    },
    {
      "epoch": 1.3783240335796512,
      "grad_norm": 1.6497410535812378,
      "learning_rate": 1.6277529336606035e-05,
      "loss": 1.7147,
      "step": 13710
    },
    {
      "epoch": 1.379329412356105,
      "grad_norm": 2.1325864791870117,
      "learning_rate": 1.6267442251437408e-05,
      "loss": 1.8,
      "step": 13720
    },
    {
      "epoch": 1.3803347911325592,
      "grad_norm": 2.145456314086914,
      "learning_rate": 1.6257355166268788e-05,
      "loss": 1.7039,
      "step": 13730
    },
    {
      "epoch": 1.3813401699090133,
      "grad_norm": 2.3281264305114746,
      "learning_rate": 1.6247268081100164e-05,
      "loss": 1.7344,
      "step": 13740
    },
    {
      "epoch": 1.3823455486854672,
      "grad_norm": 1.7353408336639404,
      "learning_rate": 1.6237180995931544e-05,
      "loss": 1.767,
      "step": 13750
    },
    {
      "epoch": 1.3833509274619213,
      "grad_norm": 1.9811922311782837,
      "learning_rate": 1.622709391076292e-05,
      "loss": 1.7911,
      "step": 13760
    },
    {
      "epoch": 1.3843563062383752,
      "grad_norm": 2.6598308086395264,
      "learning_rate": 1.62170068255943e-05,
      "loss": 1.7921,
      "step": 13770
    },
    {
      "epoch": 1.3853616850148294,
      "grad_norm": 1.9451466798782349,
      "learning_rate": 1.6206919740425673e-05,
      "loss": 1.7932,
      "step": 13780
    },
    {
      "epoch": 1.3863670637912833,
      "grad_norm": 1.8806300163269043,
      "learning_rate": 1.6196832655257053e-05,
      "loss": 1.7629,
      "step": 13790
    },
    {
      "epoch": 1.3873724425677374,
      "grad_norm": 2.4695301055908203,
      "learning_rate": 1.618674557008843e-05,
      "loss": 1.7213,
      "step": 13800
    },
    {
      "epoch": 1.3883778213441915,
      "grad_norm": 1.6517258882522583,
      "learning_rate": 1.617665848491981e-05,
      "loss": 1.8047,
      "step": 13810
    },
    {
      "epoch": 1.3893832001206454,
      "grad_norm": 2.1940841674804688,
      "learning_rate": 1.6166571399751188e-05,
      "loss": 1.7878,
      "step": 13820
    },
    {
      "epoch": 1.3903885788970995,
      "grad_norm": 1.6872413158416748,
      "learning_rate": 1.6156484314582565e-05,
      "loss": 1.7815,
      "step": 13830
    },
    {
      "epoch": 1.3913939576735534,
      "grad_norm": 2.1241745948791504,
      "learning_rate": 1.614639722941394e-05,
      "loss": 1.7771,
      "step": 13840
    },
    {
      "epoch": 1.3923993364500076,
      "grad_norm": 1.7751827239990234,
      "learning_rate": 1.6136310144245317e-05,
      "loss": 1.7394,
      "step": 13850
    },
    {
      "epoch": 1.3934047152264615,
      "grad_norm": 1.8907243013381958,
      "learning_rate": 1.6126223059076697e-05,
      "loss": 1.6847,
      "step": 13860
    },
    {
      "epoch": 1.3944100940029156,
      "grad_norm": 1.9568010568618774,
      "learning_rate": 1.6116135973908073e-05,
      "loss": 1.6763,
      "step": 13870
    },
    {
      "epoch": 1.3954154727793697,
      "grad_norm": 1.6441131830215454,
      "learning_rate": 1.6106048888739453e-05,
      "loss": 1.772,
      "step": 13880
    },
    {
      "epoch": 1.3964208515558236,
      "grad_norm": 2.21773099899292,
      "learning_rate": 1.609596180357083e-05,
      "loss": 1.7265,
      "step": 13890
    },
    {
      "epoch": 1.3974262303322778,
      "grad_norm": 2.04360032081604,
      "learning_rate": 1.6085874718402206e-05,
      "loss": 1.7423,
      "step": 13900
    },
    {
      "epoch": 1.3984316091087317,
      "grad_norm": 1.7338725328445435,
      "learning_rate": 1.6075787633233582e-05,
      "loss": 1.7021,
      "step": 13910
    },
    {
      "epoch": 1.3994369878851858,
      "grad_norm": 1.7933653593063354,
      "learning_rate": 1.606570054806496e-05,
      "loss": 1.7925,
      "step": 13920
    },
    {
      "epoch": 1.4004423666616397,
      "grad_norm": 1.8573769330978394,
      "learning_rate": 1.6055613462896338e-05,
      "loss": 1.6976,
      "step": 13930
    },
    {
      "epoch": 1.4014477454380938,
      "grad_norm": 1.8672934770584106,
      "learning_rate": 1.6045526377727718e-05,
      "loss": 1.7643,
      "step": 13940
    },
    {
      "epoch": 1.402453124214548,
      "grad_norm": 2.282923936843872,
      "learning_rate": 1.6035439292559094e-05,
      "loss": 1.7193,
      "step": 13950
    },
    {
      "epoch": 1.4034585029910018,
      "grad_norm": 2.0755748748779297,
      "learning_rate": 1.602535220739047e-05,
      "loss": 1.7647,
      "step": 13960
    },
    {
      "epoch": 1.404463881767456,
      "grad_norm": 1.8154815435409546,
      "learning_rate": 1.601526512222185e-05,
      "loss": 1.7603,
      "step": 13970
    },
    {
      "epoch": 1.4054692605439099,
      "grad_norm": 2.714620590209961,
      "learning_rate": 1.6005178037053226e-05,
      "loss": 1.772,
      "step": 13980
    },
    {
      "epoch": 1.406474639320364,
      "grad_norm": 3.5682640075683594,
      "learning_rate": 1.5995090951884606e-05,
      "loss": 1.7737,
      "step": 13990
    },
    {
      "epoch": 1.4074800180968179,
      "grad_norm": 2.5118021965026855,
      "learning_rate": 1.5985003866715982e-05,
      "loss": 1.7424,
      "step": 14000
    },
    {
      "epoch": 1.408485396873272,
      "grad_norm": 2.2717666625976562,
      "learning_rate": 1.5974916781547362e-05,
      "loss": 1.7645,
      "step": 14010
    },
    {
      "epoch": 1.4094907756497261,
      "grad_norm": 1.689096450805664,
      "learning_rate": 1.5964829696378735e-05,
      "loss": 1.7922,
      "step": 14020
    },
    {
      "epoch": 1.41049615442618,
      "grad_norm": 1.6798350811004639,
      "learning_rate": 1.5954742611210115e-05,
      "loss": 1.7178,
      "step": 14030
    },
    {
      "epoch": 1.4115015332026342,
      "grad_norm": 1.93679678440094,
      "learning_rate": 1.594465552604149e-05,
      "loss": 1.773,
      "step": 14040
    },
    {
      "epoch": 1.412506911979088,
      "grad_norm": 1.8357717990875244,
      "learning_rate": 1.593456844087287e-05,
      "loss": 1.7504,
      "step": 14050
    },
    {
      "epoch": 1.4135122907555422,
      "grad_norm": 2.090078353881836,
      "learning_rate": 1.5924481355704247e-05,
      "loss": 1.8368,
      "step": 14060
    },
    {
      "epoch": 1.414517669531996,
      "grad_norm": 2.522559404373169,
      "learning_rate": 1.5914394270535627e-05,
      "loss": 1.7981,
      "step": 14070
    },
    {
      "epoch": 1.4155230483084502,
      "grad_norm": 2.051882028579712,
      "learning_rate": 1.5904307185367e-05,
      "loss": 1.8269,
      "step": 14080
    },
    {
      "epoch": 1.4165284270849043,
      "grad_norm": 1.6299188137054443,
      "learning_rate": 1.589422010019838e-05,
      "loss": 1.7514,
      "step": 14090
    },
    {
      "epoch": 1.4175338058613582,
      "grad_norm": 1.8288556337356567,
      "learning_rate": 1.5884133015029756e-05,
      "loss": 1.8193,
      "step": 14100
    },
    {
      "epoch": 1.4185391846378124,
      "grad_norm": 2.4230761528015137,
      "learning_rate": 1.5874045929861135e-05,
      "loss": 1.7734,
      "step": 14110
    },
    {
      "epoch": 1.4195445634142663,
      "grad_norm": 1.9829959869384766,
      "learning_rate": 1.586395884469251e-05,
      "loss": 1.7722,
      "step": 14120
    },
    {
      "epoch": 1.4205499421907204,
      "grad_norm": 1.849257230758667,
      "learning_rate": 1.585387175952389e-05,
      "loss": 1.7761,
      "step": 14130
    },
    {
      "epoch": 1.4215553209671743,
      "grad_norm": 1.9065037965774536,
      "learning_rate": 1.5843784674355268e-05,
      "loss": 1.6852,
      "step": 14140
    },
    {
      "epoch": 1.4225606997436284,
      "grad_norm": 1.6126099824905396,
      "learning_rate": 1.5833697589186644e-05,
      "loss": 1.8476,
      "step": 14150
    },
    {
      "epoch": 1.4235660785200825,
      "grad_norm": 2.0375218391418457,
      "learning_rate": 1.5823610504018024e-05,
      "loss": 1.6902,
      "step": 14160
    },
    {
      "epoch": 1.4245714572965364,
      "grad_norm": 1.7687650918960571,
      "learning_rate": 1.58135234188494e-05,
      "loss": 1.8105,
      "step": 14170
    },
    {
      "epoch": 1.4255768360729906,
      "grad_norm": 2.0218284130096436,
      "learning_rate": 1.580343633368078e-05,
      "loss": 1.8092,
      "step": 14180
    },
    {
      "epoch": 1.4265822148494445,
      "grad_norm": 2.067605495452881,
      "learning_rate": 1.5793349248512156e-05,
      "loss": 1.6992,
      "step": 14190
    },
    {
      "epoch": 1.4275875936258986,
      "grad_norm": 1.8013941049575806,
      "learning_rate": 1.5783262163343532e-05,
      "loss": 1.7912,
      "step": 14200
    },
    {
      "epoch": 1.4285929724023525,
      "grad_norm": 2.3397605419158936,
      "learning_rate": 1.577317507817491e-05,
      "loss": 1.7398,
      "step": 14210
    },
    {
      "epoch": 1.4295983511788066,
      "grad_norm": 2.410527467727661,
      "learning_rate": 1.5763087993006288e-05,
      "loss": 1.6695,
      "step": 14220
    },
    {
      "epoch": 1.4306037299552608,
      "grad_norm": 1.920563817024231,
      "learning_rate": 1.5753000907837665e-05,
      "loss": 1.7604,
      "step": 14230
    },
    {
      "epoch": 1.4316091087317147,
      "grad_norm": 2.0254180431365967,
      "learning_rate": 1.5742913822669044e-05,
      "loss": 1.8551,
      "step": 14240
    },
    {
      "epoch": 1.4326144875081688,
      "grad_norm": 2.3423397541046143,
      "learning_rate": 1.573282673750042e-05,
      "loss": 1.856,
      "step": 14250
    },
    {
      "epoch": 1.4336198662846227,
      "grad_norm": 2.0733556747436523,
      "learning_rate": 1.5722739652331797e-05,
      "loss": 1.8123,
      "step": 14260
    },
    {
      "epoch": 1.4346252450610768,
      "grad_norm": 2.1510322093963623,
      "learning_rate": 1.5712652567163177e-05,
      "loss": 1.781,
      "step": 14270
    },
    {
      "epoch": 1.4356306238375307,
      "grad_norm": 1.9973748922348022,
      "learning_rate": 1.5702565481994553e-05,
      "loss": 1.6949,
      "step": 14280
    },
    {
      "epoch": 1.4366360026139848,
      "grad_norm": 2.217707395553589,
      "learning_rate": 1.5692478396825933e-05,
      "loss": 1.7262,
      "step": 14290
    },
    {
      "epoch": 1.437641381390439,
      "grad_norm": 1.9953222274780273,
      "learning_rate": 1.568239131165731e-05,
      "loss": 1.797,
      "step": 14300
    },
    {
      "epoch": 1.4386467601668929,
      "grad_norm": 1.7881613969802856,
      "learning_rate": 1.567230422648869e-05,
      "loss": 1.6956,
      "step": 14310
    },
    {
      "epoch": 1.439652138943347,
      "grad_norm": 2.5890214443206787,
      "learning_rate": 1.566221714132006e-05,
      "loss": 1.7259,
      "step": 14320
    },
    {
      "epoch": 1.4406575177198009,
      "grad_norm": 1.4973734617233276,
      "learning_rate": 1.565213005615144e-05,
      "loss": 1.7299,
      "step": 14330
    },
    {
      "epoch": 1.441662896496255,
      "grad_norm": 2.2892706394195557,
      "learning_rate": 1.5642042970982818e-05,
      "loss": 1.8027,
      "step": 14340
    },
    {
      "epoch": 1.442668275272709,
      "grad_norm": 2.2132792472839355,
      "learning_rate": 1.5631955885814197e-05,
      "loss": 1.7067,
      "step": 14350
    },
    {
      "epoch": 1.443673654049163,
      "grad_norm": 2.4117002487182617,
      "learning_rate": 1.5621868800645574e-05,
      "loss": 1.7515,
      "step": 14360
    },
    {
      "epoch": 1.4446790328256172,
      "grad_norm": 2.1628386974334717,
      "learning_rate": 1.5611781715476953e-05,
      "loss": 1.8174,
      "step": 14370
    },
    {
      "epoch": 1.445684411602071,
      "grad_norm": 2.1777944564819336,
      "learning_rate": 1.5601694630308326e-05,
      "loss": 1.7637,
      "step": 14380
    },
    {
      "epoch": 1.4466897903785252,
      "grad_norm": 1.5621373653411865,
      "learning_rate": 1.5591607545139706e-05,
      "loss": 1.7292,
      "step": 14390
    },
    {
      "epoch": 1.447695169154979,
      "grad_norm": 2.452202796936035,
      "learning_rate": 1.5581520459971082e-05,
      "loss": 1.7492,
      "step": 14400
    },
    {
      "epoch": 1.4487005479314332,
      "grad_norm": 2.1451478004455566,
      "learning_rate": 1.5571433374802462e-05,
      "loss": 1.7751,
      "step": 14410
    },
    {
      "epoch": 1.4497059267078871,
      "grad_norm": 2.389122724533081,
      "learning_rate": 1.5561346289633838e-05,
      "loss": 1.7793,
      "step": 14420
    },
    {
      "epoch": 1.4507113054843412,
      "grad_norm": 2.28670072555542,
      "learning_rate": 1.5551259204465218e-05,
      "loss": 1.7451,
      "step": 14430
    },
    {
      "epoch": 1.4517166842607954,
      "grad_norm": 3.2495203018188477,
      "learning_rate": 1.5541172119296594e-05,
      "loss": 1.7523,
      "step": 14440
    },
    {
      "epoch": 1.4527220630372493,
      "grad_norm": 1.9275799989700317,
      "learning_rate": 1.553108503412797e-05,
      "loss": 1.7931,
      "step": 14450
    },
    {
      "epoch": 1.4537274418137034,
      "grad_norm": 2.3990554809570312,
      "learning_rate": 1.552099794895935e-05,
      "loss": 1.7298,
      "step": 14460
    },
    {
      "epoch": 1.4547328205901573,
      "grad_norm": 2.159024953842163,
      "learning_rate": 1.5510910863790727e-05,
      "loss": 1.7176,
      "step": 14470
    },
    {
      "epoch": 1.4557381993666114,
      "grad_norm": 1.9683771133422852,
      "learning_rate": 1.5500823778622106e-05,
      "loss": 1.7839,
      "step": 14480
    },
    {
      "epoch": 1.4567435781430653,
      "grad_norm": 1.8824224472045898,
      "learning_rate": 1.5490736693453483e-05,
      "loss": 1.6463,
      "step": 14490
    },
    {
      "epoch": 1.4577489569195194,
      "grad_norm": 2.0998706817626953,
      "learning_rate": 1.548064960828486e-05,
      "loss": 1.8181,
      "step": 14500
    },
    {
      "epoch": 1.4587543356959736,
      "grad_norm": 1.8992323875427246,
      "learning_rate": 1.5470562523116235e-05,
      "loss": 1.7734,
      "step": 14510
    },
    {
      "epoch": 1.4597597144724275,
      "grad_norm": 1.4998215436935425,
      "learning_rate": 1.5460475437947615e-05,
      "loss": 1.7257,
      "step": 14520
    },
    {
      "epoch": 1.4607650932488816,
      "grad_norm": 2.8798160552978516,
      "learning_rate": 1.545038835277899e-05,
      "loss": 1.7716,
      "step": 14530
    },
    {
      "epoch": 1.4617704720253355,
      "grad_norm": 1.949168086051941,
      "learning_rate": 1.544030126761037e-05,
      "loss": 1.7495,
      "step": 14540
    },
    {
      "epoch": 1.4627758508017896,
      "grad_norm": 1.8596445322036743,
      "learning_rate": 1.5430214182441747e-05,
      "loss": 1.7496,
      "step": 14550
    },
    {
      "epoch": 1.4637812295782435,
      "grad_norm": 1.6343141794204712,
      "learning_rate": 1.5420127097273124e-05,
      "loss": 1.8195,
      "step": 14560
    },
    {
      "epoch": 1.4647866083546977,
      "grad_norm": 2.1502411365509033,
      "learning_rate": 1.54100400121045e-05,
      "loss": 1.8048,
      "step": 14570
    },
    {
      "epoch": 1.4657919871311518,
      "grad_norm": 2.4197895526885986,
      "learning_rate": 1.539995292693588e-05,
      "loss": 1.6601,
      "step": 14580
    },
    {
      "epoch": 1.4667973659076057,
      "grad_norm": 2.596733570098877,
      "learning_rate": 1.538986584176726e-05,
      "loss": 1.7483,
      "step": 14590
    },
    {
      "epoch": 1.4678027446840598,
      "grad_norm": 2.241575241088867,
      "learning_rate": 1.5379778756598636e-05,
      "loss": 1.6803,
      "step": 14600
    },
    {
      "epoch": 1.4688081234605137,
      "grad_norm": 2.3279170989990234,
      "learning_rate": 1.5369691671430015e-05,
      "loss": 1.7085,
      "step": 14610
    },
    {
      "epoch": 1.4698135022369678,
      "grad_norm": 2.638793468475342,
      "learning_rate": 1.5359604586261388e-05,
      "loss": 1.7713,
      "step": 14620
    },
    {
      "epoch": 1.4708188810134217,
      "grad_norm": 2.035865306854248,
      "learning_rate": 1.5349517501092768e-05,
      "loss": 1.7728,
      "step": 14630
    },
    {
      "epoch": 1.4718242597898759,
      "grad_norm": 1.9947842359542847,
      "learning_rate": 1.5339430415924144e-05,
      "loss": 1.8931,
      "step": 14640
    },
    {
      "epoch": 1.47282963856633,
      "grad_norm": 1.9842058420181274,
      "learning_rate": 1.5329343330755524e-05,
      "loss": 1.8151,
      "step": 14650
    },
    {
      "epoch": 1.4738350173427839,
      "grad_norm": 1.8180783987045288,
      "learning_rate": 1.53192562455869e-05,
      "loss": 1.7522,
      "step": 14660
    },
    {
      "epoch": 1.4748403961192378,
      "grad_norm": 2.305403470993042,
      "learning_rate": 1.530916916041828e-05,
      "loss": 1.7296,
      "step": 14670
    },
    {
      "epoch": 1.475845774895692,
      "grad_norm": 2.1370463371276855,
      "learning_rate": 1.5299082075249656e-05,
      "loss": 1.7295,
      "step": 14680
    },
    {
      "epoch": 1.476851153672146,
      "grad_norm": 2.2018051147460938,
      "learning_rate": 1.5288994990081033e-05,
      "loss": 1.7678,
      "step": 14690
    },
    {
      "epoch": 1.4778565324486,
      "grad_norm": 1.9135938882827759,
      "learning_rate": 1.527890790491241e-05,
      "loss": 1.7216,
      "step": 14700
    },
    {
      "epoch": 1.478861911225054,
      "grad_norm": 3.1601924896240234,
      "learning_rate": 1.526882081974379e-05,
      "loss": 1.7616,
      "step": 14710
    },
    {
      "epoch": 1.4798672900015082,
      "grad_norm": 1.6397910118103027,
      "learning_rate": 1.5258733734575167e-05,
      "loss": 1.7605,
      "step": 14720
    },
    {
      "epoch": 1.480872668777962,
      "grad_norm": 2.2982256412506104,
      "learning_rate": 1.5248646649406545e-05,
      "loss": 1.76,
      "step": 14730
    },
    {
      "epoch": 1.481878047554416,
      "grad_norm": 2.170193910598755,
      "learning_rate": 1.5238559564237923e-05,
      "loss": 1.8035,
      "step": 14740
    },
    {
      "epoch": 1.4828834263308701,
      "grad_norm": 1.9665768146514893,
      "learning_rate": 1.5228472479069297e-05,
      "loss": 1.7558,
      "step": 14750
    },
    {
      "epoch": 1.4838888051073242,
      "grad_norm": 2.3685243129730225,
      "learning_rate": 1.5218385393900675e-05,
      "loss": 1.7636,
      "step": 14760
    },
    {
      "epoch": 1.4848941838837781,
      "grad_norm": 2.2523436546325684,
      "learning_rate": 1.5208298308732053e-05,
      "loss": 1.8148,
      "step": 14770
    },
    {
      "epoch": 1.4858995626602323,
      "grad_norm": 2.039876937866211,
      "learning_rate": 1.5198211223563431e-05,
      "loss": 1.6737,
      "step": 14780
    },
    {
      "epoch": 1.4869049414366864,
      "grad_norm": 1.8960906267166138,
      "learning_rate": 1.518812413839481e-05,
      "loss": 1.8115,
      "step": 14790
    },
    {
      "epoch": 1.4879103202131403,
      "grad_norm": 2.307302236557007,
      "learning_rate": 1.5178037053226187e-05,
      "loss": 1.8358,
      "step": 14800
    },
    {
      "epoch": 1.4889156989895942,
      "grad_norm": 1.6459988355636597,
      "learning_rate": 1.5167949968057564e-05,
      "loss": 1.7982,
      "step": 14810
    },
    {
      "epoch": 1.4899210777660483,
      "grad_norm": 1.829891324043274,
      "learning_rate": 1.5157862882888942e-05,
      "loss": 1.6828,
      "step": 14820
    },
    {
      "epoch": 1.4909264565425024,
      "grad_norm": 1.7935705184936523,
      "learning_rate": 1.514777579772032e-05,
      "loss": 1.8078,
      "step": 14830
    },
    {
      "epoch": 1.4919318353189563,
      "grad_norm": 1.8450056314468384,
      "learning_rate": 1.5137688712551698e-05,
      "loss": 1.7936,
      "step": 14840
    },
    {
      "epoch": 1.4929372140954105,
      "grad_norm": 2.776939630508423,
      "learning_rate": 1.5127601627383076e-05,
      "loss": 1.7823,
      "step": 14850
    },
    {
      "epoch": 1.4939425928718646,
      "grad_norm": 2.085806131362915,
      "learning_rate": 1.5117514542214454e-05,
      "loss": 1.8058,
      "step": 14860
    },
    {
      "epoch": 1.4949479716483185,
      "grad_norm": 2.1308672428131104,
      "learning_rate": 1.5107427457045828e-05,
      "loss": 1.774,
      "step": 14870
    },
    {
      "epoch": 1.4959533504247724,
      "grad_norm": 1.9952815771102905,
      "learning_rate": 1.5097340371877206e-05,
      "loss": 1.7039,
      "step": 14880
    },
    {
      "epoch": 1.4969587292012265,
      "grad_norm": 1.940025806427002,
      "learning_rate": 1.5087253286708584e-05,
      "loss": 1.7941,
      "step": 14890
    },
    {
      "epoch": 1.4979641079776806,
      "grad_norm": 2.124074697494507,
      "learning_rate": 1.5077166201539962e-05,
      "loss": 1.749,
      "step": 14900
    },
    {
      "epoch": 1.4989694867541346,
      "grad_norm": 2.0220730304718018,
      "learning_rate": 1.506707911637134e-05,
      "loss": 1.8187,
      "step": 14910
    },
    {
      "epoch": 1.4999748655305887,
      "grad_norm": 2.0160069465637207,
      "learning_rate": 1.5056992031202718e-05,
      "loss": 1.8249,
      "step": 14920
    },
    {
      "epoch": 1.5009802443070428,
      "grad_norm": 2.0468709468841553,
      "learning_rate": 1.5046904946034093e-05,
      "loss": 1.7765,
      "step": 14930
    },
    {
      "epoch": 1.5019856230834967,
      "grad_norm": 1.511995792388916,
      "learning_rate": 1.5036817860865471e-05,
      "loss": 1.8107,
      "step": 14940
    },
    {
      "epoch": 1.5029910018599506,
      "grad_norm": 2.1980502605438232,
      "learning_rate": 1.5026730775696849e-05,
      "loss": 1.8255,
      "step": 14950
    },
    {
      "epoch": 1.5039963806364047,
      "grad_norm": 2.1295230388641357,
      "learning_rate": 1.5016643690528229e-05,
      "loss": 1.7537,
      "step": 14960
    },
    {
      "epoch": 1.5050017594128589,
      "grad_norm": 1.5653012990951538,
      "learning_rate": 1.5006556605359607e-05,
      "loss": 1.7182,
      "step": 14970
    },
    {
      "epoch": 1.5060071381893128,
      "grad_norm": 1.831078052520752,
      "learning_rate": 1.4996469520190983e-05,
      "loss": 1.7928,
      "step": 14980
    },
    {
      "epoch": 1.5070125169657669,
      "grad_norm": 2.0547499656677246,
      "learning_rate": 1.4986382435022361e-05,
      "loss": 1.7354,
      "step": 14990
    },
    {
      "epoch": 1.508017895742221,
      "grad_norm": 2.238276243209839,
      "learning_rate": 1.4976295349853737e-05,
      "loss": 1.7555,
      "step": 15000
    },
    {
      "epoch": 1.509023274518675,
      "grad_norm": 2.3976292610168457,
      "learning_rate": 1.4966208264685115e-05,
      "loss": 1.7879,
      "step": 15010
    },
    {
      "epoch": 1.5100286532951288,
      "grad_norm": 1.630341649055481,
      "learning_rate": 1.4956121179516493e-05,
      "loss": 1.7711,
      "step": 15020
    },
    {
      "epoch": 1.511034032071583,
      "grad_norm": 2.096127510070801,
      "learning_rate": 1.494603409434787e-05,
      "loss": 1.732,
      "step": 15030
    },
    {
      "epoch": 1.512039410848037,
      "grad_norm": 2.2026703357696533,
      "learning_rate": 1.4935947009179248e-05,
      "loss": 1.8138,
      "step": 15040
    },
    {
      "epoch": 1.513044789624491,
      "grad_norm": 1.8783890008926392,
      "learning_rate": 1.4925859924010626e-05,
      "loss": 1.8269,
      "step": 15050
    },
    {
      "epoch": 1.514050168400945,
      "grad_norm": 2.347686767578125,
      "learning_rate": 1.4915772838842002e-05,
      "loss": 1.7411,
      "step": 15060
    },
    {
      "epoch": 1.5150555471773992,
      "grad_norm": 1.7831441164016724,
      "learning_rate": 1.490568575367338e-05,
      "loss": 1.7834,
      "step": 15070
    },
    {
      "epoch": 1.5160609259538531,
      "grad_norm": 2.0140490531921387,
      "learning_rate": 1.4895598668504758e-05,
      "loss": 1.8193,
      "step": 15080
    },
    {
      "epoch": 1.517066304730307,
      "grad_norm": 2.074958324432373,
      "learning_rate": 1.4885511583336134e-05,
      "loss": 1.7136,
      "step": 15090
    },
    {
      "epoch": 1.5180716835067611,
      "grad_norm": 2.4204158782958984,
      "learning_rate": 1.4875424498167512e-05,
      "loss": 1.7234,
      "step": 15100
    },
    {
      "epoch": 1.5190770622832153,
      "grad_norm": 1.5286544561386108,
      "learning_rate": 1.4865337412998892e-05,
      "loss": 1.7465,
      "step": 15110
    },
    {
      "epoch": 1.5200824410596692,
      "grad_norm": 1.6452876329421997,
      "learning_rate": 1.4855250327830268e-05,
      "loss": 1.7747,
      "step": 15120
    },
    {
      "epoch": 1.5210878198361233,
      "grad_norm": 1.878740906715393,
      "learning_rate": 1.4845163242661646e-05,
      "loss": 1.6911,
      "step": 15130
    },
    {
      "epoch": 1.5220931986125774,
      "grad_norm": 1.802652359008789,
      "learning_rate": 1.4835076157493024e-05,
      "loss": 1.7756,
      "step": 15140
    },
    {
      "epoch": 1.5230985773890313,
      "grad_norm": 1.6921448707580566,
      "learning_rate": 1.48249890723244e-05,
      "loss": 1.7584,
      "step": 15150
    },
    {
      "epoch": 1.5241039561654852,
      "grad_norm": 2.6994807720184326,
      "learning_rate": 1.4814901987155779e-05,
      "loss": 1.7821,
      "step": 15160
    },
    {
      "epoch": 1.5251093349419393,
      "grad_norm": 1.7913087606430054,
      "learning_rate": 1.4804814901987157e-05,
      "loss": 1.7641,
      "step": 15170
    },
    {
      "epoch": 1.5261147137183935,
      "grad_norm": 1.755441665649414,
      "learning_rate": 1.4794727816818533e-05,
      "loss": 1.8419,
      "step": 15180
    },
    {
      "epoch": 1.5271200924948474,
      "grad_norm": 1.7874586582183838,
      "learning_rate": 1.4784640731649911e-05,
      "loss": 1.84,
      "step": 15190
    },
    {
      "epoch": 1.5281254712713015,
      "grad_norm": 2.4183342456817627,
      "learning_rate": 1.4774553646481289e-05,
      "loss": 1.7924,
      "step": 15200
    },
    {
      "epoch": 1.5291308500477556,
      "grad_norm": 2.001824140548706,
      "learning_rate": 1.4764466561312665e-05,
      "loss": 1.7703,
      "step": 15210
    },
    {
      "epoch": 1.5301362288242095,
      "grad_norm": 1.6505087614059448,
      "learning_rate": 1.4754379476144043e-05,
      "loss": 1.7683,
      "step": 15220
    },
    {
      "epoch": 1.5311416076006634,
      "grad_norm": 2.535264492034912,
      "learning_rate": 1.4744292390975421e-05,
      "loss": 1.7309,
      "step": 15230
    },
    {
      "epoch": 1.5321469863771175,
      "grad_norm": 2.310784339904785,
      "learning_rate": 1.4734205305806798e-05,
      "loss": 1.8251,
      "step": 15240
    },
    {
      "epoch": 1.5331523651535717,
      "grad_norm": 1.7496311664581299,
      "learning_rate": 1.4724118220638176e-05,
      "loss": 1.6694,
      "step": 15250
    },
    {
      "epoch": 1.5341577439300256,
      "grad_norm": 2.776393413543701,
      "learning_rate": 1.4714031135469555e-05,
      "loss": 1.7347,
      "step": 15260
    },
    {
      "epoch": 1.5351631227064797,
      "grad_norm": 1.8477075099945068,
      "learning_rate": 1.4703944050300932e-05,
      "loss": 1.7471,
      "step": 15270
    },
    {
      "epoch": 1.5361685014829338,
      "grad_norm": 2.612471342086792,
      "learning_rate": 1.469385696513231e-05,
      "loss": 1.7461,
      "step": 15280
    },
    {
      "epoch": 1.5371738802593877,
      "grad_norm": 2.6396307945251465,
      "learning_rate": 1.4683769879963688e-05,
      "loss": 1.7513,
      "step": 15290
    },
    {
      "epoch": 1.5381792590358416,
      "grad_norm": 2.290071964263916,
      "learning_rate": 1.4673682794795066e-05,
      "loss": 1.772,
      "step": 15300
    },
    {
      "epoch": 1.5391846378122958,
      "grad_norm": 1.926568865776062,
      "learning_rate": 1.4663595709626442e-05,
      "loss": 1.7857,
      "step": 15310
    },
    {
      "epoch": 1.5401900165887499,
      "grad_norm": 2.4126169681549072,
      "learning_rate": 1.465350862445782e-05,
      "loss": 1.7624,
      "step": 15320
    },
    {
      "epoch": 1.5411953953652038,
      "grad_norm": 2.597407341003418,
      "learning_rate": 1.4643421539289198e-05,
      "loss": 1.8112,
      "step": 15330
    },
    {
      "epoch": 1.542200774141658,
      "grad_norm": 2.9229624271392822,
      "learning_rate": 1.4634343162637437e-05,
      "loss": 1.8033,
      "step": 15340
    },
    {
      "epoch": 1.543206152918112,
      "grad_norm": 1.6931827068328857,
      "learning_rate": 1.4624256077468815e-05,
      "loss": 1.7761,
      "step": 15350
    },
    {
      "epoch": 1.544211531694566,
      "grad_norm": 2.204486608505249,
      "learning_rate": 1.4614168992300191e-05,
      "loss": 1.7639,
      "step": 15360
    },
    {
      "epoch": 1.5452169104710198,
      "grad_norm": 1.8999789953231812,
      "learning_rate": 1.4604081907131569e-05,
      "loss": 1.8317,
      "step": 15370
    },
    {
      "epoch": 1.546222289247474,
      "grad_norm": 1.6930898427963257,
      "learning_rate": 1.4593994821962947e-05,
      "loss": 1.7348,
      "step": 15380
    },
    {
      "epoch": 1.547227668023928,
      "grad_norm": 1.9179986715316772,
      "learning_rate": 1.4583907736794323e-05,
      "loss": 1.825,
      "step": 15390
    },
    {
      "epoch": 1.548233046800382,
      "grad_norm": 1.6990270614624023,
      "learning_rate": 1.4573820651625703e-05,
      "loss": 1.7876,
      "step": 15400
    },
    {
      "epoch": 1.549238425576836,
      "grad_norm": 1.525109052658081,
      "learning_rate": 1.4563733566457081e-05,
      "loss": 1.7897,
      "step": 15410
    },
    {
      "epoch": 1.5502438043532902,
      "grad_norm": 2.0249741077423096,
      "learning_rate": 1.4553646481288457e-05,
      "loss": 1.7969,
      "step": 15420
    },
    {
      "epoch": 1.5512491831297441,
      "grad_norm": 1.9293185472488403,
      "learning_rate": 1.4543559396119835e-05,
      "loss": 1.7158,
      "step": 15430
    },
    {
      "epoch": 1.552254561906198,
      "grad_norm": 2.566296100616455,
      "learning_rate": 1.4533472310951213e-05,
      "loss": 1.7705,
      "step": 15440
    },
    {
      "epoch": 1.5532599406826522,
      "grad_norm": 2.1092898845672607,
      "learning_rate": 1.452338522578259e-05,
      "loss": 1.7924,
      "step": 15450
    },
    {
      "epoch": 1.5542653194591063,
      "grad_norm": 1.6731383800506592,
      "learning_rate": 1.4513298140613968e-05,
      "loss": 1.7742,
      "step": 15460
    },
    {
      "epoch": 1.5552706982355602,
      "grad_norm": 1.9832167625427246,
      "learning_rate": 1.4503211055445346e-05,
      "loss": 1.8017,
      "step": 15470
    },
    {
      "epoch": 1.5562760770120143,
      "grad_norm": 2.2289440631866455,
      "learning_rate": 1.4493123970276722e-05,
      "loss": 1.7226,
      "step": 15480
    },
    {
      "epoch": 1.5572814557884684,
      "grad_norm": 2.343344211578369,
      "learning_rate": 1.44830368851081e-05,
      "loss": 1.715,
      "step": 15490
    },
    {
      "epoch": 1.5582868345649223,
      "grad_norm": 2.2160308361053467,
      "learning_rate": 1.4472949799939478e-05,
      "loss": 1.8578,
      "step": 15500
    },
    {
      "epoch": 1.5592922133413762,
      "grad_norm": 1.7891690731048584,
      "learning_rate": 1.4462862714770854e-05,
      "loss": 1.6829,
      "step": 15510
    },
    {
      "epoch": 1.5602975921178304,
      "grad_norm": 2.1168291568756104,
      "learning_rate": 1.4452775629602232e-05,
      "loss": 1.8211,
      "step": 15520
    },
    {
      "epoch": 1.5613029708942845,
      "grad_norm": 1.749712347984314,
      "learning_rate": 1.444268854443361e-05,
      "loss": 1.7572,
      "step": 15530
    },
    {
      "epoch": 1.5623083496707384,
      "grad_norm": 1.5249195098876953,
      "learning_rate": 1.4432601459264987e-05,
      "loss": 1.8028,
      "step": 15540
    },
    {
      "epoch": 1.5633137284471925,
      "grad_norm": 1.918365240097046,
      "learning_rate": 1.4422514374096366e-05,
      "loss": 1.8169,
      "step": 15550
    },
    {
      "epoch": 1.5643191072236466,
      "grad_norm": 1.8071701526641846,
      "learning_rate": 1.4412427288927744e-05,
      "loss": 1.7604,
      "step": 15560
    },
    {
      "epoch": 1.5653244860001005,
      "grad_norm": 3.387070655822754,
      "learning_rate": 1.440234020375912e-05,
      "loss": 1.7358,
      "step": 15570
    },
    {
      "epoch": 1.5663298647765544,
      "grad_norm": 1.9863581657409668,
      "learning_rate": 1.4392253118590499e-05,
      "loss": 1.7952,
      "step": 15580
    },
    {
      "epoch": 1.5673352435530086,
      "grad_norm": 2.2681260108947754,
      "learning_rate": 1.4382166033421877e-05,
      "loss": 1.7926,
      "step": 15590
    },
    {
      "epoch": 1.5683406223294627,
      "grad_norm": 1.5874192714691162,
      "learning_rate": 1.4372078948253253e-05,
      "loss": 1.7334,
      "step": 15600
    },
    {
      "epoch": 1.5693460011059166,
      "grad_norm": 1.7024706602096558,
      "learning_rate": 1.4361991863084631e-05,
      "loss": 1.7971,
      "step": 15610
    },
    {
      "epoch": 1.5703513798823707,
      "grad_norm": 2.6533586978912354,
      "learning_rate": 1.4351904777916009e-05,
      "loss": 1.7676,
      "step": 15620
    },
    {
      "epoch": 1.5713567586588248,
      "grad_norm": 2.0092296600341797,
      "learning_rate": 1.4341817692747385e-05,
      "loss": 1.7532,
      "step": 15630
    },
    {
      "epoch": 1.5723621374352788,
      "grad_norm": 1.958570122718811,
      "learning_rate": 1.4331730607578763e-05,
      "loss": 1.6984,
      "step": 15640
    },
    {
      "epoch": 1.5733675162117327,
      "grad_norm": 2.161742687225342,
      "learning_rate": 1.4321643522410141e-05,
      "loss": 1.7802,
      "step": 15650
    },
    {
      "epoch": 1.5743728949881868,
      "grad_norm": 1.7930704355239868,
      "learning_rate": 1.4311556437241518e-05,
      "loss": 1.7801,
      "step": 15660
    },
    {
      "epoch": 1.575378273764641,
      "grad_norm": 1.935646891593933,
      "learning_rate": 1.4301469352072896e-05,
      "loss": 1.6863,
      "step": 15670
    },
    {
      "epoch": 1.5763836525410948,
      "grad_norm": 1.7401868104934692,
      "learning_rate": 1.4291382266904274e-05,
      "loss": 1.806,
      "step": 15680
    },
    {
      "epoch": 1.577389031317549,
      "grad_norm": 1.624564528465271,
      "learning_rate": 1.428129518173565e-05,
      "loss": 1.7005,
      "step": 15690
    },
    {
      "epoch": 1.578394410094003,
      "grad_norm": 2.052250862121582,
      "learning_rate": 1.427120809656703e-05,
      "loss": 1.8154,
      "step": 15700
    },
    {
      "epoch": 1.579399788870457,
      "grad_norm": 1.848488688468933,
      "learning_rate": 1.4261121011398408e-05,
      "loss": 1.7217,
      "step": 15710
    },
    {
      "epoch": 1.5804051676469109,
      "grad_norm": 2.398432493209839,
      "learning_rate": 1.4251033926229784e-05,
      "loss": 1.7254,
      "step": 15720
    },
    {
      "epoch": 1.581410546423365,
      "grad_norm": 1.8199951648712158,
      "learning_rate": 1.4240946841061162e-05,
      "loss": 1.8375,
      "step": 15730
    },
    {
      "epoch": 1.582415925199819,
      "grad_norm": 2.082034111022949,
      "learning_rate": 1.423085975589254e-05,
      "loss": 1.8348,
      "step": 15740
    },
    {
      "epoch": 1.583421303976273,
      "grad_norm": 1.6256139278411865,
      "learning_rate": 1.4220772670723916e-05,
      "loss": 1.8076,
      "step": 15750
    },
    {
      "epoch": 1.5844266827527271,
      "grad_norm": 2.1617705821990967,
      "learning_rate": 1.4210685585555294e-05,
      "loss": 1.6935,
      "step": 15760
    },
    {
      "epoch": 1.5854320615291813,
      "grad_norm": 1.9025375843048096,
      "learning_rate": 1.4200598500386672e-05,
      "loss": 1.7517,
      "step": 15770
    },
    {
      "epoch": 1.5864374403056352,
      "grad_norm": 2.1931300163269043,
      "learning_rate": 1.4190511415218049e-05,
      "loss": 1.8129,
      "step": 15780
    },
    {
      "epoch": 1.587442819082089,
      "grad_norm": 1.835216760635376,
      "learning_rate": 1.4180424330049427e-05,
      "loss": 1.7569,
      "step": 15790
    },
    {
      "epoch": 1.5884481978585432,
      "grad_norm": 2.9753360748291016,
      "learning_rate": 1.4170337244880805e-05,
      "loss": 1.7937,
      "step": 15800
    },
    {
      "epoch": 1.5894535766349973,
      "grad_norm": 2.5367777347564697,
      "learning_rate": 1.4160250159712181e-05,
      "loss": 1.7855,
      "step": 15810
    },
    {
      "epoch": 1.5904589554114512,
      "grad_norm": 1.7016615867614746,
      "learning_rate": 1.4150163074543559e-05,
      "loss": 1.7673,
      "step": 15820
    },
    {
      "epoch": 1.5914643341879053,
      "grad_norm": 2.4306161403656006,
      "learning_rate": 1.4140075989374937e-05,
      "loss": 1.7543,
      "step": 15830
    },
    {
      "epoch": 1.5924697129643595,
      "grad_norm": 2.0667076110839844,
      "learning_rate": 1.4129988904206313e-05,
      "loss": 1.7445,
      "step": 15840
    },
    {
      "epoch": 1.5934750917408134,
      "grad_norm": 2.142411231994629,
      "learning_rate": 1.4119901819037693e-05,
      "loss": 1.7625,
      "step": 15850
    },
    {
      "epoch": 1.5944804705172673,
      "grad_norm": 2.181635618209839,
      "learning_rate": 1.4109814733869071e-05,
      "loss": 1.8151,
      "step": 15860
    },
    {
      "epoch": 1.5954858492937214,
      "grad_norm": 2.2074334621429443,
      "learning_rate": 1.4099727648700447e-05,
      "loss": 1.7602,
      "step": 15870
    },
    {
      "epoch": 1.5964912280701755,
      "grad_norm": 2.080932140350342,
      "learning_rate": 1.4089640563531825e-05,
      "loss": 1.828,
      "step": 15880
    },
    {
      "epoch": 1.5974966068466294,
      "grad_norm": 2.358135461807251,
      "learning_rate": 1.4079553478363203e-05,
      "loss": 1.7048,
      "step": 15890
    },
    {
      "epoch": 1.5985019856230835,
      "grad_norm": 1.8244625329971313,
      "learning_rate": 1.4069466393194581e-05,
      "loss": 1.7769,
      "step": 15900
    },
    {
      "epoch": 1.5995073643995377,
      "grad_norm": 2.526425838470459,
      "learning_rate": 1.4059379308025958e-05,
      "loss": 1.7323,
      "step": 15910
    },
    {
      "epoch": 1.6005127431759916,
      "grad_norm": 1.8403575420379639,
      "learning_rate": 1.4049292222857336e-05,
      "loss": 1.7493,
      "step": 15920
    },
    {
      "epoch": 1.6015181219524455,
      "grad_norm": 2.3434791564941406,
      "learning_rate": 1.4039205137688714e-05,
      "loss": 1.8001,
      "step": 15930
    },
    {
      "epoch": 1.6025235007288996,
      "grad_norm": 2.7045419216156006,
      "learning_rate": 1.402911805252009e-05,
      "loss": 1.7744,
      "step": 15940
    },
    {
      "epoch": 1.6035288795053537,
      "grad_norm": 2.215778350830078,
      "learning_rate": 1.4019030967351468e-05,
      "loss": 1.7382,
      "step": 15950
    },
    {
      "epoch": 1.6045342582818076,
      "grad_norm": 2.2393712997436523,
      "learning_rate": 1.4008943882182846e-05,
      "loss": 1.8076,
      "step": 15960
    },
    {
      "epoch": 1.6055396370582617,
      "grad_norm": 2.0663726329803467,
      "learning_rate": 1.3998856797014222e-05,
      "loss": 1.8048,
      "step": 15970
    },
    {
      "epoch": 1.6065450158347159,
      "grad_norm": 1.8614095449447632,
      "learning_rate": 1.39887697118456e-05,
      "loss": 1.8256,
      "step": 15980
    },
    {
      "epoch": 1.6075503946111698,
      "grad_norm": 2.410991668701172,
      "learning_rate": 1.3978682626676978e-05,
      "loss": 1.7259,
      "step": 15990
    },
    {
      "epoch": 1.6085557733876237,
      "grad_norm": 2.2524447441101074,
      "learning_rate": 1.3968595541508355e-05,
      "loss": 1.6895,
      "step": 16000
    },
    {
      "epoch": 1.6095611521640778,
      "grad_norm": 1.7224773168563843,
      "learning_rate": 1.3958508456339734e-05,
      "loss": 1.8531,
      "step": 16010
    },
    {
      "epoch": 1.610566530940532,
      "grad_norm": 1.7608270645141602,
      "learning_rate": 1.3948421371171112e-05,
      "loss": 1.8181,
      "step": 16020
    },
    {
      "epoch": 1.6115719097169858,
      "grad_norm": 1.945777416229248,
      "learning_rate": 1.3938334286002489e-05,
      "loss": 1.8079,
      "step": 16030
    },
    {
      "epoch": 1.61257728849344,
      "grad_norm": 1.7517660856246948,
      "learning_rate": 1.3928247200833867e-05,
      "loss": 1.8039,
      "step": 16040
    },
    {
      "epoch": 1.613582667269894,
      "grad_norm": 2.208327293395996,
      "learning_rate": 1.3918160115665245e-05,
      "loss": 1.744,
      "step": 16050
    },
    {
      "epoch": 1.614588046046348,
      "grad_norm": 1.9574003219604492,
      "learning_rate": 1.3908073030496621e-05,
      "loss": 1.6951,
      "step": 16060
    },
    {
      "epoch": 1.6155934248228019,
      "grad_norm": 2.0216481685638428,
      "learning_rate": 1.3897985945327999e-05,
      "loss": 1.7789,
      "step": 16070
    },
    {
      "epoch": 1.616598803599256,
      "grad_norm": 1.6288104057312012,
      "learning_rate": 1.3887898860159377e-05,
      "loss": 1.7393,
      "step": 16080
    },
    {
      "epoch": 1.6176041823757101,
      "grad_norm": 2.4291625022888184,
      "learning_rate": 1.3877811774990753e-05,
      "loss": 1.7338,
      "step": 16090
    },
    {
      "epoch": 1.618609561152164,
      "grad_norm": 1.9969784021377563,
      "learning_rate": 1.3867724689822131e-05,
      "loss": 1.8008,
      "step": 16100
    },
    {
      "epoch": 1.6196149399286182,
      "grad_norm": 2.0089282989501953,
      "learning_rate": 1.385763760465351e-05,
      "loss": 1.8071,
      "step": 16110
    },
    {
      "epoch": 1.6206203187050723,
      "grad_norm": 2.008530378341675,
      "learning_rate": 1.3847550519484886e-05,
      "loss": 1.8522,
      "step": 16120
    },
    {
      "epoch": 1.6216256974815262,
      "grad_norm": 1.9545814990997314,
      "learning_rate": 1.3837463434316264e-05,
      "loss": 1.751,
      "step": 16130
    },
    {
      "epoch": 1.62263107625798,
      "grad_norm": 1.6356467008590698,
      "learning_rate": 1.3827376349147642e-05,
      "loss": 1.7964,
      "step": 16140
    },
    {
      "epoch": 1.6236364550344342,
      "grad_norm": 2.235161304473877,
      "learning_rate": 1.3817289263979018e-05,
      "loss": 1.7538,
      "step": 16150
    },
    {
      "epoch": 1.6246418338108883,
      "grad_norm": 1.9356882572174072,
      "learning_rate": 1.3807202178810398e-05,
      "loss": 1.8044,
      "step": 16160
    },
    {
      "epoch": 1.6256472125873422,
      "grad_norm": 1.6485861539840698,
      "learning_rate": 1.3797115093641776e-05,
      "loss": 1.7398,
      "step": 16170
    },
    {
      "epoch": 1.6266525913637964,
      "grad_norm": 2.254971742630005,
      "learning_rate": 1.3787028008473152e-05,
      "loss": 1.7775,
      "step": 16180
    },
    {
      "epoch": 1.6276579701402505,
      "grad_norm": 1.5806540250778198,
      "learning_rate": 1.377694092330453e-05,
      "loss": 1.8074,
      "step": 16190
    },
    {
      "epoch": 1.6286633489167044,
      "grad_norm": 4.098565101623535,
      "learning_rate": 1.3766853838135908e-05,
      "loss": 1.7234,
      "step": 16200
    },
    {
      "epoch": 1.6296687276931583,
      "grad_norm": 1.8720338344573975,
      "learning_rate": 1.3756766752967284e-05,
      "loss": 1.6994,
      "step": 16210
    },
    {
      "epoch": 1.6306741064696124,
      "grad_norm": 1.9926146268844604,
      "learning_rate": 1.3746679667798662e-05,
      "loss": 1.8698,
      "step": 16220
    },
    {
      "epoch": 1.6316794852460665,
      "grad_norm": 2.69014310836792,
      "learning_rate": 1.373659258263004e-05,
      "loss": 1.8792,
      "step": 16230
    },
    {
      "epoch": 1.6326848640225204,
      "grad_norm": 1.6398987770080566,
      "learning_rate": 1.3726505497461417e-05,
      "loss": 1.7916,
      "step": 16240
    },
    {
      "epoch": 1.6336902427989746,
      "grad_norm": 2.283961534500122,
      "learning_rate": 1.3716418412292795e-05,
      "loss": 1.7473,
      "step": 16250
    },
    {
      "epoch": 1.6346956215754287,
      "grad_norm": 2.258171319961548,
      "learning_rate": 1.3706331327124173e-05,
      "loss": 1.7976,
      "step": 16260
    },
    {
      "epoch": 1.6357010003518826,
      "grad_norm": 2.5002408027648926,
      "learning_rate": 1.3696244241955549e-05,
      "loss": 1.7548,
      "step": 16270
    },
    {
      "epoch": 1.6367063791283365,
      "grad_norm": 2.2574381828308105,
      "learning_rate": 1.3686157156786927e-05,
      "loss": 1.8066,
      "step": 16280
    },
    {
      "epoch": 1.6377117579047906,
      "grad_norm": 2.593258857727051,
      "learning_rate": 1.3676070071618305e-05,
      "loss": 1.7848,
      "step": 16290
    },
    {
      "epoch": 1.6387171366812447,
      "grad_norm": 2.7183306217193604,
      "learning_rate": 1.3665982986449681e-05,
      "loss": 1.6862,
      "step": 16300
    },
    {
      "epoch": 1.6397225154576986,
      "grad_norm": 3.643519163131714,
      "learning_rate": 1.3655895901281061e-05,
      "loss": 1.8067,
      "step": 16310
    },
    {
      "epoch": 1.6407278942341526,
      "grad_norm": 2.6271698474884033,
      "learning_rate": 1.3645808816112439e-05,
      "loss": 1.7652,
      "step": 16320
    },
    {
      "epoch": 1.641733273010607,
      "grad_norm": 2.2700765132904053,
      "learning_rate": 1.3635721730943815e-05,
      "loss": 1.7347,
      "step": 16330
    },
    {
      "epoch": 1.6427386517870608,
      "grad_norm": 1.877632737159729,
      "learning_rate": 1.3625634645775193e-05,
      "loss": 1.6341,
      "step": 16340
    },
    {
      "epoch": 1.6437440305635147,
      "grad_norm": 2.08323335647583,
      "learning_rate": 1.3615547560606571e-05,
      "loss": 1.8504,
      "step": 16350
    },
    {
      "epoch": 1.6447494093399688,
      "grad_norm": 2.8244669437408447,
      "learning_rate": 1.3605460475437948e-05,
      "loss": 1.7651,
      "step": 16360
    },
    {
      "epoch": 1.645754788116423,
      "grad_norm": 1.8651692867279053,
      "learning_rate": 1.3595373390269326e-05,
      "loss": 1.7523,
      "step": 16370
    },
    {
      "epoch": 1.6467601668928769,
      "grad_norm": 1.9248584508895874,
      "learning_rate": 1.3585286305100704e-05,
      "loss": 1.9112,
      "step": 16380
    },
    {
      "epoch": 1.6477655456693308,
      "grad_norm": 1.6906325817108154,
      "learning_rate": 1.357519921993208e-05,
      "loss": 1.6971,
      "step": 16390
    },
    {
      "epoch": 1.648770924445785,
      "grad_norm": 2.9337198734283447,
      "learning_rate": 1.3565112134763458e-05,
      "loss": 1.7627,
      "step": 16400
    },
    {
      "epoch": 1.649776303222239,
      "grad_norm": 3.2550442218780518,
      "learning_rate": 1.3555025049594836e-05,
      "loss": 1.7848,
      "step": 16410
    },
    {
      "epoch": 1.650781681998693,
      "grad_norm": 1.7871460914611816,
      "learning_rate": 1.3544937964426212e-05,
      "loss": 1.7472,
      "step": 16420
    },
    {
      "epoch": 1.651787060775147,
      "grad_norm": 2.3543753623962402,
      "learning_rate": 1.353485087925759e-05,
      "loss": 1.8195,
      "step": 16430
    },
    {
      "epoch": 1.6527924395516012,
      "grad_norm": 2.0135557651519775,
      "learning_rate": 1.3524763794088968e-05,
      "loss": 1.7911,
      "step": 16440
    },
    {
      "epoch": 1.653797818328055,
      "grad_norm": 3.1840689182281494,
      "learning_rate": 1.3514676708920345e-05,
      "loss": 1.7833,
      "step": 16450
    },
    {
      "epoch": 1.654803197104509,
      "grad_norm": 1.866120457649231,
      "learning_rate": 1.3504589623751724e-05,
      "loss": 1.7904,
      "step": 16460
    },
    {
      "epoch": 1.6558085758809633,
      "grad_norm": 2.0066442489624023,
      "learning_rate": 1.3494502538583102e-05,
      "loss": 1.852,
      "step": 16470
    },
    {
      "epoch": 1.6568139546574172,
      "grad_norm": 3.336463451385498,
      "learning_rate": 1.3484415453414479e-05,
      "loss": 1.7101,
      "step": 16480
    },
    {
      "epoch": 1.6578193334338711,
      "grad_norm": 1.9455209970474243,
      "learning_rate": 1.3474328368245857e-05,
      "loss": 1.7217,
      "step": 16490
    },
    {
      "epoch": 1.6588247122103252,
      "grad_norm": 2.2633285522460938,
      "learning_rate": 1.3464241283077235e-05,
      "loss": 1.7349,
      "step": 16500
    },
    {
      "epoch": 1.6598300909867794,
      "grad_norm": 1.7669402360916138,
      "learning_rate": 1.3454154197908611e-05,
      "loss": 1.8118,
      "step": 16510
    },
    {
      "epoch": 1.6608354697632333,
      "grad_norm": 1.8668408393859863,
      "learning_rate": 1.3444067112739989e-05,
      "loss": 1.7989,
      "step": 16520
    },
    {
      "epoch": 1.6618408485396872,
      "grad_norm": 1.960026741027832,
      "learning_rate": 1.3433980027571367e-05,
      "loss": 1.7076,
      "step": 16530
    },
    {
      "epoch": 1.6628462273161415,
      "grad_norm": 2.489645004272461,
      "learning_rate": 1.3423892942402743e-05,
      "loss": 1.8758,
      "step": 16540
    },
    {
      "epoch": 1.6638516060925954,
      "grad_norm": 2.407989740371704,
      "learning_rate": 1.3413805857234121e-05,
      "loss": 1.7696,
      "step": 16550
    },
    {
      "epoch": 1.6648569848690493,
      "grad_norm": 2.719665765762329,
      "learning_rate": 1.34037187720655e-05,
      "loss": 1.7985,
      "step": 16560
    },
    {
      "epoch": 1.6658623636455034,
      "grad_norm": 1.9025678634643555,
      "learning_rate": 1.3393631686896876e-05,
      "loss": 1.7803,
      "step": 16570
    },
    {
      "epoch": 1.6668677424219576,
      "grad_norm": 2.315281867980957,
      "learning_rate": 1.3383544601728254e-05,
      "loss": 1.7253,
      "step": 16580
    },
    {
      "epoch": 1.6678731211984115,
      "grad_norm": 1.8087530136108398,
      "learning_rate": 1.3373457516559632e-05,
      "loss": 1.7889,
      "step": 16590
    },
    {
      "epoch": 1.6688784999748654,
      "grad_norm": 1.9063743352890015,
      "learning_rate": 1.3363370431391008e-05,
      "loss": 1.7555,
      "step": 16600
    },
    {
      "epoch": 1.6698838787513197,
      "grad_norm": 2.022878885269165,
      "learning_rate": 1.3353283346222388e-05,
      "loss": 1.7,
      "step": 16610
    },
    {
      "epoch": 1.6708892575277736,
      "grad_norm": 2.1962907314300537,
      "learning_rate": 1.3343196261053766e-05,
      "loss": 1.6656,
      "step": 16620
    },
    {
      "epoch": 1.6718946363042275,
      "grad_norm": 2.4802088737487793,
      "learning_rate": 1.3333109175885142e-05,
      "loss": 1.8068,
      "step": 16630
    },
    {
      "epoch": 1.6729000150806816,
      "grad_norm": 2.5448226928710938,
      "learning_rate": 1.332302209071652e-05,
      "loss": 1.7585,
      "step": 16640
    },
    {
      "epoch": 1.6739053938571358,
      "grad_norm": 1.839407205581665,
      "learning_rate": 1.3312935005547898e-05,
      "loss": 1.7545,
      "step": 16650
    },
    {
      "epoch": 1.6749107726335897,
      "grad_norm": 2.3256258964538574,
      "learning_rate": 1.3302847920379274e-05,
      "loss": 1.7408,
      "step": 16660
    },
    {
      "epoch": 1.6759161514100436,
      "grad_norm": 2.1519532203674316,
      "learning_rate": 1.3292760835210652e-05,
      "loss": 1.7327,
      "step": 16670
    },
    {
      "epoch": 1.676921530186498,
      "grad_norm": 1.8563086986541748,
      "learning_rate": 1.328267375004203e-05,
      "loss": 1.784,
      "step": 16680
    },
    {
      "epoch": 1.6779269089629518,
      "grad_norm": 2.2265570163726807,
      "learning_rate": 1.3272586664873407e-05,
      "loss": 1.8049,
      "step": 16690
    },
    {
      "epoch": 1.6789322877394057,
      "grad_norm": 2.0471999645233154,
      "learning_rate": 1.3262499579704785e-05,
      "loss": 1.7892,
      "step": 16700
    },
    {
      "epoch": 1.6799376665158599,
      "grad_norm": 1.9472640752792358,
      "learning_rate": 1.3252412494536163e-05,
      "loss": 1.777,
      "step": 16710
    },
    {
      "epoch": 1.680943045292314,
      "grad_norm": 1.780590534210205,
      "learning_rate": 1.3242325409367539e-05,
      "loss": 1.7334,
      "step": 16720
    },
    {
      "epoch": 1.6819484240687679,
      "grad_norm": 1.811577558517456,
      "learning_rate": 1.3232238324198917e-05,
      "loss": 1.7913,
      "step": 16730
    },
    {
      "epoch": 1.6829538028452218,
      "grad_norm": 1.8114941120147705,
      "learning_rate": 1.3222151239030295e-05,
      "loss": 1.7601,
      "step": 16740
    },
    {
      "epoch": 1.6839591816216761,
      "grad_norm": 2.046640157699585,
      "learning_rate": 1.3212064153861671e-05,
      "loss": 1.7653,
      "step": 16750
    },
    {
      "epoch": 1.68496456039813,
      "grad_norm": 1.7970484495162964,
      "learning_rate": 1.3201977068693051e-05,
      "loss": 1.7905,
      "step": 16760
    },
    {
      "epoch": 1.685969939174584,
      "grad_norm": 1.701686143875122,
      "learning_rate": 1.3191889983524429e-05,
      "loss": 1.699,
      "step": 16770
    },
    {
      "epoch": 1.686975317951038,
      "grad_norm": 2.152989387512207,
      "learning_rate": 1.3181802898355805e-05,
      "loss": 1.7667,
      "step": 16780
    },
    {
      "epoch": 1.6879806967274922,
      "grad_norm": 3.0227017402648926,
      "learning_rate": 1.3171715813187183e-05,
      "loss": 1.7165,
      "step": 16790
    },
    {
      "epoch": 1.688986075503946,
      "grad_norm": 2.7291600704193115,
      "learning_rate": 1.3161628728018561e-05,
      "loss": 1.7466,
      "step": 16800
    },
    {
      "epoch": 1.6899914542804,
      "grad_norm": 1.7200967073440552,
      "learning_rate": 1.3151541642849938e-05,
      "loss": 1.7606,
      "step": 16810
    },
    {
      "epoch": 1.6909968330568543,
      "grad_norm": 1.7022724151611328,
      "learning_rate": 1.3141454557681316e-05,
      "loss": 1.7628,
      "step": 16820
    },
    {
      "epoch": 1.6920022118333082,
      "grad_norm": 1.7184816598892212,
      "learning_rate": 1.3131367472512694e-05,
      "loss": 1.8426,
      "step": 16830
    },
    {
      "epoch": 1.6930075906097621,
      "grad_norm": 2.0437607765197754,
      "learning_rate": 1.312128038734407e-05,
      "loss": 1.8212,
      "step": 16840
    },
    {
      "epoch": 1.6940129693862163,
      "grad_norm": 1.6451265811920166,
      "learning_rate": 1.3111193302175448e-05,
      "loss": 1.7324,
      "step": 16850
    },
    {
      "epoch": 1.6950183481626704,
      "grad_norm": 1.776581883430481,
      "learning_rate": 1.3101106217006826e-05,
      "loss": 1.7852,
      "step": 16860
    },
    {
      "epoch": 1.6960237269391243,
      "grad_norm": 1.761877179145813,
      "learning_rate": 1.3091019131838202e-05,
      "loss": 1.8065,
      "step": 16870
    },
    {
      "epoch": 1.6970291057155782,
      "grad_norm": 2.5158820152282715,
      "learning_rate": 1.308093204666958e-05,
      "loss": 1.7271,
      "step": 16880
    },
    {
      "epoch": 1.6980344844920325,
      "grad_norm": 2.1297450065612793,
      "learning_rate": 1.3070844961500958e-05,
      "loss": 1.8463,
      "step": 16890
    },
    {
      "epoch": 1.6990398632684864,
      "grad_norm": 1.8579180240631104,
      "learning_rate": 1.3060757876332335e-05,
      "loss": 1.796,
      "step": 16900
    },
    {
      "epoch": 1.7000452420449403,
      "grad_norm": 1.8873622417449951,
      "learning_rate": 1.3050670791163713e-05,
      "loss": 1.7766,
      "step": 16910
    },
    {
      "epoch": 1.7010506208213945,
      "grad_norm": 2.4056732654571533,
      "learning_rate": 1.3040583705995092e-05,
      "loss": 1.8233,
      "step": 16920
    },
    {
      "epoch": 1.7020559995978486,
      "grad_norm": 1.7716022729873657,
      "learning_rate": 1.3030496620826469e-05,
      "loss": 1.7125,
      "step": 16930
    },
    {
      "epoch": 1.7030613783743025,
      "grad_norm": 2.169111728668213,
      "learning_rate": 1.3020409535657847e-05,
      "loss": 1.7081,
      "step": 16940
    },
    {
      "epoch": 1.7040667571507564,
      "grad_norm": 2.2215824127197266,
      "learning_rate": 1.3010322450489225e-05,
      "loss": 1.796,
      "step": 16950
    },
    {
      "epoch": 1.7050721359272107,
      "grad_norm": 1.7541640996932983,
      "learning_rate": 1.3000235365320601e-05,
      "loss": 1.7237,
      "step": 16960
    },
    {
      "epoch": 1.7060775147036646,
      "grad_norm": 2.023813009262085,
      "learning_rate": 1.2990148280151979e-05,
      "loss": 1.7324,
      "step": 16970
    },
    {
      "epoch": 1.7070828934801185,
      "grad_norm": 2.1455800533294678,
      "learning_rate": 1.2980061194983357e-05,
      "loss": 1.7579,
      "step": 16980
    },
    {
      "epoch": 1.7080882722565727,
      "grad_norm": 1.9567383527755737,
      "learning_rate": 1.2969974109814733e-05,
      "loss": 1.7812,
      "step": 16990
    },
    {
      "epoch": 1.7090936510330268,
      "grad_norm": 2.0603721141815186,
      "learning_rate": 1.2959887024646111e-05,
      "loss": 1.7954,
      "step": 17000
    },
    {
      "epoch": 1.7100990298094807,
      "grad_norm": 1.945662498474121,
      "learning_rate": 1.294979993947749e-05,
      "loss": 1.7529,
      "step": 17010
    },
    {
      "epoch": 1.7111044085859346,
      "grad_norm": 1.871134638786316,
      "learning_rate": 1.2939712854308866e-05,
      "loss": 1.8068,
      "step": 17020
    },
    {
      "epoch": 1.712109787362389,
      "grad_norm": 1.7856132984161377,
      "learning_rate": 1.2929625769140244e-05,
      "loss": 1.7485,
      "step": 17030
    },
    {
      "epoch": 1.7131151661388428,
      "grad_norm": 2.0285983085632324,
      "learning_rate": 1.2919538683971622e-05,
      "loss": 1.68,
      "step": 17040
    },
    {
      "epoch": 1.7141205449152968,
      "grad_norm": 2.019111156463623,
      "learning_rate": 1.2909451598802998e-05,
      "loss": 1.8258,
      "step": 17050
    },
    {
      "epoch": 1.7151259236917509,
      "grad_norm": 2.393078327178955,
      "learning_rate": 1.2899364513634376e-05,
      "loss": 1.7053,
      "step": 17060
    },
    {
      "epoch": 1.716131302468205,
      "grad_norm": 2.0878489017486572,
      "learning_rate": 1.2889277428465756e-05,
      "loss": 1.7441,
      "step": 17070
    },
    {
      "epoch": 1.717136681244659,
      "grad_norm": 2.1863961219787598,
      "learning_rate": 1.2879190343297132e-05,
      "loss": 1.6979,
      "step": 17080
    },
    {
      "epoch": 1.7181420600211128,
      "grad_norm": 1.8528939485549927,
      "learning_rate": 1.286910325812851e-05,
      "loss": 1.764,
      "step": 17090
    },
    {
      "epoch": 1.719147438797567,
      "grad_norm": 1.9219257831573486,
      "learning_rate": 1.2859016172959888e-05,
      "loss": 1.7651,
      "step": 17100
    },
    {
      "epoch": 1.720152817574021,
      "grad_norm": 2.4044623374938965,
      "learning_rate": 1.2848929087791266e-05,
      "loss": 1.6865,
      "step": 17110
    },
    {
      "epoch": 1.721158196350475,
      "grad_norm": 1.61761474609375,
      "learning_rate": 1.2838842002622642e-05,
      "loss": 1.8148,
      "step": 17120
    },
    {
      "epoch": 1.722163575126929,
      "grad_norm": 1.8457647562026978,
      "learning_rate": 1.282875491745402e-05,
      "loss": 1.722,
      "step": 17130
    },
    {
      "epoch": 1.7231689539033832,
      "grad_norm": 2.412083864212036,
      "learning_rate": 1.2818667832285398e-05,
      "loss": 1.7754,
      "step": 17140
    },
    {
      "epoch": 1.724174332679837,
      "grad_norm": 1.8610098361968994,
      "learning_rate": 1.2808580747116775e-05,
      "loss": 1.753,
      "step": 17150
    },
    {
      "epoch": 1.725179711456291,
      "grad_norm": 2.075812578201294,
      "learning_rate": 1.2798493661948153e-05,
      "loss": 1.7434,
      "step": 17160
    },
    {
      "epoch": 1.7261850902327451,
      "grad_norm": 2.0178487300872803,
      "learning_rate": 1.278840657677953e-05,
      "loss": 1.7913,
      "step": 17170
    },
    {
      "epoch": 1.7271904690091993,
      "grad_norm": 2.3821287155151367,
      "learning_rate": 1.2778319491610907e-05,
      "loss": 1.7583,
      "step": 17180
    },
    {
      "epoch": 1.7281958477856532,
      "grad_norm": 2.057530164718628,
      "learning_rate": 1.2768232406442285e-05,
      "loss": 1.7058,
      "step": 17190
    },
    {
      "epoch": 1.7292012265621073,
      "grad_norm": 2.8761379718780518,
      "learning_rate": 1.2758145321273663e-05,
      "loss": 1.7412,
      "step": 17200
    },
    {
      "epoch": 1.7302066053385614,
      "grad_norm": 2.892970085144043,
      "learning_rate": 1.274805823610504e-05,
      "loss": 1.803,
      "step": 17210
    },
    {
      "epoch": 1.7312119841150153,
      "grad_norm": 2.0436952114105225,
      "learning_rate": 1.2737971150936419e-05,
      "loss": 1.7949,
      "step": 17220
    },
    {
      "epoch": 1.7322173628914692,
      "grad_norm": 1.9317604303359985,
      "learning_rate": 1.2727884065767797e-05,
      "loss": 1.6749,
      "step": 17230
    },
    {
      "epoch": 1.7332227416679233,
      "grad_norm": 1.9610315561294556,
      "learning_rate": 1.2717796980599173e-05,
      "loss": 1.7678,
      "step": 17240
    },
    {
      "epoch": 1.7342281204443775,
      "grad_norm": 1.6807509660720825,
      "learning_rate": 1.2707709895430551e-05,
      "loss": 1.7541,
      "step": 17250
    },
    {
      "epoch": 1.7352334992208314,
      "grad_norm": 1.5924851894378662,
      "learning_rate": 1.269762281026193e-05,
      "loss": 1.7964,
      "step": 17260
    },
    {
      "epoch": 1.7362388779972855,
      "grad_norm": 2.3062336444854736,
      "learning_rate": 1.2687535725093306e-05,
      "loss": 1.7865,
      "step": 17270
    },
    {
      "epoch": 1.7372442567737396,
      "grad_norm": 2.204763174057007,
      "learning_rate": 1.2677448639924684e-05,
      "loss": 1.7653,
      "step": 17280
    },
    {
      "epoch": 1.7382496355501935,
      "grad_norm": 1.7516897916793823,
      "learning_rate": 1.2667361554756062e-05,
      "loss": 1.8025,
      "step": 17290
    },
    {
      "epoch": 1.7392550143266474,
      "grad_norm": 2.1064600944519043,
      "learning_rate": 1.2657274469587438e-05,
      "loss": 1.8299,
      "step": 17300
    },
    {
      "epoch": 1.7402603931031015,
      "grad_norm": 1.9896665811538696,
      "learning_rate": 1.2647187384418816e-05,
      "loss": 1.7423,
      "step": 17310
    },
    {
      "epoch": 1.7412657718795557,
      "grad_norm": 2.9894182682037354,
      "learning_rate": 1.2637100299250194e-05,
      "loss": 1.7717,
      "step": 17320
    },
    {
      "epoch": 1.7422711506560096,
      "grad_norm": 2.19156551361084,
      "learning_rate": 1.262701321408157e-05,
      "loss": 1.7976,
      "step": 17330
    },
    {
      "epoch": 1.7432765294324637,
      "grad_norm": 2.534385919570923,
      "learning_rate": 1.2616926128912948e-05,
      "loss": 1.7923,
      "step": 17340
    },
    {
      "epoch": 1.7442819082089178,
      "grad_norm": 1.8045969009399414,
      "learning_rate": 1.2606839043744326e-05,
      "loss": 1.8234,
      "step": 17350
    },
    {
      "epoch": 1.7452872869853717,
      "grad_norm": 2.324601650238037,
      "learning_rate": 1.2596751958575703e-05,
      "loss": 1.778,
      "step": 17360
    },
    {
      "epoch": 1.7462926657618256,
      "grad_norm": 1.8455201387405396,
      "learning_rate": 1.2586664873407082e-05,
      "loss": 1.7794,
      "step": 17370
    },
    {
      "epoch": 1.7472980445382797,
      "grad_norm": 2.1015090942382812,
      "learning_rate": 1.257657778823846e-05,
      "loss": 1.8553,
      "step": 17380
    },
    {
      "epoch": 1.7483034233147339,
      "grad_norm": 2.029061794281006,
      "learning_rate": 1.2566490703069837e-05,
      "loss": 1.8051,
      "step": 17390
    },
    {
      "epoch": 1.7493088020911878,
      "grad_norm": 2.418126106262207,
      "learning_rate": 1.2556403617901215e-05,
      "loss": 1.6658,
      "step": 17400
    },
    {
      "epoch": 1.750314180867642,
      "grad_norm": 2.956718683242798,
      "learning_rate": 1.2546316532732593e-05,
      "loss": 1.7547,
      "step": 17410
    },
    {
      "epoch": 1.751319559644096,
      "grad_norm": 2.3070638179779053,
      "learning_rate": 1.2536229447563969e-05,
      "loss": 1.8116,
      "step": 17420
    },
    {
      "epoch": 1.75232493842055,
      "grad_norm": 1.682600736618042,
      "learning_rate": 1.2526142362395347e-05,
      "loss": 1.6866,
      "step": 17430
    },
    {
      "epoch": 1.7533303171970038,
      "grad_norm": 1.9359899759292603,
      "learning_rate": 1.2516055277226725e-05,
      "loss": 1.7216,
      "step": 17440
    },
    {
      "epoch": 1.754335695973458,
      "grad_norm": 2.3292839527130127,
      "learning_rate": 1.2505968192058101e-05,
      "loss": 1.7664,
      "step": 17450
    },
    {
      "epoch": 1.755341074749912,
      "grad_norm": 1.56162691116333,
      "learning_rate": 1.249588110688948e-05,
      "loss": 1.774,
      "step": 17460
    },
    {
      "epoch": 1.756346453526366,
      "grad_norm": 1.801126480102539,
      "learning_rate": 1.2485794021720857e-05,
      "loss": 1.7093,
      "step": 17470
    },
    {
      "epoch": 1.75735183230282,
      "grad_norm": 2.015803337097168,
      "learning_rate": 1.2475706936552234e-05,
      "loss": 1.7396,
      "step": 17480
    },
    {
      "epoch": 1.7583572110792742,
      "grad_norm": 1.7454262971878052,
      "learning_rate": 1.2465619851383612e-05,
      "loss": 1.781,
      "step": 17490
    },
    {
      "epoch": 1.7593625898557281,
      "grad_norm": 1.9071329832077026,
      "learning_rate": 1.245553276621499e-05,
      "loss": 1.7554,
      "step": 17500
    },
    {
      "epoch": 1.760367968632182,
      "grad_norm": 1.591475248336792,
      "learning_rate": 1.2445445681046366e-05,
      "loss": 1.8479,
      "step": 17510
    },
    {
      "epoch": 1.7613733474086362,
      "grad_norm": 1.767370581626892,
      "learning_rate": 1.2435358595877746e-05,
      "loss": 1.7808,
      "step": 17520
    },
    {
      "epoch": 1.7623787261850903,
      "grad_norm": 2.861241340637207,
      "learning_rate": 1.2425271510709124e-05,
      "loss": 1.726,
      "step": 17530
    },
    {
      "epoch": 1.7633841049615442,
      "grad_norm": 1.7223981618881226,
      "learning_rate": 1.24151844255405e-05,
      "loss": 1.7381,
      "step": 17540
    },
    {
      "epoch": 1.7643894837379983,
      "grad_norm": 1.9924222230911255,
      "learning_rate": 1.2405097340371878e-05,
      "loss": 1.7395,
      "step": 17550
    },
    {
      "epoch": 1.7653948625144524,
      "grad_norm": 1.9704129695892334,
      "learning_rate": 1.2395010255203256e-05,
      "loss": 1.7927,
      "step": 17560
    },
    {
      "epoch": 1.7664002412909063,
      "grad_norm": 1.907914400100708,
      "learning_rate": 1.2384923170034632e-05,
      "loss": 1.7666,
      "step": 17570
    },
    {
      "epoch": 1.7674056200673602,
      "grad_norm": 2.1565494537353516,
      "learning_rate": 1.237483608486601e-05,
      "loss": 1.792,
      "step": 17580
    },
    {
      "epoch": 1.7684109988438144,
      "grad_norm": 2.393343448638916,
      "learning_rate": 1.2364748999697388e-05,
      "loss": 1.7842,
      "step": 17590
    },
    {
      "epoch": 1.7694163776202685,
      "grad_norm": 1.5019259452819824,
      "learning_rate": 1.2354661914528765e-05,
      "loss": 1.7246,
      "step": 17600
    },
    {
      "epoch": 1.7704217563967224,
      "grad_norm": 2.3094325065612793,
      "learning_rate": 1.2344574829360143e-05,
      "loss": 1.7769,
      "step": 17610
    },
    {
      "epoch": 1.7714271351731765,
      "grad_norm": 2.652273416519165,
      "learning_rate": 1.233448774419152e-05,
      "loss": 1.782,
      "step": 17620
    },
    {
      "epoch": 1.7724325139496306,
      "grad_norm": 2.2700140476226807,
      "learning_rate": 1.2324400659022897e-05,
      "loss": 1.6861,
      "step": 17630
    },
    {
      "epoch": 1.7734378927260845,
      "grad_norm": 1.572161316871643,
      "learning_rate": 1.2314313573854275e-05,
      "loss": 1.6943,
      "step": 17640
    },
    {
      "epoch": 1.7744432715025384,
      "grad_norm": 2.3919191360473633,
      "learning_rate": 1.2304226488685653e-05,
      "loss": 1.7438,
      "step": 17650
    },
    {
      "epoch": 1.7754486502789926,
      "grad_norm": 2.2066116333007812,
      "learning_rate": 1.229413940351703e-05,
      "loss": 1.8184,
      "step": 17660
    },
    {
      "epoch": 1.7764540290554467,
      "grad_norm": 2.4972712993621826,
      "learning_rate": 1.2284052318348409e-05,
      "loss": 1.695,
      "step": 17670
    },
    {
      "epoch": 1.7774594078319006,
      "grad_norm": 1.6899240016937256,
      "learning_rate": 1.2273965233179787e-05,
      "loss": 1.7548,
      "step": 17680
    },
    {
      "epoch": 1.7784647866083547,
      "grad_norm": 3.870410203933716,
      "learning_rate": 1.2263878148011163e-05,
      "loss": 1.7412,
      "step": 17690
    },
    {
      "epoch": 1.7794701653848088,
      "grad_norm": 2.332932710647583,
      "learning_rate": 1.2253791062842541e-05,
      "loss": 1.7429,
      "step": 17700
    },
    {
      "epoch": 1.7804755441612627,
      "grad_norm": 2.0137205123901367,
      "learning_rate": 1.224370397767392e-05,
      "loss": 1.7929,
      "step": 17710
    },
    {
      "epoch": 1.7814809229377166,
      "grad_norm": 1.7060976028442383,
      "learning_rate": 1.2233616892505296e-05,
      "loss": 1.8099,
      "step": 17720
    },
    {
      "epoch": 1.7824863017141708,
      "grad_norm": 1.7806370258331299,
      "learning_rate": 1.2223529807336674e-05,
      "loss": 1.6876,
      "step": 17730
    },
    {
      "epoch": 1.783491680490625,
      "grad_norm": 2.103161334991455,
      "learning_rate": 1.2213442722168052e-05,
      "loss": 1.7565,
      "step": 17740
    },
    {
      "epoch": 1.7844970592670788,
      "grad_norm": 2.1954286098480225,
      "learning_rate": 1.2203355636999428e-05,
      "loss": 1.8988,
      "step": 17750
    },
    {
      "epoch": 1.785502438043533,
      "grad_norm": 2.1020314693450928,
      "learning_rate": 1.2193268551830806e-05,
      "loss": 1.8572,
      "step": 17760
    },
    {
      "epoch": 1.786507816819987,
      "grad_norm": 1.661226749420166,
      "learning_rate": 1.2183181466662184e-05,
      "loss": 1.7033,
      "step": 17770
    },
    {
      "epoch": 1.787513195596441,
      "grad_norm": 1.6110481023788452,
      "learning_rate": 1.217309438149356e-05,
      "loss": 1.7746,
      "step": 17780
    },
    {
      "epoch": 1.7885185743728949,
      "grad_norm": 1.6161962747573853,
      "learning_rate": 1.2163007296324938e-05,
      "loss": 1.7635,
      "step": 17790
    },
    {
      "epoch": 1.789523953149349,
      "grad_norm": 1.9352540969848633,
      "learning_rate": 1.2152920211156316e-05,
      "loss": 1.8033,
      "step": 17800
    },
    {
      "epoch": 1.790529331925803,
      "grad_norm": 2.5106215476989746,
      "learning_rate": 1.2142833125987693e-05,
      "loss": 1.7351,
      "step": 17810
    },
    {
      "epoch": 1.791534710702257,
      "grad_norm": 1.9968589544296265,
      "learning_rate": 1.213274604081907e-05,
      "loss": 1.8141,
      "step": 17820
    },
    {
      "epoch": 1.7925400894787111,
      "grad_norm": 1.6462366580963135,
      "learning_rate": 1.212265895565045e-05,
      "loss": 1.7936,
      "step": 17830
    },
    {
      "epoch": 1.7935454682551653,
      "grad_norm": 1.8081640005111694,
      "learning_rate": 1.2112571870481827e-05,
      "loss": 1.7574,
      "step": 17840
    },
    {
      "epoch": 1.7945508470316192,
      "grad_norm": 2.0144155025482178,
      "learning_rate": 1.2102484785313205e-05,
      "loss": 1.7602,
      "step": 17850
    },
    {
      "epoch": 1.795556225808073,
      "grad_norm": 1.935066819190979,
      "learning_rate": 1.2092397700144583e-05,
      "loss": 1.7593,
      "step": 17860
    },
    {
      "epoch": 1.7965616045845272,
      "grad_norm": 1.827635407447815,
      "learning_rate": 1.2082310614975959e-05,
      "loss": 1.7492,
      "step": 17870
    },
    {
      "epoch": 1.7975669833609813,
      "grad_norm": 1.9203156232833862,
      "learning_rate": 1.2072223529807337e-05,
      "loss": 1.825,
      "step": 17880
    },
    {
      "epoch": 1.7985723621374352,
      "grad_norm": 2.455195665359497,
      "learning_rate": 1.2062136444638715e-05,
      "loss": 1.7269,
      "step": 17890
    },
    {
      "epoch": 1.7995777409138893,
      "grad_norm": 2.0340921878814697,
      "learning_rate": 1.2052049359470091e-05,
      "loss": 1.6807,
      "step": 17900
    },
    {
      "epoch": 1.8005831196903435,
      "grad_norm": 2.1336452960968018,
      "learning_rate": 1.204196227430147e-05,
      "loss": 1.743,
      "step": 17910
    },
    {
      "epoch": 1.8015884984667974,
      "grad_norm": 1.7831244468688965,
      "learning_rate": 1.2031875189132847e-05,
      "loss": 1.7394,
      "step": 17920
    },
    {
      "epoch": 1.8025938772432513,
      "grad_norm": 2.0009586811065674,
      "learning_rate": 1.2021788103964224e-05,
      "loss": 1.7164,
      "step": 17930
    },
    {
      "epoch": 1.8035992560197054,
      "grad_norm": 2.3429195880889893,
      "learning_rate": 1.2011701018795602e-05,
      "loss": 1.7388,
      "step": 17940
    },
    {
      "epoch": 1.8046046347961595,
      "grad_norm": 2.470890522003174,
      "learning_rate": 1.200161393362698e-05,
      "loss": 1.8279,
      "step": 17950
    },
    {
      "epoch": 1.8056100135726134,
      "grad_norm": 1.8346246480941772,
      "learning_rate": 1.1991526848458356e-05,
      "loss": 1.7873,
      "step": 17960
    },
    {
      "epoch": 1.8066153923490675,
      "grad_norm": 2.067232847213745,
      "learning_rate": 1.1981439763289734e-05,
      "loss": 1.812,
      "step": 17970
    },
    {
      "epoch": 1.8076207711255217,
      "grad_norm": 1.8089812994003296,
      "learning_rate": 1.1971352678121114e-05,
      "loss": 1.7237,
      "step": 17980
    },
    {
      "epoch": 1.8086261499019756,
      "grad_norm": 1.9443050622940063,
      "learning_rate": 1.196126559295249e-05,
      "loss": 1.7668,
      "step": 17990
    },
    {
      "epoch": 1.8096315286784295,
      "grad_norm": 2.0087647438049316,
      "learning_rate": 1.1951178507783868e-05,
      "loss": 1.8229,
      "step": 18000
    },
    {
      "epoch": 1.8106369074548836,
      "grad_norm": 2.004772901535034,
      "learning_rate": 1.1941091422615246e-05,
      "loss": 1.7309,
      "step": 18010
    },
    {
      "epoch": 1.8116422862313377,
      "grad_norm": 2.1284568309783936,
      "learning_rate": 1.1931004337446622e-05,
      "loss": 1.6931,
      "step": 18020
    },
    {
      "epoch": 1.8126476650077916,
      "grad_norm": 1.953857183456421,
      "learning_rate": 1.1920917252278e-05,
      "loss": 1.8054,
      "step": 18030
    },
    {
      "epoch": 1.8136530437842457,
      "grad_norm": 2.4362282752990723,
      "learning_rate": 1.1910830167109378e-05,
      "loss": 1.7755,
      "step": 18040
    },
    {
      "epoch": 1.8146584225606999,
      "grad_norm": 1.7280172109603882,
      "learning_rate": 1.1900743081940755e-05,
      "loss": 1.7726,
      "step": 18050
    },
    {
      "epoch": 1.8156638013371538,
      "grad_norm": 2.582416534423828,
      "learning_rate": 1.1890655996772133e-05,
      "loss": 1.7855,
      "step": 18060
    },
    {
      "epoch": 1.8166691801136077,
      "grad_norm": 2.042121410369873,
      "learning_rate": 1.188056891160351e-05,
      "loss": 1.8165,
      "step": 18070
    },
    {
      "epoch": 1.8176745588900618,
      "grad_norm": 2.0384504795074463,
      "learning_rate": 1.1870481826434887e-05,
      "loss": 1.7555,
      "step": 18080
    },
    {
      "epoch": 1.818679937666516,
      "grad_norm": 2.034360647201538,
      "learning_rate": 1.1860394741266265e-05,
      "loss": 1.6711,
      "step": 18090
    },
    {
      "epoch": 1.8196853164429698,
      "grad_norm": 2.4431650638580322,
      "learning_rate": 1.1850307656097643e-05,
      "loss": 1.7132,
      "step": 18100
    },
    {
      "epoch": 1.820690695219424,
      "grad_norm": 1.915967583656311,
      "learning_rate": 1.184022057092902e-05,
      "loss": 1.7558,
      "step": 18110
    },
    {
      "epoch": 1.821696073995878,
      "grad_norm": 2.4904844760894775,
      "learning_rate": 1.1830133485760397e-05,
      "loss": 1.8042,
      "step": 18120
    },
    {
      "epoch": 1.822701452772332,
      "grad_norm": 1.8343170881271362,
      "learning_rate": 1.1820046400591777e-05,
      "loss": 1.7022,
      "step": 18130
    },
    {
      "epoch": 1.8237068315487859,
      "grad_norm": 2.4791371822357178,
      "learning_rate": 1.1809959315423153e-05,
      "loss": 1.8054,
      "step": 18140
    },
    {
      "epoch": 1.82471221032524,
      "grad_norm": 2.768047332763672,
      "learning_rate": 1.1799872230254531e-05,
      "loss": 1.7331,
      "step": 18150
    },
    {
      "epoch": 1.8257175891016941,
      "grad_norm": 2.003774881362915,
      "learning_rate": 1.178978514508591e-05,
      "loss": 1.7463,
      "step": 18160
    },
    {
      "epoch": 1.826722967878148,
      "grad_norm": 2.5350799560546875,
      "learning_rate": 1.1779698059917286e-05,
      "loss": 1.706,
      "step": 18170
    },
    {
      "epoch": 1.8277283466546022,
      "grad_norm": 1.9051190614700317,
      "learning_rate": 1.1769610974748664e-05,
      "loss": 1.9157,
      "step": 18180
    },
    {
      "epoch": 1.8287337254310563,
      "grad_norm": 2.1652941703796387,
      "learning_rate": 1.1759523889580042e-05,
      "loss": 1.8424,
      "step": 18190
    },
    {
      "epoch": 1.8297391042075102,
      "grad_norm": 1.8993386030197144,
      "learning_rate": 1.1749436804411418e-05,
      "loss": 1.7355,
      "step": 18200
    },
    {
      "epoch": 1.830744482983964,
      "grad_norm": 2.969874143600464,
      "learning_rate": 1.1739349719242796e-05,
      "loss": 1.7742,
      "step": 18210
    },
    {
      "epoch": 1.8317498617604182,
      "grad_norm": 1.9313522577285767,
      "learning_rate": 1.1729262634074174e-05,
      "loss": 1.7959,
      "step": 18220
    },
    {
      "epoch": 1.8327552405368723,
      "grad_norm": 1.9036731719970703,
      "learning_rate": 1.171917554890555e-05,
      "loss": 1.7645,
      "step": 18230
    },
    {
      "epoch": 1.8337606193133262,
      "grad_norm": 1.8194552659988403,
      "learning_rate": 1.1709088463736928e-05,
      "loss": 1.7956,
      "step": 18240
    },
    {
      "epoch": 1.8347659980897804,
      "grad_norm": 1.9071354866027832,
      "learning_rate": 1.1699001378568306e-05,
      "loss": 1.8029,
      "step": 18250
    },
    {
      "epoch": 1.8357713768662345,
      "grad_norm": 2.24910831451416,
      "learning_rate": 1.1688914293399683e-05,
      "loss": 1.8393,
      "step": 18260
    },
    {
      "epoch": 1.8367767556426884,
      "grad_norm": 2.163668155670166,
      "learning_rate": 1.167882720823106e-05,
      "loss": 1.8236,
      "step": 18270
    },
    {
      "epoch": 1.8377821344191423,
      "grad_norm": 2.7243478298187256,
      "learning_rate": 1.166874012306244e-05,
      "loss": 1.7292,
      "step": 18280
    },
    {
      "epoch": 1.8387875131955964,
      "grad_norm": 2.1080217361450195,
      "learning_rate": 1.1658653037893817e-05,
      "loss": 1.667,
      "step": 18290
    },
    {
      "epoch": 1.8397928919720505,
      "grad_norm": 1.7189658880233765,
      "learning_rate": 1.1648565952725195e-05,
      "loss": 1.7917,
      "step": 18300
    },
    {
      "epoch": 1.8407982707485044,
      "grad_norm": 2.811537742614746,
      "learning_rate": 1.1638478867556573e-05,
      "loss": 1.6874,
      "step": 18310
    },
    {
      "epoch": 1.8418036495249586,
      "grad_norm": 1.832444667816162,
      "learning_rate": 1.162839178238795e-05,
      "loss": 1.7995,
      "step": 18320
    },
    {
      "epoch": 1.8428090283014127,
      "grad_norm": 2.603501319885254,
      "learning_rate": 1.1618304697219327e-05,
      "loss": 1.7645,
      "step": 18330
    },
    {
      "epoch": 1.8438144070778666,
      "grad_norm": 1.7953605651855469,
      "learning_rate": 1.1608217612050705e-05,
      "loss": 1.7571,
      "step": 18340
    },
    {
      "epoch": 1.8448197858543205,
      "grad_norm": 2.507450819015503,
      "learning_rate": 1.1598130526882083e-05,
      "loss": 1.6598,
      "step": 18350
    },
    {
      "epoch": 1.8458251646307746,
      "grad_norm": 2.225985288619995,
      "learning_rate": 1.158804344171346e-05,
      "loss": 1.6948,
      "step": 18360
    },
    {
      "epoch": 1.8468305434072287,
      "grad_norm": 1.9750206470489502,
      "learning_rate": 1.1577956356544837e-05,
      "loss": 1.7622,
      "step": 18370
    },
    {
      "epoch": 1.8478359221836826,
      "grad_norm": 1.7437622547149658,
      "learning_rate": 1.1567869271376215e-05,
      "loss": 1.8055,
      "step": 18380
    },
    {
      "epoch": 1.8488413009601368,
      "grad_norm": 2.602736473083496,
      "learning_rate": 1.1557782186207592e-05,
      "loss": 1.798,
      "step": 18390
    },
    {
      "epoch": 1.849846679736591,
      "grad_norm": 2.614804744720459,
      "learning_rate": 1.154769510103897e-05,
      "loss": 1.8,
      "step": 18400
    },
    {
      "epoch": 1.8508520585130448,
      "grad_norm": 2.940141201019287,
      "learning_rate": 1.1537608015870348e-05,
      "loss": 1.7841,
      "step": 18410
    },
    {
      "epoch": 1.8518574372894987,
      "grad_norm": 2.560650110244751,
      "learning_rate": 1.1527520930701724e-05,
      "loss": 1.8617,
      "step": 18420
    },
    {
      "epoch": 1.8528628160659528,
      "grad_norm": 2.401546001434326,
      "learning_rate": 1.1517433845533104e-05,
      "loss": 1.7446,
      "step": 18430
    },
    {
      "epoch": 1.853868194842407,
      "grad_norm": 1.780454397201538,
      "learning_rate": 1.1507346760364482e-05,
      "loss": 1.7748,
      "step": 18440
    },
    {
      "epoch": 1.8548735736188608,
      "grad_norm": 2.3021883964538574,
      "learning_rate": 1.1497259675195858e-05,
      "loss": 1.7867,
      "step": 18450
    },
    {
      "epoch": 1.855878952395315,
      "grad_norm": 2.236462354660034,
      "learning_rate": 1.1487172590027236e-05,
      "loss": 1.7027,
      "step": 18460
    },
    {
      "epoch": 1.856884331171769,
      "grad_norm": 2.208111047744751,
      "learning_rate": 1.1477085504858614e-05,
      "loss": 1.7458,
      "step": 18470
    },
    {
      "epoch": 1.857889709948223,
      "grad_norm": 1.8962291479110718,
      "learning_rate": 1.146699841968999e-05,
      "loss": 1.813,
      "step": 18480
    },
    {
      "epoch": 1.858895088724677,
      "grad_norm": 1.9418344497680664,
      "learning_rate": 1.1456911334521368e-05,
      "loss": 1.7895,
      "step": 18490
    },
    {
      "epoch": 1.859900467501131,
      "grad_norm": 1.8074817657470703,
      "learning_rate": 1.1446824249352746e-05,
      "loss": 1.8362,
      "step": 18500
    },
    {
      "epoch": 1.8609058462775852,
      "grad_norm": 2.212456703186035,
      "learning_rate": 1.1436737164184123e-05,
      "loss": 1.7311,
      "step": 18510
    },
    {
      "epoch": 1.861911225054039,
      "grad_norm": 1.949718713760376,
      "learning_rate": 1.14266500790155e-05,
      "loss": 1.7563,
      "step": 18520
    },
    {
      "epoch": 1.8629166038304932,
      "grad_norm": 1.680271863937378,
      "learning_rate": 1.1416562993846879e-05,
      "loss": 1.7369,
      "step": 18530
    },
    {
      "epoch": 1.8639219826069473,
      "grad_norm": 1.8634859323501587,
      "learning_rate": 1.1406475908678255e-05,
      "loss": 1.7384,
      "step": 18540
    },
    {
      "epoch": 1.8649273613834012,
      "grad_norm": 1.8870930671691895,
      "learning_rate": 1.1396388823509633e-05,
      "loss": 1.7464,
      "step": 18550
    },
    {
      "epoch": 1.865932740159855,
      "grad_norm": 2.5645194053649902,
      "learning_rate": 1.1386301738341011e-05,
      "loss": 1.7364,
      "step": 18560
    },
    {
      "epoch": 1.8669381189363092,
      "grad_norm": 2.2714240550994873,
      "learning_rate": 1.1376214653172387e-05,
      "loss": 1.6787,
      "step": 18570
    },
    {
      "epoch": 1.8679434977127634,
      "grad_norm": 2.6845903396606445,
      "learning_rate": 1.1366127568003767e-05,
      "loss": 1.726,
      "step": 18580
    },
    {
      "epoch": 1.8689488764892173,
      "grad_norm": 2.027388334274292,
      "learning_rate": 1.1356040482835145e-05,
      "loss": 1.7342,
      "step": 18590
    },
    {
      "epoch": 1.8699542552656714,
      "grad_norm": 1.6419240236282349,
      "learning_rate": 1.1345953397666521e-05,
      "loss": 1.7964,
      "step": 18600
    },
    {
      "epoch": 1.8709596340421255,
      "grad_norm": 2.401106595993042,
      "learning_rate": 1.13358663124979e-05,
      "loss": 1.7114,
      "step": 18610
    },
    {
      "epoch": 1.8719650128185794,
      "grad_norm": 2.629854679107666,
      "learning_rate": 1.1325779227329277e-05,
      "loss": 1.8416,
      "step": 18620
    },
    {
      "epoch": 1.8729703915950333,
      "grad_norm": 2.103508710861206,
      "learning_rate": 1.1315692142160654e-05,
      "loss": 1.7913,
      "step": 18630
    },
    {
      "epoch": 1.8739757703714874,
      "grad_norm": 2.440268039703369,
      "learning_rate": 1.1305605056992032e-05,
      "loss": 1.759,
      "step": 18640
    },
    {
      "epoch": 1.8749811491479416,
      "grad_norm": 2.680016279220581,
      "learning_rate": 1.129551797182341e-05,
      "loss": 1.7882,
      "step": 18650
    },
    {
      "epoch": 1.8759865279243955,
      "grad_norm": 1.8165404796600342,
      "learning_rate": 1.1285430886654786e-05,
      "loss": 1.757,
      "step": 18660
    },
    {
      "epoch": 1.8769919067008496,
      "grad_norm": 1.9997541904449463,
      "learning_rate": 1.1275343801486164e-05,
      "loss": 1.7,
      "step": 18670
    },
    {
      "epoch": 1.8779972854773037,
      "grad_norm": 1.724434494972229,
      "learning_rate": 1.1265256716317542e-05,
      "loss": 1.7345,
      "step": 18680
    },
    {
      "epoch": 1.8790026642537576,
      "grad_norm": 2.0345630645751953,
      "learning_rate": 1.1255169631148918e-05,
      "loss": 1.7743,
      "step": 18690
    },
    {
      "epoch": 1.8800080430302115,
      "grad_norm": 1.6999057531356812,
      "learning_rate": 1.1245082545980296e-05,
      "loss": 1.7569,
      "step": 18700
    },
    {
      "epoch": 1.8810134218066656,
      "grad_norm": 1.8601696491241455,
      "learning_rate": 1.1234995460811674e-05,
      "loss": 1.85,
      "step": 18710
    },
    {
      "epoch": 1.8820188005831198,
      "grad_norm": 2.562319755554199,
      "learning_rate": 1.122490837564305e-05,
      "loss": 1.7842,
      "step": 18720
    },
    {
      "epoch": 1.8830241793595737,
      "grad_norm": 1.861325740814209,
      "learning_rate": 1.1214821290474429e-05,
      "loss": 1.7518,
      "step": 18730
    },
    {
      "epoch": 1.8840295581360278,
      "grad_norm": 3.0532314777374268,
      "learning_rate": 1.1204734205305808e-05,
      "loss": 1.7609,
      "step": 18740
    },
    {
      "epoch": 1.885034936912482,
      "grad_norm": 2.0075929164886475,
      "learning_rate": 1.1194647120137185e-05,
      "loss": 1.7057,
      "step": 18750
    },
    {
      "epoch": 1.8860403156889358,
      "grad_norm": 1.7844182252883911,
      "learning_rate": 1.1184560034968563e-05,
      "loss": 1.7354,
      "step": 18760
    },
    {
      "epoch": 1.8870456944653897,
      "grad_norm": 2.1950230598449707,
      "learning_rate": 1.117447294979994e-05,
      "loss": 1.7785,
      "step": 18770
    },
    {
      "epoch": 1.8880510732418438,
      "grad_norm": 2.074916124343872,
      "learning_rate": 1.1164385864631317e-05,
      "loss": 1.7383,
      "step": 18780
    },
    {
      "epoch": 1.889056452018298,
      "grad_norm": 1.9865379333496094,
      "learning_rate": 1.1154298779462695e-05,
      "loss": 1.747,
      "step": 18790
    },
    {
      "epoch": 1.8900618307947519,
      "grad_norm": 2.210970163345337,
      "learning_rate": 1.1144211694294073e-05,
      "loss": 1.7587,
      "step": 18800
    },
    {
      "epoch": 1.891067209571206,
      "grad_norm": 1.7622380256652832,
      "learning_rate": 1.113412460912545e-05,
      "loss": 1.735,
      "step": 18810
    },
    {
      "epoch": 1.8920725883476601,
      "grad_norm": 2.496854543685913,
      "learning_rate": 1.1124037523956827e-05,
      "loss": 1.784,
      "step": 18820
    },
    {
      "epoch": 1.893077967124114,
      "grad_norm": 1.8988990783691406,
      "learning_rate": 1.1113950438788205e-05,
      "loss": 1.8401,
      "step": 18830
    },
    {
      "epoch": 1.894083345900568,
      "grad_norm": 2.0800559520721436,
      "learning_rate": 1.1103863353619582e-05,
      "loss": 1.7,
      "step": 18840
    },
    {
      "epoch": 1.895088724677022,
      "grad_norm": 1.893925666809082,
      "learning_rate": 1.109377626845096e-05,
      "loss": 1.8113,
      "step": 18850
    },
    {
      "epoch": 1.8960941034534762,
      "grad_norm": 2.027886390686035,
      "learning_rate": 1.1083689183282338e-05,
      "loss": 1.681,
      "step": 18860
    },
    {
      "epoch": 1.89709948222993,
      "grad_norm": 2.369760036468506,
      "learning_rate": 1.1073602098113714e-05,
      "loss": 1.7932,
      "step": 18870
    },
    {
      "epoch": 1.8981048610063842,
      "grad_norm": 1.8249832391738892,
      "learning_rate": 1.1063515012945092e-05,
      "loss": 1.7723,
      "step": 18880
    },
    {
      "epoch": 1.8991102397828383,
      "grad_norm": 2.209534168243408,
      "learning_rate": 1.1053427927776472e-05,
      "loss": 1.7425,
      "step": 18890
    },
    {
      "epoch": 1.9001156185592922,
      "grad_norm": 2.3336777687072754,
      "learning_rate": 1.1043340842607848e-05,
      "loss": 1.7618,
      "step": 18900
    },
    {
      "epoch": 1.9011209973357461,
      "grad_norm": 1.948614239692688,
      "learning_rate": 1.1033253757439226e-05,
      "loss": 1.7164,
      "step": 18910
    },
    {
      "epoch": 1.9021263761122003,
      "grad_norm": 2.026451826095581,
      "learning_rate": 1.1023166672270604e-05,
      "loss": 1.7576,
      "step": 18920
    },
    {
      "epoch": 1.9031317548886544,
      "grad_norm": 1.7875603437423706,
      "learning_rate": 1.101307958710198e-05,
      "loss": 1.7976,
      "step": 18930
    },
    {
      "epoch": 1.9041371336651083,
      "grad_norm": 1.8668581247329712,
      "learning_rate": 1.1002992501933358e-05,
      "loss": 1.7883,
      "step": 18940
    },
    {
      "epoch": 1.9051425124415624,
      "grad_norm": 1.8856236934661865,
      "learning_rate": 1.0992905416764736e-05,
      "loss": 1.8521,
      "step": 18950
    },
    {
      "epoch": 1.9061478912180165,
      "grad_norm": 1.67019784450531,
      "learning_rate": 1.0982818331596113e-05,
      "loss": 1.8048,
      "step": 18960
    },
    {
      "epoch": 1.9071532699944704,
      "grad_norm": 2.632934093475342,
      "learning_rate": 1.097273124642749e-05,
      "loss": 1.7175,
      "step": 18970
    },
    {
      "epoch": 1.9081586487709243,
      "grad_norm": 2.041445016860962,
      "learning_rate": 1.0962644161258869e-05,
      "loss": 1.7172,
      "step": 18980
    },
    {
      "epoch": 1.9091640275473785,
      "grad_norm": 2.302359104156494,
      "learning_rate": 1.0952557076090245e-05,
      "loss": 1.7648,
      "step": 18990
    },
    {
      "epoch": 1.9101694063238326,
      "grad_norm": 2.635732412338257,
      "learning_rate": 1.0942469990921623e-05,
      "loss": 1.7748,
      "step": 19000
    },
    {
      "epoch": 1.9111747851002865,
      "grad_norm": 1.8454828262329102,
      "learning_rate": 1.0932382905753001e-05,
      "loss": 1.8143,
      "step": 19010
    },
    {
      "epoch": 1.9121801638767406,
      "grad_norm": 2.2574803829193115,
      "learning_rate": 1.0922295820584377e-05,
      "loss": 1.7741,
      "step": 19020
    },
    {
      "epoch": 1.9131855426531947,
      "grad_norm": 2.870717763900757,
      "learning_rate": 1.0912208735415755e-05,
      "loss": 1.7769,
      "step": 19030
    },
    {
      "epoch": 1.9141909214296486,
      "grad_norm": 1.658968210220337,
      "learning_rate": 1.0902121650247135e-05,
      "loss": 1.7477,
      "step": 19040
    },
    {
      "epoch": 1.9151963002061025,
      "grad_norm": 2.0865836143493652,
      "learning_rate": 1.0892034565078511e-05,
      "loss": 1.8006,
      "step": 19050
    },
    {
      "epoch": 1.9162016789825567,
      "grad_norm": 1.86741304397583,
      "learning_rate": 1.088194747990989e-05,
      "loss": 1.8432,
      "step": 19060
    },
    {
      "epoch": 1.9172070577590108,
      "grad_norm": 2.616093397140503,
      "learning_rate": 1.0871860394741267e-05,
      "loss": 1.7153,
      "step": 19070
    },
    {
      "epoch": 1.9182124365354647,
      "grad_norm": 2.338817834854126,
      "learning_rate": 1.0861773309572644e-05,
      "loss": 1.6497,
      "step": 19080
    },
    {
      "epoch": 1.9192178153119188,
      "grad_norm": 2.2473881244659424,
      "learning_rate": 1.0851686224404022e-05,
      "loss": 1.7766,
      "step": 19090
    },
    {
      "epoch": 1.920223194088373,
      "grad_norm": 1.8536217212677002,
      "learning_rate": 1.08415991392354e-05,
      "loss": 1.8208,
      "step": 19100
    },
    {
      "epoch": 1.9212285728648268,
      "grad_norm": 1.8774909973144531,
      "learning_rate": 1.0831512054066776e-05,
      "loss": 1.7259,
      "step": 19110
    },
    {
      "epoch": 1.9222339516412807,
      "grad_norm": 2.26936936378479,
      "learning_rate": 1.0821424968898154e-05,
      "loss": 1.7078,
      "step": 19120
    },
    {
      "epoch": 1.9232393304177349,
      "grad_norm": 2.3989579677581787,
      "learning_rate": 1.0811337883729532e-05,
      "loss": 1.6835,
      "step": 19130
    },
    {
      "epoch": 1.924244709194189,
      "grad_norm": 1.8953137397766113,
      "learning_rate": 1.0801250798560908e-05,
      "loss": 1.7953,
      "step": 19140
    },
    {
      "epoch": 1.925250087970643,
      "grad_norm": 1.6342002153396606,
      "learning_rate": 1.0791163713392286e-05,
      "loss": 1.7073,
      "step": 19150
    },
    {
      "epoch": 1.926255466747097,
      "grad_norm": 2.0299320220947266,
      "learning_rate": 1.0781076628223664e-05,
      "loss": 1.6587,
      "step": 19160
    },
    {
      "epoch": 1.9272608455235511,
      "grad_norm": 2.356109380722046,
      "learning_rate": 1.077098954305504e-05,
      "loss": 1.7585,
      "step": 19170
    },
    {
      "epoch": 1.928266224300005,
      "grad_norm": 1.8497002124786377,
      "learning_rate": 1.0760902457886419e-05,
      "loss": 1.7401,
      "step": 19180
    },
    {
      "epoch": 1.929271603076459,
      "grad_norm": 1.6088991165161133,
      "learning_rate": 1.0750815372717798e-05,
      "loss": 1.7674,
      "step": 19190
    },
    {
      "epoch": 1.930276981852913,
      "grad_norm": 1.8828723430633545,
      "learning_rate": 1.0740728287549175e-05,
      "loss": 1.7159,
      "step": 19200
    },
    {
      "epoch": 1.9312823606293672,
      "grad_norm": 1.6926394701004028,
      "learning_rate": 1.0730641202380553e-05,
      "loss": 1.7412,
      "step": 19210
    },
    {
      "epoch": 1.932287739405821,
      "grad_norm": 2.340695858001709,
      "learning_rate": 1.072055411721193e-05,
      "loss": 1.7358,
      "step": 19220
    },
    {
      "epoch": 1.9332931181822752,
      "grad_norm": 1.9400943517684937,
      "learning_rate": 1.0710467032043307e-05,
      "loss": 1.7119,
      "step": 19230
    },
    {
      "epoch": 1.9342984969587294,
      "grad_norm": 1.520889401435852,
      "learning_rate": 1.0700379946874685e-05,
      "loss": 1.7118,
      "step": 19240
    },
    {
      "epoch": 1.9353038757351833,
      "grad_norm": 2.1143219470977783,
      "learning_rate": 1.0690292861706063e-05,
      "loss": 1.6926,
      "step": 19250
    },
    {
      "epoch": 1.9363092545116372,
      "grad_norm": 1.7124948501586914,
      "learning_rate": 1.068020577653744e-05,
      "loss": 1.6956,
      "step": 19260
    },
    {
      "epoch": 1.9373146332880913,
      "grad_norm": 2.4777791500091553,
      "learning_rate": 1.0670118691368817e-05,
      "loss": 1.7572,
      "step": 19270
    },
    {
      "epoch": 1.9383200120645454,
      "grad_norm": 2.1496942043304443,
      "learning_rate": 1.0660031606200195e-05,
      "loss": 1.8004,
      "step": 19280
    },
    {
      "epoch": 1.9393253908409993,
      "grad_norm": 1.9890514612197876,
      "learning_rate": 1.0649944521031572e-05,
      "loss": 1.7273,
      "step": 19290
    },
    {
      "epoch": 1.9403307696174532,
      "grad_norm": 1.5927029848098755,
      "learning_rate": 1.063985743586295e-05,
      "loss": 1.798,
      "step": 19300
    },
    {
      "epoch": 1.9413361483939076,
      "grad_norm": 2.044027090072632,
      "learning_rate": 1.0629770350694328e-05,
      "loss": 1.7914,
      "step": 19310
    },
    {
      "epoch": 1.9423415271703615,
      "grad_norm": 2.0406477451324463,
      "learning_rate": 1.0619683265525704e-05,
      "loss": 1.7259,
      "step": 19320
    },
    {
      "epoch": 1.9433469059468154,
      "grad_norm": 1.8683819770812988,
      "learning_rate": 1.0609596180357082e-05,
      "loss": 1.7515,
      "step": 19330
    },
    {
      "epoch": 1.9443522847232695,
      "grad_norm": 2.138937473297119,
      "learning_rate": 1.0599509095188462e-05,
      "loss": 1.5706,
      "step": 19340
    },
    {
      "epoch": 1.9453576634997236,
      "grad_norm": 2.4361648559570312,
      "learning_rate": 1.0589422010019838e-05,
      "loss": 1.8229,
      "step": 19350
    },
    {
      "epoch": 1.9463630422761775,
      "grad_norm": 1.9219917058944702,
      "learning_rate": 1.0579334924851216e-05,
      "loss": 1.7902,
      "step": 19360
    },
    {
      "epoch": 1.9473684210526314,
      "grad_norm": 2.2497267723083496,
      "learning_rate": 1.0569247839682594e-05,
      "loss": 1.7529,
      "step": 19370
    },
    {
      "epoch": 1.9483737998290858,
      "grad_norm": 2.351102828979492,
      "learning_rate": 1.055916075451397e-05,
      "loss": 1.656,
      "step": 19380
    },
    {
      "epoch": 1.9493791786055397,
      "grad_norm": 2.088933229446411,
      "learning_rate": 1.0550082377862211e-05,
      "loss": 1.7731,
      "step": 19390
    },
    {
      "epoch": 1.9503845573819936,
      "grad_norm": 1.9480878114700317,
      "learning_rate": 1.0539995292693589e-05,
      "loss": 1.8108,
      "step": 19400
    },
    {
      "epoch": 1.9513899361584477,
      "grad_norm": 1.8364320993423462,
      "learning_rate": 1.0529908207524965e-05,
      "loss": 1.8112,
      "step": 19410
    },
    {
      "epoch": 1.9523953149349018,
      "grad_norm": 2.466085910797119,
      "learning_rate": 1.0519821122356343e-05,
      "loss": 1.7646,
      "step": 19420
    },
    {
      "epoch": 1.9534006937113557,
      "grad_norm": 1.5241847038269043,
      "learning_rate": 1.0509734037187721e-05,
      "loss": 1.7832,
      "step": 19430
    },
    {
      "epoch": 1.9544060724878096,
      "grad_norm": 1.8621811866760254,
      "learning_rate": 1.0499646952019098e-05,
      "loss": 1.7946,
      "step": 19440
    },
    {
      "epoch": 1.955411451264264,
      "grad_norm": 2.4274837970733643,
      "learning_rate": 1.0489559866850476e-05,
      "loss": 1.6439,
      "step": 19450
    },
    {
      "epoch": 1.9564168300407179,
      "grad_norm": 1.9573832750320435,
      "learning_rate": 1.0479472781681854e-05,
      "loss": 1.6672,
      "step": 19460
    },
    {
      "epoch": 1.9574222088171718,
      "grad_norm": 1.886580467224121,
      "learning_rate": 1.046938569651323e-05,
      "loss": 1.6823,
      "step": 19470
    },
    {
      "epoch": 1.958427587593626,
      "grad_norm": 2.0986886024475098,
      "learning_rate": 1.045929861134461e-05,
      "loss": 1.801,
      "step": 19480
    },
    {
      "epoch": 1.95943296637008,
      "grad_norm": 1.6138074398040771,
      "learning_rate": 1.0449211526175988e-05,
      "loss": 1.8496,
      "step": 19490
    },
    {
      "epoch": 1.960438345146534,
      "grad_norm": 2.1175382137298584,
      "learning_rate": 1.0439124441007364e-05,
      "loss": 1.7839,
      "step": 19500
    },
    {
      "epoch": 1.9614437239229878,
      "grad_norm": 2.1682186126708984,
      "learning_rate": 1.0429037355838742e-05,
      "loss": 1.8025,
      "step": 19510
    },
    {
      "epoch": 1.9624491026994422,
      "grad_norm": 1.9227112531661987,
      "learning_rate": 1.041895027067012e-05,
      "loss": 1.7357,
      "step": 19520
    },
    {
      "epoch": 1.963454481475896,
      "grad_norm": 2.4629323482513428,
      "learning_rate": 1.0408863185501496e-05,
      "loss": 1.781,
      "step": 19530
    },
    {
      "epoch": 1.96445986025235,
      "grad_norm": 1.8194794654846191,
      "learning_rate": 1.0398776100332874e-05,
      "loss": 1.7703,
      "step": 19540
    },
    {
      "epoch": 1.965465239028804,
      "grad_norm": 2.1773858070373535,
      "learning_rate": 1.0388689015164252e-05,
      "loss": 1.6342,
      "step": 19550
    },
    {
      "epoch": 1.9664706178052582,
      "grad_norm": 2.5268189907073975,
      "learning_rate": 1.0378601929995629e-05,
      "loss": 1.7507,
      "step": 19560
    },
    {
      "epoch": 1.9674759965817121,
      "grad_norm": 2.607060194015503,
      "learning_rate": 1.0368514844827007e-05,
      "loss": 1.7314,
      "step": 19570
    },
    {
      "epoch": 1.968481375358166,
      "grad_norm": 1.8432056903839111,
      "learning_rate": 1.0358427759658385e-05,
      "loss": 1.8196,
      "step": 19580
    },
    {
      "epoch": 1.9694867541346204,
      "grad_norm": 1.8365942239761353,
      "learning_rate": 1.034834067448976e-05,
      "loss": 1.7017,
      "step": 19590
    },
    {
      "epoch": 1.9704921329110743,
      "grad_norm": 1.958957552909851,
      "learning_rate": 1.0338253589321139e-05,
      "loss": 1.7715,
      "step": 19600
    },
    {
      "epoch": 1.9714975116875282,
      "grad_norm": 3.47558331489563,
      "learning_rate": 1.0328166504152517e-05,
      "loss": 1.7283,
      "step": 19610
    },
    {
      "epoch": 1.9725028904639823,
      "grad_norm": 2.030648708343506,
      "learning_rate": 1.0318079418983893e-05,
      "loss": 1.7281,
      "step": 19620
    },
    {
      "epoch": 1.9735082692404364,
      "grad_norm": 2.2434518337249756,
      "learning_rate": 1.0307992333815273e-05,
      "loss": 1.7292,
      "step": 19630
    },
    {
      "epoch": 1.9745136480168903,
      "grad_norm": 1.9944945573806763,
      "learning_rate": 1.0297905248646651e-05,
      "loss": 1.8164,
      "step": 19640
    },
    {
      "epoch": 1.9755190267933442,
      "grad_norm": 2.0598888397216797,
      "learning_rate": 1.0287818163478027e-05,
      "loss": 1.7743,
      "step": 19650
    },
    {
      "epoch": 1.9765244055697986,
      "grad_norm": 3.531519651412964,
      "learning_rate": 1.0277731078309405e-05,
      "loss": 1.7387,
      "step": 19660
    },
    {
      "epoch": 1.9775297843462525,
      "grad_norm": 1.8301972150802612,
      "learning_rate": 1.0267643993140783e-05,
      "loss": 1.781,
      "step": 19670
    },
    {
      "epoch": 1.9785351631227064,
      "grad_norm": 2.655339002609253,
      "learning_rate": 1.025755690797216e-05,
      "loss": 1.7733,
      "step": 19680
    },
    {
      "epoch": 1.9795405418991605,
      "grad_norm": 2.3374431133270264,
      "learning_rate": 1.0247469822803538e-05,
      "loss": 1.731,
      "step": 19690
    },
    {
      "epoch": 1.9805459206756146,
      "grad_norm": 2.3909006118774414,
      "learning_rate": 1.0237382737634916e-05,
      "loss": 1.8636,
      "step": 19700
    },
    {
      "epoch": 1.9815512994520685,
      "grad_norm": 2.4032604694366455,
      "learning_rate": 1.0227295652466292e-05,
      "loss": 1.8089,
      "step": 19710
    },
    {
      "epoch": 1.9825566782285224,
      "grad_norm": 1.6909639835357666,
      "learning_rate": 1.021720856729767e-05,
      "loss": 1.6712,
      "step": 19720
    },
    {
      "epoch": 1.9835620570049768,
      "grad_norm": 1.5769336223602295,
      "learning_rate": 1.0207121482129048e-05,
      "loss": 1.7879,
      "step": 19730
    },
    {
      "epoch": 1.9845674357814307,
      "grad_norm": 1.7321512699127197,
      "learning_rate": 1.0197034396960424e-05,
      "loss": 1.717,
      "step": 19740
    },
    {
      "epoch": 1.9855728145578846,
      "grad_norm": 2.6008105278015137,
      "learning_rate": 1.0186947311791802e-05,
      "loss": 1.7134,
      "step": 19750
    },
    {
      "epoch": 1.9865781933343387,
      "grad_norm": 1.9494410753250122,
      "learning_rate": 1.017686022662318e-05,
      "loss": 1.8472,
      "step": 19760
    },
    {
      "epoch": 1.9875835721107928,
      "grad_norm": 2.1603686809539795,
      "learning_rate": 1.0166773141454557e-05,
      "loss": 1.7482,
      "step": 19770
    },
    {
      "epoch": 1.9885889508872467,
      "grad_norm": 2.0228326320648193,
      "learning_rate": 1.0156686056285936e-05,
      "loss": 1.7106,
      "step": 19780
    },
    {
      "epoch": 1.9895943296637006,
      "grad_norm": 2.1449620723724365,
      "learning_rate": 1.0146598971117314e-05,
      "loss": 1.7598,
      "step": 19790
    },
    {
      "epoch": 1.990599708440155,
      "grad_norm": 2.7031705379486084,
      "learning_rate": 1.013651188594869e-05,
      "loss": 1.7391,
      "step": 19800
    },
    {
      "epoch": 1.991605087216609,
      "grad_norm": 2.125162363052368,
      "learning_rate": 1.0126424800780069e-05,
      "loss": 1.8927,
      "step": 19810
    },
    {
      "epoch": 1.9926104659930628,
      "grad_norm": 1.7996710538864136,
      "learning_rate": 1.0116337715611447e-05,
      "loss": 1.7943,
      "step": 19820
    },
    {
      "epoch": 1.993615844769517,
      "grad_norm": 2.438270330429077,
      "learning_rate": 1.0106250630442823e-05,
      "loss": 1.8167,
      "step": 19830
    },
    {
      "epoch": 1.994621223545971,
      "grad_norm": 2.243076801300049,
      "learning_rate": 1.0096163545274201e-05,
      "loss": 1.7312,
      "step": 19840
    },
    {
      "epoch": 1.995626602322425,
      "grad_norm": 2.4719491004943848,
      "learning_rate": 1.0086076460105579e-05,
      "loss": 1.7206,
      "step": 19850
    },
    {
      "epoch": 1.9966319810988788,
      "grad_norm": 2.4505786895751953,
      "learning_rate": 1.0075989374936955e-05,
      "loss": 1.7685,
      "step": 19860
    },
    {
      "epoch": 1.9976373598753332,
      "grad_norm": 1.8833750486373901,
      "learning_rate": 1.0065902289768333e-05,
      "loss": 1.7525,
      "step": 19870
    },
    {
      "epoch": 1.998642738651787,
      "grad_norm": 1.9172894954681396,
      "learning_rate": 1.0055815204599711e-05,
      "loss": 1.8533,
      "step": 19880
    },
    {
      "epoch": 1.999648117428241,
      "grad_norm": 2.4324395656585693,
      "learning_rate": 1.0045728119431088e-05,
      "loss": 1.7735,
      "step": 19890
    },
    {
      "epoch": 2.0006032272658723,
      "grad_norm": 2.1381561756134033,
      "learning_rate": 1.0035641034262466e-05,
      "loss": 1.7855,
      "step": 19900
    },
    {
      "epoch": 2.0016086060423266,
      "grad_norm": 1.6049660444259644,
      "learning_rate": 1.0025553949093844e-05,
      "loss": 1.7898,
      "step": 19910
    },
    {
      "epoch": 2.0026139848187805,
      "grad_norm": 1.9050407409667969,
      "learning_rate": 1.001546686392522e-05,
      "loss": 1.7786,
      "step": 19920
    },
    {
      "epoch": 2.0036193635952344,
      "grad_norm": 1.7321195602416992,
      "learning_rate": 1.00053797787566e-05,
      "loss": 1.6983,
      "step": 19930
    },
    {
      "epoch": 2.0046247423716888,
      "grad_norm": 1.8559852838516235,
      "learning_rate": 9.995292693587978e-06,
      "loss": 1.7858,
      "step": 19940
    },
    {
      "epoch": 2.0056301211481427,
      "grad_norm": 2.161604166030884,
      "learning_rate": 9.985205608419354e-06,
      "loss": 1.7566,
      "step": 19950
    },
    {
      "epoch": 2.0066354999245966,
      "grad_norm": 2.2760796546936035,
      "learning_rate": 9.975118523250732e-06,
      "loss": 1.8033,
      "step": 19960
    },
    {
      "epoch": 2.0076408787010505,
      "grad_norm": 2.093167304992676,
      "learning_rate": 9.96503143808211e-06,
      "loss": 1.8063,
      "step": 19970
    },
    {
      "epoch": 2.008646257477505,
      "grad_norm": 1.9353300333023071,
      "learning_rate": 9.954944352913486e-06,
      "loss": 1.8561,
      "step": 19980
    },
    {
      "epoch": 2.0096516362539587,
      "grad_norm": 2.76849365234375,
      "learning_rate": 9.944857267744864e-06,
      "loss": 1.7617,
      "step": 19990
    },
    {
      "epoch": 2.0106570150304126,
      "grad_norm": 1.8086637258529663,
      "learning_rate": 9.934770182576242e-06,
      "loss": 1.7722,
      "step": 20000
    },
    {
      "epoch": 2.011662393806867,
      "grad_norm": 1.7244255542755127,
      "learning_rate": 9.924683097407619e-06,
      "loss": 1.8392,
      "step": 20010
    },
    {
      "epoch": 2.012667772583321,
      "grad_norm": 1.7738924026489258,
      "learning_rate": 9.914596012238997e-06,
      "loss": 1.7839,
      "step": 20020
    },
    {
      "epoch": 2.0136731513597748,
      "grad_norm": 2.0997369289398193,
      "learning_rate": 9.904508927070375e-06,
      "loss": 1.7199,
      "step": 20030
    },
    {
      "epoch": 2.0146785301362287,
      "grad_norm": 2.394824266433716,
      "learning_rate": 9.894421841901751e-06,
      "loss": 1.7655,
      "step": 20040
    },
    {
      "epoch": 2.015683908912683,
      "grad_norm": 1.8709295988082886,
      "learning_rate": 9.884334756733129e-06,
      "loss": 1.7927,
      "step": 20050
    },
    {
      "epoch": 2.016689287689137,
      "grad_norm": 2.56097412109375,
      "learning_rate": 9.874247671564507e-06,
      "loss": 1.777,
      "step": 20060
    },
    {
      "epoch": 2.017694666465591,
      "grad_norm": 2.1431610584259033,
      "learning_rate": 9.864160586395883e-06,
      "loss": 1.7338,
      "step": 20070
    },
    {
      "epoch": 2.018700045242045,
      "grad_norm": 2.021583080291748,
      "learning_rate": 9.854073501227261e-06,
      "loss": 1.7434,
      "step": 20080
    },
    {
      "epoch": 2.019705424018499,
      "grad_norm": 2.2814037799835205,
      "learning_rate": 9.843986416058641e-06,
      "loss": 1.7964,
      "step": 20090
    },
    {
      "epoch": 2.020710802794953,
      "grad_norm": 2.3041906356811523,
      "learning_rate": 9.833899330890017e-06,
      "loss": 1.8216,
      "step": 20100
    },
    {
      "epoch": 2.021716181571407,
      "grad_norm": 2.3677589893341064,
      "learning_rate": 9.823812245721395e-06,
      "loss": 1.8128,
      "step": 20110
    },
    {
      "epoch": 2.022721560347861,
      "grad_norm": 1.7341357469558716,
      "learning_rate": 9.813725160552773e-06,
      "loss": 1.6889,
      "step": 20120
    },
    {
      "epoch": 2.023726939124315,
      "grad_norm": 2.5517678260803223,
      "learning_rate": 9.80363807538415e-06,
      "loss": 1.737,
      "step": 20130
    },
    {
      "epoch": 2.024732317900769,
      "grad_norm": 2.5869102478027344,
      "learning_rate": 9.793550990215528e-06,
      "loss": 1.7226,
      "step": 20140
    },
    {
      "epoch": 2.025737696677223,
      "grad_norm": 2.218358278274536,
      "learning_rate": 9.783463905046906e-06,
      "loss": 1.744,
      "step": 20150
    },
    {
      "epoch": 2.0267430754536773,
      "grad_norm": 2.137760877609253,
      "learning_rate": 9.773376819878284e-06,
      "loss": 1.8154,
      "step": 20160
    },
    {
      "epoch": 2.027748454230131,
      "grad_norm": 3.4330389499664307,
      "learning_rate": 9.76328973470966e-06,
      "loss": 1.6836,
      "step": 20170
    },
    {
      "epoch": 2.028753833006585,
      "grad_norm": 2.1369574069976807,
      "learning_rate": 9.753202649541038e-06,
      "loss": 1.8071,
      "step": 20180
    },
    {
      "epoch": 2.0297592117830394,
      "grad_norm": 2.3911666870117188,
      "learning_rate": 9.743115564372416e-06,
      "loss": 1.7379,
      "step": 20190
    },
    {
      "epoch": 2.0307645905594933,
      "grad_norm": 1.7122970819473267,
      "learning_rate": 9.733028479203792e-06,
      "loss": 1.8463,
      "step": 20200
    },
    {
      "epoch": 2.0317699693359472,
      "grad_norm": 2.69169545173645,
      "learning_rate": 9.72294139403517e-06,
      "loss": 1.7912,
      "step": 20210
    },
    {
      "epoch": 2.032775348112401,
      "grad_norm": 1.8298941850662231,
      "learning_rate": 9.712854308866548e-06,
      "loss": 1.7784,
      "step": 20220
    },
    {
      "epoch": 2.0337807268888555,
      "grad_norm": 1.4890795946121216,
      "learning_rate": 9.702767223697925e-06,
      "loss": 1.8217,
      "step": 20230
    },
    {
      "epoch": 2.0347861056653094,
      "grad_norm": 2.0687990188598633,
      "learning_rate": 9.692680138529304e-06,
      "loss": 1.7324,
      "step": 20240
    },
    {
      "epoch": 2.0357914844417633,
      "grad_norm": 2.5002334117889404,
      "learning_rate": 9.682593053360682e-06,
      "loss": 1.693,
      "step": 20250
    },
    {
      "epoch": 2.0367968632182176,
      "grad_norm": 1.4502081871032715,
      "learning_rate": 9.672505968192059e-06,
      "loss": 1.7565,
      "step": 20260
    },
    {
      "epoch": 2.0378022419946715,
      "grad_norm": 2.058001756668091,
      "learning_rate": 9.662418883023437e-06,
      "loss": 1.7718,
      "step": 20270
    },
    {
      "epoch": 2.0388076207711254,
      "grad_norm": 2.142857074737549,
      "learning_rate": 9.652331797854815e-06,
      "loss": 1.8964,
      "step": 20280
    },
    {
      "epoch": 2.0398129995475793,
      "grad_norm": 1.8091192245483398,
      "learning_rate": 9.642244712686191e-06,
      "loss": 1.8552,
      "step": 20290
    },
    {
      "epoch": 2.0408183783240337,
      "grad_norm": 3.0161657333374023,
      "learning_rate": 9.632157627517569e-06,
      "loss": 1.753,
      "step": 20300
    },
    {
      "epoch": 2.0418237571004876,
      "grad_norm": 2.805729389190674,
      "learning_rate": 9.622070542348947e-06,
      "loss": 1.7417,
      "step": 20310
    },
    {
      "epoch": 2.0428291358769415,
      "grad_norm": 2.348142623901367,
      "learning_rate": 9.611983457180323e-06,
      "loss": 1.6926,
      "step": 20320
    },
    {
      "epoch": 2.043834514653396,
      "grad_norm": 2.2139675617218018,
      "learning_rate": 9.601896372011701e-06,
      "loss": 1.7066,
      "step": 20330
    },
    {
      "epoch": 2.0448398934298497,
      "grad_norm": 1.871466875076294,
      "learning_rate": 9.59180928684308e-06,
      "loss": 1.6906,
      "step": 20340
    },
    {
      "epoch": 2.0458452722063036,
      "grad_norm": 2.1083881855010986,
      "learning_rate": 9.581722201674456e-06,
      "loss": 1.7577,
      "step": 20350
    },
    {
      "epoch": 2.0468506509827575,
      "grad_norm": 2.6588926315307617,
      "learning_rate": 9.571635116505834e-06,
      "loss": 1.8654,
      "step": 20360
    },
    {
      "epoch": 2.047856029759212,
      "grad_norm": 1.8743705749511719,
      "learning_rate": 9.561548031337212e-06,
      "loss": 1.776,
      "step": 20370
    },
    {
      "epoch": 2.048861408535666,
      "grad_norm": 2.809412717819214,
      "learning_rate": 9.551460946168588e-06,
      "loss": 1.7597,
      "step": 20380
    },
    {
      "epoch": 2.0498667873121197,
      "grad_norm": 1.6570460796356201,
      "learning_rate": 9.541373860999968e-06,
      "loss": 1.7706,
      "step": 20390
    },
    {
      "epoch": 2.050872166088574,
      "grad_norm": 2.185887336730957,
      "learning_rate": 9.531286775831346e-06,
      "loss": 1.8071,
      "step": 20400
    },
    {
      "epoch": 2.051877544865028,
      "grad_norm": 2.9246749877929688,
      "learning_rate": 9.521199690662722e-06,
      "loss": 1.7725,
      "step": 20410
    },
    {
      "epoch": 2.052882923641482,
      "grad_norm": 2.8133902549743652,
      "learning_rate": 9.5111126054941e-06,
      "loss": 1.7091,
      "step": 20420
    },
    {
      "epoch": 2.0538883024179357,
      "grad_norm": 3.4212706089019775,
      "learning_rate": 9.501025520325478e-06,
      "loss": 1.7522,
      "step": 20430
    },
    {
      "epoch": 2.05489368119439,
      "grad_norm": 2.1726253032684326,
      "learning_rate": 9.490938435156854e-06,
      "loss": 1.7896,
      "step": 20440
    },
    {
      "epoch": 2.055899059970844,
      "grad_norm": 1.9035348892211914,
      "learning_rate": 9.480851349988232e-06,
      "loss": 1.77,
      "step": 20450
    },
    {
      "epoch": 2.056904438747298,
      "grad_norm": 1.9918886423110962,
      "learning_rate": 9.47076426481961e-06,
      "loss": 1.8842,
      "step": 20460
    },
    {
      "epoch": 2.0579098175237522,
      "grad_norm": 1.6505115032196045,
      "learning_rate": 9.460677179650987e-06,
      "loss": 1.7204,
      "step": 20470
    },
    {
      "epoch": 2.058915196300206,
      "grad_norm": 1.7019810676574707,
      "learning_rate": 9.450590094482365e-06,
      "loss": 1.7097,
      "step": 20480
    },
    {
      "epoch": 2.05992057507666,
      "grad_norm": 2.122239112854004,
      "learning_rate": 9.440503009313743e-06,
      "loss": 1.7617,
      "step": 20490
    },
    {
      "epoch": 2.060925953853114,
      "grad_norm": 1.7798184156417847,
      "learning_rate": 9.430415924145119e-06,
      "loss": 1.688,
      "step": 20500
    },
    {
      "epoch": 2.0619313326295683,
      "grad_norm": 3.35170841217041,
      "learning_rate": 9.420328838976497e-06,
      "loss": 1.6804,
      "step": 20510
    },
    {
      "epoch": 2.062936711406022,
      "grad_norm": 2.2181501388549805,
      "learning_rate": 9.410241753807875e-06,
      "loss": 1.8062,
      "step": 20520
    },
    {
      "epoch": 2.063942090182476,
      "grad_norm": 1.8788942098617554,
      "learning_rate": 9.400154668639251e-06,
      "loss": 1.7724,
      "step": 20530
    },
    {
      "epoch": 2.0649474689589304,
      "grad_norm": 2.015895366668701,
      "learning_rate": 9.390067583470631e-06,
      "loss": 1.8301,
      "step": 20540
    },
    {
      "epoch": 2.0659528477353843,
      "grad_norm": 2.560744047164917,
      "learning_rate": 9.379980498302009e-06,
      "loss": 1.757,
      "step": 20550
    },
    {
      "epoch": 2.0669582265118382,
      "grad_norm": 1.8441771268844604,
      "learning_rate": 9.369893413133385e-06,
      "loss": 1.7995,
      "step": 20560
    },
    {
      "epoch": 2.067963605288292,
      "grad_norm": 1.858530879020691,
      "learning_rate": 9.359806327964763e-06,
      "loss": 1.7818,
      "step": 20570
    },
    {
      "epoch": 2.0689689840647465,
      "grad_norm": 1.9558916091918945,
      "learning_rate": 9.349719242796141e-06,
      "loss": 1.8214,
      "step": 20580
    },
    {
      "epoch": 2.0699743628412004,
      "grad_norm": 2.9117212295532227,
      "learning_rate": 9.339632157627518e-06,
      "loss": 1.7,
      "step": 20590
    },
    {
      "epoch": 2.0709797416176543,
      "grad_norm": 2.0607550144195557,
      "learning_rate": 9.329545072458896e-06,
      "loss": 1.7657,
      "step": 20600
    },
    {
      "epoch": 2.0719851203941086,
      "grad_norm": 2.0142533779144287,
      "learning_rate": 9.319457987290274e-06,
      "loss": 1.6718,
      "step": 20610
    },
    {
      "epoch": 2.0729904991705626,
      "grad_norm": 1.9982154369354248,
      "learning_rate": 9.30937090212165e-06,
      "loss": 1.8518,
      "step": 20620
    },
    {
      "epoch": 2.0739958779470165,
      "grad_norm": 2.831970453262329,
      "learning_rate": 9.299283816953028e-06,
      "loss": 1.7231,
      "step": 20630
    },
    {
      "epoch": 2.0750012567234704,
      "grad_norm": 2.0274312496185303,
      "learning_rate": 9.289196731784406e-06,
      "loss": 1.7919,
      "step": 20640
    },
    {
      "epoch": 2.0760066354999247,
      "grad_norm": 1.9717907905578613,
      "learning_rate": 9.279109646615782e-06,
      "loss": 1.7658,
      "step": 20650
    },
    {
      "epoch": 2.0770120142763786,
      "grad_norm": 2.214594841003418,
      "learning_rate": 9.26902256144716e-06,
      "loss": 1.7869,
      "step": 20660
    },
    {
      "epoch": 2.0780173930528325,
      "grad_norm": 1.9509769678115845,
      "learning_rate": 9.258935476278538e-06,
      "loss": 1.7657,
      "step": 20670
    },
    {
      "epoch": 2.079022771829287,
      "grad_norm": 3.0488765239715576,
      "learning_rate": 9.248848391109915e-06,
      "loss": 1.7703,
      "step": 20680
    },
    {
      "epoch": 2.0800281506057408,
      "grad_norm": 1.9357579946517944,
      "learning_rate": 9.238761305941294e-06,
      "loss": 1.7355,
      "step": 20690
    },
    {
      "epoch": 2.0810335293821947,
      "grad_norm": 1.9666955471038818,
      "learning_rate": 9.228674220772672e-06,
      "loss": 1.7375,
      "step": 20700
    },
    {
      "epoch": 2.0820389081586486,
      "grad_norm": 1.9449416399002075,
      "learning_rate": 9.218587135604049e-06,
      "loss": 1.7343,
      "step": 20710
    },
    {
      "epoch": 2.083044286935103,
      "grad_norm": 2.263061285018921,
      "learning_rate": 9.208500050435427e-06,
      "loss": 1.8251,
      "step": 20720
    },
    {
      "epoch": 2.084049665711557,
      "grad_norm": 1.7635984420776367,
      "learning_rate": 9.198412965266805e-06,
      "loss": 1.7605,
      "step": 20730
    },
    {
      "epoch": 2.0850550444880107,
      "grad_norm": 1.8752325773239136,
      "learning_rate": 9.188325880098181e-06,
      "loss": 1.7871,
      "step": 20740
    },
    {
      "epoch": 2.086060423264465,
      "grad_norm": 1.9074310064315796,
      "learning_rate": 9.178238794929559e-06,
      "loss": 1.7951,
      "step": 20750
    },
    {
      "epoch": 2.087065802040919,
      "grad_norm": 2.1245176792144775,
      "learning_rate": 9.168151709760937e-06,
      "loss": 1.7687,
      "step": 20760
    },
    {
      "epoch": 2.088071180817373,
      "grad_norm": 1.9491970539093018,
      "learning_rate": 9.158064624592313e-06,
      "loss": 1.8405,
      "step": 20770
    },
    {
      "epoch": 2.0890765595938268,
      "grad_norm": 2.1418139934539795,
      "learning_rate": 9.147977539423691e-06,
      "loss": 1.7808,
      "step": 20780
    },
    {
      "epoch": 2.090081938370281,
      "grad_norm": 1.7003896236419678,
      "learning_rate": 9.13789045425507e-06,
      "loss": 1.8038,
      "step": 20790
    },
    {
      "epoch": 2.091087317146735,
      "grad_norm": 1.9300020933151245,
      "learning_rate": 9.127803369086446e-06,
      "loss": 1.7532,
      "step": 20800
    },
    {
      "epoch": 2.092092695923189,
      "grad_norm": 2.637960433959961,
      "learning_rate": 9.117716283917824e-06,
      "loss": 1.6934,
      "step": 20810
    },
    {
      "epoch": 2.0930980746996433,
      "grad_norm": 1.6198320388793945,
      "learning_rate": 9.107629198749202e-06,
      "loss": 1.6662,
      "step": 20820
    },
    {
      "epoch": 2.094103453476097,
      "grad_norm": 1.9355672597885132,
      "learning_rate": 9.097542113580578e-06,
      "loss": 1.7275,
      "step": 20830
    },
    {
      "epoch": 2.095108832252551,
      "grad_norm": 2.465646743774414,
      "learning_rate": 9.087455028411956e-06,
      "loss": 1.6865,
      "step": 20840
    },
    {
      "epoch": 2.096114211029005,
      "grad_norm": 2.1645426750183105,
      "learning_rate": 9.077367943243336e-06,
      "loss": 1.6481,
      "step": 20850
    },
    {
      "epoch": 2.0971195898054593,
      "grad_norm": 2.3518738746643066,
      "learning_rate": 9.067280858074712e-06,
      "loss": 1.7706,
      "step": 20860
    },
    {
      "epoch": 2.098124968581913,
      "grad_norm": 2.488812208175659,
      "learning_rate": 9.05719377290609e-06,
      "loss": 1.7696,
      "step": 20870
    },
    {
      "epoch": 2.099130347358367,
      "grad_norm": 1.7736701965332031,
      "learning_rate": 9.047106687737468e-06,
      "loss": 1.7728,
      "step": 20880
    },
    {
      "epoch": 2.1001357261348215,
      "grad_norm": 1.9768147468566895,
      "learning_rate": 9.037019602568844e-06,
      "loss": 1.8076,
      "step": 20890
    },
    {
      "epoch": 2.1011411049112754,
      "grad_norm": 2.0441336631774902,
      "learning_rate": 9.026932517400222e-06,
      "loss": 1.6978,
      "step": 20900
    },
    {
      "epoch": 2.1021464836877293,
      "grad_norm": 1.7705880403518677,
      "learning_rate": 9.0168454322316e-06,
      "loss": 1.7055,
      "step": 20910
    },
    {
      "epoch": 2.103151862464183,
      "grad_norm": 1.7641866207122803,
      "learning_rate": 9.006758347062977e-06,
      "loss": 1.7299,
      "step": 20920
    },
    {
      "epoch": 2.1041572412406375,
      "grad_norm": 1.9975131750106812,
      "learning_rate": 8.996671261894355e-06,
      "loss": 1.721,
      "step": 20930
    },
    {
      "epoch": 2.1051626200170914,
      "grad_norm": 2.5393307209014893,
      "learning_rate": 8.986584176725733e-06,
      "loss": 1.7509,
      "step": 20940
    },
    {
      "epoch": 2.1061679987935453,
      "grad_norm": 1.8431905508041382,
      "learning_rate": 8.976497091557109e-06,
      "loss": 1.8063,
      "step": 20950
    },
    {
      "epoch": 2.1071733775699997,
      "grad_norm": 1.9392471313476562,
      "learning_rate": 8.966410006388487e-06,
      "loss": 1.8071,
      "step": 20960
    },
    {
      "epoch": 2.1081787563464536,
      "grad_norm": 2.4866750240325928,
      "learning_rate": 8.956322921219865e-06,
      "loss": 1.7387,
      "step": 20970
    },
    {
      "epoch": 2.1091841351229075,
      "grad_norm": 2.1890931129455566,
      "learning_rate": 8.946235836051241e-06,
      "loss": 1.7818,
      "step": 20980
    },
    {
      "epoch": 2.1101895138993614,
      "grad_norm": 1.6440082788467407,
      "learning_rate": 8.93614875088262e-06,
      "loss": 1.777,
      "step": 20990
    },
    {
      "epoch": 2.1111948926758157,
      "grad_norm": 1.86918044090271,
      "learning_rate": 8.926061665713999e-06,
      "loss": 1.7537,
      "step": 21000
    },
    {
      "epoch": 2.1122002714522696,
      "grad_norm": 1.8902610540390015,
      "learning_rate": 8.915974580545375e-06,
      "loss": 1.7523,
      "step": 21010
    },
    {
      "epoch": 2.1132056502287235,
      "grad_norm": 1.9385024309158325,
      "learning_rate": 8.905887495376753e-06,
      "loss": 1.7576,
      "step": 21020
    },
    {
      "epoch": 2.114211029005178,
      "grad_norm": 2.0679333209991455,
      "learning_rate": 8.895800410208131e-06,
      "loss": 1.8206,
      "step": 21030
    },
    {
      "epoch": 2.115216407781632,
      "grad_norm": 2.295196533203125,
      "learning_rate": 8.885713325039508e-06,
      "loss": 1.7206,
      "step": 21040
    },
    {
      "epoch": 2.1162217865580857,
      "grad_norm": 1.6681357622146606,
      "learning_rate": 8.875626239870886e-06,
      "loss": 1.7607,
      "step": 21050
    },
    {
      "epoch": 2.1172271653345396,
      "grad_norm": 1.8164397478103638,
      "learning_rate": 8.865539154702264e-06,
      "loss": 1.7328,
      "step": 21060
    },
    {
      "epoch": 2.118232544110994,
      "grad_norm": 2.2346229553222656,
      "learning_rate": 8.85545206953364e-06,
      "loss": 1.6847,
      "step": 21070
    },
    {
      "epoch": 2.119237922887448,
      "grad_norm": 2.3532943725585938,
      "learning_rate": 8.845364984365018e-06,
      "loss": 1.6518,
      "step": 21080
    },
    {
      "epoch": 2.1202433016639017,
      "grad_norm": 2.3350374698638916,
      "learning_rate": 8.835277899196396e-06,
      "loss": 1.761,
      "step": 21090
    },
    {
      "epoch": 2.121248680440356,
      "grad_norm": 2.5506911277770996,
      "learning_rate": 8.825190814027772e-06,
      "loss": 1.665,
      "step": 21100
    },
    {
      "epoch": 2.12225405921681,
      "grad_norm": 1.879258632659912,
      "learning_rate": 8.81510372885915e-06,
      "loss": 1.8098,
      "step": 21110
    },
    {
      "epoch": 2.123259437993264,
      "grad_norm": 2.0782551765441895,
      "learning_rate": 8.805016643690528e-06,
      "loss": 1.6887,
      "step": 21120
    },
    {
      "epoch": 2.124264816769718,
      "grad_norm": 2.2690017223358154,
      "learning_rate": 8.794929558521905e-06,
      "loss": 1.7791,
      "step": 21130
    },
    {
      "epoch": 2.125270195546172,
      "grad_norm": 2.8006985187530518,
      "learning_rate": 8.784842473353283e-06,
      "loss": 1.7602,
      "step": 21140
    },
    {
      "epoch": 2.126275574322626,
      "grad_norm": 2.1942553520202637,
      "learning_rate": 8.774755388184662e-06,
      "loss": 1.7703,
      "step": 21150
    },
    {
      "epoch": 2.12728095309908,
      "grad_norm": 2.313657522201538,
      "learning_rate": 8.764668303016039e-06,
      "loss": 1.7508,
      "step": 21160
    },
    {
      "epoch": 2.1282863318755343,
      "grad_norm": 1.8422828912734985,
      "learning_rate": 8.754581217847417e-06,
      "loss": 1.7239,
      "step": 21170
    },
    {
      "epoch": 2.129291710651988,
      "grad_norm": 2.8878777027130127,
      "learning_rate": 8.744494132678795e-06,
      "loss": 1.7055,
      "step": 21180
    },
    {
      "epoch": 2.130297089428442,
      "grad_norm": 2.7598023414611816,
      "learning_rate": 8.734407047510171e-06,
      "loss": 1.781,
      "step": 21190
    },
    {
      "epoch": 2.131302468204896,
      "grad_norm": 2.1770403385162354,
      "learning_rate": 8.724319962341549e-06,
      "loss": 1.7911,
      "step": 21200
    },
    {
      "epoch": 2.1323078469813503,
      "grad_norm": 2.231781244277954,
      "learning_rate": 8.714232877172927e-06,
      "loss": 1.7952,
      "step": 21210
    },
    {
      "epoch": 2.1333132257578042,
      "grad_norm": 2.502087354660034,
      "learning_rate": 8.704145792004303e-06,
      "loss": 1.8695,
      "step": 21220
    },
    {
      "epoch": 2.134318604534258,
      "grad_norm": 2.0809834003448486,
      "learning_rate": 8.694058706835681e-06,
      "loss": 1.6942,
      "step": 21230
    },
    {
      "epoch": 2.1353239833107125,
      "grad_norm": 2.5295495986938477,
      "learning_rate": 8.68397162166706e-06,
      "loss": 1.7242,
      "step": 21240
    },
    {
      "epoch": 2.1363293620871664,
      "grad_norm": 1.8209633827209473,
      "learning_rate": 8.673884536498436e-06,
      "loss": 1.7437,
      "step": 21250
    },
    {
      "epoch": 2.1373347408636203,
      "grad_norm": 1.8428542613983154,
      "learning_rate": 8.663797451329814e-06,
      "loss": 1.8031,
      "step": 21260
    },
    {
      "epoch": 2.138340119640074,
      "grad_norm": 2.227004051208496,
      "learning_rate": 8.653710366161192e-06,
      "loss": 1.6921,
      "step": 21270
    },
    {
      "epoch": 2.1393454984165285,
      "grad_norm": 1.941362977027893,
      "learning_rate": 8.643623280992568e-06,
      "loss": 1.8005,
      "step": 21280
    },
    {
      "epoch": 2.1403508771929824,
      "grad_norm": 2.364626884460449,
      "learning_rate": 8.633536195823946e-06,
      "loss": 1.7788,
      "step": 21290
    },
    {
      "epoch": 2.1413562559694364,
      "grad_norm": 1.5681393146514893,
      "learning_rate": 8.623449110655326e-06,
      "loss": 1.6575,
      "step": 21300
    },
    {
      "epoch": 2.1423616347458907,
      "grad_norm": 2.4659323692321777,
      "learning_rate": 8.613362025486702e-06,
      "loss": 1.6961,
      "step": 21310
    },
    {
      "epoch": 2.1433670135223446,
      "grad_norm": 1.941394567489624,
      "learning_rate": 8.60327494031808e-06,
      "loss": 1.7838,
      "step": 21320
    },
    {
      "epoch": 2.1443723922987985,
      "grad_norm": 2.232038974761963,
      "learning_rate": 8.593187855149458e-06,
      "loss": 1.7874,
      "step": 21330
    },
    {
      "epoch": 2.1453777710752524,
      "grad_norm": 2.6925806999206543,
      "learning_rate": 8.583100769980834e-06,
      "loss": 1.859,
      "step": 21340
    },
    {
      "epoch": 2.1463831498517068,
      "grad_norm": 1.720206618309021,
      "learning_rate": 8.573013684812212e-06,
      "loss": 1.7224,
      "step": 21350
    },
    {
      "epoch": 2.1473885286281607,
      "grad_norm": 1.8691778182983398,
      "learning_rate": 8.56292659964359e-06,
      "loss": 1.7913,
      "step": 21360
    },
    {
      "epoch": 2.1483939074046146,
      "grad_norm": 2.73649001121521,
      "learning_rate": 8.552839514474968e-06,
      "loss": 1.7619,
      "step": 21370
    },
    {
      "epoch": 2.149399286181069,
      "grad_norm": 2.5660791397094727,
      "learning_rate": 8.542752429306345e-06,
      "loss": 1.7551,
      "step": 21380
    },
    {
      "epoch": 2.150404664957523,
      "grad_norm": 1.7980366945266724,
      "learning_rate": 8.532665344137723e-06,
      "loss": 1.7101,
      "step": 21390
    },
    {
      "epoch": 2.1514100437339767,
      "grad_norm": 2.0755724906921387,
      "learning_rate": 8.5225782589691e-06,
      "loss": 1.7418,
      "step": 21400
    },
    {
      "epoch": 2.1524154225104306,
      "grad_norm": 1.917846918106079,
      "learning_rate": 8.512491173800477e-06,
      "loss": 1.7416,
      "step": 21410
    },
    {
      "epoch": 2.153420801286885,
      "grad_norm": 2.4469258785247803,
      "learning_rate": 8.502404088631855e-06,
      "loss": 1.6719,
      "step": 21420
    },
    {
      "epoch": 2.154426180063339,
      "grad_norm": 1.8490344285964966,
      "learning_rate": 8.492317003463233e-06,
      "loss": 1.6937,
      "step": 21430
    },
    {
      "epoch": 2.1554315588397928,
      "grad_norm": 2.1968586444854736,
      "learning_rate": 8.48222991829461e-06,
      "loss": 1.7669,
      "step": 21440
    },
    {
      "epoch": 2.156436937616247,
      "grad_norm": 2.1970975399017334,
      "learning_rate": 8.472142833125989e-06,
      "loss": 1.7295,
      "step": 21450
    },
    {
      "epoch": 2.157442316392701,
      "grad_norm": 1.783897042274475,
      "learning_rate": 8.462055747957367e-06,
      "loss": 1.7437,
      "step": 21460
    },
    {
      "epoch": 2.158447695169155,
      "grad_norm": 2.1141977310180664,
      "learning_rate": 8.451968662788743e-06,
      "loss": 1.6524,
      "step": 21470
    },
    {
      "epoch": 2.159453073945609,
      "grad_norm": 2.407369613647461,
      "learning_rate": 8.441881577620121e-06,
      "loss": 1.7256,
      "step": 21480
    },
    {
      "epoch": 2.160458452722063,
      "grad_norm": 1.9270697832107544,
      "learning_rate": 8.4317944924515e-06,
      "loss": 1.7407,
      "step": 21490
    },
    {
      "epoch": 2.161463831498517,
      "grad_norm": 1.563111662864685,
      "learning_rate": 8.421707407282876e-06,
      "loss": 1.8073,
      "step": 21500
    },
    {
      "epoch": 2.162469210274971,
      "grad_norm": 1.7105090618133545,
      "learning_rate": 8.411620322114254e-06,
      "loss": 1.7675,
      "step": 21510
    },
    {
      "epoch": 2.1634745890514253,
      "grad_norm": 2.0262093544006348,
      "learning_rate": 8.401533236945632e-06,
      "loss": 1.7677,
      "step": 21520
    },
    {
      "epoch": 2.164479967827879,
      "grad_norm": 1.890944004058838,
      "learning_rate": 8.391446151777008e-06,
      "loss": 1.7707,
      "step": 21530
    },
    {
      "epoch": 2.165485346604333,
      "grad_norm": 1.8739806413650513,
      "learning_rate": 8.382367775125248e-06,
      "loss": 1.8266,
      "step": 21540
    },
    {
      "epoch": 2.166490725380787,
      "grad_norm": 1.9084240198135376,
      "learning_rate": 8.372280689956625e-06,
      "loss": 1.7511,
      "step": 21550
    },
    {
      "epoch": 2.1674961041572414,
      "grad_norm": 1.7420437335968018,
      "learning_rate": 8.362193604788003e-06,
      "loss": 1.8113,
      "step": 21560
    },
    {
      "epoch": 2.1685014829336953,
      "grad_norm": 1.8543314933776855,
      "learning_rate": 8.35210651961938e-06,
      "loss": 1.7863,
      "step": 21570
    },
    {
      "epoch": 2.169506861710149,
      "grad_norm": 2.5571343898773193,
      "learning_rate": 8.342019434450757e-06,
      "loss": 1.7511,
      "step": 21580
    },
    {
      "epoch": 2.1705122404866035,
      "grad_norm": 1.6578408479690552,
      "learning_rate": 8.331932349282137e-06,
      "loss": 1.8268,
      "step": 21590
    },
    {
      "epoch": 2.1715176192630574,
      "grad_norm": 2.077228307723999,
      "learning_rate": 8.321845264113515e-06,
      "loss": 1.6778,
      "step": 21600
    },
    {
      "epoch": 2.1725229980395113,
      "grad_norm": 1.9429939985275269,
      "learning_rate": 8.311758178944891e-06,
      "loss": 1.7526,
      "step": 21610
    },
    {
      "epoch": 2.1735283768159652,
      "grad_norm": 2.2527389526367188,
      "learning_rate": 8.301671093776269e-06,
      "loss": 1.6881,
      "step": 21620
    },
    {
      "epoch": 2.1745337555924196,
      "grad_norm": 1.7550320625305176,
      "learning_rate": 8.291584008607647e-06,
      "loss": 1.7862,
      "step": 21630
    },
    {
      "epoch": 2.1755391343688735,
      "grad_norm": 1.9681264162063599,
      "learning_rate": 8.281496923439023e-06,
      "loss": 1.7965,
      "step": 21640
    },
    {
      "epoch": 2.1765445131453274,
      "grad_norm": 2.270267963409424,
      "learning_rate": 8.271409838270401e-06,
      "loss": 1.7496,
      "step": 21650
    },
    {
      "epoch": 2.1775498919217817,
      "grad_norm": 2.4535512924194336,
      "learning_rate": 8.26132275310178e-06,
      "loss": 1.7683,
      "step": 21660
    },
    {
      "epoch": 2.1785552706982356,
      "grad_norm": 2.1935133934020996,
      "learning_rate": 8.251235667933156e-06,
      "loss": 1.7124,
      "step": 21670
    },
    {
      "epoch": 2.1795606494746895,
      "grad_norm": 2.2539260387420654,
      "learning_rate": 8.241148582764534e-06,
      "loss": 1.6894,
      "step": 21680
    },
    {
      "epoch": 2.1805660282511434,
      "grad_norm": 2.002634048461914,
      "learning_rate": 8.231061497595912e-06,
      "loss": 1.7292,
      "step": 21690
    },
    {
      "epoch": 2.1815714070275978,
      "grad_norm": 2.1245319843292236,
      "learning_rate": 8.220974412427288e-06,
      "loss": 1.7634,
      "step": 21700
    },
    {
      "epoch": 2.1825767858040517,
      "grad_norm": 2.01487135887146,
      "learning_rate": 8.210887327258666e-06,
      "loss": 1.8269,
      "step": 21710
    },
    {
      "epoch": 2.1835821645805056,
      "grad_norm": 1.7412738800048828,
      "learning_rate": 8.200800242090044e-06,
      "loss": 1.6748,
      "step": 21720
    },
    {
      "epoch": 2.18458754335696,
      "grad_norm": 1.8523212671279907,
      "learning_rate": 8.19071315692142e-06,
      "loss": 1.723,
      "step": 21730
    },
    {
      "epoch": 2.185592922133414,
      "grad_norm": 1.8642776012420654,
      "learning_rate": 8.1806260717528e-06,
      "loss": 1.7797,
      "step": 21740
    },
    {
      "epoch": 2.1865983009098677,
      "grad_norm": 2.314974546432495,
      "learning_rate": 8.170538986584178e-06,
      "loss": 1.8529,
      "step": 21750
    },
    {
      "epoch": 2.1876036796863216,
      "grad_norm": 2.47969913482666,
      "learning_rate": 8.160451901415554e-06,
      "loss": 1.759,
      "step": 21760
    },
    {
      "epoch": 2.188609058462776,
      "grad_norm": 2.3432414531707764,
      "learning_rate": 8.150364816246932e-06,
      "loss": 1.7129,
      "step": 21770
    },
    {
      "epoch": 2.18961443723923,
      "grad_norm": 1.5546903610229492,
      "learning_rate": 8.14027773107831e-06,
      "loss": 1.7935,
      "step": 21780
    },
    {
      "epoch": 2.190619816015684,
      "grad_norm": 2.1789188385009766,
      "learning_rate": 8.130190645909687e-06,
      "loss": 1.7043,
      "step": 21790
    },
    {
      "epoch": 2.191625194792138,
      "grad_norm": 1.945982575416565,
      "learning_rate": 8.120103560741065e-06,
      "loss": 1.8337,
      "step": 21800
    },
    {
      "epoch": 2.192630573568592,
      "grad_norm": 1.7990432977676392,
      "learning_rate": 8.110016475572443e-06,
      "loss": 1.7788,
      "step": 21810
    },
    {
      "epoch": 2.193635952345046,
      "grad_norm": 2.3723955154418945,
      "learning_rate": 8.099929390403819e-06,
      "loss": 1.8014,
      "step": 21820
    },
    {
      "epoch": 2.1946413311215,
      "grad_norm": 2.7757840156555176,
      "learning_rate": 8.089842305235197e-06,
      "loss": 1.7267,
      "step": 21830
    },
    {
      "epoch": 2.195646709897954,
      "grad_norm": 2.326549530029297,
      "learning_rate": 8.079755220066575e-06,
      "loss": 1.7395,
      "step": 21840
    },
    {
      "epoch": 2.196652088674408,
      "grad_norm": 1.9567807912826538,
      "learning_rate": 8.069668134897951e-06,
      "loss": 1.8392,
      "step": 21850
    },
    {
      "epoch": 2.197657467450862,
      "grad_norm": 1.9119473695755005,
      "learning_rate": 8.05958104972933e-06,
      "loss": 1.8287,
      "step": 21860
    },
    {
      "epoch": 2.1986628462273163,
      "grad_norm": 2.018789291381836,
      "learning_rate": 8.049493964560707e-06,
      "loss": 1.7638,
      "step": 21870
    },
    {
      "epoch": 2.1996682250037702,
      "grad_norm": 2.6515135765075684,
      "learning_rate": 8.039406879392084e-06,
      "loss": 1.7086,
      "step": 21880
    },
    {
      "epoch": 2.200673603780224,
      "grad_norm": 2.193061113357544,
      "learning_rate": 8.029319794223463e-06,
      "loss": 1.8388,
      "step": 21890
    },
    {
      "epoch": 2.201678982556678,
      "grad_norm": 2.0182337760925293,
      "learning_rate": 8.019232709054841e-06,
      "loss": 1.7733,
      "step": 21900
    },
    {
      "epoch": 2.2026843613331324,
      "grad_norm": 2.1457157135009766,
      "learning_rate": 8.009145623886218e-06,
      "loss": 1.8231,
      "step": 21910
    },
    {
      "epoch": 2.2036897401095863,
      "grad_norm": 2.149627685546875,
      "learning_rate": 7.999058538717596e-06,
      "loss": 1.7459,
      "step": 21920
    },
    {
      "epoch": 2.20469511888604,
      "grad_norm": 1.9223171472549438,
      "learning_rate": 7.988971453548974e-06,
      "loss": 1.8296,
      "step": 21930
    },
    {
      "epoch": 2.2057004976624945,
      "grad_norm": 2.4473817348480225,
      "learning_rate": 7.97888436838035e-06,
      "loss": 1.7179,
      "step": 21940
    },
    {
      "epoch": 2.2067058764389484,
      "grad_norm": 2.3236660957336426,
      "learning_rate": 7.968797283211728e-06,
      "loss": 1.7115,
      "step": 21950
    },
    {
      "epoch": 2.2077112552154023,
      "grad_norm": 2.294142246246338,
      "learning_rate": 7.958710198043106e-06,
      "loss": 1.7275,
      "step": 21960
    },
    {
      "epoch": 2.2087166339918562,
      "grad_norm": 2.1599795818328857,
      "learning_rate": 7.948623112874484e-06,
      "loss": 1.7314,
      "step": 21970
    },
    {
      "epoch": 2.2097220127683106,
      "grad_norm": 1.939881443977356,
      "learning_rate": 7.93853602770586e-06,
      "loss": 1.7358,
      "step": 21980
    },
    {
      "epoch": 2.2107273915447645,
      "grad_norm": 1.8736127614974976,
      "learning_rate": 7.9294576510541e-06,
      "loss": 1.7468,
      "step": 21990
    },
    {
      "epoch": 2.2117327703212184,
      "grad_norm": 1.9113749265670776,
      "learning_rate": 7.919370565885477e-06,
      "loss": 1.7706,
      "step": 22000
    },
    {
      "epoch": 2.2127381490976727,
      "grad_norm": 1.7956023216247559,
      "learning_rate": 7.909283480716855e-06,
      "loss": 1.7606,
      "step": 22010
    },
    {
      "epoch": 2.2137435278741266,
      "grad_norm": 1.9383597373962402,
      "learning_rate": 7.899196395548233e-06,
      "loss": 1.6886,
      "step": 22020
    },
    {
      "epoch": 2.2147489066505806,
      "grad_norm": 1.9339370727539062,
      "learning_rate": 7.889109310379611e-06,
      "loss": 1.7446,
      "step": 22030
    },
    {
      "epoch": 2.2157542854270345,
      "grad_norm": 2.6783950328826904,
      "learning_rate": 7.879022225210989e-06,
      "loss": 1.7149,
      "step": 22040
    },
    {
      "epoch": 2.216759664203489,
      "grad_norm": 1.8856149911880493,
      "learning_rate": 7.868935140042367e-06,
      "loss": 1.7087,
      "step": 22050
    },
    {
      "epoch": 2.2177650429799427,
      "grad_norm": 2.1327919960021973,
      "learning_rate": 7.858848054873743e-06,
      "loss": 1.7595,
      "step": 22060
    },
    {
      "epoch": 2.2187704217563966,
      "grad_norm": 1.919685959815979,
      "learning_rate": 7.848760969705121e-06,
      "loss": 1.7635,
      "step": 22070
    },
    {
      "epoch": 2.219775800532851,
      "grad_norm": 2.0146846771240234,
      "learning_rate": 7.8386738845365e-06,
      "loss": 1.747,
      "step": 22080
    },
    {
      "epoch": 2.220781179309305,
      "grad_norm": 1.7582168579101562,
      "learning_rate": 7.828586799367876e-06,
      "loss": 1.8559,
      "step": 22090
    },
    {
      "epoch": 2.2217865580857588,
      "grad_norm": 1.7707008123397827,
      "learning_rate": 7.818499714199254e-06,
      "loss": 1.8064,
      "step": 22100
    },
    {
      "epoch": 2.2227919368622127,
      "grad_norm": 2.952253580093384,
      "learning_rate": 7.808412629030632e-06,
      "loss": 1.7076,
      "step": 22110
    },
    {
      "epoch": 2.223797315638667,
      "grad_norm": 2.0184192657470703,
      "learning_rate": 7.798325543862008e-06,
      "loss": 1.7968,
      "step": 22120
    },
    {
      "epoch": 2.224802694415121,
      "grad_norm": 2.0769760608673096,
      "learning_rate": 7.788238458693386e-06,
      "loss": 1.7864,
      "step": 22130
    },
    {
      "epoch": 2.225808073191575,
      "grad_norm": 2.0673208236694336,
      "learning_rate": 7.778151373524764e-06,
      "loss": 1.7823,
      "step": 22140
    },
    {
      "epoch": 2.226813451968029,
      "grad_norm": 2.2405996322631836,
      "learning_rate": 7.76806428835614e-06,
      "loss": 1.7674,
      "step": 22150
    },
    {
      "epoch": 2.227818830744483,
      "grad_norm": 2.7895114421844482,
      "learning_rate": 7.757977203187518e-06,
      "loss": 1.8042,
      "step": 22160
    },
    {
      "epoch": 2.228824209520937,
      "grad_norm": 2.026500940322876,
      "learning_rate": 7.747890118018896e-06,
      "loss": 1.7174,
      "step": 22170
    },
    {
      "epoch": 2.229829588297391,
      "grad_norm": 2.215350866317749,
      "learning_rate": 7.737803032850274e-06,
      "loss": 1.7715,
      "step": 22180
    },
    {
      "epoch": 2.230834967073845,
      "grad_norm": 2.260409355163574,
      "learning_rate": 7.727715947681652e-06,
      "loss": 1.8007,
      "step": 22190
    },
    {
      "epoch": 2.231840345850299,
      "grad_norm": 1.9716072082519531,
      "learning_rate": 7.71762886251303e-06,
      "loss": 1.7646,
      "step": 22200
    },
    {
      "epoch": 2.232845724626753,
      "grad_norm": 2.1030020713806152,
      "learning_rate": 7.707541777344407e-06,
      "loss": 1.748,
      "step": 22210
    },
    {
      "epoch": 2.2338511034032074,
      "grad_norm": 2.9261882305145264,
      "learning_rate": 7.697454692175785e-06,
      "loss": 1.7409,
      "step": 22220
    },
    {
      "epoch": 2.2348564821796613,
      "grad_norm": 2.0104668140411377,
      "learning_rate": 7.687367607007163e-06,
      "loss": 1.8167,
      "step": 22230
    },
    {
      "epoch": 2.235861860956115,
      "grad_norm": 2.2852072715759277,
      "learning_rate": 7.677280521838539e-06,
      "loss": 1.761,
      "step": 22240
    },
    {
      "epoch": 2.236867239732569,
      "grad_norm": 2.4786837100982666,
      "learning_rate": 7.667193436669917e-06,
      "loss": 1.7985,
      "step": 22250
    },
    {
      "epoch": 2.2378726185090234,
      "grad_norm": 1.7156544923782349,
      "learning_rate": 7.657106351501295e-06,
      "loss": 1.7169,
      "step": 22260
    },
    {
      "epoch": 2.2388779972854773,
      "grad_norm": 2.50805926322937,
      "learning_rate": 7.647019266332671e-06,
      "loss": 1.8052,
      "step": 22270
    },
    {
      "epoch": 2.239883376061931,
      "grad_norm": 2.045663356781006,
      "learning_rate": 7.63693218116405e-06,
      "loss": 1.7783,
      "step": 22280
    },
    {
      "epoch": 2.2408887548383856,
      "grad_norm": 2.350597620010376,
      "learning_rate": 7.626845095995427e-06,
      "loss": 1.8091,
      "step": 22290
    },
    {
      "epoch": 2.2418941336148395,
      "grad_norm": 1.8272082805633545,
      "learning_rate": 7.6167580108268045e-06,
      "loss": 1.7243,
      "step": 22300
    },
    {
      "epoch": 2.2428995123912934,
      "grad_norm": 2.0600013732910156,
      "learning_rate": 7.6066709256581825e-06,
      "loss": 1.7371,
      "step": 22310
    },
    {
      "epoch": 2.2439048911677473,
      "grad_norm": 1.8546621799468994,
      "learning_rate": 7.5965838404895606e-06,
      "loss": 1.7318,
      "step": 22320
    },
    {
      "epoch": 2.2449102699442016,
      "grad_norm": 2.8597397804260254,
      "learning_rate": 7.586496755320937e-06,
      "loss": 1.7697,
      "step": 22330
    },
    {
      "epoch": 2.2459156487206555,
      "grad_norm": 1.6998687982559204,
      "learning_rate": 7.576409670152315e-06,
      "loss": 1.7732,
      "step": 22340
    },
    {
      "epoch": 2.2469210274971094,
      "grad_norm": 2.8102524280548096,
      "learning_rate": 7.566322584983693e-06,
      "loss": 1.7544,
      "step": 22350
    },
    {
      "epoch": 2.2479264062735638,
      "grad_norm": 1.8788416385650635,
      "learning_rate": 7.556235499815069e-06,
      "loss": 1.7822,
      "step": 22360
    },
    {
      "epoch": 2.2489317850500177,
      "grad_norm": 2.386382579803467,
      "learning_rate": 7.546148414646448e-06,
      "loss": 1.7088,
      "step": 22370
    },
    {
      "epoch": 2.2499371638264716,
      "grad_norm": 1.8724147081375122,
      "learning_rate": 7.536061329477826e-06,
      "loss": 1.7618,
      "step": 22380
    },
    {
      "epoch": 2.2509425426029255,
      "grad_norm": 2.7129926681518555,
      "learning_rate": 7.525974244309202e-06,
      "loss": 1.7298,
      "step": 22390
    },
    {
      "epoch": 2.25194792137938,
      "grad_norm": 2.926173448562622,
      "learning_rate": 7.51588715914058e-06,
      "loss": 1.7621,
      "step": 22400
    },
    {
      "epoch": 2.2529533001558337,
      "grad_norm": 2.1729648113250732,
      "learning_rate": 7.505800073971958e-06,
      "loss": 1.7986,
      "step": 22410
    },
    {
      "epoch": 2.2539586789322876,
      "grad_norm": 1.8406037092208862,
      "learning_rate": 7.4957129888033355e-06,
      "loss": 1.8073,
      "step": 22420
    },
    {
      "epoch": 2.254964057708742,
      "grad_norm": 2.019477605819702,
      "learning_rate": 7.485625903634713e-06,
      "loss": 1.7542,
      "step": 22430
    },
    {
      "epoch": 2.255969436485196,
      "grad_norm": 2.187166929244995,
      "learning_rate": 7.475538818466091e-06,
      "loss": 1.7051,
      "step": 22440
    },
    {
      "epoch": 2.25697481526165,
      "grad_norm": 2.050340414047241,
      "learning_rate": 7.465451733297469e-06,
      "loss": 1.7666,
      "step": 22450
    },
    {
      "epoch": 2.2579801940381037,
      "grad_norm": 2.1791720390319824,
      "learning_rate": 7.455364648128846e-06,
      "loss": 1.7568,
      "step": 22460
    },
    {
      "epoch": 2.258985572814558,
      "grad_norm": 1.9635379314422607,
      "learning_rate": 7.445277562960224e-06,
      "loss": 1.6839,
      "step": 22470
    },
    {
      "epoch": 2.259990951591012,
      "grad_norm": 1.8984434604644775,
      "learning_rate": 7.435190477791601e-06,
      "loss": 1.7445,
      "step": 22480
    },
    {
      "epoch": 2.260996330367466,
      "grad_norm": 2.065499782562256,
      "learning_rate": 7.425103392622978e-06,
      "loss": 1.7177,
      "step": 22490
    },
    {
      "epoch": 2.26200170914392,
      "grad_norm": 1.5635952949523926,
      "learning_rate": 7.415016307454356e-06,
      "loss": 1.7195,
      "step": 22500
    },
    {
      "epoch": 2.263007087920374,
      "grad_norm": 2.0847713947296143,
      "learning_rate": 7.404929222285733e-06,
      "loss": 1.7018,
      "step": 22510
    },
    {
      "epoch": 2.264012466696828,
      "grad_norm": 2.315721035003662,
      "learning_rate": 7.394842137117111e-06,
      "loss": 1.75,
      "step": 22520
    },
    {
      "epoch": 2.265017845473282,
      "grad_norm": 1.5705169439315796,
      "learning_rate": 7.384755051948489e-06,
      "loss": 1.8161,
      "step": 22530
    },
    {
      "epoch": 2.2660232242497362,
      "grad_norm": 2.678987741470337,
      "learning_rate": 7.3746679667798666e-06,
      "loss": 1.7783,
      "step": 22540
    },
    {
      "epoch": 2.26702860302619,
      "grad_norm": 2.058701515197754,
      "learning_rate": 7.364580881611244e-06,
      "loss": 1.7778,
      "step": 22550
    },
    {
      "epoch": 2.268033981802644,
      "grad_norm": 1.7737337350845337,
      "learning_rate": 7.354493796442622e-06,
      "loss": 1.8059,
      "step": 22560
    },
    {
      "epoch": 2.2690393605790984,
      "grad_norm": 4.749547004699707,
      "learning_rate": 7.344406711273999e-06,
      "loss": 1.8795,
      "step": 22570
    },
    {
      "epoch": 2.2700447393555523,
      "grad_norm": 2.3210246562957764,
      "learning_rate": 7.334319626105376e-06,
      "loss": 1.7713,
      "step": 22580
    },
    {
      "epoch": 2.271050118132006,
      "grad_norm": 2.0576395988464355,
      "learning_rate": 7.324232540936754e-06,
      "loss": 1.6959,
      "step": 22590
    },
    {
      "epoch": 2.27205549690846,
      "grad_norm": 1.8345177173614502,
      "learning_rate": 7.314145455768132e-06,
      "loss": 1.7959,
      "step": 22600
    },
    {
      "epoch": 2.2730608756849144,
      "grad_norm": 1.9520728588104248,
      "learning_rate": 7.304058370599509e-06,
      "loss": 1.7242,
      "step": 22610
    },
    {
      "epoch": 2.2740662544613683,
      "grad_norm": 1.8320366144180298,
      "learning_rate": 7.293971285430887e-06,
      "loss": 1.7849,
      "step": 22620
    },
    {
      "epoch": 2.2750716332378222,
      "grad_norm": 2.199122190475464,
      "learning_rate": 7.283884200262264e-06,
      "loss": 1.6836,
      "step": 22630
    },
    {
      "epoch": 2.2760770120142766,
      "grad_norm": 1.732527732849121,
      "learning_rate": 7.2737971150936416e-06,
      "loss": 1.8119,
      "step": 22640
    },
    {
      "epoch": 2.2770823907907305,
      "grad_norm": 2.0604958534240723,
      "learning_rate": 7.2637100299250196e-06,
      "loss": 1.7519,
      "step": 22650
    },
    {
      "epoch": 2.2780877695671844,
      "grad_norm": 2.0313282012939453,
      "learning_rate": 7.253622944756397e-06,
      "loss": 1.7035,
      "step": 22660
    },
    {
      "epoch": 2.2790931483436383,
      "grad_norm": 1.6900025606155396,
      "learning_rate": 7.243535859587775e-06,
      "loss": 1.7309,
      "step": 22670
    },
    {
      "epoch": 2.2800985271200926,
      "grad_norm": 1.6868420839309692,
      "learning_rate": 7.233448774419153e-06,
      "loss": 1.8075,
      "step": 22680
    },
    {
      "epoch": 2.2811039058965465,
      "grad_norm": 2.035414457321167,
      "learning_rate": 7.22336168925053e-06,
      "loss": 1.7592,
      "step": 22690
    },
    {
      "epoch": 2.2821092846730004,
      "grad_norm": 2.0661234855651855,
      "learning_rate": 7.213274604081907e-06,
      "loss": 1.7965,
      "step": 22700
    },
    {
      "epoch": 2.283114663449455,
      "grad_norm": 2.149578332901001,
      "learning_rate": 7.203187518913285e-06,
      "loss": 1.7611,
      "step": 22710
    },
    {
      "epoch": 2.2841200422259087,
      "grad_norm": 2.8738858699798584,
      "learning_rate": 7.193100433744662e-06,
      "loss": 1.7305,
      "step": 22720
    },
    {
      "epoch": 2.2851254210023626,
      "grad_norm": 2.4178757667541504,
      "learning_rate": 7.183013348576039e-06,
      "loss": 1.7042,
      "step": 22730
    },
    {
      "epoch": 2.2861307997788165,
      "grad_norm": 2.103825092315674,
      "learning_rate": 7.172926263407417e-06,
      "loss": 1.7172,
      "step": 22740
    },
    {
      "epoch": 2.287136178555271,
      "grad_norm": 2.5633184909820557,
      "learning_rate": 7.162839178238795e-06,
      "loss": 1.7429,
      "step": 22750
    },
    {
      "epoch": 2.2881415573317248,
      "grad_norm": 1.814850091934204,
      "learning_rate": 7.1527520930701726e-06,
      "loss": 1.7871,
      "step": 22760
    },
    {
      "epoch": 2.2891469361081787,
      "grad_norm": 1.8852444887161255,
      "learning_rate": 7.1426650079015506e-06,
      "loss": 1.7846,
      "step": 22770
    },
    {
      "epoch": 2.290152314884633,
      "grad_norm": 1.9675308465957642,
      "learning_rate": 7.132577922732928e-06,
      "loss": 1.7116,
      "step": 22780
    },
    {
      "epoch": 2.291157693661087,
      "grad_norm": 2.792762279510498,
      "learning_rate": 7.122490837564305e-06,
      "loss": 1.8762,
      "step": 22790
    },
    {
      "epoch": 2.292163072437541,
      "grad_norm": 2.0866012573242188,
      "learning_rate": 7.112403752395683e-06,
      "loss": 1.6847,
      "step": 22800
    },
    {
      "epoch": 2.2931684512139947,
      "grad_norm": 2.589116096496582,
      "learning_rate": 7.10231666722706e-06,
      "loss": 1.812,
      "step": 22810
    },
    {
      "epoch": 2.294173829990449,
      "grad_norm": 2.1948320865631104,
      "learning_rate": 7.092229582058437e-06,
      "loss": 1.6958,
      "step": 22820
    },
    {
      "epoch": 2.295179208766903,
      "grad_norm": 2.556627035140991,
      "learning_rate": 7.082142496889816e-06,
      "loss": 1.8104,
      "step": 22830
    },
    {
      "epoch": 2.296184587543357,
      "grad_norm": 1.5499992370605469,
      "learning_rate": 7.072055411721193e-06,
      "loss": 1.747,
      "step": 22840
    },
    {
      "epoch": 2.297189966319811,
      "grad_norm": 1.9241725206375122,
      "learning_rate": 7.06196832655257e-06,
      "loss": 1.7751,
      "step": 22850
    },
    {
      "epoch": 2.298195345096265,
      "grad_norm": 1.8939236402511597,
      "learning_rate": 7.051881241383948e-06,
      "loss": 1.7437,
      "step": 22860
    },
    {
      "epoch": 2.299200723872719,
      "grad_norm": 1.9046590328216553,
      "learning_rate": 7.0417941562153256e-06,
      "loss": 1.7479,
      "step": 22870
    },
    {
      "epoch": 2.300206102649173,
      "grad_norm": 1.5231056213378906,
      "learning_rate": 7.0317070710467036e-06,
      "loss": 1.7902,
      "step": 22880
    },
    {
      "epoch": 2.3012114814256273,
      "grad_norm": 2.170095682144165,
      "learning_rate": 7.021619985878081e-06,
      "loss": 1.7721,
      "step": 22890
    },
    {
      "epoch": 2.302216860202081,
      "grad_norm": 2.047612190246582,
      "learning_rate": 7.011532900709459e-06,
      "loss": 1.7086,
      "step": 22900
    },
    {
      "epoch": 2.303222238978535,
      "grad_norm": 2.0078916549682617,
      "learning_rate": 7.001445815540837e-06,
      "loss": 1.8072,
      "step": 22910
    },
    {
      "epoch": 2.3042276177549894,
      "grad_norm": 1.9243896007537842,
      "learning_rate": 6.991358730372214e-06,
      "loss": 1.7902,
      "step": 22920
    },
    {
      "epoch": 2.3052329965314433,
      "grad_norm": 2.135018825531006,
      "learning_rate": 6.981271645203591e-06,
      "loss": 1.7623,
      "step": 22930
    },
    {
      "epoch": 2.306238375307897,
      "grad_norm": 1.9001668691635132,
      "learning_rate": 6.971184560034969e-06,
      "loss": 1.7319,
      "step": 22940
    },
    {
      "epoch": 2.307243754084351,
      "grad_norm": 3.9065101146698,
      "learning_rate": 6.961097474866346e-06,
      "loss": 1.8121,
      "step": 22950
    },
    {
      "epoch": 2.3082491328608055,
      "grad_norm": 1.9146631956100464,
      "learning_rate": 6.951010389697723e-06,
      "loss": 1.8388,
      "step": 22960
    },
    {
      "epoch": 2.3092545116372594,
      "grad_norm": 1.7287116050720215,
      "learning_rate": 6.940923304529101e-06,
      "loss": 1.7215,
      "step": 22970
    },
    {
      "epoch": 2.3102598904137133,
      "grad_norm": 4.319527626037598,
      "learning_rate": 6.930836219360479e-06,
      "loss": 1.7988,
      "step": 22980
    },
    {
      "epoch": 2.3112652691901676,
      "grad_norm": 1.7471262216567993,
      "learning_rate": 6.9207491341918566e-06,
      "loss": 1.7616,
      "step": 22990
    },
    {
      "epoch": 2.3122706479666215,
      "grad_norm": 2.6296825408935547,
      "learning_rate": 6.910662049023235e-06,
      "loss": 1.7052,
      "step": 23000
    },
    {
      "epoch": 2.3132760267430754,
      "grad_norm": 2.0150766372680664,
      "learning_rate": 6.900574963854612e-06,
      "loss": 1.7378,
      "step": 23010
    },
    {
      "epoch": 2.3142814055195293,
      "grad_norm": 2.0545201301574707,
      "learning_rate": 6.890487878685989e-06,
      "loss": 1.7876,
      "step": 23020
    },
    {
      "epoch": 2.3152867842959837,
      "grad_norm": 3.616729974746704,
      "learning_rate": 6.880400793517367e-06,
      "loss": 1.8111,
      "step": 23030
    },
    {
      "epoch": 2.3162921630724376,
      "grad_norm": 2.4543845653533936,
      "learning_rate": 6.870313708348744e-06,
      "loss": 1.7366,
      "step": 23040
    },
    {
      "epoch": 2.3172975418488915,
      "grad_norm": 2.426811933517456,
      "learning_rate": 6.860226623180122e-06,
      "loss": 1.8673,
      "step": 23050
    },
    {
      "epoch": 2.318302920625346,
      "grad_norm": 2.3896796703338623,
      "learning_rate": 6.8501395380115e-06,
      "loss": 1.749,
      "step": 23060
    },
    {
      "epoch": 2.3193082994017997,
      "grad_norm": 1.6569372415542603,
      "learning_rate": 6.840052452842877e-06,
      "loss": 1.7906,
      "step": 23070
    },
    {
      "epoch": 2.3203136781782536,
      "grad_norm": 2.6648993492126465,
      "learning_rate": 6.829965367674254e-06,
      "loss": 1.8407,
      "step": 23080
    },
    {
      "epoch": 2.3213190569547075,
      "grad_norm": 2.0051305294036865,
      "learning_rate": 6.819878282505632e-06,
      "loss": 1.7737,
      "step": 23090
    },
    {
      "epoch": 2.322324435731162,
      "grad_norm": 2.8842380046844482,
      "learning_rate": 6.8097911973370096e-06,
      "loss": 1.7244,
      "step": 23100
    },
    {
      "epoch": 2.3233298145076158,
      "grad_norm": 1.922214150428772,
      "learning_rate": 6.799704112168387e-06,
      "loss": 1.7861,
      "step": 23110
    },
    {
      "epoch": 2.3243351932840697,
      "grad_norm": 2.137550115585327,
      "learning_rate": 6.789617026999765e-06,
      "loss": 1.6655,
      "step": 23120
    },
    {
      "epoch": 2.325340572060524,
      "grad_norm": 2.9024605751037598,
      "learning_rate": 6.779529941831143e-06,
      "loss": 1.7456,
      "step": 23130
    },
    {
      "epoch": 2.326345950836978,
      "grad_norm": 2.3398168087005615,
      "learning_rate": 6.76944285666252e-06,
      "loss": 1.7627,
      "step": 23140
    },
    {
      "epoch": 2.327351329613432,
      "grad_norm": 2.460981845855713,
      "learning_rate": 6.759355771493898e-06,
      "loss": 1.6866,
      "step": 23150
    },
    {
      "epoch": 2.3283567083898857,
      "grad_norm": 2.12492036819458,
      "learning_rate": 6.749268686325275e-06,
      "loss": 1.814,
      "step": 23160
    },
    {
      "epoch": 2.32936208716634,
      "grad_norm": 1.846731185913086,
      "learning_rate": 6.739181601156652e-06,
      "loss": 1.7725,
      "step": 23170
    },
    {
      "epoch": 2.330367465942794,
      "grad_norm": 2.269559144973755,
      "learning_rate": 6.72909451598803e-06,
      "loss": 1.7037,
      "step": 23180
    },
    {
      "epoch": 2.331372844719248,
      "grad_norm": 1.8509917259216309,
      "learning_rate": 6.719007430819407e-06,
      "loss": 1.7651,
      "step": 23190
    },
    {
      "epoch": 2.3323782234957022,
      "grad_norm": 1.852292776107788,
      "learning_rate": 6.7089203456507846e-06,
      "loss": 1.8177,
      "step": 23200
    },
    {
      "epoch": 2.333383602272156,
      "grad_norm": 2.0872316360473633,
      "learning_rate": 6.6988332604821634e-06,
      "loss": 1.8318,
      "step": 23210
    },
    {
      "epoch": 2.33438898104861,
      "grad_norm": 2.6113715171813965,
      "learning_rate": 6.688746175313541e-06,
      "loss": 1.8152,
      "step": 23220
    },
    {
      "epoch": 2.335394359825064,
      "grad_norm": 1.9707486629486084,
      "learning_rate": 6.678659090144918e-06,
      "loss": 1.8218,
      "step": 23230
    },
    {
      "epoch": 2.3363997386015183,
      "grad_norm": 1.9477189779281616,
      "learning_rate": 6.668572004976296e-06,
      "loss": 1.7437,
      "step": 23240
    },
    {
      "epoch": 2.337405117377972,
      "grad_norm": 2.718698024749756,
      "learning_rate": 6.658484919807673e-06,
      "loss": 1.743,
      "step": 23250
    },
    {
      "epoch": 2.338410496154426,
      "grad_norm": 2.0994389057159424,
      "learning_rate": 6.64839783463905e-06,
      "loss": 1.7529,
      "step": 23260
    },
    {
      "epoch": 2.3394158749308804,
      "grad_norm": 1.9478329420089722,
      "learning_rate": 6.638310749470428e-06,
      "loss": 1.7333,
      "step": 23270
    },
    {
      "epoch": 2.3404212537073343,
      "grad_norm": 1.9110954999923706,
      "learning_rate": 6.628223664301806e-06,
      "loss": 1.7472,
      "step": 23280
    },
    {
      "epoch": 2.3414266324837882,
      "grad_norm": 2.001526355743408,
      "learning_rate": 6.618136579133183e-06,
      "loss": 1.7148,
      "step": 23290
    },
    {
      "epoch": 2.342432011260242,
      "grad_norm": 2.997875213623047,
      "learning_rate": 6.608049493964561e-06,
      "loss": 1.8349,
      "step": 23300
    },
    {
      "epoch": 2.3434373900366965,
      "grad_norm": 1.921696424484253,
      "learning_rate": 6.597962408795938e-06,
      "loss": 1.8063,
      "step": 23310
    },
    {
      "epoch": 2.3444427688131504,
      "grad_norm": 2.0734081268310547,
      "learning_rate": 6.587875323627316e-06,
      "loss": 1.7222,
      "step": 23320
    },
    {
      "epoch": 2.3454481475896043,
      "grad_norm": 2.159538984298706,
      "learning_rate": 6.577788238458694e-06,
      "loss": 1.7344,
      "step": 23330
    },
    {
      "epoch": 2.3464535263660586,
      "grad_norm": 2.080608367919922,
      "learning_rate": 6.567701153290071e-06,
      "loss": 1.8428,
      "step": 23340
    },
    {
      "epoch": 2.3474589051425125,
      "grad_norm": 2.0088250637054443,
      "learning_rate": 6.557614068121448e-06,
      "loss": 1.7137,
      "step": 23350
    },
    {
      "epoch": 2.3484642839189664,
      "grad_norm": 1.8502161502838135,
      "learning_rate": 6.547526982952827e-06,
      "loss": 1.8222,
      "step": 23360
    },
    {
      "epoch": 2.3494696626954203,
      "grad_norm": 1.7339222431182861,
      "learning_rate": 6.537439897784204e-06,
      "loss": 1.6855,
      "step": 23370
    },
    {
      "epoch": 2.3504750414718747,
      "grad_norm": 1.685134768486023,
      "learning_rate": 6.527352812615581e-06,
      "loss": 1.7993,
      "step": 23380
    },
    {
      "epoch": 2.3514804202483286,
      "grad_norm": 2.7441816329956055,
      "learning_rate": 6.517265727446959e-06,
      "loss": 1.7827,
      "step": 23390
    },
    {
      "epoch": 2.3524857990247825,
      "grad_norm": 2.0140836238861084,
      "learning_rate": 6.507178642278336e-06,
      "loss": 1.7827,
      "step": 23400
    },
    {
      "epoch": 2.353491177801237,
      "grad_norm": 2.076854705810547,
      "learning_rate": 6.497091557109713e-06,
      "loss": 1.758,
      "step": 23410
    },
    {
      "epoch": 2.3544965565776907,
      "grad_norm": 2.0012919902801514,
      "learning_rate": 6.487004471941091e-06,
      "loss": 1.7766,
      "step": 23420
    },
    {
      "epoch": 2.3555019353541446,
      "grad_norm": 1.9440120458602905,
      "learning_rate": 6.4769173867724694e-06,
      "loss": 1.6876,
      "step": 23430
    },
    {
      "epoch": 2.3565073141305986,
      "grad_norm": 1.799381136894226,
      "learning_rate": 6.466830301603847e-06,
      "loss": 1.8341,
      "step": 23440
    },
    {
      "epoch": 2.357512692907053,
      "grad_norm": 2.0369489192962646,
      "learning_rate": 6.456743216435225e-06,
      "loss": 1.7627,
      "step": 23450
    },
    {
      "epoch": 2.358518071683507,
      "grad_norm": 2.2547922134399414,
      "learning_rate": 6.446656131266602e-06,
      "loss": 1.7672,
      "step": 23460
    },
    {
      "epoch": 2.3595234504599607,
      "grad_norm": 1.8307842016220093,
      "learning_rate": 6.436569046097979e-06,
      "loss": 1.7012,
      "step": 23470
    },
    {
      "epoch": 2.360528829236415,
      "grad_norm": 2.3445372581481934,
      "learning_rate": 6.426481960929357e-06,
      "loss": 1.7113,
      "step": 23480
    },
    {
      "epoch": 2.361534208012869,
      "grad_norm": 1.7964564561843872,
      "learning_rate": 6.416394875760734e-06,
      "loss": 1.7363,
      "step": 23490
    },
    {
      "epoch": 2.362539586789323,
      "grad_norm": 2.341038465499878,
      "learning_rate": 6.406307790592112e-06,
      "loss": 1.7477,
      "step": 23500
    },
    {
      "epoch": 2.3635449655657768,
      "grad_norm": 1.7519011497497559,
      "learning_rate": 6.39622070542349e-06,
      "loss": 1.752,
      "step": 23510
    },
    {
      "epoch": 2.364550344342231,
      "grad_norm": 2.587050199508667,
      "learning_rate": 6.386133620254867e-06,
      "loss": 1.792,
      "step": 23520
    },
    {
      "epoch": 2.365555723118685,
      "grad_norm": 2.214808702468872,
      "learning_rate": 6.376046535086245e-06,
      "loss": 1.8275,
      "step": 23530
    },
    {
      "epoch": 2.366561101895139,
      "grad_norm": 1.930902361869812,
      "learning_rate": 6.3659594499176224e-06,
      "loss": 1.7775,
      "step": 23540
    },
    {
      "epoch": 2.3675664806715933,
      "grad_norm": 2.4733433723449707,
      "learning_rate": 6.355872364749e-06,
      "loss": 1.7159,
      "step": 23550
    },
    {
      "epoch": 2.368571859448047,
      "grad_norm": 2.572470188140869,
      "learning_rate": 6.345785279580378e-06,
      "loss": 1.7436,
      "step": 23560
    },
    {
      "epoch": 2.369577238224501,
      "grad_norm": 3.01147723197937,
      "learning_rate": 6.335698194411755e-06,
      "loss": 1.679,
      "step": 23570
    },
    {
      "epoch": 2.370582617000955,
      "grad_norm": 2.515557289123535,
      "learning_rate": 6.325611109243133e-06,
      "loss": 1.7702,
      "step": 23580
    },
    {
      "epoch": 2.3715879957774093,
      "grad_norm": 1.9460258483886719,
      "learning_rate": 6.315524024074511e-06,
      "loss": 1.769,
      "step": 23590
    },
    {
      "epoch": 2.372593374553863,
      "grad_norm": 1.768412709236145,
      "learning_rate": 6.305436938905888e-06,
      "loss": 1.7342,
      "step": 23600
    },
    {
      "epoch": 2.373598753330317,
      "grad_norm": 1.9400458335876465,
      "learning_rate": 6.295349853737265e-06,
      "loss": 1.7848,
      "step": 23610
    },
    {
      "epoch": 2.3746041321067715,
      "grad_norm": 1.9900546073913574,
      "learning_rate": 6.285262768568643e-06,
      "loss": 1.7283,
      "step": 23620
    },
    {
      "epoch": 2.3756095108832254,
      "grad_norm": 1.9401975870132446,
      "learning_rate": 6.27517568340002e-06,
      "loss": 1.8431,
      "step": 23630
    },
    {
      "epoch": 2.3766148896596793,
      "grad_norm": 3.5711162090301514,
      "learning_rate": 6.2650885982313974e-06,
      "loss": 1.7774,
      "step": 23640
    },
    {
      "epoch": 2.377620268436133,
      "grad_norm": 2.808833122253418,
      "learning_rate": 6.2550015130627754e-06,
      "loss": 1.8073,
      "step": 23650
    },
    {
      "epoch": 2.3786256472125875,
      "grad_norm": 1.9919993877410889,
      "learning_rate": 6.2449144278941534e-06,
      "loss": 1.7184,
      "step": 23660
    },
    {
      "epoch": 2.3796310259890414,
      "grad_norm": 1.8523035049438477,
      "learning_rate": 6.234827342725531e-06,
      "loss": 1.7279,
      "step": 23670
    },
    {
      "epoch": 2.3806364047654953,
      "grad_norm": 2.3456830978393555,
      "learning_rate": 6.224740257556909e-06,
      "loss": 1.6633,
      "step": 23680
    },
    {
      "epoch": 2.3816417835419497,
      "grad_norm": 3.963391065597534,
      "learning_rate": 6.214653172388286e-06,
      "loss": 1.7834,
      "step": 23690
    },
    {
      "epoch": 2.3826471623184036,
      "grad_norm": 2.5943312644958496,
      "learning_rate": 6.204566087219663e-06,
      "loss": 1.7643,
      "step": 23700
    },
    {
      "epoch": 2.3836525410948575,
      "grad_norm": 1.6088842153549194,
      "learning_rate": 6.194479002051041e-06,
      "loss": 1.6802,
      "step": 23710
    },
    {
      "epoch": 2.3846579198713114,
      "grad_norm": 1.8192434310913086,
      "learning_rate": 6.184391916882418e-06,
      "loss": 1.698,
      "step": 23720
    },
    {
      "epoch": 2.3856632986477657,
      "grad_norm": 1.9451324939727783,
      "learning_rate": 6.174304831713795e-06,
      "loss": 1.7047,
      "step": 23730
    },
    {
      "epoch": 2.3866686774242196,
      "grad_norm": 1.9357279539108276,
      "learning_rate": 6.164217746545174e-06,
      "loss": 1.7239,
      "step": 23740
    },
    {
      "epoch": 2.3876740562006735,
      "grad_norm": 2.0084939002990723,
      "learning_rate": 6.154130661376551e-06,
      "loss": 1.692,
      "step": 23750
    },
    {
      "epoch": 2.388679434977128,
      "grad_norm": 2.022207498550415,
      "learning_rate": 6.1440435762079284e-06,
      "loss": 1.8006,
      "step": 23760
    },
    {
      "epoch": 2.3896848137535818,
      "grad_norm": 2.293240547180176,
      "learning_rate": 6.1339564910393064e-06,
      "loss": 1.7874,
      "step": 23770
    },
    {
      "epoch": 2.3906901925300357,
      "grad_norm": 1.9008855819702148,
      "learning_rate": 6.123869405870684e-06,
      "loss": 1.8194,
      "step": 23780
    },
    {
      "epoch": 2.3916955713064896,
      "grad_norm": 2.0847766399383545,
      "learning_rate": 6.113782320702061e-06,
      "loss": 1.7828,
      "step": 23790
    },
    {
      "epoch": 2.392700950082944,
      "grad_norm": 2.85870623588562,
      "learning_rate": 6.103695235533439e-06,
      "loss": 1.7004,
      "step": 23800
    },
    {
      "epoch": 2.393706328859398,
      "grad_norm": 2.4714531898498535,
      "learning_rate": 6.093608150364817e-06,
      "loss": 1.6828,
      "step": 23810
    },
    {
      "epoch": 2.3947117076358517,
      "grad_norm": 1.9065781831741333,
      "learning_rate": 6.083521065196194e-06,
      "loss": 1.7524,
      "step": 23820
    },
    {
      "epoch": 2.395717086412306,
      "grad_norm": 1.7615878582000732,
      "learning_rate": 6.073433980027572e-06,
      "loss": 1.7697,
      "step": 23830
    },
    {
      "epoch": 2.39672246518876,
      "grad_norm": 2.6372547149658203,
      "learning_rate": 6.063346894858949e-06,
      "loss": 1.7539,
      "step": 23840
    },
    {
      "epoch": 2.397727843965214,
      "grad_norm": 2.3068196773529053,
      "learning_rate": 6.053259809690326e-06,
      "loss": 1.7755,
      "step": 23850
    },
    {
      "epoch": 2.398733222741668,
      "grad_norm": 2.089735269546509,
      "learning_rate": 6.043172724521704e-06,
      "loss": 1.8476,
      "step": 23860
    },
    {
      "epoch": 2.399738601518122,
      "grad_norm": 3.091411590576172,
      "learning_rate": 6.0330856393530814e-06,
      "loss": 1.7353,
      "step": 23870
    },
    {
      "epoch": 2.400743980294576,
      "grad_norm": 2.211153507232666,
      "learning_rate": 6.022998554184459e-06,
      "loss": 1.6966,
      "step": 23880
    },
    {
      "epoch": 2.40174935907103,
      "grad_norm": 2.675245761871338,
      "learning_rate": 6.0129114690158375e-06,
      "loss": 1.7555,
      "step": 23890
    },
    {
      "epoch": 2.4027547378474843,
      "grad_norm": 2.2039318084716797,
      "learning_rate": 6.002824383847215e-06,
      "loss": 1.7579,
      "step": 23900
    },
    {
      "epoch": 2.403760116623938,
      "grad_norm": 2.491442918777466,
      "learning_rate": 5.992737298678592e-06,
      "loss": 1.7304,
      "step": 23910
    },
    {
      "epoch": 2.404765495400392,
      "grad_norm": 2.2446377277374268,
      "learning_rate": 5.98265021350997e-06,
      "loss": 1.7277,
      "step": 23920
    },
    {
      "epoch": 2.405770874176846,
      "grad_norm": 2.166949987411499,
      "learning_rate": 5.972563128341347e-06,
      "loss": 1.7538,
      "step": 23930
    },
    {
      "epoch": 2.4067762529533003,
      "grad_norm": 1.9408912658691406,
      "learning_rate": 5.962476043172724e-06,
      "loss": 1.7589,
      "step": 23940
    },
    {
      "epoch": 2.4077816317297542,
      "grad_norm": 2.1295979022979736,
      "learning_rate": 5.952388958004102e-06,
      "loss": 1.7862,
      "step": 23950
    },
    {
      "epoch": 2.408787010506208,
      "grad_norm": 2.0654048919677734,
      "learning_rate": 5.94230187283548e-06,
      "loss": 1.7834,
      "step": 23960
    },
    {
      "epoch": 2.4097923892826625,
      "grad_norm": 2.4461891651153564,
      "learning_rate": 5.932214787666857e-06,
      "loss": 1.7853,
      "step": 23970
    },
    {
      "epoch": 2.4107977680591164,
      "grad_norm": 2.024315118789673,
      "learning_rate": 5.922127702498235e-06,
      "loss": 1.6755,
      "step": 23980
    },
    {
      "epoch": 2.4118031468355703,
      "grad_norm": 2.546607732772827,
      "learning_rate": 5.9120406173296124e-06,
      "loss": 1.7712,
      "step": 23990
    },
    {
      "epoch": 2.412808525612024,
      "grad_norm": 2.349531650543213,
      "learning_rate": 5.90195353216099e-06,
      "loss": 1.7419,
      "step": 24000
    },
    {
      "epoch": 2.4138139043884785,
      "grad_norm": 2.065027952194214,
      "learning_rate": 5.891866446992368e-06,
      "loss": 1.7709,
      "step": 24010
    },
    {
      "epoch": 2.4148192831649324,
      "grad_norm": 1.7980477809906006,
      "learning_rate": 5.881779361823745e-06,
      "loss": 1.7777,
      "step": 24020
    },
    {
      "epoch": 2.4158246619413863,
      "grad_norm": 2.2682571411132812,
      "learning_rate": 5.871692276655122e-06,
      "loss": 1.7625,
      "step": 24030
    },
    {
      "epoch": 2.4168300407178407,
      "grad_norm": 1.929672360420227,
      "learning_rate": 5.861605191486501e-06,
      "loss": 1.8145,
      "step": 24040
    },
    {
      "epoch": 2.4178354194942946,
      "grad_norm": 1.9532432556152344,
      "learning_rate": 5.851518106317878e-06,
      "loss": 1.7176,
      "step": 24050
    },
    {
      "epoch": 2.4188407982707485,
      "grad_norm": 2.460659980773926,
      "learning_rate": 5.841431021149255e-06,
      "loss": 1.677,
      "step": 24060
    },
    {
      "epoch": 2.4198461770472024,
      "grad_norm": 2.371908187866211,
      "learning_rate": 5.831343935980633e-06,
      "loss": 1.6904,
      "step": 24070
    },
    {
      "epoch": 2.4208515558236567,
      "grad_norm": 2.2782390117645264,
      "learning_rate": 5.82125685081201e-06,
      "loss": 1.6618,
      "step": 24080
    },
    {
      "epoch": 2.4218569346001106,
      "grad_norm": 1.9118133783340454,
      "learning_rate": 5.811169765643388e-06,
      "loss": 1.7424,
      "step": 24090
    },
    {
      "epoch": 2.4228623133765645,
      "grad_norm": 2.7532148361206055,
      "learning_rate": 5.8010826804747655e-06,
      "loss": 1.7551,
      "step": 24100
    },
    {
      "epoch": 2.423867692153019,
      "grad_norm": 1.9025293588638306,
      "learning_rate": 5.790995595306143e-06,
      "loss": 1.8362,
      "step": 24110
    },
    {
      "epoch": 2.424873070929473,
      "grad_norm": 2.5122568607330322,
      "learning_rate": 5.7809085101375215e-06,
      "loss": 1.8182,
      "step": 24120
    },
    {
      "epoch": 2.4258784497059267,
      "grad_norm": 1.9999902248382568,
      "learning_rate": 5.770821424968899e-06,
      "loss": 1.7408,
      "step": 24130
    },
    {
      "epoch": 2.4268838284823806,
      "grad_norm": 2.74228572845459,
      "learning_rate": 5.760734339800276e-06,
      "loss": 1.7557,
      "step": 24140
    },
    {
      "epoch": 2.427889207258835,
      "grad_norm": 2.1820459365844727,
      "learning_rate": 5.750647254631654e-06,
      "loss": 1.8277,
      "step": 24150
    },
    {
      "epoch": 2.428894586035289,
      "grad_norm": 3.116722345352173,
      "learning_rate": 5.740560169463031e-06,
      "loss": 1.7783,
      "step": 24160
    },
    {
      "epoch": 2.4298999648117428,
      "grad_norm": 2.0064749717712402,
      "learning_rate": 5.730473084294408e-06,
      "loss": 1.7413,
      "step": 24170
    },
    {
      "epoch": 2.430905343588197,
      "grad_norm": 2.035494804382324,
      "learning_rate": 5.720385999125786e-06,
      "loss": 1.8023,
      "step": 24180
    },
    {
      "epoch": 2.431910722364651,
      "grad_norm": 1.9523727893829346,
      "learning_rate": 5.710298913957164e-06,
      "loss": 1.6861,
      "step": 24190
    },
    {
      "epoch": 2.432916101141105,
      "grad_norm": 1.8391965627670288,
      "learning_rate": 5.700211828788541e-06,
      "loss": 1.738,
      "step": 24200
    },
    {
      "epoch": 2.433921479917559,
      "grad_norm": 2.0386180877685547,
      "learning_rate": 5.690124743619919e-06,
      "loss": 1.7623,
      "step": 24210
    },
    {
      "epoch": 2.434926858694013,
      "grad_norm": 1.9551268815994263,
      "learning_rate": 5.6800376584512965e-06,
      "loss": 1.7828,
      "step": 24220
    },
    {
      "epoch": 2.435932237470467,
      "grad_norm": 2.3711700439453125,
      "learning_rate": 5.669950573282674e-06,
      "loss": 1.7175,
      "step": 24230
    },
    {
      "epoch": 2.436937616246921,
      "grad_norm": 2.0878944396972656,
      "learning_rate": 5.659863488114052e-06,
      "loss": 1.7269,
      "step": 24240
    },
    {
      "epoch": 2.4379429950233753,
      "grad_norm": 2.226250410079956,
      "learning_rate": 5.649776402945429e-06,
      "loss": 1.736,
      "step": 24250
    },
    {
      "epoch": 2.438948373799829,
      "grad_norm": 2.085599660873413,
      "learning_rate": 5.639689317776806e-06,
      "loss": 1.7537,
      "step": 24260
    },
    {
      "epoch": 2.439953752576283,
      "grad_norm": 2.181725263595581,
      "learning_rate": 5.629602232608185e-06,
      "loss": 1.6708,
      "step": 24270
    },
    {
      "epoch": 2.440959131352737,
      "grad_norm": 1.855733871459961,
      "learning_rate": 5.619515147439562e-06,
      "loss": 1.7007,
      "step": 24280
    },
    {
      "epoch": 2.4419645101291914,
      "grad_norm": 1.8004693984985352,
      "learning_rate": 5.609428062270939e-06,
      "loss": 1.8298,
      "step": 24290
    },
    {
      "epoch": 2.4429698889056453,
      "grad_norm": 1.6483947038650513,
      "learning_rate": 5.599340977102317e-06,
      "loss": 1.7522,
      "step": 24300
    },
    {
      "epoch": 2.443975267682099,
      "grad_norm": 2.937859535217285,
      "learning_rate": 5.589253891933694e-06,
      "loss": 1.6928,
      "step": 24310
    },
    {
      "epoch": 2.4449806464585535,
      "grad_norm": 1.9785019159317017,
      "learning_rate": 5.5791668067650715e-06,
      "loss": 1.7324,
      "step": 24320
    },
    {
      "epoch": 2.4459860252350074,
      "grad_norm": 2.6414785385131836,
      "learning_rate": 5.5690797215964495e-06,
      "loss": 1.8017,
      "step": 24330
    },
    {
      "epoch": 2.4469914040114613,
      "grad_norm": 2.373189926147461,
      "learning_rate": 5.5589926364278275e-06,
      "loss": 1.7965,
      "step": 24340
    },
    {
      "epoch": 2.447996782787915,
      "grad_norm": 2.3238117694854736,
      "learning_rate": 5.548905551259205e-06,
      "loss": 1.7209,
      "step": 24350
    },
    {
      "epoch": 2.4490021615643696,
      "grad_norm": 2.003368616104126,
      "learning_rate": 5.538818466090583e-06,
      "loss": 1.7864,
      "step": 24360
    },
    {
      "epoch": 2.4500075403408235,
      "grad_norm": 1.8879621028900146,
      "learning_rate": 5.52873138092196e-06,
      "loss": 1.7033,
      "step": 24370
    },
    {
      "epoch": 2.4510129191172774,
      "grad_norm": 1.7945650815963745,
      "learning_rate": 5.518644295753337e-06,
      "loss": 1.7943,
      "step": 24380
    },
    {
      "epoch": 2.4520182978937317,
      "grad_norm": 2.049712657928467,
      "learning_rate": 5.508557210584715e-06,
      "loss": 1.8618,
      "step": 24390
    },
    {
      "epoch": 2.4530236766701856,
      "grad_norm": 2.479219436645508,
      "learning_rate": 5.498470125416092e-06,
      "loss": 1.7329,
      "step": 24400
    },
    {
      "epoch": 2.4540290554466395,
      "grad_norm": 2.2920188903808594,
      "learning_rate": 5.488383040247469e-06,
      "loss": 1.8316,
      "step": 24410
    },
    {
      "epoch": 2.4550344342230934,
      "grad_norm": 2.622443675994873,
      "learning_rate": 5.478295955078848e-06,
      "loss": 1.8508,
      "step": 24420
    },
    {
      "epoch": 2.4560398129995478,
      "grad_norm": 2.1598029136657715,
      "learning_rate": 5.468208869910225e-06,
      "loss": 1.8116,
      "step": 24430
    },
    {
      "epoch": 2.4570451917760017,
      "grad_norm": 1.6157244443893433,
      "learning_rate": 5.4581217847416025e-06,
      "loss": 1.7782,
      "step": 24440
    },
    {
      "epoch": 2.4580505705524556,
      "grad_norm": 2.049347400665283,
      "learning_rate": 5.4480346995729805e-06,
      "loss": 1.7301,
      "step": 24450
    },
    {
      "epoch": 2.4590559493289095,
      "grad_norm": 2.0646839141845703,
      "learning_rate": 5.437947614404358e-06,
      "loss": 1.752,
      "step": 24460
    },
    {
      "epoch": 2.460061328105364,
      "grad_norm": 1.664711594581604,
      "learning_rate": 5.427860529235735e-06,
      "loss": 1.7883,
      "step": 24470
    },
    {
      "epoch": 2.4610667068818177,
      "grad_norm": 1.8109655380249023,
      "learning_rate": 5.417773444067113e-06,
      "loss": 1.729,
      "step": 24480
    },
    {
      "epoch": 2.4620720856582716,
      "grad_norm": 1.7847881317138672,
      "learning_rate": 5.407686358898491e-06,
      "loss": 1.7036,
      "step": 24490
    },
    {
      "epoch": 2.463077464434726,
      "grad_norm": 2.6228342056274414,
      "learning_rate": 5.397599273729868e-06,
      "loss": 1.7526,
      "step": 24500
    },
    {
      "epoch": 2.46408284321118,
      "grad_norm": 1.9553604125976562,
      "learning_rate": 5.387512188561246e-06,
      "loss": 1.789,
      "step": 24510
    },
    {
      "epoch": 2.4650882219876338,
      "grad_norm": 2.45357084274292,
      "learning_rate": 5.377425103392623e-06,
      "loss": 1.7171,
      "step": 24520
    },
    {
      "epoch": 2.4660936007640877,
      "grad_norm": 2.1236519813537598,
      "learning_rate": 5.367338018224e-06,
      "loss": 1.8113,
      "step": 24530
    },
    {
      "epoch": 2.467098979540542,
      "grad_norm": 2.4124755859375,
      "learning_rate": 5.357250933055378e-06,
      "loss": 1.7901,
      "step": 24540
    },
    {
      "epoch": 2.468104358316996,
      "grad_norm": 2.3450121879577637,
      "learning_rate": 5.3471638478867555e-06,
      "loss": 1.7535,
      "step": 24550
    },
    {
      "epoch": 2.46910973709345,
      "grad_norm": 2.2554686069488525,
      "learning_rate": 5.337076762718133e-06,
      "loss": 1.8064,
      "step": 24560
    },
    {
      "epoch": 2.470115115869904,
      "grad_norm": 1.8238540887832642,
      "learning_rate": 5.3269896775495115e-06,
      "loss": 1.6793,
      "step": 24570
    },
    {
      "epoch": 2.471120494646358,
      "grad_norm": 2.559873104095459,
      "learning_rate": 5.316902592380889e-06,
      "loss": 1.7974,
      "step": 24580
    },
    {
      "epoch": 2.472125873422812,
      "grad_norm": 2.0294201374053955,
      "learning_rate": 5.306815507212266e-06,
      "loss": 1.6754,
      "step": 24590
    },
    {
      "epoch": 2.473131252199266,
      "grad_norm": 2.0862627029418945,
      "learning_rate": 5.296728422043644e-06,
      "loss": 1.7245,
      "step": 24600
    },
    {
      "epoch": 2.4741366309757202,
      "grad_norm": 2.4630324840545654,
      "learning_rate": 5.286641336875021e-06,
      "loss": 1.8132,
      "step": 24610
    },
    {
      "epoch": 2.475142009752174,
      "grad_norm": 2.2487151622772217,
      "learning_rate": 5.276554251706398e-06,
      "loss": 1.7835,
      "step": 24620
    },
    {
      "epoch": 2.476147388528628,
      "grad_norm": 1.8784295320510864,
      "learning_rate": 5.266467166537776e-06,
      "loss": 1.8628,
      "step": 24630
    },
    {
      "epoch": 2.4771527673050824,
      "grad_norm": 2.392148494720459,
      "learning_rate": 5.256380081369153e-06,
      "loss": 1.6952,
      "step": 24640
    },
    {
      "epoch": 2.4781581460815363,
      "grad_norm": 2.250680446624756,
      "learning_rate": 5.246292996200531e-06,
      "loss": 1.667,
      "step": 24650
    },
    {
      "epoch": 2.47916352485799,
      "grad_norm": 1.9101508855819702,
      "learning_rate": 5.236205911031909e-06,
      "loss": 1.7565,
      "step": 24660
    },
    {
      "epoch": 2.480168903634444,
      "grad_norm": 2.03403639793396,
      "learning_rate": 5.2261188258632865e-06,
      "loss": 1.7497,
      "step": 24670
    },
    {
      "epoch": 2.4811742824108984,
      "grad_norm": 1.7193524837493896,
      "learning_rate": 5.216031740694664e-06,
      "loss": 1.8263,
      "step": 24680
    },
    {
      "epoch": 2.4821796611873523,
      "grad_norm": 2.019157648086548,
      "learning_rate": 5.205944655526042e-06,
      "loss": 1.777,
      "step": 24690
    },
    {
      "epoch": 2.4831850399638062,
      "grad_norm": 2.696690082550049,
      "learning_rate": 5.195857570357419e-06,
      "loss": 1.7519,
      "step": 24700
    },
    {
      "epoch": 2.4841904187402606,
      "grad_norm": 2.0132110118865967,
      "learning_rate": 5.185770485188797e-06,
      "loss": 1.7953,
      "step": 24710
    },
    {
      "epoch": 2.4851957975167145,
      "grad_norm": 1.830523133277893,
      "learning_rate": 5.175683400020175e-06,
      "loss": 1.7524,
      "step": 24720
    },
    {
      "epoch": 2.4862011762931684,
      "grad_norm": 2.322242021560669,
      "learning_rate": 5.165596314851552e-06,
      "loss": 1.7316,
      "step": 24730
    },
    {
      "epoch": 2.4872065550696223,
      "grad_norm": 1.8760591745376587,
      "learning_rate": 5.15550922968293e-06,
      "loss": 1.8417,
      "step": 24740
    },
    {
      "epoch": 2.4882119338460766,
      "grad_norm": 1.898146152496338,
      "learning_rate": 5.145422144514307e-06,
      "loss": 1.7924,
      "step": 24750
    },
    {
      "epoch": 2.4892173126225305,
      "grad_norm": 1.967466950416565,
      "learning_rate": 5.135335059345684e-06,
      "loss": 1.6772,
      "step": 24760
    },
    {
      "epoch": 2.4902226913989844,
      "grad_norm": 2.278794765472412,
      "learning_rate": 5.125247974177062e-06,
      "loss": 1.7977,
      "step": 24770
    },
    {
      "epoch": 2.4912280701754383,
      "grad_norm": 2.127316474914551,
      "learning_rate": 5.1151608890084395e-06,
      "loss": 1.7682,
      "step": 24780
    },
    {
      "epoch": 2.4922334489518927,
      "grad_norm": 1.955605387687683,
      "learning_rate": 5.105073803839817e-06,
      "loss": 1.7438,
      "step": 24790
    },
    {
      "epoch": 2.4932388277283466,
      "grad_norm": 2.2523484230041504,
      "learning_rate": 5.0949867186711955e-06,
      "loss": 1.7403,
      "step": 24800
    },
    {
      "epoch": 2.4942442065048005,
      "grad_norm": 1.9171762466430664,
      "learning_rate": 5.084899633502573e-06,
      "loss": 1.7408,
      "step": 24810
    },
    {
      "epoch": 2.495249585281255,
      "grad_norm": 1.7291260957717896,
      "learning_rate": 5.07481254833395e-06,
      "loss": 1.7243,
      "step": 24820
    },
    {
      "epoch": 2.4962549640577087,
      "grad_norm": 2.7070770263671875,
      "learning_rate": 5.064725463165328e-06,
      "loss": 1.7255,
      "step": 24830
    },
    {
      "epoch": 2.4972603428341626,
      "grad_norm": 2.7149007320404053,
      "learning_rate": 5.054638377996705e-06,
      "loss": 1.7919,
      "step": 24840
    },
    {
      "epoch": 2.4982657216106166,
      "grad_norm": 2.091151237487793,
      "learning_rate": 5.044551292828082e-06,
      "loss": 1.8561,
      "step": 24850
    },
    {
      "epoch": 2.499271100387071,
      "grad_norm": 2.2658650875091553,
      "learning_rate": 5.03446420765946e-06,
      "loss": 1.7407,
      "step": 24860
    },
    {
      "epoch": 2.500276479163525,
      "grad_norm": 2.2947144508361816,
      "learning_rate": 5.024377122490838e-06,
      "loss": 1.7637,
      "step": 24870
    },
    {
      "epoch": 2.501281857939979,
      "grad_norm": 1.7529352903366089,
      "learning_rate": 5.014290037322215e-06,
      "loss": 1.7958,
      "step": 24880
    },
    {
      "epoch": 2.502287236716433,
      "grad_norm": 1.8893564939498901,
      "learning_rate": 5.004202952153593e-06,
      "loss": 1.7556,
      "step": 24890
    },
    {
      "epoch": 2.503292615492887,
      "grad_norm": 2.01352858543396,
      "learning_rate": 4.9941158669849705e-06,
      "loss": 1.7259,
      "step": 24900
    },
    {
      "epoch": 2.504297994269341,
      "grad_norm": 2.392336368560791,
      "learning_rate": 4.984028781816348e-06,
      "loss": 1.8208,
      "step": 24910
    },
    {
      "epoch": 2.5053033730457948,
      "grad_norm": 1.719208002090454,
      "learning_rate": 4.973941696647726e-06,
      "loss": 1.7287,
      "step": 24920
    },
    {
      "epoch": 2.506308751822249,
      "grad_norm": 2.6030707359313965,
      "learning_rate": 4.963854611479103e-06,
      "loss": 1.7507,
      "step": 24930
    },
    {
      "epoch": 2.507314130598703,
      "grad_norm": 1.8616880178451538,
      "learning_rate": 4.95376752631048e-06,
      "loss": 1.8069,
      "step": 24940
    },
    {
      "epoch": 2.5083195093751574,
      "grad_norm": 2.2808423042297363,
      "learning_rate": 4.943680441141859e-06,
      "loss": 1.7689,
      "step": 24950
    },
    {
      "epoch": 2.5093248881516113,
      "grad_norm": 1.8609123229980469,
      "learning_rate": 4.933593355973236e-06,
      "loss": 1.7628,
      "step": 24960
    },
    {
      "epoch": 2.510330266928065,
      "grad_norm": 2.144871711730957,
      "learning_rate": 4.923506270804613e-06,
      "loss": 1.6558,
      "step": 24970
    },
    {
      "epoch": 2.511335645704519,
      "grad_norm": 1.8276411294937134,
      "learning_rate": 4.913419185635991e-06,
      "loss": 1.7801,
      "step": 24980
    },
    {
      "epoch": 2.512341024480973,
      "grad_norm": 3.0802829265594482,
      "learning_rate": 4.903332100467368e-06,
      "loss": 1.8295,
      "step": 24990
    },
    {
      "epoch": 2.5133464032574273,
      "grad_norm": 3.6669459342956543,
      "learning_rate": 4.8932450152987455e-06,
      "loss": 1.7252,
      "step": 25000
    },
    {
      "epoch": 2.514351782033881,
      "grad_norm": 2.548267364501953,
      "learning_rate": 4.8831579301301235e-06,
      "loss": 1.6762,
      "step": 25010
    },
    {
      "epoch": 2.5153571608103356,
      "grad_norm": 1.6481634378433228,
      "learning_rate": 4.873070844961501e-06,
      "loss": 1.7971,
      "step": 25020
    },
    {
      "epoch": 2.5163625395867895,
      "grad_norm": 2.484992265701294,
      "learning_rate": 4.862983759792879e-06,
      "loss": 1.7833,
      "step": 25030
    },
    {
      "epoch": 2.5173679183632434,
      "grad_norm": 2.0282387733459473,
      "learning_rate": 4.852896674624257e-06,
      "loss": 1.6774,
      "step": 25040
    },
    {
      "epoch": 2.5183732971396973,
      "grad_norm": 1.8343353271484375,
      "learning_rate": 4.842809589455634e-06,
      "loss": 1.7732,
      "step": 25050
    },
    {
      "epoch": 2.519378675916151,
      "grad_norm": 2.723728895187378,
      "learning_rate": 4.832722504287011e-06,
      "loss": 1.7837,
      "step": 25060
    },
    {
      "epoch": 2.5203840546926055,
      "grad_norm": 2.96882700920105,
      "learning_rate": 4.822635419118389e-06,
      "loss": 1.7009,
      "step": 25070
    },
    {
      "epoch": 2.5213894334690594,
      "grad_norm": 1.764053225517273,
      "learning_rate": 4.812548333949766e-06,
      "loss": 1.746,
      "step": 25080
    },
    {
      "epoch": 2.5223948122455138,
      "grad_norm": 1.9234611988067627,
      "learning_rate": 4.802461248781143e-06,
      "loss": 1.7115,
      "step": 25090
    },
    {
      "epoch": 2.5234001910219677,
      "grad_norm": 2.1205992698669434,
      "learning_rate": 4.792374163612522e-06,
      "loss": 1.7346,
      "step": 25100
    },
    {
      "epoch": 2.5244055697984216,
      "grad_norm": 2.321150541305542,
      "learning_rate": 4.782287078443899e-06,
      "loss": 1.7108,
      "step": 25110
    },
    {
      "epoch": 2.5254109485748755,
      "grad_norm": 2.2802648544311523,
      "learning_rate": 4.7721999932752765e-06,
      "loss": 1.7887,
      "step": 25120
    },
    {
      "epoch": 2.5264163273513294,
      "grad_norm": 3.0098071098327637,
      "learning_rate": 4.7621129081066545e-06,
      "loss": 1.7492,
      "step": 25130
    },
    {
      "epoch": 2.5274217061277837,
      "grad_norm": 1.594374418258667,
      "learning_rate": 4.752025822938032e-06,
      "loss": 1.7895,
      "step": 25140
    },
    {
      "epoch": 2.5284270849042376,
      "grad_norm": 1.9988789558410645,
      "learning_rate": 4.741938737769409e-06,
      "loss": 1.8141,
      "step": 25150
    },
    {
      "epoch": 2.529432463680692,
      "grad_norm": 2.078726053237915,
      "learning_rate": 4.731851652600787e-06,
      "loss": 1.7794,
      "step": 25160
    },
    {
      "epoch": 2.530437842457146,
      "grad_norm": 1.7156846523284912,
      "learning_rate": 4.721764567432164e-06,
      "loss": 1.7074,
      "step": 25170
    },
    {
      "epoch": 2.5314432212335998,
      "grad_norm": 2.113762617111206,
      "learning_rate": 4.711677482263542e-06,
      "loss": 1.7267,
      "step": 25180
    },
    {
      "epoch": 2.5324486000100537,
      "grad_norm": 2.3501524925231934,
      "learning_rate": 4.70159039709492e-06,
      "loss": 1.8118,
      "step": 25190
    },
    {
      "epoch": 2.5334539787865076,
      "grad_norm": 2.4110167026519775,
      "learning_rate": 4.691503311926297e-06,
      "loss": 1.7432,
      "step": 25200
    },
    {
      "epoch": 2.534459357562962,
      "grad_norm": 2.20771861076355,
      "learning_rate": 4.681416226757674e-06,
      "loss": 1.7403,
      "step": 25210
    },
    {
      "epoch": 2.535464736339416,
      "grad_norm": 1.8738844394683838,
      "learning_rate": 4.671329141589052e-06,
      "loss": 1.7636,
      "step": 25220
    },
    {
      "epoch": 2.53647011511587,
      "grad_norm": 1.9207589626312256,
      "learning_rate": 4.6612420564204295e-06,
      "loss": 1.758,
      "step": 25230
    },
    {
      "epoch": 2.537475493892324,
      "grad_norm": 1.7979698181152344,
      "learning_rate": 4.651154971251807e-06,
      "loss": 1.7531,
      "step": 25240
    },
    {
      "epoch": 2.538480872668778,
      "grad_norm": 1.9396765232086182,
      "learning_rate": 4.6410678860831855e-06,
      "loss": 1.7007,
      "step": 25250
    },
    {
      "epoch": 2.539486251445232,
      "grad_norm": 2.9816296100616455,
      "learning_rate": 4.630980800914563e-06,
      "loss": 1.7261,
      "step": 25260
    },
    {
      "epoch": 2.540491630221686,
      "grad_norm": 1.7918716669082642,
      "learning_rate": 4.62089371574594e-06,
      "loss": 1.8121,
      "step": 25270
    },
    {
      "epoch": 2.54149700899814,
      "grad_norm": 1.9679707288742065,
      "learning_rate": 4.610806630577318e-06,
      "loss": 1.7797,
      "step": 25280
    },
    {
      "epoch": 2.542502387774594,
      "grad_norm": 2.26884388923645,
      "learning_rate": 4.600719545408695e-06,
      "loss": 1.806,
      "step": 25290
    },
    {
      "epoch": 2.5435077665510484,
      "grad_norm": 2.654892921447754,
      "learning_rate": 4.590632460240073e-06,
      "loss": 1.7959,
      "step": 25300
    },
    {
      "epoch": 2.5445131453275023,
      "grad_norm": 2.2007088661193848,
      "learning_rate": 4.58054537507145e-06,
      "loss": 1.7721,
      "step": 25310
    },
    {
      "epoch": 2.545518524103956,
      "grad_norm": 2.1082816123962402,
      "learning_rate": 4.570458289902827e-06,
      "loss": 1.7461,
      "step": 25320
    },
    {
      "epoch": 2.54652390288041,
      "grad_norm": 2.138967990875244,
      "learning_rate": 4.560371204734206e-06,
      "loss": 1.8408,
      "step": 25330
    },
    {
      "epoch": 2.547529281656864,
      "grad_norm": 2.5318517684936523,
      "learning_rate": 4.550284119565583e-06,
      "loss": 1.7625,
      "step": 25340
    },
    {
      "epoch": 2.5485346604333183,
      "grad_norm": 2.293943405151367,
      "learning_rate": 4.5401970343969605e-06,
      "loss": 1.7687,
      "step": 25350
    },
    {
      "epoch": 2.5495400392097722,
      "grad_norm": 2.939497470855713,
      "learning_rate": 4.5301099492283385e-06,
      "loss": 1.6376,
      "step": 25360
    },
    {
      "epoch": 2.5505454179862266,
      "grad_norm": 2.144035577774048,
      "learning_rate": 4.520022864059716e-06,
      "loss": 1.8069,
      "step": 25370
    },
    {
      "epoch": 2.5515507967626805,
      "grad_norm": 1.7919955253601074,
      "learning_rate": 4.509935778891093e-06,
      "loss": 1.7584,
      "step": 25380
    },
    {
      "epoch": 2.5525561755391344,
      "grad_norm": 1.9807604551315308,
      "learning_rate": 4.499848693722471e-06,
      "loss": 1.7489,
      "step": 25390
    },
    {
      "epoch": 2.5535615543155883,
      "grad_norm": 1.5982615947723389,
      "learning_rate": 4.489761608553849e-06,
      "loss": 1.7291,
      "step": 25400
    },
    {
      "epoch": 2.554566933092042,
      "grad_norm": 2.615678548812866,
      "learning_rate": 4.479674523385226e-06,
      "loss": 1.753,
      "step": 25410
    },
    {
      "epoch": 2.5555723118684965,
      "grad_norm": 2.2233166694641113,
      "learning_rate": 4.469587438216604e-06,
      "loss": 1.7415,
      "step": 25420
    },
    {
      "epoch": 2.5565776906449504,
      "grad_norm": 3.6438493728637695,
      "learning_rate": 4.459500353047981e-06,
      "loss": 1.7243,
      "step": 25430
    },
    {
      "epoch": 2.557583069421405,
      "grad_norm": 1.9947283267974854,
      "learning_rate": 4.449413267879358e-06,
      "loss": 1.6618,
      "step": 25440
    },
    {
      "epoch": 2.5585884481978587,
      "grad_norm": 1.7766845226287842,
      "learning_rate": 4.439326182710736e-06,
      "loss": 1.7068,
      "step": 25450
    },
    {
      "epoch": 2.5595938269743126,
      "grad_norm": 2.5017435550689697,
      "learning_rate": 4.4292390975421135e-06,
      "loss": 1.8269,
      "step": 25460
    },
    {
      "epoch": 2.5605992057507665,
      "grad_norm": 2.07167387008667,
      "learning_rate": 4.419152012373491e-06,
      "loss": 1.7878,
      "step": 25470
    },
    {
      "epoch": 2.5616045845272204,
      "grad_norm": 3.2832753658294678,
      "learning_rate": 4.4090649272048695e-06,
      "loss": 1.7641,
      "step": 25480
    },
    {
      "epoch": 2.5626099633036747,
      "grad_norm": 2.063920736312866,
      "learning_rate": 4.398977842036247e-06,
      "loss": 1.7892,
      "step": 25490
    },
    {
      "epoch": 2.5636153420801286,
      "grad_norm": 2.1879384517669678,
      "learning_rate": 4.388890756867624e-06,
      "loss": 1.8214,
      "step": 25500
    },
    {
      "epoch": 2.564620720856583,
      "grad_norm": 1.8408265113830566,
      "learning_rate": 4.378803671699002e-06,
      "loss": 1.7175,
      "step": 25510
    },
    {
      "epoch": 2.565626099633037,
      "grad_norm": 1.902620553970337,
      "learning_rate": 4.368716586530379e-06,
      "loss": 1.7175,
      "step": 25520
    },
    {
      "epoch": 2.566631478409491,
      "grad_norm": 1.811523675918579,
      "learning_rate": 4.358629501361756e-06,
      "loss": 1.7435,
      "step": 25530
    },
    {
      "epoch": 2.5676368571859447,
      "grad_norm": 2.2205731868743896,
      "learning_rate": 4.348542416193134e-06,
      "loss": 1.7512,
      "step": 25540
    },
    {
      "epoch": 2.5686422359623986,
      "grad_norm": 2.487980604171753,
      "learning_rate": 4.338455331024511e-06,
      "loss": 1.7713,
      "step": 25550
    },
    {
      "epoch": 2.569647614738853,
      "grad_norm": 2.367617607116699,
      "learning_rate": 4.328368245855889e-06,
      "loss": 1.7708,
      "step": 25560
    },
    {
      "epoch": 2.570652993515307,
      "grad_norm": 2.451991558074951,
      "learning_rate": 4.318281160687267e-06,
      "loss": 1.7415,
      "step": 25570
    },
    {
      "epoch": 2.571658372291761,
      "grad_norm": 2.294661045074463,
      "learning_rate": 4.3081940755186445e-06,
      "loss": 1.633,
      "step": 25580
    },
    {
      "epoch": 2.572663751068215,
      "grad_norm": 2.387929916381836,
      "learning_rate": 4.298106990350022e-06,
      "loss": 1.7477,
      "step": 25590
    },
    {
      "epoch": 2.573669129844669,
      "grad_norm": 2.1769049167633057,
      "learning_rate": 4.2880199051814e-06,
      "loss": 1.7322,
      "step": 25600
    },
    {
      "epoch": 2.574674508621123,
      "grad_norm": 1.8879121541976929,
      "learning_rate": 4.277932820012777e-06,
      "loss": 1.7238,
      "step": 25610
    },
    {
      "epoch": 2.575679887397577,
      "grad_norm": 2.003762722015381,
      "learning_rate": 4.267845734844154e-06,
      "loss": 1.738,
      "step": 25620
    },
    {
      "epoch": 2.576685266174031,
      "grad_norm": 2.6701560020446777,
      "learning_rate": 4.257758649675533e-06,
      "loss": 1.7837,
      "step": 25630
    },
    {
      "epoch": 2.577690644950485,
      "grad_norm": 2.5076937675476074,
      "learning_rate": 4.24767156450691e-06,
      "loss": 1.7241,
      "step": 25640
    },
    {
      "epoch": 2.5786960237269394,
      "grad_norm": 1.8768081665039062,
      "learning_rate": 4.237584479338287e-06,
      "loss": 1.7888,
      "step": 25650
    },
    {
      "epoch": 2.5797014025033933,
      "grad_norm": 1.7879931926727295,
      "learning_rate": 4.227497394169665e-06,
      "loss": 1.7059,
      "step": 25660
    },
    {
      "epoch": 2.580706781279847,
      "grad_norm": 2.0668935775756836,
      "learning_rate": 4.217410309001042e-06,
      "loss": 1.8527,
      "step": 25670
    },
    {
      "epoch": 2.581712160056301,
      "grad_norm": 2.0699350833892822,
      "learning_rate": 4.2073232238324195e-06,
      "loss": 1.7666,
      "step": 25680
    },
    {
      "epoch": 2.582717538832755,
      "grad_norm": 2.1087305545806885,
      "learning_rate": 4.1972361386637975e-06,
      "loss": 1.7459,
      "step": 25690
    },
    {
      "epoch": 2.5837229176092094,
      "grad_norm": 1.8214010000228882,
      "learning_rate": 4.187149053495175e-06,
      "loss": 1.7209,
      "step": 25700
    },
    {
      "epoch": 2.5847282963856633,
      "grad_norm": 2.2148377895355225,
      "learning_rate": 4.177061968326553e-06,
      "loss": 1.7097,
      "step": 25710
    },
    {
      "epoch": 2.585733675162117,
      "grad_norm": 1.7934280633926392,
      "learning_rate": 4.166974883157931e-06,
      "loss": 1.8013,
      "step": 25720
    },
    {
      "epoch": 2.5867390539385715,
      "grad_norm": 1.9537996053695679,
      "learning_rate": 4.156887797989308e-06,
      "loss": 1.7794,
      "step": 25730
    },
    {
      "epoch": 2.5877444327150254,
      "grad_norm": 2.5648398399353027,
      "learning_rate": 4.146800712820685e-06,
      "loss": 1.7426,
      "step": 25740
    },
    {
      "epoch": 2.5887498114914793,
      "grad_norm": 1.8476709127426147,
      "learning_rate": 4.136713627652063e-06,
      "loss": 1.6945,
      "step": 25750
    },
    {
      "epoch": 2.589755190267933,
      "grad_norm": 1.8455255031585693,
      "learning_rate": 4.12662654248344e-06,
      "loss": 1.7239,
      "step": 25760
    },
    {
      "epoch": 2.5907605690443876,
      "grad_norm": 2.440582752227783,
      "learning_rate": 4.116539457314817e-06,
      "loss": 1.8097,
      "step": 25770
    },
    {
      "epoch": 2.5917659478208415,
      "grad_norm": 2.4656498432159424,
      "learning_rate": 4.106452372146196e-06,
      "loss": 1.7595,
      "step": 25780
    },
    {
      "epoch": 2.5927713265972954,
      "grad_norm": 1.7718305587768555,
      "learning_rate": 4.096365286977573e-06,
      "loss": 1.758,
      "step": 25790
    },
    {
      "epoch": 2.5937767053737497,
      "grad_norm": 2.117746353149414,
      "learning_rate": 4.0862782018089505e-06,
      "loss": 1.6583,
      "step": 25800
    },
    {
      "epoch": 2.5947820841502036,
      "grad_norm": 2.4815285205841064,
      "learning_rate": 4.0761911166403285e-06,
      "loss": 1.7316,
      "step": 25810
    },
    {
      "epoch": 2.5957874629266575,
      "grad_norm": 1.9882694482803345,
      "learning_rate": 4.066104031471706e-06,
      "loss": 1.7978,
      "step": 25820
    },
    {
      "epoch": 2.5967928417031114,
      "grad_norm": 2.6296544075012207,
      "learning_rate": 4.056016946303083e-06,
      "loss": 1.7617,
      "step": 25830
    },
    {
      "epoch": 2.5977982204795658,
      "grad_norm": 2.2093698978424072,
      "learning_rate": 4.045929861134461e-06,
      "loss": 1.7501,
      "step": 25840
    },
    {
      "epoch": 2.5988035992560197,
      "grad_norm": 2.0770835876464844,
      "learning_rate": 4.035842775965838e-06,
      "loss": 1.7931,
      "step": 25850
    },
    {
      "epoch": 2.5998089780324736,
      "grad_norm": 2.0081069469451904,
      "learning_rate": 4.025755690797216e-06,
      "loss": 1.8414,
      "step": 25860
    },
    {
      "epoch": 2.600814356808928,
      "grad_norm": 3.6317310333251953,
      "learning_rate": 4.015668605628594e-06,
      "loss": 1.6914,
      "step": 25870
    },
    {
      "epoch": 2.601819735585382,
      "grad_norm": 1.9724175930023193,
      "learning_rate": 4.005581520459971e-06,
      "loss": 1.8175,
      "step": 25880
    },
    {
      "epoch": 2.6028251143618357,
      "grad_norm": 2.0041842460632324,
      "learning_rate": 3.995494435291348e-06,
      "loss": 1.743,
      "step": 25890
    },
    {
      "epoch": 2.6038304931382896,
      "grad_norm": 1.6366478204727173,
      "learning_rate": 3.985407350122726e-06,
      "loss": 1.7125,
      "step": 25900
    },
    {
      "epoch": 2.604835871914744,
      "grad_norm": 1.8693718910217285,
      "learning_rate": 3.9753202649541035e-06,
      "loss": 1.7875,
      "step": 25910
    },
    {
      "epoch": 2.605841250691198,
      "grad_norm": 2.3219614028930664,
      "learning_rate": 3.9652331797854815e-06,
      "loss": 1.7146,
      "step": 25920
    },
    {
      "epoch": 2.6068466294676518,
      "grad_norm": 2.4808170795440674,
      "learning_rate": 3.955146094616859e-06,
      "loss": 1.8213,
      "step": 25930
    },
    {
      "epoch": 2.607852008244106,
      "grad_norm": 2.649904251098633,
      "learning_rate": 3.945059009448237e-06,
      "loss": 1.7805,
      "step": 25940
    },
    {
      "epoch": 2.60885738702056,
      "grad_norm": 2.2402429580688477,
      "learning_rate": 3.934971924279615e-06,
      "loss": 1.7578,
      "step": 25950
    },
    {
      "epoch": 2.609862765797014,
      "grad_norm": 1.893703579902649,
      "learning_rate": 3.924884839110992e-06,
      "loss": 1.7594,
      "step": 25960
    },
    {
      "epoch": 2.610868144573468,
      "grad_norm": 2.303140163421631,
      "learning_rate": 3.914797753942369e-06,
      "loss": 1.7698,
      "step": 25970
    },
    {
      "epoch": 2.611873523349922,
      "grad_norm": 2.0961081981658936,
      "learning_rate": 3.904710668773747e-06,
      "loss": 1.7233,
      "step": 25980
    },
    {
      "epoch": 2.612878902126376,
      "grad_norm": 2.2019834518432617,
      "learning_rate": 3.894623583605124e-06,
      "loss": 1.7388,
      "step": 25990
    },
    {
      "epoch": 2.61388428090283,
      "grad_norm": 1.5934287309646606,
      "learning_rate": 3.884536498436501e-06,
      "loss": 1.7536,
      "step": 26000
    },
    {
      "epoch": 2.6148896596792843,
      "grad_norm": 2.356487274169922,
      "learning_rate": 3.87444941326788e-06,
      "loss": 1.7166,
      "step": 26010
    },
    {
      "epoch": 2.6158950384557382,
      "grad_norm": 2.9526233673095703,
      "learning_rate": 3.864362328099257e-06,
      "loss": 1.7555,
      "step": 26020
    },
    {
      "epoch": 2.616900417232192,
      "grad_norm": 3.3036277294158936,
      "learning_rate": 3.8542752429306345e-06,
      "loss": 1.7228,
      "step": 26030
    },
    {
      "epoch": 2.617905796008646,
      "grad_norm": 2.3780226707458496,
      "learning_rate": 3.8441881577620125e-06,
      "loss": 1.7362,
      "step": 26040
    },
    {
      "epoch": 2.6189111747851004,
      "grad_norm": 2.2311391830444336,
      "learning_rate": 3.83410107259339e-06,
      "loss": 1.8507,
      "step": 26050
    },
    {
      "epoch": 2.6199165535615543,
      "grad_norm": 2.0301897525787354,
      "learning_rate": 3.824013987424767e-06,
      "loss": 1.7408,
      "step": 26060
    },
    {
      "epoch": 2.620921932338008,
      "grad_norm": 1.845258355140686,
      "learning_rate": 3.8139269022561453e-06,
      "loss": 1.7604,
      "step": 26070
    },
    {
      "epoch": 2.6219273111144625,
      "grad_norm": 2.221876859664917,
      "learning_rate": 3.8038398170875225e-06,
      "loss": 1.7448,
      "step": 26080
    },
    {
      "epoch": 2.6229326898909164,
      "grad_norm": 1.9286774396896362,
      "learning_rate": 3.7937527319188996e-06,
      "loss": 1.7792,
      "step": 26090
    },
    {
      "epoch": 2.6239380686673703,
      "grad_norm": 1.8053961992263794,
      "learning_rate": 3.7836656467502776e-06,
      "loss": 1.7917,
      "step": 26100
    },
    {
      "epoch": 2.6249434474438242,
      "grad_norm": 2.2623226642608643,
      "learning_rate": 3.773578561581655e-06,
      "loss": 1.7108,
      "step": 26110
    },
    {
      "epoch": 2.6259488262202786,
      "grad_norm": 2.101438045501709,
      "learning_rate": 3.7634914764130324e-06,
      "loss": 1.7761,
      "step": 26120
    },
    {
      "epoch": 2.6269542049967325,
      "grad_norm": 1.8165470361709595,
      "learning_rate": 3.7534043912444104e-06,
      "loss": 1.6863,
      "step": 26130
    },
    {
      "epoch": 2.6279595837731864,
      "grad_norm": 2.723757028579712,
      "learning_rate": 3.743317306075788e-06,
      "loss": 1.7836,
      "step": 26140
    },
    {
      "epoch": 2.6289649625496407,
      "grad_norm": 2.7842719554901123,
      "learning_rate": 3.7332302209071655e-06,
      "loss": 1.7253,
      "step": 26150
    },
    {
      "epoch": 2.6299703413260946,
      "grad_norm": 2.388043165206909,
      "learning_rate": 3.7231431357385427e-06,
      "loss": 1.6747,
      "step": 26160
    },
    {
      "epoch": 2.6309757201025485,
      "grad_norm": 2.403873920440674,
      "learning_rate": 3.7130560505699203e-06,
      "loss": 1.6998,
      "step": 26170
    },
    {
      "epoch": 2.6319810988790024,
      "grad_norm": 1.9019640684127808,
      "learning_rate": 3.7029689654012983e-06,
      "loss": 1.7987,
      "step": 26180
    },
    {
      "epoch": 2.632986477655457,
      "grad_norm": 1.618779182434082,
      "learning_rate": 3.6928818802326755e-06,
      "loss": 1.7556,
      "step": 26190
    },
    {
      "epoch": 2.6339918564319107,
      "grad_norm": 1.631973385810852,
      "learning_rate": 3.682794795064053e-06,
      "loss": 1.7414,
      "step": 26200
    },
    {
      "epoch": 2.6349972352083646,
      "grad_norm": 2.1615240573883057,
      "learning_rate": 3.6727077098954306e-06,
      "loss": 1.7631,
      "step": 26210
    },
    {
      "epoch": 2.636002613984819,
      "grad_norm": 2.1854794025421143,
      "learning_rate": 3.662620624726808e-06,
      "loss": 1.7712,
      "step": 26220
    },
    {
      "epoch": 2.637007992761273,
      "grad_norm": 2.539463996887207,
      "learning_rate": 3.652533539558186e-06,
      "loss": 1.7531,
      "step": 26230
    },
    {
      "epoch": 2.6380133715377267,
      "grad_norm": 1.7795493602752686,
      "learning_rate": 3.6424464543895634e-06,
      "loss": 1.7028,
      "step": 26240
    },
    {
      "epoch": 2.6390187503141807,
      "grad_norm": 2.494581460952759,
      "learning_rate": 3.6323593692209405e-06,
      "loss": 1.7157,
      "step": 26250
    },
    {
      "epoch": 2.640024129090635,
      "grad_norm": 2.284339427947998,
      "learning_rate": 3.6222722840523185e-06,
      "loss": 1.7287,
      "step": 26260
    },
    {
      "epoch": 2.641029507867089,
      "grad_norm": 1.9727132320404053,
      "learning_rate": 3.612185198883696e-06,
      "loss": 1.7241,
      "step": 26270
    },
    {
      "epoch": 2.642034886643543,
      "grad_norm": 2.685242176055908,
      "learning_rate": 3.6020981137150733e-06,
      "loss": 1.7804,
      "step": 26280
    },
    {
      "epoch": 2.643040265419997,
      "grad_norm": 2.2884891033172607,
      "learning_rate": 3.592011028546451e-06,
      "loss": 1.7584,
      "step": 26290
    },
    {
      "epoch": 2.644045644196451,
      "grad_norm": 2.2602641582489014,
      "learning_rate": 3.581923943377829e-06,
      "loss": 1.7806,
      "step": 26300
    },
    {
      "epoch": 2.645051022972905,
      "grad_norm": 2.6456589698791504,
      "learning_rate": 3.571836858209206e-06,
      "loss": 1.7065,
      "step": 26310
    },
    {
      "epoch": 2.646056401749359,
      "grad_norm": 1.832617998123169,
      "learning_rate": 3.5617497730405836e-06,
      "loss": 1.7319,
      "step": 26320
    },
    {
      "epoch": 2.647061780525813,
      "grad_norm": 2.5052802562713623,
      "learning_rate": 3.5516626878719616e-06,
      "loss": 1.7033,
      "step": 26330
    },
    {
      "epoch": 2.648067159302267,
      "grad_norm": 2.0475196838378906,
      "learning_rate": 3.541575602703339e-06,
      "loss": 1.6886,
      "step": 26340
    },
    {
      "epoch": 2.649072538078721,
      "grad_norm": 1.9505246877670288,
      "learning_rate": 3.5314885175347164e-06,
      "loss": 1.79,
      "step": 26350
    },
    {
      "epoch": 2.6500779168551754,
      "grad_norm": 2.0290725231170654,
      "learning_rate": 3.521401432366094e-06,
      "loss": 1.6643,
      "step": 26360
    },
    {
      "epoch": 2.6510832956316293,
      "grad_norm": 2.3852500915527344,
      "learning_rate": 3.511314347197472e-06,
      "loss": 1.7265,
      "step": 26370
    },
    {
      "epoch": 2.652088674408083,
      "grad_norm": 2.160520076751709,
      "learning_rate": 3.501227262028849e-06,
      "loss": 1.7823,
      "step": 26380
    },
    {
      "epoch": 2.653094053184537,
      "grad_norm": 2.242309808731079,
      "learning_rate": 3.4911401768602267e-06,
      "loss": 1.8048,
      "step": 26390
    },
    {
      "epoch": 2.6540994319609914,
      "grad_norm": 2.197554349899292,
      "learning_rate": 3.4810530916916043e-06,
      "loss": 1.7462,
      "step": 26400
    },
    {
      "epoch": 2.6551048107374453,
      "grad_norm": 2.4317729473114014,
      "learning_rate": 3.470966006522982e-06,
      "loss": 1.6811,
      "step": 26410
    },
    {
      "epoch": 2.656110189513899,
      "grad_norm": 2.6613242626190186,
      "learning_rate": 3.4608789213543595e-06,
      "loss": 1.7778,
      "step": 26420
    },
    {
      "epoch": 2.6571155682903536,
      "grad_norm": 2.5611143112182617,
      "learning_rate": 3.450791836185737e-06,
      "loss": 1.8271,
      "step": 26430
    },
    {
      "epoch": 2.6581209470668075,
      "grad_norm": 1.915554404258728,
      "learning_rate": 3.4407047510171142e-06,
      "loss": 1.8195,
      "step": 26440
    },
    {
      "epoch": 2.6591263258432614,
      "grad_norm": 1.8422623872756958,
      "learning_rate": 3.4306176658484922e-06,
      "loss": 1.7539,
      "step": 26450
    },
    {
      "epoch": 2.6601317046197153,
      "grad_norm": 2.0150787830352783,
      "learning_rate": 3.42053058067987e-06,
      "loss": 1.7637,
      "step": 26460
    },
    {
      "epoch": 2.6611370833961696,
      "grad_norm": 1.843369722366333,
      "learning_rate": 3.410443495511247e-06,
      "loss": 1.8379,
      "step": 26470
    },
    {
      "epoch": 2.6621424621726235,
      "grad_norm": 2.563232898712158,
      "learning_rate": 3.4003564103426246e-06,
      "loss": 1.7899,
      "step": 26480
    },
    {
      "epoch": 2.6631478409490774,
      "grad_norm": 2.4713189601898193,
      "learning_rate": 3.3902693251740026e-06,
      "loss": 1.7509,
      "step": 26490
    },
    {
      "epoch": 2.6641532197255318,
      "grad_norm": 1.999507188796997,
      "learning_rate": 3.3801822400053797e-06,
      "loss": 1.821,
      "step": 26500
    },
    {
      "epoch": 2.6651585985019857,
      "grad_norm": 1.9819544553756714,
      "learning_rate": 3.3700951548367573e-06,
      "loss": 1.8521,
      "step": 26510
    },
    {
      "epoch": 2.6661639772784396,
      "grad_norm": 1.953561544418335,
      "learning_rate": 3.3600080696681353e-06,
      "loss": 1.7907,
      "step": 26520
    },
    {
      "epoch": 2.6671693560548935,
      "grad_norm": 2.065284013748169,
      "learning_rate": 3.3499209844995125e-06,
      "loss": 1.7747,
      "step": 26530
    },
    {
      "epoch": 2.668174734831348,
      "grad_norm": 1.929301381111145,
      "learning_rate": 3.340842607847752e-06,
      "loss": 1.755,
      "step": 26540
    },
    {
      "epoch": 2.6691801136078017,
      "grad_norm": 2.159306049346924,
      "learning_rate": 3.33075552267913e-06,
      "loss": 1.7213,
      "step": 26550
    },
    {
      "epoch": 2.6701854923842556,
      "grad_norm": 2.4483208656311035,
      "learning_rate": 3.3206684375105076e-06,
      "loss": 1.8381,
      "step": 26560
    },
    {
      "epoch": 2.67119087116071,
      "grad_norm": 2.584632396697998,
      "learning_rate": 3.3105813523418848e-06,
      "loss": 1.7488,
      "step": 26570
    },
    {
      "epoch": 2.672196249937164,
      "grad_norm": 2.2206547260284424,
      "learning_rate": 3.3004942671732628e-06,
      "loss": 1.8297,
      "step": 26580
    },
    {
      "epoch": 2.6732016287136178,
      "grad_norm": 2.2752623558044434,
      "learning_rate": 3.2904071820046403e-06,
      "loss": 1.7462,
      "step": 26590
    },
    {
      "epoch": 2.6742070074900717,
      "grad_norm": 2.698014974594116,
      "learning_rate": 3.2803200968360175e-06,
      "loss": 1.6843,
      "step": 26600
    },
    {
      "epoch": 2.675212386266526,
      "grad_norm": 2.457606077194214,
      "learning_rate": 3.270233011667395e-06,
      "loss": 1.7666,
      "step": 26610
    },
    {
      "epoch": 2.67621776504298,
      "grad_norm": 2.658456325531006,
      "learning_rate": 3.260145926498773e-06,
      "loss": 1.6783,
      "step": 26620
    },
    {
      "epoch": 2.677223143819434,
      "grad_norm": 1.935433030128479,
      "learning_rate": 3.2500588413301503e-06,
      "loss": 1.711,
      "step": 26630
    },
    {
      "epoch": 2.678228522595888,
      "grad_norm": 2.3885715007781982,
      "learning_rate": 3.239971756161528e-06,
      "loss": 1.7529,
      "step": 26640
    },
    {
      "epoch": 2.679233901372342,
      "grad_norm": 2.325146198272705,
      "learning_rate": 3.2298846709929054e-06,
      "loss": 1.6942,
      "step": 26650
    },
    {
      "epoch": 2.680239280148796,
      "grad_norm": 2.3983757495880127,
      "learning_rate": 3.219797585824283e-06,
      "loss": 1.754,
      "step": 26660
    },
    {
      "epoch": 2.68124465892525,
      "grad_norm": 2.5142149925231934,
      "learning_rate": 3.2097105006556606e-06,
      "loss": 1.7292,
      "step": 26670
    },
    {
      "epoch": 2.6822500377017042,
      "grad_norm": 1.9380046129226685,
      "learning_rate": 3.199623415487038e-06,
      "loss": 1.7768,
      "step": 26680
    },
    {
      "epoch": 2.683255416478158,
      "grad_norm": 2.4109842777252197,
      "learning_rate": 3.1895363303184158e-06,
      "loss": 1.8157,
      "step": 26690
    },
    {
      "epoch": 2.684260795254612,
      "grad_norm": 1.8699225187301636,
      "learning_rate": 3.1794492451497933e-06,
      "loss": 1.8374,
      "step": 26700
    },
    {
      "epoch": 2.6852661740310664,
      "grad_norm": 1.8651750087738037,
      "learning_rate": 3.169362159981171e-06,
      "loss": 1.6983,
      "step": 26710
    },
    {
      "epoch": 2.6862715528075203,
      "grad_norm": 1.944521188735962,
      "learning_rate": 3.1592750748125485e-06,
      "loss": 1.7252,
      "step": 26720
    },
    {
      "epoch": 2.687276931583974,
      "grad_norm": 1.6867594718933105,
      "learning_rate": 3.1491879896439257e-06,
      "loss": 1.8037,
      "step": 26730
    },
    {
      "epoch": 2.688282310360428,
      "grad_norm": 2.01436710357666,
      "learning_rate": 3.1391009044753037e-06,
      "loss": 1.8162,
      "step": 26740
    },
    {
      "epoch": 2.6892876891368824,
      "grad_norm": 2.364997625350952,
      "learning_rate": 3.1290138193066813e-06,
      "loss": 1.6325,
      "step": 26750
    },
    {
      "epoch": 2.6902930679133363,
      "grad_norm": 1.6009339094161987,
      "learning_rate": 3.1189267341380584e-06,
      "loss": 1.6976,
      "step": 26760
    },
    {
      "epoch": 2.6912984466897902,
      "grad_norm": 1.8648015260696411,
      "learning_rate": 3.1088396489694364e-06,
      "loss": 1.6755,
      "step": 26770
    },
    {
      "epoch": 2.6923038254662446,
      "grad_norm": 1.7850017547607422,
      "learning_rate": 3.098752563800814e-06,
      "loss": 1.8128,
      "step": 26780
    },
    {
      "epoch": 2.6933092042426985,
      "grad_norm": 2.2968590259552,
      "learning_rate": 3.088665478632191e-06,
      "loss": 1.7789,
      "step": 26790
    },
    {
      "epoch": 2.6943145830191524,
      "grad_norm": 2.021186351776123,
      "learning_rate": 3.0785783934635688e-06,
      "loss": 1.7078,
      "step": 26800
    },
    {
      "epoch": 2.6953199617956063,
      "grad_norm": 2.5586652755737305,
      "learning_rate": 3.0684913082949468e-06,
      "loss": 1.8255,
      "step": 26810
    },
    {
      "epoch": 2.6963253405720606,
      "grad_norm": 2.5628910064697266,
      "learning_rate": 3.058404223126324e-06,
      "loss": 1.7131,
      "step": 26820
    },
    {
      "epoch": 2.6973307193485145,
      "grad_norm": 1.968574047088623,
      "learning_rate": 3.0483171379577015e-06,
      "loss": 1.7127,
      "step": 26830
    },
    {
      "epoch": 2.6983360981249684,
      "grad_norm": 1.7660340070724487,
      "learning_rate": 3.038230052789079e-06,
      "loss": 1.7599,
      "step": 26840
    },
    {
      "epoch": 2.699341476901423,
      "grad_norm": 1.903481125831604,
      "learning_rate": 3.0281429676204567e-06,
      "loss": 1.7759,
      "step": 26850
    },
    {
      "epoch": 2.7003468556778767,
      "grad_norm": 2.549762725830078,
      "learning_rate": 3.0180558824518343e-06,
      "loss": 1.8426,
      "step": 26860
    },
    {
      "epoch": 2.7013522344543306,
      "grad_norm": 1.9624216556549072,
      "learning_rate": 3.007968797283212e-06,
      "loss": 1.7527,
      "step": 26870
    },
    {
      "epoch": 2.7023576132307845,
      "grad_norm": 1.9600998163223267,
      "learning_rate": 2.997881712114589e-06,
      "loss": 1.724,
      "step": 26880
    },
    {
      "epoch": 2.703362992007239,
      "grad_norm": 1.9937009811401367,
      "learning_rate": 2.987794626945967e-06,
      "loss": 1.8005,
      "step": 26890
    },
    {
      "epoch": 2.7043683707836927,
      "grad_norm": 2.6526641845703125,
      "learning_rate": 2.9777075417773446e-06,
      "loss": 1.6347,
      "step": 26900
    },
    {
      "epoch": 2.7053737495601466,
      "grad_norm": 2.0944790840148926,
      "learning_rate": 2.9676204566087218e-06,
      "loss": 1.8073,
      "step": 26910
    },
    {
      "epoch": 2.706379128336601,
      "grad_norm": 2.7984976768493652,
      "learning_rate": 2.9575333714400994e-06,
      "loss": 1.7924,
      "step": 26920
    },
    {
      "epoch": 2.707384507113055,
      "grad_norm": 1.6669518947601318,
      "learning_rate": 2.9474462862714774e-06,
      "loss": 1.7651,
      "step": 26930
    },
    {
      "epoch": 2.708389885889509,
      "grad_norm": 1.977272391319275,
      "learning_rate": 2.9373592011028545e-06,
      "loss": 1.7222,
      "step": 26940
    },
    {
      "epoch": 2.7093952646659627,
      "grad_norm": 2.9789843559265137,
      "learning_rate": 2.927272115934232e-06,
      "loss": 1.793,
      "step": 26950
    },
    {
      "epoch": 2.710400643442417,
      "grad_norm": 2.039738178253174,
      "learning_rate": 2.91718503076561e-06,
      "loss": 1.7432,
      "step": 26960
    },
    {
      "epoch": 2.711406022218871,
      "grad_norm": 1.9286706447601318,
      "learning_rate": 2.9070979455969877e-06,
      "loss": 1.821,
      "step": 26970
    },
    {
      "epoch": 2.712411400995325,
      "grad_norm": 2.5369632244110107,
      "learning_rate": 2.897010860428365e-06,
      "loss": 1.8091,
      "step": 26980
    },
    {
      "epoch": 2.713416779771779,
      "grad_norm": 2.36629319190979,
      "learning_rate": 2.8869237752597424e-06,
      "loss": 1.7271,
      "step": 26990
    },
    {
      "epoch": 2.714422158548233,
      "grad_norm": 2.2267708778381348,
      "learning_rate": 2.8768366900911205e-06,
      "loss": 1.7014,
      "step": 27000
    },
    {
      "epoch": 2.715427537324687,
      "grad_norm": 2.2642242908477783,
      "learning_rate": 2.8667496049224976e-06,
      "loss": 1.6512,
      "step": 27010
    },
    {
      "epoch": 2.716432916101141,
      "grad_norm": 2.3482167720794678,
      "learning_rate": 2.856662519753875e-06,
      "loss": 1.7074,
      "step": 27020
    },
    {
      "epoch": 2.7174382948775953,
      "grad_norm": 2.411478281021118,
      "learning_rate": 2.8465754345852528e-06,
      "loss": 1.6584,
      "step": 27030
    },
    {
      "epoch": 2.718443673654049,
      "grad_norm": 1.9248250722885132,
      "learning_rate": 2.8364883494166304e-06,
      "loss": 1.7915,
      "step": 27040
    },
    {
      "epoch": 2.719449052430503,
      "grad_norm": 1.9212766885757446,
      "learning_rate": 2.826401264248008e-06,
      "loss": 1.7988,
      "step": 27050
    },
    {
      "epoch": 2.7204544312069574,
      "grad_norm": 2.0667266845703125,
      "learning_rate": 2.8163141790793855e-06,
      "loss": 1.7674,
      "step": 27060
    },
    {
      "epoch": 2.7214598099834113,
      "grad_norm": 1.982231616973877,
      "learning_rate": 2.8062270939107627e-06,
      "loss": 1.7714,
      "step": 27070
    },
    {
      "epoch": 2.722465188759865,
      "grad_norm": 2.028337001800537,
      "learning_rate": 2.7961400087421407e-06,
      "loss": 1.7095,
      "step": 27080
    },
    {
      "epoch": 2.723470567536319,
      "grad_norm": 2.3041834831237793,
      "learning_rate": 2.7860529235735183e-06,
      "loss": 1.8216,
      "step": 27090
    },
    {
      "epoch": 2.7244759463127735,
      "grad_norm": 2.0141847133636475,
      "learning_rate": 2.7759658384048954e-06,
      "loss": 1.6738,
      "step": 27100
    },
    {
      "epoch": 2.7254813250892274,
      "grad_norm": 1.79133141040802,
      "learning_rate": 2.765878753236273e-06,
      "loss": 1.7222,
      "step": 27110
    },
    {
      "epoch": 2.7264867038656813,
      "grad_norm": 1.9536221027374268,
      "learning_rate": 2.755791668067651e-06,
      "loss": 1.728,
      "step": 27120
    },
    {
      "epoch": 2.7274920826421356,
      "grad_norm": 1.824765682220459,
      "learning_rate": 2.745704582899028e-06,
      "loss": 1.7836,
      "step": 27130
    },
    {
      "epoch": 2.7284974614185895,
      "grad_norm": 2.153477191925049,
      "learning_rate": 2.7356174977304058e-06,
      "loss": 1.7171,
      "step": 27140
    },
    {
      "epoch": 2.7295028401950434,
      "grad_norm": 4.011164665222168,
      "learning_rate": 2.725530412561784e-06,
      "loss": 1.7633,
      "step": 27150
    },
    {
      "epoch": 2.7305082189714973,
      "grad_norm": 2.2630832195281982,
      "learning_rate": 2.715443327393161e-06,
      "loss": 1.6977,
      "step": 27160
    },
    {
      "epoch": 2.7315135977479517,
      "grad_norm": 1.8604258298873901,
      "learning_rate": 2.7053562422245385e-06,
      "loss": 1.6868,
      "step": 27170
    },
    {
      "epoch": 2.7325189765244056,
      "grad_norm": 2.245316505432129,
      "learning_rate": 2.695269157055916e-06,
      "loss": 1.7895,
      "step": 27180
    },
    {
      "epoch": 2.7335243553008595,
      "grad_norm": 1.863674283027649,
      "learning_rate": 2.6851820718872937e-06,
      "loss": 1.7379,
      "step": 27190
    },
    {
      "epoch": 2.734529734077314,
      "grad_norm": 2.0178165435791016,
      "learning_rate": 2.6750949867186713e-06,
      "loss": 1.7477,
      "step": 27200
    },
    {
      "epoch": 2.7355351128537677,
      "grad_norm": 1.66182541847229,
      "learning_rate": 2.665007901550049e-06,
      "loss": 1.756,
      "step": 27210
    },
    {
      "epoch": 2.7365404916302216,
      "grad_norm": 1.846899390220642,
      "learning_rate": 2.654920816381426e-06,
      "loss": 1.7012,
      "step": 27220
    },
    {
      "epoch": 2.7375458704066755,
      "grad_norm": 2.3165109157562256,
      "learning_rate": 2.644833731212804e-06,
      "loss": 1.7814,
      "step": 27230
    },
    {
      "epoch": 2.73855124918313,
      "grad_norm": 2.319654941558838,
      "learning_rate": 2.6347466460441816e-06,
      "loss": 1.7861,
      "step": 27240
    },
    {
      "epoch": 2.7395566279595838,
      "grad_norm": 2.209031820297241,
      "learning_rate": 2.6246595608755588e-06,
      "loss": 1.6549,
      "step": 27250
    },
    {
      "epoch": 2.7405620067360377,
      "grad_norm": 1.9094737768173218,
      "learning_rate": 2.6145724757069364e-06,
      "loss": 1.6909,
      "step": 27260
    },
    {
      "epoch": 2.741567385512492,
      "grad_norm": 2.91713809967041,
      "learning_rate": 2.6044853905383144e-06,
      "loss": 1.7837,
      "step": 27270
    },
    {
      "epoch": 2.742572764288946,
      "grad_norm": 3.6018950939178467,
      "learning_rate": 2.594398305369692e-06,
      "loss": 1.7443,
      "step": 27280
    },
    {
      "epoch": 2.7435781430654,
      "grad_norm": 2.041043758392334,
      "learning_rate": 2.584311220201069e-06,
      "loss": 1.7053,
      "step": 27290
    },
    {
      "epoch": 2.7445835218418537,
      "grad_norm": 2.7902512550354004,
      "learning_rate": 2.5742241350324467e-06,
      "loss": 1.799,
      "step": 27300
    },
    {
      "epoch": 2.745588900618308,
      "grad_norm": 2.4288249015808105,
      "learning_rate": 2.5641370498638247e-06,
      "loss": 1.8273,
      "step": 27310
    },
    {
      "epoch": 2.746594279394762,
      "grad_norm": 2.720423460006714,
      "learning_rate": 2.554049964695202e-06,
      "loss": 1.8013,
      "step": 27320
    },
    {
      "epoch": 2.747599658171216,
      "grad_norm": 2.1909453868865967,
      "learning_rate": 2.5439628795265795e-06,
      "loss": 1.7098,
      "step": 27330
    },
    {
      "epoch": 2.74860503694767,
      "grad_norm": 2.248643636703491,
      "learning_rate": 2.5338757943579575e-06,
      "loss": 1.8358,
      "step": 27340
    },
    {
      "epoch": 2.749610415724124,
      "grad_norm": 1.9617791175842285,
      "learning_rate": 2.5237887091893346e-06,
      "loss": 1.7641,
      "step": 27350
    },
    {
      "epoch": 2.750615794500578,
      "grad_norm": 2.099431276321411,
      "learning_rate": 2.513701624020712e-06,
      "loss": 1.8696,
      "step": 27360
    },
    {
      "epoch": 2.751621173277032,
      "grad_norm": 2.273128032684326,
      "learning_rate": 2.50361453885209e-06,
      "loss": 1.7793,
      "step": 27370
    },
    {
      "epoch": 2.7526265520534863,
      "grad_norm": 2.1859962940216064,
      "learning_rate": 2.4935274536834674e-06,
      "loss": 1.8286,
      "step": 27380
    },
    {
      "epoch": 2.75363193082994,
      "grad_norm": 2.4755191802978516,
      "learning_rate": 2.483440368514845e-06,
      "loss": 1.6792,
      "step": 27390
    },
    {
      "epoch": 2.754637309606394,
      "grad_norm": 2.4040756225585938,
      "learning_rate": 2.4733532833462225e-06,
      "loss": 1.8261,
      "step": 27400
    },
    {
      "epoch": 2.7556426883828484,
      "grad_norm": 1.948516845703125,
      "learning_rate": 2.4632661981775997e-06,
      "loss": 1.7449,
      "step": 27410
    },
    {
      "epoch": 2.7566480671593023,
      "grad_norm": 1.8406598567962646,
      "learning_rate": 2.4531791130089777e-06,
      "loss": 1.7938,
      "step": 27420
    },
    {
      "epoch": 2.7576534459357562,
      "grad_norm": 2.686530351638794,
      "learning_rate": 2.4430920278403553e-06,
      "loss": 1.7383,
      "step": 27430
    },
    {
      "epoch": 2.75865882471221,
      "grad_norm": 2.754728078842163,
      "learning_rate": 2.4330049426717325e-06,
      "loss": 1.7169,
      "step": 27440
    },
    {
      "epoch": 2.7596642034886645,
      "grad_norm": 2.5321249961853027,
      "learning_rate": 2.42291785750311e-06,
      "loss": 1.678,
      "step": 27450
    },
    {
      "epoch": 2.7606695822651184,
      "grad_norm": 1.9813514947891235,
      "learning_rate": 2.412830772334488e-06,
      "loss": 1.7194,
      "step": 27460
    },
    {
      "epoch": 2.7616749610415723,
      "grad_norm": 2.3308818340301514,
      "learning_rate": 2.4027436871658652e-06,
      "loss": 1.7516,
      "step": 27470
    },
    {
      "epoch": 2.7626803398180266,
      "grad_norm": 2.3737072944641113,
      "learning_rate": 2.392656601997243e-06,
      "loss": 1.727,
      "step": 27480
    },
    {
      "epoch": 2.7636857185944805,
      "grad_norm": 2.3959150314331055,
      "learning_rate": 2.3825695168286204e-06,
      "loss": 1.7734,
      "step": 27490
    },
    {
      "epoch": 2.7646910973709344,
      "grad_norm": 2.018920660018921,
      "learning_rate": 2.372482431659998e-06,
      "loss": 1.7492,
      "step": 27500
    },
    {
      "epoch": 2.7656964761473883,
      "grad_norm": 1.893959403038025,
      "learning_rate": 2.3623953464913755e-06,
      "loss": 1.7856,
      "step": 27510
    },
    {
      "epoch": 2.7667018549238427,
      "grad_norm": 2.8784701824188232,
      "learning_rate": 2.352308261322753e-06,
      "loss": 1.7916,
      "step": 27520
    },
    {
      "epoch": 2.7677072337002966,
      "grad_norm": 2.8201918601989746,
      "learning_rate": 2.3422211761541307e-06,
      "loss": 1.8417,
      "step": 27530
    },
    {
      "epoch": 2.7687126124767505,
      "grad_norm": 2.0619850158691406,
      "learning_rate": 2.3321340909855083e-06,
      "loss": 1.7379,
      "step": 27540
    },
    {
      "epoch": 2.769717991253205,
      "grad_norm": 2.028705358505249,
      "learning_rate": 2.322047005816886e-06,
      "loss": 1.8227,
      "step": 27550
    },
    {
      "epoch": 2.7707233700296587,
      "grad_norm": 2.428764581680298,
      "learning_rate": 2.311959920648263e-06,
      "loss": 1.815,
      "step": 27560
    },
    {
      "epoch": 2.7717287488061126,
      "grad_norm": 2.1779260635375977,
      "learning_rate": 2.301872835479641e-06,
      "loss": 1.7251,
      "step": 27570
    },
    {
      "epoch": 2.7727341275825665,
      "grad_norm": 2.178713083267212,
      "learning_rate": 2.2917857503110186e-06,
      "loss": 1.8038,
      "step": 27580
    },
    {
      "epoch": 2.773739506359021,
      "grad_norm": 2.947880268096924,
      "learning_rate": 2.2816986651423962e-06,
      "loss": 1.6987,
      "step": 27590
    },
    {
      "epoch": 2.774744885135475,
      "grad_norm": 2.6735823154449463,
      "learning_rate": 2.2716115799737734e-06,
      "loss": 1.68,
      "step": 27600
    },
    {
      "epoch": 2.7757502639119287,
      "grad_norm": 2.4370343685150146,
      "learning_rate": 2.2615244948051514e-06,
      "loss": 1.6888,
      "step": 27610
    },
    {
      "epoch": 2.776755642688383,
      "grad_norm": 3.1061954498291016,
      "learning_rate": 2.251437409636529e-06,
      "loss": 1.7559,
      "step": 27620
    },
    {
      "epoch": 2.777761021464837,
      "grad_norm": 2.2997794151306152,
      "learning_rate": 2.241350324467906e-06,
      "loss": 1.8505,
      "step": 27630
    },
    {
      "epoch": 2.778766400241291,
      "grad_norm": 1.8449023962020874,
      "learning_rate": 2.2312632392992837e-06,
      "loss": 1.7973,
      "step": 27640
    },
    {
      "epoch": 2.7797717790177447,
      "grad_norm": 2.3335139751434326,
      "learning_rate": 2.2211761541306617e-06,
      "loss": 1.7943,
      "step": 27650
    },
    {
      "epoch": 2.780777157794199,
      "grad_norm": 1.6460773944854736,
      "learning_rate": 2.211089068962039e-06,
      "loss": 1.8114,
      "step": 27660
    },
    {
      "epoch": 2.781782536570653,
      "grad_norm": 2.522003173828125,
      "learning_rate": 2.2010019837934165e-06,
      "loss": 1.7049,
      "step": 27670
    },
    {
      "epoch": 2.782787915347107,
      "grad_norm": 1.6207101345062256,
      "learning_rate": 2.1909148986247945e-06,
      "loss": 1.679,
      "step": 27680
    },
    {
      "epoch": 2.7837932941235612,
      "grad_norm": 2.1222290992736816,
      "learning_rate": 2.1808278134561716e-06,
      "loss": 1.7029,
      "step": 27690
    },
    {
      "epoch": 2.784798672900015,
      "grad_norm": 2.1304054260253906,
      "learning_rate": 2.1707407282875492e-06,
      "loss": 1.8296,
      "step": 27700
    },
    {
      "epoch": 2.785804051676469,
      "grad_norm": 1.9548320770263672,
      "learning_rate": 2.160653643118927e-06,
      "loss": 1.8311,
      "step": 27710
    },
    {
      "epoch": 2.786809430452923,
      "grad_norm": 1.9550504684448242,
      "learning_rate": 2.1505665579503044e-06,
      "loss": 1.7239,
      "step": 27720
    },
    {
      "epoch": 2.7878148092293773,
      "grad_norm": 1.7695293426513672,
      "learning_rate": 2.140479472781682e-06,
      "loss": 1.7074,
      "step": 27730
    },
    {
      "epoch": 2.788820188005831,
      "grad_norm": 1.766672134399414,
      "learning_rate": 2.1303923876130596e-06,
      "loss": 1.7005,
      "step": 27740
    },
    {
      "epoch": 2.789825566782285,
      "grad_norm": 1.9716726541519165,
      "learning_rate": 2.1203053024444367e-06,
      "loss": 1.8113,
      "step": 27750
    },
    {
      "epoch": 2.7908309455587395,
      "grad_norm": 1.9652179479599,
      "learning_rate": 2.1102182172758147e-06,
      "loss": 1.7205,
      "step": 27760
    },
    {
      "epoch": 2.7918363243351934,
      "grad_norm": 2.4370133876800537,
      "learning_rate": 2.1001311321071923e-06,
      "loss": 1.783,
      "step": 27770
    },
    {
      "epoch": 2.7928417031116473,
      "grad_norm": 2.640106678009033,
      "learning_rate": 2.0900440469385695e-06,
      "loss": 1.7146,
      "step": 27780
    },
    {
      "epoch": 2.793847081888101,
      "grad_norm": 2.5506744384765625,
      "learning_rate": 2.079956961769947e-06,
      "loss": 1.7742,
      "step": 27790
    },
    {
      "epoch": 2.7948524606645555,
      "grad_norm": 2.134411096572876,
      "learning_rate": 2.069869876601325e-06,
      "loss": 1.7486,
      "step": 27800
    },
    {
      "epoch": 2.7958578394410094,
      "grad_norm": 1.8164931535720825,
      "learning_rate": 2.0597827914327022e-06,
      "loss": 1.7081,
      "step": 27810
    },
    {
      "epoch": 2.7968632182174633,
      "grad_norm": 2.0797016620635986,
      "learning_rate": 2.04969570626408e-06,
      "loss": 1.7559,
      "step": 27820
    },
    {
      "epoch": 2.7978685969939177,
      "grad_norm": 1.7960031032562256,
      "learning_rate": 2.0396086210954574e-06,
      "loss": 1.7953,
      "step": 27830
    },
    {
      "epoch": 2.7988739757703716,
      "grad_norm": 2.0923149585723877,
      "learning_rate": 2.029521535926835e-06,
      "loss": 1.713,
      "step": 27840
    },
    {
      "epoch": 2.7998793545468255,
      "grad_norm": 1.6516807079315186,
      "learning_rate": 2.0194344507582126e-06,
      "loss": 1.7648,
      "step": 27850
    },
    {
      "epoch": 2.8008847333232794,
      "grad_norm": 1.9200944900512695,
      "learning_rate": 2.00934736558959e-06,
      "loss": 1.6808,
      "step": 27860
    },
    {
      "epoch": 2.8018901120997337,
      "grad_norm": 2.213649034500122,
      "learning_rate": 1.9992602804209677e-06,
      "loss": 1.7573,
      "step": 27870
    },
    {
      "epoch": 2.8028954908761876,
      "grad_norm": 2.5217413902282715,
      "learning_rate": 1.9891731952523453e-06,
      "loss": 1.7627,
      "step": 27880
    },
    {
      "epoch": 2.8039008696526415,
      "grad_norm": 2.0349106788635254,
      "learning_rate": 1.979086110083723e-06,
      "loss": 1.7322,
      "step": 27890
    },
    {
      "epoch": 2.804906248429096,
      "grad_norm": 1.748179316520691,
      "learning_rate": 1.9689990249151005e-06,
      "loss": 1.7507,
      "step": 27900
    },
    {
      "epoch": 2.8059116272055498,
      "grad_norm": 2.1982369422912598,
      "learning_rate": 1.958911939746478e-06,
      "loss": 1.8045,
      "step": 27910
    },
    {
      "epoch": 2.8069170059820037,
      "grad_norm": 2.287410020828247,
      "learning_rate": 1.9488248545778557e-06,
      "loss": 1.7927,
      "step": 27920
    },
    {
      "epoch": 2.8079223847584576,
      "grad_norm": 2.309445381164551,
      "learning_rate": 1.9387377694092332e-06,
      "loss": 1.7308,
      "step": 27930
    },
    {
      "epoch": 2.808927763534912,
      "grad_norm": 2.628310441970825,
      "learning_rate": 1.9286506842406104e-06,
      "loss": 1.7037,
      "step": 27940
    },
    {
      "epoch": 2.809933142311366,
      "grad_norm": 1.9025280475616455,
      "learning_rate": 1.9185635990719884e-06,
      "loss": 1.8264,
      "step": 27950
    },
    {
      "epoch": 2.8109385210878197,
      "grad_norm": 2.1616246700286865,
      "learning_rate": 1.908476513903366e-06,
      "loss": 1.7203,
      "step": 27960
    },
    {
      "epoch": 2.811943899864274,
      "grad_norm": 2.0698623657226562,
      "learning_rate": 1.8983894287347431e-06,
      "loss": 1.8556,
      "step": 27970
    },
    {
      "epoch": 2.812949278640728,
      "grad_norm": 2.2118656635284424,
      "learning_rate": 1.888302343566121e-06,
      "loss": 1.7649,
      "step": 27980
    },
    {
      "epoch": 2.813954657417182,
      "grad_norm": 1.7335665225982666,
      "learning_rate": 1.8782152583974985e-06,
      "loss": 1.7939,
      "step": 27990
    },
    {
      "epoch": 2.8149600361936358,
      "grad_norm": 2.7217819690704346,
      "learning_rate": 1.8681281732288761e-06,
      "loss": 1.6892,
      "step": 28000
    },
    {
      "epoch": 2.81596541497009,
      "grad_norm": 2.597277879714966,
      "learning_rate": 1.8580410880602535e-06,
      "loss": 1.7907,
      "step": 28010
    },
    {
      "epoch": 2.816970793746544,
      "grad_norm": 2.394247531890869,
      "learning_rate": 1.847954002891631e-06,
      "loss": 1.7846,
      "step": 28020
    },
    {
      "epoch": 2.817976172522998,
      "grad_norm": 1.8246698379516602,
      "learning_rate": 1.8378669177230087e-06,
      "loss": 1.7529,
      "step": 28030
    },
    {
      "epoch": 2.8189815512994523,
      "grad_norm": 2.1806833744049072,
      "learning_rate": 1.8277798325543862e-06,
      "loss": 1.7463,
      "step": 28040
    },
    {
      "epoch": 2.819986930075906,
      "grad_norm": 1.8717188835144043,
      "learning_rate": 1.8176927473857638e-06,
      "loss": 1.6335,
      "step": 28050
    },
    {
      "epoch": 2.82099230885236,
      "grad_norm": 2.263242244720459,
      "learning_rate": 1.8076056622171414e-06,
      "loss": 1.7616,
      "step": 28060
    },
    {
      "epoch": 2.821997687628814,
      "grad_norm": 2.1257121562957764,
      "learning_rate": 1.797518577048519e-06,
      "loss": 1.7128,
      "step": 28070
    },
    {
      "epoch": 2.8230030664052683,
      "grad_norm": 1.9216195344924927,
      "learning_rate": 1.7874314918798964e-06,
      "loss": 1.6485,
      "step": 28080
    },
    {
      "epoch": 2.8240084451817222,
      "grad_norm": 2.644592046737671,
      "learning_rate": 1.7773444067112742e-06,
      "loss": 1.7386,
      "step": 28090
    },
    {
      "epoch": 2.825013823958176,
      "grad_norm": 2.0411646366119385,
      "learning_rate": 1.7672573215426515e-06,
      "loss": 1.7393,
      "step": 28100
    },
    {
      "epoch": 2.8260192027346305,
      "grad_norm": 2.450981855392456,
      "learning_rate": 1.7571702363740293e-06,
      "loss": 1.7145,
      "step": 28110
    },
    {
      "epoch": 2.8270245815110844,
      "grad_norm": 2.404165744781494,
      "learning_rate": 1.7470831512054067e-06,
      "loss": 1.7408,
      "step": 28120
    },
    {
      "epoch": 2.8280299602875383,
      "grad_norm": 2.1579220294952393,
      "learning_rate": 1.7369960660367843e-06,
      "loss": 1.7308,
      "step": 28130
    },
    {
      "epoch": 2.829035339063992,
      "grad_norm": 2.562384843826294,
      "learning_rate": 1.7269089808681619e-06,
      "loss": 1.7953,
      "step": 28140
    },
    {
      "epoch": 2.8300407178404465,
      "grad_norm": 1.8932980298995972,
      "learning_rate": 1.7168218956995395e-06,
      "loss": 1.7467,
      "step": 28150
    },
    {
      "epoch": 2.8310460966169004,
      "grad_norm": 1.8477909564971924,
      "learning_rate": 1.7067348105309168e-06,
      "loss": 1.6745,
      "step": 28160
    },
    {
      "epoch": 2.8320514753933543,
      "grad_norm": 1.685307502746582,
      "learning_rate": 1.6966477253622946e-06,
      "loss": 1.7344,
      "step": 28170
    },
    {
      "epoch": 2.8330568541698087,
      "grad_norm": 2.2532942295074463,
      "learning_rate": 1.686560640193672e-06,
      "loss": 1.7557,
      "step": 28180
    },
    {
      "epoch": 2.8340622329462626,
      "grad_norm": 1.9184205532073975,
      "learning_rate": 1.6764735550250496e-06,
      "loss": 1.8839,
      "step": 28190
    },
    {
      "epoch": 2.8350676117227165,
      "grad_norm": 3.2579379081726074,
      "learning_rate": 1.6663864698564272e-06,
      "loss": 1.7107,
      "step": 28200
    },
    {
      "epoch": 2.8360729904991704,
      "grad_norm": 2.297278642654419,
      "learning_rate": 1.6562993846878047e-06,
      "loss": 1.7828,
      "step": 28210
    },
    {
      "epoch": 2.8370783692756247,
      "grad_norm": 2.270883560180664,
      "learning_rate": 1.6462122995191823e-06,
      "loss": 1.7284,
      "step": 28220
    },
    {
      "epoch": 2.8380837480520786,
      "grad_norm": 1.9089421033859253,
      "learning_rate": 1.63612521435056e-06,
      "loss": 1.6927,
      "step": 28230
    },
    {
      "epoch": 2.8390891268285325,
      "grad_norm": 1.8368523120880127,
      "learning_rate": 1.6260381291819375e-06,
      "loss": 1.7291,
      "step": 28240
    },
    {
      "epoch": 2.840094505604987,
      "grad_norm": 2.3840320110321045,
      "learning_rate": 1.6159510440133149e-06,
      "loss": 1.6835,
      "step": 28250
    },
    {
      "epoch": 2.841099884381441,
      "grad_norm": 2.4241065979003906,
      "learning_rate": 1.6058639588446927e-06,
      "loss": 1.7659,
      "step": 28260
    },
    {
      "epoch": 2.8421052631578947,
      "grad_norm": 2.0835182666778564,
      "learning_rate": 1.59577687367607e-06,
      "loss": 1.8004,
      "step": 28270
    },
    {
      "epoch": 2.8431106419343486,
      "grad_norm": 2.353322744369507,
      "learning_rate": 1.5856897885074478e-06,
      "loss": 1.7539,
      "step": 28280
    },
    {
      "epoch": 2.844116020710803,
      "grad_norm": 2.0590643882751465,
      "learning_rate": 1.5756027033388252e-06,
      "loss": 1.796,
      "step": 28290
    },
    {
      "epoch": 2.845121399487257,
      "grad_norm": 1.8751838207244873,
      "learning_rate": 1.5655156181702028e-06,
      "loss": 1.7813,
      "step": 28300
    },
    {
      "epoch": 2.8461267782637107,
      "grad_norm": 2.548022985458374,
      "learning_rate": 1.5554285330015804e-06,
      "loss": 1.7372,
      "step": 28310
    },
    {
      "epoch": 2.847132157040165,
      "grad_norm": 2.1955313682556152,
      "learning_rate": 1.545341447832958e-06,
      "loss": 1.803,
      "step": 28320
    },
    {
      "epoch": 2.848137535816619,
      "grad_norm": 1.5981745719909668,
      "learning_rate": 1.5352543626643353e-06,
      "loss": 1.7968,
      "step": 28330
    },
    {
      "epoch": 2.849142914593073,
      "grad_norm": 1.9486137628555298,
      "learning_rate": 1.5251672774957131e-06,
      "loss": 1.742,
      "step": 28340
    },
    {
      "epoch": 2.850148293369527,
      "grad_norm": 2.57059907913208,
      "learning_rate": 1.5150801923270905e-06,
      "loss": 1.7594,
      "step": 28350
    },
    {
      "epoch": 2.851153672145981,
      "grad_norm": 2.0413694381713867,
      "learning_rate": 1.504993107158468e-06,
      "loss": 1.7822,
      "step": 28360
    },
    {
      "epoch": 2.852159050922435,
      "grad_norm": 3.310325860977173,
      "learning_rate": 1.4949060219898457e-06,
      "loss": 1.7324,
      "step": 28370
    },
    {
      "epoch": 2.853164429698889,
      "grad_norm": 2.61299204826355,
      "learning_rate": 1.4848189368212233e-06,
      "loss": 1.8261,
      "step": 28380
    },
    {
      "epoch": 2.8541698084753433,
      "grad_norm": 1.7345900535583496,
      "learning_rate": 1.4747318516526006e-06,
      "loss": 1.739,
      "step": 28390
    },
    {
      "epoch": 2.855175187251797,
      "grad_norm": 1.9528942108154297,
      "learning_rate": 1.4646447664839784e-06,
      "loss": 1.7645,
      "step": 28400
    },
    {
      "epoch": 2.856180566028251,
      "grad_norm": 2.1096718311309814,
      "learning_rate": 1.454557681315356e-06,
      "loss": 1.7184,
      "step": 28410
    },
    {
      "epoch": 2.857185944804705,
      "grad_norm": 1.8919577598571777,
      "learning_rate": 1.4444705961467336e-06,
      "loss": 1.7582,
      "step": 28420
    },
    {
      "epoch": 2.8581913235811593,
      "grad_norm": 2.541628360748291,
      "learning_rate": 1.4343835109781112e-06,
      "loss": 1.7883,
      "step": 28430
    },
    {
      "epoch": 2.8591967023576133,
      "grad_norm": 2.130580186843872,
      "learning_rate": 1.4242964258094885e-06,
      "loss": 1.6835,
      "step": 28440
    },
    {
      "epoch": 2.860202081134067,
      "grad_norm": 2.2761070728302,
      "learning_rate": 1.4142093406408663e-06,
      "loss": 1.753,
      "step": 28450
    },
    {
      "epoch": 2.8612074599105215,
      "grad_norm": 1.9444003105163574,
      "learning_rate": 1.4041222554722437e-06,
      "loss": 1.7931,
      "step": 28460
    },
    {
      "epoch": 2.8622128386869754,
      "grad_norm": 2.0479705333709717,
      "learning_rate": 1.3940351703036213e-06,
      "loss": 1.7931,
      "step": 28470
    },
    {
      "epoch": 2.8632182174634293,
      "grad_norm": 1.9604945182800293,
      "learning_rate": 1.3839480851349989e-06,
      "loss": 1.8668,
      "step": 28480
    },
    {
      "epoch": 2.864223596239883,
      "grad_norm": 1.772045612335205,
      "learning_rate": 1.3738609999663765e-06,
      "loss": 1.7816,
      "step": 28490
    },
    {
      "epoch": 2.8652289750163376,
      "grad_norm": 2.251114845275879,
      "learning_rate": 1.3637739147977538e-06,
      "loss": 1.7213,
      "step": 28500
    },
    {
      "epoch": 2.8662343537927915,
      "grad_norm": 1.9079221487045288,
      "learning_rate": 1.3536868296291316e-06,
      "loss": 1.8102,
      "step": 28510
    },
    {
      "epoch": 2.8672397325692454,
      "grad_norm": 1.902426838874817,
      "learning_rate": 1.343599744460509e-06,
      "loss": 1.7626,
      "step": 28520
    },
    {
      "epoch": 2.8682451113456997,
      "grad_norm": 2.3549726009368896,
      "learning_rate": 1.3335126592918866e-06,
      "loss": 1.6594,
      "step": 28530
    },
    {
      "epoch": 2.8692504901221536,
      "grad_norm": 2.4913644790649414,
      "learning_rate": 1.3234255741232642e-06,
      "loss": 1.7659,
      "step": 28540
    },
    {
      "epoch": 2.8702558688986075,
      "grad_norm": 1.958832859992981,
      "learning_rate": 1.3133384889546418e-06,
      "loss": 1.7715,
      "step": 28550
    },
    {
      "epoch": 2.8712612476750614,
      "grad_norm": 1.9527080059051514,
      "learning_rate": 1.3032514037860191e-06,
      "loss": 1.7671,
      "step": 28560
    },
    {
      "epoch": 2.8722666264515158,
      "grad_norm": 2.316498279571533,
      "learning_rate": 1.293164318617397e-06,
      "loss": 1.7599,
      "step": 28570
    },
    {
      "epoch": 2.8732720052279697,
      "grad_norm": 2.9208362102508545,
      "learning_rate": 1.2830772334487745e-06,
      "loss": 1.7066,
      "step": 28580
    },
    {
      "epoch": 2.8742773840044236,
      "grad_norm": 2.1643171310424805,
      "learning_rate": 1.2739988567970143e-06,
      "loss": 1.7388,
      "step": 28590
    },
    {
      "epoch": 2.875282762780878,
      "grad_norm": 1.9347740411758423,
      "learning_rate": 1.2639117716283918e-06,
      "loss": 1.7697,
      "step": 28600
    },
    {
      "epoch": 2.876288141557332,
      "grad_norm": 1.7360066175460815,
      "learning_rate": 1.2538246864597694e-06,
      "loss": 1.7493,
      "step": 28610
    },
    {
      "epoch": 2.8772935203337857,
      "grad_norm": 2.2816622257232666,
      "learning_rate": 1.2437376012911468e-06,
      "loss": 1.748,
      "step": 28620
    },
    {
      "epoch": 2.8782988991102396,
      "grad_norm": 1.8678802251815796,
      "learning_rate": 1.2336505161225246e-06,
      "loss": 1.8093,
      "step": 28630
    },
    {
      "epoch": 2.879304277886694,
      "grad_norm": 2.016618490219116,
      "learning_rate": 1.223563430953902e-06,
      "loss": 1.7436,
      "step": 28640
    },
    {
      "epoch": 2.880309656663148,
      "grad_norm": 1.9223363399505615,
      "learning_rate": 1.2134763457852795e-06,
      "loss": 1.7426,
      "step": 28650
    },
    {
      "epoch": 2.8813150354396018,
      "grad_norm": 2.1603598594665527,
      "learning_rate": 1.2033892606166571e-06,
      "loss": 1.7668,
      "step": 28660
    },
    {
      "epoch": 2.882320414216056,
      "grad_norm": 1.976111888885498,
      "learning_rate": 1.1933021754480347e-06,
      "loss": 1.8012,
      "step": 28670
    },
    {
      "epoch": 2.88332579299251,
      "grad_norm": 2.0443999767303467,
      "learning_rate": 1.1832150902794123e-06,
      "loss": 1.7919,
      "step": 28680
    },
    {
      "epoch": 2.884331171768964,
      "grad_norm": 2.6655876636505127,
      "learning_rate": 1.1731280051107899e-06,
      "loss": 1.7058,
      "step": 28690
    },
    {
      "epoch": 2.885336550545418,
      "grad_norm": 2.722909450531006,
      "learning_rate": 1.1630409199421675e-06,
      "loss": 1.7137,
      "step": 28700
    },
    {
      "epoch": 2.886341929321872,
      "grad_norm": 2.21040678024292,
      "learning_rate": 1.1529538347735448e-06,
      "loss": 1.7035,
      "step": 28710
    },
    {
      "epoch": 2.887347308098326,
      "grad_norm": 2.2496676445007324,
      "learning_rate": 1.1428667496049226e-06,
      "loss": 1.8134,
      "step": 28720
    },
    {
      "epoch": 2.88835268687478,
      "grad_norm": 2.1860737800598145,
      "learning_rate": 1.1327796644363e-06,
      "loss": 1.7832,
      "step": 28730
    },
    {
      "epoch": 2.8893580656512343,
      "grad_norm": 2.158747673034668,
      "learning_rate": 1.1226925792676778e-06,
      "loss": 1.6677,
      "step": 28740
    },
    {
      "epoch": 2.8903634444276882,
      "grad_norm": 2.550360918045044,
      "learning_rate": 1.1126054940990552e-06,
      "loss": 1.7426,
      "step": 28750
    },
    {
      "epoch": 2.891368823204142,
      "grad_norm": 2.0409765243530273,
      "learning_rate": 1.1025184089304328e-06,
      "loss": 1.7224,
      "step": 28760
    },
    {
      "epoch": 2.892374201980596,
      "grad_norm": 1.741823434829712,
      "learning_rate": 1.0924313237618103e-06,
      "loss": 1.7357,
      "step": 28770
    },
    {
      "epoch": 2.8933795807570504,
      "grad_norm": 2.116945505142212,
      "learning_rate": 1.082344238593188e-06,
      "loss": 1.6864,
      "step": 28780
    },
    {
      "epoch": 2.8943849595335043,
      "grad_norm": 2.4826393127441406,
      "learning_rate": 1.0722571534245653e-06,
      "loss": 1.7885,
      "step": 28790
    },
    {
      "epoch": 2.895390338309958,
      "grad_norm": 1.610116720199585,
      "learning_rate": 1.062170068255943e-06,
      "loss": 1.8341,
      "step": 28800
    },
    {
      "epoch": 2.8963957170864125,
      "grad_norm": 2.0318713188171387,
      "learning_rate": 1.0520829830873205e-06,
      "loss": 1.7659,
      "step": 28810
    },
    {
      "epoch": 2.8974010958628664,
      "grad_norm": 2.4569408893585205,
      "learning_rate": 1.041995897918698e-06,
      "loss": 1.713,
      "step": 28820
    },
    {
      "epoch": 2.8984064746393203,
      "grad_norm": 1.7990232706069946,
      "learning_rate": 1.0319088127500756e-06,
      "loss": 1.7577,
      "step": 28830
    },
    {
      "epoch": 2.8994118534157742,
      "grad_norm": 1.9749646186828613,
      "learning_rate": 1.0218217275814532e-06,
      "loss": 1.866,
      "step": 28840
    },
    {
      "epoch": 2.9004172321922286,
      "grad_norm": 2.1732661724090576,
      "learning_rate": 1.0117346424128308e-06,
      "loss": 1.7658,
      "step": 28850
    },
    {
      "epoch": 2.9014226109686825,
      "grad_norm": 2.002945899963379,
      "learning_rate": 1.0016475572442084e-06,
      "loss": 1.6807,
      "step": 28860
    },
    {
      "epoch": 2.9024279897451364,
      "grad_norm": 2.4242732524871826,
      "learning_rate": 9.91560472075586e-07,
      "loss": 1.778,
      "step": 28870
    },
    {
      "epoch": 2.9034333685215907,
      "grad_norm": 2.148759126663208,
      "learning_rate": 9.814733869069636e-07,
      "loss": 1.6783,
      "step": 28880
    },
    {
      "epoch": 2.9044387472980446,
      "grad_norm": 2.2072560787200928,
      "learning_rate": 9.713863017383411e-07,
      "loss": 1.7399,
      "step": 28890
    },
    {
      "epoch": 2.9054441260744985,
      "grad_norm": 2.4547271728515625,
      "learning_rate": 9.612992165697185e-07,
      "loss": 1.7676,
      "step": 28900
    },
    {
      "epoch": 2.9064495048509524,
      "grad_norm": 2.0154364109039307,
      "learning_rate": 9.512121314010962e-07,
      "loss": 1.8227,
      "step": 28910
    },
    {
      "epoch": 2.907454883627407,
      "grad_norm": 2.267524242401123,
      "learning_rate": 9.411250462324737e-07,
      "loss": 1.8482,
      "step": 28920
    },
    {
      "epoch": 2.9084602624038607,
      "grad_norm": 2.3597495555877686,
      "learning_rate": 9.310379610638513e-07,
      "loss": 1.7228,
      "step": 28930
    },
    {
      "epoch": 2.9094656411803146,
      "grad_norm": 1.8872798681259155,
      "learning_rate": 9.209508758952287e-07,
      "loss": 1.6785,
      "step": 28940
    },
    {
      "epoch": 2.910471019956769,
      "grad_norm": 2.14355731010437,
      "learning_rate": 9.108637907266064e-07,
      "loss": 1.7755,
      "step": 28950
    },
    {
      "epoch": 2.911476398733223,
      "grad_norm": 2.4265756607055664,
      "learning_rate": 9.00776705557984e-07,
      "loss": 1.769,
      "step": 28960
    },
    {
      "epoch": 2.9124817775096767,
      "grad_norm": 2.1035988330841064,
      "learning_rate": 8.906896203893615e-07,
      "loss": 1.8001,
      "step": 28970
    },
    {
      "epoch": 2.9134871562861306,
      "grad_norm": 2.187680244445801,
      "learning_rate": 8.806025352207391e-07,
      "loss": 1.7507,
      "step": 28980
    },
    {
      "epoch": 2.914492535062585,
      "grad_norm": 1.8428817987442017,
      "learning_rate": 8.705154500521167e-07,
      "loss": 1.7438,
      "step": 28990
    },
    {
      "epoch": 2.915497913839039,
      "grad_norm": 2.9331793785095215,
      "learning_rate": 8.604283648834943e-07,
      "loss": 1.8162,
      "step": 29000
    },
    {
      "epoch": 2.916503292615493,
      "grad_norm": 1.9749118089675903,
      "learning_rate": 8.503412797148717e-07,
      "loss": 1.7366,
      "step": 29010
    },
    {
      "epoch": 2.917508671391947,
      "grad_norm": 1.7633200883865356,
      "learning_rate": 8.402541945462493e-07,
      "loss": 1.7598,
      "step": 29020
    },
    {
      "epoch": 2.918514050168401,
      "grad_norm": 3.1552650928497314,
      "learning_rate": 8.301671093776269e-07,
      "loss": 1.6965,
      "step": 29030
    },
    {
      "epoch": 2.919519428944855,
      "grad_norm": 1.9388232231140137,
      "learning_rate": 8.200800242090044e-07,
      "loss": 1.837,
      "step": 29040
    },
    {
      "epoch": 2.920524807721309,
      "grad_norm": 1.870115876197815,
      "learning_rate": 8.09992939040382e-07,
      "loss": 1.7466,
      "step": 29050
    },
    {
      "epoch": 2.921530186497763,
      "grad_norm": 2.079615592956543,
      "learning_rate": 7.999058538717595e-07,
      "loss": 1.6793,
      "step": 29060
    },
    {
      "epoch": 2.922535565274217,
      "grad_norm": 2.581089735031128,
      "learning_rate": 7.898187687031371e-07,
      "loss": 1.7323,
      "step": 29070
    },
    {
      "epoch": 2.923540944050671,
      "grad_norm": 2.8018734455108643,
      "learning_rate": 7.797316835345146e-07,
      "loss": 1.7957,
      "step": 29080
    },
    {
      "epoch": 2.9245463228271253,
      "grad_norm": 2.031064748764038,
      "learning_rate": 7.696445983658922e-07,
      "loss": 1.7055,
      "step": 29090
    },
    {
      "epoch": 2.9255517016035792,
      "grad_norm": 2.4694299697875977,
      "learning_rate": 7.595575131972698e-07,
      "loss": 1.7624,
      "step": 29100
    },
    {
      "epoch": 2.926557080380033,
      "grad_norm": 2.026562213897705,
      "learning_rate": 7.494704280286473e-07,
      "loss": 1.7028,
      "step": 29110
    },
    {
      "epoch": 2.927562459156487,
      "grad_norm": 2.1565611362457275,
      "learning_rate": 7.393833428600248e-07,
      "loss": 1.7463,
      "step": 29120
    },
    {
      "epoch": 2.9285678379329414,
      "grad_norm": 1.9168674945831299,
      "learning_rate": 7.292962576914025e-07,
      "loss": 1.7565,
      "step": 29130
    },
    {
      "epoch": 2.9295732167093953,
      "grad_norm": 1.9221385717391968,
      "learning_rate": 7.192091725227801e-07,
      "loss": 1.7845,
      "step": 29140
    },
    {
      "epoch": 2.930578595485849,
      "grad_norm": 1.7646675109863281,
      "learning_rate": 7.091220873541576e-07,
      "loss": 1.7749,
      "step": 29150
    },
    {
      "epoch": 2.9315839742623035,
      "grad_norm": 2.0433740615844727,
      "learning_rate": 6.990350021855352e-07,
      "loss": 1.7828,
      "step": 29160
    },
    {
      "epoch": 2.9325893530387575,
      "grad_norm": 2.028310537338257,
      "learning_rate": 6.889479170169128e-07,
      "loss": 1.729,
      "step": 29170
    },
    {
      "epoch": 2.9335947318152114,
      "grad_norm": 1.6397840976715088,
      "learning_rate": 6.788608318482902e-07,
      "loss": 1.7715,
      "step": 29180
    },
    {
      "epoch": 2.9346001105916653,
      "grad_norm": 1.9753895998001099,
      "learning_rate": 6.687737466796678e-07,
      "loss": 1.7764,
      "step": 29190
    },
    {
      "epoch": 2.9356054893681196,
      "grad_norm": 1.5748963356018066,
      "learning_rate": 6.586866615110454e-07,
      "loss": 1.8179,
      "step": 29200
    },
    {
      "epoch": 2.9366108681445735,
      "grad_norm": 2.3291666507720947,
      "learning_rate": 6.48599576342423e-07,
      "loss": 1.822,
      "step": 29210
    },
    {
      "epoch": 2.9376162469210274,
      "grad_norm": 2.1813716888427734,
      "learning_rate": 6.385124911738005e-07,
      "loss": 1.7442,
      "step": 29220
    },
    {
      "epoch": 2.9386216256974818,
      "grad_norm": 1.9014763832092285,
      "learning_rate": 6.28425406005178e-07,
      "loss": 1.7327,
      "step": 29230
    },
    {
      "epoch": 2.9396270044739357,
      "grad_norm": 1.9276602268218994,
      "learning_rate": 6.183383208365556e-07,
      "loss": 1.7513,
      "step": 29240
    },
    {
      "epoch": 2.9406323832503896,
      "grad_norm": 2.9264180660247803,
      "learning_rate": 6.082512356679331e-07,
      "loss": 1.7442,
      "step": 29250
    },
    {
      "epoch": 2.9416377620268435,
      "grad_norm": 2.561718225479126,
      "learning_rate": 5.981641504993107e-07,
      "loss": 1.7545,
      "step": 29260
    },
    {
      "epoch": 2.942643140803298,
      "grad_norm": 2.1020126342773438,
      "learning_rate": 5.880770653306883e-07,
      "loss": 1.6519,
      "step": 29270
    },
    {
      "epoch": 2.9436485195797517,
      "grad_norm": 2.3245556354522705,
      "learning_rate": 5.779899801620658e-07,
      "loss": 1.7395,
      "step": 29280
    },
    {
      "epoch": 2.9446538983562056,
      "grad_norm": 2.3967819213867188,
      "learning_rate": 5.679028949934433e-07,
      "loss": 1.7346,
      "step": 29290
    },
    {
      "epoch": 2.94565927713266,
      "grad_norm": 1.8805917501449585,
      "learning_rate": 5.578158098248209e-07,
      "loss": 1.7059,
      "step": 29300
    },
    {
      "epoch": 2.946664655909114,
      "grad_norm": 1.963614583015442,
      "learning_rate": 5.477287246561986e-07,
      "loss": 1.72,
      "step": 29310
    },
    {
      "epoch": 2.9476700346855678,
      "grad_norm": 2.496314287185669,
      "learning_rate": 5.376416394875761e-07,
      "loss": 1.8086,
      "step": 29320
    },
    {
      "epoch": 2.9486754134620217,
      "grad_norm": 1.8039155006408691,
      "learning_rate": 5.275545543189537e-07,
      "loss": 1.7831,
      "step": 29330
    },
    {
      "epoch": 2.9496807922384756,
      "grad_norm": 2.9030661582946777,
      "learning_rate": 5.174674691503313e-07,
      "loss": 1.687,
      "step": 29340
    },
    {
      "epoch": 2.95068617101493,
      "grad_norm": 2.0130221843719482,
      "learning_rate": 5.073803839817087e-07,
      "loss": 1.6549,
      "step": 29350
    },
    {
      "epoch": 2.951691549791384,
      "grad_norm": 3.1211416721343994,
      "learning_rate": 4.972932988130863e-07,
      "loss": 1.6925,
      "step": 29360
    },
    {
      "epoch": 2.952696928567838,
      "grad_norm": 2.01823091506958,
      "learning_rate": 4.872062136444639e-07,
      "loss": 1.7417,
      "step": 29370
    },
    {
      "epoch": 2.953702307344292,
      "grad_norm": 2.1196353435516357,
      "learning_rate": 4.771191284758415e-07,
      "loss": 1.8345,
      "step": 29380
    },
    {
      "epoch": 2.954707686120746,
      "grad_norm": 2.3191258907318115,
      "learning_rate": 4.6703204330721903e-07,
      "loss": 1.7909,
      "step": 29390
    },
    {
      "epoch": 2.9557130648972,
      "grad_norm": 2.0403378009796143,
      "learning_rate": 4.5694495813859656e-07,
      "loss": 1.6897,
      "step": 29400
    },
    {
      "epoch": 2.9567184436736538,
      "grad_norm": 2.022120714187622,
      "learning_rate": 4.468578729699741e-07,
      "loss": 1.7208,
      "step": 29410
    },
    {
      "epoch": 2.957723822450108,
      "grad_norm": 2.1198863983154297,
      "learning_rate": 4.367707878013517e-07,
      "loss": 1.7956,
      "step": 29420
    },
    {
      "epoch": 2.958729201226562,
      "grad_norm": 2.2390177249908447,
      "learning_rate": 4.266837026327292e-07,
      "loss": 1.7426,
      "step": 29430
    },
    {
      "epoch": 2.9597345800030164,
      "grad_norm": 2.324462413787842,
      "learning_rate": 4.165966174641068e-07,
      "loss": 1.7678,
      "step": 29440
    },
    {
      "epoch": 2.9607399587794703,
      "grad_norm": 1.9470629692077637,
      "learning_rate": 4.065095322954844e-07,
      "loss": 1.7094,
      "step": 29450
    },
    {
      "epoch": 2.961745337555924,
      "grad_norm": 1.745173692703247,
      "learning_rate": 3.9642244712686196e-07,
      "loss": 1.7378,
      "step": 29460
    },
    {
      "epoch": 2.962750716332378,
      "grad_norm": 2.401315689086914,
      "learning_rate": 3.863353619582395e-07,
      "loss": 1.7693,
      "step": 29470
    },
    {
      "epoch": 2.963756095108832,
      "grad_norm": 1.8141533136367798,
      "learning_rate": 3.76248276789617e-07,
      "loss": 1.7567,
      "step": 29480
    },
    {
      "epoch": 2.9647614738852863,
      "grad_norm": 2.3713831901550293,
      "learning_rate": 3.661611916209946e-07,
      "loss": 1.723,
      "step": 29490
    },
    {
      "epoch": 2.9657668526617402,
      "grad_norm": 1.87952721118927,
      "learning_rate": 3.5607410645237214e-07,
      "loss": 1.7884,
      "step": 29500
    },
    {
      "epoch": 2.9667722314381946,
      "grad_norm": 2.0545291900634766,
      "learning_rate": 3.459870212837497e-07,
      "loss": 1.7106,
      "step": 29510
    },
    {
      "epoch": 2.9677776102146485,
      "grad_norm": 2.2246689796447754,
      "learning_rate": 3.3589993611512725e-07,
      "loss": 1.7503,
      "step": 29520
    },
    {
      "epoch": 2.9687829889911024,
      "grad_norm": 2.0791196823120117,
      "learning_rate": 3.258128509465048e-07,
      "loss": 1.7341,
      "step": 29530
    },
    {
      "epoch": 2.9697883677675563,
      "grad_norm": 2.80257248878479,
      "learning_rate": 3.157257657778824e-07,
      "loss": 1.8123,
      "step": 29540
    },
    {
      "epoch": 2.97079374654401,
      "grad_norm": 1.889230728149414,
      "learning_rate": 3.0563868060925995e-07,
      "loss": 1.7815,
      "step": 29550
    },
    {
      "epoch": 2.9717991253204645,
      "grad_norm": 2.044426441192627,
      "learning_rate": 2.9555159544063754e-07,
      "loss": 1.6965,
      "step": 29560
    },
    {
      "epoch": 2.9728045040969184,
      "grad_norm": 2.0391738414764404,
      "learning_rate": 2.8546451027201507e-07,
      "loss": 1.7913,
      "step": 29570
    },
    {
      "epoch": 2.973809882873373,
      "grad_norm": 2.53741717338562,
      "learning_rate": 2.7537742510339265e-07,
      "loss": 1.7741,
      "step": 29580
    },
    {
      "epoch": 2.9748152616498267,
      "grad_norm": 2.079418420791626,
      "learning_rate": 2.652903399347702e-07,
      "loss": 1.7472,
      "step": 29590
    },
    {
      "epoch": 2.9758206404262806,
      "grad_norm": 3.002739191055298,
      "learning_rate": 2.552032547661477e-07,
      "loss": 1.7957,
      "step": 29600
    },
    {
      "epoch": 2.9768260192027345,
      "grad_norm": 1.8353331089019775,
      "learning_rate": 2.451161695975253e-07,
      "loss": 1.7362,
      "step": 29610
    },
    {
      "epoch": 2.9778313979791884,
      "grad_norm": 2.2593681812286377,
      "learning_rate": 2.3502908442890286e-07,
      "loss": 1.7516,
      "step": 29620
    },
    {
      "epoch": 2.9788367767556427,
      "grad_norm": 2.026782989501953,
      "learning_rate": 2.249419992602804e-07,
      "loss": 1.7673,
      "step": 29630
    },
    {
      "epoch": 2.9798421555320966,
      "grad_norm": 2.197559356689453,
      "learning_rate": 2.1485491409165797e-07,
      "loss": 1.749,
      "step": 29640
    },
    {
      "epoch": 2.980847534308551,
      "grad_norm": 2.0328502655029297,
      "learning_rate": 2.0476782892303556e-07,
      "loss": 1.7112,
      "step": 29650
    },
    {
      "epoch": 2.981852913085005,
      "grad_norm": 1.667616367340088,
      "learning_rate": 1.946807437544131e-07,
      "loss": 1.7069,
      "step": 29660
    },
    {
      "epoch": 2.982858291861459,
      "grad_norm": 2.4644367694854736,
      "learning_rate": 1.8459365858579067e-07,
      "loss": 1.783,
      "step": 29670
    },
    {
      "epoch": 2.9838636706379127,
      "grad_norm": 2.8913042545318604,
      "learning_rate": 1.7450657341716823e-07,
      "loss": 1.7835,
      "step": 29680
    },
    {
      "epoch": 2.9848690494143666,
      "grad_norm": 1.8802474737167358,
      "learning_rate": 1.6441948824854576e-07,
      "loss": 1.7081,
      "step": 29690
    },
    {
      "epoch": 2.985874428190821,
      "grad_norm": 1.914397120475769,
      "learning_rate": 1.5433240307992334e-07,
      "loss": 1.7585,
      "step": 29700
    },
    {
      "epoch": 2.986879806967275,
      "grad_norm": 1.85233736038208,
      "learning_rate": 1.442453179113009e-07,
      "loss": 1.7369,
      "step": 29710
    },
    {
      "epoch": 2.987885185743729,
      "grad_norm": 2.2939341068267822,
      "learning_rate": 1.3415823274267846e-07,
      "loss": 1.7405,
      "step": 29720
    },
    {
      "epoch": 2.988890564520183,
      "grad_norm": 2.1593685150146484,
      "learning_rate": 1.2407114757405602e-07,
      "loss": 1.7228,
      "step": 29730
    },
    {
      "epoch": 2.989895943296637,
      "grad_norm": 2.067064046859741,
      "learning_rate": 1.1398406240543357e-07,
      "loss": 1.6891,
      "step": 29740
    },
    {
      "epoch": 2.990901322073091,
      "grad_norm": 2.1965625286102295,
      "learning_rate": 1.0389697723681113e-07,
      "loss": 1.754,
      "step": 29750
    },
    {
      "epoch": 2.991906700849545,
      "grad_norm": 2.0264174938201904,
      "learning_rate": 9.38098920681887e-08,
      "loss": 1.7404,
      "step": 29760
    },
    {
      "epoch": 2.992912079625999,
      "grad_norm": 1.8703030347824097,
      "learning_rate": 8.372280689956625e-08,
      "loss": 1.7151,
      "step": 29770
    },
    {
      "epoch": 2.993917458402453,
      "grad_norm": 2.517634630203247,
      "learning_rate": 7.363572173094382e-08,
      "loss": 1.8415,
      "step": 29780
    },
    {
      "epoch": 2.9949228371789074,
      "grad_norm": 1.984563946723938,
      "learning_rate": 6.354863656232138e-08,
      "loss": 1.6858,
      "step": 29790
    },
    {
      "epoch": 2.9959282159553613,
      "grad_norm": 2.2270402908325195,
      "learning_rate": 5.3461551393698934e-08,
      "loss": 1.6968,
      "step": 29800
    },
    {
      "epoch": 2.996933594731815,
      "grad_norm": 2.811335325241089,
      "learning_rate": 4.337446622507649e-08,
      "loss": 1.6834,
      "step": 29810
    },
    {
      "epoch": 2.997938973508269,
      "grad_norm": 2.3980729579925537,
      "learning_rate": 3.3287381056454056e-08,
      "loss": 1.6563,
      "step": 29820
    },
    {
      "epoch": 2.998944352284723,
      "grad_norm": 1.9923732280731201,
      "learning_rate": 2.3200295887831613e-08,
      "loss": 1.6844,
      "step": 29830
    },
    {
      "epoch": 2.9999497310611773,
      "grad_norm": 2.026641368865967,
      "learning_rate": 1.3113210719209173e-08,
      "loss": 1.7889,
      "step": 29840
    }
  ],
  "logging_steps": 10,
  "max_steps": 29841,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5319356623028224e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
