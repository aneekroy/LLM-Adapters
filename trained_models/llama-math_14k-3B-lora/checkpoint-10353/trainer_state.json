{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 10353,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002898340699949279,
      "grad_norm": 0.6910264492034912,
      "learning_rate": 2.7e-06,
      "loss": 1.4904,
      "step": 10
    },
    {
      "epoch": 0.005796681399898558,
      "grad_norm": 0.6914367079734802,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 1.5762,
      "step": 20
    },
    {
      "epoch": 0.008695022099847837,
      "grad_norm": 0.4879246950149536,
      "learning_rate": 8.7e-06,
      "loss": 1.5407,
      "step": 30
    },
    {
      "epoch": 0.011593362799797117,
      "grad_norm": 0.49816980957984924,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 1.5636,
      "step": 40
    },
    {
      "epoch": 0.014491703499746395,
      "grad_norm": 0.4503786563873291,
      "learning_rate": 1.47e-05,
      "loss": 1.5225,
      "step": 50
    },
    {
      "epoch": 0.017390044199695674,
      "grad_norm": 0.5808457732200623,
      "learning_rate": 1.77e-05,
      "loss": 1.5003,
      "step": 60
    },
    {
      "epoch": 0.020288384899644952,
      "grad_norm": 0.5096230506896973,
      "learning_rate": 2.07e-05,
      "loss": 1.4428,
      "step": 70
    },
    {
      "epoch": 0.023186725599594234,
      "grad_norm": 0.568947970867157,
      "learning_rate": 2.37e-05,
      "loss": 1.3416,
      "step": 80
    },
    {
      "epoch": 0.026085066299543512,
      "grad_norm": 0.6562936902046204,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 1.2531,
      "step": 90
    },
    {
      "epoch": 0.02898340699949279,
      "grad_norm": 0.8283164501190186,
      "learning_rate": 2.97e-05,
      "loss": 1.1747,
      "step": 100
    },
    {
      "epoch": 0.03188174769944207,
      "grad_norm": 0.7638701796531677,
      "learning_rate": 2.9973666244026138e-05,
      "loss": 1.1164,
      "step": 110
    },
    {
      "epoch": 0.03478008839939135,
      "grad_norm": 0.85160893201828,
      "learning_rate": 2.9944406515166293e-05,
      "loss": 1.1365,
      "step": 120
    },
    {
      "epoch": 0.037678429099340625,
      "grad_norm": 0.8847641944885254,
      "learning_rate": 2.9915146786306445e-05,
      "loss": 1.0911,
      "step": 130
    },
    {
      "epoch": 0.040576769799289904,
      "grad_norm": 0.6501070857048035,
      "learning_rate": 2.98858870574466e-05,
      "loss": 1.0214,
      "step": 140
    },
    {
      "epoch": 0.04347511049923919,
      "grad_norm": 0.7580230236053467,
      "learning_rate": 2.985662732858676e-05,
      "loss": 0.9869,
      "step": 150
    },
    {
      "epoch": 0.04637345119918847,
      "grad_norm": 0.7018323540687561,
      "learning_rate": 2.982736759972691e-05,
      "loss": 0.9763,
      "step": 160
    },
    {
      "epoch": 0.049271791899137746,
      "grad_norm": 0.628864586353302,
      "learning_rate": 2.9798107870867066e-05,
      "loss": 0.9542,
      "step": 170
    },
    {
      "epoch": 0.052170132599087024,
      "grad_norm": 0.7659925222396851,
      "learning_rate": 2.9768848142007218e-05,
      "loss": 0.9524,
      "step": 180
    },
    {
      "epoch": 0.0550684732990363,
      "grad_norm": 0.7421122193336487,
      "learning_rate": 2.9739588413147374e-05,
      "loss": 0.9751,
      "step": 190
    },
    {
      "epoch": 0.05796681399898558,
      "grad_norm": 0.7919326424598694,
      "learning_rate": 2.9710328684287526e-05,
      "loss": 0.9577,
      "step": 200
    },
    {
      "epoch": 0.06086515469893486,
      "grad_norm": 0.8732537031173706,
      "learning_rate": 2.968106895542768e-05,
      "loss": 0.9764,
      "step": 210
    },
    {
      "epoch": 0.06376349539888414,
      "grad_norm": 1.0083054304122925,
      "learning_rate": 2.9651809226567833e-05,
      "loss": 0.9485,
      "step": 220
    },
    {
      "epoch": 0.06666183609883342,
      "grad_norm": 0.9413350224494934,
      "learning_rate": 2.9622549497707988e-05,
      "loss": 0.9597,
      "step": 230
    },
    {
      "epoch": 0.0695601767987827,
      "grad_norm": 0.7289038300514221,
      "learning_rate": 2.959328976884814e-05,
      "loss": 0.9211,
      "step": 240
    },
    {
      "epoch": 0.07245851749873197,
      "grad_norm": 0.7997011542320251,
      "learning_rate": 2.9564030039988296e-05,
      "loss": 0.9577,
      "step": 250
    },
    {
      "epoch": 0.07535685819868125,
      "grad_norm": 0.8378467559814453,
      "learning_rate": 2.953477031112845e-05,
      "loss": 0.9937,
      "step": 260
    },
    {
      "epoch": 0.07825519889863053,
      "grad_norm": 0.7868074178695679,
      "learning_rate": 2.9505510582268606e-05,
      "loss": 0.9722,
      "step": 270
    },
    {
      "epoch": 0.08115353959857981,
      "grad_norm": 0.7717200517654419,
      "learning_rate": 2.947625085340876e-05,
      "loss": 0.9852,
      "step": 280
    },
    {
      "epoch": 0.08405188029852909,
      "grad_norm": 1.1071149110794067,
      "learning_rate": 2.9446991124548914e-05,
      "loss": 0.9317,
      "step": 290
    },
    {
      "epoch": 0.08695022099847838,
      "grad_norm": 0.7822626233100891,
      "learning_rate": 2.941773139568907e-05,
      "loss": 0.9326,
      "step": 300
    },
    {
      "epoch": 0.08984856169842766,
      "grad_norm": 0.7084053158760071,
      "learning_rate": 2.938847166682922e-05,
      "loss": 0.9092,
      "step": 310
    },
    {
      "epoch": 0.09274690239837693,
      "grad_norm": 1.0141271352767944,
      "learning_rate": 2.9359211937969376e-05,
      "loss": 0.9786,
      "step": 320
    },
    {
      "epoch": 0.09564524309832621,
      "grad_norm": 0.821510374546051,
      "learning_rate": 2.9329952209109528e-05,
      "loss": 0.9281,
      "step": 330
    },
    {
      "epoch": 0.09854358379827549,
      "grad_norm": 1.0398234128952026,
      "learning_rate": 2.9300692480249684e-05,
      "loss": 0.9598,
      "step": 340
    },
    {
      "epoch": 0.10144192449822477,
      "grad_norm": 0.7773541212081909,
      "learning_rate": 2.9271432751389836e-05,
      "loss": 0.9543,
      "step": 350
    },
    {
      "epoch": 0.10434026519817405,
      "grad_norm": 0.8599953651428223,
      "learning_rate": 2.924217302252999e-05,
      "loss": 0.8998,
      "step": 360
    },
    {
      "epoch": 0.10723860589812333,
      "grad_norm": 0.9364563822746277,
      "learning_rate": 2.9212913293670146e-05,
      "loss": 0.9148,
      "step": 370
    },
    {
      "epoch": 0.1101369465980726,
      "grad_norm": 0.8514395952224731,
      "learning_rate": 2.9183653564810298e-05,
      "loss": 0.928,
      "step": 380
    },
    {
      "epoch": 0.11303528729802188,
      "grad_norm": 1.0285131931304932,
      "learning_rate": 2.9154393835950457e-05,
      "loss": 0.9668,
      "step": 390
    },
    {
      "epoch": 0.11593362799797116,
      "grad_norm": 0.8786579966545105,
      "learning_rate": 2.912513410709061e-05,
      "loss": 0.8892,
      "step": 400
    },
    {
      "epoch": 0.11883196869792044,
      "grad_norm": 0.8748263716697693,
      "learning_rate": 2.9095874378230764e-05,
      "loss": 0.9529,
      "step": 410
    },
    {
      "epoch": 0.12173030939786972,
      "grad_norm": 1.207933783531189,
      "learning_rate": 2.9066614649370916e-05,
      "loss": 0.9033,
      "step": 420
    },
    {
      "epoch": 0.124628650097819,
      "grad_norm": 1.0090701580047607,
      "learning_rate": 2.903735492051107e-05,
      "loss": 0.9332,
      "step": 430
    },
    {
      "epoch": 0.12752699079776827,
      "grad_norm": 1.0470784902572632,
      "learning_rate": 2.9008095191651224e-05,
      "loss": 0.9372,
      "step": 440
    },
    {
      "epoch": 0.13042533149771757,
      "grad_norm": 1.0461698770523071,
      "learning_rate": 2.897883546279138e-05,
      "loss": 0.9275,
      "step": 450
    },
    {
      "epoch": 0.13332367219766683,
      "grad_norm": 1.1230888366699219,
      "learning_rate": 2.894957573393153e-05,
      "loss": 0.939,
      "step": 460
    },
    {
      "epoch": 0.13622201289761612,
      "grad_norm": 1.1646445989608765,
      "learning_rate": 2.8920316005071686e-05,
      "loss": 0.8926,
      "step": 470
    },
    {
      "epoch": 0.1391203535975654,
      "grad_norm": 1.2699614763259888,
      "learning_rate": 2.8891056276211842e-05,
      "loss": 0.9218,
      "step": 480
    },
    {
      "epoch": 0.14201869429751468,
      "grad_norm": 1.0922670364379883,
      "learning_rate": 2.8861796547351994e-05,
      "loss": 0.9026,
      "step": 490
    },
    {
      "epoch": 0.14491703499746394,
      "grad_norm": 1.0282280445098877,
      "learning_rate": 2.883253681849215e-05,
      "loss": 0.9033,
      "step": 500
    },
    {
      "epoch": 0.14781537569741324,
      "grad_norm": 1.3141381740570068,
      "learning_rate": 2.8803277089632304e-05,
      "loss": 0.9492,
      "step": 510
    },
    {
      "epoch": 0.1507137163973625,
      "grad_norm": 0.9391818046569824,
      "learning_rate": 2.877401736077246e-05,
      "loss": 0.8952,
      "step": 520
    },
    {
      "epoch": 0.1536120570973118,
      "grad_norm": 0.958692729473114,
      "learning_rate": 2.8744757631912612e-05,
      "loss": 0.9201,
      "step": 530
    },
    {
      "epoch": 0.15651039779726106,
      "grad_norm": 0.917717456817627,
      "learning_rate": 2.8715497903052767e-05,
      "loss": 0.9322,
      "step": 540
    },
    {
      "epoch": 0.15940873849721035,
      "grad_norm": 1.0045287609100342,
      "learning_rate": 2.868623817419292e-05,
      "loss": 0.9117,
      "step": 550
    },
    {
      "epoch": 0.16230707919715961,
      "grad_norm": 0.9948278069496155,
      "learning_rate": 2.8656978445333074e-05,
      "loss": 0.9023,
      "step": 560
    },
    {
      "epoch": 0.1652054198971089,
      "grad_norm": 1.0290881395339966,
      "learning_rate": 2.8627718716473226e-05,
      "loss": 0.8773,
      "step": 570
    },
    {
      "epoch": 0.16810376059705817,
      "grad_norm": 1.4110438823699951,
      "learning_rate": 2.8598458987613382e-05,
      "loss": 0.8826,
      "step": 580
    },
    {
      "epoch": 0.17100210129700746,
      "grad_norm": 1.3016327619552612,
      "learning_rate": 2.8569199258753537e-05,
      "loss": 0.9385,
      "step": 590
    },
    {
      "epoch": 0.17390044199695676,
      "grad_norm": 1.118533968925476,
      "learning_rate": 2.853993952989369e-05,
      "loss": 0.9014,
      "step": 600
    },
    {
      "epoch": 0.17679878269690602,
      "grad_norm": 0.9670349359512329,
      "learning_rate": 2.8510679801033844e-05,
      "loss": 0.911,
      "step": 610
    },
    {
      "epoch": 0.1796971233968553,
      "grad_norm": 0.9766395688056946,
      "learning_rate": 2.8481420072173996e-05,
      "loss": 0.8981,
      "step": 620
    },
    {
      "epoch": 0.18259546409680458,
      "grad_norm": 0.893198549747467,
      "learning_rate": 2.8452160343314155e-05,
      "loss": 0.9088,
      "step": 630
    },
    {
      "epoch": 0.18549380479675387,
      "grad_norm": 0.9866193532943726,
      "learning_rate": 2.8422900614454307e-05,
      "loss": 0.8501,
      "step": 640
    },
    {
      "epoch": 0.18839214549670313,
      "grad_norm": 1.084509253501892,
      "learning_rate": 2.8393640885594462e-05,
      "loss": 0.9305,
      "step": 650
    },
    {
      "epoch": 0.19129048619665243,
      "grad_norm": 1.0652515888214111,
      "learning_rate": 2.8364381156734614e-05,
      "loss": 0.9092,
      "step": 660
    },
    {
      "epoch": 0.1941888268966017,
      "grad_norm": 1.180055856704712,
      "learning_rate": 2.833512142787477e-05,
      "loss": 0.8972,
      "step": 670
    },
    {
      "epoch": 0.19708716759655098,
      "grad_norm": 1.2414605617523193,
      "learning_rate": 2.8305861699014922e-05,
      "loss": 0.8664,
      "step": 680
    },
    {
      "epoch": 0.19998550829650025,
      "grad_norm": 1.043639898300171,
      "learning_rate": 2.8276601970155077e-05,
      "loss": 0.8821,
      "step": 690
    },
    {
      "epoch": 0.20288384899644954,
      "grad_norm": 1.0937316417694092,
      "learning_rate": 2.8247342241295232e-05,
      "loss": 0.9298,
      "step": 700
    },
    {
      "epoch": 0.2057821896963988,
      "grad_norm": 1.2370545864105225,
      "learning_rate": 2.8218082512435384e-05,
      "loss": 0.8495,
      "step": 710
    },
    {
      "epoch": 0.2086805303963481,
      "grad_norm": 0.9994855523109436,
      "learning_rate": 2.818882278357554e-05,
      "loss": 0.928,
      "step": 720
    },
    {
      "epoch": 0.21157887109629736,
      "grad_norm": 1.0372546911239624,
      "learning_rate": 2.8159563054715692e-05,
      "loss": 0.953,
      "step": 730
    },
    {
      "epoch": 0.21447721179624665,
      "grad_norm": 1.1301555633544922,
      "learning_rate": 2.8130303325855847e-05,
      "loss": 0.9022,
      "step": 740
    },
    {
      "epoch": 0.21737555249619592,
      "grad_norm": 1.231650471687317,
      "learning_rate": 2.8101043596996002e-05,
      "loss": 0.8629,
      "step": 750
    },
    {
      "epoch": 0.2202738931961452,
      "grad_norm": 1.2426495552062988,
      "learning_rate": 2.8071783868136158e-05,
      "loss": 0.9115,
      "step": 760
    },
    {
      "epoch": 0.22317223389609447,
      "grad_norm": 0.9801905155181885,
      "learning_rate": 2.804252413927631e-05,
      "loss": 0.8414,
      "step": 770
    },
    {
      "epoch": 0.22607057459604377,
      "grad_norm": 1.2423139810562134,
      "learning_rate": 2.8013264410416465e-05,
      "loss": 0.9039,
      "step": 780
    },
    {
      "epoch": 0.22896891529599303,
      "grad_norm": 1.22016179561615,
      "learning_rate": 2.7984004681556617e-05,
      "loss": 0.9496,
      "step": 790
    },
    {
      "epoch": 0.23186725599594232,
      "grad_norm": 1.0902771949768066,
      "learning_rate": 2.7954744952696772e-05,
      "loss": 0.915,
      "step": 800
    },
    {
      "epoch": 0.23476559669589162,
      "grad_norm": 0.9486859440803528,
      "learning_rate": 2.7925485223836928e-05,
      "loss": 0.8786,
      "step": 810
    },
    {
      "epoch": 0.23766393739584088,
      "grad_norm": 1.3923839330673218,
      "learning_rate": 2.789622549497708e-05,
      "loss": 0.8718,
      "step": 820
    },
    {
      "epoch": 0.24056227809579017,
      "grad_norm": 1.4518966674804688,
      "learning_rate": 2.7866965766117235e-05,
      "loss": 0.8767,
      "step": 830
    },
    {
      "epoch": 0.24346061879573944,
      "grad_norm": 1.251938819885254,
      "learning_rate": 2.7837706037257387e-05,
      "loss": 0.9203,
      "step": 840
    },
    {
      "epoch": 0.24635895949568873,
      "grad_norm": 0.9935092329978943,
      "learning_rate": 2.7808446308397542e-05,
      "loss": 0.8644,
      "step": 850
    },
    {
      "epoch": 0.249257300195638,
      "grad_norm": 1.0347315073013306,
      "learning_rate": 2.7779186579537694e-05,
      "loss": 0.8665,
      "step": 860
    },
    {
      "epoch": 0.2521556408955873,
      "grad_norm": 1.2371271848678589,
      "learning_rate": 2.7749926850677853e-05,
      "loss": 0.9719,
      "step": 870
    },
    {
      "epoch": 0.25505398159553655,
      "grad_norm": 1.4779818058013916,
      "learning_rate": 2.7720667121818005e-05,
      "loss": 0.9202,
      "step": 880
    },
    {
      "epoch": 0.2579523222954858,
      "grad_norm": 1.33644437789917,
      "learning_rate": 2.769140739295816e-05,
      "loss": 0.9025,
      "step": 890
    },
    {
      "epoch": 0.26085066299543513,
      "grad_norm": 1.3140952587127686,
      "learning_rate": 2.7662147664098312e-05,
      "loss": 0.8546,
      "step": 900
    },
    {
      "epoch": 0.2637490036953844,
      "grad_norm": 1.3362756967544556,
      "learning_rate": 2.7632887935238468e-05,
      "loss": 0.9014,
      "step": 910
    },
    {
      "epoch": 0.26664734439533366,
      "grad_norm": 1.4182583093643188,
      "learning_rate": 2.7603628206378623e-05,
      "loss": 0.9459,
      "step": 920
    },
    {
      "epoch": 0.2695456850952829,
      "grad_norm": 1.3038769960403442,
      "learning_rate": 2.7574368477518775e-05,
      "loss": 0.9279,
      "step": 930
    },
    {
      "epoch": 0.27244402579523225,
      "grad_norm": 1.105399250984192,
      "learning_rate": 2.754510874865893e-05,
      "loss": 0.9158,
      "step": 940
    },
    {
      "epoch": 0.2753423664951815,
      "grad_norm": 1.0510600805282593,
      "learning_rate": 2.7515849019799083e-05,
      "loss": 0.9334,
      "step": 950
    },
    {
      "epoch": 0.2782407071951308,
      "grad_norm": 1.1788643598556519,
      "learning_rate": 2.7486589290939238e-05,
      "loss": 0.8597,
      "step": 960
    },
    {
      "epoch": 0.28113904789508004,
      "grad_norm": 1.2561324834823608,
      "learning_rate": 2.745732956207939e-05,
      "loss": 0.8886,
      "step": 970
    },
    {
      "epoch": 0.28403738859502936,
      "grad_norm": 1.2054704427719116,
      "learning_rate": 2.7428069833219545e-05,
      "loss": 0.9171,
      "step": 980
    },
    {
      "epoch": 0.2869357292949786,
      "grad_norm": 1.4353606700897217,
      "learning_rate": 2.73988101043597e-05,
      "loss": 0.8789,
      "step": 990
    },
    {
      "epoch": 0.2898340699949279,
      "grad_norm": 1.1557574272155762,
      "learning_rate": 2.7369550375499856e-05,
      "loss": 0.8734,
      "step": 1000
    },
    {
      "epoch": 0.2927324106948772,
      "grad_norm": 1.0467464923858643,
      "learning_rate": 2.7340290646640008e-05,
      "loss": 0.9258,
      "step": 1010
    },
    {
      "epoch": 0.2956307513948265,
      "grad_norm": 1.2090234756469727,
      "learning_rate": 2.7311030917780163e-05,
      "loss": 0.8995,
      "step": 1020
    },
    {
      "epoch": 0.29852909209477574,
      "grad_norm": 1.3610193729400635,
      "learning_rate": 2.728177118892032e-05,
      "loss": 0.8748,
      "step": 1030
    },
    {
      "epoch": 0.301427432794725,
      "grad_norm": 1.1105715036392212,
      "learning_rate": 2.725251146006047e-05,
      "loss": 0.9026,
      "step": 1040
    },
    {
      "epoch": 0.3043257734946743,
      "grad_norm": 1.1190853118896484,
      "learning_rate": 2.7223251731200626e-05,
      "loss": 0.8295,
      "step": 1050
    },
    {
      "epoch": 0.3072241141946236,
      "grad_norm": 1.3249413967132568,
      "learning_rate": 2.7193992002340778e-05,
      "loss": 0.901,
      "step": 1060
    },
    {
      "epoch": 0.31012245489457285,
      "grad_norm": 1.4877322912216187,
      "learning_rate": 2.7164732273480933e-05,
      "loss": 0.9021,
      "step": 1070
    },
    {
      "epoch": 0.3130207955945221,
      "grad_norm": 1.2175993919372559,
      "learning_rate": 2.7135472544621085e-05,
      "loss": 0.8987,
      "step": 1080
    },
    {
      "epoch": 0.31591913629447144,
      "grad_norm": 1.504135012626648,
      "learning_rate": 2.710621281576124e-05,
      "loss": 0.9099,
      "step": 1090
    },
    {
      "epoch": 0.3188174769944207,
      "grad_norm": 1.242238998413086,
      "learning_rate": 2.7076953086901396e-05,
      "loss": 0.8789,
      "step": 1100
    },
    {
      "epoch": 0.32171581769436997,
      "grad_norm": 1.1817708015441895,
      "learning_rate": 2.704769335804155e-05,
      "loss": 0.8714,
      "step": 1110
    },
    {
      "epoch": 0.32461415839431923,
      "grad_norm": 1.1511441469192505,
      "learning_rate": 2.7018433629181703e-05,
      "loss": 0.8711,
      "step": 1120
    },
    {
      "epoch": 0.32751249909426855,
      "grad_norm": 1.2187455892562866,
      "learning_rate": 2.698917390032186e-05,
      "loss": 0.8611,
      "step": 1130
    },
    {
      "epoch": 0.3304108397942178,
      "grad_norm": 1.452215552330017,
      "learning_rate": 2.6959914171462014e-05,
      "loss": 0.8688,
      "step": 1140
    },
    {
      "epoch": 0.3333091804941671,
      "grad_norm": 1.2491434812545776,
      "learning_rate": 2.6930654442602166e-05,
      "loss": 0.8748,
      "step": 1150
    },
    {
      "epoch": 0.33620752119411634,
      "grad_norm": 1.62991201877594,
      "learning_rate": 2.690139471374232e-05,
      "loss": 0.9478,
      "step": 1160
    },
    {
      "epoch": 0.33910586189406566,
      "grad_norm": 1.3282746076583862,
      "learning_rate": 2.6872134984882473e-05,
      "loss": 0.8858,
      "step": 1170
    },
    {
      "epoch": 0.3420042025940149,
      "grad_norm": 1.2107800245285034,
      "learning_rate": 2.684287525602263e-05,
      "loss": 0.8482,
      "step": 1180
    },
    {
      "epoch": 0.3449025432939642,
      "grad_norm": 1.212971806526184,
      "learning_rate": 2.681361552716278e-05,
      "loss": 0.9426,
      "step": 1190
    },
    {
      "epoch": 0.3478008839939135,
      "grad_norm": 1.501225471496582,
      "learning_rate": 2.6784355798302936e-05,
      "loss": 0.909,
      "step": 1200
    },
    {
      "epoch": 0.3506992246938628,
      "grad_norm": 1.331976294517517,
      "learning_rate": 2.6755096069443088e-05,
      "loss": 0.8926,
      "step": 1210
    },
    {
      "epoch": 0.35359756539381204,
      "grad_norm": 1.2488893270492554,
      "learning_rate": 2.6725836340583247e-05,
      "loss": 0.8302,
      "step": 1220
    },
    {
      "epoch": 0.3564959060937613,
      "grad_norm": 1.4553574323654175,
      "learning_rate": 2.66965766117234e-05,
      "loss": 0.9244,
      "step": 1230
    },
    {
      "epoch": 0.3593942467937106,
      "grad_norm": 1.1644861698150635,
      "learning_rate": 2.6667316882863554e-05,
      "loss": 0.9134,
      "step": 1240
    },
    {
      "epoch": 0.3622925874936599,
      "grad_norm": 1.332711935043335,
      "learning_rate": 2.663805715400371e-05,
      "loss": 0.8733,
      "step": 1250
    },
    {
      "epoch": 0.36519092819360915,
      "grad_norm": 1.37628972530365,
      "learning_rate": 2.660879742514386e-05,
      "loss": 0.8753,
      "step": 1260
    },
    {
      "epoch": 0.3680892688935584,
      "grad_norm": 1.3309943675994873,
      "learning_rate": 2.6579537696284017e-05,
      "loss": 0.8957,
      "step": 1270
    },
    {
      "epoch": 0.37098760959350774,
      "grad_norm": 1.2598142623901367,
      "learning_rate": 2.655027796742417e-05,
      "loss": 0.8797,
      "step": 1280
    },
    {
      "epoch": 0.373885950293457,
      "grad_norm": 1.2162960767745972,
      "learning_rate": 2.6521018238564324e-05,
      "loss": 0.8428,
      "step": 1290
    },
    {
      "epoch": 0.37678429099340627,
      "grad_norm": 1.1423568725585938,
      "learning_rate": 2.6491758509704476e-05,
      "loss": 0.9243,
      "step": 1300
    },
    {
      "epoch": 0.37968263169335553,
      "grad_norm": 1.2369141578674316,
      "learning_rate": 2.646249878084463e-05,
      "loss": 0.8534,
      "step": 1310
    },
    {
      "epoch": 0.38258097239330485,
      "grad_norm": 1.8136287927627563,
      "learning_rate": 2.6433239051984783e-05,
      "loss": 0.8617,
      "step": 1320
    },
    {
      "epoch": 0.3854793130932541,
      "grad_norm": 1.3318501710891724,
      "learning_rate": 2.640397932312494e-05,
      "loss": 0.8786,
      "step": 1330
    },
    {
      "epoch": 0.3883776537932034,
      "grad_norm": 1.3074418306350708,
      "learning_rate": 2.6374719594265094e-05,
      "loss": 0.8787,
      "step": 1340
    },
    {
      "epoch": 0.39127599449315265,
      "grad_norm": 1.1670410633087158,
      "learning_rate": 2.634545986540525e-05,
      "loss": 0.9122,
      "step": 1350
    },
    {
      "epoch": 0.39417433519310197,
      "grad_norm": 1.2949100732803345,
      "learning_rate": 2.63162001365454e-05,
      "loss": 0.8743,
      "step": 1360
    },
    {
      "epoch": 0.39707267589305123,
      "grad_norm": 1.183980107307434,
      "learning_rate": 2.6286940407685557e-05,
      "loss": 0.8803,
      "step": 1370
    },
    {
      "epoch": 0.3999710165930005,
      "grad_norm": 1.1525731086730957,
      "learning_rate": 2.6257680678825712e-05,
      "loss": 0.9291,
      "step": 1380
    },
    {
      "epoch": 0.40286935729294976,
      "grad_norm": 1.1755003929138184,
      "learning_rate": 2.6228420949965864e-05,
      "loss": 0.8223,
      "step": 1390
    },
    {
      "epoch": 0.4057676979928991,
      "grad_norm": 1.5017238855361938,
      "learning_rate": 2.619916122110602e-05,
      "loss": 0.9084,
      "step": 1400
    },
    {
      "epoch": 0.40866603869284834,
      "grad_norm": 1.3843084573745728,
      "learning_rate": 2.616990149224617e-05,
      "loss": 0.8933,
      "step": 1410
    },
    {
      "epoch": 0.4115643793927976,
      "grad_norm": 1.2243154048919678,
      "learning_rate": 2.6140641763386327e-05,
      "loss": 0.8915,
      "step": 1420
    },
    {
      "epoch": 0.41446272009274693,
      "grad_norm": 2.3163535594940186,
      "learning_rate": 2.611138203452648e-05,
      "loss": 0.9149,
      "step": 1430
    },
    {
      "epoch": 0.4173610607926962,
      "grad_norm": 1.3018426895141602,
      "learning_rate": 2.6082122305666634e-05,
      "loss": 0.8289,
      "step": 1440
    },
    {
      "epoch": 0.42025940149264546,
      "grad_norm": 1.07835054397583,
      "learning_rate": 2.6052862576806786e-05,
      "loss": 0.9146,
      "step": 1450
    },
    {
      "epoch": 0.4231577421925947,
      "grad_norm": 1.308356523513794,
      "learning_rate": 2.6023602847946945e-05,
      "loss": 0.8704,
      "step": 1460
    },
    {
      "epoch": 0.42605608289254404,
      "grad_norm": 1.254543423652649,
      "learning_rate": 2.5994343119087097e-05,
      "loss": 0.8569,
      "step": 1470
    },
    {
      "epoch": 0.4289544235924933,
      "grad_norm": 1.457005500793457,
      "learning_rate": 2.5965083390227252e-05,
      "loss": 0.8587,
      "step": 1480
    },
    {
      "epoch": 0.43185276429244257,
      "grad_norm": 1.369938850402832,
      "learning_rate": 2.5935823661367407e-05,
      "loss": 0.8316,
      "step": 1490
    },
    {
      "epoch": 0.43475110499239183,
      "grad_norm": 1.4121698141098022,
      "learning_rate": 2.590656393250756e-05,
      "loss": 0.8751,
      "step": 1500
    },
    {
      "epoch": 0.43764944569234115,
      "grad_norm": 1.4145679473876953,
      "learning_rate": 2.5877304203647715e-05,
      "loss": 0.8479,
      "step": 1510
    },
    {
      "epoch": 0.4405477863922904,
      "grad_norm": 1.5878751277923584,
      "learning_rate": 2.5848044474787867e-05,
      "loss": 0.831,
      "step": 1520
    },
    {
      "epoch": 0.4434461270922397,
      "grad_norm": 1.1164807081222534,
      "learning_rate": 2.5818784745928022e-05,
      "loss": 0.8945,
      "step": 1530
    },
    {
      "epoch": 0.44634446779218895,
      "grad_norm": 1.2600536346435547,
      "learning_rate": 2.5789525017068174e-05,
      "loss": 0.8729,
      "step": 1540
    },
    {
      "epoch": 0.44924280849213827,
      "grad_norm": 1.411799669265747,
      "learning_rate": 2.576026528820833e-05,
      "loss": 0.8922,
      "step": 1550
    },
    {
      "epoch": 0.45214114919208753,
      "grad_norm": 1.2550870180130005,
      "learning_rate": 2.573100555934848e-05,
      "loss": 0.8256,
      "step": 1560
    },
    {
      "epoch": 0.4550394898920368,
      "grad_norm": 1.4524996280670166,
      "learning_rate": 2.5701745830488637e-05,
      "loss": 0.8934,
      "step": 1570
    },
    {
      "epoch": 0.45793783059198606,
      "grad_norm": 1.3843027353286743,
      "learning_rate": 2.5672486101628792e-05,
      "loss": 0.8657,
      "step": 1580
    },
    {
      "epoch": 0.4608361712919354,
      "grad_norm": 1.4566541910171509,
      "learning_rate": 2.5643226372768947e-05,
      "loss": 0.8551,
      "step": 1590
    },
    {
      "epoch": 0.46373451199188465,
      "grad_norm": 1.962372064590454,
      "learning_rate": 2.5613966643909103e-05,
      "loss": 0.8751,
      "step": 1600
    },
    {
      "epoch": 0.4666328526918339,
      "grad_norm": 1.384137749671936,
      "learning_rate": 2.5584706915049255e-05,
      "loss": 0.8828,
      "step": 1610
    },
    {
      "epoch": 0.46953119339178323,
      "grad_norm": 1.307927131652832,
      "learning_rate": 2.555544718618941e-05,
      "loss": 0.9025,
      "step": 1620
    },
    {
      "epoch": 0.4724295340917325,
      "grad_norm": 1.263610601425171,
      "learning_rate": 2.5526187457329562e-05,
      "loss": 0.8486,
      "step": 1630
    },
    {
      "epoch": 0.47532787479168176,
      "grad_norm": 1.388692021369934,
      "learning_rate": 2.5496927728469717e-05,
      "loss": 0.8432,
      "step": 1640
    },
    {
      "epoch": 0.478226215491631,
      "grad_norm": 1.1334228515625,
      "learning_rate": 2.546766799960987e-05,
      "loss": 0.8204,
      "step": 1650
    },
    {
      "epoch": 0.48112455619158034,
      "grad_norm": 1.3656973838806152,
      "learning_rate": 2.5438408270750025e-05,
      "loss": 0.8536,
      "step": 1660
    },
    {
      "epoch": 0.4840228968915296,
      "grad_norm": 1.4803073406219482,
      "learning_rate": 2.5409148541890177e-05,
      "loss": 0.848,
      "step": 1670
    },
    {
      "epoch": 0.4869212375914789,
      "grad_norm": 1.3389496803283691,
      "learning_rate": 2.5379888813030332e-05,
      "loss": 0.8216,
      "step": 1680
    },
    {
      "epoch": 0.48981957829142814,
      "grad_norm": 1.3452893495559692,
      "learning_rate": 2.5350629084170484e-05,
      "loss": 0.8099,
      "step": 1690
    },
    {
      "epoch": 0.49271791899137746,
      "grad_norm": 1.601650595664978,
      "learning_rate": 2.5321369355310643e-05,
      "loss": 0.9171,
      "step": 1700
    },
    {
      "epoch": 0.4956162596913267,
      "grad_norm": 1.293866515159607,
      "learning_rate": 2.5292109626450798e-05,
      "loss": 0.855,
      "step": 1710
    },
    {
      "epoch": 0.498514600391276,
      "grad_norm": 1.3094253540039062,
      "learning_rate": 2.526284989759095e-05,
      "loss": 0.8794,
      "step": 1720
    },
    {
      "epoch": 0.5014129410912253,
      "grad_norm": 1.4510518312454224,
      "learning_rate": 2.5233590168731106e-05,
      "loss": 0.8652,
      "step": 1730
    },
    {
      "epoch": 0.5043112817911746,
      "grad_norm": 1.1973965167999268,
      "learning_rate": 2.5204330439871257e-05,
      "loss": 0.8776,
      "step": 1740
    },
    {
      "epoch": 0.5072096224911238,
      "grad_norm": 1.2950069904327393,
      "learning_rate": 2.5175070711011413e-05,
      "loss": 0.8689,
      "step": 1750
    },
    {
      "epoch": 0.5101079631910731,
      "grad_norm": 1.82252037525177,
      "learning_rate": 2.5145810982151565e-05,
      "loss": 0.8787,
      "step": 1760
    },
    {
      "epoch": 0.5130063038910224,
      "grad_norm": 1.3151756525039673,
      "learning_rate": 2.511655125329172e-05,
      "loss": 0.8768,
      "step": 1770
    },
    {
      "epoch": 0.5159046445909716,
      "grad_norm": 1.523553729057312,
      "learning_rate": 2.5087291524431872e-05,
      "loss": 0.828,
      "step": 1780
    },
    {
      "epoch": 0.518802985290921,
      "grad_norm": 1.4939448833465576,
      "learning_rate": 2.5058031795572027e-05,
      "loss": 0.8701,
      "step": 1790
    },
    {
      "epoch": 0.5217013259908703,
      "grad_norm": 1.4368760585784912,
      "learning_rate": 2.502877206671218e-05,
      "loss": 0.8332,
      "step": 1800
    },
    {
      "epoch": 0.5245996666908195,
      "grad_norm": 1.3084473609924316,
      "learning_rate": 2.4999512337852335e-05,
      "loss": 0.8645,
      "step": 1810
    },
    {
      "epoch": 0.5274980073907688,
      "grad_norm": 1.190513014793396,
      "learning_rate": 2.4970252608992494e-05,
      "loss": 0.7957,
      "step": 1820
    },
    {
      "epoch": 0.5303963480907181,
      "grad_norm": 1.4284590482711792,
      "learning_rate": 2.4940992880132646e-05,
      "loss": 0.8962,
      "step": 1830
    },
    {
      "epoch": 0.5332946887906673,
      "grad_norm": 1.39321768283844,
      "learning_rate": 2.49117331512728e-05,
      "loss": 0.8253,
      "step": 1840
    },
    {
      "epoch": 0.5361930294906166,
      "grad_norm": 1.2532862424850464,
      "learning_rate": 2.4882473422412953e-05,
      "loss": 0.8486,
      "step": 1850
    },
    {
      "epoch": 0.5390913701905659,
      "grad_norm": 1.2693310976028442,
      "learning_rate": 2.4853213693553108e-05,
      "loss": 0.8531,
      "step": 1860
    },
    {
      "epoch": 0.5419897108905152,
      "grad_norm": 1.5996760129928589,
      "learning_rate": 2.482395396469326e-05,
      "loss": 0.909,
      "step": 1870
    },
    {
      "epoch": 0.5448880515904645,
      "grad_norm": 1.237993597984314,
      "learning_rate": 2.4794694235833416e-05,
      "loss": 0.8212,
      "step": 1880
    },
    {
      "epoch": 0.5477863922904137,
      "grad_norm": 1.6530269384384155,
      "learning_rate": 2.4765434506973567e-05,
      "loss": 0.9016,
      "step": 1890
    },
    {
      "epoch": 0.550684732990363,
      "grad_norm": 1.240643858909607,
      "learning_rate": 2.4736174778113723e-05,
      "loss": 0.8626,
      "step": 1900
    },
    {
      "epoch": 0.5535830736903123,
      "grad_norm": 1.5372411012649536,
      "learning_rate": 2.4706915049253875e-05,
      "loss": 0.8467,
      "step": 1910
    },
    {
      "epoch": 0.5564814143902616,
      "grad_norm": 1.4558846950531006,
      "learning_rate": 2.467765532039403e-05,
      "loss": 0.8528,
      "step": 1920
    },
    {
      "epoch": 0.5593797550902109,
      "grad_norm": 1.298606276512146,
      "learning_rate": 2.4648395591534186e-05,
      "loss": 0.8758,
      "step": 1930
    },
    {
      "epoch": 0.5622780957901601,
      "grad_norm": 1.6030486822128296,
      "learning_rate": 2.461913586267434e-05,
      "loss": 0.8487,
      "step": 1940
    },
    {
      "epoch": 0.5651764364901094,
      "grad_norm": 1.2583656311035156,
      "learning_rate": 2.4589876133814496e-05,
      "loss": 0.8981,
      "step": 1950
    },
    {
      "epoch": 0.5680747771900587,
      "grad_norm": 1.8382164239883423,
      "learning_rate": 2.4560616404954648e-05,
      "loss": 0.7968,
      "step": 1960
    },
    {
      "epoch": 0.5709731178900079,
      "grad_norm": 1.514898657798767,
      "learning_rate": 2.4531356676094804e-05,
      "loss": 0.8533,
      "step": 1970
    },
    {
      "epoch": 0.5738714585899573,
      "grad_norm": 1.5946706533432007,
      "learning_rate": 2.4502096947234956e-05,
      "loss": 0.8888,
      "step": 1980
    },
    {
      "epoch": 0.5767697992899066,
      "grad_norm": 1.5617563724517822,
      "learning_rate": 2.447283721837511e-05,
      "loss": 0.8387,
      "step": 1990
    },
    {
      "epoch": 0.5796681399898558,
      "grad_norm": 2.0273220539093018,
      "learning_rate": 2.4443577489515263e-05,
      "loss": 0.8689,
      "step": 2000
    },
    {
      "epoch": 0.5825664806898051,
      "grad_norm": 1.0960315465927124,
      "learning_rate": 2.4414317760655418e-05,
      "loss": 0.8243,
      "step": 2010
    },
    {
      "epoch": 0.5854648213897544,
      "grad_norm": 1.4365547895431519,
      "learning_rate": 2.438505803179557e-05,
      "loss": 0.8578,
      "step": 2020
    },
    {
      "epoch": 0.5883631620897036,
      "grad_norm": 1.3741360902786255,
      "learning_rate": 2.4355798302935726e-05,
      "loss": 0.8559,
      "step": 2030
    },
    {
      "epoch": 0.591261502789653,
      "grad_norm": 1.5457944869995117,
      "learning_rate": 2.432653857407588e-05,
      "loss": 0.8209,
      "step": 2040
    },
    {
      "epoch": 0.5941598434896022,
      "grad_norm": 1.1857012510299683,
      "learning_rate": 2.4297278845216036e-05,
      "loss": 0.8506,
      "step": 2050
    },
    {
      "epoch": 0.5970581841895515,
      "grad_norm": 1.6900160312652588,
      "learning_rate": 2.426801911635619e-05,
      "loss": 0.9127,
      "step": 2060
    },
    {
      "epoch": 0.5999565248895008,
      "grad_norm": 1.4448155164718628,
      "learning_rate": 2.4238759387496344e-05,
      "loss": 0.8857,
      "step": 2070
    },
    {
      "epoch": 0.60285486558945,
      "grad_norm": 1.4431557655334473,
      "learning_rate": 2.42094996586365e-05,
      "loss": 0.8527,
      "step": 2080
    },
    {
      "epoch": 0.6057532062893993,
      "grad_norm": 1.3373472690582275,
      "learning_rate": 2.418023992977665e-05,
      "loss": 0.8577,
      "step": 2090
    },
    {
      "epoch": 0.6086515469893486,
      "grad_norm": 1.5876868963241577,
      "learning_rate": 2.4150980200916806e-05,
      "loss": 0.9003,
      "step": 2100
    },
    {
      "epoch": 0.6115498876892979,
      "grad_norm": 1.561641812324524,
      "learning_rate": 2.4121720472056958e-05,
      "loss": 0.861,
      "step": 2110
    },
    {
      "epoch": 0.6144482283892472,
      "grad_norm": 1.3280630111694336,
      "learning_rate": 2.4092460743197114e-05,
      "loss": 0.8305,
      "step": 2120
    },
    {
      "epoch": 0.6173465690891964,
      "grad_norm": 1.5482535362243652,
      "learning_rate": 2.4063201014337266e-05,
      "loss": 0.8657,
      "step": 2130
    },
    {
      "epoch": 0.6202449097891457,
      "grad_norm": 1.3530014753341675,
      "learning_rate": 2.403394128547742e-05,
      "loss": 0.8279,
      "step": 2140
    },
    {
      "epoch": 0.623143250489095,
      "grad_norm": 1.3755624294281006,
      "learning_rate": 2.4004681556617576e-05,
      "loss": 0.821,
      "step": 2150
    },
    {
      "epoch": 0.6260415911890442,
      "grad_norm": 1.404031753540039,
      "learning_rate": 2.3975421827757728e-05,
      "loss": 0.8082,
      "step": 2160
    },
    {
      "epoch": 0.6289399318889936,
      "grad_norm": 1.3341163396835327,
      "learning_rate": 2.3946162098897887e-05,
      "loss": 0.8154,
      "step": 2170
    },
    {
      "epoch": 0.6318382725889429,
      "grad_norm": 1.375126600265503,
      "learning_rate": 2.391690237003804e-05,
      "loss": 0.8115,
      "step": 2180
    },
    {
      "epoch": 0.6347366132888921,
      "grad_norm": 1.574288249015808,
      "learning_rate": 2.3887642641178194e-05,
      "loss": 0.8191,
      "step": 2190
    },
    {
      "epoch": 0.6376349539888414,
      "grad_norm": 1.4847670793533325,
      "learning_rate": 2.3858382912318346e-05,
      "loss": 0.8145,
      "step": 2200
    },
    {
      "epoch": 0.6405332946887907,
      "grad_norm": 1.5132156610488892,
      "learning_rate": 2.38291231834585e-05,
      "loss": 0.8609,
      "step": 2210
    },
    {
      "epoch": 0.6434316353887399,
      "grad_norm": 1.3112412691116333,
      "learning_rate": 2.3799863454598654e-05,
      "loss": 0.8175,
      "step": 2220
    },
    {
      "epoch": 0.6463299760886893,
      "grad_norm": 1.2409124374389648,
      "learning_rate": 2.377060372573881e-05,
      "loss": 0.8938,
      "step": 2230
    },
    {
      "epoch": 0.6492283167886385,
      "grad_norm": 1.5430325269699097,
      "learning_rate": 2.374134399687896e-05,
      "loss": 0.8797,
      "step": 2240
    },
    {
      "epoch": 0.6521266574885878,
      "grad_norm": 1.5697087049484253,
      "learning_rate": 2.3712084268019116e-05,
      "loss": 0.8226,
      "step": 2250
    },
    {
      "epoch": 0.6550249981885371,
      "grad_norm": 1.4323654174804688,
      "learning_rate": 2.368282453915927e-05,
      "loss": 0.826,
      "step": 2260
    },
    {
      "epoch": 0.6579233388884863,
      "grad_norm": 1.5626685619354248,
      "learning_rate": 2.3653564810299424e-05,
      "loss": 0.7994,
      "step": 2270
    },
    {
      "epoch": 0.6608216795884356,
      "grad_norm": 1.1792423725128174,
      "learning_rate": 2.362430508143958e-05,
      "loss": 0.7771,
      "step": 2280
    },
    {
      "epoch": 0.663720020288385,
      "grad_norm": 1.3228245973587036,
      "learning_rate": 2.3595045352579734e-05,
      "loss": 0.8199,
      "step": 2290
    },
    {
      "epoch": 0.6666183609883342,
      "grad_norm": 1.365393877029419,
      "learning_rate": 2.356578562371989e-05,
      "loss": 0.8148,
      "step": 2300
    },
    {
      "epoch": 0.6695167016882835,
      "grad_norm": 1.3783243894577026,
      "learning_rate": 2.353652589486004e-05,
      "loss": 0.8352,
      "step": 2310
    },
    {
      "epoch": 0.6724150423882327,
      "grad_norm": 1.4973610639572144,
      "learning_rate": 2.3507266166000197e-05,
      "loss": 0.7866,
      "step": 2320
    },
    {
      "epoch": 0.675313383088182,
      "grad_norm": 1.4965370893478394,
      "learning_rate": 2.347800643714035e-05,
      "loss": 0.8377,
      "step": 2330
    },
    {
      "epoch": 0.6782117237881313,
      "grad_norm": 1.7222802639007568,
      "learning_rate": 2.3448746708280504e-05,
      "loss": 0.8277,
      "step": 2340
    },
    {
      "epoch": 0.6811100644880805,
      "grad_norm": 1.4667978286743164,
      "learning_rate": 2.3419486979420656e-05,
      "loss": 0.8169,
      "step": 2350
    },
    {
      "epoch": 0.6840084051880299,
      "grad_norm": 1.6826497316360474,
      "learning_rate": 2.339022725056081e-05,
      "loss": 0.8654,
      "step": 2360
    },
    {
      "epoch": 0.6869067458879792,
      "grad_norm": 1.6340490579605103,
      "learning_rate": 2.3360967521700967e-05,
      "loss": 0.8513,
      "step": 2370
    },
    {
      "epoch": 0.6898050865879284,
      "grad_norm": 1.3660898208618164,
      "learning_rate": 2.333170779284112e-05,
      "loss": 0.8628,
      "step": 2380
    },
    {
      "epoch": 0.6927034272878777,
      "grad_norm": 1.6309499740600586,
      "learning_rate": 2.3302448063981274e-05,
      "loss": 0.8528,
      "step": 2390
    },
    {
      "epoch": 0.695601767987827,
      "grad_norm": 1.3042734861373901,
      "learning_rate": 2.3273188335121426e-05,
      "loss": 0.8837,
      "step": 2400
    },
    {
      "epoch": 0.6985001086877762,
      "grad_norm": 1.6256147623062134,
      "learning_rate": 2.3243928606261585e-05,
      "loss": 0.8304,
      "step": 2410
    },
    {
      "epoch": 0.7013984493877256,
      "grad_norm": 1.4961172342300415,
      "learning_rate": 2.3214668877401737e-05,
      "loss": 0.8935,
      "step": 2420
    },
    {
      "epoch": 0.7042967900876748,
      "grad_norm": 1.6352620124816895,
      "learning_rate": 2.3185409148541892e-05,
      "loss": 0.9215,
      "step": 2430
    },
    {
      "epoch": 0.7071951307876241,
      "grad_norm": 1.3855648040771484,
      "learning_rate": 2.3156149419682044e-05,
      "loss": 0.8189,
      "step": 2440
    },
    {
      "epoch": 0.7100934714875734,
      "grad_norm": 2.1157989501953125,
      "learning_rate": 2.31268896908222e-05,
      "loss": 0.7928,
      "step": 2450
    },
    {
      "epoch": 0.7129918121875226,
      "grad_norm": 1.4614135026931763,
      "learning_rate": 2.3097629961962352e-05,
      "loss": 0.8825,
      "step": 2460
    },
    {
      "epoch": 0.7158901528874719,
      "grad_norm": 1.233720302581787,
      "learning_rate": 2.3068370233102507e-05,
      "loss": 0.8237,
      "step": 2470
    },
    {
      "epoch": 0.7187884935874213,
      "grad_norm": 1.4894895553588867,
      "learning_rate": 2.3039110504242662e-05,
      "loss": 0.8531,
      "step": 2480
    },
    {
      "epoch": 0.7216868342873705,
      "grad_norm": 1.4314333200454712,
      "learning_rate": 2.3009850775382814e-05,
      "loss": 0.8031,
      "step": 2490
    },
    {
      "epoch": 0.7245851749873198,
      "grad_norm": 1.5179351568222046,
      "learning_rate": 2.298059104652297e-05,
      "loss": 0.7856,
      "step": 2500
    },
    {
      "epoch": 0.727483515687269,
      "grad_norm": 1.4902690649032593,
      "learning_rate": 2.2951331317663122e-05,
      "loss": 0.843,
      "step": 2510
    },
    {
      "epoch": 0.7303818563872183,
      "grad_norm": 1.4824994802474976,
      "learning_rate": 2.2922071588803277e-05,
      "loss": 0.7891,
      "step": 2520
    },
    {
      "epoch": 0.7332801970871676,
      "grad_norm": 1.2545102834701538,
      "learning_rate": 2.2892811859943432e-05,
      "loss": 0.8826,
      "step": 2530
    },
    {
      "epoch": 0.7361785377871168,
      "grad_norm": 1.596333622932434,
      "learning_rate": 2.2863552131083588e-05,
      "loss": 0.8528,
      "step": 2540
    },
    {
      "epoch": 0.7390768784870662,
      "grad_norm": 1.3387072086334229,
      "learning_rate": 2.283429240222374e-05,
      "loss": 0.7922,
      "step": 2550
    },
    {
      "epoch": 0.7419752191870155,
      "grad_norm": 1.5349425077438354,
      "learning_rate": 2.2805032673363895e-05,
      "loss": 0.8374,
      "step": 2560
    },
    {
      "epoch": 0.7448735598869647,
      "grad_norm": 1.4213930368423462,
      "learning_rate": 2.2775772944504047e-05,
      "loss": 0.8261,
      "step": 2570
    },
    {
      "epoch": 0.747771900586914,
      "grad_norm": 1.7229437828063965,
      "learning_rate": 2.2746513215644202e-05,
      "loss": 0.8451,
      "step": 2580
    },
    {
      "epoch": 0.7506702412868632,
      "grad_norm": 1.5626248121261597,
      "learning_rate": 2.2717253486784358e-05,
      "loss": 0.841,
      "step": 2590
    },
    {
      "epoch": 0.7535685819868125,
      "grad_norm": 1.2556672096252441,
      "learning_rate": 2.268799375792451e-05,
      "loss": 0.81,
      "step": 2600
    },
    {
      "epoch": 0.7564669226867619,
      "grad_norm": 1.4101181030273438,
      "learning_rate": 2.2658734029064665e-05,
      "loss": 0.7945,
      "step": 2610
    },
    {
      "epoch": 0.7593652633867111,
      "grad_norm": 1.2747739553451538,
      "learning_rate": 2.2629474300204817e-05,
      "loss": 0.8023,
      "step": 2620
    },
    {
      "epoch": 0.7622636040866604,
      "grad_norm": 1.602884292602539,
      "learning_rate": 2.2600214571344972e-05,
      "loss": 0.879,
      "step": 2630
    },
    {
      "epoch": 0.7651619447866097,
      "grad_norm": 1.8806114196777344,
      "learning_rate": 2.2570954842485124e-05,
      "loss": 0.8182,
      "step": 2640
    },
    {
      "epoch": 0.7680602854865589,
      "grad_norm": 1.3961138725280762,
      "learning_rate": 2.2541695113625283e-05,
      "loss": 0.7778,
      "step": 2650
    },
    {
      "epoch": 0.7709586261865082,
      "grad_norm": 1.293471336364746,
      "learning_rate": 2.2512435384765435e-05,
      "loss": 0.8312,
      "step": 2660
    },
    {
      "epoch": 0.7738569668864576,
      "grad_norm": 1.4049639701843262,
      "learning_rate": 2.248317565590559e-05,
      "loss": 0.8788,
      "step": 2670
    },
    {
      "epoch": 0.7767553075864068,
      "grad_norm": 1.444871187210083,
      "learning_rate": 2.2453915927045742e-05,
      "loss": 0.8504,
      "step": 2680
    },
    {
      "epoch": 0.7796536482863561,
      "grad_norm": 1.3325746059417725,
      "learning_rate": 2.2424656198185898e-05,
      "loss": 0.8476,
      "step": 2690
    },
    {
      "epoch": 0.7825519889863053,
      "grad_norm": 1.626424789428711,
      "learning_rate": 2.2395396469326053e-05,
      "loss": 0.8228,
      "step": 2700
    },
    {
      "epoch": 0.7854503296862546,
      "grad_norm": 1.6076058149337769,
      "learning_rate": 2.2366136740466205e-05,
      "loss": 0.8725,
      "step": 2710
    },
    {
      "epoch": 0.7883486703862039,
      "grad_norm": 1.5909113883972168,
      "learning_rate": 2.233687701160636e-05,
      "loss": 0.805,
      "step": 2720
    },
    {
      "epoch": 0.7912470110861531,
      "grad_norm": 1.6337509155273438,
      "learning_rate": 2.2307617282746512e-05,
      "loss": 0.8052,
      "step": 2730
    },
    {
      "epoch": 0.7941453517861025,
      "grad_norm": 2.01606822013855,
      "learning_rate": 2.2278357553886668e-05,
      "loss": 0.8523,
      "step": 2740
    },
    {
      "epoch": 0.7970436924860518,
      "grad_norm": 1.2886604070663452,
      "learning_rate": 2.224909782502682e-05,
      "loss": 0.8962,
      "step": 2750
    },
    {
      "epoch": 0.799942033186001,
      "grad_norm": 1.717017412185669,
      "learning_rate": 2.2219838096166975e-05,
      "loss": 0.8335,
      "step": 2760
    },
    {
      "epoch": 0.8028403738859503,
      "grad_norm": 1.6534494161605835,
      "learning_rate": 2.219057836730713e-05,
      "loss": 0.8099,
      "step": 2770
    },
    {
      "epoch": 0.8057387145858995,
      "grad_norm": 1.433722972869873,
      "learning_rate": 2.2161318638447286e-05,
      "loss": 0.8584,
      "step": 2780
    },
    {
      "epoch": 0.8086370552858488,
      "grad_norm": 1.6200268268585205,
      "learning_rate": 2.2132058909587438e-05,
      "loss": 0.8798,
      "step": 2790
    },
    {
      "epoch": 0.8115353959857982,
      "grad_norm": 1.1788195371627808,
      "learning_rate": 2.2102799180727593e-05,
      "loss": 0.8088,
      "step": 2800
    },
    {
      "epoch": 0.8144337366857474,
      "grad_norm": 1.4410810470581055,
      "learning_rate": 2.207353945186775e-05,
      "loss": 0.8561,
      "step": 2810
    },
    {
      "epoch": 0.8173320773856967,
      "grad_norm": 1.5079829692840576,
      "learning_rate": 2.20442797230079e-05,
      "loss": 0.8038,
      "step": 2820
    },
    {
      "epoch": 0.820230418085646,
      "grad_norm": 1.4872692823410034,
      "learning_rate": 2.2015019994148056e-05,
      "loss": 0.8448,
      "step": 2830
    },
    {
      "epoch": 0.8231287587855952,
      "grad_norm": 1.499887228012085,
      "learning_rate": 2.1985760265288208e-05,
      "loss": 0.8431,
      "step": 2840
    },
    {
      "epoch": 0.8260270994855445,
      "grad_norm": 1.5485994815826416,
      "learning_rate": 2.1956500536428363e-05,
      "loss": 0.8594,
      "step": 2850
    },
    {
      "epoch": 0.8289254401854939,
      "grad_norm": 1.76557457447052,
      "learning_rate": 2.1927240807568515e-05,
      "loss": 0.8532,
      "step": 2860
    },
    {
      "epoch": 0.8318237808854431,
      "grad_norm": 1.4592503309249878,
      "learning_rate": 2.189798107870867e-05,
      "loss": 0.7848,
      "step": 2870
    },
    {
      "epoch": 0.8347221215853924,
      "grad_norm": 1.337551236152649,
      "learning_rate": 2.1868721349848823e-05,
      "loss": 0.8607,
      "step": 2880
    },
    {
      "epoch": 0.8376204622853416,
      "grad_norm": 1.5464731454849243,
      "learning_rate": 2.183946162098898e-05,
      "loss": 0.8139,
      "step": 2890
    },
    {
      "epoch": 0.8405188029852909,
      "grad_norm": 1.631286382675171,
      "learning_rate": 2.1810201892129133e-05,
      "loss": 0.8583,
      "step": 2900
    },
    {
      "epoch": 0.8434171436852402,
      "grad_norm": 1.354520559310913,
      "learning_rate": 2.178094216326929e-05,
      "loss": 0.8776,
      "step": 2910
    },
    {
      "epoch": 0.8463154843851894,
      "grad_norm": 1.5762245655059814,
      "learning_rate": 2.1751682434409444e-05,
      "loss": 0.869,
      "step": 2920
    },
    {
      "epoch": 0.8492138250851388,
      "grad_norm": 1.451209306716919,
      "learning_rate": 2.1722422705549596e-05,
      "loss": 0.8196,
      "step": 2930
    },
    {
      "epoch": 0.8521121657850881,
      "grad_norm": 1.623870849609375,
      "learning_rate": 2.169316297668975e-05,
      "loss": 0.8979,
      "step": 2940
    },
    {
      "epoch": 0.8550105064850373,
      "grad_norm": 1.366109848022461,
      "learning_rate": 2.1663903247829903e-05,
      "loss": 0.8666,
      "step": 2950
    },
    {
      "epoch": 0.8579088471849866,
      "grad_norm": 1.4370596408843994,
      "learning_rate": 2.163464351897006e-05,
      "loss": 0.7386,
      "step": 2960
    },
    {
      "epoch": 0.8608071878849358,
      "grad_norm": 1.4427939653396606,
      "learning_rate": 2.160538379011021e-05,
      "loss": 0.8228,
      "step": 2970
    },
    {
      "epoch": 0.8637055285848851,
      "grad_norm": 1.4134714603424072,
      "learning_rate": 2.1576124061250366e-05,
      "loss": 0.7697,
      "step": 2980
    },
    {
      "epoch": 0.8666038692848345,
      "grad_norm": 1.4552778005599976,
      "learning_rate": 2.1546864332390518e-05,
      "loss": 0.8052,
      "step": 2990
    },
    {
      "epoch": 0.8695022099847837,
      "grad_norm": 1.2726489305496216,
      "learning_rate": 2.1517604603530677e-05,
      "loss": 0.8461,
      "step": 3000
    },
    {
      "epoch": 0.872400550684733,
      "grad_norm": 1.5088001489639282,
      "learning_rate": 2.148834487467083e-05,
      "loss": 0.8317,
      "step": 3010
    },
    {
      "epoch": 0.8752988913846823,
      "grad_norm": 1.4710649251937866,
      "learning_rate": 2.1459085145810984e-05,
      "loss": 0.7866,
      "step": 3020
    },
    {
      "epoch": 0.8781972320846315,
      "grad_norm": 1.5878509283065796,
      "learning_rate": 2.142982541695114e-05,
      "loss": 0.8602,
      "step": 3030
    },
    {
      "epoch": 0.8810955727845808,
      "grad_norm": 1.285683512687683,
      "learning_rate": 2.140056568809129e-05,
      "loss": 0.8166,
      "step": 3040
    },
    {
      "epoch": 0.8839939134845302,
      "grad_norm": 1.3711918592453003,
      "learning_rate": 2.1371305959231447e-05,
      "loss": 0.8452,
      "step": 3050
    },
    {
      "epoch": 0.8868922541844794,
      "grad_norm": 1.432786464691162,
      "learning_rate": 2.13420462303716e-05,
      "loss": 0.8383,
      "step": 3060
    },
    {
      "epoch": 0.8897905948844287,
      "grad_norm": 1.422122836112976,
      "learning_rate": 2.1312786501511754e-05,
      "loss": 0.8333,
      "step": 3070
    },
    {
      "epoch": 0.8926889355843779,
      "grad_norm": 1.1578731536865234,
      "learning_rate": 2.1283526772651906e-05,
      "loss": 0.7967,
      "step": 3080
    },
    {
      "epoch": 0.8955872762843272,
      "grad_norm": 1.5307798385620117,
      "learning_rate": 2.125426704379206e-05,
      "loss": 0.8269,
      "step": 3090
    },
    {
      "epoch": 0.8984856169842765,
      "grad_norm": 1.8614212274551392,
      "learning_rate": 2.1225007314932213e-05,
      "loss": 0.8672,
      "step": 3100
    },
    {
      "epoch": 0.9013839576842257,
      "grad_norm": 1.4887582063674927,
      "learning_rate": 2.119574758607237e-05,
      "loss": 0.8274,
      "step": 3110
    },
    {
      "epoch": 0.9042822983841751,
      "grad_norm": 1.5970746278762817,
      "learning_rate": 2.1166487857212524e-05,
      "loss": 0.8282,
      "step": 3120
    },
    {
      "epoch": 0.9071806390841244,
      "grad_norm": 1.655535340309143,
      "learning_rate": 2.113722812835268e-05,
      "loss": 0.8029,
      "step": 3130
    },
    {
      "epoch": 0.9100789797840736,
      "grad_norm": 1.188284158706665,
      "learning_rate": 2.110796839949283e-05,
      "loss": 0.7877,
      "step": 3140
    },
    {
      "epoch": 0.9129773204840229,
      "grad_norm": 1.4670881032943726,
      "learning_rate": 2.1078708670632987e-05,
      "loss": 0.8159,
      "step": 3150
    },
    {
      "epoch": 0.9158756611839721,
      "grad_norm": 1.5054377317428589,
      "learning_rate": 2.1049448941773142e-05,
      "loss": 0.8223,
      "step": 3160
    },
    {
      "epoch": 0.9187740018839214,
      "grad_norm": 1.2504961490631104,
      "learning_rate": 2.1020189212913294e-05,
      "loss": 0.8577,
      "step": 3170
    },
    {
      "epoch": 0.9216723425838708,
      "grad_norm": 1.4198757410049438,
      "learning_rate": 2.099092948405345e-05,
      "loss": 0.832,
      "step": 3180
    },
    {
      "epoch": 0.92457068328382,
      "grad_norm": 1.2543909549713135,
      "learning_rate": 2.09616697551936e-05,
      "loss": 0.8164,
      "step": 3190
    },
    {
      "epoch": 0.9274690239837693,
      "grad_norm": 1.502808690071106,
      "learning_rate": 2.0932410026333757e-05,
      "loss": 0.7925,
      "step": 3200
    },
    {
      "epoch": 0.9303673646837186,
      "grad_norm": 1.2221564054489136,
      "learning_rate": 2.090315029747391e-05,
      "loss": 0.7982,
      "step": 3210
    },
    {
      "epoch": 0.9332657053836678,
      "grad_norm": 1.2653131484985352,
      "learning_rate": 2.0873890568614064e-05,
      "loss": 0.8049,
      "step": 3220
    },
    {
      "epoch": 0.9361640460836171,
      "grad_norm": 1.3864349126815796,
      "learning_rate": 2.0844630839754216e-05,
      "loss": 0.8244,
      "step": 3230
    },
    {
      "epoch": 0.9390623867835665,
      "grad_norm": 1.2949591875076294,
      "learning_rate": 2.0815371110894375e-05,
      "loss": 0.8545,
      "step": 3240
    },
    {
      "epoch": 0.9419607274835157,
      "grad_norm": 1.5501495599746704,
      "learning_rate": 2.0786111382034527e-05,
      "loss": 0.8197,
      "step": 3250
    },
    {
      "epoch": 0.944859068183465,
      "grad_norm": 1.3750133514404297,
      "learning_rate": 2.0756851653174682e-05,
      "loss": 0.8144,
      "step": 3260
    },
    {
      "epoch": 0.9477574088834142,
      "grad_norm": 1.5038188695907593,
      "learning_rate": 2.0727591924314837e-05,
      "loss": 0.8951,
      "step": 3270
    },
    {
      "epoch": 0.9506557495833635,
      "grad_norm": 1.7765617370605469,
      "learning_rate": 2.069833219545499e-05,
      "loss": 0.8633,
      "step": 3280
    },
    {
      "epoch": 0.9535540902833128,
      "grad_norm": 1.4099206924438477,
      "learning_rate": 2.0669072466595145e-05,
      "loss": 0.8019,
      "step": 3290
    },
    {
      "epoch": 0.956452430983262,
      "grad_norm": 1.8188388347625732,
      "learning_rate": 2.0639812737735297e-05,
      "loss": 0.8603,
      "step": 3300
    },
    {
      "epoch": 0.9593507716832114,
      "grad_norm": 1.4164273738861084,
      "learning_rate": 2.0610553008875452e-05,
      "loss": 0.8221,
      "step": 3310
    },
    {
      "epoch": 0.9622491123831607,
      "grad_norm": 1.5986411571502686,
      "learning_rate": 2.0581293280015604e-05,
      "loss": 0.7999,
      "step": 3320
    },
    {
      "epoch": 0.9651474530831099,
      "grad_norm": 1.8896023035049438,
      "learning_rate": 2.055203355115576e-05,
      "loss": 0.7988,
      "step": 3330
    },
    {
      "epoch": 0.9680457937830592,
      "grad_norm": 1.7464263439178467,
      "learning_rate": 2.052277382229591e-05,
      "loss": 0.7748,
      "step": 3340
    },
    {
      "epoch": 0.9709441344830084,
      "grad_norm": 1.4457241296768188,
      "learning_rate": 2.0493514093436067e-05,
      "loss": 0.7888,
      "step": 3350
    },
    {
      "epoch": 0.9738424751829577,
      "grad_norm": 1.5523854494094849,
      "learning_rate": 2.0464254364576222e-05,
      "loss": 0.8188,
      "step": 3360
    },
    {
      "epoch": 0.9767408158829071,
      "grad_norm": 1.4314804077148438,
      "learning_rate": 2.0434994635716377e-05,
      "loss": 0.8568,
      "step": 3370
    },
    {
      "epoch": 0.9796391565828563,
      "grad_norm": 1.6589417457580566,
      "learning_rate": 2.0405734906856533e-05,
      "loss": 0.8171,
      "step": 3380
    },
    {
      "epoch": 0.9825374972828056,
      "grad_norm": 1.5929819345474243,
      "learning_rate": 2.0376475177996685e-05,
      "loss": 0.8216,
      "step": 3390
    },
    {
      "epoch": 0.9854358379827549,
      "grad_norm": 1.55243718624115,
      "learning_rate": 2.034721544913684e-05,
      "loss": 0.8628,
      "step": 3400
    },
    {
      "epoch": 0.9883341786827041,
      "grad_norm": 1.4322335720062256,
      "learning_rate": 2.0317955720276992e-05,
      "loss": 0.8466,
      "step": 3410
    },
    {
      "epoch": 0.9912325193826534,
      "grad_norm": 1.509302020072937,
      "learning_rate": 2.0288695991417147e-05,
      "loss": 0.828,
      "step": 3420
    },
    {
      "epoch": 0.9941308600826028,
      "grad_norm": 1.4545068740844727,
      "learning_rate": 2.02594362625573e-05,
      "loss": 0.8137,
      "step": 3430
    },
    {
      "epoch": 0.997029200782552,
      "grad_norm": 1.5562840700149536,
      "learning_rate": 2.0230176533697455e-05,
      "loss": 0.7978,
      "step": 3440
    },
    {
      "epoch": 0.9999275414825013,
      "grad_norm": 1.6293245553970337,
      "learning_rate": 2.0200916804837607e-05,
      "loss": 0.8679,
      "step": 3450
    },
    {
      "epoch": 1.0026085066299544,
      "grad_norm": 1.4699733257293701,
      "learning_rate": 2.0171657075977762e-05,
      "loss": 0.8355,
      "step": 3460
    },
    {
      "epoch": 1.0055068473299036,
      "grad_norm": 1.2859042882919312,
      "learning_rate": 2.0142397347117914e-05,
      "loss": 0.8394,
      "step": 3470
    },
    {
      "epoch": 1.008405188029853,
      "grad_norm": 1.4995588064193726,
      "learning_rate": 2.0113137618258073e-05,
      "loss": 0.7956,
      "step": 3480
    },
    {
      "epoch": 1.0113035287298022,
      "grad_norm": 1.5414553880691528,
      "learning_rate": 2.0083877889398228e-05,
      "loss": 0.8447,
      "step": 3490
    },
    {
      "epoch": 1.0142018694297514,
      "grad_norm": 1.674368977546692,
      "learning_rate": 2.005461816053838e-05,
      "loss": 0.8626,
      "step": 3500
    },
    {
      "epoch": 1.0171002101297006,
      "grad_norm": 1.2135223150253296,
      "learning_rate": 2.0025358431678535e-05,
      "loss": 0.8122,
      "step": 3510
    },
    {
      "epoch": 1.01999855082965,
      "grad_norm": 2.037078380584717,
      "learning_rate": 1.9996098702818687e-05,
      "loss": 0.821,
      "step": 3520
    },
    {
      "epoch": 1.0228968915295993,
      "grad_norm": 1.617884874343872,
      "learning_rate": 1.9966838973958843e-05,
      "loss": 0.7957,
      "step": 3530
    },
    {
      "epoch": 1.0257952322295485,
      "grad_norm": 1.420351266860962,
      "learning_rate": 1.9937579245098995e-05,
      "loss": 0.8188,
      "step": 3540
    },
    {
      "epoch": 1.028693572929498,
      "grad_norm": 1.7060160636901855,
      "learning_rate": 1.990831951623915e-05,
      "loss": 0.8389,
      "step": 3550
    },
    {
      "epoch": 1.0315919136294471,
      "grad_norm": 1.9332331418991089,
      "learning_rate": 1.9879059787379302e-05,
      "loss": 0.845,
      "step": 3560
    },
    {
      "epoch": 1.0344902543293963,
      "grad_norm": 1.3728270530700684,
      "learning_rate": 1.9849800058519457e-05,
      "loss": 0.8328,
      "step": 3570
    },
    {
      "epoch": 1.0373885950293458,
      "grad_norm": 1.264907956123352,
      "learning_rate": 1.982054032965961e-05,
      "loss": 0.8416,
      "step": 3580
    },
    {
      "epoch": 1.040286935729295,
      "grad_norm": 1.4626821279525757,
      "learning_rate": 1.9791280600799765e-05,
      "loss": 0.7865,
      "step": 3590
    },
    {
      "epoch": 1.0431852764292442,
      "grad_norm": 1.8757399320602417,
      "learning_rate": 1.9762020871939924e-05,
      "loss": 0.8105,
      "step": 3600
    },
    {
      "epoch": 1.0460836171291936,
      "grad_norm": 2.362244129180908,
      "learning_rate": 1.9732761143080075e-05,
      "loss": 0.8322,
      "step": 3610
    },
    {
      "epoch": 1.0489819578291428,
      "grad_norm": 1.524034857749939,
      "learning_rate": 1.970350141422023e-05,
      "loss": 0.7913,
      "step": 3620
    },
    {
      "epoch": 1.051880298529092,
      "grad_norm": 1.2164968252182007,
      "learning_rate": 1.9674241685360383e-05,
      "loss": 0.7825,
      "step": 3630
    },
    {
      "epoch": 1.0547786392290415,
      "grad_norm": 1.3799991607666016,
      "learning_rate": 1.9644981956500538e-05,
      "loss": 0.845,
      "step": 3640
    },
    {
      "epoch": 1.0576769799289907,
      "grad_norm": 1.4133681058883667,
      "learning_rate": 1.961572222764069e-05,
      "loss": 0.799,
      "step": 3650
    },
    {
      "epoch": 1.0605753206289399,
      "grad_norm": 1.403192162513733,
      "learning_rate": 1.9586462498780845e-05,
      "loss": 0.7892,
      "step": 3660
    },
    {
      "epoch": 1.063473661328889,
      "grad_norm": 1.9759477376937866,
      "learning_rate": 1.9557202769920997e-05,
      "loss": 0.8065,
      "step": 3670
    },
    {
      "epoch": 1.0663720020288385,
      "grad_norm": 1.5158523321151733,
      "learning_rate": 1.9527943041061153e-05,
      "loss": 0.8377,
      "step": 3680
    },
    {
      "epoch": 1.0692703427287877,
      "grad_norm": 1.313046932220459,
      "learning_rate": 1.9498683312201305e-05,
      "loss": 0.7383,
      "step": 3690
    },
    {
      "epoch": 1.072168683428737,
      "grad_norm": 1.6011909246444702,
      "learning_rate": 1.946942358334146e-05,
      "loss": 0.8246,
      "step": 3700
    },
    {
      "epoch": 1.0750670241286864,
      "grad_norm": 2.321014404296875,
      "learning_rate": 1.9440163854481616e-05,
      "loss": 0.8325,
      "step": 3710
    },
    {
      "epoch": 1.0779653648286356,
      "grad_norm": 1.22357976436615,
      "learning_rate": 1.941090412562177e-05,
      "loss": 0.8187,
      "step": 3720
    },
    {
      "epoch": 1.0808637055285848,
      "grad_norm": 1.3937125205993652,
      "learning_rate": 1.9381644396761926e-05,
      "loss": 0.8159,
      "step": 3730
    },
    {
      "epoch": 1.0837620462285342,
      "grad_norm": 1.5602796077728271,
      "learning_rate": 1.9352384667902078e-05,
      "loss": 0.827,
      "step": 3740
    },
    {
      "epoch": 1.0866603869284834,
      "grad_norm": 1.6623365879058838,
      "learning_rate": 1.9323124939042234e-05,
      "loss": 0.7943,
      "step": 3750
    },
    {
      "epoch": 1.0895587276284326,
      "grad_norm": 1.4485135078430176,
      "learning_rate": 1.9293865210182386e-05,
      "loss": 0.8223,
      "step": 3760
    },
    {
      "epoch": 1.092457068328382,
      "grad_norm": 1.5103403329849243,
      "learning_rate": 1.926460548132254e-05,
      "loss": 0.8493,
      "step": 3770
    },
    {
      "epoch": 1.0953554090283313,
      "grad_norm": 1.560512900352478,
      "learning_rate": 1.9235345752462693e-05,
      "loss": 0.7867,
      "step": 3780
    },
    {
      "epoch": 1.0982537497282805,
      "grad_norm": 2.1003096103668213,
      "learning_rate": 1.9206086023602848e-05,
      "loss": 0.8196,
      "step": 3790
    },
    {
      "epoch": 1.10115209042823,
      "grad_norm": 1.5993285179138184,
      "learning_rate": 1.9176826294743e-05,
      "loss": 0.8443,
      "step": 3800
    },
    {
      "epoch": 1.1040504311281791,
      "grad_norm": 1.5985180139541626,
      "learning_rate": 1.9147566565883156e-05,
      "loss": 0.8267,
      "step": 3810
    },
    {
      "epoch": 1.1069487718281283,
      "grad_norm": 1.242127776145935,
      "learning_rate": 1.911830683702331e-05,
      "loss": 0.8298,
      "step": 3820
    },
    {
      "epoch": 1.1098471125280778,
      "grad_norm": 1.4258760213851929,
      "learning_rate": 1.9089047108163463e-05,
      "loss": 0.7622,
      "step": 3830
    },
    {
      "epoch": 1.112745453228027,
      "grad_norm": 1.6890672445297241,
      "learning_rate": 1.905978737930362e-05,
      "loss": 0.8187,
      "step": 3840
    },
    {
      "epoch": 1.1156437939279762,
      "grad_norm": 1.550704836845398,
      "learning_rate": 1.9030527650443774e-05,
      "loss": 0.8424,
      "step": 3850
    },
    {
      "epoch": 1.1185421346279254,
      "grad_norm": 1.4318784475326538,
      "learning_rate": 1.900126792158393e-05,
      "loss": 0.801,
      "step": 3860
    },
    {
      "epoch": 1.1214404753278748,
      "grad_norm": 1.6849331855773926,
      "learning_rate": 1.897200819272408e-05,
      "loss": 0.8154,
      "step": 3870
    },
    {
      "epoch": 1.124338816027824,
      "grad_norm": 1.6653780937194824,
      "learning_rate": 1.8942748463864236e-05,
      "loss": 0.838,
      "step": 3880
    },
    {
      "epoch": 1.1272371567277735,
      "grad_norm": 1.6507112979888916,
      "learning_rate": 1.8913488735004388e-05,
      "loss": 0.8207,
      "step": 3890
    },
    {
      "epoch": 1.1301354974277227,
      "grad_norm": 1.2721474170684814,
      "learning_rate": 1.8884229006144544e-05,
      "loss": 0.7975,
      "step": 3900
    },
    {
      "epoch": 1.1330338381276719,
      "grad_norm": 1.7879514694213867,
      "learning_rate": 1.8854969277284696e-05,
      "loss": 0.7645,
      "step": 3910
    },
    {
      "epoch": 1.135932178827621,
      "grad_norm": 1.5875015258789062,
      "learning_rate": 1.882570954842485e-05,
      "loss": 0.7634,
      "step": 3920
    },
    {
      "epoch": 1.1388305195275705,
      "grad_norm": 1.6133368015289307,
      "learning_rate": 1.8796449819565006e-05,
      "loss": 0.8345,
      "step": 3930
    },
    {
      "epoch": 1.1417288602275197,
      "grad_norm": 1.5612218379974365,
      "learning_rate": 1.8767190090705158e-05,
      "loss": 0.7692,
      "step": 3940
    },
    {
      "epoch": 1.144627200927469,
      "grad_norm": 1.8605525493621826,
      "learning_rate": 1.8737930361845314e-05,
      "loss": 0.8428,
      "step": 3950
    },
    {
      "epoch": 1.1475255416274184,
      "grad_norm": 1.4341702461242676,
      "learning_rate": 1.870867063298547e-05,
      "loss": 0.8255,
      "step": 3960
    },
    {
      "epoch": 1.1504238823273676,
      "grad_norm": 1.8902758359909058,
      "learning_rate": 1.8679410904125624e-05,
      "loss": 0.8145,
      "step": 3970
    },
    {
      "epoch": 1.1533222230273168,
      "grad_norm": 1.5721267461776733,
      "learning_rate": 1.8650151175265776e-05,
      "loss": 0.8221,
      "step": 3980
    },
    {
      "epoch": 1.1562205637272662,
      "grad_norm": 1.551946759223938,
      "learning_rate": 1.862089144640593e-05,
      "loss": 0.786,
      "step": 3990
    },
    {
      "epoch": 1.1591189044272154,
      "grad_norm": 1.3490889072418213,
      "learning_rate": 1.8591631717546084e-05,
      "loss": 0.8377,
      "step": 4000
    },
    {
      "epoch": 1.1620172451271646,
      "grad_norm": 1.395007848739624,
      "learning_rate": 1.856237198868624e-05,
      "loss": 0.7959,
      "step": 4010
    },
    {
      "epoch": 1.164915585827114,
      "grad_norm": 1.6271227598190308,
      "learning_rate": 1.853311225982639e-05,
      "loss": 0.8112,
      "step": 4020
    },
    {
      "epoch": 1.1678139265270633,
      "grad_norm": 1.3277052640914917,
      "learning_rate": 1.8503852530966546e-05,
      "loss": 0.8001,
      "step": 4030
    },
    {
      "epoch": 1.1707122672270125,
      "grad_norm": 1.6950856447219849,
      "learning_rate": 1.84745928021067e-05,
      "loss": 0.787,
      "step": 4040
    },
    {
      "epoch": 1.1736106079269617,
      "grad_norm": 1.6063294410705566,
      "learning_rate": 1.8445333073246854e-05,
      "loss": 0.8224,
      "step": 4050
    },
    {
      "epoch": 1.1765089486269111,
      "grad_norm": 1.5941709280014038,
      "learning_rate": 1.841607334438701e-05,
      "loss": 0.7951,
      "step": 4060
    },
    {
      "epoch": 1.1794072893268603,
      "grad_norm": 1.388599157333374,
      "learning_rate": 1.8386813615527164e-05,
      "loss": 0.8666,
      "step": 4070
    },
    {
      "epoch": 1.1823056300268098,
      "grad_norm": 1.4083751440048218,
      "learning_rate": 1.835755388666732e-05,
      "loss": 0.7922,
      "step": 4080
    },
    {
      "epoch": 1.185203970726759,
      "grad_norm": 1.3721750974655151,
      "learning_rate": 1.832829415780747e-05,
      "loss": 0.7928,
      "step": 4090
    },
    {
      "epoch": 1.1881023114267082,
      "grad_norm": 1.5114967823028564,
      "learning_rate": 1.8299034428947627e-05,
      "loss": 0.82,
      "step": 4100
    },
    {
      "epoch": 1.1910006521266574,
      "grad_norm": 1.651502013206482,
      "learning_rate": 1.826977470008778e-05,
      "loss": 0.7757,
      "step": 4110
    },
    {
      "epoch": 1.1938989928266068,
      "grad_norm": 1.6870572566986084,
      "learning_rate": 1.8240514971227934e-05,
      "loss": 0.8103,
      "step": 4120
    },
    {
      "epoch": 1.196797333526556,
      "grad_norm": 1.7923808097839355,
      "learning_rate": 1.8211255242368086e-05,
      "loss": 0.8457,
      "step": 4130
    },
    {
      "epoch": 1.1996956742265052,
      "grad_norm": 1.5688174962997437,
      "learning_rate": 1.818199551350824e-05,
      "loss": 0.8234,
      "step": 4140
    },
    {
      "epoch": 1.2025940149264547,
      "grad_norm": 1.3954662084579468,
      "learning_rate": 1.8152735784648397e-05,
      "loss": 0.8384,
      "step": 4150
    },
    {
      "epoch": 1.2054923556264039,
      "grad_norm": 1.734068751335144,
      "learning_rate": 1.812347605578855e-05,
      "loss": 0.8437,
      "step": 4160
    },
    {
      "epoch": 1.208390696326353,
      "grad_norm": 1.5512213706970215,
      "learning_rate": 1.8094216326928704e-05,
      "loss": 0.831,
      "step": 4170
    },
    {
      "epoch": 1.2112890370263025,
      "grad_norm": 1.7880759239196777,
      "learning_rate": 1.8064956598068856e-05,
      "loss": 0.8179,
      "step": 4180
    },
    {
      "epoch": 1.2141873777262517,
      "grad_norm": 1.4621922969818115,
      "learning_rate": 1.8035696869209015e-05,
      "loss": 0.8746,
      "step": 4190
    },
    {
      "epoch": 1.217085718426201,
      "grad_norm": 1.5137345790863037,
      "learning_rate": 1.8006437140349167e-05,
      "loss": 0.8154,
      "step": 4200
    },
    {
      "epoch": 1.2199840591261504,
      "grad_norm": 1.5671677589416504,
      "learning_rate": 1.7977177411489322e-05,
      "loss": 0.8257,
      "step": 4210
    },
    {
      "epoch": 1.2228823998260996,
      "grad_norm": 1.5441718101501465,
      "learning_rate": 1.7947917682629474e-05,
      "loss": 0.8138,
      "step": 4220
    },
    {
      "epoch": 1.2257807405260488,
      "grad_norm": 1.6707130670547485,
      "learning_rate": 1.791865795376963e-05,
      "loss": 0.8411,
      "step": 4230
    },
    {
      "epoch": 1.228679081225998,
      "grad_norm": 1.406284213066101,
      "learning_rate": 1.788939822490978e-05,
      "loss": 0.8262,
      "step": 4240
    },
    {
      "epoch": 1.2315774219259474,
      "grad_norm": 1.604556918144226,
      "learning_rate": 1.7860138496049937e-05,
      "loss": 0.8013,
      "step": 4250
    },
    {
      "epoch": 1.2344757626258966,
      "grad_norm": 1.8756991624832153,
      "learning_rate": 1.7830878767190092e-05,
      "loss": 0.8021,
      "step": 4260
    },
    {
      "epoch": 1.237374103325846,
      "grad_norm": 1.7779076099395752,
      "learning_rate": 1.7801619038330244e-05,
      "loss": 0.8133,
      "step": 4270
    },
    {
      "epoch": 1.2402724440257953,
      "grad_norm": 1.6854655742645264,
      "learning_rate": 1.77723593094704e-05,
      "loss": 0.8403,
      "step": 4280
    },
    {
      "epoch": 1.2431707847257445,
      "grad_norm": 1.9591865539550781,
      "learning_rate": 1.774309958061055e-05,
      "loss": 0.7943,
      "step": 4290
    },
    {
      "epoch": 1.2460691254256937,
      "grad_norm": 1.515794038772583,
      "learning_rate": 1.7713839851750707e-05,
      "loss": 0.821,
      "step": 4300
    },
    {
      "epoch": 1.2489674661256431,
      "grad_norm": 1.5868602991104126,
      "learning_rate": 1.7684580122890862e-05,
      "loss": 0.7846,
      "step": 4310
    },
    {
      "epoch": 1.2518658068255923,
      "grad_norm": 1.388081669807434,
      "learning_rate": 1.7655320394031018e-05,
      "loss": 0.8029,
      "step": 4320
    },
    {
      "epoch": 1.2547641475255416,
      "grad_norm": 1.4929925203323364,
      "learning_rate": 1.762606066517117e-05,
      "loss": 0.7983,
      "step": 4330
    },
    {
      "epoch": 1.257662488225491,
      "grad_norm": 1.7426847219467163,
      "learning_rate": 1.7596800936311325e-05,
      "loss": 0.8063,
      "step": 4340
    },
    {
      "epoch": 1.2605608289254402,
      "grad_norm": 1.6136187314987183,
      "learning_rate": 1.7567541207451477e-05,
      "loss": 0.8066,
      "step": 4350
    },
    {
      "epoch": 1.2634591696253894,
      "grad_norm": 1.7628281116485596,
      "learning_rate": 1.7538281478591632e-05,
      "loss": 0.77,
      "step": 4360
    },
    {
      "epoch": 1.2663575103253386,
      "grad_norm": 1.6854642629623413,
      "learning_rate": 1.7509021749731788e-05,
      "loss": 0.8279,
      "step": 4370
    },
    {
      "epoch": 1.269255851025288,
      "grad_norm": 1.3485571146011353,
      "learning_rate": 1.747976202087194e-05,
      "loss": 0.8391,
      "step": 4380
    },
    {
      "epoch": 1.2721541917252372,
      "grad_norm": 1.5637578964233398,
      "learning_rate": 1.7450502292012095e-05,
      "loss": 0.8064,
      "step": 4390
    },
    {
      "epoch": 1.2750525324251867,
      "grad_norm": 1.5232601165771484,
      "learning_rate": 1.7421242563152247e-05,
      "loss": 0.7912,
      "step": 4400
    },
    {
      "epoch": 1.2779508731251359,
      "grad_norm": 1.7983317375183105,
      "learning_rate": 1.7391982834292402e-05,
      "loss": 0.7684,
      "step": 4410
    },
    {
      "epoch": 1.280849213825085,
      "grad_norm": 1.8111987113952637,
      "learning_rate": 1.7362723105432554e-05,
      "loss": 0.819,
      "step": 4420
    },
    {
      "epoch": 1.2837475545250343,
      "grad_norm": 2.017749071121216,
      "learning_rate": 1.7333463376572713e-05,
      "loss": 0.8574,
      "step": 4430
    },
    {
      "epoch": 1.2866458952249837,
      "grad_norm": 1.382508397102356,
      "learning_rate": 1.7304203647712865e-05,
      "loss": 0.7895,
      "step": 4440
    },
    {
      "epoch": 1.289544235924933,
      "grad_norm": 1.5064895153045654,
      "learning_rate": 1.727494391885302e-05,
      "loss": 0.792,
      "step": 4450
    },
    {
      "epoch": 1.2924425766248824,
      "grad_norm": 1.7977186441421509,
      "learning_rate": 1.7245684189993172e-05,
      "loss": 0.7992,
      "step": 4460
    },
    {
      "epoch": 1.2953409173248316,
      "grad_norm": 1.4003864526748657,
      "learning_rate": 1.7216424461133328e-05,
      "loss": 0.8059,
      "step": 4470
    },
    {
      "epoch": 1.2982392580247808,
      "grad_norm": 1.9271281957626343,
      "learning_rate": 1.7187164732273483e-05,
      "loss": 0.7693,
      "step": 4480
    },
    {
      "epoch": 1.30113759872473,
      "grad_norm": 1.7477785348892212,
      "learning_rate": 1.7157905003413635e-05,
      "loss": 0.83,
      "step": 4490
    },
    {
      "epoch": 1.3040359394246794,
      "grad_norm": 1.540985107421875,
      "learning_rate": 1.712864527455379e-05,
      "loss": 0.8463,
      "step": 4500
    },
    {
      "epoch": 1.3069342801246286,
      "grad_norm": 1.7510998249053955,
      "learning_rate": 1.7099385545693942e-05,
      "loss": 0.8272,
      "step": 4510
    },
    {
      "epoch": 1.3098326208245779,
      "grad_norm": 1.4256850481033325,
      "learning_rate": 1.7070125816834098e-05,
      "loss": 0.8342,
      "step": 4520
    },
    {
      "epoch": 1.3127309615245273,
      "grad_norm": 1.3458172082901,
      "learning_rate": 1.704086608797425e-05,
      "loss": 0.7957,
      "step": 4530
    },
    {
      "epoch": 1.3156293022244765,
      "grad_norm": 1.6698424816131592,
      "learning_rate": 1.7011606359114405e-05,
      "loss": 0.8425,
      "step": 4540
    },
    {
      "epoch": 1.3185276429244257,
      "grad_norm": 1.5734453201293945,
      "learning_rate": 1.698234663025456e-05,
      "loss": 0.7652,
      "step": 4550
    },
    {
      "epoch": 1.321425983624375,
      "grad_norm": 1.970935344696045,
      "learning_rate": 1.6953086901394716e-05,
      "loss": 0.7874,
      "step": 4560
    },
    {
      "epoch": 1.3243243243243243,
      "grad_norm": 1.372573971748352,
      "learning_rate": 1.6923827172534868e-05,
      "loss": 0.8605,
      "step": 4570
    },
    {
      "epoch": 1.3272226650242736,
      "grad_norm": 2.0005578994750977,
      "learning_rate": 1.6894567443675023e-05,
      "loss": 0.8745,
      "step": 4580
    },
    {
      "epoch": 1.330121005724223,
      "grad_norm": 1.373012661933899,
      "learning_rate": 1.686530771481518e-05,
      "loss": 0.7827,
      "step": 4590
    },
    {
      "epoch": 1.3330193464241722,
      "grad_norm": 1.593336582183838,
      "learning_rate": 1.683604798595533e-05,
      "loss": 0.8087,
      "step": 4600
    },
    {
      "epoch": 1.3359176871241214,
      "grad_norm": 1.8745741844177246,
      "learning_rate": 1.6806788257095486e-05,
      "loss": 0.8566,
      "step": 4610
    },
    {
      "epoch": 1.3388160278240706,
      "grad_norm": 1.555289626121521,
      "learning_rate": 1.6777528528235638e-05,
      "loss": 0.782,
      "step": 4620
    },
    {
      "epoch": 1.34171436852402,
      "grad_norm": 2.0381827354431152,
      "learning_rate": 1.6748268799375793e-05,
      "loss": 0.8261,
      "step": 4630
    },
    {
      "epoch": 1.3446127092239692,
      "grad_norm": 1.869950294494629,
      "learning_rate": 1.6719009070515945e-05,
      "loss": 0.7895,
      "step": 4640
    },
    {
      "epoch": 1.3475110499239187,
      "grad_norm": 1.4894152879714966,
      "learning_rate": 1.66897493416561e-05,
      "loss": 0.8051,
      "step": 4650
    },
    {
      "epoch": 1.350409390623868,
      "grad_norm": 1.5473014116287231,
      "learning_rate": 1.6660489612796252e-05,
      "loss": 0.8133,
      "step": 4660
    },
    {
      "epoch": 1.353307731323817,
      "grad_norm": 1.5909615755081177,
      "learning_rate": 1.663122988393641e-05,
      "loss": 0.8223,
      "step": 4670
    },
    {
      "epoch": 1.3562060720237663,
      "grad_norm": 1.4471195936203003,
      "learning_rate": 1.6601970155076563e-05,
      "loss": 0.8101,
      "step": 4680
    },
    {
      "epoch": 1.3591044127237157,
      "grad_norm": 2.020709276199341,
      "learning_rate": 1.657271042621672e-05,
      "loss": 0.8073,
      "step": 4690
    },
    {
      "epoch": 1.362002753423665,
      "grad_norm": 1.9314271211624146,
      "learning_rate": 1.6543450697356874e-05,
      "loss": 0.8211,
      "step": 4700
    },
    {
      "epoch": 1.3649010941236142,
      "grad_norm": 1.36197829246521,
      "learning_rate": 1.6514190968497026e-05,
      "loss": 0.8407,
      "step": 4710
    },
    {
      "epoch": 1.3677994348235636,
      "grad_norm": 1.5600054264068604,
      "learning_rate": 1.648493123963718e-05,
      "loss": 0.8341,
      "step": 4720
    },
    {
      "epoch": 1.3706977755235128,
      "grad_norm": 1.6521389484405518,
      "learning_rate": 1.6455671510777333e-05,
      "loss": 0.7843,
      "step": 4730
    },
    {
      "epoch": 1.373596116223462,
      "grad_norm": 1.5927916765213013,
      "learning_rate": 1.642641178191749e-05,
      "loss": 0.8354,
      "step": 4740
    },
    {
      "epoch": 1.3764944569234112,
      "grad_norm": 1.6726380586624146,
      "learning_rate": 1.639715205305764e-05,
      "loss": 0.8374,
      "step": 4750
    },
    {
      "epoch": 1.3793927976233606,
      "grad_norm": 1.499463677406311,
      "learning_rate": 1.6367892324197796e-05,
      "loss": 0.779,
      "step": 4760
    },
    {
      "epoch": 1.3822911383233099,
      "grad_norm": 1.3007324934005737,
      "learning_rate": 1.6338632595337948e-05,
      "loss": 0.8146,
      "step": 4770
    },
    {
      "epoch": 1.3851894790232593,
      "grad_norm": 1.4621033668518066,
      "learning_rate": 1.6309372866478103e-05,
      "loss": 0.7542,
      "step": 4780
    },
    {
      "epoch": 1.3880878197232085,
      "grad_norm": 1.500508427619934,
      "learning_rate": 1.628011313761826e-05,
      "loss": 0.8031,
      "step": 4790
    },
    {
      "epoch": 1.3909861604231577,
      "grad_norm": 1.4414575099945068,
      "learning_rate": 1.6250853408758414e-05,
      "loss": 0.848,
      "step": 4800
    },
    {
      "epoch": 1.393884501123107,
      "grad_norm": 1.441428780555725,
      "learning_rate": 1.6221593679898566e-05,
      "loss": 0.8189,
      "step": 4810
    },
    {
      "epoch": 1.3967828418230563,
      "grad_norm": 1.7825231552124023,
      "learning_rate": 1.619233395103872e-05,
      "loss": 0.7962,
      "step": 4820
    },
    {
      "epoch": 1.3996811825230056,
      "grad_norm": 1.5360989570617676,
      "learning_rate": 1.6163074222178877e-05,
      "loss": 0.7897,
      "step": 4830
    },
    {
      "epoch": 1.402579523222955,
      "grad_norm": 1.509477972984314,
      "learning_rate": 1.613381449331903e-05,
      "loss": 0.8161,
      "step": 4840
    },
    {
      "epoch": 1.4054778639229042,
      "grad_norm": 1.5130615234375,
      "learning_rate": 1.6104554764459184e-05,
      "loss": 0.7988,
      "step": 4850
    },
    {
      "epoch": 1.4083762046228534,
      "grad_norm": 1.4647066593170166,
      "learning_rate": 1.6075295035599336e-05,
      "loss": 0.8201,
      "step": 4860
    },
    {
      "epoch": 1.4112745453228026,
      "grad_norm": 1.8241599798202515,
      "learning_rate": 1.604603530673949e-05,
      "loss": 0.8314,
      "step": 4870
    },
    {
      "epoch": 1.414172886022752,
      "grad_norm": 1.4556833505630493,
      "learning_rate": 1.6016775577879643e-05,
      "loss": 0.8074,
      "step": 4880
    },
    {
      "epoch": 1.4170712267227012,
      "grad_norm": 1.6618982553482056,
      "learning_rate": 1.59875158490198e-05,
      "loss": 0.8448,
      "step": 4890
    },
    {
      "epoch": 1.4199695674226505,
      "grad_norm": 1.4426939487457275,
      "learning_rate": 1.595825612015995e-05,
      "loss": 0.7584,
      "step": 4900
    },
    {
      "epoch": 1.4228679081226,
      "grad_norm": 1.7148686647415161,
      "learning_rate": 1.592899639130011e-05,
      "loss": 0.811,
      "step": 4910
    },
    {
      "epoch": 1.425766248822549,
      "grad_norm": 1.593692660331726,
      "learning_rate": 1.589973666244026e-05,
      "loss": 0.8131,
      "step": 4920
    },
    {
      "epoch": 1.4286645895224983,
      "grad_norm": 1.9762685298919678,
      "learning_rate": 1.5870476933580417e-05,
      "loss": 0.8122,
      "step": 4930
    },
    {
      "epoch": 1.4315629302224475,
      "grad_norm": 1.8057396411895752,
      "learning_rate": 1.5841217204720572e-05,
      "loss": 0.818,
      "step": 4940
    },
    {
      "epoch": 1.434461270922397,
      "grad_norm": 1.978319525718689,
      "learning_rate": 1.5811957475860724e-05,
      "loss": 0.8588,
      "step": 4950
    },
    {
      "epoch": 1.4373596116223462,
      "grad_norm": 1.6745561361312866,
      "learning_rate": 1.578269774700088e-05,
      "loss": 0.8477,
      "step": 4960
    },
    {
      "epoch": 1.4402579523222956,
      "grad_norm": 1.811799168586731,
      "learning_rate": 1.575343801814103e-05,
      "loss": 0.7962,
      "step": 4970
    },
    {
      "epoch": 1.4431562930222448,
      "grad_norm": 1.8677761554718018,
      "learning_rate": 1.5724178289281187e-05,
      "loss": 0.8329,
      "step": 4980
    },
    {
      "epoch": 1.446054633722194,
      "grad_norm": 1.3128600120544434,
      "learning_rate": 1.569491856042134e-05,
      "loss": 0.8186,
      "step": 4990
    },
    {
      "epoch": 1.4489529744221432,
      "grad_norm": 1.5922040939331055,
      "learning_rate": 1.5665658831561494e-05,
      "loss": 0.8089,
      "step": 5000
    },
    {
      "epoch": 1.4518513151220926,
      "grad_norm": 1.553749918937683,
      "learning_rate": 1.5636399102701646e-05,
      "loss": 0.8235,
      "step": 5010
    },
    {
      "epoch": 1.4547496558220419,
      "grad_norm": 1.4513181447982788,
      "learning_rate": 1.5607139373841805e-05,
      "loss": 0.8,
      "step": 5020
    },
    {
      "epoch": 1.4576479965219913,
      "grad_norm": 1.4595470428466797,
      "learning_rate": 1.5577879644981957e-05,
      "loss": 0.7836,
      "step": 5030
    },
    {
      "epoch": 1.4605463372219405,
      "grad_norm": 1.5432686805725098,
      "learning_rate": 1.5548619916122112e-05,
      "loss": 0.7758,
      "step": 5040
    },
    {
      "epoch": 1.4634446779218897,
      "grad_norm": 1.767669439315796,
      "learning_rate": 1.5519360187262267e-05,
      "loss": 0.7895,
      "step": 5050
    },
    {
      "epoch": 1.466343018621839,
      "grad_norm": 1.6731973886489868,
      "learning_rate": 1.549010045840242e-05,
      "loss": 0.8241,
      "step": 5060
    },
    {
      "epoch": 1.4692413593217883,
      "grad_norm": 1.5980437994003296,
      "learning_rate": 1.5460840729542575e-05,
      "loss": 0.7589,
      "step": 5070
    },
    {
      "epoch": 1.4721397000217376,
      "grad_norm": 2.1730523109436035,
      "learning_rate": 1.5431581000682727e-05,
      "loss": 0.8363,
      "step": 5080
    },
    {
      "epoch": 1.4750380407216868,
      "grad_norm": 1.4792779684066772,
      "learning_rate": 1.5402321271822882e-05,
      "loss": 0.8064,
      "step": 5090
    },
    {
      "epoch": 1.4779363814216362,
      "grad_norm": 1.575103521347046,
      "learning_rate": 1.5373061542963034e-05,
      "loss": 0.7644,
      "step": 5100
    },
    {
      "epoch": 1.4808347221215854,
      "grad_norm": 1.8850635290145874,
      "learning_rate": 1.534380181410319e-05,
      "loss": 0.832,
      "step": 5110
    },
    {
      "epoch": 1.4837330628215346,
      "grad_norm": 1.337811827659607,
      "learning_rate": 1.531454208524334e-05,
      "loss": 0.8367,
      "step": 5120
    },
    {
      "epoch": 1.4866314035214838,
      "grad_norm": 1.9237931966781616,
      "learning_rate": 1.5285282356383497e-05,
      "loss": 0.8664,
      "step": 5130
    },
    {
      "epoch": 1.4895297442214332,
      "grad_norm": 1.431839942932129,
      "learning_rate": 1.5256022627523654e-05,
      "loss": 0.7981,
      "step": 5140
    },
    {
      "epoch": 1.4924280849213825,
      "grad_norm": 1.5017859935760498,
      "learning_rate": 1.5226762898663807e-05,
      "loss": 0.8046,
      "step": 5150
    },
    {
      "epoch": 1.495326425621332,
      "grad_norm": 2.0204222202301025,
      "learning_rate": 1.5197503169803961e-05,
      "loss": 0.8573,
      "step": 5160
    },
    {
      "epoch": 1.498224766321281,
      "grad_norm": 1.502009630203247,
      "learning_rate": 1.5168243440944115e-05,
      "loss": 0.7928,
      "step": 5170
    },
    {
      "epoch": 1.5011231070212303,
      "grad_norm": 1.4816863536834717,
      "learning_rate": 1.5138983712084268e-05,
      "loss": 0.7866,
      "step": 5180
    },
    {
      "epoch": 1.5040214477211795,
      "grad_norm": 1.5889933109283447,
      "learning_rate": 1.5109723983224422e-05,
      "loss": 0.7595,
      "step": 5190
    },
    {
      "epoch": 1.506919788421129,
      "grad_norm": 1.5034630298614502,
      "learning_rate": 1.5080464254364577e-05,
      "loss": 0.7807,
      "step": 5200
    },
    {
      "epoch": 1.5098181291210782,
      "grad_norm": 1.9230492115020752,
      "learning_rate": 1.5051204525504731e-05,
      "loss": 0.8056,
      "step": 5210
    },
    {
      "epoch": 1.5127164698210276,
      "grad_norm": 1.6153405904769897,
      "learning_rate": 1.5021944796644885e-05,
      "loss": 0.8207,
      "step": 5220
    },
    {
      "epoch": 1.5156148105209768,
      "grad_norm": 1.631255030632019,
      "learning_rate": 1.499268506778504e-05,
      "loss": 0.8165,
      "step": 5230
    },
    {
      "epoch": 1.518513151220926,
      "grad_norm": 1.6789946556091309,
      "learning_rate": 1.4963425338925194e-05,
      "loss": 0.8285,
      "step": 5240
    },
    {
      "epoch": 1.5214114919208752,
      "grad_norm": 1.5509060621261597,
      "learning_rate": 1.4934165610065347e-05,
      "loss": 0.7926,
      "step": 5250
    },
    {
      "epoch": 1.5243098326208244,
      "grad_norm": 1.615769624710083,
      "learning_rate": 1.4904905881205501e-05,
      "loss": 0.8389,
      "step": 5260
    },
    {
      "epoch": 1.5272081733207739,
      "grad_norm": 1.658879041671753,
      "learning_rate": 1.4875646152345655e-05,
      "loss": 0.7843,
      "step": 5270
    },
    {
      "epoch": 1.5301065140207233,
      "grad_norm": 1.572218418121338,
      "learning_rate": 1.4846386423485808e-05,
      "loss": 0.7637,
      "step": 5280
    },
    {
      "epoch": 1.5330048547206725,
      "grad_norm": 1.5958391427993774,
      "learning_rate": 1.4817126694625964e-05,
      "loss": 0.7803,
      "step": 5290
    },
    {
      "epoch": 1.5359031954206217,
      "grad_norm": 1.7990509271621704,
      "learning_rate": 1.4787866965766117e-05,
      "loss": 0.8259,
      "step": 5300
    },
    {
      "epoch": 1.538801536120571,
      "grad_norm": 1.8269257545471191,
      "learning_rate": 1.4758607236906271e-05,
      "loss": 0.8572,
      "step": 5310
    },
    {
      "epoch": 1.5416998768205201,
      "grad_norm": 1.351591944694519,
      "learning_rate": 1.4729347508046426e-05,
      "loss": 0.7638,
      "step": 5320
    },
    {
      "epoch": 1.5445982175204696,
      "grad_norm": 1.4854739904403687,
      "learning_rate": 1.470008777918658e-05,
      "loss": 0.8172,
      "step": 5330
    },
    {
      "epoch": 1.5474965582204188,
      "grad_norm": 1.4648224115371704,
      "learning_rate": 1.4670828050326734e-05,
      "loss": 0.846,
      "step": 5340
    },
    {
      "epoch": 1.5503948989203682,
      "grad_norm": 1.6794888973236084,
      "learning_rate": 1.4641568321466889e-05,
      "loss": 0.8238,
      "step": 5350
    },
    {
      "epoch": 1.5532932396203174,
      "grad_norm": 1.464909315109253,
      "learning_rate": 1.4612308592607043e-05,
      "loss": 0.7385,
      "step": 5360
    },
    {
      "epoch": 1.5561915803202666,
      "grad_norm": 1.702012300491333,
      "learning_rate": 1.4583048863747196e-05,
      "loss": 0.8444,
      "step": 5370
    },
    {
      "epoch": 1.5590899210202158,
      "grad_norm": 1.7118546962738037,
      "learning_rate": 1.455378913488735e-05,
      "loss": 0.8014,
      "step": 5380
    },
    {
      "epoch": 1.5619882617201652,
      "grad_norm": 1.7524687051773071,
      "learning_rate": 1.4524529406027504e-05,
      "loss": 0.8815,
      "step": 5390
    },
    {
      "epoch": 1.5648866024201145,
      "grad_norm": 1.7262446880340576,
      "learning_rate": 1.4495269677167657e-05,
      "loss": 0.7749,
      "step": 5400
    },
    {
      "epoch": 1.567784943120064,
      "grad_norm": 1.4721463918685913,
      "learning_rate": 1.4466009948307813e-05,
      "loss": 0.7841,
      "step": 5410
    },
    {
      "epoch": 1.570683283820013,
      "grad_norm": 1.7325936555862427,
      "learning_rate": 1.4436750219447966e-05,
      "loss": 0.8208,
      "step": 5420
    },
    {
      "epoch": 1.5735816245199623,
      "grad_norm": 1.459064245223999,
      "learning_rate": 1.4407490490588122e-05,
      "loss": 0.829,
      "step": 5430
    },
    {
      "epoch": 1.5764799652199115,
      "grad_norm": 2.37611722946167,
      "learning_rate": 1.4378230761728275e-05,
      "loss": 0.8174,
      "step": 5440
    },
    {
      "epoch": 1.5793783059198607,
      "grad_norm": 1.935472011566162,
      "learning_rate": 1.4348971032868429e-05,
      "loss": 0.8587,
      "step": 5450
    },
    {
      "epoch": 1.5822766466198102,
      "grad_norm": 1.754341721534729,
      "learning_rate": 1.4319711304008583e-05,
      "loss": 0.7904,
      "step": 5460
    },
    {
      "epoch": 1.5851749873197596,
      "grad_norm": 1.7790569067001343,
      "learning_rate": 1.4290451575148738e-05,
      "loss": 0.9037,
      "step": 5470
    },
    {
      "epoch": 1.5880733280197088,
      "grad_norm": 1.8647918701171875,
      "learning_rate": 1.4261191846288892e-05,
      "loss": 0.8335,
      "step": 5480
    },
    {
      "epoch": 1.590971668719658,
      "grad_norm": 1.6344166994094849,
      "learning_rate": 1.4231932117429045e-05,
      "loss": 0.7759,
      "step": 5490
    },
    {
      "epoch": 1.5938700094196072,
      "grad_norm": 1.7699410915374756,
      "learning_rate": 1.4202672388569199e-05,
      "loss": 0.7824,
      "step": 5500
    },
    {
      "epoch": 1.5967683501195564,
      "grad_norm": 1.3481018543243408,
      "learning_rate": 1.4173412659709353e-05,
      "loss": 0.799,
      "step": 5510
    },
    {
      "epoch": 1.5996666908195059,
      "grad_norm": 1.8439619541168213,
      "learning_rate": 1.4144152930849506e-05,
      "loss": 0.8758,
      "step": 5520
    },
    {
      "epoch": 1.602565031519455,
      "grad_norm": 1.8073043823242188,
      "learning_rate": 1.4114893201989662e-05,
      "loss": 0.829,
      "step": 5530
    },
    {
      "epoch": 1.6054633722194045,
      "grad_norm": 1.523716926574707,
      "learning_rate": 1.4085633473129817e-05,
      "loss": 0.8387,
      "step": 5540
    },
    {
      "epoch": 1.6083617129193537,
      "grad_norm": 1.9719517230987549,
      "learning_rate": 1.405637374426997e-05,
      "loss": 0.8535,
      "step": 5550
    },
    {
      "epoch": 1.611260053619303,
      "grad_norm": 2.0711729526519775,
      "learning_rate": 1.4027114015410125e-05,
      "loss": 0.7415,
      "step": 5560
    },
    {
      "epoch": 1.6141583943192521,
      "grad_norm": 1.8905869722366333,
      "learning_rate": 1.3997854286550278e-05,
      "loss": 0.7832,
      "step": 5570
    },
    {
      "epoch": 1.6170567350192016,
      "grad_norm": 1.8284876346588135,
      "learning_rate": 1.3968594557690432e-05,
      "loss": 0.7913,
      "step": 5580
    },
    {
      "epoch": 1.6199550757191508,
      "grad_norm": 1.5364829301834106,
      "learning_rate": 1.3939334828830587e-05,
      "loss": 0.8491,
      "step": 5590
    },
    {
      "epoch": 1.6228534164191002,
      "grad_norm": 2.0331640243530273,
      "learning_rate": 1.3910075099970741e-05,
      "loss": 0.8121,
      "step": 5600
    },
    {
      "epoch": 1.6257517571190494,
      "grad_norm": 1.5406885147094727,
      "learning_rate": 1.3880815371110895e-05,
      "loss": 0.8694,
      "step": 5610
    },
    {
      "epoch": 1.6286500978189986,
      "grad_norm": 1.8176535367965698,
      "learning_rate": 1.3851555642251048e-05,
      "loss": 0.7813,
      "step": 5620
    },
    {
      "epoch": 1.6315484385189478,
      "grad_norm": 1.6815555095672607,
      "learning_rate": 1.3822295913391202e-05,
      "loss": 0.8051,
      "step": 5630
    },
    {
      "epoch": 1.634446779218897,
      "grad_norm": 1.8796361684799194,
      "learning_rate": 1.3793036184531356e-05,
      "loss": 0.829,
      "step": 5640
    },
    {
      "epoch": 1.6373451199188465,
      "grad_norm": 1.482505202293396,
      "learning_rate": 1.3763776455671513e-05,
      "loss": 0.7732,
      "step": 5650
    },
    {
      "epoch": 1.640243460618796,
      "grad_norm": 1.5728970766067505,
      "learning_rate": 1.3734516726811666e-05,
      "loss": 0.8322,
      "step": 5660
    },
    {
      "epoch": 1.643141801318745,
      "grad_norm": 2.050051689147949,
      "learning_rate": 1.370525699795182e-05,
      "loss": 0.7915,
      "step": 5670
    },
    {
      "epoch": 1.6460401420186943,
      "grad_norm": 1.6873191595077515,
      "learning_rate": 1.3675997269091974e-05,
      "loss": 0.8093,
      "step": 5680
    },
    {
      "epoch": 1.6489384827186435,
      "grad_norm": 1.8450874090194702,
      "learning_rate": 1.3646737540232127e-05,
      "loss": 0.8586,
      "step": 5690
    },
    {
      "epoch": 1.6518368234185927,
      "grad_norm": 1.9210203886032104,
      "learning_rate": 1.3617477811372281e-05,
      "loss": 0.7932,
      "step": 5700
    },
    {
      "epoch": 1.6547351641185422,
      "grad_norm": 1.96488356590271,
      "learning_rate": 1.3588218082512436e-05,
      "loss": 0.7964,
      "step": 5710
    },
    {
      "epoch": 1.6576335048184914,
      "grad_norm": 1.754610538482666,
      "learning_rate": 1.355895835365259e-05,
      "loss": 0.7765,
      "step": 5720
    },
    {
      "epoch": 1.6605318455184408,
      "grad_norm": 1.4913208484649658,
      "learning_rate": 1.3529698624792744e-05,
      "loss": 0.7637,
      "step": 5730
    },
    {
      "epoch": 1.66343018621839,
      "grad_norm": 1.9562122821807861,
      "learning_rate": 1.3500438895932897e-05,
      "loss": 0.8682,
      "step": 5740
    },
    {
      "epoch": 1.6663285269183392,
      "grad_norm": 1.4415221214294434,
      "learning_rate": 1.3471179167073051e-05,
      "loss": 0.7721,
      "step": 5750
    },
    {
      "epoch": 1.6692268676182884,
      "grad_norm": 1.6410396099090576,
      "learning_rate": 1.3441919438213206e-05,
      "loss": 0.8294,
      "step": 5760
    },
    {
      "epoch": 1.6721252083182379,
      "grad_norm": 1.8044066429138184,
      "learning_rate": 1.3412659709353362e-05,
      "loss": 0.8134,
      "step": 5770
    },
    {
      "epoch": 1.675023549018187,
      "grad_norm": 1.448054552078247,
      "learning_rate": 1.3383399980493515e-05,
      "loss": 0.8271,
      "step": 5780
    },
    {
      "epoch": 1.6779218897181365,
      "grad_norm": 1.5125361680984497,
      "learning_rate": 1.3354140251633669e-05,
      "loss": 0.8163,
      "step": 5790
    },
    {
      "epoch": 1.6808202304180857,
      "grad_norm": 1.9501363039016724,
      "learning_rate": 1.3324880522773823e-05,
      "loss": 0.8141,
      "step": 5800
    },
    {
      "epoch": 1.683718571118035,
      "grad_norm": 1.7636923789978027,
      "learning_rate": 1.3295620793913976e-05,
      "loss": 0.8617,
      "step": 5810
    },
    {
      "epoch": 1.6866169118179841,
      "grad_norm": 1.7420204877853394,
      "learning_rate": 1.326636106505413e-05,
      "loss": 0.7952,
      "step": 5820
    },
    {
      "epoch": 1.6895152525179333,
      "grad_norm": 1.664961814880371,
      "learning_rate": 1.3237101336194285e-05,
      "loss": 0.8365,
      "step": 5830
    },
    {
      "epoch": 1.6924135932178828,
      "grad_norm": 1.6835819482803345,
      "learning_rate": 1.3207841607334439e-05,
      "loss": 0.7829,
      "step": 5840
    },
    {
      "epoch": 1.6953119339178322,
      "grad_norm": 1.5840095281600952,
      "learning_rate": 1.3178581878474593e-05,
      "loss": 0.8408,
      "step": 5850
    },
    {
      "epoch": 1.6982102746177814,
      "grad_norm": 1.4156079292297363,
      "learning_rate": 1.3149322149614746e-05,
      "loss": 0.7683,
      "step": 5860
    },
    {
      "epoch": 1.7011086153177306,
      "grad_norm": 1.5915184020996094,
      "learning_rate": 1.3120062420754902e-05,
      "loss": 0.8768,
      "step": 5870
    },
    {
      "epoch": 1.7040069560176798,
      "grad_norm": 1.4680129289627075,
      "learning_rate": 1.3090802691895055e-05,
      "loss": 0.7704,
      "step": 5880
    },
    {
      "epoch": 1.706905296717629,
      "grad_norm": 1.6626311540603638,
      "learning_rate": 1.306154296303521e-05,
      "loss": 0.8261,
      "step": 5890
    },
    {
      "epoch": 1.7098036374175785,
      "grad_norm": 1.5206201076507568,
      "learning_rate": 1.3032283234175364e-05,
      "loss": 0.7395,
      "step": 5900
    },
    {
      "epoch": 1.7127019781175277,
      "grad_norm": 1.8960165977478027,
      "learning_rate": 1.3003023505315518e-05,
      "loss": 0.8276,
      "step": 5910
    },
    {
      "epoch": 1.715600318817477,
      "grad_norm": 1.617822289466858,
      "learning_rate": 1.2973763776455672e-05,
      "loss": 0.8095,
      "step": 5920
    },
    {
      "epoch": 1.7184986595174263,
      "grad_norm": 1.9673298597335815,
      "learning_rate": 1.2944504047595825e-05,
      "loss": 0.8336,
      "step": 5930
    },
    {
      "epoch": 1.7213970002173755,
      "grad_norm": 1.700469970703125,
      "learning_rate": 1.2915244318735979e-05,
      "loss": 0.8311,
      "step": 5940
    },
    {
      "epoch": 1.7242953409173247,
      "grad_norm": 1.5535691976547241,
      "learning_rate": 1.2885984589876134e-05,
      "loss": 0.844,
      "step": 5950
    },
    {
      "epoch": 1.7271936816172742,
      "grad_norm": 1.5110960006713867,
      "learning_rate": 1.2856724861016288e-05,
      "loss": 0.8112,
      "step": 5960
    },
    {
      "epoch": 1.7300920223172234,
      "grad_norm": 1.7153162956237793,
      "learning_rate": 1.2827465132156442e-05,
      "loss": 0.8282,
      "step": 5970
    },
    {
      "epoch": 1.7329903630171728,
      "grad_norm": 1.5378247499465942,
      "learning_rate": 1.2798205403296597e-05,
      "loss": 0.7822,
      "step": 5980
    },
    {
      "epoch": 1.735888703717122,
      "grad_norm": 1.6140074729919434,
      "learning_rate": 1.276894567443675e-05,
      "loss": 0.7784,
      "step": 5990
    },
    {
      "epoch": 1.7387870444170712,
      "grad_norm": 1.8792505264282227,
      "learning_rate": 1.2739685945576906e-05,
      "loss": 0.8009,
      "step": 6000
    },
    {
      "epoch": 1.7416853851170204,
      "grad_norm": 1.499100923538208,
      "learning_rate": 1.2713352189603043e-05,
      "loss": 0.8015,
      "step": 6010
    },
    {
      "epoch": 1.7445837258169696,
      "grad_norm": 2.2390942573547363,
      "learning_rate": 1.2684092460743198e-05,
      "loss": 0.7775,
      "step": 6020
    },
    {
      "epoch": 1.747482066516919,
      "grad_norm": 1.658419132232666,
      "learning_rate": 1.2654832731883352e-05,
      "loss": 0.7879,
      "step": 6030
    },
    {
      "epoch": 1.7503804072168685,
      "grad_norm": 1.72236967086792,
      "learning_rate": 1.2625573003023505e-05,
      "loss": 0.8331,
      "step": 6040
    },
    {
      "epoch": 1.7532787479168177,
      "grad_norm": 1.6108440160751343,
      "learning_rate": 1.259631327416366e-05,
      "loss": 0.8485,
      "step": 6050
    },
    {
      "epoch": 1.756177088616767,
      "grad_norm": 1.7042276859283447,
      "learning_rate": 1.2567053545303814e-05,
      "loss": 0.856,
      "step": 6060
    },
    {
      "epoch": 1.7590754293167161,
      "grad_norm": 1.7870829105377197,
      "learning_rate": 1.2537793816443968e-05,
      "loss": 0.8324,
      "step": 6070
    },
    {
      "epoch": 1.7619737700166653,
      "grad_norm": 1.7232036590576172,
      "learning_rate": 1.2508534087584122e-05,
      "loss": 0.8018,
      "step": 6080
    },
    {
      "epoch": 1.7648721107166148,
      "grad_norm": 1.4934923648834229,
      "learning_rate": 1.2479274358724275e-05,
      "loss": 0.8034,
      "step": 6090
    },
    {
      "epoch": 1.767770451416564,
      "grad_norm": 1.821303367614746,
      "learning_rate": 1.2450014629864429e-05,
      "loss": 0.8508,
      "step": 6100
    },
    {
      "epoch": 1.7706687921165134,
      "grad_norm": 1.6054227352142334,
      "learning_rate": 1.2420754901004584e-05,
      "loss": 0.8202,
      "step": 6110
    },
    {
      "epoch": 1.7735671328164626,
      "grad_norm": 1.8308095932006836,
      "learning_rate": 1.2391495172144738e-05,
      "loss": 0.8251,
      "step": 6120
    },
    {
      "epoch": 1.7764654735164118,
      "grad_norm": 1.501051425933838,
      "learning_rate": 1.2362235443284894e-05,
      "loss": 0.8145,
      "step": 6130
    },
    {
      "epoch": 1.779363814216361,
      "grad_norm": 1.6164562702178955,
      "learning_rate": 1.2332975714425047e-05,
      "loss": 0.8107,
      "step": 6140
    },
    {
      "epoch": 1.7822621549163105,
      "grad_norm": 1.6207633018493652,
      "learning_rate": 1.23037159855652e-05,
      "loss": 0.8077,
      "step": 6150
    },
    {
      "epoch": 1.7851604956162597,
      "grad_norm": 1.6278650760650635,
      "learning_rate": 1.2274456256705355e-05,
      "loss": 0.8148,
      "step": 6160
    },
    {
      "epoch": 1.788058836316209,
      "grad_norm": 1.4453848600387573,
      "learning_rate": 1.224519652784551e-05,
      "loss": 0.828,
      "step": 6170
    },
    {
      "epoch": 1.7909571770161583,
      "grad_norm": 1.5582807064056396,
      "learning_rate": 1.2215936798985664e-05,
      "loss": 0.7933,
      "step": 6180
    },
    {
      "epoch": 1.7938555177161075,
      "grad_norm": 2.200942039489746,
      "learning_rate": 1.2186677070125817e-05,
      "loss": 0.7967,
      "step": 6190
    },
    {
      "epoch": 1.7967538584160567,
      "grad_norm": 1.6947152614593506,
      "learning_rate": 1.2157417341265971e-05,
      "loss": 0.8297,
      "step": 6200
    },
    {
      "epoch": 1.799652199116006,
      "grad_norm": 2.0205588340759277,
      "learning_rate": 1.2128157612406125e-05,
      "loss": 0.7754,
      "step": 6210
    },
    {
      "epoch": 1.8025505398159554,
      "grad_norm": 1.5669476985931396,
      "learning_rate": 1.2098897883546278e-05,
      "loss": 0.8349,
      "step": 6220
    },
    {
      "epoch": 1.8054488805159048,
      "grad_norm": 1.5617080926895142,
      "learning_rate": 1.2069638154686434e-05,
      "loss": 0.8528,
      "step": 6230
    },
    {
      "epoch": 1.808347221215854,
      "grad_norm": 1.4784518480300903,
      "learning_rate": 1.2040378425826589e-05,
      "loss": 0.8167,
      "step": 6240
    },
    {
      "epoch": 1.8112455619158032,
      "grad_norm": 1.8916518688201904,
      "learning_rate": 1.2011118696966743e-05,
      "loss": 0.7974,
      "step": 6250
    },
    {
      "epoch": 1.8141439026157524,
      "grad_norm": 1.6123101711273193,
      "learning_rate": 1.1981858968106896e-05,
      "loss": 0.7731,
      "step": 6260
    },
    {
      "epoch": 1.8170422433157016,
      "grad_norm": 1.9191168546676636,
      "learning_rate": 1.195259923924705e-05,
      "loss": 0.7553,
      "step": 6270
    },
    {
      "epoch": 1.819940584015651,
      "grad_norm": 1.9029592275619507,
      "learning_rate": 1.1923339510387204e-05,
      "loss": 0.8358,
      "step": 6280
    },
    {
      "epoch": 1.8228389247156003,
      "grad_norm": 1.574682593345642,
      "learning_rate": 1.1894079781527359e-05,
      "loss": 0.7845,
      "step": 6290
    },
    {
      "epoch": 1.8257372654155497,
      "grad_norm": 2.095121145248413,
      "learning_rate": 1.1864820052667513e-05,
      "loss": 0.7961,
      "step": 6300
    },
    {
      "epoch": 1.828635606115499,
      "grad_norm": 1.5165525674819946,
      "learning_rate": 1.1835560323807666e-05,
      "loss": 0.8173,
      "step": 6310
    },
    {
      "epoch": 1.8315339468154481,
      "grad_norm": 1.746576189994812,
      "learning_rate": 1.180630059494782e-05,
      "loss": 0.8471,
      "step": 6320
    },
    {
      "epoch": 1.8344322875153973,
      "grad_norm": 1.6939704418182373,
      "learning_rate": 1.1777040866087974e-05,
      "loss": 0.7726,
      "step": 6330
    },
    {
      "epoch": 1.8373306282153468,
      "grad_norm": 1.828922152519226,
      "learning_rate": 1.1747781137228127e-05,
      "loss": 0.8096,
      "step": 6340
    },
    {
      "epoch": 1.840228968915296,
      "grad_norm": 1.5767995119094849,
      "learning_rate": 1.1718521408368284e-05,
      "loss": 0.8169,
      "step": 6350
    },
    {
      "epoch": 1.8431273096152454,
      "grad_norm": 1.7371941804885864,
      "learning_rate": 1.1689261679508438e-05,
      "loss": 0.7933,
      "step": 6360
    },
    {
      "epoch": 1.8460256503151946,
      "grad_norm": 1.9210057258605957,
      "learning_rate": 1.1660001950648592e-05,
      "loss": 0.8091,
      "step": 6370
    },
    {
      "epoch": 1.8489239910151438,
      "grad_norm": 2.079472064971924,
      "learning_rate": 1.1630742221788745e-05,
      "loss": 0.8033,
      "step": 6380
    },
    {
      "epoch": 1.851822331715093,
      "grad_norm": 1.5681641101837158,
      "learning_rate": 1.1601482492928899e-05,
      "loss": 0.8337,
      "step": 6390
    },
    {
      "epoch": 1.8547206724150422,
      "grad_norm": 1.5034077167510986,
      "learning_rate": 1.1572222764069053e-05,
      "loss": 0.7734,
      "step": 6400
    },
    {
      "epoch": 1.8576190131149917,
      "grad_norm": 1.6667215824127197,
      "learning_rate": 1.1542963035209208e-05,
      "loss": 0.7794,
      "step": 6410
    },
    {
      "epoch": 1.860517353814941,
      "grad_norm": 1.6976845264434814,
      "learning_rate": 1.1513703306349362e-05,
      "loss": 0.843,
      "step": 6420
    },
    {
      "epoch": 1.8634156945148903,
      "grad_norm": 1.620267629623413,
      "learning_rate": 1.1484443577489515e-05,
      "loss": 0.7658,
      "step": 6430
    },
    {
      "epoch": 1.8663140352148395,
      "grad_norm": 1.8477914333343506,
      "learning_rate": 1.1455183848629669e-05,
      "loss": 0.8149,
      "step": 6440
    },
    {
      "epoch": 1.8692123759147887,
      "grad_norm": 1.627842664718628,
      "learning_rate": 1.1425924119769823e-05,
      "loss": 0.7785,
      "step": 6450
    },
    {
      "epoch": 1.872110716614738,
      "grad_norm": 1.7545316219329834,
      "learning_rate": 1.1396664390909978e-05,
      "loss": 0.7854,
      "step": 6460
    },
    {
      "epoch": 1.8750090573146874,
      "grad_norm": 1.6169121265411377,
      "learning_rate": 1.1367404662050133e-05,
      "loss": 0.7637,
      "step": 6470
    },
    {
      "epoch": 1.8779073980146366,
      "grad_norm": 1.689279556274414,
      "learning_rate": 1.1338144933190287e-05,
      "loss": 0.7565,
      "step": 6480
    },
    {
      "epoch": 1.880805738714586,
      "grad_norm": 1.697396993637085,
      "learning_rate": 1.130888520433044e-05,
      "loss": 0.831,
      "step": 6490
    },
    {
      "epoch": 1.8837040794145352,
      "grad_norm": 1.5219265222549438,
      "learning_rate": 1.1279625475470594e-05,
      "loss": 0.7922,
      "step": 6500
    },
    {
      "epoch": 1.8866024201144844,
      "grad_norm": 2.0567269325256348,
      "learning_rate": 1.1250365746610748e-05,
      "loss": 0.7764,
      "step": 6510
    },
    {
      "epoch": 1.8895007608144336,
      "grad_norm": 1.8072097301483154,
      "learning_rate": 1.1221106017750902e-05,
      "loss": 0.8055,
      "step": 6520
    },
    {
      "epoch": 1.892399101514383,
      "grad_norm": 1.5541632175445557,
      "learning_rate": 1.1191846288891057e-05,
      "loss": 0.8804,
      "step": 6530
    },
    {
      "epoch": 1.8952974422143323,
      "grad_norm": 1.6826351881027222,
      "learning_rate": 1.116258656003121e-05,
      "loss": 0.8069,
      "step": 6540
    },
    {
      "epoch": 1.8981957829142817,
      "grad_norm": 1.8820780515670776,
      "learning_rate": 1.1133326831171364e-05,
      "loss": 0.7949,
      "step": 6550
    },
    {
      "epoch": 1.901094123614231,
      "grad_norm": 1.941704511642456,
      "learning_rate": 1.1104067102311518e-05,
      "loss": 0.7748,
      "step": 6560
    },
    {
      "epoch": 1.9039924643141801,
      "grad_norm": 1.6193327903747559,
      "learning_rate": 1.1074807373451673e-05,
      "loss": 0.805,
      "step": 6570
    },
    {
      "epoch": 1.9068908050141293,
      "grad_norm": 1.8675819635391235,
      "learning_rate": 1.1045547644591827e-05,
      "loss": 0.8397,
      "step": 6580
    },
    {
      "epoch": 1.9097891457140785,
      "grad_norm": 1.8318700790405273,
      "learning_rate": 1.1016287915731982e-05,
      "loss": 0.8219,
      "step": 6590
    },
    {
      "epoch": 1.912687486414028,
      "grad_norm": 1.8557705879211426,
      "learning_rate": 1.0987028186872136e-05,
      "loss": 0.8273,
      "step": 6600
    },
    {
      "epoch": 1.9155858271139774,
      "grad_norm": 1.5218480825424194,
      "learning_rate": 1.095776845801229e-05,
      "loss": 0.819,
      "step": 6610
    },
    {
      "epoch": 1.9184841678139266,
      "grad_norm": 1.7683309316635132,
      "learning_rate": 1.0928508729152443e-05,
      "loss": 0.799,
      "step": 6620
    },
    {
      "epoch": 1.9213825085138758,
      "grad_norm": 1.5368576049804688,
      "learning_rate": 1.0899249000292597e-05,
      "loss": 0.7967,
      "step": 6630
    },
    {
      "epoch": 1.924280849213825,
      "grad_norm": 1.7362347841262817,
      "learning_rate": 1.086998927143275e-05,
      "loss": 0.8149,
      "step": 6640
    },
    {
      "epoch": 1.9271791899137742,
      "grad_norm": 1.8911266326904297,
      "learning_rate": 1.0840729542572906e-05,
      "loss": 0.8418,
      "step": 6650
    },
    {
      "epoch": 1.9300775306137237,
      "grad_norm": 1.5761927366256714,
      "learning_rate": 1.081146981371306e-05,
      "loss": 0.8084,
      "step": 6660
    },
    {
      "epoch": 1.9329758713136729,
      "grad_norm": 1.9095675945281982,
      "learning_rate": 1.0782210084853213e-05,
      "loss": 0.7779,
      "step": 6670
    },
    {
      "epoch": 1.9358742120136223,
      "grad_norm": 1.6240016222000122,
      "learning_rate": 1.0752950355993367e-05,
      "loss": 0.806,
      "step": 6680
    },
    {
      "epoch": 1.9387725527135715,
      "grad_norm": 1.8041585683822632,
      "learning_rate": 1.0723690627133522e-05,
      "loss": 0.7956,
      "step": 6690
    },
    {
      "epoch": 1.9416708934135207,
      "grad_norm": 1.5724056959152222,
      "learning_rate": 1.0694430898273676e-05,
      "loss": 0.762,
      "step": 6700
    },
    {
      "epoch": 1.94456923411347,
      "grad_norm": 1.9022865295410156,
      "learning_rate": 1.0665171169413831e-05,
      "loss": 0.8007,
      "step": 6710
    },
    {
      "epoch": 1.9474675748134194,
      "grad_norm": 1.4234689474105835,
      "learning_rate": 1.0635911440553985e-05,
      "loss": 0.8037,
      "step": 6720
    },
    {
      "epoch": 1.9503659155133686,
      "grad_norm": 1.6759498119354248,
      "learning_rate": 1.0606651711694139e-05,
      "loss": 0.8544,
      "step": 6730
    },
    {
      "epoch": 1.953264256213318,
      "grad_norm": 1.4833943843841553,
      "learning_rate": 1.0577391982834292e-05,
      "loss": 0.8229,
      "step": 6740
    },
    {
      "epoch": 1.9561625969132672,
      "grad_norm": 1.2630398273468018,
      "learning_rate": 1.0548132253974446e-05,
      "loss": 0.7871,
      "step": 6750
    },
    {
      "epoch": 1.9590609376132164,
      "grad_norm": 1.5747337341308594,
      "learning_rate": 1.05188725251146e-05,
      "loss": 0.7954,
      "step": 6760
    },
    {
      "epoch": 1.9619592783131656,
      "grad_norm": 1.8191792964935303,
      "learning_rate": 1.0489612796254755e-05,
      "loss": 0.7686,
      "step": 6770
    },
    {
      "epoch": 1.9648576190131148,
      "grad_norm": 1.767425775527954,
      "learning_rate": 1.0460353067394909e-05,
      "loss": 0.7957,
      "step": 6780
    },
    {
      "epoch": 1.9677559597130643,
      "grad_norm": 1.7056940793991089,
      "learning_rate": 1.0431093338535062e-05,
      "loss": 0.8038,
      "step": 6790
    },
    {
      "epoch": 1.9706543004130137,
      "grad_norm": 1.7113821506500244,
      "learning_rate": 1.0401833609675218e-05,
      "loss": 0.7135,
      "step": 6800
    },
    {
      "epoch": 1.973552641112963,
      "grad_norm": 1.6576733589172363,
      "learning_rate": 1.0372573880815371e-05,
      "loss": 0.785,
      "step": 6810
    },
    {
      "epoch": 1.9764509818129121,
      "grad_norm": 1.5782257318496704,
      "learning_rate": 1.0343314151955525e-05,
      "loss": 0.8305,
      "step": 6820
    },
    {
      "epoch": 1.9793493225128613,
      "grad_norm": 1.7125515937805176,
      "learning_rate": 1.031405442309568e-05,
      "loss": 0.7692,
      "step": 6830
    },
    {
      "epoch": 1.9822476632128105,
      "grad_norm": 1.495237946510315,
      "learning_rate": 1.0284794694235834e-05,
      "loss": 0.7785,
      "step": 6840
    },
    {
      "epoch": 1.98514600391276,
      "grad_norm": 1.5419800281524658,
      "learning_rate": 1.0255534965375988e-05,
      "loss": 0.8431,
      "step": 6850
    },
    {
      "epoch": 1.9880443446127092,
      "grad_norm": 1.4543524980545044,
      "learning_rate": 1.0226275236516141e-05,
      "loss": 0.812,
      "step": 6860
    },
    {
      "epoch": 1.9909426853126586,
      "grad_norm": 1.349943995475769,
      "learning_rate": 1.0197015507656295e-05,
      "loss": 0.7874,
      "step": 6870
    },
    {
      "epoch": 1.9938410260126078,
      "grad_norm": 1.6428899765014648,
      "learning_rate": 1.016775577879645e-05,
      "loss": 0.8336,
      "step": 6880
    },
    {
      "epoch": 1.996739366712557,
      "grad_norm": 1.6029598712921143,
      "learning_rate": 1.0138496049936604e-05,
      "loss": 0.8296,
      "step": 6890
    },
    {
      "epoch": 1.9996377074125062,
      "grad_norm": 1.710696816444397,
      "learning_rate": 1.0109236321076758e-05,
      "loss": 0.8121,
      "step": 6900
    },
    {
      "epoch": 2.0023186725599595,
      "grad_norm": 1.7604484558105469,
      "learning_rate": 1.0082902565102896e-05,
      "loss": 0.8053,
      "step": 6910
    },
    {
      "epoch": 2.0052170132599088,
      "grad_norm": 1.6337084770202637,
      "learning_rate": 1.005364283624305e-05,
      "loss": 0.789,
      "step": 6920
    },
    {
      "epoch": 2.008115353959858,
      "grad_norm": 1.401936650276184,
      "learning_rate": 1.0024383107383205e-05,
      "loss": 0.8091,
      "step": 6930
    },
    {
      "epoch": 2.011013694659807,
      "grad_norm": 1.554939866065979,
      "learning_rate": 9.99512337852336e-06,
      "loss": 0.7908,
      "step": 6940
    },
    {
      "epoch": 2.0139120353597564,
      "grad_norm": 1.6314853429794312,
      "learning_rate": 9.965863649663514e-06,
      "loss": 0.7757,
      "step": 6950
    },
    {
      "epoch": 2.016810376059706,
      "grad_norm": 1.897456169128418,
      "learning_rate": 9.936603920803668e-06,
      "loss": 0.8691,
      "step": 6960
    },
    {
      "epoch": 2.0197087167596552,
      "grad_norm": 1.55472993850708,
      "learning_rate": 9.907344191943822e-06,
      "loss": 0.8105,
      "step": 6970
    },
    {
      "epoch": 2.0226070574596045,
      "grad_norm": 2.216630458831787,
      "learning_rate": 9.878084463083975e-06,
      "loss": 0.8216,
      "step": 6980
    },
    {
      "epoch": 2.0255053981595537,
      "grad_norm": 1.5660126209259033,
      "learning_rate": 9.84882473422413e-06,
      "loss": 0.8209,
      "step": 6990
    },
    {
      "epoch": 2.028403738859503,
      "grad_norm": 2.0047531127929688,
      "learning_rate": 9.819565005364284e-06,
      "loss": 0.8561,
      "step": 7000
    },
    {
      "epoch": 2.031302079559452,
      "grad_norm": 1.9339698553085327,
      "learning_rate": 9.790305276504438e-06,
      "loss": 0.7926,
      "step": 7010
    },
    {
      "epoch": 2.0342004202594013,
      "grad_norm": 2.2505781650543213,
      "learning_rate": 9.761045547644592e-06,
      "loss": 0.8092,
      "step": 7020
    },
    {
      "epoch": 2.037098760959351,
      "grad_norm": 1.5731041431427002,
      "learning_rate": 9.731785818784745e-06,
      "loss": 0.7646,
      "step": 7030
    },
    {
      "epoch": 2.0399971016593,
      "grad_norm": 1.500109076499939,
      "learning_rate": 9.702526089924899e-06,
      "loss": 0.7643,
      "step": 7040
    },
    {
      "epoch": 2.0428954423592494,
      "grad_norm": 1.78873872756958,
      "learning_rate": 9.673266361065054e-06,
      "loss": 0.8063,
      "step": 7050
    },
    {
      "epoch": 2.0457937830591986,
      "grad_norm": 2.0747640132904053,
      "learning_rate": 9.64400663220521e-06,
      "loss": 0.8076,
      "step": 7060
    },
    {
      "epoch": 2.0486921237591478,
      "grad_norm": 1.5219733715057373,
      "learning_rate": 9.614746903345363e-06,
      "loss": 0.8332,
      "step": 7070
    },
    {
      "epoch": 2.051590464459097,
      "grad_norm": 1.5500223636627197,
      "learning_rate": 9.585487174485517e-06,
      "loss": 0.7927,
      "step": 7080
    },
    {
      "epoch": 2.0544888051590466,
      "grad_norm": 1.7484688758850098,
      "learning_rate": 9.55622744562567e-06,
      "loss": 0.827,
      "step": 7090
    },
    {
      "epoch": 2.057387145858996,
      "grad_norm": 2.004563570022583,
      "learning_rate": 9.526967716765824e-06,
      "loss": 0.7934,
      "step": 7100
    },
    {
      "epoch": 2.060285486558945,
      "grad_norm": 2.0259814262390137,
      "learning_rate": 9.49770798790598e-06,
      "loss": 0.7915,
      "step": 7110
    },
    {
      "epoch": 2.0631838272588943,
      "grad_norm": 1.4920425415039062,
      "learning_rate": 9.468448259046133e-06,
      "loss": 0.7684,
      "step": 7120
    },
    {
      "epoch": 2.0660821679588435,
      "grad_norm": 1.9758154153823853,
      "learning_rate": 9.439188530186287e-06,
      "loss": 0.7995,
      "step": 7130
    },
    {
      "epoch": 2.0689805086587927,
      "grad_norm": 1.927504062652588,
      "learning_rate": 9.40992880132644e-06,
      "loss": 0.7853,
      "step": 7140
    },
    {
      "epoch": 2.0718788493587423,
      "grad_norm": 1.826377272605896,
      "learning_rate": 9.380669072466594e-06,
      "loss": 0.8521,
      "step": 7150
    },
    {
      "epoch": 2.0747771900586915,
      "grad_norm": 1.7701389789581299,
      "learning_rate": 9.351409343606748e-06,
      "loss": 0.8108,
      "step": 7160
    },
    {
      "epoch": 2.0776755307586408,
      "grad_norm": 2.1421010494232178,
      "learning_rate": 9.322149614746905e-06,
      "loss": 0.8011,
      "step": 7170
    },
    {
      "epoch": 2.08057387145859,
      "grad_norm": 1.8944193124771118,
      "learning_rate": 9.292889885887059e-06,
      "loss": 0.8215,
      "step": 7180
    },
    {
      "epoch": 2.083472212158539,
      "grad_norm": 1.8493096828460693,
      "learning_rate": 9.263630157027212e-06,
      "loss": 0.7959,
      "step": 7190
    },
    {
      "epoch": 2.0863705528584884,
      "grad_norm": 1.609816551208496,
      "learning_rate": 9.234370428167366e-06,
      "loss": 0.8666,
      "step": 7200
    },
    {
      "epoch": 2.0892688935584376,
      "grad_norm": 1.83210027217865,
      "learning_rate": 9.20511069930752e-06,
      "loss": 0.8431,
      "step": 7210
    },
    {
      "epoch": 2.0921672342583872,
      "grad_norm": 1.4191479682922363,
      "learning_rate": 9.175850970447673e-06,
      "loss": 0.7934,
      "step": 7220
    },
    {
      "epoch": 2.0950655749583365,
      "grad_norm": 1.7443617582321167,
      "learning_rate": 9.146591241587829e-06,
      "loss": 0.7598,
      "step": 7230
    },
    {
      "epoch": 2.0979639156582857,
      "grad_norm": 1.8677799701690674,
      "learning_rate": 9.117331512727982e-06,
      "loss": 0.799,
      "step": 7240
    },
    {
      "epoch": 2.100862256358235,
      "grad_norm": 1.6999346017837524,
      "learning_rate": 9.088071783868136e-06,
      "loss": 0.8527,
      "step": 7250
    },
    {
      "epoch": 2.103760597058184,
      "grad_norm": 1.6973439455032349,
      "learning_rate": 9.05881205500829e-06,
      "loss": 0.8273,
      "step": 7260
    },
    {
      "epoch": 2.1066589377581333,
      "grad_norm": 1.8241196870803833,
      "learning_rate": 9.029552326148443e-06,
      "loss": 0.7929,
      "step": 7270
    },
    {
      "epoch": 2.109557278458083,
      "grad_norm": 1.6454997062683105,
      "learning_rate": 9.000292597288599e-06,
      "loss": 0.7924,
      "step": 7280
    },
    {
      "epoch": 2.112455619158032,
      "grad_norm": 1.6560593843460083,
      "learning_rate": 8.971032868428754e-06,
      "loss": 0.7845,
      "step": 7290
    },
    {
      "epoch": 2.1153539598579814,
      "grad_norm": 1.58511483669281,
      "learning_rate": 8.941773139568908e-06,
      "loss": 0.7883,
      "step": 7300
    },
    {
      "epoch": 2.1182523005579306,
      "grad_norm": 1.7070852518081665,
      "learning_rate": 8.912513410709061e-06,
      "loss": 0.8011,
      "step": 7310
    },
    {
      "epoch": 2.1211506412578798,
      "grad_norm": 1.7120178937911987,
      "learning_rate": 8.883253681849215e-06,
      "loss": 0.7906,
      "step": 7320
    },
    {
      "epoch": 2.124048981957829,
      "grad_norm": 1.6075071096420288,
      "learning_rate": 8.853993952989369e-06,
      "loss": 0.8346,
      "step": 7330
    },
    {
      "epoch": 2.126947322657778,
      "grad_norm": 1.814520239830017,
      "learning_rate": 8.824734224129522e-06,
      "loss": 0.8227,
      "step": 7340
    },
    {
      "epoch": 2.129845663357728,
      "grad_norm": 1.4946637153625488,
      "learning_rate": 8.795474495269678e-06,
      "loss": 0.8016,
      "step": 7350
    },
    {
      "epoch": 2.132744004057677,
      "grad_norm": 1.6790462732315063,
      "learning_rate": 8.766214766409831e-06,
      "loss": 0.7405,
      "step": 7360
    },
    {
      "epoch": 2.1356423447576263,
      "grad_norm": 1.6686402559280396,
      "learning_rate": 8.736955037549985e-06,
      "loss": 0.8389,
      "step": 7370
    },
    {
      "epoch": 2.1385406854575755,
      "grad_norm": 1.5358747243881226,
      "learning_rate": 8.707695308690139e-06,
      "loss": 0.806,
      "step": 7380
    },
    {
      "epoch": 2.1414390261575247,
      "grad_norm": 1.5255261659622192,
      "learning_rate": 8.678435579830294e-06,
      "loss": 0.8081,
      "step": 7390
    },
    {
      "epoch": 2.144337366857474,
      "grad_norm": 1.763447642326355,
      "learning_rate": 8.649175850970448e-06,
      "loss": 0.7574,
      "step": 7400
    },
    {
      "epoch": 2.1472357075574235,
      "grad_norm": 1.644636631011963,
      "learning_rate": 8.619916122110603e-06,
      "loss": 0.7743,
      "step": 7410
    },
    {
      "epoch": 2.1501340482573728,
      "grad_norm": 1.5395747423171997,
      "learning_rate": 8.590656393250757e-06,
      "loss": 0.7861,
      "step": 7420
    },
    {
      "epoch": 2.153032388957322,
      "grad_norm": 2.2070889472961426,
      "learning_rate": 8.56139666439091e-06,
      "loss": 0.812,
      "step": 7430
    },
    {
      "epoch": 2.155930729657271,
      "grad_norm": 1.59730863571167,
      "learning_rate": 8.532136935531064e-06,
      "loss": 0.7538,
      "step": 7440
    },
    {
      "epoch": 2.1588290703572204,
      "grad_norm": 1.708517074584961,
      "learning_rate": 8.502877206671218e-06,
      "loss": 0.7653,
      "step": 7450
    },
    {
      "epoch": 2.1617274110571696,
      "grad_norm": 1.7058099508285522,
      "learning_rate": 8.473617477811371e-06,
      "loss": 0.8236,
      "step": 7460
    },
    {
      "epoch": 2.1646257517571192,
      "grad_norm": 1.8470959663391113,
      "learning_rate": 8.444357748951527e-06,
      "loss": 0.7804,
      "step": 7470
    },
    {
      "epoch": 2.1675240924570685,
      "grad_norm": 1.773233413696289,
      "learning_rate": 8.41509802009168e-06,
      "loss": 0.8104,
      "step": 7480
    },
    {
      "epoch": 2.1704224331570177,
      "grad_norm": 2.1254942417144775,
      "learning_rate": 8.385838291231834e-06,
      "loss": 0.7985,
      "step": 7490
    },
    {
      "epoch": 2.173320773856967,
      "grad_norm": 1.5268315076828003,
      "learning_rate": 8.35657856237199e-06,
      "loss": 0.8332,
      "step": 7500
    },
    {
      "epoch": 2.176219114556916,
      "grad_norm": 1.6150381565093994,
      "learning_rate": 8.327318833512143e-06,
      "loss": 0.7819,
      "step": 7510
    },
    {
      "epoch": 2.1791174552568653,
      "grad_norm": 1.9047136306762695,
      "learning_rate": 8.298059104652297e-06,
      "loss": 0.7803,
      "step": 7520
    },
    {
      "epoch": 2.182015795956815,
      "grad_norm": 2.219712018966675,
      "learning_rate": 8.268799375792452e-06,
      "loss": 0.8443,
      "step": 7530
    },
    {
      "epoch": 2.184914136656764,
      "grad_norm": 1.7556862831115723,
      "learning_rate": 8.239539646932606e-06,
      "loss": 0.8176,
      "step": 7540
    },
    {
      "epoch": 2.1878124773567134,
      "grad_norm": 2.3622891902923584,
      "learning_rate": 8.21027991807276e-06,
      "loss": 0.8028,
      "step": 7550
    },
    {
      "epoch": 2.1907108180566626,
      "grad_norm": 1.814190149307251,
      "learning_rate": 8.181020189212913e-06,
      "loss": 0.7856,
      "step": 7560
    },
    {
      "epoch": 2.1936091587566118,
      "grad_norm": 2.662764549255371,
      "learning_rate": 8.151760460353067e-06,
      "loss": 0.8285,
      "step": 7570
    },
    {
      "epoch": 2.196507499456561,
      "grad_norm": 1.7061020135879517,
      "learning_rate": 8.12250073149322e-06,
      "loss": 0.8041,
      "step": 7580
    },
    {
      "epoch": 2.19940584015651,
      "grad_norm": 1.6821565628051758,
      "learning_rate": 8.093241002633376e-06,
      "loss": 0.8112,
      "step": 7590
    },
    {
      "epoch": 2.20230418085646,
      "grad_norm": 1.6385993957519531,
      "learning_rate": 8.06398127377353e-06,
      "loss": 0.8125,
      "step": 7600
    },
    {
      "epoch": 2.205202521556409,
      "grad_norm": 1.707349419593811,
      "learning_rate": 8.034721544913685e-06,
      "loss": 0.8091,
      "step": 7610
    },
    {
      "epoch": 2.2081008622563583,
      "grad_norm": 1.4672951698303223,
      "learning_rate": 8.005461816053839e-06,
      "loss": 0.7906,
      "step": 7620
    },
    {
      "epoch": 2.2109992029563075,
      "grad_norm": 1.8149067163467407,
      "learning_rate": 7.976202087193992e-06,
      "loss": 0.7901,
      "step": 7630
    },
    {
      "epoch": 2.2138975436562567,
      "grad_norm": 1.861430048942566,
      "learning_rate": 7.946942358334146e-06,
      "loss": 0.7819,
      "step": 7640
    },
    {
      "epoch": 2.216795884356206,
      "grad_norm": 1.7464015483856201,
      "learning_rate": 7.917682629474301e-06,
      "loss": 0.8254,
      "step": 7650
    },
    {
      "epoch": 2.2196942250561555,
      "grad_norm": 1.9418485164642334,
      "learning_rate": 7.888422900614455e-06,
      "loss": 0.7895,
      "step": 7660
    },
    {
      "epoch": 2.2225925657561048,
      "grad_norm": 1.793944001197815,
      "learning_rate": 7.859163171754609e-06,
      "loss": 0.7884,
      "step": 7670
    },
    {
      "epoch": 2.225490906456054,
      "grad_norm": 1.810387134552002,
      "learning_rate": 7.829903442894762e-06,
      "loss": 0.8032,
      "step": 7680
    },
    {
      "epoch": 2.228389247156003,
      "grad_norm": 1.89700448513031,
      "learning_rate": 7.800643714034916e-06,
      "loss": 0.8057,
      "step": 7690
    },
    {
      "epoch": 2.2312875878559524,
      "grad_norm": 2.0181148052215576,
      "learning_rate": 7.77138398517507e-06,
      "loss": 0.8148,
      "step": 7700
    },
    {
      "epoch": 2.2341859285559016,
      "grad_norm": 1.6263854503631592,
      "learning_rate": 7.742124256315225e-06,
      "loss": 0.7988,
      "step": 7710
    },
    {
      "epoch": 2.237084269255851,
      "grad_norm": 1.7909117937088013,
      "learning_rate": 7.71286452745538e-06,
      "loss": 0.8303,
      "step": 7720
    },
    {
      "epoch": 2.2399826099558005,
      "grad_norm": 1.9873411655426025,
      "learning_rate": 7.683604798595534e-06,
      "loss": 0.8305,
      "step": 7730
    },
    {
      "epoch": 2.2428809506557497,
      "grad_norm": 1.6272053718566895,
      "learning_rate": 7.654345069735688e-06,
      "loss": 0.7936,
      "step": 7740
    },
    {
      "epoch": 2.245779291355699,
      "grad_norm": 1.7600573301315308,
      "learning_rate": 7.625085340875841e-06,
      "loss": 0.7646,
      "step": 7750
    },
    {
      "epoch": 2.248677632055648,
      "grad_norm": 1.7968989610671997,
      "learning_rate": 7.595825612015995e-06,
      "loss": 0.7853,
      "step": 7760
    },
    {
      "epoch": 2.2515759727555973,
      "grad_norm": 2.0353894233703613,
      "learning_rate": 7.56656588315615e-06,
      "loss": 0.8201,
      "step": 7770
    },
    {
      "epoch": 2.254474313455547,
      "grad_norm": 2.1260433197021484,
      "learning_rate": 7.537306154296304e-06,
      "loss": 0.7866,
      "step": 7780
    },
    {
      "epoch": 2.257372654155496,
      "grad_norm": 2.1721441745758057,
      "learning_rate": 7.5080464254364576e-06,
      "loss": 0.8396,
      "step": 7790
    },
    {
      "epoch": 2.2602709948554454,
      "grad_norm": 2.0329954624176025,
      "learning_rate": 7.478786696576611e-06,
      "loss": 0.7727,
      "step": 7800
    },
    {
      "epoch": 2.2631693355553946,
      "grad_norm": 1.7332600355148315,
      "learning_rate": 7.4495269677167666e-06,
      "loss": 0.7949,
      "step": 7810
    },
    {
      "epoch": 2.2660676762553438,
      "grad_norm": 1.50151526927948,
      "learning_rate": 7.42026723885692e-06,
      "loss": 0.7872,
      "step": 7820
    },
    {
      "epoch": 2.268966016955293,
      "grad_norm": 1.9312936067581177,
      "learning_rate": 7.391007509997074e-06,
      "loss": 0.7787,
      "step": 7830
    },
    {
      "epoch": 2.271864357655242,
      "grad_norm": 2.143935441970825,
      "learning_rate": 7.361747781137228e-06,
      "loss": 0.8482,
      "step": 7840
    },
    {
      "epoch": 2.2747626983551914,
      "grad_norm": 1.6487077474594116,
      "learning_rate": 7.332488052277382e-06,
      "loss": 0.7737,
      "step": 7850
    },
    {
      "epoch": 2.277661039055141,
      "grad_norm": 1.6095056533813477,
      "learning_rate": 7.303228323417537e-06,
      "loss": 0.8093,
      "step": 7860
    },
    {
      "epoch": 2.2805593797550903,
      "grad_norm": 1.7672297954559326,
      "learning_rate": 7.273968594557691e-06,
      "loss": 0.7726,
      "step": 7870
    },
    {
      "epoch": 2.2834577204550395,
      "grad_norm": 1.758821964263916,
      "learning_rate": 7.244708865697845e-06,
      "loss": 0.8273,
      "step": 7880
    },
    {
      "epoch": 2.2863560611549887,
      "grad_norm": 1.6849018335342407,
      "learning_rate": 7.215449136837998e-06,
      "loss": 0.7895,
      "step": 7890
    },
    {
      "epoch": 2.289254401854938,
      "grad_norm": 1.981433391571045,
      "learning_rate": 7.186189407978153e-06,
      "loss": 0.8141,
      "step": 7900
    },
    {
      "epoch": 2.2921527425548875,
      "grad_norm": 1.7280669212341309,
      "learning_rate": 7.156929679118307e-06,
      "loss": 0.8217,
      "step": 7910
    },
    {
      "epoch": 2.2950510832548368,
      "grad_norm": 1.6658449172973633,
      "learning_rate": 7.127669950258461e-06,
      "loss": 0.8251,
      "step": 7920
    },
    {
      "epoch": 2.297949423954786,
      "grad_norm": 1.8325806856155396,
      "learning_rate": 7.098410221398616e-06,
      "loss": 0.8293,
      "step": 7930
    },
    {
      "epoch": 2.300847764654735,
      "grad_norm": 1.631427526473999,
      "learning_rate": 7.069150492538769e-06,
      "loss": 0.7533,
      "step": 7940
    },
    {
      "epoch": 2.3037461053546844,
      "grad_norm": 1.5259052515029907,
      "learning_rate": 7.039890763678923e-06,
      "loss": 0.8195,
      "step": 7950
    },
    {
      "epoch": 2.3066444460546336,
      "grad_norm": 1.5319621562957764,
      "learning_rate": 7.0106310348190774e-06,
      "loss": 0.7748,
      "step": 7960
    },
    {
      "epoch": 2.309542786754583,
      "grad_norm": 2.1461338996887207,
      "learning_rate": 6.981371305959232e-06,
      "loss": 0.8499,
      "step": 7970
    },
    {
      "epoch": 2.3124411274545325,
      "grad_norm": 2.192300319671631,
      "learning_rate": 6.952111577099386e-06,
      "loss": 0.8235,
      "step": 7980
    },
    {
      "epoch": 2.3153394681544817,
      "grad_norm": 1.963497519493103,
      "learning_rate": 6.92285184823954e-06,
      "loss": 0.7612,
      "step": 7990
    },
    {
      "epoch": 2.318237808854431,
      "grad_norm": 1.4251810312271118,
      "learning_rate": 6.893592119379694e-06,
      "loss": 0.8048,
      "step": 8000
    },
    {
      "epoch": 2.32113614955438,
      "grad_norm": 1.6776248216629028,
      "learning_rate": 6.8643323905198475e-06,
      "loss": 0.7836,
      "step": 8010
    },
    {
      "epoch": 2.3240344902543293,
      "grad_norm": 1.9973976612091064,
      "learning_rate": 6.835072661660002e-06,
      "loss": 0.7904,
      "step": 8020
    },
    {
      "epoch": 2.3269328309542785,
      "grad_norm": 1.983920931816101,
      "learning_rate": 6.8058129328001565e-06,
      "loss": 0.8476,
      "step": 8030
    },
    {
      "epoch": 2.329831171654228,
      "grad_norm": 1.9098546504974365,
      "learning_rate": 6.77655320394031e-06,
      "loss": 0.8123,
      "step": 8040
    },
    {
      "epoch": 2.3327295123541774,
      "grad_norm": 1.8671027421951294,
      "learning_rate": 6.747293475080465e-06,
      "loss": 0.7927,
      "step": 8050
    },
    {
      "epoch": 2.3356278530541266,
      "grad_norm": 1.6930712461471558,
      "learning_rate": 6.718033746220618e-06,
      "loss": 0.7447,
      "step": 8060
    },
    {
      "epoch": 2.3385261937540758,
      "grad_norm": 1.7654305696487427,
      "learning_rate": 6.688774017360772e-06,
      "loss": 0.7803,
      "step": 8070
    },
    {
      "epoch": 2.341424534454025,
      "grad_norm": 1.699243426322937,
      "learning_rate": 6.659514288500927e-06,
      "loss": 0.7917,
      "step": 8080
    },
    {
      "epoch": 2.344322875153974,
      "grad_norm": 1.4687811136245728,
      "learning_rate": 6.630254559641081e-06,
      "loss": 0.7967,
      "step": 8090
    },
    {
      "epoch": 2.3472212158539234,
      "grad_norm": 1.8711079359054565,
      "learning_rate": 6.600994830781235e-06,
      "loss": 0.7958,
      "step": 8100
    },
    {
      "epoch": 2.350119556553873,
      "grad_norm": 1.705163598060608,
      "learning_rate": 6.571735101921389e-06,
      "loss": 0.7767,
      "step": 8110
    },
    {
      "epoch": 2.3530178972538223,
      "grad_norm": 1.7753868103027344,
      "learning_rate": 6.542475373061543e-06,
      "loss": 0.8133,
      "step": 8120
    },
    {
      "epoch": 2.3559162379537715,
      "grad_norm": 2.0454232692718506,
      "learning_rate": 6.5132156442016965e-06,
      "loss": 0.8222,
      "step": 8130
    },
    {
      "epoch": 2.3588145786537207,
      "grad_norm": 1.921226978302002,
      "learning_rate": 6.483955915341852e-06,
      "loss": 0.7943,
      "step": 8140
    },
    {
      "epoch": 2.36171291935367,
      "grad_norm": 1.8224294185638428,
      "learning_rate": 6.4546961864820055e-06,
      "loss": 0.8507,
      "step": 8150
    },
    {
      "epoch": 2.3646112600536195,
      "grad_norm": 1.8341566324234009,
      "learning_rate": 6.425436457622159e-06,
      "loss": 0.7611,
      "step": 8160
    },
    {
      "epoch": 2.3675096007535688,
      "grad_norm": 1.567777156829834,
      "learning_rate": 6.396176728762314e-06,
      "loss": 0.8226,
      "step": 8170
    },
    {
      "epoch": 2.370407941453518,
      "grad_norm": 1.9791436195373535,
      "learning_rate": 6.366916999902467e-06,
      "loss": 0.771,
      "step": 8180
    },
    {
      "epoch": 2.373306282153467,
      "grad_norm": 1.751860499382019,
      "learning_rate": 6.337657271042621e-06,
      "loss": 0.8136,
      "step": 8190
    },
    {
      "epoch": 2.3762046228534164,
      "grad_norm": 1.9040411710739136,
      "learning_rate": 6.308397542182776e-06,
      "loss": 0.8067,
      "step": 8200
    },
    {
      "epoch": 2.3791029635533656,
      "grad_norm": 1.7621041536331177,
      "learning_rate": 6.27913781332293e-06,
      "loss": 0.7609,
      "step": 8210
    },
    {
      "epoch": 2.382001304253315,
      "grad_norm": 1.7198487520217896,
      "learning_rate": 6.249878084463084e-06,
      "loss": 0.768,
      "step": 8220
    },
    {
      "epoch": 2.384899644953264,
      "grad_norm": 1.4797191619873047,
      "learning_rate": 6.220618355603238e-06,
      "loss": 0.795,
      "step": 8230
    },
    {
      "epoch": 2.3877979856532137,
      "grad_norm": 1.8497501611709595,
      "learning_rate": 6.191358626743392e-06,
      "loss": 0.7627,
      "step": 8240
    },
    {
      "epoch": 2.390696326353163,
      "grad_norm": 1.8487863540649414,
      "learning_rate": 6.162098897883546e-06,
      "loss": 0.8181,
      "step": 8250
    },
    {
      "epoch": 2.393594667053112,
      "grad_norm": 2.0099456310272217,
      "learning_rate": 6.132839169023701e-06,
      "loss": 0.7674,
      "step": 8260
    },
    {
      "epoch": 2.3964930077530613,
      "grad_norm": 1.6938666105270386,
      "learning_rate": 6.1035794401638546e-06,
      "loss": 0.8388,
      "step": 8270
    },
    {
      "epoch": 2.3993913484530105,
      "grad_norm": 1.9051768779754639,
      "learning_rate": 6.074319711304009e-06,
      "loss": 0.8311,
      "step": 8280
    },
    {
      "epoch": 2.40228968915296,
      "grad_norm": 1.7331442832946777,
      "learning_rate": 6.045059982444163e-06,
      "loss": 0.8674,
      "step": 8290
    },
    {
      "epoch": 2.4051880298529094,
      "grad_norm": 1.484811544418335,
      "learning_rate": 6.015800253584316e-06,
      "loss": 0.823,
      "step": 8300
    },
    {
      "epoch": 2.4080863705528586,
      "grad_norm": 1.5409866571426392,
      "learning_rate": 5.986540524724472e-06,
      "loss": 0.7958,
      "step": 8310
    },
    {
      "epoch": 2.4109847112528078,
      "grad_norm": 1.4100165367126465,
      "learning_rate": 5.957280795864625e-06,
      "loss": 0.7259,
      "step": 8320
    },
    {
      "epoch": 2.413883051952757,
      "grad_norm": 2.014096736907959,
      "learning_rate": 5.928021067004779e-06,
      "loss": 0.8364,
      "step": 8330
    },
    {
      "epoch": 2.416781392652706,
      "grad_norm": 2.459787368774414,
      "learning_rate": 5.898761338144934e-06,
      "loss": 0.8153,
      "step": 8340
    },
    {
      "epoch": 2.4196797333526554,
      "grad_norm": 1.7244303226470947,
      "learning_rate": 5.869501609285087e-06,
      "loss": 0.8073,
      "step": 8350
    },
    {
      "epoch": 2.422578074052605,
      "grad_norm": 1.5715826749801636,
      "learning_rate": 5.840241880425242e-06,
      "loss": 0.7798,
      "step": 8360
    },
    {
      "epoch": 2.4254764147525543,
      "grad_norm": 1.7402589321136475,
      "learning_rate": 5.810982151565396e-06,
      "loss": 0.7942,
      "step": 8370
    },
    {
      "epoch": 2.4283747554525035,
      "grad_norm": 1.9270434379577637,
      "learning_rate": 5.78172242270555e-06,
      "loss": 0.8365,
      "step": 8380
    },
    {
      "epoch": 2.4312730961524527,
      "grad_norm": 1.886460781097412,
      "learning_rate": 5.752462693845704e-06,
      "loss": 0.8158,
      "step": 8390
    },
    {
      "epoch": 2.434171436852402,
      "grad_norm": 1.8475532531738281,
      "learning_rate": 5.723202964985858e-06,
      "loss": 0.7923,
      "step": 8400
    },
    {
      "epoch": 2.437069777552351,
      "grad_norm": 1.4243881702423096,
      "learning_rate": 5.693943236126012e-06,
      "loss": 0.7822,
      "step": 8410
    },
    {
      "epoch": 2.4399681182523008,
      "grad_norm": 1.6102908849716187,
      "learning_rate": 5.664683507266166e-06,
      "loss": 0.7929,
      "step": 8420
    },
    {
      "epoch": 2.44286645895225,
      "grad_norm": 2.2724521160125732,
      "learning_rate": 5.635423778406321e-06,
      "loss": 0.8105,
      "step": 8430
    },
    {
      "epoch": 2.445764799652199,
      "grad_norm": 2.008500337600708,
      "learning_rate": 5.6061640495464745e-06,
      "loss": 0.7692,
      "step": 8440
    },
    {
      "epoch": 2.4486631403521484,
      "grad_norm": 1.7154499292373657,
      "learning_rate": 5.576904320686628e-06,
      "loss": 0.7894,
      "step": 8450
    },
    {
      "epoch": 2.4515614810520976,
      "grad_norm": 1.8132388591766357,
      "learning_rate": 5.547644591826783e-06,
      "loss": 0.7867,
      "step": 8460
    },
    {
      "epoch": 2.454459821752047,
      "grad_norm": 1.8599332571029663,
      "learning_rate": 5.518384862966937e-06,
      "loss": 0.8379,
      "step": 8470
    },
    {
      "epoch": 2.457358162451996,
      "grad_norm": 1.820002794265747,
      "learning_rate": 5.489125134107091e-06,
      "loss": 0.7571,
      "step": 8480
    },
    {
      "epoch": 2.4602565031519457,
      "grad_norm": 1.7036726474761963,
      "learning_rate": 5.459865405247245e-06,
      "loss": 0.7528,
      "step": 8490
    },
    {
      "epoch": 2.463154843851895,
      "grad_norm": 2.0404489040374756,
      "learning_rate": 5.430605676387399e-06,
      "loss": 0.8111,
      "step": 8500
    },
    {
      "epoch": 2.466053184551844,
      "grad_norm": 1.6350589990615845,
      "learning_rate": 5.401345947527553e-06,
      "loss": 0.8011,
      "step": 8510
    },
    {
      "epoch": 2.4689515252517933,
      "grad_norm": 1.6564781665802002,
      "learning_rate": 5.372086218667707e-06,
      "loss": 0.7697,
      "step": 8520
    },
    {
      "epoch": 2.4718498659517425,
      "grad_norm": 1.580283761024475,
      "learning_rate": 5.342826489807862e-06,
      "loss": 0.7723,
      "step": 8530
    },
    {
      "epoch": 2.474748206651692,
      "grad_norm": 2.034533977508545,
      "learning_rate": 5.313566760948015e-06,
      "loss": 0.8145,
      "step": 8540
    },
    {
      "epoch": 2.4776465473516414,
      "grad_norm": 1.8999948501586914,
      "learning_rate": 5.28430703208817e-06,
      "loss": 0.7776,
      "step": 8550
    },
    {
      "epoch": 2.4805448880515906,
      "grad_norm": 1.9180790185928345,
      "learning_rate": 5.2550473032283235e-06,
      "loss": 0.7754,
      "step": 8560
    },
    {
      "epoch": 2.4834432287515398,
      "grad_norm": 1.7273041009902954,
      "learning_rate": 5.225787574368477e-06,
      "loss": 0.8048,
      "step": 8570
    },
    {
      "epoch": 2.486341569451489,
      "grad_norm": 2.0073659420013428,
      "learning_rate": 5.196527845508632e-06,
      "loss": 0.7968,
      "step": 8580
    },
    {
      "epoch": 2.489239910151438,
      "grad_norm": 1.8194537162780762,
      "learning_rate": 5.167268116648786e-06,
      "loss": 0.7784,
      "step": 8590
    },
    {
      "epoch": 2.4921382508513874,
      "grad_norm": 1.77597177028656,
      "learning_rate": 5.13800838778894e-06,
      "loss": 0.8216,
      "step": 8600
    },
    {
      "epoch": 2.4950365915513366,
      "grad_norm": 1.9144107103347778,
      "learning_rate": 5.108748658929094e-06,
      "loss": 0.7498,
      "step": 8610
    },
    {
      "epoch": 2.4979349322512863,
      "grad_norm": 1.818861722946167,
      "learning_rate": 5.079488930069248e-06,
      "loss": 0.814,
      "step": 8620
    },
    {
      "epoch": 2.5008332729512355,
      "grad_norm": 2.174074649810791,
      "learning_rate": 5.050229201209402e-06,
      "loss": 0.8397,
      "step": 8630
    },
    {
      "epoch": 2.5037316136511847,
      "grad_norm": 1.7260127067565918,
      "learning_rate": 5.020969472349557e-06,
      "loss": 0.8041,
      "step": 8640
    },
    {
      "epoch": 2.506629954351134,
      "grad_norm": 2.14902925491333,
      "learning_rate": 4.991709743489711e-06,
      "loss": 0.8266,
      "step": 8650
    },
    {
      "epoch": 2.509528295051083,
      "grad_norm": 1.9443490505218506,
      "learning_rate": 4.962450014629864e-06,
      "loss": 0.7451,
      "step": 8660
    },
    {
      "epoch": 2.5124266357510328,
      "grad_norm": 1.9557271003723145,
      "learning_rate": 4.933190285770019e-06,
      "loss": 0.7318,
      "step": 8670
    },
    {
      "epoch": 2.515324976450982,
      "grad_norm": 1.9685710668563843,
      "learning_rate": 4.9039305569101725e-06,
      "loss": 0.8163,
      "step": 8680
    },
    {
      "epoch": 2.518223317150931,
      "grad_norm": 1.6682878732681274,
      "learning_rate": 4.874670828050326e-06,
      "loss": 0.8537,
      "step": 8690
    },
    {
      "epoch": 2.5211216578508804,
      "grad_norm": 1.8661333322525024,
      "learning_rate": 4.8454110991904816e-06,
      "loss": 0.8316,
      "step": 8700
    },
    {
      "epoch": 2.5240199985508296,
      "grad_norm": 1.8527559041976929,
      "learning_rate": 4.816151370330635e-06,
      "loss": 0.7957,
      "step": 8710
    },
    {
      "epoch": 2.526918339250779,
      "grad_norm": 1.5986055135726929,
      "learning_rate": 4.786891641470789e-06,
      "loss": 0.8177,
      "step": 8720
    },
    {
      "epoch": 2.529816679950728,
      "grad_norm": 1.775898814201355,
      "learning_rate": 4.757631912610943e-06,
      "loss": 0.7875,
      "step": 8730
    },
    {
      "epoch": 2.532715020650677,
      "grad_norm": 1.774922251701355,
      "learning_rate": 4.728372183751097e-06,
      "loss": 0.7795,
      "step": 8740
    },
    {
      "epoch": 2.535613361350627,
      "grad_norm": 1.568153977394104,
      "learning_rate": 4.6991124548912516e-06,
      "loss": 0.7506,
      "step": 8750
    },
    {
      "epoch": 2.538511702050576,
      "grad_norm": 1.6249738931655884,
      "learning_rate": 4.669852726031406e-06,
      "loss": 0.7898,
      "step": 8760
    },
    {
      "epoch": 2.5414100427505253,
      "grad_norm": 1.747087001800537,
      "learning_rate": 4.64059299717156e-06,
      "loss": 0.7952,
      "step": 8770
    },
    {
      "epoch": 2.5443083834504745,
      "grad_norm": 1.788835883140564,
      "learning_rate": 4.611333268311713e-06,
      "loss": 0.8501,
      "step": 8780
    },
    {
      "epoch": 2.547206724150424,
      "grad_norm": 1.656449556350708,
      "learning_rate": 4.582073539451868e-06,
      "loss": 0.8424,
      "step": 8790
    },
    {
      "epoch": 2.5501050648503734,
      "grad_norm": 1.7308248281478882,
      "learning_rate": 4.5528138105920216e-06,
      "loss": 0.8005,
      "step": 8800
    },
    {
      "epoch": 2.5530034055503226,
      "grad_norm": 1.8166561126708984,
      "learning_rate": 4.523554081732176e-06,
      "loss": 0.8146,
      "step": 8810
    },
    {
      "epoch": 2.5559017462502718,
      "grad_norm": 1.8868898153305054,
      "learning_rate": 4.494294352872331e-06,
      "loss": 0.7644,
      "step": 8820
    },
    {
      "epoch": 2.558800086950221,
      "grad_norm": 2.037041664123535,
      "learning_rate": 4.465034624012484e-06,
      "loss": 0.8104,
      "step": 8830
    },
    {
      "epoch": 2.56169842765017,
      "grad_norm": 2.110666513442993,
      "learning_rate": 4.435774895152638e-06,
      "loss": 0.7958,
      "step": 8840
    },
    {
      "epoch": 2.5645967683501194,
      "grad_norm": 1.838175892829895,
      "learning_rate": 4.4065151662927924e-06,
      "loss": 0.8123,
      "step": 8850
    },
    {
      "epoch": 2.5674951090500686,
      "grad_norm": 1.819381594657898,
      "learning_rate": 4.377255437432947e-06,
      "loss": 0.7675,
      "step": 8860
    },
    {
      "epoch": 2.5703934497500183,
      "grad_norm": 1.6737287044525146,
      "learning_rate": 4.347995708573101e-06,
      "loss": 0.8047,
      "step": 8870
    },
    {
      "epoch": 2.5732917904499675,
      "grad_norm": 1.9863090515136719,
      "learning_rate": 4.318735979713255e-06,
      "loss": 0.8348,
      "step": 8880
    },
    {
      "epoch": 2.5761901311499167,
      "grad_norm": 2.0914082527160645,
      "learning_rate": 4.289476250853409e-06,
      "loss": 0.8166,
      "step": 8890
    },
    {
      "epoch": 2.579088471849866,
      "grad_norm": 1.7834433317184448,
      "learning_rate": 4.2602165219935624e-06,
      "loss": 0.7821,
      "step": 8900
    },
    {
      "epoch": 2.581986812549815,
      "grad_norm": 2.020134210586548,
      "learning_rate": 4.230956793133717e-06,
      "loss": 0.781,
      "step": 8910
    },
    {
      "epoch": 2.5848851532497648,
      "grad_norm": 1.7714340686798096,
      "learning_rate": 4.2016970642738715e-06,
      "loss": 0.8289,
      "step": 8920
    },
    {
      "epoch": 2.587783493949714,
      "grad_norm": 1.8203942775726318,
      "learning_rate": 4.172437335414025e-06,
      "loss": 0.7637,
      "step": 8930
    },
    {
      "epoch": 2.590681834649663,
      "grad_norm": 1.7648597955703735,
      "learning_rate": 4.14317760655418e-06,
      "loss": 0.7599,
      "step": 8940
    },
    {
      "epoch": 2.5935801753496124,
      "grad_norm": 1.7094814777374268,
      "learning_rate": 4.113917877694333e-06,
      "loss": 0.728,
      "step": 8950
    },
    {
      "epoch": 2.5964785160495616,
      "grad_norm": 2.12361478805542,
      "learning_rate": 4.084658148834487e-06,
      "loss": 0.8256,
      "step": 8960
    },
    {
      "epoch": 2.599376856749511,
      "grad_norm": 1.7782366275787354,
      "learning_rate": 4.0553984199746415e-06,
      "loss": 0.8146,
      "step": 8970
    },
    {
      "epoch": 2.60227519744946,
      "grad_norm": 1.9516689777374268,
      "learning_rate": 4.026138691114796e-06,
      "loss": 0.7776,
      "step": 8980
    },
    {
      "epoch": 2.605173538149409,
      "grad_norm": 2.107851505279541,
      "learning_rate": 3.99687896225495e-06,
      "loss": 0.8199,
      "step": 8990
    },
    {
      "epoch": 2.608071878849359,
      "grad_norm": 1.9855600595474243,
      "learning_rate": 3.967619233395104e-06,
      "loss": 0.8032,
      "step": 9000
    },
    {
      "epoch": 2.610970219549308,
      "grad_norm": 1.4288341999053955,
      "learning_rate": 3.938359504535258e-06,
      "loss": 0.7698,
      "step": 9010
    },
    {
      "epoch": 2.6138685602492573,
      "grad_norm": 2.161349296569824,
      "learning_rate": 3.9090997756754115e-06,
      "loss": 0.7753,
      "step": 9020
    },
    {
      "epoch": 2.6167669009492065,
      "grad_norm": 1.7498632669448853,
      "learning_rate": 3.879840046815567e-06,
      "loss": 0.7686,
      "step": 9030
    },
    {
      "epoch": 2.6196652416491557,
      "grad_norm": 1.7475734949111938,
      "learning_rate": 3.8505803179557205e-06,
      "loss": 0.7901,
      "step": 9040
    },
    {
      "epoch": 2.6225635823491054,
      "grad_norm": 1.8636219501495361,
      "learning_rate": 3.821320589095874e-06,
      "loss": 0.7824,
      "step": 9050
    },
    {
      "epoch": 2.6254619230490546,
      "grad_norm": 1.6647690534591675,
      "learning_rate": 3.7920608602360287e-06,
      "loss": 0.8066,
      "step": 9060
    },
    {
      "epoch": 2.6283602637490038,
      "grad_norm": 1.5468580722808838,
      "learning_rate": 3.7628011313761828e-06,
      "loss": 0.8039,
      "step": 9070
    },
    {
      "epoch": 2.631258604448953,
      "grad_norm": 1.863183856010437,
      "learning_rate": 3.733541402516337e-06,
      "loss": 0.824,
      "step": 9080
    },
    {
      "epoch": 2.634156945148902,
      "grad_norm": 1.7031875848770142,
      "learning_rate": 3.704281673656491e-06,
      "loss": 0.78,
      "step": 9090
    },
    {
      "epoch": 2.6370552858488514,
      "grad_norm": 1.727840542793274,
      "learning_rate": 3.675021944796645e-06,
      "loss": 0.7866,
      "step": 9100
    },
    {
      "epoch": 2.6399536265488006,
      "grad_norm": 1.8204058408737183,
      "learning_rate": 3.645762215936799e-06,
      "loss": 0.8262,
      "step": 9110
    },
    {
      "epoch": 2.64285196724875,
      "grad_norm": 1.6540013551712036,
      "learning_rate": 3.616502487076953e-06,
      "loss": 0.7903,
      "step": 9120
    },
    {
      "epoch": 2.6457503079486995,
      "grad_norm": 1.6152008771896362,
      "learning_rate": 3.5872427582171073e-06,
      "loss": 0.817,
      "step": 9130
    },
    {
      "epoch": 2.6486486486486487,
      "grad_norm": 1.5525990724563599,
      "learning_rate": 3.5579830293572614e-06,
      "loss": 0.8119,
      "step": 9140
    },
    {
      "epoch": 2.651546989348598,
      "grad_norm": 1.8283042907714844,
      "learning_rate": 3.5287233004974155e-06,
      "loss": 0.8033,
      "step": 9150
    },
    {
      "epoch": 2.654445330048547,
      "grad_norm": 1.9039345979690552,
      "learning_rate": 3.4994635716375695e-06,
      "loss": 0.7867,
      "step": 9160
    },
    {
      "epoch": 2.6573436707484968,
      "grad_norm": 1.3142679929733276,
      "learning_rate": 3.4702038427777236e-06,
      "loss": 0.7533,
      "step": 9170
    },
    {
      "epoch": 2.660242011448446,
      "grad_norm": 1.7404433488845825,
      "learning_rate": 3.440944113917878e-06,
      "loss": 0.8164,
      "step": 9180
    },
    {
      "epoch": 2.663140352148395,
      "grad_norm": 1.9498614072799683,
      "learning_rate": 3.411684385058032e-06,
      "loss": 0.7999,
      "step": 9190
    },
    {
      "epoch": 2.6660386928483444,
      "grad_norm": 2.007570266723633,
      "learning_rate": 3.382424656198186e-06,
      "loss": 0.7811,
      "step": 9200
    },
    {
      "epoch": 2.6689370335482936,
      "grad_norm": 1.8536208868026733,
      "learning_rate": 3.3531649273383404e-06,
      "loss": 0.8531,
      "step": 9210
    },
    {
      "epoch": 2.671835374248243,
      "grad_norm": 1.7498247623443604,
      "learning_rate": 3.323905198478494e-06,
      "loss": 0.7645,
      "step": 9220
    },
    {
      "epoch": 2.674733714948192,
      "grad_norm": 1.6845979690551758,
      "learning_rate": 3.294645469618648e-06,
      "loss": 0.7707,
      "step": 9230
    },
    {
      "epoch": 2.677632055648141,
      "grad_norm": 1.7457013130187988,
      "learning_rate": 3.2653857407588027e-06,
      "loss": 0.8007,
      "step": 9240
    },
    {
      "epoch": 2.680530396348091,
      "grad_norm": 1.6746119260787964,
      "learning_rate": 3.2361260118989563e-06,
      "loss": 0.7835,
      "step": 9250
    },
    {
      "epoch": 2.68342873704804,
      "grad_norm": 1.6001123189926147,
      "learning_rate": 3.2068662830391104e-06,
      "loss": 0.7741,
      "step": 9260
    },
    {
      "epoch": 2.6863270777479893,
      "grad_norm": 1.6679717302322388,
      "learning_rate": 3.177606554179265e-06,
      "loss": 0.7929,
      "step": 9270
    },
    {
      "epoch": 2.6892254184479385,
      "grad_norm": 2.2668347358703613,
      "learning_rate": 3.1483468253194186e-06,
      "loss": 0.8115,
      "step": 9280
    },
    {
      "epoch": 2.6921237591478877,
      "grad_norm": 1.6358165740966797,
      "learning_rate": 3.1190870964595727e-06,
      "loss": 0.7944,
      "step": 9290
    },
    {
      "epoch": 2.6950220998478374,
      "grad_norm": 1.5554101467132568,
      "learning_rate": 3.089827367599727e-06,
      "loss": 0.7969,
      "step": 9300
    },
    {
      "epoch": 2.6979204405477866,
      "grad_norm": 2.4016618728637695,
      "learning_rate": 3.060567638739881e-06,
      "loss": 0.8097,
      "step": 9310
    },
    {
      "epoch": 2.700818781247736,
      "grad_norm": 1.8046226501464844,
      "learning_rate": 3.0313079098800354e-06,
      "loss": 0.7609,
      "step": 9320
    },
    {
      "epoch": 2.703717121947685,
      "grad_norm": 1.7672913074493408,
      "learning_rate": 3.0020481810201894e-06,
      "loss": 0.8391,
      "step": 9330
    },
    {
      "epoch": 2.706615462647634,
      "grad_norm": 1.6907461881637573,
      "learning_rate": 2.972788452160343e-06,
      "loss": 0.7794,
      "step": 9340
    },
    {
      "epoch": 2.7095138033475834,
      "grad_norm": 1.5240851640701294,
      "learning_rate": 2.9435287233004976e-06,
      "loss": 0.7554,
      "step": 9350
    },
    {
      "epoch": 2.7124121440475326,
      "grad_norm": 1.8728221654891968,
      "learning_rate": 2.9142689944406517e-06,
      "loss": 0.7686,
      "step": 9360
    },
    {
      "epoch": 2.715310484747482,
      "grad_norm": 1.9523096084594727,
      "learning_rate": 2.8850092655808054e-06,
      "loss": 0.811,
      "step": 9370
    },
    {
      "epoch": 2.7182088254474315,
      "grad_norm": 2.0112767219543457,
      "learning_rate": 2.85574953672096e-06,
      "loss": 0.8402,
      "step": 9380
    },
    {
      "epoch": 2.7211071661473807,
      "grad_norm": 2.0855631828308105,
      "learning_rate": 2.826489807861114e-06,
      "loss": 0.7855,
      "step": 9390
    },
    {
      "epoch": 2.72400550684733,
      "grad_norm": 1.727917194366455,
      "learning_rate": 2.7972300790012676e-06,
      "loss": 0.7983,
      "step": 9400
    },
    {
      "epoch": 2.726903847547279,
      "grad_norm": 1.9551056623458862,
      "learning_rate": 2.767970350141422e-06,
      "loss": 0.7977,
      "step": 9410
    },
    {
      "epoch": 2.7298021882472283,
      "grad_norm": 1.5652958154678345,
      "learning_rate": 2.7387106212815762e-06,
      "loss": 0.8036,
      "step": 9420
    },
    {
      "epoch": 2.732700528947178,
      "grad_norm": 1.7407348155975342,
      "learning_rate": 2.7094508924217303e-06,
      "loss": 0.7738,
      "step": 9430
    },
    {
      "epoch": 2.735598869647127,
      "grad_norm": 1.7199572324752808,
      "learning_rate": 2.6801911635618844e-06,
      "loss": 0.7792,
      "step": 9440
    },
    {
      "epoch": 2.7384972103470764,
      "grad_norm": 1.8868001699447632,
      "learning_rate": 2.6509314347020385e-06,
      "loss": 0.7939,
      "step": 9450
    },
    {
      "epoch": 2.7413955510470256,
      "grad_norm": 1.700895071029663,
      "learning_rate": 2.6216717058421926e-06,
      "loss": 0.7591,
      "step": 9460
    },
    {
      "epoch": 2.744293891746975,
      "grad_norm": 1.6928188800811768,
      "learning_rate": 2.5924119769823467e-06,
      "loss": 0.765,
      "step": 9470
    },
    {
      "epoch": 2.747192232446924,
      "grad_norm": 1.533440113067627,
      "learning_rate": 2.5631522481225007e-06,
      "loss": 0.7813,
      "step": 9480
    },
    {
      "epoch": 2.750090573146873,
      "grad_norm": 1.8492263555526733,
      "learning_rate": 2.533892519262655e-06,
      "loss": 0.7666,
      "step": 9490
    },
    {
      "epoch": 2.7529889138468224,
      "grad_norm": 1.790122628211975,
      "learning_rate": 2.504632790402809e-06,
      "loss": 0.767,
      "step": 9500
    },
    {
      "epoch": 2.755887254546772,
      "grad_norm": 1.9427448511123657,
      "learning_rate": 2.475373061542963e-06,
      "loss": 0.74,
      "step": 9510
    },
    {
      "epoch": 2.7587855952467213,
      "grad_norm": 1.850662112236023,
      "learning_rate": 2.4461133326831175e-06,
      "loss": 0.7687,
      "step": 9520
    },
    {
      "epoch": 2.7616839359466705,
      "grad_norm": 1.8059899806976318,
      "learning_rate": 2.416853603823271e-06,
      "loss": 0.7992,
      "step": 9530
    },
    {
      "epoch": 2.7645822766466197,
      "grad_norm": 1.6672329902648926,
      "learning_rate": 2.3875938749634253e-06,
      "loss": 0.8155,
      "step": 9540
    },
    {
      "epoch": 2.7674806173465694,
      "grad_norm": 1.9962708950042725,
      "learning_rate": 2.3583341461035798e-06,
      "loss": 0.8316,
      "step": 9550
    },
    {
      "epoch": 2.7703789580465186,
      "grad_norm": 1.7160168886184692,
      "learning_rate": 2.3290744172437334e-06,
      "loss": 0.7905,
      "step": 9560
    },
    {
      "epoch": 2.773277298746468,
      "grad_norm": 1.7710872888565063,
      "learning_rate": 2.299814688383888e-06,
      "loss": 0.8479,
      "step": 9570
    },
    {
      "epoch": 2.776175639446417,
      "grad_norm": 1.6981184482574463,
      "learning_rate": 2.270554959524042e-06,
      "loss": 0.7918,
      "step": 9580
    },
    {
      "epoch": 2.779073980146366,
      "grad_norm": 1.8771785497665405,
      "learning_rate": 2.2412952306641957e-06,
      "loss": 0.8349,
      "step": 9590
    },
    {
      "epoch": 2.7819723208463154,
      "grad_norm": 2.0881826877593994,
      "learning_rate": 2.21203550180435e-06,
      "loss": 0.7978,
      "step": 9600
    },
    {
      "epoch": 2.7848706615462646,
      "grad_norm": 1.5564846992492676,
      "learning_rate": 2.1827757729445043e-06,
      "loss": 0.8249,
      "step": 9610
    },
    {
      "epoch": 2.787769002246214,
      "grad_norm": 1.8409279584884644,
      "learning_rate": 2.153516044084658e-06,
      "loss": 0.7555,
      "step": 9620
    },
    {
      "epoch": 2.7906673429461635,
      "grad_norm": 2.038074493408203,
      "learning_rate": 2.1242563152248125e-06,
      "loss": 0.8103,
      "step": 9630
    },
    {
      "epoch": 2.7935656836461127,
      "grad_norm": 1.5004807710647583,
      "learning_rate": 2.0949965863649665e-06,
      "loss": 0.8471,
      "step": 9640
    },
    {
      "epoch": 2.796464024346062,
      "grad_norm": 1.741165041923523,
      "learning_rate": 2.06573685750512e-06,
      "loss": 0.7977,
      "step": 9650
    },
    {
      "epoch": 2.799362365046011,
      "grad_norm": 2.1986405849456787,
      "learning_rate": 2.0364771286452747e-06,
      "loss": 0.8088,
      "step": 9660
    },
    {
      "epoch": 2.8022607057459603,
      "grad_norm": 2.1113367080688477,
      "learning_rate": 2.007217399785429e-06,
      "loss": 0.7917,
      "step": 9670
    },
    {
      "epoch": 2.80515904644591,
      "grad_norm": 1.7547074556350708,
      "learning_rate": 1.977957670925583e-06,
      "loss": 0.8413,
      "step": 9680
    },
    {
      "epoch": 2.808057387145859,
      "grad_norm": 2.1132314205169678,
      "learning_rate": 1.948697942065737e-06,
      "loss": 0.7763,
      "step": 9690
    },
    {
      "epoch": 2.8109557278458084,
      "grad_norm": 1.769264817237854,
      "learning_rate": 1.919438213205891e-06,
      "loss": 0.8065,
      "step": 9700
    },
    {
      "epoch": 2.8138540685457576,
      "grad_norm": 2.2121498584747314,
      "learning_rate": 1.890178484346045e-06,
      "loss": 0.8762,
      "step": 9710
    },
    {
      "epoch": 2.816752409245707,
      "grad_norm": 1.8204619884490967,
      "learning_rate": 1.8609187554861992e-06,
      "loss": 0.812,
      "step": 9720
    },
    {
      "epoch": 2.819650749945656,
      "grad_norm": 2.0178489685058594,
      "learning_rate": 1.8316590266263533e-06,
      "loss": 0.8145,
      "step": 9730
    },
    {
      "epoch": 2.822549090645605,
      "grad_norm": 1.8312357664108276,
      "learning_rate": 1.8023992977665074e-06,
      "loss": 0.7562,
      "step": 9740
    },
    {
      "epoch": 2.8254474313455544,
      "grad_norm": 1.6193314790725708,
      "learning_rate": 1.7731395689066615e-06,
      "loss": 0.773,
      "step": 9750
    },
    {
      "epoch": 2.828345772045504,
      "grad_norm": 1.776340365409851,
      "learning_rate": 1.7438798400468156e-06,
      "loss": 0.7796,
      "step": 9760
    },
    {
      "epoch": 2.8312441127454533,
      "grad_norm": 2.045384645462036,
      "learning_rate": 1.7146201111869697e-06,
      "loss": 0.7882,
      "step": 9770
    },
    {
      "epoch": 2.8341424534454025,
      "grad_norm": 1.9688247442245483,
      "learning_rate": 1.6853603823271238e-06,
      "loss": 0.7776,
      "step": 9780
    },
    {
      "epoch": 2.8370407941453517,
      "grad_norm": 1.930256962776184,
      "learning_rate": 1.6561006534672778e-06,
      "loss": 0.7771,
      "step": 9790
    },
    {
      "epoch": 2.839939134845301,
      "grad_norm": 1.7483731508255005,
      "learning_rate": 1.6268409246074321e-06,
      "loss": 0.7949,
      "step": 9800
    },
    {
      "epoch": 2.8428374755452506,
      "grad_norm": 1.8235665559768677,
      "learning_rate": 1.597581195747586e-06,
      "loss": 0.8,
      "step": 9810
    },
    {
      "epoch": 2.8457358162452,
      "grad_norm": 1.6846411228179932,
      "learning_rate": 1.5683214668877401e-06,
      "loss": 0.8199,
      "step": 9820
    },
    {
      "epoch": 2.848634156945149,
      "grad_norm": 2.1643283367156982,
      "learning_rate": 1.5390617380278944e-06,
      "loss": 0.8077,
      "step": 9830
    },
    {
      "epoch": 2.851532497645098,
      "grad_norm": 1.8038699626922607,
      "learning_rate": 1.5098020091680485e-06,
      "loss": 0.7666,
      "step": 9840
    },
    {
      "epoch": 2.8544308383450474,
      "grad_norm": 1.9079856872558594,
      "learning_rate": 1.4805422803082024e-06,
      "loss": 0.8221,
      "step": 9850
    },
    {
      "epoch": 2.8573291790449966,
      "grad_norm": 2.034492015838623,
      "learning_rate": 1.4512825514483567e-06,
      "loss": 0.7807,
      "step": 9860
    },
    {
      "epoch": 2.860227519744946,
      "grad_norm": 1.726387858390808,
      "learning_rate": 1.4220228225885108e-06,
      "loss": 0.7436,
      "step": 9870
    },
    {
      "epoch": 2.863125860444895,
      "grad_norm": 1.7810168266296387,
      "learning_rate": 1.3927630937286646e-06,
      "loss": 0.7876,
      "step": 9880
    },
    {
      "epoch": 2.8660242011448447,
      "grad_norm": 1.907565712928772,
      "learning_rate": 1.363503364868819e-06,
      "loss": 0.7821,
      "step": 9890
    },
    {
      "epoch": 2.868922541844794,
      "grad_norm": 2.061994791030884,
      "learning_rate": 1.334243636008973e-06,
      "loss": 0.8123,
      "step": 9900
    },
    {
      "epoch": 2.871820882544743,
      "grad_norm": 2.1050453186035156,
      "learning_rate": 1.3049839071491273e-06,
      "loss": 0.778,
      "step": 9910
    },
    {
      "epoch": 2.8747192232446923,
      "grad_norm": 1.6092778444290161,
      "learning_rate": 1.2757241782892812e-06,
      "loss": 0.7978,
      "step": 9920
    },
    {
      "epoch": 2.877617563944642,
      "grad_norm": 1.8800511360168457,
      "learning_rate": 1.2464644494294353e-06,
      "loss": 0.7983,
      "step": 9930
    },
    {
      "epoch": 2.880515904644591,
      "grad_norm": 1.4117463827133179,
      "learning_rate": 1.2172047205695896e-06,
      "loss": 0.7526,
      "step": 9940
    },
    {
      "epoch": 2.8834142453445404,
      "grad_norm": 1.5632601976394653,
      "learning_rate": 1.1879449917097434e-06,
      "loss": 0.7942,
      "step": 9950
    },
    {
      "epoch": 2.8863125860444896,
      "grad_norm": 1.6290919780731201,
      "learning_rate": 1.1586852628498975e-06,
      "loss": 0.7838,
      "step": 9960
    },
    {
      "epoch": 2.889210926744439,
      "grad_norm": 1.6707639694213867,
      "learning_rate": 1.1294255339900518e-06,
      "loss": 0.794,
      "step": 9970
    },
    {
      "epoch": 2.892109267444388,
      "grad_norm": 2.029572010040283,
      "learning_rate": 1.100165805130206e-06,
      "loss": 0.8185,
      "step": 9980
    },
    {
      "epoch": 2.895007608144337,
      "grad_norm": 2.343855381011963,
      "learning_rate": 1.0709060762703598e-06,
      "loss": 0.8058,
      "step": 9990
    },
    {
      "epoch": 2.8979059488442864,
      "grad_norm": 1.7061867713928223,
      "learning_rate": 1.041646347410514e-06,
      "loss": 0.7708,
      "step": 10000
    },
    {
      "epoch": 2.900804289544236,
      "grad_norm": 1.9227294921875,
      "learning_rate": 1.0123866185506682e-06,
      "loss": 0.7859,
      "step": 10010
    },
    {
      "epoch": 2.9037026302441853,
      "grad_norm": 1.8226938247680664,
      "learning_rate": 9.83126889690822e-07,
      "loss": 0.8371,
      "step": 10020
    },
    {
      "epoch": 2.9066009709441345,
      "grad_norm": 1.891126275062561,
      "learning_rate": 9.538671608309764e-07,
      "loss": 0.7682,
      "step": 10030
    },
    {
      "epoch": 2.9094993116440837,
      "grad_norm": 1.8180471658706665,
      "learning_rate": 9.246074319711304e-07,
      "loss": 0.7842,
      "step": 10040
    },
    {
      "epoch": 2.912397652344033,
      "grad_norm": 1.67965829372406,
      "learning_rate": 8.953477031112845e-07,
      "loss": 0.796,
      "step": 10050
    },
    {
      "epoch": 2.9152959930439826,
      "grad_norm": 1.8143454790115356,
      "learning_rate": 8.660879742514386e-07,
      "loss": 0.8104,
      "step": 10060
    },
    {
      "epoch": 2.918194333743932,
      "grad_norm": 1.8490571975708008,
      "learning_rate": 8.368282453915927e-07,
      "loss": 0.7634,
      "step": 10070
    },
    {
      "epoch": 2.921092674443881,
      "grad_norm": 2.134320020675659,
      "learning_rate": 8.075685165317469e-07,
      "loss": 0.7878,
      "step": 10080
    },
    {
      "epoch": 2.92399101514383,
      "grad_norm": 1.8905552625656128,
      "learning_rate": 7.78308787671901e-07,
      "loss": 0.8415,
      "step": 10090
    },
    {
      "epoch": 2.9268893558437794,
      "grad_norm": 2.006852865219116,
      "learning_rate": 7.49049058812055e-07,
      "loss": 0.8027,
      "step": 10100
    },
    {
      "epoch": 2.9297876965437286,
      "grad_norm": 1.7065216302871704,
      "learning_rate": 7.197893299522092e-07,
      "loss": 0.8003,
      "step": 10110
    },
    {
      "epoch": 2.932686037243678,
      "grad_norm": 1.9282772541046143,
      "learning_rate": 6.905296010923632e-07,
      "loss": 0.7755,
      "step": 10120
    },
    {
      "epoch": 2.935584377943627,
      "grad_norm": 1.6953338384628296,
      "learning_rate": 6.612698722325173e-07,
      "loss": 0.8143,
      "step": 10130
    },
    {
      "epoch": 2.9384827186435767,
      "grad_norm": 1.7176063060760498,
      "learning_rate": 6.320101433726714e-07,
      "loss": 0.7935,
      "step": 10140
    },
    {
      "epoch": 2.941381059343526,
      "grad_norm": 1.7198935747146606,
      "learning_rate": 6.027504145128256e-07,
      "loss": 0.7813,
      "step": 10150
    },
    {
      "epoch": 2.944279400043475,
      "grad_norm": 1.8119585514068604,
      "learning_rate": 5.734906856529796e-07,
      "loss": 0.824,
      "step": 10160
    },
    {
      "epoch": 2.9471777407434243,
      "grad_norm": 1.5954276323318481,
      "learning_rate": 5.442309567931337e-07,
      "loss": 0.7554,
      "step": 10170
    },
    {
      "epoch": 2.9500760814433735,
      "grad_norm": 1.9471969604492188,
      "learning_rate": 5.149712279332879e-07,
      "loss": 0.8183,
      "step": 10180
    },
    {
      "epoch": 2.952974422143323,
      "grad_norm": 1.587199330329895,
      "learning_rate": 4.857114990734418e-07,
      "loss": 0.7644,
      "step": 10190
    },
    {
      "epoch": 2.9558727628432724,
      "grad_norm": 1.9379390478134155,
      "learning_rate": 4.5645177021359604e-07,
      "loss": 0.8634,
      "step": 10200
    },
    {
      "epoch": 2.9587711035432216,
      "grad_norm": 1.7748210430145264,
      "learning_rate": 4.271920413537501e-07,
      "loss": 0.8301,
      "step": 10210
    },
    {
      "epoch": 2.961669444243171,
      "grad_norm": 1.9231797456741333,
      "learning_rate": 3.979323124939042e-07,
      "loss": 0.7755,
      "step": 10220
    },
    {
      "epoch": 2.96456778494312,
      "grad_norm": 1.66973078250885,
      "learning_rate": 3.6867258363405835e-07,
      "loss": 0.7748,
      "step": 10230
    },
    {
      "epoch": 2.967466125643069,
      "grad_norm": 1.6645779609680176,
      "learning_rate": 3.3941285477421244e-07,
      "loss": 0.8492,
      "step": 10240
    },
    {
      "epoch": 2.9703644663430184,
      "grad_norm": 1.8186150789260864,
      "learning_rate": 3.101531259143665e-07,
      "loss": 0.7971,
      "step": 10250
    },
    {
      "epoch": 2.9732628070429676,
      "grad_norm": 1.6417038440704346,
      "learning_rate": 2.8089339705452066e-07,
      "loss": 0.8516,
      "step": 10260
    },
    {
      "epoch": 2.9761611477429173,
      "grad_norm": 1.6665585041046143,
      "learning_rate": 2.5163366819467475e-07,
      "loss": 0.8232,
      "step": 10270
    },
    {
      "epoch": 2.9790594884428665,
      "grad_norm": 1.7159855365753174,
      "learning_rate": 2.2237393933482884e-07,
      "loss": 0.8566,
      "step": 10280
    },
    {
      "epoch": 2.9819578291428157,
      "grad_norm": 1.7916271686553955,
      "learning_rate": 1.9311421047498295e-07,
      "loss": 0.768,
      "step": 10290
    },
    {
      "epoch": 2.984856169842765,
      "grad_norm": 2.0254454612731934,
      "learning_rate": 1.6385448161513704e-07,
      "loss": 0.7773,
      "step": 10300
    },
    {
      "epoch": 2.9877545105427146,
      "grad_norm": 1.7038676738739014,
      "learning_rate": 1.3459475275529112e-07,
      "loss": 0.7584,
      "step": 10310
    },
    {
      "epoch": 2.990652851242664,
      "grad_norm": 1.7040413618087769,
      "learning_rate": 1.0533502389544524e-07,
      "loss": 0.7997,
      "step": 10320
    },
    {
      "epoch": 2.993551191942613,
      "grad_norm": 1.8922111988067627,
      "learning_rate": 7.607529503559935e-08,
      "loss": 0.784,
      "step": 10330
    },
    {
      "epoch": 2.996449532642562,
      "grad_norm": 2.3639309406280518,
      "learning_rate": 4.6815566175753435e-08,
      "loss": 0.8369,
      "step": 10340
    },
    {
      "epoch": 2.9993478733425114,
      "grad_norm": 1.7335094213485718,
      "learning_rate": 1.755583731590754e-08,
      "loss": 0.7979,
      "step": 10350
    }
  ],
  "logging_steps": 10,
  "max_steps": 10353,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4138020118449357e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
