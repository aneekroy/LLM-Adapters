{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.983159582999198,
  "eval_steps": 500,
  "global_step": 37200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0008019246190858059,
      "grad_norm": 0.4150756895542145,
      "learning_rate": 2.7e-06,
      "loss": 1.1769,
      "step": 10
    },
    {
      "epoch": 0.0016038492381716118,
      "grad_norm": 0.52243572473526,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 1.159,
      "step": 20
    },
    {
      "epoch": 0.0024057738572574178,
      "grad_norm": 0.5157697796821594,
      "learning_rate": 8.7e-06,
      "loss": 1.1091,
      "step": 30
    },
    {
      "epoch": 0.0032076984763432237,
      "grad_norm": 0.49532097578048706,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 1.1103,
      "step": 40
    },
    {
      "epoch": 0.00400962309542903,
      "grad_norm": 0.542272686958313,
      "learning_rate": 1.47e-05,
      "loss": 1.0985,
      "step": 50
    },
    {
      "epoch": 0.0048115477145148355,
      "grad_norm": 0.49381041526794434,
      "learning_rate": 1.77e-05,
      "loss": 1.0987,
      "step": 60
    },
    {
      "epoch": 0.0056134723336006415,
      "grad_norm": 0.5171446800231934,
      "learning_rate": 2.07e-05,
      "loss": 1.0385,
      "step": 70
    },
    {
      "epoch": 0.006415396952686447,
      "grad_norm": 0.6460515856742859,
      "learning_rate": 2.37e-05,
      "loss": 1.0367,
      "step": 80
    },
    {
      "epoch": 0.007217321571772253,
      "grad_norm": 0.6793761849403381,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.9624,
      "step": 90
    },
    {
      "epoch": 0.00801924619085806,
      "grad_norm": 0.928916335105896,
      "learning_rate": 2.97e-05,
      "loss": 1.0602,
      "step": 100
    },
    {
      "epoch": 0.008821170809943865,
      "grad_norm": 0.8819836974143982,
      "learning_rate": 2.999276333422675e-05,
      "loss": 0.9075,
      "step": 110
    },
    {
      "epoch": 0.009623095429029671,
      "grad_norm": 0.9083362817764282,
      "learning_rate": 2.998472259447869e-05,
      "loss": 0.9429,
      "step": 120
    },
    {
      "epoch": 0.010425020048115477,
      "grad_norm": 0.6408305168151855,
      "learning_rate": 2.997668185473064e-05,
      "loss": 0.764,
      "step": 130
    },
    {
      "epoch": 0.011226944667201283,
      "grad_norm": 0.6254210472106934,
      "learning_rate": 2.996864111498258e-05,
      "loss": 0.7677,
      "step": 140
    },
    {
      "epoch": 0.012028869286287089,
      "grad_norm": 0.7190901041030884,
      "learning_rate": 2.9960600375234524e-05,
      "loss": 0.7706,
      "step": 150
    },
    {
      "epoch": 0.012830793905372895,
      "grad_norm": 0.8051023483276367,
      "learning_rate": 2.9952559635486465e-05,
      "loss": 0.786,
      "step": 160
    },
    {
      "epoch": 0.0136327185244587,
      "grad_norm": 0.6933835744857788,
      "learning_rate": 2.9944518895738406e-05,
      "loss": 0.7818,
      "step": 170
    },
    {
      "epoch": 0.014434643143544507,
      "grad_norm": 0.5823143124580383,
      "learning_rate": 2.9936478155990354e-05,
      "loss": 0.7037,
      "step": 180
    },
    {
      "epoch": 0.015236567762630313,
      "grad_norm": 0.6339190602302551,
      "learning_rate": 2.9928437416242294e-05,
      "loss": 0.7767,
      "step": 190
    },
    {
      "epoch": 0.01603849238171612,
      "grad_norm": 0.9598032832145691,
      "learning_rate": 2.992039667649424e-05,
      "loss": 0.794,
      "step": 200
    },
    {
      "epoch": 0.016840417000801924,
      "grad_norm": 0.6425575613975525,
      "learning_rate": 2.991235593674618e-05,
      "loss": 0.6968,
      "step": 210
    },
    {
      "epoch": 0.01764234161988773,
      "grad_norm": 0.7948846817016602,
      "learning_rate": 2.9904315196998124e-05,
      "loss": 0.7326,
      "step": 220
    },
    {
      "epoch": 0.018444266238973536,
      "grad_norm": 0.7649801969528198,
      "learning_rate": 2.9896274457250068e-05,
      "loss": 0.7522,
      "step": 230
    },
    {
      "epoch": 0.019246190858059342,
      "grad_norm": 0.7827355265617371,
      "learning_rate": 2.9888233717502013e-05,
      "loss": 0.6641,
      "step": 240
    },
    {
      "epoch": 0.020048115477145148,
      "grad_norm": 0.9208796620368958,
      "learning_rate": 2.9880192977753953e-05,
      "loss": 0.7837,
      "step": 250
    },
    {
      "epoch": 0.020850040096230954,
      "grad_norm": 0.8324866890907288,
      "learning_rate": 2.9872152238005898e-05,
      "loss": 0.712,
      "step": 260
    },
    {
      "epoch": 0.02165196471531676,
      "grad_norm": 0.890400230884552,
      "learning_rate": 2.986411149825784e-05,
      "loss": 0.7211,
      "step": 270
    },
    {
      "epoch": 0.022453889334402566,
      "grad_norm": 0.8571463823318481,
      "learning_rate": 2.9856070758509786e-05,
      "loss": 0.7673,
      "step": 280
    },
    {
      "epoch": 0.023255813953488372,
      "grad_norm": 0.9528970718383789,
      "learning_rate": 2.9848030018761727e-05,
      "loss": 0.7895,
      "step": 290
    },
    {
      "epoch": 0.024057738572574178,
      "grad_norm": 0.834936261177063,
      "learning_rate": 2.9839989279013668e-05,
      "loss": 0.6994,
      "step": 300
    },
    {
      "epoch": 0.024859663191659984,
      "grad_norm": 0.803431510925293,
      "learning_rate": 2.9831948539265612e-05,
      "loss": 0.6853,
      "step": 310
    },
    {
      "epoch": 0.02566158781074579,
      "grad_norm": 0.8546168804168701,
      "learning_rate": 2.9823907799517557e-05,
      "loss": 0.768,
      "step": 320
    },
    {
      "epoch": 0.026463512429831595,
      "grad_norm": 0.9440674781799316,
      "learning_rate": 2.98158670597695e-05,
      "loss": 0.7774,
      "step": 330
    },
    {
      "epoch": 0.0272654370489174,
      "grad_norm": 0.7645198106765747,
      "learning_rate": 2.9807826320021442e-05,
      "loss": 0.7489,
      "step": 340
    },
    {
      "epoch": 0.028067361668003207,
      "grad_norm": 0.8236692547798157,
      "learning_rate": 2.9799785580273386e-05,
      "loss": 0.6968,
      "step": 350
    },
    {
      "epoch": 0.028869286287089013,
      "grad_norm": 0.8994466066360474,
      "learning_rate": 2.9791744840525327e-05,
      "loss": 0.6968,
      "step": 360
    },
    {
      "epoch": 0.02967121090617482,
      "grad_norm": 0.9039525985717773,
      "learning_rate": 2.9783704100777275e-05,
      "loss": 0.7725,
      "step": 370
    },
    {
      "epoch": 0.030473135525260625,
      "grad_norm": 0.8437119126319885,
      "learning_rate": 2.9775663361029216e-05,
      "loss": 0.7927,
      "step": 380
    },
    {
      "epoch": 0.03127506014434643,
      "grad_norm": 1.0214049816131592,
      "learning_rate": 2.976762262128116e-05,
      "loss": 0.8277,
      "step": 390
    },
    {
      "epoch": 0.03207698476343224,
      "grad_norm": 0.8267617225646973,
      "learning_rate": 2.97595818815331e-05,
      "loss": 0.7523,
      "step": 400
    },
    {
      "epoch": 0.03287890938251804,
      "grad_norm": 0.9444456100463867,
      "learning_rate": 2.9751541141785045e-05,
      "loss": 0.7871,
      "step": 410
    },
    {
      "epoch": 0.03368083400160385,
      "grad_norm": 0.9377192258834839,
      "learning_rate": 2.974350040203699e-05,
      "loss": 0.7493,
      "step": 420
    },
    {
      "epoch": 0.034482758620689655,
      "grad_norm": 0.7436213493347168,
      "learning_rate": 2.973545966228893e-05,
      "loss": 0.7748,
      "step": 430
    },
    {
      "epoch": 0.03528468323977546,
      "grad_norm": 0.9615027904510498,
      "learning_rate": 2.9727418922540875e-05,
      "loss": 0.7146,
      "step": 440
    },
    {
      "epoch": 0.03608660785886127,
      "grad_norm": 0.9137765169143677,
      "learning_rate": 2.9719378182792815e-05,
      "loss": 0.8054,
      "step": 450
    },
    {
      "epoch": 0.03688853247794707,
      "grad_norm": 0.9460374712944031,
      "learning_rate": 2.971133744304476e-05,
      "loss": 0.6672,
      "step": 460
    },
    {
      "epoch": 0.03769045709703288,
      "grad_norm": 0.8545435070991516,
      "learning_rate": 2.9703296703296704e-05,
      "loss": 0.7244,
      "step": 470
    },
    {
      "epoch": 0.038492381716118684,
      "grad_norm": 0.8879959583282471,
      "learning_rate": 2.969525596354865e-05,
      "loss": 0.8201,
      "step": 480
    },
    {
      "epoch": 0.03929430633520449,
      "grad_norm": 0.8452432751655579,
      "learning_rate": 2.968721522380059e-05,
      "loss": 0.7099,
      "step": 490
    },
    {
      "epoch": 0.040096230954290296,
      "grad_norm": 0.7458198666572571,
      "learning_rate": 2.9679174484052534e-05,
      "loss": 0.7243,
      "step": 500
    },
    {
      "epoch": 0.0408981555733761,
      "grad_norm": 0.7678550481796265,
      "learning_rate": 2.9671133744304478e-05,
      "loss": 0.7128,
      "step": 510
    },
    {
      "epoch": 0.04170008019246191,
      "grad_norm": 0.8461776971817017,
      "learning_rate": 2.9663093004556422e-05,
      "loss": 0.7063,
      "step": 520
    },
    {
      "epoch": 0.042502004811547714,
      "grad_norm": 0.8375746011734009,
      "learning_rate": 2.9655052264808363e-05,
      "loss": 0.7799,
      "step": 530
    },
    {
      "epoch": 0.04330392943063352,
      "grad_norm": 1.01679527759552,
      "learning_rate": 2.9647011525060307e-05,
      "loss": 0.7974,
      "step": 540
    },
    {
      "epoch": 0.044105854049719326,
      "grad_norm": 0.9922707080841064,
      "learning_rate": 2.9638970785312248e-05,
      "loss": 0.6831,
      "step": 550
    },
    {
      "epoch": 0.04490777866880513,
      "grad_norm": 1.0225942134857178,
      "learning_rate": 2.9630930045564193e-05,
      "loss": 0.7901,
      "step": 560
    },
    {
      "epoch": 0.04570970328789094,
      "grad_norm": 1.0071187019348145,
      "learning_rate": 2.9622889305816137e-05,
      "loss": 0.7253,
      "step": 570
    },
    {
      "epoch": 0.046511627906976744,
      "grad_norm": 1.087259292602539,
      "learning_rate": 2.9614848566068078e-05,
      "loss": 0.7718,
      "step": 580
    },
    {
      "epoch": 0.04731355252606255,
      "grad_norm": 1.0318697690963745,
      "learning_rate": 2.9606807826320022e-05,
      "loss": 0.7452,
      "step": 590
    },
    {
      "epoch": 0.048115477145148355,
      "grad_norm": 1.0039955377578735,
      "learning_rate": 2.9598767086571963e-05,
      "loss": 0.7234,
      "step": 600
    },
    {
      "epoch": 0.04891740176423416,
      "grad_norm": 0.9428607821464539,
      "learning_rate": 2.959072634682391e-05,
      "loss": 0.7901,
      "step": 610
    },
    {
      "epoch": 0.04971932638331997,
      "grad_norm": 1.1390825510025024,
      "learning_rate": 2.958268560707585e-05,
      "loss": 0.7625,
      "step": 620
    },
    {
      "epoch": 0.05052125100240577,
      "grad_norm": 0.9553754329681396,
      "learning_rate": 2.9574644867327796e-05,
      "loss": 0.7063,
      "step": 630
    },
    {
      "epoch": 0.05132317562149158,
      "grad_norm": 1.0075968503952026,
      "learning_rate": 2.9566604127579737e-05,
      "loss": 0.6972,
      "step": 640
    },
    {
      "epoch": 0.052125100240577385,
      "grad_norm": 1.1442655324935913,
      "learning_rate": 2.955856338783168e-05,
      "loss": 0.6987,
      "step": 650
    },
    {
      "epoch": 0.05292702485966319,
      "grad_norm": 1.0053627490997314,
      "learning_rate": 2.9550522648083625e-05,
      "loss": 0.7812,
      "step": 660
    },
    {
      "epoch": 0.053728949478749,
      "grad_norm": 0.9883642792701721,
      "learning_rate": 2.954248190833557e-05,
      "loss": 0.7569,
      "step": 670
    },
    {
      "epoch": 0.0545308740978348,
      "grad_norm": 0.983532190322876,
      "learning_rate": 2.953444116858751e-05,
      "loss": 0.7249,
      "step": 680
    },
    {
      "epoch": 0.05533279871692061,
      "grad_norm": 0.9141905307769775,
      "learning_rate": 2.952640042883945e-05,
      "loss": 0.7584,
      "step": 690
    },
    {
      "epoch": 0.056134723336006415,
      "grad_norm": 0.9624403715133667,
      "learning_rate": 2.95183596890914e-05,
      "loss": 0.7344,
      "step": 700
    },
    {
      "epoch": 0.05693664795509222,
      "grad_norm": 1.0677217245101929,
      "learning_rate": 2.951031894934334e-05,
      "loss": 0.756,
      "step": 710
    },
    {
      "epoch": 0.057738572574178026,
      "grad_norm": 0.8812543749809265,
      "learning_rate": 2.9502278209595284e-05,
      "loss": 0.721,
      "step": 720
    },
    {
      "epoch": 0.05854049719326383,
      "grad_norm": 0.998536229133606,
      "learning_rate": 2.9494237469847225e-05,
      "loss": 0.7645,
      "step": 730
    },
    {
      "epoch": 0.05934242181234964,
      "grad_norm": 1.3698025941848755,
      "learning_rate": 2.948619673009917e-05,
      "loss": 0.7361,
      "step": 740
    },
    {
      "epoch": 0.060144346431435444,
      "grad_norm": 0.8830180168151855,
      "learning_rate": 2.9478155990351114e-05,
      "loss": 0.6333,
      "step": 750
    },
    {
      "epoch": 0.06094627105052125,
      "grad_norm": 0.8728967905044556,
      "learning_rate": 2.9470115250603058e-05,
      "loss": 0.7011,
      "step": 760
    },
    {
      "epoch": 0.061748195669607056,
      "grad_norm": 0.8373199105262756,
      "learning_rate": 2.9462074510855e-05,
      "loss": 0.6657,
      "step": 770
    },
    {
      "epoch": 0.06255012028869286,
      "grad_norm": 1.1377276182174683,
      "learning_rate": 2.9454033771106943e-05,
      "loss": 0.7495,
      "step": 780
    },
    {
      "epoch": 0.06335204490777867,
      "grad_norm": 1.0665040016174316,
      "learning_rate": 2.9445993031358884e-05,
      "loss": 0.7133,
      "step": 790
    },
    {
      "epoch": 0.06415396952686447,
      "grad_norm": 1.214994192123413,
      "learning_rate": 2.9437952291610832e-05,
      "loss": 0.7461,
      "step": 800
    },
    {
      "epoch": 0.06495589414595028,
      "grad_norm": 0.8178333640098572,
      "learning_rate": 2.9429911551862773e-05,
      "loss": 0.7061,
      "step": 810
    },
    {
      "epoch": 0.06575781876503609,
      "grad_norm": 1.034265160560608,
      "learning_rate": 2.9421870812114714e-05,
      "loss": 0.7744,
      "step": 820
    },
    {
      "epoch": 0.06655974338412189,
      "grad_norm": 0.9703115820884705,
      "learning_rate": 2.9413830072366658e-05,
      "loss": 0.7191,
      "step": 830
    },
    {
      "epoch": 0.0673616680032077,
      "grad_norm": 1.0353131294250488,
      "learning_rate": 2.94057893326186e-05,
      "loss": 0.7695,
      "step": 840
    },
    {
      "epoch": 0.0681635926222935,
      "grad_norm": 1.0522944927215576,
      "learning_rate": 2.9397748592870546e-05,
      "loss": 0.6743,
      "step": 850
    },
    {
      "epoch": 0.06896551724137931,
      "grad_norm": 1.1600208282470703,
      "learning_rate": 2.9389707853122487e-05,
      "loss": 0.7508,
      "step": 860
    },
    {
      "epoch": 0.06976744186046512,
      "grad_norm": 1.3138258457183838,
      "learning_rate": 2.938166711337443e-05,
      "loss": 0.7123,
      "step": 870
    },
    {
      "epoch": 0.07056936647955092,
      "grad_norm": 1.1077395677566528,
      "learning_rate": 2.9373626373626373e-05,
      "loss": 0.7582,
      "step": 880
    },
    {
      "epoch": 0.07137129109863673,
      "grad_norm": 1.0162676572799683,
      "learning_rate": 2.936558563387832e-05,
      "loss": 0.7229,
      "step": 890
    },
    {
      "epoch": 0.07217321571772253,
      "grad_norm": 1.098257303237915,
      "learning_rate": 2.935754489413026e-05,
      "loss": 0.6784,
      "step": 900
    },
    {
      "epoch": 0.07297514033680834,
      "grad_norm": 0.9897691011428833,
      "learning_rate": 2.9349504154382205e-05,
      "loss": 0.8147,
      "step": 910
    },
    {
      "epoch": 0.07377706495589414,
      "grad_norm": 0.919794499874115,
      "learning_rate": 2.9341463414634146e-05,
      "loss": 0.7165,
      "step": 920
    },
    {
      "epoch": 0.07457898957497995,
      "grad_norm": 1.1452885866165161,
      "learning_rate": 2.933342267488609e-05,
      "loss": 0.7645,
      "step": 930
    },
    {
      "epoch": 0.07538091419406576,
      "grad_norm": 0.9888048768043518,
      "learning_rate": 2.9325381935138035e-05,
      "loss": 0.686,
      "step": 940
    },
    {
      "epoch": 0.07618283881315156,
      "grad_norm": 0.988930344581604,
      "learning_rate": 2.9317341195389976e-05,
      "loss": 0.7787,
      "step": 950
    },
    {
      "epoch": 0.07698476343223737,
      "grad_norm": 0.9723939895629883,
      "learning_rate": 2.930930045564192e-05,
      "loss": 0.7004,
      "step": 960
    },
    {
      "epoch": 0.07778668805132317,
      "grad_norm": 0.9883845448493958,
      "learning_rate": 2.930125971589386e-05,
      "loss": 0.7769,
      "step": 970
    },
    {
      "epoch": 0.07858861267040898,
      "grad_norm": 1.1376419067382812,
      "learning_rate": 2.9293218976145805e-05,
      "loss": 0.7061,
      "step": 980
    },
    {
      "epoch": 0.07939053728949479,
      "grad_norm": 1.1623252630233765,
      "learning_rate": 2.928517823639775e-05,
      "loss": 0.7307,
      "step": 990
    },
    {
      "epoch": 0.08019246190858059,
      "grad_norm": 0.8817927241325378,
      "learning_rate": 2.9277137496649694e-05,
      "loss": 0.6463,
      "step": 1000
    },
    {
      "epoch": 0.0809943865276664,
      "grad_norm": 1.0952805280685425,
      "learning_rate": 2.9269096756901635e-05,
      "loss": 0.7447,
      "step": 1010
    },
    {
      "epoch": 0.0817963111467522,
      "grad_norm": 1.1339685916900635,
      "learning_rate": 2.926105601715358e-05,
      "loss": 0.7111,
      "step": 1020
    },
    {
      "epoch": 0.08259823576583801,
      "grad_norm": 1.39863121509552,
      "learning_rate": 2.925301527740552e-05,
      "loss": 0.6955,
      "step": 1030
    },
    {
      "epoch": 0.08340016038492382,
      "grad_norm": 1.2126712799072266,
      "learning_rate": 2.9244974537657468e-05,
      "loss": 0.7443,
      "step": 1040
    },
    {
      "epoch": 0.08420208500400962,
      "grad_norm": 1.1142477989196777,
      "learning_rate": 2.923693379790941e-05,
      "loss": 0.754,
      "step": 1050
    },
    {
      "epoch": 0.08500400962309543,
      "grad_norm": 1.074295997619629,
      "learning_rate": 2.9228893058161353e-05,
      "loss": 0.7245,
      "step": 1060
    },
    {
      "epoch": 0.08580593424218123,
      "grad_norm": 0.9821667671203613,
      "learning_rate": 2.9220852318413294e-05,
      "loss": 0.6514,
      "step": 1070
    },
    {
      "epoch": 0.08660785886126704,
      "grad_norm": 1.06246817111969,
      "learning_rate": 2.9212811578665238e-05,
      "loss": 0.6817,
      "step": 1080
    },
    {
      "epoch": 0.08740978348035285,
      "grad_norm": 1.1921309232711792,
      "learning_rate": 2.9204770838917182e-05,
      "loss": 0.7395,
      "step": 1090
    },
    {
      "epoch": 0.08821170809943865,
      "grad_norm": 1.058695912361145,
      "learning_rate": 2.9196730099169123e-05,
      "loss": 0.7197,
      "step": 1100
    },
    {
      "epoch": 0.08901363271852446,
      "grad_norm": 1.1420072317123413,
      "learning_rate": 2.9188689359421067e-05,
      "loss": 0.8233,
      "step": 1110
    },
    {
      "epoch": 0.08981555733761026,
      "grad_norm": 1.0180747509002686,
      "learning_rate": 2.918064861967301e-05,
      "loss": 0.7091,
      "step": 1120
    },
    {
      "epoch": 0.09061748195669607,
      "grad_norm": 0.853503942489624,
      "learning_rate": 2.9172607879924956e-05,
      "loss": 0.69,
      "step": 1130
    },
    {
      "epoch": 0.09141940657578188,
      "grad_norm": 0.989206075668335,
      "learning_rate": 2.9164567140176897e-05,
      "loss": 0.7104,
      "step": 1140
    },
    {
      "epoch": 0.09222133119486768,
      "grad_norm": 0.9883906841278076,
      "learning_rate": 2.915652640042884e-05,
      "loss": 0.6828,
      "step": 1150
    },
    {
      "epoch": 0.09302325581395349,
      "grad_norm": 1.581724762916565,
      "learning_rate": 2.9148485660680782e-05,
      "loss": 0.6556,
      "step": 1160
    },
    {
      "epoch": 0.09382518043303929,
      "grad_norm": 1.2269543409347534,
      "learning_rate": 2.9140444920932726e-05,
      "loss": 0.7058,
      "step": 1170
    },
    {
      "epoch": 0.0946271050521251,
      "grad_norm": 0.9443817734718323,
      "learning_rate": 2.913240418118467e-05,
      "loss": 0.7284,
      "step": 1180
    },
    {
      "epoch": 0.0954290296712109,
      "grad_norm": 1.1516519784927368,
      "learning_rate": 2.9124363441436615e-05,
      "loss": 0.7432,
      "step": 1190
    },
    {
      "epoch": 0.09623095429029671,
      "grad_norm": 1.2649160623550415,
      "learning_rate": 2.9116322701688556e-05,
      "loss": 0.6806,
      "step": 1200
    },
    {
      "epoch": 0.09703287890938252,
      "grad_norm": 1.1610801219940186,
      "learning_rate": 2.9108281961940497e-05,
      "loss": 0.6702,
      "step": 1210
    },
    {
      "epoch": 0.09783480352846832,
      "grad_norm": 1.0515034198760986,
      "learning_rate": 2.9100241222192445e-05,
      "loss": 0.7666,
      "step": 1220
    },
    {
      "epoch": 0.09863672814755413,
      "grad_norm": 1.0936610698699951,
      "learning_rate": 2.9092200482444385e-05,
      "loss": 0.7079,
      "step": 1230
    },
    {
      "epoch": 0.09943865276663993,
      "grad_norm": 1.0045905113220215,
      "learning_rate": 2.908415974269633e-05,
      "loss": 0.7417,
      "step": 1240
    },
    {
      "epoch": 0.10024057738572574,
      "grad_norm": 1.1504695415496826,
      "learning_rate": 2.907611900294827e-05,
      "loss": 0.7744,
      "step": 1250
    },
    {
      "epoch": 0.10104250200481155,
      "grad_norm": 1.2954753637313843,
      "learning_rate": 2.9068078263200215e-05,
      "loss": 0.7045,
      "step": 1260
    },
    {
      "epoch": 0.10184442662389735,
      "grad_norm": 1.0239059925079346,
      "learning_rate": 2.906003752345216e-05,
      "loss": 0.6729,
      "step": 1270
    },
    {
      "epoch": 0.10264635124298316,
      "grad_norm": 1.1272388696670532,
      "learning_rate": 2.9051996783704103e-05,
      "loss": 0.6965,
      "step": 1280
    },
    {
      "epoch": 0.10344827586206896,
      "grad_norm": 1.711249589920044,
      "learning_rate": 2.9043956043956044e-05,
      "loss": 0.7179,
      "step": 1290
    },
    {
      "epoch": 0.10425020048115477,
      "grad_norm": 1.0865614414215088,
      "learning_rate": 2.903591530420799e-05,
      "loss": 0.7155,
      "step": 1300
    },
    {
      "epoch": 0.10505212510024058,
      "grad_norm": 1.2402853965759277,
      "learning_rate": 2.902787456445993e-05,
      "loss": 0.689,
      "step": 1310
    },
    {
      "epoch": 0.10585404971932638,
      "grad_norm": 0.9961714744567871,
      "learning_rate": 2.9019833824711874e-05,
      "loss": 0.6966,
      "step": 1320
    },
    {
      "epoch": 0.10665597433841219,
      "grad_norm": 1.2621030807495117,
      "learning_rate": 2.9011793084963818e-05,
      "loss": 0.7295,
      "step": 1330
    },
    {
      "epoch": 0.107457898957498,
      "grad_norm": 1.0754213333129883,
      "learning_rate": 2.900375234521576e-05,
      "loss": 0.6818,
      "step": 1340
    },
    {
      "epoch": 0.1082598235765838,
      "grad_norm": 1.0086421966552734,
      "learning_rate": 2.8995711605467703e-05,
      "loss": 0.7387,
      "step": 1350
    },
    {
      "epoch": 0.1090617481956696,
      "grad_norm": 1.002028465270996,
      "learning_rate": 2.8987670865719644e-05,
      "loss": 0.7314,
      "step": 1360
    },
    {
      "epoch": 0.10986367281475541,
      "grad_norm": 1.1983188390731812,
      "learning_rate": 2.8979630125971592e-05,
      "loss": 0.721,
      "step": 1370
    },
    {
      "epoch": 0.11066559743384122,
      "grad_norm": 1.4285142421722412,
      "learning_rate": 2.8971589386223533e-05,
      "loss": 0.6945,
      "step": 1380
    },
    {
      "epoch": 0.11146752205292702,
      "grad_norm": 1.2068039178848267,
      "learning_rate": 2.8963548646475477e-05,
      "loss": 0.7254,
      "step": 1390
    },
    {
      "epoch": 0.11226944667201283,
      "grad_norm": 1.2147190570831299,
      "learning_rate": 2.8955507906727418e-05,
      "loss": 0.6902,
      "step": 1400
    },
    {
      "epoch": 0.11307137129109864,
      "grad_norm": 1.136001467704773,
      "learning_rate": 2.8947467166979366e-05,
      "loss": 0.7158,
      "step": 1410
    },
    {
      "epoch": 0.11387329591018444,
      "grad_norm": 1.0795681476593018,
      "learning_rate": 2.8939426427231307e-05,
      "loss": 0.7086,
      "step": 1420
    },
    {
      "epoch": 0.11467522052927025,
      "grad_norm": 1.1467537879943848,
      "learning_rate": 2.893138568748325e-05,
      "loss": 0.6877,
      "step": 1430
    },
    {
      "epoch": 0.11547714514835605,
      "grad_norm": 1.0382484197616577,
      "learning_rate": 2.8923344947735192e-05,
      "loss": 0.6981,
      "step": 1440
    },
    {
      "epoch": 0.11627906976744186,
      "grad_norm": 1.0512715578079224,
      "learning_rate": 2.8915304207987133e-05,
      "loss": 0.67,
      "step": 1450
    },
    {
      "epoch": 0.11708099438652766,
      "grad_norm": 1.2766400575637817,
      "learning_rate": 2.890726346823908e-05,
      "loss": 0.688,
      "step": 1460
    },
    {
      "epoch": 0.11788291900561347,
      "grad_norm": 1.101596474647522,
      "learning_rate": 2.889922272849102e-05,
      "loss": 0.7104,
      "step": 1470
    },
    {
      "epoch": 0.11868484362469928,
      "grad_norm": 1.073608636856079,
      "learning_rate": 2.8891181988742966e-05,
      "loss": 0.6786,
      "step": 1480
    },
    {
      "epoch": 0.11948676824378508,
      "grad_norm": 1.2698017358779907,
      "learning_rate": 2.8883141248994906e-05,
      "loss": 0.7409,
      "step": 1490
    },
    {
      "epoch": 0.12028869286287089,
      "grad_norm": 0.9690842628479004,
      "learning_rate": 2.887510050924685e-05,
      "loss": 0.6441,
      "step": 1500
    },
    {
      "epoch": 0.1210906174819567,
      "grad_norm": 1.235939621925354,
      "learning_rate": 2.8867059769498795e-05,
      "loss": 0.656,
      "step": 1510
    },
    {
      "epoch": 0.1218925421010425,
      "grad_norm": 1.1652436256408691,
      "learning_rate": 2.885901902975074e-05,
      "loss": 0.7049,
      "step": 1520
    },
    {
      "epoch": 0.1226944667201283,
      "grad_norm": 1.0674641132354736,
      "learning_rate": 2.885097829000268e-05,
      "loss": 0.7001,
      "step": 1530
    },
    {
      "epoch": 0.12349639133921411,
      "grad_norm": 1.2583633661270142,
      "learning_rate": 2.8842937550254625e-05,
      "loss": 0.7464,
      "step": 1540
    },
    {
      "epoch": 0.12429831595829992,
      "grad_norm": 0.9919222593307495,
      "learning_rate": 2.8834896810506565e-05,
      "loss": 0.7313,
      "step": 1550
    },
    {
      "epoch": 0.12510024057738572,
      "grad_norm": 1.0768464803695679,
      "learning_rate": 2.8826856070758513e-05,
      "loss": 0.6915,
      "step": 1560
    },
    {
      "epoch": 0.12590216519647154,
      "grad_norm": 1.0315322875976562,
      "learning_rate": 2.8818815331010454e-05,
      "loss": 0.641,
      "step": 1570
    },
    {
      "epoch": 0.12670408981555734,
      "grad_norm": 1.2100284099578857,
      "learning_rate": 2.8810774591262395e-05,
      "loss": 0.7107,
      "step": 1580
    },
    {
      "epoch": 0.12750601443464316,
      "grad_norm": 1.2032612562179565,
      "learning_rate": 2.880273385151434e-05,
      "loss": 0.6767,
      "step": 1590
    },
    {
      "epoch": 0.12830793905372895,
      "grad_norm": 1.045975923538208,
      "learning_rate": 2.8794693111766283e-05,
      "loss": 0.7304,
      "step": 1600
    },
    {
      "epoch": 0.12910986367281477,
      "grad_norm": 1.2371019124984741,
      "learning_rate": 2.8786652372018228e-05,
      "loss": 0.686,
      "step": 1610
    },
    {
      "epoch": 0.12991178829190056,
      "grad_norm": 1.1647975444793701,
      "learning_rate": 2.877861163227017e-05,
      "loss": 0.7137,
      "step": 1620
    },
    {
      "epoch": 0.13071371291098638,
      "grad_norm": 1.312893033027649,
      "learning_rate": 2.8770570892522113e-05,
      "loss": 0.6935,
      "step": 1630
    },
    {
      "epoch": 0.13151563753007217,
      "grad_norm": 1.1965354681015015,
      "learning_rate": 2.8762530152774054e-05,
      "loss": 0.6639,
      "step": 1640
    },
    {
      "epoch": 0.132317562149158,
      "grad_norm": 1.305239200592041,
      "learning_rate": 2.8754489413026e-05,
      "loss": 0.7347,
      "step": 1650
    },
    {
      "epoch": 0.13311948676824378,
      "grad_norm": 1.1128461360931396,
      "learning_rate": 2.8746448673277942e-05,
      "loss": 0.6843,
      "step": 1660
    },
    {
      "epoch": 0.1339214113873296,
      "grad_norm": 1.2432193756103516,
      "learning_rate": 2.8738407933529887e-05,
      "loss": 0.7302,
      "step": 1670
    },
    {
      "epoch": 0.1347233360064154,
      "grad_norm": 1.1406821012496948,
      "learning_rate": 2.8730367193781828e-05,
      "loss": 0.7101,
      "step": 1680
    },
    {
      "epoch": 0.13552526062550121,
      "grad_norm": 1.1109751462936401,
      "learning_rate": 2.8722326454033772e-05,
      "loss": 0.7044,
      "step": 1690
    },
    {
      "epoch": 0.136327185244587,
      "grad_norm": 1.2967517375946045,
      "learning_rate": 2.8714285714285716e-05,
      "loss": 0.7444,
      "step": 1700
    },
    {
      "epoch": 0.13712910986367283,
      "grad_norm": 1.1223375797271729,
      "learning_rate": 2.8706244974537657e-05,
      "loss": 0.6985,
      "step": 1710
    },
    {
      "epoch": 0.13793103448275862,
      "grad_norm": 1.3412798643112183,
      "learning_rate": 2.86982042347896e-05,
      "loss": 0.6792,
      "step": 1720
    },
    {
      "epoch": 0.13873295910184444,
      "grad_norm": 1.2000699043273926,
      "learning_rate": 2.8690163495041542e-05,
      "loss": 0.7732,
      "step": 1730
    },
    {
      "epoch": 0.13953488372093023,
      "grad_norm": 1.1153552532196045,
      "learning_rate": 2.8682122755293487e-05,
      "loss": 0.6596,
      "step": 1740
    },
    {
      "epoch": 0.14033680834001605,
      "grad_norm": 1.1979347467422485,
      "learning_rate": 2.867408201554543e-05,
      "loss": 0.7452,
      "step": 1750
    },
    {
      "epoch": 0.14113873295910184,
      "grad_norm": 1.1981383562088013,
      "learning_rate": 2.8666041275797375e-05,
      "loss": 0.7008,
      "step": 1760
    },
    {
      "epoch": 0.14194065757818766,
      "grad_norm": 1.1381473541259766,
      "learning_rate": 2.8658000536049316e-05,
      "loss": 0.6898,
      "step": 1770
    },
    {
      "epoch": 0.14274258219727345,
      "grad_norm": 0.9700839519500732,
      "learning_rate": 2.864995979630126e-05,
      "loss": 0.716,
      "step": 1780
    },
    {
      "epoch": 0.14354450681635927,
      "grad_norm": 1.1768304109573364,
      "learning_rate": 2.8641919056553205e-05,
      "loss": 0.6353,
      "step": 1790
    },
    {
      "epoch": 0.14434643143544507,
      "grad_norm": 1.2487956285476685,
      "learning_rate": 2.863387831680515e-05,
      "loss": 0.7054,
      "step": 1800
    },
    {
      "epoch": 0.14514835605453089,
      "grad_norm": 1.118896484375,
      "learning_rate": 2.862583757705709e-05,
      "loss": 0.7609,
      "step": 1810
    },
    {
      "epoch": 0.14595028067361668,
      "grad_norm": 1.2519032955169678,
      "learning_rate": 2.8617796837309034e-05,
      "loss": 0.7283,
      "step": 1820
    },
    {
      "epoch": 0.1467522052927025,
      "grad_norm": 1.1461694240570068,
      "learning_rate": 2.8609756097560975e-05,
      "loss": 0.7215,
      "step": 1830
    },
    {
      "epoch": 0.1475541299117883,
      "grad_norm": 1.681997537612915,
      "learning_rate": 2.860171535781292e-05,
      "loss": 0.7116,
      "step": 1840
    },
    {
      "epoch": 0.1483560545308741,
      "grad_norm": 1.1730676889419556,
      "learning_rate": 2.8593674618064864e-05,
      "loss": 0.7544,
      "step": 1850
    },
    {
      "epoch": 0.1491579791499599,
      "grad_norm": 1.0555686950683594,
      "learning_rate": 2.8585633878316805e-05,
      "loss": 0.6808,
      "step": 1860
    },
    {
      "epoch": 0.14995990376904572,
      "grad_norm": 1.4724262952804565,
      "learning_rate": 2.857759313856875e-05,
      "loss": 0.6719,
      "step": 1870
    },
    {
      "epoch": 0.1507618283881315,
      "grad_norm": 1.2399553060531616,
      "learning_rate": 2.856955239882069e-05,
      "loss": 0.6909,
      "step": 1880
    },
    {
      "epoch": 0.15156375300721733,
      "grad_norm": 1.1648740768432617,
      "learning_rate": 2.8561511659072637e-05,
      "loss": 0.683,
      "step": 1890
    },
    {
      "epoch": 0.15236567762630313,
      "grad_norm": 1.0664191246032715,
      "learning_rate": 2.855347091932458e-05,
      "loss": 0.6686,
      "step": 1900
    },
    {
      "epoch": 0.15316760224538895,
      "grad_norm": 1.2364016771316528,
      "learning_rate": 2.8545430179576523e-05,
      "loss": 0.6496,
      "step": 1910
    },
    {
      "epoch": 0.15396952686447474,
      "grad_norm": 1.161456823348999,
      "learning_rate": 2.8537389439828464e-05,
      "loss": 0.7469,
      "step": 1920
    },
    {
      "epoch": 0.15477145148356056,
      "grad_norm": 1.3521878719329834,
      "learning_rate": 2.8529348700080408e-05,
      "loss": 0.7614,
      "step": 1930
    },
    {
      "epoch": 0.15557337610264635,
      "grad_norm": 1.475624680519104,
      "learning_rate": 2.8521307960332352e-05,
      "loss": 0.714,
      "step": 1940
    },
    {
      "epoch": 0.15637530072173217,
      "grad_norm": 1.226532220840454,
      "learning_rate": 2.8513267220584296e-05,
      "loss": 0.7097,
      "step": 1950
    },
    {
      "epoch": 0.15717722534081796,
      "grad_norm": 1.2691049575805664,
      "learning_rate": 2.8505226480836237e-05,
      "loss": 0.7011,
      "step": 1960
    },
    {
      "epoch": 0.15797914995990378,
      "grad_norm": 1.1062794923782349,
      "learning_rate": 2.8497185741088178e-05,
      "loss": 0.6703,
      "step": 1970
    },
    {
      "epoch": 0.15878107457898957,
      "grad_norm": 1.7385019063949585,
      "learning_rate": 2.8489145001340126e-05,
      "loss": 0.6917,
      "step": 1980
    },
    {
      "epoch": 0.1595829991980754,
      "grad_norm": 1.0776734352111816,
      "learning_rate": 2.8481104261592067e-05,
      "loss": 0.7191,
      "step": 1990
    },
    {
      "epoch": 0.16038492381716118,
      "grad_norm": 1.358702540397644,
      "learning_rate": 2.847306352184401e-05,
      "loss": 0.7382,
      "step": 2000
    },
    {
      "epoch": 0.161186848436247,
      "grad_norm": 1.3354593515396118,
      "learning_rate": 2.8465022782095952e-05,
      "loss": 0.6856,
      "step": 2010
    },
    {
      "epoch": 0.1619887730553328,
      "grad_norm": 1.2884225845336914,
      "learning_rate": 2.8456982042347896e-05,
      "loss": 0.6578,
      "step": 2020
    },
    {
      "epoch": 0.16279069767441862,
      "grad_norm": 1.1466935873031616,
      "learning_rate": 2.844894130259984e-05,
      "loss": 0.6656,
      "step": 2030
    },
    {
      "epoch": 0.1635926222935044,
      "grad_norm": 1.0398629903793335,
      "learning_rate": 2.8440900562851785e-05,
      "loss": 0.7228,
      "step": 2040
    },
    {
      "epoch": 0.16439454691259023,
      "grad_norm": 1.4720852375030518,
      "learning_rate": 2.8432859823103726e-05,
      "loss": 0.7942,
      "step": 2050
    },
    {
      "epoch": 0.16519647153167602,
      "grad_norm": 1.2446779012680054,
      "learning_rate": 2.842481908335567e-05,
      "loss": 0.6487,
      "step": 2060
    },
    {
      "epoch": 0.16599839615076184,
      "grad_norm": 1.0160878896713257,
      "learning_rate": 2.841677834360761e-05,
      "loss": 0.7061,
      "step": 2070
    },
    {
      "epoch": 0.16680032076984763,
      "grad_norm": 1.1956353187561035,
      "learning_rate": 2.840873760385956e-05,
      "loss": 0.7394,
      "step": 2080
    },
    {
      "epoch": 0.16760224538893345,
      "grad_norm": 1.226434350013733,
      "learning_rate": 2.84006968641115e-05,
      "loss": 0.7155,
      "step": 2090
    },
    {
      "epoch": 0.16840417000801924,
      "grad_norm": 1.2869939804077148,
      "learning_rate": 2.839265612436344e-05,
      "loss": 0.7279,
      "step": 2100
    },
    {
      "epoch": 0.16920609462710506,
      "grad_norm": 1.4467957019805908,
      "learning_rate": 2.8384615384615385e-05,
      "loss": 0.7033,
      "step": 2110
    },
    {
      "epoch": 0.17000801924619086,
      "grad_norm": 1.4550247192382812,
      "learning_rate": 2.8376574644867326e-05,
      "loss": 0.7054,
      "step": 2120
    },
    {
      "epoch": 0.17080994386527668,
      "grad_norm": 1.063491702079773,
      "learning_rate": 2.8368533905119273e-05,
      "loss": 0.7092,
      "step": 2130
    },
    {
      "epoch": 0.17161186848436247,
      "grad_norm": 1.1575268507003784,
      "learning_rate": 2.8360493165371214e-05,
      "loss": 0.6491,
      "step": 2140
    },
    {
      "epoch": 0.1724137931034483,
      "grad_norm": 1.2756191492080688,
      "learning_rate": 2.835245242562316e-05,
      "loss": 0.6903,
      "step": 2150
    },
    {
      "epoch": 0.17321571772253408,
      "grad_norm": 1.3267854452133179,
      "learning_rate": 2.83444116858751e-05,
      "loss": 0.7203,
      "step": 2160
    },
    {
      "epoch": 0.1740176423416199,
      "grad_norm": 1.263391137123108,
      "learning_rate": 2.8336370946127047e-05,
      "loss": 0.7271,
      "step": 2170
    },
    {
      "epoch": 0.1748195669607057,
      "grad_norm": 1.2485965490341187,
      "learning_rate": 2.8328330206378988e-05,
      "loss": 0.7301,
      "step": 2180
    },
    {
      "epoch": 0.1756214915797915,
      "grad_norm": 1.3176075220108032,
      "learning_rate": 2.8320289466630932e-05,
      "loss": 0.6849,
      "step": 2190
    },
    {
      "epoch": 0.1764234161988773,
      "grad_norm": 1.102700114250183,
      "learning_rate": 2.8312248726882873e-05,
      "loss": 0.6953,
      "step": 2200
    },
    {
      "epoch": 0.17722534081796312,
      "grad_norm": 1.4640315771102905,
      "learning_rate": 2.8304207987134817e-05,
      "loss": 0.741,
      "step": 2210
    },
    {
      "epoch": 0.17802726543704891,
      "grad_norm": 1.0849765539169312,
      "learning_rate": 2.8296167247386762e-05,
      "loss": 0.6632,
      "step": 2220
    },
    {
      "epoch": 0.17882919005613473,
      "grad_norm": 1.2297309637069702,
      "learning_rate": 2.8288126507638703e-05,
      "loss": 0.7788,
      "step": 2230
    },
    {
      "epoch": 0.17963111467522053,
      "grad_norm": 1.5869308710098267,
      "learning_rate": 2.8280085767890647e-05,
      "loss": 0.68,
      "step": 2240
    },
    {
      "epoch": 0.18043303929430635,
      "grad_norm": 1.1003040075302124,
      "learning_rate": 2.8272045028142588e-05,
      "loss": 0.7173,
      "step": 2250
    },
    {
      "epoch": 0.18123496391339214,
      "grad_norm": 1.3158869743347168,
      "learning_rate": 2.8264004288394532e-05,
      "loss": 0.7685,
      "step": 2260
    },
    {
      "epoch": 0.18203688853247796,
      "grad_norm": 1.184168815612793,
      "learning_rate": 2.8255963548646476e-05,
      "loss": 0.7014,
      "step": 2270
    },
    {
      "epoch": 0.18283881315156375,
      "grad_norm": 1.2237827777862549,
      "learning_rate": 2.824792280889842e-05,
      "loss": 0.7082,
      "step": 2280
    },
    {
      "epoch": 0.18364073777064957,
      "grad_norm": 1.324613332748413,
      "learning_rate": 2.823988206915036e-05,
      "loss": 0.724,
      "step": 2290
    },
    {
      "epoch": 0.18444266238973536,
      "grad_norm": 1.0310778617858887,
      "learning_rate": 2.8231841329402306e-05,
      "loss": 0.7276,
      "step": 2300
    },
    {
      "epoch": 0.18524458700882118,
      "grad_norm": 1.1572126150131226,
      "learning_rate": 2.8223800589654247e-05,
      "loss": 0.6676,
      "step": 2310
    },
    {
      "epoch": 0.18604651162790697,
      "grad_norm": 1.2612636089324951,
      "learning_rate": 2.8215759849906194e-05,
      "loss": 0.7049,
      "step": 2320
    },
    {
      "epoch": 0.1868484362469928,
      "grad_norm": 1.2031675577163696,
      "learning_rate": 2.8207719110158135e-05,
      "loss": 0.7269,
      "step": 2330
    },
    {
      "epoch": 0.18765036086607859,
      "grad_norm": 1.2740963697433472,
      "learning_rate": 2.819967837041008e-05,
      "loss": 0.6952,
      "step": 2340
    },
    {
      "epoch": 0.1884522854851644,
      "grad_norm": 1.1696792840957642,
      "learning_rate": 2.819163763066202e-05,
      "loss": 0.7597,
      "step": 2350
    },
    {
      "epoch": 0.1892542101042502,
      "grad_norm": 1.3465232849121094,
      "learning_rate": 2.8183596890913965e-05,
      "loss": 0.746,
      "step": 2360
    },
    {
      "epoch": 0.19005613472333602,
      "grad_norm": 1.1005533933639526,
      "learning_rate": 2.817555615116591e-05,
      "loss": 0.6901,
      "step": 2370
    },
    {
      "epoch": 0.1908580593424218,
      "grad_norm": 1.2689634561538696,
      "learning_rate": 2.816751541141785e-05,
      "loss": 0.7193,
      "step": 2380
    },
    {
      "epoch": 0.19165998396150763,
      "grad_norm": 1.0931408405303955,
      "learning_rate": 2.8159474671669794e-05,
      "loss": 0.6901,
      "step": 2390
    },
    {
      "epoch": 0.19246190858059342,
      "grad_norm": 1.143662929534912,
      "learning_rate": 2.8151433931921735e-05,
      "loss": 0.7298,
      "step": 2400
    },
    {
      "epoch": 0.19326383319967924,
      "grad_norm": 1.2793620824813843,
      "learning_rate": 2.8143393192173683e-05,
      "loss": 0.7498,
      "step": 2410
    },
    {
      "epoch": 0.19406575781876503,
      "grad_norm": 1.2672717571258545,
      "learning_rate": 2.8135352452425624e-05,
      "loss": 0.7364,
      "step": 2420
    },
    {
      "epoch": 0.19486768243785085,
      "grad_norm": 1.1909414529800415,
      "learning_rate": 2.8127311712677568e-05,
      "loss": 0.723,
      "step": 2430
    },
    {
      "epoch": 0.19566960705693665,
      "grad_norm": 1.1180099248886108,
      "learning_rate": 2.811927097292951e-05,
      "loss": 0.673,
      "step": 2440
    },
    {
      "epoch": 0.19647153167602247,
      "grad_norm": 1.3022527694702148,
      "learning_rate": 2.8111230233181453e-05,
      "loss": 0.7308,
      "step": 2450
    },
    {
      "epoch": 0.19727345629510826,
      "grad_norm": 1.4314836263656616,
      "learning_rate": 2.8103189493433398e-05,
      "loss": 0.7736,
      "step": 2460
    },
    {
      "epoch": 0.19807538091419408,
      "grad_norm": 1.0928032398223877,
      "learning_rate": 2.8095148753685342e-05,
      "loss": 0.6621,
      "step": 2470
    },
    {
      "epoch": 0.19887730553327987,
      "grad_norm": 1.0964932441711426,
      "learning_rate": 2.8087108013937283e-05,
      "loss": 0.6532,
      "step": 2480
    },
    {
      "epoch": 0.1996792301523657,
      "grad_norm": 1.2455346584320068,
      "learning_rate": 2.8079067274189224e-05,
      "loss": 0.7092,
      "step": 2490
    },
    {
      "epoch": 0.20048115477145148,
      "grad_norm": 1.0293021202087402,
      "learning_rate": 2.807102653444117e-05,
      "loss": 0.6613,
      "step": 2500
    },
    {
      "epoch": 0.2012830793905373,
      "grad_norm": 1.332572102546692,
      "learning_rate": 2.8062985794693112e-05,
      "loss": 0.6744,
      "step": 2510
    },
    {
      "epoch": 0.2020850040096231,
      "grad_norm": 1.222720742225647,
      "learning_rate": 2.8054945054945057e-05,
      "loss": 0.614,
      "step": 2520
    },
    {
      "epoch": 0.2028869286287089,
      "grad_norm": 1.3630516529083252,
      "learning_rate": 2.8046904315196997e-05,
      "loss": 0.7384,
      "step": 2530
    },
    {
      "epoch": 0.2036888532477947,
      "grad_norm": 1.2412872314453125,
      "learning_rate": 2.8038863575448942e-05,
      "loss": 0.6966,
      "step": 2540
    },
    {
      "epoch": 0.20449077786688052,
      "grad_norm": 1.2595316171646118,
      "learning_rate": 2.8030822835700886e-05,
      "loss": 0.6487,
      "step": 2550
    },
    {
      "epoch": 0.20529270248596632,
      "grad_norm": 1.3425788879394531,
      "learning_rate": 2.802278209595283e-05,
      "loss": 0.6688,
      "step": 2560
    },
    {
      "epoch": 0.20609462710505214,
      "grad_norm": 1.1307815313339233,
      "learning_rate": 2.801474135620477e-05,
      "loss": 0.6937,
      "step": 2570
    },
    {
      "epoch": 0.20689655172413793,
      "grad_norm": 1.329699993133545,
      "learning_rate": 2.8006700616456715e-05,
      "loss": 0.6832,
      "step": 2580
    },
    {
      "epoch": 0.20769847634322375,
      "grad_norm": 1.3541148900985718,
      "learning_rate": 2.7998659876708656e-05,
      "loss": 0.7258,
      "step": 2590
    },
    {
      "epoch": 0.20850040096230954,
      "grad_norm": 1.375917911529541,
      "learning_rate": 2.79906191369606e-05,
      "loss": 0.6784,
      "step": 2600
    },
    {
      "epoch": 0.20930232558139536,
      "grad_norm": 1.155934453010559,
      "learning_rate": 2.7982578397212545e-05,
      "loss": 0.7242,
      "step": 2610
    },
    {
      "epoch": 0.21010425020048115,
      "grad_norm": 1.4004085063934326,
      "learning_rate": 2.7974537657464486e-05,
      "loss": 0.6453,
      "step": 2620
    },
    {
      "epoch": 0.21090617481956697,
      "grad_norm": 1.1470630168914795,
      "learning_rate": 2.796649691771643e-05,
      "loss": 0.7516,
      "step": 2630
    },
    {
      "epoch": 0.21170809943865276,
      "grad_norm": 1.0676405429840088,
      "learning_rate": 2.795845617796837e-05,
      "loss": 0.7384,
      "step": 2640
    },
    {
      "epoch": 0.21251002405773858,
      "grad_norm": 1.129374384880066,
      "learning_rate": 2.795041543822032e-05,
      "loss": 0.6543,
      "step": 2650
    },
    {
      "epoch": 0.21331194867682438,
      "grad_norm": 1.2388191223144531,
      "learning_rate": 2.794237469847226e-05,
      "loss": 0.7469,
      "step": 2660
    },
    {
      "epoch": 0.2141138732959102,
      "grad_norm": 1.368729591369629,
      "learning_rate": 2.7934333958724204e-05,
      "loss": 0.6995,
      "step": 2670
    },
    {
      "epoch": 0.214915797914996,
      "grad_norm": 1.1921528577804565,
      "learning_rate": 2.7926293218976145e-05,
      "loss": 0.6911,
      "step": 2680
    },
    {
      "epoch": 0.2157177225340818,
      "grad_norm": 1.2743302583694458,
      "learning_rate": 2.7918252479228093e-05,
      "loss": 0.7677,
      "step": 2690
    },
    {
      "epoch": 0.2165196471531676,
      "grad_norm": 1.2678956985473633,
      "learning_rate": 2.7910211739480033e-05,
      "loss": 0.7366,
      "step": 2700
    },
    {
      "epoch": 0.21732157177225342,
      "grad_norm": 1.3928769826889038,
      "learning_rate": 2.7902170999731978e-05,
      "loss": 0.6859,
      "step": 2710
    },
    {
      "epoch": 0.2181234963913392,
      "grad_norm": 1.3273508548736572,
      "learning_rate": 2.789413025998392e-05,
      "loss": 0.7331,
      "step": 2720
    },
    {
      "epoch": 0.21892542101042503,
      "grad_norm": 1.2567895650863647,
      "learning_rate": 2.7886089520235863e-05,
      "loss": 0.6638,
      "step": 2730
    },
    {
      "epoch": 0.21972734562951082,
      "grad_norm": 1.352447748184204,
      "learning_rate": 2.7878048780487807e-05,
      "loss": 0.6736,
      "step": 2740
    },
    {
      "epoch": 0.22052927024859664,
      "grad_norm": 1.2092972993850708,
      "learning_rate": 2.7870008040739748e-05,
      "loss": 0.754,
      "step": 2750
    },
    {
      "epoch": 0.22133119486768243,
      "grad_norm": 1.66580069065094,
      "learning_rate": 2.7861967300991692e-05,
      "loss": 0.7223,
      "step": 2760
    },
    {
      "epoch": 0.22213311948676825,
      "grad_norm": 1.2040449380874634,
      "learning_rate": 2.7853926561243633e-05,
      "loss": 0.6372,
      "step": 2770
    },
    {
      "epoch": 0.22293504410585405,
      "grad_norm": 1.1956772804260254,
      "learning_rate": 2.7845885821495578e-05,
      "loss": 0.712,
      "step": 2780
    },
    {
      "epoch": 0.22373696872493987,
      "grad_norm": 1.3589715957641602,
      "learning_rate": 2.7837845081747522e-05,
      "loss": 0.7636,
      "step": 2790
    },
    {
      "epoch": 0.22453889334402566,
      "grad_norm": 1.1383614540100098,
      "learning_rate": 2.7829804341999466e-05,
      "loss": 0.6687,
      "step": 2800
    },
    {
      "epoch": 0.22534081796311148,
      "grad_norm": 1.2345526218414307,
      "learning_rate": 2.7821763602251407e-05,
      "loss": 0.761,
      "step": 2810
    },
    {
      "epoch": 0.22614274258219727,
      "grad_norm": 1.2527092695236206,
      "learning_rate": 2.781372286250335e-05,
      "loss": 0.7024,
      "step": 2820
    },
    {
      "epoch": 0.2269446672012831,
      "grad_norm": 1.2153605222702026,
      "learning_rate": 2.7805682122755292e-05,
      "loss": 0.6896,
      "step": 2830
    },
    {
      "epoch": 0.22774659182036888,
      "grad_norm": 1.1706089973449707,
      "learning_rate": 2.779764138300724e-05,
      "loss": 0.6603,
      "step": 2840
    },
    {
      "epoch": 0.2285485164394547,
      "grad_norm": 1.1848770380020142,
      "learning_rate": 2.778960064325918e-05,
      "loss": 0.7623,
      "step": 2850
    },
    {
      "epoch": 0.2293504410585405,
      "grad_norm": 1.388327717781067,
      "learning_rate": 2.7781559903511122e-05,
      "loss": 0.6868,
      "step": 2860
    },
    {
      "epoch": 0.2301523656776263,
      "grad_norm": 1.2684729099273682,
      "learning_rate": 2.7773519163763066e-05,
      "loss": 0.6866,
      "step": 2870
    },
    {
      "epoch": 0.2309542902967121,
      "grad_norm": 1.1859331130981445,
      "learning_rate": 2.776547842401501e-05,
      "loss": 0.701,
      "step": 2880
    },
    {
      "epoch": 0.23175621491579793,
      "grad_norm": 1.3259532451629639,
      "learning_rate": 2.7757437684266955e-05,
      "loss": 0.626,
      "step": 2890
    },
    {
      "epoch": 0.23255813953488372,
      "grad_norm": 1.7594594955444336,
      "learning_rate": 2.7749396944518896e-05,
      "loss": 0.6277,
      "step": 2900
    },
    {
      "epoch": 0.23336006415396954,
      "grad_norm": 1.5130584239959717,
      "learning_rate": 2.774135620477084e-05,
      "loss": 0.6754,
      "step": 2910
    },
    {
      "epoch": 0.23416198877305533,
      "grad_norm": 1.239683747291565,
      "learning_rate": 2.773331546502278e-05,
      "loss": 0.7031,
      "step": 2920
    },
    {
      "epoch": 0.23496391339214115,
      "grad_norm": 1.6030703783035278,
      "learning_rate": 2.772527472527473e-05,
      "loss": 0.706,
      "step": 2930
    },
    {
      "epoch": 0.23576583801122694,
      "grad_norm": 1.1770617961883545,
      "learning_rate": 2.771723398552667e-05,
      "loss": 0.6897,
      "step": 2940
    },
    {
      "epoch": 0.23656776263031276,
      "grad_norm": 1.6128101348876953,
      "learning_rate": 2.7709193245778614e-05,
      "loss": 0.6868,
      "step": 2950
    },
    {
      "epoch": 0.23736968724939855,
      "grad_norm": 1.4743447303771973,
      "learning_rate": 2.7701152506030554e-05,
      "loss": 0.7237,
      "step": 2960
    },
    {
      "epoch": 0.23817161186848437,
      "grad_norm": 1.4269345998764038,
      "learning_rate": 2.76931117662825e-05,
      "loss": 0.7802,
      "step": 2970
    },
    {
      "epoch": 0.23897353648757017,
      "grad_norm": 1.2369463443756104,
      "learning_rate": 2.7685071026534443e-05,
      "loss": 0.742,
      "step": 2980
    },
    {
      "epoch": 0.23977546110665598,
      "grad_norm": 1.2344512939453125,
      "learning_rate": 2.7677030286786384e-05,
      "loss": 0.6715,
      "step": 2990
    },
    {
      "epoch": 0.24057738572574178,
      "grad_norm": 1.6047359704971313,
      "learning_rate": 2.7668989547038328e-05,
      "loss": 0.7828,
      "step": 3000
    },
    {
      "epoch": 0.2413793103448276,
      "grad_norm": 1.8304802179336548,
      "learning_rate": 2.766094880729027e-05,
      "loss": 0.6405,
      "step": 3010
    },
    {
      "epoch": 0.2421812349639134,
      "grad_norm": 1.481393814086914,
      "learning_rate": 2.7652908067542213e-05,
      "loss": 0.6954,
      "step": 3020
    },
    {
      "epoch": 0.2429831595829992,
      "grad_norm": 1.1658331155776978,
      "learning_rate": 2.7644867327794158e-05,
      "loss": 0.7159,
      "step": 3030
    },
    {
      "epoch": 0.243785084202085,
      "grad_norm": 1.4772590398788452,
      "learning_rate": 2.7636826588046102e-05,
      "loss": 0.6976,
      "step": 3040
    },
    {
      "epoch": 0.24458700882117082,
      "grad_norm": 1.4719051122665405,
      "learning_rate": 2.7628785848298043e-05,
      "loss": 0.7221,
      "step": 3050
    },
    {
      "epoch": 0.2453889334402566,
      "grad_norm": 1.1181812286376953,
      "learning_rate": 2.7620745108549987e-05,
      "loss": 0.7463,
      "step": 3060
    },
    {
      "epoch": 0.24619085805934243,
      "grad_norm": 1.4126627445220947,
      "learning_rate": 2.761270436880193e-05,
      "loss": 0.6791,
      "step": 3070
    },
    {
      "epoch": 0.24699278267842822,
      "grad_norm": 1.1266727447509766,
      "learning_rate": 2.7604663629053876e-05,
      "loss": 0.6802,
      "step": 3080
    },
    {
      "epoch": 0.24779470729751404,
      "grad_norm": 1.2473570108413696,
      "learning_rate": 2.7596622889305817e-05,
      "loss": 0.6671,
      "step": 3090
    },
    {
      "epoch": 0.24859663191659984,
      "grad_norm": 1.4647961854934692,
      "learning_rate": 2.758858214955776e-05,
      "loss": 0.7686,
      "step": 3100
    },
    {
      "epoch": 0.24939855653568566,
      "grad_norm": 1.2344894409179688,
      "learning_rate": 2.7580541409809702e-05,
      "loss": 0.7242,
      "step": 3110
    },
    {
      "epoch": 0.25020048115477145,
      "grad_norm": 1.3332574367523193,
      "learning_rate": 2.7572500670061646e-05,
      "loss": 0.7711,
      "step": 3120
    },
    {
      "epoch": 0.25100240577385724,
      "grad_norm": 1.1299149990081787,
      "learning_rate": 2.756445993031359e-05,
      "loss": 0.6711,
      "step": 3130
    },
    {
      "epoch": 0.2518043303929431,
      "grad_norm": 1.238970160484314,
      "learning_rate": 2.755641919056553e-05,
      "loss": 0.6819,
      "step": 3140
    },
    {
      "epoch": 0.2526062550120289,
      "grad_norm": 1.1102591753005981,
      "learning_rate": 2.7548378450817476e-05,
      "loss": 0.6269,
      "step": 3150
    },
    {
      "epoch": 0.25340817963111467,
      "grad_norm": 1.112412691116333,
      "learning_rate": 2.7540337711069417e-05,
      "loss": 0.6998,
      "step": 3160
    },
    {
      "epoch": 0.25421010425020046,
      "grad_norm": 1.5945782661437988,
      "learning_rate": 2.7532296971321364e-05,
      "loss": 0.7779,
      "step": 3170
    },
    {
      "epoch": 0.2550120288692863,
      "grad_norm": 1.3791157007217407,
      "learning_rate": 2.7524256231573305e-05,
      "loss": 0.6709,
      "step": 3180
    },
    {
      "epoch": 0.2558139534883721,
      "grad_norm": 1.37564218044281,
      "learning_rate": 2.751621549182525e-05,
      "loss": 0.6499,
      "step": 3190
    },
    {
      "epoch": 0.2566158781074579,
      "grad_norm": 1.2717801332473755,
      "learning_rate": 2.750817475207719e-05,
      "loss": 0.728,
      "step": 3200
    },
    {
      "epoch": 0.2574178027265437,
      "grad_norm": 1.25578773021698,
      "learning_rate": 2.7500134012329135e-05,
      "loss": 0.6554,
      "step": 3210
    },
    {
      "epoch": 0.25821972734562953,
      "grad_norm": 1.3012021780014038,
      "learning_rate": 2.749209327258108e-05,
      "loss": 0.6579,
      "step": 3220
    },
    {
      "epoch": 0.2590216519647153,
      "grad_norm": 1.312204360961914,
      "learning_rate": 2.7484052532833023e-05,
      "loss": 0.6786,
      "step": 3230
    },
    {
      "epoch": 0.2598235765838011,
      "grad_norm": 1.1518656015396118,
      "learning_rate": 2.7476011793084964e-05,
      "loss": 0.6775,
      "step": 3240
    },
    {
      "epoch": 0.2606255012028869,
      "grad_norm": 1.4823096990585327,
      "learning_rate": 2.7467971053336905e-05,
      "loss": 0.7107,
      "step": 3250
    },
    {
      "epoch": 0.26142742582197276,
      "grad_norm": 1.3166048526763916,
      "learning_rate": 2.7459930313588853e-05,
      "loss": 0.7052,
      "step": 3260
    },
    {
      "epoch": 0.26222935044105855,
      "grad_norm": 1.3408595323562622,
      "learning_rate": 2.7451889573840794e-05,
      "loss": 0.7003,
      "step": 3270
    },
    {
      "epoch": 0.26303127506014434,
      "grad_norm": 1.3239787817001343,
      "learning_rate": 2.7443848834092738e-05,
      "loss": 0.6228,
      "step": 3280
    },
    {
      "epoch": 0.26383319967923013,
      "grad_norm": 1.1979196071624756,
      "learning_rate": 2.743580809434468e-05,
      "loss": 0.7195,
      "step": 3290
    },
    {
      "epoch": 0.264635124298316,
      "grad_norm": 1.4733883142471313,
      "learning_rate": 2.7427767354596623e-05,
      "loss": 0.715,
      "step": 3300
    },
    {
      "epoch": 0.2654370489174018,
      "grad_norm": 1.3892152309417725,
      "learning_rate": 2.7419726614848567e-05,
      "loss": 0.6839,
      "step": 3310
    },
    {
      "epoch": 0.26623897353648757,
      "grad_norm": 1.323082685470581,
      "learning_rate": 2.741168587510051e-05,
      "loss": 0.6571,
      "step": 3320
    },
    {
      "epoch": 0.26704089815557336,
      "grad_norm": 1.1051445007324219,
      "learning_rate": 2.7403645135352453e-05,
      "loss": 0.6489,
      "step": 3330
    },
    {
      "epoch": 0.2678428227746592,
      "grad_norm": 1.7054699659347534,
      "learning_rate": 2.7395604395604397e-05,
      "loss": 0.7261,
      "step": 3340
    },
    {
      "epoch": 0.268644747393745,
      "grad_norm": 1.2970112562179565,
      "learning_rate": 2.7387563655856338e-05,
      "loss": 0.7687,
      "step": 3350
    },
    {
      "epoch": 0.2694466720128308,
      "grad_norm": 1.2439377307891846,
      "learning_rate": 2.7379522916108285e-05,
      "loss": 0.6654,
      "step": 3360
    },
    {
      "epoch": 0.2702485966319166,
      "grad_norm": 1.3435392379760742,
      "learning_rate": 2.7371482176360226e-05,
      "loss": 0.687,
      "step": 3370
    },
    {
      "epoch": 0.27105052125100243,
      "grad_norm": 1.519597053527832,
      "learning_rate": 2.7363441436612167e-05,
      "loss": 0.6812,
      "step": 3380
    },
    {
      "epoch": 0.2718524458700882,
      "grad_norm": 1.249914288520813,
      "learning_rate": 2.735540069686411e-05,
      "loss": 0.6919,
      "step": 3390
    },
    {
      "epoch": 0.272654370489174,
      "grad_norm": 1.2712420225143433,
      "learning_rate": 2.7347359957116052e-05,
      "loss": 0.7294,
      "step": 3400
    },
    {
      "epoch": 0.2734562951082598,
      "grad_norm": 1.4228181838989258,
      "learning_rate": 2.7339319217368e-05,
      "loss": 0.7086,
      "step": 3410
    },
    {
      "epoch": 0.27425821972734565,
      "grad_norm": 1.1158366203308105,
      "learning_rate": 2.733127847761994e-05,
      "loss": 0.7071,
      "step": 3420
    },
    {
      "epoch": 0.27506014434643145,
      "grad_norm": 1.199724555015564,
      "learning_rate": 2.7323237737871885e-05,
      "loss": 0.7408,
      "step": 3430
    },
    {
      "epoch": 0.27586206896551724,
      "grad_norm": 1.3493765592575073,
      "learning_rate": 2.7315196998123826e-05,
      "loss": 0.6695,
      "step": 3440
    },
    {
      "epoch": 0.27666399358460303,
      "grad_norm": 1.1790651082992554,
      "learning_rate": 2.7307156258375774e-05,
      "loss": 0.7099,
      "step": 3450
    },
    {
      "epoch": 0.2774659182036889,
      "grad_norm": 1.2985628843307495,
      "learning_rate": 2.7299115518627715e-05,
      "loss": 0.5874,
      "step": 3460
    },
    {
      "epoch": 0.27826784282277467,
      "grad_norm": 1.5048694610595703,
      "learning_rate": 2.729107477887966e-05,
      "loss": 0.7113,
      "step": 3470
    },
    {
      "epoch": 0.27906976744186046,
      "grad_norm": 1.4260791540145874,
      "learning_rate": 2.72830340391316e-05,
      "loss": 0.6847,
      "step": 3480
    },
    {
      "epoch": 0.27987169206094625,
      "grad_norm": 1.37535560131073,
      "learning_rate": 2.7274993299383544e-05,
      "loss": 0.6717,
      "step": 3490
    },
    {
      "epoch": 0.2806736166800321,
      "grad_norm": 1.2773722410202026,
      "learning_rate": 2.726695255963549e-05,
      "loss": 0.7153,
      "step": 3500
    },
    {
      "epoch": 0.2814755412991179,
      "grad_norm": 1.2860431671142578,
      "learning_rate": 2.725891181988743e-05,
      "loss": 0.7282,
      "step": 3510
    },
    {
      "epoch": 0.2822774659182037,
      "grad_norm": 1.3279943466186523,
      "learning_rate": 2.7250871080139374e-05,
      "loss": 0.7071,
      "step": 3520
    },
    {
      "epoch": 0.2830793905372895,
      "grad_norm": 1.2521461248397827,
      "learning_rate": 2.7242830340391315e-05,
      "loss": 0.7528,
      "step": 3530
    },
    {
      "epoch": 0.2838813151563753,
      "grad_norm": 1.332499623298645,
      "learning_rate": 2.723478960064326e-05,
      "loss": 0.6725,
      "step": 3540
    },
    {
      "epoch": 0.2846832397754611,
      "grad_norm": 1.3360998630523682,
      "learning_rate": 2.7226748860895203e-05,
      "loss": 0.7244,
      "step": 3550
    },
    {
      "epoch": 0.2854851643945469,
      "grad_norm": 1.289066195487976,
      "learning_rate": 2.7218708121147148e-05,
      "loss": 0.6806,
      "step": 3560
    },
    {
      "epoch": 0.2862870890136327,
      "grad_norm": 1.4257333278656006,
      "learning_rate": 2.721066738139909e-05,
      "loss": 0.6814,
      "step": 3570
    },
    {
      "epoch": 0.28708901363271855,
      "grad_norm": 1.275481939315796,
      "learning_rate": 2.7202626641651033e-05,
      "loss": 0.7127,
      "step": 3580
    },
    {
      "epoch": 0.28789093825180434,
      "grad_norm": 1.338498592376709,
      "learning_rate": 2.7194585901902974e-05,
      "loss": 0.6174,
      "step": 3590
    },
    {
      "epoch": 0.28869286287089013,
      "grad_norm": 1.2473459243774414,
      "learning_rate": 2.718654516215492e-05,
      "loss": 0.7191,
      "step": 3600
    },
    {
      "epoch": 0.2894947874899759,
      "grad_norm": 1.3632886409759521,
      "learning_rate": 2.7178504422406862e-05,
      "loss": 0.7014,
      "step": 3610
    },
    {
      "epoch": 0.29029671210906177,
      "grad_norm": 1.1874092817306519,
      "learning_rate": 2.7170463682658806e-05,
      "loss": 0.6731,
      "step": 3620
    },
    {
      "epoch": 0.29109863672814756,
      "grad_norm": 1.1466155052185059,
      "learning_rate": 2.7162422942910747e-05,
      "loss": 0.7417,
      "step": 3630
    },
    {
      "epoch": 0.29190056134723336,
      "grad_norm": 1.4729987382888794,
      "learning_rate": 2.715438220316269e-05,
      "loss": 0.6849,
      "step": 3640
    },
    {
      "epoch": 0.29270248596631915,
      "grad_norm": 1.2862406969070435,
      "learning_rate": 2.7146341463414636e-05,
      "loss": 0.6961,
      "step": 3650
    },
    {
      "epoch": 0.293504410585405,
      "grad_norm": 1.3069179058074951,
      "learning_rate": 2.7138300723666577e-05,
      "loss": 0.7452,
      "step": 3660
    },
    {
      "epoch": 0.2943063352044908,
      "grad_norm": 1.308868408203125,
      "learning_rate": 2.713025998391852e-05,
      "loss": 0.6676,
      "step": 3670
    },
    {
      "epoch": 0.2951082598235766,
      "grad_norm": 1.5883796215057373,
      "learning_rate": 2.7122219244170462e-05,
      "loss": 0.6766,
      "step": 3680
    },
    {
      "epoch": 0.29591018444266237,
      "grad_norm": 1.1936514377593994,
      "learning_rate": 2.711417850442241e-05,
      "loss": 0.7086,
      "step": 3690
    },
    {
      "epoch": 0.2967121090617482,
      "grad_norm": 1.4493666887283325,
      "learning_rate": 2.710613776467435e-05,
      "loss": 0.7503,
      "step": 3700
    },
    {
      "epoch": 0.297514033680834,
      "grad_norm": 1.3393522500991821,
      "learning_rate": 2.7098097024926295e-05,
      "loss": 0.7318,
      "step": 3710
    },
    {
      "epoch": 0.2983159582999198,
      "grad_norm": 1.376129150390625,
      "learning_rate": 2.7090056285178236e-05,
      "loss": 0.6794,
      "step": 3720
    },
    {
      "epoch": 0.2991178829190056,
      "grad_norm": 1.175036907196045,
      "learning_rate": 2.708201554543018e-05,
      "loss": 0.6675,
      "step": 3730
    },
    {
      "epoch": 0.29991980753809144,
      "grad_norm": 1.340356469154358,
      "learning_rate": 2.7073974805682124e-05,
      "loss": 0.7451,
      "step": 3740
    },
    {
      "epoch": 0.30072173215717724,
      "grad_norm": 1.2139426469802856,
      "learning_rate": 2.706593406593407e-05,
      "loss": 0.6696,
      "step": 3750
    },
    {
      "epoch": 0.301523656776263,
      "grad_norm": 1.4059453010559082,
      "learning_rate": 2.705789332618601e-05,
      "loss": 0.6884,
      "step": 3760
    },
    {
      "epoch": 0.3023255813953488,
      "grad_norm": 1.2275553941726685,
      "learning_rate": 2.704985258643795e-05,
      "loss": 0.7207,
      "step": 3770
    },
    {
      "epoch": 0.30312750601443467,
      "grad_norm": 1.1621880531311035,
      "learning_rate": 2.7041811846689898e-05,
      "loss": 0.6614,
      "step": 3780
    },
    {
      "epoch": 0.30392943063352046,
      "grad_norm": 1.141856074333191,
      "learning_rate": 2.703377110694184e-05,
      "loss": 0.6693,
      "step": 3790
    },
    {
      "epoch": 0.30473135525260625,
      "grad_norm": 1.7810356616973877,
      "learning_rate": 2.7025730367193783e-05,
      "loss": 0.7013,
      "step": 3800
    },
    {
      "epoch": 0.30553327987169204,
      "grad_norm": 1.0560115575790405,
      "learning_rate": 2.7017689627445724e-05,
      "loss": 0.6911,
      "step": 3810
    },
    {
      "epoch": 0.3063352044907779,
      "grad_norm": 1.3953994512557983,
      "learning_rate": 2.700964888769767e-05,
      "loss": 0.6748,
      "step": 3820
    },
    {
      "epoch": 0.3071371291098637,
      "grad_norm": 1.3055156469345093,
      "learning_rate": 2.7001608147949613e-05,
      "loss": 0.72,
      "step": 3830
    },
    {
      "epoch": 0.3079390537289495,
      "grad_norm": 1.394063115119934,
      "learning_rate": 2.6993567408201557e-05,
      "loss": 0.7029,
      "step": 3840
    },
    {
      "epoch": 0.30874097834803527,
      "grad_norm": 1.238799810409546,
      "learning_rate": 2.6985526668453498e-05,
      "loss": 0.6538,
      "step": 3850
    },
    {
      "epoch": 0.3095429029671211,
      "grad_norm": 1.3970905542373657,
      "learning_rate": 2.6977485928705442e-05,
      "loss": 0.6422,
      "step": 3860
    },
    {
      "epoch": 0.3103448275862069,
      "grad_norm": 1.4238157272338867,
      "learning_rate": 2.6969445188957383e-05,
      "loss": 0.71,
      "step": 3870
    },
    {
      "epoch": 0.3111467522052927,
      "grad_norm": 1.4086976051330566,
      "learning_rate": 2.696140444920933e-05,
      "loss": 0.7278,
      "step": 3880
    },
    {
      "epoch": 0.3119486768243785,
      "grad_norm": 1.5958658456802368,
      "learning_rate": 2.6953363709461272e-05,
      "loss": 0.7299,
      "step": 3890
    },
    {
      "epoch": 0.31275060144346434,
      "grad_norm": 1.463208556175232,
      "learning_rate": 2.6945322969713213e-05,
      "loss": 0.6154,
      "step": 3900
    },
    {
      "epoch": 0.31355252606255013,
      "grad_norm": 1.3493372201919556,
      "learning_rate": 2.6937282229965157e-05,
      "loss": 0.6564,
      "step": 3910
    },
    {
      "epoch": 0.3143544506816359,
      "grad_norm": 1.2116044759750366,
      "learning_rate": 2.6929241490217098e-05,
      "loss": 0.6961,
      "step": 3920
    },
    {
      "epoch": 0.3151563753007217,
      "grad_norm": 1.6271846294403076,
      "learning_rate": 2.6921200750469046e-05,
      "loss": 0.6726,
      "step": 3930
    },
    {
      "epoch": 0.31595829991980756,
      "grad_norm": 1.266928791999817,
      "learning_rate": 2.6913160010720986e-05,
      "loss": 0.6456,
      "step": 3940
    },
    {
      "epoch": 0.31676022453889335,
      "grad_norm": 1.5525591373443604,
      "learning_rate": 2.690511927097293e-05,
      "loss": 0.7159,
      "step": 3950
    },
    {
      "epoch": 0.31756214915797915,
      "grad_norm": 1.156586766242981,
      "learning_rate": 2.689707853122487e-05,
      "loss": 0.6852,
      "step": 3960
    },
    {
      "epoch": 0.31836407377706494,
      "grad_norm": 1.426152229309082,
      "learning_rate": 2.688903779147682e-05,
      "loss": 0.7613,
      "step": 3970
    },
    {
      "epoch": 0.3191659983961508,
      "grad_norm": 1.1644079685211182,
      "learning_rate": 2.688099705172876e-05,
      "loss": 0.6571,
      "step": 3980
    },
    {
      "epoch": 0.3199679230152366,
      "grad_norm": 1.4330220222473145,
      "learning_rate": 2.6872956311980705e-05,
      "loss": 0.6963,
      "step": 3990
    },
    {
      "epoch": 0.32076984763432237,
      "grad_norm": 1.4708040952682495,
      "learning_rate": 2.6864915572232645e-05,
      "loss": 0.6745,
      "step": 4000
    },
    {
      "epoch": 0.32157177225340816,
      "grad_norm": 1.408339023590088,
      "learning_rate": 2.685687483248459e-05,
      "loss": 0.7298,
      "step": 4010
    },
    {
      "epoch": 0.322373696872494,
      "grad_norm": 1.6679844856262207,
      "learning_rate": 2.6848834092736534e-05,
      "loss": 0.7245,
      "step": 4020
    },
    {
      "epoch": 0.3231756214915798,
      "grad_norm": 1.6917152404785156,
      "learning_rate": 2.6840793352988475e-05,
      "loss": 0.6741,
      "step": 4030
    },
    {
      "epoch": 0.3239775461106656,
      "grad_norm": 1.5486981868743896,
      "learning_rate": 2.683275261324042e-05,
      "loss": 0.6376,
      "step": 4040
    },
    {
      "epoch": 0.3247794707297514,
      "grad_norm": 1.374445915222168,
      "learning_rate": 2.682471187349236e-05,
      "loss": 0.6868,
      "step": 4050
    },
    {
      "epoch": 0.32558139534883723,
      "grad_norm": 1.3082059621810913,
      "learning_rate": 2.6816671133744304e-05,
      "loss": 0.6632,
      "step": 4060
    },
    {
      "epoch": 0.326383319967923,
      "grad_norm": 1.3039522171020508,
      "learning_rate": 2.680863039399625e-05,
      "loss": 0.7072,
      "step": 4070
    },
    {
      "epoch": 0.3271852445870088,
      "grad_norm": 1.4480149745941162,
      "learning_rate": 2.6800589654248193e-05,
      "loss": 0.6466,
      "step": 4080
    },
    {
      "epoch": 0.3279871692060946,
      "grad_norm": 1.7448878288269043,
      "learning_rate": 2.6792548914500134e-05,
      "loss": 0.6689,
      "step": 4090
    },
    {
      "epoch": 0.32878909382518046,
      "grad_norm": 1.331762433052063,
      "learning_rate": 2.6784508174752078e-05,
      "loss": 0.646,
      "step": 4100
    },
    {
      "epoch": 0.32959101844426625,
      "grad_norm": 1.4008983373641968,
      "learning_rate": 2.677646743500402e-05,
      "loss": 0.7188,
      "step": 4110
    },
    {
      "epoch": 0.33039294306335204,
      "grad_norm": 1.2694036960601807,
      "learning_rate": 2.6768426695255967e-05,
      "loss": 0.6484,
      "step": 4120
    },
    {
      "epoch": 0.33119486768243783,
      "grad_norm": 1.1670457124710083,
      "learning_rate": 2.6760385955507908e-05,
      "loss": 0.6553,
      "step": 4130
    },
    {
      "epoch": 0.3319967923015237,
      "grad_norm": 1.3747659921646118,
      "learning_rate": 2.6752345215759852e-05,
      "loss": 0.6892,
      "step": 4140
    },
    {
      "epoch": 0.33279871692060947,
      "grad_norm": 1.31494140625,
      "learning_rate": 2.6744304476011793e-05,
      "loss": 0.586,
      "step": 4150
    },
    {
      "epoch": 0.33360064153969526,
      "grad_norm": 1.1140280961990356,
      "learning_rate": 2.6736263736263737e-05,
      "loss": 0.6859,
      "step": 4160
    },
    {
      "epoch": 0.33440256615878106,
      "grad_norm": 1.3584774732589722,
      "learning_rate": 2.672822299651568e-05,
      "loss": 0.6266,
      "step": 4170
    },
    {
      "epoch": 0.3352044907778669,
      "grad_norm": 1.2587864398956299,
      "learning_rate": 2.6720182256767622e-05,
      "loss": 0.6981,
      "step": 4180
    },
    {
      "epoch": 0.3360064153969527,
      "grad_norm": 1.467444658279419,
      "learning_rate": 2.6712141517019567e-05,
      "loss": 0.7255,
      "step": 4190
    },
    {
      "epoch": 0.3368083400160385,
      "grad_norm": 1.4112021923065186,
      "learning_rate": 2.6704100777271508e-05,
      "loss": 0.7707,
      "step": 4200
    },
    {
      "epoch": 0.3376102646351243,
      "grad_norm": 1.4514498710632324,
      "learning_rate": 2.6696060037523455e-05,
      "loss": 0.6747,
      "step": 4210
    },
    {
      "epoch": 0.3384121892542101,
      "grad_norm": 1.2422306537628174,
      "learning_rate": 2.6688019297775396e-05,
      "loss": 0.7209,
      "step": 4220
    },
    {
      "epoch": 0.3392141138732959,
      "grad_norm": 1.5833909511566162,
      "learning_rate": 2.667997855802734e-05,
      "loss": 0.7072,
      "step": 4230
    },
    {
      "epoch": 0.3400160384923817,
      "grad_norm": 1.5849460363388062,
      "learning_rate": 2.667193781827928e-05,
      "loss": 0.6744,
      "step": 4240
    },
    {
      "epoch": 0.3408179631114675,
      "grad_norm": 1.344948172569275,
      "learning_rate": 2.6663897078531226e-05,
      "loss": 0.6446,
      "step": 4250
    },
    {
      "epoch": 0.34161988773055335,
      "grad_norm": 1.493038296699524,
      "learning_rate": 2.665585633878317e-05,
      "loss": 0.703,
      "step": 4260
    },
    {
      "epoch": 0.34242181234963914,
      "grad_norm": 1.2848249673843384,
      "learning_rate": 2.664781559903511e-05,
      "loss": 0.6607,
      "step": 4270
    },
    {
      "epoch": 0.34322373696872494,
      "grad_norm": 1.2264009714126587,
      "learning_rate": 2.6639774859287055e-05,
      "loss": 0.6406,
      "step": 4280
    },
    {
      "epoch": 0.3440256615878107,
      "grad_norm": 1.2598135471343994,
      "learning_rate": 2.6631734119538996e-05,
      "loss": 0.6837,
      "step": 4290
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 1.2774949073791504,
      "learning_rate": 2.662369337979094e-05,
      "loss": 0.6661,
      "step": 4300
    },
    {
      "epoch": 0.34562951082598237,
      "grad_norm": 1.4539273977279663,
      "learning_rate": 2.6615652640042885e-05,
      "loss": 0.7272,
      "step": 4310
    },
    {
      "epoch": 0.34643143544506816,
      "grad_norm": 1.5051454305648804,
      "learning_rate": 2.660761190029483e-05,
      "loss": 0.6133,
      "step": 4320
    },
    {
      "epoch": 0.34723336006415395,
      "grad_norm": 1.1195663213729858,
      "learning_rate": 2.659957116054677e-05,
      "loss": 0.6625,
      "step": 4330
    },
    {
      "epoch": 0.3480352846832398,
      "grad_norm": 1.8605748414993286,
      "learning_rate": 2.6591530420798714e-05,
      "loss": 0.7285,
      "step": 4340
    },
    {
      "epoch": 0.3488372093023256,
      "grad_norm": 1.5024367570877075,
      "learning_rate": 2.658348968105066e-05,
      "loss": 0.7456,
      "step": 4350
    },
    {
      "epoch": 0.3496391339214114,
      "grad_norm": 1.7314289808273315,
      "learning_rate": 2.6575448941302603e-05,
      "loss": 0.6711,
      "step": 4360
    },
    {
      "epoch": 0.3504410585404972,
      "grad_norm": 1.443629503250122,
      "learning_rate": 2.6567408201554544e-05,
      "loss": 0.6694,
      "step": 4370
    },
    {
      "epoch": 0.351242983159583,
      "grad_norm": 1.2698780298233032,
      "learning_rate": 2.6559367461806488e-05,
      "loss": 0.6395,
      "step": 4380
    },
    {
      "epoch": 0.3520449077786688,
      "grad_norm": 1.3728258609771729,
      "learning_rate": 2.655132672205843e-05,
      "loss": 0.7065,
      "step": 4390
    },
    {
      "epoch": 0.3528468323977546,
      "grad_norm": 1.465989589691162,
      "learning_rate": 2.6543285982310373e-05,
      "loss": 0.6617,
      "step": 4400
    },
    {
      "epoch": 0.3536487570168404,
      "grad_norm": 1.2824124097824097,
      "learning_rate": 2.6535245242562317e-05,
      "loss": 0.6273,
      "step": 4410
    },
    {
      "epoch": 0.35445068163592625,
      "grad_norm": 1.4932323694229126,
      "learning_rate": 2.6527204502814258e-05,
      "loss": 0.6718,
      "step": 4420
    },
    {
      "epoch": 0.35525260625501204,
      "grad_norm": 1.327997088432312,
      "learning_rate": 2.6519163763066202e-05,
      "loss": 0.6886,
      "step": 4430
    },
    {
      "epoch": 0.35605453087409783,
      "grad_norm": 1.299928069114685,
      "learning_rate": 2.6511123023318143e-05,
      "loss": 0.7236,
      "step": 4440
    },
    {
      "epoch": 0.3568564554931836,
      "grad_norm": 1.1934539079666138,
      "learning_rate": 2.650308228357009e-05,
      "loss": 0.6044,
      "step": 4450
    },
    {
      "epoch": 0.35765838011226947,
      "grad_norm": 1.4952009916305542,
      "learning_rate": 2.6495041543822032e-05,
      "loss": 0.7341,
      "step": 4460
    },
    {
      "epoch": 0.35846030473135526,
      "grad_norm": 1.282098412513733,
      "learning_rate": 2.6487000804073976e-05,
      "loss": 0.7072,
      "step": 4470
    },
    {
      "epoch": 0.35926222935044105,
      "grad_norm": 1.3508974313735962,
      "learning_rate": 2.6478960064325917e-05,
      "loss": 0.6823,
      "step": 4480
    },
    {
      "epoch": 0.36006415396952685,
      "grad_norm": 1.4093331098556519,
      "learning_rate": 2.647091932457786e-05,
      "loss": 0.6477,
      "step": 4490
    },
    {
      "epoch": 0.3608660785886127,
      "grad_norm": 1.758288025856018,
      "learning_rate": 2.6462878584829806e-05,
      "loss": 0.6765,
      "step": 4500
    },
    {
      "epoch": 0.3616680032076985,
      "grad_norm": 1.3051730394363403,
      "learning_rate": 2.645483784508175e-05,
      "loss": 0.6392,
      "step": 4510
    },
    {
      "epoch": 0.3624699278267843,
      "grad_norm": 1.4361337423324585,
      "learning_rate": 2.644679710533369e-05,
      "loss": 0.6889,
      "step": 4520
    },
    {
      "epoch": 0.36327185244587007,
      "grad_norm": 1.2497137784957886,
      "learning_rate": 2.6438756365585632e-05,
      "loss": 0.6145,
      "step": 4530
    },
    {
      "epoch": 0.3640737770649559,
      "grad_norm": 1.6355708837509155,
      "learning_rate": 2.643071562583758e-05,
      "loss": 0.7229,
      "step": 4540
    },
    {
      "epoch": 0.3648757016840417,
      "grad_norm": 1.1821281909942627,
      "learning_rate": 2.642267488608952e-05,
      "loss": 0.7975,
      "step": 4550
    },
    {
      "epoch": 0.3656776263031275,
      "grad_norm": 1.3762140274047852,
      "learning_rate": 2.6414634146341465e-05,
      "loss": 0.7176,
      "step": 4560
    },
    {
      "epoch": 0.3664795509222133,
      "grad_norm": 1.326390027999878,
      "learning_rate": 2.6406593406593406e-05,
      "loss": 0.7002,
      "step": 4570
    },
    {
      "epoch": 0.36728147554129914,
      "grad_norm": 1.4600329399108887,
      "learning_rate": 2.639855266684535e-05,
      "loss": 0.707,
      "step": 4580
    },
    {
      "epoch": 0.36808340016038493,
      "grad_norm": 1.5142492055892944,
      "learning_rate": 2.6390511927097294e-05,
      "loss": 0.6482,
      "step": 4590
    },
    {
      "epoch": 0.3688853247794707,
      "grad_norm": 1.2761139869689941,
      "learning_rate": 2.638247118734924e-05,
      "loss": 0.6916,
      "step": 4600
    },
    {
      "epoch": 0.3696872493985565,
      "grad_norm": 1.420301079750061,
      "learning_rate": 2.637443044760118e-05,
      "loss": 0.7925,
      "step": 4610
    },
    {
      "epoch": 0.37048917401764236,
      "grad_norm": 1.5497711896896362,
      "learning_rate": 2.6366389707853124e-05,
      "loss": 0.706,
      "step": 4620
    },
    {
      "epoch": 0.37129109863672816,
      "grad_norm": 1.6252834796905518,
      "learning_rate": 2.6358348968105065e-05,
      "loss": 0.714,
      "step": 4630
    },
    {
      "epoch": 0.37209302325581395,
      "grad_norm": 1.260815143585205,
      "learning_rate": 2.6350308228357012e-05,
      "loss": 0.6185,
      "step": 4640
    },
    {
      "epoch": 0.37289494787489974,
      "grad_norm": 1.2553550004959106,
      "learning_rate": 2.6342267488608953e-05,
      "loss": 0.6515,
      "step": 4650
    },
    {
      "epoch": 0.3736968724939856,
      "grad_norm": 1.254427433013916,
      "learning_rate": 2.6334226748860894e-05,
      "loss": 0.68,
      "step": 4660
    },
    {
      "epoch": 0.3744987971130714,
      "grad_norm": 1.3828951120376587,
      "learning_rate": 2.632618600911284e-05,
      "loss": 0.6204,
      "step": 4670
    },
    {
      "epoch": 0.37530072173215717,
      "grad_norm": 1.4873510599136353,
      "learning_rate": 2.631814526936478e-05,
      "loss": 0.7485,
      "step": 4680
    },
    {
      "epoch": 0.37610264635124296,
      "grad_norm": 1.475427269935608,
      "learning_rate": 2.6310104529616727e-05,
      "loss": 0.6988,
      "step": 4690
    },
    {
      "epoch": 0.3769045709703288,
      "grad_norm": 1.1634472608566284,
      "learning_rate": 2.6302063789868668e-05,
      "loss": 0.636,
      "step": 4700
    },
    {
      "epoch": 0.3777064955894146,
      "grad_norm": 1.4800456762313843,
      "learning_rate": 2.6294023050120612e-05,
      "loss": 0.658,
      "step": 4710
    },
    {
      "epoch": 0.3785084202085004,
      "grad_norm": 1.3266701698303223,
      "learning_rate": 2.6285982310372553e-05,
      "loss": 0.6918,
      "step": 4720
    },
    {
      "epoch": 0.3793103448275862,
      "grad_norm": 1.2153596878051758,
      "learning_rate": 2.62779415706245e-05,
      "loss": 0.6866,
      "step": 4730
    },
    {
      "epoch": 0.38011226944667204,
      "grad_norm": 1.1577067375183105,
      "learning_rate": 2.626990083087644e-05,
      "loss": 0.7049,
      "step": 4740
    },
    {
      "epoch": 0.3809141940657578,
      "grad_norm": 1.184875249862671,
      "learning_rate": 2.6261860091128386e-05,
      "loss": 0.641,
      "step": 4750
    },
    {
      "epoch": 0.3817161186848436,
      "grad_norm": 1.349164605140686,
      "learning_rate": 2.6253819351380327e-05,
      "loss": 0.6813,
      "step": 4760
    },
    {
      "epoch": 0.3825180433039294,
      "grad_norm": 1.5306353569030762,
      "learning_rate": 2.624577861163227e-05,
      "loss": 0.6826,
      "step": 4770
    },
    {
      "epoch": 0.38331996792301526,
      "grad_norm": 1.3529186248779297,
      "learning_rate": 2.6237737871884215e-05,
      "loss": 0.7113,
      "step": 4780
    },
    {
      "epoch": 0.38412189254210105,
      "grad_norm": 1.4610172510147095,
      "learning_rate": 2.6229697132136156e-05,
      "loss": 0.7161,
      "step": 4790
    },
    {
      "epoch": 0.38492381716118684,
      "grad_norm": 1.1546754837036133,
      "learning_rate": 2.62216563923881e-05,
      "loss": 0.6981,
      "step": 4800
    },
    {
      "epoch": 0.38572574178027264,
      "grad_norm": 1.401661992073059,
      "learning_rate": 2.621361565264004e-05,
      "loss": 0.7458,
      "step": 4810
    },
    {
      "epoch": 0.3865276663993585,
      "grad_norm": 1.3625566959381104,
      "learning_rate": 2.6205574912891986e-05,
      "loss": 0.7287,
      "step": 4820
    },
    {
      "epoch": 0.3873295910184443,
      "grad_norm": 1.3340716361999512,
      "learning_rate": 2.619753417314393e-05,
      "loss": 0.6208,
      "step": 4830
    },
    {
      "epoch": 0.38813151563753007,
      "grad_norm": 1.3721083402633667,
      "learning_rate": 2.6189493433395874e-05,
      "loss": 0.6519,
      "step": 4840
    },
    {
      "epoch": 0.38893344025661586,
      "grad_norm": 1.5517231225967407,
      "learning_rate": 2.6181452693647815e-05,
      "loss": 0.7437,
      "step": 4850
    },
    {
      "epoch": 0.3897353648757017,
      "grad_norm": 1.22273850440979,
      "learning_rate": 2.617341195389976e-05,
      "loss": 0.6966,
      "step": 4860
    },
    {
      "epoch": 0.3905372894947875,
      "grad_norm": 1.456031084060669,
      "learning_rate": 2.61653712141517e-05,
      "loss": 0.6219,
      "step": 4870
    },
    {
      "epoch": 0.3913392141138733,
      "grad_norm": 1.3208340406417847,
      "learning_rate": 2.6157330474403648e-05,
      "loss": 0.618,
      "step": 4880
    },
    {
      "epoch": 0.3921411387329591,
      "grad_norm": 1.2643529176712036,
      "learning_rate": 2.614928973465559e-05,
      "loss": 0.8039,
      "step": 4890
    },
    {
      "epoch": 0.39294306335204493,
      "grad_norm": 1.4721916913986206,
      "learning_rate": 2.6141248994907533e-05,
      "loss": 0.7233,
      "step": 4900
    },
    {
      "epoch": 0.3937449879711307,
      "grad_norm": 1.5039186477661133,
      "learning_rate": 2.6133208255159474e-05,
      "loss": 0.6915,
      "step": 4910
    },
    {
      "epoch": 0.3945469125902165,
      "grad_norm": 1.5700896978378296,
      "learning_rate": 2.612516751541142e-05,
      "loss": 0.6818,
      "step": 4920
    },
    {
      "epoch": 0.3953488372093023,
      "grad_norm": 1.3143219947814941,
      "learning_rate": 2.6117126775663363e-05,
      "loss": 0.7283,
      "step": 4930
    },
    {
      "epoch": 0.39615076182838815,
      "grad_norm": 1.4266284704208374,
      "learning_rate": 2.6109086035915304e-05,
      "loss": 0.6354,
      "step": 4940
    },
    {
      "epoch": 0.39695268644747395,
      "grad_norm": 1.3869071006774902,
      "learning_rate": 2.6101045296167248e-05,
      "loss": 0.6968,
      "step": 4950
    },
    {
      "epoch": 0.39775461106655974,
      "grad_norm": 1.375720500946045,
      "learning_rate": 2.609300455641919e-05,
      "loss": 0.6552,
      "step": 4960
    },
    {
      "epoch": 0.39855653568564553,
      "grad_norm": 1.2487709522247314,
      "learning_rate": 2.6084963816671137e-05,
      "loss": 0.6627,
      "step": 4970
    },
    {
      "epoch": 0.3993584603047314,
      "grad_norm": 1.7530601024627686,
      "learning_rate": 2.6076923076923077e-05,
      "loss": 0.6646,
      "step": 4980
    },
    {
      "epoch": 0.40016038492381717,
      "grad_norm": 1.878753900527954,
      "learning_rate": 2.6068882337175022e-05,
      "loss": 0.7433,
      "step": 4990
    },
    {
      "epoch": 0.40096230954290296,
      "grad_norm": 1.5451455116271973,
      "learning_rate": 2.6060841597426963e-05,
      "loss": 0.7313,
      "step": 5000
    },
    {
      "epoch": 0.40176423416198875,
      "grad_norm": 1.4482688903808594,
      "learning_rate": 2.6052800857678907e-05,
      "loss": 0.7075,
      "step": 5010
    },
    {
      "epoch": 0.4025661587810746,
      "grad_norm": 1.2274023294448853,
      "learning_rate": 2.604476011793085e-05,
      "loss": 0.7352,
      "step": 5020
    },
    {
      "epoch": 0.4033680834001604,
      "grad_norm": 1.2639957666397095,
      "learning_rate": 2.6036719378182796e-05,
      "loss": 0.7057,
      "step": 5030
    },
    {
      "epoch": 0.4041700080192462,
      "grad_norm": 1.4007443189620972,
      "learning_rate": 2.6028678638434736e-05,
      "loss": 0.6543,
      "step": 5040
    },
    {
      "epoch": 0.404971932638332,
      "grad_norm": 1.8878834247589111,
      "learning_rate": 2.6020637898686677e-05,
      "loss": 0.7655,
      "step": 5050
    },
    {
      "epoch": 0.4057738572574178,
      "grad_norm": 1.3196524381637573,
      "learning_rate": 2.6012597158938625e-05,
      "loss": 0.7261,
      "step": 5060
    },
    {
      "epoch": 0.4065757818765036,
      "grad_norm": 1.4307860136032104,
      "learning_rate": 2.6004556419190566e-05,
      "loss": 0.6459,
      "step": 5070
    },
    {
      "epoch": 0.4073777064955894,
      "grad_norm": 1.3653755187988281,
      "learning_rate": 2.599651567944251e-05,
      "loss": 0.75,
      "step": 5080
    },
    {
      "epoch": 0.4081796311146752,
      "grad_norm": 1.204946517944336,
      "learning_rate": 2.598847493969445e-05,
      "loss": 0.6961,
      "step": 5090
    },
    {
      "epoch": 0.40898155573376105,
      "grad_norm": 1.443925142288208,
      "learning_rate": 2.5980434199946395e-05,
      "loss": 0.6809,
      "step": 5100
    },
    {
      "epoch": 0.40978348035284684,
      "grad_norm": 1.707584261894226,
      "learning_rate": 2.597239346019834e-05,
      "loss": 0.7279,
      "step": 5110
    },
    {
      "epoch": 0.41058540497193263,
      "grad_norm": 1.3822742700576782,
      "learning_rate": 2.5964352720450284e-05,
      "loss": 0.644,
      "step": 5120
    },
    {
      "epoch": 0.4113873295910184,
      "grad_norm": 1.4418118000030518,
      "learning_rate": 2.5956311980702225e-05,
      "loss": 0.7067,
      "step": 5130
    },
    {
      "epoch": 0.41218925421010427,
      "grad_norm": 1.3994890451431274,
      "learning_rate": 2.594827124095417e-05,
      "loss": 0.7288,
      "step": 5140
    },
    {
      "epoch": 0.41299117882919006,
      "grad_norm": 1.2182058095932007,
      "learning_rate": 2.594023050120611e-05,
      "loss": 0.65,
      "step": 5150
    },
    {
      "epoch": 0.41379310344827586,
      "grad_norm": 1.3668252229690552,
      "learning_rate": 2.5932189761458058e-05,
      "loss": 0.6554,
      "step": 5160
    },
    {
      "epoch": 0.41459502806736165,
      "grad_norm": 1.8300530910491943,
      "learning_rate": 2.592414902171e-05,
      "loss": 0.6838,
      "step": 5170
    },
    {
      "epoch": 0.4153969526864475,
      "grad_norm": 1.2564281225204468,
      "learning_rate": 2.591610828196194e-05,
      "loss": 0.6703,
      "step": 5180
    },
    {
      "epoch": 0.4161988773055333,
      "grad_norm": 1.4118387699127197,
      "learning_rate": 2.5908067542213884e-05,
      "loss": 0.6131,
      "step": 5190
    },
    {
      "epoch": 0.4170008019246191,
      "grad_norm": 1.380588173866272,
      "learning_rate": 2.5900026802465825e-05,
      "loss": 0.6831,
      "step": 5200
    },
    {
      "epoch": 0.41780272654370487,
      "grad_norm": 1.366448998451233,
      "learning_rate": 2.5891986062717772e-05,
      "loss": 0.6304,
      "step": 5210
    },
    {
      "epoch": 0.4186046511627907,
      "grad_norm": 1.4108256101608276,
      "learning_rate": 2.5883945322969713e-05,
      "loss": 0.727,
      "step": 5220
    },
    {
      "epoch": 0.4194065757818765,
      "grad_norm": 1.1640132665634155,
      "learning_rate": 2.5875904583221658e-05,
      "loss": 0.6375,
      "step": 5230
    },
    {
      "epoch": 0.4202085004009623,
      "grad_norm": 1.3061258792877197,
      "learning_rate": 2.58678638434736e-05,
      "loss": 0.6987,
      "step": 5240
    },
    {
      "epoch": 0.4210104250200481,
      "grad_norm": 1.098460078239441,
      "learning_rate": 2.5859823103725546e-05,
      "loss": 0.6736,
      "step": 5250
    },
    {
      "epoch": 0.42181234963913394,
      "grad_norm": 1.3068914413452148,
      "learning_rate": 2.5851782363977487e-05,
      "loss": 0.7285,
      "step": 5260
    },
    {
      "epoch": 0.42261427425821974,
      "grad_norm": 1.5854556560516357,
      "learning_rate": 2.584374162422943e-05,
      "loss": 0.6925,
      "step": 5270
    },
    {
      "epoch": 0.4234161988773055,
      "grad_norm": 1.2861034870147705,
      "learning_rate": 2.5835700884481372e-05,
      "loss": 0.6568,
      "step": 5280
    },
    {
      "epoch": 0.4242181234963913,
      "grad_norm": 1.6399773359298706,
      "learning_rate": 2.5827660144733317e-05,
      "loss": 0.6122,
      "step": 5290
    },
    {
      "epoch": 0.42502004811547717,
      "grad_norm": 1.2503740787506104,
      "learning_rate": 2.581961940498526e-05,
      "loss": 0.6951,
      "step": 5300
    },
    {
      "epoch": 0.42582197273456296,
      "grad_norm": 1.3692373037338257,
      "learning_rate": 2.5811578665237202e-05,
      "loss": 0.6496,
      "step": 5310
    },
    {
      "epoch": 0.42662389735364875,
      "grad_norm": 1.3479827642440796,
      "learning_rate": 2.5803537925489146e-05,
      "loss": 0.668,
      "step": 5320
    },
    {
      "epoch": 0.42742582197273454,
      "grad_norm": 1.4476292133331299,
      "learning_rate": 2.5795497185741087e-05,
      "loss": 0.6584,
      "step": 5330
    },
    {
      "epoch": 0.4282277465918204,
      "grad_norm": 1.4932197332382202,
      "learning_rate": 2.578745644599303e-05,
      "loss": 0.723,
      "step": 5340
    },
    {
      "epoch": 0.4290296712109062,
      "grad_norm": 1.3727428913116455,
      "learning_rate": 2.5779415706244976e-05,
      "loss": 0.6885,
      "step": 5350
    },
    {
      "epoch": 0.429831595829992,
      "grad_norm": 1.7781904935836792,
      "learning_rate": 2.577137496649692e-05,
      "loss": 0.6613,
      "step": 5360
    },
    {
      "epoch": 0.43063352044907777,
      "grad_norm": 1.485204815864563,
      "learning_rate": 2.576333422674886e-05,
      "loss": 0.6274,
      "step": 5370
    },
    {
      "epoch": 0.4314354450681636,
      "grad_norm": 1.230501651763916,
      "learning_rate": 2.5755293487000805e-05,
      "loss": 0.6335,
      "step": 5380
    },
    {
      "epoch": 0.4322373696872494,
      "grad_norm": 1.4061537981033325,
      "learning_rate": 2.5747252747252746e-05,
      "loss": 0.6726,
      "step": 5390
    },
    {
      "epoch": 0.4330392943063352,
      "grad_norm": 1.3605988025665283,
      "learning_rate": 2.5739212007504694e-05,
      "loss": 0.7175,
      "step": 5400
    },
    {
      "epoch": 0.433841218925421,
      "grad_norm": 1.4120222330093384,
      "learning_rate": 2.5731171267756634e-05,
      "loss": 0.6787,
      "step": 5410
    },
    {
      "epoch": 0.43464314354450684,
      "grad_norm": 1.5375206470489502,
      "learning_rate": 2.572313052800858e-05,
      "loss": 0.698,
      "step": 5420
    },
    {
      "epoch": 0.43544506816359263,
      "grad_norm": 1.6929353475570679,
      "learning_rate": 2.571508978826052e-05,
      "loss": 0.6507,
      "step": 5430
    },
    {
      "epoch": 0.4362469927826784,
      "grad_norm": 1.398346185684204,
      "learning_rate": 2.5707049048512464e-05,
      "loss": 0.567,
      "step": 5440
    },
    {
      "epoch": 0.4370489174017642,
      "grad_norm": 1.3473865985870361,
      "learning_rate": 2.5699008308764408e-05,
      "loss": 0.6584,
      "step": 5450
    },
    {
      "epoch": 0.43785084202085006,
      "grad_norm": 1.3031221628189087,
      "learning_rate": 2.569096756901635e-05,
      "loss": 0.6865,
      "step": 5460
    },
    {
      "epoch": 0.43865276663993585,
      "grad_norm": 1.281085729598999,
      "learning_rate": 2.5682926829268293e-05,
      "loss": 0.6817,
      "step": 5470
    },
    {
      "epoch": 0.43945469125902165,
      "grad_norm": 1.4692312479019165,
      "learning_rate": 2.5674886089520234e-05,
      "loss": 0.6553,
      "step": 5480
    },
    {
      "epoch": 0.44025661587810744,
      "grad_norm": 1.2314651012420654,
      "learning_rate": 2.5666845349772182e-05,
      "loss": 0.6807,
      "step": 5490
    },
    {
      "epoch": 0.4410585404971933,
      "grad_norm": 1.4164707660675049,
      "learning_rate": 2.5658804610024123e-05,
      "loss": 0.6497,
      "step": 5500
    },
    {
      "epoch": 0.4418604651162791,
      "grad_norm": 1.389666199684143,
      "learning_rate": 2.5650763870276067e-05,
      "loss": 0.6567,
      "step": 5510
    },
    {
      "epoch": 0.44266238973536487,
      "grad_norm": 1.4563981294631958,
      "learning_rate": 2.5642723130528008e-05,
      "loss": 0.7301,
      "step": 5520
    },
    {
      "epoch": 0.44346431435445066,
      "grad_norm": 1.7739766836166382,
      "learning_rate": 2.5634682390779952e-05,
      "loss": 0.6822,
      "step": 5530
    },
    {
      "epoch": 0.4442662389735365,
      "grad_norm": 1.2671180963516235,
      "learning_rate": 2.5626641651031897e-05,
      "loss": 0.6337,
      "step": 5540
    },
    {
      "epoch": 0.4450681635926223,
      "grad_norm": 1.5538268089294434,
      "learning_rate": 2.5618600911283838e-05,
      "loss": 0.6575,
      "step": 5550
    },
    {
      "epoch": 0.4458700882117081,
      "grad_norm": 1.203936219215393,
      "learning_rate": 2.5610560171535782e-05,
      "loss": 0.7299,
      "step": 5560
    },
    {
      "epoch": 0.4466720128307939,
      "grad_norm": 1.3051601648330688,
      "learning_rate": 2.5602519431787723e-05,
      "loss": 0.6582,
      "step": 5570
    },
    {
      "epoch": 0.44747393744987973,
      "grad_norm": 1.374153733253479,
      "learning_rate": 2.5594478692039667e-05,
      "loss": 0.6343,
      "step": 5580
    },
    {
      "epoch": 0.4482758620689655,
      "grad_norm": 1.5395699739456177,
      "learning_rate": 2.558643795229161e-05,
      "loss": 0.6605,
      "step": 5590
    },
    {
      "epoch": 0.4490777866880513,
      "grad_norm": 1.4637012481689453,
      "learning_rate": 2.5578397212543556e-05,
      "loss": 0.7042,
      "step": 5600
    },
    {
      "epoch": 0.4498797113071371,
      "grad_norm": 1.1889667510986328,
      "learning_rate": 2.5570356472795497e-05,
      "loss": 0.674,
      "step": 5610
    },
    {
      "epoch": 0.45068163592622296,
      "grad_norm": 1.4162499904632568,
      "learning_rate": 2.556231573304744e-05,
      "loss": 0.7621,
      "step": 5620
    },
    {
      "epoch": 0.45148356054530875,
      "grad_norm": 1.249654769897461,
      "learning_rate": 2.5554274993299385e-05,
      "loss": 0.7402,
      "step": 5630
    },
    {
      "epoch": 0.45228548516439454,
      "grad_norm": 1.4526365995407104,
      "learning_rate": 2.554623425355133e-05,
      "loss": 0.6385,
      "step": 5640
    },
    {
      "epoch": 0.45308740978348033,
      "grad_norm": 1.6453267335891724,
      "learning_rate": 2.553819351380327e-05,
      "loss": 0.6858,
      "step": 5650
    },
    {
      "epoch": 0.4538893344025662,
      "grad_norm": 1.422897458076477,
      "learning_rate": 2.5530152774055215e-05,
      "loss": 0.6982,
      "step": 5660
    },
    {
      "epoch": 0.454691259021652,
      "grad_norm": 1.415698528289795,
      "learning_rate": 2.5522112034307156e-05,
      "loss": 0.6584,
      "step": 5670
    },
    {
      "epoch": 0.45549318364073776,
      "grad_norm": 1.724273920059204,
      "learning_rate": 2.55140712945591e-05,
      "loss": 0.6455,
      "step": 5680
    },
    {
      "epoch": 0.45629510825982356,
      "grad_norm": 1.564996361732483,
      "learning_rate": 2.5506030554811044e-05,
      "loss": 0.6989,
      "step": 5690
    },
    {
      "epoch": 0.4570970328789094,
      "grad_norm": 1.4876163005828857,
      "learning_rate": 2.5497989815062985e-05,
      "loss": 0.6306,
      "step": 5700
    },
    {
      "epoch": 0.4578989574979952,
      "grad_norm": 1.6831690073013306,
      "learning_rate": 2.548994907531493e-05,
      "loss": 0.7694,
      "step": 5710
    },
    {
      "epoch": 0.458700882117081,
      "grad_norm": 1.3620749711990356,
      "learning_rate": 2.548190833556687e-05,
      "loss": 0.66,
      "step": 5720
    },
    {
      "epoch": 0.4595028067361668,
      "grad_norm": 1.5000319480895996,
      "learning_rate": 2.5473867595818818e-05,
      "loss": 0.6298,
      "step": 5730
    },
    {
      "epoch": 0.4603047313552526,
      "grad_norm": 1.5334563255310059,
      "learning_rate": 2.546582685607076e-05,
      "loss": 0.6929,
      "step": 5740
    },
    {
      "epoch": 0.4611066559743384,
      "grad_norm": 1.2920781373977661,
      "learning_rate": 2.5457786116322703e-05,
      "loss": 0.6615,
      "step": 5750
    },
    {
      "epoch": 0.4619085805934242,
      "grad_norm": 1.3648006916046143,
      "learning_rate": 2.5449745376574644e-05,
      "loss": 0.6639,
      "step": 5760
    },
    {
      "epoch": 0.46271050521251,
      "grad_norm": 1.588402271270752,
      "learning_rate": 2.5441704636826588e-05,
      "loss": 0.6705,
      "step": 5770
    },
    {
      "epoch": 0.46351242983159585,
      "grad_norm": 1.6551185846328735,
      "learning_rate": 2.5433663897078533e-05,
      "loss": 0.6292,
      "step": 5780
    },
    {
      "epoch": 0.46431435445068164,
      "grad_norm": 1.3624347448349,
      "learning_rate": 2.5425623157330477e-05,
      "loss": 0.6552,
      "step": 5790
    },
    {
      "epoch": 0.46511627906976744,
      "grad_norm": 1.4768354892730713,
      "learning_rate": 2.5417582417582418e-05,
      "loss": 0.65,
      "step": 5800
    },
    {
      "epoch": 0.4659182036888532,
      "grad_norm": 1.6437208652496338,
      "learning_rate": 2.540954167783436e-05,
      "loss": 0.6643,
      "step": 5810
    },
    {
      "epoch": 0.4667201283079391,
      "grad_norm": 1.6720266342163086,
      "learning_rate": 2.5401500938086306e-05,
      "loss": 0.6616,
      "step": 5820
    },
    {
      "epoch": 0.46752205292702487,
      "grad_norm": 1.6331313848495483,
      "learning_rate": 2.5393460198338247e-05,
      "loss": 0.6442,
      "step": 5830
    },
    {
      "epoch": 0.46832397754611066,
      "grad_norm": 1.4888676404953003,
      "learning_rate": 2.538541945859019e-05,
      "loss": 0.6865,
      "step": 5840
    },
    {
      "epoch": 0.46912590216519645,
      "grad_norm": 1.3636720180511475,
      "learning_rate": 2.5377378718842132e-05,
      "loss": 0.6095,
      "step": 5850
    },
    {
      "epoch": 0.4699278267842823,
      "grad_norm": 1.4120413064956665,
      "learning_rate": 2.5369337979094077e-05,
      "loss": 0.6898,
      "step": 5860
    },
    {
      "epoch": 0.4707297514033681,
      "grad_norm": 1.3850243091583252,
      "learning_rate": 2.536129723934602e-05,
      "loss": 0.6686,
      "step": 5870
    },
    {
      "epoch": 0.4715316760224539,
      "grad_norm": 1.3121241331100464,
      "learning_rate": 2.5353256499597965e-05,
      "loss": 0.6621,
      "step": 5880
    },
    {
      "epoch": 0.4723336006415397,
      "grad_norm": 1.4379326105117798,
      "learning_rate": 2.5345215759849906e-05,
      "loss": 0.6088,
      "step": 5890
    },
    {
      "epoch": 0.4731355252606255,
      "grad_norm": 1.9724371433258057,
      "learning_rate": 2.533717502010185e-05,
      "loss": 0.6229,
      "step": 5900
    },
    {
      "epoch": 0.4739374498797113,
      "grad_norm": 1.3067076206207275,
      "learning_rate": 2.532913428035379e-05,
      "loss": 0.7002,
      "step": 5910
    },
    {
      "epoch": 0.4747393744987971,
      "grad_norm": 1.4461714029312134,
      "learning_rate": 2.532109354060574e-05,
      "loss": 0.7691,
      "step": 5920
    },
    {
      "epoch": 0.4755412991178829,
      "grad_norm": 1.3771802186965942,
      "learning_rate": 2.531305280085768e-05,
      "loss": 0.7411,
      "step": 5930
    },
    {
      "epoch": 0.47634322373696875,
      "grad_norm": 1.3496553897857666,
      "learning_rate": 2.530501206110962e-05,
      "loss": 0.6351,
      "step": 5940
    },
    {
      "epoch": 0.47714514835605454,
      "grad_norm": 1.6278393268585205,
      "learning_rate": 2.5296971321361565e-05,
      "loss": 0.6839,
      "step": 5950
    },
    {
      "epoch": 0.47794707297514033,
      "grad_norm": 1.3204329013824463,
      "learning_rate": 2.5288930581613506e-05,
      "loss": 0.6652,
      "step": 5960
    },
    {
      "epoch": 0.4787489975942261,
      "grad_norm": 1.2129840850830078,
      "learning_rate": 2.5280889841865454e-05,
      "loss": 0.6456,
      "step": 5970
    },
    {
      "epoch": 0.47955092221331197,
      "grad_norm": 1.42304527759552,
      "learning_rate": 2.5272849102117395e-05,
      "loss": 0.6737,
      "step": 5980
    },
    {
      "epoch": 0.48035284683239776,
      "grad_norm": 1.3434722423553467,
      "learning_rate": 2.526480836236934e-05,
      "loss": 0.6802,
      "step": 5990
    },
    {
      "epoch": 0.48115477145148355,
      "grad_norm": 1.2287012338638306,
      "learning_rate": 2.525676762262128e-05,
      "loss": 0.6479,
      "step": 6000
    },
    {
      "epoch": 0.48195669607056935,
      "grad_norm": 1.5465272665023804,
      "learning_rate": 2.5248726882873228e-05,
      "loss": 0.6658,
      "step": 6010
    },
    {
      "epoch": 0.4827586206896552,
      "grad_norm": 1.2544654607772827,
      "learning_rate": 2.524068614312517e-05,
      "loss": 0.6601,
      "step": 6020
    },
    {
      "epoch": 0.483560545308741,
      "grad_norm": 1.429421305656433,
      "learning_rate": 2.5232645403377113e-05,
      "loss": 0.7145,
      "step": 6030
    },
    {
      "epoch": 0.4843624699278268,
      "grad_norm": 1.474980354309082,
      "learning_rate": 2.5224604663629054e-05,
      "loss": 0.6525,
      "step": 6040
    },
    {
      "epoch": 0.48516439454691257,
      "grad_norm": 1.7167783975601196,
      "learning_rate": 2.5216563923880998e-05,
      "loss": 0.6903,
      "step": 6050
    },
    {
      "epoch": 0.4859663191659984,
      "grad_norm": 1.4152476787567139,
      "learning_rate": 2.5208523184132942e-05,
      "loss": 0.6822,
      "step": 6060
    },
    {
      "epoch": 0.4867682437850842,
      "grad_norm": 1.6668126583099365,
      "learning_rate": 2.5200482444384883e-05,
      "loss": 0.7478,
      "step": 6070
    },
    {
      "epoch": 0.48757016840417,
      "grad_norm": 1.4881173372268677,
      "learning_rate": 2.5192441704636827e-05,
      "loss": 0.6712,
      "step": 6080
    },
    {
      "epoch": 0.4883720930232558,
      "grad_norm": 1.3155386447906494,
      "learning_rate": 2.5184400964888768e-05,
      "loss": 0.7122,
      "step": 6090
    },
    {
      "epoch": 0.48917401764234164,
      "grad_norm": 1.1484034061431885,
      "learning_rate": 2.5176360225140713e-05,
      "loss": 0.6687,
      "step": 6100
    },
    {
      "epoch": 0.48997594226142743,
      "grad_norm": 1.748390793800354,
      "learning_rate": 2.5168319485392657e-05,
      "loss": 0.6468,
      "step": 6110
    },
    {
      "epoch": 0.4907778668805132,
      "grad_norm": 1.5009911060333252,
      "learning_rate": 2.51602787456446e-05,
      "loss": 0.7056,
      "step": 6120
    },
    {
      "epoch": 0.491579791499599,
      "grad_norm": 1.301276683807373,
      "learning_rate": 2.5153042079871347e-05,
      "loss": 0.659,
      "step": 6130
    },
    {
      "epoch": 0.49238171611868486,
      "grad_norm": 1.4360891580581665,
      "learning_rate": 2.514500134012329e-05,
      "loss": 0.6781,
      "step": 6140
    },
    {
      "epoch": 0.49318364073777066,
      "grad_norm": 1.5364012718200684,
      "learning_rate": 2.5136960600375232e-05,
      "loss": 0.6867,
      "step": 6150
    },
    {
      "epoch": 0.49398556535685645,
      "grad_norm": 1.203221082687378,
      "learning_rate": 2.512891986062718e-05,
      "loss": 0.6448,
      "step": 6160
    },
    {
      "epoch": 0.49478748997594224,
      "grad_norm": 1.4185965061187744,
      "learning_rate": 2.512087912087912e-05,
      "loss": 0.5967,
      "step": 6170
    },
    {
      "epoch": 0.4955894145950281,
      "grad_norm": 1.313093900680542,
      "learning_rate": 2.5112838381131065e-05,
      "loss": 0.6539,
      "step": 6180
    },
    {
      "epoch": 0.4963913392141139,
      "grad_norm": 1.5452418327331543,
      "learning_rate": 2.5104797641383006e-05,
      "loss": 0.7761,
      "step": 6190
    },
    {
      "epoch": 0.4971932638331997,
      "grad_norm": 1.1939698457717896,
      "learning_rate": 2.509675690163495e-05,
      "loss": 0.665,
      "step": 6200
    },
    {
      "epoch": 0.49799518845228546,
      "grad_norm": 1.418261170387268,
      "learning_rate": 2.5088716161886895e-05,
      "loss": 0.6551,
      "step": 6210
    },
    {
      "epoch": 0.4987971130713713,
      "grad_norm": 1.2646615505218506,
      "learning_rate": 2.508067542213884e-05,
      "loss": 0.6588,
      "step": 6220
    },
    {
      "epoch": 0.4995990376904571,
      "grad_norm": 1.403818964958191,
      "learning_rate": 2.507263468239078e-05,
      "loss": 0.635,
      "step": 6230
    },
    {
      "epoch": 0.5004009623095429,
      "grad_norm": 1.3416862487792969,
      "learning_rate": 2.5064593942642724e-05,
      "loss": 0.7142,
      "step": 6240
    },
    {
      "epoch": 0.5012028869286287,
      "grad_norm": 1.5185320377349854,
      "learning_rate": 2.5056553202894665e-05,
      "loss": 0.6868,
      "step": 6250
    },
    {
      "epoch": 0.5020048115477145,
      "grad_norm": 1.5945640802383423,
      "learning_rate": 2.504851246314661e-05,
      "loss": 0.6503,
      "step": 6260
    },
    {
      "epoch": 0.5028067361668003,
      "grad_norm": 1.615295648574829,
      "learning_rate": 2.5040471723398554e-05,
      "loss": 0.6756,
      "step": 6270
    },
    {
      "epoch": 0.5036086607858862,
      "grad_norm": 1.3526742458343506,
      "learning_rate": 2.5032430983650495e-05,
      "loss": 0.7007,
      "step": 6280
    },
    {
      "epoch": 0.504410585404972,
      "grad_norm": 1.57024347782135,
      "learning_rate": 2.502439024390244e-05,
      "loss": 0.6283,
      "step": 6290
    },
    {
      "epoch": 0.5052125100240578,
      "grad_norm": 1.5496745109558105,
      "learning_rate": 2.5016349504154383e-05,
      "loss": 0.6103,
      "step": 6300
    },
    {
      "epoch": 0.5060144346431436,
      "grad_norm": 1.4565675258636475,
      "learning_rate": 2.5008308764406328e-05,
      "loss": 0.7716,
      "step": 6310
    },
    {
      "epoch": 0.5068163592622293,
      "grad_norm": 1.4019216299057007,
      "learning_rate": 2.500026802465827e-05,
      "loss": 0.6618,
      "step": 6320
    },
    {
      "epoch": 0.5076182838813151,
      "grad_norm": 1.3851364850997925,
      "learning_rate": 2.4992227284910213e-05,
      "loss": 0.6614,
      "step": 6330
    },
    {
      "epoch": 0.5084202085004009,
      "grad_norm": 1.3909962177276611,
      "learning_rate": 2.4984186545162154e-05,
      "loss": 0.683,
      "step": 6340
    },
    {
      "epoch": 0.5092221331194867,
      "grad_norm": 1.3661892414093018,
      "learning_rate": 2.49761458054141e-05,
      "loss": 0.692,
      "step": 6350
    },
    {
      "epoch": 0.5100240577385726,
      "grad_norm": 1.5082815885543823,
      "learning_rate": 2.4968105065666042e-05,
      "loss": 0.6719,
      "step": 6360
    },
    {
      "epoch": 0.5108259823576584,
      "grad_norm": 1.3493750095367432,
      "learning_rate": 2.4960064325917987e-05,
      "loss": 0.6767,
      "step": 6370
    },
    {
      "epoch": 0.5116279069767442,
      "grad_norm": 1.126526951789856,
      "learning_rate": 2.4952023586169927e-05,
      "loss": 0.6165,
      "step": 6380
    },
    {
      "epoch": 0.51242983159583,
      "grad_norm": 1.322588562965393,
      "learning_rate": 2.494398284642187e-05,
      "loss": 0.588,
      "step": 6390
    },
    {
      "epoch": 0.5132317562149158,
      "grad_norm": 1.6498210430145264,
      "learning_rate": 2.4935942106673816e-05,
      "loss": 0.7241,
      "step": 6400
    },
    {
      "epoch": 0.5140336808340016,
      "grad_norm": 1.6071135997772217,
      "learning_rate": 2.4927901366925757e-05,
      "loss": 0.6735,
      "step": 6410
    },
    {
      "epoch": 0.5148356054530874,
      "grad_norm": 1.2107224464416504,
      "learning_rate": 2.49198606271777e-05,
      "loss": 0.6955,
      "step": 6420
    },
    {
      "epoch": 0.5156375300721732,
      "grad_norm": 1.207350254058838,
      "learning_rate": 2.4911819887429642e-05,
      "loss": 0.6531,
      "step": 6430
    },
    {
      "epoch": 0.5164394546912591,
      "grad_norm": 1.235963225364685,
      "learning_rate": 2.4903779147681586e-05,
      "loss": 0.6361,
      "step": 6440
    },
    {
      "epoch": 0.5172413793103449,
      "grad_norm": 1.5302670001983643,
      "learning_rate": 2.489573840793353e-05,
      "loss": 0.6903,
      "step": 6450
    },
    {
      "epoch": 0.5180433039294307,
      "grad_norm": 1.3371268510818481,
      "learning_rate": 2.4887697668185475e-05,
      "loss": 0.6872,
      "step": 6460
    },
    {
      "epoch": 0.5188452285485164,
      "grad_norm": 1.3374152183532715,
      "learning_rate": 2.4879656928437416e-05,
      "loss": 0.7386,
      "step": 6470
    },
    {
      "epoch": 0.5196471531676022,
      "grad_norm": 1.5988813638687134,
      "learning_rate": 2.487161618868936e-05,
      "loss": 0.6825,
      "step": 6480
    },
    {
      "epoch": 0.520449077786688,
      "grad_norm": 1.4467394351959229,
      "learning_rate": 2.4863575448941304e-05,
      "loss": 0.6938,
      "step": 6490
    },
    {
      "epoch": 0.5212510024057738,
      "grad_norm": 1.4216135740280151,
      "learning_rate": 2.485553470919325e-05,
      "loss": 0.6867,
      "step": 6500
    },
    {
      "epoch": 0.5220529270248596,
      "grad_norm": 1.545670986175537,
      "learning_rate": 2.484749396944519e-05,
      "loss": 0.642,
      "step": 6510
    },
    {
      "epoch": 0.5228548516439455,
      "grad_norm": 1.4368095397949219,
      "learning_rate": 2.483945322969713e-05,
      "loss": 0.6876,
      "step": 6520
    },
    {
      "epoch": 0.5236567762630313,
      "grad_norm": 1.535338282585144,
      "learning_rate": 2.4831412489949075e-05,
      "loss": 0.6805,
      "step": 6530
    },
    {
      "epoch": 0.5244587008821171,
      "grad_norm": 1.3718007802963257,
      "learning_rate": 2.482337175020102e-05,
      "loss": 0.6923,
      "step": 6540
    },
    {
      "epoch": 0.5252606255012029,
      "grad_norm": 1.5396205186843872,
      "learning_rate": 2.4815331010452963e-05,
      "loss": 0.6504,
      "step": 6550
    },
    {
      "epoch": 0.5260625501202887,
      "grad_norm": 1.2104400396347046,
      "learning_rate": 2.4807290270704904e-05,
      "loss": 0.7215,
      "step": 6560
    },
    {
      "epoch": 0.5268644747393745,
      "grad_norm": 1.5358327627182007,
      "learning_rate": 2.479924953095685e-05,
      "loss": 0.7254,
      "step": 6570
    },
    {
      "epoch": 0.5276663993584603,
      "grad_norm": 1.4680747985839844,
      "learning_rate": 2.479120879120879e-05,
      "loss": 0.6518,
      "step": 6580
    },
    {
      "epoch": 0.5284683239775461,
      "grad_norm": 1.387466549873352,
      "learning_rate": 2.4783168051460737e-05,
      "loss": 0.6393,
      "step": 6590
    },
    {
      "epoch": 0.529270248596632,
      "grad_norm": 1.7300046682357788,
      "learning_rate": 2.4775127311712678e-05,
      "loss": 0.6734,
      "step": 6600
    },
    {
      "epoch": 0.5300721732157178,
      "grad_norm": 1.577527403831482,
      "learning_rate": 2.4767086571964622e-05,
      "loss": 0.6648,
      "step": 6610
    },
    {
      "epoch": 0.5308740978348035,
      "grad_norm": 1.4828956127166748,
      "learning_rate": 2.4759045832216563e-05,
      "loss": 0.7205,
      "step": 6620
    },
    {
      "epoch": 0.5316760224538893,
      "grad_norm": 1.2832814455032349,
      "learning_rate": 2.4751005092468508e-05,
      "loss": 0.6525,
      "step": 6630
    },
    {
      "epoch": 0.5324779470729751,
      "grad_norm": 1.4084343910217285,
      "learning_rate": 2.4742964352720452e-05,
      "loss": 0.755,
      "step": 6640
    },
    {
      "epoch": 0.5332798716920609,
      "grad_norm": 1.6494646072387695,
      "learning_rate": 2.4734923612972393e-05,
      "loss": 0.7216,
      "step": 6650
    },
    {
      "epoch": 0.5340817963111467,
      "grad_norm": 1.4509137868881226,
      "learning_rate": 2.4726882873224337e-05,
      "loss": 0.6399,
      "step": 6660
    },
    {
      "epoch": 0.5348837209302325,
      "grad_norm": 1.3868886232376099,
      "learning_rate": 2.4718842133476278e-05,
      "loss": 0.615,
      "step": 6670
    },
    {
      "epoch": 0.5356856455493184,
      "grad_norm": 1.322594404220581,
      "learning_rate": 2.4710801393728226e-05,
      "loss": 0.6356,
      "step": 6680
    },
    {
      "epoch": 0.5364875701684042,
      "grad_norm": 1.513569951057434,
      "learning_rate": 2.4702760653980167e-05,
      "loss": 0.6068,
      "step": 6690
    },
    {
      "epoch": 0.53728949478749,
      "grad_norm": 1.8389792442321777,
      "learning_rate": 2.469471991423211e-05,
      "loss": 0.6459,
      "step": 6700
    },
    {
      "epoch": 0.5380914194065758,
      "grad_norm": 1.2743096351623535,
      "learning_rate": 2.4686679174484052e-05,
      "loss": 0.6512,
      "step": 6710
    },
    {
      "epoch": 0.5388933440256616,
      "grad_norm": 1.4067413806915283,
      "learning_rate": 2.4678638434735996e-05,
      "loss": 0.7338,
      "step": 6720
    },
    {
      "epoch": 0.5396952686447474,
      "grad_norm": 1.5946859121322632,
      "learning_rate": 2.467059769498794e-05,
      "loss": 0.652,
      "step": 6730
    },
    {
      "epoch": 0.5404971932638332,
      "grad_norm": 1.4740142822265625,
      "learning_rate": 2.4662556955239885e-05,
      "loss": 0.7355,
      "step": 6740
    },
    {
      "epoch": 0.541299117882919,
      "grad_norm": 1.1830593347549438,
      "learning_rate": 2.4654516215491826e-05,
      "loss": 0.6729,
      "step": 6750
    },
    {
      "epoch": 0.5421010425020049,
      "grad_norm": 1.568752646446228,
      "learning_rate": 2.464647547574377e-05,
      "loss": 0.6508,
      "step": 6760
    },
    {
      "epoch": 0.5429029671210907,
      "grad_norm": 1.3361172676086426,
      "learning_rate": 2.463843473599571e-05,
      "loss": 0.7479,
      "step": 6770
    },
    {
      "epoch": 0.5437048917401764,
      "grad_norm": 1.7572683095932007,
      "learning_rate": 2.4630393996247655e-05,
      "loss": 0.6537,
      "step": 6780
    },
    {
      "epoch": 0.5445068163592622,
      "grad_norm": 2.0497992038726807,
      "learning_rate": 2.46223532564996e-05,
      "loss": 0.6373,
      "step": 6790
    },
    {
      "epoch": 0.545308740978348,
      "grad_norm": 1.4797468185424805,
      "learning_rate": 2.461431251675154e-05,
      "loss": 0.7399,
      "step": 6800
    },
    {
      "epoch": 0.5461106655974338,
      "grad_norm": 1.357932209968567,
      "learning_rate": 2.4606271777003484e-05,
      "loss": 0.7281,
      "step": 6810
    },
    {
      "epoch": 0.5469125902165196,
      "grad_norm": 1.3753349781036377,
      "learning_rate": 2.459823103725543e-05,
      "loss": 0.6704,
      "step": 6820
    },
    {
      "epoch": 0.5477145148356054,
      "grad_norm": 1.5188096761703491,
      "learning_rate": 2.4590190297507373e-05,
      "loss": 0.7311,
      "step": 6830
    },
    {
      "epoch": 0.5485164394546913,
      "grad_norm": 1.5151455402374268,
      "learning_rate": 2.4582149557759314e-05,
      "loss": 0.606,
      "step": 6840
    },
    {
      "epoch": 0.5493183640737771,
      "grad_norm": 1.5071719884872437,
      "learning_rate": 2.4574108818011258e-05,
      "loss": 0.5783,
      "step": 6850
    },
    {
      "epoch": 0.5501202886928629,
      "grad_norm": 1.538533091545105,
      "learning_rate": 2.45660680782632e-05,
      "loss": 0.6448,
      "step": 6860
    },
    {
      "epoch": 0.5509222133119487,
      "grad_norm": 1.3719375133514404,
      "learning_rate": 2.4558027338515147e-05,
      "loss": 0.6049,
      "step": 6870
    },
    {
      "epoch": 0.5517241379310345,
      "grad_norm": 1.473191738128662,
      "learning_rate": 2.4549986598767088e-05,
      "loss": 0.7454,
      "step": 6880
    },
    {
      "epoch": 0.5525260625501203,
      "grad_norm": 1.604529857635498,
      "learning_rate": 2.4541945859019032e-05,
      "loss": 0.6695,
      "step": 6890
    },
    {
      "epoch": 0.5533279871692061,
      "grad_norm": 1.662230134010315,
      "learning_rate": 2.4533905119270973e-05,
      "loss": 0.6599,
      "step": 6900
    },
    {
      "epoch": 0.5541299117882919,
      "grad_norm": 1.3842670917510986,
      "learning_rate": 2.4525864379522914e-05,
      "loss": 0.7024,
      "step": 6910
    },
    {
      "epoch": 0.5549318364073778,
      "grad_norm": 1.553720474243164,
      "learning_rate": 2.451782363977486e-05,
      "loss": 0.6132,
      "step": 6920
    },
    {
      "epoch": 0.5557337610264635,
      "grad_norm": 1.5673251152038574,
      "learning_rate": 2.4509782900026802e-05,
      "loss": 0.6814,
      "step": 6930
    },
    {
      "epoch": 0.5565356856455493,
      "grad_norm": 1.3722639083862305,
      "learning_rate": 2.4501742160278747e-05,
      "loss": 0.6847,
      "step": 6940
    },
    {
      "epoch": 0.5573376102646351,
      "grad_norm": 1.3932132720947266,
      "learning_rate": 2.4493701420530688e-05,
      "loss": 0.6817,
      "step": 6950
    },
    {
      "epoch": 0.5581395348837209,
      "grad_norm": 1.6447645425796509,
      "learning_rate": 2.4485660680782632e-05,
      "loss": 0.724,
      "step": 6960
    },
    {
      "epoch": 0.5589414595028067,
      "grad_norm": 1.5404659509658813,
      "learning_rate": 2.4477619941034576e-05,
      "loss": 0.6293,
      "step": 6970
    },
    {
      "epoch": 0.5597433841218925,
      "grad_norm": 1.3124771118164062,
      "learning_rate": 2.446957920128652e-05,
      "loss": 0.6685,
      "step": 6980
    },
    {
      "epoch": 0.5605453087409783,
      "grad_norm": 1.3881545066833496,
      "learning_rate": 2.446153846153846e-05,
      "loss": 0.6592,
      "step": 6990
    },
    {
      "epoch": 0.5613472333600642,
      "grad_norm": 1.336012840270996,
      "learning_rate": 2.4453497721790406e-05,
      "loss": 0.6171,
      "step": 7000
    },
    {
      "epoch": 0.56214915797915,
      "grad_norm": 1.1174499988555908,
      "learning_rate": 2.444545698204235e-05,
      "loss": 0.6092,
      "step": 7010
    },
    {
      "epoch": 0.5629510825982358,
      "grad_norm": 1.4477587938308716,
      "learning_rate": 2.4437416242294294e-05,
      "loss": 0.6744,
      "step": 7020
    },
    {
      "epoch": 0.5637530072173216,
      "grad_norm": 1.4485560655593872,
      "learning_rate": 2.4429375502546235e-05,
      "loss": 0.7079,
      "step": 7030
    },
    {
      "epoch": 0.5645549318364074,
      "grad_norm": 1.5009130239486694,
      "learning_rate": 2.4421334762798176e-05,
      "loss": 0.6975,
      "step": 7040
    },
    {
      "epoch": 0.5653568564554932,
      "grad_norm": 1.5130678415298462,
      "learning_rate": 2.441329402305012e-05,
      "loss": 0.6581,
      "step": 7050
    },
    {
      "epoch": 0.566158781074579,
      "grad_norm": 1.39114248752594,
      "learning_rate": 2.4405253283302065e-05,
      "loss": 0.647,
      "step": 7060
    },
    {
      "epoch": 0.5669607056936647,
      "grad_norm": 1.253562331199646,
      "learning_rate": 2.439721254355401e-05,
      "loss": 0.7206,
      "step": 7070
    },
    {
      "epoch": 0.5677626303127506,
      "grad_norm": 1.5653319358825684,
      "learning_rate": 2.438917180380595e-05,
      "loss": 0.6745,
      "step": 7080
    },
    {
      "epoch": 0.5685645549318364,
      "grad_norm": 1.327545166015625,
      "learning_rate": 2.4381131064057894e-05,
      "loss": 0.6105,
      "step": 7090
    },
    {
      "epoch": 0.5693664795509222,
      "grad_norm": 1.6740840673446655,
      "learning_rate": 2.4373090324309835e-05,
      "loss": 0.7584,
      "step": 7100
    },
    {
      "epoch": 0.570168404170008,
      "grad_norm": 1.463643193244934,
      "learning_rate": 2.4365049584561783e-05,
      "loss": 0.6442,
      "step": 7110
    },
    {
      "epoch": 0.5709703287890938,
      "grad_norm": 1.2502461671829224,
      "learning_rate": 2.4357008844813724e-05,
      "loss": 0.6199,
      "step": 7120
    },
    {
      "epoch": 0.5717722534081796,
      "grad_norm": 1.2649667263031006,
      "learning_rate": 2.4348968105065668e-05,
      "loss": 0.6643,
      "step": 7130
    },
    {
      "epoch": 0.5725741780272654,
      "grad_norm": 1.5358110666275024,
      "learning_rate": 2.434092736531761e-05,
      "loss": 0.6894,
      "step": 7140
    },
    {
      "epoch": 0.5733761026463512,
      "grad_norm": 1.7191540002822876,
      "learning_rate": 2.4332886625569553e-05,
      "loss": 0.6535,
      "step": 7150
    },
    {
      "epoch": 0.5741780272654371,
      "grad_norm": 1.580820083618164,
      "learning_rate": 2.4324845885821497e-05,
      "loss": 0.6442,
      "step": 7160
    },
    {
      "epoch": 0.5749799518845229,
      "grad_norm": 1.5099056959152222,
      "learning_rate": 2.4316805146073438e-05,
      "loss": 0.6674,
      "step": 7170
    },
    {
      "epoch": 0.5757818765036087,
      "grad_norm": 1.3577016592025757,
      "learning_rate": 2.4308764406325383e-05,
      "loss": 0.6318,
      "step": 7180
    },
    {
      "epoch": 0.5765838011226945,
      "grad_norm": 1.3216521739959717,
      "learning_rate": 2.4300723666577323e-05,
      "loss": 0.6367,
      "step": 7190
    },
    {
      "epoch": 0.5773857257417803,
      "grad_norm": 1.706687092781067,
      "learning_rate": 2.429268292682927e-05,
      "loss": 0.7135,
      "step": 7200
    },
    {
      "epoch": 0.5781876503608661,
      "grad_norm": 1.474128246307373,
      "learning_rate": 2.4284642187081212e-05,
      "loss": 0.6803,
      "step": 7210
    },
    {
      "epoch": 0.5789895749799518,
      "grad_norm": 1.6140666007995605,
      "learning_rate": 2.4276601447333156e-05,
      "loss": 0.6763,
      "step": 7220
    },
    {
      "epoch": 0.5797914995990376,
      "grad_norm": 1.666592001914978,
      "learning_rate": 2.4268560707585097e-05,
      "loss": 0.7189,
      "step": 7230
    },
    {
      "epoch": 0.5805934242181235,
      "grad_norm": 1.34894597530365,
      "learning_rate": 2.426051996783704e-05,
      "loss": 0.6483,
      "step": 7240
    },
    {
      "epoch": 0.5813953488372093,
      "grad_norm": 1.8397992849349976,
      "learning_rate": 2.4252479228088986e-05,
      "loss": 0.7221,
      "step": 7250
    },
    {
      "epoch": 0.5821972734562951,
      "grad_norm": 1.4339399337768555,
      "learning_rate": 2.424443848834093e-05,
      "loss": 0.6256,
      "step": 7260
    },
    {
      "epoch": 0.5829991980753809,
      "grad_norm": 1.4581068754196167,
      "learning_rate": 2.423639774859287e-05,
      "loss": 0.6839,
      "step": 7270
    },
    {
      "epoch": 0.5838011226944667,
      "grad_norm": 1.834567904472351,
      "learning_rate": 2.4228357008844815e-05,
      "loss": 0.7546,
      "step": 7280
    },
    {
      "epoch": 0.5846030473135525,
      "grad_norm": 1.2312943935394287,
      "learning_rate": 2.4220316269096756e-05,
      "loss": 0.6338,
      "step": 7290
    },
    {
      "epoch": 0.5854049719326383,
      "grad_norm": 1.5554276704788208,
      "learning_rate": 2.42122755293487e-05,
      "loss": 0.6264,
      "step": 7300
    },
    {
      "epoch": 0.5862068965517241,
      "grad_norm": 1.6027227640151978,
      "learning_rate": 2.4204234789600645e-05,
      "loss": 0.626,
      "step": 7310
    },
    {
      "epoch": 0.58700882117081,
      "grad_norm": 1.3009828329086304,
      "learning_rate": 2.4196194049852586e-05,
      "loss": 0.6497,
      "step": 7320
    },
    {
      "epoch": 0.5878107457898958,
      "grad_norm": 1.7115610837936401,
      "learning_rate": 2.418815331010453e-05,
      "loss": 0.7092,
      "step": 7330
    },
    {
      "epoch": 0.5886126704089816,
      "grad_norm": 1.416305661201477,
      "learning_rate": 2.418011257035647e-05,
      "loss": 0.6636,
      "step": 7340
    },
    {
      "epoch": 0.5894145950280674,
      "grad_norm": 1.6352729797363281,
      "learning_rate": 2.417207183060842e-05,
      "loss": 0.65,
      "step": 7350
    },
    {
      "epoch": 0.5902165196471532,
      "grad_norm": 1.3543329238891602,
      "learning_rate": 2.416403109086036e-05,
      "loss": 0.6017,
      "step": 7360
    },
    {
      "epoch": 0.591018444266239,
      "grad_norm": 1.246993899345398,
      "learning_rate": 2.4155990351112304e-05,
      "loss": 0.6064,
      "step": 7370
    },
    {
      "epoch": 0.5918203688853247,
      "grad_norm": 1.6611446142196655,
      "learning_rate": 2.4147949611364245e-05,
      "loss": 0.6761,
      "step": 7380
    },
    {
      "epoch": 0.5926222935044105,
      "grad_norm": 1.7344567775726318,
      "learning_rate": 2.4139908871616192e-05,
      "loss": 0.6867,
      "step": 7390
    },
    {
      "epoch": 0.5934242181234964,
      "grad_norm": 1.3550808429718018,
      "learning_rate": 2.4131868131868133e-05,
      "loss": 0.6846,
      "step": 7400
    },
    {
      "epoch": 0.5942261427425822,
      "grad_norm": 1.8376104831695557,
      "learning_rate": 2.4123827392120074e-05,
      "loss": 0.683,
      "step": 7410
    },
    {
      "epoch": 0.595028067361668,
      "grad_norm": 1.498412847518921,
      "learning_rate": 2.411578665237202e-05,
      "loss": 0.6353,
      "step": 7420
    },
    {
      "epoch": 0.5958299919807538,
      "grad_norm": 1.4615963697433472,
      "learning_rate": 2.410774591262396e-05,
      "loss": 0.6914,
      "step": 7430
    },
    {
      "epoch": 0.5966319165998396,
      "grad_norm": 1.6269434690475464,
      "learning_rate": 2.4099705172875907e-05,
      "loss": 0.6311,
      "step": 7440
    },
    {
      "epoch": 0.5974338412189254,
      "grad_norm": 1.5810155868530273,
      "learning_rate": 2.4091664433127848e-05,
      "loss": 0.7424,
      "step": 7450
    },
    {
      "epoch": 0.5982357658380112,
      "grad_norm": 1.3318839073181152,
      "learning_rate": 2.4083623693379792e-05,
      "loss": 0.6635,
      "step": 7460
    },
    {
      "epoch": 0.599037690457097,
      "grad_norm": 2.1634774208068848,
      "learning_rate": 2.4075582953631733e-05,
      "loss": 0.7453,
      "step": 7470
    },
    {
      "epoch": 0.5998396150761829,
      "grad_norm": 1.626232624053955,
      "learning_rate": 2.4068346287858483e-05,
      "loss": 0.6588,
      "step": 7480
    },
    {
      "epoch": 0.6006415396952687,
      "grad_norm": 1.3204745054244995,
      "learning_rate": 2.4060305548110427e-05,
      "loss": 0.6018,
      "step": 7490
    },
    {
      "epoch": 0.6014434643143545,
      "grad_norm": 1.7416746616363525,
      "learning_rate": 2.405226480836237e-05,
      "loss": 0.7071,
      "step": 7500
    },
    {
      "epoch": 0.6022453889334403,
      "grad_norm": 1.7814481258392334,
      "learning_rate": 2.4044224068614312e-05,
      "loss": 0.6777,
      "step": 7510
    },
    {
      "epoch": 0.603047313552526,
      "grad_norm": 1.6145960092544556,
      "learning_rate": 2.4036183328866256e-05,
      "loss": 0.7093,
      "step": 7520
    },
    {
      "epoch": 0.6038492381716118,
      "grad_norm": 1.468445062637329,
      "learning_rate": 2.4028142589118197e-05,
      "loss": 0.7115,
      "step": 7530
    },
    {
      "epoch": 0.6046511627906976,
      "grad_norm": 1.4567890167236328,
      "learning_rate": 2.4020101849370145e-05,
      "loss": 0.7344,
      "step": 7540
    },
    {
      "epoch": 0.6054530874097834,
      "grad_norm": 1.5717805624008179,
      "learning_rate": 2.4012061109622086e-05,
      "loss": 0.7029,
      "step": 7550
    },
    {
      "epoch": 0.6062550120288693,
      "grad_norm": 1.5281498432159424,
      "learning_rate": 2.400402036987403e-05,
      "loss": 0.6294,
      "step": 7560
    },
    {
      "epoch": 0.6070569366479551,
      "grad_norm": 1.4618169069290161,
      "learning_rate": 2.399597963012597e-05,
      "loss": 0.6445,
      "step": 7570
    },
    {
      "epoch": 0.6078588612670409,
      "grad_norm": 1.6376748085021973,
      "learning_rate": 2.3987938890377915e-05,
      "loss": 0.6848,
      "step": 7580
    },
    {
      "epoch": 0.6086607858861267,
      "grad_norm": 1.34766685962677,
      "learning_rate": 2.397989815062986e-05,
      "loss": 0.6626,
      "step": 7590
    },
    {
      "epoch": 0.6094627105052125,
      "grad_norm": 1.434476613998413,
      "learning_rate": 2.3971857410881804e-05,
      "loss": 0.6693,
      "step": 7600
    },
    {
      "epoch": 0.6102646351242983,
      "grad_norm": 1.6238644123077393,
      "learning_rate": 2.3963816671133745e-05,
      "loss": 0.6575,
      "step": 7610
    },
    {
      "epoch": 0.6110665597433841,
      "grad_norm": 1.419975996017456,
      "learning_rate": 2.3955775931385686e-05,
      "loss": 0.6873,
      "step": 7620
    },
    {
      "epoch": 0.6118684843624699,
      "grad_norm": 1.5538275241851807,
      "learning_rate": 2.394773519163763e-05,
      "loss": 0.6978,
      "step": 7630
    },
    {
      "epoch": 0.6126704089815558,
      "grad_norm": 1.3709203004837036,
      "learning_rate": 2.3939694451889574e-05,
      "loss": 0.6597,
      "step": 7640
    },
    {
      "epoch": 0.6134723336006416,
      "grad_norm": 1.3569422960281372,
      "learning_rate": 2.393165371214152e-05,
      "loss": 0.7252,
      "step": 7650
    },
    {
      "epoch": 0.6142742582197274,
      "grad_norm": 1.4501760005950928,
      "learning_rate": 2.392361297239346e-05,
      "loss": 0.6941,
      "step": 7660
    },
    {
      "epoch": 0.6150761828388132,
      "grad_norm": 1.7873798608779907,
      "learning_rate": 2.3915572232645404e-05,
      "loss": 0.6856,
      "step": 7670
    },
    {
      "epoch": 0.615878107457899,
      "grad_norm": 1.2325406074523926,
      "learning_rate": 2.3907531492897348e-05,
      "loss": 0.7417,
      "step": 7680
    },
    {
      "epoch": 0.6166800320769847,
      "grad_norm": 1.4722753763198853,
      "learning_rate": 2.3899490753149292e-05,
      "loss": 0.7214,
      "step": 7690
    },
    {
      "epoch": 0.6174819566960705,
      "grad_norm": 1.2363460063934326,
      "learning_rate": 2.3891450013401233e-05,
      "loss": 0.6476,
      "step": 7700
    },
    {
      "epoch": 0.6182838813151563,
      "grad_norm": 1.5177451372146606,
      "learning_rate": 2.3883409273653178e-05,
      "loss": 0.6633,
      "step": 7710
    },
    {
      "epoch": 0.6190858059342422,
      "grad_norm": 1.6389787197113037,
      "learning_rate": 2.387536853390512e-05,
      "loss": 0.6648,
      "step": 7720
    },
    {
      "epoch": 0.619887730553328,
      "grad_norm": 1.5645545721054077,
      "learning_rate": 2.3867327794157063e-05,
      "loss": 0.6982,
      "step": 7730
    },
    {
      "epoch": 0.6206896551724138,
      "grad_norm": 1.5563817024230957,
      "learning_rate": 2.3859287054409007e-05,
      "loss": 0.6335,
      "step": 7740
    },
    {
      "epoch": 0.6214915797914996,
      "grad_norm": 1.348140001296997,
      "learning_rate": 2.3851246314660948e-05,
      "loss": 0.6367,
      "step": 7750
    },
    {
      "epoch": 0.6222935044105854,
      "grad_norm": 1.6972074508666992,
      "learning_rate": 2.3843205574912892e-05,
      "loss": 0.6667,
      "step": 7760
    },
    {
      "epoch": 0.6230954290296712,
      "grad_norm": 1.4394199848175049,
      "learning_rate": 2.3835164835164833e-05,
      "loss": 0.6503,
      "step": 7770
    },
    {
      "epoch": 0.623897353648757,
      "grad_norm": 1.4313286542892456,
      "learning_rate": 2.382712409541678e-05,
      "loss": 0.7066,
      "step": 7780
    },
    {
      "epoch": 0.6246992782678428,
      "grad_norm": 1.6840189695358276,
      "learning_rate": 2.3819083355668722e-05,
      "loss": 0.6218,
      "step": 7790
    },
    {
      "epoch": 0.6255012028869287,
      "grad_norm": 1.36837637424469,
      "learning_rate": 2.3811042615920666e-05,
      "loss": 0.706,
      "step": 7800
    },
    {
      "epoch": 0.6263031275060145,
      "grad_norm": 1.546742558479309,
      "learning_rate": 2.3803001876172607e-05,
      "loss": 0.6354,
      "step": 7810
    },
    {
      "epoch": 0.6271050521251003,
      "grad_norm": 1.444999098777771,
      "learning_rate": 2.379496113642455e-05,
      "loss": 0.6327,
      "step": 7820
    },
    {
      "epoch": 0.627906976744186,
      "grad_norm": 1.3940327167510986,
      "learning_rate": 2.3786920396676495e-05,
      "loss": 0.6509,
      "step": 7830
    },
    {
      "epoch": 0.6287089013632718,
      "grad_norm": 1.4994806051254272,
      "learning_rate": 2.377887965692844e-05,
      "loss": 0.6461,
      "step": 7840
    },
    {
      "epoch": 0.6295108259823576,
      "grad_norm": 1.574729084968567,
      "learning_rate": 2.377083891718038e-05,
      "loss": 0.65,
      "step": 7850
    },
    {
      "epoch": 0.6303127506014434,
      "grad_norm": 1.4350061416625977,
      "learning_rate": 2.3762798177432325e-05,
      "loss": 0.6621,
      "step": 7860
    },
    {
      "epoch": 0.6311146752205292,
      "grad_norm": 1.4073131084442139,
      "learning_rate": 2.375475743768427e-05,
      "loss": 0.6835,
      "step": 7870
    },
    {
      "epoch": 0.6319165998396151,
      "grad_norm": 1.4048455953598022,
      "learning_rate": 2.374671669793621e-05,
      "loss": 0.7757,
      "step": 7880
    },
    {
      "epoch": 0.6327185244587009,
      "grad_norm": 1.3121910095214844,
      "learning_rate": 2.3738675958188154e-05,
      "loss": 0.6406,
      "step": 7890
    },
    {
      "epoch": 0.6335204490777867,
      "grad_norm": 1.2420154809951782,
      "learning_rate": 2.3730635218440095e-05,
      "loss": 0.6993,
      "step": 7900
    },
    {
      "epoch": 0.6343223736968725,
      "grad_norm": 1.6821855306625366,
      "learning_rate": 2.372259447869204e-05,
      "loss": 0.6932,
      "step": 7910
    },
    {
      "epoch": 0.6351242983159583,
      "grad_norm": 1.3297191858291626,
      "learning_rate": 2.3714553738943984e-05,
      "loss": 0.6542,
      "step": 7920
    },
    {
      "epoch": 0.6359262229350441,
      "grad_norm": 1.654143214225769,
      "learning_rate": 2.3706512999195928e-05,
      "loss": 0.6843,
      "step": 7930
    },
    {
      "epoch": 0.6367281475541299,
      "grad_norm": 1.5098918676376343,
      "learning_rate": 2.369847225944787e-05,
      "loss": 0.7212,
      "step": 7940
    },
    {
      "epoch": 0.6375300721732157,
      "grad_norm": 1.5260941982269287,
      "learning_rate": 2.3690431519699813e-05,
      "loss": 0.7156,
      "step": 7950
    },
    {
      "epoch": 0.6383319967923016,
      "grad_norm": 1.3204742670059204,
      "learning_rate": 2.3682390779951754e-05,
      "loss": 0.6893,
      "step": 7960
    },
    {
      "epoch": 0.6391339214113874,
      "grad_norm": 1.5333858728408813,
      "learning_rate": 2.3674350040203702e-05,
      "loss": 0.6135,
      "step": 7970
    },
    {
      "epoch": 0.6399358460304732,
      "grad_norm": 1.5395864248275757,
      "learning_rate": 2.3666309300455643e-05,
      "loss": 0.7413,
      "step": 7980
    },
    {
      "epoch": 0.640737770649559,
      "grad_norm": 1.544877529144287,
      "learning_rate": 2.3658268560707584e-05,
      "loss": 0.7434,
      "step": 7990
    },
    {
      "epoch": 0.6415396952686447,
      "grad_norm": 1.3235459327697754,
      "learning_rate": 2.3650227820959528e-05,
      "loss": 0.6463,
      "step": 8000
    },
    {
      "epoch": 0.6423416198877305,
      "grad_norm": 1.2824525833129883,
      "learning_rate": 2.364218708121147e-05,
      "loss": 0.6863,
      "step": 8010
    },
    {
      "epoch": 0.6431435445068163,
      "grad_norm": 1.3462104797363281,
      "learning_rate": 2.3634146341463417e-05,
      "loss": 0.6105,
      "step": 8020
    },
    {
      "epoch": 0.6439454691259021,
      "grad_norm": 1.3761237859725952,
      "learning_rate": 2.3626105601715358e-05,
      "loss": 0.7095,
      "step": 8030
    },
    {
      "epoch": 0.644747393744988,
      "grad_norm": 1.4721766710281372,
      "learning_rate": 2.3618064861967302e-05,
      "loss": 0.6507,
      "step": 8040
    },
    {
      "epoch": 0.6455493183640738,
      "grad_norm": 1.498293161392212,
      "learning_rate": 2.3610024122219243e-05,
      "loss": 0.7162,
      "step": 8050
    },
    {
      "epoch": 0.6463512429831596,
      "grad_norm": 1.583693265914917,
      "learning_rate": 2.360198338247119e-05,
      "loss": 0.7066,
      "step": 8060
    },
    {
      "epoch": 0.6471531676022454,
      "grad_norm": 1.287359595298767,
      "learning_rate": 2.359394264272313e-05,
      "loss": 0.6141,
      "step": 8070
    },
    {
      "epoch": 0.6479550922213312,
      "grad_norm": 1.3595892190933228,
      "learning_rate": 2.3585901902975076e-05,
      "loss": 0.6722,
      "step": 8080
    },
    {
      "epoch": 0.648757016840417,
      "grad_norm": 1.3608605861663818,
      "learning_rate": 2.3577861163227017e-05,
      "loss": 0.6162,
      "step": 8090
    },
    {
      "epoch": 0.6495589414595028,
      "grad_norm": 1.6392322778701782,
      "learning_rate": 2.356982042347896e-05,
      "loss": 0.6317,
      "step": 8100
    },
    {
      "epoch": 0.6503608660785886,
      "grad_norm": 1.7616558074951172,
      "learning_rate": 2.3561779683730905e-05,
      "loss": 0.6187,
      "step": 8110
    },
    {
      "epoch": 0.6511627906976745,
      "grad_norm": 1.6534514427185059,
      "learning_rate": 2.3553738943982846e-05,
      "loss": 0.6417,
      "step": 8120
    },
    {
      "epoch": 0.6519647153167603,
      "grad_norm": 1.5980764627456665,
      "learning_rate": 2.354569820423479e-05,
      "loss": 0.7265,
      "step": 8130
    },
    {
      "epoch": 0.652766639935846,
      "grad_norm": 1.4976580142974854,
      "learning_rate": 2.353765746448673e-05,
      "loss": 0.6179,
      "step": 8140
    },
    {
      "epoch": 0.6535685645549318,
      "grad_norm": 1.2588005065917969,
      "learning_rate": 2.3529616724738676e-05,
      "loss": 0.6798,
      "step": 8150
    },
    {
      "epoch": 0.6543704891740176,
      "grad_norm": 1.4106016159057617,
      "learning_rate": 2.352157598499062e-05,
      "loss": 0.6368,
      "step": 8160
    },
    {
      "epoch": 0.6551724137931034,
      "grad_norm": 1.572182059288025,
      "learning_rate": 2.3513535245242564e-05,
      "loss": 0.7525,
      "step": 8170
    },
    {
      "epoch": 0.6559743384121892,
      "grad_norm": 1.6379425525665283,
      "learning_rate": 2.3505494505494505e-05,
      "loss": 0.596,
      "step": 8180
    },
    {
      "epoch": 0.656776263031275,
      "grad_norm": 1.5827901363372803,
      "learning_rate": 2.349745376574645e-05,
      "loss": 0.6095,
      "step": 8190
    },
    {
      "epoch": 0.6575781876503609,
      "grad_norm": 1.190811276435852,
      "learning_rate": 2.348941302599839e-05,
      "loss": 0.6573,
      "step": 8200
    },
    {
      "epoch": 0.6583801122694467,
      "grad_norm": 1.6309674978256226,
      "learning_rate": 2.3481372286250338e-05,
      "loss": 0.6593,
      "step": 8210
    },
    {
      "epoch": 0.6591820368885325,
      "grad_norm": 1.3941704034805298,
      "learning_rate": 2.347333154650228e-05,
      "loss": 0.6923,
      "step": 8220
    },
    {
      "epoch": 0.6599839615076183,
      "grad_norm": 1.4193907976150513,
      "learning_rate": 2.3465290806754223e-05,
      "loss": 0.6402,
      "step": 8230
    },
    {
      "epoch": 0.6607858861267041,
      "grad_norm": 1.4109399318695068,
      "learning_rate": 2.3457250067006164e-05,
      "loss": 0.6499,
      "step": 8240
    },
    {
      "epoch": 0.6615878107457899,
      "grad_norm": 1.6135509014129639,
      "learning_rate": 2.3449209327258108e-05,
      "loss": 0.6382,
      "step": 8250
    },
    {
      "epoch": 0.6623897353648757,
      "grad_norm": 1.3875833749771118,
      "learning_rate": 2.3441168587510053e-05,
      "loss": 0.6868,
      "step": 8260
    },
    {
      "epoch": 0.6631916599839615,
      "grad_norm": 1.7267488241195679,
      "learning_rate": 2.3433127847761993e-05,
      "loss": 0.6694,
      "step": 8270
    },
    {
      "epoch": 0.6639935846030474,
      "grad_norm": 1.5713422298431396,
      "learning_rate": 2.3425087108013938e-05,
      "loss": 0.6063,
      "step": 8280
    },
    {
      "epoch": 0.6647955092221332,
      "grad_norm": 1.3839322328567505,
      "learning_rate": 2.341704636826588e-05,
      "loss": 0.5958,
      "step": 8290
    },
    {
      "epoch": 0.6655974338412189,
      "grad_norm": 1.3006577491760254,
      "learning_rate": 2.3409005628517826e-05,
      "loss": 0.6423,
      "step": 8300
    },
    {
      "epoch": 0.6663993584603047,
      "grad_norm": 1.560510277748108,
      "learning_rate": 2.3400964888769767e-05,
      "loss": 0.6806,
      "step": 8310
    },
    {
      "epoch": 0.6672012830793905,
      "grad_norm": 1.5439229011535645,
      "learning_rate": 2.339292414902171e-05,
      "loss": 0.813,
      "step": 8320
    },
    {
      "epoch": 0.6680032076984763,
      "grad_norm": 1.5203354358673096,
      "learning_rate": 2.3384883409273652e-05,
      "loss": 0.6788,
      "step": 8330
    },
    {
      "epoch": 0.6688051323175621,
      "grad_norm": 1.260633111000061,
      "learning_rate": 2.3376842669525597e-05,
      "loss": 0.6818,
      "step": 8340
    },
    {
      "epoch": 0.6696070569366479,
      "grad_norm": 1.5600262880325317,
      "learning_rate": 2.336880192977754e-05,
      "loss": 0.6942,
      "step": 8350
    },
    {
      "epoch": 0.6704089815557338,
      "grad_norm": 1.7840181589126587,
      "learning_rate": 2.3360761190029485e-05,
      "loss": 0.6355,
      "step": 8360
    },
    {
      "epoch": 0.6712109061748196,
      "grad_norm": 1.6135222911834717,
      "learning_rate": 2.3352720450281426e-05,
      "loss": 0.7088,
      "step": 8370
    },
    {
      "epoch": 0.6720128307939054,
      "grad_norm": 1.4443055391311646,
      "learning_rate": 2.3344679710533367e-05,
      "loss": 0.6615,
      "step": 8380
    },
    {
      "epoch": 0.6728147554129912,
      "grad_norm": 1.6683852672576904,
      "learning_rate": 2.3336638970785315e-05,
      "loss": 0.6179,
      "step": 8390
    },
    {
      "epoch": 0.673616680032077,
      "grad_norm": 1.56551194190979,
      "learning_rate": 2.3328598231037256e-05,
      "loss": 0.6818,
      "step": 8400
    },
    {
      "epoch": 0.6744186046511628,
      "grad_norm": 1.3222646713256836,
      "learning_rate": 2.33205574912892e-05,
      "loss": 0.5821,
      "step": 8410
    },
    {
      "epoch": 0.6752205292702486,
      "grad_norm": 1.3669103384017944,
      "learning_rate": 2.331251675154114e-05,
      "loss": 0.652,
      "step": 8420
    },
    {
      "epoch": 0.6760224538893344,
      "grad_norm": 1.4360406398773193,
      "learning_rate": 2.3304476011793085e-05,
      "loss": 0.5965,
      "step": 8430
    },
    {
      "epoch": 0.6768243785084203,
      "grad_norm": 1.3546627759933472,
      "learning_rate": 2.329643527204503e-05,
      "loss": 0.7132,
      "step": 8440
    },
    {
      "epoch": 0.677626303127506,
      "grad_norm": 1.4911179542541504,
      "learning_rate": 2.3288394532296974e-05,
      "loss": 0.6928,
      "step": 8450
    },
    {
      "epoch": 0.6784282277465918,
      "grad_norm": 1.4453922510147095,
      "learning_rate": 2.3280353792548915e-05,
      "loss": 0.6388,
      "step": 8460
    },
    {
      "epoch": 0.6792301523656776,
      "grad_norm": 1.4255704879760742,
      "learning_rate": 2.327231305280086e-05,
      "loss": 0.6289,
      "step": 8470
    },
    {
      "epoch": 0.6800320769847634,
      "grad_norm": 1.467756748199463,
      "learning_rate": 2.32642723130528e-05,
      "loss": 0.6608,
      "step": 8480
    },
    {
      "epoch": 0.6808340016038492,
      "grad_norm": 1.5753271579742432,
      "learning_rate": 2.3256231573304747e-05,
      "loss": 0.7086,
      "step": 8490
    },
    {
      "epoch": 0.681635926222935,
      "grad_norm": 1.3637570142745972,
      "learning_rate": 2.324819083355669e-05,
      "loss": 0.6783,
      "step": 8500
    },
    {
      "epoch": 0.6824378508420208,
      "grad_norm": 1.4708364009857178,
      "learning_rate": 2.324015009380863e-05,
      "loss": 0.6637,
      "step": 8510
    },
    {
      "epoch": 0.6832397754611067,
      "grad_norm": 1.3607527017593384,
      "learning_rate": 2.3232109354060574e-05,
      "loss": 0.6142,
      "step": 8520
    },
    {
      "epoch": 0.6840417000801925,
      "grad_norm": 1.357495903968811,
      "learning_rate": 2.3224068614312514e-05,
      "loss": 0.6879,
      "step": 8530
    },
    {
      "epoch": 0.6848436246992783,
      "grad_norm": 1.4100370407104492,
      "learning_rate": 2.3216027874564462e-05,
      "loss": 0.6684,
      "step": 8540
    },
    {
      "epoch": 0.6856455493183641,
      "grad_norm": 1.8402122259140015,
      "learning_rate": 2.3207987134816403e-05,
      "loss": 0.7103,
      "step": 8550
    },
    {
      "epoch": 0.6864474739374499,
      "grad_norm": 1.5928481817245483,
      "learning_rate": 2.3199946395068347e-05,
      "loss": 0.6634,
      "step": 8560
    },
    {
      "epoch": 0.6872493985565357,
      "grad_norm": 1.6394203901290894,
      "learning_rate": 2.3191905655320288e-05,
      "loss": 0.6556,
      "step": 8570
    },
    {
      "epoch": 0.6880513231756215,
      "grad_norm": 1.6228772401809692,
      "learning_rate": 2.3183864915572236e-05,
      "loss": 0.6342,
      "step": 8580
    },
    {
      "epoch": 0.6888532477947072,
      "grad_norm": 1.5670281648635864,
      "learning_rate": 2.3175824175824177e-05,
      "loss": 0.6425,
      "step": 8590
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 1.4748194217681885,
      "learning_rate": 2.316778343607612e-05,
      "loss": 0.672,
      "step": 8600
    },
    {
      "epoch": 0.6904570970328789,
      "grad_norm": 1.7396392822265625,
      "learning_rate": 2.3159742696328062e-05,
      "loss": 0.6465,
      "step": 8610
    },
    {
      "epoch": 0.6912590216519647,
      "grad_norm": 1.517343282699585,
      "learning_rate": 2.3151701956580006e-05,
      "loss": 0.6297,
      "step": 8620
    },
    {
      "epoch": 0.6920609462710505,
      "grad_norm": 1.7317347526550293,
      "learning_rate": 2.314366121683195e-05,
      "loss": 0.6917,
      "step": 8630
    },
    {
      "epoch": 0.6928628708901363,
      "grad_norm": 1.5530383586883545,
      "learning_rate": 2.313562047708389e-05,
      "loss": 0.6796,
      "step": 8640
    },
    {
      "epoch": 0.6936647955092221,
      "grad_norm": 1.331114411354065,
      "learning_rate": 2.3127579737335836e-05,
      "loss": 0.5993,
      "step": 8650
    },
    {
      "epoch": 0.6944667201283079,
      "grad_norm": 1.7665373086929321,
      "learning_rate": 2.3119538997587777e-05,
      "loss": 0.6332,
      "step": 8660
    },
    {
      "epoch": 0.6952686447473937,
      "grad_norm": 1.5135129690170288,
      "learning_rate": 2.311149825783972e-05,
      "loss": 0.6313,
      "step": 8670
    },
    {
      "epoch": 0.6960705693664796,
      "grad_norm": 1.5314767360687256,
      "learning_rate": 2.3103457518091665e-05,
      "loss": 0.6752,
      "step": 8680
    },
    {
      "epoch": 0.6968724939855654,
      "grad_norm": 1.5127780437469482,
      "learning_rate": 2.309541677834361e-05,
      "loss": 0.6959,
      "step": 8690
    },
    {
      "epoch": 0.6976744186046512,
      "grad_norm": 1.7154964208602905,
      "learning_rate": 2.308737603859555e-05,
      "loss": 0.7837,
      "step": 8700
    },
    {
      "epoch": 0.698476343223737,
      "grad_norm": 1.3798431158065796,
      "learning_rate": 2.3079335298847495e-05,
      "loss": 0.6602,
      "step": 8710
    },
    {
      "epoch": 0.6992782678428228,
      "grad_norm": 1.3601187467575073,
      "learning_rate": 2.3071294559099436e-05,
      "loss": 0.6578,
      "step": 8720
    },
    {
      "epoch": 0.7000801924619086,
      "grad_norm": 1.7398688793182373,
      "learning_rate": 2.3063253819351383e-05,
      "loss": 0.6366,
      "step": 8730
    },
    {
      "epoch": 0.7008821170809943,
      "grad_norm": 1.370004653930664,
      "learning_rate": 2.3055213079603324e-05,
      "loss": 0.705,
      "step": 8740
    },
    {
      "epoch": 0.7016840417000801,
      "grad_norm": 1.1609314680099487,
      "learning_rate": 2.304717233985527e-05,
      "loss": 0.5626,
      "step": 8750
    },
    {
      "epoch": 0.702485966319166,
      "grad_norm": 1.4067585468292236,
      "learning_rate": 2.303913160010721e-05,
      "loss": 0.7088,
      "step": 8760
    },
    {
      "epoch": 0.7032878909382518,
      "grad_norm": 1.7085057497024536,
      "learning_rate": 2.3031090860359154e-05,
      "loss": 0.7417,
      "step": 8770
    },
    {
      "epoch": 0.7040898155573376,
      "grad_norm": 1.4117330312728882,
      "learning_rate": 2.3023050120611098e-05,
      "loss": 0.7254,
      "step": 8780
    },
    {
      "epoch": 0.7048917401764234,
      "grad_norm": 1.666351556777954,
      "learning_rate": 2.301500938086304e-05,
      "loss": 0.646,
      "step": 8790
    },
    {
      "epoch": 0.7056936647955092,
      "grad_norm": 1.504648208618164,
      "learning_rate": 2.3006968641114983e-05,
      "loss": 0.6805,
      "step": 8800
    },
    {
      "epoch": 0.706495589414595,
      "grad_norm": 1.4368337392807007,
      "learning_rate": 2.2998927901366924e-05,
      "loss": 0.6965,
      "step": 8810
    },
    {
      "epoch": 0.7072975140336808,
      "grad_norm": 1.8408488035202026,
      "learning_rate": 2.2990887161618872e-05,
      "loss": 0.602,
      "step": 8820
    },
    {
      "epoch": 0.7080994386527666,
      "grad_norm": 1.498738408088684,
      "learning_rate": 2.2982846421870813e-05,
      "loss": 0.7701,
      "step": 8830
    },
    {
      "epoch": 0.7089013632718525,
      "grad_norm": 1.5537358522415161,
      "learning_rate": 2.2974805682122757e-05,
      "loss": 0.7174,
      "step": 8840
    },
    {
      "epoch": 0.7097032878909383,
      "grad_norm": 1.496809720993042,
      "learning_rate": 2.2966764942374698e-05,
      "loss": 0.6925,
      "step": 8850
    },
    {
      "epoch": 0.7105052125100241,
      "grad_norm": 1.6306389570236206,
      "learning_rate": 2.2958724202626642e-05,
      "loss": 0.6636,
      "step": 8860
    },
    {
      "epoch": 0.7113071371291099,
      "grad_norm": 1.5448029041290283,
      "learning_rate": 2.2950683462878586e-05,
      "loss": 0.6613,
      "step": 8870
    },
    {
      "epoch": 0.7121090617481957,
      "grad_norm": 1.5875195264816284,
      "learning_rate": 2.294264272313053e-05,
      "loss": 0.6668,
      "step": 8880
    },
    {
      "epoch": 0.7129109863672815,
      "grad_norm": 1.7146098613739014,
      "learning_rate": 2.293460198338247e-05,
      "loss": 0.6496,
      "step": 8890
    },
    {
      "epoch": 0.7137129109863672,
      "grad_norm": 1.352565050125122,
      "learning_rate": 2.2926561243634413e-05,
      "loss": 0.6248,
      "step": 8900
    },
    {
      "epoch": 0.714514835605453,
      "grad_norm": 1.6391113996505737,
      "learning_rate": 2.2918520503886357e-05,
      "loss": 0.6505,
      "step": 8910
    },
    {
      "epoch": 0.7153167602245389,
      "grad_norm": 1.2682714462280273,
      "learning_rate": 2.29104797641383e-05,
      "loss": 0.6191,
      "step": 8920
    },
    {
      "epoch": 0.7161186848436247,
      "grad_norm": 1.5236296653747559,
      "learning_rate": 2.2902439024390245e-05,
      "loss": 0.6527,
      "step": 8930
    },
    {
      "epoch": 0.7169206094627105,
      "grad_norm": 1.3599692583084106,
      "learning_rate": 2.2894398284642186e-05,
      "loss": 0.669,
      "step": 8940
    },
    {
      "epoch": 0.7177225340817963,
      "grad_norm": 1.4250624179840088,
      "learning_rate": 2.288635754489413e-05,
      "loss": 0.6583,
      "step": 8950
    },
    {
      "epoch": 0.7185244587008821,
      "grad_norm": 1.5861575603485107,
      "learning_rate": 2.2878316805146075e-05,
      "loss": 0.665,
      "step": 8960
    },
    {
      "epoch": 0.7193263833199679,
      "grad_norm": 1.3487495183944702,
      "learning_rate": 2.287027606539802e-05,
      "loss": 0.678,
      "step": 8970
    },
    {
      "epoch": 0.7201283079390537,
      "grad_norm": 1.4986158609390259,
      "learning_rate": 2.286223532564996e-05,
      "loss": 0.6832,
      "step": 8980
    },
    {
      "epoch": 0.7209302325581395,
      "grad_norm": 1.419221043586731,
      "learning_rate": 2.2854194585901904e-05,
      "loss": 0.6273,
      "step": 8990
    },
    {
      "epoch": 0.7217321571772254,
      "grad_norm": 1.5663084983825684,
      "learning_rate": 2.2846153846153845e-05,
      "loss": 0.7035,
      "step": 9000
    },
    {
      "epoch": 0.7225340817963112,
      "grad_norm": 1.5089938640594482,
      "learning_rate": 2.2838113106405793e-05,
      "loss": 0.6362,
      "step": 9010
    },
    {
      "epoch": 0.723336006415397,
      "grad_norm": 1.5320806503295898,
      "learning_rate": 2.2830072366657734e-05,
      "loss": 0.5904,
      "step": 9020
    },
    {
      "epoch": 0.7241379310344828,
      "grad_norm": 1.4939173460006714,
      "learning_rate": 2.2822031626909675e-05,
      "loss": 0.6027,
      "step": 9030
    },
    {
      "epoch": 0.7249398556535686,
      "grad_norm": 1.6538349390029907,
      "learning_rate": 2.281399088716162e-05,
      "loss": 0.6523,
      "step": 9040
    },
    {
      "epoch": 0.7257417802726543,
      "grad_norm": 1.3756611347198486,
      "learning_rate": 2.280595014741356e-05,
      "loss": 0.5819,
      "step": 9050
    },
    {
      "epoch": 0.7265437048917401,
      "grad_norm": 1.6490975618362427,
      "learning_rate": 2.2797909407665508e-05,
      "loss": 0.7216,
      "step": 9060
    },
    {
      "epoch": 0.7273456295108259,
      "grad_norm": 1.362243890762329,
      "learning_rate": 2.278986866791745e-05,
      "loss": 0.6532,
      "step": 9070
    },
    {
      "epoch": 0.7281475541299118,
      "grad_norm": 1.4858181476593018,
      "learning_rate": 2.2781827928169393e-05,
      "loss": 0.674,
      "step": 9080
    },
    {
      "epoch": 0.7289494787489976,
      "grad_norm": 1.6253901720046997,
      "learning_rate": 2.2773787188421334e-05,
      "loss": 0.6949,
      "step": 9090
    },
    {
      "epoch": 0.7297514033680834,
      "grad_norm": 1.5666394233703613,
      "learning_rate": 2.2765746448673278e-05,
      "loss": 0.6057,
      "step": 9100
    },
    {
      "epoch": 0.7305533279871692,
      "grad_norm": 1.6006929874420166,
      "learning_rate": 2.2757705708925222e-05,
      "loss": 0.6634,
      "step": 9110
    },
    {
      "epoch": 0.731355252606255,
      "grad_norm": 1.4604007005691528,
      "learning_rate": 2.2749664969177167e-05,
      "loss": 0.6504,
      "step": 9120
    },
    {
      "epoch": 0.7321571772253408,
      "grad_norm": 1.3895481824874878,
      "learning_rate": 2.2741624229429108e-05,
      "loss": 0.6596,
      "step": 9130
    },
    {
      "epoch": 0.7329591018444266,
      "grad_norm": 1.4345145225524902,
      "learning_rate": 2.2733583489681052e-05,
      "loss": 0.71,
      "step": 9140
    },
    {
      "epoch": 0.7337610264635124,
      "grad_norm": 1.494054913520813,
      "learning_rate": 2.2725542749932996e-05,
      "loss": 0.6767,
      "step": 9150
    },
    {
      "epoch": 0.7345629510825983,
      "grad_norm": 1.5380204916000366,
      "learning_rate": 2.2717502010184937e-05,
      "loss": 0.6724,
      "step": 9160
    },
    {
      "epoch": 0.7353648757016841,
      "grad_norm": 1.4803428649902344,
      "learning_rate": 2.270946127043688e-05,
      "loss": 0.6895,
      "step": 9170
    },
    {
      "epoch": 0.7361668003207699,
      "grad_norm": 1.6676987409591675,
      "learning_rate": 2.2701420530688822e-05,
      "loss": 0.6929,
      "step": 9180
    },
    {
      "epoch": 0.7369687249398557,
      "grad_norm": 1.4128493070602417,
      "learning_rate": 2.2693379790940766e-05,
      "loss": 0.7146,
      "step": 9190
    },
    {
      "epoch": 0.7377706495589414,
      "grad_norm": 1.561131477355957,
      "learning_rate": 2.268533905119271e-05,
      "loss": 0.6429,
      "step": 9200
    },
    {
      "epoch": 0.7385725741780272,
      "grad_norm": 1.8200875520706177,
      "learning_rate": 2.2677298311444655e-05,
      "loss": 0.6631,
      "step": 9210
    },
    {
      "epoch": 0.739374498797113,
      "grad_norm": 1.3447819948196411,
      "learning_rate": 2.2669257571696596e-05,
      "loss": 0.645,
      "step": 9220
    },
    {
      "epoch": 0.7401764234161988,
      "grad_norm": 1.6287815570831299,
      "learning_rate": 2.266121683194854e-05,
      "loss": 0.6837,
      "step": 9230
    },
    {
      "epoch": 0.7409783480352847,
      "grad_norm": 1.3640576601028442,
      "learning_rate": 2.265317609220048e-05,
      "loss": 0.5699,
      "step": 9240
    },
    {
      "epoch": 0.7417802726543705,
      "grad_norm": 1.38620924949646,
      "learning_rate": 2.264513535245243e-05,
      "loss": 0.6286,
      "step": 9250
    },
    {
      "epoch": 0.7425821972734563,
      "grad_norm": 1.727220892906189,
      "learning_rate": 2.263709461270437e-05,
      "loss": 0.6913,
      "step": 9260
    },
    {
      "epoch": 0.7433841218925421,
      "grad_norm": 1.3944283723831177,
      "learning_rate": 2.2629053872956314e-05,
      "loss": 0.6681,
      "step": 9270
    },
    {
      "epoch": 0.7441860465116279,
      "grad_norm": 1.4341381788253784,
      "learning_rate": 2.2621013133208255e-05,
      "loss": 0.6911,
      "step": 9280
    },
    {
      "epoch": 0.7449879711307137,
      "grad_norm": 1.8614869117736816,
      "learning_rate": 2.2612972393460196e-05,
      "loss": 0.6547,
      "step": 9290
    },
    {
      "epoch": 0.7457898957497995,
      "grad_norm": 2.0286171436309814,
      "learning_rate": 2.2604931653712144e-05,
      "loss": 0.7277,
      "step": 9300
    },
    {
      "epoch": 0.7465918203688853,
      "grad_norm": 1.333455204963684,
      "learning_rate": 2.2596890913964084e-05,
      "loss": 0.6409,
      "step": 9310
    },
    {
      "epoch": 0.7473937449879712,
      "grad_norm": 1.5355974435806274,
      "learning_rate": 2.258885017421603e-05,
      "loss": 0.6839,
      "step": 9320
    },
    {
      "epoch": 0.748195669607057,
      "grad_norm": 1.726668357849121,
      "learning_rate": 2.258080943446797e-05,
      "loss": 0.6975,
      "step": 9330
    },
    {
      "epoch": 0.7489975942261428,
      "grad_norm": 1.4531466960906982,
      "learning_rate": 2.2572768694719917e-05,
      "loss": 0.6668,
      "step": 9340
    },
    {
      "epoch": 0.7497995188452286,
      "grad_norm": 1.621229887008667,
      "learning_rate": 2.2564727954971858e-05,
      "loss": 0.6512,
      "step": 9350
    },
    {
      "epoch": 0.7506014434643143,
      "grad_norm": 1.2683968544006348,
      "learning_rate": 2.2556687215223802e-05,
      "loss": 0.6756,
      "step": 9360
    },
    {
      "epoch": 0.7514033680834001,
      "grad_norm": 1.096062421798706,
      "learning_rate": 2.2548646475475743e-05,
      "loss": 0.6168,
      "step": 9370
    },
    {
      "epoch": 0.7522052927024859,
      "grad_norm": 1.3602193593978882,
      "learning_rate": 2.2540605735727688e-05,
      "loss": 0.715,
      "step": 9380
    },
    {
      "epoch": 0.7530072173215717,
      "grad_norm": 1.4195295572280884,
      "learning_rate": 2.2532564995979632e-05,
      "loss": 0.6333,
      "step": 9390
    },
    {
      "epoch": 0.7538091419406576,
      "grad_norm": 1.5775458812713623,
      "learning_rate": 2.2524524256231573e-05,
      "loss": 0.6411,
      "step": 9400
    },
    {
      "epoch": 0.7546110665597434,
      "grad_norm": 1.559654712677002,
      "learning_rate": 2.2516483516483517e-05,
      "loss": 0.659,
      "step": 9410
    },
    {
      "epoch": 0.7554129911788292,
      "grad_norm": 1.3684532642364502,
      "learning_rate": 2.2508442776735458e-05,
      "loss": 0.6372,
      "step": 9420
    },
    {
      "epoch": 0.756214915797915,
      "grad_norm": 1.664876937866211,
      "learning_rate": 2.2500402036987402e-05,
      "loss": 0.6585,
      "step": 9430
    },
    {
      "epoch": 0.7570168404170008,
      "grad_norm": 1.4620904922485352,
      "learning_rate": 2.2492361297239347e-05,
      "loss": 0.5816,
      "step": 9440
    },
    {
      "epoch": 0.7578187650360866,
      "grad_norm": 1.6000369787216187,
      "learning_rate": 2.248432055749129e-05,
      "loss": 0.6666,
      "step": 9450
    },
    {
      "epoch": 0.7586206896551724,
      "grad_norm": 1.5176031589508057,
      "learning_rate": 2.2476279817743232e-05,
      "loss": 0.7362,
      "step": 9460
    },
    {
      "epoch": 0.7594226142742582,
      "grad_norm": 1.9480907917022705,
      "learning_rate": 2.2468239077995176e-05,
      "loss": 0.6796,
      "step": 9470
    },
    {
      "epoch": 0.7602245388933441,
      "grad_norm": 1.7057052850723267,
      "learning_rate": 2.2460198338247117e-05,
      "loss": 0.6408,
      "step": 9480
    },
    {
      "epoch": 0.7610264635124299,
      "grad_norm": 1.5373895168304443,
      "learning_rate": 2.2452157598499065e-05,
      "loss": 0.7309,
      "step": 9490
    },
    {
      "epoch": 0.7618283881315157,
      "grad_norm": 1.7589935064315796,
      "learning_rate": 2.2444116858751006e-05,
      "loss": 0.6919,
      "step": 9500
    },
    {
      "epoch": 0.7626303127506014,
      "grad_norm": 1.5324363708496094,
      "learning_rate": 2.243607611900295e-05,
      "loss": 0.6012,
      "step": 9510
    },
    {
      "epoch": 0.7634322373696872,
      "grad_norm": 1.3218270540237427,
      "learning_rate": 2.242803537925489e-05,
      "loss": 0.6521,
      "step": 9520
    },
    {
      "epoch": 0.764234161988773,
      "grad_norm": 1.35279381275177,
      "learning_rate": 2.2419994639506835e-05,
      "loss": 0.6298,
      "step": 9530
    },
    {
      "epoch": 0.7650360866078588,
      "grad_norm": 1.5037565231323242,
      "learning_rate": 2.241195389975878e-05,
      "loss": 0.5843,
      "step": 9540
    },
    {
      "epoch": 0.7658380112269446,
      "grad_norm": 1.615881085395813,
      "learning_rate": 2.240391316001072e-05,
      "loss": 0.65,
      "step": 9550
    },
    {
      "epoch": 0.7666399358460305,
      "grad_norm": 1.4802460670471191,
      "learning_rate": 2.2395872420262665e-05,
      "loss": 0.6792,
      "step": 9560
    },
    {
      "epoch": 0.7674418604651163,
      "grad_norm": 1.5739222764968872,
      "learning_rate": 2.2387831680514605e-05,
      "loss": 0.6366,
      "step": 9570
    },
    {
      "epoch": 0.7682437850842021,
      "grad_norm": 1.5877968072891235,
      "learning_rate": 2.2379790940766553e-05,
      "loss": 0.6942,
      "step": 9580
    },
    {
      "epoch": 0.7690457097032879,
      "grad_norm": 1.2071768045425415,
      "learning_rate": 2.2371750201018494e-05,
      "loss": 0.6502,
      "step": 9590
    },
    {
      "epoch": 0.7698476343223737,
      "grad_norm": 1.6922098398208618,
      "learning_rate": 2.236370946127044e-05,
      "loss": 0.6334,
      "step": 9600
    },
    {
      "epoch": 0.7706495589414595,
      "grad_norm": 1.355512022972107,
      "learning_rate": 2.235566872152238e-05,
      "loss": 0.637,
      "step": 9610
    },
    {
      "epoch": 0.7714514835605453,
      "grad_norm": 1.475191354751587,
      "learning_rate": 2.2347627981774324e-05,
      "loss": 0.6362,
      "step": 9620
    },
    {
      "epoch": 0.7722534081796311,
      "grad_norm": 1.276739239692688,
      "learning_rate": 2.2339587242026268e-05,
      "loss": 0.6363,
      "step": 9630
    },
    {
      "epoch": 0.773055332798717,
      "grad_norm": 1.450940728187561,
      "learning_rate": 2.2331546502278212e-05,
      "loss": 0.697,
      "step": 9640
    },
    {
      "epoch": 0.7738572574178028,
      "grad_norm": 1.3265563249588013,
      "learning_rate": 2.2323505762530153e-05,
      "loss": 0.6714,
      "step": 9650
    },
    {
      "epoch": 0.7746591820368885,
      "grad_norm": 1.4070887565612793,
      "learning_rate": 2.2315465022782094e-05,
      "loss": 0.6374,
      "step": 9660
    },
    {
      "epoch": 0.7754611066559743,
      "grad_norm": 1.4560092687606812,
      "learning_rate": 2.230742428303404e-05,
      "loss": 0.6872,
      "step": 9670
    },
    {
      "epoch": 0.7762630312750601,
      "grad_norm": 1.4650245904922485,
      "learning_rate": 2.2299383543285982e-05,
      "loss": 0.6323,
      "step": 9680
    },
    {
      "epoch": 0.7770649558941459,
      "grad_norm": 1.3958818912506104,
      "learning_rate": 2.2291342803537927e-05,
      "loss": 0.6637,
      "step": 9690
    },
    {
      "epoch": 0.7778668805132317,
      "grad_norm": 1.7501236200332642,
      "learning_rate": 2.2283302063789868e-05,
      "loss": 0.6794,
      "step": 9700
    },
    {
      "epoch": 0.7786688051323175,
      "grad_norm": 1.554792046546936,
      "learning_rate": 2.2275261324041812e-05,
      "loss": 0.6415,
      "step": 9710
    },
    {
      "epoch": 0.7794707297514034,
      "grad_norm": 1.6929129362106323,
      "learning_rate": 2.2267220584293756e-05,
      "loss": 0.6476,
      "step": 9720
    },
    {
      "epoch": 0.7802726543704892,
      "grad_norm": 1.559113621711731,
      "learning_rate": 2.22591798445457e-05,
      "loss": 0.6844,
      "step": 9730
    },
    {
      "epoch": 0.781074578989575,
      "grad_norm": 1.3380666971206665,
      "learning_rate": 2.225113910479764e-05,
      "loss": 0.6768,
      "step": 9740
    },
    {
      "epoch": 0.7818765036086608,
      "grad_norm": 1.4147493839263916,
      "learning_rate": 2.2243098365049586e-05,
      "loss": 0.6533,
      "step": 9750
    },
    {
      "epoch": 0.7826784282277466,
      "grad_norm": 1.4363689422607422,
      "learning_rate": 2.2235057625301527e-05,
      "loss": 0.7009,
      "step": 9760
    },
    {
      "epoch": 0.7834803528468324,
      "grad_norm": 1.4046368598937988,
      "learning_rate": 2.2227016885553474e-05,
      "loss": 0.5868,
      "step": 9770
    },
    {
      "epoch": 0.7842822774659182,
      "grad_norm": 1.6652770042419434,
      "learning_rate": 2.2218976145805415e-05,
      "loss": 0.6698,
      "step": 9780
    },
    {
      "epoch": 0.785084202085004,
      "grad_norm": 1.4506244659423828,
      "learning_rate": 2.2210935406057356e-05,
      "loss": 0.6529,
      "step": 9790
    },
    {
      "epoch": 0.7858861267040899,
      "grad_norm": 1.6525708436965942,
      "learning_rate": 2.22028946663093e-05,
      "loss": 0.7047,
      "step": 9800
    },
    {
      "epoch": 0.7866880513231757,
      "grad_norm": 1.4985742568969727,
      "learning_rate": 2.219485392656124e-05,
      "loss": 0.7448,
      "step": 9810
    },
    {
      "epoch": 0.7874899759422614,
      "grad_norm": 1.6514743566513062,
      "learning_rate": 2.218681318681319e-05,
      "loss": 0.6275,
      "step": 9820
    },
    {
      "epoch": 0.7882919005613472,
      "grad_norm": 1.4187830686569214,
      "learning_rate": 2.217877244706513e-05,
      "loss": 0.688,
      "step": 9830
    },
    {
      "epoch": 0.789093825180433,
      "grad_norm": 1.5476337671279907,
      "learning_rate": 2.2170731707317074e-05,
      "loss": 0.605,
      "step": 9840
    },
    {
      "epoch": 0.7898957497995188,
      "grad_norm": 1.3041794300079346,
      "learning_rate": 2.2162690967569015e-05,
      "loss": 0.6038,
      "step": 9850
    },
    {
      "epoch": 0.7906976744186046,
      "grad_norm": 1.6636464595794678,
      "learning_rate": 2.2154650227820963e-05,
      "loss": 0.6287,
      "step": 9860
    },
    {
      "epoch": 0.7914995990376904,
      "grad_norm": 1.7788476943969727,
      "learning_rate": 2.2146609488072904e-05,
      "loss": 0.7273,
      "step": 9870
    },
    {
      "epoch": 0.7923015236567763,
      "grad_norm": 1.5357460975646973,
      "learning_rate": 2.2138568748324848e-05,
      "loss": 0.6309,
      "step": 9880
    },
    {
      "epoch": 0.7931034482758621,
      "grad_norm": 1.4081048965454102,
      "learning_rate": 2.213052800857679e-05,
      "loss": 0.6712,
      "step": 9890
    },
    {
      "epoch": 0.7939053728949479,
      "grad_norm": 1.4751853942871094,
      "learning_rate": 2.2122487268828733e-05,
      "loss": 0.6358,
      "step": 9900
    },
    {
      "epoch": 0.7947072975140337,
      "grad_norm": 1.7099618911743164,
      "learning_rate": 2.2114446529080677e-05,
      "loss": 0.636,
      "step": 9910
    },
    {
      "epoch": 0.7955092221331195,
      "grad_norm": 1.6330389976501465,
      "learning_rate": 2.210640578933262e-05,
      "loss": 0.6394,
      "step": 9920
    },
    {
      "epoch": 0.7963111467522053,
      "grad_norm": 1.834102988243103,
      "learning_rate": 2.2098365049584563e-05,
      "loss": 0.7199,
      "step": 9930
    },
    {
      "epoch": 0.7971130713712911,
      "grad_norm": 1.5546635389328003,
      "learning_rate": 2.2090324309836504e-05,
      "loss": 0.6727,
      "step": 9940
    },
    {
      "epoch": 0.7979149959903769,
      "grad_norm": 1.4967010021209717,
      "learning_rate": 2.2082283570088448e-05,
      "loss": 0.6007,
      "step": 9950
    },
    {
      "epoch": 0.7987169206094628,
      "grad_norm": 1.5089813470840454,
      "learning_rate": 2.2074242830340392e-05,
      "loss": 0.728,
      "step": 9960
    },
    {
      "epoch": 0.7995188452285485,
      "grad_norm": 1.5843373537063599,
      "learning_rate": 2.2066202090592336e-05,
      "loss": 0.6301,
      "step": 9970
    },
    {
      "epoch": 0.8003207698476343,
      "grad_norm": 1.4262007474899292,
      "learning_rate": 2.2058161350844277e-05,
      "loss": 0.6933,
      "step": 9980
    },
    {
      "epoch": 0.8011226944667201,
      "grad_norm": 1.4996482133865356,
      "learning_rate": 2.205012061109622e-05,
      "loss": 0.6686,
      "step": 9990
    },
    {
      "epoch": 0.8019246190858059,
      "grad_norm": 1.4448363780975342,
      "learning_rate": 2.2042079871348162e-05,
      "loss": 0.6315,
      "step": 10000
    },
    {
      "epoch": 0.8027265437048917,
      "grad_norm": 1.5428794622421265,
      "learning_rate": 2.203403913160011e-05,
      "loss": 0.692,
      "step": 10010
    },
    {
      "epoch": 0.8035284683239775,
      "grad_norm": 1.5630680322647095,
      "learning_rate": 2.202599839185205e-05,
      "loss": 0.7458,
      "step": 10020
    },
    {
      "epoch": 0.8043303929430633,
      "grad_norm": 1.5251336097717285,
      "learning_rate": 2.2017957652103995e-05,
      "loss": 0.6275,
      "step": 10030
    },
    {
      "epoch": 0.8051323175621492,
      "grad_norm": 1.7983497381210327,
      "learning_rate": 2.2009916912355936e-05,
      "loss": 0.6763,
      "step": 10040
    },
    {
      "epoch": 0.805934242181235,
      "grad_norm": 1.6624298095703125,
      "learning_rate": 2.200187617260788e-05,
      "loss": 0.7191,
      "step": 10050
    },
    {
      "epoch": 0.8067361668003208,
      "grad_norm": 1.6112223863601685,
      "learning_rate": 2.1993835432859825e-05,
      "loss": 0.587,
      "step": 10060
    },
    {
      "epoch": 0.8075380914194066,
      "grad_norm": 1.321194052696228,
      "learning_rate": 2.1985794693111766e-05,
      "loss": 0.6045,
      "step": 10070
    },
    {
      "epoch": 0.8083400160384924,
      "grad_norm": 1.3784130811691284,
      "learning_rate": 2.197775395336371e-05,
      "loss": 0.6624,
      "step": 10080
    },
    {
      "epoch": 0.8091419406575782,
      "grad_norm": 1.6372244358062744,
      "learning_rate": 2.196971321361565e-05,
      "loss": 0.6714,
      "step": 10090
    },
    {
      "epoch": 0.809943865276664,
      "grad_norm": 1.6072931289672852,
      "learning_rate": 2.19616724738676e-05,
      "loss": 0.6784,
      "step": 10100
    },
    {
      "epoch": 0.8107457898957497,
      "grad_norm": 1.657578468322754,
      "learning_rate": 2.195363173411954e-05,
      "loss": 0.6593,
      "step": 10110
    },
    {
      "epoch": 0.8115477145148356,
      "grad_norm": 1.762872338294983,
      "learning_rate": 2.1945590994371484e-05,
      "loss": 0.6617,
      "step": 10120
    },
    {
      "epoch": 0.8123496391339214,
      "grad_norm": 2.117156982421875,
      "learning_rate": 2.1937550254623425e-05,
      "loss": 0.6812,
      "step": 10130
    },
    {
      "epoch": 0.8131515637530072,
      "grad_norm": 1.2454249858856201,
      "learning_rate": 2.192950951487537e-05,
      "loss": 0.567,
      "step": 10140
    },
    {
      "epoch": 0.813953488372093,
      "grad_norm": 1.7480720281600952,
      "learning_rate": 2.1921468775127313e-05,
      "loss": 0.6724,
      "step": 10150
    },
    {
      "epoch": 0.8147554129911788,
      "grad_norm": 1.5144813060760498,
      "learning_rate": 2.1913428035379258e-05,
      "loss": 0.5892,
      "step": 10160
    },
    {
      "epoch": 0.8155573376102646,
      "grad_norm": 1.3297450542449951,
      "learning_rate": 2.19053872956312e-05,
      "loss": 0.673,
      "step": 10170
    },
    {
      "epoch": 0.8163592622293504,
      "grad_norm": 1.725665807723999,
      "learning_rate": 2.189734655588314e-05,
      "loss": 0.6816,
      "step": 10180
    },
    {
      "epoch": 0.8171611868484362,
      "grad_norm": 1.6091234683990479,
      "learning_rate": 2.1889305816135084e-05,
      "loss": 0.5615,
      "step": 10190
    },
    {
      "epoch": 0.8179631114675221,
      "grad_norm": 1.5791319608688354,
      "learning_rate": 2.1881265076387028e-05,
      "loss": 0.7319,
      "step": 10200
    },
    {
      "epoch": 0.8187650360866079,
      "grad_norm": 1.4076675176620483,
      "learning_rate": 2.1873224336638972e-05,
      "loss": 0.7144,
      "step": 10210
    },
    {
      "epoch": 0.8195669607056937,
      "grad_norm": 1.7638376951217651,
      "learning_rate": 2.1865183596890913e-05,
      "loss": 0.6697,
      "step": 10220
    },
    {
      "epoch": 0.8203688853247795,
      "grad_norm": 1.9160417318344116,
      "learning_rate": 2.1857142857142857e-05,
      "loss": 0.6323,
      "step": 10230
    },
    {
      "epoch": 0.8211708099438653,
      "grad_norm": 1.5033191442489624,
      "learning_rate": 2.1849102117394802e-05,
      "loss": 0.6801,
      "step": 10240
    },
    {
      "epoch": 0.8219727345629511,
      "grad_norm": 1.516488790512085,
      "learning_rate": 2.1841061377646746e-05,
      "loss": 0.6977,
      "step": 10250
    },
    {
      "epoch": 0.8227746591820368,
      "grad_norm": 1.4169211387634277,
      "learning_rate": 2.1833020637898687e-05,
      "loss": 0.5943,
      "step": 10260
    },
    {
      "epoch": 0.8235765838011226,
      "grad_norm": 1.549169659614563,
      "learning_rate": 2.182497989815063e-05,
      "loss": 0.6071,
      "step": 10270
    },
    {
      "epoch": 0.8243785084202085,
      "grad_norm": 1.3948620557785034,
      "learning_rate": 2.1816939158402572e-05,
      "loss": 0.7044,
      "step": 10280
    },
    {
      "epoch": 0.8251804330392943,
      "grad_norm": 1.6498494148254395,
      "learning_rate": 2.180889841865452e-05,
      "loss": 0.6827,
      "step": 10290
    },
    {
      "epoch": 0.8259823576583801,
      "grad_norm": 1.4764387607574463,
      "learning_rate": 2.180085767890646e-05,
      "loss": 0.6289,
      "step": 10300
    },
    {
      "epoch": 0.8267842822774659,
      "grad_norm": 1.5707108974456787,
      "learning_rate": 2.17928169391584e-05,
      "loss": 0.681,
      "step": 10310
    },
    {
      "epoch": 0.8275862068965517,
      "grad_norm": 1.7729215621948242,
      "learning_rate": 2.1784776199410346e-05,
      "loss": 0.6443,
      "step": 10320
    },
    {
      "epoch": 0.8283881315156375,
      "grad_norm": 1.675121784210205,
      "learning_rate": 2.1776735459662287e-05,
      "loss": 0.6637,
      "step": 10330
    },
    {
      "epoch": 0.8291900561347233,
      "grad_norm": 1.6239780187606812,
      "learning_rate": 2.1768694719914234e-05,
      "loss": 0.7086,
      "step": 10340
    },
    {
      "epoch": 0.8299919807538091,
      "grad_norm": 1.5225129127502441,
      "learning_rate": 2.1760653980166175e-05,
      "loss": 0.6104,
      "step": 10350
    },
    {
      "epoch": 0.830793905372895,
      "grad_norm": 1.5815746784210205,
      "learning_rate": 2.175261324041812e-05,
      "loss": 0.6524,
      "step": 10360
    },
    {
      "epoch": 0.8315958299919808,
      "grad_norm": 1.4070730209350586,
      "learning_rate": 2.174457250067006e-05,
      "loss": 0.6891,
      "step": 10370
    },
    {
      "epoch": 0.8323977546110666,
      "grad_norm": 1.53733491897583,
      "learning_rate": 2.1736531760922005e-05,
      "loss": 0.68,
      "step": 10380
    },
    {
      "epoch": 0.8331996792301524,
      "grad_norm": 1.5915088653564453,
      "learning_rate": 2.172849102117395e-05,
      "loss": 0.5988,
      "step": 10390
    },
    {
      "epoch": 0.8340016038492382,
      "grad_norm": 1.5257117748260498,
      "learning_rate": 2.1720450281425893e-05,
      "loss": 0.5892,
      "step": 10400
    },
    {
      "epoch": 0.834803528468324,
      "grad_norm": 1.5397056341171265,
      "learning_rate": 2.1712409541677834e-05,
      "loss": 0.6659,
      "step": 10410
    },
    {
      "epoch": 0.8356054530874097,
      "grad_norm": 1.6231422424316406,
      "learning_rate": 2.170436880192978e-05,
      "loss": 0.6651,
      "step": 10420
    },
    {
      "epoch": 0.8364073777064955,
      "grad_norm": 1.7367140054702759,
      "learning_rate": 2.1696328062181723e-05,
      "loss": 0.744,
      "step": 10430
    },
    {
      "epoch": 0.8372093023255814,
      "grad_norm": 1.5110507011413574,
      "learning_rate": 2.1688287322433664e-05,
      "loss": 0.6508,
      "step": 10440
    },
    {
      "epoch": 0.8380112269446672,
      "grad_norm": 1.5623985528945923,
      "learning_rate": 2.1680246582685608e-05,
      "loss": 0.6525,
      "step": 10450
    },
    {
      "epoch": 0.838813151563753,
      "grad_norm": 1.4744658470153809,
      "learning_rate": 2.167220584293755e-05,
      "loss": 0.7115,
      "step": 10460
    },
    {
      "epoch": 0.8396150761828388,
      "grad_norm": 1.6013407707214355,
      "learning_rate": 2.1664165103189493e-05,
      "loss": 0.678,
      "step": 10470
    },
    {
      "epoch": 0.8404170008019246,
      "grad_norm": 1.451097011566162,
      "learning_rate": 2.1656124363441438e-05,
      "loss": 0.6475,
      "step": 10480
    },
    {
      "epoch": 0.8412189254210104,
      "grad_norm": 1.8249578475952148,
      "learning_rate": 2.1648083623693382e-05,
      "loss": 0.7153,
      "step": 10490
    },
    {
      "epoch": 0.8420208500400962,
      "grad_norm": 1.7209030389785767,
      "learning_rate": 2.1640042883945323e-05,
      "loss": 0.7787,
      "step": 10500
    },
    {
      "epoch": 0.842822774659182,
      "grad_norm": 1.525815725326538,
      "learning_rate": 2.1632002144197267e-05,
      "loss": 0.6365,
      "step": 10510
    },
    {
      "epoch": 0.8436246992782679,
      "grad_norm": 1.4548536539077759,
      "learning_rate": 2.1623961404449208e-05,
      "loss": 0.6097,
      "step": 10520
    },
    {
      "epoch": 0.8444266238973537,
      "grad_norm": 1.5962848663330078,
      "learning_rate": 2.1615920664701156e-05,
      "loss": 0.6587,
      "step": 10530
    },
    {
      "epoch": 0.8452285485164395,
      "grad_norm": 1.6248242855072021,
      "learning_rate": 2.1607879924953097e-05,
      "loss": 0.6384,
      "step": 10540
    },
    {
      "epoch": 0.8460304731355253,
      "grad_norm": 1.6311055421829224,
      "learning_rate": 2.159983918520504e-05,
      "loss": 0.5878,
      "step": 10550
    },
    {
      "epoch": 0.846832397754611,
      "grad_norm": 1.6749095916748047,
      "learning_rate": 2.1591798445456982e-05,
      "loss": 0.6903,
      "step": 10560
    },
    {
      "epoch": 0.8476343223736968,
      "grad_norm": 1.6055773496627808,
      "learning_rate": 2.1583757705708923e-05,
      "loss": 0.6628,
      "step": 10570
    },
    {
      "epoch": 0.8484362469927826,
      "grad_norm": 1.6581366062164307,
      "learning_rate": 2.157571696596087e-05,
      "loss": 0.6046,
      "step": 10580
    },
    {
      "epoch": 0.8492381716118684,
      "grad_norm": 1.6553406715393066,
      "learning_rate": 2.156767622621281e-05,
      "loss": 0.6679,
      "step": 10590
    },
    {
      "epoch": 0.8500400962309543,
      "grad_norm": 1.285948634147644,
      "learning_rate": 2.1559635486464756e-05,
      "loss": 0.6333,
      "step": 10600
    },
    {
      "epoch": 0.8508420208500401,
      "grad_norm": 1.5054417848587036,
      "learning_rate": 2.1551594746716696e-05,
      "loss": 0.6514,
      "step": 10610
    },
    {
      "epoch": 0.8516439454691259,
      "grad_norm": 1.4932152032852173,
      "learning_rate": 2.1543554006968644e-05,
      "loss": 0.574,
      "step": 10620
    },
    {
      "epoch": 0.8524458700882117,
      "grad_norm": 1.4924259185791016,
      "learning_rate": 2.1535513267220585e-05,
      "loss": 0.744,
      "step": 10630
    },
    {
      "epoch": 0.8532477947072975,
      "grad_norm": 1.6186552047729492,
      "learning_rate": 2.152747252747253e-05,
      "loss": 0.6322,
      "step": 10640
    },
    {
      "epoch": 0.8540497193263833,
      "grad_norm": 1.3470120429992676,
      "learning_rate": 2.151943178772447e-05,
      "loss": 0.691,
      "step": 10650
    },
    {
      "epoch": 0.8548516439454691,
      "grad_norm": 1.5723739862442017,
      "learning_rate": 2.1511391047976414e-05,
      "loss": 0.7162,
      "step": 10660
    },
    {
      "epoch": 0.8556535685645549,
      "grad_norm": 1.543594241142273,
      "learning_rate": 2.150335030822836e-05,
      "loss": 0.6706,
      "step": 10670
    },
    {
      "epoch": 0.8564554931836408,
      "grad_norm": 1.5856622457504272,
      "learning_rate": 2.14953095684803e-05,
      "loss": 0.7392,
      "step": 10680
    },
    {
      "epoch": 0.8572574178027266,
      "grad_norm": 1.6706514358520508,
      "learning_rate": 2.1487268828732244e-05,
      "loss": 0.6788,
      "step": 10690
    },
    {
      "epoch": 0.8580593424218124,
      "grad_norm": 1.5857644081115723,
      "learning_rate": 2.1479228088984185e-05,
      "loss": 0.5957,
      "step": 10700
    },
    {
      "epoch": 0.8588612670408982,
      "grad_norm": 1.8194674253463745,
      "learning_rate": 2.147118734923613e-05,
      "loss": 0.6911,
      "step": 10710
    },
    {
      "epoch": 0.859663191659984,
      "grad_norm": 1.5268003940582275,
      "learning_rate": 2.1463146609488073e-05,
      "loss": 0.6933,
      "step": 10720
    },
    {
      "epoch": 0.8604651162790697,
      "grad_norm": 1.615464448928833,
      "learning_rate": 2.1455105869740018e-05,
      "loss": 0.7532,
      "step": 10730
    },
    {
      "epoch": 0.8612670408981555,
      "grad_norm": 1.6440356969833374,
      "learning_rate": 2.144706512999196e-05,
      "loss": 0.7161,
      "step": 10740
    },
    {
      "epoch": 0.8620689655172413,
      "grad_norm": 1.3037400245666504,
      "learning_rate": 2.1439024390243903e-05,
      "loss": 0.6798,
      "step": 10750
    },
    {
      "epoch": 0.8628708901363272,
      "grad_norm": 1.333130955696106,
      "learning_rate": 2.1430983650495844e-05,
      "loss": 0.627,
      "step": 10760
    },
    {
      "epoch": 0.863672814755413,
      "grad_norm": 1.6557567119598389,
      "learning_rate": 2.142294291074779e-05,
      "loss": 0.6615,
      "step": 10770
    },
    {
      "epoch": 0.8644747393744988,
      "grad_norm": 1.545514464378357,
      "learning_rate": 2.1414902170999732e-05,
      "loss": 0.6714,
      "step": 10780
    },
    {
      "epoch": 0.8652766639935846,
      "grad_norm": 1.548508644104004,
      "learning_rate": 2.1407665505226482e-05,
      "loss": 0.6401,
      "step": 10790
    },
    {
      "epoch": 0.8660785886126704,
      "grad_norm": 1.612942099571228,
      "learning_rate": 2.1399624765478423e-05,
      "loss": 0.6794,
      "step": 10800
    },
    {
      "epoch": 0.8668805132317562,
      "grad_norm": 1.626323938369751,
      "learning_rate": 2.1391584025730367e-05,
      "loss": 0.6387,
      "step": 10810
    },
    {
      "epoch": 0.867682437850842,
      "grad_norm": 1.8996576070785522,
      "learning_rate": 2.138354328598231e-05,
      "loss": 0.7535,
      "step": 10820
    },
    {
      "epoch": 0.8684843624699278,
      "grad_norm": 1.4799044132232666,
      "learning_rate": 2.1375502546234256e-05,
      "loss": 0.5506,
      "step": 10830
    },
    {
      "epoch": 0.8692862870890137,
      "grad_norm": 1.3550182580947876,
      "learning_rate": 2.1367461806486197e-05,
      "loss": 0.6386,
      "step": 10840
    },
    {
      "epoch": 0.8700882117080995,
      "grad_norm": 1.8696095943450928,
      "learning_rate": 2.135942106673814e-05,
      "loss": 0.6674,
      "step": 10850
    },
    {
      "epoch": 0.8708901363271853,
      "grad_norm": 1.4971479177474976,
      "learning_rate": 2.1351380326990082e-05,
      "loss": 0.6777,
      "step": 10860
    },
    {
      "epoch": 0.871692060946271,
      "grad_norm": 1.60968017578125,
      "learning_rate": 2.134333958724203e-05,
      "loss": 0.6462,
      "step": 10870
    },
    {
      "epoch": 0.8724939855653568,
      "grad_norm": 1.4302093982696533,
      "learning_rate": 2.133529884749397e-05,
      "loss": 0.674,
      "step": 10880
    },
    {
      "epoch": 0.8732959101844426,
      "grad_norm": 1.9120841026306152,
      "learning_rate": 2.132725810774591e-05,
      "loss": 0.7267,
      "step": 10890
    },
    {
      "epoch": 0.8740978348035284,
      "grad_norm": 1.5277385711669922,
      "learning_rate": 2.1319217367997856e-05,
      "loss": 0.6485,
      "step": 10900
    },
    {
      "epoch": 0.8748997594226142,
      "grad_norm": 1.5768303871154785,
      "learning_rate": 2.13111766282498e-05,
      "loss": 0.6003,
      "step": 10910
    },
    {
      "epoch": 0.8757016840417001,
      "grad_norm": 1.6544601917266846,
      "learning_rate": 2.1303135888501744e-05,
      "loss": 0.6693,
      "step": 10920
    },
    {
      "epoch": 0.8765036086607859,
      "grad_norm": 1.5813674926757812,
      "learning_rate": 2.1295095148753685e-05,
      "loss": 0.6767,
      "step": 10930
    },
    {
      "epoch": 0.8773055332798717,
      "grad_norm": 1.5111252069473267,
      "learning_rate": 2.128705440900563e-05,
      "loss": 0.6709,
      "step": 10940
    },
    {
      "epoch": 0.8781074578989575,
      "grad_norm": 1.390989899635315,
      "learning_rate": 2.127901366925757e-05,
      "loss": 0.6558,
      "step": 10950
    },
    {
      "epoch": 0.8789093825180433,
      "grad_norm": 1.5803028345108032,
      "learning_rate": 2.1270972929509518e-05,
      "loss": 0.6668,
      "step": 10960
    },
    {
      "epoch": 0.8797113071371291,
      "grad_norm": 1.3946950435638428,
      "learning_rate": 2.126293218976146e-05,
      "loss": 0.6378,
      "step": 10970
    },
    {
      "epoch": 0.8805132317562149,
      "grad_norm": 1.8146389722824097,
      "learning_rate": 2.1254891450013403e-05,
      "loss": 0.6733,
      "step": 10980
    },
    {
      "epoch": 0.8813151563753007,
      "grad_norm": 1.8933945894241333,
      "learning_rate": 2.1246850710265344e-05,
      "loss": 0.6695,
      "step": 10990
    },
    {
      "epoch": 0.8821170809943866,
      "grad_norm": 1.574724555015564,
      "learning_rate": 2.123880997051729e-05,
      "loss": 0.6078,
      "step": 11000
    },
    {
      "epoch": 0.8829190056134724,
      "grad_norm": 1.7676550149917603,
      "learning_rate": 2.1230769230769233e-05,
      "loss": 0.6995,
      "step": 11010
    },
    {
      "epoch": 0.8837209302325582,
      "grad_norm": 1.8817551136016846,
      "learning_rate": 2.1222728491021174e-05,
      "loss": 0.6738,
      "step": 11020
    },
    {
      "epoch": 0.884522854851644,
      "grad_norm": 1.5119432210922241,
      "learning_rate": 2.1214687751273118e-05,
      "loss": 0.672,
      "step": 11030
    },
    {
      "epoch": 0.8853247794707297,
      "grad_norm": 1.6151436567306519,
      "learning_rate": 2.120664701152506e-05,
      "loss": 0.6528,
      "step": 11040
    },
    {
      "epoch": 0.8861267040898155,
      "grad_norm": 1.3353008031845093,
      "learning_rate": 2.1198606271777003e-05,
      "loss": 0.6573,
      "step": 11050
    },
    {
      "epoch": 0.8869286287089013,
      "grad_norm": 1.8095804452896118,
      "learning_rate": 2.1190565532028947e-05,
      "loss": 0.6652,
      "step": 11060
    },
    {
      "epoch": 0.8877305533279871,
      "grad_norm": 1.6300902366638184,
      "learning_rate": 2.118252479228089e-05,
      "loss": 0.5823,
      "step": 11070
    },
    {
      "epoch": 0.888532477947073,
      "grad_norm": 1.6057814359664917,
      "learning_rate": 2.1174484052532832e-05,
      "loss": 0.6209,
      "step": 11080
    },
    {
      "epoch": 0.8893344025661588,
      "grad_norm": 1.8411976099014282,
      "learning_rate": 2.1166443312784777e-05,
      "loss": 0.6925,
      "step": 11090
    },
    {
      "epoch": 0.8901363271852446,
      "grad_norm": 1.4501392841339111,
      "learning_rate": 2.115840257303672e-05,
      "loss": 0.6049,
      "step": 11100
    },
    {
      "epoch": 0.8909382518043304,
      "grad_norm": 1.7569122314453125,
      "learning_rate": 2.1150361833288665e-05,
      "loss": 0.6644,
      "step": 11110
    },
    {
      "epoch": 0.8917401764234162,
      "grad_norm": 1.5550063848495483,
      "learning_rate": 2.1142321093540606e-05,
      "loss": 0.7057,
      "step": 11120
    },
    {
      "epoch": 0.892542101042502,
      "grad_norm": 1.524433970451355,
      "learning_rate": 2.113428035379255e-05,
      "loss": 0.6076,
      "step": 11130
    },
    {
      "epoch": 0.8933440256615878,
      "grad_norm": 1.5171589851379395,
      "learning_rate": 2.112623961404449e-05,
      "loss": 0.7061,
      "step": 11140
    },
    {
      "epoch": 0.8941459502806736,
      "grad_norm": 1.5300496816635132,
      "learning_rate": 2.1118198874296436e-05,
      "loss": 0.6231,
      "step": 11150
    },
    {
      "epoch": 0.8949478748997595,
      "grad_norm": 1.7642220258712769,
      "learning_rate": 2.111015813454838e-05,
      "loss": 0.6836,
      "step": 11160
    },
    {
      "epoch": 0.8957497995188453,
      "grad_norm": 1.7451320886611938,
      "learning_rate": 2.110211739480032e-05,
      "loss": 0.6736,
      "step": 11170
    },
    {
      "epoch": 0.896551724137931,
      "grad_norm": 1.8786307573318481,
      "learning_rate": 2.1094076655052265e-05,
      "loss": 0.5856,
      "step": 11180
    },
    {
      "epoch": 0.8973536487570168,
      "grad_norm": 1.8560250997543335,
      "learning_rate": 2.1086035915304206e-05,
      "loss": 0.64,
      "step": 11190
    },
    {
      "epoch": 0.8981555733761026,
      "grad_norm": 1.4658429622650146,
      "learning_rate": 2.1077995175556154e-05,
      "loss": 0.6495,
      "step": 11200
    },
    {
      "epoch": 0.8989574979951884,
      "grad_norm": 1.4415427446365356,
      "learning_rate": 2.1069954435808095e-05,
      "loss": 0.672,
      "step": 11210
    },
    {
      "epoch": 0.8997594226142742,
      "grad_norm": 2.0099258422851562,
      "learning_rate": 2.106191369606004e-05,
      "loss": 0.6765,
      "step": 11220
    },
    {
      "epoch": 0.90056134723336,
      "grad_norm": 1.640870213508606,
      "learning_rate": 2.105387295631198e-05,
      "loss": 0.6343,
      "step": 11230
    },
    {
      "epoch": 0.9013632718524459,
      "grad_norm": 1.274356484413147,
      "learning_rate": 2.1045832216563924e-05,
      "loss": 0.617,
      "step": 11240
    },
    {
      "epoch": 0.9021651964715317,
      "grad_norm": 1.7841159105300903,
      "learning_rate": 2.103779147681587e-05,
      "loss": 0.6328,
      "step": 11250
    },
    {
      "epoch": 0.9029671210906175,
      "grad_norm": 1.4490766525268555,
      "learning_rate": 2.102975073706781e-05,
      "loss": 0.5808,
      "step": 11260
    },
    {
      "epoch": 0.9037690457097033,
      "grad_norm": 1.3317091464996338,
      "learning_rate": 2.1021709997319754e-05,
      "loss": 0.6699,
      "step": 11270
    },
    {
      "epoch": 0.9045709703287891,
      "grad_norm": 1.4836986064910889,
      "learning_rate": 2.1013669257571695e-05,
      "loss": 0.6186,
      "step": 11280
    },
    {
      "epoch": 0.9053728949478749,
      "grad_norm": 1.7662863731384277,
      "learning_rate": 2.1005628517823642e-05,
      "loss": 0.6509,
      "step": 11290
    },
    {
      "epoch": 0.9061748195669607,
      "grad_norm": 1.5438884496688843,
      "learning_rate": 2.0997587778075583e-05,
      "loss": 0.6793,
      "step": 11300
    },
    {
      "epoch": 0.9069767441860465,
      "grad_norm": 1.6209596395492554,
      "learning_rate": 2.0989547038327527e-05,
      "loss": 0.6758,
      "step": 11310
    },
    {
      "epoch": 0.9077786688051324,
      "grad_norm": 1.8553240299224854,
      "learning_rate": 2.098150629857947e-05,
      "loss": 0.6867,
      "step": 11320
    },
    {
      "epoch": 0.9085805934242182,
      "grad_norm": 1.7221202850341797,
      "learning_rate": 2.0973465558831413e-05,
      "loss": 0.6262,
      "step": 11330
    },
    {
      "epoch": 0.909382518043304,
      "grad_norm": 1.6741607189178467,
      "learning_rate": 2.0965424819083357e-05,
      "loss": 0.669,
      "step": 11340
    },
    {
      "epoch": 0.9101844426623897,
      "grad_norm": 1.4755192995071411,
      "learning_rate": 2.09573840793353e-05,
      "loss": 0.5622,
      "step": 11350
    },
    {
      "epoch": 0.9109863672814755,
      "grad_norm": 1.5685731172561646,
      "learning_rate": 2.0949343339587242e-05,
      "loss": 0.7214,
      "step": 11360
    },
    {
      "epoch": 0.9117882919005613,
      "grad_norm": 1.7003923654556274,
      "learning_rate": 2.0941302599839186e-05,
      "loss": 0.6841,
      "step": 11370
    },
    {
      "epoch": 0.9125902165196471,
      "grad_norm": 1.525530219078064,
      "learning_rate": 2.0933261860091127e-05,
      "loss": 0.6463,
      "step": 11380
    },
    {
      "epoch": 0.9133921411387329,
      "grad_norm": 1.9976203441619873,
      "learning_rate": 2.092522112034307e-05,
      "loss": 0.6457,
      "step": 11390
    },
    {
      "epoch": 0.9141940657578188,
      "grad_norm": 1.5989713668823242,
      "learning_rate": 2.0917180380595016e-05,
      "loss": 0.6567,
      "step": 11400
    },
    {
      "epoch": 0.9149959903769046,
      "grad_norm": 1.5944924354553223,
      "learning_rate": 2.0909139640846957e-05,
      "loss": 0.6573,
      "step": 11410
    },
    {
      "epoch": 0.9157979149959904,
      "grad_norm": 1.378880500793457,
      "learning_rate": 2.09010989010989e-05,
      "loss": 0.6133,
      "step": 11420
    },
    {
      "epoch": 0.9165998396150762,
      "grad_norm": 1.7691149711608887,
      "learning_rate": 2.0893058161350845e-05,
      "loss": 0.6,
      "step": 11430
    },
    {
      "epoch": 0.917401764234162,
      "grad_norm": 1.50770902633667,
      "learning_rate": 2.088501742160279e-05,
      "loss": 0.7135,
      "step": 11440
    },
    {
      "epoch": 0.9182036888532478,
      "grad_norm": 1.7349623441696167,
      "learning_rate": 2.087697668185473e-05,
      "loss": 0.6886,
      "step": 11450
    },
    {
      "epoch": 0.9190056134723336,
      "grad_norm": 1.5852617025375366,
      "learning_rate": 2.0868935942106675e-05,
      "loss": 0.6775,
      "step": 11460
    },
    {
      "epoch": 0.9198075380914194,
      "grad_norm": 1.46060049533844,
      "learning_rate": 2.0860895202358616e-05,
      "loss": 0.5943,
      "step": 11470
    },
    {
      "epoch": 0.9206094627105053,
      "grad_norm": 1.8658816814422607,
      "learning_rate": 2.0852854462610563e-05,
      "loss": 0.6875,
      "step": 11480
    },
    {
      "epoch": 0.921411387329591,
      "grad_norm": 1.9927054643630981,
      "learning_rate": 2.0844813722862504e-05,
      "loss": 0.6321,
      "step": 11490
    },
    {
      "epoch": 0.9222133119486768,
      "grad_norm": 1.4784067869186401,
      "learning_rate": 2.083677298311445e-05,
      "loss": 0.6357,
      "step": 11500
    },
    {
      "epoch": 0.9230152365677626,
      "grad_norm": 1.4609367847442627,
      "learning_rate": 2.082873224336639e-05,
      "loss": 0.6539,
      "step": 11510
    },
    {
      "epoch": 0.9238171611868484,
      "grad_norm": 1.6365151405334473,
      "learning_rate": 2.082069150361833e-05,
      "loss": 0.7304,
      "step": 11520
    },
    {
      "epoch": 0.9246190858059342,
      "grad_norm": 1.7412540912628174,
      "learning_rate": 2.0812650763870278e-05,
      "loss": 0.6407,
      "step": 11530
    },
    {
      "epoch": 0.92542101042502,
      "grad_norm": 1.6068954467773438,
      "learning_rate": 2.080461002412222e-05,
      "loss": 0.7246,
      "step": 11540
    },
    {
      "epoch": 0.9262229350441058,
      "grad_norm": 1.5381125211715698,
      "learning_rate": 2.0796569284374163e-05,
      "loss": 0.6681,
      "step": 11550
    },
    {
      "epoch": 0.9270248596631917,
      "grad_norm": 1.6544370651245117,
      "learning_rate": 2.0788528544626104e-05,
      "loss": 0.6164,
      "step": 11560
    },
    {
      "epoch": 0.9278267842822775,
      "grad_norm": 1.919067621231079,
      "learning_rate": 2.078048780487805e-05,
      "loss": 0.7425,
      "step": 11570
    },
    {
      "epoch": 0.9286287089013633,
      "grad_norm": 1.393457055091858,
      "learning_rate": 2.0772447065129993e-05,
      "loss": 0.6591,
      "step": 11580
    },
    {
      "epoch": 0.9294306335204491,
      "grad_norm": 1.569411277770996,
      "learning_rate": 2.0764406325381937e-05,
      "loss": 0.6052,
      "step": 11590
    },
    {
      "epoch": 0.9302325581395349,
      "grad_norm": 1.476577877998352,
      "learning_rate": 2.0756365585633878e-05,
      "loss": 0.657,
      "step": 11600
    },
    {
      "epoch": 0.9310344827586207,
      "grad_norm": 1.8229143619537354,
      "learning_rate": 2.0748324845885822e-05,
      "loss": 0.6619,
      "step": 11610
    },
    {
      "epoch": 0.9318364073777065,
      "grad_norm": 1.677626132965088,
      "learning_rate": 2.0740284106137767e-05,
      "loss": 0.6714,
      "step": 11620
    },
    {
      "epoch": 0.9326383319967922,
      "grad_norm": 1.5660537481307983,
      "learning_rate": 2.073224336638971e-05,
      "loss": 0.6596,
      "step": 11630
    },
    {
      "epoch": 0.9334402566158782,
      "grad_norm": 1.6973038911819458,
      "learning_rate": 2.0724202626641652e-05,
      "loss": 0.688,
      "step": 11640
    },
    {
      "epoch": 0.9342421812349639,
      "grad_norm": 1.6163651943206787,
      "learning_rate": 2.0716161886893593e-05,
      "loss": 0.6278,
      "step": 11650
    },
    {
      "epoch": 0.9350441058540497,
      "grad_norm": 1.8677423000335693,
      "learning_rate": 2.0708121147145537e-05,
      "loss": 0.6684,
      "step": 11660
    },
    {
      "epoch": 0.9358460304731355,
      "grad_norm": 1.403778076171875,
      "learning_rate": 2.070008040739748e-05,
      "loss": 0.5476,
      "step": 11670
    },
    {
      "epoch": 0.9366479550922213,
      "grad_norm": 1.7172001600265503,
      "learning_rate": 2.0692039667649426e-05,
      "loss": 0.631,
      "step": 11680
    },
    {
      "epoch": 0.9374498797113071,
      "grad_norm": 1.8007404804229736,
      "learning_rate": 2.0683998927901366e-05,
      "loss": 0.7019,
      "step": 11690
    },
    {
      "epoch": 0.9382518043303929,
      "grad_norm": 1.5682390928268433,
      "learning_rate": 2.067595818815331e-05,
      "loss": 0.668,
      "step": 11700
    },
    {
      "epoch": 0.9390537289494787,
      "grad_norm": 1.350838541984558,
      "learning_rate": 2.066791744840525e-05,
      "loss": 0.6615,
      "step": 11710
    },
    {
      "epoch": 0.9398556535685646,
      "grad_norm": 1.810073733329773,
      "learning_rate": 2.06598767086572e-05,
      "loss": 0.5907,
      "step": 11720
    },
    {
      "epoch": 0.9406575781876504,
      "grad_norm": 1.3841345310211182,
      "learning_rate": 2.065183596890914e-05,
      "loss": 0.6654,
      "step": 11730
    },
    {
      "epoch": 0.9414595028067362,
      "grad_norm": 1.3977068662643433,
      "learning_rate": 2.0643795229161084e-05,
      "loss": 0.6332,
      "step": 11740
    },
    {
      "epoch": 0.942261427425822,
      "grad_norm": 2.1192171573638916,
      "learning_rate": 2.0635754489413025e-05,
      "loss": 0.6869,
      "step": 11750
    },
    {
      "epoch": 0.9430633520449078,
      "grad_norm": 1.39041268825531,
      "learning_rate": 2.062771374966497e-05,
      "loss": 0.5622,
      "step": 11760
    },
    {
      "epoch": 0.9438652766639936,
      "grad_norm": 1.4611897468566895,
      "learning_rate": 2.0619673009916914e-05,
      "loss": 0.667,
      "step": 11770
    },
    {
      "epoch": 0.9446672012830793,
      "grad_norm": 2.105988025665283,
      "learning_rate": 2.0611632270168855e-05,
      "loss": 0.7186,
      "step": 11780
    },
    {
      "epoch": 0.9454691259021651,
      "grad_norm": 1.716004729270935,
      "learning_rate": 2.06035915304208e-05,
      "loss": 0.6466,
      "step": 11790
    },
    {
      "epoch": 0.946271050521251,
      "grad_norm": 1.4571707248687744,
      "learning_rate": 2.059555079067274e-05,
      "loss": 0.6766,
      "step": 11800
    },
    {
      "epoch": 0.9470729751403368,
      "grad_norm": 1.7675467729568481,
      "learning_rate": 2.0587510050924688e-05,
      "loss": 0.6489,
      "step": 11810
    },
    {
      "epoch": 0.9478748997594226,
      "grad_norm": 1.521578311920166,
      "learning_rate": 2.057946931117663e-05,
      "loss": 0.5897,
      "step": 11820
    },
    {
      "epoch": 0.9486768243785084,
      "grad_norm": 1.6106621026992798,
      "learning_rate": 2.0571428571428573e-05,
      "loss": 0.6005,
      "step": 11830
    },
    {
      "epoch": 0.9494787489975942,
      "grad_norm": 1.7111997604370117,
      "learning_rate": 2.0563387831680514e-05,
      "loss": 0.6752,
      "step": 11840
    },
    {
      "epoch": 0.95028067361668,
      "grad_norm": 1.8150758743286133,
      "learning_rate": 2.0555347091932458e-05,
      "loss": 0.6939,
      "step": 11850
    },
    {
      "epoch": 0.9510825982357658,
      "grad_norm": 1.8891501426696777,
      "learning_rate": 2.0547306352184402e-05,
      "loss": 0.6753,
      "step": 11860
    },
    {
      "epoch": 0.9518845228548516,
      "grad_norm": 1.64958918094635,
      "learning_rate": 2.0539265612436347e-05,
      "loss": 0.6595,
      "step": 11870
    },
    {
      "epoch": 0.9526864474739375,
      "grad_norm": 1.296335220336914,
      "learning_rate": 2.0531224872688288e-05,
      "loss": 0.61,
      "step": 11880
    },
    {
      "epoch": 0.9534883720930233,
      "grad_norm": 1.7871085405349731,
      "learning_rate": 2.0523184132940232e-05,
      "loss": 0.7143,
      "step": 11890
    },
    {
      "epoch": 0.9542902967121091,
      "grad_norm": 1.9632889032363892,
      "learning_rate": 2.0515143393192173e-05,
      "loss": 0.5885,
      "step": 11900
    },
    {
      "epoch": 0.9550922213311949,
      "grad_norm": 1.5585951805114746,
      "learning_rate": 2.0507102653444117e-05,
      "loss": 0.6478,
      "step": 11910
    },
    {
      "epoch": 0.9558941459502807,
      "grad_norm": 1.678614616394043,
      "learning_rate": 2.049906191369606e-05,
      "loss": 0.6907,
      "step": 11920
    },
    {
      "epoch": 0.9566960705693665,
      "grad_norm": 2.006092071533203,
      "learning_rate": 2.0491021173948002e-05,
      "loss": 0.6362,
      "step": 11930
    },
    {
      "epoch": 0.9574979951884522,
      "grad_norm": 1.7626819610595703,
      "learning_rate": 2.0482980434199947e-05,
      "loss": 0.6362,
      "step": 11940
    },
    {
      "epoch": 0.958299919807538,
      "grad_norm": 1.4126487970352173,
      "learning_rate": 2.0474939694451887e-05,
      "loss": 0.5753,
      "step": 11950
    },
    {
      "epoch": 0.9591018444266239,
      "grad_norm": 1.4986519813537598,
      "learning_rate": 2.0466898954703835e-05,
      "loss": 0.6236,
      "step": 11960
    },
    {
      "epoch": 0.9599037690457097,
      "grad_norm": 1.8838597536087036,
      "learning_rate": 2.0458858214955776e-05,
      "loss": 0.7348,
      "step": 11970
    },
    {
      "epoch": 0.9607056936647955,
      "grad_norm": 1.875244140625,
      "learning_rate": 2.045081747520772e-05,
      "loss": 0.6735,
      "step": 11980
    },
    {
      "epoch": 0.9615076182838813,
      "grad_norm": 1.5501071214675903,
      "learning_rate": 2.044277673545966e-05,
      "loss": 0.6306,
      "step": 11990
    },
    {
      "epoch": 0.9623095429029671,
      "grad_norm": 1.403921365737915,
      "learning_rate": 2.043473599571161e-05,
      "loss": 0.5564,
      "step": 12000
    },
    {
      "epoch": 0.9631114675220529,
      "grad_norm": 1.8249527215957642,
      "learning_rate": 2.042669525596355e-05,
      "loss": 0.6031,
      "step": 12010
    },
    {
      "epoch": 0.9639133921411387,
      "grad_norm": 1.5871604681015015,
      "learning_rate": 2.0418654516215494e-05,
      "loss": 0.6186,
      "step": 12020
    },
    {
      "epoch": 0.9647153167602245,
      "grad_norm": 1.7052255868911743,
      "learning_rate": 2.0410613776467435e-05,
      "loss": 0.6657,
      "step": 12030
    },
    {
      "epoch": 0.9655172413793104,
      "grad_norm": 1.3706976175308228,
      "learning_rate": 2.0402573036719376e-05,
      "loss": 0.6245,
      "step": 12040
    },
    {
      "epoch": 0.9663191659983962,
      "grad_norm": 1.8971930742263794,
      "learning_rate": 2.0394532296971324e-05,
      "loss": 0.6479,
      "step": 12050
    },
    {
      "epoch": 0.967121090617482,
      "grad_norm": 1.651242971420288,
      "learning_rate": 2.0386491557223264e-05,
      "loss": 0.6421,
      "step": 12060
    },
    {
      "epoch": 0.9679230152365678,
      "grad_norm": 1.6692827939987183,
      "learning_rate": 2.037845081747521e-05,
      "loss": 0.6619,
      "step": 12070
    },
    {
      "epoch": 0.9687249398556536,
      "grad_norm": 1.5448369979858398,
      "learning_rate": 2.037041007772715e-05,
      "loss": 0.6889,
      "step": 12080
    },
    {
      "epoch": 0.9695268644747393,
      "grad_norm": 1.622653603553772,
      "learning_rate": 2.0362369337979094e-05,
      "loss": 0.6059,
      "step": 12090
    },
    {
      "epoch": 0.9703287890938251,
      "grad_norm": 1.7144278287887573,
      "learning_rate": 2.0354328598231038e-05,
      "loss": 0.6059,
      "step": 12100
    },
    {
      "epoch": 0.9711307137129109,
      "grad_norm": 1.4728354215621948,
      "learning_rate": 2.0346287858482983e-05,
      "loss": 0.5661,
      "step": 12110
    },
    {
      "epoch": 0.9719326383319968,
      "grad_norm": 1.4725408554077148,
      "learning_rate": 2.0338247118734923e-05,
      "loss": 0.5955,
      "step": 12120
    },
    {
      "epoch": 0.9727345629510826,
      "grad_norm": 1.4646481275558472,
      "learning_rate": 2.0330206378986868e-05,
      "loss": 0.632,
      "step": 12130
    },
    {
      "epoch": 0.9735364875701684,
      "grad_norm": 1.5307549238204956,
      "learning_rate": 2.032216563923881e-05,
      "loss": 0.6539,
      "step": 12140
    },
    {
      "epoch": 0.9743384121892542,
      "grad_norm": 1.6194391250610352,
      "learning_rate": 2.0314124899490756e-05,
      "loss": 0.6221,
      "step": 12150
    },
    {
      "epoch": 0.97514033680834,
      "grad_norm": 1.7695547342300415,
      "learning_rate": 2.0306084159742697e-05,
      "loss": 0.6653,
      "step": 12160
    },
    {
      "epoch": 0.9759422614274258,
      "grad_norm": 2.0755622386932373,
      "learning_rate": 2.0298043419994638e-05,
      "loss": 0.7093,
      "step": 12170
    },
    {
      "epoch": 0.9767441860465116,
      "grad_norm": 1.7377732992172241,
      "learning_rate": 2.0290002680246582e-05,
      "loss": 0.6017,
      "step": 12180
    },
    {
      "epoch": 0.9775461106655974,
      "grad_norm": 1.6253020763397217,
      "learning_rate": 2.0281961940498527e-05,
      "loss": 0.6946,
      "step": 12190
    },
    {
      "epoch": 0.9783480352846833,
      "grad_norm": 1.633209466934204,
      "learning_rate": 2.027392120075047e-05,
      "loss": 0.6193,
      "step": 12200
    },
    {
      "epoch": 0.9791499599037691,
      "grad_norm": 1.7978041172027588,
      "learning_rate": 2.0265880461002412e-05,
      "loss": 0.6935,
      "step": 12210
    },
    {
      "epoch": 0.9799518845228549,
      "grad_norm": 1.7034307718276978,
      "learning_rate": 2.0257839721254356e-05,
      "loss": 0.6815,
      "step": 12220
    },
    {
      "epoch": 0.9807538091419407,
      "grad_norm": 1.7802218198776245,
      "learning_rate": 2.0249798981506297e-05,
      "loss": 0.6569,
      "step": 12230
    },
    {
      "epoch": 0.9815557337610264,
      "grad_norm": 1.568719744682312,
      "learning_rate": 2.0241758241758245e-05,
      "loss": 0.5961,
      "step": 12240
    },
    {
      "epoch": 0.9823576583801122,
      "grad_norm": 1.5573230981826782,
      "learning_rate": 2.0233717502010186e-05,
      "loss": 0.6071,
      "step": 12250
    },
    {
      "epoch": 0.983159582999198,
      "grad_norm": 1.6355618238449097,
      "learning_rate": 2.022567676226213e-05,
      "loss": 0.5651,
      "step": 12260
    },
    {
      "epoch": 0.9839615076182838,
      "grad_norm": 2.013319253921509,
      "learning_rate": 2.021763602251407e-05,
      "loss": 0.7162,
      "step": 12270
    },
    {
      "epoch": 0.9847634322373697,
      "grad_norm": 1.3374675512313843,
      "learning_rate": 2.0209595282766015e-05,
      "loss": 0.634,
      "step": 12280
    },
    {
      "epoch": 0.9855653568564555,
      "grad_norm": 1.4918874502182007,
      "learning_rate": 2.020155454301796e-05,
      "loss": 0.7053,
      "step": 12290
    },
    {
      "epoch": 0.9863672814755413,
      "grad_norm": 1.600995421409607,
      "learning_rate": 2.01935138032699e-05,
      "loss": 0.6615,
      "step": 12300
    },
    {
      "epoch": 0.9871692060946271,
      "grad_norm": 1.5213685035705566,
      "learning_rate": 2.0185473063521845e-05,
      "loss": 0.6677,
      "step": 12310
    },
    {
      "epoch": 0.9879711307137129,
      "grad_norm": 1.8249447345733643,
      "learning_rate": 2.0177432323773786e-05,
      "loss": 0.6485,
      "step": 12320
    },
    {
      "epoch": 0.9887730553327987,
      "grad_norm": 1.8583205938339233,
      "learning_rate": 2.016939158402573e-05,
      "loss": 0.5917,
      "step": 12330
    },
    {
      "epoch": 0.9895749799518845,
      "grad_norm": 1.8705495595932007,
      "learning_rate": 2.0161350844277674e-05,
      "loss": 0.7132,
      "step": 12340
    },
    {
      "epoch": 0.9903769045709703,
      "grad_norm": 1.474217414855957,
      "learning_rate": 2.015331010452962e-05,
      "loss": 0.6556,
      "step": 12350
    },
    {
      "epoch": 0.9911788291900562,
      "grad_norm": 1.7067214250564575,
      "learning_rate": 2.014526936478156e-05,
      "loss": 0.6421,
      "step": 12360
    },
    {
      "epoch": 0.991980753809142,
      "grad_norm": 1.8331166505813599,
      "learning_rate": 2.0137228625033504e-05,
      "loss": 0.6489,
      "step": 12370
    },
    {
      "epoch": 0.9927826784282278,
      "grad_norm": 1.8284862041473389,
      "learning_rate": 2.0129187885285448e-05,
      "loss": 0.6399,
      "step": 12380
    },
    {
      "epoch": 0.9935846030473136,
      "grad_norm": 1.5430724620819092,
      "learning_rate": 2.0121147145537392e-05,
      "loss": 0.6481,
      "step": 12390
    },
    {
      "epoch": 0.9943865276663993,
      "grad_norm": 1.6345661878585815,
      "learning_rate": 2.0113106405789333e-05,
      "loss": 0.6432,
      "step": 12400
    },
    {
      "epoch": 0.9951884522854851,
      "grad_norm": 2.2294318675994873,
      "learning_rate": 2.0105065666041277e-05,
      "loss": 0.623,
      "step": 12410
    },
    {
      "epoch": 0.9959903769045709,
      "grad_norm": 1.6264369487762451,
      "learning_rate": 2.0097024926293218e-05,
      "loss": 0.6344,
      "step": 12420
    },
    {
      "epoch": 0.9967923015236567,
      "grad_norm": 1.6610454320907593,
      "learning_rate": 2.0088984186545163e-05,
      "loss": 0.6317,
      "step": 12430
    },
    {
      "epoch": 0.9975942261427426,
      "grad_norm": 1.7980177402496338,
      "learning_rate": 2.0080943446797107e-05,
      "loss": 0.6151,
      "step": 12440
    },
    {
      "epoch": 0.9983961507618284,
      "grad_norm": 1.6129801273345947,
      "learning_rate": 2.0072902707049048e-05,
      "loss": 0.6673,
      "step": 12450
    },
    {
      "epoch": 0.9991980753809142,
      "grad_norm": 1.6268315315246582,
      "learning_rate": 2.0064861967300992e-05,
      "loss": 0.6154,
      "step": 12460
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.3658455610275269,
      "learning_rate": 2.0056821227552933e-05,
      "loss": 0.6724,
      "step": 12470
    },
    {
      "epoch": 1.0008019246190858,
      "grad_norm": 1.6743685007095337,
      "learning_rate": 2.004878048780488e-05,
      "loss": 0.6886,
      "step": 12480
    },
    {
      "epoch": 1.0016038492381716,
      "grad_norm": 1.6682652235031128,
      "learning_rate": 2.004073974805682e-05,
      "loss": 0.6031,
      "step": 12490
    },
    {
      "epoch": 1.0024057738572574,
      "grad_norm": 1.6032010316848755,
      "learning_rate": 2.0032699008308766e-05,
      "loss": 0.6652,
      "step": 12500
    },
    {
      "epoch": 1.0032076984763432,
      "grad_norm": 1.885646104812622,
      "learning_rate": 2.0024658268560707e-05,
      "loss": 0.6716,
      "step": 12510
    },
    {
      "epoch": 1.004009623095429,
      "grad_norm": 1.8333353996276855,
      "learning_rate": 2.001661752881265e-05,
      "loss": 0.6084,
      "step": 12520
    },
    {
      "epoch": 1.0048115477145148,
      "grad_norm": 1.4073306322097778,
      "learning_rate": 2.0008576789064595e-05,
      "loss": 0.5698,
      "step": 12530
    },
    {
      "epoch": 1.0056134723336005,
      "grad_norm": 1.6621149778366089,
      "learning_rate": 2.000053604931654e-05,
      "loss": 0.7598,
      "step": 12540
    },
    {
      "epoch": 1.0064153969526863,
      "grad_norm": 1.9990960359573364,
      "learning_rate": 1.999249530956848e-05,
      "loss": 0.6617,
      "step": 12550
    },
    {
      "epoch": 1.0072173215717724,
      "grad_norm": 1.754319429397583,
      "learning_rate": 1.998445456982042e-05,
      "loss": 0.6233,
      "step": 12560
    },
    {
      "epoch": 1.0080192461908581,
      "grad_norm": 1.8155851364135742,
      "learning_rate": 1.997641383007237e-05,
      "loss": 0.5922,
      "step": 12570
    },
    {
      "epoch": 1.008821170809944,
      "grad_norm": 1.7287046909332275,
      "learning_rate": 1.996837309032431e-05,
      "loss": 0.6041,
      "step": 12580
    },
    {
      "epoch": 1.0096230954290297,
      "grad_norm": 1.5283222198486328,
      "learning_rate": 1.9960332350576254e-05,
      "loss": 0.6841,
      "step": 12590
    },
    {
      "epoch": 1.0104250200481155,
      "grad_norm": 1.6527128219604492,
      "learning_rate": 1.9952291610828195e-05,
      "loss": 0.6559,
      "step": 12600
    },
    {
      "epoch": 1.0112269446672013,
      "grad_norm": 1.8089942932128906,
      "learning_rate": 1.994425087108014e-05,
      "loss": 0.6402,
      "step": 12610
    },
    {
      "epoch": 1.012028869286287,
      "grad_norm": 1.5179808139801025,
      "learning_rate": 1.9936210131332084e-05,
      "loss": 0.5925,
      "step": 12620
    },
    {
      "epoch": 1.012830793905373,
      "grad_norm": 2.326981782913208,
      "learning_rate": 1.9928169391584028e-05,
      "loss": 0.6803,
      "step": 12630
    },
    {
      "epoch": 1.0136327185244587,
      "grad_norm": 2.001828670501709,
      "learning_rate": 1.992012865183597e-05,
      "loss": 0.7007,
      "step": 12640
    },
    {
      "epoch": 1.0144346431435445,
      "grad_norm": 1.672060489654541,
      "learning_rate": 1.9912087912087913e-05,
      "loss": 0.6212,
      "step": 12650
    },
    {
      "epoch": 1.0152365677626303,
      "grad_norm": 1.3792171478271484,
      "learning_rate": 1.9904047172339854e-05,
      "loss": 0.6638,
      "step": 12660
    },
    {
      "epoch": 1.016038492381716,
      "grad_norm": 1.7795480489730835,
      "learning_rate": 1.98960064325918e-05,
      "loss": 0.7046,
      "step": 12670
    },
    {
      "epoch": 1.0168404170008019,
      "grad_norm": 1.8078111410140991,
      "learning_rate": 1.9887965692843743e-05,
      "loss": 0.6247,
      "step": 12680
    },
    {
      "epoch": 1.0176423416198876,
      "grad_norm": 1.6612544059753418,
      "learning_rate": 1.9879924953095684e-05,
      "loss": 0.6605,
      "step": 12690
    },
    {
      "epoch": 1.0184442662389734,
      "grad_norm": 1.7308176755905151,
      "learning_rate": 1.9871884213347628e-05,
      "loss": 0.602,
      "step": 12700
    },
    {
      "epoch": 1.0192461908580595,
      "grad_norm": 1.5146799087524414,
      "learning_rate": 1.9863843473599572e-05,
      "loss": 0.669,
      "step": 12710
    },
    {
      "epoch": 1.0200481154771452,
      "grad_norm": 1.782724380493164,
      "learning_rate": 1.9855802733851516e-05,
      "loss": 0.6056,
      "step": 12720
    },
    {
      "epoch": 1.020850040096231,
      "grad_norm": 1.745592713356018,
      "learning_rate": 1.9847761994103457e-05,
      "loss": 0.5609,
      "step": 12730
    },
    {
      "epoch": 1.0216519647153168,
      "grad_norm": 1.8312904834747314,
      "learning_rate": 1.98397212543554e-05,
      "loss": 0.6701,
      "step": 12740
    },
    {
      "epoch": 1.0224538893344026,
      "grad_norm": 1.3853397369384766,
      "learning_rate": 1.9831680514607343e-05,
      "loss": 0.6536,
      "step": 12750
    },
    {
      "epoch": 1.0232558139534884,
      "grad_norm": 1.8963291645050049,
      "learning_rate": 1.982363977485929e-05,
      "loss": 0.6594,
      "step": 12760
    },
    {
      "epoch": 1.0240577385725742,
      "grad_norm": 1.8179689645767212,
      "learning_rate": 1.981559903511123e-05,
      "loss": 0.7312,
      "step": 12770
    },
    {
      "epoch": 1.02485966319166,
      "grad_norm": 1.6165237426757812,
      "learning_rate": 1.9807558295363175e-05,
      "loss": 0.6462,
      "step": 12780
    },
    {
      "epoch": 1.0256615878107458,
      "grad_norm": 1.6547828912734985,
      "learning_rate": 1.9799517555615116e-05,
      "loss": 0.6243,
      "step": 12790
    },
    {
      "epoch": 1.0264635124298316,
      "grad_norm": 2.1497137546539307,
      "learning_rate": 1.9791476815867057e-05,
      "loss": 0.6774,
      "step": 12800
    },
    {
      "epoch": 1.0272654370489174,
      "grad_norm": 1.6589466333389282,
      "learning_rate": 1.9783436076119005e-05,
      "loss": 0.5744,
      "step": 12810
    },
    {
      "epoch": 1.0280673616680032,
      "grad_norm": 1.765059471130371,
      "learning_rate": 1.9775395336370946e-05,
      "loss": 0.6182,
      "step": 12820
    },
    {
      "epoch": 1.028869286287089,
      "grad_norm": 1.5458413362503052,
      "learning_rate": 1.976735459662289e-05,
      "loss": 0.5929,
      "step": 12830
    },
    {
      "epoch": 1.0296712109061747,
      "grad_norm": 1.6679606437683105,
      "learning_rate": 1.975931385687483e-05,
      "loss": 0.624,
      "step": 12840
    },
    {
      "epoch": 1.0304731355252605,
      "grad_norm": 1.8527203798294067,
      "learning_rate": 1.9751273117126775e-05,
      "loss": 0.6772,
      "step": 12850
    },
    {
      "epoch": 1.0312750601443463,
      "grad_norm": 1.4964779615402222,
      "learning_rate": 1.974323237737872e-05,
      "loss": 0.5988,
      "step": 12860
    },
    {
      "epoch": 1.0320769847634321,
      "grad_norm": 2.1049716472625732,
      "learning_rate": 1.9735191637630664e-05,
      "loss": 0.6844,
      "step": 12870
    },
    {
      "epoch": 1.0328789093825181,
      "grad_norm": 1.9150301218032837,
      "learning_rate": 1.9727150897882605e-05,
      "loss": 0.5651,
      "step": 12880
    },
    {
      "epoch": 1.033680834001604,
      "grad_norm": 1.5275410413742065,
      "learning_rate": 1.971911015813455e-05,
      "loss": 0.7268,
      "step": 12890
    },
    {
      "epoch": 1.0344827586206897,
      "grad_norm": 1.8838156461715698,
      "learning_rate": 1.9711069418386493e-05,
      "loss": 0.6471,
      "step": 12900
    },
    {
      "epoch": 1.0352846832397755,
      "grad_norm": 1.5535231828689575,
      "learning_rate": 1.9703028678638438e-05,
      "loss": 0.6569,
      "step": 12910
    },
    {
      "epoch": 1.0360866078588613,
      "grad_norm": 1.6325554847717285,
      "learning_rate": 1.969498793889038e-05,
      "loss": 0.6321,
      "step": 12920
    },
    {
      "epoch": 1.036888532477947,
      "grad_norm": 1.6400294303894043,
      "learning_rate": 1.968694719914232e-05,
      "loss": 0.6413,
      "step": 12930
    },
    {
      "epoch": 1.037690457097033,
      "grad_norm": 1.6500575542449951,
      "learning_rate": 1.9678906459394264e-05,
      "loss": 0.5964,
      "step": 12940
    },
    {
      "epoch": 1.0384923817161187,
      "grad_norm": 1.735721230506897,
      "learning_rate": 1.9670865719646208e-05,
      "loss": 0.6512,
      "step": 12950
    },
    {
      "epoch": 1.0392943063352045,
      "grad_norm": 1.9778283834457397,
      "learning_rate": 1.9662824979898152e-05,
      "loss": 0.6915,
      "step": 12960
    },
    {
      "epoch": 1.0400962309542903,
      "grad_norm": 1.531970500946045,
      "learning_rate": 1.9654784240150093e-05,
      "loss": 0.5497,
      "step": 12970
    },
    {
      "epoch": 1.040898155573376,
      "grad_norm": 1.7219816446304321,
      "learning_rate": 1.9646743500402038e-05,
      "loss": 0.587,
      "step": 12980
    },
    {
      "epoch": 1.0417000801924619,
      "grad_norm": 1.403779149055481,
      "learning_rate": 1.963870276065398e-05,
      "loss": 0.5932,
      "step": 12990
    },
    {
      "epoch": 1.0425020048115476,
      "grad_norm": 1.6958779096603394,
      "learning_rate": 1.9630662020905926e-05,
      "loss": 0.6491,
      "step": 13000
    },
    {
      "epoch": 1.0433039294306334,
      "grad_norm": 2.056743860244751,
      "learning_rate": 1.9622621281157867e-05,
      "loss": 0.6905,
      "step": 13010
    },
    {
      "epoch": 1.0441058540497192,
      "grad_norm": 1.912355661392212,
      "learning_rate": 1.961458054140981e-05,
      "loss": 0.6603,
      "step": 13020
    },
    {
      "epoch": 1.0449077786688052,
      "grad_norm": 1.8834363222122192,
      "learning_rate": 1.9606539801661752e-05,
      "loss": 0.6696,
      "step": 13030
    },
    {
      "epoch": 1.045709703287891,
      "grad_norm": 1.6293531656265259,
      "learning_rate": 1.9598499061913696e-05,
      "loss": 0.6114,
      "step": 13040
    },
    {
      "epoch": 1.0465116279069768,
      "grad_norm": 1.7482372522354126,
      "learning_rate": 1.959045832216564e-05,
      "loss": 0.6441,
      "step": 13050
    },
    {
      "epoch": 1.0473135525260626,
      "grad_norm": 1.7062009572982788,
      "learning_rate": 1.958241758241758e-05,
      "loss": 0.6182,
      "step": 13060
    },
    {
      "epoch": 1.0481154771451484,
      "grad_norm": 1.8458802700042725,
      "learning_rate": 1.9574376842669526e-05,
      "loss": 0.5921,
      "step": 13070
    },
    {
      "epoch": 1.0489174017642342,
      "grad_norm": 1.548221230506897,
      "learning_rate": 1.9566336102921467e-05,
      "loss": 0.6107,
      "step": 13080
    },
    {
      "epoch": 1.04971932638332,
      "grad_norm": 1.5446670055389404,
      "learning_rate": 1.9558295363173415e-05,
      "loss": 0.6931,
      "step": 13090
    },
    {
      "epoch": 1.0505212510024058,
      "grad_norm": 1.4376977682113647,
      "learning_rate": 1.9550254623425355e-05,
      "loss": 0.6667,
      "step": 13100
    },
    {
      "epoch": 1.0513231756214916,
      "grad_norm": 1.536828637123108,
      "learning_rate": 1.95422138836773e-05,
      "loss": 0.6032,
      "step": 13110
    },
    {
      "epoch": 1.0521251002405774,
      "grad_norm": 1.8769704103469849,
      "learning_rate": 1.953417314392924e-05,
      "loss": 0.6615,
      "step": 13120
    },
    {
      "epoch": 1.0529270248596632,
      "grad_norm": 1.8993234634399414,
      "learning_rate": 1.9526132404181185e-05,
      "loss": 0.6267,
      "step": 13130
    },
    {
      "epoch": 1.053728949478749,
      "grad_norm": 1.8884636163711548,
      "learning_rate": 1.951809166443313e-05,
      "loss": 0.6043,
      "step": 13140
    },
    {
      "epoch": 1.0545308740978347,
      "grad_norm": 1.5824244022369385,
      "learning_rate": 1.9510050924685074e-05,
      "loss": 0.6612,
      "step": 13150
    },
    {
      "epoch": 1.0553327987169205,
      "grad_norm": 1.6130484342575073,
      "learning_rate": 1.9502010184937014e-05,
      "loss": 0.6896,
      "step": 13160
    },
    {
      "epoch": 1.0561347233360063,
      "grad_norm": 1.7351078987121582,
      "learning_rate": 1.949396944518896e-05,
      "loss": 0.651,
      "step": 13170
    },
    {
      "epoch": 1.0569366479550921,
      "grad_norm": 2.0277271270751953,
      "learning_rate": 1.94859287054409e-05,
      "loss": 0.627,
      "step": 13180
    },
    {
      "epoch": 1.057738572574178,
      "grad_norm": 1.7561763525009155,
      "learning_rate": 1.9477887965692844e-05,
      "loss": 0.6218,
      "step": 13190
    },
    {
      "epoch": 1.058540497193264,
      "grad_norm": 1.641928791999817,
      "learning_rate": 1.9469847225944788e-05,
      "loss": 0.6243,
      "step": 13200
    },
    {
      "epoch": 1.0593424218123497,
      "grad_norm": 2.160677433013916,
      "learning_rate": 1.946180648619673e-05,
      "loss": 0.6293,
      "step": 13210
    },
    {
      "epoch": 1.0601443464314355,
      "grad_norm": 1.7560007572174072,
      "learning_rate": 1.9453765746448673e-05,
      "loss": 0.6616,
      "step": 13220
    },
    {
      "epoch": 1.0609462710505213,
      "grad_norm": 1.4219378232955933,
      "learning_rate": 1.9445725006700614e-05,
      "loss": 0.6021,
      "step": 13230
    },
    {
      "epoch": 1.061748195669607,
      "grad_norm": 1.715781331062317,
      "learning_rate": 1.9437684266952562e-05,
      "loss": 0.6133,
      "step": 13240
    },
    {
      "epoch": 1.062550120288693,
      "grad_norm": 1.8240203857421875,
      "learning_rate": 1.9429643527204503e-05,
      "loss": 0.5822,
      "step": 13250
    },
    {
      "epoch": 1.0633520449077787,
      "grad_norm": 1.6879812479019165,
      "learning_rate": 1.9421602787456447e-05,
      "loss": 0.5975,
      "step": 13260
    },
    {
      "epoch": 1.0641539695268645,
      "grad_norm": 1.4787172079086304,
      "learning_rate": 1.9413562047708388e-05,
      "loss": 0.5858,
      "step": 13270
    },
    {
      "epoch": 1.0649558941459503,
      "grad_norm": 1.6693559885025024,
      "learning_rate": 1.9405521307960336e-05,
      "loss": 0.6022,
      "step": 13280
    },
    {
      "epoch": 1.065757818765036,
      "grad_norm": 1.7237848043441772,
      "learning_rate": 1.9397480568212277e-05,
      "loss": 0.6395,
      "step": 13290
    },
    {
      "epoch": 1.0665597433841218,
      "grad_norm": 1.8247284889221191,
      "learning_rate": 1.938943982846422e-05,
      "loss": 0.6175,
      "step": 13300
    },
    {
      "epoch": 1.0673616680032076,
      "grad_norm": 1.7257905006408691,
      "learning_rate": 1.9381399088716162e-05,
      "loss": 0.6003,
      "step": 13310
    },
    {
      "epoch": 1.0681635926222934,
      "grad_norm": 1.7185124158859253,
      "learning_rate": 1.9373358348968103e-05,
      "loss": 0.6415,
      "step": 13320
    },
    {
      "epoch": 1.0689655172413792,
      "grad_norm": 1.6805963516235352,
      "learning_rate": 1.936531760922005e-05,
      "loss": 0.6139,
      "step": 13330
    },
    {
      "epoch": 1.069767441860465,
      "grad_norm": 1.2918407917022705,
      "learning_rate": 1.935727686947199e-05,
      "loss": 0.5933,
      "step": 13340
    },
    {
      "epoch": 1.070569366479551,
      "grad_norm": 1.7674835920333862,
      "learning_rate": 1.9349236129723936e-05,
      "loss": 0.7105,
      "step": 13350
    },
    {
      "epoch": 1.0713712910986368,
      "grad_norm": 1.6596019268035889,
      "learning_rate": 1.9341195389975877e-05,
      "loss": 0.6728,
      "step": 13360
    },
    {
      "epoch": 1.0721732157177226,
      "grad_norm": 1.6307014226913452,
      "learning_rate": 1.9333958724202626e-05,
      "loss": 0.6794,
      "step": 13370
    },
    {
      "epoch": 1.0729751403368084,
      "grad_norm": 1.5609468221664429,
      "learning_rate": 1.932591798445457e-05,
      "loss": 0.5673,
      "step": 13380
    },
    {
      "epoch": 1.0737770649558942,
      "grad_norm": 1.7369458675384521,
      "learning_rate": 1.9317877244706515e-05,
      "loss": 0.5939,
      "step": 13390
    },
    {
      "epoch": 1.07457898957498,
      "grad_norm": 1.603314757347107,
      "learning_rate": 1.9309836504958456e-05,
      "loss": 0.58,
      "step": 13400
    },
    {
      "epoch": 1.0753809141940658,
      "grad_norm": 1.4080575704574585,
      "learning_rate": 1.93017957652104e-05,
      "loss": 0.6827,
      "step": 13410
    },
    {
      "epoch": 1.0761828388131516,
      "grad_norm": 1.634245753288269,
      "learning_rate": 1.929375502546234e-05,
      "loss": 0.6035,
      "step": 13420
    },
    {
      "epoch": 1.0769847634322374,
      "grad_norm": 1.811569333076477,
      "learning_rate": 1.928571428571429e-05,
      "loss": 0.7084,
      "step": 13430
    },
    {
      "epoch": 1.0777866880513232,
      "grad_norm": 1.565852165222168,
      "learning_rate": 1.927767354596623e-05,
      "loss": 0.6044,
      "step": 13440
    },
    {
      "epoch": 1.078588612670409,
      "grad_norm": 1.5223501920700073,
      "learning_rate": 1.9269632806218174e-05,
      "loss": 0.6005,
      "step": 13450
    },
    {
      "epoch": 1.0793905372894947,
      "grad_norm": 1.459338903427124,
      "learning_rate": 1.9261592066470114e-05,
      "loss": 0.6073,
      "step": 13460
    },
    {
      "epoch": 1.0801924619085805,
      "grad_norm": 2.083961248397827,
      "learning_rate": 1.925355132672206e-05,
      "loss": 0.6697,
      "step": 13470
    },
    {
      "epoch": 1.0809943865276663,
      "grad_norm": 1.5530366897583008,
      "learning_rate": 1.9245510586974003e-05,
      "loss": 0.5943,
      "step": 13480
    },
    {
      "epoch": 1.0817963111467521,
      "grad_norm": 1.7909424304962158,
      "learning_rate": 1.9237469847225947e-05,
      "loss": 0.6606,
      "step": 13490
    },
    {
      "epoch": 1.082598235765838,
      "grad_norm": 1.6374810934066772,
      "learning_rate": 1.9229429107477888e-05,
      "loss": 0.6219,
      "step": 13500
    },
    {
      "epoch": 1.0834001603849237,
      "grad_norm": 1.6582791805267334,
      "learning_rate": 1.922138836772983e-05,
      "loss": 0.69,
      "step": 13510
    },
    {
      "epoch": 1.0842020850040097,
      "grad_norm": 1.588626503944397,
      "learning_rate": 1.9213347627981773e-05,
      "loss": 0.6657,
      "step": 13520
    },
    {
      "epoch": 1.0850040096230955,
      "grad_norm": 2.5487143993377686,
      "learning_rate": 1.9205306888233718e-05,
      "loss": 0.7045,
      "step": 13530
    },
    {
      "epoch": 1.0858059342421813,
      "grad_norm": 1.631978988647461,
      "learning_rate": 1.9197266148485662e-05,
      "loss": 0.6015,
      "step": 13540
    },
    {
      "epoch": 1.086607858861267,
      "grad_norm": 1.5800414085388184,
      "learning_rate": 1.9189225408737603e-05,
      "loss": 0.6849,
      "step": 13550
    },
    {
      "epoch": 1.0874097834803529,
      "grad_norm": 1.5061819553375244,
      "learning_rate": 1.9181184668989547e-05,
      "loss": 0.6629,
      "step": 13560
    },
    {
      "epoch": 1.0882117080994387,
      "grad_norm": 1.6565557718276978,
      "learning_rate": 1.917314392924149e-05,
      "loss": 0.6369,
      "step": 13570
    },
    {
      "epoch": 1.0890136327185245,
      "grad_norm": 1.7333033084869385,
      "learning_rate": 1.9165103189493436e-05,
      "loss": 0.6029,
      "step": 13580
    },
    {
      "epoch": 1.0898155573376103,
      "grad_norm": 1.8294404745101929,
      "learning_rate": 1.9157062449745377e-05,
      "loss": 0.6596,
      "step": 13590
    },
    {
      "epoch": 1.090617481956696,
      "grad_norm": 1.7621124982833862,
      "learning_rate": 1.914902170999732e-05,
      "loss": 0.6225,
      "step": 13600
    },
    {
      "epoch": 1.0914194065757818,
      "grad_norm": 1.8011071681976318,
      "learning_rate": 1.9140980970249262e-05,
      "loss": 0.5839,
      "step": 13610
    },
    {
      "epoch": 1.0922213311948676,
      "grad_norm": 1.6180989742279053,
      "learning_rate": 1.913294023050121e-05,
      "loss": 0.5574,
      "step": 13620
    },
    {
      "epoch": 1.0930232558139534,
      "grad_norm": 1.5912818908691406,
      "learning_rate": 1.912489949075315e-05,
      "loss": 0.6073,
      "step": 13630
    },
    {
      "epoch": 1.0938251804330392,
      "grad_norm": 1.6119654178619385,
      "learning_rate": 1.911685875100509e-05,
      "loss": 0.6578,
      "step": 13640
    },
    {
      "epoch": 1.094627105052125,
      "grad_norm": 1.6607617139816284,
      "learning_rate": 1.9108818011257036e-05,
      "loss": 0.5744,
      "step": 13650
    },
    {
      "epoch": 1.0954290296712108,
      "grad_norm": 1.6204255819320679,
      "learning_rate": 1.9100777271508977e-05,
      "loss": 0.6494,
      "step": 13660
    },
    {
      "epoch": 1.0962309542902968,
      "grad_norm": 1.6086094379425049,
      "learning_rate": 1.9092736531760924e-05,
      "loss": 0.595,
      "step": 13670
    },
    {
      "epoch": 1.0970328789093826,
      "grad_norm": 1.676257848739624,
      "learning_rate": 1.9084695792012865e-05,
      "loss": 0.6286,
      "step": 13680
    },
    {
      "epoch": 1.0978348035284684,
      "grad_norm": 1.7995692491531372,
      "learning_rate": 1.907665505226481e-05,
      "loss": 0.6034,
      "step": 13690
    },
    {
      "epoch": 1.0986367281475542,
      "grad_norm": 2.095123052597046,
      "learning_rate": 1.906861431251675e-05,
      "loss": 0.6325,
      "step": 13700
    },
    {
      "epoch": 1.09943865276664,
      "grad_norm": 1.6759960651397705,
      "learning_rate": 1.9060573572768695e-05,
      "loss": 0.7029,
      "step": 13710
    },
    {
      "epoch": 1.1002405773857258,
      "grad_norm": 1.5699782371520996,
      "learning_rate": 1.905253283302064e-05,
      "loss": 0.6016,
      "step": 13720
    },
    {
      "epoch": 1.1010425020048116,
      "grad_norm": 1.6696754693984985,
      "learning_rate": 1.9044492093272583e-05,
      "loss": 0.6085,
      "step": 13730
    },
    {
      "epoch": 1.1018444266238974,
      "grad_norm": 1.7654789686203003,
      "learning_rate": 1.9036451353524524e-05,
      "loss": 0.6102,
      "step": 13740
    },
    {
      "epoch": 1.1026463512429832,
      "grad_norm": 1.9852560758590698,
      "learning_rate": 1.902841061377647e-05,
      "loss": 0.6222,
      "step": 13750
    },
    {
      "epoch": 1.103448275862069,
      "grad_norm": 1.5720012187957764,
      "learning_rate": 1.9020369874028413e-05,
      "loss": 0.6346,
      "step": 13760
    },
    {
      "epoch": 1.1042502004811547,
      "grad_norm": 1.384774923324585,
      "learning_rate": 1.9012329134280354e-05,
      "loss": 0.6303,
      "step": 13770
    },
    {
      "epoch": 1.1050521251002405,
      "grad_norm": 1.9962806701660156,
      "learning_rate": 1.9004288394532298e-05,
      "loss": 0.6551,
      "step": 13780
    },
    {
      "epoch": 1.1058540497193263,
      "grad_norm": 1.8960480690002441,
      "learning_rate": 1.899624765478424e-05,
      "loss": 0.6328,
      "step": 13790
    },
    {
      "epoch": 1.1066559743384121,
      "grad_norm": 2.159111738204956,
      "learning_rate": 1.8988206915036183e-05,
      "loss": 0.5992,
      "step": 13800
    },
    {
      "epoch": 1.107457898957498,
      "grad_norm": 1.5183731317520142,
      "learning_rate": 1.8980166175288127e-05,
      "loss": 0.6247,
      "step": 13810
    },
    {
      "epoch": 1.1082598235765837,
      "grad_norm": 1.706178903579712,
      "learning_rate": 1.897212543554007e-05,
      "loss": 0.6459,
      "step": 13820
    },
    {
      "epoch": 1.1090617481956695,
      "grad_norm": 1.6347585916519165,
      "learning_rate": 1.8964084695792013e-05,
      "loss": 0.6163,
      "step": 13830
    },
    {
      "epoch": 1.1098636728147555,
      "grad_norm": 1.533334493637085,
      "learning_rate": 1.8956043956043957e-05,
      "loss": 0.6213,
      "step": 13840
    },
    {
      "epoch": 1.1106655974338413,
      "grad_norm": 1.5612761974334717,
      "learning_rate": 1.8948003216295898e-05,
      "loss": 0.614,
      "step": 13850
    },
    {
      "epoch": 1.111467522052927,
      "grad_norm": 1.4864223003387451,
      "learning_rate": 1.8939962476547845e-05,
      "loss": 0.6754,
      "step": 13860
    },
    {
      "epoch": 1.1122694466720129,
      "grad_norm": 1.862100601196289,
      "learning_rate": 1.8931921736799786e-05,
      "loss": 0.5594,
      "step": 13870
    },
    {
      "epoch": 1.1130713712910987,
      "grad_norm": 2.0856144428253174,
      "learning_rate": 1.892388099705173e-05,
      "loss": 0.6756,
      "step": 13880
    },
    {
      "epoch": 1.1138732959101845,
      "grad_norm": 1.6658891439437866,
      "learning_rate": 1.891584025730367e-05,
      "loss": 0.6186,
      "step": 13890
    },
    {
      "epoch": 1.1146752205292703,
      "grad_norm": 1.598567247390747,
      "learning_rate": 1.8907799517555612e-05,
      "loss": 0.644,
      "step": 13900
    },
    {
      "epoch": 1.115477145148356,
      "grad_norm": 1.8254904747009277,
      "learning_rate": 1.889975877780756e-05,
      "loss": 0.6665,
      "step": 13910
    },
    {
      "epoch": 1.1162790697674418,
      "grad_norm": 1.8081119060516357,
      "learning_rate": 1.88917180380595e-05,
      "loss": 0.6203,
      "step": 13920
    },
    {
      "epoch": 1.1170809943865276,
      "grad_norm": 1.632903814315796,
      "learning_rate": 1.8883677298311445e-05,
      "loss": 0.6363,
      "step": 13930
    },
    {
      "epoch": 1.1178829190056134,
      "grad_norm": 1.706418514251709,
      "learning_rate": 1.8875636558563386e-05,
      "loss": 0.6393,
      "step": 13940
    },
    {
      "epoch": 1.1186848436246992,
      "grad_norm": 1.772438406944275,
      "learning_rate": 1.8867595818815334e-05,
      "loss": 0.6531,
      "step": 13950
    },
    {
      "epoch": 1.119486768243785,
      "grad_norm": 1.6625159978866577,
      "learning_rate": 1.8859555079067275e-05,
      "loss": 0.6817,
      "step": 13960
    },
    {
      "epoch": 1.1202886928628708,
      "grad_norm": 1.6731529235839844,
      "learning_rate": 1.885151433931922e-05,
      "loss": 0.6718,
      "step": 13970
    },
    {
      "epoch": 1.1210906174819566,
      "grad_norm": 1.6343638896942139,
      "learning_rate": 1.884347359957116e-05,
      "loss": 0.605,
      "step": 13980
    },
    {
      "epoch": 1.1218925421010426,
      "grad_norm": 1.8132818937301636,
      "learning_rate": 1.8835432859823104e-05,
      "loss": 0.6842,
      "step": 13990
    },
    {
      "epoch": 1.1226944667201284,
      "grad_norm": 1.7453277111053467,
      "learning_rate": 1.882739212007505e-05,
      "loss": 0.6767,
      "step": 14000
    },
    {
      "epoch": 1.1234963913392142,
      "grad_norm": 1.465591311454773,
      "learning_rate": 1.8819351380326993e-05,
      "loss": 0.5779,
      "step": 14010
    },
    {
      "epoch": 1.1242983159583,
      "grad_norm": 1.6383037567138672,
      "learning_rate": 1.8811310640578934e-05,
      "loss": 0.5706,
      "step": 14020
    },
    {
      "epoch": 1.1251002405773858,
      "grad_norm": 1.70350182056427,
      "learning_rate": 1.8803269900830875e-05,
      "loss": 0.6288,
      "step": 14030
    },
    {
      "epoch": 1.1259021651964716,
      "grad_norm": 1.7142540216445923,
      "learning_rate": 1.879522916108282e-05,
      "loss": 0.599,
      "step": 14040
    },
    {
      "epoch": 1.1267040898155574,
      "grad_norm": 1.6148688793182373,
      "learning_rate": 1.8787188421334763e-05,
      "loss": 0.6171,
      "step": 14050
    },
    {
      "epoch": 1.1275060144346432,
      "grad_norm": 1.3541122674942017,
      "learning_rate": 1.8779147681586708e-05,
      "loss": 0.6255,
      "step": 14060
    },
    {
      "epoch": 1.128307939053729,
      "grad_norm": 1.733359456062317,
      "learning_rate": 1.877110694183865e-05,
      "loss": 0.6477,
      "step": 14070
    },
    {
      "epoch": 1.1291098636728147,
      "grad_norm": 1.6377763748168945,
      "learning_rate": 1.8763066202090593e-05,
      "loss": 0.645,
      "step": 14080
    },
    {
      "epoch": 1.1299117882919005,
      "grad_norm": 1.8297325372695923,
      "learning_rate": 1.8755025462342534e-05,
      "loss": 0.6046,
      "step": 14090
    },
    {
      "epoch": 1.1307137129109863,
      "grad_norm": 1.8238531351089478,
      "learning_rate": 1.874698472259448e-05,
      "loss": 0.7347,
      "step": 14100
    },
    {
      "epoch": 1.1315156375300721,
      "grad_norm": 1.9204987287521362,
      "learning_rate": 1.8738943982846422e-05,
      "loss": 0.6083,
      "step": 14110
    },
    {
      "epoch": 1.132317562149158,
      "grad_norm": 2.39507794380188,
      "learning_rate": 1.8730903243098366e-05,
      "loss": 0.6463,
      "step": 14120
    },
    {
      "epoch": 1.1331194867682437,
      "grad_norm": 1.6719387769699097,
      "learning_rate": 1.8722862503350307e-05,
      "loss": 0.5855,
      "step": 14130
    },
    {
      "epoch": 1.1339214113873295,
      "grad_norm": 1.7065752744674683,
      "learning_rate": 1.8714821763602255e-05,
      "loss": 0.6525,
      "step": 14140
    },
    {
      "epoch": 1.1347233360064153,
      "grad_norm": 1.8826532363891602,
      "learning_rate": 1.8706781023854196e-05,
      "loss": 0.5851,
      "step": 14150
    },
    {
      "epoch": 1.1355252606255013,
      "grad_norm": 1.4863488674163818,
      "learning_rate": 1.8698740284106137e-05,
      "loss": 0.6944,
      "step": 14160
    },
    {
      "epoch": 1.136327185244587,
      "grad_norm": 2.462195873260498,
      "learning_rate": 1.869069954435808e-05,
      "loss": 0.6485,
      "step": 14170
    },
    {
      "epoch": 1.1371291098636729,
      "grad_norm": 1.5923445224761963,
      "learning_rate": 1.8682658804610022e-05,
      "loss": 0.6499,
      "step": 14180
    },
    {
      "epoch": 1.1379310344827587,
      "grad_norm": 1.22882080078125,
      "learning_rate": 1.867461806486197e-05,
      "loss": 0.6742,
      "step": 14190
    },
    {
      "epoch": 1.1387329591018445,
      "grad_norm": 1.8983584642410278,
      "learning_rate": 1.866657732511391e-05,
      "loss": 0.5769,
      "step": 14200
    },
    {
      "epoch": 1.1395348837209303,
      "grad_norm": 1.6067945957183838,
      "learning_rate": 1.8658536585365855e-05,
      "loss": 0.5595,
      "step": 14210
    },
    {
      "epoch": 1.140336808340016,
      "grad_norm": 1.7337571382522583,
      "learning_rate": 1.8650495845617796e-05,
      "loss": 0.6159,
      "step": 14220
    },
    {
      "epoch": 1.1411387329591018,
      "grad_norm": 2.0924861431121826,
      "learning_rate": 1.864245510586974e-05,
      "loss": 0.6099,
      "step": 14230
    },
    {
      "epoch": 1.1419406575781876,
      "grad_norm": 1.8928865194320679,
      "learning_rate": 1.8634414366121684e-05,
      "loss": 0.6661,
      "step": 14240
    },
    {
      "epoch": 1.1427425821972734,
      "grad_norm": 1.673601746559143,
      "learning_rate": 1.862637362637363e-05,
      "loss": 0.6353,
      "step": 14250
    },
    {
      "epoch": 1.1435445068163592,
      "grad_norm": 1.7883714437484741,
      "learning_rate": 1.861833288662557e-05,
      "loss": 0.6162,
      "step": 14260
    },
    {
      "epoch": 1.144346431435445,
      "grad_norm": 1.8706166744232178,
      "learning_rate": 1.8610292146877514e-05,
      "loss": 0.6212,
      "step": 14270
    },
    {
      "epoch": 1.1451483560545308,
      "grad_norm": 1.915215015411377,
      "learning_rate": 1.8602251407129458e-05,
      "loss": 0.6122,
      "step": 14280
    },
    {
      "epoch": 1.1459502806736166,
      "grad_norm": 1.7117596864700317,
      "learning_rate": 1.85942106673814e-05,
      "loss": 0.6522,
      "step": 14290
    },
    {
      "epoch": 1.1467522052927026,
      "grad_norm": 1.7606104612350464,
      "learning_rate": 1.8586169927633343e-05,
      "loss": 0.6441,
      "step": 14300
    },
    {
      "epoch": 1.1475541299117884,
      "grad_norm": 1.9257251024246216,
      "learning_rate": 1.8578129187885284e-05,
      "loss": 0.6522,
      "step": 14310
    },
    {
      "epoch": 1.1483560545308742,
      "grad_norm": 1.6053045988082886,
      "learning_rate": 1.857008844813723e-05,
      "loss": 0.7036,
      "step": 14320
    },
    {
      "epoch": 1.14915797914996,
      "grad_norm": 1.6661725044250488,
      "learning_rate": 1.8562047708389173e-05,
      "loss": 0.6252,
      "step": 14330
    },
    {
      "epoch": 1.1499599037690458,
      "grad_norm": 1.7273025512695312,
      "learning_rate": 1.8554006968641117e-05,
      "loss": 0.5831,
      "step": 14340
    },
    {
      "epoch": 1.1507618283881316,
      "grad_norm": 2.3603808879852295,
      "learning_rate": 1.8545966228893058e-05,
      "loss": 0.6327,
      "step": 14350
    },
    {
      "epoch": 1.1515637530072174,
      "grad_norm": 1.7036563158035278,
      "learning_rate": 1.8537925489145002e-05,
      "loss": 0.6366,
      "step": 14360
    },
    {
      "epoch": 1.1523656776263032,
      "grad_norm": 1.5598756074905396,
      "learning_rate": 1.8529884749396943e-05,
      "loss": 0.6543,
      "step": 14370
    },
    {
      "epoch": 1.153167602245389,
      "grad_norm": 1.9147765636444092,
      "learning_rate": 1.852184400964889e-05,
      "loss": 0.6186,
      "step": 14380
    },
    {
      "epoch": 1.1539695268644747,
      "grad_norm": 1.6179172992706299,
      "learning_rate": 1.8513803269900832e-05,
      "loss": 0.6347,
      "step": 14390
    },
    {
      "epoch": 1.1547714514835605,
      "grad_norm": 1.8341444730758667,
      "learning_rate": 1.8505762530152776e-05,
      "loss": 0.6668,
      "step": 14400
    },
    {
      "epoch": 1.1555733761026463,
      "grad_norm": 1.6785222291946411,
      "learning_rate": 1.8497721790404717e-05,
      "loss": 0.6393,
      "step": 14410
    },
    {
      "epoch": 1.1563753007217321,
      "grad_norm": 1.6678589582443237,
      "learning_rate": 1.8489681050656658e-05,
      "loss": 0.6836,
      "step": 14420
    },
    {
      "epoch": 1.157177225340818,
      "grad_norm": 1.8386845588684082,
      "learning_rate": 1.8481640310908606e-05,
      "loss": 0.6095,
      "step": 14430
    },
    {
      "epoch": 1.1579791499599037,
      "grad_norm": 1.9544204473495483,
      "learning_rate": 1.8473599571160546e-05,
      "loss": 0.6546,
      "step": 14440
    },
    {
      "epoch": 1.1587810745789895,
      "grad_norm": 1.999610424041748,
      "learning_rate": 1.846555883141249e-05,
      "loss": 0.5838,
      "step": 14450
    },
    {
      "epoch": 1.1595829991980753,
      "grad_norm": 1.7083529233932495,
      "learning_rate": 1.845751809166443e-05,
      "loss": 0.6698,
      "step": 14460
    },
    {
      "epoch": 1.160384923817161,
      "grad_norm": 1.793802261352539,
      "learning_rate": 1.844947735191638e-05,
      "loss": 0.5349,
      "step": 14470
    },
    {
      "epoch": 1.161186848436247,
      "grad_norm": 1.445790410041809,
      "learning_rate": 1.844143661216832e-05,
      "loss": 0.581,
      "step": 14480
    },
    {
      "epoch": 1.1619887730553329,
      "grad_norm": 1.8609232902526855,
      "learning_rate": 1.8433395872420265e-05,
      "loss": 0.6428,
      "step": 14490
    },
    {
      "epoch": 1.1627906976744187,
      "grad_norm": 1.7217435836791992,
      "learning_rate": 1.8425355132672205e-05,
      "loss": 0.7091,
      "step": 14500
    },
    {
      "epoch": 1.1635926222935045,
      "grad_norm": 1.9428153038024902,
      "learning_rate": 1.841731439292415e-05,
      "loss": 0.6352,
      "step": 14510
    },
    {
      "epoch": 1.1643945469125903,
      "grad_norm": 2.064832925796509,
      "learning_rate": 1.8409273653176094e-05,
      "loss": 0.5821,
      "step": 14520
    },
    {
      "epoch": 1.165196471531676,
      "grad_norm": 1.6100835800170898,
      "learning_rate": 1.8401232913428035e-05,
      "loss": 0.6234,
      "step": 14530
    },
    {
      "epoch": 1.1659983961507618,
      "grad_norm": 1.74643874168396,
      "learning_rate": 1.839319217367998e-05,
      "loss": 0.717,
      "step": 14540
    },
    {
      "epoch": 1.1668003207698476,
      "grad_norm": 1.8171404600143433,
      "learning_rate": 1.838515143393192e-05,
      "loss": 0.6501,
      "step": 14550
    },
    {
      "epoch": 1.1676022453889334,
      "grad_norm": 1.5973345041275024,
      "learning_rate": 1.8377110694183864e-05,
      "loss": 0.6486,
      "step": 14560
    },
    {
      "epoch": 1.1684041700080192,
      "grad_norm": 1.7824984788894653,
      "learning_rate": 1.836906995443581e-05,
      "loss": 0.6165,
      "step": 14570
    },
    {
      "epoch": 1.169206094627105,
      "grad_norm": 1.8373264074325562,
      "learning_rate": 1.8361029214687753e-05,
      "loss": 0.6773,
      "step": 14580
    },
    {
      "epoch": 1.1700080192461908,
      "grad_norm": 1.779106855392456,
      "learning_rate": 1.8352988474939694e-05,
      "loss": 0.5901,
      "step": 14590
    },
    {
      "epoch": 1.1708099438652766,
      "grad_norm": 1.7328150272369385,
      "learning_rate": 1.8344947735191638e-05,
      "loss": 0.6128,
      "step": 14600
    },
    {
      "epoch": 1.1716118684843624,
      "grad_norm": 1.7751672267913818,
      "learning_rate": 1.833690699544358e-05,
      "loss": 0.6116,
      "step": 14610
    },
    {
      "epoch": 1.1724137931034484,
      "grad_norm": 1.5570346117019653,
      "learning_rate": 1.8328866255695527e-05,
      "loss": 0.6678,
      "step": 14620
    },
    {
      "epoch": 1.1732157177225342,
      "grad_norm": 1.7399818897247314,
      "learning_rate": 1.8320825515947468e-05,
      "loss": 0.6077,
      "step": 14630
    },
    {
      "epoch": 1.17401764234162,
      "grad_norm": 2.2047948837280273,
      "learning_rate": 1.8312784776199412e-05,
      "loss": 0.7463,
      "step": 14640
    },
    {
      "epoch": 1.1748195669607058,
      "grad_norm": 1.6854546070098877,
      "learning_rate": 1.8304744036451353e-05,
      "loss": 0.6576,
      "step": 14650
    },
    {
      "epoch": 1.1756214915797916,
      "grad_norm": 1.559329628944397,
      "learning_rate": 1.8296703296703297e-05,
      "loss": 0.7085,
      "step": 14660
    },
    {
      "epoch": 1.1764234161988774,
      "grad_norm": 1.8023232221603394,
      "learning_rate": 1.828866255695524e-05,
      "loss": 0.6208,
      "step": 14670
    },
    {
      "epoch": 1.1772253408179632,
      "grad_norm": 1.4615999460220337,
      "learning_rate": 1.8280621817207182e-05,
      "loss": 0.5697,
      "step": 14680
    },
    {
      "epoch": 1.178027265437049,
      "grad_norm": 1.7814093828201294,
      "learning_rate": 1.8272581077459127e-05,
      "loss": 0.6834,
      "step": 14690
    },
    {
      "epoch": 1.1788291900561347,
      "grad_norm": 1.3979158401489258,
      "learning_rate": 1.8264540337711068e-05,
      "loss": 0.5859,
      "step": 14700
    },
    {
      "epoch": 1.1796311146752205,
      "grad_norm": 1.8694720268249512,
      "learning_rate": 1.8256499597963015e-05,
      "loss": 0.725,
      "step": 14710
    },
    {
      "epoch": 1.1804330392943063,
      "grad_norm": 1.720062255859375,
      "learning_rate": 1.8248458858214956e-05,
      "loss": 0.6234,
      "step": 14720
    },
    {
      "epoch": 1.181234963913392,
      "grad_norm": 1.8090221881866455,
      "learning_rate": 1.82404181184669e-05,
      "loss": 0.6587,
      "step": 14730
    },
    {
      "epoch": 1.182036888532478,
      "grad_norm": 1.5842440128326416,
      "learning_rate": 1.823237737871884e-05,
      "loss": 0.6379,
      "step": 14740
    },
    {
      "epoch": 1.1828388131515637,
      "grad_norm": 2.2015461921691895,
      "learning_rate": 1.8224336638970786e-05,
      "loss": 0.7035,
      "step": 14750
    },
    {
      "epoch": 1.1836407377706495,
      "grad_norm": 1.7753804922103882,
      "learning_rate": 1.821629589922273e-05,
      "loss": 0.606,
      "step": 14760
    },
    {
      "epoch": 1.1844426623897353,
      "grad_norm": 1.640458106994629,
      "learning_rate": 1.8208255159474674e-05,
      "loss": 0.6377,
      "step": 14770
    },
    {
      "epoch": 1.185244587008821,
      "grad_norm": 1.5446008443832397,
      "learning_rate": 1.8200214419726615e-05,
      "loss": 0.6165,
      "step": 14780
    },
    {
      "epoch": 1.1860465116279069,
      "grad_norm": 1.9036743640899658,
      "learning_rate": 1.8192173679978556e-05,
      "loss": 0.681,
      "step": 14790
    },
    {
      "epoch": 1.1868484362469929,
      "grad_norm": 1.555706262588501,
      "learning_rate": 1.81841329402305e-05,
      "loss": 0.6432,
      "step": 14800
    },
    {
      "epoch": 1.1876503608660787,
      "grad_norm": 1.6751420497894287,
      "learning_rate": 1.8176092200482445e-05,
      "loss": 0.59,
      "step": 14810
    },
    {
      "epoch": 1.1884522854851645,
      "grad_norm": 2.021397590637207,
      "learning_rate": 1.816805146073439e-05,
      "loss": 0.5721,
      "step": 14820
    },
    {
      "epoch": 1.1892542101042503,
      "grad_norm": 1.8122775554656982,
      "learning_rate": 1.816001072098633e-05,
      "loss": 0.6503,
      "step": 14830
    },
    {
      "epoch": 1.190056134723336,
      "grad_norm": 1.8132072687149048,
      "learning_rate": 1.8151969981238274e-05,
      "loss": 0.6406,
      "step": 14840
    },
    {
      "epoch": 1.1908580593424218,
      "grad_norm": 1.9154281616210938,
      "learning_rate": 1.814392924149022e-05,
      "loss": 0.6392,
      "step": 14850
    },
    {
      "epoch": 1.1916599839615076,
      "grad_norm": 2.1299543380737305,
      "learning_rate": 1.8135888501742163e-05,
      "loss": 0.6413,
      "step": 14860
    },
    {
      "epoch": 1.1924619085805934,
      "grad_norm": 2.2212605476379395,
      "learning_rate": 1.8127847761994104e-05,
      "loss": 0.5694,
      "step": 14870
    },
    {
      "epoch": 1.1932638331996792,
      "grad_norm": 1.747727632522583,
      "learning_rate": 1.8119807022246048e-05,
      "loss": 0.6438,
      "step": 14880
    },
    {
      "epoch": 1.194065757818765,
      "grad_norm": 1.791260004043579,
      "learning_rate": 1.811176628249799e-05,
      "loss": 0.613,
      "step": 14890
    },
    {
      "epoch": 1.1948676824378508,
      "grad_norm": 1.8771047592163086,
      "learning_rate": 1.8103725542749936e-05,
      "loss": 0.5628,
      "step": 14900
    },
    {
      "epoch": 1.1956696070569366,
      "grad_norm": 1.7367713451385498,
      "learning_rate": 1.8095684803001877e-05,
      "loss": 0.628,
      "step": 14910
    },
    {
      "epoch": 1.1964715316760224,
      "grad_norm": 1.5593657493591309,
      "learning_rate": 1.8087644063253818e-05,
      "loss": 0.707,
      "step": 14920
    },
    {
      "epoch": 1.1972734562951082,
      "grad_norm": 1.6758472919464111,
      "learning_rate": 1.8079603323505762e-05,
      "loss": 0.5956,
      "step": 14930
    },
    {
      "epoch": 1.1980753809141942,
      "grad_norm": 1.6047399044036865,
      "learning_rate": 1.8071562583757703e-05,
      "loss": 0.6069,
      "step": 14940
    },
    {
      "epoch": 1.19887730553328,
      "grad_norm": 1.7487367391586304,
      "learning_rate": 1.806352184400965e-05,
      "loss": 0.5592,
      "step": 14950
    },
    {
      "epoch": 1.1996792301523658,
      "grad_norm": 1.5885027647018433,
      "learning_rate": 1.8055481104261592e-05,
      "loss": 0.6049,
      "step": 14960
    },
    {
      "epoch": 1.2004811547714516,
      "grad_norm": 1.6879827976226807,
      "learning_rate": 1.8047440364513536e-05,
      "loss": 0.6448,
      "step": 14970
    },
    {
      "epoch": 1.2012830793905374,
      "grad_norm": 1.4497005939483643,
      "learning_rate": 1.8039399624765477e-05,
      "loss": 0.5764,
      "step": 14980
    },
    {
      "epoch": 1.2020850040096231,
      "grad_norm": 1.521064281463623,
      "learning_rate": 1.803135888501742e-05,
      "loss": 0.6791,
      "step": 14990
    },
    {
      "epoch": 1.202886928628709,
      "grad_norm": 1.7804193496704102,
      "learning_rate": 1.8023318145269366e-05,
      "loss": 0.6753,
      "step": 15000
    },
    {
      "epoch": 1.2036888532477947,
      "grad_norm": 1.6928856372833252,
      "learning_rate": 1.801527740552131e-05,
      "loss": 0.6818,
      "step": 15010
    },
    {
      "epoch": 1.2044907778668805,
      "grad_norm": 1.664268970489502,
      "learning_rate": 1.800723666577325e-05,
      "loss": 0.6339,
      "step": 15020
    },
    {
      "epoch": 1.2052927024859663,
      "grad_norm": 1.7989473342895508,
      "learning_rate": 1.7999195926025195e-05,
      "loss": 0.7059,
      "step": 15030
    },
    {
      "epoch": 1.206094627105052,
      "grad_norm": 1.7299470901489258,
      "learning_rate": 1.799115518627714e-05,
      "loss": 0.5941,
      "step": 15040
    },
    {
      "epoch": 1.206896551724138,
      "grad_norm": 1.7213943004608154,
      "learning_rate": 1.798311444652908e-05,
      "loss": 0.6414,
      "step": 15050
    },
    {
      "epoch": 1.2076984763432237,
      "grad_norm": 1.5314885377883911,
      "learning_rate": 1.7975073706781025e-05,
      "loss": 0.5846,
      "step": 15060
    },
    {
      "epoch": 1.2085004009623095,
      "grad_norm": 1.5613723993301392,
      "learning_rate": 1.7967032967032966e-05,
      "loss": 0.6151,
      "step": 15070
    },
    {
      "epoch": 1.2093023255813953,
      "grad_norm": 2.212838649749756,
      "learning_rate": 1.795899222728491e-05,
      "loss": 0.6362,
      "step": 15080
    },
    {
      "epoch": 1.210104250200481,
      "grad_norm": 1.666305661201477,
      "learning_rate": 1.7950951487536854e-05,
      "loss": 0.5906,
      "step": 15090
    },
    {
      "epoch": 1.2109061748195669,
      "grad_norm": 1.553574562072754,
      "learning_rate": 1.79429107477888e-05,
      "loss": 0.5817,
      "step": 15100
    },
    {
      "epoch": 1.2117080994386527,
      "grad_norm": 1.6463595628738403,
      "learning_rate": 1.793487000804074e-05,
      "loss": 0.5661,
      "step": 15110
    },
    {
      "epoch": 1.2125100240577387,
      "grad_norm": 1.879906177520752,
      "learning_rate": 1.7926829268292684e-05,
      "loss": 0.686,
      "step": 15120
    },
    {
      "epoch": 1.2133119486768245,
      "grad_norm": 1.8222174644470215,
      "learning_rate": 1.7918788528544625e-05,
      "loss": 0.5695,
      "step": 15130
    },
    {
      "epoch": 1.2141138732959103,
      "grad_norm": 1.689801573753357,
      "learning_rate": 1.7910747788796572e-05,
      "loss": 0.6422,
      "step": 15140
    },
    {
      "epoch": 1.214915797914996,
      "grad_norm": 1.632575511932373,
      "learning_rate": 1.7902707049048513e-05,
      "loss": 0.5794,
      "step": 15150
    },
    {
      "epoch": 1.2157177225340818,
      "grad_norm": 1.6744433641433716,
      "learning_rate": 1.7894666309300457e-05,
      "loss": 0.6193,
      "step": 15160
    },
    {
      "epoch": 1.2165196471531676,
      "grad_norm": 1.7540218830108643,
      "learning_rate": 1.78866255695524e-05,
      "loss": 0.6293,
      "step": 15170
    },
    {
      "epoch": 1.2173215717722534,
      "grad_norm": 1.9354099035263062,
      "learning_rate": 1.787858482980434e-05,
      "loss": 0.6138,
      "step": 15180
    },
    {
      "epoch": 1.2181234963913392,
      "grad_norm": 1.472372055053711,
      "learning_rate": 1.7870544090056287e-05,
      "loss": 0.6177,
      "step": 15190
    },
    {
      "epoch": 1.218925421010425,
      "grad_norm": 1.966772198677063,
      "learning_rate": 1.7862503350308228e-05,
      "loss": 0.6296,
      "step": 15200
    },
    {
      "epoch": 1.2197273456295108,
      "grad_norm": 2.1250839233398438,
      "learning_rate": 1.7854462610560172e-05,
      "loss": 0.6286,
      "step": 15210
    },
    {
      "epoch": 1.2205292702485966,
      "grad_norm": 1.7230091094970703,
      "learning_rate": 1.7846421870812113e-05,
      "loss": 0.6315,
      "step": 15220
    },
    {
      "epoch": 1.2213311948676824,
      "grad_norm": 1.9860817193984985,
      "learning_rate": 1.783838113106406e-05,
      "loss": 0.5776,
      "step": 15230
    },
    {
      "epoch": 1.2221331194867682,
      "grad_norm": 1.6090528964996338,
      "learning_rate": 1.7830340391316e-05,
      "loss": 0.6051,
      "step": 15240
    },
    {
      "epoch": 1.222935044105854,
      "grad_norm": 1.6149680614471436,
      "learning_rate": 1.7822299651567946e-05,
      "loss": 0.5682,
      "step": 15250
    },
    {
      "epoch": 1.22373696872494,
      "grad_norm": 1.8764073848724365,
      "learning_rate": 1.7814258911819887e-05,
      "loss": 0.6647,
      "step": 15260
    },
    {
      "epoch": 1.2245388933440258,
      "grad_norm": 1.4273743629455566,
      "learning_rate": 1.780621817207183e-05,
      "loss": 0.5914,
      "step": 15270
    },
    {
      "epoch": 1.2253408179631116,
      "grad_norm": 2.010429859161377,
      "learning_rate": 1.7798177432323775e-05,
      "loss": 0.707,
      "step": 15280
    },
    {
      "epoch": 1.2261427425821974,
      "grad_norm": 1.509750247001648,
      "learning_rate": 1.779013669257572e-05,
      "loss": 0.6606,
      "step": 15290
    },
    {
      "epoch": 1.2269446672012831,
      "grad_norm": 1.9990696907043457,
      "learning_rate": 1.778209595282766e-05,
      "loss": 0.5642,
      "step": 15300
    },
    {
      "epoch": 1.227746591820369,
      "grad_norm": 2.111618757247925,
      "learning_rate": 1.77740552130796e-05,
      "loss": 0.7074,
      "step": 15310
    },
    {
      "epoch": 1.2285485164394547,
      "grad_norm": 1.567197322845459,
      "learning_rate": 1.7766014473331546e-05,
      "loss": 0.6042,
      "step": 15320
    },
    {
      "epoch": 1.2293504410585405,
      "grad_norm": 1.896511435508728,
      "learning_rate": 1.775797373358349e-05,
      "loss": 0.7816,
      "step": 15330
    },
    {
      "epoch": 1.2301523656776263,
      "grad_norm": 1.5476813316345215,
      "learning_rate": 1.7749932993835434e-05,
      "loss": 0.6377,
      "step": 15340
    },
    {
      "epoch": 1.230954290296712,
      "grad_norm": 1.7235625982284546,
      "learning_rate": 1.7741892254087375e-05,
      "loss": 0.6044,
      "step": 15350
    },
    {
      "epoch": 1.231756214915798,
      "grad_norm": 1.5092676877975464,
      "learning_rate": 1.773385151433932e-05,
      "loss": 0.5836,
      "step": 15360
    },
    {
      "epoch": 1.2325581395348837,
      "grad_norm": 1.8625125885009766,
      "learning_rate": 1.772581077459126e-05,
      "loss": 0.6968,
      "step": 15370
    },
    {
      "epoch": 1.2333600641539695,
      "grad_norm": 1.867527961730957,
      "learning_rate": 1.7717770034843208e-05,
      "loss": 0.6325,
      "step": 15380
    },
    {
      "epoch": 1.2341619887730553,
      "grad_norm": 1.512209177017212,
      "learning_rate": 1.770972929509515e-05,
      "loss": 0.6881,
      "step": 15390
    },
    {
      "epoch": 1.234963913392141,
      "grad_norm": 1.5589591264724731,
      "learning_rate": 1.7701688555347093e-05,
      "loss": 0.6423,
      "step": 15400
    },
    {
      "epoch": 1.2357658380112269,
      "grad_norm": 1.5617775917053223,
      "learning_rate": 1.7693647815599034e-05,
      "loss": 0.6504,
      "step": 15410
    },
    {
      "epoch": 1.2365677626303127,
      "grad_norm": 1.506040096282959,
      "learning_rate": 1.7685607075850982e-05,
      "loss": 0.6876,
      "step": 15420
    },
    {
      "epoch": 1.2373696872493984,
      "grad_norm": 1.6778048276901245,
      "learning_rate": 1.7677566336102923e-05,
      "loss": 0.5933,
      "step": 15430
    },
    {
      "epoch": 1.2381716118684845,
      "grad_norm": 1.9478651285171509,
      "learning_rate": 1.7669525596354864e-05,
      "loss": 0.7191,
      "step": 15440
    },
    {
      "epoch": 1.2389735364875702,
      "grad_norm": 1.8631439208984375,
      "learning_rate": 1.7661484856606808e-05,
      "loss": 0.6184,
      "step": 15450
    },
    {
      "epoch": 1.239775461106656,
      "grad_norm": 1.7141566276550293,
      "learning_rate": 1.765344411685875e-05,
      "loss": 0.6403,
      "step": 15460
    },
    {
      "epoch": 1.2405773857257418,
      "grad_norm": 1.7954438924789429,
      "learning_rate": 1.7645403377110697e-05,
      "loss": 0.6417,
      "step": 15470
    },
    {
      "epoch": 1.2413793103448276,
      "grad_norm": 1.8103585243225098,
      "learning_rate": 1.7637362637362637e-05,
      "loss": 0.6027,
      "step": 15480
    },
    {
      "epoch": 1.2421812349639134,
      "grad_norm": 1.5922940969467163,
      "learning_rate": 1.7629321897614582e-05,
      "loss": 0.6203,
      "step": 15490
    },
    {
      "epoch": 1.2429831595829992,
      "grad_norm": 1.7636479139328003,
      "learning_rate": 1.7621281157866523e-05,
      "loss": 0.5677,
      "step": 15500
    },
    {
      "epoch": 1.243785084202085,
      "grad_norm": 1.5630844831466675,
      "learning_rate": 1.7613240418118467e-05,
      "loss": 0.5911,
      "step": 15510
    },
    {
      "epoch": 1.2445870088211708,
      "grad_norm": 1.8050190210342407,
      "learning_rate": 1.760519967837041e-05,
      "loss": 0.6145,
      "step": 15520
    },
    {
      "epoch": 1.2453889334402566,
      "grad_norm": 1.8482030630111694,
      "learning_rate": 1.7597158938622356e-05,
      "loss": 0.6379,
      "step": 15530
    },
    {
      "epoch": 1.2461908580593424,
      "grad_norm": 1.8280060291290283,
      "learning_rate": 1.7589118198874296e-05,
      "loss": 0.636,
      "step": 15540
    },
    {
      "epoch": 1.2469927826784282,
      "grad_norm": 1.4962759017944336,
      "learning_rate": 1.758107745912624e-05,
      "loss": 0.6342,
      "step": 15550
    },
    {
      "epoch": 1.247794707297514,
      "grad_norm": 1.7583565711975098,
      "learning_rate": 1.7573036719378185e-05,
      "loss": 0.6414,
      "step": 15560
    },
    {
      "epoch": 1.2485966319165998,
      "grad_norm": 1.7637286186218262,
      "learning_rate": 1.7564995979630126e-05,
      "loss": 0.5735,
      "step": 15570
    },
    {
      "epoch": 1.2493985565356858,
      "grad_norm": 1.892454743385315,
      "learning_rate": 1.755695523988207e-05,
      "loss": 0.6401,
      "step": 15580
    },
    {
      "epoch": 1.2502004811547716,
      "grad_norm": 1.646873116493225,
      "learning_rate": 1.754891450013401e-05,
      "loss": 0.5755,
      "step": 15590
    },
    {
      "epoch": 1.2510024057738574,
      "grad_norm": 1.8512674570083618,
      "learning_rate": 1.7540873760385955e-05,
      "loss": 0.7433,
      "step": 15600
    },
    {
      "epoch": 1.2518043303929431,
      "grad_norm": 1.714450478553772,
      "learning_rate": 1.75328330206379e-05,
      "loss": 0.5687,
      "step": 15610
    },
    {
      "epoch": 1.252606255012029,
      "grad_norm": 1.7026242017745972,
      "learning_rate": 1.7524792280889844e-05,
      "loss": 0.5986,
      "step": 15620
    },
    {
      "epoch": 1.2534081796311147,
      "grad_norm": 1.6214118003845215,
      "learning_rate": 1.7516751541141785e-05,
      "loss": 0.6528,
      "step": 15630
    },
    {
      "epoch": 1.2542101042502005,
      "grad_norm": 1.581540822982788,
      "learning_rate": 1.750871080139373e-05,
      "loss": 0.604,
      "step": 15640
    },
    {
      "epoch": 1.2550120288692863,
      "grad_norm": 1.6850769519805908,
      "learning_rate": 1.750067006164567e-05,
      "loss": 0.5987,
      "step": 15650
    },
    {
      "epoch": 1.255813953488372,
      "grad_norm": 1.6737957000732422,
      "learning_rate": 1.7492629321897618e-05,
      "loss": 0.6497,
      "step": 15660
    },
    {
      "epoch": 1.256615878107458,
      "grad_norm": 1.514340877532959,
      "learning_rate": 1.748458858214956e-05,
      "loss": 0.5504,
      "step": 15670
    },
    {
      "epoch": 1.2574178027265437,
      "grad_norm": 1.8270151615142822,
      "learning_rate": 1.7476547842401503e-05,
      "loss": 0.6055,
      "step": 15680
    },
    {
      "epoch": 1.2582197273456295,
      "grad_norm": 1.9335719347000122,
      "learning_rate": 1.7468507102653444e-05,
      "loss": 0.6758,
      "step": 15690
    },
    {
      "epoch": 1.2590216519647153,
      "grad_norm": 1.6170176267623901,
      "learning_rate": 1.7460466362905385e-05,
      "loss": 0.5881,
      "step": 15700
    },
    {
      "epoch": 1.259823576583801,
      "grad_norm": 1.7656596899032593,
      "learning_rate": 1.7452425623157332e-05,
      "loss": 0.6489,
      "step": 15710
    },
    {
      "epoch": 1.2606255012028869,
      "grad_norm": 1.7940294742584229,
      "learning_rate": 1.7444384883409273e-05,
      "loss": 0.6503,
      "step": 15720
    },
    {
      "epoch": 1.2614274258219726,
      "grad_norm": 1.900373935699463,
      "learning_rate": 1.7436344143661218e-05,
      "loss": 0.5994,
      "step": 15730
    },
    {
      "epoch": 1.2622293504410584,
      "grad_norm": 1.7346657514572144,
      "learning_rate": 1.742830340391316e-05,
      "loss": 0.6659,
      "step": 15740
    },
    {
      "epoch": 1.2630312750601442,
      "grad_norm": 1.806484580039978,
      "learning_rate": 1.7420262664165106e-05,
      "loss": 0.5388,
      "step": 15750
    },
    {
      "epoch": 1.26383319967923,
      "grad_norm": 1.842267394065857,
      "learning_rate": 1.7412221924417047e-05,
      "loss": 0.6172,
      "step": 15760
    },
    {
      "epoch": 1.264635124298316,
      "grad_norm": 1.813433051109314,
      "learning_rate": 1.740418118466899e-05,
      "loss": 0.6173,
      "step": 15770
    },
    {
      "epoch": 1.2654370489174018,
      "grad_norm": 2.004713296890259,
      "learning_rate": 1.7396140444920932e-05,
      "loss": 0.5994,
      "step": 15780
    },
    {
      "epoch": 1.2662389735364876,
      "grad_norm": 1.7443737983703613,
      "learning_rate": 1.7388099705172877e-05,
      "loss": 0.5969,
      "step": 15790
    },
    {
      "epoch": 1.2670408981555734,
      "grad_norm": 1.6715856790542603,
      "learning_rate": 1.738005896542482e-05,
      "loss": 0.5972,
      "step": 15800
    },
    {
      "epoch": 1.2678428227746592,
      "grad_norm": 1.5222623348236084,
      "learning_rate": 1.7372018225676765e-05,
      "loss": 0.6055,
      "step": 15810
    },
    {
      "epoch": 1.268644747393745,
      "grad_norm": 2.0080440044403076,
      "learning_rate": 1.7363977485928706e-05,
      "loss": 0.6729,
      "step": 15820
    },
    {
      "epoch": 1.2694466720128308,
      "grad_norm": 1.6446223258972168,
      "learning_rate": 1.7355936746180647e-05,
      "loss": 0.6578,
      "step": 15830
    },
    {
      "epoch": 1.2702485966319166,
      "grad_norm": 1.6553699970245361,
      "learning_rate": 1.734789600643259e-05,
      "loss": 0.6358,
      "step": 15840
    },
    {
      "epoch": 1.2710505212510024,
      "grad_norm": 2.313021421432495,
      "learning_rate": 1.7339855266684536e-05,
      "loss": 0.6608,
      "step": 15850
    },
    {
      "epoch": 1.2718524458700882,
      "grad_norm": 1.8485366106033325,
      "learning_rate": 1.733181452693648e-05,
      "loss": 0.6737,
      "step": 15860
    },
    {
      "epoch": 1.272654370489174,
      "grad_norm": 1.7349166870117188,
      "learning_rate": 1.732377378718842e-05,
      "loss": 0.6006,
      "step": 15870
    },
    {
      "epoch": 1.2734562951082598,
      "grad_norm": 1.6248706579208374,
      "learning_rate": 1.7315733047440365e-05,
      "loss": 0.6334,
      "step": 15880
    },
    {
      "epoch": 1.2742582197273458,
      "grad_norm": 2.2336907386779785,
      "learning_rate": 1.7307692307692306e-05,
      "loss": 0.6601,
      "step": 15890
    },
    {
      "epoch": 1.2750601443464316,
      "grad_norm": 1.6903926134109497,
      "learning_rate": 1.7299651567944254e-05,
      "loss": 0.6057,
      "step": 15900
    },
    {
      "epoch": 1.2758620689655173,
      "grad_norm": 1.4969171285629272,
      "learning_rate": 1.7291610828196194e-05,
      "loss": 0.6519,
      "step": 15910
    },
    {
      "epoch": 1.2766639935846031,
      "grad_norm": 1.4775601625442505,
      "learning_rate": 1.728357008844814e-05,
      "loss": 0.6003,
      "step": 15920
    },
    {
      "epoch": 1.277465918203689,
      "grad_norm": 1.7836918830871582,
      "learning_rate": 1.727552934870008e-05,
      "loss": 0.5859,
      "step": 15930
    },
    {
      "epoch": 1.2782678428227747,
      "grad_norm": 2.4668374061584473,
      "learning_rate": 1.7267488608952024e-05,
      "loss": 0.6343,
      "step": 15940
    },
    {
      "epoch": 1.2790697674418605,
      "grad_norm": 1.6890884637832642,
      "learning_rate": 1.7259447869203968e-05,
      "loss": 0.6907,
      "step": 15950
    },
    {
      "epoch": 1.2798716920609463,
      "grad_norm": 1.842853307723999,
      "learning_rate": 1.725140712945591e-05,
      "loss": 0.5776,
      "step": 15960
    },
    {
      "epoch": 1.280673616680032,
      "grad_norm": 1.829272747039795,
      "learning_rate": 1.7243366389707853e-05,
      "loss": 0.6365,
      "step": 15970
    },
    {
      "epoch": 1.281475541299118,
      "grad_norm": 1.8329824209213257,
      "learning_rate": 1.7235325649959794e-05,
      "loss": 0.6271,
      "step": 15980
    },
    {
      "epoch": 1.2822774659182037,
      "grad_norm": 1.5944010019302368,
      "learning_rate": 1.7227284910211742e-05,
      "loss": 0.6401,
      "step": 15990
    },
    {
      "epoch": 1.2830793905372895,
      "grad_norm": 1.7920933961868286,
      "learning_rate": 1.7219244170463683e-05,
      "loss": 0.5778,
      "step": 16000
    },
    {
      "epoch": 1.2838813151563753,
      "grad_norm": 1.7009950876235962,
      "learning_rate": 1.7211203430715627e-05,
      "loss": 0.6214,
      "step": 16010
    },
    {
      "epoch": 1.284683239775461,
      "grad_norm": 1.6271408796310425,
      "learning_rate": 1.7203162690967568e-05,
      "loss": 0.7119,
      "step": 16020
    },
    {
      "epoch": 1.2854851643945469,
      "grad_norm": 1.7140686511993408,
      "learning_rate": 1.7195121951219512e-05,
      "loss": 0.6579,
      "step": 16030
    },
    {
      "epoch": 1.2862870890136326,
      "grad_norm": 1.7415837049484253,
      "learning_rate": 1.7187081211471457e-05,
      "loss": 0.6825,
      "step": 16040
    },
    {
      "epoch": 1.2870890136327184,
      "grad_norm": 1.8653028011322021,
      "learning_rate": 1.71790404717234e-05,
      "loss": 0.582,
      "step": 16050
    },
    {
      "epoch": 1.2878909382518042,
      "grad_norm": 1.7259422540664673,
      "learning_rate": 1.7170999731975342e-05,
      "loss": 0.5984,
      "step": 16060
    },
    {
      "epoch": 1.28869286287089,
      "grad_norm": 2.179605007171631,
      "learning_rate": 1.7162958992227283e-05,
      "loss": 0.6938,
      "step": 16070
    },
    {
      "epoch": 1.2894947874899758,
      "grad_norm": 1.8518222570419312,
      "learning_rate": 1.7154918252479227e-05,
      "loss": 0.6849,
      "step": 16080
    },
    {
      "epoch": 1.2902967121090618,
      "grad_norm": 2.1049411296844482,
      "learning_rate": 1.714687751273117e-05,
      "loss": 0.6715,
      "step": 16090
    },
    {
      "epoch": 1.2910986367281476,
      "grad_norm": 1.8388382196426392,
      "learning_rate": 1.7138836772983116e-05,
      "loss": 0.5617,
      "step": 16100
    },
    {
      "epoch": 1.2919005613472334,
      "grad_norm": 1.6495779752731323,
      "learning_rate": 1.7130796033235057e-05,
      "loss": 0.6468,
      "step": 16110
    },
    {
      "epoch": 1.2927024859663192,
      "grad_norm": 2.0012855529785156,
      "learning_rate": 1.7122755293487e-05,
      "loss": 0.6828,
      "step": 16120
    },
    {
      "epoch": 1.293504410585405,
      "grad_norm": 1.8279573917388916,
      "learning_rate": 1.7114714553738945e-05,
      "loss": 0.6665,
      "step": 16130
    },
    {
      "epoch": 1.2943063352044908,
      "grad_norm": 1.7906969785690308,
      "learning_rate": 1.710667381399089e-05,
      "loss": 0.6093,
      "step": 16140
    },
    {
      "epoch": 1.2951082598235766,
      "grad_norm": 2.0168709754943848,
      "learning_rate": 1.709863307424283e-05,
      "loss": 0.5656,
      "step": 16150
    },
    {
      "epoch": 1.2959101844426624,
      "grad_norm": 2.0814123153686523,
      "learning_rate": 1.7090592334494775e-05,
      "loss": 0.5913,
      "step": 16160
    },
    {
      "epoch": 1.2967121090617482,
      "grad_norm": 2.042715549468994,
      "learning_rate": 1.7082551594746716e-05,
      "loss": 0.6468,
      "step": 16170
    },
    {
      "epoch": 1.297514033680834,
      "grad_norm": 1.5197185277938843,
      "learning_rate": 1.7074510854998663e-05,
      "loss": 0.6233,
      "step": 16180
    },
    {
      "epoch": 1.2983159582999197,
      "grad_norm": 1.626234769821167,
      "learning_rate": 1.7066470115250604e-05,
      "loss": 0.5856,
      "step": 16190
    },
    {
      "epoch": 1.2991178829190055,
      "grad_norm": 1.8056693077087402,
      "learning_rate": 1.7058429375502545e-05,
      "loss": 0.6824,
      "step": 16200
    },
    {
      "epoch": 1.2999198075380916,
      "grad_norm": 1.6301498413085938,
      "learning_rate": 1.705038863575449e-05,
      "loss": 0.622,
      "step": 16210
    },
    {
      "epoch": 1.3007217321571773,
      "grad_norm": 1.4811457395553589,
      "learning_rate": 1.704234789600643e-05,
      "loss": 0.5794,
      "step": 16220
    },
    {
      "epoch": 1.3015236567762631,
      "grad_norm": 1.6097360849380493,
      "learning_rate": 1.7034307156258378e-05,
      "loss": 0.6027,
      "step": 16230
    },
    {
      "epoch": 1.302325581395349,
      "grad_norm": 1.5198719501495361,
      "learning_rate": 1.702626641651032e-05,
      "loss": 0.5173,
      "step": 16240
    },
    {
      "epoch": 1.3031275060144347,
      "grad_norm": 1.7623876333236694,
      "learning_rate": 1.7018225676762263e-05,
      "loss": 0.6006,
      "step": 16250
    },
    {
      "epoch": 1.3039294306335205,
      "grad_norm": 1.7226158380508423,
      "learning_rate": 1.7010184937014204e-05,
      "loss": 0.6167,
      "step": 16260
    },
    {
      "epoch": 1.3047313552526063,
      "grad_norm": 1.728520154953003,
      "learning_rate": 1.7002144197266148e-05,
      "loss": 0.704,
      "step": 16270
    },
    {
      "epoch": 1.305533279871692,
      "grad_norm": 1.8009965419769287,
      "learning_rate": 1.6994103457518093e-05,
      "loss": 0.5866,
      "step": 16280
    },
    {
      "epoch": 1.306335204490778,
      "grad_norm": 1.5057425498962402,
      "learning_rate": 1.6986062717770037e-05,
      "loss": 0.644,
      "step": 16290
    },
    {
      "epoch": 1.3071371291098637,
      "grad_norm": 2.0355007648468018,
      "learning_rate": 1.6978021978021978e-05,
      "loss": 0.5813,
      "step": 16300
    },
    {
      "epoch": 1.3079390537289495,
      "grad_norm": 1.9356293678283691,
      "learning_rate": 1.6969981238273922e-05,
      "loss": 0.63,
      "step": 16310
    },
    {
      "epoch": 1.3087409783480353,
      "grad_norm": 2.44625186920166,
      "learning_rate": 1.6961940498525866e-05,
      "loss": 0.5951,
      "step": 16320
    },
    {
      "epoch": 1.309542902967121,
      "grad_norm": 1.6668822765350342,
      "learning_rate": 1.6953899758777807e-05,
      "loss": 0.6207,
      "step": 16330
    },
    {
      "epoch": 1.3103448275862069,
      "grad_norm": 1.8125431537628174,
      "learning_rate": 1.694585901902975e-05,
      "loss": 0.6516,
      "step": 16340
    },
    {
      "epoch": 1.3111467522052926,
      "grad_norm": 1.971166729927063,
      "learning_rate": 1.6937818279281692e-05,
      "loss": 0.6234,
      "step": 16350
    },
    {
      "epoch": 1.3119486768243784,
      "grad_norm": 1.6632084846496582,
      "learning_rate": 1.6929777539533637e-05,
      "loss": 0.6232,
      "step": 16360
    },
    {
      "epoch": 1.3127506014434642,
      "grad_norm": 1.8618624210357666,
      "learning_rate": 1.692173679978558e-05,
      "loss": 0.6132,
      "step": 16370
    },
    {
      "epoch": 1.31355252606255,
      "grad_norm": 1.9524856805801392,
      "learning_rate": 1.6913696060037525e-05,
      "loss": 0.5403,
      "step": 16380
    },
    {
      "epoch": 1.3143544506816358,
      "grad_norm": 1.880910038948059,
      "learning_rate": 1.6905655320289466e-05,
      "loss": 0.6683,
      "step": 16390
    },
    {
      "epoch": 1.3151563753007216,
      "grad_norm": 1.5730409622192383,
      "learning_rate": 1.689761458054141e-05,
      "loss": 0.5789,
      "step": 16400
    },
    {
      "epoch": 1.3159582999198076,
      "grad_norm": 1.7266916036605835,
      "learning_rate": 1.688957384079335e-05,
      "loss": 0.66,
      "step": 16410
    },
    {
      "epoch": 1.3167602245388934,
      "grad_norm": 1.737673044204712,
      "learning_rate": 1.68815331010453e-05,
      "loss": 0.56,
      "step": 16420
    },
    {
      "epoch": 1.3175621491579792,
      "grad_norm": 1.7531427145004272,
      "learning_rate": 1.687349236129724e-05,
      "loss": 0.5718,
      "step": 16430
    },
    {
      "epoch": 1.318364073777065,
      "grad_norm": 2.5870509147644043,
      "learning_rate": 1.6865451621549184e-05,
      "loss": 0.6472,
      "step": 16440
    },
    {
      "epoch": 1.3191659983961508,
      "grad_norm": 1.9196947813034058,
      "learning_rate": 1.6857410881801125e-05,
      "loss": 0.7098,
      "step": 16450
    },
    {
      "epoch": 1.3199679230152366,
      "grad_norm": 1.6393619775772095,
      "learning_rate": 1.6849370142053066e-05,
      "loss": 0.64,
      "step": 16460
    },
    {
      "epoch": 1.3207698476343224,
      "grad_norm": 1.8894578218460083,
      "learning_rate": 1.6841329402305014e-05,
      "loss": 0.6528,
      "step": 16470
    },
    {
      "epoch": 1.3215717722534082,
      "grad_norm": 1.74313223361969,
      "learning_rate": 1.6833288662556955e-05,
      "loss": 0.5604,
      "step": 16480
    },
    {
      "epoch": 1.322373696872494,
      "grad_norm": 1.6927334070205688,
      "learning_rate": 1.68252479228089e-05,
      "loss": 0.5949,
      "step": 16490
    },
    {
      "epoch": 1.3231756214915797,
      "grad_norm": 1.7276780605316162,
      "learning_rate": 1.681720718306084e-05,
      "loss": 0.6491,
      "step": 16500
    },
    {
      "epoch": 1.3239775461106655,
      "grad_norm": 1.8662387132644653,
      "learning_rate": 1.6809166443312788e-05,
      "loss": 0.6652,
      "step": 16510
    },
    {
      "epoch": 1.3247794707297513,
      "grad_norm": 2.0484492778778076,
      "learning_rate": 1.680112570356473e-05,
      "loss": 0.6216,
      "step": 16520
    },
    {
      "epoch": 1.3255813953488373,
      "grad_norm": 2.1266567707061768,
      "learning_rate": 1.6793084963816673e-05,
      "loss": 0.7273,
      "step": 16530
    },
    {
      "epoch": 1.3263833199679231,
      "grad_norm": 1.7887489795684814,
      "learning_rate": 1.6785044224068614e-05,
      "loss": 0.6824,
      "step": 16540
    },
    {
      "epoch": 1.327185244587009,
      "grad_norm": 1.8963984251022339,
      "learning_rate": 1.6777003484320558e-05,
      "loss": 0.6078,
      "step": 16550
    },
    {
      "epoch": 1.3279871692060947,
      "grad_norm": 1.5415476560592651,
      "learning_rate": 1.6768962744572502e-05,
      "loss": 0.5441,
      "step": 16560
    },
    {
      "epoch": 1.3287890938251805,
      "grad_norm": 1.8208463191986084,
      "learning_rate": 1.6760922004824446e-05,
      "loss": 0.6386,
      "step": 16570
    },
    {
      "epoch": 1.3295910184442663,
      "grad_norm": 1.7544292211532593,
      "learning_rate": 1.6752881265076387e-05,
      "loss": 0.5872,
      "step": 16580
    },
    {
      "epoch": 1.330392943063352,
      "grad_norm": 1.5621789693832397,
      "learning_rate": 1.6744840525328328e-05,
      "loss": 0.6733,
      "step": 16590
    },
    {
      "epoch": 1.3311948676824379,
      "grad_norm": 1.6405296325683594,
      "learning_rate": 1.6736799785580273e-05,
      "loss": 0.5543,
      "step": 16600
    },
    {
      "epoch": 1.3319967923015237,
      "grad_norm": 2.250051498413086,
      "learning_rate": 1.6728759045832217e-05,
      "loss": 0.5714,
      "step": 16610
    },
    {
      "epoch": 1.3327987169206095,
      "grad_norm": 1.9213895797729492,
      "learning_rate": 1.672071830608416e-05,
      "loss": 0.6061,
      "step": 16620
    },
    {
      "epoch": 1.3336006415396953,
      "grad_norm": 1.852483868598938,
      "learning_rate": 1.6712677566336102e-05,
      "loss": 0.6191,
      "step": 16630
    },
    {
      "epoch": 1.334402566158781,
      "grad_norm": 1.5860995054244995,
      "learning_rate": 1.6704636826588046e-05,
      "loss": 0.639,
      "step": 16640
    },
    {
      "epoch": 1.3352044907778668,
      "grad_norm": 1.8660951852798462,
      "learning_rate": 1.6696596086839987e-05,
      "loss": 0.6208,
      "step": 16650
    },
    {
      "epoch": 1.3360064153969526,
      "grad_norm": 1.6045540571212769,
      "learning_rate": 1.6688555347091935e-05,
      "loss": 0.669,
      "step": 16660
    },
    {
      "epoch": 1.3368083400160384,
      "grad_norm": 1.8505910634994507,
      "learning_rate": 1.6680514607343876e-05,
      "loss": 0.6139,
      "step": 16670
    },
    {
      "epoch": 1.3376102646351242,
      "grad_norm": 1.987541913986206,
      "learning_rate": 1.667247386759582e-05,
      "loss": 0.5757,
      "step": 16680
    },
    {
      "epoch": 1.33841218925421,
      "grad_norm": 1.4744800329208374,
      "learning_rate": 1.666443312784776e-05,
      "loss": 0.611,
      "step": 16690
    },
    {
      "epoch": 1.3392141138732958,
      "grad_norm": 2.08025860786438,
      "learning_rate": 1.665639238809971e-05,
      "loss": 0.5853,
      "step": 16700
    },
    {
      "epoch": 1.3400160384923816,
      "grad_norm": 2.1682238578796387,
      "learning_rate": 1.664835164835165e-05,
      "loss": 0.5824,
      "step": 16710
    },
    {
      "epoch": 1.3408179631114674,
      "grad_norm": 1.9347690343856812,
      "learning_rate": 1.664031090860359e-05,
      "loss": 0.577,
      "step": 16720
    },
    {
      "epoch": 1.3416198877305534,
      "grad_norm": 1.537519097328186,
      "learning_rate": 1.6632270168855535e-05,
      "loss": 0.6487,
      "step": 16730
    },
    {
      "epoch": 1.3424218123496392,
      "grad_norm": 1.9566715955734253,
      "learning_rate": 1.6624229429107476e-05,
      "loss": 0.6418,
      "step": 16740
    },
    {
      "epoch": 1.343223736968725,
      "grad_norm": 1.8500412702560425,
      "learning_rate": 1.6616188689359423e-05,
      "loss": 0.6045,
      "step": 16750
    },
    {
      "epoch": 1.3440256615878108,
      "grad_norm": 1.9648603200912476,
      "learning_rate": 1.6608147949611364e-05,
      "loss": 0.6426,
      "step": 16760
    },
    {
      "epoch": 1.3448275862068966,
      "grad_norm": 1.6111987829208374,
      "learning_rate": 1.660010720986331e-05,
      "loss": 0.6237,
      "step": 16770
    },
    {
      "epoch": 1.3456295108259824,
      "grad_norm": 1.6746468544006348,
      "learning_rate": 1.659206647011525e-05,
      "loss": 0.7001,
      "step": 16780
    },
    {
      "epoch": 1.3464314354450682,
      "grad_norm": 2.010396957397461,
      "learning_rate": 1.6584025730367194e-05,
      "loss": 0.5906,
      "step": 16790
    },
    {
      "epoch": 1.347233360064154,
      "grad_norm": 1.707045316696167,
      "learning_rate": 1.6575984990619138e-05,
      "loss": 0.6643,
      "step": 16800
    },
    {
      "epoch": 1.3480352846832397,
      "grad_norm": 1.6007357835769653,
      "learning_rate": 1.6567944250871082e-05,
      "loss": 0.6097,
      "step": 16810
    },
    {
      "epoch": 1.3488372093023255,
      "grad_norm": 1.6433154344558716,
      "learning_rate": 1.6559903511123023e-05,
      "loss": 0.6635,
      "step": 16820
    },
    {
      "epoch": 1.3496391339214113,
      "grad_norm": 1.502601146697998,
      "learning_rate": 1.6551862771374968e-05,
      "loss": 0.635,
      "step": 16830
    },
    {
      "epoch": 1.3504410585404971,
      "grad_norm": 1.4711443185806274,
      "learning_rate": 1.6543822031626912e-05,
      "loss": 0.6098,
      "step": 16840
    },
    {
      "epoch": 1.3512429831595831,
      "grad_norm": 2.000129222869873,
      "learning_rate": 1.6535781291878853e-05,
      "loss": 0.6088,
      "step": 16850
    },
    {
      "epoch": 1.352044907778669,
      "grad_norm": 1.8879115581512451,
      "learning_rate": 1.6527740552130797e-05,
      "loss": 0.6309,
      "step": 16860
    },
    {
      "epoch": 1.3528468323977547,
      "grad_norm": 1.8781639337539673,
      "learning_rate": 1.6519699812382738e-05,
      "loss": 0.6192,
      "step": 16870
    },
    {
      "epoch": 1.3536487570168405,
      "grad_norm": 1.8822280168533325,
      "learning_rate": 1.6511659072634682e-05,
      "loss": 0.6113,
      "step": 16880
    },
    {
      "epoch": 1.3544506816359263,
      "grad_norm": 2.3512332439422607,
      "learning_rate": 1.6503618332886627e-05,
      "loss": 0.6153,
      "step": 16890
    },
    {
      "epoch": 1.355252606255012,
      "grad_norm": 1.5576541423797607,
      "learning_rate": 1.649557759313857e-05,
      "loss": 0.5833,
      "step": 16900
    },
    {
      "epoch": 1.3560545308740979,
      "grad_norm": 1.5295295715332031,
      "learning_rate": 1.6487536853390512e-05,
      "loss": 0.6092,
      "step": 16910
    },
    {
      "epoch": 1.3568564554931837,
      "grad_norm": 1.5897647142410278,
      "learning_rate": 1.6479496113642456e-05,
      "loss": 0.6874,
      "step": 16920
    },
    {
      "epoch": 1.3576583801122695,
      "grad_norm": 1.814406394958496,
      "learning_rate": 1.6471455373894397e-05,
      "loss": 0.6643,
      "step": 16930
    },
    {
      "epoch": 1.3584603047313553,
      "grad_norm": 1.6792926788330078,
      "learning_rate": 1.6463414634146345e-05,
      "loss": 0.6284,
      "step": 16940
    },
    {
      "epoch": 1.359262229350441,
      "grad_norm": 1.7989351749420166,
      "learning_rate": 1.6455373894398285e-05,
      "loss": 0.5541,
      "step": 16950
    },
    {
      "epoch": 1.3600641539695268,
      "grad_norm": 1.6804505586624146,
      "learning_rate": 1.644733315465023e-05,
      "loss": 0.635,
      "step": 16960
    },
    {
      "epoch": 1.3608660785886126,
      "grad_norm": 1.5447062253952026,
      "learning_rate": 1.643929241490217e-05,
      "loss": 0.6486,
      "step": 16970
    },
    {
      "epoch": 1.3616680032076984,
      "grad_norm": 1.6918022632598877,
      "learning_rate": 1.643125167515411e-05,
      "loss": 0.5622,
      "step": 16980
    },
    {
      "epoch": 1.3624699278267842,
      "grad_norm": 1.6345802545547485,
      "learning_rate": 1.642321093540606e-05,
      "loss": 0.6056,
      "step": 16990
    },
    {
      "epoch": 1.36327185244587,
      "grad_norm": 1.9909257888793945,
      "learning_rate": 1.6415170195658e-05,
      "loss": 0.598,
      "step": 17000
    },
    {
      "epoch": 1.3640737770649558,
      "grad_norm": 1.869077205657959,
      "learning_rate": 1.6407129455909944e-05,
      "loss": 0.6643,
      "step": 17010
    },
    {
      "epoch": 1.3648757016840416,
      "grad_norm": 1.7706636190414429,
      "learning_rate": 1.6399088716161885e-05,
      "loss": 0.611,
      "step": 17020
    },
    {
      "epoch": 1.3656776263031274,
      "grad_norm": 1.617131233215332,
      "learning_rate": 1.6391047976413833e-05,
      "loss": 0.6113,
      "step": 17030
    },
    {
      "epoch": 1.3664795509222132,
      "grad_norm": 1.9359393119812012,
      "learning_rate": 1.6383007236665774e-05,
      "loss": 0.6518,
      "step": 17040
    },
    {
      "epoch": 1.3672814755412992,
      "grad_norm": 1.7442435026168823,
      "learning_rate": 1.6374966496917718e-05,
      "loss": 0.6796,
      "step": 17050
    },
    {
      "epoch": 1.368083400160385,
      "grad_norm": 2.136524200439453,
      "learning_rate": 1.636692575716966e-05,
      "loss": 0.7071,
      "step": 17060
    },
    {
      "epoch": 1.3688853247794708,
      "grad_norm": 1.8261473178863525,
      "learning_rate": 1.6358885017421603e-05,
      "loss": 0.6403,
      "step": 17070
    },
    {
      "epoch": 1.3696872493985566,
      "grad_norm": 1.4651740789413452,
      "learning_rate": 1.6350844277673548e-05,
      "loss": 0.5578,
      "step": 17080
    },
    {
      "epoch": 1.3704891740176424,
      "grad_norm": 1.595777988433838,
      "learning_rate": 1.6342803537925492e-05,
      "loss": 0.5813,
      "step": 17090
    },
    {
      "epoch": 1.3712910986367282,
      "grad_norm": 1.5298998355865479,
      "learning_rate": 1.6334762798177433e-05,
      "loss": 0.5604,
      "step": 17100
    },
    {
      "epoch": 1.372093023255814,
      "grad_norm": 1.8021767139434814,
      "learning_rate": 1.6326722058429374e-05,
      "loss": 0.6492,
      "step": 17110
    },
    {
      "epoch": 1.3728949478748997,
      "grad_norm": 1.9020968675613403,
      "learning_rate": 1.6318681318681318e-05,
      "loss": 0.68,
      "step": 17120
    },
    {
      "epoch": 1.3736968724939855,
      "grad_norm": 1.7488080263137817,
      "learning_rate": 1.6310640578933262e-05,
      "loss": 0.6741,
      "step": 17130
    },
    {
      "epoch": 1.3744987971130713,
      "grad_norm": 1.8181402683258057,
      "learning_rate": 1.6302599839185207e-05,
      "loss": 0.6012,
      "step": 17140
    },
    {
      "epoch": 1.3753007217321571,
      "grad_norm": 1.9184684753417969,
      "learning_rate": 1.6294559099437148e-05,
      "loss": 0.6512,
      "step": 17150
    },
    {
      "epoch": 1.376102646351243,
      "grad_norm": 1.7974193096160889,
      "learning_rate": 1.6286518359689092e-05,
      "loss": 0.6073,
      "step": 17160
    },
    {
      "epoch": 1.376904570970329,
      "grad_norm": 1.6913566589355469,
      "learning_rate": 1.6278477619941033e-05,
      "loss": 0.6806,
      "step": 17170
    },
    {
      "epoch": 1.3777064955894147,
      "grad_norm": 1.7586791515350342,
      "learning_rate": 1.627043688019298e-05,
      "loss": 0.6015,
      "step": 17180
    },
    {
      "epoch": 1.3785084202085005,
      "grad_norm": 1.664923906326294,
      "learning_rate": 1.626239614044492e-05,
      "loss": 0.5798,
      "step": 17190
    },
    {
      "epoch": 1.3793103448275863,
      "grad_norm": 2.6141915321350098,
      "learning_rate": 1.6254355400696866e-05,
      "loss": 0.6715,
      "step": 17200
    },
    {
      "epoch": 1.380112269446672,
      "grad_norm": 2.02042293548584,
      "learning_rate": 1.6246314660948807e-05,
      "loss": 0.6022,
      "step": 17210
    },
    {
      "epoch": 1.3809141940657579,
      "grad_norm": NaN,
      "learning_rate": 1.623827392120075e-05,
      "loss": 0.6372,
      "step": 17220
    },
    {
      "epoch": 1.3817161186848437,
      "grad_norm": 1.8353620767593384,
      "learning_rate": 1.62310372554275e-05,
      "loss": 0.6298,
      "step": 17230
    },
    {
      "epoch": 1.3825180433039295,
      "grad_norm": 1.5458965301513672,
      "learning_rate": 1.6222996515679445e-05,
      "loss": 0.6424,
      "step": 17240
    },
    {
      "epoch": 1.3833199679230153,
      "grad_norm": 1.6084150075912476,
      "learning_rate": 1.6214955775931386e-05,
      "loss": 0.5677,
      "step": 17250
    },
    {
      "epoch": 1.384121892542101,
      "grad_norm": 1.5327856540679932,
      "learning_rate": 1.620691503618333e-05,
      "loss": 0.6386,
      "step": 17260
    },
    {
      "epoch": 1.3849238171611868,
      "grad_norm": 1.7775707244873047,
      "learning_rate": 1.619887429643527e-05,
      "loss": 0.6445,
      "step": 17270
    },
    {
      "epoch": 1.3857257417802726,
      "grad_norm": 1.6953362226486206,
      "learning_rate": 1.619083355668722e-05,
      "loss": 0.5984,
      "step": 17280
    },
    {
      "epoch": 1.3865276663993584,
      "grad_norm": 1.8278902769088745,
      "learning_rate": 1.618279281693916e-05,
      "loss": 0.5875,
      "step": 17290
    },
    {
      "epoch": 1.3873295910184442,
      "grad_norm": 1.9048796892166138,
      "learning_rate": 1.61747520771911e-05,
      "loss": 0.5968,
      "step": 17300
    },
    {
      "epoch": 1.38813151563753,
      "grad_norm": 1.853515386581421,
      "learning_rate": 1.6166711337443044e-05,
      "loss": 0.701,
      "step": 17310
    },
    {
      "epoch": 1.3889334402566158,
      "grad_norm": 1.6397751569747925,
      "learning_rate": 1.615867059769499e-05,
      "loss": 0.5296,
      "step": 17320
    },
    {
      "epoch": 1.3897353648757016,
      "grad_norm": 1.9924159049987793,
      "learning_rate": 1.6150629857946933e-05,
      "loss": 0.7509,
      "step": 17330
    },
    {
      "epoch": 1.3905372894947874,
      "grad_norm": 1.6811367273330688,
      "learning_rate": 1.6142589118198874e-05,
      "loss": 0.5614,
      "step": 17340
    },
    {
      "epoch": 1.3913392141138732,
      "grad_norm": 1.9549041986465454,
      "learning_rate": 1.6134548378450818e-05,
      "loss": 0.6273,
      "step": 17350
    },
    {
      "epoch": 1.392141138732959,
      "grad_norm": 1.6626187562942505,
      "learning_rate": 1.612650763870276e-05,
      "loss": 0.6816,
      "step": 17360
    },
    {
      "epoch": 1.392943063352045,
      "grad_norm": 1.644802451133728,
      "learning_rate": 1.6118466898954707e-05,
      "loss": 0.6218,
      "step": 17370
    },
    {
      "epoch": 1.3937449879711308,
      "grad_norm": 1.5810394287109375,
      "learning_rate": 1.6110426159206648e-05,
      "loss": 0.5884,
      "step": 17380
    },
    {
      "epoch": 1.3945469125902166,
      "grad_norm": 1.6616462469100952,
      "learning_rate": 1.6102385419458592e-05,
      "loss": 0.6317,
      "step": 17390
    },
    {
      "epoch": 1.3953488372093024,
      "grad_norm": 1.8367259502410889,
      "learning_rate": 1.6094344679710533e-05,
      "loss": 0.6046,
      "step": 17400
    },
    {
      "epoch": 1.3961507618283882,
      "grad_norm": 2.379577159881592,
      "learning_rate": 1.6086303939962477e-05,
      "loss": 0.6917,
      "step": 17410
    },
    {
      "epoch": 1.396952686447474,
      "grad_norm": 1.785103678703308,
      "learning_rate": 1.607826320021442e-05,
      "loss": 0.6069,
      "step": 17420
    },
    {
      "epoch": 1.3977546110665597,
      "grad_norm": 1.9257960319519043,
      "learning_rate": 1.6070222460466362e-05,
      "loss": 0.6332,
      "step": 17430
    },
    {
      "epoch": 1.3985565356856455,
      "grad_norm": 1.91647207736969,
      "learning_rate": 1.6062181720718307e-05,
      "loss": 0.5863,
      "step": 17440
    },
    {
      "epoch": 1.3993584603047313,
      "grad_norm": 1.6661536693572998,
      "learning_rate": 1.6054140980970248e-05,
      "loss": 0.5915,
      "step": 17450
    },
    {
      "epoch": 1.4001603849238171,
      "grad_norm": 1.5945789813995361,
      "learning_rate": 1.6046100241222192e-05,
      "loss": 0.5878,
      "step": 17460
    },
    {
      "epoch": 1.400962309542903,
      "grad_norm": 1.7577170133590698,
      "learning_rate": 1.6038059501474136e-05,
      "loss": 0.6039,
      "step": 17470
    },
    {
      "epoch": 1.4017642341619887,
      "grad_norm": 1.8455114364624023,
      "learning_rate": 1.603001876172608e-05,
      "loss": 0.6441,
      "step": 17480
    },
    {
      "epoch": 1.4025661587810747,
      "grad_norm": 1.5244224071502686,
      "learning_rate": 1.602197802197802e-05,
      "loss": 0.6215,
      "step": 17490
    },
    {
      "epoch": 1.4033680834001605,
      "grad_norm": 1.5575602054595947,
      "learning_rate": 1.6013937282229966e-05,
      "loss": 0.5883,
      "step": 17500
    },
    {
      "epoch": 1.4041700080192463,
      "grad_norm": 1.777387261390686,
      "learning_rate": 1.600589654248191e-05,
      "loss": 0.6855,
      "step": 17510
    },
    {
      "epoch": 1.404971932638332,
      "grad_norm": 1.941112756729126,
      "learning_rate": 1.5997855802733854e-05,
      "loss": 0.6341,
      "step": 17520
    },
    {
      "epoch": 1.4057738572574179,
      "grad_norm": 1.8661367893218994,
      "learning_rate": 1.5989815062985795e-05,
      "loss": 0.5822,
      "step": 17530
    },
    {
      "epoch": 1.4065757818765037,
      "grad_norm": 1.9722185134887695,
      "learning_rate": 1.598177432323774e-05,
      "loss": 0.5852,
      "step": 17540
    },
    {
      "epoch": 1.4073777064955895,
      "grad_norm": 1.4420570135116577,
      "learning_rate": 1.597373358348968e-05,
      "loss": 0.6638,
      "step": 17550
    },
    {
      "epoch": 1.4081796311146753,
      "grad_norm": 1.86166250705719,
      "learning_rate": 1.5965692843741625e-05,
      "loss": 0.6373,
      "step": 17560
    },
    {
      "epoch": 1.408981555733761,
      "grad_norm": 1.7824838161468506,
      "learning_rate": 1.595765210399357e-05,
      "loss": 0.6461,
      "step": 17570
    },
    {
      "epoch": 1.4097834803528468,
      "grad_norm": 1.869950771331787,
      "learning_rate": 1.594961136424551e-05,
      "loss": 0.574,
      "step": 17580
    },
    {
      "epoch": 1.4105854049719326,
      "grad_norm": 1.7532429695129395,
      "learning_rate": 1.5941570624497454e-05,
      "loss": 0.6787,
      "step": 17590
    },
    {
      "epoch": 1.4113873295910184,
      "grad_norm": 2.236849069595337,
      "learning_rate": 1.5933529884749395e-05,
      "loss": 0.6233,
      "step": 17600
    },
    {
      "epoch": 1.4121892542101042,
      "grad_norm": 1.8079653978347778,
      "learning_rate": 1.5925489145001343e-05,
      "loss": 0.6666,
      "step": 17610
    },
    {
      "epoch": 1.41299117882919,
      "grad_norm": 2.025912284851074,
      "learning_rate": 1.5917448405253284e-05,
      "loss": 0.5817,
      "step": 17620
    },
    {
      "epoch": 1.4137931034482758,
      "grad_norm": 1.758656620979309,
      "learning_rate": 1.5909407665505228e-05,
      "loss": 0.5812,
      "step": 17630
    },
    {
      "epoch": 1.4145950280673616,
      "grad_norm": 1.5593445301055908,
      "learning_rate": 1.590136692575717e-05,
      "loss": 0.593,
      "step": 17640
    },
    {
      "epoch": 1.4153969526864474,
      "grad_norm": 1.6079961061477661,
      "learning_rate": 1.5893326186009113e-05,
      "loss": 0.6147,
      "step": 17650
    },
    {
      "epoch": 1.4161988773055332,
      "grad_norm": 1.868879795074463,
      "learning_rate": 1.5885285446261057e-05,
      "loss": 0.7009,
      "step": 17660
    },
    {
      "epoch": 1.417000801924619,
      "grad_norm": 1.6770720481872559,
      "learning_rate": 1.5877244706513e-05,
      "loss": 0.5431,
      "step": 17670
    },
    {
      "epoch": 1.4178027265437048,
      "grad_norm": 1.5797206163406372,
      "learning_rate": 1.5869203966764943e-05,
      "loss": 0.6581,
      "step": 17680
    },
    {
      "epoch": 1.4186046511627908,
      "grad_norm": 1.9326587915420532,
      "learning_rate": 1.5861163227016883e-05,
      "loss": 0.5779,
      "step": 17690
    },
    {
      "epoch": 1.4194065757818766,
      "grad_norm": 1.8871369361877441,
      "learning_rate": 1.585312248726883e-05,
      "loss": 0.6473,
      "step": 17700
    },
    {
      "epoch": 1.4202085004009624,
      "grad_norm": 1.5505518913269043,
      "learning_rate": 1.5845081747520772e-05,
      "loss": 0.6797,
      "step": 17710
    },
    {
      "epoch": 1.4210104250200482,
      "grad_norm": 1.687201738357544,
      "learning_rate": 1.5837041007772716e-05,
      "loss": 0.6162,
      "step": 17720
    },
    {
      "epoch": 1.421812349639134,
      "grad_norm": 1.6984764337539673,
      "learning_rate": 1.5829000268024657e-05,
      "loss": 0.6699,
      "step": 17730
    },
    {
      "epoch": 1.4226142742582197,
      "grad_norm": 1.9258259534835815,
      "learning_rate": 1.58209595282766e-05,
      "loss": 0.6717,
      "step": 17740
    },
    {
      "epoch": 1.4234161988773055,
      "grad_norm": 1.8625749349594116,
      "learning_rate": 1.5812918788528546e-05,
      "loss": 0.6901,
      "step": 17750
    },
    {
      "epoch": 1.4242181234963913,
      "grad_norm": 1.5796797275543213,
      "learning_rate": 1.580487804878049e-05,
      "loss": 0.654,
      "step": 17760
    },
    {
      "epoch": 1.4250200481154771,
      "grad_norm": 1.5265341997146606,
      "learning_rate": 1.579683730903243e-05,
      "loss": 0.5858,
      "step": 17770
    },
    {
      "epoch": 1.425821972734563,
      "grad_norm": 2.2651162147521973,
      "learning_rate": 1.5788796569284375e-05,
      "loss": 0.6479,
      "step": 17780
    },
    {
      "epoch": 1.4266238973536487,
      "grad_norm": 1.7728886604309082,
      "learning_rate": 1.5780755829536316e-05,
      "loss": 0.5903,
      "step": 17790
    },
    {
      "epoch": 1.4274258219727345,
      "grad_norm": 1.8574507236480713,
      "learning_rate": 1.577271508978826e-05,
      "loss": 0.6491,
      "step": 17800
    },
    {
      "epoch": 1.4282277465918205,
      "grad_norm": 1.6521204710006714,
      "learning_rate": 1.5764674350040205e-05,
      "loss": 0.5582,
      "step": 17810
    },
    {
      "epoch": 1.4290296712109063,
      "grad_norm": 1.7962416410446167,
      "learning_rate": 1.5756633610292146e-05,
      "loss": 0.6665,
      "step": 17820
    },
    {
      "epoch": 1.429831595829992,
      "grad_norm": 1.7417806386947632,
      "learning_rate": 1.574859287054409e-05,
      "loss": 0.6307,
      "step": 17830
    },
    {
      "epoch": 1.4306335204490779,
      "grad_norm": 1.4652734994888306,
      "learning_rate": 1.574055213079603e-05,
      "loss": 0.575,
      "step": 17840
    },
    {
      "epoch": 1.4314354450681637,
      "grad_norm": 1.9486045837402344,
      "learning_rate": 1.573251139104798e-05,
      "loss": 0.6302,
      "step": 17850
    },
    {
      "epoch": 1.4322373696872495,
      "grad_norm": 1.8456873893737793,
      "learning_rate": 1.572447065129992e-05,
      "loss": 0.6107,
      "step": 17860
    },
    {
      "epoch": 1.4330392943063353,
      "grad_norm": 2.0225002765655518,
      "learning_rate": 1.5716429911551864e-05,
      "loss": 0.667,
      "step": 17870
    },
    {
      "epoch": 1.433841218925421,
      "grad_norm": 1.680469274520874,
      "learning_rate": 1.5708389171803805e-05,
      "loss": 0.62,
      "step": 17880
    },
    {
      "epoch": 1.4346431435445068,
      "grad_norm": 1.8979542255401611,
      "learning_rate": 1.5700348432055752e-05,
      "loss": 0.6878,
      "step": 17890
    },
    {
      "epoch": 1.4354450681635926,
      "grad_norm": 1.6836442947387695,
      "learning_rate": 1.5692307692307693e-05,
      "loss": 0.6074,
      "step": 17900
    },
    {
      "epoch": 1.4362469927826784,
      "grad_norm": 1.72724449634552,
      "learning_rate": 1.5684266952559638e-05,
      "loss": 0.6556,
      "step": 17910
    },
    {
      "epoch": 1.4370489174017642,
      "grad_norm": 1.4857803583145142,
      "learning_rate": 1.567622621281158e-05,
      "loss": 0.6391,
      "step": 17920
    },
    {
      "epoch": 1.43785084202085,
      "grad_norm": 2.018937110900879,
      "learning_rate": 1.566818547306352e-05,
      "loss": 0.6164,
      "step": 17930
    },
    {
      "epoch": 1.4386527666399358,
      "grad_norm": 1.7032727003097534,
      "learning_rate": 1.5660144733315467e-05,
      "loss": 0.6381,
      "step": 17940
    },
    {
      "epoch": 1.4394546912590216,
      "grad_norm": 1.6125311851501465,
      "learning_rate": 1.5652103993567408e-05,
      "loss": 0.6276,
      "step": 17950
    },
    {
      "epoch": 1.4402566158781074,
      "grad_norm": 1.7130166292190552,
      "learning_rate": 1.5644063253819352e-05,
      "loss": 0.5667,
      "step": 17960
    },
    {
      "epoch": 1.4410585404971932,
      "grad_norm": 1.8761221170425415,
      "learning_rate": 1.5636022514071293e-05,
      "loss": 0.6279,
      "step": 17970
    },
    {
      "epoch": 1.441860465116279,
      "grad_norm": 1.7963405847549438,
      "learning_rate": 1.5627981774323237e-05,
      "loss": 0.624,
      "step": 17980
    },
    {
      "epoch": 1.4426623897353648,
      "grad_norm": 1.6231839656829834,
      "learning_rate": 1.561994103457518e-05,
      "loss": 0.6833,
      "step": 17990
    },
    {
      "epoch": 1.4434643143544506,
      "grad_norm": 2.2447569370269775,
      "learning_rate": 1.5611900294827126e-05,
      "loss": 0.6299,
      "step": 18000
    },
    {
      "epoch": 1.4442662389735366,
      "grad_norm": 2.213068723678589,
      "learning_rate": 1.5603859555079067e-05,
      "loss": 0.6513,
      "step": 18010
    },
    {
      "epoch": 1.4450681635926224,
      "grad_norm": 1.7978253364562988,
      "learning_rate": 1.559581881533101e-05,
      "loss": 0.6254,
      "step": 18020
    },
    {
      "epoch": 1.4458700882117081,
      "grad_norm": 1.7637571096420288,
      "learning_rate": 1.5587778075582952e-05,
      "loss": 0.593,
      "step": 18030
    },
    {
      "epoch": 1.446672012830794,
      "grad_norm": 1.826664924621582,
      "learning_rate": 1.55797373358349e-05,
      "loss": 0.5985,
      "step": 18040
    },
    {
      "epoch": 1.4474739374498797,
      "grad_norm": 1.5888904333114624,
      "learning_rate": 1.557169659608684e-05,
      "loss": 0.5609,
      "step": 18050
    },
    {
      "epoch": 1.4482758620689655,
      "grad_norm": 1.6216449737548828,
      "learning_rate": 1.556365585633878e-05,
      "loss": 0.6265,
      "step": 18060
    },
    {
      "epoch": 1.4490777866880513,
      "grad_norm": 1.8375436067581177,
      "learning_rate": 1.5555615116590726e-05,
      "loss": 0.5942,
      "step": 18070
    },
    {
      "epoch": 1.449879711307137,
      "grad_norm": 1.94721519947052,
      "learning_rate": 1.554757437684267e-05,
      "loss": 0.6422,
      "step": 18080
    },
    {
      "epoch": 1.450681635926223,
      "grad_norm": 1.6361854076385498,
      "learning_rate": 1.5539533637094614e-05,
      "loss": 0.6784,
      "step": 18090
    },
    {
      "epoch": 1.4514835605453087,
      "grad_norm": 1.6797313690185547,
      "learning_rate": 1.5531492897346555e-05,
      "loss": 0.6379,
      "step": 18100
    },
    {
      "epoch": 1.4522854851643945,
      "grad_norm": 1.9884934425354004,
      "learning_rate": 1.55234521575985e-05,
      "loss": 0.6,
      "step": 18110
    },
    {
      "epoch": 1.4530874097834803,
      "grad_norm": 1.7761762142181396,
      "learning_rate": 1.551541141785044e-05,
      "loss": 0.6736,
      "step": 18120
    },
    {
      "epoch": 1.4538893344025663,
      "grad_norm": 1.7752939462661743,
      "learning_rate": 1.5507370678102388e-05,
      "loss": 0.5999,
      "step": 18130
    },
    {
      "epoch": 1.454691259021652,
      "grad_norm": 1.8923702239990234,
      "learning_rate": 1.549932993835433e-05,
      "loss": 0.565,
      "step": 18140
    },
    {
      "epoch": 1.4554931836407379,
      "grad_norm": 1.6813037395477295,
      "learning_rate": 1.5491289198606273e-05,
      "loss": 0.573,
      "step": 18150
    },
    {
      "epoch": 1.4562951082598237,
      "grad_norm": 1.7079696655273438,
      "learning_rate": 1.5483248458858214e-05,
      "loss": 0.5732,
      "step": 18160
    },
    {
      "epoch": 1.4570970328789095,
      "grad_norm": 2.0324337482452393,
      "learning_rate": 1.547520771911016e-05,
      "loss": 0.6748,
      "step": 18170
    },
    {
      "epoch": 1.4578989574979953,
      "grad_norm": 1.5761369466781616,
      "learning_rate": 1.5467166979362103e-05,
      "loss": 0.6348,
      "step": 18180
    },
    {
      "epoch": 1.458700882117081,
      "grad_norm": 1.8914015293121338,
      "learning_rate": 1.5459126239614044e-05,
      "loss": 0.663,
      "step": 18190
    },
    {
      "epoch": 1.4595028067361668,
      "grad_norm": 1.8912192583084106,
      "learning_rate": 1.5451085499865988e-05,
      "loss": 0.6328,
      "step": 18200
    },
    {
      "epoch": 1.4603047313552526,
      "grad_norm": 1.9420709609985352,
      "learning_rate": 1.544304476011793e-05,
      "loss": 0.6375,
      "step": 18210
    },
    {
      "epoch": 1.4611066559743384,
      "grad_norm": 2.0116002559661865,
      "learning_rate": 1.5435004020369873e-05,
      "loss": 0.5901,
      "step": 18220
    },
    {
      "epoch": 1.4619085805934242,
      "grad_norm": 1.6797031164169312,
      "learning_rate": 1.5426963280621818e-05,
      "loss": 0.6358,
      "step": 18230
    },
    {
      "epoch": 1.46271050521251,
      "grad_norm": 2.1541645526885986,
      "learning_rate": 1.5418922540873762e-05,
      "loss": 0.5764,
      "step": 18240
    },
    {
      "epoch": 1.4635124298315958,
      "grad_norm": 1.6944835186004639,
      "learning_rate": 1.5410881801125703e-05,
      "loss": 0.5945,
      "step": 18250
    },
    {
      "epoch": 1.4643143544506816,
      "grad_norm": 1.7109384536743164,
      "learning_rate": 1.5402841061377647e-05,
      "loss": 0.5871,
      "step": 18260
    },
    {
      "epoch": 1.4651162790697674,
      "grad_norm": 2.1672139167785645,
      "learning_rate": 1.539480032162959e-05,
      "loss": 0.6071,
      "step": 18270
    },
    {
      "epoch": 1.4659182036888532,
      "grad_norm": 1.6337554454803467,
      "learning_rate": 1.5386759581881536e-05,
      "loss": 0.6414,
      "step": 18280
    },
    {
      "epoch": 1.466720128307939,
      "grad_norm": 1.3997318744659424,
      "learning_rate": 1.5378718842133476e-05,
      "loss": 0.5995,
      "step": 18290
    },
    {
      "epoch": 1.4675220529270248,
      "grad_norm": 1.7461018562316895,
      "learning_rate": 1.537067810238542e-05,
      "loss": 0.5708,
      "step": 18300
    },
    {
      "epoch": 1.4683239775461105,
      "grad_norm": 2.017918109893799,
      "learning_rate": 1.536263736263736e-05,
      "loss": 0.5585,
      "step": 18310
    },
    {
      "epoch": 1.4691259021651963,
      "grad_norm": 2.006765127182007,
      "learning_rate": 1.5354596622889306e-05,
      "loss": 0.7022,
      "step": 18320
    },
    {
      "epoch": 1.4699278267842824,
      "grad_norm": 1.7572300434112549,
      "learning_rate": 1.534655588314125e-05,
      "loss": 0.5952,
      "step": 18330
    },
    {
      "epoch": 1.4707297514033681,
      "grad_norm": 1.6955372095108032,
      "learning_rate": 1.533851514339319e-05,
      "loss": 0.6488,
      "step": 18340
    },
    {
      "epoch": 1.471531676022454,
      "grad_norm": 1.617516040802002,
      "learning_rate": 1.5330474403645135e-05,
      "loss": 0.6089,
      "step": 18350
    },
    {
      "epoch": 1.4723336006415397,
      "grad_norm": 1.8398841619491577,
      "learning_rate": 1.5322433663897076e-05,
      "loss": 0.6265,
      "step": 18360
    },
    {
      "epoch": 1.4731355252606255,
      "grad_norm": 1.6454192399978638,
      "learning_rate": 1.5314392924149024e-05,
      "loss": 0.6336,
      "step": 18370
    },
    {
      "epoch": 1.4739374498797113,
      "grad_norm": 1.9409061670303345,
      "learning_rate": 1.5306352184400965e-05,
      "loss": 0.6538,
      "step": 18380
    },
    {
      "epoch": 1.474739374498797,
      "grad_norm": 1.462316632270813,
      "learning_rate": 1.529831144465291e-05,
      "loss": 0.6352,
      "step": 18390
    },
    {
      "epoch": 1.475541299117883,
      "grad_norm": 2.0954766273498535,
      "learning_rate": 1.529027070490485e-05,
      "loss": 0.5892,
      "step": 18400
    },
    {
      "epoch": 1.4763432237369687,
      "grad_norm": 1.680786371231079,
      "learning_rate": 1.5282229965156794e-05,
      "loss": 0.5391,
      "step": 18410
    },
    {
      "epoch": 1.4771451483560545,
      "grad_norm": 1.7277042865753174,
      "learning_rate": 1.527418922540874e-05,
      "loss": 0.6468,
      "step": 18420
    },
    {
      "epoch": 1.4779470729751403,
      "grad_norm": 1.8014417886734009,
      "learning_rate": 1.5266148485660683e-05,
      "loss": 0.595,
      "step": 18430
    },
    {
      "epoch": 1.478748997594226,
      "grad_norm": 1.7316880226135254,
      "learning_rate": 1.5258107745912624e-05,
      "loss": 0.6204,
      "step": 18440
    },
    {
      "epoch": 1.479550922213312,
      "grad_norm": 1.8325111865997314,
      "learning_rate": 1.5250067006164567e-05,
      "loss": 0.6441,
      "step": 18450
    },
    {
      "epoch": 1.4803528468323979,
      "grad_norm": 1.5562148094177246,
      "learning_rate": 1.5242026266416512e-05,
      "loss": 0.5613,
      "step": 18460
    },
    {
      "epoch": 1.4811547714514837,
      "grad_norm": 2.028394937515259,
      "learning_rate": 1.5233985526668455e-05,
      "loss": 0.6624,
      "step": 18470
    },
    {
      "epoch": 1.4819566960705695,
      "grad_norm": 1.791490912437439,
      "learning_rate": 1.5225944786920398e-05,
      "loss": 0.6444,
      "step": 18480
    },
    {
      "epoch": 1.4827586206896552,
      "grad_norm": 1.6601872444152832,
      "learning_rate": 1.521790404717234e-05,
      "loss": 0.6607,
      "step": 18490
    },
    {
      "epoch": 1.483560545308741,
      "grad_norm": 1.9691048860549927,
      "learning_rate": 1.5209863307424283e-05,
      "loss": 0.6144,
      "step": 18500
    },
    {
      "epoch": 1.4843624699278268,
      "grad_norm": 1.6933116912841797,
      "learning_rate": 1.5201822567676227e-05,
      "loss": 0.6505,
      "step": 18510
    },
    {
      "epoch": 1.4851643945469126,
      "grad_norm": 1.6713669300079346,
      "learning_rate": 1.519378182792817e-05,
      "loss": 0.544,
      "step": 18520
    },
    {
      "epoch": 1.4859663191659984,
      "grad_norm": 1.889767050743103,
      "learning_rate": 1.5185741088180112e-05,
      "loss": 0.6405,
      "step": 18530
    },
    {
      "epoch": 1.4867682437850842,
      "grad_norm": 1.5565822124481201,
      "learning_rate": 1.5177700348432055e-05,
      "loss": 0.5815,
      "step": 18540
    },
    {
      "epoch": 1.48757016840417,
      "grad_norm": 1.8181791305541992,
      "learning_rate": 1.5169659608683998e-05,
      "loss": 0.658,
      "step": 18550
    },
    {
      "epoch": 1.4883720930232558,
      "grad_norm": 1.8347418308258057,
      "learning_rate": 1.5161618868935944e-05,
      "loss": 0.5949,
      "step": 18560
    },
    {
      "epoch": 1.4891740176423416,
      "grad_norm": 1.703500509262085,
      "learning_rate": 1.5153578129187886e-05,
      "loss": 0.5514,
      "step": 18570
    },
    {
      "epoch": 1.4899759422614274,
      "grad_norm": 1.6158123016357422,
      "learning_rate": 1.5145537389439829e-05,
      "loss": 0.6605,
      "step": 18580
    },
    {
      "epoch": 1.4907778668805132,
      "grad_norm": 1.8241134881973267,
      "learning_rate": 1.5137496649691771e-05,
      "loss": 0.6272,
      "step": 18590
    },
    {
      "epoch": 1.491579791499599,
      "grad_norm": 1.8579407930374146,
      "learning_rate": 1.5129455909943717e-05,
      "loss": 0.6292,
      "step": 18600
    },
    {
      "epoch": 1.4923817161186848,
      "grad_norm": 1.5825515985488892,
      "learning_rate": 1.512141517019566e-05,
      "loss": 0.5824,
      "step": 18610
    },
    {
      "epoch": 1.4931836407377705,
      "grad_norm": 1.687256932258606,
      "learning_rate": 1.5113374430447602e-05,
      "loss": 0.6045,
      "step": 18620
    },
    {
      "epoch": 1.4939855653568563,
      "grad_norm": 1.8861277103424072,
      "learning_rate": 1.5105333690699545e-05,
      "loss": 0.5816,
      "step": 18630
    },
    {
      "epoch": 1.4947874899759421,
      "grad_norm": 1.8244891166687012,
      "learning_rate": 1.5097292950951486e-05,
      "loss": 0.6729,
      "step": 18640
    },
    {
      "epoch": 1.4955894145950281,
      "grad_norm": 2.5511839389801025,
      "learning_rate": 1.5089252211203432e-05,
      "loss": 0.5689,
      "step": 18650
    },
    {
      "epoch": 1.496391339214114,
      "grad_norm": 2.4148571491241455,
      "learning_rate": 1.5081211471455375e-05,
      "loss": 0.6425,
      "step": 18660
    },
    {
      "epoch": 1.4971932638331997,
      "grad_norm": 1.5653928518295288,
      "learning_rate": 1.5073170731707317e-05,
      "loss": 0.593,
      "step": 18670
    },
    {
      "epoch": 1.4979951884522855,
      "grad_norm": 1.6514922380447388,
      "learning_rate": 1.506512999195926e-05,
      "loss": 0.6121,
      "step": 18680
    },
    {
      "epoch": 1.4987971130713713,
      "grad_norm": 1.639042615890503,
      "learning_rate": 1.5057089252211202e-05,
      "loss": 0.5677,
      "step": 18690
    },
    {
      "epoch": 1.499599037690457,
      "grad_norm": 1.8847825527191162,
      "learning_rate": 1.5049048512463148e-05,
      "loss": 0.6602,
      "step": 18700
    },
    {
      "epoch": 1.500400962309543,
      "grad_norm": 1.789297103881836,
      "learning_rate": 1.5041007772715091e-05,
      "loss": 0.6011,
      "step": 18710
    },
    {
      "epoch": 1.5012028869286287,
      "grad_norm": 1.7603042125701904,
      "learning_rate": 1.5032967032967034e-05,
      "loss": 0.5882,
      "step": 18720
    },
    {
      "epoch": 1.5020048115477145,
      "grad_norm": 1.811551570892334,
      "learning_rate": 1.5024926293218976e-05,
      "loss": 0.6162,
      "step": 18730
    },
    {
      "epoch": 1.5028067361668003,
      "grad_norm": 1.6122159957885742,
      "learning_rate": 1.5016885553470919e-05,
      "loss": 0.6224,
      "step": 18740
    },
    {
      "epoch": 1.5036086607858863,
      "grad_norm": 1.9110699892044067,
      "learning_rate": 1.5008844813722865e-05,
      "loss": 0.6121,
      "step": 18750
    },
    {
      "epoch": 1.504410585404972,
      "grad_norm": 1.716500997543335,
      "learning_rate": 1.5000804073974807e-05,
      "loss": 0.5902,
      "step": 18760
    },
    {
      "epoch": 1.5052125100240579,
      "grad_norm": 1.5690929889678955,
      "learning_rate": 1.4992763334226748e-05,
      "loss": 0.5942,
      "step": 18770
    },
    {
      "epoch": 1.5060144346431437,
      "grad_norm": 1.8634278774261475,
      "learning_rate": 1.4984722594478692e-05,
      "loss": 0.674,
      "step": 18780
    },
    {
      "epoch": 1.5068163592622295,
      "grad_norm": 1.8203315734863281,
      "learning_rate": 1.4976681854730635e-05,
      "loss": 0.6279,
      "step": 18790
    },
    {
      "epoch": 1.5076182838813152,
      "grad_norm": 2.0977017879486084,
      "learning_rate": 1.4968641114982578e-05,
      "loss": 0.5701,
      "step": 18800
    },
    {
      "epoch": 1.508420208500401,
      "grad_norm": 1.9720569849014282,
      "learning_rate": 1.4960600375234522e-05,
      "loss": 0.6178,
      "step": 18810
    },
    {
      "epoch": 1.5092221331194868,
      "grad_norm": 1.784419059753418,
      "learning_rate": 1.4952559635486465e-05,
      "loss": 0.6023,
      "step": 18820
    },
    {
      "epoch": 1.5100240577385726,
      "grad_norm": 2.064018487930298,
      "learning_rate": 1.4944518895738409e-05,
      "loss": 0.6212,
      "step": 18830
    },
    {
      "epoch": 1.5108259823576584,
      "grad_norm": 1.7551919221878052,
      "learning_rate": 1.4936478155990351e-05,
      "loss": 0.6277,
      "step": 18840
    },
    {
      "epoch": 1.5116279069767442,
      "grad_norm": 1.96952486038208,
      "learning_rate": 1.4928437416242296e-05,
      "loss": 0.6172,
      "step": 18850
    },
    {
      "epoch": 1.51242983159583,
      "grad_norm": 1.8839391469955444,
      "learning_rate": 1.4920396676494238e-05,
      "loss": 0.5838,
      "step": 18860
    },
    {
      "epoch": 1.5132317562149158,
      "grad_norm": 1.6604034900665283,
      "learning_rate": 1.4912355936746181e-05,
      "loss": 0.5876,
      "step": 18870
    },
    {
      "epoch": 1.5140336808340016,
      "grad_norm": 1.7104929685592651,
      "learning_rate": 1.4904315196998125e-05,
      "loss": 0.6186,
      "step": 18880
    },
    {
      "epoch": 1.5148356054530874,
      "grad_norm": 1.8057377338409424,
      "learning_rate": 1.4896274457250066e-05,
      "loss": 0.6281,
      "step": 18890
    },
    {
      "epoch": 1.5156375300721732,
      "grad_norm": 1.7234119176864624,
      "learning_rate": 1.488823371750201e-05,
      "loss": 0.5697,
      "step": 18900
    },
    {
      "epoch": 1.516439454691259,
      "grad_norm": 1.7871836423873901,
      "learning_rate": 1.4880192977753953e-05,
      "loss": 0.6141,
      "step": 18910
    },
    {
      "epoch": 1.5172413793103448,
      "grad_norm": 1.685081958770752,
      "learning_rate": 1.4872152238005897e-05,
      "loss": 0.5668,
      "step": 18920
    },
    {
      "epoch": 1.5180433039294305,
      "grad_norm": 1.9002395868301392,
      "learning_rate": 1.486411149825784e-05,
      "loss": 0.6007,
      "step": 18930
    },
    {
      "epoch": 1.5188452285485163,
      "grad_norm": 1.7789255380630493,
      "learning_rate": 1.4856070758509783e-05,
      "loss": 0.6731,
      "step": 18940
    },
    {
      "epoch": 1.5196471531676021,
      "grad_norm": 1.8018219470977783,
      "learning_rate": 1.4848030018761727e-05,
      "loss": 0.6215,
      "step": 18950
    },
    {
      "epoch": 1.520449077786688,
      "grad_norm": 1.9888148307800293,
      "learning_rate": 1.483998927901367e-05,
      "loss": 0.6359,
      "step": 18960
    },
    {
      "epoch": 1.5212510024057737,
      "grad_norm": 1.6754834651947021,
      "learning_rate": 1.4831948539265614e-05,
      "loss": 0.5507,
      "step": 18970
    },
    {
      "epoch": 1.5220529270248595,
      "grad_norm": 2.1857330799102783,
      "learning_rate": 1.4823907799517556e-05,
      "loss": 0.6374,
      "step": 18980
    },
    {
      "epoch": 1.5228548516439455,
      "grad_norm": 2.280933141708374,
      "learning_rate": 1.4815867059769499e-05,
      "loss": 0.6609,
      "step": 18990
    },
    {
      "epoch": 1.5236567762630313,
      "grad_norm": 1.9281376600265503,
      "learning_rate": 1.4807826320021443e-05,
      "loss": 0.6364,
      "step": 19000
    },
    {
      "epoch": 1.524458700882117,
      "grad_norm": 1.6693229675292969,
      "learning_rate": 1.4799785580273386e-05,
      "loss": 0.613,
      "step": 19010
    },
    {
      "epoch": 1.525260625501203,
      "grad_norm": 2.1001975536346436,
      "learning_rate": 1.4791744840525328e-05,
      "loss": 0.5959,
      "step": 19020
    },
    {
      "epoch": 1.5260625501202887,
      "grad_norm": 1.8168840408325195,
      "learning_rate": 1.4783704100777271e-05,
      "loss": 0.5431,
      "step": 19030
    },
    {
      "epoch": 1.5268644747393745,
      "grad_norm": 1.814458966255188,
      "learning_rate": 1.4775663361029215e-05,
      "loss": 0.6166,
      "step": 19040
    },
    {
      "epoch": 1.5276663993584603,
      "grad_norm": 1.5427814722061157,
      "learning_rate": 1.4767622621281158e-05,
      "loss": 0.5607,
      "step": 19050
    },
    {
      "epoch": 1.528468323977546,
      "grad_norm": 1.815938949584961,
      "learning_rate": 1.47595818815331e-05,
      "loss": 0.5873,
      "step": 19060
    },
    {
      "epoch": 1.529270248596632,
      "grad_norm": 2.066114902496338,
      "learning_rate": 1.4751541141785045e-05,
      "loss": 0.634,
      "step": 19070
    },
    {
      "epoch": 1.5300721732157179,
      "grad_norm": 1.7389432191848755,
      "learning_rate": 1.4743500402036987e-05,
      "loss": 0.6092,
      "step": 19080
    },
    {
      "epoch": 1.5308740978348037,
      "grad_norm": 1.927320957183838,
      "learning_rate": 1.4735459662288932e-05,
      "loss": 0.7129,
      "step": 19090
    },
    {
      "epoch": 1.5316760224538895,
      "grad_norm": 1.5554848909378052,
      "learning_rate": 1.4727418922540874e-05,
      "loss": 0.5702,
      "step": 19100
    },
    {
      "epoch": 1.5324779470729752,
      "grad_norm": 1.8192845582962036,
      "learning_rate": 1.4719378182792818e-05,
      "loss": 0.5715,
      "step": 19110
    },
    {
      "epoch": 1.533279871692061,
      "grad_norm": 1.7444188594818115,
      "learning_rate": 1.4711337443044761e-05,
      "loss": 0.6418,
      "step": 19120
    },
    {
      "epoch": 1.5340817963111468,
      "grad_norm": 1.7405509948730469,
      "learning_rate": 1.4703296703296704e-05,
      "loss": 0.5704,
      "step": 19130
    },
    {
      "epoch": 1.5348837209302326,
      "grad_norm": 1.8798497915267944,
      "learning_rate": 1.4695255963548648e-05,
      "loss": 0.6188,
      "step": 19140
    },
    {
      "epoch": 1.5356856455493184,
      "grad_norm": 1.8485889434814453,
      "learning_rate": 1.4687215223800589e-05,
      "loss": 0.6816,
      "step": 19150
    },
    {
      "epoch": 1.5364875701684042,
      "grad_norm": 1.7882652282714844,
      "learning_rate": 1.4679174484052533e-05,
      "loss": 0.5901,
      "step": 19160
    },
    {
      "epoch": 1.53728949478749,
      "grad_norm": 2.330425977706909,
      "learning_rate": 1.4671133744304476e-05,
      "loss": 0.581,
      "step": 19170
    },
    {
      "epoch": 1.5380914194065758,
      "grad_norm": 1.9061812162399292,
      "learning_rate": 1.4663093004556418e-05,
      "loss": 0.6374,
      "step": 19180
    },
    {
      "epoch": 1.5388933440256616,
      "grad_norm": 1.7626821994781494,
      "learning_rate": 1.4655052264808363e-05,
      "loss": 0.5965,
      "step": 19190
    },
    {
      "epoch": 1.5396952686447474,
      "grad_norm": 1.6828986406326294,
      "learning_rate": 1.4647011525060305e-05,
      "loss": 0.5396,
      "step": 19200
    },
    {
      "epoch": 1.5404971932638332,
      "grad_norm": 1.9345799684524536,
      "learning_rate": 1.463897078531225e-05,
      "loss": 0.6509,
      "step": 19210
    },
    {
      "epoch": 1.541299117882919,
      "grad_norm": 1.4871690273284912,
      "learning_rate": 1.4630930045564192e-05,
      "loss": 0.6298,
      "step": 19220
    },
    {
      "epoch": 1.5421010425020047,
      "grad_norm": 1.7337946891784668,
      "learning_rate": 1.4622889305816136e-05,
      "loss": 0.6524,
      "step": 19230
    },
    {
      "epoch": 1.5429029671210905,
      "grad_norm": 1.578479290008545,
      "learning_rate": 1.4614848566068079e-05,
      "loss": 0.6508,
      "step": 19240
    },
    {
      "epoch": 1.5437048917401763,
      "grad_norm": 1.8024590015411377,
      "learning_rate": 1.4606807826320022e-05,
      "loss": 0.6107,
      "step": 19250
    },
    {
      "epoch": 1.5445068163592621,
      "grad_norm": 2.6805641651153564,
      "learning_rate": 1.4598767086571966e-05,
      "loss": 0.6552,
      "step": 19260
    },
    {
      "epoch": 1.545308740978348,
      "grad_norm": 1.6259324550628662,
      "learning_rate": 1.4590726346823909e-05,
      "loss": 0.5238,
      "step": 19270
    },
    {
      "epoch": 1.5461106655974337,
      "grad_norm": 1.5568578243255615,
      "learning_rate": 1.4582685607075851e-05,
      "loss": 0.6194,
      "step": 19280
    },
    {
      "epoch": 1.5469125902165195,
      "grad_norm": 1.5920698642730713,
      "learning_rate": 1.4574644867327794e-05,
      "loss": 0.5809,
      "step": 19290
    },
    {
      "epoch": 1.5477145148356053,
      "grad_norm": 1.690671443939209,
      "learning_rate": 1.4566604127579738e-05,
      "loss": 0.6353,
      "step": 19300
    },
    {
      "epoch": 1.5485164394546913,
      "grad_norm": 1.7032124996185303,
      "learning_rate": 1.455856338783168e-05,
      "loss": 0.6872,
      "step": 19310
    },
    {
      "epoch": 1.549318364073777,
      "grad_norm": 1.9128111600875854,
      "learning_rate": 1.4550522648083623e-05,
      "loss": 0.6458,
      "step": 19320
    },
    {
      "epoch": 1.550120288692863,
      "grad_norm": 1.8513861894607544,
      "learning_rate": 1.4542481908335567e-05,
      "loss": 0.6122,
      "step": 19330
    },
    {
      "epoch": 1.5509222133119487,
      "grad_norm": 2.1996705532073975,
      "learning_rate": 1.453444116858751e-05,
      "loss": 0.6562,
      "step": 19340
    },
    {
      "epoch": 1.5517241379310345,
      "grad_norm": 1.8797374963760376,
      "learning_rate": 1.4526400428839454e-05,
      "loss": 0.6452,
      "step": 19350
    },
    {
      "epoch": 1.5525260625501203,
      "grad_norm": 1.792238473892212,
      "learning_rate": 1.4518359689091397e-05,
      "loss": 0.6037,
      "step": 19360
    },
    {
      "epoch": 1.553327987169206,
      "grad_norm": 1.9987703561782837,
      "learning_rate": 1.451031894934334e-05,
      "loss": 0.5983,
      "step": 19370
    },
    {
      "epoch": 1.5541299117882919,
      "grad_norm": 1.6406341791152954,
      "learning_rate": 1.4502278209595284e-05,
      "loss": 0.6291,
      "step": 19380
    },
    {
      "epoch": 1.5549318364073779,
      "grad_norm": 2.048107624053955,
      "learning_rate": 1.4494237469847226e-05,
      "loss": 0.627,
      "step": 19390
    },
    {
      "epoch": 1.5557337610264637,
      "grad_norm": 1.6008787155151367,
      "learning_rate": 1.448619673009917e-05,
      "loss": 0.6573,
      "step": 19400
    },
    {
      "epoch": 1.5565356856455494,
      "grad_norm": 1.9870246648788452,
      "learning_rate": 1.4478155990351112e-05,
      "loss": 0.6042,
      "step": 19410
    },
    {
      "epoch": 1.5573376102646352,
      "grad_norm": 1.8436975479125977,
      "learning_rate": 1.4470115250603056e-05,
      "loss": 0.5379,
      "step": 19420
    },
    {
      "epoch": 1.558139534883721,
      "grad_norm": 2.1498141288757324,
      "learning_rate": 1.4462074510854999e-05,
      "loss": 0.6178,
      "step": 19430
    },
    {
      "epoch": 1.5589414595028068,
      "grad_norm": 1.9221775531768799,
      "learning_rate": 1.4454033771106941e-05,
      "loss": 0.639,
      "step": 19440
    },
    {
      "epoch": 1.5597433841218926,
      "grad_norm": 1.9488626718521118,
      "learning_rate": 1.4445993031358885e-05,
      "loss": 0.582,
      "step": 19450
    },
    {
      "epoch": 1.5605453087409784,
      "grad_norm": 1.809650182723999,
      "learning_rate": 1.4437952291610828e-05,
      "loss": 0.6183,
      "step": 19460
    },
    {
      "epoch": 1.5613472333600642,
      "grad_norm": 1.553636074066162,
      "learning_rate": 1.4429911551862772e-05,
      "loss": 0.6062,
      "step": 19470
    },
    {
      "epoch": 1.56214915797915,
      "grad_norm": 1.9237157106399536,
      "learning_rate": 1.4421870812114715e-05,
      "loss": 0.6636,
      "step": 19480
    },
    {
      "epoch": 1.5629510825982358,
      "grad_norm": 1.8083614110946655,
      "learning_rate": 1.441383007236666e-05,
      "loss": 0.7081,
      "step": 19490
    },
    {
      "epoch": 1.5637530072173216,
      "grad_norm": 1.9901756048202515,
      "learning_rate": 1.4405789332618602e-05,
      "loss": 0.6582,
      "step": 19500
    },
    {
      "epoch": 1.5645549318364074,
      "grad_norm": 1.909067153930664,
      "learning_rate": 1.4397748592870544e-05,
      "loss": 0.5982,
      "step": 19510
    },
    {
      "epoch": 1.5653568564554932,
      "grad_norm": 1.7077118158340454,
      "learning_rate": 1.4389707853122489e-05,
      "loss": 0.6864,
      "step": 19520
    },
    {
      "epoch": 1.566158781074579,
      "grad_norm": 1.8141934871673584,
      "learning_rate": 1.4381667113374431e-05,
      "loss": 0.602,
      "step": 19530
    },
    {
      "epoch": 1.5669607056936647,
      "grad_norm": 2.049177646636963,
      "learning_rate": 1.4373626373626374e-05,
      "loss": 0.5841,
      "step": 19540
    },
    {
      "epoch": 1.5677626303127505,
      "grad_norm": 2.076075315475464,
      "learning_rate": 1.4365585633878316e-05,
      "loss": 0.5897,
      "step": 19550
    },
    {
      "epoch": 1.5685645549318363,
      "grad_norm": 1.8030730485916138,
      "learning_rate": 1.435754489413026e-05,
      "loss": 0.6209,
      "step": 19560
    },
    {
      "epoch": 1.5693664795509221,
      "grad_norm": 1.79572331905365,
      "learning_rate": 1.4349504154382203e-05,
      "loss": 0.6201,
      "step": 19570
    },
    {
      "epoch": 1.570168404170008,
      "grad_norm": 2.1086902618408203,
      "learning_rate": 1.4341463414634146e-05,
      "loss": 0.6423,
      "step": 19580
    },
    {
      "epoch": 1.5709703287890937,
      "grad_norm": 1.997798204421997,
      "learning_rate": 1.433342267488609e-05,
      "loss": 0.6656,
      "step": 19590
    },
    {
      "epoch": 1.5717722534081795,
      "grad_norm": 1.9248394966125488,
      "learning_rate": 1.4325381935138033e-05,
      "loss": 0.5716,
      "step": 19600
    },
    {
      "epoch": 1.5725741780272653,
      "grad_norm": 1.5700478553771973,
      "learning_rate": 1.4317341195389977e-05,
      "loss": 0.5627,
      "step": 19610
    },
    {
      "epoch": 1.573376102646351,
      "grad_norm": 1.9568437337875366,
      "learning_rate": 1.430930045564192e-05,
      "loss": 0.5769,
      "step": 19620
    },
    {
      "epoch": 1.574178027265437,
      "grad_norm": 1.9179623126983643,
      "learning_rate": 1.4301259715893862e-05,
      "loss": 0.5655,
      "step": 19630
    },
    {
      "epoch": 1.5749799518845229,
      "grad_norm": 1.3583736419677734,
      "learning_rate": 1.4293218976145807e-05,
      "loss": 0.66,
      "step": 19640
    },
    {
      "epoch": 1.5757818765036087,
      "grad_norm": 1.8362623453140259,
      "learning_rate": 1.428517823639775e-05,
      "loss": 0.6846,
      "step": 19650
    },
    {
      "epoch": 1.5765838011226945,
      "grad_norm": 2.0095601081848145,
      "learning_rate": 1.4277137496649692e-05,
      "loss": 0.5556,
      "step": 19660
    },
    {
      "epoch": 1.5773857257417803,
      "grad_norm": 1.6087902784347534,
      "learning_rate": 1.4269096756901634e-05,
      "loss": 0.5699,
      "step": 19670
    },
    {
      "epoch": 1.578187650360866,
      "grad_norm": 1.8601713180541992,
      "learning_rate": 1.4261056017153579e-05,
      "loss": 0.6764,
      "step": 19680
    },
    {
      "epoch": 1.5789895749799518,
      "grad_norm": 1.8459662199020386,
      "learning_rate": 1.4253015277405521e-05,
      "loss": 0.6068,
      "step": 19690
    },
    {
      "epoch": 1.5797914995990376,
      "grad_norm": 1.8397852182388306,
      "learning_rate": 1.4244974537657464e-05,
      "loss": 0.7181,
      "step": 19700
    },
    {
      "epoch": 1.5805934242181237,
      "grad_norm": 1.955883502960205,
      "learning_rate": 1.4236933797909408e-05,
      "loss": 0.536,
      "step": 19710
    },
    {
      "epoch": 1.5813953488372094,
      "grad_norm": 1.6033101081848145,
      "learning_rate": 1.422889305816135e-05,
      "loss": 0.6242,
      "step": 19720
    },
    {
      "epoch": 1.5821972734562952,
      "grad_norm": 1.9048467874526978,
      "learning_rate": 1.4220852318413295e-05,
      "loss": 0.576,
      "step": 19730
    },
    {
      "epoch": 1.582999198075381,
      "grad_norm": 1.7447353601455688,
      "learning_rate": 1.4212811578665238e-05,
      "loss": 0.594,
      "step": 19740
    },
    {
      "epoch": 1.5838011226944668,
      "grad_norm": 1.943565011024475,
      "learning_rate": 1.4204770838917182e-05,
      "loss": 0.6174,
      "step": 19750
    },
    {
      "epoch": 1.5846030473135526,
      "grad_norm": 1.651036262512207,
      "learning_rate": 1.4196730099169125e-05,
      "loss": 0.583,
      "step": 19760
    },
    {
      "epoch": 1.5854049719326384,
      "grad_norm": 1.9529756307601929,
      "learning_rate": 1.4188689359421067e-05,
      "loss": 0.6257,
      "step": 19770
    },
    {
      "epoch": 1.5862068965517242,
      "grad_norm": 1.5856887102127075,
      "learning_rate": 1.4180648619673011e-05,
      "loss": 0.6128,
      "step": 19780
    },
    {
      "epoch": 1.58700882117081,
      "grad_norm": 1.8568382263183594,
      "learning_rate": 1.4172607879924952e-05,
      "loss": 0.6228,
      "step": 19790
    },
    {
      "epoch": 1.5878107457898958,
      "grad_norm": 1.688895344734192,
      "learning_rate": 1.4164567140176897e-05,
      "loss": 0.6423,
      "step": 19800
    },
    {
      "epoch": 1.5886126704089816,
      "grad_norm": 1.9121955633163452,
      "learning_rate": 1.415652640042884e-05,
      "loss": 0.5721,
      "step": 19810
    },
    {
      "epoch": 1.5894145950280674,
      "grad_norm": 1.681604027748108,
      "learning_rate": 1.4148485660680782e-05,
      "loss": 0.623,
      "step": 19820
    },
    {
      "epoch": 1.5902165196471532,
      "grad_norm": 1.7113075256347656,
      "learning_rate": 1.4140444920932726e-05,
      "loss": 0.6379,
      "step": 19830
    },
    {
      "epoch": 1.591018444266239,
      "grad_norm": 1.71171236038208,
      "learning_rate": 1.4132404181184669e-05,
      "loss": 0.6603,
      "step": 19840
    },
    {
      "epoch": 1.5918203688853247,
      "grad_norm": 1.5906591415405273,
      "learning_rate": 1.4124363441436613e-05,
      "loss": 0.5976,
      "step": 19850
    },
    {
      "epoch": 1.5926222935044105,
      "grad_norm": 1.669518232345581,
      "learning_rate": 1.4116322701688556e-05,
      "loss": 0.5601,
      "step": 19860
    },
    {
      "epoch": 1.5934242181234963,
      "grad_norm": 2.3713338375091553,
      "learning_rate": 1.41082819619405e-05,
      "loss": 0.6772,
      "step": 19870
    },
    {
      "epoch": 1.5942261427425821,
      "grad_norm": 1.746310830116272,
      "learning_rate": 1.4100241222192442e-05,
      "loss": 0.6754,
      "step": 19880
    },
    {
      "epoch": 1.595028067361668,
      "grad_norm": 1.9006916284561157,
      "learning_rate": 1.4092200482444385e-05,
      "loss": 0.6075,
      "step": 19890
    },
    {
      "epoch": 1.5958299919807537,
      "grad_norm": 1.94176185131073,
      "learning_rate": 1.408415974269633e-05,
      "loss": 0.6828,
      "step": 19900
    },
    {
      "epoch": 1.5966319165998395,
      "grad_norm": 1.5765557289123535,
      "learning_rate": 1.4076119002948272e-05,
      "loss": 0.5668,
      "step": 19910
    },
    {
      "epoch": 1.5974338412189253,
      "grad_norm": 1.753126621246338,
      "learning_rate": 1.4068078263200215e-05,
      "loss": 0.6161,
      "step": 19920
    },
    {
      "epoch": 1.598235765838011,
      "grad_norm": 2.1439411640167236,
      "learning_rate": 1.4060037523452157e-05,
      "loss": 0.6233,
      "step": 19930
    },
    {
      "epoch": 1.5990376904570969,
      "grad_norm": 1.6605702638626099,
      "learning_rate": 1.4051996783704101e-05,
      "loss": 0.6131,
      "step": 19940
    },
    {
      "epoch": 1.5998396150761829,
      "grad_norm": 1.8965919017791748,
      "learning_rate": 1.4043956043956044e-05,
      "loss": 0.6874,
      "step": 19950
    },
    {
      "epoch": 1.6006415396952687,
      "grad_norm": 1.5541402101516724,
      "learning_rate": 1.4035915304207987e-05,
      "loss": 0.6662,
      "step": 19960
    },
    {
      "epoch": 1.6014434643143545,
      "grad_norm": 1.5459107160568237,
      "learning_rate": 1.4027874564459931e-05,
      "loss": 0.7014,
      "step": 19970
    },
    {
      "epoch": 1.6022453889334403,
      "grad_norm": 2.0072293281555176,
      "learning_rate": 1.4019833824711873e-05,
      "loss": 0.6765,
      "step": 19980
    },
    {
      "epoch": 1.603047313552526,
      "grad_norm": 1.8683321475982666,
      "learning_rate": 1.4011793084963818e-05,
      "loss": 0.5943,
      "step": 19990
    },
    {
      "epoch": 1.6038492381716118,
      "grad_norm": 2.0443520545959473,
      "learning_rate": 1.400375234521576e-05,
      "loss": 0.6338,
      "step": 20000
    },
    {
      "epoch": 1.6046511627906976,
      "grad_norm": 1.6691585779190063,
      "learning_rate": 1.3995711605467703e-05,
      "loss": 0.5904,
      "step": 20010
    },
    {
      "epoch": 1.6054530874097834,
      "grad_norm": 1.5617101192474365,
      "learning_rate": 1.3987670865719647e-05,
      "loss": 0.6758,
      "step": 20020
    },
    {
      "epoch": 1.6062550120288694,
      "grad_norm": 1.895246982574463,
      "learning_rate": 1.397963012597159e-05,
      "loss": 0.6038,
      "step": 20030
    },
    {
      "epoch": 1.6070569366479552,
      "grad_norm": 1.6804728507995605,
      "learning_rate": 1.3971589386223534e-05,
      "loss": 0.6,
      "step": 20040
    },
    {
      "epoch": 1.607858861267041,
      "grad_norm": 2.2015624046325684,
      "learning_rate": 1.3963548646475475e-05,
      "loss": 0.6246,
      "step": 20050
    },
    {
      "epoch": 1.6086607858861268,
      "grad_norm": 1.8677701950073242,
      "learning_rate": 1.395550790672742e-05,
      "loss": 0.5859,
      "step": 20060
    },
    {
      "epoch": 1.6094627105052126,
      "grad_norm": 1.797810673713684,
      "learning_rate": 1.3947467166979362e-05,
      "loss": 0.5848,
      "step": 20070
    },
    {
      "epoch": 1.6102646351242984,
      "grad_norm": 1.9247701168060303,
      "learning_rate": 1.3939426427231305e-05,
      "loss": 0.6487,
      "step": 20080
    },
    {
      "epoch": 1.6110665597433842,
      "grad_norm": 1.8350235223770142,
      "learning_rate": 1.3931385687483249e-05,
      "loss": 0.6412,
      "step": 20090
    },
    {
      "epoch": 1.61186848436247,
      "grad_norm": 1.8458776473999023,
      "learning_rate": 1.3923344947735191e-05,
      "loss": 0.5941,
      "step": 20100
    },
    {
      "epoch": 1.6126704089815558,
      "grad_norm": 1.8469852209091187,
      "learning_rate": 1.3915304207987136e-05,
      "loss": 0.6828,
      "step": 20110
    },
    {
      "epoch": 1.6134723336006416,
      "grad_norm": 1.7288823127746582,
      "learning_rate": 1.3907263468239078e-05,
      "loss": 0.5682,
      "step": 20120
    },
    {
      "epoch": 1.6142742582197274,
      "grad_norm": 1.8533368110656738,
      "learning_rate": 1.3899222728491023e-05,
      "loss": 0.5811,
      "step": 20130
    },
    {
      "epoch": 1.6150761828388132,
      "grad_norm": 1.6459146738052368,
      "learning_rate": 1.3891181988742965e-05,
      "loss": 0.5559,
      "step": 20140
    },
    {
      "epoch": 1.615878107457899,
      "grad_norm": 1.9692689180374146,
      "learning_rate": 1.3883141248994908e-05,
      "loss": 0.6207,
      "step": 20150
    },
    {
      "epoch": 1.6166800320769847,
      "grad_norm": 1.8919928073883057,
      "learning_rate": 1.3875100509246852e-05,
      "loss": 0.6109,
      "step": 20160
    },
    {
      "epoch": 1.6174819566960705,
      "grad_norm": 1.9317620992660522,
      "learning_rate": 1.3867059769498795e-05,
      "loss": 0.5637,
      "step": 20170
    },
    {
      "epoch": 1.6182838813151563,
      "grad_norm": 1.9089853763580322,
      "learning_rate": 1.3859019029750737e-05,
      "loss": 0.6524,
      "step": 20180
    },
    {
      "epoch": 1.6190858059342421,
      "grad_norm": 1.4565573930740356,
      "learning_rate": 1.385097829000268e-05,
      "loss": 0.6776,
      "step": 20190
    },
    {
      "epoch": 1.619887730553328,
      "grad_norm": 1.637415885925293,
      "learning_rate": 1.3842937550254624e-05,
      "loss": 0.5715,
      "step": 20200
    },
    {
      "epoch": 1.6206896551724137,
      "grad_norm": 2.306403636932373,
      "learning_rate": 1.3834896810506567e-05,
      "loss": 0.6483,
      "step": 20210
    },
    {
      "epoch": 1.6214915797914995,
      "grad_norm": 2.0872325897216797,
      "learning_rate": 1.382685607075851e-05,
      "loss": 0.6391,
      "step": 20220
    },
    {
      "epoch": 1.6222935044105853,
      "grad_norm": 1.5937025547027588,
      "learning_rate": 1.3818815331010454e-05,
      "loss": 0.6175,
      "step": 20230
    },
    {
      "epoch": 1.623095429029671,
      "grad_norm": 1.6031303405761719,
      "learning_rate": 1.3810774591262396e-05,
      "loss": 0.6394,
      "step": 20240
    },
    {
      "epoch": 1.6238973536487569,
      "grad_norm": 1.6296840906143188,
      "learning_rate": 1.380273385151434e-05,
      "loss": 0.5876,
      "step": 20250
    },
    {
      "epoch": 1.6246992782678427,
      "grad_norm": 1.7520297765731812,
      "learning_rate": 1.3794693111766283e-05,
      "loss": 0.6185,
      "step": 20260
    },
    {
      "epoch": 1.6255012028869287,
      "grad_norm": 1.5637643337249756,
      "learning_rate": 1.3786652372018226e-05,
      "loss": 0.6171,
      "step": 20270
    },
    {
      "epoch": 1.6263031275060145,
      "grad_norm": 1.4469226598739624,
      "learning_rate": 1.377861163227017e-05,
      "loss": 0.5788,
      "step": 20280
    },
    {
      "epoch": 1.6271050521251003,
      "grad_norm": 1.8280482292175293,
      "learning_rate": 1.3770570892522113e-05,
      "loss": 0.6154,
      "step": 20290
    },
    {
      "epoch": 1.627906976744186,
      "grad_norm": 1.9470568895339966,
      "learning_rate": 1.3762530152774055e-05,
      "loss": 0.6088,
      "step": 20300
    },
    {
      "epoch": 1.6287089013632718,
      "grad_norm": 1.7638450860977173,
      "learning_rate": 1.3754489413025998e-05,
      "loss": 0.5153,
      "step": 20310
    },
    {
      "epoch": 1.6295108259823576,
      "grad_norm": 1.4622145891189575,
      "learning_rate": 1.3746448673277942e-05,
      "loss": 0.6355,
      "step": 20320
    },
    {
      "epoch": 1.6303127506014434,
      "grad_norm": 1.8012783527374268,
      "learning_rate": 1.3738407933529885e-05,
      "loss": 0.693,
      "step": 20330
    },
    {
      "epoch": 1.6311146752205292,
      "grad_norm": 2.133415699005127,
      "learning_rate": 1.3730367193781827e-05,
      "loss": 0.6404,
      "step": 20340
    },
    {
      "epoch": 1.6319165998396152,
      "grad_norm": 1.8804837465286255,
      "learning_rate": 1.3722326454033772e-05,
      "loss": 0.6086,
      "step": 20350
    },
    {
      "epoch": 1.632718524458701,
      "grad_norm": 1.948021650314331,
      "learning_rate": 1.3714285714285714e-05,
      "loss": 0.6011,
      "step": 20360
    },
    {
      "epoch": 1.6335204490777868,
      "grad_norm": 1.86703360080719,
      "learning_rate": 1.3706244974537658e-05,
      "loss": 0.6022,
      "step": 20370
    },
    {
      "epoch": 1.6343223736968726,
      "grad_norm": 1.7120373249053955,
      "learning_rate": 1.3698204234789601e-05,
      "loss": 0.642,
      "step": 20380
    },
    {
      "epoch": 1.6351242983159584,
      "grad_norm": 1.9078819751739502,
      "learning_rate": 1.3690163495041545e-05,
      "loss": 0.63,
      "step": 20390
    },
    {
      "epoch": 1.6359262229350442,
      "grad_norm": 1.817366361618042,
      "learning_rate": 1.3682122755293488e-05,
      "loss": 0.5508,
      "step": 20400
    },
    {
      "epoch": 1.63672814755413,
      "grad_norm": 1.6669433116912842,
      "learning_rate": 1.367408201554543e-05,
      "loss": 0.5689,
      "step": 20410
    },
    {
      "epoch": 1.6375300721732158,
      "grad_norm": 1.8274301290512085,
      "learning_rate": 1.3666041275797375e-05,
      "loss": 0.6251,
      "step": 20420
    },
    {
      "epoch": 1.6383319967923016,
      "grad_norm": 1.6342216730117798,
      "learning_rate": 1.3658000536049316e-05,
      "loss": 0.6515,
      "step": 20430
    },
    {
      "epoch": 1.6391339214113874,
      "grad_norm": 1.5489784479141235,
      "learning_rate": 1.364995979630126e-05,
      "loss": 0.6651,
      "step": 20440
    },
    {
      "epoch": 1.6399358460304732,
      "grad_norm": 1.9515011310577393,
      "learning_rate": 1.3641919056553203e-05,
      "loss": 0.6596,
      "step": 20450
    },
    {
      "epoch": 1.640737770649559,
      "grad_norm": 1.6267329454421997,
      "learning_rate": 1.3633878316805145e-05,
      "loss": 0.6881,
      "step": 20460
    },
    {
      "epoch": 1.6415396952686447,
      "grad_norm": 1.6673496961593628,
      "learning_rate": 1.362583757705709e-05,
      "loss": 0.5341,
      "step": 20470
    },
    {
      "epoch": 1.6423416198877305,
      "grad_norm": 1.6043198108673096,
      "learning_rate": 1.3617796837309032e-05,
      "loss": 0.6229,
      "step": 20480
    },
    {
      "epoch": 1.6431435445068163,
      "grad_norm": 2.0953545570373535,
      "learning_rate": 1.3609756097560976e-05,
      "loss": 0.6061,
      "step": 20490
    },
    {
      "epoch": 1.6439454691259021,
      "grad_norm": 1.813044786453247,
      "learning_rate": 1.3601715357812919e-05,
      "loss": 0.6496,
      "step": 20500
    },
    {
      "epoch": 1.644747393744988,
      "grad_norm": 2.0482215881347656,
      "learning_rate": 1.3593674618064863e-05,
      "loss": 0.562,
      "step": 20510
    },
    {
      "epoch": 1.6455493183640737,
      "grad_norm": 1.8332335948944092,
      "learning_rate": 1.3585633878316806e-05,
      "loss": 0.6026,
      "step": 20520
    },
    {
      "epoch": 1.6463512429831595,
      "grad_norm": 1.873741626739502,
      "learning_rate": 1.3577593138568748e-05,
      "loss": 0.6311,
      "step": 20530
    },
    {
      "epoch": 1.6471531676022453,
      "grad_norm": 1.6828757524490356,
      "learning_rate": 1.3569552398820693e-05,
      "loss": 0.6137,
      "step": 20540
    },
    {
      "epoch": 1.647955092221331,
      "grad_norm": 1.8048816919326782,
      "learning_rate": 1.3561511659072635e-05,
      "loss": 0.558,
      "step": 20550
    },
    {
      "epoch": 1.6487570168404169,
      "grad_norm": 1.9561980962753296,
      "learning_rate": 1.3553470919324578e-05,
      "loss": 0.595,
      "step": 20560
    },
    {
      "epoch": 1.6495589414595027,
      "grad_norm": 1.6558949947357178,
      "learning_rate": 1.354543017957652e-05,
      "loss": 0.6366,
      "step": 20570
    },
    {
      "epoch": 1.6503608660785885,
      "grad_norm": 1.4892966747283936,
      "learning_rate": 1.3537389439828465e-05,
      "loss": 0.5686,
      "step": 20580
    },
    {
      "epoch": 1.6511627906976745,
      "grad_norm": 2.234372854232788,
      "learning_rate": 1.3529348700080407e-05,
      "loss": 0.6478,
      "step": 20590
    },
    {
      "epoch": 1.6519647153167603,
      "grad_norm": 2.028038501739502,
      "learning_rate": 1.352130796033235e-05,
      "loss": 0.6184,
      "step": 20600
    },
    {
      "epoch": 1.652766639935846,
      "grad_norm": 1.9908870458602905,
      "learning_rate": 1.3513267220584294e-05,
      "loss": 0.5543,
      "step": 20610
    },
    {
      "epoch": 1.6535685645549318,
      "grad_norm": 1.7065434455871582,
      "learning_rate": 1.3505226480836237e-05,
      "loss": 0.5651,
      "step": 20620
    },
    {
      "epoch": 1.6543704891740176,
      "grad_norm": 2.273810863494873,
      "learning_rate": 1.3497185741088181e-05,
      "loss": 0.7505,
      "step": 20630
    },
    {
      "epoch": 1.6551724137931034,
      "grad_norm": 2.660891056060791,
      "learning_rate": 1.3489145001340124e-05,
      "loss": 0.6459,
      "step": 20640
    },
    {
      "epoch": 1.6559743384121892,
      "grad_norm": 1.8030760288238525,
      "learning_rate": 1.3481104261592066e-05,
      "loss": 0.639,
      "step": 20650
    },
    {
      "epoch": 1.656776263031275,
      "grad_norm": 1.9752527475357056,
      "learning_rate": 1.347306352184401e-05,
      "loss": 0.6224,
      "step": 20660
    },
    {
      "epoch": 1.657578187650361,
      "grad_norm": 1.6554673910140991,
      "learning_rate": 1.3465022782095953e-05,
      "loss": 0.559,
      "step": 20670
    },
    {
      "epoch": 1.6583801122694468,
      "grad_norm": 1.6894198656082153,
      "learning_rate": 1.3456982042347898e-05,
      "loss": 0.5929,
      "step": 20680
    },
    {
      "epoch": 1.6591820368885326,
      "grad_norm": 1.9609957933425903,
      "learning_rate": 1.3448941302599838e-05,
      "loss": 0.6223,
      "step": 20690
    },
    {
      "epoch": 1.6599839615076184,
      "grad_norm": 1.8344775438308716,
      "learning_rate": 1.3440900562851783e-05,
      "loss": 0.6118,
      "step": 20700
    },
    {
      "epoch": 1.6607858861267042,
      "grad_norm": 1.7542439699172974,
      "learning_rate": 1.3432859823103725e-05,
      "loss": 0.5931,
      "step": 20710
    },
    {
      "epoch": 1.66158781074579,
      "grad_norm": 1.7358912229537964,
      "learning_rate": 1.3424819083355668e-05,
      "loss": 0.6347,
      "step": 20720
    },
    {
      "epoch": 1.6623897353648758,
      "grad_norm": 1.6428014039993286,
      "learning_rate": 1.3416778343607612e-05,
      "loss": 0.6384,
      "step": 20730
    },
    {
      "epoch": 1.6631916599839616,
      "grad_norm": 2.148973226547241,
      "learning_rate": 1.3408737603859555e-05,
      "loss": 0.6336,
      "step": 20740
    },
    {
      "epoch": 1.6639935846030474,
      "grad_norm": 1.8740612268447876,
      "learning_rate": 1.3400696864111499e-05,
      "loss": 0.6136,
      "step": 20750
    },
    {
      "epoch": 1.6647955092221332,
      "grad_norm": 1.6515400409698486,
      "learning_rate": 1.3392656124363442e-05,
      "loss": 0.5932,
      "step": 20760
    },
    {
      "epoch": 1.665597433841219,
      "grad_norm": 1.7150510549545288,
      "learning_rate": 1.3384615384615386e-05,
      "loss": 0.6296,
      "step": 20770
    },
    {
      "epoch": 1.6663993584603047,
      "grad_norm": 1.87776780128479,
      "learning_rate": 1.3376574644867329e-05,
      "loss": 0.6516,
      "step": 20780
    },
    {
      "epoch": 1.6672012830793905,
      "grad_norm": 1.8829612731933594,
      "learning_rate": 1.3368533905119271e-05,
      "loss": 0.6476,
      "step": 20790
    },
    {
      "epoch": 1.6680032076984763,
      "grad_norm": 1.8027725219726562,
      "learning_rate": 1.3360493165371215e-05,
      "loss": 0.6195,
      "step": 20800
    },
    {
      "epoch": 1.6688051323175621,
      "grad_norm": 2.345355749130249,
      "learning_rate": 1.3352452425623158e-05,
      "loss": 0.6352,
      "step": 20810
    },
    {
      "epoch": 1.669607056936648,
      "grad_norm": 2.118077516555786,
      "learning_rate": 1.33444116858751e-05,
      "loss": 0.6123,
      "step": 20820
    },
    {
      "epoch": 1.6704089815557337,
      "grad_norm": 1.750555396080017,
      "learning_rate": 1.3336370946127043e-05,
      "loss": 0.5934,
      "step": 20830
    },
    {
      "epoch": 1.6712109061748195,
      "grad_norm": 1.9088057279586792,
      "learning_rate": 1.3328330206378988e-05,
      "loss": 0.6447,
      "step": 20840
    },
    {
      "epoch": 1.6720128307939053,
      "grad_norm": 1.71204674243927,
      "learning_rate": 1.332028946663093e-05,
      "loss": 0.6286,
      "step": 20850
    },
    {
      "epoch": 1.672814755412991,
      "grad_norm": 1.8266853094100952,
      "learning_rate": 1.3312248726882873e-05,
      "loss": 0.6092,
      "step": 20860
    },
    {
      "epoch": 1.6736166800320769,
      "grad_norm": 1.8175865411758423,
      "learning_rate": 1.3304207987134817e-05,
      "loss": 0.5561,
      "step": 20870
    },
    {
      "epoch": 1.6744186046511627,
      "grad_norm": 1.9277023077011108,
      "learning_rate": 1.329616724738676e-05,
      "loss": 0.6704,
      "step": 20880
    },
    {
      "epoch": 1.6752205292702484,
      "grad_norm": 1.5631141662597656,
      "learning_rate": 1.3288126507638704e-05,
      "loss": 0.652,
      "step": 20890
    },
    {
      "epoch": 1.6760224538893342,
      "grad_norm": 1.6562416553497314,
      "learning_rate": 1.3280085767890647e-05,
      "loss": 0.6365,
      "step": 20900
    },
    {
      "epoch": 1.6768243785084203,
      "grad_norm": 1.924482822418213,
      "learning_rate": 1.3272045028142589e-05,
      "loss": 0.6034,
      "step": 20910
    },
    {
      "epoch": 1.677626303127506,
      "grad_norm": 1.7244335412979126,
      "learning_rate": 1.3264004288394533e-05,
      "loss": 0.6187,
      "step": 20920
    },
    {
      "epoch": 1.6784282277465918,
      "grad_norm": 1.8115546703338623,
      "learning_rate": 1.3255963548646476e-05,
      "loss": 0.6569,
      "step": 20930
    },
    {
      "epoch": 1.6792301523656776,
      "grad_norm": 1.913817048072815,
      "learning_rate": 1.324792280889842e-05,
      "loss": 0.6335,
      "step": 20940
    },
    {
      "epoch": 1.6800320769847634,
      "grad_norm": 1.6079394817352295,
      "learning_rate": 1.3239882069150361e-05,
      "loss": 0.6408,
      "step": 20950
    },
    {
      "epoch": 1.6808340016038492,
      "grad_norm": 2.1425724029541016,
      "learning_rate": 1.3231841329402305e-05,
      "loss": 0.6506,
      "step": 20960
    },
    {
      "epoch": 1.681635926222935,
      "grad_norm": 2.038984775543213,
      "learning_rate": 1.3223800589654248e-05,
      "loss": 0.6272,
      "step": 20970
    },
    {
      "epoch": 1.6824378508420208,
      "grad_norm": 1.9453305006027222,
      "learning_rate": 1.321575984990619e-05,
      "loss": 0.6486,
      "step": 20980
    },
    {
      "epoch": 1.6832397754611068,
      "grad_norm": 1.8235455751419067,
      "learning_rate": 1.3207719110158135e-05,
      "loss": 0.6457,
      "step": 20990
    },
    {
      "epoch": 1.6840417000801926,
      "grad_norm": 1.8374934196472168,
      "learning_rate": 1.3199678370410078e-05,
      "loss": 0.6206,
      "step": 21000
    },
    {
      "epoch": 1.6848436246992784,
      "grad_norm": 1.943958044052124,
      "learning_rate": 1.3191637630662022e-05,
      "loss": 0.6168,
      "step": 21010
    },
    {
      "epoch": 1.6856455493183642,
      "grad_norm": 1.7476531267166138,
      "learning_rate": 1.3183596890913964e-05,
      "loss": 0.6437,
      "step": 21020
    },
    {
      "epoch": 1.68644747393745,
      "grad_norm": 2.2672924995422363,
      "learning_rate": 1.3175556151165909e-05,
      "loss": 0.6261,
      "step": 21030
    },
    {
      "epoch": 1.6872493985565358,
      "grad_norm": 2.034266948699951,
      "learning_rate": 1.3167515411417851e-05,
      "loss": 0.5981,
      "step": 21040
    },
    {
      "epoch": 1.6880513231756216,
      "grad_norm": 1.6001187562942505,
      "learning_rate": 1.3159474671669794e-05,
      "loss": 0.6257,
      "step": 21050
    },
    {
      "epoch": 1.6888532477947074,
      "grad_norm": 1.6755958795547485,
      "learning_rate": 1.3151433931921738e-05,
      "loss": 0.6327,
      "step": 21060
    },
    {
      "epoch": 1.6896551724137931,
      "grad_norm": 1.658993124961853,
      "learning_rate": 1.3143393192173679e-05,
      "loss": 0.5874,
      "step": 21070
    },
    {
      "epoch": 1.690457097032879,
      "grad_norm": 1.673509955406189,
      "learning_rate": 1.3135352452425623e-05,
      "loss": 0.5978,
      "step": 21080
    },
    {
      "epoch": 1.6912590216519647,
      "grad_norm": 1.802841067314148,
      "learning_rate": 1.3127311712677566e-05,
      "loss": 0.6532,
      "step": 21090
    },
    {
      "epoch": 1.6920609462710505,
      "grad_norm": 1.967010259628296,
      "learning_rate": 1.3119270972929509e-05,
      "loss": 0.6318,
      "step": 21100
    },
    {
      "epoch": 1.6928628708901363,
      "grad_norm": 1.8841016292572021,
      "learning_rate": 1.3111230233181453e-05,
      "loss": 0.6388,
      "step": 21110
    },
    {
      "epoch": 1.693664795509222,
      "grad_norm": 1.6487846374511719,
      "learning_rate": 1.3103189493433395e-05,
      "loss": 0.5686,
      "step": 21120
    },
    {
      "epoch": 1.694466720128308,
      "grad_norm": 1.980215311050415,
      "learning_rate": 1.309514875368534e-05,
      "loss": 0.6014,
      "step": 21130
    },
    {
      "epoch": 1.6952686447473937,
      "grad_norm": 2.123262405395508,
      "learning_rate": 1.3087108013937282e-05,
      "loss": 0.6125,
      "step": 21140
    },
    {
      "epoch": 1.6960705693664795,
      "grad_norm": 2.020596981048584,
      "learning_rate": 1.3079067274189227e-05,
      "loss": 0.6318,
      "step": 21150
    },
    {
      "epoch": 1.6968724939855653,
      "grad_norm": 1.884948492050171,
      "learning_rate": 1.307102653444117e-05,
      "loss": 0.6153,
      "step": 21160
    },
    {
      "epoch": 1.697674418604651,
      "grad_norm": 1.84113347530365,
      "learning_rate": 1.3062985794693112e-05,
      "loss": 0.6161,
      "step": 21170
    },
    {
      "epoch": 1.6984763432237369,
      "grad_norm": 1.7135932445526123,
      "learning_rate": 1.3054945054945056e-05,
      "loss": 0.6027,
      "step": 21180
    },
    {
      "epoch": 1.6992782678428227,
      "grad_norm": 1.6409794092178345,
      "learning_rate": 1.3046904315196999e-05,
      "loss": 0.615,
      "step": 21190
    },
    {
      "epoch": 1.7000801924619084,
      "grad_norm": 1.769020438194275,
      "learning_rate": 1.3038863575448941e-05,
      "loss": 0.6856,
      "step": 21200
    },
    {
      "epoch": 1.7008821170809942,
      "grad_norm": 1.6652991771697998,
      "learning_rate": 1.3030822835700884e-05,
      "loss": 0.6391,
      "step": 21210
    },
    {
      "epoch": 1.70168404170008,
      "grad_norm": 1.8918169736862183,
      "learning_rate": 1.3022782095952828e-05,
      "loss": 0.6548,
      "step": 21220
    },
    {
      "epoch": 1.702485966319166,
      "grad_norm": 1.8484928607940674,
      "learning_rate": 1.301474135620477e-05,
      "loss": 0.6653,
      "step": 21230
    },
    {
      "epoch": 1.7032878909382518,
      "grad_norm": 1.7919888496398926,
      "learning_rate": 1.3006700616456713e-05,
      "loss": 0.6096,
      "step": 21240
    },
    {
      "epoch": 1.7040898155573376,
      "grad_norm": 2.0302700996398926,
      "learning_rate": 1.2998659876708658e-05,
      "loss": 0.5903,
      "step": 21250
    },
    {
      "epoch": 1.7048917401764234,
      "grad_norm": 1.6231956481933594,
      "learning_rate": 1.29906191369606e-05,
      "loss": 0.573,
      "step": 21260
    },
    {
      "epoch": 1.7056936647955092,
      "grad_norm": 1.828885793685913,
      "learning_rate": 1.2982578397212545e-05,
      "loss": 0.5949,
      "step": 21270
    },
    {
      "epoch": 1.706495589414595,
      "grad_norm": 1.7254436016082764,
      "learning_rate": 1.2975341731439292e-05,
      "loss": 0.5854,
      "step": 21280
    },
    {
      "epoch": 1.7072975140336808,
      "grad_norm": 1.682106614112854,
      "learning_rate": 1.2967300991691235e-05,
      "loss": 0.5771,
      "step": 21290
    },
    {
      "epoch": 1.7080994386527666,
      "grad_norm": 1.9871211051940918,
      "learning_rate": 1.295926025194318e-05,
      "loss": 0.611,
      "step": 21300
    },
    {
      "epoch": 1.7089013632718526,
      "grad_norm": 1.5507736206054688,
      "learning_rate": 1.2951219512195122e-05,
      "loss": 0.5621,
      "step": 21310
    },
    {
      "epoch": 1.7097032878909384,
      "grad_norm": 1.6142823696136475,
      "learning_rate": 1.2943178772447066e-05,
      "loss": 0.691,
      "step": 21320
    },
    {
      "epoch": 1.7105052125100242,
      "grad_norm": 1.6572413444519043,
      "learning_rate": 1.2935138032699009e-05,
      "loss": 0.5845,
      "step": 21330
    },
    {
      "epoch": 1.71130713712911,
      "grad_norm": 1.756540298461914,
      "learning_rate": 1.2927097292950951e-05,
      "loss": 0.681,
      "step": 21340
    },
    {
      "epoch": 1.7121090617481958,
      "grad_norm": 1.8416351079940796,
      "learning_rate": 1.2919056553202896e-05,
      "loss": 0.5646,
      "step": 21350
    },
    {
      "epoch": 1.7129109863672816,
      "grad_norm": 1.8476098775863647,
      "learning_rate": 1.2911015813454838e-05,
      "loss": 0.7454,
      "step": 21360
    },
    {
      "epoch": 1.7137129109863674,
      "grad_norm": 1.7110155820846558,
      "learning_rate": 1.2902975073706783e-05,
      "loss": 0.5476,
      "step": 21370
    },
    {
      "epoch": 1.7145148356054531,
      "grad_norm": 2.0642035007476807,
      "learning_rate": 1.2894934333958725e-05,
      "loss": 0.5955,
      "step": 21380
    },
    {
      "epoch": 1.715316760224539,
      "grad_norm": 1.8342558145523071,
      "learning_rate": 1.2886893594210668e-05,
      "loss": 0.6341,
      "step": 21390
    },
    {
      "epoch": 1.7161186848436247,
      "grad_norm": 1.8710092306137085,
      "learning_rate": 1.287885285446261e-05,
      "loss": 0.6224,
      "step": 21400
    },
    {
      "epoch": 1.7169206094627105,
      "grad_norm": 2.0277583599090576,
      "learning_rate": 1.2870812114714553e-05,
      "loss": 0.6351,
      "step": 21410
    },
    {
      "epoch": 1.7177225340817963,
      "grad_norm": 1.7628412246704102,
      "learning_rate": 1.2862771374966497e-05,
      "loss": 0.6429,
      "step": 21420
    },
    {
      "epoch": 1.718524458700882,
      "grad_norm": 1.9339911937713623,
      "learning_rate": 1.285473063521844e-05,
      "loss": 0.6454,
      "step": 21430
    },
    {
      "epoch": 1.719326383319968,
      "grad_norm": 1.5943992137908936,
      "learning_rate": 1.2846689895470384e-05,
      "loss": 0.6904,
      "step": 21440
    },
    {
      "epoch": 1.7201283079390537,
      "grad_norm": 1.832985520362854,
      "learning_rate": 1.2838649155722327e-05,
      "loss": 0.6814,
      "step": 21450
    },
    {
      "epoch": 1.7209302325581395,
      "grad_norm": 1.9294129610061646,
      "learning_rate": 1.283060841597427e-05,
      "loss": 0.6192,
      "step": 21460
    },
    {
      "epoch": 1.7217321571772253,
      "grad_norm": 1.9527677297592163,
      "learning_rate": 1.2822567676226214e-05,
      "loss": 0.6519,
      "step": 21470
    },
    {
      "epoch": 1.722534081796311,
      "grad_norm": 1.7728325128555298,
      "learning_rate": 1.2814526936478156e-05,
      "loss": 0.5642,
      "step": 21480
    },
    {
      "epoch": 1.7233360064153969,
      "grad_norm": 1.694561243057251,
      "learning_rate": 1.28064861967301e-05,
      "loss": 0.6046,
      "step": 21490
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 2.0535178184509277,
      "learning_rate": 1.2798445456982043e-05,
      "loss": 0.601,
      "step": 21500
    },
    {
      "epoch": 1.7249398556535684,
      "grad_norm": 1.9183045625686646,
      "learning_rate": 1.2790404717233987e-05,
      "loss": 0.5639,
      "step": 21510
    },
    {
      "epoch": 1.7257417802726542,
      "grad_norm": 1.9122823476791382,
      "learning_rate": 1.2782363977485928e-05,
      "loss": 0.5372,
      "step": 21520
    },
    {
      "epoch": 1.72654370489174,
      "grad_norm": 1.595725655555725,
      "learning_rate": 1.2774323237737871e-05,
      "loss": 0.5935,
      "step": 21530
    },
    {
      "epoch": 1.7273456295108258,
      "grad_norm": 1.6530625820159912,
      "learning_rate": 1.2766282497989815e-05,
      "loss": 0.5594,
      "step": 21540
    },
    {
      "epoch": 1.7281475541299118,
      "grad_norm": 1.7797378301620483,
      "learning_rate": 1.2758241758241758e-05,
      "loss": 0.6171,
      "step": 21550
    },
    {
      "epoch": 1.7289494787489976,
      "grad_norm": 1.9210871458053589,
      "learning_rate": 1.2750201018493702e-05,
      "loss": 0.6434,
      "step": 21560
    },
    {
      "epoch": 1.7297514033680834,
      "grad_norm": 1.8389601707458496,
      "learning_rate": 1.2742160278745645e-05,
      "loss": 0.5627,
      "step": 21570
    },
    {
      "epoch": 1.7305533279871692,
      "grad_norm": 2.1173362731933594,
      "learning_rate": 1.2734119538997587e-05,
      "loss": 0.6742,
      "step": 21580
    },
    {
      "epoch": 1.731355252606255,
      "grad_norm": 1.6601563692092896,
      "learning_rate": 1.2726078799249532e-05,
      "loss": 0.5928,
      "step": 21590
    },
    {
      "epoch": 1.7321571772253408,
      "grad_norm": 1.6981996297836304,
      "learning_rate": 1.2718038059501474e-05,
      "loss": 0.6147,
      "step": 21600
    },
    {
      "epoch": 1.7329591018444266,
      "grad_norm": 1.8198810815811157,
      "learning_rate": 1.2709997319753418e-05,
      "loss": 0.6682,
      "step": 21610
    },
    {
      "epoch": 1.7337610264635124,
      "grad_norm": 1.738183617591858,
      "learning_rate": 1.2701956580005361e-05,
      "loss": 0.5594,
      "step": 21620
    },
    {
      "epoch": 1.7345629510825984,
      "grad_norm": 1.7102290391921997,
      "learning_rate": 1.2693915840257305e-05,
      "loss": 0.6886,
      "step": 21630
    },
    {
      "epoch": 1.7353648757016842,
      "grad_norm": 1.8954005241394043,
      "learning_rate": 1.2685875100509248e-05,
      "loss": 0.6628,
      "step": 21640
    },
    {
      "epoch": 1.73616680032077,
      "grad_norm": 1.665963053703308,
      "learning_rate": 1.2677834360761189e-05,
      "loss": 0.6069,
      "step": 21650
    },
    {
      "epoch": 1.7369687249398558,
      "grad_norm": 1.6295840740203857,
      "learning_rate": 1.2669793621013133e-05,
      "loss": 0.5732,
      "step": 21660
    },
    {
      "epoch": 1.7377706495589416,
      "grad_norm": 1.7912110090255737,
      "learning_rate": 1.2661752881265076e-05,
      "loss": 0.5843,
      "step": 21670
    },
    {
      "epoch": 1.7385725741780274,
      "grad_norm": 1.972636342048645,
      "learning_rate": 1.265371214151702e-05,
      "loss": 0.5707,
      "step": 21680
    },
    {
      "epoch": 1.7393744987971131,
      "grad_norm": 1.9825255870819092,
      "learning_rate": 1.2645671401768963e-05,
      "loss": 0.5789,
      "step": 21690
    },
    {
      "epoch": 1.740176423416199,
      "grad_norm": 1.5641549825668335,
      "learning_rate": 1.2637630662020907e-05,
      "loss": 0.5995,
      "step": 21700
    },
    {
      "epoch": 1.7409783480352847,
      "grad_norm": 1.753685474395752,
      "learning_rate": 1.262958992227285e-05,
      "loss": 0.6646,
      "step": 21710
    },
    {
      "epoch": 1.7417802726543705,
      "grad_norm": 1.8268475532531738,
      "learning_rate": 1.2621549182524792e-05,
      "loss": 0.6292,
      "step": 21720
    },
    {
      "epoch": 1.7425821972734563,
      "grad_norm": 2.104264259338379,
      "learning_rate": 1.2613508442776736e-05,
      "loss": 0.6456,
      "step": 21730
    },
    {
      "epoch": 1.743384121892542,
      "grad_norm": 1.804202914237976,
      "learning_rate": 1.2605467703028679e-05,
      "loss": 0.5832,
      "step": 21740
    },
    {
      "epoch": 1.744186046511628,
      "grad_norm": 1.4947441816329956,
      "learning_rate": 1.2597426963280623e-05,
      "loss": 0.577,
      "step": 21750
    },
    {
      "epoch": 1.7449879711307137,
      "grad_norm": 1.8775300979614258,
      "learning_rate": 1.2590190297507371e-05,
      "loss": 0.5718,
      "step": 21760
    },
    {
      "epoch": 1.7457898957497995,
      "grad_norm": 1.8621121644973755,
      "learning_rate": 1.2582149557759314e-05,
      "loss": 0.6042,
      "step": 21770
    },
    {
      "epoch": 1.7465918203688853,
      "grad_norm": 1.9065572023391724,
      "learning_rate": 1.2574108818011258e-05,
      "loss": 0.6558,
      "step": 21780
    },
    {
      "epoch": 1.747393744987971,
      "grad_norm": 1.7183549404144287,
      "learning_rate": 1.25660680782632e-05,
      "loss": 0.6161,
      "step": 21790
    },
    {
      "epoch": 1.7481956696070569,
      "grad_norm": 1.6538939476013184,
      "learning_rate": 1.2558027338515145e-05,
      "loss": 0.657,
      "step": 21800
    },
    {
      "epoch": 1.7489975942261426,
      "grad_norm": 1.997412085533142,
      "learning_rate": 1.2549986598767087e-05,
      "loss": 0.6263,
      "step": 21810
    },
    {
      "epoch": 1.7497995188452284,
      "grad_norm": 1.5631442070007324,
      "learning_rate": 1.254194585901903e-05,
      "loss": 0.6269,
      "step": 21820
    },
    {
      "epoch": 1.7506014434643142,
      "grad_norm": 1.754533290863037,
      "learning_rate": 1.2533905119270974e-05,
      "loss": 0.6232,
      "step": 21830
    },
    {
      "epoch": 1.7514033680834,
      "grad_norm": 1.532883882522583,
      "learning_rate": 1.2525864379522917e-05,
      "loss": 0.6557,
      "step": 21840
    },
    {
      "epoch": 1.7522052927024858,
      "grad_norm": 1.7804157733917236,
      "learning_rate": 1.251782363977486e-05,
      "loss": 0.6337,
      "step": 21850
    },
    {
      "epoch": 1.7530072173215716,
      "grad_norm": 1.9910825490951538,
      "learning_rate": 1.2509782900026802e-05,
      "loss": 0.5923,
      "step": 21860
    },
    {
      "epoch": 1.7538091419406576,
      "grad_norm": 1.8352957963943481,
      "learning_rate": 1.2501742160278745e-05,
      "loss": 0.6247,
      "step": 21870
    },
    {
      "epoch": 1.7546110665597434,
      "grad_norm": 1.794782280921936,
      "learning_rate": 1.2493701420530689e-05,
      "loss": 0.5765,
      "step": 21880
    },
    {
      "epoch": 1.7554129911788292,
      "grad_norm": 1.6269830465316772,
      "learning_rate": 1.2485660680782632e-05,
      "loss": 0.6035,
      "step": 21890
    },
    {
      "epoch": 1.756214915797915,
      "grad_norm": 1.662943959236145,
      "learning_rate": 1.2477619941034576e-05,
      "loss": 0.5407,
      "step": 21900
    },
    {
      "epoch": 1.7570168404170008,
      "grad_norm": 1.7362172603607178,
      "learning_rate": 1.2469579201286518e-05,
      "loss": 0.5717,
      "step": 21910
    },
    {
      "epoch": 1.7578187650360866,
      "grad_norm": 1.553372859954834,
      "learning_rate": 1.2461538461538463e-05,
      "loss": 0.6576,
      "step": 21920
    },
    {
      "epoch": 1.7586206896551724,
      "grad_norm": 1.701390266418457,
      "learning_rate": 1.2453497721790405e-05,
      "loss": 0.5315,
      "step": 21930
    },
    {
      "epoch": 1.7594226142742582,
      "grad_norm": 1.6950231790542603,
      "learning_rate": 1.2445456982042348e-05,
      "loss": 0.5593,
      "step": 21940
    },
    {
      "epoch": 1.7602245388933442,
      "grad_norm": 2.043086528778076,
      "learning_rate": 1.2437416242294292e-05,
      "loss": 0.6919,
      "step": 21950
    },
    {
      "epoch": 1.76102646351243,
      "grad_norm": 2.1310713291168213,
      "learning_rate": 1.2429375502546235e-05,
      "loss": 0.6008,
      "step": 21960
    },
    {
      "epoch": 1.7618283881315158,
      "grad_norm": 1.7002654075622559,
      "learning_rate": 1.2421334762798177e-05,
      "loss": 0.5895,
      "step": 21970
    },
    {
      "epoch": 1.7626303127506016,
      "grad_norm": 1.9956660270690918,
      "learning_rate": 1.241329402305012e-05,
      "loss": 0.6224,
      "step": 21980
    },
    {
      "epoch": 1.7634322373696873,
      "grad_norm": 1.858399748802185,
      "learning_rate": 1.2405253283302064e-05,
      "loss": 0.5927,
      "step": 21990
    },
    {
      "epoch": 1.7642341619887731,
      "grad_norm": 1.8834121227264404,
      "learning_rate": 1.2397212543554007e-05,
      "loss": 0.6492,
      "step": 22000
    },
    {
      "epoch": 1.765036086607859,
      "grad_norm": 1.5789000988006592,
      "learning_rate": 1.238917180380595e-05,
      "loss": 0.6425,
      "step": 22010
    },
    {
      "epoch": 1.7658380112269447,
      "grad_norm": 1.64198637008667,
      "learning_rate": 1.2381131064057894e-05,
      "loss": 0.6503,
      "step": 22020
    },
    {
      "epoch": 1.7666399358460305,
      "grad_norm": 1.739954948425293,
      "learning_rate": 1.2373090324309836e-05,
      "loss": 0.5953,
      "step": 22030
    },
    {
      "epoch": 1.7674418604651163,
      "grad_norm": 1.8906461000442505,
      "learning_rate": 1.236504958456178e-05,
      "loss": 0.6187,
      "step": 22040
    },
    {
      "epoch": 1.768243785084202,
      "grad_norm": 1.9399107694625854,
      "learning_rate": 1.2357008844813723e-05,
      "loss": 0.6436,
      "step": 22050
    },
    {
      "epoch": 1.769045709703288,
      "grad_norm": 2.028630495071411,
      "learning_rate": 1.2348968105065666e-05,
      "loss": 0.6347,
      "step": 22060
    },
    {
      "epoch": 1.7698476343223737,
      "grad_norm": 1.9330384731292725,
      "learning_rate": 1.234092736531761e-05,
      "loss": 0.6241,
      "step": 22070
    },
    {
      "epoch": 1.7706495589414595,
      "grad_norm": 1.7110767364501953,
      "learning_rate": 1.2332886625569553e-05,
      "loss": 0.6025,
      "step": 22080
    },
    {
      "epoch": 1.7714514835605453,
      "grad_norm": 1.8204889297485352,
      "learning_rate": 1.2324845885821497e-05,
      "loss": 0.6283,
      "step": 22090
    },
    {
      "epoch": 1.772253408179631,
      "grad_norm": 1.6520556211471558,
      "learning_rate": 1.2316805146073438e-05,
      "loss": 0.6334,
      "step": 22100
    },
    {
      "epoch": 1.7730553327987169,
      "grad_norm": 1.8302651643753052,
      "learning_rate": 1.2308764406325382e-05,
      "loss": 0.6186,
      "step": 22110
    },
    {
      "epoch": 1.7738572574178026,
      "grad_norm": 1.9839035272598267,
      "learning_rate": 1.2300723666577325e-05,
      "loss": 0.6363,
      "step": 22120
    },
    {
      "epoch": 1.7746591820368884,
      "grad_norm": 1.7042274475097656,
      "learning_rate": 1.2292682926829267e-05,
      "loss": 0.5399,
      "step": 22130
    },
    {
      "epoch": 1.7754611066559742,
      "grad_norm": 2.069105386734009,
      "learning_rate": 1.2284642187081212e-05,
      "loss": 0.6039,
      "step": 22140
    },
    {
      "epoch": 1.77626303127506,
      "grad_norm": 1.6875230073928833,
      "learning_rate": 1.2276601447333154e-05,
      "loss": 0.6539,
      "step": 22150
    },
    {
      "epoch": 1.7770649558941458,
      "grad_norm": 1.591194987297058,
      "learning_rate": 1.2268560707585099e-05,
      "loss": 0.6101,
      "step": 22160
    },
    {
      "epoch": 1.7778668805132316,
      "grad_norm": 1.7993550300598145,
      "learning_rate": 1.2260519967837041e-05,
      "loss": 0.5995,
      "step": 22170
    },
    {
      "epoch": 1.7786688051323174,
      "grad_norm": 2.039811611175537,
      "learning_rate": 1.2252479228088986e-05,
      "loss": 0.5733,
      "step": 22180
    },
    {
      "epoch": 1.7794707297514034,
      "grad_norm": 1.735222578048706,
      "learning_rate": 1.2244438488340928e-05,
      "loss": 0.6177,
      "step": 22190
    },
    {
      "epoch": 1.7802726543704892,
      "grad_norm": 1.6937335729599,
      "learning_rate": 1.223639774859287e-05,
      "loss": 0.6647,
      "step": 22200
    },
    {
      "epoch": 1.781074578989575,
      "grad_norm": 1.7480058670043945,
      "learning_rate": 1.2228357008844815e-05,
      "loss": 0.5875,
      "step": 22210
    },
    {
      "epoch": 1.7818765036086608,
      "grad_norm": 1.7150017023086548,
      "learning_rate": 1.2220316269096758e-05,
      "loss": 0.5964,
      "step": 22220
    },
    {
      "epoch": 1.7826784282277466,
      "grad_norm": 1.8950896263122559,
      "learning_rate": 1.22122755293487e-05,
      "loss": 0.5963,
      "step": 22230
    },
    {
      "epoch": 1.7834803528468324,
      "grad_norm": 1.6184834241867065,
      "learning_rate": 1.2204234789600643e-05,
      "loss": 0.6128,
      "step": 22240
    },
    {
      "epoch": 1.7842822774659182,
      "grad_norm": 1.9710198640823364,
      "learning_rate": 1.2196194049852587e-05,
      "loss": 0.6783,
      "step": 22250
    },
    {
      "epoch": 1.785084202085004,
      "grad_norm": 1.7551566362380981,
      "learning_rate": 1.218815331010453e-05,
      "loss": 0.6243,
      "step": 22260
    },
    {
      "epoch": 1.78588612670409,
      "grad_norm": 1.5248075723648071,
      "learning_rate": 1.2180112570356472e-05,
      "loss": 0.5972,
      "step": 22270
    },
    {
      "epoch": 1.7866880513231758,
      "grad_norm": 2.0610976219177246,
      "learning_rate": 1.2172071830608417e-05,
      "loss": 0.6558,
      "step": 22280
    },
    {
      "epoch": 1.7874899759422616,
      "grad_norm": 2.154100179672241,
      "learning_rate": 1.2164031090860359e-05,
      "loss": 0.6387,
      "step": 22290
    },
    {
      "epoch": 1.7882919005613473,
      "grad_norm": 1.813170313835144,
      "learning_rate": 1.2155990351112303e-05,
      "loss": 0.6222,
      "step": 22300
    },
    {
      "epoch": 1.7890938251804331,
      "grad_norm": 1.9421547651290894,
      "learning_rate": 1.2147949611364246e-05,
      "loss": 0.5989,
      "step": 22310
    },
    {
      "epoch": 1.789895749799519,
      "grad_norm": 1.528606653213501,
      "learning_rate": 1.2139908871616189e-05,
      "loss": 0.6475,
      "step": 22320
    },
    {
      "epoch": 1.7906976744186047,
      "grad_norm": 2.243860960006714,
      "learning_rate": 1.2131868131868133e-05,
      "loss": 0.6369,
      "step": 22330
    },
    {
      "epoch": 1.7914995990376905,
      "grad_norm": 2.071195363998413,
      "learning_rate": 1.2123827392120076e-05,
      "loss": 0.628,
      "step": 22340
    },
    {
      "epoch": 1.7923015236567763,
      "grad_norm": 1.655824065208435,
      "learning_rate": 1.211578665237202e-05,
      "loss": 0.5819,
      "step": 22350
    },
    {
      "epoch": 1.793103448275862,
      "grad_norm": 2.065993547439575,
      "learning_rate": 1.210774591262396e-05,
      "loss": 0.5983,
      "step": 22360
    },
    {
      "epoch": 1.793905372894948,
      "grad_norm": 1.519183874130249,
      "learning_rate": 1.2099705172875905e-05,
      "loss": 0.5927,
      "step": 22370
    },
    {
      "epoch": 1.7947072975140337,
      "grad_norm": 1.6618432998657227,
      "learning_rate": 1.2091664433127848e-05,
      "loss": 0.5367,
      "step": 22380
    },
    {
      "epoch": 1.7955092221331195,
      "grad_norm": 1.9137861728668213,
      "learning_rate": 1.208362369337979e-05,
      "loss": 0.5939,
      "step": 22390
    },
    {
      "epoch": 1.7963111467522053,
      "grad_norm": 1.837602138519287,
      "learning_rate": 1.2075582953631734e-05,
      "loss": 0.7045,
      "step": 22400
    },
    {
      "epoch": 1.797113071371291,
      "grad_norm": 1.5790268182754517,
      "learning_rate": 1.2067542213883677e-05,
      "loss": 0.5867,
      "step": 22410
    },
    {
      "epoch": 1.7979149959903769,
      "grad_norm": 1.531327247619629,
      "learning_rate": 1.2059501474135621e-05,
      "loss": 0.5652,
      "step": 22420
    },
    {
      "epoch": 1.7987169206094626,
      "grad_norm": 1.9677914381027222,
      "learning_rate": 1.2051460734387564e-05,
      "loss": 0.5962,
      "step": 22430
    },
    {
      "epoch": 1.7995188452285484,
      "grad_norm": 2.5704712867736816,
      "learning_rate": 1.2043419994639508e-05,
      "loss": 0.5709,
      "step": 22440
    },
    {
      "epoch": 1.8003207698476342,
      "grad_norm": 1.6521916389465332,
      "learning_rate": 1.2035379254891451e-05,
      "loss": 0.5589,
      "step": 22450
    },
    {
      "epoch": 1.80112269446672,
      "grad_norm": 1.4723783731460571,
      "learning_rate": 1.2027338515143393e-05,
      "loss": 0.6704,
      "step": 22460
    },
    {
      "epoch": 1.8019246190858058,
      "grad_norm": 2.1009206771850586,
      "learning_rate": 1.2019297775395338e-05,
      "loss": 0.632,
      "step": 22470
    },
    {
      "epoch": 1.8027265437048916,
      "grad_norm": 2.149841070175171,
      "learning_rate": 1.201125703564728e-05,
      "loss": 0.6165,
      "step": 22480
    },
    {
      "epoch": 1.8035284683239774,
      "grad_norm": 1.6995190382003784,
      "learning_rate": 1.2003216295899223e-05,
      "loss": 0.6291,
      "step": 22490
    },
    {
      "epoch": 1.8043303929430632,
      "grad_norm": 1.7796847820281982,
      "learning_rate": 1.1995175556151166e-05,
      "loss": 0.5288,
      "step": 22500
    },
    {
      "epoch": 1.8051323175621492,
      "grad_norm": 1.9065877199172974,
      "learning_rate": 1.1987134816403108e-05,
      "loss": 0.5781,
      "step": 22510
    },
    {
      "epoch": 1.805934242181235,
      "grad_norm": 2.0396037101745605,
      "learning_rate": 1.1979094076655052e-05,
      "loss": 0.6129,
      "step": 22520
    },
    {
      "epoch": 1.8067361668003208,
      "grad_norm": 1.8062641620635986,
      "learning_rate": 1.1971053336906995e-05,
      "loss": 0.6009,
      "step": 22530
    },
    {
      "epoch": 1.8075380914194066,
      "grad_norm": 1.725550889968872,
      "learning_rate": 1.196301259715894e-05,
      "loss": 0.646,
      "step": 22540
    },
    {
      "epoch": 1.8083400160384924,
      "grad_norm": 1.4600948095321655,
      "learning_rate": 1.1954971857410882e-05,
      "loss": 0.5381,
      "step": 22550
    },
    {
      "epoch": 1.8091419406575782,
      "grad_norm": 1.9092572927474976,
      "learning_rate": 1.1946931117662826e-05,
      "loss": 0.6757,
      "step": 22560
    },
    {
      "epoch": 1.809943865276664,
      "grad_norm": 1.7495787143707275,
      "learning_rate": 1.1938890377914769e-05,
      "loss": 0.6199,
      "step": 22570
    },
    {
      "epoch": 1.8107457898957497,
      "grad_norm": 1.863503336906433,
      "learning_rate": 1.1930849638166711e-05,
      "loss": 0.7194,
      "step": 22580
    },
    {
      "epoch": 1.8115477145148358,
      "grad_norm": 2.0339388847351074,
      "learning_rate": 1.1922808898418656e-05,
      "loss": 0.5986,
      "step": 22590
    },
    {
      "epoch": 1.8123496391339216,
      "grad_norm": 1.8862725496292114,
      "learning_rate": 1.1914768158670598e-05,
      "loss": 0.6093,
      "step": 22600
    },
    {
      "epoch": 1.8131515637530073,
      "grad_norm": 1.8610409498214722,
      "learning_rate": 1.1906727418922541e-05,
      "loss": 0.6309,
      "step": 22610
    },
    {
      "epoch": 1.8139534883720931,
      "grad_norm": 1.505244255065918,
      "learning_rate": 1.1898686679174483e-05,
      "loss": 0.6317,
      "step": 22620
    },
    {
      "epoch": 1.814755412991179,
      "grad_norm": 2.060736894607544,
      "learning_rate": 1.1890645939426428e-05,
      "loss": 0.6606,
      "step": 22630
    },
    {
      "epoch": 1.8155573376102647,
      "grad_norm": 1.8788889646530151,
      "learning_rate": 1.188260519967837e-05,
      "loss": 0.58,
      "step": 22640
    },
    {
      "epoch": 1.8163592622293505,
      "grad_norm": 1.9643901586532593,
      "learning_rate": 1.1874564459930313e-05,
      "loss": 0.6147,
      "step": 22650
    },
    {
      "epoch": 1.8171611868484363,
      "grad_norm": 1.7717355489730835,
      "learning_rate": 1.1866523720182257e-05,
      "loss": 0.6675,
      "step": 22660
    },
    {
      "epoch": 1.817963111467522,
      "grad_norm": 1.6596345901489258,
      "learning_rate": 1.18584829804342e-05,
      "loss": 0.5681,
      "step": 22670
    },
    {
      "epoch": 1.818765036086608,
      "grad_norm": 1.889686107635498,
      "learning_rate": 1.1850442240686144e-05,
      "loss": 0.5797,
      "step": 22680
    },
    {
      "epoch": 1.8195669607056937,
      "grad_norm": 1.9992870092391968,
      "learning_rate": 1.1842401500938087e-05,
      "loss": 0.6184,
      "step": 22690
    },
    {
      "epoch": 1.8203688853247795,
      "grad_norm": 1.719231128692627,
      "learning_rate": 1.183436076119003e-05,
      "loss": 0.6493,
      "step": 22700
    },
    {
      "epoch": 1.8211708099438653,
      "grad_norm": 2.053715705871582,
      "learning_rate": 1.1826320021441974e-05,
      "loss": 0.6206,
      "step": 22710
    },
    {
      "epoch": 1.821972734562951,
      "grad_norm": 1.9181616306304932,
      "learning_rate": 1.1818279281693916e-05,
      "loss": 0.6234,
      "step": 22720
    },
    {
      "epoch": 1.8227746591820368,
      "grad_norm": 1.6209999322891235,
      "learning_rate": 1.181023854194586e-05,
      "loss": 0.573,
      "step": 22730
    },
    {
      "epoch": 1.8235765838011226,
      "grad_norm": 1.6680541038513184,
      "learning_rate": 1.1802197802197801e-05,
      "loss": 0.6446,
      "step": 22740
    },
    {
      "epoch": 1.8243785084202084,
      "grad_norm": 1.991833209991455,
      "learning_rate": 1.1794157062449746e-05,
      "loss": 0.6216,
      "step": 22750
    },
    {
      "epoch": 1.8251804330392942,
      "grad_norm": 1.8751270771026611,
      "learning_rate": 1.1786116322701688e-05,
      "loss": 0.6051,
      "step": 22760
    },
    {
      "epoch": 1.82598235765838,
      "grad_norm": 2.2655029296875,
      "learning_rate": 1.1778075582953631e-05,
      "loss": 0.6077,
      "step": 22770
    },
    {
      "epoch": 1.8267842822774658,
      "grad_norm": 1.642248272895813,
      "learning_rate": 1.1770034843205575e-05,
      "loss": 0.6468,
      "step": 22780
    },
    {
      "epoch": 1.8275862068965516,
      "grad_norm": 2.080220937728882,
      "learning_rate": 1.1761994103457518e-05,
      "loss": 0.6109,
      "step": 22790
    },
    {
      "epoch": 1.8283881315156374,
      "grad_norm": 1.8134357929229736,
      "learning_rate": 1.1753953363709462e-05,
      "loss": 0.718,
      "step": 22800
    },
    {
      "epoch": 1.8291900561347232,
      "grad_norm": 1.793900489807129,
      "learning_rate": 1.1745912623961405e-05,
      "loss": 0.6257,
      "step": 22810
    },
    {
      "epoch": 1.829991980753809,
      "grad_norm": 1.897613525390625,
      "learning_rate": 1.1737871884213349e-05,
      "loss": 0.6299,
      "step": 22820
    },
    {
      "epoch": 1.830793905372895,
      "grad_norm": 1.692848801612854,
      "learning_rate": 1.1729831144465292e-05,
      "loss": 0.6167,
      "step": 22830
    },
    {
      "epoch": 1.8315958299919808,
      "grad_norm": 1.8027215003967285,
      "learning_rate": 1.1721790404717234e-05,
      "loss": 0.564,
      "step": 22840
    },
    {
      "epoch": 1.8323977546110666,
      "grad_norm": 1.8635269403457642,
      "learning_rate": 1.1713749664969178e-05,
      "loss": 0.6552,
      "step": 22850
    },
    {
      "epoch": 1.8331996792301524,
      "grad_norm": 1.6708600521087646,
      "learning_rate": 1.1705708925221121e-05,
      "loss": 0.5298,
      "step": 22860
    },
    {
      "epoch": 1.8340016038492382,
      "grad_norm": 1.720523715019226,
      "learning_rate": 1.1697668185473064e-05,
      "loss": 0.6864,
      "step": 22870
    },
    {
      "epoch": 1.834803528468324,
      "grad_norm": 1.578709602355957,
      "learning_rate": 1.1689627445725006e-05,
      "loss": 0.652,
      "step": 22880
    },
    {
      "epoch": 1.8356054530874097,
      "grad_norm": 1.5226446390151978,
      "learning_rate": 1.168158670597695e-05,
      "loss": 0.6469,
      "step": 22890
    },
    {
      "epoch": 1.8364073777064955,
      "grad_norm": 1.8739334344863892,
      "learning_rate": 1.1673545966228893e-05,
      "loss": 0.5448,
      "step": 22900
    },
    {
      "epoch": 1.8372093023255816,
      "grad_norm": 2.1649081707000732,
      "learning_rate": 1.1665505226480836e-05,
      "loss": 0.6462,
      "step": 22910
    },
    {
      "epoch": 1.8380112269446673,
      "grad_norm": 1.633776068687439,
      "learning_rate": 1.165746448673278e-05,
      "loss": 0.6173,
      "step": 22920
    },
    {
      "epoch": 1.8388131515637531,
      "grad_norm": 1.9171735048294067,
      "learning_rate": 1.1649423746984723e-05,
      "loss": 0.6271,
      "step": 22930
    },
    {
      "epoch": 1.839615076182839,
      "grad_norm": 2.0331554412841797,
      "learning_rate": 1.1641383007236667e-05,
      "loss": 0.6767,
      "step": 22940
    },
    {
      "epoch": 1.8404170008019247,
      "grad_norm": 1.8493465185165405,
      "learning_rate": 1.163334226748861e-05,
      "loss": 0.6372,
      "step": 22950
    },
    {
      "epoch": 1.8412189254210105,
      "grad_norm": 1.7355408668518066,
      "learning_rate": 1.1625301527740552e-05,
      "loss": 0.6151,
      "step": 22960
    },
    {
      "epoch": 1.8420208500400963,
      "grad_norm": 1.9351065158843994,
      "learning_rate": 1.1617260787992496e-05,
      "loss": 0.6353,
      "step": 22970
    },
    {
      "epoch": 1.842822774659182,
      "grad_norm": 2.541487693786621,
      "learning_rate": 1.1609220048244439e-05,
      "loss": 0.6764,
      "step": 22980
    },
    {
      "epoch": 1.8436246992782679,
      "grad_norm": 1.8170689344406128,
      "learning_rate": 1.1601179308496383e-05,
      "loss": 0.5431,
      "step": 22990
    },
    {
      "epoch": 1.8444266238973537,
      "grad_norm": 1.782904863357544,
      "learning_rate": 1.1593138568748324e-05,
      "loss": 0.543,
      "step": 23000
    },
    {
      "epoch": 1.8452285485164395,
      "grad_norm": 1.6440573930740356,
      "learning_rate": 1.1585097829000268e-05,
      "loss": 0.5687,
      "step": 23010
    },
    {
      "epoch": 1.8460304731355253,
      "grad_norm": 1.8978780508041382,
      "learning_rate": 1.1577057089252211e-05,
      "loss": 0.6828,
      "step": 23020
    },
    {
      "epoch": 1.846832397754611,
      "grad_norm": 2.3118948936462402,
      "learning_rate": 1.1569016349504154e-05,
      "loss": 0.6343,
      "step": 23030
    },
    {
      "epoch": 1.8476343223736968,
      "grad_norm": 1.6404436826705933,
      "learning_rate": 1.1560975609756098e-05,
      "loss": 0.6328,
      "step": 23040
    },
    {
      "epoch": 1.8484362469927826,
      "grad_norm": 2.114220142364502,
      "learning_rate": 1.155293487000804e-05,
      "loss": 0.6477,
      "step": 23050
    },
    {
      "epoch": 1.8492381716118684,
      "grad_norm": 1.9973338842391968,
      "learning_rate": 1.1544894130259985e-05,
      "loss": 0.6674,
      "step": 23060
    },
    {
      "epoch": 1.8500400962309542,
      "grad_norm": 2.031982421875,
      "learning_rate": 1.1536853390511927e-05,
      "loss": 0.6235,
      "step": 23070
    },
    {
      "epoch": 1.85084202085004,
      "grad_norm": 1.6904202699661255,
      "learning_rate": 1.1528812650763872e-05,
      "loss": 0.6484,
      "step": 23080
    },
    {
      "epoch": 1.8516439454691258,
      "grad_norm": 1.7152189016342163,
      "learning_rate": 1.1520771911015814e-05,
      "loss": 0.5865,
      "step": 23090
    },
    {
      "epoch": 1.8524458700882116,
      "grad_norm": 1.6919286251068115,
      "learning_rate": 1.1512731171267757e-05,
      "loss": 0.6219,
      "step": 23100
    },
    {
      "epoch": 1.8532477947072974,
      "grad_norm": 2.0095791816711426,
      "learning_rate": 1.1504690431519701e-05,
      "loss": 0.6205,
      "step": 23110
    },
    {
      "epoch": 1.8540497193263832,
      "grad_norm": 2.2622029781341553,
      "learning_rate": 1.1496649691771644e-05,
      "loss": 0.6144,
      "step": 23120
    },
    {
      "epoch": 1.854851643945469,
      "grad_norm": 1.923101782798767,
      "learning_rate": 1.1488608952023586e-05,
      "loss": 0.5435,
      "step": 23130
    },
    {
      "epoch": 1.8556535685645548,
      "grad_norm": 1.6781742572784424,
      "learning_rate": 1.1480568212275529e-05,
      "loss": 0.6031,
      "step": 23140
    },
    {
      "epoch": 1.8564554931836408,
      "grad_norm": 1.8719218969345093,
      "learning_rate": 1.1472527472527472e-05,
      "loss": 0.5916,
      "step": 23150
    },
    {
      "epoch": 1.8572574178027266,
      "grad_norm": 1.963281512260437,
      "learning_rate": 1.1464486732779416e-05,
      "loss": 0.5851,
      "step": 23160
    },
    {
      "epoch": 1.8580593424218124,
      "grad_norm": 1.6054284572601318,
      "learning_rate": 1.1456445993031358e-05,
      "loss": 0.6173,
      "step": 23170
    },
    {
      "epoch": 1.8588612670408982,
      "grad_norm": 2.278876543045044,
      "learning_rate": 1.1448405253283303e-05,
      "loss": 0.6383,
      "step": 23180
    },
    {
      "epoch": 1.859663191659984,
      "grad_norm": 1.709058165550232,
      "learning_rate": 1.1440364513535245e-05,
      "loss": 0.6389,
      "step": 23190
    },
    {
      "epoch": 1.8604651162790697,
      "grad_norm": 1.6459364891052246,
      "learning_rate": 1.143232377378719e-05,
      "loss": 0.6111,
      "step": 23200
    },
    {
      "epoch": 1.8612670408981555,
      "grad_norm": 1.609650731086731,
      "learning_rate": 1.1424283034039132e-05,
      "loss": 0.6089,
      "step": 23210
    },
    {
      "epoch": 1.8620689655172413,
      "grad_norm": 1.9361789226531982,
      "learning_rate": 1.1416242294291075e-05,
      "loss": 0.661,
      "step": 23220
    },
    {
      "epoch": 1.8628708901363273,
      "grad_norm": 1.843138337135315,
      "learning_rate": 1.1408201554543019e-05,
      "loss": 0.5552,
      "step": 23230
    },
    {
      "epoch": 1.8636728147554131,
      "grad_norm": 1.9270315170288086,
      "learning_rate": 1.1400160814794962e-05,
      "loss": 0.6388,
      "step": 23240
    },
    {
      "epoch": 1.864474739374499,
      "grad_norm": 1.8858819007873535,
      "learning_rate": 1.1392120075046904e-05,
      "loss": 0.6528,
      "step": 23250
    },
    {
      "epoch": 1.8652766639935847,
      "grad_norm": 1.5649290084838867,
      "learning_rate": 1.1384079335298847e-05,
      "loss": 0.6368,
      "step": 23260
    },
    {
      "epoch": 1.8660785886126705,
      "grad_norm": 2.0195109844207764,
      "learning_rate": 1.1376038595550791e-05,
      "loss": 0.6534,
      "step": 23270
    },
    {
      "epoch": 1.8668805132317563,
      "grad_norm": 1.803655982017517,
      "learning_rate": 1.1367997855802734e-05,
      "loss": 0.6458,
      "step": 23280
    },
    {
      "epoch": 1.867682437850842,
      "grad_norm": 1.7050597667694092,
      "learning_rate": 1.1359957116054676e-05,
      "loss": 0.6334,
      "step": 23290
    },
    {
      "epoch": 1.8684843624699279,
      "grad_norm": 1.70803701877594,
      "learning_rate": 1.135191637630662e-05,
      "loss": 0.5838,
      "step": 23300
    },
    {
      "epoch": 1.8692862870890137,
      "grad_norm": 1.8421348333358765,
      "learning_rate": 1.1343875636558563e-05,
      "loss": 0.5612,
      "step": 23310
    },
    {
      "epoch": 1.8700882117080995,
      "grad_norm": 1.749299168586731,
      "learning_rate": 1.1335834896810508e-05,
      "loss": 0.5852,
      "step": 23320
    },
    {
      "epoch": 1.8708901363271853,
      "grad_norm": 1.7642921209335327,
      "learning_rate": 1.132779415706245e-05,
      "loss": 0.6299,
      "step": 23330
    },
    {
      "epoch": 1.871692060946271,
      "grad_norm": 1.9601696729660034,
      "learning_rate": 1.1319753417314393e-05,
      "loss": 0.6441,
      "step": 23340
    },
    {
      "epoch": 1.8724939855653568,
      "grad_norm": 2.097261667251587,
      "learning_rate": 1.1311712677566337e-05,
      "loss": 0.6396,
      "step": 23350
    },
    {
      "epoch": 1.8732959101844426,
      "grad_norm": 1.8772166967391968,
      "learning_rate": 1.130367193781828e-05,
      "loss": 0.5821,
      "step": 23360
    },
    {
      "epoch": 1.8740978348035284,
      "grad_norm": 1.659551978111267,
      "learning_rate": 1.1295631198070224e-05,
      "loss": 0.6169,
      "step": 23370
    },
    {
      "epoch": 1.8748997594226142,
      "grad_norm": 1.865131139755249,
      "learning_rate": 1.1287590458322165e-05,
      "loss": 0.5158,
      "step": 23380
    },
    {
      "epoch": 1.8757016840417,
      "grad_norm": 1.8903272151947021,
      "learning_rate": 1.1279549718574109e-05,
      "loss": 0.6299,
      "step": 23390
    },
    {
      "epoch": 1.8765036086607858,
      "grad_norm": 1.8532445430755615,
      "learning_rate": 1.1271508978826052e-05,
      "loss": 0.6151,
      "step": 23400
    },
    {
      "epoch": 1.8773055332798716,
      "grad_norm": 1.7055598497390747,
      "learning_rate": 1.1263468239077994e-05,
      "loss": 0.6062,
      "step": 23410
    },
    {
      "epoch": 1.8781074578989574,
      "grad_norm": 1.9335664510726929,
      "learning_rate": 1.1255427499329939e-05,
      "loss": 0.6509,
      "step": 23420
    },
    {
      "epoch": 1.8789093825180432,
      "grad_norm": 1.5670491456985474,
      "learning_rate": 1.1247386759581881e-05,
      "loss": 0.5931,
      "step": 23430
    },
    {
      "epoch": 1.879711307137129,
      "grad_norm": 1.9906362295150757,
      "learning_rate": 1.1239346019833825e-05,
      "loss": 0.6712,
      "step": 23440
    },
    {
      "epoch": 1.8805132317562148,
      "grad_norm": 1.9905129671096802,
      "learning_rate": 1.1231305280085768e-05,
      "loss": 0.6074,
      "step": 23450
    },
    {
      "epoch": 1.8813151563753006,
      "grad_norm": 1.8618472814559937,
      "learning_rate": 1.1223264540337712e-05,
      "loss": 0.5916,
      "step": 23460
    },
    {
      "epoch": 1.8821170809943866,
      "grad_norm": 2.4064319133758545,
      "learning_rate": 1.1215223800589655e-05,
      "loss": 0.6208,
      "step": 23470
    },
    {
      "epoch": 1.8829190056134724,
      "grad_norm": 1.7340534925460815,
      "learning_rate": 1.1207183060841598e-05,
      "loss": 0.5827,
      "step": 23480
    },
    {
      "epoch": 1.8837209302325582,
      "grad_norm": 1.5382026433944702,
      "learning_rate": 1.1199142321093542e-05,
      "loss": 0.5694,
      "step": 23490
    },
    {
      "epoch": 1.884522854851644,
      "grad_norm": 1.6385586261749268,
      "learning_rate": 1.1191101581345484e-05,
      "loss": 0.601,
      "step": 23500
    },
    {
      "epoch": 1.8853247794707297,
      "grad_norm": 1.6181683540344238,
      "learning_rate": 1.1183060841597427e-05,
      "loss": 0.5613,
      "step": 23510
    },
    {
      "epoch": 1.8861267040898155,
      "grad_norm": 1.8334054946899414,
      "learning_rate": 1.117502010184937e-05,
      "loss": 0.6315,
      "step": 23520
    },
    {
      "epoch": 1.8869286287089013,
      "grad_norm": 1.9158365726470947,
      "learning_rate": 1.1166979362101314e-05,
      "loss": 0.6449,
      "step": 23530
    },
    {
      "epoch": 1.8877305533279871,
      "grad_norm": 2.1676888465881348,
      "learning_rate": 1.1158938622353256e-05,
      "loss": 0.6569,
      "step": 23540
    },
    {
      "epoch": 1.8885324779470731,
      "grad_norm": 1.7428998947143555,
      "learning_rate": 1.1150897882605199e-05,
      "loss": 0.5748,
      "step": 23550
    },
    {
      "epoch": 1.889334402566159,
      "grad_norm": 1.9096088409423828,
      "learning_rate": 1.1142857142857143e-05,
      "loss": 0.6023,
      "step": 23560
    },
    {
      "epoch": 1.8901363271852447,
      "grad_norm": 1.580085277557373,
      "learning_rate": 1.1134816403109086e-05,
      "loss": 0.6294,
      "step": 23570
    },
    {
      "epoch": 1.8909382518043305,
      "grad_norm": 2.1810810565948486,
      "learning_rate": 1.112677566336103e-05,
      "loss": 0.6689,
      "step": 23580
    },
    {
      "epoch": 1.8917401764234163,
      "grad_norm": 1.9578615427017212,
      "learning_rate": 1.1118734923612973e-05,
      "loss": 0.5907,
      "step": 23590
    },
    {
      "epoch": 1.892542101042502,
      "grad_norm": 2.410917282104492,
      "learning_rate": 1.1110694183864915e-05,
      "loss": 0.5849,
      "step": 23600
    },
    {
      "epoch": 1.8933440256615879,
      "grad_norm": 1.8912737369537354,
      "learning_rate": 1.110265344411686e-05,
      "loss": 0.6337,
      "step": 23610
    },
    {
      "epoch": 1.8941459502806737,
      "grad_norm": 1.8467739820480347,
      "learning_rate": 1.1094612704368802e-05,
      "loss": 0.6236,
      "step": 23620
    },
    {
      "epoch": 1.8949478748997595,
      "grad_norm": 1.8215231895446777,
      "learning_rate": 1.1086571964620747e-05,
      "loss": 0.6438,
      "step": 23630
    },
    {
      "epoch": 1.8957497995188453,
      "grad_norm": 1.8870176076889038,
      "learning_rate": 1.1078531224872688e-05,
      "loss": 0.5806,
      "step": 23640
    },
    {
      "epoch": 1.896551724137931,
      "grad_norm": 1.5100748538970947,
      "learning_rate": 1.1070490485124632e-05,
      "loss": 0.5659,
      "step": 23650
    },
    {
      "epoch": 1.8973536487570168,
      "grad_norm": 1.6904733180999756,
      "learning_rate": 1.1062449745376574e-05,
      "loss": 0.6174,
      "step": 23660
    },
    {
      "epoch": 1.8981555733761026,
      "grad_norm": 1.4664316177368164,
      "learning_rate": 1.1054409005628517e-05,
      "loss": 0.5537,
      "step": 23670
    },
    {
      "epoch": 1.8989574979951884,
      "grad_norm": 1.907200574874878,
      "learning_rate": 1.1046368265880461e-05,
      "loss": 0.6857,
      "step": 23680
    },
    {
      "epoch": 1.8997594226142742,
      "grad_norm": 1.9509299993515015,
      "learning_rate": 1.1038327526132404e-05,
      "loss": 0.6297,
      "step": 23690
    },
    {
      "epoch": 1.90056134723336,
      "grad_norm": 1.761915683746338,
      "learning_rate": 1.1030286786384348e-05,
      "loss": 0.6681,
      "step": 23700
    },
    {
      "epoch": 1.9013632718524458,
      "grad_norm": 1.9438847303390503,
      "learning_rate": 1.102224604663629e-05,
      "loss": 0.6281,
      "step": 23710
    },
    {
      "epoch": 1.9021651964715316,
      "grad_norm": 1.886862874031067,
      "learning_rate": 1.1014205306888235e-05,
      "loss": 0.6861,
      "step": 23720
    },
    {
      "epoch": 1.9029671210906174,
      "grad_norm": 1.8583897352218628,
      "learning_rate": 1.1006164567140178e-05,
      "loss": 0.6056,
      "step": 23730
    },
    {
      "epoch": 1.9037690457097032,
      "grad_norm": 1.726235032081604,
      "learning_rate": 1.099812382739212e-05,
      "loss": 0.5976,
      "step": 23740
    },
    {
      "epoch": 1.904570970328789,
      "grad_norm": 1.8232377767562866,
      "learning_rate": 1.0990083087644065e-05,
      "loss": 0.6679,
      "step": 23750
    },
    {
      "epoch": 1.9053728949478748,
      "grad_norm": 1.616972804069519,
      "learning_rate": 1.0982042347896007e-05,
      "loss": 0.6085,
      "step": 23760
    },
    {
      "epoch": 1.9061748195669606,
      "grad_norm": 2.0768380165100098,
      "learning_rate": 1.097400160814795e-05,
      "loss": 0.5901,
      "step": 23770
    },
    {
      "epoch": 1.9069767441860463,
      "grad_norm": 1.8221395015716553,
      "learning_rate": 1.0965960868399892e-05,
      "loss": 0.671,
      "step": 23780
    },
    {
      "epoch": 1.9077786688051324,
      "grad_norm": 1.7880221605300903,
      "learning_rate": 1.0957920128651835e-05,
      "loss": 0.593,
      "step": 23790
    },
    {
      "epoch": 1.9085805934242182,
      "grad_norm": 1.7622709274291992,
      "learning_rate": 1.094987938890378e-05,
      "loss": 0.6176,
      "step": 23800
    },
    {
      "epoch": 1.909382518043304,
      "grad_norm": 2.1056787967681885,
      "learning_rate": 1.0941838649155722e-05,
      "loss": 0.5761,
      "step": 23810
    },
    {
      "epoch": 1.9101844426623897,
      "grad_norm": 1.8999855518341064,
      "learning_rate": 1.0933797909407666e-05,
      "loss": 0.583,
      "step": 23820
    },
    {
      "epoch": 1.9109863672814755,
      "grad_norm": 1.9046714305877686,
      "learning_rate": 1.0925757169659609e-05,
      "loss": 0.6312,
      "step": 23830
    },
    {
      "epoch": 1.9117882919005613,
      "grad_norm": 1.8126466274261475,
      "learning_rate": 1.0917716429911553e-05,
      "loss": 0.6798,
      "step": 23840
    },
    {
      "epoch": 1.9125902165196471,
      "grad_norm": 1.8201603889465332,
      "learning_rate": 1.0909675690163496e-05,
      "loss": 0.6115,
      "step": 23850
    },
    {
      "epoch": 1.913392141138733,
      "grad_norm": 1.5554205179214478,
      "learning_rate": 1.0901634950415438e-05,
      "loss": 0.6396,
      "step": 23860
    },
    {
      "epoch": 1.914194065757819,
      "grad_norm": 1.9834285974502563,
      "learning_rate": 1.0893594210667382e-05,
      "loss": 0.6146,
      "step": 23870
    },
    {
      "epoch": 1.9149959903769047,
      "grad_norm": 1.8664106130599976,
      "learning_rate": 1.0885553470919325e-05,
      "loss": 0.6293,
      "step": 23880
    },
    {
      "epoch": 1.9157979149959905,
      "grad_norm": 1.8751786947250366,
      "learning_rate": 1.087751273117127e-05,
      "loss": 0.5817,
      "step": 23890
    },
    {
      "epoch": 1.9165998396150763,
      "grad_norm": 2.0241293907165527,
      "learning_rate": 1.086947199142321e-05,
      "loss": 0.5886,
      "step": 23900
    },
    {
      "epoch": 1.917401764234162,
      "grad_norm": 1.8233946561813354,
      "learning_rate": 1.0861431251675155e-05,
      "loss": 0.6758,
      "step": 23910
    },
    {
      "epoch": 1.9182036888532479,
      "grad_norm": 1.6360599994659424,
      "learning_rate": 1.0853390511927097e-05,
      "loss": 0.6379,
      "step": 23920
    },
    {
      "epoch": 1.9190056134723337,
      "grad_norm": 2.0071070194244385,
      "learning_rate": 1.084534977217904e-05,
      "loss": 0.689,
      "step": 23930
    },
    {
      "epoch": 1.9198075380914195,
      "grad_norm": 1.6737966537475586,
      "learning_rate": 1.0837309032430984e-05,
      "loss": 0.5752,
      "step": 23940
    },
    {
      "epoch": 1.9206094627105053,
      "grad_norm": 1.6390554904937744,
      "learning_rate": 1.0829268292682927e-05,
      "loss": 0.5983,
      "step": 23950
    },
    {
      "epoch": 1.921411387329591,
      "grad_norm": 2.06382417678833,
      "learning_rate": 1.0821227552934871e-05,
      "loss": 0.6039,
      "step": 23960
    },
    {
      "epoch": 1.9222133119486768,
      "grad_norm": 1.8147269487380981,
      "learning_rate": 1.0813186813186814e-05,
      "loss": 0.6584,
      "step": 23970
    },
    {
      "epoch": 1.9230152365677626,
      "grad_norm": 1.5954500436782837,
      "learning_rate": 1.0805146073438756e-05,
      "loss": 0.5871,
      "step": 23980
    },
    {
      "epoch": 1.9238171611868484,
      "grad_norm": 2.0553290843963623,
      "learning_rate": 1.07971053336907e-05,
      "loss": 0.6651,
      "step": 23990
    },
    {
      "epoch": 1.9246190858059342,
      "grad_norm": 1.7770591974258423,
      "learning_rate": 1.0789064593942643e-05,
      "loss": 0.6566,
      "step": 24000
    },
    {
      "epoch": 1.92542101042502,
      "grad_norm": 1.6143455505371094,
      "learning_rate": 1.0781023854194587e-05,
      "loss": 0.6043,
      "step": 24010
    },
    {
      "epoch": 1.9262229350441058,
      "grad_norm": 2.102112054824829,
      "learning_rate": 1.077298311444653e-05,
      "loss": 0.6106,
      "step": 24020
    },
    {
      "epoch": 1.9270248596631916,
      "grad_norm": 1.8974899053573608,
      "learning_rate": 1.0764942374698472e-05,
      "loss": 0.7186,
      "step": 24030
    },
    {
      "epoch": 1.9278267842822774,
      "grad_norm": 1.6194705963134766,
      "learning_rate": 1.0756901634950415e-05,
      "loss": 0.5992,
      "step": 24040
    },
    {
      "epoch": 1.9286287089013632,
      "grad_norm": 1.907405138015747,
      "learning_rate": 1.0748860895202358e-05,
      "loss": 0.6528,
      "step": 24050
    },
    {
      "epoch": 1.929430633520449,
      "grad_norm": 1.7240206003189087,
      "learning_rate": 1.0740820155454302e-05,
      "loss": 0.606,
      "step": 24060
    },
    {
      "epoch": 1.9302325581395348,
      "grad_norm": 1.7495232820510864,
      "learning_rate": 1.0732779415706245e-05,
      "loss": 0.575,
      "step": 24070
    },
    {
      "epoch": 1.9310344827586206,
      "grad_norm": 1.8512119054794312,
      "learning_rate": 1.0724738675958189e-05,
      "loss": 0.6057,
      "step": 24080
    },
    {
      "epoch": 1.9318364073777063,
      "grad_norm": 2.065094470977783,
      "learning_rate": 1.0716697936210131e-05,
      "loss": 0.597,
      "step": 24090
    },
    {
      "epoch": 1.9326383319967921,
      "grad_norm": 1.541042447090149,
      "learning_rate": 1.0708657196462076e-05,
      "loss": 0.5573,
      "step": 24100
    },
    {
      "epoch": 1.9334402566158782,
      "grad_norm": 1.6161452531814575,
      "learning_rate": 1.0700616456714018e-05,
      "loss": 0.6115,
      "step": 24110
    },
    {
      "epoch": 1.934242181234964,
      "grad_norm": 1.799229621887207,
      "learning_rate": 1.0692575716965961e-05,
      "loss": 0.5898,
      "step": 24120
    },
    {
      "epoch": 1.9350441058540497,
      "grad_norm": 2.0785305500030518,
      "learning_rate": 1.0684534977217905e-05,
      "loss": 0.6034,
      "step": 24130
    },
    {
      "epoch": 1.9358460304731355,
      "grad_norm": 1.7605924606323242,
      "learning_rate": 1.0676494237469848e-05,
      "loss": 0.6198,
      "step": 24140
    },
    {
      "epoch": 1.9366479550922213,
      "grad_norm": 1.8250904083251953,
      "learning_rate": 1.066845349772179e-05,
      "loss": 0.5784,
      "step": 24150
    },
    {
      "epoch": 1.937449879711307,
      "grad_norm": 1.8822113275527954,
      "learning_rate": 1.0660412757973733e-05,
      "loss": 0.6199,
      "step": 24160
    },
    {
      "epoch": 1.938251804330393,
      "grad_norm": 1.8756765127182007,
      "learning_rate": 1.0652372018225677e-05,
      "loss": 0.5946,
      "step": 24170
    },
    {
      "epoch": 1.9390537289494787,
      "grad_norm": 1.6626758575439453,
      "learning_rate": 1.064433127847762e-05,
      "loss": 0.5327,
      "step": 24180
    },
    {
      "epoch": 1.9398556535685647,
      "grad_norm": 1.7988406419754028,
      "learning_rate": 1.0636290538729563e-05,
      "loss": 0.5669,
      "step": 24190
    },
    {
      "epoch": 1.9406575781876505,
      "grad_norm": 2.251466989517212,
      "learning_rate": 1.0628249798981507e-05,
      "loss": 0.5741,
      "step": 24200
    },
    {
      "epoch": 1.9414595028067363,
      "grad_norm": 1.7251007556915283,
      "learning_rate": 1.062020905923345e-05,
      "loss": 0.6048,
      "step": 24210
    },
    {
      "epoch": 1.942261427425822,
      "grad_norm": 1.5666238069534302,
      "learning_rate": 1.0612168319485394e-05,
      "loss": 0.6001,
      "step": 24220
    },
    {
      "epoch": 1.9430633520449079,
      "grad_norm": 2.0041213035583496,
      "learning_rate": 1.0604127579737336e-05,
      "loss": 0.6021,
      "step": 24230
    },
    {
      "epoch": 1.9438652766639937,
      "grad_norm": 2.1901469230651855,
      "learning_rate": 1.0596086839989279e-05,
      "loss": 0.6329,
      "step": 24240
    },
    {
      "epoch": 1.9446672012830795,
      "grad_norm": 1.7227236032485962,
      "learning_rate": 1.0588046100241223e-05,
      "loss": 0.5815,
      "step": 24250
    },
    {
      "epoch": 1.9454691259021653,
      "grad_norm": 1.6758079528808594,
      "learning_rate": 1.0580005360493166e-05,
      "loss": 0.5823,
      "step": 24260
    },
    {
      "epoch": 1.946271050521251,
      "grad_norm": 1.9195421934127808,
      "learning_rate": 1.057196462074511e-05,
      "loss": 0.5827,
      "step": 24270
    },
    {
      "epoch": 1.9470729751403368,
      "grad_norm": 2.3095149993896484,
      "learning_rate": 1.0563923880997051e-05,
      "loss": 0.6325,
      "step": 24280
    },
    {
      "epoch": 1.9478748997594226,
      "grad_norm": 2.0098254680633545,
      "learning_rate": 1.0555883141248995e-05,
      "loss": 0.6382,
      "step": 24290
    },
    {
      "epoch": 1.9486768243785084,
      "grad_norm": 1.681911587715149,
      "learning_rate": 1.0547842401500938e-05,
      "loss": 0.6052,
      "step": 24300
    },
    {
      "epoch": 1.9494787489975942,
      "grad_norm": 1.8628901243209839,
      "learning_rate": 1.053980166175288e-05,
      "loss": 0.6605,
      "step": 24310
    },
    {
      "epoch": 1.95028067361668,
      "grad_norm": 1.7674576044082642,
      "learning_rate": 1.0531760922004825e-05,
      "loss": 0.5836,
      "step": 24320
    },
    {
      "epoch": 1.9510825982357658,
      "grad_norm": 1.8358557224273682,
      "learning_rate": 1.0523720182256767e-05,
      "loss": 0.6109,
      "step": 24330
    },
    {
      "epoch": 1.9518845228548516,
      "grad_norm": 1.80141019821167,
      "learning_rate": 1.0515679442508712e-05,
      "loss": 0.6406,
      "step": 24340
    },
    {
      "epoch": 1.9526864474739374,
      "grad_norm": 1.73600172996521,
      "learning_rate": 1.0507638702760654e-05,
      "loss": 0.6418,
      "step": 24350
    },
    {
      "epoch": 1.9534883720930232,
      "grad_norm": 1.637955904006958,
      "learning_rate": 1.0499597963012598e-05,
      "loss": 0.6188,
      "step": 24360
    },
    {
      "epoch": 1.954290296712109,
      "grad_norm": 1.8427720069885254,
      "learning_rate": 1.0491557223264541e-05,
      "loss": 0.57,
      "step": 24370
    },
    {
      "epoch": 1.9550922213311948,
      "grad_norm": 1.8932180404663086,
      "learning_rate": 1.0483516483516484e-05,
      "loss": 0.6832,
      "step": 24380
    },
    {
      "epoch": 1.9558941459502805,
      "grad_norm": 1.8744584321975708,
      "learning_rate": 1.0475475743768428e-05,
      "loss": 0.5707,
      "step": 24390
    },
    {
      "epoch": 1.9566960705693663,
      "grad_norm": 1.4234699010849,
      "learning_rate": 1.046743500402037e-05,
      "loss": 0.6038,
      "step": 24400
    },
    {
      "epoch": 1.9574979951884521,
      "grad_norm": 1.7216389179229736,
      "learning_rate": 1.0459394264272313e-05,
      "loss": 0.5626,
      "step": 24410
    },
    {
      "epoch": 1.958299919807538,
      "grad_norm": 1.825763463973999,
      "learning_rate": 1.0451353524524256e-05,
      "loss": 0.6277,
      "step": 24420
    },
    {
      "epoch": 1.959101844426624,
      "grad_norm": 1.7488746643066406,
      "learning_rate": 1.0443312784776198e-05,
      "loss": 0.6343,
      "step": 24430
    },
    {
      "epoch": 1.9599037690457097,
      "grad_norm": 1.6010868549346924,
      "learning_rate": 1.0435272045028143e-05,
      "loss": 0.568,
      "step": 24440
    },
    {
      "epoch": 1.9607056936647955,
      "grad_norm": 1.8856571912765503,
      "learning_rate": 1.0427231305280085e-05,
      "loss": 0.6934,
      "step": 24450
    },
    {
      "epoch": 1.9615076182838813,
      "grad_norm": 1.8142789602279663,
      "learning_rate": 1.041919056553203e-05,
      "loss": 0.643,
      "step": 24460
    },
    {
      "epoch": 1.962309542902967,
      "grad_norm": 1.8058117628097534,
      "learning_rate": 1.0411149825783972e-05,
      "loss": 0.6428,
      "step": 24470
    },
    {
      "epoch": 1.963111467522053,
      "grad_norm": 1.8207080364227295,
      "learning_rate": 1.0403109086035916e-05,
      "loss": 0.5935,
      "step": 24480
    },
    {
      "epoch": 1.9639133921411387,
      "grad_norm": 1.7036054134368896,
      "learning_rate": 1.0395068346287859e-05,
      "loss": 0.6152,
      "step": 24490
    },
    {
      "epoch": 1.9647153167602245,
      "grad_norm": 1.7914403676986694,
      "learning_rate": 1.0387027606539802e-05,
      "loss": 0.6647,
      "step": 24500
    },
    {
      "epoch": 1.9655172413793105,
      "grad_norm": 1.5504040718078613,
      "learning_rate": 1.0378986866791746e-05,
      "loss": 0.5806,
      "step": 24510
    },
    {
      "epoch": 1.9663191659983963,
      "grad_norm": 1.8387234210968018,
      "learning_rate": 1.0370946127043689e-05,
      "loss": 0.5688,
      "step": 24520
    },
    {
      "epoch": 1.967121090617482,
      "grad_norm": 1.7597033977508545,
      "learning_rate": 1.0362905387295633e-05,
      "loss": 0.5768,
      "step": 24530
    },
    {
      "epoch": 1.9679230152365679,
      "grad_norm": 2.104787826538086,
      "learning_rate": 1.0354864647547574e-05,
      "loss": 0.6144,
      "step": 24540
    },
    {
      "epoch": 1.9687249398556537,
      "grad_norm": 1.8712587356567383,
      "learning_rate": 1.0346823907799518e-05,
      "loss": 0.6266,
      "step": 24550
    },
    {
      "epoch": 1.9695268644747395,
      "grad_norm": 2.4391021728515625,
      "learning_rate": 1.033878316805146e-05,
      "loss": 0.6386,
      "step": 24560
    },
    {
      "epoch": 1.9703287890938253,
      "grad_norm": 2.1146092414855957,
      "learning_rate": 1.0330742428303403e-05,
      "loss": 0.6035,
      "step": 24570
    },
    {
      "epoch": 1.971130713712911,
      "grad_norm": 2.027406930923462,
      "learning_rate": 1.0322701688555347e-05,
      "loss": 0.6074,
      "step": 24580
    },
    {
      "epoch": 1.9719326383319968,
      "grad_norm": 1.628345251083374,
      "learning_rate": 1.031466094880729e-05,
      "loss": 0.5923,
      "step": 24590
    },
    {
      "epoch": 1.9727345629510826,
      "grad_norm": 2.3634347915649414,
      "learning_rate": 1.0306620209059234e-05,
      "loss": 0.6359,
      "step": 24600
    },
    {
      "epoch": 1.9735364875701684,
      "grad_norm": 2.1355538368225098,
      "learning_rate": 1.0298579469311177e-05,
      "loss": 0.5932,
      "step": 24610
    },
    {
      "epoch": 1.9743384121892542,
      "grad_norm": 1.7001285552978516,
      "learning_rate": 1.029053872956312e-05,
      "loss": 0.6254,
      "step": 24620
    },
    {
      "epoch": 1.97514033680834,
      "grad_norm": 2.1603362560272217,
      "learning_rate": 1.0282497989815064e-05,
      "loss": 0.6486,
      "step": 24630
    },
    {
      "epoch": 1.9759422614274258,
      "grad_norm": 2.172701597213745,
      "learning_rate": 1.0274457250067006e-05,
      "loss": 0.6298,
      "step": 24640
    },
    {
      "epoch": 1.9767441860465116,
      "grad_norm": 2.035604238510132,
      "learning_rate": 1.026641651031895e-05,
      "loss": 0.6438,
      "step": 24650
    },
    {
      "epoch": 1.9775461106655974,
      "grad_norm": 1.6862013339996338,
      "learning_rate": 1.0258375770570893e-05,
      "loss": 0.6097,
      "step": 24660
    },
    {
      "epoch": 1.9783480352846832,
      "grad_norm": 2.010490894317627,
      "learning_rate": 1.0250335030822836e-05,
      "loss": 0.5513,
      "step": 24670
    },
    {
      "epoch": 1.979149959903769,
      "grad_norm": 1.8697315454483032,
      "learning_rate": 1.0242294291074779e-05,
      "loss": 0.6676,
      "step": 24680
    },
    {
      "epoch": 1.9799518845228548,
      "grad_norm": 1.6262656450271606,
      "learning_rate": 1.0234253551326721e-05,
      "loss": 0.6088,
      "step": 24690
    },
    {
      "epoch": 1.9807538091419405,
      "grad_norm": 1.9475661516189575,
      "learning_rate": 1.0226212811578665e-05,
      "loss": 0.6311,
      "step": 24700
    },
    {
      "epoch": 1.9815557337610263,
      "grad_norm": 2.192349910736084,
      "learning_rate": 1.0218172071830608e-05,
      "loss": 0.6072,
      "step": 24710
    },
    {
      "epoch": 1.9823576583801121,
      "grad_norm": 1.685804843902588,
      "learning_rate": 1.0210131332082552e-05,
      "loss": 0.5452,
      "step": 24720
    },
    {
      "epoch": 1.983159582999198,
      "grad_norm": 1.5660446882247925,
      "learning_rate": 1.0202090592334495e-05,
      "loss": 0.6317,
      "step": 24730
    },
    {
      "epoch": 1.9839615076182837,
      "grad_norm": 1.8145922422409058,
      "learning_rate": 1.019404985258644e-05,
      "loss": 0.7154,
      "step": 24740
    },
    {
      "epoch": 1.9847634322373697,
      "grad_norm": 2.142789125442505,
      "learning_rate": 1.0186009112838382e-05,
      "loss": 0.6812,
      "step": 24750
    },
    {
      "epoch": 1.9855653568564555,
      "grad_norm": 1.797759771347046,
      "learning_rate": 1.0177968373090324e-05,
      "loss": 0.6175,
      "step": 24760
    },
    {
      "epoch": 1.9863672814755413,
      "grad_norm": 1.532357931137085,
      "learning_rate": 1.0169927633342269e-05,
      "loss": 0.6197,
      "step": 24770
    },
    {
      "epoch": 1.987169206094627,
      "grad_norm": 1.7117828130722046,
      "learning_rate": 1.0161886893594211e-05,
      "loss": 0.5566,
      "step": 24780
    },
    {
      "epoch": 1.987971130713713,
      "grad_norm": 1.8635883331298828,
      "learning_rate": 1.0153846153846154e-05,
      "loss": 0.6159,
      "step": 24790
    },
    {
      "epoch": 1.9887730553327987,
      "grad_norm": 1.6814178228378296,
      "learning_rate": 1.0145805414098096e-05,
      "loss": 0.6302,
      "step": 24800
    },
    {
      "epoch": 1.9895749799518845,
      "grad_norm": 2.0547964572906494,
      "learning_rate": 1.013776467435004e-05,
      "loss": 0.5784,
      "step": 24810
    },
    {
      "epoch": 1.9903769045709703,
      "grad_norm": 1.8302732706069946,
      "learning_rate": 1.0129723934601983e-05,
      "loss": 0.6091,
      "step": 24820
    },
    {
      "epoch": 1.9911788291900563,
      "grad_norm": 2.1149237155914307,
      "learning_rate": 1.0121683194853926e-05,
      "loss": 0.6373,
      "step": 24830
    },
    {
      "epoch": 1.991980753809142,
      "grad_norm": 2.0562455654144287,
      "learning_rate": 1.011364245510587e-05,
      "loss": 0.5899,
      "step": 24840
    },
    {
      "epoch": 1.9927826784282279,
      "grad_norm": 1.8163176774978638,
      "learning_rate": 1.0105601715357813e-05,
      "loss": 0.5654,
      "step": 24850
    },
    {
      "epoch": 1.9935846030473137,
      "grad_norm": 1.9164800643920898,
      "learning_rate": 1.0097560975609757e-05,
      "loss": 0.5609,
      "step": 24860
    },
    {
      "epoch": 1.9943865276663995,
      "grad_norm": 2.201969623565674,
      "learning_rate": 1.00895202358617e-05,
      "loss": 0.6374,
      "step": 24870
    },
    {
      "epoch": 1.9951884522854852,
      "grad_norm": 1.9074501991271973,
      "learning_rate": 1.0081479496113642e-05,
      "loss": 0.5941,
      "step": 24880
    },
    {
      "epoch": 1.995990376904571,
      "grad_norm": 1.678184986114502,
      "learning_rate": 1.0073438756365587e-05,
      "loss": 0.5619,
      "step": 24890
    },
    {
      "epoch": 1.9967923015236568,
      "grad_norm": 1.7355103492736816,
      "learning_rate": 1.006539801661753e-05,
      "loss": 0.6311,
      "step": 24900
    },
    {
      "epoch": 1.9975942261427426,
      "grad_norm": 2.160268545150757,
      "learning_rate": 1.0057357276869473e-05,
      "loss": 0.6172,
      "step": 24910
    },
    {
      "epoch": 1.9983961507618284,
      "grad_norm": 1.960184097290039,
      "learning_rate": 1.0049316537121414e-05,
      "loss": 0.6014,
      "step": 24920
    },
    {
      "epoch": 1.9991980753809142,
      "grad_norm": 2.089038610458374,
      "learning_rate": 1.0041275797373359e-05,
      "loss": 0.6406,
      "step": 24930
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.820109248161316,
      "learning_rate": 1.0033235057625301e-05,
      "loss": 0.5779,
      "step": 24940
    },
    {
      "epoch": 2.000801924619086,
      "grad_norm": 2.016017198562622,
      "learning_rate": 1.0025194317877244e-05,
      "loss": 0.5805,
      "step": 24950
    },
    {
      "epoch": 2.0016038492381716,
      "grad_norm": 1.7194123268127441,
      "learning_rate": 1.0017153578129188e-05,
      "loss": 0.6122,
      "step": 24960
    },
    {
      "epoch": 2.0024057738572574,
      "grad_norm": 1.5911695957183838,
      "learning_rate": 1.000911283838113e-05,
      "loss": 0.6641,
      "step": 24970
    },
    {
      "epoch": 2.003207698476343,
      "grad_norm": 1.835953950881958,
      "learning_rate": 1.0001072098633075e-05,
      "loss": 0.6257,
      "step": 24980
    },
    {
      "epoch": 2.004009623095429,
      "grad_norm": 1.7140135765075684,
      "learning_rate": 9.993031358885018e-06,
      "loss": 0.5225,
      "step": 24990
    },
    {
      "epoch": 2.0048115477145148,
      "grad_norm": 2.1212263107299805,
      "learning_rate": 9.984990619136962e-06,
      "loss": 0.6441,
      "step": 25000
    },
    {
      "epoch": 2.0056134723336005,
      "grad_norm": 1.8410495519638062,
      "learning_rate": 9.976949879388905e-06,
      "loss": 0.6123,
      "step": 25010
    },
    {
      "epoch": 2.0064153969526863,
      "grad_norm": 2.0616071224212646,
      "learning_rate": 9.968909139640847e-06,
      "loss": 0.6357,
      "step": 25020
    },
    {
      "epoch": 2.007217321571772,
      "grad_norm": 2.0262327194213867,
      "learning_rate": 9.960868399892791e-06,
      "loss": 0.5764,
      "step": 25030
    },
    {
      "epoch": 2.008019246190858,
      "grad_norm": 1.9695559740066528,
      "learning_rate": 9.952827660144734e-06,
      "loss": 0.6578,
      "step": 25040
    },
    {
      "epoch": 2.0088211708099437,
      "grad_norm": 1.74245285987854,
      "learning_rate": 9.944786920396677e-06,
      "loss": 0.6284,
      "step": 25050
    },
    {
      "epoch": 2.0096230954290295,
      "grad_norm": 1.8635989427566528,
      "learning_rate": 9.93674618064862e-06,
      "loss": 0.6317,
      "step": 25060
    },
    {
      "epoch": 2.0104250200481153,
      "grad_norm": 1.9537127017974854,
      "learning_rate": 9.928705440900562e-06,
      "loss": 0.5935,
      "step": 25070
    },
    {
      "epoch": 2.011226944667201,
      "grad_norm": 1.9672666788101196,
      "learning_rate": 9.920664701152506e-06,
      "loss": 0.6435,
      "step": 25080
    },
    {
      "epoch": 2.012028869286287,
      "grad_norm": 1.5450637340545654,
      "learning_rate": 9.912623961404449e-06,
      "loss": 0.6113,
      "step": 25090
    },
    {
      "epoch": 2.0128307939053727,
      "grad_norm": 2.154810667037964,
      "learning_rate": 9.904583221656393e-06,
      "loss": 0.6476,
      "step": 25100
    },
    {
      "epoch": 2.013632718524459,
      "grad_norm": 1.9588075876235962,
      "learning_rate": 9.896542481908336e-06,
      "loss": 0.6137,
      "step": 25110
    },
    {
      "epoch": 2.0144346431435447,
      "grad_norm": 2.2909090518951416,
      "learning_rate": 9.88850174216028e-06,
      "loss": 0.6199,
      "step": 25120
    },
    {
      "epoch": 2.0152365677626305,
      "grad_norm": 2.0890235900878906,
      "learning_rate": 9.880461002412222e-06,
      "loss": 0.6218,
      "step": 25130
    },
    {
      "epoch": 2.0160384923817163,
      "grad_norm": 1.985506296157837,
      "learning_rate": 9.872420262664165e-06,
      "loss": 0.5413,
      "step": 25140
    },
    {
      "epoch": 2.016840417000802,
      "grad_norm": 1.6739712953567505,
      "learning_rate": 9.86437952291611e-06,
      "loss": 0.6,
      "step": 25150
    },
    {
      "epoch": 2.017642341619888,
      "grad_norm": 1.9582678079605103,
      "learning_rate": 9.856338783168052e-06,
      "loss": 0.6256,
      "step": 25160
    },
    {
      "epoch": 2.0184442662389737,
      "grad_norm": 1.4383842945098877,
      "learning_rate": 9.848298043419996e-06,
      "loss": 0.5729,
      "step": 25170
    },
    {
      "epoch": 2.0192461908580595,
      "grad_norm": 1.9706584215164185,
      "learning_rate": 9.841061377646744e-06,
      "loss": 0.6186,
      "step": 25180
    },
    {
      "epoch": 2.0200481154771452,
      "grad_norm": 1.8427485227584839,
      "learning_rate": 9.833020637898687e-06,
      "loss": 0.6903,
      "step": 25190
    },
    {
      "epoch": 2.020850040096231,
      "grad_norm": 1.908705234527588,
      "learning_rate": 9.824979898150631e-06,
      "loss": 0.5975,
      "step": 25200
    },
    {
      "epoch": 2.021651964715317,
      "grad_norm": 1.9302815198898315,
      "learning_rate": 9.816939158402574e-06,
      "loss": 0.623,
      "step": 25210
    },
    {
      "epoch": 2.0224538893344026,
      "grad_norm": 2.0477488040924072,
      "learning_rate": 9.808898418654518e-06,
      "loss": 0.6183,
      "step": 25220
    },
    {
      "epoch": 2.0232558139534884,
      "grad_norm": 1.6330265998840332,
      "learning_rate": 9.80085767890646e-06,
      "loss": 0.6414,
      "step": 25230
    },
    {
      "epoch": 2.024057738572574,
      "grad_norm": 1.9597371816635132,
      "learning_rate": 9.792816939158401e-06,
      "loss": 0.6096,
      "step": 25240
    },
    {
      "epoch": 2.02485966319166,
      "grad_norm": 1.5250811576843262,
      "learning_rate": 9.784776199410346e-06,
      "loss": 0.5303,
      "step": 25250
    },
    {
      "epoch": 2.025661587810746,
      "grad_norm": 2.070378303527832,
      "learning_rate": 9.776735459662288e-06,
      "loss": 0.6598,
      "step": 25260
    },
    {
      "epoch": 2.0264635124298316,
      "grad_norm": 1.6494827270507812,
      "learning_rate": 9.768694719914232e-06,
      "loss": 0.6917,
      "step": 25270
    },
    {
      "epoch": 2.0272654370489174,
      "grad_norm": 2.063901424407959,
      "learning_rate": 9.760653980166175e-06,
      "loss": 0.5999,
      "step": 25280
    },
    {
      "epoch": 2.028067361668003,
      "grad_norm": 1.8474178314208984,
      "learning_rate": 9.75261324041812e-06,
      "loss": 0.5973,
      "step": 25290
    },
    {
      "epoch": 2.028869286287089,
      "grad_norm": 1.7992860078811646,
      "learning_rate": 9.744572500670062e-06,
      "loss": 0.5882,
      "step": 25300
    },
    {
      "epoch": 2.0296712109061747,
      "grad_norm": 2.18465518951416,
      "learning_rate": 9.736531760922005e-06,
      "loss": 0.6076,
      "step": 25310
    },
    {
      "epoch": 2.0304731355252605,
      "grad_norm": 1.8128620386123657,
      "learning_rate": 9.728491021173949e-06,
      "loss": 0.5786,
      "step": 25320
    },
    {
      "epoch": 2.0312750601443463,
      "grad_norm": 1.839489459991455,
      "learning_rate": 9.720450281425891e-06,
      "loss": 0.6413,
      "step": 25330
    },
    {
      "epoch": 2.032076984763432,
      "grad_norm": 1.7906670570373535,
      "learning_rate": 9.712409541677836e-06,
      "loss": 0.6785,
      "step": 25340
    },
    {
      "epoch": 2.032878909382518,
      "grad_norm": 1.8482234477996826,
      "learning_rate": 9.704368801929778e-06,
      "loss": 0.5787,
      "step": 25350
    },
    {
      "epoch": 2.0336808340016037,
      "grad_norm": 1.6190122365951538,
      "learning_rate": 9.696328062181721e-06,
      "loss": 0.5794,
      "step": 25360
    },
    {
      "epoch": 2.0344827586206895,
      "grad_norm": 2.152799606323242,
      "learning_rate": 9.688287322433664e-06,
      "loss": 0.5848,
      "step": 25370
    },
    {
      "epoch": 2.0352846832397753,
      "grad_norm": 2.0353522300720215,
      "learning_rate": 9.680246582685606e-06,
      "loss": 0.5868,
      "step": 25380
    },
    {
      "epoch": 2.036086607858861,
      "grad_norm": 1.9395313262939453,
      "learning_rate": 9.67220584293755e-06,
      "loss": 0.6115,
      "step": 25390
    },
    {
      "epoch": 2.036888532477947,
      "grad_norm": 1.8958015441894531,
      "learning_rate": 9.664165103189493e-06,
      "loss": 0.6488,
      "step": 25400
    },
    {
      "epoch": 2.0376904570970327,
      "grad_norm": 1.8827184438705444,
      "learning_rate": 9.656124363441437e-06,
      "loss": 0.5669,
      "step": 25410
    },
    {
      "epoch": 2.038492381716119,
      "grad_norm": 1.6251271963119507,
      "learning_rate": 9.64808362369338e-06,
      "loss": 0.5615,
      "step": 25420
    },
    {
      "epoch": 2.0392943063352047,
      "grad_norm": 1.8901749849319458,
      "learning_rate": 9.640042883945322e-06,
      "loss": 0.5803,
      "step": 25430
    },
    {
      "epoch": 2.0400962309542905,
      "grad_norm": 2.2406375408172607,
      "learning_rate": 9.632002144197267e-06,
      "loss": 0.6363,
      "step": 25440
    },
    {
      "epoch": 2.0408981555733763,
      "grad_norm": 1.8554216623306274,
      "learning_rate": 9.62396140444921e-06,
      "loss": 0.6158,
      "step": 25450
    },
    {
      "epoch": 2.041700080192462,
      "grad_norm": 1.995977520942688,
      "learning_rate": 9.615920664701154e-06,
      "loss": 0.582,
      "step": 25460
    },
    {
      "epoch": 2.042502004811548,
      "grad_norm": 1.9117491245269775,
      "learning_rate": 9.607879924953096e-06,
      "loss": 0.5757,
      "step": 25470
    },
    {
      "epoch": 2.0433039294306337,
      "grad_norm": 2.013646364212036,
      "learning_rate": 9.59983918520504e-06,
      "loss": 0.6409,
      "step": 25480
    },
    {
      "epoch": 2.0441058540497195,
      "grad_norm": 2.2193939685821533,
      "learning_rate": 9.591798445456983e-06,
      "loss": 0.6085,
      "step": 25490
    },
    {
      "epoch": 2.0449077786688052,
      "grad_norm": 1.7488149404525757,
      "learning_rate": 9.583757705708924e-06,
      "loss": 0.6177,
      "step": 25500
    },
    {
      "epoch": 2.045709703287891,
      "grad_norm": 1.605348825454712,
      "learning_rate": 9.575716965960868e-06,
      "loss": 0.5915,
      "step": 25510
    },
    {
      "epoch": 2.046511627906977,
      "grad_norm": 1.9121335744857788,
      "learning_rate": 9.567676226212811e-06,
      "loss": 0.5558,
      "step": 25520
    },
    {
      "epoch": 2.0473135525260626,
      "grad_norm": 1.8880937099456787,
      "learning_rate": 9.559635486464755e-06,
      "loss": 0.6426,
      "step": 25530
    },
    {
      "epoch": 2.0481154771451484,
      "grad_norm": 2.092334508895874,
      "learning_rate": 9.551594746716698e-06,
      "loss": 0.6252,
      "step": 25540
    },
    {
      "epoch": 2.048917401764234,
      "grad_norm": 1.854239821434021,
      "learning_rate": 9.54355400696864e-06,
      "loss": 0.5878,
      "step": 25550
    },
    {
      "epoch": 2.04971932638332,
      "grad_norm": 1.8099586963653564,
      "learning_rate": 9.535513267220585e-06,
      "loss": 0.5623,
      "step": 25560
    },
    {
      "epoch": 2.050521251002406,
      "grad_norm": 1.9747700691223145,
      "learning_rate": 9.527472527472527e-06,
      "loss": 0.5644,
      "step": 25570
    },
    {
      "epoch": 2.0513231756214916,
      "grad_norm": 1.820066213607788,
      "learning_rate": 9.519431787724472e-06,
      "loss": 0.6289,
      "step": 25580
    },
    {
      "epoch": 2.0521251002405774,
      "grad_norm": 1.765764594078064,
      "learning_rate": 9.511391047976414e-06,
      "loss": 0.5648,
      "step": 25590
    },
    {
      "epoch": 2.052927024859663,
      "grad_norm": 2.070816993713379,
      "learning_rate": 9.503350308228358e-06,
      "loss": 0.577,
      "step": 25600
    },
    {
      "epoch": 2.053728949478749,
      "grad_norm": 2.138768196105957,
      "learning_rate": 9.495309568480301e-06,
      "loss": 0.5345,
      "step": 25610
    },
    {
      "epoch": 2.0545308740978347,
      "grad_norm": 1.6294327974319458,
      "learning_rate": 9.487268828732244e-06,
      "loss": 0.6793,
      "step": 25620
    },
    {
      "epoch": 2.0553327987169205,
      "grad_norm": 1.9267449378967285,
      "learning_rate": 9.479228088984186e-06,
      "loss": 0.5418,
      "step": 25630
    },
    {
      "epoch": 2.0561347233360063,
      "grad_norm": 1.880377173423767,
      "learning_rate": 9.471187349236129e-06,
      "loss": 0.6127,
      "step": 25640
    },
    {
      "epoch": 2.056936647955092,
      "grad_norm": 1.9274049997329712,
      "learning_rate": 9.463146609488073e-06,
      "loss": 0.5444,
      "step": 25650
    },
    {
      "epoch": 2.057738572574178,
      "grad_norm": 2.1187398433685303,
      "learning_rate": 9.455105869740016e-06,
      "loss": 0.555,
      "step": 25660
    },
    {
      "epoch": 2.0585404971932637,
      "grad_norm": 1.9309433698654175,
      "learning_rate": 9.44706512999196e-06,
      "loss": 0.607,
      "step": 25670
    },
    {
      "epoch": 2.0593424218123495,
      "grad_norm": 1.9783378839492798,
      "learning_rate": 9.439024390243903e-06,
      "loss": 0.6154,
      "step": 25680
    },
    {
      "epoch": 2.0601443464314353,
      "grad_norm": 1.6552186012268066,
      "learning_rate": 9.430983650495845e-06,
      "loss": 0.5361,
      "step": 25690
    },
    {
      "epoch": 2.060946271050521,
      "grad_norm": 1.6109693050384521,
      "learning_rate": 9.42294291074779e-06,
      "loss": 0.5899,
      "step": 25700
    },
    {
      "epoch": 2.061748195669607,
      "grad_norm": 1.7270156145095825,
      "learning_rate": 9.414902170999732e-06,
      "loss": 0.6478,
      "step": 25710
    },
    {
      "epoch": 2.0625501202886927,
      "grad_norm": 1.6882396936416626,
      "learning_rate": 9.406861431251676e-06,
      "loss": 0.6001,
      "step": 25720
    },
    {
      "epoch": 2.0633520449077785,
      "grad_norm": 1.981526255607605,
      "learning_rate": 9.398820691503619e-06,
      "loss": 0.6493,
      "step": 25730
    },
    {
      "epoch": 2.0641539695268643,
      "grad_norm": 1.7543821334838867,
      "learning_rate": 9.390779951755562e-06,
      "loss": 0.6224,
      "step": 25740
    },
    {
      "epoch": 2.0649558941459505,
      "grad_norm": 1.6953834295272827,
      "learning_rate": 9.382739212007506e-06,
      "loss": 0.5893,
      "step": 25750
    },
    {
      "epoch": 2.0657578187650363,
      "grad_norm": 2.1187586784362793,
      "learning_rate": 9.374698472259447e-06,
      "loss": 0.5583,
      "step": 25760
    },
    {
      "epoch": 2.066559743384122,
      "grad_norm": 1.8047980070114136,
      "learning_rate": 9.366657732511391e-06,
      "loss": 0.619,
      "step": 25770
    },
    {
      "epoch": 2.067361668003208,
      "grad_norm": 1.7567336559295654,
      "learning_rate": 9.358616992763334e-06,
      "loss": 0.544,
      "step": 25780
    },
    {
      "epoch": 2.0681635926222937,
      "grad_norm": 2.152219772338867,
      "learning_rate": 9.350576253015278e-06,
      "loss": 0.6458,
      "step": 25790
    },
    {
      "epoch": 2.0689655172413794,
      "grad_norm": 2.2055485248565674,
      "learning_rate": 9.34253551326722e-06,
      "loss": 0.6295,
      "step": 25800
    },
    {
      "epoch": 2.0697674418604652,
      "grad_norm": 1.7754322290420532,
      "learning_rate": 9.334494773519163e-06,
      "loss": 0.5603,
      "step": 25810
    },
    {
      "epoch": 2.070569366479551,
      "grad_norm": 1.9086662530899048,
      "learning_rate": 9.326454033771107e-06,
      "loss": 0.6345,
      "step": 25820
    },
    {
      "epoch": 2.071371291098637,
      "grad_norm": 1.5839380025863647,
      "learning_rate": 9.31841329402305e-06,
      "loss": 0.6203,
      "step": 25830
    },
    {
      "epoch": 2.0721732157177226,
      "grad_norm": 1.7862621545791626,
      "learning_rate": 9.310372554274994e-06,
      "loss": 0.6185,
      "step": 25840
    },
    {
      "epoch": 2.0729751403368084,
      "grad_norm": 2.021599292755127,
      "learning_rate": 9.302331814526937e-06,
      "loss": 0.6348,
      "step": 25850
    },
    {
      "epoch": 2.073777064955894,
      "grad_norm": 2.221599578857422,
      "learning_rate": 9.294291074778881e-06,
      "loss": 0.6073,
      "step": 25860
    },
    {
      "epoch": 2.07457898957498,
      "grad_norm": 1.8101736307144165,
      "learning_rate": 9.286250335030824e-06,
      "loss": 0.5257,
      "step": 25870
    },
    {
      "epoch": 2.075380914194066,
      "grad_norm": 2.146162748336792,
      "learning_rate": 9.278209595282766e-06,
      "loss": 0.5248,
      "step": 25880
    },
    {
      "epoch": 2.0761828388131516,
      "grad_norm": 2.0838699340820312,
      "learning_rate": 9.270168855534709e-06,
      "loss": 0.6263,
      "step": 25890
    },
    {
      "epoch": 2.0769847634322374,
      "grad_norm": 2.091913938522339,
      "learning_rate": 9.262128115786652e-06,
      "loss": 0.616,
      "step": 25900
    },
    {
      "epoch": 2.077786688051323,
      "grad_norm": 1.6134341955184937,
      "learning_rate": 9.254087376038596e-06,
      "loss": 0.6017,
      "step": 25910
    },
    {
      "epoch": 2.078588612670409,
      "grad_norm": 2.009681224822998,
      "learning_rate": 9.246046636290538e-06,
      "loss": 0.5863,
      "step": 25920
    },
    {
      "epoch": 2.0793905372894947,
      "grad_norm": 1.7664365768432617,
      "learning_rate": 9.238005896542483e-06,
      "loss": 0.6028,
      "step": 25930
    },
    {
      "epoch": 2.0801924619085805,
      "grad_norm": 2.1402430534362793,
      "learning_rate": 9.229965156794425e-06,
      "loss": 0.664,
      "step": 25940
    },
    {
      "epoch": 2.0809943865276663,
      "grad_norm": 1.7247265577316284,
      "learning_rate": 9.221924417046368e-06,
      "loss": 0.6797,
      "step": 25950
    },
    {
      "epoch": 2.081796311146752,
      "grad_norm": 1.7564637660980225,
      "learning_rate": 9.213883677298312e-06,
      "loss": 0.5801,
      "step": 25960
    },
    {
      "epoch": 2.082598235765838,
      "grad_norm": 2.022517204284668,
      "learning_rate": 9.205842937550255e-06,
      "loss": 0.5871,
      "step": 25970
    },
    {
      "epoch": 2.0834001603849237,
      "grad_norm": 1.452157974243164,
      "learning_rate": 9.197802197802199e-06,
      "loss": 0.554,
      "step": 25980
    },
    {
      "epoch": 2.0842020850040095,
      "grad_norm": 1.6493316888809204,
      "learning_rate": 9.189761458054142e-06,
      "loss": 0.5623,
      "step": 25990
    },
    {
      "epoch": 2.0850040096230953,
      "grad_norm": 2.3685967922210693,
      "learning_rate": 9.181720718306084e-06,
      "loss": 0.6114,
      "step": 26000
    },
    {
      "epoch": 2.085805934242181,
      "grad_norm": 1.5329583883285522,
      "learning_rate": 9.173679978558027e-06,
      "loss": 0.6671,
      "step": 26010
    },
    {
      "epoch": 2.086607858861267,
      "grad_norm": 2.0339508056640625,
      "learning_rate": 9.16563923880997e-06,
      "loss": 0.6147,
      "step": 26020
    },
    {
      "epoch": 2.0874097834803527,
      "grad_norm": 1.8268643617630005,
      "learning_rate": 9.157598499061914e-06,
      "loss": 0.5663,
      "step": 26030
    },
    {
      "epoch": 2.0882117080994385,
      "grad_norm": 1.9081705808639526,
      "learning_rate": 9.149557759313856e-06,
      "loss": 0.6353,
      "step": 26040
    },
    {
      "epoch": 2.0890136327185242,
      "grad_norm": 1.7167683839797974,
      "learning_rate": 9.1415170195658e-06,
      "loss": 0.6659,
      "step": 26050
    },
    {
      "epoch": 2.0898155573376105,
      "grad_norm": 1.9997084140777588,
      "learning_rate": 9.133476279817743e-06,
      "loss": 0.5953,
      "step": 26060
    },
    {
      "epoch": 2.0906174819566963,
      "grad_norm": 1.575775384902954,
      "learning_rate": 9.125435540069686e-06,
      "loss": 0.6101,
      "step": 26070
    },
    {
      "epoch": 2.091419406575782,
      "grad_norm": 1.4931939840316772,
      "learning_rate": 9.11739480032163e-06,
      "loss": 0.6139,
      "step": 26080
    },
    {
      "epoch": 2.092221331194868,
      "grad_norm": 2.2136430740356445,
      "learning_rate": 9.109354060573573e-06,
      "loss": 0.6206,
      "step": 26090
    },
    {
      "epoch": 2.0930232558139537,
      "grad_norm": 2.198542594909668,
      "learning_rate": 9.101313320825517e-06,
      "loss": 0.5432,
      "step": 26100
    },
    {
      "epoch": 2.0938251804330394,
      "grad_norm": 1.932120442390442,
      "learning_rate": 9.09327258107746e-06,
      "loss": 0.5716,
      "step": 26110
    },
    {
      "epoch": 2.0946271050521252,
      "grad_norm": 2.2362492084503174,
      "learning_rate": 9.085231841329404e-06,
      "loss": 0.6789,
      "step": 26120
    },
    {
      "epoch": 2.095429029671211,
      "grad_norm": 1.7955936193466187,
      "learning_rate": 9.077191101581347e-06,
      "loss": 0.6078,
      "step": 26130
    },
    {
      "epoch": 2.096230954290297,
      "grad_norm": 1.9513076543807983,
      "learning_rate": 9.069150361833287e-06,
      "loss": 0.5784,
      "step": 26140
    },
    {
      "epoch": 2.0970328789093826,
      "grad_norm": 1.6639759540557861,
      "learning_rate": 9.061109622085232e-06,
      "loss": 0.5446,
      "step": 26150
    },
    {
      "epoch": 2.0978348035284684,
      "grad_norm": 1.5742768049240112,
      "learning_rate": 9.053068882337174e-06,
      "loss": 0.5376,
      "step": 26160
    },
    {
      "epoch": 2.098636728147554,
      "grad_norm": 1.7123154401779175,
      "learning_rate": 9.045028142589119e-06,
      "loss": 0.6022,
      "step": 26170
    },
    {
      "epoch": 2.09943865276664,
      "grad_norm": 2.0619752407073975,
      "learning_rate": 9.036987402841061e-06,
      "loss": 0.5993,
      "step": 26180
    },
    {
      "epoch": 2.100240577385726,
      "grad_norm": 1.8714897632598877,
      "learning_rate": 9.028946663093004e-06,
      "loss": 0.6515,
      "step": 26190
    },
    {
      "epoch": 2.1010425020048116,
      "grad_norm": 1.804067850112915,
      "learning_rate": 9.020905923344948e-06,
      "loss": 0.6405,
      "step": 26200
    },
    {
      "epoch": 2.1018444266238974,
      "grad_norm": 1.888728380203247,
      "learning_rate": 9.01286518359689e-06,
      "loss": 0.6238,
      "step": 26210
    },
    {
      "epoch": 2.102646351242983,
      "grad_norm": 1.7826793193817139,
      "learning_rate": 9.004824443848835e-06,
      "loss": 0.6102,
      "step": 26220
    },
    {
      "epoch": 2.103448275862069,
      "grad_norm": 1.6927522420883179,
      "learning_rate": 8.996783704100778e-06,
      "loss": 0.5691,
      "step": 26230
    },
    {
      "epoch": 2.1042502004811547,
      "grad_norm": 2.0720815658569336,
      "learning_rate": 8.988742964352722e-06,
      "loss": 0.6265,
      "step": 26240
    },
    {
      "epoch": 2.1050521251002405,
      "grad_norm": 1.9225366115570068,
      "learning_rate": 8.980702224604664e-06,
      "loss": 0.6329,
      "step": 26250
    },
    {
      "epoch": 2.1058540497193263,
      "grad_norm": 1.9814220666885376,
      "learning_rate": 8.972661484856607e-06,
      "loss": 0.6313,
      "step": 26260
    },
    {
      "epoch": 2.106655974338412,
      "grad_norm": 1.6870617866516113,
      "learning_rate": 8.96462074510855e-06,
      "loss": 0.5611,
      "step": 26270
    },
    {
      "epoch": 2.107457898957498,
      "grad_norm": 1.5733463764190674,
      "learning_rate": 8.956580005360492e-06,
      "loss": 0.5357,
      "step": 26280
    },
    {
      "epoch": 2.1082598235765837,
      "grad_norm": 1.8913016319274902,
      "learning_rate": 8.948539265612437e-06,
      "loss": 0.5651,
      "step": 26290
    },
    {
      "epoch": 2.1090617481956695,
      "grad_norm": 2.0369114875793457,
      "learning_rate": 8.94049852586438e-06,
      "loss": 0.5429,
      "step": 26300
    },
    {
      "epoch": 2.1098636728147553,
      "grad_norm": 2.0761542320251465,
      "learning_rate": 8.932457786116323e-06,
      "loss": 0.5478,
      "step": 26310
    },
    {
      "epoch": 2.110665597433841,
      "grad_norm": 1.8165652751922607,
      "learning_rate": 8.924417046368266e-06,
      "loss": 0.582,
      "step": 26320
    },
    {
      "epoch": 2.111467522052927,
      "grad_norm": 1.8524997234344482,
      "learning_rate": 8.916376306620209e-06,
      "loss": 0.5576,
      "step": 26330
    },
    {
      "epoch": 2.1122694466720127,
      "grad_norm": 1.7911863327026367,
      "learning_rate": 8.908335566872153e-06,
      "loss": 0.6107,
      "step": 26340
    },
    {
      "epoch": 2.1130713712910985,
      "grad_norm": 1.8835997581481934,
      "learning_rate": 8.900294827124096e-06,
      "loss": 0.6136,
      "step": 26350
    },
    {
      "epoch": 2.1138732959101842,
      "grad_norm": 1.7734062671661377,
      "learning_rate": 8.89225408737604e-06,
      "loss": 0.6273,
      "step": 26360
    },
    {
      "epoch": 2.11467522052927,
      "grad_norm": 1.8745636940002441,
      "learning_rate": 8.884213347627982e-06,
      "loss": 0.61,
      "step": 26370
    },
    {
      "epoch": 2.115477145148356,
      "grad_norm": 1.7394990921020508,
      "learning_rate": 8.876172607879925e-06,
      "loss": 0.5392,
      "step": 26380
    },
    {
      "epoch": 2.116279069767442,
      "grad_norm": 2.0688483715057373,
      "learning_rate": 8.86813186813187e-06,
      "loss": 0.6504,
      "step": 26390
    },
    {
      "epoch": 2.117080994386528,
      "grad_norm": 1.5463637113571167,
      "learning_rate": 8.86009112838381e-06,
      "loss": 0.5538,
      "step": 26400
    },
    {
      "epoch": 2.1178829190056137,
      "grad_norm": 1.765638828277588,
      "learning_rate": 8.852050388635754e-06,
      "loss": 0.615,
      "step": 26410
    },
    {
      "epoch": 2.1186848436246994,
      "grad_norm": 2.3786065578460693,
      "learning_rate": 8.844009648887697e-06,
      "loss": 0.5589,
      "step": 26420
    },
    {
      "epoch": 2.1194867682437852,
      "grad_norm": 1.7724000215530396,
      "learning_rate": 8.835968909139641e-06,
      "loss": 0.5845,
      "step": 26430
    },
    {
      "epoch": 2.120288692862871,
      "grad_norm": 2.265615701675415,
      "learning_rate": 8.827928169391584e-06,
      "loss": 0.624,
      "step": 26440
    },
    {
      "epoch": 2.121090617481957,
      "grad_norm": 2.0704073905944824,
      "learning_rate": 8.819887429643527e-06,
      "loss": 0.5974,
      "step": 26450
    },
    {
      "epoch": 2.1218925421010426,
      "grad_norm": 2.286639928817749,
      "learning_rate": 8.811846689895471e-06,
      "loss": 0.6584,
      "step": 26460
    },
    {
      "epoch": 2.1226944667201284,
      "grad_norm": 1.9493894577026367,
      "learning_rate": 8.803805950147413e-06,
      "loss": 0.5396,
      "step": 26470
    },
    {
      "epoch": 2.123496391339214,
      "grad_norm": 2.1405344009399414,
      "learning_rate": 8.795765210399358e-06,
      "loss": 0.5936,
      "step": 26480
    },
    {
      "epoch": 2.1242983159583,
      "grad_norm": 1.7839325666427612,
      "learning_rate": 8.7877244706513e-06,
      "loss": 0.563,
      "step": 26490
    },
    {
      "epoch": 2.125100240577386,
      "grad_norm": 1.5285426378250122,
      "learning_rate": 8.779683730903245e-06,
      "loss": 0.6317,
      "step": 26500
    },
    {
      "epoch": 2.1259021651964716,
      "grad_norm": 2.0700459480285645,
      "learning_rate": 8.771642991155187e-06,
      "loss": 0.6771,
      "step": 26510
    },
    {
      "epoch": 2.1267040898155574,
      "grad_norm": 2.0512168407440186,
      "learning_rate": 8.76360225140713e-06,
      "loss": 0.5566,
      "step": 26520
    },
    {
      "epoch": 2.127506014434643,
      "grad_norm": 1.7517399787902832,
      "learning_rate": 8.755561511659072e-06,
      "loss": 0.5941,
      "step": 26530
    },
    {
      "epoch": 2.128307939053729,
      "grad_norm": 1.8967055082321167,
      "learning_rate": 8.747520771911015e-06,
      "loss": 0.6711,
      "step": 26540
    },
    {
      "epoch": 2.1291098636728147,
      "grad_norm": 1.8644193410873413,
      "learning_rate": 8.73948003216296e-06,
      "loss": 0.5763,
      "step": 26550
    },
    {
      "epoch": 2.1299117882919005,
      "grad_norm": 1.675015926361084,
      "learning_rate": 8.731439292414902e-06,
      "loss": 0.6481,
      "step": 26560
    },
    {
      "epoch": 2.1307137129109863,
      "grad_norm": 2.2109837532043457,
      "learning_rate": 8.723398552666846e-06,
      "loss": 0.6339,
      "step": 26570
    },
    {
      "epoch": 2.131515637530072,
      "grad_norm": 2.1735899448394775,
      "learning_rate": 8.715357812918789e-06,
      "loss": 0.6609,
      "step": 26580
    },
    {
      "epoch": 2.132317562149158,
      "grad_norm": 1.6230287551879883,
      "learning_rate": 8.707317073170731e-06,
      "loss": 0.5895,
      "step": 26590
    },
    {
      "epoch": 2.1331194867682437,
      "grad_norm": 1.8280876874923706,
      "learning_rate": 8.699276333422676e-06,
      "loss": 0.6655,
      "step": 26600
    },
    {
      "epoch": 2.1339214113873295,
      "grad_norm": 1.8261767625808716,
      "learning_rate": 8.691235593674618e-06,
      "loss": 0.5637,
      "step": 26610
    },
    {
      "epoch": 2.1347233360064153,
      "grad_norm": 1.7815265655517578,
      "learning_rate": 8.683194853926563e-06,
      "loss": 0.6391,
      "step": 26620
    },
    {
      "epoch": 2.135525260625501,
      "grad_norm": 2.092587471008301,
      "learning_rate": 8.675154114178505e-06,
      "loss": 0.6039,
      "step": 26630
    },
    {
      "epoch": 2.136327185244587,
      "grad_norm": 1.8694673776626587,
      "learning_rate": 8.667113374430448e-06,
      "loss": 0.5512,
      "step": 26640
    },
    {
      "epoch": 2.1371291098636727,
      "grad_norm": 1.808689832687378,
      "learning_rate": 8.65907263468239e-06,
      "loss": 0.5911,
      "step": 26650
    },
    {
      "epoch": 2.1379310344827585,
      "grad_norm": 1.7992571592330933,
      "learning_rate": 8.651031894934333e-06,
      "loss": 0.6448,
      "step": 26660
    },
    {
      "epoch": 2.1387329591018442,
      "grad_norm": 1.7986871004104614,
      "learning_rate": 8.642991155186277e-06,
      "loss": 0.6001,
      "step": 26670
    },
    {
      "epoch": 2.13953488372093,
      "grad_norm": 2.2001359462738037,
      "learning_rate": 8.63495041543822e-06,
      "loss": 0.5467,
      "step": 26680
    },
    {
      "epoch": 2.140336808340016,
      "grad_norm": 1.6945546865463257,
      "learning_rate": 8.626909675690164e-06,
      "loss": 0.661,
      "step": 26690
    },
    {
      "epoch": 2.141138732959102,
      "grad_norm": 1.8921442031860352,
      "learning_rate": 8.618868935942107e-06,
      "loss": 0.551,
      "step": 26700
    },
    {
      "epoch": 2.141940657578188,
      "grad_norm": 2.0246148109436035,
      "learning_rate": 8.61082819619405e-06,
      "loss": 0.628,
      "step": 26710
    },
    {
      "epoch": 2.1427425821972736,
      "grad_norm": 1.8059706687927246,
      "learning_rate": 8.602787456445994e-06,
      "loss": 0.6004,
      "step": 26720
    },
    {
      "epoch": 2.1435445068163594,
      "grad_norm": 1.9236255884170532,
      "learning_rate": 8.594746716697936e-06,
      "loss": 0.6514,
      "step": 26730
    },
    {
      "epoch": 2.1443464314354452,
      "grad_norm": 1.6462957859039307,
      "learning_rate": 8.58670597694988e-06,
      "loss": 0.5705,
      "step": 26740
    },
    {
      "epoch": 2.145148356054531,
      "grad_norm": 1.9412459135055542,
      "learning_rate": 8.578665237201823e-06,
      "loss": 0.6537,
      "step": 26750
    },
    {
      "epoch": 2.145950280673617,
      "grad_norm": 2.189802885055542,
      "learning_rate": 8.570624497453767e-06,
      "loss": 0.6277,
      "step": 26760
    },
    {
      "epoch": 2.1467522052927026,
      "grad_norm": 2.438896656036377,
      "learning_rate": 8.56258375770571e-06,
      "loss": 0.7085,
      "step": 26770
    },
    {
      "epoch": 2.1475541299117884,
      "grad_norm": 2.03467059135437,
      "learning_rate": 8.554543017957651e-06,
      "loss": 0.575,
      "step": 26780
    },
    {
      "epoch": 2.148356054530874,
      "grad_norm": 1.7265654802322388,
      "learning_rate": 8.546502278209595e-06,
      "loss": 0.5837,
      "step": 26790
    },
    {
      "epoch": 2.14915797914996,
      "grad_norm": 1.7829551696777344,
      "learning_rate": 8.538461538461538e-06,
      "loss": 0.5944,
      "step": 26800
    },
    {
      "epoch": 2.1499599037690458,
      "grad_norm": 1.8112707138061523,
      "learning_rate": 8.530420798713482e-06,
      "loss": 0.5553,
      "step": 26810
    },
    {
      "epoch": 2.1507618283881316,
      "grad_norm": 1.8230485916137695,
      "learning_rate": 8.522380058965425e-06,
      "loss": 0.5888,
      "step": 26820
    },
    {
      "epoch": 2.1515637530072174,
      "grad_norm": 1.8783801794052124,
      "learning_rate": 8.514339319217367e-06,
      "loss": 0.5984,
      "step": 26830
    },
    {
      "epoch": 2.152365677626303,
      "grad_norm": 1.9512776136398315,
      "learning_rate": 8.506298579469312e-06,
      "loss": 0.5721,
      "step": 26840
    },
    {
      "epoch": 2.153167602245389,
      "grad_norm": 1.6808041334152222,
      "learning_rate": 8.498257839721254e-06,
      "loss": 0.5699,
      "step": 26850
    },
    {
      "epoch": 2.1539695268644747,
      "grad_norm": 2.0431103706359863,
      "learning_rate": 8.490217099973198e-06,
      "loss": 0.6178,
      "step": 26860
    },
    {
      "epoch": 2.1547714514835605,
      "grad_norm": 1.7851659059524536,
      "learning_rate": 8.482176360225141e-06,
      "loss": 0.6205,
      "step": 26870
    },
    {
      "epoch": 2.1555733761026463,
      "grad_norm": 1.904035210609436,
      "learning_rate": 8.474135620477085e-06,
      "loss": 0.5634,
      "step": 26880
    },
    {
      "epoch": 2.156375300721732,
      "grad_norm": 2.209219455718994,
      "learning_rate": 8.466094880729028e-06,
      "loss": 0.622,
      "step": 26890
    },
    {
      "epoch": 2.157177225340818,
      "grad_norm": 1.7970478534698486,
      "learning_rate": 8.45805414098097e-06,
      "loss": 0.5684,
      "step": 26900
    },
    {
      "epoch": 2.1579791499599037,
      "grad_norm": 1.9006544351577759,
      "learning_rate": 8.450013401232913e-06,
      "loss": 0.6369,
      "step": 26910
    },
    {
      "epoch": 2.1587810745789895,
      "grad_norm": 1.9705612659454346,
      "learning_rate": 8.441972661484856e-06,
      "loss": 0.6464,
      "step": 26920
    },
    {
      "epoch": 2.1595829991980753,
      "grad_norm": 2.026265859603882,
      "learning_rate": 8.4339319217368e-06,
      "loss": 0.5944,
      "step": 26930
    },
    {
      "epoch": 2.160384923817161,
      "grad_norm": 1.815311074256897,
      "learning_rate": 8.425891181988743e-06,
      "loss": 0.6,
      "step": 26940
    },
    {
      "epoch": 2.161186848436247,
      "grad_norm": 1.7233107089996338,
      "learning_rate": 8.417850442240687e-06,
      "loss": 0.5417,
      "step": 26950
    },
    {
      "epoch": 2.1619887730553327,
      "grad_norm": 1.7815663814544678,
      "learning_rate": 8.40980970249263e-06,
      "loss": 0.5374,
      "step": 26960
    },
    {
      "epoch": 2.1627906976744184,
      "grad_norm": 2.016810417175293,
      "learning_rate": 8.401768962744572e-06,
      "loss": 0.5523,
      "step": 26970
    },
    {
      "epoch": 2.1635926222935042,
      "grad_norm": 1.9093378782272339,
      "learning_rate": 8.393728222996516e-06,
      "loss": 0.5587,
      "step": 26980
    },
    {
      "epoch": 2.16439454691259,
      "grad_norm": 1.8743538856506348,
      "learning_rate": 8.385687483248459e-06,
      "loss": 0.6069,
      "step": 26990
    },
    {
      "epoch": 2.165196471531676,
      "grad_norm": 1.748665452003479,
      "learning_rate": 8.377646743500403e-06,
      "loss": 0.6253,
      "step": 27000
    },
    {
      "epoch": 2.165998396150762,
      "grad_norm": 2.2069525718688965,
      "learning_rate": 8.369606003752346e-06,
      "loss": 0.6502,
      "step": 27010
    },
    {
      "epoch": 2.1668003207698474,
      "grad_norm": 1.902370810508728,
      "learning_rate": 8.361565264004288e-06,
      "loss": 0.6685,
      "step": 27020
    },
    {
      "epoch": 2.1676022453889336,
      "grad_norm": 1.6722891330718994,
      "learning_rate": 8.353524524256233e-06,
      "loss": 0.5538,
      "step": 27030
    },
    {
      "epoch": 2.1684041700080194,
      "grad_norm": 2.1014654636383057,
      "learning_rate": 8.345483784508174e-06,
      "loss": 0.6378,
      "step": 27040
    },
    {
      "epoch": 2.1692060946271052,
      "grad_norm": 1.5915343761444092,
      "learning_rate": 8.337443044760118e-06,
      "loss": 0.646,
      "step": 27050
    },
    {
      "epoch": 2.170008019246191,
      "grad_norm": 1.7990832328796387,
      "learning_rate": 8.32940230501206e-06,
      "loss": 0.5703,
      "step": 27060
    },
    {
      "epoch": 2.170809943865277,
      "grad_norm": 1.6927218437194824,
      "learning_rate": 8.321361565264005e-06,
      "loss": 0.5233,
      "step": 27070
    },
    {
      "epoch": 2.1716118684843626,
      "grad_norm": 1.938126564025879,
      "learning_rate": 8.313320825515947e-06,
      "loss": 0.5658,
      "step": 27080
    },
    {
      "epoch": 2.1724137931034484,
      "grad_norm": 1.7556469440460205,
      "learning_rate": 8.30528008576789e-06,
      "loss": 0.6608,
      "step": 27090
    },
    {
      "epoch": 2.173215717722534,
      "grad_norm": 1.7978596687316895,
      "learning_rate": 8.297239346019834e-06,
      "loss": 0.666,
      "step": 27100
    },
    {
      "epoch": 2.17401764234162,
      "grad_norm": 2.0010478496551514,
      "learning_rate": 8.289198606271777e-06,
      "loss": 0.6256,
      "step": 27110
    },
    {
      "epoch": 2.1748195669607058,
      "grad_norm": 1.8505241870880127,
      "learning_rate": 8.281157866523721e-06,
      "loss": 0.5928,
      "step": 27120
    },
    {
      "epoch": 2.1756214915797916,
      "grad_norm": 1.983415961265564,
      "learning_rate": 8.273117126775664e-06,
      "loss": 0.5975,
      "step": 27130
    },
    {
      "epoch": 2.1764234161988774,
      "grad_norm": 1.6456035375595093,
      "learning_rate": 8.265076387027608e-06,
      "loss": 0.5842,
      "step": 27140
    },
    {
      "epoch": 2.177225340817963,
      "grad_norm": 1.9399117231369019,
      "learning_rate": 8.25703564727955e-06,
      "loss": 0.6313,
      "step": 27150
    },
    {
      "epoch": 2.178027265437049,
      "grad_norm": 1.8650542497634888,
      "learning_rate": 8.248994907531493e-06,
      "loss": 0.6104,
      "step": 27160
    },
    {
      "epoch": 2.1788291900561347,
      "grad_norm": 1.7434159517288208,
      "learning_rate": 8.240954167783436e-06,
      "loss": 0.6229,
      "step": 27170
    },
    {
      "epoch": 2.1796311146752205,
      "grad_norm": 1.844972848892212,
      "learning_rate": 8.232913428035378e-06,
      "loss": 0.6144,
      "step": 27180
    },
    {
      "epoch": 2.1804330392943063,
      "grad_norm": 1.699877142906189,
      "learning_rate": 8.224872688287323e-06,
      "loss": 0.5581,
      "step": 27190
    },
    {
      "epoch": 2.181234963913392,
      "grad_norm": 1.7966904640197754,
      "learning_rate": 8.216831948539265e-06,
      "loss": 0.5802,
      "step": 27200
    },
    {
      "epoch": 2.182036888532478,
      "grad_norm": 1.9409207105636597,
      "learning_rate": 8.20879120879121e-06,
      "loss": 0.5927,
      "step": 27210
    },
    {
      "epoch": 2.1828388131515637,
      "grad_norm": 1.9351675510406494,
      "learning_rate": 8.200750469043152e-06,
      "loss": 0.6277,
      "step": 27220
    },
    {
      "epoch": 2.1836407377706495,
      "grad_norm": 1.7326703071594238,
      "learning_rate": 8.192709729295095e-06,
      "loss": 0.6009,
      "step": 27230
    },
    {
      "epoch": 2.1844426623897353,
      "grad_norm": 1.9688186645507812,
      "learning_rate": 8.184668989547039e-06,
      "loss": 0.5769,
      "step": 27240
    },
    {
      "epoch": 2.185244587008821,
      "grad_norm": 1.9648778438568115,
      "learning_rate": 8.176628249798982e-06,
      "loss": 0.5937,
      "step": 27250
    },
    {
      "epoch": 2.186046511627907,
      "grad_norm": 2.2400765419006348,
      "learning_rate": 8.168587510050926e-06,
      "loss": 0.6318,
      "step": 27260
    },
    {
      "epoch": 2.1868484362469927,
      "grad_norm": 1.7187986373901367,
      "learning_rate": 8.160546770302869e-06,
      "loss": 0.6021,
      "step": 27270
    },
    {
      "epoch": 2.1876503608660784,
      "grad_norm": 1.9977784156799316,
      "learning_rate": 8.152506030554811e-06,
      "loss": 0.5777,
      "step": 27280
    },
    {
      "epoch": 2.1884522854851642,
      "grad_norm": 2.0269689559936523,
      "learning_rate": 8.144465290806754e-06,
      "loss": 0.6337,
      "step": 27290
    },
    {
      "epoch": 2.18925421010425,
      "grad_norm": 1.8084968328475952,
      "learning_rate": 8.137228625033503e-06,
      "loss": 0.5888,
      "step": 27300
    },
    {
      "epoch": 2.190056134723336,
      "grad_norm": 1.7612707614898682,
      "learning_rate": 8.129187885285446e-06,
      "loss": 0.6146,
      "step": 27310
    },
    {
      "epoch": 2.1908580593424216,
      "grad_norm": 2.0393049716949463,
      "learning_rate": 8.12114714553739e-06,
      "loss": 0.6056,
      "step": 27320
    },
    {
      "epoch": 2.1916599839615074,
      "grad_norm": 2.75048828125,
      "learning_rate": 8.113106405789333e-06,
      "loss": 0.6535,
      "step": 27330
    },
    {
      "epoch": 2.1924619085805936,
      "grad_norm": 2.2423479557037354,
      "learning_rate": 8.105065666041277e-06,
      "loss": 0.5986,
      "step": 27340
    },
    {
      "epoch": 2.1932638331996794,
      "grad_norm": 1.9977339506149292,
      "learning_rate": 8.09702492629322e-06,
      "loss": 0.6627,
      "step": 27350
    },
    {
      "epoch": 2.1940657578187652,
      "grad_norm": 1.9629377126693726,
      "learning_rate": 8.088984186545162e-06,
      "loss": 0.5411,
      "step": 27360
    },
    {
      "epoch": 2.194867682437851,
      "grad_norm": 1.9583086967468262,
      "learning_rate": 8.080943446797105e-06,
      "loss": 0.5713,
      "step": 27370
    },
    {
      "epoch": 2.195669607056937,
      "grad_norm": 1.6809897422790527,
      "learning_rate": 8.072902707049047e-06,
      "loss": 0.6266,
      "step": 27380
    },
    {
      "epoch": 2.1964715316760226,
      "grad_norm": 1.686741590499878,
      "learning_rate": 8.064861967300992e-06,
      "loss": 0.5605,
      "step": 27390
    },
    {
      "epoch": 2.1972734562951084,
      "grad_norm": 1.9086549282073975,
      "learning_rate": 8.056821227552934e-06,
      "loss": 0.6524,
      "step": 27400
    },
    {
      "epoch": 2.198075380914194,
      "grad_norm": 2.131058931350708,
      "learning_rate": 8.048780487804879e-06,
      "loss": 0.6528,
      "step": 27410
    },
    {
      "epoch": 2.19887730553328,
      "grad_norm": 1.596124291419983,
      "learning_rate": 8.040739748056821e-06,
      "loss": 0.5598,
      "step": 27420
    },
    {
      "epoch": 2.1996792301523658,
      "grad_norm": 1.7443301677703857,
      "learning_rate": 8.032699008308766e-06,
      "loss": 0.573,
      "step": 27430
    },
    {
      "epoch": 2.2004811547714516,
      "grad_norm": 1.8355355262756348,
      "learning_rate": 8.024658268560708e-06,
      "loss": 0.6244,
      "step": 27440
    },
    {
      "epoch": 2.2012830793905374,
      "grad_norm": 1.8555909395217896,
      "learning_rate": 8.01661752881265e-06,
      "loss": 0.5935,
      "step": 27450
    },
    {
      "epoch": 2.202085004009623,
      "grad_norm": 1.863534927368164,
      "learning_rate": 8.008576789064595e-06,
      "loss": 0.6035,
      "step": 27460
    },
    {
      "epoch": 2.202886928628709,
      "grad_norm": 2.066845178604126,
      "learning_rate": 8.000536049316538e-06,
      "loss": 0.6287,
      "step": 27470
    },
    {
      "epoch": 2.2036888532477947,
      "grad_norm": 1.9792293310165405,
      "learning_rate": 7.992495309568482e-06,
      "loss": 0.5794,
      "step": 27480
    },
    {
      "epoch": 2.2044907778668805,
      "grad_norm": 1.5876823663711548,
      "learning_rate": 7.984454569820423e-06,
      "loss": 0.5701,
      "step": 27490
    },
    {
      "epoch": 2.2052927024859663,
      "grad_norm": 2.280761241912842,
      "learning_rate": 7.976413830072367e-06,
      "loss": 0.5836,
      "step": 27500
    },
    {
      "epoch": 2.206094627105052,
      "grad_norm": 2.003192663192749,
      "learning_rate": 7.96837309032431e-06,
      "loss": 0.5592,
      "step": 27510
    },
    {
      "epoch": 2.206896551724138,
      "grad_norm": 1.6222304105758667,
      "learning_rate": 7.960332350576252e-06,
      "loss": 0.586,
      "step": 27520
    },
    {
      "epoch": 2.2076984763432237,
      "grad_norm": 1.7373628616333008,
      "learning_rate": 7.952291610828197e-06,
      "loss": 0.6123,
      "step": 27530
    },
    {
      "epoch": 2.2085004009623095,
      "grad_norm": 1.8009265661239624,
      "learning_rate": 7.944250871080139e-06,
      "loss": 0.6032,
      "step": 27540
    },
    {
      "epoch": 2.2093023255813953,
      "grad_norm": 1.6456118822097778,
      "learning_rate": 7.936210131332083e-06,
      "loss": 0.5218,
      "step": 27550
    },
    {
      "epoch": 2.210104250200481,
      "grad_norm": 2.074359893798828,
      "learning_rate": 7.928169391584026e-06,
      "loss": 0.5809,
      "step": 27560
    },
    {
      "epoch": 2.210906174819567,
      "grad_norm": 2.2051773071289062,
      "learning_rate": 7.920128651835969e-06,
      "loss": 0.5589,
      "step": 27570
    },
    {
      "epoch": 2.2117080994386527,
      "grad_norm": 1.9998835325241089,
      "learning_rate": 7.912087912087913e-06,
      "loss": 0.5801,
      "step": 27580
    },
    {
      "epoch": 2.2125100240577384,
      "grad_norm": 1.9958248138427734,
      "learning_rate": 7.904047172339856e-06,
      "loss": 0.6068,
      "step": 27590
    },
    {
      "epoch": 2.2133119486768242,
      "grad_norm": 1.636936902999878,
      "learning_rate": 7.8960064325918e-06,
      "loss": 0.5657,
      "step": 27600
    },
    {
      "epoch": 2.21411387329591,
      "grad_norm": 2.136525869369507,
      "learning_rate": 7.887965692843742e-06,
      "loss": 0.669,
      "step": 27610
    },
    {
      "epoch": 2.214915797914996,
      "grad_norm": 2.2334043979644775,
      "learning_rate": 7.879924953095685e-06,
      "loss": 0.6881,
      "step": 27620
    },
    {
      "epoch": 2.2157177225340816,
      "grad_norm": 1.6827595233917236,
      "learning_rate": 7.871884213347628e-06,
      "loss": 0.5945,
      "step": 27630
    },
    {
      "epoch": 2.2165196471531674,
      "grad_norm": 2.178325653076172,
      "learning_rate": 7.86384347359957e-06,
      "loss": 0.6115,
      "step": 27640
    },
    {
      "epoch": 2.2173215717722536,
      "grad_norm": 2.0570616722106934,
      "learning_rate": 7.855802733851514e-06,
      "loss": 0.6439,
      "step": 27650
    },
    {
      "epoch": 2.218123496391339,
      "grad_norm": 2.1139605045318604,
      "learning_rate": 7.847761994103457e-06,
      "loss": 0.6262,
      "step": 27660
    },
    {
      "epoch": 2.2189254210104252,
      "grad_norm": 1.701650857925415,
      "learning_rate": 7.839721254355401e-06,
      "loss": 0.5882,
      "step": 27670
    },
    {
      "epoch": 2.219727345629511,
      "grad_norm": 1.6745710372924805,
      "learning_rate": 7.831680514607344e-06,
      "loss": 0.5486,
      "step": 27680
    },
    {
      "epoch": 2.220529270248597,
      "grad_norm": 2.125173807144165,
      "learning_rate": 7.823639774859288e-06,
      "loss": 0.6575,
      "step": 27690
    },
    {
      "epoch": 2.2213311948676826,
      "grad_norm": 1.9441341161727905,
      "learning_rate": 7.815599035111231e-06,
      "loss": 0.6157,
      "step": 27700
    },
    {
      "epoch": 2.2221331194867684,
      "grad_norm": 1.8943169116973877,
      "learning_rate": 7.807558295363173e-06,
      "loss": 0.5956,
      "step": 27710
    },
    {
      "epoch": 2.222935044105854,
      "grad_norm": 1.5835312604904175,
      "learning_rate": 7.799517555615118e-06,
      "loss": 0.6012,
      "step": 27720
    },
    {
      "epoch": 2.22373696872494,
      "grad_norm": 1.6946839094161987,
      "learning_rate": 7.79147681586706e-06,
      "loss": 0.5956,
      "step": 27730
    },
    {
      "epoch": 2.2245388933440258,
      "grad_norm": 1.700021505355835,
      "learning_rate": 7.783436076119003e-06,
      "loss": 0.6169,
      "step": 27740
    },
    {
      "epoch": 2.2253408179631116,
      "grad_norm": 1.7500622272491455,
      "learning_rate": 7.775395336370946e-06,
      "loss": 0.6343,
      "step": 27750
    },
    {
      "epoch": 2.2261427425821974,
      "grad_norm": 1.9249942302703857,
      "learning_rate": 7.767354596622888e-06,
      "loss": 0.5919,
      "step": 27760
    },
    {
      "epoch": 2.226944667201283,
      "grad_norm": 2.1365323066711426,
      "learning_rate": 7.759313856874832e-06,
      "loss": 0.6177,
      "step": 27770
    },
    {
      "epoch": 2.227746591820369,
      "grad_norm": 1.8988322019577026,
      "learning_rate": 7.751273117126775e-06,
      "loss": 0.6097,
      "step": 27780
    },
    {
      "epoch": 2.2285485164394547,
      "grad_norm": 1.7784770727157593,
      "learning_rate": 7.74323237737872e-06,
      "loss": 0.6274,
      "step": 27790
    },
    {
      "epoch": 2.2293504410585405,
      "grad_norm": 1.8370436429977417,
      "learning_rate": 7.735191637630662e-06,
      "loss": 0.5841,
      "step": 27800
    },
    {
      "epoch": 2.2301523656776263,
      "grad_norm": 2.314556121826172,
      "learning_rate": 7.727150897882606e-06,
      "loss": 0.5885,
      "step": 27810
    },
    {
      "epoch": 2.230954290296712,
      "grad_norm": 2.2078018188476562,
      "learning_rate": 7.719110158134549e-06,
      "loss": 0.599,
      "step": 27820
    },
    {
      "epoch": 2.231756214915798,
      "grad_norm": 2.0088658332824707,
      "learning_rate": 7.711069418386491e-06,
      "loss": 0.649,
      "step": 27830
    },
    {
      "epoch": 2.2325581395348837,
      "grad_norm": 2.0058796405792236,
      "learning_rate": 7.703028678638436e-06,
      "loss": 0.5615,
      "step": 27840
    },
    {
      "epoch": 2.2333600641539695,
      "grad_norm": 1.673527717590332,
      "learning_rate": 7.694987938890378e-06,
      "loss": 0.637,
      "step": 27850
    },
    {
      "epoch": 2.2341619887730553,
      "grad_norm": 1.785502314567566,
      "learning_rate": 7.686947199142323e-06,
      "loss": 0.5868,
      "step": 27860
    },
    {
      "epoch": 2.234963913392141,
      "grad_norm": 2.0095109939575195,
      "learning_rate": 7.678906459394263e-06,
      "loss": 0.6275,
      "step": 27870
    },
    {
      "epoch": 2.235765838011227,
      "grad_norm": 1.9518954753875732,
      "learning_rate": 7.670865719646208e-06,
      "loss": 0.6496,
      "step": 27880
    },
    {
      "epoch": 2.2365677626303127,
      "grad_norm": 1.699265718460083,
      "learning_rate": 7.66282497989815e-06,
      "loss": 0.5657,
      "step": 27890
    },
    {
      "epoch": 2.2373696872493984,
      "grad_norm": 1.769114375114441,
      "learning_rate": 7.654784240150093e-06,
      "loss": 0.6178,
      "step": 27900
    },
    {
      "epoch": 2.2381716118684842,
      "grad_norm": 1.8492391109466553,
      "learning_rate": 7.646743500402037e-06,
      "loss": 0.6385,
      "step": 27910
    },
    {
      "epoch": 2.23897353648757,
      "grad_norm": 1.7445743083953857,
      "learning_rate": 7.63870276065398e-06,
      "loss": 0.5835,
      "step": 27920
    },
    {
      "epoch": 2.239775461106656,
      "grad_norm": 2.0422072410583496,
      "learning_rate": 7.630662020905924e-06,
      "loss": 0.6456,
      "step": 27930
    },
    {
      "epoch": 2.2405773857257416,
      "grad_norm": 1.560572862625122,
      "learning_rate": 7.622621281157867e-06,
      "loss": 0.6184,
      "step": 27940
    },
    {
      "epoch": 2.2413793103448274,
      "grad_norm": 1.6692909002304077,
      "learning_rate": 7.614580541409809e-06,
      "loss": 0.5385,
      "step": 27950
    },
    {
      "epoch": 2.242181234963913,
      "grad_norm": 2.0085763931274414,
      "learning_rate": 7.606539801661754e-06,
      "loss": 0.596,
      "step": 27960
    },
    {
      "epoch": 2.242983159582999,
      "grad_norm": 1.9682589769363403,
      "learning_rate": 7.598499061913695e-06,
      "loss": 0.6111,
      "step": 27970
    },
    {
      "epoch": 2.2437850842020852,
      "grad_norm": 1.6069332361221313,
      "learning_rate": 7.59045832216564e-06,
      "loss": 0.5663,
      "step": 27980
    },
    {
      "epoch": 2.244587008821171,
      "grad_norm": 2.1881325244903564,
      "learning_rate": 7.582417582417582e-06,
      "loss": 0.6173,
      "step": 27990
    },
    {
      "epoch": 2.245388933440257,
      "grad_norm": 1.8090721368789673,
      "learning_rate": 7.5743768426695265e-06,
      "loss": 0.629,
      "step": 28000
    },
    {
      "epoch": 2.2461908580593426,
      "grad_norm": 2.036102294921875,
      "learning_rate": 7.566336102921469e-06,
      "loss": 0.6026,
      "step": 28010
    },
    {
      "epoch": 2.2469927826784284,
      "grad_norm": 1.8843885660171509,
      "learning_rate": 7.558295363173412e-06,
      "loss": 0.5564,
      "step": 28020
    },
    {
      "epoch": 2.247794707297514,
      "grad_norm": 2.194596290588379,
      "learning_rate": 7.550254623425355e-06,
      "loss": 0.6938,
      "step": 28030
    },
    {
      "epoch": 2.2485966319166,
      "grad_norm": 1.5975924730300903,
      "learning_rate": 7.542213883677298e-06,
      "loss": 0.606,
      "step": 28040
    },
    {
      "epoch": 2.2493985565356858,
      "grad_norm": 2.4007954597473145,
      "learning_rate": 7.534173143929242e-06,
      "loss": 0.596,
      "step": 28050
    },
    {
      "epoch": 2.2502004811547716,
      "grad_norm": 1.5520766973495483,
      "learning_rate": 7.526132404181185e-06,
      "loss": 0.6528,
      "step": 28060
    },
    {
      "epoch": 2.2510024057738574,
      "grad_norm": 1.8481942415237427,
      "learning_rate": 7.518091664433129e-06,
      "loss": 0.634,
      "step": 28070
    },
    {
      "epoch": 2.251804330392943,
      "grad_norm": 2.138934373855591,
      "learning_rate": 7.5100509246850715e-06,
      "loss": 0.5711,
      "step": 28080
    },
    {
      "epoch": 2.252606255012029,
      "grad_norm": 2.226422071456909,
      "learning_rate": 7.502010184937014e-06,
      "loss": 0.635,
      "step": 28090
    },
    {
      "epoch": 2.2534081796311147,
      "grad_norm": 1.821584939956665,
      "learning_rate": 7.4939694451889576e-06,
      "loss": 0.6119,
      "step": 28100
    },
    {
      "epoch": 2.2542101042502005,
      "grad_norm": 2.269995927810669,
      "learning_rate": 7.4859287054409e-06,
      "loss": 0.5697,
      "step": 28110
    },
    {
      "epoch": 2.2550120288692863,
      "grad_norm": 1.8678128719329834,
      "learning_rate": 7.477887965692844e-06,
      "loss": 0.5421,
      "step": 28120
    },
    {
      "epoch": 2.255813953488372,
      "grad_norm": 2.176070213317871,
      "learning_rate": 7.469847225944787e-06,
      "loss": 0.6232,
      "step": 28130
    },
    {
      "epoch": 2.256615878107458,
      "grad_norm": 2.0343680381774902,
      "learning_rate": 7.4618064861967305e-06,
      "loss": 0.5888,
      "step": 28140
    },
    {
      "epoch": 2.2574178027265437,
      "grad_norm": 1.969567894935608,
      "learning_rate": 7.453765746448674e-06,
      "loss": 0.655,
      "step": 28150
    },
    {
      "epoch": 2.2582197273456295,
      "grad_norm": 2.0136868953704834,
      "learning_rate": 7.4457250067006165e-06,
      "loss": 0.537,
      "step": 28160
    },
    {
      "epoch": 2.2590216519647153,
      "grad_norm": 1.9647325277328491,
      "learning_rate": 7.43768426695256e-06,
      "loss": 0.5767,
      "step": 28170
    },
    {
      "epoch": 2.259823576583801,
      "grad_norm": 2.1073617935180664,
      "learning_rate": 7.4296435272045026e-06,
      "loss": 0.6617,
      "step": 28180
    },
    {
      "epoch": 2.260625501202887,
      "grad_norm": 1.998895525932312,
      "learning_rate": 7.421602787456446e-06,
      "loss": 0.6185,
      "step": 28190
    },
    {
      "epoch": 2.2614274258219726,
      "grad_norm": 1.715530276298523,
      "learning_rate": 7.4135620477083894e-06,
      "loss": 0.5767,
      "step": 28200
    },
    {
      "epoch": 2.2622293504410584,
      "grad_norm": 1.888304352760315,
      "learning_rate": 7.405521307960333e-06,
      "loss": 0.6921,
      "step": 28210
    },
    {
      "epoch": 2.2630312750601442,
      "grad_norm": 1.8430702686309814,
      "learning_rate": 7.397480568212276e-06,
      "loss": 0.5762,
      "step": 28220
    },
    {
      "epoch": 2.26383319967923,
      "grad_norm": 2.1753993034362793,
      "learning_rate": 7.389439828464219e-06,
      "loss": 0.5883,
      "step": 28230
    },
    {
      "epoch": 2.264635124298316,
      "grad_norm": 2.05191969871521,
      "learning_rate": 7.3813990887161615e-06,
      "loss": 0.6626,
      "step": 28240
    },
    {
      "epoch": 2.2654370489174016,
      "grad_norm": 2.854691743850708,
      "learning_rate": 7.373358348968105e-06,
      "loss": 0.6204,
      "step": 28250
    },
    {
      "epoch": 2.2662389735364874,
      "grad_norm": 1.6799206733703613,
      "learning_rate": 7.365317609220048e-06,
      "loss": 0.5773,
      "step": 28260
    },
    {
      "epoch": 2.267040898155573,
      "grad_norm": 1.993338942527771,
      "learning_rate": 7.357276869471992e-06,
      "loss": 0.6275,
      "step": 28270
    },
    {
      "epoch": 2.267842822774659,
      "grad_norm": 1.7470226287841797,
      "learning_rate": 7.349236129723935e-06,
      "loss": 0.5944,
      "step": 28280
    },
    {
      "epoch": 2.268644747393745,
      "grad_norm": 2.0568745136260986,
      "learning_rate": 7.341195389975878e-06,
      "loss": 0.6006,
      "step": 28290
    },
    {
      "epoch": 2.2694466720128306,
      "grad_norm": 1.9914069175720215,
      "learning_rate": 7.333154650227821e-06,
      "loss": 0.6102,
      "step": 28300
    },
    {
      "epoch": 2.270248596631917,
      "grad_norm": 1.8722769021987915,
      "learning_rate": 7.325113910479764e-06,
      "loss": 0.6892,
      "step": 28310
    },
    {
      "epoch": 2.2710505212510026,
      "grad_norm": 2.042264699935913,
      "learning_rate": 7.317073170731707e-06,
      "loss": 0.6786,
      "step": 28320
    },
    {
      "epoch": 2.2718524458700884,
      "grad_norm": 1.8168401718139648,
      "learning_rate": 7.309032430983651e-06,
      "loss": 0.6596,
      "step": 28330
    },
    {
      "epoch": 2.272654370489174,
      "grad_norm": 1.8103997707366943,
      "learning_rate": 7.300991691235594e-06,
      "loss": 0.5705,
      "step": 28340
    },
    {
      "epoch": 2.27345629510826,
      "grad_norm": 1.6935224533081055,
      "learning_rate": 7.292950951487537e-06,
      "loss": 0.58,
      "step": 28350
    },
    {
      "epoch": 2.2742582197273458,
      "grad_norm": 1.8436322212219238,
      "learning_rate": 7.28491021173948e-06,
      "loss": 0.6102,
      "step": 28360
    },
    {
      "epoch": 2.2750601443464316,
      "grad_norm": 2.1412620544433594,
      "learning_rate": 7.276869471991423e-06,
      "loss": 0.6149,
      "step": 28370
    },
    {
      "epoch": 2.2758620689655173,
      "grad_norm": 1.8273273706436157,
      "learning_rate": 7.268828732243366e-06,
      "loss": 0.6187,
      "step": 28380
    },
    {
      "epoch": 2.276663993584603,
      "grad_norm": 2.2419016361236572,
      "learning_rate": 7.26078799249531e-06,
      "loss": 0.614,
      "step": 28390
    },
    {
      "epoch": 2.277465918203689,
      "grad_norm": 2.0236830711364746,
      "learning_rate": 7.252747252747253e-06,
      "loss": 0.6145,
      "step": 28400
    },
    {
      "epoch": 2.2782678428227747,
      "grad_norm": 2.0041587352752686,
      "learning_rate": 7.244706512999197e-06,
      "loss": 0.5411,
      "step": 28410
    },
    {
      "epoch": 2.2790697674418605,
      "grad_norm": 2.0051181316375732,
      "learning_rate": 7.236665773251139e-06,
      "loss": 0.6502,
      "step": 28420
    },
    {
      "epoch": 2.2798716920609463,
      "grad_norm": 2.0500168800354004,
      "learning_rate": 7.228625033503082e-06,
      "loss": 0.6345,
      "step": 28430
    },
    {
      "epoch": 2.280673616680032,
      "grad_norm": 1.8438537120819092,
      "learning_rate": 7.220584293755025e-06,
      "loss": 0.672,
      "step": 28440
    },
    {
      "epoch": 2.281475541299118,
      "grad_norm": 1.832673192024231,
      "learning_rate": 7.212543554006969e-06,
      "loss": 0.6196,
      "step": 28450
    },
    {
      "epoch": 2.2822774659182037,
      "grad_norm": 1.7977616786956787,
      "learning_rate": 7.204502814258912e-06,
      "loss": 0.5839,
      "step": 28460
    },
    {
      "epoch": 2.2830793905372895,
      "grad_norm": 2.0258002281188965,
      "learning_rate": 7.196462074510856e-06,
      "loss": 0.6383,
      "step": 28470
    },
    {
      "epoch": 2.2838813151563753,
      "grad_norm": 2.3895928859710693,
      "learning_rate": 7.188421334762798e-06,
      "loss": 0.5809,
      "step": 28480
    },
    {
      "epoch": 2.284683239775461,
      "grad_norm": 1.812801480293274,
      "learning_rate": 7.180380595014742e-06,
      "loss": 0.6349,
      "step": 28490
    },
    {
      "epoch": 2.285485164394547,
      "grad_norm": 2.1393015384674072,
      "learning_rate": 7.172339855266684e-06,
      "loss": 0.6153,
      "step": 28500
    },
    {
      "epoch": 2.2862870890136326,
      "grad_norm": 1.8574100732803345,
      "learning_rate": 7.164299115518628e-06,
      "loss": 0.5594,
      "step": 28510
    },
    {
      "epoch": 2.2870890136327184,
      "grad_norm": 2.067880630493164,
      "learning_rate": 7.156258375770571e-06,
      "loss": 0.5344,
      "step": 28520
    },
    {
      "epoch": 2.2878909382518042,
      "grad_norm": 1.9011818170547485,
      "learning_rate": 7.148217636022515e-06,
      "loss": 0.6159,
      "step": 28530
    },
    {
      "epoch": 2.28869286287089,
      "grad_norm": 1.8803818225860596,
      "learning_rate": 7.140176896274458e-06,
      "loss": 0.5626,
      "step": 28540
    },
    {
      "epoch": 2.289494787489976,
      "grad_norm": 1.7313531637191772,
      "learning_rate": 7.132136156526401e-06,
      "loss": 0.5887,
      "step": 28550
    },
    {
      "epoch": 2.2902967121090616,
      "grad_norm": 1.7804675102233887,
      "learning_rate": 7.124095416778343e-06,
      "loss": 0.5838,
      "step": 28560
    },
    {
      "epoch": 2.2910986367281474,
      "grad_norm": 2.0534307956695557,
      "learning_rate": 7.116054677030287e-06,
      "loss": 0.6152,
      "step": 28570
    },
    {
      "epoch": 2.291900561347233,
      "grad_norm": 1.8124696016311646,
      "learning_rate": 7.10801393728223e-06,
      "loss": 0.6195,
      "step": 28580
    },
    {
      "epoch": 2.292702485966319,
      "grad_norm": 1.939900279045105,
      "learning_rate": 7.0999731975341736e-06,
      "loss": 0.6382,
      "step": 28590
    },
    {
      "epoch": 2.293504410585405,
      "grad_norm": 1.8842624425888062,
      "learning_rate": 7.091932457786117e-06,
      "loss": 0.6204,
      "step": 28600
    },
    {
      "epoch": 2.2943063352044906,
      "grad_norm": 2.0300493240356445,
      "learning_rate": 7.08389171803806e-06,
      "loss": 0.5882,
      "step": 28610
    },
    {
      "epoch": 2.295108259823577,
      "grad_norm": 1.5830612182617188,
      "learning_rate": 7.075850978290003e-06,
      "loss": 0.5488,
      "step": 28620
    },
    {
      "epoch": 2.295910184442662,
      "grad_norm": 1.7404223680496216,
      "learning_rate": 7.067810238541946e-06,
      "loss": 0.6265,
      "step": 28630
    },
    {
      "epoch": 2.2967121090617484,
      "grad_norm": 1.9129493236541748,
      "learning_rate": 7.059769498793889e-06,
      "loss": 0.6165,
      "step": 28640
    },
    {
      "epoch": 2.297514033680834,
      "grad_norm": 1.9495246410369873,
      "learning_rate": 7.0517287590458325e-06,
      "loss": 0.5759,
      "step": 28650
    },
    {
      "epoch": 2.29831595829992,
      "grad_norm": 2.1915626525878906,
      "learning_rate": 7.043688019297776e-06,
      "loss": 0.5804,
      "step": 28660
    },
    {
      "epoch": 2.2991178829190058,
      "grad_norm": 2.0757505893707275,
      "learning_rate": 7.0356472795497186e-06,
      "loss": 0.5933,
      "step": 28670
    },
    {
      "epoch": 2.2999198075380916,
      "grad_norm": 1.8138325214385986,
      "learning_rate": 7.027606539801662e-06,
      "loss": 0.654,
      "step": 28680
    },
    {
      "epoch": 2.3007217321571773,
      "grad_norm": 1.6413007974624634,
      "learning_rate": 7.019565800053605e-06,
      "loss": 0.6444,
      "step": 28690
    },
    {
      "epoch": 2.301523656776263,
      "grad_norm": 1.7668514251708984,
      "learning_rate": 7.011525060305548e-06,
      "loss": 0.6059,
      "step": 28700
    },
    {
      "epoch": 2.302325581395349,
      "grad_norm": 1.8382160663604736,
      "learning_rate": 7.0034843205574915e-06,
      "loss": 0.5352,
      "step": 28710
    },
    {
      "epoch": 2.3031275060144347,
      "grad_norm": 1.5849568843841553,
      "learning_rate": 6.995443580809435e-06,
      "loss": 0.5721,
      "step": 28720
    },
    {
      "epoch": 2.3039294306335205,
      "grad_norm": 1.9506094455718994,
      "learning_rate": 6.987402841061378e-06,
      "loss": 0.5858,
      "step": 28730
    },
    {
      "epoch": 2.3047313552526063,
      "grad_norm": 1.8581430912017822,
      "learning_rate": 6.979362101313321e-06,
      "loss": 0.6067,
      "step": 28740
    },
    {
      "epoch": 2.305533279871692,
      "grad_norm": 1.7799370288848877,
      "learning_rate": 6.9713213615652636e-06,
      "loss": 0.556,
      "step": 28750
    },
    {
      "epoch": 2.306335204490778,
      "grad_norm": 1.723779320716858,
      "learning_rate": 6.963280621817207e-06,
      "loss": 0.6004,
      "step": 28760
    },
    {
      "epoch": 2.3071371291098637,
      "grad_norm": 2.516568422317505,
      "learning_rate": 6.9552398820691505e-06,
      "loss": 0.6403,
      "step": 28770
    },
    {
      "epoch": 2.3079390537289495,
      "grad_norm": 2.026040554046631,
      "learning_rate": 6.947199142321094e-06,
      "loss": 0.5682,
      "step": 28780
    },
    {
      "epoch": 2.3087409783480353,
      "grad_norm": 1.8335165977478027,
      "learning_rate": 6.939158402573037e-06,
      "loss": 0.5265,
      "step": 28790
    },
    {
      "epoch": 2.309542902967121,
      "grad_norm": 1.8368949890136719,
      "learning_rate": 6.93111766282498e-06,
      "loss": 0.567,
      "step": 28800
    },
    {
      "epoch": 2.310344827586207,
      "grad_norm": 1.8907955884933472,
      "learning_rate": 6.923076923076923e-06,
      "loss": 0.5473,
      "step": 28810
    },
    {
      "epoch": 2.3111467522052926,
      "grad_norm": 1.8761470317840576,
      "learning_rate": 6.915036183328866e-06,
      "loss": 0.6654,
      "step": 28820
    },
    {
      "epoch": 2.3119486768243784,
      "grad_norm": 1.9228827953338623,
      "learning_rate": 6.906995443580809e-06,
      "loss": 0.6547,
      "step": 28830
    },
    {
      "epoch": 2.3127506014434642,
      "grad_norm": 2.162837505340576,
      "learning_rate": 6.898954703832753e-06,
      "loss": 0.5798,
      "step": 28840
    },
    {
      "epoch": 2.31355252606255,
      "grad_norm": 1.837914228439331,
      "learning_rate": 6.890913964084696e-06,
      "loss": 0.5387,
      "step": 28850
    },
    {
      "epoch": 2.314354450681636,
      "grad_norm": 1.9528340101242065,
      "learning_rate": 6.88287322433664e-06,
      "loss": 0.5715,
      "step": 28860
    },
    {
      "epoch": 2.3151563753007216,
      "grad_norm": 1.7629952430725098,
      "learning_rate": 6.874832484588582e-06,
      "loss": 0.5938,
      "step": 28870
    },
    {
      "epoch": 2.3159582999198074,
      "grad_norm": 2.379518985748291,
      "learning_rate": 6.866791744840525e-06,
      "loss": 0.6308,
      "step": 28880
    },
    {
      "epoch": 2.316760224538893,
      "grad_norm": 1.7799066305160522,
      "learning_rate": 6.858751005092468e-06,
      "loss": 0.6515,
      "step": 28890
    },
    {
      "epoch": 2.317562149157979,
      "grad_norm": 2.0111019611358643,
      "learning_rate": 6.850710265344412e-06,
      "loss": 0.6288,
      "step": 28900
    },
    {
      "epoch": 2.3183640737770648,
      "grad_norm": 1.9068392515182495,
      "learning_rate": 6.842669525596355e-06,
      "loss": 0.5486,
      "step": 28910
    },
    {
      "epoch": 2.3191659983961506,
      "grad_norm": 1.9102977514266968,
      "learning_rate": 6.834628785848299e-06,
      "loss": 0.6212,
      "step": 28920
    },
    {
      "epoch": 2.319967923015237,
      "grad_norm": 1.7341428995132446,
      "learning_rate": 6.826588046100241e-06,
      "loss": 0.6118,
      "step": 28930
    },
    {
      "epoch": 2.320769847634322,
      "grad_norm": 2.0192418098449707,
      "learning_rate": 6.818547306352185e-06,
      "loss": 0.6175,
      "step": 28940
    },
    {
      "epoch": 2.3215717722534084,
      "grad_norm": 1.9506688117980957,
      "learning_rate": 6.810506566604127e-06,
      "loss": 0.5833,
      "step": 28950
    },
    {
      "epoch": 2.322373696872494,
      "grad_norm": 2.506100654602051,
      "learning_rate": 6.802465826856071e-06,
      "loss": 0.6323,
      "step": 28960
    },
    {
      "epoch": 2.32317562149158,
      "grad_norm": 2.124614715576172,
      "learning_rate": 6.794425087108014e-06,
      "loss": 0.6009,
      "step": 28970
    },
    {
      "epoch": 2.3239775461106658,
      "grad_norm": 1.9261558055877686,
      "learning_rate": 6.786384347359958e-06,
      "loss": 0.6171,
      "step": 28980
    },
    {
      "epoch": 2.3247794707297516,
      "grad_norm": 1.5903886556625366,
      "learning_rate": 6.7783436076119e-06,
      "loss": 0.5661,
      "step": 28990
    },
    {
      "epoch": 2.3255813953488373,
      "grad_norm": 1.5490375757217407,
      "learning_rate": 6.770302867863844e-06,
      "loss": 0.6345,
      "step": 29000
    },
    {
      "epoch": 2.326383319967923,
      "grad_norm": 1.9055095911026,
      "learning_rate": 6.762262128115786e-06,
      "loss": 0.6182,
      "step": 29010
    },
    {
      "epoch": 2.327185244587009,
      "grad_norm": 1.705163598060608,
      "learning_rate": 6.75422138836773e-06,
      "loss": 0.5688,
      "step": 29020
    },
    {
      "epoch": 2.3279871692060947,
      "grad_norm": 1.7520405054092407,
      "learning_rate": 6.746180648619673e-06,
      "loss": 0.5518,
      "step": 29030
    },
    {
      "epoch": 2.3287890938251805,
      "grad_norm": 2.064265727996826,
      "learning_rate": 6.738139908871617e-06,
      "loss": 0.6205,
      "step": 29040
    },
    {
      "epoch": 2.3295910184442663,
      "grad_norm": 1.5445799827575684,
      "learning_rate": 6.73009916912356e-06,
      "loss": 0.5476,
      "step": 29050
    },
    {
      "epoch": 2.330392943063352,
      "grad_norm": 1.898723840713501,
      "learning_rate": 6.722058429375503e-06,
      "loss": 0.6088,
      "step": 29060
    },
    {
      "epoch": 2.331194867682438,
      "grad_norm": 1.770776629447937,
      "learning_rate": 6.714017689627445e-06,
      "loss": 0.5544,
      "step": 29070
    },
    {
      "epoch": 2.3319967923015237,
      "grad_norm": 2.037701368331909,
      "learning_rate": 6.705976949879389e-06,
      "loss": 0.6523,
      "step": 29080
    },
    {
      "epoch": 2.3327987169206095,
      "grad_norm": 1.6191034317016602,
      "learning_rate": 6.697936210131332e-06,
      "loss": 0.5497,
      "step": 29090
    },
    {
      "epoch": 2.3336006415396953,
      "grad_norm": 1.5747846364974976,
      "learning_rate": 6.689895470383276e-06,
      "loss": 0.585,
      "step": 29100
    },
    {
      "epoch": 2.334402566158781,
      "grad_norm": 1.6836155652999878,
      "learning_rate": 6.681854730635219e-06,
      "loss": 0.5198,
      "step": 29110
    },
    {
      "epoch": 2.335204490777867,
      "grad_norm": 2.2844061851501465,
      "learning_rate": 6.673813990887162e-06,
      "loss": 0.642,
      "step": 29120
    },
    {
      "epoch": 2.3360064153969526,
      "grad_norm": 1.8653379678726196,
      "learning_rate": 6.665773251139105e-06,
      "loss": 0.6191,
      "step": 29130
    },
    {
      "epoch": 2.3368083400160384,
      "grad_norm": 2.1131603717803955,
      "learning_rate": 6.657732511391048e-06,
      "loss": 0.607,
      "step": 29140
    },
    {
      "epoch": 2.3376102646351242,
      "grad_norm": 1.6654330492019653,
      "learning_rate": 6.649691771642991e-06,
      "loss": 0.6224,
      "step": 29150
    },
    {
      "epoch": 2.33841218925421,
      "grad_norm": 2.0022244453430176,
      "learning_rate": 6.6416510318949346e-06,
      "loss": 0.5947,
      "step": 29160
    },
    {
      "epoch": 2.339214113873296,
      "grad_norm": 2.1006252765655518,
      "learning_rate": 6.633610292146878e-06,
      "loss": 0.6167,
      "step": 29170
    },
    {
      "epoch": 2.3400160384923816,
      "grad_norm": 1.9412175416946411,
      "learning_rate": 6.6255695523988215e-06,
      "loss": 0.6389,
      "step": 29180
    },
    {
      "epoch": 2.3408179631114674,
      "grad_norm": 1.9712884426116943,
      "learning_rate": 6.617528812650764e-06,
      "loss": 0.5965,
      "step": 29190
    },
    {
      "epoch": 2.341619887730553,
      "grad_norm": 2.0471041202545166,
      "learning_rate": 6.609488072902707e-06,
      "loss": 0.5957,
      "step": 29200
    },
    {
      "epoch": 2.342421812349639,
      "grad_norm": 1.9410759210586548,
      "learning_rate": 6.60144733315465e-06,
      "loss": 0.6168,
      "step": 29210
    },
    {
      "epoch": 2.3432237369687248,
      "grad_norm": 1.6736292839050293,
      "learning_rate": 6.5934065934065935e-06,
      "loss": 0.5882,
      "step": 29220
    },
    {
      "epoch": 2.3440256615878106,
      "grad_norm": 1.6447824239730835,
      "learning_rate": 6.585365853658537e-06,
      "loss": 0.5893,
      "step": 29230
    },
    {
      "epoch": 2.344827586206897,
      "grad_norm": 1.812218427658081,
      "learning_rate": 6.57732511391048e-06,
      "loss": 0.5987,
      "step": 29240
    },
    {
      "epoch": 2.345629510825982,
      "grad_norm": 1.7606219053268433,
      "learning_rate": 6.569284374162423e-06,
      "loss": 0.5757,
      "step": 29250
    },
    {
      "epoch": 2.3464314354450684,
      "grad_norm": 1.9278446435928345,
      "learning_rate": 6.5612436344143665e-06,
      "loss": 0.6118,
      "step": 29260
    },
    {
      "epoch": 2.3472333600641537,
      "grad_norm": 1.900802731513977,
      "learning_rate": 6.553202894666309e-06,
      "loss": 0.6151,
      "step": 29270
    },
    {
      "epoch": 2.34803528468324,
      "grad_norm": 1.9078216552734375,
      "learning_rate": 6.5451621549182525e-06,
      "loss": 0.617,
      "step": 29280
    },
    {
      "epoch": 2.3488372093023258,
      "grad_norm": 1.8380640745162964,
      "learning_rate": 6.537121415170196e-06,
      "loss": 0.5384,
      "step": 29290
    },
    {
      "epoch": 2.3496391339214115,
      "grad_norm": 1.476712942123413,
      "learning_rate": 6.529080675422139e-06,
      "loss": 0.5713,
      "step": 29300
    },
    {
      "epoch": 2.3504410585404973,
      "grad_norm": 1.8033539056777954,
      "learning_rate": 6.521039935674083e-06,
      "loss": 0.6758,
      "step": 29310
    },
    {
      "epoch": 2.351242983159583,
      "grad_norm": 1.616228699684143,
      "learning_rate": 6.5129991959260254e-06,
      "loss": 0.6054,
      "step": 29320
    },
    {
      "epoch": 2.352044907778669,
      "grad_norm": 1.7777738571166992,
      "learning_rate": 6.504958456177968e-06,
      "loss": 0.6089,
      "step": 29330
    },
    {
      "epoch": 2.3528468323977547,
      "grad_norm": 1.7634462118148804,
      "learning_rate": 6.4969177164299115e-06,
      "loss": 0.7244,
      "step": 29340
    },
    {
      "epoch": 2.3536487570168405,
      "grad_norm": 1.9496104717254639,
      "learning_rate": 6.488876976681855e-06,
      "loss": 0.6133,
      "step": 29350
    },
    {
      "epoch": 2.3544506816359263,
      "grad_norm": 1.8398038148880005,
      "learning_rate": 6.480836236933798e-06,
      "loss": 0.6283,
      "step": 29360
    },
    {
      "epoch": 2.355252606255012,
      "grad_norm": 1.873185634613037,
      "learning_rate": 6.472795497185742e-06,
      "loss": 0.5835,
      "step": 29370
    },
    {
      "epoch": 2.356054530874098,
      "grad_norm": 2.039405107498169,
      "learning_rate": 6.464754757437684e-06,
      "loss": 0.6005,
      "step": 29380
    },
    {
      "epoch": 2.3568564554931837,
      "grad_norm": 1.9898916482925415,
      "learning_rate": 6.456714017689627e-06,
      "loss": 0.6172,
      "step": 29390
    },
    {
      "epoch": 2.3576583801122695,
      "grad_norm": 2.2884323596954346,
      "learning_rate": 6.4486732779415704e-06,
      "loss": 0.6965,
      "step": 29400
    },
    {
      "epoch": 2.3584603047313553,
      "grad_norm": 1.9133102893829346,
      "learning_rate": 6.440632538193514e-06,
      "loss": 0.57,
      "step": 29410
    },
    {
      "epoch": 2.359262229350441,
      "grad_norm": 2.2036964893341064,
      "learning_rate": 6.432591798445457e-06,
      "loss": 0.6155,
      "step": 29420
    },
    {
      "epoch": 2.360064153969527,
      "grad_norm": 1.8822811841964722,
      "learning_rate": 6.424551058697401e-06,
      "loss": 0.5479,
      "step": 29430
    },
    {
      "epoch": 2.3608660785886126,
      "grad_norm": 1.7462962865829468,
      "learning_rate": 6.416510318949343e-06,
      "loss": 0.6195,
      "step": 29440
    },
    {
      "epoch": 2.3616680032076984,
      "grad_norm": 1.9256702661514282,
      "learning_rate": 6.408469579201287e-06,
      "loss": 0.5902,
      "step": 29450
    },
    {
      "epoch": 2.362469927826784,
      "grad_norm": 1.615444540977478,
      "learning_rate": 6.400428839453229e-06,
      "loss": 0.6416,
      "step": 29460
    },
    {
      "epoch": 2.36327185244587,
      "grad_norm": 1.886233925819397,
      "learning_rate": 6.392388099705173e-06,
      "loss": 0.5895,
      "step": 29470
    },
    {
      "epoch": 2.364073777064956,
      "grad_norm": 1.740344524383545,
      "learning_rate": 6.384347359957116e-06,
      "loss": 0.6268,
      "step": 29480
    },
    {
      "epoch": 2.3648757016840416,
      "grad_norm": 1.8435384035110474,
      "learning_rate": 6.37630662020906e-06,
      "loss": 0.562,
      "step": 29490
    },
    {
      "epoch": 2.3656776263031274,
      "grad_norm": 1.8090074062347412,
      "learning_rate": 6.368265880461003e-06,
      "loss": 0.6403,
      "step": 29500
    },
    {
      "epoch": 2.366479550922213,
      "grad_norm": 1.8288357257843018,
      "learning_rate": 6.360225140712946e-06,
      "loss": 0.6447,
      "step": 29510
    },
    {
      "epoch": 2.367281475541299,
      "grad_norm": 1.7312300205230713,
      "learning_rate": 6.352184400964888e-06,
      "loss": 0.5693,
      "step": 29520
    },
    {
      "epoch": 2.3680834001603848,
      "grad_norm": 1.8817768096923828,
      "learning_rate": 6.344143661216832e-06,
      "loss": 0.6067,
      "step": 29530
    },
    {
      "epoch": 2.3688853247794706,
      "grad_norm": 1.8859052658081055,
      "learning_rate": 6.336102921468775e-06,
      "loss": 0.5535,
      "step": 29540
    },
    {
      "epoch": 2.3696872493985564,
      "grad_norm": 2.1937966346740723,
      "learning_rate": 6.328062181720719e-06,
      "loss": 0.6044,
      "step": 29550
    },
    {
      "epoch": 2.370489174017642,
      "grad_norm": 1.6807105541229248,
      "learning_rate": 6.320021441972662e-06,
      "loss": 0.6014,
      "step": 29560
    },
    {
      "epoch": 2.3712910986367284,
      "grad_norm": 2.1312637329101562,
      "learning_rate": 6.311980702224605e-06,
      "loss": 0.6844,
      "step": 29570
    },
    {
      "epoch": 2.3720930232558137,
      "grad_norm": 1.7671489715576172,
      "learning_rate": 6.303939962476548e-06,
      "loss": 0.5934,
      "step": 29580
    },
    {
      "epoch": 2.3728949478749,
      "grad_norm": 1.7710436582565308,
      "learning_rate": 6.295899222728491e-06,
      "loss": 0.535,
      "step": 29590
    },
    {
      "epoch": 2.3736968724939858,
      "grad_norm": 2.0218372344970703,
      "learning_rate": 6.287858482980434e-06,
      "loss": 0.6314,
      "step": 29600
    },
    {
      "epoch": 2.3744987971130715,
      "grad_norm": 1.794454574584961,
      "learning_rate": 6.279817743232378e-06,
      "loss": 0.5792,
      "step": 29610
    },
    {
      "epoch": 2.3753007217321573,
      "grad_norm": 1.5623180866241455,
      "learning_rate": 6.271777003484321e-06,
      "loss": 0.6094,
      "step": 29620
    },
    {
      "epoch": 2.376102646351243,
      "grad_norm": 1.848748803138733,
      "learning_rate": 6.2637362637362645e-06,
      "loss": 0.5855,
      "step": 29630
    },
    {
      "epoch": 2.376904570970329,
      "grad_norm": 1.8394209146499634,
      "learning_rate": 6.255695523988207e-06,
      "loss": 0.5279,
      "step": 29640
    },
    {
      "epoch": 2.3777064955894147,
      "grad_norm": 1.8645024299621582,
      "learning_rate": 6.24765478424015e-06,
      "loss": 0.547,
      "step": 29650
    },
    {
      "epoch": 2.3785084202085005,
      "grad_norm": 1.7708443403244019,
      "learning_rate": 6.239614044492093e-06,
      "loss": 0.5984,
      "step": 29660
    },
    {
      "epoch": 2.3793103448275863,
      "grad_norm": 1.9801708459854126,
      "learning_rate": 6.231573304744037e-06,
      "loss": 0.629,
      "step": 29670
    },
    {
      "epoch": 2.380112269446672,
      "grad_norm": 2.1013987064361572,
      "learning_rate": 6.22353256499598e-06,
      "loss": 0.5643,
      "step": 29680
    },
    {
      "epoch": 2.380914194065758,
      "grad_norm": 1.9605329036712646,
      "learning_rate": 6.2154918252479235e-06,
      "loss": 0.5918,
      "step": 29690
    },
    {
      "epoch": 2.3817161186848437,
      "grad_norm": 1.7319748401641846,
      "learning_rate": 6.207451085499866e-06,
      "loss": 0.5442,
      "step": 29700
    },
    {
      "epoch": 2.3825180433039295,
      "grad_norm": 1.8687776327133179,
      "learning_rate": 6.199410345751809e-06,
      "loss": 0.6631,
      "step": 29710
    },
    {
      "epoch": 2.3833199679230153,
      "grad_norm": 2.0954113006591797,
      "learning_rate": 6.191369606003752e-06,
      "loss": 0.6243,
      "step": 29720
    },
    {
      "epoch": 2.384121892542101,
      "grad_norm": 2.072331190109253,
      "learning_rate": 6.183328866255696e-06,
      "loss": 0.5722,
      "step": 29730
    },
    {
      "epoch": 2.384923817161187,
      "grad_norm": 1.813993215560913,
      "learning_rate": 6.175288126507639e-06,
      "loss": 0.5746,
      "step": 29740
    },
    {
      "epoch": 2.3857257417802726,
      "grad_norm": 1.721771240234375,
      "learning_rate": 6.1672473867595825e-06,
      "loss": 0.5972,
      "step": 29750
    },
    {
      "epoch": 2.3865276663993584,
      "grad_norm": 1.8826665878295898,
      "learning_rate": 6.159206647011525e-06,
      "loss": 0.6211,
      "step": 29760
    },
    {
      "epoch": 2.387329591018444,
      "grad_norm": 1.6676887273788452,
      "learning_rate": 6.1511659072634685e-06,
      "loss": 0.631,
      "step": 29770
    },
    {
      "epoch": 2.38813151563753,
      "grad_norm": 1.975753903388977,
      "learning_rate": 6.143125167515411e-06,
      "loss": 0.6097,
      "step": 29780
    },
    {
      "epoch": 2.388933440256616,
      "grad_norm": 1.8215787410736084,
      "learning_rate": 6.1350844277673545e-06,
      "loss": 0.5772,
      "step": 29790
    },
    {
      "epoch": 2.3897353648757016,
      "grad_norm": 2.22532057762146,
      "learning_rate": 6.127043688019298e-06,
      "loss": 0.6376,
      "step": 29800
    },
    {
      "epoch": 2.3905372894947874,
      "grad_norm": 1.8164054155349731,
      "learning_rate": 6.1190029482712414e-06,
      "loss": 0.5968,
      "step": 29810
    },
    {
      "epoch": 2.391339214113873,
      "grad_norm": 1.9000688791275024,
      "learning_rate": 6.110962208523185e-06,
      "loss": 0.5848,
      "step": 29820
    },
    {
      "epoch": 2.392141138732959,
      "grad_norm": 2.0003442764282227,
      "learning_rate": 6.1029214687751275e-06,
      "loss": 0.7021,
      "step": 29830
    },
    {
      "epoch": 2.3929430633520448,
      "grad_norm": 1.8099535703659058,
      "learning_rate": 6.09488072902707e-06,
      "loss": 0.6092,
      "step": 29840
    },
    {
      "epoch": 2.3937449879711306,
      "grad_norm": 2.4185092449188232,
      "learning_rate": 6.0868399892790135e-06,
      "loss": 0.6203,
      "step": 29850
    },
    {
      "epoch": 2.3945469125902163,
      "grad_norm": 1.9103246927261353,
      "learning_rate": 6.078799249530957e-06,
      "loss": 0.5422,
      "step": 29860
    },
    {
      "epoch": 2.395348837209302,
      "grad_norm": 1.91151762008667,
      "learning_rate": 6.0707585097829e-06,
      "loss": 0.6757,
      "step": 29870
    },
    {
      "epoch": 2.3961507618283884,
      "grad_norm": 1.8266438245773315,
      "learning_rate": 6.062717770034844e-06,
      "loss": 0.5869,
      "step": 29880
    },
    {
      "epoch": 2.3969526864474737,
      "grad_norm": 1.536070704460144,
      "learning_rate": 6.0546770302867864e-06,
      "loss": 0.6415,
      "step": 29890
    },
    {
      "epoch": 2.39775461106656,
      "grad_norm": 1.6033529043197632,
      "learning_rate": 6.04663629053873e-06,
      "loss": 0.5485,
      "step": 29900
    },
    {
      "epoch": 2.3985565356856453,
      "grad_norm": 1.7245596647262573,
      "learning_rate": 6.0385955507906725e-06,
      "loss": 0.613,
      "step": 29910
    },
    {
      "epoch": 2.3993584603047315,
      "grad_norm": 1.896815299987793,
      "learning_rate": 6.030554811042616e-06,
      "loss": 0.5483,
      "step": 29920
    },
    {
      "epoch": 2.4001603849238173,
      "grad_norm": 1.7739452123641968,
      "learning_rate": 6.022514071294559e-06,
      "loss": 0.6727,
      "step": 29930
    },
    {
      "epoch": 2.400962309542903,
      "grad_norm": 1.816879153251648,
      "learning_rate": 6.014473331546503e-06,
      "loss": 0.5509,
      "step": 29940
    },
    {
      "epoch": 2.401764234161989,
      "grad_norm": 2.1576836109161377,
      "learning_rate": 6.006432591798446e-06,
      "loss": 0.6328,
      "step": 29950
    },
    {
      "epoch": 2.4025661587810747,
      "grad_norm": 1.9598608016967773,
      "learning_rate": 5.998391852050389e-06,
      "loss": 0.6079,
      "step": 29960
    },
    {
      "epoch": 2.4033680834001605,
      "grad_norm": 1.8217453956604004,
      "learning_rate": 5.9903511123023314e-06,
      "loss": 0.603,
      "step": 29970
    },
    {
      "epoch": 2.4041700080192463,
      "grad_norm": 2.1888537406921387,
      "learning_rate": 5.982310372554275e-06,
      "loss": 0.6116,
      "step": 29980
    },
    {
      "epoch": 2.404971932638332,
      "grad_norm": 2.416801691055298,
      "learning_rate": 5.974269632806218e-06,
      "loss": 0.5718,
      "step": 29990
    },
    {
      "epoch": 2.405773857257418,
      "grad_norm": 1.8179582357406616,
      "learning_rate": 5.966228893058162e-06,
      "loss": 0.5931,
      "step": 30000
    },
    {
      "epoch": 2.4065757818765037,
      "grad_norm": 1.9551728963851929,
      "learning_rate": 5.958188153310105e-06,
      "loss": 0.6164,
      "step": 30010
    },
    {
      "epoch": 2.4073777064955895,
      "grad_norm": 2.000823497772217,
      "learning_rate": 5.950147413562048e-06,
      "loss": 0.6546,
      "step": 30020
    },
    {
      "epoch": 2.4081796311146753,
      "grad_norm": 1.8591899871826172,
      "learning_rate": 5.94210667381399e-06,
      "loss": 0.62,
      "step": 30030
    },
    {
      "epoch": 2.408981555733761,
      "grad_norm": 2.3659658432006836,
      "learning_rate": 5.934065934065934e-06,
      "loss": 0.6942,
      "step": 30040
    },
    {
      "epoch": 2.409783480352847,
      "grad_norm": 1.6302376985549927,
      "learning_rate": 5.926025194317877e-06,
      "loss": 0.5464,
      "step": 30050
    },
    {
      "epoch": 2.4105854049719326,
      "grad_norm": 1.900294303894043,
      "learning_rate": 5.917984454569821e-06,
      "loss": 0.5796,
      "step": 30060
    },
    {
      "epoch": 2.4113873295910184,
      "grad_norm": 1.991676688194275,
      "learning_rate": 5.909943714821764e-06,
      "loss": 0.604,
      "step": 30070
    },
    {
      "epoch": 2.412189254210104,
      "grad_norm": 2.174595355987549,
      "learning_rate": 5.901902975073707e-06,
      "loss": 0.5897,
      "step": 30080
    },
    {
      "epoch": 2.41299117882919,
      "grad_norm": 2.415179491043091,
      "learning_rate": 5.89386223532565e-06,
      "loss": 0.6027,
      "step": 30090
    },
    {
      "epoch": 2.413793103448276,
      "grad_norm": 1.7864125967025757,
      "learning_rate": 5.885821495577593e-06,
      "loss": 0.5915,
      "step": 30100
    },
    {
      "epoch": 2.4145950280673616,
      "grad_norm": 2.229764223098755,
      "learning_rate": 5.877780755829536e-06,
      "loss": 0.6336,
      "step": 30110
    },
    {
      "epoch": 2.4153969526864474,
      "grad_norm": 1.9912625551223755,
      "learning_rate": 5.86974001608148e-06,
      "loss": 0.5825,
      "step": 30120
    },
    {
      "epoch": 2.416198877305533,
      "grad_norm": 2.127354383468628,
      "learning_rate": 5.861699276333423e-06,
      "loss": 0.5604,
      "step": 30130
    },
    {
      "epoch": 2.417000801924619,
      "grad_norm": 1.5447075366973877,
      "learning_rate": 5.853658536585367e-06,
      "loss": 0.6009,
      "step": 30140
    },
    {
      "epoch": 2.4178027265437048,
      "grad_norm": 1.9402185678482056,
      "learning_rate": 5.845617796837309e-06,
      "loss": 0.6046,
      "step": 30150
    },
    {
      "epoch": 2.4186046511627906,
      "grad_norm": 1.7411373853683472,
      "learning_rate": 5.837577057089252e-06,
      "loss": 0.5707,
      "step": 30160
    },
    {
      "epoch": 2.4194065757818763,
      "grad_norm": 1.8484363555908203,
      "learning_rate": 5.829536317341195e-06,
      "loss": 0.6392,
      "step": 30170
    },
    {
      "epoch": 2.420208500400962,
      "grad_norm": 1.8148225545883179,
      "learning_rate": 5.821495577593139e-06,
      "loss": 0.6667,
      "step": 30180
    },
    {
      "epoch": 2.421010425020048,
      "grad_norm": 1.7671959400177002,
      "learning_rate": 5.813454837845082e-06,
      "loss": 0.5396,
      "step": 30190
    },
    {
      "epoch": 2.4218123496391337,
      "grad_norm": 1.8500298261642456,
      "learning_rate": 5.8054140980970255e-06,
      "loss": 0.5936,
      "step": 30200
    },
    {
      "epoch": 2.42261427425822,
      "grad_norm": 1.5831794738769531,
      "learning_rate": 5.797373358348968e-06,
      "loss": 0.6243,
      "step": 30210
    },
    {
      "epoch": 2.4234161988773053,
      "grad_norm": 2.172687292098999,
      "learning_rate": 5.789332618600912e-06,
      "loss": 0.6254,
      "step": 30220
    },
    {
      "epoch": 2.4242181234963915,
      "grad_norm": 2.0452799797058105,
      "learning_rate": 5.781291878852854e-06,
      "loss": 0.6184,
      "step": 30230
    },
    {
      "epoch": 2.4250200481154773,
      "grad_norm": 1.7079601287841797,
      "learning_rate": 5.773251139104798e-06,
      "loss": 0.5689,
      "step": 30240
    },
    {
      "epoch": 2.425821972734563,
      "grad_norm": 2.242523193359375,
      "learning_rate": 5.765210399356741e-06,
      "loss": 0.6173,
      "step": 30250
    },
    {
      "epoch": 2.426623897353649,
      "grad_norm": 1.8920509815216064,
      "learning_rate": 5.7571696596086845e-06,
      "loss": 0.6099,
      "step": 30260
    },
    {
      "epoch": 2.4274258219727347,
      "grad_norm": 2.0534613132476807,
      "learning_rate": 5.749128919860628e-06,
      "loss": 0.5854,
      "step": 30270
    },
    {
      "epoch": 2.4282277465918205,
      "grad_norm": 1.591917872428894,
      "learning_rate": 5.7410881801125705e-06,
      "loss": 0.5754,
      "step": 30280
    },
    {
      "epoch": 2.4290296712109063,
      "grad_norm": 2.2388389110565186,
      "learning_rate": 5.733047440364513e-06,
      "loss": 0.5575,
      "step": 30290
    },
    {
      "epoch": 2.429831595829992,
      "grad_norm": 1.850765585899353,
      "learning_rate": 5.725006700616457e-06,
      "loss": 0.6933,
      "step": 30300
    },
    {
      "epoch": 2.430633520449078,
      "grad_norm": 1.8542522192001343,
      "learning_rate": 5.7169659608684e-06,
      "loss": 0.6172,
      "step": 30310
    },
    {
      "epoch": 2.4314354450681637,
      "grad_norm": 2.0742435455322266,
      "learning_rate": 5.7089252211203435e-06,
      "loss": 0.5802,
      "step": 30320
    },
    {
      "epoch": 2.4322373696872495,
      "grad_norm": 1.8501266241073608,
      "learning_rate": 5.700884481372287e-06,
      "loss": 0.6157,
      "step": 30330
    },
    {
      "epoch": 2.4330392943063353,
      "grad_norm": 1.709868311882019,
      "learning_rate": 5.6928437416242295e-06,
      "loss": 0.5929,
      "step": 30340
    },
    {
      "epoch": 2.433841218925421,
      "grad_norm": 2.1932930946350098,
      "learning_rate": 5.684803001876172e-06,
      "loss": 0.6294,
      "step": 30350
    },
    {
      "epoch": 2.434643143544507,
      "grad_norm": 2.1239240169525146,
      "learning_rate": 5.6767622621281156e-06,
      "loss": 0.5955,
      "step": 30360
    },
    {
      "epoch": 2.4354450681635926,
      "grad_norm": 2.355520486831665,
      "learning_rate": 5.668721522380059e-06,
      "loss": 0.5886,
      "step": 30370
    },
    {
      "epoch": 2.4362469927826784,
      "grad_norm": 2.0241177082061768,
      "learning_rate": 5.6606807826320024e-06,
      "loss": 0.6103,
      "step": 30380
    },
    {
      "epoch": 2.437048917401764,
      "grad_norm": 1.9085580110549927,
      "learning_rate": 5.652640042883946e-06,
      "loss": 0.5819,
      "step": 30390
    },
    {
      "epoch": 2.43785084202085,
      "grad_norm": 2.16153621673584,
      "learning_rate": 5.644599303135889e-06,
      "loss": 0.5769,
      "step": 30400
    },
    {
      "epoch": 2.438652766639936,
      "grad_norm": 2.2087039947509766,
      "learning_rate": 5.636558563387832e-06,
      "loss": 0.6529,
      "step": 30410
    },
    {
      "epoch": 2.4394546912590216,
      "grad_norm": 2.120204210281372,
      "learning_rate": 5.6285178236397745e-06,
      "loss": 0.5764,
      "step": 30420
    },
    {
      "epoch": 2.4402566158781074,
      "grad_norm": 1.5996458530426025,
      "learning_rate": 5.620477083891718e-06,
      "loss": 0.5672,
      "step": 30430
    },
    {
      "epoch": 2.441058540497193,
      "grad_norm": 1.9447499513626099,
      "learning_rate": 5.612436344143661e-06,
      "loss": 0.5868,
      "step": 30440
    },
    {
      "epoch": 2.441860465116279,
      "grad_norm": 1.80408775806427,
      "learning_rate": 5.604395604395605e-06,
      "loss": 0.6981,
      "step": 30450
    },
    {
      "epoch": 2.4426623897353648,
      "grad_norm": 2.0977187156677246,
      "learning_rate": 5.596354864647548e-06,
      "loss": 0.6071,
      "step": 30460
    },
    {
      "epoch": 2.4434643143544506,
      "grad_norm": 2.2967138290405273,
      "learning_rate": 5.588314124899491e-06,
      "loss": 0.5944,
      "step": 30470
    },
    {
      "epoch": 2.4442662389735363,
      "grad_norm": 1.7148020267486572,
      "learning_rate": 5.5802733851514335e-06,
      "loss": 0.5517,
      "step": 30480
    },
    {
      "epoch": 2.445068163592622,
      "grad_norm": 1.8688207864761353,
      "learning_rate": 5.572232645403377e-06,
      "loss": 0.5891,
      "step": 30490
    },
    {
      "epoch": 2.445870088211708,
      "grad_norm": 1.9468191862106323,
      "learning_rate": 5.56419190565532e-06,
      "loss": 0.603,
      "step": 30500
    },
    {
      "epoch": 2.4466720128307937,
      "grad_norm": 1.7536686658859253,
      "learning_rate": 5.556151165907264e-06,
      "loss": 0.6261,
      "step": 30510
    },
    {
      "epoch": 2.44747393744988,
      "grad_norm": 2.0837955474853516,
      "learning_rate": 5.548110426159207e-06,
      "loss": 0.6063,
      "step": 30520
    },
    {
      "epoch": 2.4482758620689653,
      "grad_norm": 1.8229116201400757,
      "learning_rate": 5.54006968641115e-06,
      "loss": 0.655,
      "step": 30530
    },
    {
      "epoch": 2.4490777866880515,
      "grad_norm": 1.9622962474822998,
      "learning_rate": 5.532028946663093e-06,
      "loss": 0.6147,
      "step": 30540
    },
    {
      "epoch": 2.449879711307137,
      "grad_norm": 1.7008768320083618,
      "learning_rate": 5.523988206915036e-06,
      "loss": 0.624,
      "step": 30550
    },
    {
      "epoch": 2.450681635926223,
      "grad_norm": 1.9580072164535522,
      "learning_rate": 5.515947467166979e-06,
      "loss": 0.6013,
      "step": 30560
    },
    {
      "epoch": 2.451483560545309,
      "grad_norm": 1.737690806388855,
      "learning_rate": 5.507906727418923e-06,
      "loss": 0.5574,
      "step": 30570
    },
    {
      "epoch": 2.4522854851643947,
      "grad_norm": 1.908284306526184,
      "learning_rate": 5.499865987670866e-06,
      "loss": 0.6183,
      "step": 30580
    },
    {
      "epoch": 2.4530874097834805,
      "grad_norm": 1.502858281135559,
      "learning_rate": 5.49182524792281e-06,
      "loss": 0.5902,
      "step": 30590
    },
    {
      "epoch": 2.4538893344025663,
      "grad_norm": 1.978886365890503,
      "learning_rate": 5.483784508174752e-06,
      "loss": 0.5612,
      "step": 30600
    },
    {
      "epoch": 2.454691259021652,
      "grad_norm": 2.06242299079895,
      "learning_rate": 5.475743768426695e-06,
      "loss": 0.6573,
      "step": 30610
    },
    {
      "epoch": 2.455493183640738,
      "grad_norm": 1.8464694023132324,
      "learning_rate": 5.467703028678638e-06,
      "loss": 0.6205,
      "step": 30620
    },
    {
      "epoch": 2.4562951082598237,
      "grad_norm": 2.0276291370391846,
      "learning_rate": 5.459662288930582e-06,
      "loss": 0.5497,
      "step": 30630
    },
    {
      "epoch": 2.4570970328789095,
      "grad_norm": 1.912367820739746,
      "learning_rate": 5.451621549182525e-06,
      "loss": 0.5688,
      "step": 30640
    },
    {
      "epoch": 2.4578989574979953,
      "grad_norm": 1.8830432891845703,
      "learning_rate": 5.443580809434469e-06,
      "loss": 0.5511,
      "step": 30650
    },
    {
      "epoch": 2.458700882117081,
      "grad_norm": 2.001680850982666,
      "learning_rate": 5.435540069686411e-06,
      "loss": 0.5864,
      "step": 30660
    },
    {
      "epoch": 2.459502806736167,
      "grad_norm": 1.609394907951355,
      "learning_rate": 5.427499329938354e-06,
      "loss": 0.5183,
      "step": 30670
    },
    {
      "epoch": 2.4603047313552526,
      "grad_norm": 1.9815647602081299,
      "learning_rate": 5.419458590190297e-06,
      "loss": 0.5811,
      "step": 30680
    },
    {
      "epoch": 2.4611066559743384,
      "grad_norm": 1.5949891805648804,
      "learning_rate": 5.411417850442241e-06,
      "loss": 0.613,
      "step": 30690
    },
    {
      "epoch": 2.461908580593424,
      "grad_norm": 2.042994260787964,
      "learning_rate": 5.403377110694184e-06,
      "loss": 0.6407,
      "step": 30700
    },
    {
      "epoch": 2.46271050521251,
      "grad_norm": 1.7877699136734009,
      "learning_rate": 5.395336370946128e-06,
      "loss": 0.5834,
      "step": 30710
    },
    {
      "epoch": 2.463512429831596,
      "grad_norm": 1.9760403633117676,
      "learning_rate": 5.387295631198071e-06,
      "loss": 0.5805,
      "step": 30720
    },
    {
      "epoch": 2.4643143544506816,
      "grad_norm": 2.1512489318847656,
      "learning_rate": 5.379254891450014e-06,
      "loss": 0.6403,
      "step": 30730
    },
    {
      "epoch": 2.4651162790697674,
      "grad_norm": 1.7230844497680664,
      "learning_rate": 5.371214151701956e-06,
      "loss": 0.6241,
      "step": 30740
    },
    {
      "epoch": 2.465918203688853,
      "grad_norm": 1.8057011365890503,
      "learning_rate": 5.3631734119539e-06,
      "loss": 0.639,
      "step": 30750
    },
    {
      "epoch": 2.466720128307939,
      "grad_norm": 1.8929091691970825,
      "learning_rate": 5.355132672205843e-06,
      "loss": 0.5936,
      "step": 30760
    },
    {
      "epoch": 2.4675220529270248,
      "grad_norm": 2.295877456665039,
      "learning_rate": 5.3470919324577866e-06,
      "loss": 0.57,
      "step": 30770
    },
    {
      "epoch": 2.4683239775461105,
      "grad_norm": 1.788917899131775,
      "learning_rate": 5.33905119270973e-06,
      "loss": 0.6115,
      "step": 30780
    },
    {
      "epoch": 2.4691259021651963,
      "grad_norm": 1.7885316610336304,
      "learning_rate": 5.331010452961673e-06,
      "loss": 0.62,
      "step": 30790
    },
    {
      "epoch": 2.469927826784282,
      "grad_norm": 2.0310277938842773,
      "learning_rate": 5.322969713213615e-06,
      "loss": 0.6149,
      "step": 30800
    },
    {
      "epoch": 2.470729751403368,
      "grad_norm": 1.8994951248168945,
      "learning_rate": 5.314928973465559e-06,
      "loss": 0.6181,
      "step": 30810
    },
    {
      "epoch": 2.4715316760224537,
      "grad_norm": 2.1632425785064697,
      "learning_rate": 5.306888233717502e-06,
      "loss": 0.6227,
      "step": 30820
    },
    {
      "epoch": 2.4723336006415395,
      "grad_norm": 2.0741164684295654,
      "learning_rate": 5.2988474939694455e-06,
      "loss": 0.5742,
      "step": 30830
    },
    {
      "epoch": 2.4731355252606253,
      "grad_norm": 1.7418991327285767,
      "learning_rate": 5.290806754221389e-06,
      "loss": 0.611,
      "step": 30840
    },
    {
      "epoch": 2.4739374498797115,
      "grad_norm": 1.9511140584945679,
      "learning_rate": 5.2827660144733316e-06,
      "loss": 0.63,
      "step": 30850
    },
    {
      "epoch": 2.474739374498797,
      "grad_norm": 1.7996084690093994,
      "learning_rate": 5.274725274725275e-06,
      "loss": 0.5896,
      "step": 30860
    },
    {
      "epoch": 2.475541299117883,
      "grad_norm": 1.839461326599121,
      "learning_rate": 5.266684534977218e-06,
      "loss": 0.6185,
      "step": 30870
    },
    {
      "epoch": 2.476343223736969,
      "grad_norm": 1.9892674684524536,
      "learning_rate": 5.258643795229161e-06,
      "loss": 0.618,
      "step": 30880
    },
    {
      "epoch": 2.4771451483560547,
      "grad_norm": 1.8831312656402588,
      "learning_rate": 5.2506030554811045e-06,
      "loss": 0.6073,
      "step": 30890
    },
    {
      "epoch": 2.4779470729751405,
      "grad_norm": 2.145620822906494,
      "learning_rate": 5.242562315733048e-06,
      "loss": 0.6672,
      "step": 30900
    },
    {
      "epoch": 2.4787489975942263,
      "grad_norm": 2.2945263385772705,
      "learning_rate": 5.234521575984991e-06,
      "loss": 0.622,
      "step": 30910
    },
    {
      "epoch": 2.479550922213312,
      "grad_norm": 2.328866481781006,
      "learning_rate": 5.226480836236934e-06,
      "loss": 0.6591,
      "step": 30920
    },
    {
      "epoch": 2.480352846832398,
      "grad_norm": 1.6534020900726318,
      "learning_rate": 5.2184400964888766e-06,
      "loss": 0.6177,
      "step": 30930
    },
    {
      "epoch": 2.4811547714514837,
      "grad_norm": 1.8761019706726074,
      "learning_rate": 5.21039935674082e-06,
      "loss": 0.6885,
      "step": 30940
    },
    {
      "epoch": 2.4819566960705695,
      "grad_norm": 1.7476752996444702,
      "learning_rate": 5.2023586169927634e-06,
      "loss": 0.6233,
      "step": 30950
    },
    {
      "epoch": 2.4827586206896552,
      "grad_norm": 1.887252926826477,
      "learning_rate": 5.194317877244707e-06,
      "loss": 0.6692,
      "step": 30960
    },
    {
      "epoch": 2.483560545308741,
      "grad_norm": 1.8063366413116455,
      "learning_rate": 5.18627713749665e-06,
      "loss": 0.6535,
      "step": 30970
    },
    {
      "epoch": 2.484362469927827,
      "grad_norm": 1.977231502532959,
      "learning_rate": 5.178236397748593e-06,
      "loss": 0.6553,
      "step": 30980
    },
    {
      "epoch": 2.4851643945469126,
      "grad_norm": 1.8521335124969482,
      "learning_rate": 5.1701956580005355e-06,
      "loss": 0.6051,
      "step": 30990
    },
    {
      "epoch": 2.4859663191659984,
      "grad_norm": 1.8025119304656982,
      "learning_rate": 5.162154918252479e-06,
      "loss": 0.6128,
      "step": 31000
    },
    {
      "epoch": 2.486768243785084,
      "grad_norm": 1.8435207605361938,
      "learning_rate": 5.154114178504422e-06,
      "loss": 0.5592,
      "step": 31010
    },
    {
      "epoch": 2.48757016840417,
      "grad_norm": 1.9848484992980957,
      "learning_rate": 5.146073438756366e-06,
      "loss": 0.62,
      "step": 31020
    },
    {
      "epoch": 2.488372093023256,
      "grad_norm": 2.022026777267456,
      "learning_rate": 5.138032699008309e-06,
      "loss": 0.5528,
      "step": 31030
    },
    {
      "epoch": 2.4891740176423416,
      "grad_norm": 1.800635814666748,
      "learning_rate": 5.129991959260253e-06,
      "loss": 0.5828,
      "step": 31040
    },
    {
      "epoch": 2.4899759422614274,
      "grad_norm": 2.284276008605957,
      "learning_rate": 5.121951219512195e-06,
      "loss": 0.6314,
      "step": 31050
    },
    {
      "epoch": 2.490777866880513,
      "grad_norm": 2.174586057662964,
      "learning_rate": 5.113910479764138e-06,
      "loss": 0.5803,
      "step": 31060
    },
    {
      "epoch": 2.491579791499599,
      "grad_norm": 1.9684627056121826,
      "learning_rate": 5.105869740016081e-06,
      "loss": 0.5904,
      "step": 31070
    },
    {
      "epoch": 2.4923817161186848,
      "grad_norm": 1.9426189661026,
      "learning_rate": 5.097829000268025e-06,
      "loss": 0.619,
      "step": 31080
    },
    {
      "epoch": 2.4931836407377705,
      "grad_norm": 1.837042212486267,
      "learning_rate": 5.089788260519968e-06,
      "loss": 0.6783,
      "step": 31090
    },
    {
      "epoch": 2.4939855653568563,
      "grad_norm": 1.8166745901107788,
      "learning_rate": 5.081747520771912e-06,
      "loss": 0.5869,
      "step": 31100
    },
    {
      "epoch": 2.494787489975942,
      "grad_norm": 1.670594573020935,
      "learning_rate": 5.073706781023854e-06,
      "loss": 0.5876,
      "step": 31110
    },
    {
      "epoch": 2.495589414595028,
      "grad_norm": 2.1846840381622314,
      "learning_rate": 5.065666041275797e-06,
      "loss": 0.6031,
      "step": 31120
    },
    {
      "epoch": 2.4963913392141137,
      "grad_norm": 1.9272347688674927,
      "learning_rate": 5.05762530152774e-06,
      "loss": 0.6024,
      "step": 31130
    },
    {
      "epoch": 2.4971932638331995,
      "grad_norm": 1.8037341833114624,
      "learning_rate": 5.049584561779684e-06,
      "loss": 0.6129,
      "step": 31140
    },
    {
      "epoch": 2.4979951884522853,
      "grad_norm": 2.00703763961792,
      "learning_rate": 5.041543822031627e-06,
      "loss": 0.6133,
      "step": 31150
    },
    {
      "epoch": 2.4987971130713715,
      "grad_norm": 1.7340267896652222,
      "learning_rate": 5.033503082283571e-06,
      "loss": 0.5579,
      "step": 31160
    },
    {
      "epoch": 2.499599037690457,
      "grad_norm": 2.1335647106170654,
      "learning_rate": 5.025462342535513e-06,
      "loss": 0.7381,
      "step": 31170
    },
    {
      "epoch": 2.500400962309543,
      "grad_norm": 2.433004856109619,
      "learning_rate": 5.017421602787457e-06,
      "loss": 0.5759,
      "step": 31180
    },
    {
      "epoch": 2.5012028869286285,
      "grad_norm": 2.023261070251465,
      "learning_rate": 5.009380863039399e-06,
      "loss": 0.5369,
      "step": 31190
    },
    {
      "epoch": 2.5020048115477147,
      "grad_norm": 1.923872709274292,
      "learning_rate": 5.001340123291343e-06,
      "loss": 0.5872,
      "step": 31200
    },
    {
      "epoch": 2.5028067361668,
      "grad_norm": 1.7362009286880493,
      "learning_rate": 4.993299383543286e-06,
      "loss": 0.5156,
      "step": 31210
    },
    {
      "epoch": 2.5036086607858863,
      "grad_norm": 2.104745864868164,
      "learning_rate": 4.98525864379523e-06,
      "loss": 0.5888,
      "step": 31220
    },
    {
      "epoch": 2.504410585404972,
      "grad_norm": 1.8496061563491821,
      "learning_rate": 4.977217904047173e-06,
      "loss": 0.6815,
      "step": 31230
    },
    {
      "epoch": 2.505212510024058,
      "grad_norm": 1.8225888013839722,
      "learning_rate": 4.969177164299116e-06,
      "loss": 0.6046,
      "step": 31240
    },
    {
      "epoch": 2.5060144346431437,
      "grad_norm": 2.1848511695861816,
      "learning_rate": 4.961136424551058e-06,
      "loss": 0.5352,
      "step": 31250
    },
    {
      "epoch": 2.5068163592622295,
      "grad_norm": 2.5636096000671387,
      "learning_rate": 4.953095684803002e-06,
      "loss": 0.5939,
      "step": 31260
    },
    {
      "epoch": 2.5076182838813152,
      "grad_norm": 2.1585586071014404,
      "learning_rate": 4.945054945054945e-06,
      "loss": 0.6135,
      "step": 31270
    },
    {
      "epoch": 2.508420208500401,
      "grad_norm": 1.9677400588989258,
      "learning_rate": 4.937014205306889e-06,
      "loss": 0.6945,
      "step": 31280
    },
    {
      "epoch": 2.509222133119487,
      "grad_norm": 2.139021873474121,
      "learning_rate": 4.928973465558832e-06,
      "loss": 0.5996,
      "step": 31290
    },
    {
      "epoch": 2.5100240577385726,
      "grad_norm": 1.6059545278549194,
      "learning_rate": 4.920932725810775e-06,
      "loss": 0.5874,
      "step": 31300
    },
    {
      "epoch": 2.5108259823576584,
      "grad_norm": 1.8855966329574585,
      "learning_rate": 4.913696060037523e-06,
      "loss": 0.6298,
      "step": 31310
    },
    {
      "epoch": 2.511627906976744,
      "grad_norm": 1.7059414386749268,
      "learning_rate": 4.905655320289467e-06,
      "loss": 0.5806,
      "step": 31320
    },
    {
      "epoch": 2.51242983159583,
      "grad_norm": 1.9550786018371582,
      "learning_rate": 4.89761458054141e-06,
      "loss": 0.5477,
      "step": 31330
    },
    {
      "epoch": 2.513231756214916,
      "grad_norm": 1.837897777557373,
      "learning_rate": 4.889573840793354e-06,
      "loss": 0.7211,
      "step": 31340
    },
    {
      "epoch": 2.5140336808340016,
      "grad_norm": 1.9975885152816772,
      "learning_rate": 4.881533101045296e-06,
      "loss": 0.5616,
      "step": 31350
    },
    {
      "epoch": 2.5148356054530874,
      "grad_norm": 1.9307441711425781,
      "learning_rate": 4.87349236129724e-06,
      "loss": 0.5963,
      "step": 31360
    },
    {
      "epoch": 2.515637530072173,
      "grad_norm": 1.815507411956787,
      "learning_rate": 4.865451621549182e-06,
      "loss": 0.6222,
      "step": 31370
    },
    {
      "epoch": 2.516439454691259,
      "grad_norm": 1.9238052368164062,
      "learning_rate": 4.857410881801126e-06,
      "loss": 0.5785,
      "step": 31380
    },
    {
      "epoch": 2.5172413793103448,
      "grad_norm": 1.9543464183807373,
      "learning_rate": 4.849370142053069e-06,
      "loss": 0.5957,
      "step": 31390
    },
    {
      "epoch": 2.5180433039294305,
      "grad_norm": 2.1300556659698486,
      "learning_rate": 4.841329402305013e-06,
      "loss": 0.5849,
      "step": 31400
    },
    {
      "epoch": 2.5188452285485163,
      "grad_norm": 1.7530403137207031,
      "learning_rate": 4.833288662556955e-06,
      "loss": 0.6473,
      "step": 31410
    },
    {
      "epoch": 2.519647153167602,
      "grad_norm": 1.8545610904693604,
      "learning_rate": 4.825247922808899e-06,
      "loss": 0.5891,
      "step": 31420
    },
    {
      "epoch": 2.520449077786688,
      "grad_norm": 2.1208534240722656,
      "learning_rate": 4.817207183060841e-06,
      "loss": 0.5845,
      "step": 31430
    },
    {
      "epoch": 2.5212510024057737,
      "grad_norm": 1.9921528100967407,
      "learning_rate": 4.809166443312785e-06,
      "loss": 0.5826,
      "step": 31440
    },
    {
      "epoch": 2.5220529270248595,
      "grad_norm": 1.7910569906234741,
      "learning_rate": 4.801125703564728e-06,
      "loss": 0.6106,
      "step": 31450
    },
    {
      "epoch": 2.5228548516439453,
      "grad_norm": 1.9725855588912964,
      "learning_rate": 4.7930849638166716e-06,
      "loss": 0.6117,
      "step": 31460
    },
    {
      "epoch": 2.5236567762630315,
      "grad_norm": 1.8692986965179443,
      "learning_rate": 4.785044224068614e-06,
      "loss": 0.6499,
      "step": 31470
    },
    {
      "epoch": 2.524458700882117,
      "grad_norm": 2.125636577606201,
      "learning_rate": 4.777003484320558e-06,
      "loss": 0.6944,
      "step": 31480
    },
    {
      "epoch": 2.525260625501203,
      "grad_norm": 1.7710930109024048,
      "learning_rate": 4.768962744572501e-06,
      "loss": 0.6402,
      "step": 31490
    },
    {
      "epoch": 2.5260625501202885,
      "grad_norm": 2.052485227584839,
      "learning_rate": 4.760922004824444e-06,
      "loss": 0.6227,
      "step": 31500
    },
    {
      "epoch": 2.5268644747393747,
      "grad_norm": 1.9614545106887817,
      "learning_rate": 4.752881265076387e-06,
      "loss": 0.5553,
      "step": 31510
    },
    {
      "epoch": 2.52766639935846,
      "grad_norm": 2.3043434619903564,
      "learning_rate": 4.7448405253283305e-06,
      "loss": 0.6243,
      "step": 31520
    },
    {
      "epoch": 2.5284683239775463,
      "grad_norm": 1.751222014427185,
      "learning_rate": 4.736799785580274e-06,
      "loss": 0.6016,
      "step": 31530
    },
    {
      "epoch": 2.529270248596632,
      "grad_norm": 1.7132072448730469,
      "learning_rate": 4.7287590458322166e-06,
      "loss": 0.5872,
      "step": 31540
    },
    {
      "epoch": 2.530072173215718,
      "grad_norm": 1.7676769495010376,
      "learning_rate": 4.72071830608416e-06,
      "loss": 0.5867,
      "step": 31550
    },
    {
      "epoch": 2.5308740978348037,
      "grad_norm": 1.813044548034668,
      "learning_rate": 4.712677566336103e-06,
      "loss": 0.7147,
      "step": 31560
    },
    {
      "epoch": 2.5316760224538895,
      "grad_norm": 1.7992666959762573,
      "learning_rate": 4.704636826588046e-06,
      "loss": 0.5676,
      "step": 31570
    },
    {
      "epoch": 2.5324779470729752,
      "grad_norm": 1.6877332925796509,
      "learning_rate": 4.6965960868399895e-06,
      "loss": 0.5612,
      "step": 31580
    },
    {
      "epoch": 2.533279871692061,
      "grad_norm": 2.0986971855163574,
      "learning_rate": 4.688555347091933e-06,
      "loss": 0.5439,
      "step": 31590
    },
    {
      "epoch": 2.534081796311147,
      "grad_norm": 2.0421488285064697,
      "learning_rate": 4.6805146073438755e-06,
      "loss": 0.6131,
      "step": 31600
    },
    {
      "epoch": 2.5348837209302326,
      "grad_norm": 1.683955192565918,
      "learning_rate": 4.672473867595819e-06,
      "loss": 0.5954,
      "step": 31610
    },
    {
      "epoch": 2.5356856455493184,
      "grad_norm": 1.6614511013031006,
      "learning_rate": 4.6644331278477616e-06,
      "loss": 0.5794,
      "step": 31620
    },
    {
      "epoch": 2.536487570168404,
      "grad_norm": 1.9876513481140137,
      "learning_rate": 4.656392388099705e-06,
      "loss": 0.6114,
      "step": 31630
    },
    {
      "epoch": 2.53728949478749,
      "grad_norm": 2.0795207023620605,
      "learning_rate": 4.6483516483516485e-06,
      "loss": 0.5495,
      "step": 31640
    },
    {
      "epoch": 2.538091419406576,
      "grad_norm": 1.8821344375610352,
      "learning_rate": 4.640310908603592e-06,
      "loss": 0.6451,
      "step": 31650
    },
    {
      "epoch": 2.5388933440256616,
      "grad_norm": 1.7766499519348145,
      "learning_rate": 4.632270168855535e-06,
      "loss": 0.5517,
      "step": 31660
    },
    {
      "epoch": 2.5396952686447474,
      "grad_norm": 2.369696855545044,
      "learning_rate": 4.624229429107478e-06,
      "loss": 0.6092,
      "step": 31670
    },
    {
      "epoch": 2.540497193263833,
      "grad_norm": 1.9434233903884888,
      "learning_rate": 4.616188689359421e-06,
      "loss": 0.6085,
      "step": 31680
    },
    {
      "epoch": 2.541299117882919,
      "grad_norm": 1.8253687620162964,
      "learning_rate": 4.608147949611364e-06,
      "loss": 0.5902,
      "step": 31690
    },
    {
      "epoch": 2.5421010425020047,
      "grad_norm": 1.8732575178146362,
      "learning_rate": 4.6001072098633074e-06,
      "loss": 0.6278,
      "step": 31700
    },
    {
      "epoch": 2.5429029671210905,
      "grad_norm": 1.9510058164596558,
      "learning_rate": 4.592066470115251e-06,
      "loss": 0.6312,
      "step": 31710
    },
    {
      "epoch": 2.5437048917401763,
      "grad_norm": 1.744484782218933,
      "learning_rate": 4.584025730367194e-06,
      "loss": 0.5397,
      "step": 31720
    },
    {
      "epoch": 2.544506816359262,
      "grad_norm": 2.272852897644043,
      "learning_rate": 4.575984990619137e-06,
      "loss": 0.5607,
      "step": 31730
    },
    {
      "epoch": 2.545308740978348,
      "grad_norm": 1.8806912899017334,
      "learning_rate": 4.56794425087108e-06,
      "loss": 0.6206,
      "step": 31740
    },
    {
      "epoch": 2.5461106655974337,
      "grad_norm": 1.7494595050811768,
      "learning_rate": 4.559903511123023e-06,
      "loss": 0.5174,
      "step": 31750
    },
    {
      "epoch": 2.5469125902165195,
      "grad_norm": 2.219010829925537,
      "learning_rate": 4.551862771374966e-06,
      "loss": 0.6456,
      "step": 31760
    },
    {
      "epoch": 2.5477145148356053,
      "grad_norm": 1.9351297616958618,
      "learning_rate": 4.54382203162691e-06,
      "loss": 0.5778,
      "step": 31770
    },
    {
      "epoch": 2.5485164394546915,
      "grad_norm": 1.5387552976608276,
      "learning_rate": 4.535781291878853e-06,
      "loss": 0.5505,
      "step": 31780
    },
    {
      "epoch": 2.549318364073777,
      "grad_norm": 2.2347428798675537,
      "learning_rate": 4.527740552130796e-06,
      "loss": 0.631,
      "step": 31790
    },
    {
      "epoch": 2.550120288692863,
      "grad_norm": 2.331634998321533,
      "learning_rate": 4.519699812382739e-06,
      "loss": 0.542,
      "step": 31800
    },
    {
      "epoch": 2.5509222133119485,
      "grad_norm": 1.8804364204406738,
      "learning_rate": 4.511659072634683e-06,
      "loss": 0.6233,
      "step": 31810
    },
    {
      "epoch": 2.5517241379310347,
      "grad_norm": 2.287755250930786,
      "learning_rate": 4.503618332886625e-06,
      "loss": 0.6083,
      "step": 31820
    },
    {
      "epoch": 2.55252606255012,
      "grad_norm": 1.7653542757034302,
      "learning_rate": 4.495577593138569e-06,
      "loss": 0.6188,
      "step": 31830
    },
    {
      "epoch": 2.5533279871692063,
      "grad_norm": 1.826124906539917,
      "learning_rate": 4.487536853390512e-06,
      "loss": 0.5736,
      "step": 31840
    },
    {
      "epoch": 2.5541299117882916,
      "grad_norm": 1.8032684326171875,
      "learning_rate": 4.479496113642456e-06,
      "loss": 0.6438,
      "step": 31850
    },
    {
      "epoch": 2.554931836407378,
      "grad_norm": 2.2048401832580566,
      "learning_rate": 4.471455373894398e-06,
      "loss": 0.5591,
      "step": 31860
    },
    {
      "epoch": 2.5557337610264637,
      "grad_norm": 1.459836721420288,
      "learning_rate": 4.463414634146342e-06,
      "loss": 0.5307,
      "step": 31870
    },
    {
      "epoch": 2.5565356856455494,
      "grad_norm": 2.0612235069274902,
      "learning_rate": 4.455373894398284e-06,
      "loss": 0.5906,
      "step": 31880
    },
    {
      "epoch": 2.5573376102646352,
      "grad_norm": 2.2169735431671143,
      "learning_rate": 4.447333154650228e-06,
      "loss": 0.5847,
      "step": 31890
    },
    {
      "epoch": 2.558139534883721,
      "grad_norm": 1.9042842388153076,
      "learning_rate": 4.439292414902171e-06,
      "loss": 0.5337,
      "step": 31900
    },
    {
      "epoch": 2.558941459502807,
      "grad_norm": 1.8754658699035645,
      "learning_rate": 4.431251675154115e-06,
      "loss": 0.5768,
      "step": 31910
    },
    {
      "epoch": 2.5597433841218926,
      "grad_norm": 1.7781224250793457,
      "learning_rate": 4.423210935406057e-06,
      "loss": 0.6377,
      "step": 31920
    },
    {
      "epoch": 2.5605453087409784,
      "grad_norm": 1.9377968311309814,
      "learning_rate": 4.415170195658001e-06,
      "loss": 0.6312,
      "step": 31930
    },
    {
      "epoch": 2.561347233360064,
      "grad_norm": 2.6132960319519043,
      "learning_rate": 4.407129455909944e-06,
      "loss": 0.6238,
      "step": 31940
    },
    {
      "epoch": 2.56214915797915,
      "grad_norm": 1.886836290359497,
      "learning_rate": 4.399088716161887e-06,
      "loss": 0.5322,
      "step": 31950
    },
    {
      "epoch": 2.562951082598236,
      "grad_norm": 1.935892105102539,
      "learning_rate": 4.39104797641383e-06,
      "loss": 0.6461,
      "step": 31960
    },
    {
      "epoch": 2.5637530072173216,
      "grad_norm": 1.7416057586669922,
      "learning_rate": 4.383007236665774e-06,
      "loss": 0.5823,
      "step": 31970
    },
    {
      "epoch": 2.5645549318364074,
      "grad_norm": 1.9889466762542725,
      "learning_rate": 4.374966496917717e-06,
      "loss": 0.6765,
      "step": 31980
    },
    {
      "epoch": 2.565356856455493,
      "grad_norm": 1.5412101745605469,
      "learning_rate": 4.36692575716966e-06,
      "loss": 0.5549,
      "step": 31990
    },
    {
      "epoch": 2.566158781074579,
      "grad_norm": 1.691728115081787,
      "learning_rate": 4.358885017421603e-06,
      "loss": 0.6102,
      "step": 32000
    },
    {
      "epoch": 2.5669607056936647,
      "grad_norm": 1.7149021625518799,
      "learning_rate": 4.350844277673546e-06,
      "loss": 0.5983,
      "step": 32010
    },
    {
      "epoch": 2.5677626303127505,
      "grad_norm": 2.00282621383667,
      "learning_rate": 4.342803537925489e-06,
      "loss": 0.5676,
      "step": 32020
    },
    {
      "epoch": 2.5685645549318363,
      "grad_norm": 1.582813024520874,
      "learning_rate": 4.3347627981774326e-06,
      "loss": 0.5457,
      "step": 32030
    },
    {
      "epoch": 2.569366479550922,
      "grad_norm": 1.9878805875778198,
      "learning_rate": 4.326722058429376e-06,
      "loss": 0.6101,
      "step": 32040
    },
    {
      "epoch": 2.570168404170008,
      "grad_norm": 2.191969156265259,
      "learning_rate": 4.318681318681319e-06,
      "loss": 0.6207,
      "step": 32050
    },
    {
      "epoch": 2.5709703287890937,
      "grad_norm": 1.937975287437439,
      "learning_rate": 4.310640578933262e-06,
      "loss": 0.6135,
      "step": 32060
    },
    {
      "epoch": 2.5717722534081795,
      "grad_norm": 2.0461490154266357,
      "learning_rate": 4.302599839185205e-06,
      "loss": 0.5791,
      "step": 32070
    },
    {
      "epoch": 2.5725741780272653,
      "grad_norm": 1.5719486474990845,
      "learning_rate": 4.294559099437148e-06,
      "loss": 0.5949,
      "step": 32080
    },
    {
      "epoch": 2.573376102646351,
      "grad_norm": 1.9027833938598633,
      "learning_rate": 4.2865183596890915e-06,
      "loss": 0.6027,
      "step": 32090
    },
    {
      "epoch": 2.574178027265437,
      "grad_norm": 2.0515964031219482,
      "learning_rate": 4.278477619941035e-06,
      "loss": 0.5738,
      "step": 32100
    },
    {
      "epoch": 2.574979951884523,
      "grad_norm": 2.235823392868042,
      "learning_rate": 4.270436880192978e-06,
      "loss": 0.6607,
      "step": 32110
    },
    {
      "epoch": 2.5757818765036085,
      "grad_norm": 1.7002742290496826,
      "learning_rate": 4.262396140444921e-06,
      "loss": 0.5848,
      "step": 32120
    },
    {
      "epoch": 2.5765838011226947,
      "grad_norm": 1.9187076091766357,
      "learning_rate": 4.2543554006968645e-06,
      "loss": 0.5621,
      "step": 32130
    },
    {
      "epoch": 2.57738572574178,
      "grad_norm": 1.785956621170044,
      "learning_rate": 4.246314660948807e-06,
      "loss": 0.6394,
      "step": 32140
    },
    {
      "epoch": 2.5781876503608663,
      "grad_norm": 1.872307300567627,
      "learning_rate": 4.2382739212007505e-06,
      "loss": 0.6174,
      "step": 32150
    },
    {
      "epoch": 2.5789895749799516,
      "grad_norm": 1.8256416320800781,
      "learning_rate": 4.230233181452694e-06,
      "loss": 0.611,
      "step": 32160
    },
    {
      "epoch": 2.579791499599038,
      "grad_norm": 2.5012331008911133,
      "learning_rate": 4.222192441704637e-06,
      "loss": 0.6749,
      "step": 32170
    },
    {
      "epoch": 2.5805934242181237,
      "grad_norm": 1.8832706212997437,
      "learning_rate": 4.21415170195658e-06,
      "loss": 0.6107,
      "step": 32180
    },
    {
      "epoch": 2.5813953488372094,
      "grad_norm": 1.8325947523117065,
      "learning_rate": 4.2061109622085234e-06,
      "loss": 0.656,
      "step": 32190
    },
    {
      "epoch": 2.5821972734562952,
      "grad_norm": 1.8640131950378418,
      "learning_rate": 4.198070222460466e-06,
      "loss": 0.5556,
      "step": 32200
    },
    {
      "epoch": 2.582999198075381,
      "grad_norm": 1.66194748878479,
      "learning_rate": 4.1900294827124095e-06,
      "loss": 0.6133,
      "step": 32210
    },
    {
      "epoch": 2.583801122694467,
      "grad_norm": 1.9451096057891846,
      "learning_rate": 4.181988742964353e-06,
      "loss": 0.6917,
      "step": 32220
    },
    {
      "epoch": 2.5846030473135526,
      "grad_norm": 1.9928218126296997,
      "learning_rate": 4.173948003216296e-06,
      "loss": 0.6133,
      "step": 32230
    },
    {
      "epoch": 2.5854049719326384,
      "grad_norm": 2.052427291870117,
      "learning_rate": 4.165907263468239e-06,
      "loss": 0.5991,
      "step": 32240
    },
    {
      "epoch": 2.586206896551724,
      "grad_norm": 2.1537580490112305,
      "learning_rate": 4.157866523720182e-06,
      "loss": 0.636,
      "step": 32250
    },
    {
      "epoch": 2.58700882117081,
      "grad_norm": 1.918195128440857,
      "learning_rate": 4.149825783972126e-06,
      "loss": 0.6824,
      "step": 32260
    },
    {
      "epoch": 2.587810745789896,
      "grad_norm": 1.5436069965362549,
      "learning_rate": 4.1417850442240684e-06,
      "loss": 0.6342,
      "step": 32270
    },
    {
      "epoch": 2.5886126704089816,
      "grad_norm": 1.6940925121307373,
      "learning_rate": 4.133744304476012e-06,
      "loss": 0.5805,
      "step": 32280
    },
    {
      "epoch": 2.5894145950280674,
      "grad_norm": 1.976335883140564,
      "learning_rate": 4.125703564727955e-06,
      "loss": 0.6372,
      "step": 32290
    },
    {
      "epoch": 2.590216519647153,
      "grad_norm": 1.679905891418457,
      "learning_rate": 4.117662824979899e-06,
      "loss": 0.5971,
      "step": 32300
    },
    {
      "epoch": 2.591018444266239,
      "grad_norm": 1.7904258966445923,
      "learning_rate": 4.109622085231841e-06,
      "loss": 0.5861,
      "step": 32310
    },
    {
      "epoch": 2.5918203688853247,
      "grad_norm": 1.8709558248519897,
      "learning_rate": 4.101581345483785e-06,
      "loss": 0.6302,
      "step": 32320
    },
    {
      "epoch": 2.5926222935044105,
      "grad_norm": 1.9733784198760986,
      "learning_rate": 4.093540605735727e-06,
      "loss": 0.6087,
      "step": 32330
    },
    {
      "epoch": 2.5934242181234963,
      "grad_norm": 1.8198370933532715,
      "learning_rate": 4.085499865987671e-06,
      "loss": 0.5789,
      "step": 32340
    },
    {
      "epoch": 2.594226142742582,
      "grad_norm": 1.9029003381729126,
      "learning_rate": 4.077459126239614e-06,
      "loss": 0.6343,
      "step": 32350
    },
    {
      "epoch": 2.595028067361668,
      "grad_norm": 1.9837075471878052,
      "learning_rate": 4.069418386491558e-06,
      "loss": 0.6375,
      "step": 32360
    },
    {
      "epoch": 2.5958299919807537,
      "grad_norm": 2.1371266841888428,
      "learning_rate": 4.0613776467435e-06,
      "loss": 0.6332,
      "step": 32370
    },
    {
      "epoch": 2.5966319165998395,
      "grad_norm": 1.934321403503418,
      "learning_rate": 4.053336906995444e-06,
      "loss": 0.631,
      "step": 32380
    },
    {
      "epoch": 2.5974338412189253,
      "grad_norm": 1.7929993867874146,
      "learning_rate": 4.045296167247386e-06,
      "loss": 0.6456,
      "step": 32390
    },
    {
      "epoch": 2.598235765838011,
      "grad_norm": 1.694541096687317,
      "learning_rate": 4.03725542749933e-06,
      "loss": 0.5664,
      "step": 32400
    },
    {
      "epoch": 2.599037690457097,
      "grad_norm": 2.0638136863708496,
      "learning_rate": 4.029214687751273e-06,
      "loss": 0.596,
      "step": 32410
    },
    {
      "epoch": 2.599839615076183,
      "grad_norm": 2.2255897521972656,
      "learning_rate": 4.021173948003217e-06,
      "loss": 0.6173,
      "step": 32420
    },
    {
      "epoch": 2.6006415396952685,
      "grad_norm": 1.9404045343399048,
      "learning_rate": 4.013133208255159e-06,
      "loss": 0.6142,
      "step": 32430
    },
    {
      "epoch": 2.6014434643143547,
      "grad_norm": 1.8339277505874634,
      "learning_rate": 4.005092468507103e-06,
      "loss": 0.5892,
      "step": 32440
    },
    {
      "epoch": 2.60224538893344,
      "grad_norm": 2.124871253967285,
      "learning_rate": 3.997051728759046e-06,
      "loss": 0.6408,
      "step": 32450
    },
    {
      "epoch": 2.6030473135525263,
      "grad_norm": 1.971071720123291,
      "learning_rate": 3.989010989010989e-06,
      "loss": 0.5731,
      "step": 32460
    },
    {
      "epoch": 2.6038492381716116,
      "grad_norm": 2.413588285446167,
      "learning_rate": 3.980970249262932e-06,
      "loss": 0.5064,
      "step": 32470
    },
    {
      "epoch": 2.604651162790698,
      "grad_norm": 1.4537935256958008,
      "learning_rate": 3.972929509514876e-06,
      "loss": 0.555,
      "step": 32480
    },
    {
      "epoch": 2.605453087409783,
      "grad_norm": 1.9254107475280762,
      "learning_rate": 3.964888769766819e-06,
      "loss": 0.5748,
      "step": 32490
    },
    {
      "epoch": 2.6062550120288694,
      "grad_norm": 2.01377010345459,
      "learning_rate": 3.956848030018762e-06,
      "loss": 0.5763,
      "step": 32500
    },
    {
      "epoch": 2.6070569366479552,
      "grad_norm": 2.524670124053955,
      "learning_rate": 3.948807290270705e-06,
      "loss": 0.6034,
      "step": 32510
    },
    {
      "epoch": 2.607858861267041,
      "grad_norm": 2.2169008255004883,
      "learning_rate": 3.940766550522648e-06,
      "loss": 0.6136,
      "step": 32520
    },
    {
      "epoch": 2.608660785886127,
      "grad_norm": 2.0918455123901367,
      "learning_rate": 3.932725810774591e-06,
      "loss": 0.5929,
      "step": 32530
    },
    {
      "epoch": 2.6094627105052126,
      "grad_norm": 1.827359676361084,
      "learning_rate": 3.924685071026535e-06,
      "loss": 0.6683,
      "step": 32540
    },
    {
      "epoch": 2.6102646351242984,
      "grad_norm": 1.9322978258132935,
      "learning_rate": 3.916644331278478e-06,
      "loss": 0.5864,
      "step": 32550
    },
    {
      "epoch": 2.611066559743384,
      "grad_norm": 1.9842101335525513,
      "learning_rate": 3.908603591530421e-06,
      "loss": 0.5449,
      "step": 32560
    },
    {
      "epoch": 2.61186848436247,
      "grad_norm": 2.0119409561157227,
      "learning_rate": 3.900562851782364e-06,
      "loss": 0.5777,
      "step": 32570
    },
    {
      "epoch": 2.612670408981556,
      "grad_norm": 1.9444981813430786,
      "learning_rate": 3.8925221120343075e-06,
      "loss": 0.6354,
      "step": 32580
    },
    {
      "epoch": 2.6134723336006416,
      "grad_norm": 2.17516827583313,
      "learning_rate": 3.88448137228625e-06,
      "loss": 0.5739,
      "step": 32590
    },
    {
      "epoch": 2.6142742582197274,
      "grad_norm": 1.9247407913208008,
      "learning_rate": 3.876440632538194e-06,
      "loss": 0.5882,
      "step": 32600
    },
    {
      "epoch": 2.615076182838813,
      "grad_norm": 2.196553945541382,
      "learning_rate": 3.868399892790137e-06,
      "loss": 0.7016,
      "step": 32610
    },
    {
      "epoch": 2.615878107457899,
      "grad_norm": 1.6852540969848633,
      "learning_rate": 3.8603591530420805e-06,
      "loss": 0.6563,
      "step": 32620
    },
    {
      "epoch": 2.6166800320769847,
      "grad_norm": 1.8656009435653687,
      "learning_rate": 3.852318413294023e-06,
      "loss": 0.6125,
      "step": 32630
    },
    {
      "epoch": 2.6174819566960705,
      "grad_norm": 1.8191347122192383,
      "learning_rate": 3.8442776735459665e-06,
      "loss": 0.6841,
      "step": 32640
    },
    {
      "epoch": 2.6182838813151563,
      "grad_norm": 2.1322031021118164,
      "learning_rate": 3.836236933797909e-06,
      "loss": 0.5734,
      "step": 32650
    },
    {
      "epoch": 2.619085805934242,
      "grad_norm": 1.9934282302856445,
      "learning_rate": 3.8281961940498525e-06,
      "loss": 0.6369,
      "step": 32660
    },
    {
      "epoch": 2.619887730553328,
      "grad_norm": 1.9994781017303467,
      "learning_rate": 3.820155454301796e-06,
      "loss": 0.5786,
      "step": 32670
    },
    {
      "epoch": 2.6206896551724137,
      "grad_norm": 2.040945291519165,
      "learning_rate": 3.8121147145537394e-06,
      "loss": 0.6025,
      "step": 32680
    },
    {
      "epoch": 2.6214915797914995,
      "grad_norm": 1.8288252353668213,
      "learning_rate": 3.804073974805682e-06,
      "loss": 0.5205,
      "step": 32690
    },
    {
      "epoch": 2.6222935044105853,
      "grad_norm": 2.2673795223236084,
      "learning_rate": 3.796033235057625e-06,
      "loss": 0.6564,
      "step": 32700
    },
    {
      "epoch": 2.623095429029671,
      "grad_norm": 1.8623963594436646,
      "learning_rate": 3.7879924953095685e-06,
      "loss": 0.6076,
      "step": 32710
    },
    {
      "epoch": 2.623897353648757,
      "grad_norm": 2.224395990371704,
      "learning_rate": 3.779951755561512e-06,
      "loss": 0.5725,
      "step": 32720
    },
    {
      "epoch": 2.6246992782678427,
      "grad_norm": 1.825726866722107,
      "learning_rate": 3.771911015813455e-06,
      "loss": 0.6134,
      "step": 32730
    },
    {
      "epoch": 2.6255012028869285,
      "grad_norm": 2.0279579162597656,
      "learning_rate": 3.7638702760653984e-06,
      "loss": 0.6805,
      "step": 32740
    },
    {
      "epoch": 2.6263031275060147,
      "grad_norm": 2.05688214302063,
      "learning_rate": 3.755829536317341e-06,
      "loss": 0.639,
      "step": 32750
    },
    {
      "epoch": 2.6271050521251,
      "grad_norm": 1.8499075174331665,
      "learning_rate": 3.7477887965692844e-06,
      "loss": 0.579,
      "step": 32760
    },
    {
      "epoch": 2.6279069767441863,
      "grad_norm": 1.799139380455017,
      "learning_rate": 3.7397480568212275e-06,
      "loss": 0.5977,
      "step": 32770
    },
    {
      "epoch": 2.6287089013632716,
      "grad_norm": 1.6885597705841064,
      "learning_rate": 3.731707317073171e-06,
      "loss": 0.6582,
      "step": 32780
    },
    {
      "epoch": 2.629510825982358,
      "grad_norm": 1.5933384895324707,
      "learning_rate": 3.723666577325114e-06,
      "loss": 0.5967,
      "step": 32790
    },
    {
      "epoch": 2.630312750601443,
      "grad_norm": 2.072525978088379,
      "learning_rate": 3.715625837577057e-06,
      "loss": 0.6626,
      "step": 32800
    },
    {
      "epoch": 2.6311146752205294,
      "grad_norm": 1.8459745645523071,
      "learning_rate": 3.7075850978290004e-06,
      "loss": 0.5907,
      "step": 32810
    },
    {
      "epoch": 2.6319165998396152,
      "grad_norm": 1.8090291023254395,
      "learning_rate": 3.699544358080944e-06,
      "loss": 0.6099,
      "step": 32820
    },
    {
      "epoch": 2.632718524458701,
      "grad_norm": 2.082120180130005,
      "learning_rate": 3.6915036183328864e-06,
      "loss": 0.6884,
      "step": 32830
    },
    {
      "epoch": 2.633520449077787,
      "grad_norm": 1.8256306648254395,
      "learning_rate": 3.68346287858483e-06,
      "loss": 0.5814,
      "step": 32840
    },
    {
      "epoch": 2.6343223736968726,
      "grad_norm": 2.1449897289276123,
      "learning_rate": 3.6754221388367733e-06,
      "loss": 0.5891,
      "step": 32850
    },
    {
      "epoch": 2.6351242983159584,
      "grad_norm": 2.0384132862091064,
      "learning_rate": 3.6673813990887163e-06,
      "loss": 0.5905,
      "step": 32860
    },
    {
      "epoch": 2.635926222935044,
      "grad_norm": 2.0850918292999268,
      "learning_rate": 3.6593406593406593e-06,
      "loss": 0.6573,
      "step": 32870
    },
    {
      "epoch": 2.63672814755413,
      "grad_norm": 2.211162567138672,
      "learning_rate": 3.6512999195926028e-06,
      "loss": 0.5896,
      "step": 32880
    },
    {
      "epoch": 2.637530072173216,
      "grad_norm": 1.9192674160003662,
      "learning_rate": 3.643259179844546e-06,
      "loss": 0.5849,
      "step": 32890
    },
    {
      "epoch": 2.6383319967923016,
      "grad_norm": 1.6913975477218628,
      "learning_rate": 3.635218440096489e-06,
      "loss": 0.621,
      "step": 32900
    },
    {
      "epoch": 2.6391339214113874,
      "grad_norm": 1.9098789691925049,
      "learning_rate": 3.6271777003484323e-06,
      "loss": 0.6088,
      "step": 32910
    },
    {
      "epoch": 2.639935846030473,
      "grad_norm": 2.001671314239502,
      "learning_rate": 3.6191369606003753e-06,
      "loss": 0.5612,
      "step": 32920
    },
    {
      "epoch": 2.640737770649559,
      "grad_norm": 1.7437595129013062,
      "learning_rate": 3.6110962208523183e-06,
      "loss": 0.6834,
      "step": 32930
    },
    {
      "epoch": 2.6415396952686447,
      "grad_norm": 2.0330793857574463,
      "learning_rate": 3.6030554811042618e-06,
      "loss": 0.6224,
      "step": 32940
    },
    {
      "epoch": 2.6423416198877305,
      "grad_norm": 1.7989798784255981,
      "learning_rate": 3.5950147413562048e-06,
      "loss": 0.5758,
      "step": 32950
    },
    {
      "epoch": 2.6431435445068163,
      "grad_norm": 1.9790922403335571,
      "learning_rate": 3.586974001608148e-06,
      "loss": 0.6503,
      "step": 32960
    },
    {
      "epoch": 2.643945469125902,
      "grad_norm": 1.6686553955078125,
      "learning_rate": 3.5789332618600912e-06,
      "loss": 0.6448,
      "step": 32970
    },
    {
      "epoch": 2.644747393744988,
      "grad_norm": 2.0486812591552734,
      "learning_rate": 3.5708925221120347e-06,
      "loss": 0.6398,
      "step": 32980
    },
    {
      "epoch": 2.6455493183640737,
      "grad_norm": 2.0865471363067627,
      "learning_rate": 3.5628517823639773e-06,
      "loss": 0.6104,
      "step": 32990
    },
    {
      "epoch": 2.6463512429831595,
      "grad_norm": 1.8274792432785034,
      "learning_rate": 3.5548110426159207e-06,
      "loss": 0.5532,
      "step": 33000
    },
    {
      "epoch": 2.6471531676022453,
      "grad_norm": 1.6847364902496338,
      "learning_rate": 3.546770302867864e-06,
      "loss": 0.6465,
      "step": 33010
    },
    {
      "epoch": 2.647955092221331,
      "grad_norm": 2.1896867752075195,
      "learning_rate": 3.538729563119807e-06,
      "loss": 0.5785,
      "step": 33020
    },
    {
      "epoch": 2.648757016840417,
      "grad_norm": 1.8018790483474731,
      "learning_rate": 3.53068882337175e-06,
      "loss": 0.6095,
      "step": 33030
    },
    {
      "epoch": 2.6495589414595027,
      "grad_norm": 1.7068897485733032,
      "learning_rate": 3.5226480836236936e-06,
      "loss": 0.5828,
      "step": 33040
    },
    {
      "epoch": 2.6503608660785885,
      "grad_norm": 1.7898156642913818,
      "learning_rate": 3.5146073438756367e-06,
      "loss": 0.5767,
      "step": 33050
    },
    {
      "epoch": 2.6511627906976747,
      "grad_norm": 2.3740272521972656,
      "learning_rate": 3.5065666041275797e-06,
      "loss": 0.6465,
      "step": 33060
    },
    {
      "epoch": 2.65196471531676,
      "grad_norm": 2.195573091506958,
      "learning_rate": 3.498525864379523e-06,
      "loss": 0.63,
      "step": 33070
    },
    {
      "epoch": 2.6527666399358463,
      "grad_norm": 1.784358024597168,
      "learning_rate": 3.490485124631466e-06,
      "loss": 0.5915,
      "step": 33080
    },
    {
      "epoch": 2.6535685645549316,
      "grad_norm": 1.562472939491272,
      "learning_rate": 3.482444384883409e-06,
      "loss": 0.5237,
      "step": 33090
    },
    {
      "epoch": 2.654370489174018,
      "grad_norm": 2.251551628112793,
      "learning_rate": 3.4744036451353526e-06,
      "loss": 0.6198,
      "step": 33100
    },
    {
      "epoch": 2.655172413793103,
      "grad_norm": 1.905706763267517,
      "learning_rate": 3.4663629053872956e-06,
      "loss": 0.6076,
      "step": 33110
    },
    {
      "epoch": 2.6559743384121894,
      "grad_norm": 1.7488447427749634,
      "learning_rate": 3.4583221656392386e-06,
      "loss": 0.562,
      "step": 33120
    },
    {
      "epoch": 2.656776263031275,
      "grad_norm": 1.8532174825668335,
      "learning_rate": 3.450281425891182e-06,
      "loss": 0.612,
      "step": 33130
    },
    {
      "epoch": 2.657578187650361,
      "grad_norm": 1.7422090768814087,
      "learning_rate": 3.4422406861431255e-06,
      "loss": 0.6308,
      "step": 33140
    },
    {
      "epoch": 2.658380112269447,
      "grad_norm": 1.89874267578125,
      "learning_rate": 3.434199946395068e-06,
      "loss": 0.5867,
      "step": 33150
    },
    {
      "epoch": 2.6591820368885326,
      "grad_norm": 1.7511852979660034,
      "learning_rate": 3.4261592066470116e-06,
      "loss": 0.5664,
      "step": 33160
    },
    {
      "epoch": 2.6599839615076184,
      "grad_norm": 1.7907466888427734,
      "learning_rate": 3.418118466898955e-06,
      "loss": 0.6347,
      "step": 33170
    },
    {
      "epoch": 2.660785886126704,
      "grad_norm": 2.058943033218384,
      "learning_rate": 3.410077727150898e-06,
      "loss": 0.5484,
      "step": 33180
    },
    {
      "epoch": 2.66158781074579,
      "grad_norm": 2.007246494293213,
      "learning_rate": 3.402036987402841e-06,
      "loss": 0.6445,
      "step": 33190
    },
    {
      "epoch": 2.6623897353648758,
      "grad_norm": 1.6978822946548462,
      "learning_rate": 3.3939962476547845e-06,
      "loss": 0.5602,
      "step": 33200
    },
    {
      "epoch": 2.6631916599839616,
      "grad_norm": 1.9526785612106323,
      "learning_rate": 3.3859555079067275e-06,
      "loss": 0.5878,
      "step": 33210
    },
    {
      "epoch": 2.6639935846030474,
      "grad_norm": 2.1020395755767822,
      "learning_rate": 3.3779147681586705e-06,
      "loss": 0.6784,
      "step": 33220
    },
    {
      "epoch": 2.664795509222133,
      "grad_norm": 2.0330238342285156,
      "learning_rate": 3.369874028410614e-06,
      "loss": 0.571,
      "step": 33230
    },
    {
      "epoch": 2.665597433841219,
      "grad_norm": 2.4164962768554688,
      "learning_rate": 3.361833288662557e-06,
      "loss": 0.6233,
      "step": 33240
    },
    {
      "epoch": 2.6663993584603047,
      "grad_norm": 1.790631890296936,
      "learning_rate": 3.3537925489145e-06,
      "loss": 0.5872,
      "step": 33250
    },
    {
      "epoch": 2.6672012830793905,
      "grad_norm": 2.1595354080200195,
      "learning_rate": 3.3457518091664435e-06,
      "loss": 0.644,
      "step": 33260
    },
    {
      "epoch": 2.6680032076984763,
      "grad_norm": 2.1059584617614746,
      "learning_rate": 3.3377110694183865e-06,
      "loss": 0.6445,
      "step": 33270
    },
    {
      "epoch": 2.668805132317562,
      "grad_norm": 1.87261962890625,
      "learning_rate": 3.3296703296703295e-06,
      "loss": 0.5807,
      "step": 33280
    },
    {
      "epoch": 2.669607056936648,
      "grad_norm": 1.85750412940979,
      "learning_rate": 3.321629589922273e-06,
      "loss": 0.5457,
      "step": 33290
    },
    {
      "epoch": 2.6704089815557337,
      "grad_norm": 2.013089895248413,
      "learning_rate": 3.3135888501742164e-06,
      "loss": 0.5989,
      "step": 33300
    },
    {
      "epoch": 2.6712109061748195,
      "grad_norm": 2.1128199100494385,
      "learning_rate": 3.305548110426159e-06,
      "loss": 0.5426,
      "step": 33310
    },
    {
      "epoch": 2.6720128307939053,
      "grad_norm": 1.9931588172912598,
      "learning_rate": 3.298311444652908e-06,
      "loss": 0.6476,
      "step": 33320
    },
    {
      "epoch": 2.672814755412991,
      "grad_norm": 1.872082233428955,
      "learning_rate": 3.2902707049048515e-06,
      "loss": 0.5882,
      "step": 33330
    },
    {
      "epoch": 2.673616680032077,
      "grad_norm": 2.153442621231079,
      "learning_rate": 3.2822299651567945e-06,
      "loss": 0.5614,
      "step": 33340
    },
    {
      "epoch": 2.6744186046511627,
      "grad_norm": 1.9969067573547363,
      "learning_rate": 3.2741892254087376e-06,
      "loss": 0.6274,
      "step": 33350
    },
    {
      "epoch": 2.6752205292702484,
      "grad_norm": 1.7022202014923096,
      "learning_rate": 3.266148485660681e-06,
      "loss": 0.6458,
      "step": 33360
    },
    {
      "epoch": 2.6760224538893342,
      "grad_norm": 2.0218429565429688,
      "learning_rate": 3.258107745912624e-06,
      "loss": 0.6187,
      "step": 33370
    },
    {
      "epoch": 2.67682437850842,
      "grad_norm": 1.7172737121582031,
      "learning_rate": 3.250067006164567e-06,
      "loss": 0.6067,
      "step": 33380
    },
    {
      "epoch": 2.6776263031275063,
      "grad_norm": 2.026334047317505,
      "learning_rate": 3.2420262664165105e-06,
      "loss": 0.5905,
      "step": 33390
    },
    {
      "epoch": 2.6784282277465916,
      "grad_norm": 1.8276057243347168,
      "learning_rate": 3.2339855266684535e-06,
      "loss": 0.6308,
      "step": 33400
    },
    {
      "epoch": 2.679230152365678,
      "grad_norm": 2.02388596534729,
      "learning_rate": 3.2259447869203965e-06,
      "loss": 0.5952,
      "step": 33410
    },
    {
      "epoch": 2.680032076984763,
      "grad_norm": 2.1784257888793945,
      "learning_rate": 3.21790404717234e-06,
      "loss": 0.6397,
      "step": 33420
    },
    {
      "epoch": 2.6808340016038494,
      "grad_norm": 1.8569217920303345,
      "learning_rate": 3.2098633074242834e-06,
      "loss": 0.5727,
      "step": 33430
    },
    {
      "epoch": 2.681635926222935,
      "grad_norm": 1.7163020372390747,
      "learning_rate": 3.201822567676226e-06,
      "loss": 0.6351,
      "step": 33440
    },
    {
      "epoch": 2.682437850842021,
      "grad_norm": 1.9985911846160889,
      "learning_rate": 3.1937818279281695e-06,
      "loss": 0.5777,
      "step": 33450
    },
    {
      "epoch": 2.683239775461107,
      "grad_norm": 2.066077709197998,
      "learning_rate": 3.185741088180113e-06,
      "loss": 0.5838,
      "step": 33460
    },
    {
      "epoch": 2.6840417000801926,
      "grad_norm": 1.9814019203186035,
      "learning_rate": 3.177700348432056e-06,
      "loss": 0.6472,
      "step": 33470
    },
    {
      "epoch": 2.6848436246992784,
      "grad_norm": 2.0858936309814453,
      "learning_rate": 3.169659608683999e-06,
      "loss": 0.6882,
      "step": 33480
    },
    {
      "epoch": 2.685645549318364,
      "grad_norm": 2.3142316341400146,
      "learning_rate": 3.1616188689359424e-06,
      "loss": 0.6093,
      "step": 33490
    },
    {
      "epoch": 2.68644747393745,
      "grad_norm": 1.7788406610488892,
      "learning_rate": 3.1535781291878854e-06,
      "loss": 0.5682,
      "step": 33500
    },
    {
      "epoch": 2.6872493985565358,
      "grad_norm": 1.8338377475738525,
      "learning_rate": 3.1455373894398284e-06,
      "loss": 0.6051,
      "step": 33510
    },
    {
      "epoch": 2.6880513231756216,
      "grad_norm": 2.4060657024383545,
      "learning_rate": 3.137496649691772e-06,
      "loss": 0.5555,
      "step": 33520
    },
    {
      "epoch": 2.6888532477947074,
      "grad_norm": 1.5594871044158936,
      "learning_rate": 3.129455909943715e-06,
      "loss": 0.5762,
      "step": 33530
    },
    {
      "epoch": 2.689655172413793,
      "grad_norm": 1.768823504447937,
      "learning_rate": 3.121415170195658e-06,
      "loss": 0.5953,
      "step": 33540
    },
    {
      "epoch": 2.690457097032879,
      "grad_norm": 1.9525655508041382,
      "learning_rate": 3.1133744304476013e-06,
      "loss": 0.5745,
      "step": 33550
    },
    {
      "epoch": 2.6912590216519647,
      "grad_norm": 2.197702169418335,
      "learning_rate": 3.1053336906995444e-06,
      "loss": 0.6708,
      "step": 33560
    },
    {
      "epoch": 2.6920609462710505,
      "grad_norm": 1.7906277179718018,
      "learning_rate": 3.0972929509514874e-06,
      "loss": 0.5509,
      "step": 33570
    },
    {
      "epoch": 2.6928628708901363,
      "grad_norm": 2.5223262310028076,
      "learning_rate": 3.089252211203431e-06,
      "loss": 0.6454,
      "step": 33580
    },
    {
      "epoch": 2.693664795509222,
      "grad_norm": 1.9229439496994019,
      "learning_rate": 3.0812114714553743e-06,
      "loss": 0.5946,
      "step": 33590
    },
    {
      "epoch": 2.694466720128308,
      "grad_norm": 2.1612255573272705,
      "learning_rate": 3.073170731707317e-06,
      "loss": 0.6344,
      "step": 33600
    },
    {
      "epoch": 2.6952686447473937,
      "grad_norm": 1.8994441032409668,
      "learning_rate": 3.0651299919592603e-06,
      "loss": 0.6156,
      "step": 33610
    },
    {
      "epoch": 2.6960705693664795,
      "grad_norm": 1.8801774978637695,
      "learning_rate": 3.0570892522112037e-06,
      "loss": 0.5871,
      "step": 33620
    },
    {
      "epoch": 2.6968724939855653,
      "grad_norm": 2.084174633026123,
      "learning_rate": 3.0490485124631468e-06,
      "loss": 0.6181,
      "step": 33630
    },
    {
      "epoch": 2.697674418604651,
      "grad_norm": 1.8943722248077393,
      "learning_rate": 3.0410077727150898e-06,
      "loss": 0.6094,
      "step": 33640
    },
    {
      "epoch": 2.698476343223737,
      "grad_norm": 1.840367317199707,
      "learning_rate": 3.0329670329670332e-06,
      "loss": 0.5774,
      "step": 33650
    },
    {
      "epoch": 2.6992782678428227,
      "grad_norm": 1.9874284267425537,
      "learning_rate": 3.0249262932189762e-06,
      "loss": 0.5955,
      "step": 33660
    },
    {
      "epoch": 2.7000801924619084,
      "grad_norm": 2.2326722145080566,
      "learning_rate": 3.0168855534709193e-06,
      "loss": 0.6478,
      "step": 33670
    },
    {
      "epoch": 2.7008821170809942,
      "grad_norm": 1.6394418478012085,
      "learning_rate": 3.0088448137228627e-06,
      "loss": 0.6173,
      "step": 33680
    },
    {
      "epoch": 2.70168404170008,
      "grad_norm": 2.0663399696350098,
      "learning_rate": 3.0008040739748057e-06,
      "loss": 0.5967,
      "step": 33690
    },
    {
      "epoch": 2.7024859663191663,
      "grad_norm": 1.7210506200790405,
      "learning_rate": 2.9927633342267488e-06,
      "loss": 0.5928,
      "step": 33700
    },
    {
      "epoch": 2.7032878909382516,
      "grad_norm": 1.9306457042694092,
      "learning_rate": 2.984722594478692e-06,
      "loss": 0.5941,
      "step": 33710
    },
    {
      "epoch": 2.704089815557338,
      "grad_norm": 2.107708692550659,
      "learning_rate": 2.9766818547306352e-06,
      "loss": 0.5731,
      "step": 33720
    },
    {
      "epoch": 2.704891740176423,
      "grad_norm": 1.8884605169296265,
      "learning_rate": 2.9686411149825782e-06,
      "loss": 0.5585,
      "step": 33730
    },
    {
      "epoch": 2.7056936647955094,
      "grad_norm": 1.8778102397918701,
      "learning_rate": 2.9606003752345217e-06,
      "loss": 0.5529,
      "step": 33740
    },
    {
      "epoch": 2.706495589414595,
      "grad_norm": 1.7278162240982056,
      "learning_rate": 2.952559635486465e-06,
      "loss": 0.5934,
      "step": 33750
    },
    {
      "epoch": 2.707297514033681,
      "grad_norm": 2.0185770988464355,
      "learning_rate": 2.9445188957384077e-06,
      "loss": 0.5724,
      "step": 33760
    },
    {
      "epoch": 2.7080994386527664,
      "grad_norm": 2.044290542602539,
      "learning_rate": 2.936478155990351e-06,
      "loss": 0.5754,
      "step": 33770
    },
    {
      "epoch": 2.7089013632718526,
      "grad_norm": 2.215453624725342,
      "learning_rate": 2.9284374162422946e-06,
      "loss": 0.6386,
      "step": 33780
    },
    {
      "epoch": 2.7097032878909384,
      "grad_norm": 1.8543567657470703,
      "learning_rate": 2.9203966764942376e-06,
      "loss": 0.6312,
      "step": 33790
    },
    {
      "epoch": 2.710505212510024,
      "grad_norm": 1.9584993124008179,
      "learning_rate": 2.9123559367461806e-06,
      "loss": 0.5962,
      "step": 33800
    },
    {
      "epoch": 2.71130713712911,
      "grad_norm": 1.7828021049499512,
      "learning_rate": 2.904315196998124e-06,
      "loss": 0.5765,
      "step": 33810
    },
    {
      "epoch": 2.7121090617481958,
      "grad_norm": 1.8741340637207031,
      "learning_rate": 2.896274457250067e-06,
      "loss": 0.655,
      "step": 33820
    },
    {
      "epoch": 2.7129109863672816,
      "grad_norm": 1.831261396408081,
      "learning_rate": 2.88823371750201e-06,
      "loss": 0.6206,
      "step": 33830
    },
    {
      "epoch": 2.7137129109863674,
      "grad_norm": 2.031015396118164,
      "learning_rate": 2.8801929777539536e-06,
      "loss": 0.589,
      "step": 33840
    },
    {
      "epoch": 2.714514835605453,
      "grad_norm": 1.8975883722305298,
      "learning_rate": 2.8721522380058966e-06,
      "loss": 0.579,
      "step": 33850
    },
    {
      "epoch": 2.715316760224539,
      "grad_norm": 1.720473051071167,
      "learning_rate": 2.8641114982578396e-06,
      "loss": 0.5918,
      "step": 33860
    },
    {
      "epoch": 2.7161186848436247,
      "grad_norm": 2.414119005203247,
      "learning_rate": 2.856070758509783e-06,
      "loss": 0.6389,
      "step": 33870
    },
    {
      "epoch": 2.7169206094627105,
      "grad_norm": 2.053623676300049,
      "learning_rate": 2.848030018761726e-06,
      "loss": 0.6137,
      "step": 33880
    },
    {
      "epoch": 2.7177225340817963,
      "grad_norm": 1.929153323173523,
      "learning_rate": 2.839989279013669e-06,
      "loss": 0.6374,
      "step": 33890
    },
    {
      "epoch": 2.718524458700882,
      "grad_norm": 2.041048049926758,
      "learning_rate": 2.8319485392656125e-06,
      "loss": 0.6068,
      "step": 33900
    },
    {
      "epoch": 2.719326383319968,
      "grad_norm": 1.705378770828247,
      "learning_rate": 2.823907799517556e-06,
      "loss": 0.5899,
      "step": 33910
    },
    {
      "epoch": 2.7201283079390537,
      "grad_norm": 2.025702714920044,
      "learning_rate": 2.8158670597694986e-06,
      "loss": 0.6512,
      "step": 33920
    },
    {
      "epoch": 2.7209302325581395,
      "grad_norm": 1.849822759628296,
      "learning_rate": 2.807826320021442e-06,
      "loss": 0.6333,
      "step": 33930
    },
    {
      "epoch": 2.7217321571772253,
      "grad_norm": 2.0743651390075684,
      "learning_rate": 2.7997855802733855e-06,
      "loss": 0.6137,
      "step": 33940
    },
    {
      "epoch": 2.722534081796311,
      "grad_norm": 1.7506399154663086,
      "learning_rate": 2.7917448405253285e-06,
      "loss": 0.5383,
      "step": 33950
    },
    {
      "epoch": 2.723336006415397,
      "grad_norm": 2.015822410583496,
      "learning_rate": 2.7837041007772715e-06,
      "loss": 0.5831,
      "step": 33960
    },
    {
      "epoch": 2.7241379310344827,
      "grad_norm": 2.3936243057250977,
      "learning_rate": 2.775663361029215e-06,
      "loss": 0.5966,
      "step": 33970
    },
    {
      "epoch": 2.7249398556535684,
      "grad_norm": 1.5556902885437012,
      "learning_rate": 2.767622621281158e-06,
      "loss": 0.6438,
      "step": 33980
    },
    {
      "epoch": 2.7257417802726542,
      "grad_norm": 1.6525309085845947,
      "learning_rate": 2.759581881533101e-06,
      "loss": 0.534,
      "step": 33990
    },
    {
      "epoch": 2.72654370489174,
      "grad_norm": 2.0419936180114746,
      "learning_rate": 2.7515411417850444e-06,
      "loss": 0.5824,
      "step": 34000
    },
    {
      "epoch": 2.727345629510826,
      "grad_norm": 1.998664140701294,
      "learning_rate": 2.7435004020369874e-06,
      "loss": 0.6216,
      "step": 34010
    },
    {
      "epoch": 2.7281475541299116,
      "grad_norm": 1.9241600036621094,
      "learning_rate": 2.7354596622889305e-06,
      "loss": 0.6168,
      "step": 34020
    },
    {
      "epoch": 2.728949478748998,
      "grad_norm": 1.6471033096313477,
      "learning_rate": 2.727418922540874e-06,
      "loss": 0.6361,
      "step": 34030
    },
    {
      "epoch": 2.729751403368083,
      "grad_norm": 1.6982368230819702,
      "learning_rate": 2.7193781827928173e-06,
      "loss": 0.5904,
      "step": 34040
    },
    {
      "epoch": 2.7305533279871694,
      "grad_norm": 1.839023232460022,
      "learning_rate": 2.71133744304476e-06,
      "loss": 0.623,
      "step": 34050
    },
    {
      "epoch": 2.731355252606255,
      "grad_norm": 1.6888357400894165,
      "learning_rate": 2.7032967032967034e-06,
      "loss": 0.6112,
      "step": 34060
    },
    {
      "epoch": 2.732157177225341,
      "grad_norm": 2.101853370666504,
      "learning_rate": 2.695255963548647e-06,
      "loss": 0.5862,
      "step": 34070
    },
    {
      "epoch": 2.7329591018444264,
      "grad_norm": 1.9282110929489136,
      "learning_rate": 2.6872152238005894e-06,
      "loss": 0.6555,
      "step": 34080
    },
    {
      "epoch": 2.7337610264635126,
      "grad_norm": 1.9537794589996338,
      "learning_rate": 2.679174484052533e-06,
      "loss": 0.5983,
      "step": 34090
    },
    {
      "epoch": 2.7345629510825984,
      "grad_norm": 2.21981143951416,
      "learning_rate": 2.6711337443044763e-06,
      "loss": 0.6299,
      "step": 34100
    },
    {
      "epoch": 2.735364875701684,
      "grad_norm": 1.6019600629806519,
      "learning_rate": 2.6630930045564193e-06,
      "loss": 0.5758,
      "step": 34110
    },
    {
      "epoch": 2.73616680032077,
      "grad_norm": 2.0196821689605713,
      "learning_rate": 2.6550522648083623e-06,
      "loss": 0.6011,
      "step": 34120
    },
    {
      "epoch": 2.7369687249398558,
      "grad_norm": 1.8276293277740479,
      "learning_rate": 2.647011525060306e-06,
      "loss": 0.6404,
      "step": 34130
    },
    {
      "epoch": 2.7377706495589416,
      "grad_norm": 1.8636035919189453,
      "learning_rate": 2.638970785312249e-06,
      "loss": 0.6125,
      "step": 34140
    },
    {
      "epoch": 2.7385725741780274,
      "grad_norm": 1.7576414346694946,
      "learning_rate": 2.630930045564192e-06,
      "loss": 0.6655,
      "step": 34150
    },
    {
      "epoch": 2.739374498797113,
      "grad_norm": 1.7113546133041382,
      "learning_rate": 2.6228893058161353e-06,
      "loss": 0.606,
      "step": 34160
    },
    {
      "epoch": 2.740176423416199,
      "grad_norm": 2.0457046031951904,
      "learning_rate": 2.6148485660680783e-06,
      "loss": 0.6208,
      "step": 34170
    },
    {
      "epoch": 2.7409783480352847,
      "grad_norm": 2.152891159057617,
      "learning_rate": 2.6068078263200213e-06,
      "loss": 0.5945,
      "step": 34180
    },
    {
      "epoch": 2.7417802726543705,
      "grad_norm": 1.8814982175827026,
      "learning_rate": 2.5987670865719648e-06,
      "loss": 0.6623,
      "step": 34190
    },
    {
      "epoch": 2.7425821972734563,
      "grad_norm": 2.051532506942749,
      "learning_rate": 2.590726346823908e-06,
      "loss": 0.6121,
      "step": 34200
    },
    {
      "epoch": 2.743384121892542,
      "grad_norm": 1.6997759342193604,
      "learning_rate": 2.582685607075851e-06,
      "loss": 0.5788,
      "step": 34210
    },
    {
      "epoch": 2.744186046511628,
      "grad_norm": 1.8963947296142578,
      "learning_rate": 2.5746448673277942e-06,
      "loss": 0.647,
      "step": 34220
    },
    {
      "epoch": 2.7449879711307137,
      "grad_norm": 1.8217201232910156,
      "learning_rate": 2.5666041275797377e-06,
      "loss": 0.636,
      "step": 34230
    },
    {
      "epoch": 2.7457898957497995,
      "grad_norm": 1.9278188943862915,
      "learning_rate": 2.5585633878316803e-06,
      "loss": 0.5818,
      "step": 34240
    },
    {
      "epoch": 2.7465918203688853,
      "grad_norm": 1.7218161821365356,
      "learning_rate": 2.5505226480836237e-06,
      "loss": 0.6543,
      "step": 34250
    },
    {
      "epoch": 2.747393744987971,
      "grad_norm": 1.705755591392517,
      "learning_rate": 2.542481908335567e-06,
      "loss": 0.5898,
      "step": 34260
    },
    {
      "epoch": 2.748195669607057,
      "grad_norm": 2.0274689197540283,
      "learning_rate": 2.53444116858751e-06,
      "loss": 0.6264,
      "step": 34270
    },
    {
      "epoch": 2.7489975942261426,
      "grad_norm": 1.9669959545135498,
      "learning_rate": 2.526400428839453e-06,
      "loss": 0.5542,
      "step": 34280
    },
    {
      "epoch": 2.7497995188452284,
      "grad_norm": 1.955945611000061,
      "learning_rate": 2.5183596890913966e-06,
      "loss": 0.5567,
      "step": 34290
    },
    {
      "epoch": 2.7506014434643142,
      "grad_norm": 2.041543960571289,
      "learning_rate": 2.5103189493433397e-06,
      "loss": 0.5729,
      "step": 34300
    },
    {
      "epoch": 2.7514033680834,
      "grad_norm": 2.3046464920043945,
      "learning_rate": 2.5022782095952827e-06,
      "loss": 0.5762,
      "step": 34310
    },
    {
      "epoch": 2.752205292702486,
      "grad_norm": 2.089887857437134,
      "learning_rate": 2.494237469847226e-06,
      "loss": 0.619,
      "step": 34320
    },
    {
      "epoch": 2.7530072173215716,
      "grad_norm": 2.120260238647461,
      "learning_rate": 2.486196730099169e-06,
      "loss": 0.6253,
      "step": 34330
    },
    {
      "epoch": 2.753809141940658,
      "grad_norm": 2.0646746158599854,
      "learning_rate": 2.478155990351112e-06,
      "loss": 0.6087,
      "step": 34340
    },
    {
      "epoch": 2.754611066559743,
      "grad_norm": 1.9777132272720337,
      "learning_rate": 2.4701152506030556e-06,
      "loss": 0.6112,
      "step": 34350
    },
    {
      "epoch": 2.7554129911788294,
      "grad_norm": 2.0350263118743896,
      "learning_rate": 2.462074510854999e-06,
      "loss": 0.6119,
      "step": 34360
    },
    {
      "epoch": 2.7562149157979148,
      "grad_norm": 1.8594344854354858,
      "learning_rate": 2.4540337711069416e-06,
      "loss": 0.5451,
      "step": 34370
    },
    {
      "epoch": 2.757016840417001,
      "grad_norm": 1.781150460243225,
      "learning_rate": 2.445993031358885e-06,
      "loss": 0.5835,
      "step": 34380
    },
    {
      "epoch": 2.7578187650360864,
      "grad_norm": 2.164151191711426,
      "learning_rate": 2.4379522916108285e-06,
      "loss": 0.5981,
      "step": 34390
    },
    {
      "epoch": 2.7586206896551726,
      "grad_norm": 1.640185832977295,
      "learning_rate": 2.429911551862771e-06,
      "loss": 0.5455,
      "step": 34400
    },
    {
      "epoch": 2.759422614274258,
      "grad_norm": 2.08711314201355,
      "learning_rate": 2.4218708121147146e-06,
      "loss": 0.5747,
      "step": 34410
    },
    {
      "epoch": 2.760224538893344,
      "grad_norm": 1.8933430910110474,
      "learning_rate": 2.413830072366658e-06,
      "loss": 0.6603,
      "step": 34420
    },
    {
      "epoch": 2.76102646351243,
      "grad_norm": 2.3577675819396973,
      "learning_rate": 2.405789332618601e-06,
      "loss": 0.5975,
      "step": 34430
    },
    {
      "epoch": 2.7618283881315158,
      "grad_norm": 2.177060604095459,
      "learning_rate": 2.397748592870544e-06,
      "loss": 0.5958,
      "step": 34440
    },
    {
      "epoch": 2.7626303127506016,
      "grad_norm": 1.9651269912719727,
      "learning_rate": 2.3897078531224875e-06,
      "loss": 0.6271,
      "step": 34450
    },
    {
      "epoch": 2.7634322373696873,
      "grad_norm": 1.7848323583602905,
      "learning_rate": 2.3816671133744305e-06,
      "loss": 0.6192,
      "step": 34460
    },
    {
      "epoch": 2.764234161988773,
      "grad_norm": 1.944693922996521,
      "learning_rate": 2.3736263736263735e-06,
      "loss": 0.6192,
      "step": 34470
    },
    {
      "epoch": 2.765036086607859,
      "grad_norm": 1.9286283254623413,
      "learning_rate": 2.365585633878317e-06,
      "loss": 0.6077,
      "step": 34480
    },
    {
      "epoch": 2.7658380112269447,
      "grad_norm": 1.8503625392913818,
      "learning_rate": 2.35754489413026e-06,
      "loss": 0.608,
      "step": 34490
    },
    {
      "epoch": 2.7666399358460305,
      "grad_norm": 1.4514961242675781,
      "learning_rate": 2.349504154382203e-06,
      "loss": 0.5528,
      "step": 34500
    },
    {
      "epoch": 2.7674418604651163,
      "grad_norm": 1.7526936531066895,
      "learning_rate": 2.3414634146341465e-06,
      "loss": 0.5886,
      "step": 34510
    },
    {
      "epoch": 2.768243785084202,
      "grad_norm": 1.846930742263794,
      "learning_rate": 2.33342267488609e-06,
      "loss": 0.53,
      "step": 34520
    },
    {
      "epoch": 2.769045709703288,
      "grad_norm": 1.685981035232544,
      "learning_rate": 2.3253819351380325e-06,
      "loss": 0.5896,
      "step": 34530
    },
    {
      "epoch": 2.7698476343223737,
      "grad_norm": 1.7731883525848389,
      "learning_rate": 2.317341195389976e-06,
      "loss": 0.6535,
      "step": 34540
    },
    {
      "epoch": 2.7706495589414595,
      "grad_norm": 1.9737385511398315,
      "learning_rate": 2.3093004556419194e-06,
      "loss": 0.5633,
      "step": 34550
    },
    {
      "epoch": 2.7714514835605453,
      "grad_norm": 2.2206337451934814,
      "learning_rate": 2.301259715893862e-06,
      "loss": 0.62,
      "step": 34560
    },
    {
      "epoch": 2.772253408179631,
      "grad_norm": 1.9300367832183838,
      "learning_rate": 2.2932189761458054e-06,
      "loss": 0.6288,
      "step": 34570
    },
    {
      "epoch": 2.773055332798717,
      "grad_norm": 2.1017162799835205,
      "learning_rate": 2.285178236397749e-06,
      "loss": 0.6191,
      "step": 34580
    },
    {
      "epoch": 2.7738572574178026,
      "grad_norm": 1.7669248580932617,
      "learning_rate": 2.277137496649692e-06,
      "loss": 0.5857,
      "step": 34590
    },
    {
      "epoch": 2.7746591820368884,
      "grad_norm": 1.8991551399230957,
      "learning_rate": 2.269096756901635e-06,
      "loss": 0.6127,
      "step": 34600
    },
    {
      "epoch": 2.7754611066559742,
      "grad_norm": 2.0205929279327393,
      "learning_rate": 2.2610560171535784e-06,
      "loss": 0.6431,
      "step": 34610
    },
    {
      "epoch": 2.77626303127506,
      "grad_norm": 2.5748631954193115,
      "learning_rate": 2.2530152774055214e-06,
      "loss": 0.6431,
      "step": 34620
    },
    {
      "epoch": 2.777064955894146,
      "grad_norm": 2.3138413429260254,
      "learning_rate": 2.2449745376574644e-06,
      "loss": 0.5686,
      "step": 34630
    },
    {
      "epoch": 2.7778668805132316,
      "grad_norm": 2.0084128379821777,
      "learning_rate": 2.236933797909408e-06,
      "loss": 0.6566,
      "step": 34640
    },
    {
      "epoch": 2.7786688051323174,
      "grad_norm": 2.031890869140625,
      "learning_rate": 2.228893058161351e-06,
      "loss": 0.6103,
      "step": 34650
    },
    {
      "epoch": 2.779470729751403,
      "grad_norm": 1.6904023885726929,
      "learning_rate": 2.220852318413294e-06,
      "loss": 0.5711,
      "step": 34660
    },
    {
      "epoch": 2.7802726543704894,
      "grad_norm": 1.8481321334838867,
      "learning_rate": 2.2128115786652373e-06,
      "loss": 0.5811,
      "step": 34670
    },
    {
      "epoch": 2.7810745789895748,
      "grad_norm": 2.3733816146850586,
      "learning_rate": 2.2047708389171808e-06,
      "loss": 0.65,
      "step": 34680
    },
    {
      "epoch": 2.781876503608661,
      "grad_norm": 2.2071921825408936,
      "learning_rate": 2.1967300991691234e-06,
      "loss": 0.6401,
      "step": 34690
    },
    {
      "epoch": 2.7826784282277464,
      "grad_norm": 2.069204568862915,
      "learning_rate": 2.188689359421067e-06,
      "loss": 0.6753,
      "step": 34700
    },
    {
      "epoch": 2.7834803528468326,
      "grad_norm": 1.6525145769119263,
      "learning_rate": 2.1806486196730102e-06,
      "loss": 0.6116,
      "step": 34710
    },
    {
      "epoch": 2.784282277465918,
      "grad_norm": 2.0114147663116455,
      "learning_rate": 2.172607879924953e-06,
      "loss": 0.6245,
      "step": 34720
    },
    {
      "epoch": 2.785084202085004,
      "grad_norm": 1.5981204509735107,
      "learning_rate": 2.1645671401768963e-06,
      "loss": 0.5521,
      "step": 34730
    },
    {
      "epoch": 2.78588612670409,
      "grad_norm": 1.9376169443130493,
      "learning_rate": 2.1565264004288397e-06,
      "loss": 0.604,
      "step": 34740
    },
    {
      "epoch": 2.7866880513231758,
      "grad_norm": 1.7917274236679077,
      "learning_rate": 2.1484856606807827e-06,
      "loss": 0.6442,
      "step": 34750
    },
    {
      "epoch": 2.7874899759422616,
      "grad_norm": 2.086873769760132,
      "learning_rate": 2.1404449209327258e-06,
      "loss": 0.6212,
      "step": 34760
    },
    {
      "epoch": 2.7882919005613473,
      "grad_norm": 1.7595096826553345,
      "learning_rate": 2.132404181184669e-06,
      "loss": 0.6364,
      "step": 34770
    },
    {
      "epoch": 2.789093825180433,
      "grad_norm": 1.8775553703308105,
      "learning_rate": 2.1243634414366122e-06,
      "loss": 0.5342,
      "step": 34780
    },
    {
      "epoch": 2.789895749799519,
      "grad_norm": 2.0033087730407715,
      "learning_rate": 2.1163227016885552e-06,
      "loss": 0.6348,
      "step": 34790
    },
    {
      "epoch": 2.7906976744186047,
      "grad_norm": 2.289321184158325,
      "learning_rate": 2.1082819619404987e-06,
      "loss": 0.5877,
      "step": 34800
    },
    {
      "epoch": 2.7914995990376905,
      "grad_norm": 1.8220441341400146,
      "learning_rate": 2.1002412221924417e-06,
      "loss": 0.5933,
      "step": 34810
    },
    {
      "epoch": 2.7923015236567763,
      "grad_norm": 1.6064860820770264,
      "learning_rate": 2.0922004824443847e-06,
      "loss": 0.5749,
      "step": 34820
    },
    {
      "epoch": 2.793103448275862,
      "grad_norm": 2.295273542404175,
      "learning_rate": 2.084159742696328e-06,
      "loss": 0.5921,
      "step": 34830
    },
    {
      "epoch": 2.793905372894948,
      "grad_norm": 1.7224754095077515,
      "learning_rate": 2.0761190029482716e-06,
      "loss": 0.6103,
      "step": 34840
    },
    {
      "epoch": 2.7947072975140337,
      "grad_norm": 1.8236721754074097,
      "learning_rate": 2.068078263200214e-06,
      "loss": 0.5936,
      "step": 34850
    },
    {
      "epoch": 2.7955092221331195,
      "grad_norm": 2.122205972671509,
      "learning_rate": 2.0600375234521577e-06,
      "loss": 0.5525,
      "step": 34860
    },
    {
      "epoch": 2.7963111467522053,
      "grad_norm": 2.2063684463500977,
      "learning_rate": 2.051996783704101e-06,
      "loss": 0.5968,
      "step": 34870
    },
    {
      "epoch": 2.797113071371291,
      "grad_norm": 1.8937853574752808,
      "learning_rate": 2.0439560439560437e-06,
      "loss": 0.5799,
      "step": 34880
    },
    {
      "epoch": 2.797914995990377,
      "grad_norm": 2.043926477432251,
      "learning_rate": 2.035915304207987e-06,
      "loss": 0.6037,
      "step": 34890
    },
    {
      "epoch": 2.7987169206094626,
      "grad_norm": 1.9208133220672607,
      "learning_rate": 2.0278745644599306e-06,
      "loss": 0.591,
      "step": 34900
    },
    {
      "epoch": 2.7995188452285484,
      "grad_norm": 1.7829045057296753,
      "learning_rate": 2.0198338247118736e-06,
      "loss": 0.7136,
      "step": 34910
    },
    {
      "epoch": 2.8003207698476342,
      "grad_norm": 1.9774161577224731,
      "learning_rate": 2.0117930849638166e-06,
      "loss": 0.632,
      "step": 34920
    },
    {
      "epoch": 2.80112269446672,
      "grad_norm": 1.6953315734863281,
      "learning_rate": 2.00375234521576e-06,
      "loss": 0.5687,
      "step": 34930
    },
    {
      "epoch": 2.801924619085806,
      "grad_norm": 2.6142804622650146,
      "learning_rate": 1.995711605467703e-06,
      "loss": 0.6837,
      "step": 34940
    },
    {
      "epoch": 2.8027265437048916,
      "grad_norm": 1.6526745557785034,
      "learning_rate": 1.987670865719646e-06,
      "loss": 0.6612,
      "step": 34950
    },
    {
      "epoch": 2.8035284683239774,
      "grad_norm": 1.8449708223342896,
      "learning_rate": 1.9796301259715895e-06,
      "loss": 0.6511,
      "step": 34960
    },
    {
      "epoch": 2.804330392943063,
      "grad_norm": 1.7270580530166626,
      "learning_rate": 1.9715893862235326e-06,
      "loss": 0.5726,
      "step": 34970
    },
    {
      "epoch": 2.8051323175621494,
      "grad_norm": 1.9587998390197754,
      "learning_rate": 1.9635486464754756e-06,
      "loss": 0.6162,
      "step": 34980
    },
    {
      "epoch": 2.8059342421812348,
      "grad_norm": 1.9108107089996338,
      "learning_rate": 1.955507906727419e-06,
      "loss": 0.708,
      "step": 34990
    },
    {
      "epoch": 2.806736166800321,
      "grad_norm": 2.1500015258789062,
      "learning_rate": 1.9474671669793625e-06,
      "loss": 0.594,
      "step": 35000
    },
    {
      "epoch": 2.8075380914194064,
      "grad_norm": 2.310567617416382,
      "learning_rate": 1.939426427231305e-06,
      "loss": 0.6559,
      "step": 35010
    },
    {
      "epoch": 2.8083400160384926,
      "grad_norm": 1.9997414350509644,
      "learning_rate": 1.9313856874832485e-06,
      "loss": 0.6456,
      "step": 35020
    },
    {
      "epoch": 2.809141940657578,
      "grad_norm": 1.6874338388442993,
      "learning_rate": 1.923344947735192e-06,
      "loss": 0.5776,
      "step": 35030
    },
    {
      "epoch": 2.809943865276664,
      "grad_norm": 2.003319501876831,
      "learning_rate": 1.9153042079871345e-06,
      "loss": 0.5647,
      "step": 35040
    },
    {
      "epoch": 2.8107457898957495,
      "grad_norm": 1.9578204154968262,
      "learning_rate": 1.907263468239078e-06,
      "loss": 0.6177,
      "step": 35050
    },
    {
      "epoch": 2.8115477145148358,
      "grad_norm": 1.934749722480774,
      "learning_rate": 1.8992227284910212e-06,
      "loss": 0.6026,
      "step": 35060
    },
    {
      "epoch": 2.8123496391339216,
      "grad_norm": 2.1214423179626465,
      "learning_rate": 1.8911819887429647e-06,
      "loss": 0.6767,
      "step": 35070
    },
    {
      "epoch": 2.8131515637530073,
      "grad_norm": 1.7561180591583252,
      "learning_rate": 1.8831412489949075e-06,
      "loss": 0.5966,
      "step": 35080
    },
    {
      "epoch": 2.813953488372093,
      "grad_norm": 1.6696521043777466,
      "learning_rate": 1.875100509246851e-06,
      "loss": 0.6366,
      "step": 35090
    },
    {
      "epoch": 2.814755412991179,
      "grad_norm": 1.739770531654358,
      "learning_rate": 1.867059769498794e-06,
      "loss": 0.5357,
      "step": 35100
    },
    {
      "epoch": 2.8155573376102647,
      "grad_norm": 1.9636976718902588,
      "learning_rate": 1.8590190297507372e-06,
      "loss": 0.5376,
      "step": 35110
    },
    {
      "epoch": 2.8163592622293505,
      "grad_norm": 1.9255032539367676,
      "learning_rate": 1.8509782900026804e-06,
      "loss": 0.5805,
      "step": 35120
    },
    {
      "epoch": 2.8171611868484363,
      "grad_norm": 1.8747142553329468,
      "learning_rate": 1.8429375502546234e-06,
      "loss": 0.6797,
      "step": 35130
    },
    {
      "epoch": 2.817963111467522,
      "grad_norm": 1.9411994218826294,
      "learning_rate": 1.8348968105065666e-06,
      "loss": 0.6156,
      "step": 35140
    },
    {
      "epoch": 2.818765036086608,
      "grad_norm": 1.981455683708191,
      "learning_rate": 1.8268560707585099e-06,
      "loss": 0.6121,
      "step": 35150
    },
    {
      "epoch": 2.8195669607056937,
      "grad_norm": 2.1765222549438477,
      "learning_rate": 1.818815331010453e-06,
      "loss": 0.6394,
      "step": 35160
    },
    {
      "epoch": 2.8203688853247795,
      "grad_norm": 2.1783907413482666,
      "learning_rate": 1.8107745912623963e-06,
      "loss": 0.6368,
      "step": 35170
    },
    {
      "epoch": 2.8211708099438653,
      "grad_norm": 2.918875217437744,
      "learning_rate": 1.8027338515143394e-06,
      "loss": 0.6932,
      "step": 35180
    },
    {
      "epoch": 2.821972734562951,
      "grad_norm": 1.4066576957702637,
      "learning_rate": 1.7946931117662826e-06,
      "loss": 0.5483,
      "step": 35190
    },
    {
      "epoch": 2.822774659182037,
      "grad_norm": 1.7501819133758545,
      "learning_rate": 1.7866523720182258e-06,
      "loss": 0.5964,
      "step": 35200
    },
    {
      "epoch": 2.8235765838011226,
      "grad_norm": 1.6955667734146118,
      "learning_rate": 1.7786116322701688e-06,
      "loss": 0.557,
      "step": 35210
    },
    {
      "epoch": 2.8243785084202084,
      "grad_norm": 1.713505506515503,
      "learning_rate": 1.770570892522112e-06,
      "loss": 0.6436,
      "step": 35220
    },
    {
      "epoch": 2.8251804330392942,
      "grad_norm": 1.9837017059326172,
      "learning_rate": 1.7625301527740553e-06,
      "loss": 0.6049,
      "step": 35230
    },
    {
      "epoch": 2.82598235765838,
      "grad_norm": 2.13645601272583,
      "learning_rate": 1.7544894130259983e-06,
      "loss": 0.5601,
      "step": 35240
    },
    {
      "epoch": 2.826784282277466,
      "grad_norm": 1.785157561302185,
      "learning_rate": 1.7464486732779418e-06,
      "loss": 0.6453,
      "step": 35250
    },
    {
      "epoch": 2.8275862068965516,
      "grad_norm": 2.1260106563568115,
      "learning_rate": 1.7384079335298848e-06,
      "loss": 0.6218,
      "step": 35260
    },
    {
      "epoch": 2.8283881315156374,
      "grad_norm": 1.8834669589996338,
      "learning_rate": 1.730367193781828e-06,
      "loss": 0.6134,
      "step": 35270
    },
    {
      "epoch": 2.829190056134723,
      "grad_norm": 1.9238412380218506,
      "learning_rate": 1.7223264540337712e-06,
      "loss": 0.5717,
      "step": 35280
    },
    {
      "epoch": 2.829991980753809,
      "grad_norm": 1.9066529273986816,
      "learning_rate": 1.7142857142857143e-06,
      "loss": 0.6172,
      "step": 35290
    },
    {
      "epoch": 2.8307939053728948,
      "grad_norm": 2.283301830291748,
      "learning_rate": 1.7062449745376575e-06,
      "loss": 0.6045,
      "step": 35300
    },
    {
      "epoch": 2.831595829991981,
      "grad_norm": 1.9088116884231567,
      "learning_rate": 1.6982042347896007e-06,
      "loss": 0.6945,
      "step": 35310
    },
    {
      "epoch": 2.8323977546110664,
      "grad_norm": 1.7707793712615967,
      "learning_rate": 1.6901634950415438e-06,
      "loss": 0.5985,
      "step": 35320
    },
    {
      "epoch": 2.8331996792301526,
      "grad_norm": 2.1714537143707275,
      "learning_rate": 1.6821227552934872e-06,
      "loss": 0.5715,
      "step": 35330
    },
    {
      "epoch": 2.834001603849238,
      "grad_norm": 1.7825435400009155,
      "learning_rate": 1.6740820155454302e-06,
      "loss": 0.62,
      "step": 35340
    },
    {
      "epoch": 2.834803528468324,
      "grad_norm": 1.565650224685669,
      "learning_rate": 1.666845349772179e-06,
      "loss": 0.6137,
      "step": 35350
    },
    {
      "epoch": 2.8356054530874095,
      "grad_norm": 1.7376863956451416,
      "learning_rate": 1.6588046100241223e-06,
      "loss": 0.5843,
      "step": 35360
    },
    {
      "epoch": 2.8364073777064958,
      "grad_norm": 1.9440563917160034,
      "learning_rate": 1.6507638702760654e-06,
      "loss": 0.5645,
      "step": 35370
    },
    {
      "epoch": 2.8372093023255816,
      "grad_norm": 1.9705880880355835,
      "learning_rate": 1.6427231305280086e-06,
      "loss": 0.6537,
      "step": 35380
    },
    {
      "epoch": 2.8380112269446673,
      "grad_norm": 2.0978405475616455,
      "learning_rate": 1.6346823907799518e-06,
      "loss": 0.5822,
      "step": 35390
    },
    {
      "epoch": 2.838813151563753,
      "grad_norm": 2.09375,
      "learning_rate": 1.6266416510318948e-06,
      "loss": 0.5919,
      "step": 35400
    },
    {
      "epoch": 2.839615076182839,
      "grad_norm": 1.7748003005981445,
      "learning_rate": 1.6186009112838383e-06,
      "loss": 0.62,
      "step": 35410
    },
    {
      "epoch": 2.8404170008019247,
      "grad_norm": 2.1287248134613037,
      "learning_rate": 1.6105601715357813e-06,
      "loss": 0.5871,
      "step": 35420
    },
    {
      "epoch": 2.8412189254210105,
      "grad_norm": 1.9367008209228516,
      "learning_rate": 1.6025194317877245e-06,
      "loss": 0.6093,
      "step": 35430
    },
    {
      "epoch": 2.8420208500400963,
      "grad_norm": 1.7795405387878418,
      "learning_rate": 1.5944786920396678e-06,
      "loss": 0.5939,
      "step": 35440
    },
    {
      "epoch": 2.842822774659182,
      "grad_norm": 2.0782883167266846,
      "learning_rate": 1.5864379522916108e-06,
      "loss": 0.6695,
      "step": 35450
    },
    {
      "epoch": 2.843624699278268,
      "grad_norm": 2.009437084197998,
      "learning_rate": 1.578397212543554e-06,
      "loss": 0.646,
      "step": 35460
    },
    {
      "epoch": 2.8444266238973537,
      "grad_norm": 1.889669418334961,
      "learning_rate": 1.5703564727954972e-06,
      "loss": 0.683,
      "step": 35470
    },
    {
      "epoch": 2.8452285485164395,
      "grad_norm": 2.276965379714966,
      "learning_rate": 1.5623157330474403e-06,
      "loss": 0.6425,
      "step": 35480
    },
    {
      "epoch": 2.8460304731355253,
      "grad_norm": 2.1793813705444336,
      "learning_rate": 1.5550790672741894e-06,
      "loss": 0.6275,
      "step": 35490
    },
    {
      "epoch": 2.846832397754611,
      "grad_norm": 1.8216170072555542,
      "learning_rate": 1.5470383275261324e-06,
      "loss": 0.5618,
      "step": 35500
    },
    {
      "epoch": 2.847634322373697,
      "grad_norm": 2.0405900478363037,
      "learning_rate": 1.5389975877780756e-06,
      "loss": 0.6313,
      "step": 35510
    },
    {
      "epoch": 2.8484362469927826,
      "grad_norm": 1.9283719062805176,
      "learning_rate": 1.5309568480300188e-06,
      "loss": 0.6594,
      "step": 35520
    },
    {
      "epoch": 2.8492381716118684,
      "grad_norm": 1.9390535354614258,
      "learning_rate": 1.5229161082819619e-06,
      "loss": 0.6262,
      "step": 35530
    },
    {
      "epoch": 2.8500400962309542,
      "grad_norm": 2.586308717727661,
      "learning_rate": 1.5148753685339053e-06,
      "loss": 0.6545,
      "step": 35540
    },
    {
      "epoch": 2.85084202085004,
      "grad_norm": 1.9020711183547974,
      "learning_rate": 1.5068346287858483e-06,
      "loss": 0.6362,
      "step": 35550
    },
    {
      "epoch": 2.851643945469126,
      "grad_norm": 1.8247359991073608,
      "learning_rate": 1.4987938890377916e-06,
      "loss": 0.5371,
      "step": 35560
    },
    {
      "epoch": 2.8524458700882116,
      "grad_norm": 2.0383384227752686,
      "learning_rate": 1.4907531492897348e-06,
      "loss": 0.589,
      "step": 35570
    },
    {
      "epoch": 2.8532477947072974,
      "grad_norm": 2.027364730834961,
      "learning_rate": 1.4827124095416778e-06,
      "loss": 0.5035,
      "step": 35580
    },
    {
      "epoch": 2.854049719326383,
      "grad_norm": 1.953137993812561,
      "learning_rate": 1.474671669793621e-06,
      "loss": 0.5864,
      "step": 35590
    },
    {
      "epoch": 2.854851643945469,
      "grad_norm": 1.904859185218811,
      "learning_rate": 1.4666309300455643e-06,
      "loss": 0.6573,
      "step": 35600
    },
    {
      "epoch": 2.8556535685645548,
      "grad_norm": 1.9921270608901978,
      "learning_rate": 1.4585901902975073e-06,
      "loss": 0.618,
      "step": 35610
    },
    {
      "epoch": 2.856455493183641,
      "grad_norm": 2.1910171508789062,
      "learning_rate": 1.4505494505494507e-06,
      "loss": 0.5956,
      "step": 35620
    },
    {
      "epoch": 2.8572574178027264,
      "grad_norm": 1.6965770721435547,
      "learning_rate": 1.4425087108013938e-06,
      "loss": 0.5807,
      "step": 35630
    },
    {
      "epoch": 2.8580593424218126,
      "grad_norm": 2.190202236175537,
      "learning_rate": 1.434467971053337e-06,
      "loss": 0.6302,
      "step": 35640
    },
    {
      "epoch": 2.858861267040898,
      "grad_norm": 1.8484212160110474,
      "learning_rate": 1.4264272313052802e-06,
      "loss": 0.5916,
      "step": 35650
    },
    {
      "epoch": 2.859663191659984,
      "grad_norm": 1.8158910274505615,
      "learning_rate": 1.4183864915572232e-06,
      "loss": 0.6541,
      "step": 35660
    },
    {
      "epoch": 2.8604651162790695,
      "grad_norm": 1.7146599292755127,
      "learning_rate": 1.4103457518091665e-06,
      "loss": 0.5999,
      "step": 35670
    },
    {
      "epoch": 2.8612670408981558,
      "grad_norm": 2.0589070320129395,
      "learning_rate": 1.4023050120611097e-06,
      "loss": 0.63,
      "step": 35680
    },
    {
      "epoch": 2.862068965517241,
      "grad_norm": 1.7616807222366333,
      "learning_rate": 1.3942642723130527e-06,
      "loss": 0.5706,
      "step": 35690
    },
    {
      "epoch": 2.8628708901363273,
      "grad_norm": 1.7661525011062622,
      "learning_rate": 1.3862235325649962e-06,
      "loss": 0.6458,
      "step": 35700
    },
    {
      "epoch": 2.863672814755413,
      "grad_norm": 1.6785333156585693,
      "learning_rate": 1.3781827928169392e-06,
      "loss": 0.6554,
      "step": 35710
    },
    {
      "epoch": 2.864474739374499,
      "grad_norm": 1.9093486070632935,
      "learning_rate": 1.3701420530688824e-06,
      "loss": 0.5821,
      "step": 35720
    },
    {
      "epoch": 2.8652766639935847,
      "grad_norm": 1.8524980545043945,
      "learning_rate": 1.3621013133208256e-06,
      "loss": 0.5671,
      "step": 35730
    },
    {
      "epoch": 2.8660785886126705,
      "grad_norm": 1.7346354722976685,
      "learning_rate": 1.3540605735727687e-06,
      "loss": 0.6138,
      "step": 35740
    },
    {
      "epoch": 2.8668805132317563,
      "grad_norm": 2.062337636947632,
      "learning_rate": 1.3460198338247119e-06,
      "loss": 0.6203,
      "step": 35750
    },
    {
      "epoch": 2.867682437850842,
      "grad_norm": 2.547499179840088,
      "learning_rate": 1.3379790940766551e-06,
      "loss": 0.6681,
      "step": 35760
    },
    {
      "epoch": 2.868484362469928,
      "grad_norm": 1.7083134651184082,
      "learning_rate": 1.3299383543285981e-06,
      "loss": 0.6204,
      "step": 35770
    },
    {
      "epoch": 2.8692862870890137,
      "grad_norm": 2.0021543502807617,
      "learning_rate": 1.3218976145805416e-06,
      "loss": 0.6119,
      "step": 35780
    },
    {
      "epoch": 2.8700882117080995,
      "grad_norm": 1.9080708026885986,
      "learning_rate": 1.3138568748324846e-06,
      "loss": 0.5951,
      "step": 35790
    },
    {
      "epoch": 2.8708901363271853,
      "grad_norm": 1.6144065856933594,
      "learning_rate": 1.3058161350844278e-06,
      "loss": 0.5443,
      "step": 35800
    },
    {
      "epoch": 2.871692060946271,
      "grad_norm": 1.8098639249801636,
      "learning_rate": 1.297775395336371e-06,
      "loss": 0.6682,
      "step": 35810
    },
    {
      "epoch": 2.872493985565357,
      "grad_norm": 1.8670436143875122,
      "learning_rate": 1.289734655588314e-06,
      "loss": 0.6543,
      "step": 35820
    },
    {
      "epoch": 2.8732959101844426,
      "grad_norm": 2.230445146560669,
      "learning_rate": 1.2816939158402573e-06,
      "loss": 0.623,
      "step": 35830
    },
    {
      "epoch": 2.8740978348035284,
      "grad_norm": 1.8490359783172607,
      "learning_rate": 1.2736531760922005e-06,
      "loss": 0.6032,
      "step": 35840
    },
    {
      "epoch": 2.874899759422614,
      "grad_norm": 2.0475594997406006,
      "learning_rate": 1.2656124363441436e-06,
      "loss": 0.57,
      "step": 35850
    },
    {
      "epoch": 2.8757016840417,
      "grad_norm": 1.630606770515442,
      "learning_rate": 1.257571696596087e-06,
      "loss": 0.6139,
      "step": 35860
    },
    {
      "epoch": 2.876503608660786,
      "grad_norm": 1.7196393013000488,
      "learning_rate": 1.24953095684803e-06,
      "loss": 0.5605,
      "step": 35870
    },
    {
      "epoch": 2.8773055332798716,
      "grad_norm": 2.1979002952575684,
      "learning_rate": 1.2414902170999733e-06,
      "loss": 0.6198,
      "step": 35880
    },
    {
      "epoch": 2.8781074578989574,
      "grad_norm": 2.0263681411743164,
      "learning_rate": 1.2334494773519165e-06,
      "loss": 0.5834,
      "step": 35890
    },
    {
      "epoch": 2.878909382518043,
      "grad_norm": 1.929652214050293,
      "learning_rate": 1.2254087376038595e-06,
      "loss": 0.5079,
      "step": 35900
    },
    {
      "epoch": 2.879711307137129,
      "grad_norm": 1.5172855854034424,
      "learning_rate": 1.2173679978558027e-06,
      "loss": 0.5521,
      "step": 35910
    },
    {
      "epoch": 2.8805132317562148,
      "grad_norm": 1.9692118167877197,
      "learning_rate": 1.209327258107746e-06,
      "loss": 0.6313,
      "step": 35920
    },
    {
      "epoch": 2.8813151563753006,
      "grad_norm": 1.898971438407898,
      "learning_rate": 1.201286518359689e-06,
      "loss": 0.5823,
      "step": 35930
    },
    {
      "epoch": 2.8821170809943863,
      "grad_norm": 2.2402586936950684,
      "learning_rate": 1.1932457786116324e-06,
      "loss": 0.6155,
      "step": 35940
    },
    {
      "epoch": 2.8829190056134726,
      "grad_norm": 2.0944907665252686,
      "learning_rate": 1.1852050388635755e-06,
      "loss": 0.6862,
      "step": 35950
    },
    {
      "epoch": 2.883720930232558,
      "grad_norm": 1.7282021045684814,
      "learning_rate": 1.1771642991155187e-06,
      "loss": 0.5882,
      "step": 35960
    },
    {
      "epoch": 2.884522854851644,
      "grad_norm": 1.7019145488739014,
      "learning_rate": 1.169123559367462e-06,
      "loss": 0.5991,
      "step": 35970
    },
    {
      "epoch": 2.8853247794707295,
      "grad_norm": 1.9791698455810547,
      "learning_rate": 1.161082819619405e-06,
      "loss": 0.5844,
      "step": 35980
    },
    {
      "epoch": 2.8861267040898158,
      "grad_norm": 1.9164437055587769,
      "learning_rate": 1.1530420798713482e-06,
      "loss": 0.561,
      "step": 35990
    },
    {
      "epoch": 2.886928628708901,
      "grad_norm": 1.6335711479187012,
      "learning_rate": 1.1450013401232914e-06,
      "loss": 0.6109,
      "step": 36000
    },
    {
      "epoch": 2.8877305533279873,
      "grad_norm": 2.1357829570770264,
      "learning_rate": 1.1369606003752344e-06,
      "loss": 0.6133,
      "step": 36010
    },
    {
      "epoch": 2.888532477947073,
      "grad_norm": 1.7830554246902466,
      "learning_rate": 1.1289198606271779e-06,
      "loss": 0.6264,
      "step": 36020
    },
    {
      "epoch": 2.889334402566159,
      "grad_norm": 2.589782476425171,
      "learning_rate": 1.1208791208791209e-06,
      "loss": 0.6487,
      "step": 36030
    },
    {
      "epoch": 2.8901363271852447,
      "grad_norm": 1.8195854425430298,
      "learning_rate": 1.1128383811310641e-06,
      "loss": 0.701,
      "step": 36040
    },
    {
      "epoch": 2.8909382518043305,
      "grad_norm": 2.0747487545013428,
      "learning_rate": 1.1047976413830073e-06,
      "loss": 0.6363,
      "step": 36050
    },
    {
      "epoch": 2.8917401764234163,
      "grad_norm": 2.1617889404296875,
      "learning_rate": 1.0967569016349504e-06,
      "loss": 0.5476,
      "step": 36060
    },
    {
      "epoch": 2.892542101042502,
      "grad_norm": 1.7481169700622559,
      "learning_rate": 1.0887161618868936e-06,
      "loss": 0.6254,
      "step": 36070
    },
    {
      "epoch": 2.893344025661588,
      "grad_norm": 1.7146275043487549,
      "learning_rate": 1.0806754221388368e-06,
      "loss": 0.6464,
      "step": 36080
    },
    {
      "epoch": 2.8941459502806737,
      "grad_norm": 1.905320405960083,
      "learning_rate": 1.0726346823907798e-06,
      "loss": 0.575,
      "step": 36090
    },
    {
      "epoch": 2.8949478748997595,
      "grad_norm": 2.4427711963653564,
      "learning_rate": 1.0645939426427233e-06,
      "loss": 0.658,
      "step": 36100
    },
    {
      "epoch": 2.8957497995188453,
      "grad_norm": 2.061117172241211,
      "learning_rate": 1.0565532028946663e-06,
      "loss": 0.7133,
      "step": 36110
    },
    {
      "epoch": 2.896551724137931,
      "grad_norm": 1.811456322669983,
      "learning_rate": 1.0485124631466095e-06,
      "loss": 0.5717,
      "step": 36120
    },
    {
      "epoch": 2.897353648757017,
      "grad_norm": 2.0203912258148193,
      "learning_rate": 1.0404717233985528e-06,
      "loss": 0.6026,
      "step": 36130
    },
    {
      "epoch": 2.8981555733761026,
      "grad_norm": 1.914724588394165,
      "learning_rate": 1.0324309836504958e-06,
      "loss": 0.5229,
      "step": 36140
    },
    {
      "epoch": 2.8989574979951884,
      "grad_norm": 1.69084894657135,
      "learning_rate": 1.024390243902439e-06,
      "loss": 0.5821,
      "step": 36150
    },
    {
      "epoch": 2.899759422614274,
      "grad_norm": 1.7271852493286133,
      "learning_rate": 1.0163495041543823e-06,
      "loss": 0.6021,
      "step": 36160
    },
    {
      "epoch": 2.90056134723336,
      "grad_norm": 2.11716890335083,
      "learning_rate": 1.0083087644063253e-06,
      "loss": 0.6452,
      "step": 36170
    },
    {
      "epoch": 2.901363271852446,
      "grad_norm": 1.920004963874817,
      "learning_rate": 1.0002680246582687e-06,
      "loss": 0.573,
      "step": 36180
    },
    {
      "epoch": 2.9021651964715316,
      "grad_norm": 1.5774881839752197,
      "learning_rate": 9.922272849102117e-07,
      "loss": 0.5875,
      "step": 36190
    },
    {
      "epoch": 2.9029671210906174,
      "grad_norm": 1.9511948823928833,
      "learning_rate": 9.84186545162155e-07,
      "loss": 0.6084,
      "step": 36200
    },
    {
      "epoch": 2.903769045709703,
      "grad_norm": 1.9218703508377075,
      "learning_rate": 9.761458054140982e-07,
      "loss": 0.6079,
      "step": 36210
    },
    {
      "epoch": 2.904570970328789,
      "grad_norm": 1.9229857921600342,
      "learning_rate": 9.681050656660412e-07,
      "loss": 0.5625,
      "step": 36220
    },
    {
      "epoch": 2.9053728949478748,
      "grad_norm": 1.729052186012268,
      "learning_rate": 9.600643259179845e-07,
      "loss": 0.6439,
      "step": 36230
    },
    {
      "epoch": 2.9061748195669606,
      "grad_norm": 1.801519751548767,
      "learning_rate": 9.520235861699277e-07,
      "loss": 0.5945,
      "step": 36240
    },
    {
      "epoch": 2.9069767441860463,
      "grad_norm": 2.1834709644317627,
      "learning_rate": 9.439828464218708e-07,
      "loss": 0.5855,
      "step": 36250
    },
    {
      "epoch": 2.9077786688051326,
      "grad_norm": 2.050875663757324,
      "learning_rate": 9.35942106673814e-07,
      "loss": 0.586,
      "step": 36260
    },
    {
      "epoch": 2.908580593424218,
      "grad_norm": 1.9253228902816772,
      "learning_rate": 9.279013669257572e-07,
      "loss": 0.6218,
      "step": 36270
    },
    {
      "epoch": 2.909382518043304,
      "grad_norm": 1.842686414718628,
      "learning_rate": 9.198606271777004e-07,
      "loss": 0.5425,
      "step": 36280
    },
    {
      "epoch": 2.9101844426623895,
      "grad_norm": 1.7674640417099,
      "learning_rate": 9.118198874296436e-07,
      "loss": 0.5806,
      "step": 36290
    },
    {
      "epoch": 2.9109863672814758,
      "grad_norm": 2.214496374130249,
      "learning_rate": 9.037791476815868e-07,
      "loss": 0.6458,
      "step": 36300
    },
    {
      "epoch": 2.911788291900561,
      "grad_norm": 2.269157648086548,
      "learning_rate": 8.957384079335299e-07,
      "loss": 0.6,
      "step": 36310
    },
    {
      "epoch": 2.9125902165196473,
      "grad_norm": 2.017674207687378,
      "learning_rate": 8.876976681854731e-07,
      "loss": 0.5906,
      "step": 36320
    },
    {
      "epoch": 2.9133921411387327,
      "grad_norm": 1.9605207443237305,
      "learning_rate": 8.796569284374163e-07,
      "loss": 0.6415,
      "step": 36330
    },
    {
      "epoch": 2.914194065757819,
      "grad_norm": 2.057040214538574,
      "learning_rate": 8.716161886893595e-07,
      "loss": 0.6216,
      "step": 36340
    },
    {
      "epoch": 2.9149959903769047,
      "grad_norm": 1.838912010192871,
      "learning_rate": 8.635754489413026e-07,
      "loss": 0.5894,
      "step": 36350
    },
    {
      "epoch": 2.9157979149959905,
      "grad_norm": 1.8225767612457275,
      "learning_rate": 8.555347091932458e-07,
      "loss": 0.5985,
      "step": 36360
    },
    {
      "epoch": 2.9165998396150763,
      "grad_norm": 1.8499900102615356,
      "learning_rate": 8.474939694451891e-07,
      "loss": 0.5556,
      "step": 36370
    },
    {
      "epoch": 2.917401764234162,
      "grad_norm": 1.9330408573150635,
      "learning_rate": 8.394532296971322e-07,
      "loss": 0.6349,
      "step": 36380
    },
    {
      "epoch": 2.918203688853248,
      "grad_norm": 2.3467190265655518,
      "learning_rate": 8.314124899490753e-07,
      "loss": 0.6397,
      "step": 36390
    },
    {
      "epoch": 2.9190056134723337,
      "grad_norm": 2.284497022628784,
      "learning_rate": 8.233717502010185e-07,
      "loss": 0.5639,
      "step": 36400
    },
    {
      "epoch": 2.9198075380914195,
      "grad_norm": 1.9733575582504272,
      "learning_rate": 8.153310104529618e-07,
      "loss": 0.5655,
      "step": 36410
    },
    {
      "epoch": 2.9206094627105053,
      "grad_norm": 1.9531974792480469,
      "learning_rate": 8.072902707049049e-07,
      "loss": 0.6027,
      "step": 36420
    },
    {
      "epoch": 2.921411387329591,
      "grad_norm": 1.7396296262741089,
      "learning_rate": 7.99249530956848e-07,
      "loss": 0.589,
      "step": 36430
    },
    {
      "epoch": 2.922213311948677,
      "grad_norm": 1.9194490909576416,
      "learning_rate": 7.912087912087912e-07,
      "loss": 0.6284,
      "step": 36440
    },
    {
      "epoch": 2.9230152365677626,
      "grad_norm": 2.0423245429992676,
      "learning_rate": 7.831680514607345e-07,
      "loss": 0.6843,
      "step": 36450
    },
    {
      "epoch": 2.9238171611868484,
      "grad_norm": 1.9482024908065796,
      "learning_rate": 7.751273117126776e-07,
      "loss": 0.6286,
      "step": 36460
    },
    {
      "epoch": 2.924619085805934,
      "grad_norm": 1.749466061592102,
      "learning_rate": 7.670865719646207e-07,
      "loss": 0.6438,
      "step": 36470
    },
    {
      "epoch": 2.92542101042502,
      "grad_norm": 2.445247173309326,
      "learning_rate": 7.59045832216564e-07,
      "loss": 0.6465,
      "step": 36480
    },
    {
      "epoch": 2.926222935044106,
      "grad_norm": 1.9219810962677002,
      "learning_rate": 7.510050924685072e-07,
      "loss": 0.5475,
      "step": 36490
    },
    {
      "epoch": 2.9270248596631916,
      "grad_norm": 1.9672313928604126,
      "learning_rate": 7.429643527204503e-07,
      "loss": 0.618,
      "step": 36500
    },
    {
      "epoch": 2.9278267842822774,
      "grad_norm": 1.7911659479141235,
      "learning_rate": 7.349236129723934e-07,
      "loss": 0.6556,
      "step": 36510
    },
    {
      "epoch": 2.928628708901363,
      "grad_norm": 2.1434237957000732,
      "learning_rate": 7.268828732243367e-07,
      "loss": 0.6488,
      "step": 36520
    },
    {
      "epoch": 2.929430633520449,
      "grad_norm": 2.3143794536590576,
      "learning_rate": 7.188421334762799e-07,
      "loss": 0.6319,
      "step": 36530
    },
    {
      "epoch": 2.9302325581395348,
      "grad_norm": 1.990975260734558,
      "learning_rate": 7.10801393728223e-07,
      "loss": 0.6587,
      "step": 36540
    },
    {
      "epoch": 2.9310344827586206,
      "grad_norm": 1.8217122554779053,
      "learning_rate": 7.027606539801662e-07,
      "loss": 0.6413,
      "step": 36550
    },
    {
      "epoch": 2.9318364073777063,
      "grad_norm": 2.045323610305786,
      "learning_rate": 6.947199142321094e-07,
      "loss": 0.6361,
      "step": 36560
    },
    {
      "epoch": 2.932638331996792,
      "grad_norm": 2.315436601638794,
      "learning_rate": 6.866791744840526e-07,
      "loss": 0.537,
      "step": 36570
    },
    {
      "epoch": 2.933440256615878,
      "grad_norm": 2.5027310848236084,
      "learning_rate": 6.786384347359957e-07,
      "loss": 0.6352,
      "step": 36580
    },
    {
      "epoch": 2.934242181234964,
      "grad_norm": 1.85934317111969,
      "learning_rate": 6.705976949879389e-07,
      "loss": 0.6143,
      "step": 36590
    },
    {
      "epoch": 2.9350441058540495,
      "grad_norm": 1.8666967153549194,
      "learning_rate": 6.625569552398821e-07,
      "loss": 0.6433,
      "step": 36600
    },
    {
      "epoch": 2.9358460304731357,
      "grad_norm": 2.14756178855896,
      "learning_rate": 6.545162154918253e-07,
      "loss": 0.6133,
      "step": 36610
    },
    {
      "epoch": 2.936647955092221,
      "grad_norm": 1.7454543113708496,
      "learning_rate": 6.464754757437685e-07,
      "loss": 0.5987,
      "step": 36620
    },
    {
      "epoch": 2.9374498797113073,
      "grad_norm": 2.494093418121338,
      "learning_rate": 6.384347359957116e-07,
      "loss": 0.5906,
      "step": 36630
    },
    {
      "epoch": 2.9382518043303927,
      "grad_norm": 1.9144623279571533,
      "learning_rate": 6.303939962476548e-07,
      "loss": 0.5775,
      "step": 36640
    },
    {
      "epoch": 2.939053728949479,
      "grad_norm": 2.1222712993621826,
      "learning_rate": 6.22353256499598e-07,
      "loss": 0.6254,
      "step": 36650
    },
    {
      "epoch": 2.9398556535685647,
      "grad_norm": 1.999367117881775,
      "learning_rate": 6.143125167515412e-07,
      "loss": 0.5955,
      "step": 36660
    },
    {
      "epoch": 2.9406575781876505,
      "grad_norm": 1.3831158876419067,
      "learning_rate": 6.062717770034843e-07,
      "loss": 0.5736,
      "step": 36670
    },
    {
      "epoch": 2.9414595028067363,
      "grad_norm": 2.0270040035247803,
      "learning_rate": 5.982310372554275e-07,
      "loss": 0.6528,
      "step": 36680
    },
    {
      "epoch": 2.942261427425822,
      "grad_norm": 1.6070044040679932,
      "learning_rate": 5.901902975073708e-07,
      "loss": 0.5736,
      "step": 36690
    },
    {
      "epoch": 2.943063352044908,
      "grad_norm": 1.6547259092330933,
      "learning_rate": 5.821495577593139e-07,
      "loss": 0.6282,
      "step": 36700
    },
    {
      "epoch": 2.9438652766639937,
      "grad_norm": 1.7367408275604248,
      "learning_rate": 5.74108818011257e-07,
      "loss": 0.6434,
      "step": 36710
    },
    {
      "epoch": 2.9446672012830795,
      "grad_norm": 2.0020086765289307,
      "learning_rate": 5.660680782632002e-07,
      "loss": 0.5994,
      "step": 36720
    },
    {
      "epoch": 2.9454691259021653,
      "grad_norm": 2.2522168159484863,
      "learning_rate": 5.580273385151435e-07,
      "loss": 0.5818,
      "step": 36730
    },
    {
      "epoch": 2.946271050521251,
      "grad_norm": 2.0516927242279053,
      "learning_rate": 5.499865987670866e-07,
      "loss": 0.6279,
      "step": 36740
    },
    {
      "epoch": 2.947072975140337,
      "grad_norm": 1.7965419292449951,
      "learning_rate": 5.419458590190297e-07,
      "loss": 0.5724,
      "step": 36750
    },
    {
      "epoch": 2.9478748997594226,
      "grad_norm": 1.7823750972747803,
      "learning_rate": 5.33905119270973e-07,
      "loss": 0.6496,
      "step": 36760
    },
    {
      "epoch": 2.9486768243785084,
      "grad_norm": 1.7452070713043213,
      "learning_rate": 5.258643795229162e-07,
      "loss": 0.6535,
      "step": 36770
    },
    {
      "epoch": 2.949478748997594,
      "grad_norm": 2.124195098876953,
      "learning_rate": 5.178236397748593e-07,
      "loss": 0.6285,
      "step": 36780
    },
    {
      "epoch": 2.95028067361668,
      "grad_norm": 1.769748568534851,
      "learning_rate": 5.097829000268024e-07,
      "loss": 0.6225,
      "step": 36790
    },
    {
      "epoch": 2.951082598235766,
      "grad_norm": 2.040635347366333,
      "learning_rate": 5.017421602787457e-07,
      "loss": 0.6497,
      "step": 36800
    },
    {
      "epoch": 2.9518845228548516,
      "grad_norm": 1.9644584655761719,
      "learning_rate": 4.937014205306889e-07,
      "loss": 0.5953,
      "step": 36810
    },
    {
      "epoch": 2.9526864474739374,
      "grad_norm": 2.039308786392212,
      "learning_rate": 4.85660680782632e-07,
      "loss": 0.63,
      "step": 36820
    },
    {
      "epoch": 2.953488372093023,
      "grad_norm": 1.908116340637207,
      "learning_rate": 4.776199410345752e-07,
      "loss": 0.5577,
      "step": 36830
    },
    {
      "epoch": 2.954290296712109,
      "grad_norm": 2.158281087875366,
      "learning_rate": 4.695792012865184e-07,
      "loss": 0.6617,
      "step": 36840
    },
    {
      "epoch": 2.9550922213311948,
      "grad_norm": 1.6459006071090698,
      "learning_rate": 4.6153846153846156e-07,
      "loss": 0.5523,
      "step": 36850
    },
    {
      "epoch": 2.9558941459502805,
      "grad_norm": 1.8903676271438599,
      "learning_rate": 4.5349772179040474e-07,
      "loss": 0.5652,
      "step": 36860
    },
    {
      "epoch": 2.9566960705693663,
      "grad_norm": 1.9846261739730835,
      "learning_rate": 4.454569820423479e-07,
      "loss": 0.5609,
      "step": 36870
    },
    {
      "epoch": 2.957497995188452,
      "grad_norm": 1.792348861694336,
      "learning_rate": 4.374162422942911e-07,
      "loss": 0.5769,
      "step": 36880
    },
    {
      "epoch": 2.958299919807538,
      "grad_norm": 1.8554993867874146,
      "learning_rate": 4.293755025462343e-07,
      "loss": 0.6519,
      "step": 36890
    },
    {
      "epoch": 2.959101844426624,
      "grad_norm": 1.8875842094421387,
      "learning_rate": 4.2133476279817745e-07,
      "loss": 0.6346,
      "step": 36900
    },
    {
      "epoch": 2.9599037690457095,
      "grad_norm": 1.9577494859695435,
      "learning_rate": 4.1329402305012063e-07,
      "loss": 0.7294,
      "step": 36910
    },
    {
      "epoch": 2.9607056936647957,
      "grad_norm": 1.9806078672409058,
      "learning_rate": 4.052532833020638e-07,
      "loss": 0.6304,
      "step": 36920
    },
    {
      "epoch": 2.961507618283881,
      "grad_norm": 2.3245418071746826,
      "learning_rate": 3.97212543554007e-07,
      "loss": 0.6008,
      "step": 36930
    },
    {
      "epoch": 2.9623095429029673,
      "grad_norm": 1.9651285409927368,
      "learning_rate": 3.8917180380595017e-07,
      "loss": 0.6169,
      "step": 36940
    },
    {
      "epoch": 2.9631114675220527,
      "grad_norm": 1.7195913791656494,
      "learning_rate": 3.8113106405789335e-07,
      "loss": 0.6132,
      "step": 36950
    },
    {
      "epoch": 2.963913392141139,
      "grad_norm": 1.5805351734161377,
      "learning_rate": 3.730903243098365e-07,
      "loss": 0.5068,
      "step": 36960
    },
    {
      "epoch": 2.9647153167602243,
      "grad_norm": 2.1120028495788574,
      "learning_rate": 3.650495845617797e-07,
      "loss": 0.615,
      "step": 36970
    },
    {
      "epoch": 2.9655172413793105,
      "grad_norm": 2.295813798904419,
      "learning_rate": 3.570088448137229e-07,
      "loss": 0.6417,
      "step": 36980
    },
    {
      "epoch": 2.9663191659983963,
      "grad_norm": 1.9219611883163452,
      "learning_rate": 3.4896810506566606e-07,
      "loss": 0.6541,
      "step": 36990
    },
    {
      "epoch": 2.967121090617482,
      "grad_norm": 1.8053793907165527,
      "learning_rate": 3.4092736531760924e-07,
      "loss": 0.542,
      "step": 37000
    },
    {
      "epoch": 2.967923015236568,
      "grad_norm": 1.8753989934921265,
      "learning_rate": 3.328866255695524e-07,
      "loss": 0.5932,
      "step": 37010
    },
    {
      "epoch": 2.9687249398556537,
      "grad_norm": 1.9273574352264404,
      "learning_rate": 3.248458858214956e-07,
      "loss": 0.6048,
      "step": 37020
    },
    {
      "epoch": 2.9695268644747395,
      "grad_norm": 1.7963944673538208,
      "learning_rate": 3.1680514607343877e-07,
      "loss": 0.6535,
      "step": 37030
    },
    {
      "epoch": 2.9703287890938253,
      "grad_norm": 2.2895429134368896,
      "learning_rate": 3.0876440632538195e-07,
      "loss": 0.6667,
      "step": 37040
    },
    {
      "epoch": 2.971130713712911,
      "grad_norm": 2.144928216934204,
      "learning_rate": 3.0072366657732513e-07,
      "loss": 0.6126,
      "step": 37050
    },
    {
      "epoch": 2.971932638331997,
      "grad_norm": 1.830757737159729,
      "learning_rate": 2.926829268292683e-07,
      "loss": 0.6345,
      "step": 37060
    },
    {
      "epoch": 2.9727345629510826,
      "grad_norm": 1.8545178174972534,
      "learning_rate": 2.846421870812115e-07,
      "loss": 0.6278,
      "step": 37070
    },
    {
      "epoch": 2.9735364875701684,
      "grad_norm": 1.9908215999603271,
      "learning_rate": 2.7660144733315466e-07,
      "loss": 0.6462,
      "step": 37080
    },
    {
      "epoch": 2.974338412189254,
      "grad_norm": 2.584942579269409,
      "learning_rate": 2.6856070758509784e-07,
      "loss": 0.6407,
      "step": 37090
    },
    {
      "epoch": 2.97514033680834,
      "grad_norm": 1.7118531465530396,
      "learning_rate": 2.60519967837041e-07,
      "loss": 0.4963,
      "step": 37100
    },
    {
      "epoch": 2.975942261427426,
      "grad_norm": 1.9135664701461792,
      "learning_rate": 2.524792280889842e-07,
      "loss": 0.6416,
      "step": 37110
    },
    {
      "epoch": 2.9767441860465116,
      "grad_norm": 1.5925853252410889,
      "learning_rate": 2.444384883409274e-07,
      "loss": 0.5577,
      "step": 37120
    },
    {
      "epoch": 2.9775461106655974,
      "grad_norm": 2.107937812805176,
      "learning_rate": 2.3639774859287058e-07,
      "loss": 0.5401,
      "step": 37130
    },
    {
      "epoch": 2.978348035284683,
      "grad_norm": 2.0318942070007324,
      "learning_rate": 2.2835700884481373e-07,
      "loss": 0.6336,
      "step": 37140
    },
    {
      "epoch": 2.979149959903769,
      "grad_norm": 1.7571340799331665,
      "learning_rate": 2.203162690967569e-07,
      "loss": 0.5809,
      "step": 37150
    },
    {
      "epoch": 2.9799518845228548,
      "grad_norm": 2.172548770904541,
      "learning_rate": 2.122755293487001e-07,
      "loss": 0.6757,
      "step": 37160
    },
    {
      "epoch": 2.9807538091419405,
      "grad_norm": 1.8069511651992798,
      "learning_rate": 2.0423478960064327e-07,
      "loss": 0.6643,
      "step": 37170
    },
    {
      "epoch": 2.9815557337610263,
      "grad_norm": 2.1469485759735107,
      "learning_rate": 1.9619404985258645e-07,
      "loss": 0.5933,
      "step": 37180
    },
    {
      "epoch": 2.982357658380112,
      "grad_norm": 1.9486488103866577,
      "learning_rate": 1.8815331010452963e-07,
      "loss": 0.6007,
      "step": 37190
    },
    {
      "epoch": 2.983159582999198,
      "grad_norm": 1.9287428855895996,
      "learning_rate": 1.801125703564728e-07,
      "loss": 0.6132,
      "step": 37200
    }
  ],
  "logging_steps": 10,
  "max_steps": 37410,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.0887761272491213e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
