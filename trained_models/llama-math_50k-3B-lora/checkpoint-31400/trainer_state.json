{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.5180433039294305,
  "eval_steps": 500,
  "global_step": 31400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0008019246190858059,
      "grad_norm": 0.5491542816162109,
      "learning_rate": 2.7e-06,
      "loss": 1.2265,
      "step": 10
    },
    {
      "epoch": 0.0016038492381716118,
      "grad_norm": 0.6212937235832214,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 1.2218,
      "step": 20
    },
    {
      "epoch": 0.0024057738572574178,
      "grad_norm": 0.41004225611686707,
      "learning_rate": 8.7e-06,
      "loss": 1.2033,
      "step": 30
    },
    {
      "epoch": 0.0032076984763432237,
      "grad_norm": 0.4447644054889679,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 1.2348,
      "step": 40
    },
    {
      "epoch": 0.00400962309542903,
      "grad_norm": 0.39295604825019836,
      "learning_rate": 1.47e-05,
      "loss": 1.1794,
      "step": 50
    },
    {
      "epoch": 0.0048115477145148355,
      "grad_norm": 0.40379026532173157,
      "learning_rate": 1.77e-05,
      "loss": 1.1889,
      "step": 60
    },
    {
      "epoch": 0.0056134723336006415,
      "grad_norm": 0.6003236174583435,
      "learning_rate": 2.07e-05,
      "loss": 1.1938,
      "step": 70
    },
    {
      "epoch": 0.006415396952686447,
      "grad_norm": 0.5329833030700684,
      "learning_rate": 2.37e-05,
      "loss": 1.0899,
      "step": 80
    },
    {
      "epoch": 0.007217321571772253,
      "grad_norm": 0.5532116293907166,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.9986,
      "step": 90
    },
    {
      "epoch": 0.00801924619085806,
      "grad_norm": 0.5823407769203186,
      "learning_rate": 2.97e-05,
      "loss": 0.9088,
      "step": 100
    },
    {
      "epoch": 0.008821170809943865,
      "grad_norm": 0.5420024991035461,
      "learning_rate": 2.999276333422675e-05,
      "loss": 0.8536,
      "step": 110
    },
    {
      "epoch": 0.009623095429029671,
      "grad_norm": 0.5519866347312927,
      "learning_rate": 2.998472259447869e-05,
      "loss": 0.8351,
      "step": 120
    },
    {
      "epoch": 0.010425020048115477,
      "grad_norm": 0.5334459543228149,
      "learning_rate": 2.997668185473064e-05,
      "loss": 0.8772,
      "step": 130
    },
    {
      "epoch": 0.011226944667201283,
      "grad_norm": 0.5906223654747009,
      "learning_rate": 2.996864111498258e-05,
      "loss": 0.8164,
      "step": 140
    },
    {
      "epoch": 0.012028869286287089,
      "grad_norm": 0.607934832572937,
      "learning_rate": 2.9960600375234524e-05,
      "loss": 0.753,
      "step": 150
    },
    {
      "epoch": 0.012830793905372895,
      "grad_norm": 0.4827706515789032,
      "learning_rate": 2.9952559635486465e-05,
      "loss": 0.8592,
      "step": 160
    },
    {
      "epoch": 0.0136327185244587,
      "grad_norm": 0.5668938755989075,
      "learning_rate": 2.9944518895738406e-05,
      "loss": 0.7527,
      "step": 170
    },
    {
      "epoch": 0.014434643143544507,
      "grad_norm": 0.633445143699646,
      "learning_rate": 2.9936478155990354e-05,
      "loss": 0.7535,
      "step": 180
    },
    {
      "epoch": 0.015236567762630313,
      "grad_norm": 0.5908792614936829,
      "learning_rate": 2.9928437416242294e-05,
      "loss": 0.7743,
      "step": 190
    },
    {
      "epoch": 0.01603849238171612,
      "grad_norm": 0.5609309077262878,
      "learning_rate": 2.992039667649424e-05,
      "loss": 0.6995,
      "step": 200
    },
    {
      "epoch": 0.016840417000801924,
      "grad_norm": 0.7352283596992493,
      "learning_rate": 2.991235593674618e-05,
      "loss": 0.7397,
      "step": 210
    },
    {
      "epoch": 0.01764234161988773,
      "grad_norm": 0.7344129681587219,
      "learning_rate": 2.9904315196998124e-05,
      "loss": 0.7787,
      "step": 220
    },
    {
      "epoch": 0.018444266238973536,
      "grad_norm": 0.6590076088905334,
      "learning_rate": 2.9896274457250068e-05,
      "loss": 0.7808,
      "step": 230
    },
    {
      "epoch": 0.019246190858059342,
      "grad_norm": 0.8099362850189209,
      "learning_rate": 2.9888233717502013e-05,
      "loss": 0.7155,
      "step": 240
    },
    {
      "epoch": 0.020048115477145148,
      "grad_norm": 0.9082175493240356,
      "learning_rate": 2.9880192977753953e-05,
      "loss": 0.7872,
      "step": 250
    },
    {
      "epoch": 0.020850040096230954,
      "grad_norm": 0.671195924282074,
      "learning_rate": 2.9872152238005898e-05,
      "loss": 0.7144,
      "step": 260
    },
    {
      "epoch": 0.02165196471531676,
      "grad_norm": 0.6982110738754272,
      "learning_rate": 2.986411149825784e-05,
      "loss": 0.7193,
      "step": 270
    },
    {
      "epoch": 0.022453889334402566,
      "grad_norm": 0.6693212389945984,
      "learning_rate": 2.9856070758509786e-05,
      "loss": 0.6759,
      "step": 280
    },
    {
      "epoch": 0.023255813953488372,
      "grad_norm": 0.7968083620071411,
      "learning_rate": 2.9848030018761727e-05,
      "loss": 0.7158,
      "step": 290
    },
    {
      "epoch": 0.024057738572574178,
      "grad_norm": 0.7326953411102295,
      "learning_rate": 2.9839989279013668e-05,
      "loss": 0.76,
      "step": 300
    },
    {
      "epoch": 0.024859663191659984,
      "grad_norm": 0.5882723927497864,
      "learning_rate": 2.9831948539265612e-05,
      "loss": 0.7051,
      "step": 310
    },
    {
      "epoch": 0.02566158781074579,
      "grad_norm": 0.8578436970710754,
      "learning_rate": 2.9823907799517557e-05,
      "loss": 0.7454,
      "step": 320
    },
    {
      "epoch": 0.026463512429831595,
      "grad_norm": 0.7751734852790833,
      "learning_rate": 2.98158670597695e-05,
      "loss": 0.7022,
      "step": 330
    },
    {
      "epoch": 0.0272654370489174,
      "grad_norm": 0.688232421875,
      "learning_rate": 2.9807826320021442e-05,
      "loss": 0.7169,
      "step": 340
    },
    {
      "epoch": 0.028067361668003207,
      "grad_norm": 0.7245734333992004,
      "learning_rate": 2.9799785580273386e-05,
      "loss": 0.7276,
      "step": 350
    },
    {
      "epoch": 0.028869286287089013,
      "grad_norm": 0.8352937698364258,
      "learning_rate": 2.9791744840525327e-05,
      "loss": 0.6661,
      "step": 360
    },
    {
      "epoch": 0.02967121090617482,
      "grad_norm": 0.7142455577850342,
      "learning_rate": 2.9783704100777275e-05,
      "loss": 0.7002,
      "step": 370
    },
    {
      "epoch": 0.030473135525260625,
      "grad_norm": 0.8627832531929016,
      "learning_rate": 2.9775663361029216e-05,
      "loss": 0.6999,
      "step": 380
    },
    {
      "epoch": 0.03127506014434643,
      "grad_norm": 0.7636879682540894,
      "learning_rate": 2.976762262128116e-05,
      "loss": 0.7427,
      "step": 390
    },
    {
      "epoch": 0.03207698476343224,
      "grad_norm": 0.7325884699821472,
      "learning_rate": 2.97595818815331e-05,
      "loss": 0.7702,
      "step": 400
    },
    {
      "epoch": 0.03287890938251804,
      "grad_norm": 0.7689812183380127,
      "learning_rate": 2.9751541141785045e-05,
      "loss": 0.6953,
      "step": 410
    },
    {
      "epoch": 0.03368083400160385,
      "grad_norm": 0.760448694229126,
      "learning_rate": 2.974350040203699e-05,
      "loss": 0.714,
      "step": 420
    },
    {
      "epoch": 0.034482758620689655,
      "grad_norm": 0.831805944442749,
      "learning_rate": 2.973545966228893e-05,
      "loss": 0.7449,
      "step": 430
    },
    {
      "epoch": 0.03528468323977546,
      "grad_norm": 1.0321253538131714,
      "learning_rate": 2.9727418922540875e-05,
      "loss": 0.7208,
      "step": 440
    },
    {
      "epoch": 0.03608660785886127,
      "grad_norm": 0.8430793285369873,
      "learning_rate": 2.9719378182792815e-05,
      "loss": 0.6648,
      "step": 450
    },
    {
      "epoch": 0.03688853247794707,
      "grad_norm": 0.8429708480834961,
      "learning_rate": 2.971133744304476e-05,
      "loss": 0.7071,
      "step": 460
    },
    {
      "epoch": 0.03769045709703288,
      "grad_norm": 0.8016588687896729,
      "learning_rate": 2.9703296703296704e-05,
      "loss": 0.7013,
      "step": 470
    },
    {
      "epoch": 0.038492381716118684,
      "grad_norm": 0.7557235956192017,
      "learning_rate": 2.969525596354865e-05,
      "loss": 0.7384,
      "step": 480
    },
    {
      "epoch": 0.03929430633520449,
      "grad_norm": 0.9600343704223633,
      "learning_rate": 2.968721522380059e-05,
      "loss": 0.7158,
      "step": 490
    },
    {
      "epoch": 0.040096230954290296,
      "grad_norm": 0.9417804479598999,
      "learning_rate": 2.9679174484052534e-05,
      "loss": 0.6476,
      "step": 500
    },
    {
      "epoch": 0.0408981555733761,
      "grad_norm": 0.9219151735305786,
      "learning_rate": 2.9671133744304478e-05,
      "loss": 0.7504,
      "step": 510
    },
    {
      "epoch": 0.04170008019246191,
      "grad_norm": 0.9023283123970032,
      "learning_rate": 2.9663093004556422e-05,
      "loss": 0.7395,
      "step": 520
    },
    {
      "epoch": 0.042502004811547714,
      "grad_norm": 0.8633768558502197,
      "learning_rate": 2.9655052264808363e-05,
      "loss": 0.7017,
      "step": 530
    },
    {
      "epoch": 0.04330392943063352,
      "grad_norm": 1.0333620309829712,
      "learning_rate": 2.9647011525060307e-05,
      "loss": 0.716,
      "step": 540
    },
    {
      "epoch": 0.044105854049719326,
      "grad_norm": 0.9293287396430969,
      "learning_rate": 2.9638970785312248e-05,
      "loss": 0.6447,
      "step": 550
    },
    {
      "epoch": 0.04490777866880513,
      "grad_norm": 0.8304601907730103,
      "learning_rate": 2.9630930045564193e-05,
      "loss": 0.6411,
      "step": 560
    },
    {
      "epoch": 0.04570970328789094,
      "grad_norm": 0.9522988796234131,
      "learning_rate": 2.9622889305816137e-05,
      "loss": 0.6962,
      "step": 570
    },
    {
      "epoch": 0.046511627906976744,
      "grad_norm": 0.8742966651916504,
      "learning_rate": 2.9614848566068078e-05,
      "loss": 0.6846,
      "step": 580
    },
    {
      "epoch": 0.04731355252606255,
      "grad_norm": 0.9198683500289917,
      "learning_rate": 2.9606807826320022e-05,
      "loss": 0.7043,
      "step": 590
    },
    {
      "epoch": 0.048115477145148355,
      "grad_norm": 0.9887529015541077,
      "learning_rate": 2.9598767086571963e-05,
      "loss": 0.6634,
      "step": 600
    },
    {
      "epoch": 0.04891740176423416,
      "grad_norm": 0.9906630516052246,
      "learning_rate": 2.959072634682391e-05,
      "loss": 0.7178,
      "step": 610
    },
    {
      "epoch": 0.04971932638331997,
      "grad_norm": 0.8276464343070984,
      "learning_rate": 2.958268560707585e-05,
      "loss": 0.6946,
      "step": 620
    },
    {
      "epoch": 0.05052125100240577,
      "grad_norm": 0.8835695385932922,
      "learning_rate": 2.9574644867327796e-05,
      "loss": 0.7543,
      "step": 630
    },
    {
      "epoch": 0.05132317562149158,
      "grad_norm": 0.9079267978668213,
      "learning_rate": 2.9566604127579737e-05,
      "loss": 0.7091,
      "step": 640
    },
    {
      "epoch": 0.052125100240577385,
      "grad_norm": 0.9226414561271667,
      "learning_rate": 2.955856338783168e-05,
      "loss": 0.6493,
      "step": 650
    },
    {
      "epoch": 0.05292702485966319,
      "grad_norm": 0.9908662438392639,
      "learning_rate": 2.9550522648083625e-05,
      "loss": 0.6867,
      "step": 660
    },
    {
      "epoch": 0.053728949478749,
      "grad_norm": 0.9789708852767944,
      "learning_rate": 2.954248190833557e-05,
      "loss": 0.6671,
      "step": 670
    },
    {
      "epoch": 0.0545308740978348,
      "grad_norm": 0.8490285873413086,
      "learning_rate": 2.953444116858751e-05,
      "loss": 0.7258,
      "step": 680
    },
    {
      "epoch": 0.05533279871692061,
      "grad_norm": 1.1348673105239868,
      "learning_rate": 2.952640042883945e-05,
      "loss": 0.6654,
      "step": 690
    },
    {
      "epoch": 0.056134723336006415,
      "grad_norm": 1.0817779302597046,
      "learning_rate": 2.95183596890914e-05,
      "loss": 0.707,
      "step": 700
    },
    {
      "epoch": 0.05693664795509222,
      "grad_norm": 0.978412389755249,
      "learning_rate": 2.951031894934334e-05,
      "loss": 0.7236,
      "step": 710
    },
    {
      "epoch": 0.057738572574178026,
      "grad_norm": 1.0662254095077515,
      "learning_rate": 2.9502278209595284e-05,
      "loss": 0.6617,
      "step": 720
    },
    {
      "epoch": 0.05854049719326383,
      "grad_norm": 1.2027446031570435,
      "learning_rate": 2.9494237469847225e-05,
      "loss": 0.6946,
      "step": 730
    },
    {
      "epoch": 0.05934242181234964,
      "grad_norm": 1.0333150625228882,
      "learning_rate": 2.948619673009917e-05,
      "loss": 0.6882,
      "step": 740
    },
    {
      "epoch": 0.060144346431435444,
      "grad_norm": 1.0065711736679077,
      "learning_rate": 2.9478155990351114e-05,
      "loss": 0.6966,
      "step": 750
    },
    {
      "epoch": 0.06094627105052125,
      "grad_norm": 1.3060935735702515,
      "learning_rate": 2.9470115250603058e-05,
      "loss": 0.73,
      "step": 760
    },
    {
      "epoch": 0.061748195669607056,
      "grad_norm": 1.0654027462005615,
      "learning_rate": 2.9462074510855e-05,
      "loss": 0.7001,
      "step": 770
    },
    {
      "epoch": 0.06255012028869286,
      "grad_norm": 1.180708885192871,
      "learning_rate": 2.9454033771106943e-05,
      "loss": 0.7301,
      "step": 780
    },
    {
      "epoch": 0.06335204490777867,
      "grad_norm": 1.23175847530365,
      "learning_rate": 2.9445993031358884e-05,
      "loss": 0.6574,
      "step": 790
    },
    {
      "epoch": 0.06415396952686447,
      "grad_norm": 0.9929754734039307,
      "learning_rate": 2.9437952291610832e-05,
      "loss": 0.652,
      "step": 800
    },
    {
      "epoch": 0.06495589414595028,
      "grad_norm": 0.8642974495887756,
      "learning_rate": 2.9429911551862773e-05,
      "loss": 0.6333,
      "step": 810
    },
    {
      "epoch": 0.06575781876503609,
      "grad_norm": 1.0094469785690308,
      "learning_rate": 2.9421870812114714e-05,
      "loss": 0.7427,
      "step": 820
    },
    {
      "epoch": 0.06655974338412189,
      "grad_norm": 1.2350850105285645,
      "learning_rate": 2.9413830072366658e-05,
      "loss": 0.6982,
      "step": 830
    },
    {
      "epoch": 0.0673616680032077,
      "grad_norm": 1.0902507305145264,
      "learning_rate": 2.94057893326186e-05,
      "loss": 0.6877,
      "step": 840
    },
    {
      "epoch": 0.0681635926222935,
      "grad_norm": 1.2492882013320923,
      "learning_rate": 2.9397748592870546e-05,
      "loss": 0.6866,
      "step": 850
    },
    {
      "epoch": 0.06896551724137931,
      "grad_norm": 1.0562392473220825,
      "learning_rate": 2.9389707853122487e-05,
      "loss": 0.6844,
      "step": 860
    },
    {
      "epoch": 0.06976744186046512,
      "grad_norm": 0.9917662739753723,
      "learning_rate": 2.938166711337443e-05,
      "loss": 0.6655,
      "step": 870
    },
    {
      "epoch": 0.07056936647955092,
      "grad_norm": 1.0306763648986816,
      "learning_rate": 2.9373626373626373e-05,
      "loss": 0.6994,
      "step": 880
    },
    {
      "epoch": 0.07137129109863673,
      "grad_norm": 1.0987879037857056,
      "learning_rate": 2.936558563387832e-05,
      "loss": 0.6597,
      "step": 890
    },
    {
      "epoch": 0.07217321571772253,
      "grad_norm": 1.237203598022461,
      "learning_rate": 2.935754489413026e-05,
      "loss": 0.7019,
      "step": 900
    },
    {
      "epoch": 0.07297514033680834,
      "grad_norm": 0.9095577597618103,
      "learning_rate": 2.9349504154382205e-05,
      "loss": 0.7559,
      "step": 910
    },
    {
      "epoch": 0.07377706495589414,
      "grad_norm": 1.0876318216323853,
      "learning_rate": 2.9341463414634146e-05,
      "loss": 0.7487,
      "step": 920
    },
    {
      "epoch": 0.07457898957497995,
      "grad_norm": 0.9533179402351379,
      "learning_rate": 2.933342267488609e-05,
      "loss": 0.6953,
      "step": 930
    },
    {
      "epoch": 0.07538091419406576,
      "grad_norm": 0.9207428693771362,
      "learning_rate": 2.9325381935138035e-05,
      "loss": 0.6893,
      "step": 940
    },
    {
      "epoch": 0.07618283881315156,
      "grad_norm": 1.0568865537643433,
      "learning_rate": 2.9317341195389976e-05,
      "loss": 0.6355,
      "step": 950
    },
    {
      "epoch": 0.07698476343223737,
      "grad_norm": 1.0734156370162964,
      "learning_rate": 2.930930045564192e-05,
      "loss": 0.6574,
      "step": 960
    },
    {
      "epoch": 0.07778668805132317,
      "grad_norm": 1.0010058879852295,
      "learning_rate": 2.930125971589386e-05,
      "loss": 0.6885,
      "step": 970
    },
    {
      "epoch": 0.07858861267040898,
      "grad_norm": 1.0934818983078003,
      "learning_rate": 2.9293218976145805e-05,
      "loss": 0.705,
      "step": 980
    },
    {
      "epoch": 0.07939053728949479,
      "grad_norm": 0.9967938661575317,
      "learning_rate": 2.928517823639775e-05,
      "loss": 0.6709,
      "step": 990
    },
    {
      "epoch": 0.08019246190858059,
      "grad_norm": 1.0365746021270752,
      "learning_rate": 2.9277137496649694e-05,
      "loss": 0.7086,
      "step": 1000
    },
    {
      "epoch": 0.0809943865276664,
      "grad_norm": 1.0288068056106567,
      "learning_rate": 2.9269096756901635e-05,
      "loss": 0.6728,
      "step": 1010
    },
    {
      "epoch": 0.0817963111467522,
      "grad_norm": 1.0228374004364014,
      "learning_rate": 2.926105601715358e-05,
      "loss": 0.7585,
      "step": 1020
    },
    {
      "epoch": 0.08259823576583801,
      "grad_norm": 1.0833452939987183,
      "learning_rate": 2.925301527740552e-05,
      "loss": 0.648,
      "step": 1030
    },
    {
      "epoch": 0.08340016038492382,
      "grad_norm": 1.0360740423202515,
      "learning_rate": 2.9244974537657468e-05,
      "loss": 0.6083,
      "step": 1040
    },
    {
      "epoch": 0.08420208500400962,
      "grad_norm": 1.08688485622406,
      "learning_rate": 2.923693379790941e-05,
      "loss": 0.6763,
      "step": 1050
    },
    {
      "epoch": 0.08500400962309543,
      "grad_norm": 1.3609563112258911,
      "learning_rate": 2.9228893058161353e-05,
      "loss": 0.6999,
      "step": 1060
    },
    {
      "epoch": 0.08580593424218123,
      "grad_norm": 1.156071424484253,
      "learning_rate": 2.9220852318413294e-05,
      "loss": 0.6625,
      "step": 1070
    },
    {
      "epoch": 0.08660785886126704,
      "grad_norm": 0.9767239689826965,
      "learning_rate": 2.9212811578665238e-05,
      "loss": 0.6844,
      "step": 1080
    },
    {
      "epoch": 0.08740978348035285,
      "grad_norm": 1.0508207082748413,
      "learning_rate": 2.9204770838917182e-05,
      "loss": 0.6862,
      "step": 1090
    },
    {
      "epoch": 0.08821170809943865,
      "grad_norm": 1.0638874769210815,
      "learning_rate": 2.9196730099169123e-05,
      "loss": 0.6985,
      "step": 1100
    },
    {
      "epoch": 0.08901363271852446,
      "grad_norm": 1.1000066995620728,
      "learning_rate": 2.9188689359421067e-05,
      "loss": 0.7137,
      "step": 1110
    },
    {
      "epoch": 0.08981555733761026,
      "grad_norm": 1.0348960161209106,
      "learning_rate": 2.918064861967301e-05,
      "loss": 0.7416,
      "step": 1120
    },
    {
      "epoch": 0.09061748195669607,
      "grad_norm": 1.0085937976837158,
      "learning_rate": 2.9172607879924956e-05,
      "loss": 0.7263,
      "step": 1130
    },
    {
      "epoch": 0.09141940657578188,
      "grad_norm": 1.1454681158065796,
      "learning_rate": 2.9164567140176897e-05,
      "loss": 0.6651,
      "step": 1140
    },
    {
      "epoch": 0.09222133119486768,
      "grad_norm": 1.164229393005371,
      "learning_rate": 2.915652640042884e-05,
      "loss": 0.6185,
      "step": 1150
    },
    {
      "epoch": 0.09302325581395349,
      "grad_norm": 1.3360008001327515,
      "learning_rate": 2.9148485660680782e-05,
      "loss": 0.5951,
      "step": 1160
    },
    {
      "epoch": 0.09382518043303929,
      "grad_norm": 1.107741117477417,
      "learning_rate": 2.9140444920932726e-05,
      "loss": 0.7022,
      "step": 1170
    },
    {
      "epoch": 0.0946271050521251,
      "grad_norm": 1.1905244588851929,
      "learning_rate": 2.913240418118467e-05,
      "loss": 0.6682,
      "step": 1180
    },
    {
      "epoch": 0.0954290296712109,
      "grad_norm": 1.1038925647735596,
      "learning_rate": 2.9124363441436615e-05,
      "loss": 0.6743,
      "step": 1190
    },
    {
      "epoch": 0.09623095429029671,
      "grad_norm": 1.1422232389450073,
      "learning_rate": 2.9116322701688556e-05,
      "loss": 0.6458,
      "step": 1200
    },
    {
      "epoch": 0.09703287890938252,
      "grad_norm": 1.187776803970337,
      "learning_rate": 2.9108281961940497e-05,
      "loss": 0.6536,
      "step": 1210
    },
    {
      "epoch": 0.09783480352846832,
      "grad_norm": 1.1882543563842773,
      "learning_rate": 2.9100241222192445e-05,
      "loss": 0.6608,
      "step": 1220
    },
    {
      "epoch": 0.09863672814755413,
      "grad_norm": 1.2782732248306274,
      "learning_rate": 2.9092200482444385e-05,
      "loss": 0.6661,
      "step": 1230
    },
    {
      "epoch": 0.09943865276663993,
      "grad_norm": 1.2225764989852905,
      "learning_rate": 2.908415974269633e-05,
      "loss": 0.6971,
      "step": 1240
    },
    {
      "epoch": 0.10024057738572574,
      "grad_norm": 1.234119176864624,
      "learning_rate": 2.907611900294827e-05,
      "loss": 0.6357,
      "step": 1250
    },
    {
      "epoch": 0.10104250200481155,
      "grad_norm": 1.2806955575942993,
      "learning_rate": 2.9068078263200215e-05,
      "loss": 0.6794,
      "step": 1260
    },
    {
      "epoch": 0.10184442662389735,
      "grad_norm": 1.1676421165466309,
      "learning_rate": 2.906003752345216e-05,
      "loss": 0.6797,
      "step": 1270
    },
    {
      "epoch": 0.10264635124298316,
      "grad_norm": 1.0736685991287231,
      "learning_rate": 2.9051996783704103e-05,
      "loss": 0.6732,
      "step": 1280
    },
    {
      "epoch": 0.10344827586206896,
      "grad_norm": 1.2485945224761963,
      "learning_rate": 2.9043956043956044e-05,
      "loss": 0.6674,
      "step": 1290
    },
    {
      "epoch": 0.10425020048115477,
      "grad_norm": 1.0776519775390625,
      "learning_rate": 2.903591530420799e-05,
      "loss": 0.678,
      "step": 1300
    },
    {
      "epoch": 0.10505212510024058,
      "grad_norm": 0.999401330947876,
      "learning_rate": 2.902787456445993e-05,
      "loss": 0.6891,
      "step": 1310
    },
    {
      "epoch": 0.10585404971932638,
      "grad_norm": 1.3395997285842896,
      "learning_rate": 2.9019833824711874e-05,
      "loss": 0.6677,
      "step": 1320
    },
    {
      "epoch": 0.10665597433841219,
      "grad_norm": 1.2542473077774048,
      "learning_rate": 2.9011793084963818e-05,
      "loss": 0.646,
      "step": 1330
    },
    {
      "epoch": 0.107457898957498,
      "grad_norm": 1.2077292203903198,
      "learning_rate": 2.900375234521576e-05,
      "loss": 0.6872,
      "step": 1340
    },
    {
      "epoch": 0.1082598235765838,
      "grad_norm": 1.1560393571853638,
      "learning_rate": 2.8995711605467703e-05,
      "loss": 0.7072,
      "step": 1350
    },
    {
      "epoch": 0.1090617481956696,
      "grad_norm": 1.080469012260437,
      "learning_rate": 2.8987670865719644e-05,
      "loss": 0.6594,
      "step": 1360
    },
    {
      "epoch": 0.10986367281475541,
      "grad_norm": 1.0619045495986938,
      "learning_rate": 2.8979630125971592e-05,
      "loss": 0.7067,
      "step": 1370
    },
    {
      "epoch": 0.11066559743384122,
      "grad_norm": 1.254980444908142,
      "learning_rate": 2.8971589386223533e-05,
      "loss": 0.6613,
      "step": 1380
    },
    {
      "epoch": 0.11146752205292702,
      "grad_norm": 1.2852439880371094,
      "learning_rate": 2.8963548646475477e-05,
      "loss": 0.668,
      "step": 1390
    },
    {
      "epoch": 0.11226944667201283,
      "grad_norm": 1.076011300086975,
      "learning_rate": 2.8955507906727418e-05,
      "loss": 0.6287,
      "step": 1400
    },
    {
      "epoch": 0.11307137129109864,
      "grad_norm": 1.2701469659805298,
      "learning_rate": 2.8947467166979366e-05,
      "loss": 0.7,
      "step": 1410
    },
    {
      "epoch": 0.11387329591018444,
      "grad_norm": 1.077094554901123,
      "learning_rate": 2.8939426427231307e-05,
      "loss": 0.6709,
      "step": 1420
    },
    {
      "epoch": 0.11467522052927025,
      "grad_norm": 1.0976704359054565,
      "learning_rate": 2.893138568748325e-05,
      "loss": 0.6486,
      "step": 1430
    },
    {
      "epoch": 0.11547714514835605,
      "grad_norm": 1.3291352987289429,
      "learning_rate": 2.8923344947735192e-05,
      "loss": 0.703,
      "step": 1440
    },
    {
      "epoch": 0.11627906976744186,
      "grad_norm": 1.2476261854171753,
      "learning_rate": 2.8915304207987133e-05,
      "loss": 0.653,
      "step": 1450
    },
    {
      "epoch": 0.11708099438652766,
      "grad_norm": 1.4944337606430054,
      "learning_rate": 2.890726346823908e-05,
      "loss": 0.7087,
      "step": 1460
    },
    {
      "epoch": 0.11788291900561347,
      "grad_norm": 1.1641639471054077,
      "learning_rate": 2.889922272849102e-05,
      "loss": 0.7174,
      "step": 1470
    },
    {
      "epoch": 0.11868484362469928,
      "grad_norm": 1.125508189201355,
      "learning_rate": 2.8891181988742966e-05,
      "loss": 0.6594,
      "step": 1480
    },
    {
      "epoch": 0.11948676824378508,
      "grad_norm": 1.1946048736572266,
      "learning_rate": 2.8883141248994906e-05,
      "loss": 0.6154,
      "step": 1490
    },
    {
      "epoch": 0.12028869286287089,
      "grad_norm": 1.1650938987731934,
      "learning_rate": 2.887510050924685e-05,
      "loss": 0.6474,
      "step": 1500
    },
    {
      "epoch": 0.1210906174819567,
      "grad_norm": 1.0864713191986084,
      "learning_rate": 2.8867059769498795e-05,
      "loss": 0.6335,
      "step": 1510
    },
    {
      "epoch": 0.1218925421010425,
      "grad_norm": 1.1483701467514038,
      "learning_rate": 2.885901902975074e-05,
      "loss": 0.6045,
      "step": 1520
    },
    {
      "epoch": 0.1226944667201283,
      "grad_norm": 1.1179178953170776,
      "learning_rate": 2.885097829000268e-05,
      "loss": 0.6514,
      "step": 1530
    },
    {
      "epoch": 0.12349639133921411,
      "grad_norm": 1.408220887184143,
      "learning_rate": 2.8842937550254625e-05,
      "loss": 0.7055,
      "step": 1540
    },
    {
      "epoch": 0.12429831595829992,
      "grad_norm": 1.159607172012329,
      "learning_rate": 2.8834896810506565e-05,
      "loss": 0.6673,
      "step": 1550
    },
    {
      "epoch": 0.12510024057738572,
      "grad_norm": 1.1424591541290283,
      "learning_rate": 2.8826856070758513e-05,
      "loss": 0.6437,
      "step": 1560
    },
    {
      "epoch": 0.12590216519647154,
      "grad_norm": 1.1251424551010132,
      "learning_rate": 2.8818815331010454e-05,
      "loss": 0.6375,
      "step": 1570
    },
    {
      "epoch": 0.12670408981555734,
      "grad_norm": 1.424228549003601,
      "learning_rate": 2.8810774591262395e-05,
      "loss": 0.6597,
      "step": 1580
    },
    {
      "epoch": 0.12750601443464316,
      "grad_norm": 1.155097484588623,
      "learning_rate": 2.880273385151434e-05,
      "loss": 0.6796,
      "step": 1590
    },
    {
      "epoch": 0.12830793905372895,
      "grad_norm": 1.6973109245300293,
      "learning_rate": 2.8794693111766283e-05,
      "loss": 0.6912,
      "step": 1600
    },
    {
      "epoch": 0.12910986367281477,
      "grad_norm": 1.4158552885055542,
      "learning_rate": 2.8786652372018228e-05,
      "loss": 0.6759,
      "step": 1610
    },
    {
      "epoch": 0.12991178829190056,
      "grad_norm": 1.201627492904663,
      "learning_rate": 2.877861163227017e-05,
      "loss": 0.6064,
      "step": 1620
    },
    {
      "epoch": 0.13071371291098638,
      "grad_norm": 1.3990315198898315,
      "learning_rate": 2.8770570892522113e-05,
      "loss": 0.6385,
      "step": 1630
    },
    {
      "epoch": 0.13151563753007217,
      "grad_norm": 1.3040239810943604,
      "learning_rate": 2.8762530152774054e-05,
      "loss": 0.6621,
      "step": 1640
    },
    {
      "epoch": 0.132317562149158,
      "grad_norm": 1.1857519149780273,
      "learning_rate": 2.8754489413026e-05,
      "loss": 0.6874,
      "step": 1650
    },
    {
      "epoch": 0.13311948676824378,
      "grad_norm": 1.156581997871399,
      "learning_rate": 2.8746448673277942e-05,
      "loss": 0.675,
      "step": 1660
    },
    {
      "epoch": 0.1339214113873296,
      "grad_norm": 1.5048511028289795,
      "learning_rate": 2.8738407933529887e-05,
      "loss": 0.6951,
      "step": 1670
    },
    {
      "epoch": 0.1347233360064154,
      "grad_norm": 1.2551019191741943,
      "learning_rate": 2.8730367193781828e-05,
      "loss": 0.6892,
      "step": 1680
    },
    {
      "epoch": 0.13552526062550121,
      "grad_norm": 1.3157366514205933,
      "learning_rate": 2.8722326454033772e-05,
      "loss": 0.6976,
      "step": 1690
    },
    {
      "epoch": 0.136327185244587,
      "grad_norm": 1.216860294342041,
      "learning_rate": 2.8714285714285716e-05,
      "loss": 0.6787,
      "step": 1700
    },
    {
      "epoch": 0.13712910986367283,
      "grad_norm": 1.116058111190796,
      "learning_rate": 2.8706244974537657e-05,
      "loss": 0.6986,
      "step": 1710
    },
    {
      "epoch": 0.13793103448275862,
      "grad_norm": 1.342057466506958,
      "learning_rate": 2.86982042347896e-05,
      "loss": 0.6771,
      "step": 1720
    },
    {
      "epoch": 0.13873295910184444,
      "grad_norm": 1.19614839553833,
      "learning_rate": 2.8690163495041542e-05,
      "loss": 0.6005,
      "step": 1730
    },
    {
      "epoch": 0.13953488372093023,
      "grad_norm": 1.131106972694397,
      "learning_rate": 2.8682122755293487e-05,
      "loss": 0.6178,
      "step": 1740
    },
    {
      "epoch": 0.14033680834001605,
      "grad_norm": 1.3164889812469482,
      "learning_rate": 2.867408201554543e-05,
      "loss": 0.7256,
      "step": 1750
    },
    {
      "epoch": 0.14113873295910184,
      "grad_norm": 1.348780870437622,
      "learning_rate": 2.8666041275797375e-05,
      "loss": 0.6515,
      "step": 1760
    },
    {
      "epoch": 0.14194065757818766,
      "grad_norm": 1.427172064781189,
      "learning_rate": 2.8658000536049316e-05,
      "loss": 0.6956,
      "step": 1770
    },
    {
      "epoch": 0.14274258219727345,
      "grad_norm": 1.2094415426254272,
      "learning_rate": 2.864995979630126e-05,
      "loss": 0.654,
      "step": 1780
    },
    {
      "epoch": 0.14354450681635927,
      "grad_norm": 1.633294939994812,
      "learning_rate": 2.8641919056553205e-05,
      "loss": 0.668,
      "step": 1790
    },
    {
      "epoch": 0.14434643143544507,
      "grad_norm": 1.1044189929962158,
      "learning_rate": 2.863387831680515e-05,
      "loss": 0.6689,
      "step": 1800
    },
    {
      "epoch": 0.14514835605453089,
      "grad_norm": 1.4610190391540527,
      "learning_rate": 2.862583757705709e-05,
      "loss": 0.715,
      "step": 1810
    },
    {
      "epoch": 0.14595028067361668,
      "grad_norm": 1.1758273839950562,
      "learning_rate": 2.8617796837309034e-05,
      "loss": 0.626,
      "step": 1820
    },
    {
      "epoch": 0.1467522052927025,
      "grad_norm": 1.3059742450714111,
      "learning_rate": 2.8609756097560975e-05,
      "loss": 0.6747,
      "step": 1830
    },
    {
      "epoch": 0.1475541299117883,
      "grad_norm": 1.2532906532287598,
      "learning_rate": 2.860171535781292e-05,
      "loss": 0.6745,
      "step": 1840
    },
    {
      "epoch": 0.1483560545308741,
      "grad_norm": 1.3600831031799316,
      "learning_rate": 2.8593674618064864e-05,
      "loss": 0.6614,
      "step": 1850
    },
    {
      "epoch": 0.1491579791499599,
      "grad_norm": 1.2334342002868652,
      "learning_rate": 2.8585633878316805e-05,
      "loss": 0.6319,
      "step": 1860
    },
    {
      "epoch": 0.14995990376904572,
      "grad_norm": 1.1513711214065552,
      "learning_rate": 2.857759313856875e-05,
      "loss": 0.6752,
      "step": 1870
    },
    {
      "epoch": 0.1507618283881315,
      "grad_norm": 1.576170563697815,
      "learning_rate": 2.856955239882069e-05,
      "loss": 0.656,
      "step": 1880
    },
    {
      "epoch": 0.15156375300721733,
      "grad_norm": 1.4188798666000366,
      "learning_rate": 2.8561511659072637e-05,
      "loss": 0.6398,
      "step": 1890
    },
    {
      "epoch": 0.15236567762630313,
      "grad_norm": 1.2777438163757324,
      "learning_rate": 2.855347091932458e-05,
      "loss": 0.6369,
      "step": 1900
    },
    {
      "epoch": 0.15316760224538895,
      "grad_norm": 1.1882728338241577,
      "learning_rate": 2.8545430179576523e-05,
      "loss": 0.6089,
      "step": 1910
    },
    {
      "epoch": 0.15396952686447474,
      "grad_norm": 1.4640578031539917,
      "learning_rate": 2.8537389439828464e-05,
      "loss": 0.6187,
      "step": 1920
    },
    {
      "epoch": 0.15477145148356056,
      "grad_norm": 1.3772484064102173,
      "learning_rate": 2.8529348700080408e-05,
      "loss": 0.6681,
      "step": 1930
    },
    {
      "epoch": 0.15557337610264635,
      "grad_norm": 1.3224893808364868,
      "learning_rate": 2.8521307960332352e-05,
      "loss": 0.6731,
      "step": 1940
    },
    {
      "epoch": 0.15637530072173217,
      "grad_norm": 1.4439939260482788,
      "learning_rate": 2.8513267220584296e-05,
      "loss": 0.6042,
      "step": 1950
    },
    {
      "epoch": 0.15717722534081796,
      "grad_norm": 1.4463586807250977,
      "learning_rate": 2.8505226480836237e-05,
      "loss": 0.7089,
      "step": 1960
    },
    {
      "epoch": 0.15797914995990378,
      "grad_norm": 1.3071085214614868,
      "learning_rate": 2.8497185741088178e-05,
      "loss": 0.5992,
      "step": 1970
    },
    {
      "epoch": 0.15878107457898957,
      "grad_norm": 1.1996148824691772,
      "learning_rate": 2.8489145001340126e-05,
      "loss": 0.6474,
      "step": 1980
    },
    {
      "epoch": 0.1595829991980754,
      "grad_norm": 1.413002848625183,
      "learning_rate": 2.8481104261592067e-05,
      "loss": 0.6549,
      "step": 1990
    },
    {
      "epoch": 0.16038492381716118,
      "grad_norm": 1.250470757484436,
      "learning_rate": 2.847306352184401e-05,
      "loss": 0.6316,
      "step": 2000
    },
    {
      "epoch": 0.161186848436247,
      "grad_norm": 1.4419384002685547,
      "learning_rate": 2.8465022782095952e-05,
      "loss": 0.696,
      "step": 2010
    },
    {
      "epoch": 0.1619887730553328,
      "grad_norm": 1.3662482500076294,
      "learning_rate": 2.8456982042347896e-05,
      "loss": 0.6584,
      "step": 2020
    },
    {
      "epoch": 0.16279069767441862,
      "grad_norm": 1.4683901071548462,
      "learning_rate": 2.844894130259984e-05,
      "loss": 0.6541,
      "step": 2030
    },
    {
      "epoch": 0.1635926222935044,
      "grad_norm": 1.288877248764038,
      "learning_rate": 2.8440900562851785e-05,
      "loss": 0.6041,
      "step": 2040
    },
    {
      "epoch": 0.16439454691259023,
      "grad_norm": 1.3636209964752197,
      "learning_rate": 2.8432859823103726e-05,
      "loss": 0.6245,
      "step": 2050
    },
    {
      "epoch": 0.16519647153167602,
      "grad_norm": 1.3350965976715088,
      "learning_rate": 2.842481908335567e-05,
      "loss": 0.6769,
      "step": 2060
    },
    {
      "epoch": 0.16599839615076184,
      "grad_norm": 1.220836877822876,
      "learning_rate": 2.841677834360761e-05,
      "loss": 0.6636,
      "step": 2070
    },
    {
      "epoch": 0.16680032076984763,
      "grad_norm": 1.1822028160095215,
      "learning_rate": 2.840873760385956e-05,
      "loss": 0.6302,
      "step": 2080
    },
    {
      "epoch": 0.16760224538893345,
      "grad_norm": 1.710001826286316,
      "learning_rate": 2.84006968641115e-05,
      "loss": 0.6508,
      "step": 2090
    },
    {
      "epoch": 0.16840417000801924,
      "grad_norm": 1.2410409450531006,
      "learning_rate": 2.839265612436344e-05,
      "loss": 0.6362,
      "step": 2100
    },
    {
      "epoch": 0.16920609462710506,
      "grad_norm": 1.583170771598816,
      "learning_rate": 2.8384615384615385e-05,
      "loss": 0.6417,
      "step": 2110
    },
    {
      "epoch": 0.17000801924619086,
      "grad_norm": 1.1059999465942383,
      "learning_rate": 2.8376574644867326e-05,
      "loss": 0.601,
      "step": 2120
    },
    {
      "epoch": 0.17080994386527668,
      "grad_norm": 1.375889539718628,
      "learning_rate": 2.8368533905119273e-05,
      "loss": 0.618,
      "step": 2130
    },
    {
      "epoch": 0.17161186848436247,
      "grad_norm": 1.5438196659088135,
      "learning_rate": 2.8360493165371214e-05,
      "loss": 0.6581,
      "step": 2140
    },
    {
      "epoch": 0.1724137931034483,
      "grad_norm": 1.4604133367538452,
      "learning_rate": 2.835245242562316e-05,
      "loss": 0.6668,
      "step": 2150
    },
    {
      "epoch": 0.17321571772253408,
      "grad_norm": 1.37827730178833,
      "learning_rate": 2.83444116858751e-05,
      "loss": 0.6236,
      "step": 2160
    },
    {
      "epoch": 0.1740176423416199,
      "grad_norm": 1.4525741338729858,
      "learning_rate": 2.8336370946127047e-05,
      "loss": 0.629,
      "step": 2170
    },
    {
      "epoch": 0.1748195669607057,
      "grad_norm": 1.4959920644760132,
      "learning_rate": 2.8328330206378988e-05,
      "loss": 0.6212,
      "step": 2180
    },
    {
      "epoch": 0.1756214915797915,
      "grad_norm": 1.3655085563659668,
      "learning_rate": 2.8320289466630932e-05,
      "loss": 0.648,
      "step": 2190
    },
    {
      "epoch": 0.1764234161988773,
      "grad_norm": 1.6240036487579346,
      "learning_rate": 2.8312248726882873e-05,
      "loss": 0.6361,
      "step": 2200
    },
    {
      "epoch": 0.17722534081796312,
      "grad_norm": 1.3106670379638672,
      "learning_rate": 2.8304207987134817e-05,
      "loss": 0.6307,
      "step": 2210
    },
    {
      "epoch": 0.17802726543704891,
      "grad_norm": 1.5450172424316406,
      "learning_rate": 2.8296167247386762e-05,
      "loss": 0.6511,
      "step": 2220
    },
    {
      "epoch": 0.17882919005613473,
      "grad_norm": 1.5136573314666748,
      "learning_rate": 2.8288126507638703e-05,
      "loss": 0.6551,
      "step": 2230
    },
    {
      "epoch": 0.17963111467522053,
      "grad_norm": 1.409317970275879,
      "learning_rate": 2.8280085767890647e-05,
      "loss": 0.5952,
      "step": 2240
    },
    {
      "epoch": 0.18043303929430635,
      "grad_norm": 1.234025239944458,
      "learning_rate": 2.8272045028142588e-05,
      "loss": 0.6166,
      "step": 2250
    },
    {
      "epoch": 0.18123496391339214,
      "grad_norm": 1.4134129285812378,
      "learning_rate": 2.8264004288394532e-05,
      "loss": 0.6386,
      "step": 2260
    },
    {
      "epoch": 0.18203688853247796,
      "grad_norm": 1.4547454118728638,
      "learning_rate": 2.8255963548646476e-05,
      "loss": 0.6772,
      "step": 2270
    },
    {
      "epoch": 0.18283881315156375,
      "grad_norm": 1.392295002937317,
      "learning_rate": 2.824792280889842e-05,
      "loss": 0.6138,
      "step": 2280
    },
    {
      "epoch": 0.18364073777064957,
      "grad_norm": 1.2586731910705566,
      "learning_rate": 2.823988206915036e-05,
      "loss": 0.6834,
      "step": 2290
    },
    {
      "epoch": 0.18444266238973536,
      "grad_norm": 1.73409104347229,
      "learning_rate": 2.8231841329402306e-05,
      "loss": 0.6372,
      "step": 2300
    },
    {
      "epoch": 0.18524458700882118,
      "grad_norm": 1.2425278425216675,
      "learning_rate": 2.8223800589654247e-05,
      "loss": 0.6644,
      "step": 2310
    },
    {
      "epoch": 0.18604651162790697,
      "grad_norm": 1.306546926498413,
      "learning_rate": 2.8215759849906194e-05,
      "loss": 0.6217,
      "step": 2320
    },
    {
      "epoch": 0.1868484362469928,
      "grad_norm": 1.2606505155563354,
      "learning_rate": 2.8207719110158135e-05,
      "loss": 0.608,
      "step": 2330
    },
    {
      "epoch": 0.18765036086607859,
      "grad_norm": 1.2405767440795898,
      "learning_rate": 2.819967837041008e-05,
      "loss": 0.6572,
      "step": 2340
    },
    {
      "epoch": 0.1884522854851644,
      "grad_norm": 1.3224447965621948,
      "learning_rate": 2.819163763066202e-05,
      "loss": 0.6488,
      "step": 2350
    },
    {
      "epoch": 0.1892542101042502,
      "grad_norm": 1.4504201412200928,
      "learning_rate": 2.8183596890913965e-05,
      "loss": 0.6578,
      "step": 2360
    },
    {
      "epoch": 0.19005613472333602,
      "grad_norm": 1.5213876962661743,
      "learning_rate": 2.817555615116591e-05,
      "loss": 0.627,
      "step": 2370
    },
    {
      "epoch": 0.1908580593424218,
      "grad_norm": 1.3789106607437134,
      "learning_rate": 2.816751541141785e-05,
      "loss": 0.6138,
      "step": 2380
    },
    {
      "epoch": 0.19165998396150763,
      "grad_norm": 1.4276163578033447,
      "learning_rate": 2.8159474671669794e-05,
      "loss": 0.593,
      "step": 2390
    },
    {
      "epoch": 0.19246190858059342,
      "grad_norm": 1.3500195741653442,
      "learning_rate": 2.8151433931921735e-05,
      "loss": 0.6289,
      "step": 2400
    },
    {
      "epoch": 0.19326383319967924,
      "grad_norm": 1.2327450513839722,
      "learning_rate": 2.8143393192173683e-05,
      "loss": 0.61,
      "step": 2410
    },
    {
      "epoch": 0.19406575781876503,
      "grad_norm": 1.345587968826294,
      "learning_rate": 2.8135352452425624e-05,
      "loss": 0.6402,
      "step": 2420
    },
    {
      "epoch": 0.19486768243785085,
      "grad_norm": 1.4042627811431885,
      "learning_rate": 2.8127311712677568e-05,
      "loss": 0.6149,
      "step": 2430
    },
    {
      "epoch": 0.19566960705693665,
      "grad_norm": 1.4328629970550537,
      "learning_rate": 2.811927097292951e-05,
      "loss": 0.692,
      "step": 2440
    },
    {
      "epoch": 0.19647153167602247,
      "grad_norm": 1.557887315750122,
      "learning_rate": 2.8111230233181453e-05,
      "loss": 0.6655,
      "step": 2450
    },
    {
      "epoch": 0.19727345629510826,
      "grad_norm": 1.348586082458496,
      "learning_rate": 2.8103189493433398e-05,
      "loss": 0.6205,
      "step": 2460
    },
    {
      "epoch": 0.19807538091419408,
      "grad_norm": 1.2503771781921387,
      "learning_rate": 2.8095148753685342e-05,
      "loss": 0.6164,
      "step": 2470
    },
    {
      "epoch": 0.19887730553327987,
      "grad_norm": 1.606690526008606,
      "learning_rate": 2.8087108013937283e-05,
      "loss": 0.6326,
      "step": 2480
    },
    {
      "epoch": 0.1996792301523657,
      "grad_norm": 1.557840347290039,
      "learning_rate": 2.8079067274189224e-05,
      "loss": 0.6259,
      "step": 2490
    },
    {
      "epoch": 0.20048115477145148,
      "grad_norm": 1.4212054014205933,
      "learning_rate": 2.807102653444117e-05,
      "loss": 0.6564,
      "step": 2500
    },
    {
      "epoch": 0.2012830793905373,
      "grad_norm": 1.2375462055206299,
      "learning_rate": 2.8062985794693112e-05,
      "loss": 0.5498,
      "step": 2510
    },
    {
      "epoch": 0.2020850040096231,
      "grad_norm": 1.5399283170700073,
      "learning_rate": 2.8054945054945057e-05,
      "loss": 0.6227,
      "step": 2520
    },
    {
      "epoch": 0.2028869286287089,
      "grad_norm": 1.4001368284225464,
      "learning_rate": 2.8046904315196997e-05,
      "loss": 0.6273,
      "step": 2530
    },
    {
      "epoch": 0.2036888532477947,
      "grad_norm": 1.3628602027893066,
      "learning_rate": 2.8038863575448942e-05,
      "loss": 0.6094,
      "step": 2540
    },
    {
      "epoch": 0.20449077786688052,
      "grad_norm": 1.276579737663269,
      "learning_rate": 2.8030822835700886e-05,
      "loss": 0.6448,
      "step": 2550
    },
    {
      "epoch": 0.20529270248596632,
      "grad_norm": 1.5460710525512695,
      "learning_rate": 2.802278209595283e-05,
      "loss": 0.6654,
      "step": 2560
    },
    {
      "epoch": 0.20609462710505214,
      "grad_norm": 1.194461703300476,
      "learning_rate": 2.801474135620477e-05,
      "loss": 0.7008,
      "step": 2570
    },
    {
      "epoch": 0.20689655172413793,
      "grad_norm": 1.4069730043411255,
      "learning_rate": 2.8006700616456715e-05,
      "loss": 0.629,
      "step": 2580
    },
    {
      "epoch": 0.20769847634322375,
      "grad_norm": 1.3398538827896118,
      "learning_rate": 2.7998659876708656e-05,
      "loss": 0.5923,
      "step": 2590
    },
    {
      "epoch": 0.20850040096230954,
      "grad_norm": 1.4880553483963013,
      "learning_rate": 2.79906191369606e-05,
      "loss": 0.6219,
      "step": 2600
    },
    {
      "epoch": 0.20930232558139536,
      "grad_norm": 1.2116053104400635,
      "learning_rate": 2.7982578397212545e-05,
      "loss": 0.6309,
      "step": 2610
    },
    {
      "epoch": 0.21010425020048115,
      "grad_norm": 1.5746179819107056,
      "learning_rate": 2.7974537657464486e-05,
      "loss": 0.6704,
      "step": 2620
    },
    {
      "epoch": 0.21090617481956697,
      "grad_norm": 1.524251937866211,
      "learning_rate": 2.796649691771643e-05,
      "loss": 0.6403,
      "step": 2630
    },
    {
      "epoch": 0.21170809943865276,
      "grad_norm": 1.2424347400665283,
      "learning_rate": 2.795845617796837e-05,
      "loss": 0.6242,
      "step": 2640
    },
    {
      "epoch": 0.21251002405773858,
      "grad_norm": 1.6001981496810913,
      "learning_rate": 2.795041543822032e-05,
      "loss": 0.6844,
      "step": 2650
    },
    {
      "epoch": 0.21331194867682438,
      "grad_norm": 1.3493460416793823,
      "learning_rate": 2.794237469847226e-05,
      "loss": 0.5944,
      "step": 2660
    },
    {
      "epoch": 0.2141138732959102,
      "grad_norm": 1.4057672023773193,
      "learning_rate": 2.7934333958724204e-05,
      "loss": 0.5667,
      "step": 2670
    },
    {
      "epoch": 0.214915797914996,
      "grad_norm": 1.5948914289474487,
      "learning_rate": 2.7926293218976145e-05,
      "loss": 0.6663,
      "step": 2680
    },
    {
      "epoch": 0.2157177225340818,
      "grad_norm": 1.1943273544311523,
      "learning_rate": 2.7918252479228093e-05,
      "loss": 0.6146,
      "step": 2690
    },
    {
      "epoch": 0.2165196471531676,
      "grad_norm": 1.5374540090560913,
      "learning_rate": 2.7910211739480033e-05,
      "loss": 0.6715,
      "step": 2700
    },
    {
      "epoch": 0.21732157177225342,
      "grad_norm": 1.4830135107040405,
      "learning_rate": 2.7902170999731978e-05,
      "loss": 0.6304,
      "step": 2710
    },
    {
      "epoch": 0.2181234963913392,
      "grad_norm": 1.5669100284576416,
      "learning_rate": 2.789413025998392e-05,
      "loss": 0.6542,
      "step": 2720
    },
    {
      "epoch": 0.21892542101042503,
      "grad_norm": 1.4189530611038208,
      "learning_rate": 2.7886089520235863e-05,
      "loss": 0.5863,
      "step": 2730
    },
    {
      "epoch": 0.21972734562951082,
      "grad_norm": 1.3934301137924194,
      "learning_rate": 2.7878048780487807e-05,
      "loss": 0.6581,
      "step": 2740
    },
    {
      "epoch": 0.22052927024859664,
      "grad_norm": 1.3852473497390747,
      "learning_rate": 2.7870008040739748e-05,
      "loss": 0.5882,
      "step": 2750
    },
    {
      "epoch": 0.22133119486768243,
      "grad_norm": 1.3593006134033203,
      "learning_rate": 2.7861967300991692e-05,
      "loss": 0.6176,
      "step": 2760
    },
    {
      "epoch": 0.22213311948676825,
      "grad_norm": 1.4836231470108032,
      "learning_rate": 2.7853926561243633e-05,
      "loss": 0.6257,
      "step": 2770
    },
    {
      "epoch": 0.22293504410585405,
      "grad_norm": 1.4195184707641602,
      "learning_rate": 2.7845885821495578e-05,
      "loss": 0.6192,
      "step": 2780
    },
    {
      "epoch": 0.22373696872493987,
      "grad_norm": 1.6719279289245605,
      "learning_rate": 2.7837845081747522e-05,
      "loss": 0.6676,
      "step": 2790
    },
    {
      "epoch": 0.22453889334402566,
      "grad_norm": 1.5049175024032593,
      "learning_rate": 2.7829804341999466e-05,
      "loss": 0.6193,
      "step": 2800
    },
    {
      "epoch": 0.22534081796311148,
      "grad_norm": 1.4094579219818115,
      "learning_rate": 2.7821763602251407e-05,
      "loss": 0.5866,
      "step": 2810
    },
    {
      "epoch": 0.22614274258219727,
      "grad_norm": 1.6260696649551392,
      "learning_rate": 2.781372286250335e-05,
      "loss": 0.6483,
      "step": 2820
    },
    {
      "epoch": 0.2269446672012831,
      "grad_norm": 1.2416391372680664,
      "learning_rate": 2.7805682122755292e-05,
      "loss": 0.6318,
      "step": 2830
    },
    {
      "epoch": 0.22774659182036888,
      "grad_norm": 1.3791108131408691,
      "learning_rate": 2.779764138300724e-05,
      "loss": 0.6377,
      "step": 2840
    },
    {
      "epoch": 0.2285485164394547,
      "grad_norm": 1.2835760116577148,
      "learning_rate": 2.778960064325918e-05,
      "loss": 0.6019,
      "step": 2850
    },
    {
      "epoch": 0.2293504410585405,
      "grad_norm": 1.455320954322815,
      "learning_rate": 2.7781559903511122e-05,
      "loss": 0.6044,
      "step": 2860
    },
    {
      "epoch": 0.2301523656776263,
      "grad_norm": 1.6636475324630737,
      "learning_rate": 2.7773519163763066e-05,
      "loss": 0.6123,
      "step": 2870
    },
    {
      "epoch": 0.2309542902967121,
      "grad_norm": 1.9701902866363525,
      "learning_rate": 2.776547842401501e-05,
      "loss": 0.5916,
      "step": 2880
    },
    {
      "epoch": 0.23175621491579793,
      "grad_norm": 1.413697361946106,
      "learning_rate": 2.7757437684266955e-05,
      "loss": 0.618,
      "step": 2890
    },
    {
      "epoch": 0.23255813953488372,
      "grad_norm": 1.5849182605743408,
      "learning_rate": 2.7749396944518896e-05,
      "loss": 0.6044,
      "step": 2900
    },
    {
      "epoch": 0.23336006415396954,
      "grad_norm": 1.4669666290283203,
      "learning_rate": 2.774135620477084e-05,
      "loss": 0.6181,
      "step": 2910
    },
    {
      "epoch": 0.23416198877305533,
      "grad_norm": 1.4171847105026245,
      "learning_rate": 2.773331546502278e-05,
      "loss": 0.5597,
      "step": 2920
    },
    {
      "epoch": 0.23496391339214115,
      "grad_norm": 1.4469823837280273,
      "learning_rate": 2.772527472527473e-05,
      "loss": 0.5896,
      "step": 2930
    },
    {
      "epoch": 0.23576583801122694,
      "grad_norm": 1.3491284847259521,
      "learning_rate": 2.771723398552667e-05,
      "loss": 0.6497,
      "step": 2940
    },
    {
      "epoch": 0.23656776263031276,
      "grad_norm": 1.5178112983703613,
      "learning_rate": 2.7709193245778614e-05,
      "loss": 0.6301,
      "step": 2950
    },
    {
      "epoch": 0.23736968724939855,
      "grad_norm": 1.6369824409484863,
      "learning_rate": 2.7701152506030554e-05,
      "loss": 0.6149,
      "step": 2960
    },
    {
      "epoch": 0.23817161186848437,
      "grad_norm": 1.3767271041870117,
      "learning_rate": 2.76931117662825e-05,
      "loss": 0.6114,
      "step": 2970
    },
    {
      "epoch": 0.23897353648757017,
      "grad_norm": 1.315126657485962,
      "learning_rate": 2.7685071026534443e-05,
      "loss": 0.5906,
      "step": 2980
    },
    {
      "epoch": 0.23977546110665598,
      "grad_norm": 1.3068922758102417,
      "learning_rate": 2.7677030286786384e-05,
      "loss": 0.6329,
      "step": 2990
    },
    {
      "epoch": 0.24057738572574178,
      "grad_norm": 1.611670732498169,
      "learning_rate": 2.7668989547038328e-05,
      "loss": 0.6542,
      "step": 3000
    },
    {
      "epoch": 0.2413793103448276,
      "grad_norm": 1.3949557542800903,
      "learning_rate": 2.766094880729027e-05,
      "loss": 0.6403,
      "step": 3010
    },
    {
      "epoch": 0.2421812349639134,
      "grad_norm": 1.3911504745483398,
      "learning_rate": 2.7652908067542213e-05,
      "loss": 0.6972,
      "step": 3020
    },
    {
      "epoch": 0.2429831595829992,
      "grad_norm": 1.3189321756362915,
      "learning_rate": 2.7644867327794158e-05,
      "loss": 0.5879,
      "step": 3030
    },
    {
      "epoch": 0.243785084202085,
      "grad_norm": 1.6265538930892944,
      "learning_rate": 2.7636826588046102e-05,
      "loss": 0.5921,
      "step": 3040
    },
    {
      "epoch": 0.24458700882117082,
      "grad_norm": 1.6278618574142456,
      "learning_rate": 2.7628785848298043e-05,
      "loss": 0.645,
      "step": 3050
    },
    {
      "epoch": 0.2453889334402566,
      "grad_norm": 1.5139058828353882,
      "learning_rate": 2.7620745108549987e-05,
      "loss": 0.6397,
      "step": 3060
    },
    {
      "epoch": 0.24619085805934243,
      "grad_norm": 1.5183194875717163,
      "learning_rate": 2.761270436880193e-05,
      "loss": 0.6339,
      "step": 3070
    },
    {
      "epoch": 0.24699278267842822,
      "grad_norm": 1.4873969554901123,
      "learning_rate": 2.7604663629053876e-05,
      "loss": 0.5826,
      "step": 3080
    },
    {
      "epoch": 0.24779470729751404,
      "grad_norm": 1.5846185684204102,
      "learning_rate": 2.7596622889305817e-05,
      "loss": 0.5986,
      "step": 3090
    },
    {
      "epoch": 0.24859663191659984,
      "grad_norm": 1.264577031135559,
      "learning_rate": 2.758858214955776e-05,
      "loss": 0.6088,
      "step": 3100
    },
    {
      "epoch": 0.24939855653568566,
      "grad_norm": 1.3503204584121704,
      "learning_rate": 2.7580541409809702e-05,
      "loss": 0.5958,
      "step": 3110
    },
    {
      "epoch": 0.25020048115477145,
      "grad_norm": 1.3581395149230957,
      "learning_rate": 2.7572500670061646e-05,
      "loss": 0.5529,
      "step": 3120
    },
    {
      "epoch": 0.25100240577385724,
      "grad_norm": 1.4771637916564941,
      "learning_rate": 2.756445993031359e-05,
      "loss": 0.6309,
      "step": 3130
    },
    {
      "epoch": 0.2518043303929431,
      "grad_norm": 1.6377919912338257,
      "learning_rate": 2.755641919056553e-05,
      "loss": 0.5873,
      "step": 3140
    },
    {
      "epoch": 0.2526062550120289,
      "grad_norm": 1.588535189628601,
      "learning_rate": 2.7548378450817476e-05,
      "loss": 0.5863,
      "step": 3150
    },
    {
      "epoch": 0.25340817963111467,
      "grad_norm": 1.4970754384994507,
      "learning_rate": 2.7540337711069417e-05,
      "loss": 0.6609,
      "step": 3160
    },
    {
      "epoch": 0.25421010425020046,
      "grad_norm": 1.4746472835540771,
      "learning_rate": 2.7532296971321364e-05,
      "loss": 0.618,
      "step": 3170
    },
    {
      "epoch": 0.2550120288692863,
      "grad_norm": 1.5188028812408447,
      "learning_rate": 2.7524256231573305e-05,
      "loss": 0.5548,
      "step": 3180
    },
    {
      "epoch": 0.2558139534883721,
      "grad_norm": 1.4017713069915771,
      "learning_rate": 2.751621549182525e-05,
      "loss": 0.6871,
      "step": 3190
    },
    {
      "epoch": 0.2566158781074579,
      "grad_norm": 1.6491930484771729,
      "learning_rate": 2.750817475207719e-05,
      "loss": 0.6402,
      "step": 3200
    },
    {
      "epoch": 0.2574178027265437,
      "grad_norm": 1.5001329183578491,
      "learning_rate": 2.7500134012329135e-05,
      "loss": 0.6132,
      "step": 3210
    },
    {
      "epoch": 0.25821972734562953,
      "grad_norm": 1.4971147775650024,
      "learning_rate": 2.749209327258108e-05,
      "loss": 0.5811,
      "step": 3220
    },
    {
      "epoch": 0.2590216519647153,
      "grad_norm": 1.339900255203247,
      "learning_rate": 2.7484052532833023e-05,
      "loss": 0.6299,
      "step": 3230
    },
    {
      "epoch": 0.2598235765838011,
      "grad_norm": 1.3588087558746338,
      "learning_rate": 2.7476011793084964e-05,
      "loss": 0.6438,
      "step": 3240
    },
    {
      "epoch": 0.2606255012028869,
      "grad_norm": 1.4845222234725952,
      "learning_rate": 2.7467971053336905e-05,
      "loss": 0.6103,
      "step": 3250
    },
    {
      "epoch": 0.26142742582197276,
      "grad_norm": 1.4144387245178223,
      "learning_rate": 2.7459930313588853e-05,
      "loss": 0.612,
      "step": 3260
    },
    {
      "epoch": 0.26222935044105855,
      "grad_norm": 1.4576947689056396,
      "learning_rate": 2.7451889573840794e-05,
      "loss": 0.6244,
      "step": 3270
    },
    {
      "epoch": 0.26303127506014434,
      "grad_norm": 1.338538646697998,
      "learning_rate": 2.7443848834092738e-05,
      "loss": 0.6239,
      "step": 3280
    },
    {
      "epoch": 0.26383319967923013,
      "grad_norm": 1.3707998991012573,
      "learning_rate": 2.743580809434468e-05,
      "loss": 0.6309,
      "step": 3290
    },
    {
      "epoch": 0.264635124298316,
      "grad_norm": 1.397597074508667,
      "learning_rate": 2.7427767354596623e-05,
      "loss": 0.6477,
      "step": 3300
    },
    {
      "epoch": 0.2654370489174018,
      "grad_norm": 1.478083848953247,
      "learning_rate": 2.7419726614848567e-05,
      "loss": 0.6123,
      "step": 3310
    },
    {
      "epoch": 0.26623897353648757,
      "grad_norm": 1.3734484910964966,
      "learning_rate": 2.741168587510051e-05,
      "loss": 0.6079,
      "step": 3320
    },
    {
      "epoch": 0.26704089815557336,
      "grad_norm": 1.542303204536438,
      "learning_rate": 2.7403645135352453e-05,
      "loss": 0.5948,
      "step": 3330
    },
    {
      "epoch": 0.2678428227746592,
      "grad_norm": 1.5639832019805908,
      "learning_rate": 2.7395604395604397e-05,
      "loss": 0.6005,
      "step": 3340
    },
    {
      "epoch": 0.268644747393745,
      "grad_norm": 1.6194173097610474,
      "learning_rate": 2.7387563655856338e-05,
      "loss": 0.6075,
      "step": 3350
    },
    {
      "epoch": 0.2694466720128308,
      "grad_norm": 1.6174107789993286,
      "learning_rate": 2.7379522916108285e-05,
      "loss": 0.5636,
      "step": 3360
    },
    {
      "epoch": 0.2702485966319166,
      "grad_norm": 1.5695592164993286,
      "learning_rate": 2.7371482176360226e-05,
      "loss": 0.5793,
      "step": 3370
    },
    {
      "epoch": 0.27105052125100243,
      "grad_norm": 1.5747427940368652,
      "learning_rate": 2.7363441436612167e-05,
      "loss": 0.6192,
      "step": 3380
    },
    {
      "epoch": 0.2718524458700882,
      "grad_norm": 1.5453064441680908,
      "learning_rate": 2.735540069686411e-05,
      "loss": 0.6597,
      "step": 3390
    },
    {
      "epoch": 0.272654370489174,
      "grad_norm": 1.3238022327423096,
      "learning_rate": 2.7347359957116052e-05,
      "loss": 0.5632,
      "step": 3400
    },
    {
      "epoch": 0.2734562951082598,
      "grad_norm": 1.285562515258789,
      "learning_rate": 2.7339319217368e-05,
      "loss": 0.6168,
      "step": 3410
    },
    {
      "epoch": 0.27425821972734565,
      "grad_norm": 1.3593552112579346,
      "learning_rate": 2.733127847761994e-05,
      "loss": 0.6016,
      "step": 3420
    },
    {
      "epoch": 0.27506014434643145,
      "grad_norm": 1.4439457654953003,
      "learning_rate": 2.7323237737871885e-05,
      "loss": 0.5974,
      "step": 3430
    },
    {
      "epoch": 0.27586206896551724,
      "grad_norm": 1.4875260591506958,
      "learning_rate": 2.7315196998123826e-05,
      "loss": 0.5927,
      "step": 3440
    },
    {
      "epoch": 0.27666399358460303,
      "grad_norm": 1.6316664218902588,
      "learning_rate": 2.7307156258375774e-05,
      "loss": 0.6493,
      "step": 3450
    },
    {
      "epoch": 0.2774659182036889,
      "grad_norm": 1.4189059734344482,
      "learning_rate": 2.7299115518627715e-05,
      "loss": 0.5676,
      "step": 3460
    },
    {
      "epoch": 0.27826784282277467,
      "grad_norm": 1.6044950485229492,
      "learning_rate": 2.729107477887966e-05,
      "loss": 0.6685,
      "step": 3470
    },
    {
      "epoch": 0.27906976744186046,
      "grad_norm": 1.615651249885559,
      "learning_rate": 2.72830340391316e-05,
      "loss": 0.661,
      "step": 3480
    },
    {
      "epoch": 0.27987169206094625,
      "grad_norm": 1.3089569807052612,
      "learning_rate": 2.7274993299383544e-05,
      "loss": 0.6126,
      "step": 3490
    },
    {
      "epoch": 0.2806736166800321,
      "grad_norm": 1.6578876972198486,
      "learning_rate": 2.726695255963549e-05,
      "loss": 0.5874,
      "step": 3500
    },
    {
      "epoch": 0.2814755412991179,
      "grad_norm": 1.483527421951294,
      "learning_rate": 2.725891181988743e-05,
      "loss": 0.6295,
      "step": 3510
    },
    {
      "epoch": 0.2822774659182037,
      "grad_norm": 1.5734176635742188,
      "learning_rate": 2.7250871080139374e-05,
      "loss": 0.6942,
      "step": 3520
    },
    {
      "epoch": 0.2830793905372895,
      "grad_norm": 1.5322977304458618,
      "learning_rate": 2.7242830340391315e-05,
      "loss": 0.5662,
      "step": 3530
    },
    {
      "epoch": 0.2838813151563753,
      "grad_norm": 1.6422761678695679,
      "learning_rate": 2.723478960064326e-05,
      "loss": 0.6303,
      "step": 3540
    },
    {
      "epoch": 0.2846832397754611,
      "grad_norm": 1.5114028453826904,
      "learning_rate": 2.7226748860895203e-05,
      "loss": 0.6202,
      "step": 3550
    },
    {
      "epoch": 0.2854851643945469,
      "grad_norm": 1.3789223432540894,
      "learning_rate": 2.7218708121147148e-05,
      "loss": 0.6133,
      "step": 3560
    },
    {
      "epoch": 0.2862870890136327,
      "grad_norm": 1.7046444416046143,
      "learning_rate": 2.721066738139909e-05,
      "loss": 0.5917,
      "step": 3570
    },
    {
      "epoch": 0.28708901363271855,
      "grad_norm": 1.5022094249725342,
      "learning_rate": 2.7202626641651033e-05,
      "loss": 0.5597,
      "step": 3580
    },
    {
      "epoch": 0.28789093825180434,
      "grad_norm": 1.348301887512207,
      "learning_rate": 2.7194585901902974e-05,
      "loss": 0.5803,
      "step": 3590
    },
    {
      "epoch": 0.28869286287089013,
      "grad_norm": 1.4106274843215942,
      "learning_rate": 2.718654516215492e-05,
      "loss": 0.5599,
      "step": 3600
    },
    {
      "epoch": 0.2894947874899759,
      "grad_norm": 1.6440179347991943,
      "learning_rate": 2.7178504422406862e-05,
      "loss": 0.6489,
      "step": 3610
    },
    {
      "epoch": 0.29029671210906177,
      "grad_norm": 1.3251760005950928,
      "learning_rate": 2.7170463682658806e-05,
      "loss": 0.6152,
      "step": 3620
    },
    {
      "epoch": 0.29109863672814756,
      "grad_norm": 1.4686747789382935,
      "learning_rate": 2.7162422942910747e-05,
      "loss": 0.63,
      "step": 3630
    },
    {
      "epoch": 0.29190056134723336,
      "grad_norm": 1.5818842649459839,
      "learning_rate": 2.715438220316269e-05,
      "loss": 0.5844,
      "step": 3640
    },
    {
      "epoch": 0.29270248596631915,
      "grad_norm": 1.5871332883834839,
      "learning_rate": 2.7146341463414636e-05,
      "loss": 0.6577,
      "step": 3650
    },
    {
      "epoch": 0.293504410585405,
      "grad_norm": 1.578505277633667,
      "learning_rate": 2.7138300723666577e-05,
      "loss": 0.6347,
      "step": 3660
    },
    {
      "epoch": 0.2943063352044908,
      "grad_norm": 1.5468961000442505,
      "learning_rate": 2.713025998391852e-05,
      "loss": 0.6045,
      "step": 3670
    },
    {
      "epoch": 0.2951082598235766,
      "grad_norm": 1.8418478965759277,
      "learning_rate": 2.7122219244170462e-05,
      "loss": 0.5659,
      "step": 3680
    },
    {
      "epoch": 0.29591018444266237,
      "grad_norm": 1.5138293504714966,
      "learning_rate": 2.711417850442241e-05,
      "loss": 0.5748,
      "step": 3690
    },
    {
      "epoch": 0.2967121090617482,
      "grad_norm": 1.4272884130477905,
      "learning_rate": 2.710613776467435e-05,
      "loss": 0.6346,
      "step": 3700
    },
    {
      "epoch": 0.297514033680834,
      "grad_norm": 1.546251893043518,
      "learning_rate": 2.7098097024926295e-05,
      "loss": 0.6011,
      "step": 3710
    },
    {
      "epoch": 0.2983159582999198,
      "grad_norm": 2.033334255218506,
      "learning_rate": 2.7090056285178236e-05,
      "loss": 0.6185,
      "step": 3720
    },
    {
      "epoch": 0.2991178829190056,
      "grad_norm": 1.4411084651947021,
      "learning_rate": 2.708201554543018e-05,
      "loss": 0.6278,
      "step": 3730
    },
    {
      "epoch": 0.29991980753809144,
      "grad_norm": 1.6519290208816528,
      "learning_rate": 2.7073974805682124e-05,
      "loss": 0.6722,
      "step": 3740
    },
    {
      "epoch": 0.30072173215717724,
      "grad_norm": NaN,
      "learning_rate": 2.706593406593407e-05,
      "loss": 0.6041,
      "step": 3750
    },
    {
      "epoch": 0.301523656776263,
      "grad_norm": 1.4494953155517578,
      "learning_rate": 2.7058697400160815e-05,
      "loss": 0.5831,
      "step": 3760
    },
    {
      "epoch": 0.3023255813953488,
      "grad_norm": 1.433707356452942,
      "learning_rate": 2.705065666041276e-05,
      "loss": 0.6915,
      "step": 3770
    },
    {
      "epoch": 0.30312750601443467,
      "grad_norm": 1.553870677947998,
      "learning_rate": 2.70426159206647e-05,
      "loss": 0.6205,
      "step": 3780
    },
    {
      "epoch": 0.30392943063352046,
      "grad_norm": 1.69192373752594,
      "learning_rate": 2.7034575180916648e-05,
      "loss": 0.6493,
      "step": 3790
    },
    {
      "epoch": 0.30473135525260625,
      "grad_norm": 1.3957420587539673,
      "learning_rate": 2.702653444116859e-05,
      "loss": 0.6119,
      "step": 3800
    },
    {
      "epoch": 0.30553327987169204,
      "grad_norm": 1.5539894104003906,
      "learning_rate": 2.7018493701420533e-05,
      "loss": 0.6431,
      "step": 3810
    },
    {
      "epoch": 0.3063352044907779,
      "grad_norm": 1.698089838027954,
      "learning_rate": 2.7010452961672474e-05,
      "loss": 0.6082,
      "step": 3820
    },
    {
      "epoch": 0.3071371291098637,
      "grad_norm": 1.5497697591781616,
      "learning_rate": 2.7002412221924415e-05,
      "loss": 0.6215,
      "step": 3830
    },
    {
      "epoch": 0.3079390537289495,
      "grad_norm": 1.4458705186843872,
      "learning_rate": 2.6994371482176362e-05,
      "loss": 0.5958,
      "step": 3840
    },
    {
      "epoch": 0.30874097834803527,
      "grad_norm": 1.4415103197097778,
      "learning_rate": 2.6986330742428303e-05,
      "loss": 0.6409,
      "step": 3850
    },
    {
      "epoch": 0.3095429029671211,
      "grad_norm": 1.347275972366333,
      "learning_rate": 2.6978290002680248e-05,
      "loss": 0.6983,
      "step": 3860
    },
    {
      "epoch": 0.3103448275862069,
      "grad_norm": 1.5121724605560303,
      "learning_rate": 2.697024926293219e-05,
      "loss": 0.6124,
      "step": 3870
    },
    {
      "epoch": 0.3111467522052927,
      "grad_norm": 1.8303495645523071,
      "learning_rate": 2.6962208523184133e-05,
      "loss": 0.6101,
      "step": 3880
    },
    {
      "epoch": 0.3119486768243785,
      "grad_norm": 1.4420201778411865,
      "learning_rate": 2.6954167783436077e-05,
      "loss": 0.6823,
      "step": 3890
    },
    {
      "epoch": 0.31275060144346434,
      "grad_norm": 1.3711342811584473,
      "learning_rate": 2.694612704368802e-05,
      "loss": 0.6288,
      "step": 3900
    },
    {
      "epoch": 0.31355252606255013,
      "grad_norm": 1.4477187395095825,
      "learning_rate": 2.6938086303939962e-05,
      "loss": 0.6035,
      "step": 3910
    },
    {
      "epoch": 0.3143544506816359,
      "grad_norm": 1.8074172735214233,
      "learning_rate": 2.6930045564191907e-05,
      "loss": 0.6084,
      "step": 3920
    },
    {
      "epoch": 0.3151563753007217,
      "grad_norm": 1.4450874328613281,
      "learning_rate": 2.692200482444385e-05,
      "loss": 0.5788,
      "step": 3930
    },
    {
      "epoch": 0.31595829991980756,
      "grad_norm": 1.5550017356872559,
      "learning_rate": 2.6913964084695795e-05,
      "loss": 0.6431,
      "step": 3940
    },
    {
      "epoch": 0.31676022453889335,
      "grad_norm": 1.458552598953247,
      "learning_rate": 2.6905923344947736e-05,
      "loss": 0.5748,
      "step": 3950
    },
    {
      "epoch": 0.31756214915797915,
      "grad_norm": 1.6673219203948975,
      "learning_rate": 2.6897882605199677e-05,
      "loss": 0.6081,
      "step": 3960
    },
    {
      "epoch": 0.31836407377706494,
      "grad_norm": 1.46927011013031,
      "learning_rate": 2.688984186545162e-05,
      "loss": 0.6495,
      "step": 3970
    },
    {
      "epoch": 0.3191659983961508,
      "grad_norm": 1.728278636932373,
      "learning_rate": 2.6881801125703565e-05,
      "loss": 0.5829,
      "step": 3980
    },
    {
      "epoch": 0.3199679230152366,
      "grad_norm": 1.5342211723327637,
      "learning_rate": 2.687376038595551e-05,
      "loss": 0.6123,
      "step": 3990
    },
    {
      "epoch": 0.32076984763432237,
      "grad_norm": 1.6328465938568115,
      "learning_rate": 2.686571964620745e-05,
      "loss": 0.6572,
      "step": 4000
    },
    {
      "epoch": 0.32157177225340816,
      "grad_norm": 1.3701305389404297,
      "learning_rate": 2.6857678906459395e-05,
      "loss": 0.5816,
      "step": 4010
    },
    {
      "epoch": 0.322373696872494,
      "grad_norm": 1.4335153102874756,
      "learning_rate": 2.6849638166711336e-05,
      "loss": 0.5676,
      "step": 4020
    },
    {
      "epoch": 0.3231756214915798,
      "grad_norm": 1.8738735914230347,
      "learning_rate": 2.6841597426963284e-05,
      "loss": 0.6153,
      "step": 4030
    },
    {
      "epoch": 0.3239775461106656,
      "grad_norm": 1.2561821937561035,
      "learning_rate": 2.6833556687215224e-05,
      "loss": 0.5816,
      "step": 4040
    },
    {
      "epoch": 0.3247794707297514,
      "grad_norm": 1.3859317302703857,
      "learning_rate": 2.682551594746717e-05,
      "loss": 0.5884,
      "step": 4050
    },
    {
      "epoch": 0.32558139534883723,
      "grad_norm": 1.6637743711471558,
      "learning_rate": 2.681747520771911e-05,
      "loss": 0.6319,
      "step": 4060
    },
    {
      "epoch": 0.326383319967923,
      "grad_norm": 1.717056155204773,
      "learning_rate": 2.6809434467971054e-05,
      "loss": 0.6311,
      "step": 4070
    },
    {
      "epoch": 0.3271852445870088,
      "grad_norm": 1.6087554693222046,
      "learning_rate": 2.6801393728222998e-05,
      "loss": 0.6652,
      "step": 4080
    },
    {
      "epoch": 0.3279871692060946,
      "grad_norm": 1.4289469718933105,
      "learning_rate": 2.679335298847494e-05,
      "loss": 0.5919,
      "step": 4090
    },
    {
      "epoch": 0.32878909382518046,
      "grad_norm": 1.4969282150268555,
      "learning_rate": 2.6785312248726883e-05,
      "loss": 0.605,
      "step": 4100
    },
    {
      "epoch": 0.32959101844426625,
      "grad_norm": 1.5455485582351685,
      "learning_rate": 2.6777271508978824e-05,
      "loss": 0.5896,
      "step": 4110
    },
    {
      "epoch": 0.33039294306335204,
      "grad_norm": 1.4553545713424683,
      "learning_rate": 2.6769230769230772e-05,
      "loss": 0.5915,
      "step": 4120
    },
    {
      "epoch": 0.33119486768243783,
      "grad_norm": 1.5617977380752563,
      "learning_rate": 2.6761190029482713e-05,
      "loss": 0.5723,
      "step": 4130
    },
    {
      "epoch": 0.3319967923015237,
      "grad_norm": 1.6689674854278564,
      "learning_rate": 2.6753149289734657e-05,
      "loss": 0.5612,
      "step": 4140
    },
    {
      "epoch": 0.33279871692060947,
      "grad_norm": 1.780930519104004,
      "learning_rate": 2.6745108549986598e-05,
      "loss": 0.5917,
      "step": 4150
    },
    {
      "epoch": 0.33360064153969526,
      "grad_norm": 1.5901604890823364,
      "learning_rate": 2.6737067810238542e-05,
      "loss": 0.5745,
      "step": 4160
    },
    {
      "epoch": 0.33440256615878106,
      "grad_norm": 1.5612314939498901,
      "learning_rate": 2.6729027070490487e-05,
      "loss": 0.5427,
      "step": 4170
    },
    {
      "epoch": 0.3352044907778669,
      "grad_norm": 1.5935319662094116,
      "learning_rate": 2.672098633074243e-05,
      "loss": 0.6155,
      "step": 4180
    },
    {
      "epoch": 0.3360064153969527,
      "grad_norm": 1.676984429359436,
      "learning_rate": 2.6712945590994372e-05,
      "loss": 0.6248,
      "step": 4190
    },
    {
      "epoch": 0.3368083400160385,
      "grad_norm": 1.7498773336410522,
      "learning_rate": 2.6704904851246316e-05,
      "loss": 0.6118,
      "step": 4200
    },
    {
      "epoch": 0.3376102646351243,
      "grad_norm": 1.4154579639434814,
      "learning_rate": 2.6696864111498257e-05,
      "loss": 0.6104,
      "step": 4210
    },
    {
      "epoch": 0.3384121892542101,
      "grad_norm": 1.607508897781372,
      "learning_rate": 2.66888233717502e-05,
      "loss": 0.5516,
      "step": 4220
    },
    {
      "epoch": 0.3392141138732959,
      "grad_norm": 1.6388139724731445,
      "learning_rate": 2.6680782632002146e-05,
      "loss": 0.6121,
      "step": 4230
    },
    {
      "epoch": 0.3400160384923817,
      "grad_norm": 1.5422872304916382,
      "learning_rate": 2.6672741892254087e-05,
      "loss": 0.6615,
      "step": 4240
    },
    {
      "epoch": 0.3408179631114675,
      "grad_norm": 1.5889458656311035,
      "learning_rate": 2.666470115250603e-05,
      "loss": 0.6379,
      "step": 4250
    },
    {
      "epoch": 0.34161988773055335,
      "grad_norm": 1.5921320915222168,
      "learning_rate": 2.6656660412757975e-05,
      "loss": 0.5681,
      "step": 4260
    },
    {
      "epoch": 0.34242181234963914,
      "grad_norm": 2.022529363632202,
      "learning_rate": 2.664861967300992e-05,
      "loss": 0.521,
      "step": 4270
    },
    {
      "epoch": 0.34322373696872494,
      "grad_norm": 1.9901739358901978,
      "learning_rate": 2.664057893326186e-05,
      "loss": 0.6057,
      "step": 4280
    },
    {
      "epoch": 0.3440256615878107,
      "grad_norm": 1.3542289733886719,
      "learning_rate": 2.6632538193513805e-05,
      "loss": 0.617,
      "step": 4290
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 1.9057998657226562,
      "learning_rate": 2.6624497453765745e-05,
      "loss": 0.6591,
      "step": 4300
    },
    {
      "epoch": 0.34562951082598237,
      "grad_norm": 1.5369259119033813,
      "learning_rate": 2.6616456714017693e-05,
      "loss": 0.6678,
      "step": 4310
    },
    {
      "epoch": 0.34643143544506816,
      "grad_norm": 1.2470067739486694,
      "learning_rate": 2.6608415974269634e-05,
      "loss": 0.598,
      "step": 4320
    },
    {
      "epoch": 0.34723336006415395,
      "grad_norm": 1.4384686946868896,
      "learning_rate": 2.660037523452158e-05,
      "loss": 0.5727,
      "step": 4330
    },
    {
      "epoch": 0.3480352846832398,
      "grad_norm": 1.593152642250061,
      "learning_rate": 2.659233449477352e-05,
      "loss": 0.611,
      "step": 4340
    },
    {
      "epoch": 0.3488372093023256,
      "grad_norm": 1.6318409442901611,
      "learning_rate": 2.658429375502546e-05,
      "loss": 0.5956,
      "step": 4350
    },
    {
      "epoch": 0.3496391339214114,
      "grad_norm": 1.657525658607483,
      "learning_rate": 2.6576253015277408e-05,
      "loss": 0.6015,
      "step": 4360
    },
    {
      "epoch": 0.3504410585404972,
      "grad_norm": 1.4977713823318481,
      "learning_rate": 2.656821227552935e-05,
      "loss": 0.6391,
      "step": 4370
    },
    {
      "epoch": 0.351242983159583,
      "grad_norm": 1.5955793857574463,
      "learning_rate": 2.6560171535781293e-05,
      "loss": 0.5601,
      "step": 4380
    },
    {
      "epoch": 0.3520449077786688,
      "grad_norm": 1.6175109148025513,
      "learning_rate": 2.6552130796033234e-05,
      "loss": 0.6594,
      "step": 4390
    },
    {
      "epoch": 0.3528468323977546,
      "grad_norm": 1.3048713207244873,
      "learning_rate": 2.6544090056285178e-05,
      "loss": 0.6049,
      "step": 4400
    },
    {
      "epoch": 0.3536487570168404,
      "grad_norm": 1.6534415483474731,
      "learning_rate": 2.6536049316537123e-05,
      "loss": 0.5542,
      "step": 4410
    },
    {
      "epoch": 0.35445068163592625,
      "grad_norm": 1.5510953664779663,
      "learning_rate": 2.6528008576789067e-05,
      "loss": 0.5834,
      "step": 4420
    },
    {
      "epoch": 0.35525260625501204,
      "grad_norm": 1.6446726322174072,
      "learning_rate": 2.6519967837041008e-05,
      "loss": 0.6208,
      "step": 4430
    },
    {
      "epoch": 0.35605453087409783,
      "grad_norm": 1.6787478923797607,
      "learning_rate": 2.6511927097292952e-05,
      "loss": 0.637,
      "step": 4440
    },
    {
      "epoch": 0.3568564554931836,
      "grad_norm": 1.218406319618225,
      "learning_rate": 2.6503886357544896e-05,
      "loss": 0.5998,
      "step": 4450
    },
    {
      "epoch": 0.35765838011226947,
      "grad_norm": 1.6132779121398926,
      "learning_rate": 2.649584561779684e-05,
      "loss": 0.6024,
      "step": 4460
    },
    {
      "epoch": 0.35846030473135526,
      "grad_norm": 1.3203647136688232,
      "learning_rate": 2.648780487804878e-05,
      "loss": 0.6188,
      "step": 4470
    },
    {
      "epoch": 0.35926222935044105,
      "grad_norm": 1.3462380170822144,
      "learning_rate": 2.6479764138300722e-05,
      "loss": 0.5885,
      "step": 4480
    },
    {
      "epoch": 0.36006415396952685,
      "grad_norm": 1.6233619451522827,
      "learning_rate": 2.6471723398552667e-05,
      "loss": 0.6207,
      "step": 4490
    },
    {
      "epoch": 0.3608660785886127,
      "grad_norm": 1.3978971242904663,
      "learning_rate": 2.646368265880461e-05,
      "loss": 0.6096,
      "step": 4500
    },
    {
      "epoch": 0.3616680032076985,
      "grad_norm": 1.7037148475646973,
      "learning_rate": 2.6455641919056555e-05,
      "loss": 0.6277,
      "step": 4510
    },
    {
      "epoch": 0.3624699278267843,
      "grad_norm": 1.5437586307525635,
      "learning_rate": 2.6447601179308496e-05,
      "loss": 0.6828,
      "step": 4520
    },
    {
      "epoch": 0.36327185244587007,
      "grad_norm": 1.3653382062911987,
      "learning_rate": 2.643956043956044e-05,
      "loss": 0.6165,
      "step": 4530
    },
    {
      "epoch": 0.3640737770649559,
      "grad_norm": 1.5418215990066528,
      "learning_rate": 2.643151969981238e-05,
      "loss": 0.577,
      "step": 4540
    },
    {
      "epoch": 0.3648757016840417,
      "grad_norm": 1.5657297372817993,
      "learning_rate": 2.642347896006433e-05,
      "loss": 0.605,
      "step": 4550
    },
    {
      "epoch": 0.3656776263031275,
      "grad_norm": 1.8022136688232422,
      "learning_rate": 2.641543822031627e-05,
      "loss": 0.6096,
      "step": 4560
    },
    {
      "epoch": 0.3664795509222133,
      "grad_norm": 1.65766441822052,
      "learning_rate": 2.6407397480568214e-05,
      "loss": 0.5738,
      "step": 4570
    },
    {
      "epoch": 0.36728147554129914,
      "grad_norm": 1.4734035730361938,
      "learning_rate": 2.6399356740820155e-05,
      "loss": 0.5844,
      "step": 4580
    },
    {
      "epoch": 0.36808340016038493,
      "grad_norm": 1.551049828529358,
      "learning_rate": 2.63913160010721e-05,
      "loss": 0.5915,
      "step": 4590
    },
    {
      "epoch": 0.3688853247794707,
      "grad_norm": 1.5841782093048096,
      "learning_rate": 2.6383275261324044e-05,
      "loss": 0.6177,
      "step": 4600
    },
    {
      "epoch": 0.3696872493985565,
      "grad_norm": 1.4425804615020752,
      "learning_rate": 2.6375234521575985e-05,
      "loss": 0.6305,
      "step": 4610
    },
    {
      "epoch": 0.37048917401764236,
      "grad_norm": 1.6226519346237183,
      "learning_rate": 2.636719378182793e-05,
      "loss": 0.6138,
      "step": 4620
    },
    {
      "epoch": 0.37129109863672816,
      "grad_norm": 1.7740142345428467,
      "learning_rate": 2.635915304207987e-05,
      "loss": 0.6334,
      "step": 4630
    },
    {
      "epoch": 0.37209302325581395,
      "grad_norm": 1.3970069885253906,
      "learning_rate": 2.6351112302331817e-05,
      "loss": 0.6253,
      "step": 4640
    },
    {
      "epoch": 0.37289494787489974,
      "grad_norm": 1.6576545238494873,
      "learning_rate": 2.634307156258376e-05,
      "loss": 0.6218,
      "step": 4650
    },
    {
      "epoch": 0.3736968724939856,
      "grad_norm": 1.516103744506836,
      "learning_rate": 2.6335030822835703e-05,
      "loss": 0.5951,
      "step": 4660
    },
    {
      "epoch": 0.3744987971130714,
      "grad_norm": 1.6015424728393555,
      "learning_rate": 2.6326990083087644e-05,
      "loss": 0.6487,
      "step": 4670
    },
    {
      "epoch": 0.37530072173215717,
      "grad_norm": 1.4253774881362915,
      "learning_rate": 2.6318949343339588e-05,
      "loss": 0.6636,
      "step": 4680
    },
    {
      "epoch": 0.37610264635124296,
      "grad_norm": 1.535227656364441,
      "learning_rate": 2.6310908603591532e-05,
      "loss": 0.5688,
      "step": 4690
    },
    {
      "epoch": 0.3769045709703288,
      "grad_norm": 1.6005113124847412,
      "learning_rate": 2.6302867863843476e-05,
      "loss": 0.6428,
      "step": 4700
    },
    {
      "epoch": 0.3777064955894146,
      "grad_norm": 1.5801719427108765,
      "learning_rate": 2.6294827124095417e-05,
      "loss": 0.5783,
      "step": 4710
    },
    {
      "epoch": 0.3785084202085004,
      "grad_norm": 1.5879523754119873,
      "learning_rate": 2.6286786384347358e-05,
      "loss": 0.5977,
      "step": 4720
    },
    {
      "epoch": 0.3793103448275862,
      "grad_norm": 1.868796944618225,
      "learning_rate": 2.6278745644599303e-05,
      "loss": 0.676,
      "step": 4730
    },
    {
      "epoch": 0.38011226944667204,
      "grad_norm": 1.3801058530807495,
      "learning_rate": 2.6270704904851247e-05,
      "loss": 0.6171,
      "step": 4740
    },
    {
      "epoch": 0.3809141940657578,
      "grad_norm": 1.7187824249267578,
      "learning_rate": 2.626266416510319e-05,
      "loss": 0.6817,
      "step": 4750
    },
    {
      "epoch": 0.3817161186848436,
      "grad_norm": 1.822066068649292,
      "learning_rate": 2.6254623425355132e-05,
      "loss": 0.5924,
      "step": 4760
    },
    {
      "epoch": 0.3825180433039294,
      "grad_norm": 1.6567187309265137,
      "learning_rate": 2.6246582685607076e-05,
      "loss": 0.6573,
      "step": 4770
    },
    {
      "epoch": 0.38331996792301526,
      "grad_norm": 1.4616279602050781,
      "learning_rate": 2.6238541945859017e-05,
      "loss": 0.6124,
      "step": 4780
    },
    {
      "epoch": 0.38412189254210105,
      "grad_norm": 1.4815911054611206,
      "learning_rate": 2.6230501206110965e-05,
      "loss": 0.6012,
      "step": 4790
    },
    {
      "epoch": 0.38492381716118684,
      "grad_norm": 1.5755914449691772,
      "learning_rate": 2.6222460466362906e-05,
      "loss": 0.6293,
      "step": 4800
    },
    {
      "epoch": 0.38572574178027264,
      "grad_norm": 1.656983733177185,
      "learning_rate": 2.621441972661485e-05,
      "loss": 0.6713,
      "step": 4810
    },
    {
      "epoch": 0.3865276663993585,
      "grad_norm": 1.6775248050689697,
      "learning_rate": 2.620637898686679e-05,
      "loss": 0.6106,
      "step": 4820
    },
    {
      "epoch": 0.3873295910184443,
      "grad_norm": 1.4061250686645508,
      "learning_rate": 2.619833824711874e-05,
      "loss": 0.6627,
      "step": 4830
    },
    {
      "epoch": 0.38813151563753007,
      "grad_norm": 1.4399563074111938,
      "learning_rate": 2.619029750737068e-05,
      "loss": 0.6164,
      "step": 4840
    },
    {
      "epoch": 0.38893344025661586,
      "grad_norm": 1.4971128702163696,
      "learning_rate": 2.618225676762262e-05,
      "loss": 0.5613,
      "step": 4850
    },
    {
      "epoch": 0.3897353648757017,
      "grad_norm": 1.4792795181274414,
      "learning_rate": 2.6174216027874565e-05,
      "loss": 0.6016,
      "step": 4860
    },
    {
      "epoch": 0.3905372894947875,
      "grad_norm": 1.7207545042037964,
      "learning_rate": 2.6166175288126506e-05,
      "loss": 0.6616,
      "step": 4870
    },
    {
      "epoch": 0.3913392141138733,
      "grad_norm": 1.5240651369094849,
      "learning_rate": 2.6158134548378453e-05,
      "loss": 0.6112,
      "step": 4880
    },
    {
      "epoch": 0.3921411387329591,
      "grad_norm": 1.361528992652893,
      "learning_rate": 2.6150093808630394e-05,
      "loss": 0.6647,
      "step": 4890
    },
    {
      "epoch": 0.39294306335204493,
      "grad_norm": 1.5272105932235718,
      "learning_rate": 2.614205306888234e-05,
      "loss": 0.5703,
      "step": 4900
    },
    {
      "epoch": 0.3937449879711307,
      "grad_norm": 1.6025593280792236,
      "learning_rate": 2.613401232913428e-05,
      "loss": 0.6578,
      "step": 4910
    },
    {
      "epoch": 0.3945469125902165,
      "grad_norm": 1.5129808187484741,
      "learning_rate": 2.6125971589386224e-05,
      "loss": 0.6181,
      "step": 4920
    },
    {
      "epoch": 0.3953488372093023,
      "grad_norm": 1.8218035697937012,
      "learning_rate": 2.6117930849638168e-05,
      "loss": 0.5732,
      "step": 4930
    },
    {
      "epoch": 0.39615076182838815,
      "grad_norm": 1.5358437299728394,
      "learning_rate": 2.6109890109890112e-05,
      "loss": 0.6215,
      "step": 4940
    },
    {
      "epoch": 0.39695268644747395,
      "grad_norm": 1.616064429283142,
      "learning_rate": 2.6101849370142053e-05,
      "loss": 0.6067,
      "step": 4950
    },
    {
      "epoch": 0.39775461106655974,
      "grad_norm": 1.4546717405319214,
      "learning_rate": 2.6093808630393997e-05,
      "loss": 0.6088,
      "step": 4960
    },
    {
      "epoch": 0.39855653568564553,
      "grad_norm": 1.3768023252487183,
      "learning_rate": 2.608576789064594e-05,
      "loss": 0.575,
      "step": 4970
    },
    {
      "epoch": 0.3993584603047314,
      "grad_norm": 1.608655333518982,
      "learning_rate": 2.6077727150897883e-05,
      "loss": 0.625,
      "step": 4980
    },
    {
      "epoch": 0.40016038492381717,
      "grad_norm": 1.7516961097717285,
      "learning_rate": 2.6069686411149827e-05,
      "loss": 0.5972,
      "step": 4990
    },
    {
      "epoch": 0.40096230954290296,
      "grad_norm": 1.5878812074661255,
      "learning_rate": 2.6061645671401768e-05,
      "loss": 0.5994,
      "step": 5000
    },
    {
      "epoch": 0.40176423416198875,
      "grad_norm": 1.577592372894287,
      "learning_rate": 2.6053604931653712e-05,
      "loss": 0.6238,
      "step": 5010
    },
    {
      "epoch": 0.4025661587810746,
      "grad_norm": 1.5830533504486084,
      "learning_rate": 2.6045564191905656e-05,
      "loss": 0.6044,
      "step": 5020
    },
    {
      "epoch": 0.4033680834001604,
      "grad_norm": 1.4136557579040527,
      "learning_rate": 2.60375234521576e-05,
      "loss": 0.6424,
      "step": 5030
    },
    {
      "epoch": 0.4041700080192462,
      "grad_norm": 1.2618482112884521,
      "learning_rate": 2.602948271240954e-05,
      "loss": 0.5831,
      "step": 5040
    },
    {
      "epoch": 0.404971932638332,
      "grad_norm": 1.7508409023284912,
      "learning_rate": 2.6021441972661486e-05,
      "loss": 0.5646,
      "step": 5050
    },
    {
      "epoch": 0.4057738572574178,
      "grad_norm": 1.4933476448059082,
      "learning_rate": 2.6013401232913427e-05,
      "loss": 0.6254,
      "step": 5060
    },
    {
      "epoch": 0.4065757818765036,
      "grad_norm": 1.587368130683899,
      "learning_rate": 2.6005360493165375e-05,
      "loss": 0.6379,
      "step": 5070
    },
    {
      "epoch": 0.4073777064955894,
      "grad_norm": 1.4154807329177856,
      "learning_rate": 2.5997319753417315e-05,
      "loss": 0.6081,
      "step": 5080
    },
    {
      "epoch": 0.4081796311146752,
      "grad_norm": 1.5304632186889648,
      "learning_rate": 2.598927901366926e-05,
      "loss": 0.5952,
      "step": 5090
    },
    {
      "epoch": 0.40898155573376105,
      "grad_norm": 1.2642375230789185,
      "learning_rate": 2.59812382739212e-05,
      "loss": 0.6031,
      "step": 5100
    },
    {
      "epoch": 0.40978348035284684,
      "grad_norm": 1.6060441732406616,
      "learning_rate": 2.597319753417314e-05,
      "loss": 0.5857,
      "step": 5110
    },
    {
      "epoch": 0.41058540497193263,
      "grad_norm": 2.1062798500061035,
      "learning_rate": 2.596515679442509e-05,
      "loss": 0.6505,
      "step": 5120
    },
    {
      "epoch": 0.4113873295910184,
      "grad_norm": 1.8217889070510864,
      "learning_rate": 2.595711605467703e-05,
      "loss": 0.6059,
      "step": 5130
    },
    {
      "epoch": 0.41218925421010427,
      "grad_norm": 1.4453837871551514,
      "learning_rate": 2.5949075314928974e-05,
      "loss": 0.635,
      "step": 5140
    },
    {
      "epoch": 0.41299117882919006,
      "grad_norm": 1.6330127716064453,
      "learning_rate": 2.5941034575180915e-05,
      "loss": 0.5969,
      "step": 5150
    },
    {
      "epoch": 0.41379310344827586,
      "grad_norm": 1.5274558067321777,
      "learning_rate": 2.593299383543286e-05,
      "loss": 0.6015,
      "step": 5160
    },
    {
      "epoch": 0.41459502806736165,
      "grad_norm": 1.7702208757400513,
      "learning_rate": 2.5924953095684804e-05,
      "loss": 0.652,
      "step": 5170
    },
    {
      "epoch": 0.4153969526864475,
      "grad_norm": 1.4910527467727661,
      "learning_rate": 2.5916912355936748e-05,
      "loss": 0.6302,
      "step": 5180
    },
    {
      "epoch": 0.4161988773055333,
      "grad_norm": 1.6416535377502441,
      "learning_rate": 2.590887161618869e-05,
      "loss": 0.6406,
      "step": 5190
    },
    {
      "epoch": 0.4170008019246191,
      "grad_norm": 1.7422605752944946,
      "learning_rate": 2.5900830876440633e-05,
      "loss": 0.6678,
      "step": 5200
    },
    {
      "epoch": 0.41780272654370487,
      "grad_norm": 1.45233952999115,
      "learning_rate": 2.5892790136692578e-05,
      "loss": 0.6232,
      "step": 5210
    },
    {
      "epoch": 0.4186046511627907,
      "grad_norm": 1.8196592330932617,
      "learning_rate": 2.5884749396944522e-05,
      "loss": 0.6446,
      "step": 5220
    },
    {
      "epoch": 0.4194065757818765,
      "grad_norm": 1.5874704122543335,
      "learning_rate": 2.5876708657196463e-05,
      "loss": 0.6122,
      "step": 5230
    },
    {
      "epoch": 0.4202085004009623,
      "grad_norm": 1.5575200319290161,
      "learning_rate": 2.5868667917448404e-05,
      "loss": 0.6301,
      "step": 5240
    },
    {
      "epoch": 0.4210104250200481,
      "grad_norm": 1.77097749710083,
      "learning_rate": 2.5860627177700348e-05,
      "loss": 0.6095,
      "step": 5250
    },
    {
      "epoch": 0.42181234963913394,
      "grad_norm": 1.4831057786941528,
      "learning_rate": 2.5852586437952292e-05,
      "loss": 0.5827,
      "step": 5260
    },
    {
      "epoch": 0.42261427425821974,
      "grad_norm": 1.5552730560302734,
      "learning_rate": 2.5844545698204237e-05,
      "loss": 0.5871,
      "step": 5270
    },
    {
      "epoch": 0.4234161988773055,
      "grad_norm": 1.3304383754730225,
      "learning_rate": 2.5836504958456178e-05,
      "loss": 0.5866,
      "step": 5280
    },
    {
      "epoch": 0.4242181234963913,
      "grad_norm": 1.5168521404266357,
      "learning_rate": 2.5828464218708122e-05,
      "loss": 0.6105,
      "step": 5290
    },
    {
      "epoch": 0.42502004811547717,
      "grad_norm": 1.3617037534713745,
      "learning_rate": 2.5820423478960063e-05,
      "loss": 0.5932,
      "step": 5300
    },
    {
      "epoch": 0.42582197273456296,
      "grad_norm": 1.2702521085739136,
      "learning_rate": 2.581238273921201e-05,
      "loss": 0.6587,
      "step": 5310
    },
    {
      "epoch": 0.42662389735364875,
      "grad_norm": 1.432910442352295,
      "learning_rate": 2.580434199946395e-05,
      "loss": 0.5938,
      "step": 5320
    },
    {
      "epoch": 0.42742582197273454,
      "grad_norm": 1.7520325183868408,
      "learning_rate": 2.5796301259715896e-05,
      "loss": 0.6216,
      "step": 5330
    },
    {
      "epoch": 0.4282277465918204,
      "grad_norm": 1.6285183429718018,
      "learning_rate": 2.5788260519967836e-05,
      "loss": 0.545,
      "step": 5340
    },
    {
      "epoch": 0.4290296712109062,
      "grad_norm": 1.6449668407440186,
      "learning_rate": 2.578021978021978e-05,
      "loss": 0.5577,
      "step": 5350
    },
    {
      "epoch": 0.429831595829992,
      "grad_norm": 1.5886681079864502,
      "learning_rate": 2.5772179040471725e-05,
      "loss": 0.5678,
      "step": 5360
    },
    {
      "epoch": 0.43063352044907777,
      "grad_norm": 1.580465316772461,
      "learning_rate": 2.5764138300723666e-05,
      "loss": 0.5599,
      "step": 5370
    },
    {
      "epoch": 0.4314354450681636,
      "grad_norm": 1.6091152429580688,
      "learning_rate": 2.575609756097561e-05,
      "loss": 0.5905,
      "step": 5380
    },
    {
      "epoch": 0.4322373696872494,
      "grad_norm": 1.5737898349761963,
      "learning_rate": 2.574805682122755e-05,
      "loss": 0.5542,
      "step": 5390
    },
    {
      "epoch": 0.4330392943063352,
      "grad_norm": 1.6390786170959473,
      "learning_rate": 2.57400160814795e-05,
      "loss": 0.6161,
      "step": 5400
    },
    {
      "epoch": 0.433841218925421,
      "grad_norm": 1.4988294839859009,
      "learning_rate": 2.573197534173144e-05,
      "loss": 0.625,
      "step": 5410
    },
    {
      "epoch": 0.43464314354450684,
      "grad_norm": 1.6460880041122437,
      "learning_rate": 2.5723934601983384e-05,
      "loss": 0.5924,
      "step": 5420
    },
    {
      "epoch": 0.43544506816359263,
      "grad_norm": 1.6054571866989136,
      "learning_rate": 2.5715893862235325e-05,
      "loss": 0.5986,
      "step": 5430
    },
    {
      "epoch": 0.4362469927826784,
      "grad_norm": 1.6282105445861816,
      "learning_rate": 2.570785312248727e-05,
      "loss": 0.6183,
      "step": 5440
    },
    {
      "epoch": 0.4370489174017642,
      "grad_norm": 1.9992282390594482,
      "learning_rate": 2.5699812382739213e-05,
      "loss": 0.5926,
      "step": 5450
    },
    {
      "epoch": 0.43785084202085006,
      "grad_norm": 1.5207819938659668,
      "learning_rate": 2.5691771642991158e-05,
      "loss": 0.5635,
      "step": 5460
    },
    {
      "epoch": 0.43865276663993585,
      "grad_norm": 1.4941203594207764,
      "learning_rate": 2.56837309032431e-05,
      "loss": 0.5926,
      "step": 5470
    },
    {
      "epoch": 0.43945469125902165,
      "grad_norm": 1.654986023902893,
      "learning_rate": 2.5675690163495043e-05,
      "loss": 0.6411,
      "step": 5480
    },
    {
      "epoch": 0.44025661587810744,
      "grad_norm": 1.6559665203094482,
      "learning_rate": 2.5667649423746984e-05,
      "loss": 0.5833,
      "step": 5490
    },
    {
      "epoch": 0.4410585404971933,
      "grad_norm": 1.3819018602371216,
      "learning_rate": 2.5659608683998928e-05,
      "loss": 0.6059,
      "step": 5500
    },
    {
      "epoch": 0.4418604651162791,
      "grad_norm": 1.578421711921692,
      "learning_rate": 2.5651567944250872e-05,
      "loss": 0.5961,
      "step": 5510
    },
    {
      "epoch": 0.44266238973536487,
      "grad_norm": 1.5918368101119995,
      "learning_rate": 2.5643527204502813e-05,
      "loss": 0.6083,
      "step": 5520
    },
    {
      "epoch": 0.44346431435445066,
      "grad_norm": 1.462402105331421,
      "learning_rate": 2.5635486464754758e-05,
      "loss": 0.5906,
      "step": 5530
    },
    {
      "epoch": 0.4442662389735365,
      "grad_norm": 1.6288669109344482,
      "learning_rate": 2.5627445725006702e-05,
      "loss": 0.6285,
      "step": 5540
    },
    {
      "epoch": 0.4450681635926223,
      "grad_norm": 1.4951895475387573,
      "learning_rate": 2.5619404985258646e-05,
      "loss": 0.6226,
      "step": 5550
    },
    {
      "epoch": 0.4458700882117081,
      "grad_norm": 1.6346086263656616,
      "learning_rate": 2.5611364245510587e-05,
      "loss": 0.6253,
      "step": 5560
    },
    {
      "epoch": 0.4466720128307939,
      "grad_norm": 1.7414339780807495,
      "learning_rate": 2.560332350576253e-05,
      "loss": 0.6037,
      "step": 5570
    },
    {
      "epoch": 0.44747393744987973,
      "grad_norm": 1.7394847869873047,
      "learning_rate": 2.5595282766014472e-05,
      "loss": 0.6503,
      "step": 5580
    },
    {
      "epoch": 0.4482758620689655,
      "grad_norm": 1.4541990756988525,
      "learning_rate": 2.558724202626642e-05,
      "loss": 0.6265,
      "step": 5590
    },
    {
      "epoch": 0.4490777866880513,
      "grad_norm": 1.722224473953247,
      "learning_rate": 2.557920128651836e-05,
      "loss": 0.622,
      "step": 5600
    },
    {
      "epoch": 0.4498797113071371,
      "grad_norm": 1.5436909198760986,
      "learning_rate": 2.5571160546770305e-05,
      "loss": 0.6245,
      "step": 5610
    },
    {
      "epoch": 0.45068163592622296,
      "grad_norm": 1.5832304954528809,
      "learning_rate": 2.5563119807022246e-05,
      "loss": 0.5729,
      "step": 5620
    },
    {
      "epoch": 0.45148356054530875,
      "grad_norm": 1.72721266746521,
      "learning_rate": 2.5555079067274187e-05,
      "loss": 0.6265,
      "step": 5630
    },
    {
      "epoch": 0.45228548516439454,
      "grad_norm": 1.6823011636734009,
      "learning_rate": 2.5547038327526135e-05,
      "loss": 0.6211,
      "step": 5640
    },
    {
      "epoch": 0.45308740978348033,
      "grad_norm": 1.4767086505889893,
      "learning_rate": 2.5538997587778076e-05,
      "loss": 0.5557,
      "step": 5650
    },
    {
      "epoch": 0.4538893344025662,
      "grad_norm": 1.7997798919677734,
      "learning_rate": 2.553095684803002e-05,
      "loss": 0.5531,
      "step": 5660
    },
    {
      "epoch": 0.454691259021652,
      "grad_norm": 1.444644570350647,
      "learning_rate": 2.552291610828196e-05,
      "loss": 0.5667,
      "step": 5670
    },
    {
      "epoch": 0.45549318364073776,
      "grad_norm": 1.4672819375991821,
      "learning_rate": 2.5514875368533905e-05,
      "loss": 0.5673,
      "step": 5680
    },
    {
      "epoch": 0.45629510825982356,
      "grad_norm": 1.567459225654602,
      "learning_rate": 2.550683462878585e-05,
      "loss": 0.6341,
      "step": 5690
    },
    {
      "epoch": 0.4570970328789094,
      "grad_norm": 1.7647836208343506,
      "learning_rate": 2.5498793889037794e-05,
      "loss": 0.6553,
      "step": 5700
    },
    {
      "epoch": 0.4578989574979952,
      "grad_norm": 1.6448445320129395,
      "learning_rate": 2.5490753149289735e-05,
      "loss": 0.6186,
      "step": 5710
    },
    {
      "epoch": 0.458700882117081,
      "grad_norm": 1.5378637313842773,
      "learning_rate": 2.548271240954168e-05,
      "loss": 0.569,
      "step": 5720
    },
    {
      "epoch": 0.4595028067361668,
      "grad_norm": 1.4073621034622192,
      "learning_rate": 2.5474671669793623e-05,
      "loss": 0.5357,
      "step": 5730
    },
    {
      "epoch": 0.4603047313552526,
      "grad_norm": 1.5316739082336426,
      "learning_rate": 2.5466630930045567e-05,
      "loss": 0.5763,
      "step": 5740
    },
    {
      "epoch": 0.4611066559743384,
      "grad_norm": 1.5343847274780273,
      "learning_rate": 2.545859019029751e-05,
      "loss": 0.568,
      "step": 5750
    },
    {
      "epoch": 0.4619085805934242,
      "grad_norm": 1.5570379495620728,
      "learning_rate": 2.545054945054945e-05,
      "loss": 0.6193,
      "step": 5760
    },
    {
      "epoch": 0.46271050521251,
      "grad_norm": 1.5265408754348755,
      "learning_rate": 2.5442508710801394e-05,
      "loss": 0.6013,
      "step": 5770
    },
    {
      "epoch": 0.46351242983159585,
      "grad_norm": NaN,
      "learning_rate": 2.5434467971053338e-05,
      "loss": 0.6399,
      "step": 5780
    },
    {
      "epoch": 0.46431435445068164,
      "grad_norm": 1.6765142679214478,
      "learning_rate": 2.5427231305280087e-05,
      "loss": 0.6801,
      "step": 5790
    },
    {
      "epoch": 0.46511627906976744,
      "grad_norm": 1.628781795501709,
      "learning_rate": 2.541919056553203e-05,
      "loss": 0.6521,
      "step": 5800
    },
    {
      "epoch": 0.4659182036888532,
      "grad_norm": 1.6031501293182373,
      "learning_rate": 2.5411149825783973e-05,
      "loss": 0.5498,
      "step": 5810
    },
    {
      "epoch": 0.4667201283079391,
      "grad_norm": 1.443633794784546,
      "learning_rate": 2.5403109086035913e-05,
      "loss": 0.5636,
      "step": 5820
    },
    {
      "epoch": 0.46752205292702487,
      "grad_norm": 1.3864365816116333,
      "learning_rate": 2.539506834628786e-05,
      "loss": 0.6042,
      "step": 5830
    },
    {
      "epoch": 0.46832397754611066,
      "grad_norm": 1.7406530380249023,
      "learning_rate": 2.5387027606539802e-05,
      "loss": 0.6003,
      "step": 5840
    },
    {
      "epoch": 0.46912590216519645,
      "grad_norm": 1.4920183420181274,
      "learning_rate": 2.5378986866791746e-05,
      "loss": 0.5988,
      "step": 5850
    },
    {
      "epoch": 0.4699278267842823,
      "grad_norm": 1.5580673217773438,
      "learning_rate": 2.5370946127043687e-05,
      "loss": 0.6027,
      "step": 5860
    },
    {
      "epoch": 0.4707297514033681,
      "grad_norm": 1.6345728635787964,
      "learning_rate": 2.536290538729563e-05,
      "loss": 0.5672,
      "step": 5870
    },
    {
      "epoch": 0.4715316760224539,
      "grad_norm": 1.6657264232635498,
      "learning_rate": 2.5354864647547576e-05,
      "loss": 0.5965,
      "step": 5880
    },
    {
      "epoch": 0.4723336006415397,
      "grad_norm": 1.5284652709960938,
      "learning_rate": 2.534682390779952e-05,
      "loss": 0.6436,
      "step": 5890
    },
    {
      "epoch": 0.4731355252606255,
      "grad_norm": 1.5368016958236694,
      "learning_rate": 2.533878316805146e-05,
      "loss": 0.5977,
      "step": 5900
    },
    {
      "epoch": 0.4739374498797113,
      "grad_norm": 1.4316089153289795,
      "learning_rate": 2.5330742428303405e-05,
      "loss": 0.6087,
      "step": 5910
    },
    {
      "epoch": 0.4747393744987971,
      "grad_norm": 1.2418997287750244,
      "learning_rate": 2.5322701688555346e-05,
      "loss": 0.5363,
      "step": 5920
    },
    {
      "epoch": 0.4755412991178829,
      "grad_norm": 1.408596396446228,
      "learning_rate": 2.5314660948807294e-05,
      "loss": 0.6936,
      "step": 5930
    },
    {
      "epoch": 0.47634322373696875,
      "grad_norm": 1.8796824216842651,
      "learning_rate": 2.5306620209059235e-05,
      "loss": 0.574,
      "step": 5940
    },
    {
      "epoch": 0.47714514835605454,
      "grad_norm": 1.4225823879241943,
      "learning_rate": 2.5298579469311176e-05,
      "loss": 0.6389,
      "step": 5950
    },
    {
      "epoch": 0.47794707297514033,
      "grad_norm": 1.4391367435455322,
      "learning_rate": 2.529053872956312e-05,
      "loss": 0.5802,
      "step": 5960
    },
    {
      "epoch": 0.4787489975942261,
      "grad_norm": 2.0265085697174072,
      "learning_rate": 2.528249798981506e-05,
      "loss": 0.5777,
      "step": 5970
    },
    {
      "epoch": 0.47955092221331197,
      "grad_norm": 1.6870598793029785,
      "learning_rate": 2.527445725006701e-05,
      "loss": 0.6148,
      "step": 5980
    },
    {
      "epoch": 0.48035284683239776,
      "grad_norm": 1.702362298965454,
      "learning_rate": 2.526641651031895e-05,
      "loss": 0.6533,
      "step": 5990
    },
    {
      "epoch": 0.48115477145148355,
      "grad_norm": 1.6342554092407227,
      "learning_rate": 2.5258375770570894e-05,
      "loss": 0.674,
      "step": 6000
    },
    {
      "epoch": 0.48195669607056935,
      "grad_norm": 1.471211552619934,
      "learning_rate": 2.5250335030822835e-05,
      "loss": 0.6134,
      "step": 6010
    },
    {
      "epoch": 0.4827586206896552,
      "grad_norm": 1.4427627325057983,
      "learning_rate": 2.5242294291074782e-05,
      "loss": 0.5803,
      "step": 6020
    },
    {
      "epoch": 0.483560545308741,
      "grad_norm": 1.7079994678497314,
      "learning_rate": 2.5234253551326723e-05,
      "loss": 0.5775,
      "step": 6030
    },
    {
      "epoch": 0.4843624699278268,
      "grad_norm": 1.6390665769577026,
      "learning_rate": 2.5226212811578667e-05,
      "loss": 0.5825,
      "step": 6040
    },
    {
      "epoch": 0.48516439454691257,
      "grad_norm": 1.5326111316680908,
      "learning_rate": 2.521817207183061e-05,
      "loss": 0.606,
      "step": 6050
    },
    {
      "epoch": 0.4859663191659984,
      "grad_norm": 1.5199825763702393,
      "learning_rate": 2.5210131332082553e-05,
      "loss": 0.6021,
      "step": 6060
    },
    {
      "epoch": 0.4867682437850842,
      "grad_norm": 1.6701099872589111,
      "learning_rate": 2.5202090592334497e-05,
      "loss": 0.6119,
      "step": 6070
    },
    {
      "epoch": 0.48757016840417,
      "grad_norm": 1.6165319681167603,
      "learning_rate": 2.5194049852586438e-05,
      "loss": 0.6239,
      "step": 6080
    },
    {
      "epoch": 0.4883720930232558,
      "grad_norm": 2.007859468460083,
      "learning_rate": 2.5186009112838382e-05,
      "loss": 0.5448,
      "step": 6090
    },
    {
      "epoch": 0.48917401764234164,
      "grad_norm": 1.6011909246444702,
      "learning_rate": 2.5177968373090323e-05,
      "loss": 0.5995,
      "step": 6100
    },
    {
      "epoch": 0.48997594226142743,
      "grad_norm": 1.548524260520935,
      "learning_rate": 2.5169927633342267e-05,
      "loss": 0.6164,
      "step": 6110
    },
    {
      "epoch": 0.4907778668805132,
      "grad_norm": 1.6018249988555908,
      "learning_rate": 2.516188689359421e-05,
      "loss": 0.5289,
      "step": 6120
    },
    {
      "epoch": 0.491579791499599,
      "grad_norm": 1.5732202529907227,
      "learning_rate": 2.5153846153846156e-05,
      "loss": 0.622,
      "step": 6130
    },
    {
      "epoch": 0.49238171611868486,
      "grad_norm": 2.0110199451446533,
      "learning_rate": 2.5145805414098097e-05,
      "loss": 0.6425,
      "step": 6140
    },
    {
      "epoch": 0.49318364073777066,
      "grad_norm": 1.7628172636032104,
      "learning_rate": 2.513776467435004e-05,
      "loss": 0.5494,
      "step": 6150
    },
    {
      "epoch": 0.49398556535685645,
      "grad_norm": 1.7444331645965576,
      "learning_rate": 2.5129723934601982e-05,
      "loss": 0.6035,
      "step": 6160
    },
    {
      "epoch": 0.49478748997594224,
      "grad_norm": 1.5544507503509521,
      "learning_rate": 2.512168319485393e-05,
      "loss": 0.5633,
      "step": 6170
    },
    {
      "epoch": 0.4955894145950281,
      "grad_norm": 1.4216164350509644,
      "learning_rate": 2.511364245510587e-05,
      "loss": 0.5323,
      "step": 6180
    },
    {
      "epoch": 0.4963913392141139,
      "grad_norm": 1.570238709449768,
      "learning_rate": 2.5105601715357815e-05,
      "loss": 0.6148,
      "step": 6190
    },
    {
      "epoch": 0.4971932638331997,
      "grad_norm": 1.691766619682312,
      "learning_rate": 2.5097560975609756e-05,
      "loss": 0.592,
      "step": 6200
    },
    {
      "epoch": 0.49799518845228546,
      "grad_norm": 1.5956631898880005,
      "learning_rate": 2.50895202358617e-05,
      "loss": 0.5739,
      "step": 6210
    },
    {
      "epoch": 0.4987971130713713,
      "grad_norm": 1.3003145456314087,
      "learning_rate": 2.5081479496113644e-05,
      "loss": 0.6017,
      "step": 6220
    },
    {
      "epoch": 0.4995990376904571,
      "grad_norm": 1.912972092628479,
      "learning_rate": 2.5073438756365585e-05,
      "loss": 0.6282,
      "step": 6230
    },
    {
      "epoch": 0.5004009623095429,
      "grad_norm": 1.3591201305389404,
      "learning_rate": 2.506539801661753e-05,
      "loss": 0.5992,
      "step": 6240
    },
    {
      "epoch": 0.5012028869286287,
      "grad_norm": 1.631271481513977,
      "learning_rate": 2.505735727686947e-05,
      "loss": 0.5964,
      "step": 6250
    },
    {
      "epoch": 0.5020048115477145,
      "grad_norm": 1.630192756652832,
      "learning_rate": 2.5049316537121418e-05,
      "loss": 0.5995,
      "step": 6260
    },
    {
      "epoch": 0.5028067361668003,
      "grad_norm": 1.6400288343429565,
      "learning_rate": 2.504127579737336e-05,
      "loss": 0.5444,
      "step": 6270
    },
    {
      "epoch": 0.5036086607858862,
      "grad_norm": 1.7261252403259277,
      "learning_rate": 2.5033235057625303e-05,
      "loss": 0.5847,
      "step": 6280
    },
    {
      "epoch": 0.504410585404972,
      "grad_norm": 1.5363014936447144,
      "learning_rate": 2.5025194317877244e-05,
      "loss": 0.6364,
      "step": 6290
    },
    {
      "epoch": 0.5052125100240578,
      "grad_norm": 1.3961668014526367,
      "learning_rate": 2.501715357812919e-05,
      "loss": 0.5965,
      "step": 6300
    },
    {
      "epoch": 0.5060144346431436,
      "grad_norm": 1.3048348426818848,
      "learning_rate": 2.5009112838381133e-05,
      "loss": 0.6398,
      "step": 6310
    },
    {
      "epoch": 0.5068163592622293,
      "grad_norm": 1.7021684646606445,
      "learning_rate": 2.5001072098633077e-05,
      "loss": 0.6646,
      "step": 6320
    },
    {
      "epoch": 0.5076182838813151,
      "grad_norm": 1.4897878170013428,
      "learning_rate": 2.4993031358885018e-05,
      "loss": 0.579,
      "step": 6330
    },
    {
      "epoch": 0.5084202085004009,
      "grad_norm": 1.7598850727081299,
      "learning_rate": 2.498499061913696e-05,
      "loss": 0.5948,
      "step": 6340
    },
    {
      "epoch": 0.5092221331194867,
      "grad_norm": 1.3248900175094604,
      "learning_rate": 2.4976949879388903e-05,
      "loss": 0.5686,
      "step": 6350
    },
    {
      "epoch": 0.5100240577385726,
      "grad_norm": 1.6727821826934814,
      "learning_rate": 2.4968909139640847e-05,
      "loss": 0.618,
      "step": 6360
    },
    {
      "epoch": 0.5108259823576584,
      "grad_norm": 1.519919514656067,
      "learning_rate": 2.4960868399892792e-05,
      "loss": 0.6422,
      "step": 6370
    },
    {
      "epoch": 0.5116279069767442,
      "grad_norm": 1.3528324365615845,
      "learning_rate": 2.4952827660144733e-05,
      "loss": 0.6482,
      "step": 6380
    },
    {
      "epoch": 0.51242983159583,
      "grad_norm": 1.738972783088684,
      "learning_rate": 2.4944786920396677e-05,
      "loss": 0.5685,
      "step": 6390
    },
    {
      "epoch": 0.5132317562149158,
      "grad_norm": 1.4480305910110474,
      "learning_rate": 2.493674618064862e-05,
      "loss": 0.572,
      "step": 6400
    },
    {
      "epoch": 0.5140336808340016,
      "grad_norm": 1.6071616411209106,
      "learning_rate": 2.4928705440900566e-05,
      "loss": 0.6141,
      "step": 6410
    },
    {
      "epoch": 0.5148356054530874,
      "grad_norm": 1.5328032970428467,
      "learning_rate": 2.4920664701152506e-05,
      "loss": 0.6121,
      "step": 6420
    },
    {
      "epoch": 0.5156375300721732,
      "grad_norm": 1.5407911539077759,
      "learning_rate": 2.491262396140445e-05,
      "loss": 0.581,
      "step": 6430
    },
    {
      "epoch": 0.5164394546912591,
      "grad_norm": 1.686139702796936,
      "learning_rate": 2.490458322165639e-05,
      "loss": 0.6094,
      "step": 6440
    },
    {
      "epoch": 0.5172413793103449,
      "grad_norm": 1.828940987586975,
      "learning_rate": 2.4896542481908336e-05,
      "loss": 0.6041,
      "step": 6450
    },
    {
      "epoch": 0.5180433039294307,
      "grad_norm": 1.7385226488113403,
      "learning_rate": 2.488850174216028e-05,
      "loss": 0.6677,
      "step": 6460
    },
    {
      "epoch": 0.5188452285485164,
      "grad_norm": 1.3660128116607666,
      "learning_rate": 2.488046100241222e-05,
      "loss": 0.633,
      "step": 6470
    },
    {
      "epoch": 0.5196471531676022,
      "grad_norm": 1.4542884826660156,
      "learning_rate": 2.4872420262664165e-05,
      "loss": 0.583,
      "step": 6480
    },
    {
      "epoch": 0.520449077786688,
      "grad_norm": 1.611895203590393,
      "learning_rate": 2.4864379522916106e-05,
      "loss": 0.6029,
      "step": 6490
    },
    {
      "epoch": 0.5212510024057738,
      "grad_norm": 1.4500828981399536,
      "learning_rate": 2.4856338783168054e-05,
      "loss": 0.6283,
      "step": 6500
    },
    {
      "epoch": 0.5220529270248596,
      "grad_norm": 1.5014456510543823,
      "learning_rate": 2.4848298043419995e-05,
      "loss": 0.6524,
      "step": 6510
    },
    {
      "epoch": 0.5228548516439455,
      "grad_norm": 1.2917581796646118,
      "learning_rate": 2.484025730367194e-05,
      "loss": 0.5608,
      "step": 6520
    },
    {
      "epoch": 0.5236567762630313,
      "grad_norm": 1.612703561782837,
      "learning_rate": 2.483221656392388e-05,
      "loss": 0.56,
      "step": 6530
    },
    {
      "epoch": 0.5244587008821171,
      "grad_norm": 1.7755030393600464,
      "learning_rate": 2.4824175824175824e-05,
      "loss": 0.563,
      "step": 6540
    },
    {
      "epoch": 0.5252606255012029,
      "grad_norm": 1.665791630744934,
      "learning_rate": 2.481613508442777e-05,
      "loss": 0.5553,
      "step": 6550
    },
    {
      "epoch": 0.5260625501202887,
      "grad_norm": 1.5581721067428589,
      "learning_rate": 2.4808094344679713e-05,
      "loss": 0.5559,
      "step": 6560
    },
    {
      "epoch": 0.5268644747393745,
      "grad_norm": 1.5431488752365112,
      "learning_rate": 2.4800053604931654e-05,
      "loss": 0.5856,
      "step": 6570
    },
    {
      "epoch": 0.5276663993584603,
      "grad_norm": 1.434761643409729,
      "learning_rate": 2.4792012865183595e-05,
      "loss": 0.6116,
      "step": 6580
    },
    {
      "epoch": 0.5284683239775461,
      "grad_norm": 1.6113218069076538,
      "learning_rate": 2.4783972125435542e-05,
      "loss": 0.5887,
      "step": 6590
    },
    {
      "epoch": 0.529270248596632,
      "grad_norm": 1.5881599187850952,
      "learning_rate": 2.4775931385687483e-05,
      "loss": 0.568,
      "step": 6600
    },
    {
      "epoch": 0.5300721732157178,
      "grad_norm": 1.7467799186706543,
      "learning_rate": 2.4767890645939428e-05,
      "loss": 0.601,
      "step": 6610
    },
    {
      "epoch": 0.5308740978348035,
      "grad_norm": 1.6382209062576294,
      "learning_rate": 2.475984990619137e-05,
      "loss": 0.6075,
      "step": 6620
    },
    {
      "epoch": 0.5316760224538893,
      "grad_norm": 1.6022700071334839,
      "learning_rate": 2.4751809166443313e-05,
      "loss": 0.5684,
      "step": 6630
    },
    {
      "epoch": 0.5324779470729751,
      "grad_norm": 1.479522943496704,
      "learning_rate": 2.4743768426695257e-05,
      "loss": 0.5548,
      "step": 6640
    },
    {
      "epoch": 0.5332798716920609,
      "grad_norm": 1.3715535402297974,
      "learning_rate": 2.47357276869472e-05,
      "loss": 0.654,
      "step": 6650
    },
    {
      "epoch": 0.5340817963111467,
      "grad_norm": 1.8191584348678589,
      "learning_rate": 2.4727686947199142e-05,
      "loss": 0.5674,
      "step": 6660
    },
    {
      "epoch": 0.5348837209302325,
      "grad_norm": 1.9051910638809204,
      "learning_rate": 2.4719646207451087e-05,
      "loss": 0.5818,
      "step": 6670
    },
    {
      "epoch": 0.5356856455493184,
      "grad_norm": 1.5410009622573853,
      "learning_rate": 2.4711605467703027e-05,
      "loss": 0.6022,
      "step": 6680
    },
    {
      "epoch": 0.5364875701684042,
      "grad_norm": 1.6393649578094482,
      "learning_rate": 2.4703564727954975e-05,
      "loss": 0.6228,
      "step": 6690
    },
    {
      "epoch": 0.53728949478749,
      "grad_norm": 1.5286184549331665,
      "learning_rate": 2.4695523988206916e-05,
      "loss": 0.6108,
      "step": 6700
    },
    {
      "epoch": 0.5380914194065758,
      "grad_norm": 1.8409037590026855,
      "learning_rate": 2.4687483248458857e-05,
      "loss": 0.5482,
      "step": 6710
    },
    {
      "epoch": 0.5388933440256616,
      "grad_norm": 1.5084295272827148,
      "learning_rate": 2.46794425087108e-05,
      "loss": 0.6075,
      "step": 6720
    },
    {
      "epoch": 0.5396952686447474,
      "grad_norm": 1.3873863220214844,
      "learning_rate": 2.4671401768962742e-05,
      "loss": 0.5935,
      "step": 6730
    },
    {
      "epoch": 0.5404971932638332,
      "grad_norm": 1.665177345275879,
      "learning_rate": 2.466336102921469e-05,
      "loss": 0.6334,
      "step": 6740
    },
    {
      "epoch": 0.541299117882919,
      "grad_norm": 1.56954026222229,
      "learning_rate": 2.465532028946663e-05,
      "loss": 0.5484,
      "step": 6750
    },
    {
      "epoch": 0.5421010425020049,
      "grad_norm": 1.8764424324035645,
      "learning_rate": 2.4647279549718575e-05,
      "loss": 0.6576,
      "step": 6760
    },
    {
      "epoch": 0.5429029671210907,
      "grad_norm": 1.457796573638916,
      "learning_rate": 2.4639238809970516e-05,
      "loss": 0.6748,
      "step": 6770
    },
    {
      "epoch": 0.5437048917401764,
      "grad_norm": 1.5188610553741455,
      "learning_rate": 2.4631198070222464e-05,
      "loss": 0.6467,
      "step": 6780
    },
    {
      "epoch": 0.5445068163592622,
      "grad_norm": 1.6398130655288696,
      "learning_rate": 2.4623157330474405e-05,
      "loss": 0.5907,
      "step": 6790
    },
    {
      "epoch": 0.545308740978348,
      "grad_norm": 1.8907690048217773,
      "learning_rate": 2.461511659072635e-05,
      "loss": 0.5801,
      "step": 6800
    },
    {
      "epoch": 0.5461106655974338,
      "grad_norm": 2.209591865539551,
      "learning_rate": 2.460707585097829e-05,
      "loss": 0.5714,
      "step": 6810
    },
    {
      "epoch": 0.5469125902165196,
      "grad_norm": 1.7670618295669556,
      "learning_rate": 2.4599035111230234e-05,
      "loss": 0.5735,
      "step": 6820
    },
    {
      "epoch": 0.5477145148356054,
      "grad_norm": 1.3760828971862793,
      "learning_rate": 2.4590994371482178e-05,
      "loss": 0.5654,
      "step": 6830
    },
    {
      "epoch": 0.5485164394546913,
      "grad_norm": 1.2556891441345215,
      "learning_rate": 2.458295363173412e-05,
      "loss": 0.562,
      "step": 6840
    },
    {
      "epoch": 0.5493183640737771,
      "grad_norm": 1.7657569646835327,
      "learning_rate": 2.4574912891986063e-05,
      "loss": 0.5735,
      "step": 6850
    },
    {
      "epoch": 0.5501202886928629,
      "grad_norm": 1.6730507612228394,
      "learning_rate": 2.4566872152238004e-05,
      "loss": 0.6356,
      "step": 6860
    },
    {
      "epoch": 0.5509222133119487,
      "grad_norm": 1.5378355979919434,
      "learning_rate": 2.455883141248995e-05,
      "loss": 0.5755,
      "step": 6870
    },
    {
      "epoch": 0.5517241379310345,
      "grad_norm": 1.6243114471435547,
      "learning_rate": 2.4550790672741893e-05,
      "loss": 0.6189,
      "step": 6880
    },
    {
      "epoch": 0.5525260625501203,
      "grad_norm": 1.8481899499893188,
      "learning_rate": 2.4542749932993837e-05,
      "loss": 0.6109,
      "step": 6890
    },
    {
      "epoch": 0.5533279871692061,
      "grad_norm": 1.6155188083648682,
      "learning_rate": 2.4534709193245778e-05,
      "loss": 0.5843,
      "step": 6900
    },
    {
      "epoch": 0.5541299117882919,
      "grad_norm": 1.425560474395752,
      "learning_rate": 2.4526668453497722e-05,
      "loss": 0.6003,
      "step": 6910
    },
    {
      "epoch": 0.5549318364073778,
      "grad_norm": 1.6668964624404907,
      "learning_rate": 2.4518627713749663e-05,
      "loss": 0.5628,
      "step": 6920
    },
    {
      "epoch": 0.5557337610264635,
      "grad_norm": 1.5478161573410034,
      "learning_rate": 2.451058697400161e-05,
      "loss": 0.6124,
      "step": 6930
    },
    {
      "epoch": 0.5565356856455493,
      "grad_norm": 1.6613256931304932,
      "learning_rate": 2.4502546234253552e-05,
      "loss": 0.5881,
      "step": 6940
    },
    {
      "epoch": 0.5573376102646351,
      "grad_norm": 1.629571557044983,
      "learning_rate": 2.4494505494505496e-05,
      "loss": 0.5933,
      "step": 6950
    },
    {
      "epoch": 0.5581395348837209,
      "grad_norm": 1.511621117591858,
      "learning_rate": 2.4486464754757437e-05,
      "loss": 0.5537,
      "step": 6960
    },
    {
      "epoch": 0.5589414595028067,
      "grad_norm": 1.509568214416504,
      "learning_rate": 2.447842401500938e-05,
      "loss": 0.6004,
      "step": 6970
    },
    {
      "epoch": 0.5597433841218925,
      "grad_norm": 1.4565097093582153,
      "learning_rate": 2.4470383275261326e-05,
      "loss": 0.5903,
      "step": 6980
    },
    {
      "epoch": 0.5605453087409783,
      "grad_norm": 1.689531683921814,
      "learning_rate": 2.4462342535513267e-05,
      "loss": 0.5716,
      "step": 6990
    },
    {
      "epoch": 0.5613472333600642,
      "grad_norm": 1.5970560312271118,
      "learning_rate": 2.445430179576521e-05,
      "loss": 0.5931,
      "step": 7000
    },
    {
      "epoch": 0.56214915797915,
      "grad_norm": 1.5816103219985962,
      "learning_rate": 2.4446261056017152e-05,
      "loss": 0.6189,
      "step": 7010
    },
    {
      "epoch": 0.5629510825982358,
      "grad_norm": 1.7831660509109497,
      "learning_rate": 2.44382203162691e-05,
      "loss": 0.5916,
      "step": 7020
    },
    {
      "epoch": 0.5637530072173216,
      "grad_norm": 1.4618968963623047,
      "learning_rate": 2.443017957652104e-05,
      "loss": 0.6091,
      "step": 7030
    },
    {
      "epoch": 0.5645549318364074,
      "grad_norm": 1.539892554283142,
      "learning_rate": 2.4422138836772985e-05,
      "loss": 0.5787,
      "step": 7040
    },
    {
      "epoch": 0.5653568564554932,
      "grad_norm": 1.5667482614517212,
      "learning_rate": 2.4414098097024926e-05,
      "loss": 0.5707,
      "step": 7050
    },
    {
      "epoch": 0.566158781074579,
      "grad_norm": 1.7562921047210693,
      "learning_rate": 2.440605735727687e-05,
      "loss": 0.558,
      "step": 7060
    },
    {
      "epoch": 0.5669607056936647,
      "grad_norm": 1.6571710109710693,
      "learning_rate": 2.4398016617528814e-05,
      "loss": 0.5382,
      "step": 7070
    },
    {
      "epoch": 0.5677626303127506,
      "grad_norm": 1.6695531606674194,
      "learning_rate": 2.438997587778076e-05,
      "loss": 0.5811,
      "step": 7080
    },
    {
      "epoch": 0.5685645549318364,
      "grad_norm": 1.8185385465621948,
      "learning_rate": 2.43819351380327e-05,
      "loss": 0.5392,
      "step": 7090
    },
    {
      "epoch": 0.5693664795509222,
      "grad_norm": 1.6550590991973877,
      "learning_rate": 2.437389439828464e-05,
      "loss": 0.5424,
      "step": 7100
    },
    {
      "epoch": 0.570168404170008,
      "grad_norm": 1.5540928840637207,
      "learning_rate": 2.4365853658536588e-05,
      "loss": 0.5705,
      "step": 7110
    },
    {
      "epoch": 0.5709703287890938,
      "grad_norm": 1.829437494277954,
      "learning_rate": 2.435781291878853e-05,
      "loss": 0.6027,
      "step": 7120
    },
    {
      "epoch": 0.5717722534081796,
      "grad_norm": 1.4956401586532593,
      "learning_rate": 2.4349772179040473e-05,
      "loss": 0.5823,
      "step": 7130
    },
    {
      "epoch": 0.5725741780272654,
      "grad_norm": 1.6887837648391724,
      "learning_rate": 2.4341731439292414e-05,
      "loss": 0.581,
      "step": 7140
    },
    {
      "epoch": 0.5733761026463512,
      "grad_norm": 1.4347306489944458,
      "learning_rate": 2.433369069954436e-05,
      "loss": 0.5541,
      "step": 7150
    },
    {
      "epoch": 0.5741780272654371,
      "grad_norm": 1.6113371849060059,
      "learning_rate": 2.4325649959796303e-05,
      "loss": 0.5482,
      "step": 7160
    },
    {
      "epoch": 0.5749799518845229,
      "grad_norm": 1.5316004753112793,
      "learning_rate": 2.4317609220048247e-05,
      "loss": 0.5706,
      "step": 7170
    },
    {
      "epoch": 0.5757818765036087,
      "grad_norm": 1.6592941284179688,
      "learning_rate": 2.4309568480300188e-05,
      "loss": 0.5683,
      "step": 7180
    },
    {
      "epoch": 0.5765838011226945,
      "grad_norm": 1.601668357849121,
      "learning_rate": 2.4301527740552132e-05,
      "loss": 0.5685,
      "step": 7190
    },
    {
      "epoch": 0.5773857257417803,
      "grad_norm": 1.5403072834014893,
      "learning_rate": 2.4293487000804073e-05,
      "loss": 0.6923,
      "step": 7200
    },
    {
      "epoch": 0.5781876503608661,
      "grad_norm": 1.4534196853637695,
      "learning_rate": 2.428544626105602e-05,
      "loss": 0.5871,
      "step": 7210
    },
    {
      "epoch": 0.5789895749799518,
      "grad_norm": 1.7172541618347168,
      "learning_rate": 2.427740552130796e-05,
      "loss": 0.5693,
      "step": 7220
    },
    {
      "epoch": 0.5797914995990376,
      "grad_norm": 1.740831732749939,
      "learning_rate": 2.4269364781559902e-05,
      "loss": 0.6066,
      "step": 7230
    },
    {
      "epoch": 0.5805934242181235,
      "grad_norm": 1.735215425491333,
      "learning_rate": 2.4261324041811847e-05,
      "loss": 0.543,
      "step": 7240
    },
    {
      "epoch": 0.5813953488372093,
      "grad_norm": 1.8853553533554077,
      "learning_rate": 2.4253283302063788e-05,
      "loss": 0.5964,
      "step": 7250
    },
    {
      "epoch": 0.5821972734562951,
      "grad_norm": 1.5479544401168823,
      "learning_rate": 2.4245242562315735e-05,
      "loss": 0.547,
      "step": 7260
    },
    {
      "epoch": 0.5829991980753809,
      "grad_norm": 1.4687905311584473,
      "learning_rate": 2.4237201822567676e-05,
      "loss": 0.5651,
      "step": 7270
    },
    {
      "epoch": 0.5838011226944667,
      "grad_norm": 1.6739606857299805,
      "learning_rate": 2.422916108281962e-05,
      "loss": 0.6093,
      "step": 7280
    },
    {
      "epoch": 0.5846030473135525,
      "grad_norm": 1.6988812685012817,
      "learning_rate": 2.422112034307156e-05,
      "loss": 0.6399,
      "step": 7290
    },
    {
      "epoch": 0.5854049719326383,
      "grad_norm": 1.4959510564804077,
      "learning_rate": 2.421307960332351e-05,
      "loss": 0.5539,
      "step": 7300
    },
    {
      "epoch": 0.5862068965517241,
      "grad_norm": 1.630903720855713,
      "learning_rate": 2.420503886357545e-05,
      "loss": 0.6321,
      "step": 7310
    },
    {
      "epoch": 0.58700882117081,
      "grad_norm": 1.806231141090393,
      "learning_rate": 2.4196998123827394e-05,
      "loss": 0.641,
      "step": 7320
    },
    {
      "epoch": 0.5878107457898958,
      "grad_norm": 1.656869649887085,
      "learning_rate": 2.4188957384079335e-05,
      "loss": 0.6391,
      "step": 7330
    },
    {
      "epoch": 0.5886126704089816,
      "grad_norm": 1.9160767793655396,
      "learning_rate": 2.418091664433128e-05,
      "loss": 0.6183,
      "step": 7340
    },
    {
      "epoch": 0.5894145950280674,
      "grad_norm": 1.6141151189804077,
      "learning_rate": 2.4172875904583224e-05,
      "loss": 0.6402,
      "step": 7350
    },
    {
      "epoch": 0.5902165196471532,
      "grad_norm": 1.4302878379821777,
      "learning_rate": 2.4164835164835165e-05,
      "loss": 0.6227,
      "step": 7360
    },
    {
      "epoch": 0.591018444266239,
      "grad_norm": 1.6843984127044678,
      "learning_rate": 2.415679442508711e-05,
      "loss": 0.5935,
      "step": 7370
    },
    {
      "epoch": 0.5918203688853247,
      "grad_norm": 1.4067926406860352,
      "learning_rate": 2.414875368533905e-05,
      "loss": 0.5532,
      "step": 7380
    },
    {
      "epoch": 0.5926222935044105,
      "grad_norm": 1.5568901300430298,
      "learning_rate": 2.4140712945590994e-05,
      "loss": 0.5724,
      "step": 7390
    },
    {
      "epoch": 0.5934242181234964,
      "grad_norm": 1.6609750986099243,
      "learning_rate": 2.413267220584294e-05,
      "loss": 0.5779,
      "step": 7400
    },
    {
      "epoch": 0.5942261427425822,
      "grad_norm": 1.438380241394043,
      "learning_rate": 2.4124631466094883e-05,
      "loss": 0.5201,
      "step": 7410
    },
    {
      "epoch": 0.595028067361668,
      "grad_norm": 1.7606489658355713,
      "learning_rate": 2.4116590726346824e-05,
      "loss": 0.5766,
      "step": 7420
    },
    {
      "epoch": 0.5958299919807538,
      "grad_norm": 1.569687008857727,
      "learning_rate": 2.4108549986598768e-05,
      "loss": 0.5583,
      "step": 7430
    },
    {
      "epoch": 0.5966319165998396,
      "grad_norm": 1.7235527038574219,
      "learning_rate": 2.410050924685071e-05,
      "loss": 0.5182,
      "step": 7440
    },
    {
      "epoch": 0.5974338412189254,
      "grad_norm": 1.8298801183700562,
      "learning_rate": 2.4092468507102657e-05,
      "loss": 0.6193,
      "step": 7450
    },
    {
      "epoch": 0.5982357658380112,
      "grad_norm": 1.6218280792236328,
      "learning_rate": 2.4084427767354597e-05,
      "loss": 0.5848,
      "step": 7460
    },
    {
      "epoch": 0.599037690457097,
      "grad_norm": 1.6946513652801514,
      "learning_rate": 2.4076387027606542e-05,
      "loss": 0.5715,
      "step": 7470
    },
    {
      "epoch": 0.5998396150761829,
      "grad_norm": 1.7370216846466064,
      "learning_rate": 2.4068346287858483e-05,
      "loss": 0.5631,
      "step": 7480
    },
    {
      "epoch": 0.6006415396952687,
      "grad_norm": 1.5277363061904907,
      "learning_rate": 2.4060305548110427e-05,
      "loss": 0.6005,
      "step": 7490
    },
    {
      "epoch": 0.6014434643143545,
      "grad_norm": 1.5387861728668213,
      "learning_rate": 2.405226480836237e-05,
      "loss": 0.6178,
      "step": 7500
    },
    {
      "epoch": 0.6022453889334403,
      "grad_norm": 1.4979884624481201,
      "learning_rate": 2.4044224068614312e-05,
      "loss": 0.6181,
      "step": 7510
    },
    {
      "epoch": 0.603047313552526,
      "grad_norm": 1.5697860717773438,
      "learning_rate": 2.4036183328866256e-05,
      "loss": 0.5986,
      "step": 7520
    },
    {
      "epoch": 0.6038492381716118,
      "grad_norm": 1.7392624616622925,
      "learning_rate": 2.4028142589118197e-05,
      "loss": 0.5821,
      "step": 7530
    },
    {
      "epoch": 0.6046511627906976,
      "grad_norm": 1.6895341873168945,
      "learning_rate": 2.4020101849370145e-05,
      "loss": 0.5632,
      "step": 7540
    },
    {
      "epoch": 0.6054530874097834,
      "grad_norm": 1.747063159942627,
      "learning_rate": 2.4012061109622086e-05,
      "loss": 0.6049,
      "step": 7550
    },
    {
      "epoch": 0.6062550120288693,
      "grad_norm": 1.72086501121521,
      "learning_rate": 2.400402036987403e-05,
      "loss": 0.5447,
      "step": 7560
    },
    {
      "epoch": 0.6070569366479551,
      "grad_norm": 1.4557068347930908,
      "learning_rate": 2.399597963012597e-05,
      "loss": 0.5381,
      "step": 7570
    },
    {
      "epoch": 0.6078588612670409,
      "grad_norm": 1.45943284034729,
      "learning_rate": 2.3987938890377915e-05,
      "loss": 0.5575,
      "step": 7580
    },
    {
      "epoch": 0.6086607858861267,
      "grad_norm": 1.7779626846313477,
      "learning_rate": 2.397989815062986e-05,
      "loss": 0.5649,
      "step": 7590
    },
    {
      "epoch": 0.6094627105052125,
      "grad_norm": 1.6001617908477783,
      "learning_rate": 2.3971857410881804e-05,
      "loss": 0.5861,
      "step": 7600
    },
    {
      "epoch": 0.6102646351242983,
      "grad_norm": 1.7669196128845215,
      "learning_rate": 2.3963816671133745e-05,
      "loss": 0.6236,
      "step": 7610
    },
    {
      "epoch": 0.6110665597433841,
      "grad_norm": 1.547188639640808,
      "learning_rate": 2.3955775931385686e-05,
      "loss": 0.5891,
      "step": 7620
    },
    {
      "epoch": 0.6118684843624699,
      "grad_norm": 1.6764822006225586,
      "learning_rate": 2.394773519163763e-05,
      "loss": 0.5725,
      "step": 7630
    },
    {
      "epoch": 0.6126704089815558,
      "grad_norm": 1.4938480854034424,
      "learning_rate": 2.3939694451889574e-05,
      "loss": 0.5497,
      "step": 7640
    },
    {
      "epoch": 0.6134723336006416,
      "grad_norm": 1.598289132118225,
      "learning_rate": 2.393165371214152e-05,
      "loss": 0.5686,
      "step": 7650
    },
    {
      "epoch": 0.6142742582197274,
      "grad_norm": 1.8353956937789917,
      "learning_rate": 2.392361297239346e-05,
      "loss": 0.5563,
      "step": 7660
    },
    {
      "epoch": 0.6150761828388132,
      "grad_norm": 1.4065226316452026,
      "learning_rate": 2.3915572232645404e-05,
      "loss": 0.6021,
      "step": 7670
    },
    {
      "epoch": 0.615878107457899,
      "grad_norm": 1.7522531747817993,
      "learning_rate": 2.3907531492897348e-05,
      "loss": 0.5972,
      "step": 7680
    },
    {
      "epoch": 0.6166800320769847,
      "grad_norm": 1.8410680294036865,
      "learning_rate": 2.3899490753149292e-05,
      "loss": 0.6137,
      "step": 7690
    },
    {
      "epoch": 0.6174819566960705,
      "grad_norm": 1.6148130893707275,
      "learning_rate": 2.3891450013401233e-05,
      "loss": 0.6136,
      "step": 7700
    },
    {
      "epoch": 0.6182838813151563,
      "grad_norm": 1.6787984371185303,
      "learning_rate": 2.3883409273653178e-05,
      "loss": 0.5932,
      "step": 7710
    },
    {
      "epoch": 0.6190858059342422,
      "grad_norm": 1.6347193717956543,
      "learning_rate": 2.387536853390512e-05,
      "loss": 0.5783,
      "step": 7720
    },
    {
      "epoch": 0.619887730553328,
      "grad_norm": 1.5094349384307861,
      "learning_rate": 2.3867327794157063e-05,
      "loss": 0.6294,
      "step": 7730
    },
    {
      "epoch": 0.6206896551724138,
      "grad_norm": 1.4915632009506226,
      "learning_rate": 2.3859287054409007e-05,
      "loss": 0.6094,
      "step": 7740
    },
    {
      "epoch": 0.6214915797914996,
      "grad_norm": 1.6095603704452515,
      "learning_rate": 2.3851246314660948e-05,
      "loss": 0.5721,
      "step": 7750
    },
    {
      "epoch": 0.6222935044105854,
      "grad_norm": 1.7750977277755737,
      "learning_rate": 2.3843205574912892e-05,
      "loss": 0.5978,
      "step": 7760
    },
    {
      "epoch": 0.6230954290296712,
      "grad_norm": 1.528498888015747,
      "learning_rate": 2.3835164835164833e-05,
      "loss": 0.5778,
      "step": 7770
    },
    {
      "epoch": 0.623897353648757,
      "grad_norm": 1.6615756750106812,
      "learning_rate": 2.382712409541678e-05,
      "loss": 0.6397,
      "step": 7780
    },
    {
      "epoch": 0.6246992782678428,
      "grad_norm": 1.2393701076507568,
      "learning_rate": 2.3819083355668722e-05,
      "loss": 0.5946,
      "step": 7790
    },
    {
      "epoch": 0.6255012028869287,
      "grad_norm": 1.660413146018982,
      "learning_rate": 2.3811042615920666e-05,
      "loss": 0.5366,
      "step": 7800
    },
    {
      "epoch": 0.6263031275060145,
      "grad_norm": 1.817903995513916,
      "learning_rate": 2.3803001876172607e-05,
      "loss": 0.5744,
      "step": 7810
    },
    {
      "epoch": 0.6271050521251003,
      "grad_norm": 1.6336512565612793,
      "learning_rate": 2.379496113642455e-05,
      "loss": 0.5714,
      "step": 7820
    },
    {
      "epoch": 0.627906976744186,
      "grad_norm": 1.3182331323623657,
      "learning_rate": 2.3786920396676495e-05,
      "loss": 0.5438,
      "step": 7830
    },
    {
      "epoch": 0.6287089013632718,
      "grad_norm": 1.5635831356048584,
      "learning_rate": 2.377887965692844e-05,
      "loss": 0.591,
      "step": 7840
    },
    {
      "epoch": 0.6295108259823576,
      "grad_norm": 1.4950916767120361,
      "learning_rate": 2.377083891718038e-05,
      "loss": 0.6182,
      "step": 7850
    },
    {
      "epoch": 0.6303127506014434,
      "grad_norm": 1.7873244285583496,
      "learning_rate": 2.3762798177432325e-05,
      "loss": 0.6461,
      "step": 7860
    },
    {
      "epoch": 0.6311146752205292,
      "grad_norm": 1.624200463294983,
      "learning_rate": 2.375475743768427e-05,
      "loss": 0.6211,
      "step": 7870
    },
    {
      "epoch": 0.6319165998396151,
      "grad_norm": 1.8888260126113892,
      "learning_rate": 2.374671669793621e-05,
      "loss": 0.6519,
      "step": 7880
    },
    {
      "epoch": 0.6327185244587009,
      "grad_norm": 1.495703935623169,
      "learning_rate": 2.3738675958188154e-05,
      "loss": 0.5247,
      "step": 7890
    },
    {
      "epoch": 0.6335204490777867,
      "grad_norm": 1.7495300769805908,
      "learning_rate": 2.3730635218440095e-05,
      "loss": 0.5909,
      "step": 7900
    },
    {
      "epoch": 0.6343223736968725,
      "grad_norm": 1.8771077394485474,
      "learning_rate": 2.372259447869204e-05,
      "loss": 0.6191,
      "step": 7910
    },
    {
      "epoch": 0.6351242983159583,
      "grad_norm": 1.654828429222107,
      "learning_rate": 2.3714553738943984e-05,
      "loss": 0.6029,
      "step": 7920
    },
    {
      "epoch": 0.6359262229350441,
      "grad_norm": 1.4856276512145996,
      "learning_rate": 2.3706512999195928e-05,
      "loss": 0.5427,
      "step": 7930
    },
    {
      "epoch": 0.6367281475541299,
      "grad_norm": 1.5561635494232178,
      "learning_rate": 2.369847225944787e-05,
      "loss": 0.5831,
      "step": 7940
    },
    {
      "epoch": 0.6375300721732157,
      "grad_norm": 1.6292824745178223,
      "learning_rate": 2.3690431519699813e-05,
      "loss": 0.5566,
      "step": 7950
    },
    {
      "epoch": 0.6383319967923016,
      "grad_norm": 1.8077163696289062,
      "learning_rate": 2.3682390779951754e-05,
      "loss": 0.6234,
      "step": 7960
    },
    {
      "epoch": 0.6391339214113874,
      "grad_norm": 1.5122292041778564,
      "learning_rate": 2.3674350040203702e-05,
      "loss": 0.6079,
      "step": 7970
    },
    {
      "epoch": 0.6399358460304732,
      "grad_norm": 1.9892973899841309,
      "learning_rate": 2.3666309300455643e-05,
      "loss": 0.6522,
      "step": 7980
    },
    {
      "epoch": 0.640737770649559,
      "grad_norm": 1.4222954511642456,
      "learning_rate": 2.3658268560707584e-05,
      "loss": 0.6196,
      "step": 7990
    },
    {
      "epoch": 0.6415396952686447,
      "grad_norm": 1.8994702100753784,
      "learning_rate": 2.3650227820959528e-05,
      "loss": 0.578,
      "step": 8000
    },
    {
      "epoch": 0.6423416198877305,
      "grad_norm": 1.7038307189941406,
      "learning_rate": 2.364218708121147e-05,
      "loss": 0.5897,
      "step": 8010
    },
    {
      "epoch": 0.6431435445068163,
      "grad_norm": 1.81538724899292,
      "learning_rate": 2.3634146341463417e-05,
      "loss": 0.6504,
      "step": 8020
    },
    {
      "epoch": 0.6439454691259021,
      "grad_norm": 1.5104303359985352,
      "learning_rate": 2.3626105601715358e-05,
      "loss": 0.5568,
      "step": 8030
    },
    {
      "epoch": 0.644747393744988,
      "grad_norm": 1.462418794631958,
      "learning_rate": 2.3618064861967302e-05,
      "loss": 0.6227,
      "step": 8040
    },
    {
      "epoch": 0.6455493183640738,
      "grad_norm": 1.610253095626831,
      "learning_rate": 2.3610024122219243e-05,
      "loss": 0.6325,
      "step": 8050
    },
    {
      "epoch": 0.6463512429831596,
      "grad_norm": 1.3784351348876953,
      "learning_rate": 2.360198338247119e-05,
      "loss": 0.5622,
      "step": 8060
    },
    {
      "epoch": 0.6471531676022454,
      "grad_norm": 1.758494257926941,
      "learning_rate": 2.359394264272313e-05,
      "loss": 0.6064,
      "step": 8070
    },
    {
      "epoch": 0.6479550922213312,
      "grad_norm": 1.59873366355896,
      "learning_rate": 2.3585901902975076e-05,
      "loss": 0.5586,
      "step": 8080
    },
    {
      "epoch": 0.648757016840417,
      "grad_norm": 1.49772310256958,
      "learning_rate": 2.3577861163227017e-05,
      "loss": 0.5935,
      "step": 8090
    },
    {
      "epoch": 0.6495589414595028,
      "grad_norm": 1.6801691055297852,
      "learning_rate": 2.356982042347896e-05,
      "loss": 0.6572,
      "step": 8100
    },
    {
      "epoch": 0.6503608660785886,
      "grad_norm": 1.5951309204101562,
      "learning_rate": 2.3561779683730905e-05,
      "loss": 0.55,
      "step": 8110
    },
    {
      "epoch": 0.6511627906976745,
      "grad_norm": 1.4831982851028442,
      "learning_rate": 2.3553738943982846e-05,
      "loss": 0.5909,
      "step": 8120
    },
    {
      "epoch": 0.6519647153167603,
      "grad_norm": 1.5904638767242432,
      "learning_rate": 2.354569820423479e-05,
      "loss": 0.5931,
      "step": 8130
    },
    {
      "epoch": 0.652766639935846,
      "grad_norm": 1.8552595376968384,
      "learning_rate": 2.353765746448673e-05,
      "loss": 0.6318,
      "step": 8140
    },
    {
      "epoch": 0.6535685645549318,
      "grad_norm": 2.1371097564697266,
      "learning_rate": 2.3529616724738676e-05,
      "loss": 0.6152,
      "step": 8150
    },
    {
      "epoch": 0.6543704891740176,
      "grad_norm": 1.6169102191925049,
      "learning_rate": 2.352157598499062e-05,
      "loss": 0.5705,
      "step": 8160
    },
    {
      "epoch": 0.6551724137931034,
      "grad_norm": 1.6510117053985596,
      "learning_rate": 2.3513535245242564e-05,
      "loss": 0.5742,
      "step": 8170
    },
    {
      "epoch": 0.6559743384121892,
      "grad_norm": 1.7638146877288818,
      "learning_rate": 2.3505494505494505e-05,
      "loss": 0.6308,
      "step": 8180
    },
    {
      "epoch": 0.656776263031275,
      "grad_norm": 1.6854923963546753,
      "learning_rate": 2.349745376574645e-05,
      "loss": 0.6042,
      "step": 8190
    },
    {
      "epoch": 0.6575781876503609,
      "grad_norm": 1.6108280420303345,
      "learning_rate": 2.348941302599839e-05,
      "loss": 0.6343,
      "step": 8200
    },
    {
      "epoch": 0.6583801122694467,
      "grad_norm": 1.4207892417907715,
      "learning_rate": 2.3481372286250338e-05,
      "loss": 0.5897,
      "step": 8210
    },
    {
      "epoch": 0.6591820368885325,
      "grad_norm": 1.5586247444152832,
      "learning_rate": 2.347333154650228e-05,
      "loss": 0.5874,
      "step": 8220
    },
    {
      "epoch": 0.6599839615076183,
      "grad_norm": 1.477112889289856,
      "learning_rate": 2.3465290806754223e-05,
      "loss": 0.5684,
      "step": 8230
    },
    {
      "epoch": 0.6607858861267041,
      "grad_norm": 1.6632030010223389,
      "learning_rate": 2.3457250067006164e-05,
      "loss": 0.6268,
      "step": 8240
    },
    {
      "epoch": 0.6615878107457899,
      "grad_norm": 1.9070980548858643,
      "learning_rate": 2.3449209327258108e-05,
      "loss": 0.6181,
      "step": 8250
    },
    {
      "epoch": 0.6623897353648757,
      "grad_norm": 1.483840823173523,
      "learning_rate": 2.3441168587510053e-05,
      "loss": 0.5599,
      "step": 8260
    },
    {
      "epoch": 0.6631916599839615,
      "grad_norm": 1.925408124923706,
      "learning_rate": 2.3433127847761993e-05,
      "loss": 0.5613,
      "step": 8270
    },
    {
      "epoch": 0.6639935846030474,
      "grad_norm": 1.540523648262024,
      "learning_rate": 2.3425087108013938e-05,
      "loss": 0.5989,
      "step": 8280
    },
    {
      "epoch": 0.6647955092221332,
      "grad_norm": 1.7614295482635498,
      "learning_rate": 2.341704636826588e-05,
      "loss": 0.5357,
      "step": 8290
    },
    {
      "epoch": 0.6655974338412189,
      "grad_norm": 1.9494178295135498,
      "learning_rate": 2.3409005628517826e-05,
      "loss": 0.5824,
      "step": 8300
    },
    {
      "epoch": 0.6663993584603047,
      "grad_norm": 1.5826079845428467,
      "learning_rate": 2.3400964888769767e-05,
      "loss": 0.5964,
      "step": 8310
    },
    {
      "epoch": 0.6672012830793905,
      "grad_norm": 1.8017278909683228,
      "learning_rate": 2.339292414902171e-05,
      "loss": 0.6146,
      "step": 8320
    },
    {
      "epoch": 0.6680032076984763,
      "grad_norm": 1.6638469696044922,
      "learning_rate": 2.3384883409273652e-05,
      "loss": 0.5964,
      "step": 8330
    },
    {
      "epoch": 0.6688051323175621,
      "grad_norm": 1.4436501264572144,
      "learning_rate": 2.3376842669525597e-05,
      "loss": 0.5727,
      "step": 8340
    },
    {
      "epoch": 0.6696070569366479,
      "grad_norm": 1.7576429843902588,
      "learning_rate": 2.336880192977754e-05,
      "loss": 0.6189,
      "step": 8350
    },
    {
      "epoch": 0.6704089815557338,
      "grad_norm": 1.8038073778152466,
      "learning_rate": 2.3360761190029485e-05,
      "loss": 0.6402,
      "step": 8360
    },
    {
      "epoch": 0.6712109061748196,
      "grad_norm": 1.4983599185943604,
      "learning_rate": 2.3352720450281426e-05,
      "loss": 0.6275,
      "step": 8370
    },
    {
      "epoch": 0.6720128307939054,
      "grad_norm": 1.6264852285385132,
      "learning_rate": 2.3344679710533367e-05,
      "loss": 0.5324,
      "step": 8380
    },
    {
      "epoch": 0.6728147554129912,
      "grad_norm": 1.6368886232376099,
      "learning_rate": 2.3336638970785315e-05,
      "loss": 0.6674,
      "step": 8390
    },
    {
      "epoch": 0.673616680032077,
      "grad_norm": 1.5572160482406616,
      "learning_rate": 2.3328598231037256e-05,
      "loss": 0.5819,
      "step": 8400
    },
    {
      "epoch": 0.6744186046511628,
      "grad_norm": 1.681965708732605,
      "learning_rate": 2.33205574912892e-05,
      "loss": 0.6234,
      "step": 8410
    },
    {
      "epoch": 0.6752205292702486,
      "grad_norm": 1.5076639652252197,
      "learning_rate": 2.331251675154114e-05,
      "loss": 0.5852,
      "step": 8420
    },
    {
      "epoch": 0.6760224538893344,
      "grad_norm": 2.018300771713257,
      "learning_rate": 2.3304476011793085e-05,
      "loss": 0.5937,
      "step": 8430
    },
    {
      "epoch": 0.6768243785084203,
      "grad_norm": 1.580674171447754,
      "learning_rate": 2.329643527204503e-05,
      "loss": 0.5718,
      "step": 8440
    },
    {
      "epoch": 0.677626303127506,
      "grad_norm": 1.749997854232788,
      "learning_rate": 2.3288394532296974e-05,
      "loss": 0.5302,
      "step": 8450
    },
    {
      "epoch": 0.6784282277465918,
      "grad_norm": 1.6217292547225952,
      "learning_rate": 2.3280353792548915e-05,
      "loss": 0.536,
      "step": 8460
    },
    {
      "epoch": 0.6792301523656776,
      "grad_norm": 1.7315281629562378,
      "learning_rate": 2.327231305280086e-05,
      "loss": 0.5899,
      "step": 8470
    },
    {
      "epoch": 0.6800320769847634,
      "grad_norm": 1.4970808029174805,
      "learning_rate": 2.32642723130528e-05,
      "loss": 0.5634,
      "step": 8480
    },
    {
      "epoch": 0.6808340016038492,
      "grad_norm": 1.634251356124878,
      "learning_rate": 2.3256231573304747e-05,
      "loss": 0.6018,
      "step": 8490
    },
    {
      "epoch": 0.681635926222935,
      "grad_norm": 1.7279819250106812,
      "learning_rate": 2.324819083355669e-05,
      "loss": 0.6147,
      "step": 8500
    },
    {
      "epoch": 0.6824378508420208,
      "grad_norm": 1.5642236471176147,
      "learning_rate": 2.324015009380863e-05,
      "loss": 0.6025,
      "step": 8510
    },
    {
      "epoch": 0.6832397754611067,
      "grad_norm": 1.8124805688858032,
      "learning_rate": 2.3232109354060574e-05,
      "loss": 0.5114,
      "step": 8520
    },
    {
      "epoch": 0.6840417000801925,
      "grad_norm": 1.689439058303833,
      "learning_rate": 2.3224068614312514e-05,
      "loss": 0.5618,
      "step": 8530
    },
    {
      "epoch": 0.6848436246992783,
      "grad_norm": 1.4498722553253174,
      "learning_rate": 2.3216027874564462e-05,
      "loss": 0.5775,
      "step": 8540
    },
    {
      "epoch": 0.6856455493183641,
      "grad_norm": 1.7595380544662476,
      "learning_rate": 2.3207987134816403e-05,
      "loss": 0.5756,
      "step": 8550
    },
    {
      "epoch": 0.6864474739374499,
      "grad_norm": 1.7973980903625488,
      "learning_rate": 2.3199946395068347e-05,
      "loss": 0.6018,
      "step": 8560
    },
    {
      "epoch": 0.6872493985565357,
      "grad_norm": 1.6684300899505615,
      "learning_rate": 2.3191905655320288e-05,
      "loss": 0.5398,
      "step": 8570
    },
    {
      "epoch": 0.6880513231756215,
      "grad_norm": 1.8009934425354004,
      "learning_rate": 2.3183864915572236e-05,
      "loss": 0.5733,
      "step": 8580
    },
    {
      "epoch": 0.6888532477947072,
      "grad_norm": 1.6983617544174194,
      "learning_rate": 2.3175824175824177e-05,
      "loss": 0.5905,
      "step": 8590
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 1.6697648763656616,
      "learning_rate": 2.316778343607612e-05,
      "loss": 0.6315,
      "step": 8600
    },
    {
      "epoch": 0.6904570970328789,
      "grad_norm": 1.5497453212738037,
      "learning_rate": 2.3159742696328062e-05,
      "loss": 0.5731,
      "step": 8610
    },
    {
      "epoch": 0.6912590216519647,
      "grad_norm": 1.513870120048523,
      "learning_rate": 2.3151701956580006e-05,
      "loss": 0.5768,
      "step": 8620
    },
    {
      "epoch": 0.6920609462710505,
      "grad_norm": 1.6179286241531372,
      "learning_rate": 2.314366121683195e-05,
      "loss": 0.5717,
      "step": 8630
    },
    {
      "epoch": 0.6928628708901363,
      "grad_norm": 1.5099700689315796,
      "learning_rate": 2.313562047708389e-05,
      "loss": 0.5933,
      "step": 8640
    },
    {
      "epoch": 0.6936647955092221,
      "grad_norm": 1.4328539371490479,
      "learning_rate": 2.3127579737335836e-05,
      "loss": 0.5678,
      "step": 8650
    },
    {
      "epoch": 0.6944667201283079,
      "grad_norm": 1.4416509866714478,
      "learning_rate": 2.3119538997587777e-05,
      "loss": 0.5411,
      "step": 8660
    },
    {
      "epoch": 0.6952686447473937,
      "grad_norm": 1.5015214681625366,
      "learning_rate": 2.311149825783972e-05,
      "loss": 0.6162,
      "step": 8670
    },
    {
      "epoch": 0.6960705693664796,
      "grad_norm": 1.591720461845398,
      "learning_rate": 2.3103457518091665e-05,
      "loss": 0.615,
      "step": 8680
    },
    {
      "epoch": 0.6968724939855654,
      "grad_norm": 1.679921269416809,
      "learning_rate": 2.309541677834361e-05,
      "loss": 0.6052,
      "step": 8690
    },
    {
      "epoch": 0.6976744186046512,
      "grad_norm": 1.9294029474258423,
      "learning_rate": 2.308737603859555e-05,
      "loss": 0.5292,
      "step": 8700
    },
    {
      "epoch": 0.698476343223737,
      "grad_norm": 1.397284746170044,
      "learning_rate": 2.3079335298847495e-05,
      "loss": 0.5396,
      "step": 8710
    },
    {
      "epoch": 0.6992782678428228,
      "grad_norm": 1.7360183000564575,
      "learning_rate": 2.3071294559099436e-05,
      "loss": 0.5567,
      "step": 8720
    },
    {
      "epoch": 0.7000801924619086,
      "grad_norm": 1.8495815992355347,
      "learning_rate": 2.3063253819351383e-05,
      "loss": 0.6107,
      "step": 8730
    },
    {
      "epoch": 0.7008821170809943,
      "grad_norm": 1.7394248247146606,
      "learning_rate": 2.3055213079603324e-05,
      "loss": 0.5842,
      "step": 8740
    },
    {
      "epoch": 0.7016840417000801,
      "grad_norm": 1.587179183959961,
      "learning_rate": 2.304717233985527e-05,
      "loss": 0.5403,
      "step": 8750
    },
    {
      "epoch": 0.702485966319166,
      "grad_norm": 1.6947472095489502,
      "learning_rate": 2.303913160010721e-05,
      "loss": 0.5804,
      "step": 8760
    },
    {
      "epoch": 0.7032878909382518,
      "grad_norm": 1.3340495824813843,
      "learning_rate": 2.3031090860359154e-05,
      "loss": 0.5424,
      "step": 8770
    },
    {
      "epoch": 0.7040898155573376,
      "grad_norm": 1.5815067291259766,
      "learning_rate": 2.3023050120611098e-05,
      "loss": 0.6217,
      "step": 8780
    },
    {
      "epoch": 0.7048917401764234,
      "grad_norm": 1.8927503824234009,
      "learning_rate": 2.301500938086304e-05,
      "loss": 0.5682,
      "step": 8790
    },
    {
      "epoch": 0.7056936647955092,
      "grad_norm": 1.6719814538955688,
      "learning_rate": 2.3006968641114983e-05,
      "loss": 0.5886,
      "step": 8800
    },
    {
      "epoch": 0.706495589414595,
      "grad_norm": 1.5622590780258179,
      "learning_rate": 2.2998927901366924e-05,
      "loss": 0.5209,
      "step": 8810
    },
    {
      "epoch": 0.7072975140336808,
      "grad_norm": 1.7815539836883545,
      "learning_rate": 2.2990887161618872e-05,
      "loss": 0.6055,
      "step": 8820
    },
    {
      "epoch": 0.7080994386527666,
      "grad_norm": 1.6649280786514282,
      "learning_rate": 2.2982846421870813e-05,
      "loss": 0.5868,
      "step": 8830
    },
    {
      "epoch": 0.7089013632718525,
      "grad_norm": 1.5077416896820068,
      "learning_rate": 2.2974805682122757e-05,
      "loss": 0.5561,
      "step": 8840
    },
    {
      "epoch": 0.7097032878909383,
      "grad_norm": 1.4762486219406128,
      "learning_rate": 2.2966764942374698e-05,
      "loss": 0.5863,
      "step": 8850
    },
    {
      "epoch": 0.7105052125100241,
      "grad_norm": 1.6665005683898926,
      "learning_rate": 2.2958724202626642e-05,
      "loss": 0.5672,
      "step": 8860
    },
    {
      "epoch": 0.7113071371291099,
      "grad_norm": 1.7533848285675049,
      "learning_rate": 2.2950683462878586e-05,
      "loss": 0.5779,
      "step": 8870
    },
    {
      "epoch": 0.7121090617481957,
      "grad_norm": 1.4613702297210693,
      "learning_rate": 2.294264272313053e-05,
      "loss": 0.553,
      "step": 8880
    },
    {
      "epoch": 0.7129109863672815,
      "grad_norm": 1.6439425945281982,
      "learning_rate": 2.293460198338247e-05,
      "loss": 0.5342,
      "step": 8890
    },
    {
      "epoch": 0.7137129109863672,
      "grad_norm": 1.5944626331329346,
      "learning_rate": 2.2926561243634413e-05,
      "loss": 0.5851,
      "step": 8900
    },
    {
      "epoch": 0.714514835605453,
      "grad_norm": 1.4430674314498901,
      "learning_rate": 2.2918520503886357e-05,
      "loss": 0.5689,
      "step": 8910
    },
    {
      "epoch": 0.7153167602245389,
      "grad_norm": 1.9141099452972412,
      "learning_rate": 2.29104797641383e-05,
      "loss": 0.5918,
      "step": 8920
    },
    {
      "epoch": 0.7161186848436247,
      "grad_norm": 1.7946791648864746,
      "learning_rate": 2.2902439024390245e-05,
      "loss": 0.6226,
      "step": 8930
    },
    {
      "epoch": 0.7169206094627105,
      "grad_norm": 1.5975624322891235,
      "learning_rate": 2.2894398284642186e-05,
      "loss": 0.5315,
      "step": 8940
    },
    {
      "epoch": 0.7177225340817963,
      "grad_norm": 1.631833791732788,
      "learning_rate": 2.288635754489413e-05,
      "loss": 0.614,
      "step": 8950
    },
    {
      "epoch": 0.7185244587008821,
      "grad_norm": 1.7723777294158936,
      "learning_rate": 2.2878316805146075e-05,
      "loss": 0.5598,
      "step": 8960
    },
    {
      "epoch": 0.7193263833199679,
      "grad_norm": 1.4822235107421875,
      "learning_rate": 2.287027606539802e-05,
      "loss": 0.5828,
      "step": 8970
    },
    {
      "epoch": 0.7201283079390537,
      "grad_norm": 1.4826083183288574,
      "learning_rate": 2.286223532564996e-05,
      "loss": 0.5654,
      "step": 8980
    },
    {
      "epoch": 0.7209302325581395,
      "grad_norm": 1.638702392578125,
      "learning_rate": 2.2854194585901904e-05,
      "loss": 0.591,
      "step": 8990
    },
    {
      "epoch": 0.7217321571772254,
      "grad_norm": 1.7191139459609985,
      "learning_rate": 2.2846153846153845e-05,
      "loss": 0.6545,
      "step": 9000
    },
    {
      "epoch": 0.7225340817963112,
      "grad_norm": 1.5814530849456787,
      "learning_rate": 2.2838113106405793e-05,
      "loss": 0.6043,
      "step": 9010
    },
    {
      "epoch": 0.723336006415397,
      "grad_norm": 1.7455003261566162,
      "learning_rate": 2.2830072366657734e-05,
      "loss": 0.5795,
      "step": 9020
    },
    {
      "epoch": 0.7241379310344828,
      "grad_norm": 1.7330663204193115,
      "learning_rate": 2.2822031626909675e-05,
      "loss": 0.5889,
      "step": 9030
    },
    {
      "epoch": 0.7249398556535686,
      "grad_norm": 1.5239289999008179,
      "learning_rate": 2.281399088716162e-05,
      "loss": 0.6187,
      "step": 9040
    },
    {
      "epoch": 0.7257417802726543,
      "grad_norm": 1.4847100973129272,
      "learning_rate": 2.280595014741356e-05,
      "loss": 0.5583,
      "step": 9050
    },
    {
      "epoch": 0.7265437048917401,
      "grad_norm": 1.571380853652954,
      "learning_rate": 2.2797909407665508e-05,
      "loss": 0.5984,
      "step": 9060
    },
    {
      "epoch": 0.7273456295108259,
      "grad_norm": 1.7780697345733643,
      "learning_rate": 2.278986866791745e-05,
      "loss": 0.5774,
      "step": 9070
    },
    {
      "epoch": 0.7281475541299118,
      "grad_norm": 1.560900092124939,
      "learning_rate": 2.2781827928169393e-05,
      "loss": 0.5896,
      "step": 9080
    },
    {
      "epoch": 0.7289494787489976,
      "grad_norm": 1.5483332872390747,
      "learning_rate": 2.2773787188421334e-05,
      "loss": 0.6035,
      "step": 9090
    },
    {
      "epoch": 0.7297514033680834,
      "grad_norm": 1.5696552991867065,
      "learning_rate": 2.2765746448673278e-05,
      "loss": 0.5732,
      "step": 9100
    },
    {
      "epoch": 0.7305533279871692,
      "grad_norm": 1.9460887908935547,
      "learning_rate": 2.2757705708925222e-05,
      "loss": 0.6016,
      "step": 9110
    },
    {
      "epoch": 0.731355252606255,
      "grad_norm": 1.59134042263031,
      "learning_rate": 2.2749664969177167e-05,
      "loss": 0.5633,
      "step": 9120
    },
    {
      "epoch": 0.7321571772253408,
      "grad_norm": 1.6849122047424316,
      "learning_rate": 2.2741624229429108e-05,
      "loss": 0.6236,
      "step": 9130
    },
    {
      "epoch": 0.7329591018444266,
      "grad_norm": 1.5167909860610962,
      "learning_rate": 2.2733583489681052e-05,
      "loss": 0.5553,
      "step": 9140
    },
    {
      "epoch": 0.7337610264635124,
      "grad_norm": 1.4962201118469238,
      "learning_rate": 2.2725542749932996e-05,
      "loss": 0.5669,
      "step": 9150
    },
    {
      "epoch": 0.7345629510825983,
      "grad_norm": 1.3439098596572876,
      "learning_rate": 2.2717502010184937e-05,
      "loss": 0.5831,
      "step": 9160
    },
    {
      "epoch": 0.7353648757016841,
      "grad_norm": 1.5265403985977173,
      "learning_rate": 2.270946127043688e-05,
      "loss": 0.5971,
      "step": 9170
    },
    {
      "epoch": 0.7361668003207699,
      "grad_norm": 1.8044402599334717,
      "learning_rate": 2.2701420530688822e-05,
      "loss": 0.6078,
      "step": 9180
    },
    {
      "epoch": 0.7369687249398557,
      "grad_norm": 1.3143095970153809,
      "learning_rate": 2.2693379790940766e-05,
      "loss": 0.5198,
      "step": 9190
    },
    {
      "epoch": 0.7377706495589414,
      "grad_norm": 1.6438498497009277,
      "learning_rate": 2.268533905119271e-05,
      "loss": 0.5679,
      "step": 9200
    },
    {
      "epoch": 0.7385725741780272,
      "grad_norm": 1.6759141683578491,
      "learning_rate": 2.2677298311444655e-05,
      "loss": 0.5931,
      "step": 9210
    },
    {
      "epoch": 0.739374498797113,
      "grad_norm": 1.5548748970031738,
      "learning_rate": 2.2669257571696596e-05,
      "loss": 0.6301,
      "step": 9220
    },
    {
      "epoch": 0.7401764234161988,
      "grad_norm": 1.5902143716812134,
      "learning_rate": 2.266121683194854e-05,
      "loss": 0.6034,
      "step": 9230
    },
    {
      "epoch": 0.7409783480352847,
      "grad_norm": 1.7686604261398315,
      "learning_rate": 2.265317609220048e-05,
      "loss": 0.5686,
      "step": 9240
    },
    {
      "epoch": 0.7417802726543705,
      "grad_norm": 1.4428315162658691,
      "learning_rate": 2.264513535245243e-05,
      "loss": 0.5635,
      "step": 9250
    },
    {
      "epoch": 0.7425821972734563,
      "grad_norm": 1.7067736387252808,
      "learning_rate": 2.263709461270437e-05,
      "loss": 0.6092,
      "step": 9260
    },
    {
      "epoch": 0.7433841218925421,
      "grad_norm": 1.5307034254074097,
      "learning_rate": 2.2629053872956314e-05,
      "loss": 0.5633,
      "step": 9270
    },
    {
      "epoch": 0.7441860465116279,
      "grad_norm": 1.5907496213912964,
      "learning_rate": 2.2621013133208255e-05,
      "loss": 0.5821,
      "step": 9280
    },
    {
      "epoch": 0.7449879711307137,
      "grad_norm": 1.8363399505615234,
      "learning_rate": 2.2612972393460196e-05,
      "loss": 0.5837,
      "step": 9290
    },
    {
      "epoch": 0.7457898957497995,
      "grad_norm": 1.4995049238204956,
      "learning_rate": 2.2604931653712144e-05,
      "loss": 0.6273,
      "step": 9300
    },
    {
      "epoch": 0.7465918203688853,
      "grad_norm": 1.5136548280715942,
      "learning_rate": 2.2596890913964084e-05,
      "loss": 0.5479,
      "step": 9310
    },
    {
      "epoch": 0.7473937449879712,
      "grad_norm": 1.5057156085968018,
      "learning_rate": 2.258885017421603e-05,
      "loss": 0.535,
      "step": 9320
    },
    {
      "epoch": 0.748195669607057,
      "grad_norm": 1.5908595323562622,
      "learning_rate": 2.258080943446797e-05,
      "loss": 0.5022,
      "step": 9330
    },
    {
      "epoch": 0.7489975942261428,
      "grad_norm": 2.0057342052459717,
      "learning_rate": 2.2572768694719917e-05,
      "loss": 0.6093,
      "step": 9340
    },
    {
      "epoch": 0.7497995188452286,
      "grad_norm": 1.5962700843811035,
      "learning_rate": 2.2564727954971858e-05,
      "loss": 0.5799,
      "step": 9350
    },
    {
      "epoch": 0.7506014434643143,
      "grad_norm": 1.6912819147109985,
      "learning_rate": 2.2556687215223802e-05,
      "loss": 0.5584,
      "step": 9360
    },
    {
      "epoch": 0.7514033680834001,
      "grad_norm": 1.6801114082336426,
      "learning_rate": 2.2548646475475743e-05,
      "loss": 0.5497,
      "step": 9370
    },
    {
      "epoch": 0.7522052927024859,
      "grad_norm": 1.7891954183578491,
      "learning_rate": 2.2540605735727688e-05,
      "loss": 0.5956,
      "step": 9380
    },
    {
      "epoch": 0.7530072173215717,
      "grad_norm": 1.7272465229034424,
      "learning_rate": 2.2532564995979632e-05,
      "loss": 0.608,
      "step": 9390
    },
    {
      "epoch": 0.7538091419406576,
      "grad_norm": 1.662414789199829,
      "learning_rate": 2.2524524256231573e-05,
      "loss": 0.5132,
      "step": 9400
    },
    {
      "epoch": 0.7546110665597434,
      "grad_norm": 1.5940663814544678,
      "learning_rate": 2.2516483516483517e-05,
      "loss": 0.5471,
      "step": 9410
    },
    {
      "epoch": 0.7554129911788292,
      "grad_norm": 1.4485340118408203,
      "learning_rate": 2.2508442776735458e-05,
      "loss": 0.5597,
      "step": 9420
    },
    {
      "epoch": 0.756214915797915,
      "grad_norm": 1.5271152257919312,
      "learning_rate": 2.2500402036987402e-05,
      "loss": 0.5966,
      "step": 9430
    },
    {
      "epoch": 0.7570168404170008,
      "grad_norm": 1.6311407089233398,
      "learning_rate": 2.2492361297239347e-05,
      "loss": 0.5985,
      "step": 9440
    },
    {
      "epoch": 0.7578187650360866,
      "grad_norm": 2.0604519844055176,
      "learning_rate": 2.248432055749129e-05,
      "loss": 0.6451,
      "step": 9450
    },
    {
      "epoch": 0.7586206896551724,
      "grad_norm": 1.5350898504257202,
      "learning_rate": 2.2476279817743232e-05,
      "loss": 0.561,
      "step": 9460
    },
    {
      "epoch": 0.7594226142742582,
      "grad_norm": 1.8427929878234863,
      "learning_rate": 2.2468239077995176e-05,
      "loss": 0.6394,
      "step": 9470
    },
    {
      "epoch": 0.7602245388933441,
      "grad_norm": 1.6479333639144897,
      "learning_rate": 2.2460198338247117e-05,
      "loss": 0.5502,
      "step": 9480
    },
    {
      "epoch": 0.7610264635124299,
      "grad_norm": 1.6068663597106934,
      "learning_rate": 2.2452157598499065e-05,
      "loss": 0.617,
      "step": 9490
    },
    {
      "epoch": 0.7618283881315157,
      "grad_norm": 1.538030743598938,
      "learning_rate": 2.2444116858751006e-05,
      "loss": 0.5561,
      "step": 9500
    },
    {
      "epoch": 0.7626303127506014,
      "grad_norm": 1.622267723083496,
      "learning_rate": 2.243607611900295e-05,
      "loss": 0.5447,
      "step": 9510
    },
    {
      "epoch": 0.7634322373696872,
      "grad_norm": 1.5327507257461548,
      "learning_rate": 2.242803537925489e-05,
      "loss": 0.6222,
      "step": 9520
    },
    {
      "epoch": 0.764234161988773,
      "grad_norm": 1.5265458822250366,
      "learning_rate": 2.2419994639506835e-05,
      "loss": 0.5274,
      "step": 9530
    },
    {
      "epoch": 0.7650360866078588,
      "grad_norm": 1.8337352275848389,
      "learning_rate": 2.241195389975878e-05,
      "loss": 0.61,
      "step": 9540
    },
    {
      "epoch": 0.7658380112269446,
      "grad_norm": 1.6806460618972778,
      "learning_rate": 2.240391316001072e-05,
      "loss": 0.6083,
      "step": 9550
    },
    {
      "epoch": 0.7666399358460305,
      "grad_norm": 1.677017092704773,
      "learning_rate": 2.2395872420262665e-05,
      "loss": 0.6176,
      "step": 9560
    },
    {
      "epoch": 0.7674418604651163,
      "grad_norm": 1.9163844585418701,
      "learning_rate": 2.2387831680514605e-05,
      "loss": 0.5818,
      "step": 9570
    },
    {
      "epoch": 0.7682437850842021,
      "grad_norm": 1.6694620847702026,
      "learning_rate": 2.2379790940766553e-05,
      "loss": 0.6549,
      "step": 9580
    },
    {
      "epoch": 0.7690457097032879,
      "grad_norm": 1.586452841758728,
      "learning_rate": 2.2371750201018494e-05,
      "loss": 0.5736,
      "step": 9590
    },
    {
      "epoch": 0.7698476343223737,
      "grad_norm": 1.6320935487747192,
      "learning_rate": 2.236370946127044e-05,
      "loss": 0.615,
      "step": 9600
    },
    {
      "epoch": 0.7706495589414595,
      "grad_norm": 1.899877667427063,
      "learning_rate": 2.235566872152238e-05,
      "loss": 0.5864,
      "step": 9610
    },
    {
      "epoch": 0.7714514835605453,
      "grad_norm": 1.7156692743301392,
      "learning_rate": 2.2347627981774324e-05,
      "loss": 0.5979,
      "step": 9620
    },
    {
      "epoch": 0.7722534081796311,
      "grad_norm": 1.4481122493743896,
      "learning_rate": 2.2339587242026268e-05,
      "loss": 0.5184,
      "step": 9630
    },
    {
      "epoch": 0.773055332798717,
      "grad_norm": 1.6095149517059326,
      "learning_rate": 2.2331546502278212e-05,
      "loss": 0.6029,
      "step": 9640
    },
    {
      "epoch": 0.7738572574178028,
      "grad_norm": 1.5062981843948364,
      "learning_rate": 2.2323505762530153e-05,
      "loss": 0.5739,
      "step": 9650
    },
    {
      "epoch": 0.7746591820368885,
      "grad_norm": 1.4179269075393677,
      "learning_rate": 2.2315465022782094e-05,
      "loss": 0.5512,
      "step": 9660
    },
    {
      "epoch": 0.7754611066559743,
      "grad_norm": 1.5529613494873047,
      "learning_rate": 2.230742428303404e-05,
      "loss": 0.5361,
      "step": 9670
    },
    {
      "epoch": 0.7762630312750601,
      "grad_norm": 1.6163122653961182,
      "learning_rate": 2.2299383543285982e-05,
      "loss": 0.583,
      "step": 9680
    },
    {
      "epoch": 0.7770649558941459,
      "grad_norm": 1.5407609939575195,
      "learning_rate": 2.2291342803537927e-05,
      "loss": 0.5521,
      "step": 9690
    },
    {
      "epoch": 0.7778668805132317,
      "grad_norm": 1.8868002891540527,
      "learning_rate": 2.2283302063789868e-05,
      "loss": 0.5677,
      "step": 9700
    },
    {
      "epoch": 0.7786688051323175,
      "grad_norm": 1.800750494003296,
      "learning_rate": 2.2275261324041812e-05,
      "loss": 0.5574,
      "step": 9710
    },
    {
      "epoch": 0.7794707297514034,
      "grad_norm": 1.6903023719787598,
      "learning_rate": 2.2267220584293756e-05,
      "loss": 0.5707,
      "step": 9720
    },
    {
      "epoch": 0.7802726543704892,
      "grad_norm": 1.7046862840652466,
      "learning_rate": 2.22591798445457e-05,
      "loss": 0.5681,
      "step": 9730
    },
    {
      "epoch": 0.781074578989575,
      "grad_norm": 1.6054778099060059,
      "learning_rate": 2.225113910479764e-05,
      "loss": 0.5598,
      "step": 9740
    },
    {
      "epoch": 0.7818765036086608,
      "grad_norm": 1.927173376083374,
      "learning_rate": 2.2243098365049586e-05,
      "loss": 0.6013,
      "step": 9750
    },
    {
      "epoch": 0.7826784282277466,
      "grad_norm": 1.841939091682434,
      "learning_rate": 2.2235057625301527e-05,
      "loss": 0.6304,
      "step": 9760
    },
    {
      "epoch": 0.7834803528468324,
      "grad_norm": 1.538893461227417,
      "learning_rate": 2.2227016885553474e-05,
      "loss": 0.6047,
      "step": 9770
    },
    {
      "epoch": 0.7842822774659182,
      "grad_norm": 1.582900881767273,
      "learning_rate": 2.2218976145805415e-05,
      "loss": 0.612,
      "step": 9780
    },
    {
      "epoch": 0.785084202085004,
      "grad_norm": 1.7892141342163086,
      "learning_rate": 2.2210935406057356e-05,
      "loss": 0.5848,
      "step": 9790
    },
    {
      "epoch": 0.7858861267040899,
      "grad_norm": 1.7069777250289917,
      "learning_rate": 2.22028946663093e-05,
      "loss": 0.5177,
      "step": 9800
    },
    {
      "epoch": 0.7866880513231757,
      "grad_norm": 1.8275967836380005,
      "learning_rate": 2.219485392656124e-05,
      "loss": 0.5692,
      "step": 9810
    },
    {
      "epoch": 0.7874899759422614,
      "grad_norm": 1.8353606462478638,
      "learning_rate": 2.218681318681319e-05,
      "loss": 0.5257,
      "step": 9820
    },
    {
      "epoch": 0.7882919005613472,
      "grad_norm": 1.5249360799789429,
      "learning_rate": 2.217877244706513e-05,
      "loss": 0.5457,
      "step": 9830
    },
    {
      "epoch": 0.789093825180433,
      "grad_norm": 1.8728373050689697,
      "learning_rate": 2.2170731707317074e-05,
      "loss": 0.5776,
      "step": 9840
    },
    {
      "epoch": 0.7898957497995188,
      "grad_norm": 1.6388009786605835,
      "learning_rate": 2.2162690967569015e-05,
      "loss": 0.5497,
      "step": 9850
    },
    {
      "epoch": 0.7906976744186046,
      "grad_norm": 1.7375632524490356,
      "learning_rate": 2.2154650227820963e-05,
      "loss": 0.5409,
      "step": 9860
    },
    {
      "epoch": 0.7914995990376904,
      "grad_norm": 1.9122040271759033,
      "learning_rate": 2.2146609488072904e-05,
      "loss": 0.586,
      "step": 9870
    },
    {
      "epoch": 0.7923015236567763,
      "grad_norm": 1.6740747690200806,
      "learning_rate": 2.2138568748324848e-05,
      "loss": 0.5512,
      "step": 9880
    },
    {
      "epoch": 0.7931034482758621,
      "grad_norm": NaN,
      "learning_rate": 2.213052800857679e-05,
      "loss": 0.5609,
      "step": 9890
    },
    {
      "epoch": 0.7939053728949479,
      "grad_norm": 1.6012513637542725,
      "learning_rate": 2.212329134280354e-05,
      "loss": 0.5342,
      "step": 9900
    },
    {
      "epoch": 0.7947072975140337,
      "grad_norm": 1.5881989002227783,
      "learning_rate": 2.211525060305548e-05,
      "loss": 0.5679,
      "step": 9910
    },
    {
      "epoch": 0.7955092221331195,
      "grad_norm": 1.5779694318771362,
      "learning_rate": 2.2107209863307427e-05,
      "loss": 0.6212,
      "step": 9920
    },
    {
      "epoch": 0.7963111467522053,
      "grad_norm": 1.5237661600112915,
      "learning_rate": 2.2099169123559368e-05,
      "loss": 0.6096,
      "step": 9930
    },
    {
      "epoch": 0.7971130713712911,
      "grad_norm": 1.5161820650100708,
      "learning_rate": 2.2091128383811312e-05,
      "loss": 0.522,
      "step": 9940
    },
    {
      "epoch": 0.7979149959903769,
      "grad_norm": 1.6703015565872192,
      "learning_rate": 2.2083087644063253e-05,
      "loss": 0.5429,
      "step": 9950
    },
    {
      "epoch": 0.7987169206094628,
      "grad_norm": 1.5020796060562134,
      "learning_rate": 2.2075046904315197e-05,
      "loss": 0.5509,
      "step": 9960
    },
    {
      "epoch": 0.7995188452285485,
      "grad_norm": 1.9547605514526367,
      "learning_rate": 2.206700616456714e-05,
      "loss": 0.5527,
      "step": 9970
    },
    {
      "epoch": 0.8003207698476343,
      "grad_norm": 1.498894453048706,
      "learning_rate": 2.2058965424819083e-05,
      "loss": 0.6049,
      "step": 9980
    },
    {
      "epoch": 0.8011226944667201,
      "grad_norm": 1.6561157703399658,
      "learning_rate": 2.2050924685071027e-05,
      "loss": 0.5355,
      "step": 9990
    },
    {
      "epoch": 0.8019246190858059,
      "grad_norm": 1.569487452507019,
      "learning_rate": 2.2042883945322968e-05,
      "loss": 0.5552,
      "step": 10000
    },
    {
      "epoch": 0.8027265437048917,
      "grad_norm": 1.6758723258972168,
      "learning_rate": 2.2034843205574915e-05,
      "loss": 0.5427,
      "step": 10010
    },
    {
      "epoch": 0.8035284683239775,
      "grad_norm": 1.5306546688079834,
      "learning_rate": 2.2027606539801665e-05,
      "loss": 0.6439,
      "step": 10020
    },
    {
      "epoch": 0.8043303929430633,
      "grad_norm": 1.637864351272583,
      "learning_rate": 2.2019565800053606e-05,
      "loss": 0.5685,
      "step": 10030
    },
    {
      "epoch": 0.8051323175621492,
      "grad_norm": 1.662790060043335,
      "learning_rate": 2.201152506030555e-05,
      "loss": 0.5357,
      "step": 10040
    },
    {
      "epoch": 0.805934242181235,
      "grad_norm": 1.6908296346664429,
      "learning_rate": 2.200348432055749e-05,
      "loss": 0.581,
      "step": 10050
    },
    {
      "epoch": 0.8067361668003208,
      "grad_norm": 1.4960943460464478,
      "learning_rate": 2.1995443580809432e-05,
      "loss": 0.5517,
      "step": 10060
    },
    {
      "epoch": 0.8075380914194066,
      "grad_norm": 1.511070728302002,
      "learning_rate": 2.198740284106138e-05,
      "loss": 0.5314,
      "step": 10070
    },
    {
      "epoch": 0.8083400160384924,
      "grad_norm": 1.5831348896026611,
      "learning_rate": 2.197936210131332e-05,
      "loss": 0.5478,
      "step": 10080
    },
    {
      "epoch": 0.8091419406575782,
      "grad_norm": 1.6939153671264648,
      "learning_rate": 2.1971321361565265e-05,
      "loss": 0.6142,
      "step": 10090
    },
    {
      "epoch": 0.809943865276664,
      "grad_norm": 1.9231683015823364,
      "learning_rate": 2.1963280621817206e-05,
      "loss": 0.5865,
      "step": 10100
    },
    {
      "epoch": 0.8107457898957497,
      "grad_norm": 1.6637104749679565,
      "learning_rate": 2.1955239882069153e-05,
      "loss": 0.5601,
      "step": 10110
    },
    {
      "epoch": 0.8115477145148356,
      "grad_norm": 2.0791172981262207,
      "learning_rate": 2.1947199142321094e-05,
      "loss": 0.5626,
      "step": 10120
    },
    {
      "epoch": 0.8123496391339214,
      "grad_norm": 1.4173592329025269,
      "learning_rate": 2.193915840257304e-05,
      "loss": 0.5959,
      "step": 10130
    },
    {
      "epoch": 0.8131515637530072,
      "grad_norm": 1.5223455429077148,
      "learning_rate": 2.193111766282498e-05,
      "loss": 0.5783,
      "step": 10140
    },
    {
      "epoch": 0.813953488372093,
      "grad_norm": 1.6232166290283203,
      "learning_rate": 2.1923076923076924e-05,
      "loss": 0.5746,
      "step": 10150
    },
    {
      "epoch": 0.8147554129911788,
      "grad_norm": 2.061596632003784,
      "learning_rate": 2.1915036183328868e-05,
      "loss": 0.636,
      "step": 10160
    },
    {
      "epoch": 0.8155573376102646,
      "grad_norm": 1.526838779449463,
      "learning_rate": 2.190699544358081e-05,
      "loss": 0.5621,
      "step": 10170
    },
    {
      "epoch": 0.8163592622293504,
      "grad_norm": 1.4711413383483887,
      "learning_rate": 2.1898954703832753e-05,
      "loss": 0.5239,
      "step": 10180
    },
    {
      "epoch": 0.8171611868484362,
      "grad_norm": 1.8006064891815186,
      "learning_rate": 2.1890913964084694e-05,
      "loss": 0.5685,
      "step": 10190
    },
    {
      "epoch": 0.8179631114675221,
      "grad_norm": 1.7364957332611084,
      "learning_rate": 2.188287322433664e-05,
      "loss": 0.5737,
      "step": 10200
    },
    {
      "epoch": 0.8187650360866079,
      "grad_norm": 1.6134302616119385,
      "learning_rate": 2.1874832484588583e-05,
      "loss": 0.622,
      "step": 10210
    },
    {
      "epoch": 0.8195669607056937,
      "grad_norm": 1.439756989479065,
      "learning_rate": 2.1866791744840527e-05,
      "loss": 0.5571,
      "step": 10220
    },
    {
      "epoch": 0.8203688853247795,
      "grad_norm": 1.572576642036438,
      "learning_rate": 2.1858751005092468e-05,
      "loss": 0.5745,
      "step": 10230
    },
    {
      "epoch": 0.8211708099438653,
      "grad_norm": 1.5003619194030762,
      "learning_rate": 2.1850710265344412e-05,
      "loss": 0.5894,
      "step": 10240
    },
    {
      "epoch": 0.8219727345629511,
      "grad_norm": 1.7185814380645752,
      "learning_rate": 2.1842669525596353e-05,
      "loss": 0.5709,
      "step": 10250
    },
    {
      "epoch": 0.8227746591820368,
      "grad_norm": 1.6465632915496826,
      "learning_rate": 2.18346287858483e-05,
      "loss": 0.5989,
      "step": 10260
    },
    {
      "epoch": 0.8235765838011226,
      "grad_norm": 1.5450845956802368,
      "learning_rate": 2.182658804610024e-05,
      "loss": 0.6083,
      "step": 10270
    },
    {
      "epoch": 0.8243785084202085,
      "grad_norm": 1.4345113039016724,
      "learning_rate": 2.1818547306352186e-05,
      "loss": 0.6088,
      "step": 10280
    },
    {
      "epoch": 0.8251804330392943,
      "grad_norm": 1.4351502656936646,
      "learning_rate": 2.1810506566604127e-05,
      "loss": 0.5564,
      "step": 10290
    },
    {
      "epoch": 0.8259823576583801,
      "grad_norm": 1.761027455329895,
      "learning_rate": 2.180246582685607e-05,
      "loss": 0.5399,
      "step": 10300
    },
    {
      "epoch": 0.8267842822774659,
      "grad_norm": 1.632964015007019,
      "learning_rate": 2.1794425087108015e-05,
      "loss": 0.6041,
      "step": 10310
    },
    {
      "epoch": 0.8275862068965517,
      "grad_norm": 1.5971462726593018,
      "learning_rate": 2.1786384347359956e-05,
      "loss": 0.5792,
      "step": 10320
    },
    {
      "epoch": 0.8283881315156375,
      "grad_norm": 1.6603622436523438,
      "learning_rate": 2.17783436076119e-05,
      "loss": 0.5721,
      "step": 10330
    },
    {
      "epoch": 0.8291900561347233,
      "grad_norm": 1.905048131942749,
      "learning_rate": 2.177030286786384e-05,
      "loss": 0.6042,
      "step": 10340
    },
    {
      "epoch": 0.8299919807538091,
      "grad_norm": 1.6066958904266357,
      "learning_rate": 2.176226212811579e-05,
      "loss": 0.6082,
      "step": 10350
    },
    {
      "epoch": 0.830793905372895,
      "grad_norm": 1.6150342226028442,
      "learning_rate": 2.175422138836773e-05,
      "loss": 0.5751,
      "step": 10360
    },
    {
      "epoch": 0.8315958299919808,
      "grad_norm": 1.6915602684020996,
      "learning_rate": 2.1746180648619674e-05,
      "loss": 0.6238,
      "step": 10370
    },
    {
      "epoch": 0.8323977546110666,
      "grad_norm": 1.6053738594055176,
      "learning_rate": 2.1738139908871615e-05,
      "loss": 0.5505,
      "step": 10380
    },
    {
      "epoch": 0.8331996792301524,
      "grad_norm": 1.932733178138733,
      "learning_rate": 2.173009916912356e-05,
      "loss": 0.5876,
      "step": 10390
    },
    {
      "epoch": 0.8340016038492382,
      "grad_norm": 1.8261055946350098,
      "learning_rate": 2.1722058429375504e-05,
      "loss": 0.5883,
      "step": 10400
    },
    {
      "epoch": 0.834803528468324,
      "grad_norm": 1.7396681308746338,
      "learning_rate": 2.1714017689627448e-05,
      "loss": 0.5974,
      "step": 10410
    },
    {
      "epoch": 0.8356054530874097,
      "grad_norm": 1.4680545330047607,
      "learning_rate": 2.170597694987939e-05,
      "loss": 0.5965,
      "step": 10420
    },
    {
      "epoch": 0.8364073777064955,
      "grad_norm": 1.9632290601730347,
      "learning_rate": 2.169793621013133e-05,
      "loss": 0.5932,
      "step": 10430
    },
    {
      "epoch": 0.8372093023255814,
      "grad_norm": 1.6013977527618408,
      "learning_rate": 2.1689895470383278e-05,
      "loss": 0.5828,
      "step": 10440
    },
    {
      "epoch": 0.8380112269446672,
      "grad_norm": 1.659497618675232,
      "learning_rate": 2.168185473063522e-05,
      "loss": 0.563,
      "step": 10450
    },
    {
      "epoch": 0.838813151563753,
      "grad_norm": 1.6522177457809448,
      "learning_rate": 2.1673813990887163e-05,
      "loss": 0.6609,
      "step": 10460
    },
    {
      "epoch": 0.8396150761828388,
      "grad_norm": 1.6108859777450562,
      "learning_rate": 2.1665773251139104e-05,
      "loss": 0.5512,
      "step": 10470
    },
    {
      "epoch": 0.8404170008019246,
      "grad_norm": 1.5681889057159424,
      "learning_rate": 2.1657732511391048e-05,
      "loss": 0.6142,
      "step": 10480
    },
    {
      "epoch": 0.8412189254210104,
      "grad_norm": 1.9363737106323242,
      "learning_rate": 2.1649691771642992e-05,
      "loss": 0.5552,
      "step": 10490
    },
    {
      "epoch": 0.8420208500400962,
      "grad_norm": 1.8177653551101685,
      "learning_rate": 2.1641651031894937e-05,
      "loss": 0.6086,
      "step": 10500
    },
    {
      "epoch": 0.842822774659182,
      "grad_norm": 1.7615293264389038,
      "learning_rate": 2.1633610292146878e-05,
      "loss": 0.6003,
      "step": 10510
    },
    {
      "epoch": 0.8436246992782679,
      "grad_norm": 1.554934024810791,
      "learning_rate": 2.1625569552398822e-05,
      "loss": 0.5222,
      "step": 10520
    },
    {
      "epoch": 0.8444266238973537,
      "grad_norm": 1.6028286218643188,
      "learning_rate": 2.1617528812650763e-05,
      "loss": 0.5688,
      "step": 10530
    },
    {
      "epoch": 0.8452285485164395,
      "grad_norm": 1.6058546304702759,
      "learning_rate": 2.160948807290271e-05,
      "loss": 0.5492,
      "step": 10540
    },
    {
      "epoch": 0.8460304731355253,
      "grad_norm": 1.8939112424850464,
      "learning_rate": 2.160144733315465e-05,
      "loss": 0.5627,
      "step": 10550
    },
    {
      "epoch": 0.846832397754611,
      "grad_norm": 1.6289610862731934,
      "learning_rate": 2.1593406593406592e-05,
      "loss": 0.5734,
      "step": 10560
    },
    {
      "epoch": 0.8476343223736968,
      "grad_norm": 1.676521897315979,
      "learning_rate": 2.1585365853658537e-05,
      "loss": 0.5905,
      "step": 10570
    },
    {
      "epoch": 0.8484362469927826,
      "grad_norm": 1.7447882890701294,
      "learning_rate": 2.1577325113910477e-05,
      "loss": 0.5623,
      "step": 10580
    },
    {
      "epoch": 0.8492381716118684,
      "grad_norm": 1.7502707242965698,
      "learning_rate": 2.1569284374162425e-05,
      "loss": 0.6507,
      "step": 10590
    },
    {
      "epoch": 0.8500400962309543,
      "grad_norm": 1.5675926208496094,
      "learning_rate": 2.1561243634414366e-05,
      "loss": 0.5564,
      "step": 10600
    },
    {
      "epoch": 0.8508420208500401,
      "grad_norm": 1.7676204442977905,
      "learning_rate": 2.155320289466631e-05,
      "loss": 0.5516,
      "step": 10610
    },
    {
      "epoch": 0.8516439454691259,
      "grad_norm": 1.493338942527771,
      "learning_rate": 2.154516215491825e-05,
      "loss": 0.521,
      "step": 10620
    },
    {
      "epoch": 0.8524458700882117,
      "grad_norm": 1.7710654735565186,
      "learning_rate": 2.15371214151702e-05,
      "loss": 0.6078,
      "step": 10630
    },
    {
      "epoch": 0.8532477947072975,
      "grad_norm": 1.5522319078445435,
      "learning_rate": 2.152908067542214e-05,
      "loss": 0.5326,
      "step": 10640
    },
    {
      "epoch": 0.8540497193263833,
      "grad_norm": 1.7702540159225464,
      "learning_rate": 2.1521039935674084e-05,
      "loss": 0.5202,
      "step": 10650
    },
    {
      "epoch": 0.8548516439454691,
      "grad_norm": 1.6621201038360596,
      "learning_rate": 2.1512999195926025e-05,
      "loss": 0.4955,
      "step": 10660
    },
    {
      "epoch": 0.8556535685645549,
      "grad_norm": 1.4644310474395752,
      "learning_rate": 2.150495845617797e-05,
      "loss": 0.5431,
      "step": 10670
    },
    {
      "epoch": 0.8564554931836408,
      "grad_norm": 1.7977254390716553,
      "learning_rate": 2.1496917716429914e-05,
      "loss": 0.5967,
      "step": 10680
    },
    {
      "epoch": 0.8572574178027266,
      "grad_norm": 1.602729320526123,
      "learning_rate": 2.1488876976681854e-05,
      "loss": 0.6134,
      "step": 10690
    },
    {
      "epoch": 0.8580593424218124,
      "grad_norm": 1.9555714130401611,
      "learning_rate": 2.14808362369338e-05,
      "loss": 0.5558,
      "step": 10700
    },
    {
      "epoch": 0.8588612670408982,
      "grad_norm": 1.505265235900879,
      "learning_rate": 2.147279549718574e-05,
      "loss": 0.5944,
      "step": 10710
    },
    {
      "epoch": 0.859663191659984,
      "grad_norm": 1.647678256034851,
      "learning_rate": 2.1464754757437684e-05,
      "loss": 0.6334,
      "step": 10720
    },
    {
      "epoch": 0.8604651162790697,
      "grad_norm": 1.8316798210144043,
      "learning_rate": 2.1456714017689628e-05,
      "loss": 0.6064,
      "step": 10730
    },
    {
      "epoch": 0.8612670408981555,
      "grad_norm": 1.633215069770813,
      "learning_rate": 2.1448673277941573e-05,
      "loss": 0.5971,
      "step": 10740
    },
    {
      "epoch": 0.8620689655172413,
      "grad_norm": 1.82414972782135,
      "learning_rate": 2.1440632538193513e-05,
      "loss": 0.5363,
      "step": 10750
    },
    {
      "epoch": 0.8628708901363272,
      "grad_norm": 1.6132696866989136,
      "learning_rate": 2.1432591798445458e-05,
      "loss": 0.611,
      "step": 10760
    },
    {
      "epoch": 0.863672814755413,
      "grad_norm": 1.6641656160354614,
      "learning_rate": 2.14245510586974e-05,
      "loss": 0.5794,
      "step": 10770
    },
    {
      "epoch": 0.8644747393744988,
      "grad_norm": 1.5602059364318848,
      "learning_rate": 2.1416510318949346e-05,
      "loss": 0.6005,
      "step": 10780
    },
    {
      "epoch": 0.8652766639935846,
      "grad_norm": 1.7839934825897217,
      "learning_rate": 2.1408469579201287e-05,
      "loss": 0.5702,
      "step": 10790
    },
    {
      "epoch": 0.8660785886126704,
      "grad_norm": 1.6075702905654907,
      "learning_rate": 2.140042883945323e-05,
      "loss": 0.558,
      "step": 10800
    },
    {
      "epoch": 0.8668805132317562,
      "grad_norm": 1.621827483177185,
      "learning_rate": 2.1392388099705172e-05,
      "loss": 0.6081,
      "step": 10810
    },
    {
      "epoch": 0.867682437850842,
      "grad_norm": 1.6456291675567627,
      "learning_rate": 2.1384347359957117e-05,
      "loss": 0.5264,
      "step": 10820
    },
    {
      "epoch": 0.8684843624699278,
      "grad_norm": 1.5750162601470947,
      "learning_rate": 2.137630662020906e-05,
      "loss": 0.5174,
      "step": 10830
    },
    {
      "epoch": 0.8692862870890137,
      "grad_norm": 1.5100488662719727,
      "learning_rate": 2.1368265880461002e-05,
      "loss": 0.6257,
      "step": 10840
    },
    {
      "epoch": 0.8700882117080995,
      "grad_norm": 1.885860800743103,
      "learning_rate": 2.1360225140712946e-05,
      "loss": 0.6444,
      "step": 10850
    },
    {
      "epoch": 0.8708901363271853,
      "grad_norm": 1.4859981536865234,
      "learning_rate": 2.1352184400964887e-05,
      "loss": 0.5182,
      "step": 10860
    },
    {
      "epoch": 0.871692060946271,
      "grad_norm": 1.8743034601211548,
      "learning_rate": 2.1344143661216835e-05,
      "loss": 0.5757,
      "step": 10870
    },
    {
      "epoch": 0.8724939855653568,
      "grad_norm": 1.8302677869796753,
      "learning_rate": 2.1336102921468776e-05,
      "loss": 0.5959,
      "step": 10880
    },
    {
      "epoch": 0.8732959101844426,
      "grad_norm": 1.7750176191329956,
      "learning_rate": 2.132806218172072e-05,
      "loss": 0.6033,
      "step": 10890
    },
    {
      "epoch": 0.8740978348035284,
      "grad_norm": 1.4155203104019165,
      "learning_rate": 2.132002144197266e-05,
      "loss": 0.5873,
      "step": 10900
    },
    {
      "epoch": 0.8748997594226142,
      "grad_norm": 1.5639017820358276,
      "learning_rate": 2.1311980702224605e-05,
      "loss": 0.5863,
      "step": 10910
    },
    {
      "epoch": 0.8757016840417001,
      "grad_norm": 1.537326693534851,
      "learning_rate": 2.130393996247655e-05,
      "loss": 0.5647,
      "step": 10920
    },
    {
      "epoch": 0.8765036086607859,
      "grad_norm": 1.7715959548950195,
      "learning_rate": 2.1295899222728494e-05,
      "loss": 0.6018,
      "step": 10930
    },
    {
      "epoch": 0.8773055332798717,
      "grad_norm": 1.720569133758545,
      "learning_rate": 2.1287858482980435e-05,
      "loss": 0.5825,
      "step": 10940
    },
    {
      "epoch": 0.8781074578989575,
      "grad_norm": 1.6727606058120728,
      "learning_rate": 2.1279817743232375e-05,
      "loss": 0.5599,
      "step": 10950
    },
    {
      "epoch": 0.8789093825180433,
      "grad_norm": 1.7116326093673706,
      "learning_rate": 2.127177700348432e-05,
      "loss": 0.5615,
      "step": 10960
    },
    {
      "epoch": 0.8797113071371291,
      "grad_norm": 1.6265828609466553,
      "learning_rate": 2.1263736263736264e-05,
      "loss": 0.5536,
      "step": 10970
    },
    {
      "epoch": 0.8805132317562149,
      "grad_norm": 1.7171920537948608,
      "learning_rate": 2.125569552398821e-05,
      "loss": 0.596,
      "step": 10980
    },
    {
      "epoch": 0.8813151563753007,
      "grad_norm": 1.7954566478729248,
      "learning_rate": 2.124765478424015e-05,
      "loss": 0.6526,
      "step": 10990
    },
    {
      "epoch": 0.8821170809943866,
      "grad_norm": 1.8815613985061646,
      "learning_rate": 2.1239614044492094e-05,
      "loss": 0.5418,
      "step": 11000
    },
    {
      "epoch": 0.8829190056134724,
      "grad_norm": 1.485711693763733,
      "learning_rate": 2.1231573304744038e-05,
      "loss": 0.5687,
      "step": 11010
    },
    {
      "epoch": 0.8837209302325582,
      "grad_norm": 1.8561010360717773,
      "learning_rate": 2.1223532564995982e-05,
      "loss": 0.5466,
      "step": 11020
    },
    {
      "epoch": 0.884522854851644,
      "grad_norm": 1.9752002954483032,
      "learning_rate": 2.1215491825247923e-05,
      "loss": 0.5764,
      "step": 11030
    },
    {
      "epoch": 0.8853247794707297,
      "grad_norm": 1.743098258972168,
      "learning_rate": 2.1207451085499867e-05,
      "loss": 0.5835,
      "step": 11040
    },
    {
      "epoch": 0.8861267040898155,
      "grad_norm": 1.8228881359100342,
      "learning_rate": 2.1199410345751808e-05,
      "loss": 0.6064,
      "step": 11050
    },
    {
      "epoch": 0.8869286287089013,
      "grad_norm": 1.6606245040893555,
      "learning_rate": 2.1191369606003756e-05,
      "loss": 0.6103,
      "step": 11060
    },
    {
      "epoch": 0.8877305533279871,
      "grad_norm": 1.7913799285888672,
      "learning_rate": 2.1183328866255697e-05,
      "loss": 0.5623,
      "step": 11070
    },
    {
      "epoch": 0.888532477947073,
      "grad_norm": 1.5530221462249756,
      "learning_rate": 2.1175288126507638e-05,
      "loss": 0.6253,
      "step": 11080
    },
    {
      "epoch": 0.8893344025661588,
      "grad_norm": 1.6740949153900146,
      "learning_rate": 2.1167247386759582e-05,
      "loss": 0.616,
      "step": 11090
    },
    {
      "epoch": 0.8901363271852446,
      "grad_norm": 1.6880770921707153,
      "learning_rate": 2.1159206647011523e-05,
      "loss": 0.5591,
      "step": 11100
    },
    {
      "epoch": 0.8909382518043304,
      "grad_norm": 1.7116458415985107,
      "learning_rate": 2.115116590726347e-05,
      "loss": 0.5382,
      "step": 11110
    },
    {
      "epoch": 0.8917401764234162,
      "grad_norm": 1.5012966394424438,
      "learning_rate": 2.114312516751541e-05,
      "loss": 0.6044,
      "step": 11120
    },
    {
      "epoch": 0.892542101042502,
      "grad_norm": 1.4772427082061768,
      "learning_rate": 2.1135084427767356e-05,
      "loss": 0.5394,
      "step": 11130
    },
    {
      "epoch": 0.8933440256615878,
      "grad_norm": 1.6950750350952148,
      "learning_rate": 2.1127043688019297e-05,
      "loss": 0.5805,
      "step": 11140
    },
    {
      "epoch": 0.8941459502806736,
      "grad_norm": 1.6106454133987427,
      "learning_rate": 2.111900294827124e-05,
      "loss": 0.5539,
      "step": 11150
    },
    {
      "epoch": 0.8949478748997595,
      "grad_norm": 1.8724716901779175,
      "learning_rate": 2.1110962208523185e-05,
      "loss": 0.5964,
      "step": 11160
    },
    {
      "epoch": 0.8957497995188453,
      "grad_norm": 1.7227493524551392,
      "learning_rate": 2.110292146877513e-05,
      "loss": 0.5528,
      "step": 11170
    },
    {
      "epoch": 0.896551724137931,
      "grad_norm": 1.5880157947540283,
      "learning_rate": 2.109488072902707e-05,
      "loss": 0.5314,
      "step": 11180
    },
    {
      "epoch": 0.8973536487570168,
      "grad_norm": 1.6958611011505127,
      "learning_rate": 2.1086839989279015e-05,
      "loss": 0.5515,
      "step": 11190
    },
    {
      "epoch": 0.8981555733761026,
      "grad_norm": 1.7312238216400146,
      "learning_rate": 2.107879924953096e-05,
      "loss": 0.5967,
      "step": 11200
    },
    {
      "epoch": 0.8989574979951884,
      "grad_norm": 1.8660367727279663,
      "learning_rate": 2.10707585097829e-05,
      "loss": 0.5615,
      "step": 11210
    },
    {
      "epoch": 0.8997594226142742,
      "grad_norm": 1.8247671127319336,
      "learning_rate": 2.1062717770034844e-05,
      "loss": 0.5755,
      "step": 11220
    },
    {
      "epoch": 0.90056134723336,
      "grad_norm": 1.5608474016189575,
      "learning_rate": 2.1054677030286785e-05,
      "loss": 0.5704,
      "step": 11230
    },
    {
      "epoch": 0.9013632718524459,
      "grad_norm": 1.61857008934021,
      "learning_rate": 2.104663629053873e-05,
      "loss": 0.5692,
      "step": 11240
    },
    {
      "epoch": 0.9021651964715317,
      "grad_norm": 1.6018670797348022,
      "learning_rate": 2.1038595550790674e-05,
      "loss": 0.6206,
      "step": 11250
    },
    {
      "epoch": 0.9029671210906175,
      "grad_norm": 1.5113517045974731,
      "learning_rate": 2.1030554811042618e-05,
      "loss": 0.5291,
      "step": 11260
    },
    {
      "epoch": 0.9037690457097033,
      "grad_norm": 1.7089136838912964,
      "learning_rate": 2.102251407129456e-05,
      "loss": 0.5517,
      "step": 11270
    },
    {
      "epoch": 0.9045709703287891,
      "grad_norm": 1.7423889636993408,
      "learning_rate": 2.1014473331546503e-05,
      "loss": 0.5792,
      "step": 11280
    },
    {
      "epoch": 0.9053728949478749,
      "grad_norm": 1.502150297164917,
      "learning_rate": 2.1006432591798444e-05,
      "loss": 0.6001,
      "step": 11290
    },
    {
      "epoch": 0.9061748195669607,
      "grad_norm": 1.6614437103271484,
      "learning_rate": 2.0998391852050392e-05,
      "loss": 0.5357,
      "step": 11300
    },
    {
      "epoch": 0.9069767441860465,
      "grad_norm": 1.6195290088653564,
      "learning_rate": 2.0990351112302333e-05,
      "loss": 0.557,
      "step": 11310
    },
    {
      "epoch": 0.9077786688051324,
      "grad_norm": 1.681809663772583,
      "learning_rate": 2.0982310372554277e-05,
      "loss": 0.5496,
      "step": 11320
    },
    {
      "epoch": 0.9085805934242182,
      "grad_norm": 1.8776882886886597,
      "learning_rate": 2.0974269632806218e-05,
      "loss": 0.5382,
      "step": 11330
    },
    {
      "epoch": 0.909382518043304,
      "grad_norm": 1.4795379638671875,
      "learning_rate": 2.096622889305816e-05,
      "loss": 0.6048,
      "step": 11340
    },
    {
      "epoch": 0.9101844426623897,
      "grad_norm": 1.564194679260254,
      "learning_rate": 2.0958188153310106e-05,
      "loss": 0.5932,
      "step": 11350
    },
    {
      "epoch": 0.9109863672814755,
      "grad_norm": 1.7948706150054932,
      "learning_rate": 2.0950147413562047e-05,
      "loss": 0.5752,
      "step": 11360
    },
    {
      "epoch": 0.9117882919005613,
      "grad_norm": 1.6278804540634155,
      "learning_rate": 2.094210667381399e-05,
      "loss": 0.5905,
      "step": 11370
    },
    {
      "epoch": 0.9125902165196471,
      "grad_norm": 1.5671290159225464,
      "learning_rate": 2.0934065934065933e-05,
      "loss": 0.6246,
      "step": 11380
    },
    {
      "epoch": 0.9133921411387329,
      "grad_norm": 1.608577013015747,
      "learning_rate": 2.092602519431788e-05,
      "loss": 0.5797,
      "step": 11390
    },
    {
      "epoch": 0.9141940657578188,
      "grad_norm": 1.4249751567840576,
      "learning_rate": 2.091798445456982e-05,
      "loss": 0.5344,
      "step": 11400
    },
    {
      "epoch": 0.9149959903769046,
      "grad_norm": 1.7734737396240234,
      "learning_rate": 2.0909943714821765e-05,
      "loss": 0.5552,
      "step": 11410
    },
    {
      "epoch": 0.9157979149959904,
      "grad_norm": 1.6669820547103882,
      "learning_rate": 2.0901902975073706e-05,
      "loss": 0.5863,
      "step": 11420
    },
    {
      "epoch": 0.9165998396150762,
      "grad_norm": 1.7110450267791748,
      "learning_rate": 2.089386223532565e-05,
      "loss": 0.5555,
      "step": 11430
    },
    {
      "epoch": 0.917401764234162,
      "grad_norm": 1.6333270072937012,
      "learning_rate": 2.0885821495577595e-05,
      "loss": 0.5754,
      "step": 11440
    },
    {
      "epoch": 0.9182036888532478,
      "grad_norm": 1.477763295173645,
      "learning_rate": 2.087778075582954e-05,
      "loss": 0.5453,
      "step": 11450
    },
    {
      "epoch": 0.9190056134723336,
      "grad_norm": 1.646971583366394,
      "learning_rate": 2.086974001608148e-05,
      "loss": 0.5632,
      "step": 11460
    },
    {
      "epoch": 0.9198075380914194,
      "grad_norm": 1.5279617309570312,
      "learning_rate": 2.086169927633342e-05,
      "loss": 0.6298,
      "step": 11470
    },
    {
      "epoch": 0.9206094627105053,
      "grad_norm": 1.7809369564056396,
      "learning_rate": 2.0853658536585365e-05,
      "loss": 0.6389,
      "step": 11480
    },
    {
      "epoch": 0.921411387329591,
      "grad_norm": 1.6080516576766968,
      "learning_rate": 2.084561779683731e-05,
      "loss": 0.59,
      "step": 11490
    },
    {
      "epoch": 0.9222133119486768,
      "grad_norm": 1.7556413412094116,
      "learning_rate": 2.0837577057089254e-05,
      "loss": 0.5487,
      "step": 11500
    },
    {
      "epoch": 0.9230152365677626,
      "grad_norm": 1.6247673034667969,
      "learning_rate": 2.0829536317341195e-05,
      "loss": 0.5498,
      "step": 11510
    },
    {
      "epoch": 0.9238171611868484,
      "grad_norm": 1.7798439264297485,
      "learning_rate": 2.082149557759314e-05,
      "loss": 0.5895,
      "step": 11520
    },
    {
      "epoch": 0.9246190858059342,
      "grad_norm": 1.6304912567138672,
      "learning_rate": 2.081345483784508e-05,
      "loss": 0.5854,
      "step": 11530
    },
    {
      "epoch": 0.92542101042502,
      "grad_norm": 1.8834903240203857,
      "learning_rate": 2.0805414098097028e-05,
      "loss": 0.5576,
      "step": 11540
    },
    {
      "epoch": 0.9262229350441058,
      "grad_norm": 1.6780242919921875,
      "learning_rate": 2.079737335834897e-05,
      "loss": 0.5795,
      "step": 11550
    },
    {
      "epoch": 0.9270248596631917,
      "grad_norm": 1.929973840713501,
      "learning_rate": 2.0789332618600913e-05,
      "loss": 0.5829,
      "step": 11560
    },
    {
      "epoch": 0.9278267842822775,
      "grad_norm": 1.754998803138733,
      "learning_rate": 2.0781291878852854e-05,
      "loss": 0.557,
      "step": 11570
    },
    {
      "epoch": 0.9286287089013633,
      "grad_norm": 1.6522374153137207,
      "learning_rate": 2.0773251139104798e-05,
      "loss": 0.5829,
      "step": 11580
    },
    {
      "epoch": 0.9294306335204491,
      "grad_norm": 1.6296489238739014,
      "learning_rate": 2.0765210399356742e-05,
      "loss": 0.6289,
      "step": 11590
    },
    {
      "epoch": 0.9302325581395349,
      "grad_norm": 1.6809855699539185,
      "learning_rate": 2.0757169659608683e-05,
      "loss": 0.5305,
      "step": 11600
    },
    {
      "epoch": 0.9310344827586207,
      "grad_norm": 1.676801323890686,
      "learning_rate": 2.0749128919860627e-05,
      "loss": 0.4962,
      "step": 11610
    },
    {
      "epoch": 0.9318364073777065,
      "grad_norm": 1.6429170370101929,
      "learning_rate": 2.074108818011257e-05,
      "loss": 0.5867,
      "step": 11620
    },
    {
      "epoch": 0.9326383319967922,
      "grad_norm": 1.4593616724014282,
      "learning_rate": 2.0733047440364516e-05,
      "loss": 0.5574,
      "step": 11630
    },
    {
      "epoch": 0.9334402566158782,
      "grad_norm": 1.8778948783874512,
      "learning_rate": 2.0725006700616457e-05,
      "loss": 0.6253,
      "step": 11640
    },
    {
      "epoch": 0.9342421812349639,
      "grad_norm": 1.679266095161438,
      "learning_rate": 2.07169659608684e-05,
      "loss": 0.5698,
      "step": 11650
    },
    {
      "epoch": 0.9350441058540497,
      "grad_norm": 1.986688494682312,
      "learning_rate": 2.0708925221120342e-05,
      "loss": 0.6363,
      "step": 11660
    },
    {
      "epoch": 0.9358460304731355,
      "grad_norm": 1.6392484903335571,
      "learning_rate": 2.0700884481372286e-05,
      "loss": 0.5547,
      "step": 11670
    },
    {
      "epoch": 0.9366479550922213,
      "grad_norm": 1.698360562324524,
      "learning_rate": 2.069284374162423e-05,
      "loss": 0.5342,
      "step": 11680
    },
    {
      "epoch": 0.9374498797113071,
      "grad_norm": 1.4930837154388428,
      "learning_rate": 2.0684803001876175e-05,
      "loss": 0.5821,
      "step": 11690
    },
    {
      "epoch": 0.9382518043303929,
      "grad_norm": 1.708495020866394,
      "learning_rate": 2.0676762262128116e-05,
      "loss": 0.5889,
      "step": 11700
    },
    {
      "epoch": 0.9390537289494787,
      "grad_norm": 1.5438694953918457,
      "learning_rate": 2.0668721522380057e-05,
      "loss": 0.5759,
      "step": 11710
    },
    {
      "epoch": 0.9398556535685646,
      "grad_norm": 1.9633077383041382,
      "learning_rate": 2.0660680782632005e-05,
      "loss": 0.5872,
      "step": 11720
    },
    {
      "epoch": 0.9406575781876504,
      "grad_norm": 1.797778606414795,
      "learning_rate": 2.0652640042883945e-05,
      "loss": 0.5563,
      "step": 11730
    },
    {
      "epoch": 0.9414595028067362,
      "grad_norm": 1.6931023597717285,
      "learning_rate": 2.064459930313589e-05,
      "loss": 0.5711,
      "step": 11740
    },
    {
      "epoch": 0.942261427425822,
      "grad_norm": 1.7735761404037476,
      "learning_rate": 2.063655856338783e-05,
      "loss": 0.5873,
      "step": 11750
    },
    {
      "epoch": 0.9430633520449078,
      "grad_norm": 2.2935359477996826,
      "learning_rate": 2.0628517823639775e-05,
      "loss": 0.6369,
      "step": 11760
    },
    {
      "epoch": 0.9438652766639936,
      "grad_norm": 1.6073869466781616,
      "learning_rate": 2.062047708389172e-05,
      "loss": 0.5497,
      "step": 11770
    },
    {
      "epoch": 0.9446672012830793,
      "grad_norm": 1.7999235391616821,
      "learning_rate": 2.0612436344143663e-05,
      "loss": 0.6064,
      "step": 11780
    },
    {
      "epoch": 0.9454691259021651,
      "grad_norm": 1.6975575685501099,
      "learning_rate": 2.0604395604395604e-05,
      "loss": 0.6164,
      "step": 11790
    },
    {
      "epoch": 0.946271050521251,
      "grad_norm": 1.4787473678588867,
      "learning_rate": 2.059635486464755e-05,
      "loss": 0.5546,
      "step": 11800
    },
    {
      "epoch": 0.9470729751403368,
      "grad_norm": 1.7204309701919556,
      "learning_rate": 2.058831412489949e-05,
      "loss": 0.5649,
      "step": 11810
    },
    {
      "epoch": 0.9478748997594226,
      "grad_norm": 1.7325398921966553,
      "learning_rate": 2.0580273385151437e-05,
      "loss": 0.6532,
      "step": 11820
    },
    {
      "epoch": 0.9486768243785084,
      "grad_norm": 1.7331736087799072,
      "learning_rate": 2.0572232645403378e-05,
      "loss": 0.5693,
      "step": 11830
    },
    {
      "epoch": 0.9494787489975942,
      "grad_norm": 1.4627985954284668,
      "learning_rate": 2.056419190565532e-05,
      "loss": 0.583,
      "step": 11840
    },
    {
      "epoch": 0.95028067361668,
      "grad_norm": 1.6372439861297607,
      "learning_rate": 2.0556151165907263e-05,
      "loss": 0.5023,
      "step": 11850
    },
    {
      "epoch": 0.9510825982357658,
      "grad_norm": 1.8518998622894287,
      "learning_rate": 2.0548110426159204e-05,
      "loss": 0.626,
      "step": 11860
    },
    {
      "epoch": 0.9518845228548516,
      "grad_norm": 1.5827754735946655,
      "learning_rate": 2.0540069686411152e-05,
      "loss": 0.5531,
      "step": 11870
    },
    {
      "epoch": 0.9526864474739375,
      "grad_norm": 1.5800154209136963,
      "learning_rate": 2.0532028946663093e-05,
      "loss": 0.5969,
      "step": 11880
    },
    {
      "epoch": 0.9534883720930233,
      "grad_norm": 1.5160037279129028,
      "learning_rate": 2.0523988206915037e-05,
      "loss": 0.589,
      "step": 11890
    },
    {
      "epoch": 0.9542902967121091,
      "grad_norm": 1.5956255197525024,
      "learning_rate": 2.0515947467166978e-05,
      "loss": 0.544,
      "step": 11900
    },
    {
      "epoch": 0.9550922213311949,
      "grad_norm": 1.6760293245315552,
      "learning_rate": 2.0507906727418926e-05,
      "loss": 0.5721,
      "step": 11910
    },
    {
      "epoch": 0.9558941459502807,
      "grad_norm": 1.5915400981903076,
      "learning_rate": 2.0499865987670867e-05,
      "loss": 0.5906,
      "step": 11920
    },
    {
      "epoch": 0.9566960705693665,
      "grad_norm": 1.7516181468963623,
      "learning_rate": 2.049182524792281e-05,
      "loss": 0.6233,
      "step": 11930
    },
    {
      "epoch": 0.9574979951884522,
      "grad_norm": 1.8384453058242798,
      "learning_rate": 2.0483784508174752e-05,
      "loss": 0.6051,
      "step": 11940
    },
    {
      "epoch": 0.958299919807538,
      "grad_norm": 1.7495838403701782,
      "learning_rate": 2.0475743768426696e-05,
      "loss": 0.574,
      "step": 11950
    },
    {
      "epoch": 0.9591018444266239,
      "grad_norm": 1.7850431203842163,
      "learning_rate": 2.046770302867864e-05,
      "loss": 0.5182,
      "step": 11960
    },
    {
      "epoch": 0.9599037690457097,
      "grad_norm": 2.0609290599823,
      "learning_rate": 2.045966228893058e-05,
      "loss": 0.571,
      "step": 11970
    },
    {
      "epoch": 0.9607056936647955,
      "grad_norm": 1.7590231895446777,
      "learning_rate": 2.0451621549182526e-05,
      "loss": 0.5553,
      "step": 11980
    },
    {
      "epoch": 0.9615076182838813,
      "grad_norm": 1.6382427215576172,
      "learning_rate": 2.0443580809434466e-05,
      "loss": 0.5324,
      "step": 11990
    },
    {
      "epoch": 0.9623095429029671,
      "grad_norm": 2.049461841583252,
      "learning_rate": 2.043554006968641e-05,
      "loss": 0.5755,
      "step": 12000
    },
    {
      "epoch": 0.9631114675220529,
      "grad_norm": 1.7203818559646606,
      "learning_rate": 2.0427499329938355e-05,
      "loss": 0.5664,
      "step": 12010
    },
    {
      "epoch": 0.9639133921411387,
      "grad_norm": 1.6487747430801392,
      "learning_rate": 2.04194585901903e-05,
      "loss": 0.6362,
      "step": 12020
    },
    {
      "epoch": 0.9647153167602245,
      "grad_norm": 1.8112280368804932,
      "learning_rate": 2.041141785044224e-05,
      "loss": 0.5732,
      "step": 12030
    },
    {
      "epoch": 0.9655172413793104,
      "grad_norm": 1.7984578609466553,
      "learning_rate": 2.0403377110694185e-05,
      "loss": 0.5868,
      "step": 12040
    },
    {
      "epoch": 0.9663191659983962,
      "grad_norm": 1.6815284490585327,
      "learning_rate": 2.0395336370946125e-05,
      "loss": 0.603,
      "step": 12050
    },
    {
      "epoch": 0.967121090617482,
      "grad_norm": 1.9398572444915771,
      "learning_rate": 2.0387295631198073e-05,
      "loss": 0.6097,
      "step": 12060
    },
    {
      "epoch": 0.9679230152365678,
      "grad_norm": 1.790528416633606,
      "learning_rate": 2.0379254891450014e-05,
      "loss": 0.6101,
      "step": 12070
    },
    {
      "epoch": 0.9687249398556536,
      "grad_norm": 1.600585699081421,
      "learning_rate": 2.0371214151701958e-05,
      "loss": 0.5602,
      "step": 12080
    },
    {
      "epoch": 0.9695268644747393,
      "grad_norm": 1.8910105228424072,
      "learning_rate": 2.03631734119539e-05,
      "loss": 0.6298,
      "step": 12090
    },
    {
      "epoch": 0.9703287890938251,
      "grad_norm": 1.809564232826233,
      "learning_rate": 2.0355132672205843e-05,
      "loss": 0.5225,
      "step": 12100
    },
    {
      "epoch": 0.9711307137129109,
      "grad_norm": 1.6657135486602783,
      "learning_rate": 2.0347091932457788e-05,
      "loss": 0.5687,
      "step": 12110
    },
    {
      "epoch": 0.9719326383319968,
      "grad_norm": 1.4773938655853271,
      "learning_rate": 2.033905119270973e-05,
      "loss": 0.5193,
      "step": 12120
    },
    {
      "epoch": 0.9727345629510826,
      "grad_norm": 1.510727882385254,
      "learning_rate": 2.0331010452961673e-05,
      "loss": 0.4787,
      "step": 12130
    },
    {
      "epoch": 0.9735364875701684,
      "grad_norm": 1.545865774154663,
      "learning_rate": 2.0322969713213614e-05,
      "loss": 0.5783,
      "step": 12140
    },
    {
      "epoch": 0.9743384121892542,
      "grad_norm": 1.5798745155334473,
      "learning_rate": 2.031492897346556e-05,
      "loss": 0.5871,
      "step": 12150
    },
    {
      "epoch": 0.97514033680834,
      "grad_norm": 1.6791008710861206,
      "learning_rate": 2.0306888233717502e-05,
      "loss": 0.601,
      "step": 12160
    },
    {
      "epoch": 0.9759422614274258,
      "grad_norm": 1.7554653882980347,
      "learning_rate": 2.0298847493969447e-05,
      "loss": 0.5614,
      "step": 12170
    },
    {
      "epoch": 0.9767441860465116,
      "grad_norm": 1.648449182510376,
      "learning_rate": 2.0290806754221388e-05,
      "loss": 0.5659,
      "step": 12180
    },
    {
      "epoch": 0.9775461106655974,
      "grad_norm": 1.721631646156311,
      "learning_rate": 2.0282766014473332e-05,
      "loss": 0.5892,
      "step": 12190
    },
    {
      "epoch": 0.9783480352846833,
      "grad_norm": 1.6488234996795654,
      "learning_rate": 2.0274725274725276e-05,
      "loss": 0.5544,
      "step": 12200
    },
    {
      "epoch": 0.9791499599037691,
      "grad_norm": 1.4695730209350586,
      "learning_rate": 2.026668453497722e-05,
      "loss": 0.5279,
      "step": 12210
    },
    {
      "epoch": 0.9799518845228549,
      "grad_norm": 1.473251461982727,
      "learning_rate": 2.025864379522916e-05,
      "loss": 0.648,
      "step": 12220
    },
    {
      "epoch": 0.9807538091419407,
      "grad_norm": 1.6635684967041016,
      "learning_rate": 2.0250603055481102e-05,
      "loss": 0.5139,
      "step": 12230
    },
    {
      "epoch": 0.9815557337610264,
      "grad_norm": 2.1329212188720703,
      "learning_rate": 2.0242562315733047e-05,
      "loss": 0.5982,
      "step": 12240
    },
    {
      "epoch": 0.9823576583801122,
      "grad_norm": 1.4783817529678345,
      "learning_rate": 2.023452157598499e-05,
      "loss": 0.61,
      "step": 12250
    },
    {
      "epoch": 0.983159582999198,
      "grad_norm": 1.5471107959747314,
      "learning_rate": 2.0226480836236935e-05,
      "loss": 0.5943,
      "step": 12260
    },
    {
      "epoch": 0.9839615076182838,
      "grad_norm": 1.5263725519180298,
      "learning_rate": 2.0218440096488876e-05,
      "loss": 0.607,
      "step": 12270
    },
    {
      "epoch": 0.9847634322373697,
      "grad_norm": 1.6574472188949585,
      "learning_rate": 2.021039935674082e-05,
      "loss": 0.5877,
      "step": 12280
    },
    {
      "epoch": 0.9855653568564555,
      "grad_norm": 1.7857418060302734,
      "learning_rate": 2.0202358616992765e-05,
      "loss": 0.5647,
      "step": 12290
    },
    {
      "epoch": 0.9863672814755413,
      "grad_norm": 1.9851787090301514,
      "learning_rate": 2.019431787724471e-05,
      "loss": 0.5136,
      "step": 12300
    },
    {
      "epoch": 0.9871692060946271,
      "grad_norm": 1.4864710569381714,
      "learning_rate": 2.018627713749665e-05,
      "loss": 0.5194,
      "step": 12310
    },
    {
      "epoch": 0.9879711307137129,
      "grad_norm": 1.7034016847610474,
      "learning_rate": 2.0178236397748594e-05,
      "loss": 0.5923,
      "step": 12320
    },
    {
      "epoch": 0.9887730553327987,
      "grad_norm": 1.6386010646820068,
      "learning_rate": 2.0170195658000535e-05,
      "loss": 0.6139,
      "step": 12330
    },
    {
      "epoch": 0.9895749799518845,
      "grad_norm": 2.063439130783081,
      "learning_rate": 2.0162154918252483e-05,
      "loss": 0.5832,
      "step": 12340
    },
    {
      "epoch": 0.9903769045709703,
      "grad_norm": 1.5923254489898682,
      "learning_rate": 2.0154114178504424e-05,
      "loss": 0.5125,
      "step": 12350
    },
    {
      "epoch": 0.9911788291900562,
      "grad_norm": 1.6862993240356445,
      "learning_rate": 2.0146073438756365e-05,
      "loss": 0.6204,
      "step": 12360
    },
    {
      "epoch": 0.991980753809142,
      "grad_norm": 1.5645862817764282,
      "learning_rate": 2.013803269900831e-05,
      "loss": 0.5724,
      "step": 12370
    },
    {
      "epoch": 0.9927826784282278,
      "grad_norm": 1.5480753183364868,
      "learning_rate": 2.012999195926025e-05,
      "loss": 0.562,
      "step": 12380
    },
    {
      "epoch": 0.9935846030473136,
      "grad_norm": 1.652211308479309,
      "learning_rate": 2.0121951219512197e-05,
      "loss": 0.5155,
      "step": 12390
    },
    {
      "epoch": 0.9943865276663993,
      "grad_norm": 1.6304610967636108,
      "learning_rate": 2.011391047976414e-05,
      "loss": 0.5657,
      "step": 12400
    },
    {
      "epoch": 0.9951884522854851,
      "grad_norm": 1.6798092126846313,
      "learning_rate": 2.0105869740016083e-05,
      "loss": 0.5611,
      "step": 12410
    },
    {
      "epoch": 0.9959903769045709,
      "grad_norm": 1.6396434307098389,
      "learning_rate": 2.0097829000268024e-05,
      "loss": 0.5397,
      "step": 12420
    },
    {
      "epoch": 0.9967923015236567,
      "grad_norm": 1.7158775329589844,
      "learning_rate": 2.0089788260519968e-05,
      "loss": 0.5834,
      "step": 12430
    },
    {
      "epoch": 0.9975942261427426,
      "grad_norm": 1.7595940828323364,
      "learning_rate": 2.0081747520771912e-05,
      "loss": 0.5868,
      "step": 12440
    },
    {
      "epoch": 0.9983961507618284,
      "grad_norm": 1.4443187713623047,
      "learning_rate": 2.0073706781023856e-05,
      "loss": 0.5851,
      "step": 12450
    },
    {
      "epoch": 0.9991980753809142,
      "grad_norm": 1.4590818881988525,
      "learning_rate": 2.0065666041275797e-05,
      "loss": 0.5481,
      "step": 12460
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.6589263677597046,
      "learning_rate": 2.005762530152774e-05,
      "loss": 0.555,
      "step": 12470
    },
    {
      "epoch": 1.0008019246190858,
      "grad_norm": 1.5535097122192383,
      "learning_rate": 2.0049584561779686e-05,
      "loss": 0.575,
      "step": 12480
    },
    {
      "epoch": 1.0016038492381716,
      "grad_norm": 1.8402068614959717,
      "learning_rate": 2.0041543822031627e-05,
      "loss": 0.6091,
      "step": 12490
    },
    {
      "epoch": 1.0024057738572574,
      "grad_norm": 1.8737995624542236,
      "learning_rate": 2.003350308228357e-05,
      "loss": 0.5832,
      "step": 12500
    },
    {
      "epoch": 1.0032076984763432,
      "grad_norm": 1.4610836505889893,
      "learning_rate": 2.0025462342535512e-05,
      "loss": 0.6061,
      "step": 12510
    },
    {
      "epoch": 1.004009623095429,
      "grad_norm": 1.750479817390442,
      "learning_rate": 2.0017421602787456e-05,
      "loss": 0.5758,
      "step": 12520
    },
    {
      "epoch": 1.0048115477145148,
      "grad_norm": 1.8121908903121948,
      "learning_rate": 2.00093808630394e-05,
      "loss": 0.5367,
      "step": 12530
    },
    {
      "epoch": 1.0056134723336005,
      "grad_norm": 1.894976258277893,
      "learning_rate": 2.0001340123291345e-05,
      "loss": 0.5756,
      "step": 12540
    },
    {
      "epoch": 1.0064153969526863,
      "grad_norm": 1.7052736282348633,
      "learning_rate": 1.9993299383543286e-05,
      "loss": 0.5567,
      "step": 12550
    },
    {
      "epoch": 1.0072173215717724,
      "grad_norm": 1.4975533485412598,
      "learning_rate": 1.998525864379523e-05,
      "loss": 0.5371,
      "step": 12560
    },
    {
      "epoch": 1.0080192461908581,
      "grad_norm": 1.5920956134796143,
      "learning_rate": 1.997721790404717e-05,
      "loss": 0.5515,
      "step": 12570
    },
    {
      "epoch": 1.008821170809944,
      "grad_norm": 1.81851065158844,
      "learning_rate": 1.996917716429912e-05,
      "loss": 0.5906,
      "step": 12580
    },
    {
      "epoch": 1.0096230954290297,
      "grad_norm": 1.6262873411178589,
      "learning_rate": 1.996113642455106e-05,
      "loss": 0.5571,
      "step": 12590
    },
    {
      "epoch": 1.0104250200481155,
      "grad_norm": 2.07757306098938,
      "learning_rate": 1.9953095684803004e-05,
      "loss": 0.5547,
      "step": 12600
    },
    {
      "epoch": 1.0112269446672013,
      "grad_norm": 1.6298507452011108,
      "learning_rate": 1.9945054945054945e-05,
      "loss": 0.5743,
      "step": 12610
    },
    {
      "epoch": 1.012028869286287,
      "grad_norm": 1.5641272068023682,
      "learning_rate": 1.9937014205306886e-05,
      "loss": 0.6159,
      "step": 12620
    },
    {
      "epoch": 1.012830793905373,
      "grad_norm": 1.7047510147094727,
      "learning_rate": 1.9928973465558833e-05,
      "loss": 0.5947,
      "step": 12630
    },
    {
      "epoch": 1.0136327185244587,
      "grad_norm": 1.679944634437561,
      "learning_rate": 1.9920932725810774e-05,
      "loss": 0.5731,
      "step": 12640
    },
    {
      "epoch": 1.0144346431435445,
      "grad_norm": 1.6650025844573975,
      "learning_rate": 1.991289198606272e-05,
      "loss": 0.5879,
      "step": 12650
    },
    {
      "epoch": 1.0152365677626303,
      "grad_norm": 1.6475648880004883,
      "learning_rate": 1.990485124631466e-05,
      "loss": 0.5537,
      "step": 12660
    },
    {
      "epoch": 1.016038492381716,
      "grad_norm": 1.9797428846359253,
      "learning_rate": 1.9896810506566607e-05,
      "loss": 0.5692,
      "step": 12670
    },
    {
      "epoch": 1.0168404170008019,
      "grad_norm": 1.8850783109664917,
      "learning_rate": 1.9888769766818548e-05,
      "loss": 0.5901,
      "step": 12680
    },
    {
      "epoch": 1.0176423416198876,
      "grad_norm": 1.6581424474716187,
      "learning_rate": 1.9880729027070492e-05,
      "loss": 0.5635,
      "step": 12690
    },
    {
      "epoch": 1.0184442662389734,
      "grad_norm": 1.6672283411026,
      "learning_rate": 1.9872688287322433e-05,
      "loss": 0.5955,
      "step": 12700
    },
    {
      "epoch": 1.0192461908580595,
      "grad_norm": 1.7651437520980835,
      "learning_rate": 1.9864647547574377e-05,
      "loss": 0.5575,
      "step": 12710
    },
    {
      "epoch": 1.0200481154771452,
      "grad_norm": 1.8595049381256104,
      "learning_rate": 1.9856606807826322e-05,
      "loss": 0.5956,
      "step": 12720
    },
    {
      "epoch": 1.020850040096231,
      "grad_norm": 1.4835573434829712,
      "learning_rate": 1.9848566068078266e-05,
      "loss": 0.5504,
      "step": 12730
    },
    {
      "epoch": 1.0216519647153168,
      "grad_norm": 1.654842734336853,
      "learning_rate": 1.9840525328330207e-05,
      "loss": 0.6571,
      "step": 12740
    },
    {
      "epoch": 1.0224538893344026,
      "grad_norm": 1.3950514793395996,
      "learning_rate": 1.9832484588582148e-05,
      "loss": 0.5786,
      "step": 12750
    },
    {
      "epoch": 1.0232558139534884,
      "grad_norm": 1.6638115644454956,
      "learning_rate": 1.9824443848834092e-05,
      "loss": 0.5631,
      "step": 12760
    },
    {
      "epoch": 1.0240577385725742,
      "grad_norm": 1.6839951276779175,
      "learning_rate": 1.9816403109086036e-05,
      "loss": 0.5326,
      "step": 12770
    },
    {
      "epoch": 1.02485966319166,
      "grad_norm": 1.6783119440078735,
      "learning_rate": 1.980836236933798e-05,
      "loss": 0.5753,
      "step": 12780
    },
    {
      "epoch": 1.0256615878107458,
      "grad_norm": 1.9029333591461182,
      "learning_rate": 1.980032162958992e-05,
      "loss": 0.5142,
      "step": 12790
    },
    {
      "epoch": 1.0264635124298316,
      "grad_norm": 1.8268080949783325,
      "learning_rate": 1.9792280889841866e-05,
      "loss": 0.5823,
      "step": 12800
    },
    {
      "epoch": 1.0272654370489174,
      "grad_norm": 1.7239669561386108,
      "learning_rate": 1.9784240150093807e-05,
      "loss": 0.5267,
      "step": 12810
    },
    {
      "epoch": 1.0280673616680032,
      "grad_norm": 1.5841176509857178,
      "learning_rate": 1.9776199410345754e-05,
      "loss": 0.5285,
      "step": 12820
    },
    {
      "epoch": 1.028869286287089,
      "grad_norm": 1.9314687252044678,
      "learning_rate": 1.9768158670597695e-05,
      "loss": 0.6015,
      "step": 12830
    },
    {
      "epoch": 1.0296712109061747,
      "grad_norm": 1.4947315454483032,
      "learning_rate": 1.976011793084964e-05,
      "loss": 0.5459,
      "step": 12840
    },
    {
      "epoch": 1.0304731355252605,
      "grad_norm": 1.9946030378341675,
      "learning_rate": 1.975207719110158e-05,
      "loss": 0.5658,
      "step": 12850
    },
    {
      "epoch": 1.0312750601443463,
      "grad_norm": 1.636190414428711,
      "learning_rate": 1.9744036451353528e-05,
      "loss": 0.5314,
      "step": 12860
    },
    {
      "epoch": 1.0320769847634321,
      "grad_norm": 1.674749732017517,
      "learning_rate": 1.973599571160547e-05,
      "loss": 0.5672,
      "step": 12870
    },
    {
      "epoch": 1.0328789093825181,
      "grad_norm": 1.9056544303894043,
      "learning_rate": 1.972795497185741e-05,
      "loss": 0.5791,
      "step": 12880
    },
    {
      "epoch": 1.033680834001604,
      "grad_norm": 1.822903037071228,
      "learning_rate": 1.9719914232109354e-05,
      "loss": 0.5585,
      "step": 12890
    },
    {
      "epoch": 1.0344827586206897,
      "grad_norm": 1.5386089086532593,
      "learning_rate": 1.9711873492361295e-05,
      "loss": 0.609,
      "step": 12900
    },
    {
      "epoch": 1.0352846832397755,
      "grad_norm": 1.7471067905426025,
      "learning_rate": 1.9703832752613243e-05,
      "loss": 0.6314,
      "step": 12910
    },
    {
      "epoch": 1.0360866078588613,
      "grad_norm": 1.8192052841186523,
      "learning_rate": 1.9695792012865184e-05,
      "loss": 0.5808,
      "step": 12920
    },
    {
      "epoch": 1.036888532477947,
      "grad_norm": 1.7145938873291016,
      "learning_rate": 1.9687751273117128e-05,
      "loss": 0.5999,
      "step": 12930
    },
    {
      "epoch": 1.037690457097033,
      "grad_norm": 1.5289562940597534,
      "learning_rate": 1.967971053336907e-05,
      "loss": 0.5894,
      "step": 12940
    },
    {
      "epoch": 1.0384923817161187,
      "grad_norm": 2.025333881378174,
      "learning_rate": 1.9671669793621013e-05,
      "loss": 0.5869,
      "step": 12950
    },
    {
      "epoch": 1.0392943063352045,
      "grad_norm": 1.6615772247314453,
      "learning_rate": 1.9663629053872958e-05,
      "loss": 0.5354,
      "step": 12960
    },
    {
      "epoch": 1.0400962309542903,
      "grad_norm": 1.3963241577148438,
      "learning_rate": 1.9655588314124902e-05,
      "loss": 0.5521,
      "step": 12970
    },
    {
      "epoch": 1.040898155573376,
      "grad_norm": 1.6927869319915771,
      "learning_rate": 1.9647547574376843e-05,
      "loss": 0.5371,
      "step": 12980
    },
    {
      "epoch": 1.0417000801924619,
      "grad_norm": 1.6557834148406982,
      "learning_rate": 1.9639506834628787e-05,
      "loss": 0.5744,
      "step": 12990
    },
    {
      "epoch": 1.0425020048115476,
      "grad_norm": 1.7090071439743042,
      "learning_rate": 1.963146609488073e-05,
      "loss": 0.5358,
      "step": 13000
    },
    {
      "epoch": 1.0433039294306334,
      "grad_norm": 1.523468255996704,
      "learning_rate": 1.9623425355132672e-05,
      "loss": 0.5237,
      "step": 13010
    },
    {
      "epoch": 1.0441058540497192,
      "grad_norm": 1.8645408153533936,
      "learning_rate": 1.9615384615384617e-05,
      "loss": 0.5441,
      "step": 13020
    },
    {
      "epoch": 1.0449077786688052,
      "grad_norm": 1.807375192642212,
      "learning_rate": 1.9607343875636557e-05,
      "loss": 0.5534,
      "step": 13030
    },
    {
      "epoch": 1.045709703287891,
      "grad_norm": 1.7134790420532227,
      "learning_rate": 1.9599303135888502e-05,
      "loss": 0.5509,
      "step": 13040
    },
    {
      "epoch": 1.0465116279069768,
      "grad_norm": 1.773179531097412,
      "learning_rate": 1.9591262396140446e-05,
      "loss": 0.5374,
      "step": 13050
    },
    {
      "epoch": 1.0473135525260626,
      "grad_norm": 1.6742169857025146,
      "learning_rate": 1.958322165639239e-05,
      "loss": 0.5223,
      "step": 13060
    },
    {
      "epoch": 1.0481154771451484,
      "grad_norm": 1.7242354154586792,
      "learning_rate": 1.957518091664433e-05,
      "loss": 0.5887,
      "step": 13070
    },
    {
      "epoch": 1.0489174017642342,
      "grad_norm": 1.8550204038619995,
      "learning_rate": 1.9567140176896275e-05,
      "loss": 0.5748,
      "step": 13080
    },
    {
      "epoch": 1.04971932638332,
      "grad_norm": 1.924431562423706,
      "learning_rate": 1.9559099437148216e-05,
      "loss": 0.6188,
      "step": 13090
    },
    {
      "epoch": 1.0505212510024058,
      "grad_norm": 1.5527229309082031,
      "learning_rate": 1.9551058697400164e-05,
      "loss": 0.5439,
      "step": 13100
    },
    {
      "epoch": 1.0513231756214916,
      "grad_norm": 1.5152796506881714,
      "learning_rate": 1.9543017957652105e-05,
      "loss": 0.5808,
      "step": 13110
    },
    {
      "epoch": 1.0521251002405774,
      "grad_norm": 1.876641869544983,
      "learning_rate": 1.9534977217904046e-05,
      "loss": 0.5832,
      "step": 13120
    },
    {
      "epoch": 1.0529270248596632,
      "grad_norm": 1.9452720880508423,
      "learning_rate": 1.952693647815599e-05,
      "loss": 0.5792,
      "step": 13130
    },
    {
      "epoch": 1.053728949478749,
      "grad_norm": 1.9438031911849976,
      "learning_rate": 1.951889573840793e-05,
      "loss": 0.5915,
      "step": 13140
    },
    {
      "epoch": 1.0545308740978347,
      "grad_norm": 1.7211493253707886,
      "learning_rate": 1.951085499865988e-05,
      "loss": 0.5743,
      "step": 13150
    },
    {
      "epoch": 1.0553327987169205,
      "grad_norm": 1.651273250579834,
      "learning_rate": 1.950281425891182e-05,
      "loss": 0.5622,
      "step": 13160
    },
    {
      "epoch": 1.0561347233360063,
      "grad_norm": 1.7368900775909424,
      "learning_rate": 1.9494773519163764e-05,
      "loss": 0.6027,
      "step": 13170
    },
    {
      "epoch": 1.0569366479550921,
      "grad_norm": 1.5249303579330444,
      "learning_rate": 1.9486732779415705e-05,
      "loss": 0.5614,
      "step": 13180
    },
    {
      "epoch": 1.057738572574178,
      "grad_norm": 1.85300612449646,
      "learning_rate": 1.9478692039667653e-05,
      "loss": 0.5201,
      "step": 13190
    },
    {
      "epoch": 1.058540497193264,
      "grad_norm": 1.9126708507537842,
      "learning_rate": 1.9470651299919593e-05,
      "loss": 0.5942,
      "step": 13200
    },
    {
      "epoch": 1.0593424218123497,
      "grad_norm": 1.495000958442688,
      "learning_rate": 1.9462610560171538e-05,
      "loss": 0.576,
      "step": 13210
    },
    {
      "epoch": 1.0601443464314355,
      "grad_norm": 1.850054144859314,
      "learning_rate": 1.945456982042348e-05,
      "loss": 0.571,
      "step": 13220
    },
    {
      "epoch": 1.0609462710505213,
      "grad_norm": 1.6303247213363647,
      "learning_rate": 1.9446529080675423e-05,
      "loss": 0.6245,
      "step": 13230
    },
    {
      "epoch": 1.061748195669607,
      "grad_norm": 1.7092162370681763,
      "learning_rate": 1.9438488340927367e-05,
      "loss": 0.5569,
      "step": 13240
    },
    {
      "epoch": 1.062550120288693,
      "grad_norm": 1.5374431610107422,
      "learning_rate": 1.9430447601179308e-05,
      "loss": 0.568,
      "step": 13250
    },
    {
      "epoch": 1.0633520449077787,
      "grad_norm": 1.6680898666381836,
      "learning_rate": 1.9422406861431252e-05,
      "loss": 0.5508,
      "step": 13260
    },
    {
      "epoch": 1.0641539695268645,
      "grad_norm": 1.6290547847747803,
      "learning_rate": 1.9414366121683193e-05,
      "loss": 0.5528,
      "step": 13270
    },
    {
      "epoch": 1.0649558941459503,
      "grad_norm": 1.6659845113754272,
      "learning_rate": 1.9406325381935138e-05,
      "loss": 0.5722,
      "step": 13280
    },
    {
      "epoch": 1.065757818765036,
      "grad_norm": 1.9637805223464966,
      "learning_rate": 1.9398284642187082e-05,
      "loss": 0.5878,
      "step": 13290
    },
    {
      "epoch": 1.0665597433841218,
      "grad_norm": 1.8414803743362427,
      "learning_rate": 1.9390243902439026e-05,
      "loss": 0.6154,
      "step": 13300
    },
    {
      "epoch": 1.0673616680032076,
      "grad_norm": 1.859203577041626,
      "learning_rate": 1.9382203162690967e-05,
      "loss": 0.5745,
      "step": 13310
    },
    {
      "epoch": 1.0681635926222934,
      "grad_norm": 1.5928702354431152,
      "learning_rate": 1.937416242294291e-05,
      "loss": 0.5056,
      "step": 13320
    },
    {
      "epoch": 1.0689655172413792,
      "grad_norm": 1.6425148248672485,
      "learning_rate": 1.9366121683194852e-05,
      "loss": 0.5935,
      "step": 13330
    },
    {
      "epoch": 1.069767441860465,
      "grad_norm": 2.0304853916168213,
      "learning_rate": 1.93580809434468e-05,
      "loss": 0.5059,
      "step": 13340
    },
    {
      "epoch": 1.070569366479551,
      "grad_norm": 2.3190624713897705,
      "learning_rate": 1.935004020369874e-05,
      "loss": 0.5487,
      "step": 13350
    },
    {
      "epoch": 1.0713712910986368,
      "grad_norm": 1.6917840242385864,
      "learning_rate": 1.9341999463950685e-05,
      "loss": 0.5935,
      "step": 13360
    },
    {
      "epoch": 1.0721732157177226,
      "grad_norm": 1.5426348447799683,
      "learning_rate": 1.9333958724202626e-05,
      "loss": 0.5426,
      "step": 13370
    },
    {
      "epoch": 1.0729751403368084,
      "grad_norm": 1.7202647924423218,
      "learning_rate": 1.932591798445457e-05,
      "loss": 0.5587,
      "step": 13380
    },
    {
      "epoch": 1.0737770649558942,
      "grad_norm": 1.8199940919876099,
      "learning_rate": 1.9317877244706515e-05,
      "loss": 0.5292,
      "step": 13390
    },
    {
      "epoch": 1.07457898957498,
      "grad_norm": 1.7835458517074585,
      "learning_rate": 1.9309836504958456e-05,
      "loss": 0.6342,
      "step": 13400
    },
    {
      "epoch": 1.0753809141940658,
      "grad_norm": 1.7126559019088745,
      "learning_rate": 1.93017957652104e-05,
      "loss": 0.5771,
      "step": 13410
    },
    {
      "epoch": 1.0761828388131516,
      "grad_norm": 1.4751821756362915,
      "learning_rate": 1.929375502546234e-05,
      "loss": 0.5893,
      "step": 13420
    },
    {
      "epoch": 1.0769847634322374,
      "grad_norm": 1.5768436193466187,
      "learning_rate": 1.928571428571429e-05,
      "loss": 0.564,
      "step": 13430
    },
    {
      "epoch": 1.0777866880513232,
      "grad_norm": 1.6607805490493774,
      "learning_rate": 1.927767354596623e-05,
      "loss": 0.6034,
      "step": 13440
    },
    {
      "epoch": 1.078588612670409,
      "grad_norm": 1.8295140266418457,
      "learning_rate": 1.9269632806218174e-05,
      "loss": 0.5976,
      "step": 13450
    },
    {
      "epoch": 1.0793905372894947,
      "grad_norm": 1.6295839548110962,
      "learning_rate": 1.9261592066470114e-05,
      "loss": 0.5114,
      "step": 13460
    },
    {
      "epoch": 1.0801924619085805,
      "grad_norm": 1.7090091705322266,
      "learning_rate": 1.925355132672206e-05,
      "loss": 0.5287,
      "step": 13470
    },
    {
      "epoch": 1.0809943865276663,
      "grad_norm": 1.731082558631897,
      "learning_rate": 1.9245510586974003e-05,
      "loss": 0.597,
      "step": 13480
    },
    {
      "epoch": 1.0817963111467521,
      "grad_norm": 1.7214593887329102,
      "learning_rate": 1.9237469847225947e-05,
      "loss": 0.5487,
      "step": 13490
    },
    {
      "epoch": 1.082598235765838,
      "grad_norm": 1.789480447769165,
      "learning_rate": 1.9229429107477888e-05,
      "loss": 0.6479,
      "step": 13500
    },
    {
      "epoch": 1.0834001603849237,
      "grad_norm": 1.6382571458816528,
      "learning_rate": 1.922138836772983e-05,
      "loss": 0.6013,
      "step": 13510
    },
    {
      "epoch": 1.0842020850040097,
      "grad_norm": 1.9188878536224365,
      "learning_rate": 1.9213347627981773e-05,
      "loss": 0.5502,
      "step": 13520
    },
    {
      "epoch": 1.0850040096230955,
      "grad_norm": 1.6384834051132202,
      "learning_rate": 1.9205306888233718e-05,
      "loss": 0.5494,
      "step": 13530
    },
    {
      "epoch": 1.0858059342421813,
      "grad_norm": 1.8485788106918335,
      "learning_rate": 1.9197266148485662e-05,
      "loss": 0.5839,
      "step": 13540
    },
    {
      "epoch": 1.086607858861267,
      "grad_norm": 1.6754212379455566,
      "learning_rate": 1.9189225408737603e-05,
      "loss": 0.5245,
      "step": 13550
    },
    {
      "epoch": 1.0874097834803529,
      "grad_norm": 1.5748310089111328,
      "learning_rate": 1.9181184668989547e-05,
      "loss": 0.5699,
      "step": 13560
    },
    {
      "epoch": 1.0882117080994387,
      "grad_norm": 1.550210952758789,
      "learning_rate": 1.917314392924149e-05,
      "loss": 0.5682,
      "step": 13570
    },
    {
      "epoch": 1.0890136327185245,
      "grad_norm": 1.5398651361465454,
      "learning_rate": 1.9165103189493436e-05,
      "loss": 0.5056,
      "step": 13580
    },
    {
      "epoch": 1.0898155573376103,
      "grad_norm": 1.9312291145324707,
      "learning_rate": 1.9157062449745377e-05,
      "loss": 0.5975,
      "step": 13590
    },
    {
      "epoch": 1.090617481956696,
      "grad_norm": 1.8198201656341553,
      "learning_rate": 1.914902170999732e-05,
      "loss": 0.5783,
      "step": 13600
    },
    {
      "epoch": 1.0914194065757818,
      "grad_norm": 1.741109013557434,
      "learning_rate": 1.9140980970249262e-05,
      "loss": 0.5511,
      "step": 13610
    },
    {
      "epoch": 1.0922213311948676,
      "grad_norm": 2.2375307083129883,
      "learning_rate": 1.913294023050121e-05,
      "loss": 0.5586,
      "step": 13620
    },
    {
      "epoch": 1.0930232558139534,
      "grad_norm": 1.5369070768356323,
      "learning_rate": 1.912489949075315e-05,
      "loss": 0.5655,
      "step": 13630
    },
    {
      "epoch": 1.0938251804330392,
      "grad_norm": 1.7502391338348389,
      "learning_rate": 1.911685875100509e-05,
      "loss": 0.5803,
      "step": 13640
    },
    {
      "epoch": 1.094627105052125,
      "grad_norm": 1.6793941259384155,
      "learning_rate": 1.9108818011257036e-05,
      "loss": 0.6065,
      "step": 13650
    },
    {
      "epoch": 1.0954290296712108,
      "grad_norm": 1.741856575012207,
      "learning_rate": 1.9100777271508977e-05,
      "loss": 0.5826,
      "step": 13660
    },
    {
      "epoch": 1.0962309542902968,
      "grad_norm": 1.7031517028808594,
      "learning_rate": 1.9092736531760924e-05,
      "loss": 0.5516,
      "step": 13670
    },
    {
      "epoch": 1.0970328789093826,
      "grad_norm": 2.0335090160369873,
      "learning_rate": 1.9084695792012865e-05,
      "loss": 0.5852,
      "step": 13680
    },
    {
      "epoch": 1.0978348035284684,
      "grad_norm": 1.7853310108184814,
      "learning_rate": 1.907665505226481e-05,
      "loss": 0.5424,
      "step": 13690
    },
    {
      "epoch": 1.0986367281475542,
      "grad_norm": 1.687635064125061,
      "learning_rate": 1.906861431251675e-05,
      "loss": 0.5717,
      "step": 13700
    },
    {
      "epoch": 1.09943865276664,
      "grad_norm": 2.1522915363311768,
      "learning_rate": 1.9060573572768695e-05,
      "loss": 0.4707,
      "step": 13710
    },
    {
      "epoch": 1.1002405773857258,
      "grad_norm": 1.6879850625991821,
      "learning_rate": 1.905253283302064e-05,
      "loss": 0.5658,
      "step": 13720
    },
    {
      "epoch": 1.1010425020048116,
      "grad_norm": 1.7651623487472534,
      "learning_rate": 1.9044492093272583e-05,
      "loss": 0.5791,
      "step": 13730
    },
    {
      "epoch": 1.1018444266238974,
      "grad_norm": 1.815320372581482,
      "learning_rate": 1.9036451353524524e-05,
      "loss": 0.5939,
      "step": 13740
    },
    {
      "epoch": 1.1026463512429832,
      "grad_norm": 1.7238131761550903,
      "learning_rate": 1.902841061377647e-05,
      "loss": 0.5192,
      "step": 13750
    },
    {
      "epoch": 1.103448275862069,
      "grad_norm": 1.8562525510787964,
      "learning_rate": 1.9020369874028413e-05,
      "loss": 0.5652,
      "step": 13760
    },
    {
      "epoch": 1.1042502004811547,
      "grad_norm": 1.7387208938598633,
      "learning_rate": 1.9012329134280354e-05,
      "loss": 0.543,
      "step": 13770
    },
    {
      "epoch": 1.1050521251002405,
      "grad_norm": 1.6800079345703125,
      "learning_rate": 1.9004288394532298e-05,
      "loss": 0.5439,
      "step": 13780
    },
    {
      "epoch": 1.1058540497193263,
      "grad_norm": 2.001199245452881,
      "learning_rate": 1.899624765478424e-05,
      "loss": 0.5834,
      "step": 13790
    },
    {
      "epoch": 1.1066559743384121,
      "grad_norm": 1.6202974319458008,
      "learning_rate": 1.8988206915036183e-05,
      "loss": 0.5416,
      "step": 13800
    },
    {
      "epoch": 1.107457898957498,
      "grad_norm": 1.6083014011383057,
      "learning_rate": 1.8980166175288127e-05,
      "loss": 0.5817,
      "step": 13810
    },
    {
      "epoch": 1.1082598235765837,
      "grad_norm": 1.740929126739502,
      "learning_rate": 1.897212543554007e-05,
      "loss": 0.576,
      "step": 13820
    },
    {
      "epoch": 1.1090617481956695,
      "grad_norm": 1.7621153593063354,
      "learning_rate": 1.8964084695792013e-05,
      "loss": 0.5196,
      "step": 13830
    },
    {
      "epoch": 1.1098636728147555,
      "grad_norm": 1.7900384664535522,
      "learning_rate": 1.8956043956043957e-05,
      "loss": 0.5975,
      "step": 13840
    },
    {
      "epoch": 1.1106655974338413,
      "grad_norm": 1.7506988048553467,
      "learning_rate": 1.8948003216295898e-05,
      "loss": 0.5687,
      "step": 13850
    },
    {
      "epoch": 1.111467522052927,
      "grad_norm": 1.906524896621704,
      "learning_rate": 1.8939962476547845e-05,
      "loss": 0.5754,
      "step": 13860
    },
    {
      "epoch": 1.1122694466720129,
      "grad_norm": 1.6936765909194946,
      "learning_rate": 1.8931921736799786e-05,
      "loss": 0.5733,
      "step": 13870
    },
    {
      "epoch": 1.1130713712910987,
      "grad_norm": 1.612202525138855,
      "learning_rate": 1.892388099705173e-05,
      "loss": 0.5386,
      "step": 13880
    },
    {
      "epoch": 1.1138732959101845,
      "grad_norm": 1.5584189891815186,
      "learning_rate": 1.891584025730367e-05,
      "loss": 0.5767,
      "step": 13890
    },
    {
      "epoch": 1.1146752205292703,
      "grad_norm": 1.7755008935928345,
      "learning_rate": 1.8907799517555612e-05,
      "loss": 0.5365,
      "step": 13900
    },
    {
      "epoch": 1.115477145148356,
      "grad_norm": 1.5861035585403442,
      "learning_rate": 1.889975877780756e-05,
      "loss": 0.5546,
      "step": 13910
    },
    {
      "epoch": 1.1162790697674418,
      "grad_norm": 1.644858956336975,
      "learning_rate": 1.88917180380595e-05,
      "loss": 0.58,
      "step": 13920
    },
    {
      "epoch": 1.1170809943865276,
      "grad_norm": 1.6364514827728271,
      "learning_rate": 1.8883677298311445e-05,
      "loss": 0.5668,
      "step": 13930
    },
    {
      "epoch": 1.1178829190056134,
      "grad_norm": 1.510752558708191,
      "learning_rate": 1.8875636558563386e-05,
      "loss": 0.5671,
      "step": 13940
    },
    {
      "epoch": 1.1186848436246992,
      "grad_norm": 1.736929178237915,
      "learning_rate": 1.8867595818815334e-05,
      "loss": 0.5271,
      "step": 13950
    },
    {
      "epoch": 1.119486768243785,
      "grad_norm": 1.8204389810562134,
      "learning_rate": 1.8859555079067275e-05,
      "loss": 0.5958,
      "step": 13960
    },
    {
      "epoch": 1.1202886928628708,
      "grad_norm": 1.7251169681549072,
      "learning_rate": 1.885151433931922e-05,
      "loss": 0.5445,
      "step": 13970
    },
    {
      "epoch": 1.1210906174819566,
      "grad_norm": 1.705215334892273,
      "learning_rate": 1.884347359957116e-05,
      "loss": 0.5427,
      "step": 13980
    },
    {
      "epoch": 1.1218925421010426,
      "grad_norm": 1.812152624130249,
      "learning_rate": 1.8835432859823104e-05,
      "loss": 0.5623,
      "step": 13990
    },
    {
      "epoch": 1.1226944667201284,
      "grad_norm": 1.721099853515625,
      "learning_rate": 1.882739212007505e-05,
      "loss": 0.5753,
      "step": 14000
    },
    {
      "epoch": 1.1234963913392142,
      "grad_norm": 1.8605438470840454,
      "learning_rate": 1.8819351380326993e-05,
      "loss": 0.572,
      "step": 14010
    },
    {
      "epoch": 1.1242983159583,
      "grad_norm": 1.6263338327407837,
      "learning_rate": 1.8811310640578934e-05,
      "loss": 0.5373,
      "step": 14020
    },
    {
      "epoch": 1.1251002405773858,
      "grad_norm": 1.8335003852844238,
      "learning_rate": 1.8803269900830875e-05,
      "loss": 0.535,
      "step": 14030
    },
    {
      "epoch": 1.1259021651964716,
      "grad_norm": 1.8806960582733154,
      "learning_rate": 1.879522916108282e-05,
      "loss": 0.568,
      "step": 14040
    },
    {
      "epoch": 1.1267040898155574,
      "grad_norm": 1.7401589155197144,
      "learning_rate": 1.8787188421334763e-05,
      "loss": 0.5394,
      "step": 14050
    },
    {
      "epoch": 1.1275060144346432,
      "grad_norm": 1.898585557937622,
      "learning_rate": 1.8779147681586708e-05,
      "loss": 0.5913,
      "step": 14060
    },
    {
      "epoch": 1.128307939053729,
      "grad_norm": 1.6907464265823364,
      "learning_rate": 1.877110694183865e-05,
      "loss": 0.5503,
      "step": 14070
    },
    {
      "epoch": 1.1291098636728147,
      "grad_norm": 1.6688328981399536,
      "learning_rate": 1.8763066202090593e-05,
      "loss": 0.5653,
      "step": 14080
    },
    {
      "epoch": 1.1299117882919005,
      "grad_norm": 1.797269344329834,
      "learning_rate": 1.8755025462342534e-05,
      "loss": 0.5306,
      "step": 14090
    },
    {
      "epoch": 1.1307137129109863,
      "grad_norm": 1.9337133169174194,
      "learning_rate": 1.874698472259448e-05,
      "loss": 0.5584,
      "step": 14100
    },
    {
      "epoch": 1.1315156375300721,
      "grad_norm": 1.7436476945877075,
      "learning_rate": 1.8738943982846422e-05,
      "loss": 0.5499,
      "step": 14110
    },
    {
      "epoch": 1.132317562149158,
      "grad_norm": 1.7094789743423462,
      "learning_rate": 1.8730903243098366e-05,
      "loss": 0.5348,
      "step": 14120
    },
    {
      "epoch": 1.1331194867682437,
      "grad_norm": 1.760783314704895,
      "learning_rate": 1.8722862503350307e-05,
      "loss": 0.6042,
      "step": 14130
    },
    {
      "epoch": 1.1339214113873295,
      "grad_norm": 1.8858839273452759,
      "learning_rate": 1.8714821763602255e-05,
      "loss": 0.5489,
      "step": 14140
    },
    {
      "epoch": 1.1347233360064153,
      "grad_norm": 1.766648769378662,
      "learning_rate": 1.8707585097829e-05,
      "loss": 0.5396,
      "step": 14150
    },
    {
      "epoch": 1.1355252606255013,
      "grad_norm": 1.6873244047164917,
      "learning_rate": 1.8699544358080945e-05,
      "loss": 0.5691,
      "step": 14160
    },
    {
      "epoch": 1.136327185244587,
      "grad_norm": 1.6526515483856201,
      "learning_rate": 1.8691503618332886e-05,
      "loss": 0.4987,
      "step": 14170
    },
    {
      "epoch": 1.1371291098636729,
      "grad_norm": 1.793956995010376,
      "learning_rate": 1.868346287858483e-05,
      "loss": 0.5535,
      "step": 14180
    },
    {
      "epoch": 1.1379310344827587,
      "grad_norm": 1.8906859159469604,
      "learning_rate": 1.867542213883677e-05,
      "loss": 0.4977,
      "step": 14190
    },
    {
      "epoch": 1.1387329591018445,
      "grad_norm": 1.8583492040634155,
      "learning_rate": 1.866738139908872e-05,
      "loss": 0.5065,
      "step": 14200
    },
    {
      "epoch": 1.1395348837209303,
      "grad_norm": 1.72079336643219,
      "learning_rate": 1.865934065934066e-05,
      "loss": 0.5703,
      "step": 14210
    },
    {
      "epoch": 1.140336808340016,
      "grad_norm": 1.864571452140808,
      "learning_rate": 1.86512999195926e-05,
      "loss": 0.5352,
      "step": 14220
    },
    {
      "epoch": 1.1411387329591018,
      "grad_norm": 1.6663318872451782,
      "learning_rate": 1.8643259179844545e-05,
      "loss": 0.5485,
      "step": 14230
    },
    {
      "epoch": 1.1419406575781876,
      "grad_norm": 1.706504225730896,
      "learning_rate": 1.863521844009649e-05,
      "loss": 0.5786,
      "step": 14240
    },
    {
      "epoch": 1.1427425821972734,
      "grad_norm": 1.6911547183990479,
      "learning_rate": 1.8627177700348434e-05,
      "loss": 0.5163,
      "step": 14250
    },
    {
      "epoch": 1.1435445068163592,
      "grad_norm": 1.8077229261398315,
      "learning_rate": 1.8619136960600375e-05,
      "loss": 0.5292,
      "step": 14260
    },
    {
      "epoch": 1.144346431435445,
      "grad_norm": 1.6707907915115356,
      "learning_rate": 1.861109622085232e-05,
      "loss": 0.594,
      "step": 14270
    },
    {
      "epoch": 1.1451483560545308,
      "grad_norm": 1.972947120666504,
      "learning_rate": 1.860305548110426e-05,
      "loss": 0.5518,
      "step": 14280
    },
    {
      "epoch": 1.1459502806736166,
      "grad_norm": 1.7807844877243042,
      "learning_rate": 1.8595014741356208e-05,
      "loss": 0.604,
      "step": 14290
    },
    {
      "epoch": 1.1467522052927026,
      "grad_norm": 2.018187999725342,
      "learning_rate": 1.858697400160815e-05,
      "loss": 0.5638,
      "step": 14300
    },
    {
      "epoch": 1.1475541299117884,
      "grad_norm": 1.5776013135910034,
      "learning_rate": 1.8578933261860093e-05,
      "loss": 0.5456,
      "step": 14310
    },
    {
      "epoch": 1.1483560545308742,
      "grad_norm": 1.7745195627212524,
      "learning_rate": 1.8570892522112034e-05,
      "loss": 0.5242,
      "step": 14320
    },
    {
      "epoch": 1.14915797914996,
      "grad_norm": 1.831662893295288,
      "learning_rate": 1.8562851782363978e-05,
      "loss": 0.5685,
      "step": 14330
    },
    {
      "epoch": 1.1499599037690458,
      "grad_norm": 1.9775137901306152,
      "learning_rate": 1.8554811042615922e-05,
      "loss": 0.5953,
      "step": 14340
    },
    {
      "epoch": 1.1507618283881316,
      "grad_norm": 1.7090704441070557,
      "learning_rate": 1.8546770302867863e-05,
      "loss": 0.5398,
      "step": 14350
    },
    {
      "epoch": 1.1515637530072174,
      "grad_norm": 1.6571091413497925,
      "learning_rate": 1.8538729563119808e-05,
      "loss": 0.5443,
      "step": 14360
    },
    {
      "epoch": 1.1523656776263032,
      "grad_norm": 1.637595295906067,
      "learning_rate": 1.853068882337175e-05,
      "loss": 0.6034,
      "step": 14370
    },
    {
      "epoch": 1.153167602245389,
      "grad_norm": 1.6079707145690918,
      "learning_rate": 1.8522648083623693e-05,
      "loss": 0.5457,
      "step": 14380
    },
    {
      "epoch": 1.1539695268644747,
      "grad_norm": 1.81046724319458,
      "learning_rate": 1.8514607343875637e-05,
      "loss": 0.5436,
      "step": 14390
    },
    {
      "epoch": 1.1547714514835605,
      "grad_norm": 1.8653632402420044,
      "learning_rate": 1.850656660412758e-05,
      "loss": 0.5764,
      "step": 14400
    },
    {
      "epoch": 1.1555733761026463,
      "grad_norm": 1.6789872646331787,
      "learning_rate": 1.8498525864379522e-05,
      "loss": 0.5256,
      "step": 14410
    },
    {
      "epoch": 1.1563753007217321,
      "grad_norm": 1.5727370977401733,
      "learning_rate": 1.8490485124631467e-05,
      "loss": 0.5636,
      "step": 14420
    },
    {
      "epoch": 1.157177225340818,
      "grad_norm": 1.7876977920532227,
      "learning_rate": 1.848244438488341e-05,
      "loss": 0.5754,
      "step": 14430
    },
    {
      "epoch": 1.1579791499599037,
      "grad_norm": 1.7087348699569702,
      "learning_rate": 1.8474403645135355e-05,
      "loss": 0.5579,
      "step": 14440
    },
    {
      "epoch": 1.1587810745789895,
      "grad_norm": 1.5800001621246338,
      "learning_rate": 1.8466362905387296e-05,
      "loss": 0.5734,
      "step": 14450
    },
    {
      "epoch": 1.1595829991980753,
      "grad_norm": 1.5474931001663208,
      "learning_rate": 1.845832216563924e-05,
      "loss": 0.5495,
      "step": 14460
    },
    {
      "epoch": 1.160384923817161,
      "grad_norm": 1.4254707098007202,
      "learning_rate": 1.845028142589118e-05,
      "loss": 0.5348,
      "step": 14470
    },
    {
      "epoch": 1.161186848436247,
      "grad_norm": 1.5284310579299927,
      "learning_rate": 1.8442240686143125e-05,
      "loss": 0.5253,
      "step": 14480
    },
    {
      "epoch": 1.1619887730553329,
      "grad_norm": 1.8455815315246582,
      "learning_rate": 1.843419994639507e-05,
      "loss": 0.5722,
      "step": 14490
    },
    {
      "epoch": 1.1627906976744187,
      "grad_norm": 1.6497074365615845,
      "learning_rate": 1.842615920664701e-05,
      "loss": 0.6101,
      "step": 14500
    },
    {
      "epoch": 1.1635926222935045,
      "grad_norm": 1.6748301982879639,
      "learning_rate": 1.8418118466898955e-05,
      "loss": 0.5174,
      "step": 14510
    },
    {
      "epoch": 1.1643945469125903,
      "grad_norm": 1.7170780897140503,
      "learning_rate": 1.8410077727150896e-05,
      "loss": 0.5617,
      "step": 14520
    },
    {
      "epoch": 1.165196471531676,
      "grad_norm": 1.7612462043762207,
      "learning_rate": 1.8402036987402844e-05,
      "loss": 0.6421,
      "step": 14530
    },
    {
      "epoch": 1.1659983961507618,
      "grad_norm": 1.8405450582504272,
      "learning_rate": 1.8393996247654784e-05,
      "loss": 0.5294,
      "step": 14540
    },
    {
      "epoch": 1.1668003207698476,
      "grad_norm": 1.6403493881225586,
      "learning_rate": 1.838595550790673e-05,
      "loss": 0.5598,
      "step": 14550
    },
    {
      "epoch": 1.1676022453889334,
      "grad_norm": 2.2944371700286865,
      "learning_rate": 1.837791476815867e-05,
      "loss": 0.543,
      "step": 14560
    },
    {
      "epoch": 1.1684041700080192,
      "grad_norm": 1.7015626430511475,
      "learning_rate": 1.8369874028410614e-05,
      "loss": 0.521,
      "step": 14570
    },
    {
      "epoch": 1.169206094627105,
      "grad_norm": 2.0368430614471436,
      "learning_rate": 1.8361833288662558e-05,
      "loss": 0.5781,
      "step": 14580
    },
    {
      "epoch": 1.1700080192461908,
      "grad_norm": 1.7037192583084106,
      "learning_rate": 1.8353792548914503e-05,
      "loss": 0.5734,
      "step": 14590
    },
    {
      "epoch": 1.1708099438652766,
      "grad_norm": 1.5805782079696655,
      "learning_rate": 1.8345751809166443e-05,
      "loss": 0.6167,
      "step": 14600
    },
    {
      "epoch": 1.1716118684843624,
      "grad_norm": 1.8381397724151611,
      "learning_rate": 1.8337711069418384e-05,
      "loss": 0.5235,
      "step": 14610
    },
    {
      "epoch": 1.1724137931034484,
      "grad_norm": 1.7607665061950684,
      "learning_rate": 1.8329670329670332e-05,
      "loss": 0.5302,
      "step": 14620
    },
    {
      "epoch": 1.1732157177225342,
      "grad_norm": 1.6605727672576904,
      "learning_rate": 1.8321629589922273e-05,
      "loss": 0.5356,
      "step": 14630
    },
    {
      "epoch": 1.17401764234162,
      "grad_norm": 1.80475914478302,
      "learning_rate": 1.8313588850174217e-05,
      "loss": 0.5827,
      "step": 14640
    },
    {
      "epoch": 1.1748195669607058,
      "grad_norm": 1.9918385744094849,
      "learning_rate": 1.8305548110426158e-05,
      "loss": 0.5201,
      "step": 14650
    },
    {
      "epoch": 1.1756214915797916,
      "grad_norm": 2.102869749069214,
      "learning_rate": 1.8297507370678102e-05,
      "loss": 0.5753,
      "step": 14660
    },
    {
      "epoch": 1.1764234161988774,
      "grad_norm": 1.7561309337615967,
      "learning_rate": 1.8289466630930047e-05,
      "loss": 0.5128,
      "step": 14670
    },
    {
      "epoch": 1.1772253408179632,
      "grad_norm": 1.7642544507980347,
      "learning_rate": 1.828142589118199e-05,
      "loss": 0.5491,
      "step": 14680
    },
    {
      "epoch": 1.178027265437049,
      "grad_norm": 1.8184621334075928,
      "learning_rate": 1.8273385151433932e-05,
      "loss": 0.5466,
      "step": 14690
    },
    {
      "epoch": 1.1788291900561347,
      "grad_norm": 1.597141146659851,
      "learning_rate": 1.8265344411685876e-05,
      "loss": 0.568,
      "step": 14700
    },
    {
      "epoch": 1.1796311146752205,
      "grad_norm": 1.814616322517395,
      "learning_rate": 1.8257303671937817e-05,
      "loss": 0.5461,
      "step": 14710
    },
    {
      "epoch": 1.1804330392943063,
      "grad_norm": 1.9858055114746094,
      "learning_rate": 1.8249262932189765e-05,
      "loss": 0.5377,
      "step": 14720
    },
    {
      "epoch": 1.181234963913392,
      "grad_norm": 1.712898850440979,
      "learning_rate": 1.8241222192441706e-05,
      "loss": 0.5225,
      "step": 14730
    },
    {
      "epoch": 1.182036888532478,
      "grad_norm": 1.9896776676177979,
      "learning_rate": 1.8233181452693647e-05,
      "loss": 0.5429,
      "step": 14740
    },
    {
      "epoch": 1.1828388131515637,
      "grad_norm": 1.5769195556640625,
      "learning_rate": 1.822514071294559e-05,
      "loss": 0.5477,
      "step": 14750
    },
    {
      "epoch": 1.1836407377706495,
      "grad_norm": 1.88321852684021,
      "learning_rate": 1.8217099973197535e-05,
      "loss": 0.5518,
      "step": 14760
    },
    {
      "epoch": 1.1844426623897353,
      "grad_norm": 2.735339879989624,
      "learning_rate": 1.820905923344948e-05,
      "loss": 0.61,
      "step": 14770
    },
    {
      "epoch": 1.185244587008821,
      "grad_norm": 1.6142587661743164,
      "learning_rate": 1.820101849370142e-05,
      "loss": 0.5583,
      "step": 14780
    },
    {
      "epoch": 1.1860465116279069,
      "grad_norm": 1.5965405702590942,
      "learning_rate": 1.8192977753953365e-05,
      "loss": 0.5864,
      "step": 14790
    },
    {
      "epoch": 1.1868484362469929,
      "grad_norm": 2.0637168884277344,
      "learning_rate": 1.8184937014205306e-05,
      "loss": 0.5794,
      "step": 14800
    },
    {
      "epoch": 1.1876503608660787,
      "grad_norm": 1.6789144277572632,
      "learning_rate": 1.8176896274457253e-05,
      "loss": 0.5918,
      "step": 14810
    },
    {
      "epoch": 1.1884522854851645,
      "grad_norm": 1.6144719123840332,
      "learning_rate": 1.8168855534709194e-05,
      "loss": 0.5608,
      "step": 14820
    },
    {
      "epoch": 1.1892542101042503,
      "grad_norm": 1.8014253377914429,
      "learning_rate": 1.816081479496114e-05,
      "loss": 0.5197,
      "step": 14830
    },
    {
      "epoch": 1.190056134723336,
      "grad_norm": 2.332106351852417,
      "learning_rate": 1.815277405521308e-05,
      "loss": 0.5857,
      "step": 14840
    },
    {
      "epoch": 1.1908580593424218,
      "grad_norm": 1.8886765241622925,
      "learning_rate": 1.8144733315465024e-05,
      "loss": 0.6583,
      "step": 14850
    },
    {
      "epoch": 1.1916599839615076,
      "grad_norm": 1.572325348854065,
      "learning_rate": 1.8136692575716968e-05,
      "loss": 0.5541,
      "step": 14860
    },
    {
      "epoch": 1.1924619085805934,
      "grad_norm": 1.578349232673645,
      "learning_rate": 1.812865183596891e-05,
      "loss": 0.5134,
      "step": 14870
    },
    {
      "epoch": 1.1932638331996792,
      "grad_norm": 1.7980419397354126,
      "learning_rate": 1.8120611096220853e-05,
      "loss": 0.5797,
      "step": 14880
    },
    {
      "epoch": 1.194065757818765,
      "grad_norm": 1.6435140371322632,
      "learning_rate": 1.8112570356472794e-05,
      "loss": 0.5752,
      "step": 14890
    },
    {
      "epoch": 1.1948676824378508,
      "grad_norm": 1.7297656536102295,
      "learning_rate": 1.8104529616724738e-05,
      "loss": 0.5985,
      "step": 14900
    },
    {
      "epoch": 1.1956696070569366,
      "grad_norm": 1.8410929441452026,
      "learning_rate": 1.8096488876976683e-05,
      "loss": 0.5977,
      "step": 14910
    },
    {
      "epoch": 1.1964715316760224,
      "grad_norm": 1.628164291381836,
      "learning_rate": 1.8088448137228627e-05,
      "loss": 0.5132,
      "step": 14920
    },
    {
      "epoch": 1.1972734562951082,
      "grad_norm": 1.851544737815857,
      "learning_rate": 1.8080407397480568e-05,
      "loss": 0.5522,
      "step": 14930
    },
    {
      "epoch": 1.1980753809141942,
      "grad_norm": 1.624396562576294,
      "learning_rate": 1.8072366657732512e-05,
      "loss": 0.5472,
      "step": 14940
    },
    {
      "epoch": 1.19887730553328,
      "grad_norm": 1.4952208995819092,
      "learning_rate": 1.8064325917984456e-05,
      "loss": 0.65,
      "step": 14950
    },
    {
      "epoch": 1.1996792301523658,
      "grad_norm": 1.702139139175415,
      "learning_rate": 1.80562851782364e-05,
      "loss": 0.5598,
      "step": 14960
    },
    {
      "epoch": 1.2004811547714516,
      "grad_norm": 1.9713720083236694,
      "learning_rate": 1.804824443848834e-05,
      "loss": 0.5293,
      "step": 14970
    },
    {
      "epoch": 1.2012830793905374,
      "grad_norm": 1.7457189559936523,
      "learning_rate": 1.8040203698740282e-05,
      "loss": 0.5845,
      "step": 14980
    },
    {
      "epoch": 1.2020850040096231,
      "grad_norm": 1.976501226425171,
      "learning_rate": 1.8032162958992227e-05,
      "loss": 0.5477,
      "step": 14990
    },
    {
      "epoch": 1.202886928628709,
      "grad_norm": 1.7839601039886475,
      "learning_rate": 1.802412221924417e-05,
      "loss": 0.555,
      "step": 15000
    },
    {
      "epoch": 1.2036888532477947,
      "grad_norm": 1.6574183702468872,
      "learning_rate": 1.8016081479496115e-05,
      "loss": 0.5518,
      "step": 15010
    },
    {
      "epoch": 1.2044907778668805,
      "grad_norm": 1.8087711334228516,
      "learning_rate": 1.8008040739748056e-05,
      "loss": 0.5919,
      "step": 15020
    },
    {
      "epoch": 1.2052927024859663,
      "grad_norm": 1.5210293531417847,
      "learning_rate": 1.8e-05,
      "loss": 0.5773,
      "step": 15030
    },
    {
      "epoch": 1.206094627105052,
      "grad_norm": 1.5279377698898315,
      "learning_rate": 1.799195926025194e-05,
      "loss": 0.5175,
      "step": 15040
    },
    {
      "epoch": 1.206896551724138,
      "grad_norm": 1.796125054359436,
      "learning_rate": 1.798391852050389e-05,
      "loss": 0.5434,
      "step": 15050
    },
    {
      "epoch": 1.2076984763432237,
      "grad_norm": 1.6527906656265259,
      "learning_rate": 1.797587778075583e-05,
      "loss": 0.5574,
      "step": 15060
    },
    {
      "epoch": 1.2085004009623095,
      "grad_norm": 1.6454497575759888,
      "learning_rate": 1.7967837041007774e-05,
      "loss": 0.5583,
      "step": 15070
    },
    {
      "epoch": 1.2093023255813953,
      "grad_norm": 1.5451136827468872,
      "learning_rate": 1.7959796301259715e-05,
      "loss": 0.615,
      "step": 15080
    },
    {
      "epoch": 1.210104250200481,
      "grad_norm": 1.695920467376709,
      "learning_rate": 1.795175556151166e-05,
      "loss": 0.619,
      "step": 15090
    },
    {
      "epoch": 1.2109061748195669,
      "grad_norm": 1.7790852785110474,
      "learning_rate": 1.7943714821763604e-05,
      "loss": 0.5814,
      "step": 15100
    },
    {
      "epoch": 1.2117080994386527,
      "grad_norm": 2.0166587829589844,
      "learning_rate": 1.7935674082015545e-05,
      "loss": 0.5796,
      "step": 15110
    },
    {
      "epoch": 1.2125100240577387,
      "grad_norm": 1.7838492393493652,
      "learning_rate": 1.792763334226749e-05,
      "loss": 0.6002,
      "step": 15120
    },
    {
      "epoch": 1.2133119486768245,
      "grad_norm": 1.7492650747299194,
      "learning_rate": 1.791959260251943e-05,
      "loss": 0.5544,
      "step": 15130
    },
    {
      "epoch": 1.2141138732959103,
      "grad_norm": 1.746446967124939,
      "learning_rate": 1.7911551862771377e-05,
      "loss": 0.5645,
      "step": 15140
    },
    {
      "epoch": 1.214915797914996,
      "grad_norm": 2.0428483486175537,
      "learning_rate": 1.790351112302332e-05,
      "loss": 0.5444,
      "step": 15150
    },
    {
      "epoch": 1.2157177225340818,
      "grad_norm": 1.6545119285583496,
      "learning_rate": 1.7895470383275263e-05,
      "loss": 0.5009,
      "step": 15160
    },
    {
      "epoch": 1.2165196471531676,
      "grad_norm": 2.145402669906616,
      "learning_rate": 1.7887429643527204e-05,
      "loss": 0.584,
      "step": 15170
    },
    {
      "epoch": 1.2173215717722534,
      "grad_norm": 1.7508702278137207,
      "learning_rate": 1.7879388903779148e-05,
      "loss": 0.5567,
      "step": 15180
    },
    {
      "epoch": 1.2181234963913392,
      "grad_norm": 1.6091736555099487,
      "learning_rate": 1.7871348164031092e-05,
      "loss": 0.5569,
      "step": 15190
    },
    {
      "epoch": 1.218925421010425,
      "grad_norm": 1.5400723218917847,
      "learning_rate": 1.7863307424283036e-05,
      "loss": 0.5342,
      "step": 15200
    },
    {
      "epoch": 1.2197273456295108,
      "grad_norm": 1.4886925220489502,
      "learning_rate": 1.7855266684534977e-05,
      "loss": 0.4984,
      "step": 15210
    },
    {
      "epoch": 1.2205292702485966,
      "grad_norm": 1.8522968292236328,
      "learning_rate": 1.784722594478692e-05,
      "loss": 0.5612,
      "step": 15220
    },
    {
      "epoch": 1.2213311948676824,
      "grad_norm": 1.7557880878448486,
      "learning_rate": 1.7839185205038863e-05,
      "loss": 0.6258,
      "step": 15230
    },
    {
      "epoch": 1.2221331194867682,
      "grad_norm": 1.5584437847137451,
      "learning_rate": 1.7831144465290807e-05,
      "loss": 0.5617,
      "step": 15240
    },
    {
      "epoch": 1.222935044105854,
      "grad_norm": 2.0823922157287598,
      "learning_rate": 1.782310372554275e-05,
      "loss": 0.5765,
      "step": 15250
    },
    {
      "epoch": 1.22373696872494,
      "grad_norm": 1.7573155164718628,
      "learning_rate": 1.7815062985794692e-05,
      "loss": 0.5296,
      "step": 15260
    },
    {
      "epoch": 1.2245388933440258,
      "grad_norm": 1.728028655052185,
      "learning_rate": 1.7807022246046636e-05,
      "loss": 0.5808,
      "step": 15270
    },
    {
      "epoch": 1.2253408179631116,
      "grad_norm": 1.6509112119674683,
      "learning_rate": 1.7798981506298577e-05,
      "loss": 0.535,
      "step": 15280
    },
    {
      "epoch": 1.2261427425821974,
      "grad_norm": 1.5333895683288574,
      "learning_rate": 1.7790940766550525e-05,
      "loss": 0.4945,
      "step": 15290
    },
    {
      "epoch": 1.2269446672012831,
      "grad_norm": 1.8496462106704712,
      "learning_rate": 1.7782900026802466e-05,
      "loss": 0.5602,
      "step": 15300
    },
    {
      "epoch": 1.227746591820369,
      "grad_norm": 1.7505196332931519,
      "learning_rate": 1.777485928705441e-05,
      "loss": 0.5526,
      "step": 15310
    },
    {
      "epoch": 1.2285485164394547,
      "grad_norm": 1.7226626873016357,
      "learning_rate": 1.776681854730635e-05,
      "loss": 0.615,
      "step": 15320
    },
    {
      "epoch": 1.2293504410585405,
      "grad_norm": 1.7782171964645386,
      "learning_rate": 1.77587778075583e-05,
      "loss": 0.5595,
      "step": 15330
    },
    {
      "epoch": 1.2301523656776263,
      "grad_norm": 1.7514779567718506,
      "learning_rate": 1.775073706781024e-05,
      "loss": 0.5641,
      "step": 15340
    },
    {
      "epoch": 1.230954290296712,
      "grad_norm": 1.9995683431625366,
      "learning_rate": 1.7742696328062184e-05,
      "loss": 0.5789,
      "step": 15350
    },
    {
      "epoch": 1.231756214915798,
      "grad_norm": 1.8637992143630981,
      "learning_rate": 1.7734655588314125e-05,
      "loss": 0.5488,
      "step": 15360
    },
    {
      "epoch": 1.2325581395348837,
      "grad_norm": 1.871654748916626,
      "learning_rate": 1.7726614848566066e-05,
      "loss": 0.5905,
      "step": 15370
    },
    {
      "epoch": 1.2333600641539695,
      "grad_norm": 1.6915698051452637,
      "learning_rate": 1.7718574108818013e-05,
      "loss": 0.6246,
      "step": 15380
    },
    {
      "epoch": 1.2341619887730553,
      "grad_norm": 1.8103107213974,
      "learning_rate": 1.7710533369069954e-05,
      "loss": 0.5847,
      "step": 15390
    },
    {
      "epoch": 1.234963913392141,
      "grad_norm": 2.244023323059082,
      "learning_rate": 1.77024926293219e-05,
      "loss": 0.6265,
      "step": 15400
    },
    {
      "epoch": 1.2357658380112269,
      "grad_norm": 1.7980612516403198,
      "learning_rate": 1.769445188957384e-05,
      "loss": 0.5774,
      "step": 15410
    },
    {
      "epoch": 1.2365677626303127,
      "grad_norm": 1.9836454391479492,
      "learning_rate": 1.7686411149825784e-05,
      "loss": 0.5703,
      "step": 15420
    },
    {
      "epoch": 1.2373696872493984,
      "grad_norm": 1.97299325466156,
      "learning_rate": 1.7678370410077728e-05,
      "loss": 0.554,
      "step": 15430
    },
    {
      "epoch": 1.2381716118684845,
      "grad_norm": 2.175837993621826,
      "learning_rate": 1.7670329670329672e-05,
      "loss": 0.5752,
      "step": 15440
    },
    {
      "epoch": 1.2389735364875702,
      "grad_norm": 1.7391724586486816,
      "learning_rate": 1.7662288930581613e-05,
      "loss": 0.5693,
      "step": 15450
    },
    {
      "epoch": 1.239775461106656,
      "grad_norm": 1.8426182270050049,
      "learning_rate": 1.7654248190833557e-05,
      "loss": 0.546,
      "step": 15460
    },
    {
      "epoch": 1.2405773857257418,
      "grad_norm": 1.5051934719085693,
      "learning_rate": 1.76462074510855e-05,
      "loss": 0.5691,
      "step": 15470
    },
    {
      "epoch": 1.2413793103448276,
      "grad_norm": 1.398010015487671,
      "learning_rate": 1.7638166711337446e-05,
      "loss": 0.5408,
      "step": 15480
    },
    {
      "epoch": 1.2421812349639134,
      "grad_norm": 1.6890544891357422,
      "learning_rate": 1.7630125971589387e-05,
      "loss": 0.6085,
      "step": 15490
    },
    {
      "epoch": 1.2429831595829992,
      "grad_norm": 2.423450469970703,
      "learning_rate": 1.7622085231841328e-05,
      "loss": 0.6372,
      "step": 15500
    },
    {
      "epoch": 1.243785084202085,
      "grad_norm": 1.730287790298462,
      "learning_rate": 1.7614044492093272e-05,
      "loss": 0.5778,
      "step": 15510
    },
    {
      "epoch": 1.2445870088211708,
      "grad_norm": 1.8046854734420776,
      "learning_rate": 1.7606003752345216e-05,
      "loss": 0.5731,
      "step": 15520
    },
    {
      "epoch": 1.2453889334402566,
      "grad_norm": 1.5712246894836426,
      "learning_rate": 1.759796301259716e-05,
      "loss": 0.5615,
      "step": 15530
    },
    {
      "epoch": 1.2461908580593424,
      "grad_norm": 1.7626838684082031,
      "learning_rate": 1.75899222728491e-05,
      "loss": 0.5685,
      "step": 15540
    },
    {
      "epoch": 1.2469927826784282,
      "grad_norm": 1.5792171955108643,
      "learning_rate": 1.7581881533101046e-05,
      "loss": 0.5926,
      "step": 15550
    },
    {
      "epoch": 1.247794707297514,
      "grad_norm": 1.6547921895980835,
      "learning_rate": 1.7573840793352987e-05,
      "loss": 0.5378,
      "step": 15560
    },
    {
      "epoch": 1.2485966319165998,
      "grad_norm": 1.9549871683120728,
      "learning_rate": 1.7565800053604935e-05,
      "loss": 0.4951,
      "step": 15570
    },
    {
      "epoch": 1.2493985565356858,
      "grad_norm": 2.235379457473755,
      "learning_rate": 1.7557759313856875e-05,
      "loss": 0.5706,
      "step": 15580
    },
    {
      "epoch": 1.2502004811547716,
      "grad_norm": 1.7332836389541626,
      "learning_rate": 1.754971857410882e-05,
      "loss": 0.5394,
      "step": 15590
    },
    {
      "epoch": 1.2510024057738574,
      "grad_norm": 1.6860862970352173,
      "learning_rate": 1.754167783436076e-05,
      "loss": 0.5587,
      "step": 15600
    },
    {
      "epoch": 1.2518043303929431,
      "grad_norm": 1.7080023288726807,
      "learning_rate": 1.7533637094612705e-05,
      "loss": 0.5872,
      "step": 15610
    },
    {
      "epoch": 1.252606255012029,
      "grad_norm": 1.860662817955017,
      "learning_rate": 1.752559635486465e-05,
      "loss": 0.5345,
      "step": 15620
    },
    {
      "epoch": 1.2534081796311147,
      "grad_norm": 1.9859095811843872,
      "learning_rate": 1.751755561511659e-05,
      "loss": 0.5902,
      "step": 15630
    },
    {
      "epoch": 1.2542101042502005,
      "grad_norm": 1.5663765668869019,
      "learning_rate": 1.7509514875368534e-05,
      "loss": 0.5811,
      "step": 15640
    },
    {
      "epoch": 1.2550120288692863,
      "grad_norm": 1.6330124139785767,
      "learning_rate": 1.7501474135620475e-05,
      "loss": 0.606,
      "step": 15650
    },
    {
      "epoch": 1.255813953488372,
      "grad_norm": 1.7726846933364868,
      "learning_rate": 1.749343339587242e-05,
      "loss": 0.5792,
      "step": 15660
    },
    {
      "epoch": 1.256615878107458,
      "grad_norm": 1.6491392850875854,
      "learning_rate": 1.7485392656124364e-05,
      "loss": 0.5269,
      "step": 15670
    },
    {
      "epoch": 1.2574178027265437,
      "grad_norm": 1.8952170610427856,
      "learning_rate": 1.7477351916376308e-05,
      "loss": 0.5117,
      "step": 15680
    },
    {
      "epoch": 1.2582197273456295,
      "grad_norm": 1.631813883781433,
      "learning_rate": 1.746931117662825e-05,
      "loss": 0.5513,
      "step": 15690
    },
    {
      "epoch": 1.2590216519647153,
      "grad_norm": 1.644333004951477,
      "learning_rate": 1.7461270436880193e-05,
      "loss": 0.5518,
      "step": 15700
    },
    {
      "epoch": 1.259823576583801,
      "grad_norm": 1.9089229106903076,
      "learning_rate": 1.7453229697132138e-05,
      "loss": 0.5585,
      "step": 15710
    },
    {
      "epoch": 1.2606255012028869,
      "grad_norm": 1.7134385108947754,
      "learning_rate": 1.7445188957384082e-05,
      "loss": 0.5597,
      "step": 15720
    },
    {
      "epoch": 1.2614274258219726,
      "grad_norm": 1.5805068016052246,
      "learning_rate": 1.7437148217636023e-05,
      "loss": 0.5885,
      "step": 15730
    },
    {
      "epoch": 1.2622293504410584,
      "grad_norm": 1.852036714553833,
      "learning_rate": 1.7429107477887967e-05,
      "loss": 0.5635,
      "step": 15740
    },
    {
      "epoch": 1.2630312750601442,
      "grad_norm": 1.6548022031784058,
      "learning_rate": 1.7421066738139908e-05,
      "loss": 0.5864,
      "step": 15750
    },
    {
      "epoch": 1.26383319967923,
      "grad_norm": 1.8758790493011475,
      "learning_rate": 1.7413025998391852e-05,
      "loss": 0.534,
      "step": 15760
    },
    {
      "epoch": 1.264635124298316,
      "grad_norm": 2.124816417694092,
      "learning_rate": 1.7404985258643797e-05,
      "loss": 0.5459,
      "step": 15770
    },
    {
      "epoch": 1.2654370489174018,
      "grad_norm": 1.5832390785217285,
      "learning_rate": 1.7396944518895738e-05,
      "loss": 0.5043,
      "step": 15780
    },
    {
      "epoch": 1.2662389735364876,
      "grad_norm": 1.4660125970840454,
      "learning_rate": 1.7388903779147682e-05,
      "loss": 0.5466,
      "step": 15790
    },
    {
      "epoch": 1.2670408981555734,
      "grad_norm": 1.98867666721344,
      "learning_rate": 1.7380863039399623e-05,
      "loss": 0.5272,
      "step": 15800
    },
    {
      "epoch": 1.2678428227746592,
      "grad_norm": 1.988526701927185,
      "learning_rate": 1.737282229965157e-05,
      "loss": 0.5224,
      "step": 15810
    },
    {
      "epoch": 1.268644747393745,
      "grad_norm": 1.74586820602417,
      "learning_rate": 1.736478155990351e-05,
      "loss": 0.6009,
      "step": 15820
    },
    {
      "epoch": 1.2694466720128308,
      "grad_norm": 1.5182892084121704,
      "learning_rate": 1.7356740820155456e-05,
      "loss": 0.5935,
      "step": 15830
    },
    {
      "epoch": 1.2702485966319166,
      "grad_norm": 1.619491457939148,
      "learning_rate": 1.7348700080407396e-05,
      "loss": 0.5509,
      "step": 15840
    },
    {
      "epoch": 1.2710505212510024,
      "grad_norm": 1.5882856845855713,
      "learning_rate": 1.734065934065934e-05,
      "loss": 0.5708,
      "step": 15850
    },
    {
      "epoch": 1.2718524458700882,
      "grad_norm": 1.6438719034194946,
      "learning_rate": 1.7332618600911285e-05,
      "loss": 0.5594,
      "step": 15860
    },
    {
      "epoch": 1.272654370489174,
      "grad_norm": 1.4775882959365845,
      "learning_rate": 1.732457786116323e-05,
      "loss": 0.5605,
      "step": 15870
    },
    {
      "epoch": 1.2734562951082598,
      "grad_norm": 1.863143801689148,
      "learning_rate": 1.731653712141517e-05,
      "loss": 0.5436,
      "step": 15880
    },
    {
      "epoch": 1.2742582197273458,
      "grad_norm": 1.8174076080322266,
      "learning_rate": 1.730849638166711e-05,
      "loss": 0.5558,
      "step": 15890
    },
    {
      "epoch": 1.2750601443464316,
      "grad_norm": 1.7740980386734009,
      "learning_rate": 1.730045564191906e-05,
      "loss": 0.6056,
      "step": 15900
    },
    {
      "epoch": 1.2758620689655173,
      "grad_norm": 1.793606162071228,
      "learning_rate": 1.7292414902171e-05,
      "loss": 0.5133,
      "step": 15910
    },
    {
      "epoch": 1.2766639935846031,
      "grad_norm": 1.9240795373916626,
      "learning_rate": 1.7284374162422944e-05,
      "loss": 0.5872,
      "step": 15920
    },
    {
      "epoch": 1.277465918203689,
      "grad_norm": 1.8536689281463623,
      "learning_rate": 1.7276333422674885e-05,
      "loss": 0.5286,
      "step": 15930
    },
    {
      "epoch": 1.2782678428227747,
      "grad_norm": 1.9156415462493896,
      "learning_rate": 1.726829268292683e-05,
      "loss": 0.5907,
      "step": 15940
    },
    {
      "epoch": 1.2790697674418605,
      "grad_norm": 1.668953776359558,
      "learning_rate": 1.7260251943178774e-05,
      "loss": 0.5943,
      "step": 15950
    },
    {
      "epoch": 1.2798716920609463,
      "grad_norm": 1.691463589668274,
      "learning_rate": 1.7252211203430718e-05,
      "loss": 0.5696,
      "step": 15960
    },
    {
      "epoch": 1.280673616680032,
      "grad_norm": 1.5574626922607422,
      "learning_rate": 1.724417046368266e-05,
      "loss": 0.5706,
      "step": 15970
    },
    {
      "epoch": 1.281475541299118,
      "grad_norm": 1.7309452295303345,
      "learning_rate": 1.7236129723934603e-05,
      "loss": 0.5658,
      "step": 15980
    },
    {
      "epoch": 1.2822774659182037,
      "grad_norm": 1.7476515769958496,
      "learning_rate": 1.7228088984186544e-05,
      "loss": 0.5665,
      "step": 15990
    },
    {
      "epoch": 1.2830793905372895,
      "grad_norm": 1.8936535120010376,
      "learning_rate": 1.722004824443849e-05,
      "loss": 0.545,
      "step": 16000
    },
    {
      "epoch": 1.2838813151563753,
      "grad_norm": 1.7650922536849976,
      "learning_rate": 1.7212007504690432e-05,
      "loss": 0.5622,
      "step": 16010
    },
    {
      "epoch": 1.284683239775461,
      "grad_norm": 1.9145143032073975,
      "learning_rate": 1.7203966764942373e-05,
      "loss": 0.4755,
      "step": 16020
    },
    {
      "epoch": 1.2854851643945469,
      "grad_norm": 1.712307095527649,
      "learning_rate": 1.7195926025194318e-05,
      "loss": 0.5585,
      "step": 16030
    },
    {
      "epoch": 1.2862870890136326,
      "grad_norm": 1.7542195320129395,
      "learning_rate": 1.7187885285446262e-05,
      "loss": 0.575,
      "step": 16040
    },
    {
      "epoch": 1.2870890136327184,
      "grad_norm": 1.8253519535064697,
      "learning_rate": 1.7179844545698206e-05,
      "loss": 0.5406,
      "step": 16050
    },
    {
      "epoch": 1.2878909382518042,
      "grad_norm": 1.899010419845581,
      "learning_rate": 1.7171803805950147e-05,
      "loss": 0.567,
      "step": 16060
    },
    {
      "epoch": 1.28869286287089,
      "grad_norm": 1.9066123962402344,
      "learning_rate": 1.716376306620209e-05,
      "loss": 0.6171,
      "step": 16070
    },
    {
      "epoch": 1.2894947874899758,
      "grad_norm": 1.7093279361724854,
      "learning_rate": 1.7155722326454032e-05,
      "loss": 0.5511,
      "step": 16080
    },
    {
      "epoch": 1.2902967121090618,
      "grad_norm": 1.8019334077835083,
      "learning_rate": 1.714768158670598e-05,
      "loss": 0.6085,
      "step": 16090
    },
    {
      "epoch": 1.2910986367281476,
      "grad_norm": 1.9128057956695557,
      "learning_rate": 1.713964084695792e-05,
      "loss": 0.5792,
      "step": 16100
    },
    {
      "epoch": 1.2919005613472334,
      "grad_norm": 1.5714854001998901,
      "learning_rate": 1.7131600107209865e-05,
      "loss": 0.5656,
      "step": 16110
    },
    {
      "epoch": 1.2927024859663192,
      "grad_norm": 1.759924054145813,
      "learning_rate": 1.7123559367461806e-05,
      "loss": 0.5727,
      "step": 16120
    },
    {
      "epoch": 1.293504410585405,
      "grad_norm": 1.7971725463867188,
      "learning_rate": 1.711551862771375e-05,
      "loss": 0.5424,
      "step": 16130
    },
    {
      "epoch": 1.2943063352044908,
      "grad_norm": 1.813043236732483,
      "learning_rate": 1.7107477887965695e-05,
      "loss": 0.5678,
      "step": 16140
    },
    {
      "epoch": 1.2951082598235766,
      "grad_norm": 1.8468061685562134,
      "learning_rate": 1.7099437148217636e-05,
      "loss": 0.6442,
      "step": 16150
    },
    {
      "epoch": 1.2959101844426624,
      "grad_norm": 2.1825499534606934,
      "learning_rate": 1.709139640846958e-05,
      "loss": 0.5744,
      "step": 16160
    },
    {
      "epoch": 1.2967121090617482,
      "grad_norm": 1.9387822151184082,
      "learning_rate": 1.708335566872152e-05,
      "loss": 0.5287,
      "step": 16170
    },
    {
      "epoch": 1.297514033680834,
      "grad_norm": 1.5532761812210083,
      "learning_rate": 1.7075314928973465e-05,
      "loss": 0.5527,
      "step": 16180
    },
    {
      "epoch": 1.2983159582999197,
      "grad_norm": 1.8213361501693726,
      "learning_rate": 1.706727418922541e-05,
      "loss": 0.5396,
      "step": 16190
    },
    {
      "epoch": 1.2991178829190055,
      "grad_norm": 1.7696967124938965,
      "learning_rate": 1.7059233449477354e-05,
      "loss": 0.591,
      "step": 16200
    },
    {
      "epoch": 1.2999198075380916,
      "grad_norm": 1.3359742164611816,
      "learning_rate": 1.7051192709729295e-05,
      "loss": 0.521,
      "step": 16210
    },
    {
      "epoch": 1.3007217321571773,
      "grad_norm": 1.6587878465652466,
      "learning_rate": 1.704315196998124e-05,
      "loss": 0.6028,
      "step": 16220
    },
    {
      "epoch": 1.3015236567762631,
      "grad_norm": 2.1130988597869873,
      "learning_rate": 1.7035111230233183e-05,
      "loss": 0.5606,
      "step": 16230
    },
    {
      "epoch": 1.302325581395349,
      "grad_norm": 2.0081236362457275,
      "learning_rate": 1.7027070490485127e-05,
      "loss": 0.6024,
      "step": 16240
    },
    {
      "epoch": 1.3031275060144347,
      "grad_norm": 1.8316932916641235,
      "learning_rate": 1.701902975073707e-05,
      "loss": 0.5478,
      "step": 16250
    },
    {
      "epoch": 1.3039294306335205,
      "grad_norm": 1.7406116724014282,
      "learning_rate": 1.7010989010989013e-05,
      "loss": 0.5215,
      "step": 16260
    },
    {
      "epoch": 1.3047313552526063,
      "grad_norm": 1.8060896396636963,
      "learning_rate": 1.7002948271240954e-05,
      "loss": 0.5656,
      "step": 16270
    },
    {
      "epoch": 1.305533279871692,
      "grad_norm": 2.412290334701538,
      "learning_rate": 1.6995711605467703e-05,
      "loss": 0.6031,
      "step": 16280
    },
    {
      "epoch": 1.306335204490778,
      "grad_norm": 1.7901886701583862,
      "learning_rate": 1.6987670865719647e-05,
      "loss": 0.533,
      "step": 16290
    },
    {
      "epoch": 1.3071371291098637,
      "grad_norm": 1.5314528942108154,
      "learning_rate": 1.697963012597159e-05,
      "loss": 0.5945,
      "step": 16300
    },
    {
      "epoch": 1.3079390537289495,
      "grad_norm": 1.8262598514556885,
      "learning_rate": 1.6971589386223533e-05,
      "loss": 0.6216,
      "step": 16310
    },
    {
      "epoch": 1.3087409783480353,
      "grad_norm": 1.9066188335418701,
      "learning_rate": 1.6963548646475477e-05,
      "loss": 0.5735,
      "step": 16320
    },
    {
      "epoch": 1.309542902967121,
      "grad_norm": 1.6737048625946045,
      "learning_rate": 1.695550790672742e-05,
      "loss": 0.5817,
      "step": 16330
    },
    {
      "epoch": 1.3103448275862069,
      "grad_norm": 2.001314878463745,
      "learning_rate": 1.6947467166979362e-05,
      "loss": 0.635,
      "step": 16340
    },
    {
      "epoch": 1.3111467522052926,
      "grad_norm": 1.6217666864395142,
      "learning_rate": 1.6939426427231306e-05,
      "loss": 0.576,
      "step": 16350
    },
    {
      "epoch": 1.3119486768243784,
      "grad_norm": 1.8344814777374268,
      "learning_rate": 1.6931385687483247e-05,
      "loss": 0.5481,
      "step": 16360
    },
    {
      "epoch": 1.3127506014434642,
      "grad_norm": 2.9726574420928955,
      "learning_rate": 1.692334494773519e-05,
      "loss": 0.4972,
      "step": 16370
    },
    {
      "epoch": 1.31355252606255,
      "grad_norm": 1.6183013916015625,
      "learning_rate": 1.6915304207987136e-05,
      "loss": 0.5411,
      "step": 16380
    },
    {
      "epoch": 1.3143544506816358,
      "grad_norm": 1.5694652795791626,
      "learning_rate": 1.690726346823908e-05,
      "loss": 0.5321,
      "step": 16390
    },
    {
      "epoch": 1.3151563753007216,
      "grad_norm": 1.8968662023544312,
      "learning_rate": 1.689922272849102e-05,
      "loss": 0.5451,
      "step": 16400
    },
    {
      "epoch": 1.3159582999198076,
      "grad_norm": 2.0752811431884766,
      "learning_rate": 1.6891181988742965e-05,
      "loss": 0.602,
      "step": 16410
    },
    {
      "epoch": 1.3167602245388934,
      "grad_norm": 1.697248935699463,
      "learning_rate": 1.6883141248994906e-05,
      "loss": 0.5151,
      "step": 16420
    },
    {
      "epoch": 1.3175621491579792,
      "grad_norm": 1.626085638999939,
      "learning_rate": 1.6875100509246854e-05,
      "loss": 0.4907,
      "step": 16430
    },
    {
      "epoch": 1.318364073777065,
      "grad_norm": 1.9028042554855347,
      "learning_rate": 1.6867059769498795e-05,
      "loss": 0.5872,
      "step": 16440
    },
    {
      "epoch": 1.3191659983961508,
      "grad_norm": 1.8158845901489258,
      "learning_rate": 1.685901902975074e-05,
      "loss": 0.5579,
      "step": 16450
    },
    {
      "epoch": 1.3199679230152366,
      "grad_norm": 1.9170364141464233,
      "learning_rate": 1.685097829000268e-05,
      "loss": 0.5251,
      "step": 16460
    },
    {
      "epoch": 1.3207698476343224,
      "grad_norm": 1.9510867595672607,
      "learning_rate": 1.684293755025462e-05,
      "loss": 0.5265,
      "step": 16470
    },
    {
      "epoch": 1.3215717722534082,
      "grad_norm": 1.6585382223129272,
      "learning_rate": 1.683489681050657e-05,
      "loss": 0.576,
      "step": 16480
    },
    {
      "epoch": 1.322373696872494,
      "grad_norm": 1.732620358467102,
      "learning_rate": 1.682685607075851e-05,
      "loss": 0.5393,
      "step": 16490
    },
    {
      "epoch": 1.3231756214915797,
      "grad_norm": 1.7740898132324219,
      "learning_rate": 1.6818815331010454e-05,
      "loss": 0.5528,
      "step": 16500
    },
    {
      "epoch": 1.3239775461106655,
      "grad_norm": 1.6994012594223022,
      "learning_rate": 1.6810774591262395e-05,
      "loss": 0.5751,
      "step": 16510
    },
    {
      "epoch": 1.3247794707297513,
      "grad_norm": 1.62062668800354,
      "learning_rate": 1.6802733851514342e-05,
      "loss": 0.5902,
      "step": 16520
    },
    {
      "epoch": 1.3255813953488373,
      "grad_norm": 1.6712254285812378,
      "learning_rate": 1.6794693111766283e-05,
      "loss": 0.5511,
      "step": 16530
    },
    {
      "epoch": 1.3263833199679231,
      "grad_norm": 1.7678263187408447,
      "learning_rate": 1.6786652372018227e-05,
      "loss": 0.5797,
      "step": 16540
    },
    {
      "epoch": 1.327185244587009,
      "grad_norm": 1.8844475746154785,
      "learning_rate": 1.677861163227017e-05,
      "loss": 0.5887,
      "step": 16550
    },
    {
      "epoch": 1.3279871692060947,
      "grad_norm": 1.8024539947509766,
      "learning_rate": 1.6770570892522113e-05,
      "loss": 0.5286,
      "step": 16560
    },
    {
      "epoch": 1.3287890938251805,
      "grad_norm": 1.7267402410507202,
      "learning_rate": 1.6762530152774057e-05,
      "loss": 0.5687,
      "step": 16570
    },
    {
      "epoch": 1.3295910184442663,
      "grad_norm": 1.6807044744491577,
      "learning_rate": 1.6754489413026e-05,
      "loss": 0.576,
      "step": 16580
    },
    {
      "epoch": 1.330392943063352,
      "grad_norm": 1.6568808555603027,
      "learning_rate": 1.6746448673277942e-05,
      "loss": 0.526,
      "step": 16590
    },
    {
      "epoch": 1.3311948676824379,
      "grad_norm": 1.6284295320510864,
      "learning_rate": 1.6738407933529883e-05,
      "loss": 0.5879,
      "step": 16600
    },
    {
      "epoch": 1.3319967923015237,
      "grad_norm": 1.7487729787826538,
      "learning_rate": 1.6730367193781827e-05,
      "loss": 0.594,
      "step": 16610
    },
    {
      "epoch": 1.3327987169206095,
      "grad_norm": 1.6965625286102295,
      "learning_rate": 1.672232645403377e-05,
      "loss": 0.6214,
      "step": 16620
    },
    {
      "epoch": 1.3336006415396953,
      "grad_norm": 1.8716473579406738,
      "learning_rate": 1.6714285714285716e-05,
      "loss": 0.574,
      "step": 16630
    },
    {
      "epoch": 1.334402566158781,
      "grad_norm": 1.8162460327148438,
      "learning_rate": 1.6706244974537657e-05,
      "loss": 0.5567,
      "step": 16640
    },
    {
      "epoch": 1.3352044907778668,
      "grad_norm": 1.5144199132919312,
      "learning_rate": 1.66982042347896e-05,
      "loss": 0.5183,
      "step": 16650
    },
    {
      "epoch": 1.3360064153969526,
      "grad_norm": 1.9152978658676147,
      "learning_rate": 1.6690163495041542e-05,
      "loss": 0.537,
      "step": 16660
    },
    {
      "epoch": 1.3368083400160384,
      "grad_norm": 1.6428660154342651,
      "learning_rate": 1.668212275529349e-05,
      "loss": 0.6189,
      "step": 16670
    },
    {
      "epoch": 1.3376102646351242,
      "grad_norm": 1.4895813465118408,
      "learning_rate": 1.667408201554543e-05,
      "loss": 0.5939,
      "step": 16680
    },
    {
      "epoch": 1.33841218925421,
      "grad_norm": 1.5894863605499268,
      "learning_rate": 1.6666041275797375e-05,
      "loss": 0.5262,
      "step": 16690
    },
    {
      "epoch": 1.3392141138732958,
      "grad_norm": 1.7666300535202026,
      "learning_rate": 1.6658000536049316e-05,
      "loss": 0.5453,
      "step": 16700
    },
    {
      "epoch": 1.3400160384923816,
      "grad_norm": 1.7439264059066772,
      "learning_rate": 1.664995979630126e-05,
      "loss": 0.5496,
      "step": 16710
    },
    {
      "epoch": 1.3408179631114674,
      "grad_norm": 1.9734513759613037,
      "learning_rate": 1.6641919056553204e-05,
      "loss": 0.5434,
      "step": 16720
    },
    {
      "epoch": 1.3416198877305534,
      "grad_norm": 1.7719812393188477,
      "learning_rate": 1.6633878316805145e-05,
      "loss": 0.5662,
      "step": 16730
    },
    {
      "epoch": 1.3424218123496392,
      "grad_norm": 1.790832281112671,
      "learning_rate": 1.662583757705709e-05,
      "loss": 0.5493,
      "step": 16740
    },
    {
      "epoch": 1.343223736968725,
      "grad_norm": 1.978283166885376,
      "learning_rate": 1.661779683730903e-05,
      "loss": 0.5085,
      "step": 16750
    },
    {
      "epoch": 1.3440256615878108,
      "grad_norm": 1.8681247234344482,
      "learning_rate": 1.6609756097560978e-05,
      "loss": 0.5517,
      "step": 16760
    },
    {
      "epoch": 1.3448275862068966,
      "grad_norm": 2.2492597103118896,
      "learning_rate": 1.660171535781292e-05,
      "loss": 0.6134,
      "step": 16770
    },
    {
      "epoch": 1.3456295108259824,
      "grad_norm": 1.895871877670288,
      "learning_rate": 1.6593674618064863e-05,
      "loss": 0.6098,
      "step": 16780
    },
    {
      "epoch": 1.3464314354450682,
      "grad_norm": 1.6332086324691772,
      "learning_rate": 1.6585633878316804e-05,
      "loss": 0.5384,
      "step": 16790
    },
    {
      "epoch": 1.347233360064154,
      "grad_norm": 1.801716685295105,
      "learning_rate": 1.657759313856875e-05,
      "loss": 0.5281,
      "step": 16800
    },
    {
      "epoch": 1.3480352846832397,
      "grad_norm": 2.1925928592681885,
      "learning_rate": 1.6569552398820693e-05,
      "loss": 0.5551,
      "step": 16810
    },
    {
      "epoch": 1.3488372093023255,
      "grad_norm": 1.7288042306900024,
      "learning_rate": 1.6561511659072637e-05,
      "loss": 0.5634,
      "step": 16820
    },
    {
      "epoch": 1.3496391339214113,
      "grad_norm": 1.836517572402954,
      "learning_rate": 1.6553470919324578e-05,
      "loss": 0.5557,
      "step": 16830
    },
    {
      "epoch": 1.3504410585404971,
      "grad_norm": 1.7719001770019531,
      "learning_rate": 1.654543017957652e-05,
      "loss": 0.5813,
      "step": 16840
    },
    {
      "epoch": 1.3512429831595831,
      "grad_norm": 1.5844396352767944,
      "learning_rate": 1.6537389439828463e-05,
      "loss": 0.5304,
      "step": 16850
    },
    {
      "epoch": 1.352044907778669,
      "grad_norm": 1.7485220432281494,
      "learning_rate": 1.6529348700080407e-05,
      "loss": 0.5943,
      "step": 16860
    },
    {
      "epoch": 1.3528468323977547,
      "grad_norm": 1.556199550628662,
      "learning_rate": 1.6521307960332352e-05,
      "loss": 0.548,
      "step": 16870
    },
    {
      "epoch": 1.3536487570168405,
      "grad_norm": 2.3217601776123047,
      "learning_rate": 1.6513267220584293e-05,
      "loss": 0.5893,
      "step": 16880
    },
    {
      "epoch": 1.3544506816359263,
      "grad_norm": 1.69918692111969,
      "learning_rate": 1.6505226480836237e-05,
      "loss": 0.5651,
      "step": 16890
    },
    {
      "epoch": 1.355252606255012,
      "grad_norm": 1.6428701877593994,
      "learning_rate": 1.649718574108818e-05,
      "loss": 0.5389,
      "step": 16900
    },
    {
      "epoch": 1.3560545308740979,
      "grad_norm": 1.5762617588043213,
      "learning_rate": 1.6489145001340126e-05,
      "loss": 0.5334,
      "step": 16910
    },
    {
      "epoch": 1.3568564554931837,
      "grad_norm": 1.7828749418258667,
      "learning_rate": 1.6481104261592066e-05,
      "loss": 0.536,
      "step": 16920
    },
    {
      "epoch": 1.3576583801122695,
      "grad_norm": 1.939171552658081,
      "learning_rate": 1.647306352184401e-05,
      "loss": 0.5249,
      "step": 16930
    },
    {
      "epoch": 1.3584603047313553,
      "grad_norm": 1.6535650491714478,
      "learning_rate": 1.646502278209595e-05,
      "loss": 0.5542,
      "step": 16940
    },
    {
      "epoch": 1.359262229350441,
      "grad_norm": 1.702207326889038,
      "learning_rate": 1.64569820423479e-05,
      "loss": 0.5884,
      "step": 16950
    },
    {
      "epoch": 1.3600641539695268,
      "grad_norm": 1.8096747398376465,
      "learning_rate": 1.644894130259984e-05,
      "loss": 0.5352,
      "step": 16960
    },
    {
      "epoch": 1.3608660785886126,
      "grad_norm": 1.5739221572875977,
      "learning_rate": 1.644090056285178e-05,
      "loss": 0.5107,
      "step": 16970
    },
    {
      "epoch": 1.3616680032076984,
      "grad_norm": 1.9501363039016724,
      "learning_rate": 1.6432859823103725e-05,
      "loss": 0.5782,
      "step": 16980
    },
    {
      "epoch": 1.3624699278267842,
      "grad_norm": 1.8153233528137207,
      "learning_rate": 1.6424819083355666e-05,
      "loss": 0.5406,
      "step": 16990
    },
    {
      "epoch": 1.36327185244587,
      "grad_norm": 1.6594716310501099,
      "learning_rate": 1.6416778343607614e-05,
      "loss": 0.566,
      "step": 17000
    },
    {
      "epoch": 1.3640737770649558,
      "grad_norm": 1.9367343187332153,
      "learning_rate": 1.6408737603859555e-05,
      "loss": 0.5665,
      "step": 17010
    },
    {
      "epoch": 1.3648757016840416,
      "grad_norm": 1.8137543201446533,
      "learning_rate": 1.64006968641115e-05,
      "loss": 0.6044,
      "step": 17020
    },
    {
      "epoch": 1.3656776263031274,
      "grad_norm": 1.7993521690368652,
      "learning_rate": 1.639265612436344e-05,
      "loss": 0.5757,
      "step": 17030
    },
    {
      "epoch": 1.3664795509222132,
      "grad_norm": 1.5517618656158447,
      "learning_rate": 1.6384615384615384e-05,
      "loss": 0.5414,
      "step": 17040
    },
    {
      "epoch": 1.3672814755412992,
      "grad_norm": 1.6987910270690918,
      "learning_rate": 1.637657464486733e-05,
      "loss": 0.5862,
      "step": 17050
    },
    {
      "epoch": 1.368083400160385,
      "grad_norm": 1.465388298034668,
      "learning_rate": 1.6368533905119273e-05,
      "loss": 0.4756,
      "step": 17060
    },
    {
      "epoch": 1.3688853247794708,
      "grad_norm": 1.8599640130996704,
      "learning_rate": 1.6360493165371214e-05,
      "loss": 0.6242,
      "step": 17070
    },
    {
      "epoch": 1.3696872493985566,
      "grad_norm": 2.1033012866973877,
      "learning_rate": 1.6352452425623158e-05,
      "loss": 0.5528,
      "step": 17080
    },
    {
      "epoch": 1.3704891740176424,
      "grad_norm": 1.7480342388153076,
      "learning_rate": 1.6344411685875102e-05,
      "loss": 0.4794,
      "step": 17090
    },
    {
      "epoch": 1.3712910986367282,
      "grad_norm": 1.7799073457717896,
      "learning_rate": 1.6336370946127043e-05,
      "loss": 0.5659,
      "step": 17100
    },
    {
      "epoch": 1.372093023255814,
      "grad_norm": 1.7099018096923828,
      "learning_rate": 1.6328330206378988e-05,
      "loss": 0.5325,
      "step": 17110
    },
    {
      "epoch": 1.3728949478748997,
      "grad_norm": 1.6080752611160278,
      "learning_rate": 1.632028946663093e-05,
      "loss": 0.6064,
      "step": 17120
    },
    {
      "epoch": 1.3736968724939855,
      "grad_norm": 1.870988130569458,
      "learning_rate": 1.6312248726882873e-05,
      "loss": 0.5672,
      "step": 17130
    },
    {
      "epoch": 1.3744987971130713,
      "grad_norm": 1.8656022548675537,
      "learning_rate": 1.6304207987134817e-05,
      "loss": 0.6076,
      "step": 17140
    },
    {
      "epoch": 1.3753007217321571,
      "grad_norm": 1.5292831659317017,
      "learning_rate": 1.629616724738676e-05,
      "loss": 0.5065,
      "step": 17150
    },
    {
      "epoch": 1.376102646351243,
      "grad_norm": 1.8765804767608643,
      "learning_rate": 1.6288126507638702e-05,
      "loss": 0.5779,
      "step": 17160
    },
    {
      "epoch": 1.376904570970329,
      "grad_norm": 1.8966221809387207,
      "learning_rate": 1.6280085767890647e-05,
      "loss": 0.5519,
      "step": 17170
    },
    {
      "epoch": 1.3777064955894147,
      "grad_norm": 1.730830430984497,
      "learning_rate": 1.6272045028142587e-05,
      "loss": 0.5085,
      "step": 17180
    },
    {
      "epoch": 1.3785084202085005,
      "grad_norm": 1.6397640705108643,
      "learning_rate": 1.6264004288394535e-05,
      "loss": 0.6054,
      "step": 17190
    },
    {
      "epoch": 1.3793103448275863,
      "grad_norm": 1.893081784248352,
      "learning_rate": 1.6255963548646476e-05,
      "loss": 0.5937,
      "step": 17200
    },
    {
      "epoch": 1.380112269446672,
      "grad_norm": 2.1447925567626953,
      "learning_rate": 1.624792280889842e-05,
      "loss": 0.5604,
      "step": 17210
    },
    {
      "epoch": 1.3809141940657579,
      "grad_norm": 1.572189450263977,
      "learning_rate": 1.623988206915036e-05,
      "loss": 0.6142,
      "step": 17220
    },
    {
      "epoch": 1.3817161186848437,
      "grad_norm": 1.8974725008010864,
      "learning_rate": 1.6231841329402302e-05,
      "loss": 0.6395,
      "step": 17230
    },
    {
      "epoch": 1.3825180433039295,
      "grad_norm": 1.5503292083740234,
      "learning_rate": 1.622380058965425e-05,
      "loss": 0.5887,
      "step": 17240
    },
    {
      "epoch": 1.3833199679230153,
      "grad_norm": 1.7061468362808228,
      "learning_rate": 1.621575984990619e-05,
      "loss": 0.5088,
      "step": 17250
    },
    {
      "epoch": 1.384121892542101,
      "grad_norm": 1.7686710357666016,
      "learning_rate": 1.6207719110158135e-05,
      "loss": 0.5003,
      "step": 17260
    },
    {
      "epoch": 1.3849238171611868,
      "grad_norm": 2.732383966445923,
      "learning_rate": 1.6199678370410076e-05,
      "loss": 0.5463,
      "step": 17270
    },
    {
      "epoch": 1.3857257417802726,
      "grad_norm": 1.867491602897644,
      "learning_rate": 1.6191637630662024e-05,
      "loss": 0.5648,
      "step": 17280
    },
    {
      "epoch": 1.3865276663993584,
      "grad_norm": 1.749354600906372,
      "learning_rate": 1.6183596890913965e-05,
      "loss": 0.5469,
      "step": 17290
    },
    {
      "epoch": 1.3873295910184442,
      "grad_norm": 1.6025381088256836,
      "learning_rate": 1.617555615116591e-05,
      "loss": 0.5455,
      "step": 17300
    },
    {
      "epoch": 1.38813151563753,
      "grad_norm": 2.2918214797973633,
      "learning_rate": 1.616751541141785e-05,
      "loss": 0.6382,
      "step": 17310
    },
    {
      "epoch": 1.3889334402566158,
      "grad_norm": 1.696401596069336,
      "learning_rate": 1.6159474671669794e-05,
      "loss": 0.5805,
      "step": 17320
    },
    {
      "epoch": 1.3897353648757016,
      "grad_norm": 1.803892970085144,
      "learning_rate": 1.6151433931921738e-05,
      "loss": 0.5363,
      "step": 17330
    },
    {
      "epoch": 1.3905372894947874,
      "grad_norm": 1.6786391735076904,
      "learning_rate": 1.6143393192173683e-05,
      "loss": 0.5445,
      "step": 17340
    },
    {
      "epoch": 1.3913392141138732,
      "grad_norm": 1.803483009338379,
      "learning_rate": 1.6135352452425623e-05,
      "loss": 0.5122,
      "step": 17350
    },
    {
      "epoch": 1.392141138732959,
      "grad_norm": 1.5319149494171143,
      "learning_rate": 1.6127311712677564e-05,
      "loss": 0.5163,
      "step": 17360
    },
    {
      "epoch": 1.392943063352045,
      "grad_norm": 1.810257077217102,
      "learning_rate": 1.611927097292951e-05,
      "loss": 0.5471,
      "step": 17370
    },
    {
      "epoch": 1.3937449879711308,
      "grad_norm": 1.728780746459961,
      "learning_rate": 1.6111230233181453e-05,
      "loss": 0.5225,
      "step": 17380
    },
    {
      "epoch": 1.3945469125902166,
      "grad_norm": 1.7988219261169434,
      "learning_rate": 1.6103189493433397e-05,
      "loss": 0.5009,
      "step": 17390
    },
    {
      "epoch": 1.3953488372093024,
      "grad_norm": 2.0303566455841064,
      "learning_rate": 1.6095148753685338e-05,
      "loss": 0.5487,
      "step": 17400
    },
    {
      "epoch": 1.3961507618283882,
      "grad_norm": 1.866344928741455,
      "learning_rate": 1.6087108013937282e-05,
      "loss": 0.577,
      "step": 17410
    },
    {
      "epoch": 1.396952686447474,
      "grad_norm": 1.7374083995819092,
      "learning_rate": 1.6079067274189223e-05,
      "loss": 0.5929,
      "step": 17420
    },
    {
      "epoch": 1.3977546110665597,
      "grad_norm": 1.9843615293502808,
      "learning_rate": 1.607102653444117e-05,
      "loss": 0.5418,
      "step": 17430
    },
    {
      "epoch": 1.3985565356856455,
      "grad_norm": 1.7515798807144165,
      "learning_rate": 1.6062985794693112e-05,
      "loss": 0.4914,
      "step": 17440
    },
    {
      "epoch": 1.3993584603047313,
      "grad_norm": 1.6655406951904297,
      "learning_rate": 1.6054945054945056e-05,
      "loss": 0.5418,
      "step": 17450
    },
    {
      "epoch": 1.4001603849238171,
      "grad_norm": 1.8470662832260132,
      "learning_rate": 1.6046904315196997e-05,
      "loss": 0.5666,
      "step": 17460
    },
    {
      "epoch": 1.400962309542903,
      "grad_norm": 1.8625187873840332,
      "learning_rate": 1.6038863575448945e-05,
      "loss": 0.5409,
      "step": 17470
    },
    {
      "epoch": 1.4017642341619887,
      "grad_norm": 1.792818307876587,
      "learning_rate": 1.6030822835700886e-05,
      "loss": 0.5765,
      "step": 17480
    },
    {
      "epoch": 1.4025661587810747,
      "grad_norm": 1.9800556898117065,
      "learning_rate": 1.6022782095952827e-05,
      "loss": 0.5667,
      "step": 17490
    },
    {
      "epoch": 1.4033680834001605,
      "grad_norm": 1.7769156694412231,
      "learning_rate": 1.601474135620477e-05,
      "loss": 0.5581,
      "step": 17500
    },
    {
      "epoch": 1.4041700080192463,
      "grad_norm": 1.8604905605316162,
      "learning_rate": 1.6006700616456712e-05,
      "loss": 0.5664,
      "step": 17510
    },
    {
      "epoch": 1.404971932638332,
      "grad_norm": 1.5949031114578247,
      "learning_rate": 1.599865987670866e-05,
      "loss": 0.5138,
      "step": 17520
    },
    {
      "epoch": 1.4057738572574179,
      "grad_norm": 1.748979926109314,
      "learning_rate": 1.59906191369606e-05,
      "loss": 0.6024,
      "step": 17530
    },
    {
      "epoch": 1.4065757818765037,
      "grad_norm": 1.810990571975708,
      "learning_rate": 1.5982578397212545e-05,
      "loss": 0.6315,
      "step": 17540
    },
    {
      "epoch": 1.4073777064955895,
      "grad_norm": 1.6265754699707031,
      "learning_rate": 1.5974537657464486e-05,
      "loss": 0.5427,
      "step": 17550
    },
    {
      "epoch": 1.4081796311146753,
      "grad_norm": 1.8331040143966675,
      "learning_rate": 1.596649691771643e-05,
      "loss": 0.5786,
      "step": 17560
    },
    {
      "epoch": 1.408981555733761,
      "grad_norm": 2.2190866470336914,
      "learning_rate": 1.5958456177968374e-05,
      "loss": 0.556,
      "step": 17570
    },
    {
      "epoch": 1.4097834803528468,
      "grad_norm": 1.988146424293518,
      "learning_rate": 1.595041543822032e-05,
      "loss": 0.6027,
      "step": 17580
    },
    {
      "epoch": 1.4105854049719326,
      "grad_norm": 1.6480964422225952,
      "learning_rate": 1.594237469847226e-05,
      "loss": 0.5351,
      "step": 17590
    },
    {
      "epoch": 1.4113873295910184,
      "grad_norm": 1.7180891036987305,
      "learning_rate": 1.5934333958724204e-05,
      "loss": 0.5228,
      "step": 17600
    },
    {
      "epoch": 1.4121892542101042,
      "grad_norm": 1.6385817527770996,
      "learning_rate": 1.5926293218976148e-05,
      "loss": 0.5719,
      "step": 17610
    },
    {
      "epoch": 1.41299117882919,
      "grad_norm": 1.684295415878296,
      "learning_rate": 1.591825247922809e-05,
      "loss": 0.6071,
      "step": 17620
    },
    {
      "epoch": 1.4137931034482758,
      "grad_norm": 1.777559757232666,
      "learning_rate": 1.5910211739480033e-05,
      "loss": 0.556,
      "step": 17630
    },
    {
      "epoch": 1.4145950280673616,
      "grad_norm": 1.7248594760894775,
      "learning_rate": 1.5902170999731974e-05,
      "loss": 0.5683,
      "step": 17640
    },
    {
      "epoch": 1.4153969526864474,
      "grad_norm": 1.6403809785842896,
      "learning_rate": 1.589413025998392e-05,
      "loss": 0.4928,
      "step": 17650
    },
    {
      "epoch": 1.4161988773055332,
      "grad_norm": 1.5645763874053955,
      "learning_rate": 1.5886089520235863e-05,
      "loss": 0.5448,
      "step": 17660
    },
    {
      "epoch": 1.417000801924619,
      "grad_norm": 1.6505534648895264,
      "learning_rate": 1.5878048780487807e-05,
      "loss": 0.5269,
      "step": 17670
    },
    {
      "epoch": 1.4178027265437048,
      "grad_norm": 1.9687427282333374,
      "learning_rate": 1.5870008040739748e-05,
      "loss": 0.5624,
      "step": 17680
    },
    {
      "epoch": 1.4186046511627908,
      "grad_norm": 1.8092983961105347,
      "learning_rate": 1.5861967300991692e-05,
      "loss": 0.5356,
      "step": 17690
    },
    {
      "epoch": 1.4194065757818766,
      "grad_norm": 1.9982744455337524,
      "learning_rate": 1.5853926561243633e-05,
      "loss": 0.6186,
      "step": 17700
    },
    {
      "epoch": 1.4202085004009624,
      "grad_norm": 1.747591495513916,
      "learning_rate": 1.584588582149558e-05,
      "loss": 0.5803,
      "step": 17710
    },
    {
      "epoch": 1.4210104250200482,
      "grad_norm": 1.839277744293213,
      "learning_rate": 1.583784508174752e-05,
      "loss": 0.5307,
      "step": 17720
    },
    {
      "epoch": 1.421812349639134,
      "grad_norm": 1.8862087726593018,
      "learning_rate": 1.5829804341999466e-05,
      "loss": 0.5831,
      "step": 17730
    },
    {
      "epoch": 1.4226142742582197,
      "grad_norm": 2.2490174770355225,
      "learning_rate": 1.5821763602251407e-05,
      "loss": 0.5928,
      "step": 17740
    },
    {
      "epoch": 1.4234161988773055,
      "grad_norm": 1.9499157667160034,
      "learning_rate": 1.5813722862503348e-05,
      "loss": 0.525,
      "step": 17750
    },
    {
      "epoch": 1.4242181234963913,
      "grad_norm": 1.664179801940918,
      "learning_rate": 1.5805682122755295e-05,
      "loss": 0.5872,
      "step": 17760
    },
    {
      "epoch": 1.4250200481154771,
      "grad_norm": 1.9034911394119263,
      "learning_rate": 1.5797641383007236e-05,
      "loss": 0.5132,
      "step": 17770
    },
    {
      "epoch": 1.425821972734563,
      "grad_norm": 1.7707159519195557,
      "learning_rate": 1.578960064325918e-05,
      "loss": 0.5501,
      "step": 17780
    },
    {
      "epoch": 1.4266238973536487,
      "grad_norm": 1.8554307222366333,
      "learning_rate": 1.578155990351112e-05,
      "loss": 0.5673,
      "step": 17790
    },
    {
      "epoch": 1.4274258219727345,
      "grad_norm": 2.171970844268799,
      "learning_rate": 1.577351916376307e-05,
      "loss": 0.5902,
      "step": 17800
    },
    {
      "epoch": 1.4282277465918205,
      "grad_norm": 1.7406326532363892,
      "learning_rate": 1.576547842401501e-05,
      "loss": 0.5814,
      "step": 17810
    },
    {
      "epoch": 1.4290296712109063,
      "grad_norm": 1.65554940700531,
      "learning_rate": 1.5757437684266954e-05,
      "loss": 0.5948,
      "step": 17820
    },
    {
      "epoch": 1.429831595829992,
      "grad_norm": 1.6587175130844116,
      "learning_rate": 1.5749396944518895e-05,
      "loss": 0.5468,
      "step": 17830
    },
    {
      "epoch": 1.4306335204490779,
      "grad_norm": 1.7895294427871704,
      "learning_rate": 1.574135620477084e-05,
      "loss": 0.5602,
      "step": 17840
    },
    {
      "epoch": 1.4314354450681637,
      "grad_norm": 1.741060733795166,
      "learning_rate": 1.5733315465022784e-05,
      "loss": 0.6101,
      "step": 17850
    },
    {
      "epoch": 1.4322373696872495,
      "grad_norm": 2.323279857635498,
      "learning_rate": 1.5725274725274728e-05,
      "loss": 0.5323,
      "step": 17860
    },
    {
      "epoch": 1.4330392943063353,
      "grad_norm": 1.9461909532546997,
      "learning_rate": 1.571723398552667e-05,
      "loss": 0.5335,
      "step": 17870
    },
    {
      "epoch": 1.433841218925421,
      "grad_norm": 1.7217167615890503,
      "learning_rate": 1.570919324577861e-05,
      "loss": 0.5935,
      "step": 17880
    },
    {
      "epoch": 1.4346431435445068,
      "grad_norm": 2.2144486904144287,
      "learning_rate": 1.5701152506030554e-05,
      "loss": 0.5983,
      "step": 17890
    },
    {
      "epoch": 1.4354450681635926,
      "grad_norm": 2.1278858184814453,
      "learning_rate": 1.56931117662825e-05,
      "loss": 0.5872,
      "step": 17900
    },
    {
      "epoch": 1.4362469927826784,
      "grad_norm": 1.830055832862854,
      "learning_rate": 1.5685071026534443e-05,
      "loss": 0.493,
      "step": 17910
    },
    {
      "epoch": 1.4370489174017642,
      "grad_norm": 2.2449793815612793,
      "learning_rate": 1.5677030286786384e-05,
      "loss": 0.6768,
      "step": 17920
    },
    {
      "epoch": 1.43785084202085,
      "grad_norm": 1.8573256731033325,
      "learning_rate": 1.5668989547038328e-05,
      "loss": 0.6247,
      "step": 17930
    },
    {
      "epoch": 1.4386527666399358,
      "grad_norm": 1.6927967071533203,
      "learning_rate": 1.566094880729027e-05,
      "loss": 0.5782,
      "step": 17940
    },
    {
      "epoch": 1.4394546912590216,
      "grad_norm": 1.720261812210083,
      "learning_rate": 1.5652908067542217e-05,
      "loss": 0.6012,
      "step": 17950
    },
    {
      "epoch": 1.4402566158781074,
      "grad_norm": 1.8963627815246582,
      "learning_rate": 1.5644867327794157e-05,
      "loss": 0.5567,
      "step": 17960
    },
    {
      "epoch": 1.4410585404971932,
      "grad_norm": 1.6870465278625488,
      "learning_rate": 1.5636826588046102e-05,
      "loss": 0.5193,
      "step": 17970
    },
    {
      "epoch": 1.441860465116279,
      "grad_norm": 1.920630931854248,
      "learning_rate": 1.5628785848298043e-05,
      "loss": 0.5175,
      "step": 17980
    },
    {
      "epoch": 1.4426623897353648,
      "grad_norm": 1.838130235671997,
      "learning_rate": 1.562074510854999e-05,
      "loss": 0.5487,
      "step": 17990
    },
    {
      "epoch": 1.4434643143544506,
      "grad_norm": 1.7519629001617432,
      "learning_rate": 1.561270436880193e-05,
      "loss": 0.5658,
      "step": 18000
    },
    {
      "epoch": 1.4442662389735366,
      "grad_norm": 1.869016408920288,
      "learning_rate": 1.5604663629053872e-05,
      "loss": 0.5413,
      "step": 18010
    },
    {
      "epoch": 1.4450681635926224,
      "grad_norm": 1.9222664833068848,
      "learning_rate": 1.5596622889305816e-05,
      "loss": 0.5638,
      "step": 18020
    },
    {
      "epoch": 1.4458700882117081,
      "grad_norm": 1.6986256837844849,
      "learning_rate": 1.5588582149557757e-05,
      "loss": 0.5859,
      "step": 18030
    },
    {
      "epoch": 1.446672012830794,
      "grad_norm": 1.7938405275344849,
      "learning_rate": 1.5580541409809705e-05,
      "loss": 0.546,
      "step": 18040
    },
    {
      "epoch": 1.4474739374498797,
      "grad_norm": 2.1411478519439697,
      "learning_rate": 1.5572500670061646e-05,
      "loss": 0.557,
      "step": 18050
    },
    {
      "epoch": 1.4482758620689655,
      "grad_norm": 2.3645925521850586,
      "learning_rate": 1.556445993031359e-05,
      "loss": 0.5681,
      "step": 18060
    },
    {
      "epoch": 1.4490777866880513,
      "grad_norm": 1.7711310386657715,
      "learning_rate": 1.555641919056553e-05,
      "loss": 0.5978,
      "step": 18070
    },
    {
      "epoch": 1.449879711307137,
      "grad_norm": 1.7324740886688232,
      "learning_rate": 1.5548378450817475e-05,
      "loss": 0.6399,
      "step": 18080
    },
    {
      "epoch": 1.450681635926223,
      "grad_norm": 1.5743356943130493,
      "learning_rate": 1.554033771106942e-05,
      "loss": 0.5296,
      "step": 18090
    },
    {
      "epoch": 1.4514835605453087,
      "grad_norm": 1.734108328819275,
      "learning_rate": 1.5532296971321364e-05,
      "loss": 0.5649,
      "step": 18100
    },
    {
      "epoch": 1.4522854851643945,
      "grad_norm": 1.9883419275283813,
      "learning_rate": 1.5524256231573305e-05,
      "loss": 0.546,
      "step": 18110
    },
    {
      "epoch": 1.4530874097834803,
      "grad_norm": 1.978906273841858,
      "learning_rate": 1.551621549182525e-05,
      "loss": 0.5029,
      "step": 18120
    },
    {
      "epoch": 1.4538893344025663,
      "grad_norm": 1.5262153148651123,
      "learning_rate": 1.550817475207719e-05,
      "loss": 0.5489,
      "step": 18130
    },
    {
      "epoch": 1.454691259021652,
      "grad_norm": 1.4623135328292847,
      "learning_rate": 1.5500134012329134e-05,
      "loss": 0.5254,
      "step": 18140
    },
    {
      "epoch": 1.4554931836407379,
      "grad_norm": 1.6533042192459106,
      "learning_rate": 1.549209327258108e-05,
      "loss": 0.602,
      "step": 18150
    },
    {
      "epoch": 1.4562951082598237,
      "grad_norm": 1.71220862865448,
      "learning_rate": 1.548405253283302e-05,
      "loss": 0.5053,
      "step": 18160
    },
    {
      "epoch": 1.4570970328789095,
      "grad_norm": 1.8302576541900635,
      "learning_rate": 1.5476011793084964e-05,
      "loss": 0.5782,
      "step": 18170
    },
    {
      "epoch": 1.4578989574979953,
      "grad_norm": 1.9091464281082153,
      "learning_rate": 1.5467971053336908e-05,
      "loss": 0.5906,
      "step": 18180
    },
    {
      "epoch": 1.458700882117081,
      "grad_norm": 1.8667856454849243,
      "learning_rate": 1.5459930313588852e-05,
      "loss": 0.5515,
      "step": 18190
    },
    {
      "epoch": 1.4595028067361668,
      "grad_norm": 1.7811558246612549,
      "learning_rate": 1.5451889573840793e-05,
      "loss": 0.5993,
      "step": 18200
    },
    {
      "epoch": 1.4603047313552526,
      "grad_norm": 1.7443211078643799,
      "learning_rate": 1.5443848834092738e-05,
      "loss": 0.5751,
      "step": 18210
    },
    {
      "epoch": 1.4611066559743384,
      "grad_norm": 1.770862340927124,
      "learning_rate": 1.543580809434468e-05,
      "loss": 0.5222,
      "step": 18220
    },
    {
      "epoch": 1.4619085805934242,
      "grad_norm": 2.1305954456329346,
      "learning_rate": 1.5427767354596626e-05,
      "loss": 0.5423,
      "step": 18230
    },
    {
      "epoch": 1.46271050521251,
      "grad_norm": 1.6042931079864502,
      "learning_rate": 1.5419726614848567e-05,
      "loss": 0.5512,
      "step": 18240
    },
    {
      "epoch": 1.4635124298315958,
      "grad_norm": 1.708251714706421,
      "learning_rate": 1.5411685875100508e-05,
      "loss": 0.5396,
      "step": 18250
    },
    {
      "epoch": 1.4643143544506816,
      "grad_norm": 1.9006571769714355,
      "learning_rate": 1.5403645135352452e-05,
      "loss": 0.5115,
      "step": 18260
    },
    {
      "epoch": 1.4651162790697674,
      "grad_norm": 1.704581618309021,
      "learning_rate": 1.5395604395604393e-05,
      "loss": 0.5579,
      "step": 18270
    },
    {
      "epoch": 1.4659182036888532,
      "grad_norm": 1.810393214225769,
      "learning_rate": 1.538756365585634e-05,
      "loss": 0.5567,
      "step": 18280
    },
    {
      "epoch": 1.466720128307939,
      "grad_norm": 1.797836422920227,
      "learning_rate": 1.5379522916108282e-05,
      "loss": 0.4965,
      "step": 18290
    },
    {
      "epoch": 1.4675220529270248,
      "grad_norm": Infinity,
      "learning_rate": 1.5371482176360226e-05,
      "loss": 0.567,
      "step": 18300
    },
    {
      "epoch": 1.4683239775461105,
      "grad_norm": 1.5596892833709717,
      "learning_rate": 1.5364245510586976e-05,
      "loss": 0.5917,
      "step": 18310
    },
    {
      "epoch": 1.4691259021651963,
      "grad_norm": 2.0027291774749756,
      "learning_rate": 1.5356204770838916e-05,
      "loss": 0.6181,
      "step": 18320
    },
    {
      "epoch": 1.4699278267842824,
      "grad_norm": 1.8507031202316284,
      "learning_rate": 1.534816403109086e-05,
      "loss": 0.5422,
      "step": 18330
    },
    {
      "epoch": 1.4707297514033681,
      "grad_norm": 1.7624883651733398,
      "learning_rate": 1.5340123291342805e-05,
      "loss": 0.6462,
      "step": 18340
    },
    {
      "epoch": 1.471531676022454,
      "grad_norm": 1.5242382287979126,
      "learning_rate": 1.5332082551594746e-05,
      "loss": 0.5347,
      "step": 18350
    },
    {
      "epoch": 1.4723336006415397,
      "grad_norm": 1.9160208702087402,
      "learning_rate": 1.532404181184669e-05,
      "loss": 0.5756,
      "step": 18360
    },
    {
      "epoch": 1.4731355252606255,
      "grad_norm": 1.5737402439117432,
      "learning_rate": 1.531600107209863e-05,
      "loss": 0.6138,
      "step": 18370
    },
    {
      "epoch": 1.4739374498797113,
      "grad_norm": 1.8235671520233154,
      "learning_rate": 1.530796033235058e-05,
      "loss": 0.5063,
      "step": 18380
    },
    {
      "epoch": 1.474739374498797,
      "grad_norm": 1.6488758325576782,
      "learning_rate": 1.529991959260252e-05,
      "loss": 0.5616,
      "step": 18390
    },
    {
      "epoch": 1.475541299117883,
      "grad_norm": 1.853745937347412,
      "learning_rate": 1.5291878852854464e-05,
      "loss": 0.5563,
      "step": 18400
    },
    {
      "epoch": 1.4763432237369687,
      "grad_norm": 1.9570696353912354,
      "learning_rate": 1.5283838113106405e-05,
      "loss": 0.574,
      "step": 18410
    },
    {
      "epoch": 1.4771451483560545,
      "grad_norm": 1.812512993812561,
      "learning_rate": 1.527579737335835e-05,
      "loss": 0.5559,
      "step": 18420
    },
    {
      "epoch": 1.4779470729751403,
      "grad_norm": 1.7216646671295166,
      "learning_rate": 1.5267756633610293e-05,
      "loss": 0.5437,
      "step": 18430
    },
    {
      "epoch": 1.478748997594226,
      "grad_norm": 1.493937611579895,
      "learning_rate": 1.5259715893862238e-05,
      "loss": 0.5638,
      "step": 18440
    },
    {
      "epoch": 1.479550922213312,
      "grad_norm": 1.5795953273773193,
      "learning_rate": 1.5251675154114179e-05,
      "loss": 0.5745,
      "step": 18450
    },
    {
      "epoch": 1.4803528468323979,
      "grad_norm": 2.0081071853637695,
      "learning_rate": 1.5243634414366121e-05,
      "loss": 0.5623,
      "step": 18460
    },
    {
      "epoch": 1.4811547714514837,
      "grad_norm": 2.0807130336761475,
      "learning_rate": 1.5235593674618067e-05,
      "loss": 0.6141,
      "step": 18470
    },
    {
      "epoch": 1.4819566960705695,
      "grad_norm": 1.8204236030578613,
      "learning_rate": 1.522755293487001e-05,
      "loss": 0.532,
      "step": 18480
    },
    {
      "epoch": 1.4827586206896552,
      "grad_norm": 1.770451307296753,
      "learning_rate": 1.5219512195121952e-05,
      "loss": 0.5595,
      "step": 18490
    },
    {
      "epoch": 1.483560545308741,
      "grad_norm": 1.7809247970581055,
      "learning_rate": 1.5211471455373895e-05,
      "loss": 0.5357,
      "step": 18500
    },
    {
      "epoch": 1.4843624699278268,
      "grad_norm": 2.0177719593048096,
      "learning_rate": 1.5203430715625836e-05,
      "loss": 0.5633,
      "step": 18510
    },
    {
      "epoch": 1.4851643945469126,
      "grad_norm": 1.8682632446289062,
      "learning_rate": 1.5195389975877782e-05,
      "loss": 0.4967,
      "step": 18520
    },
    {
      "epoch": 1.4859663191659984,
      "grad_norm": 2.0735220909118652,
      "learning_rate": 1.5187349236129725e-05,
      "loss": 0.5404,
      "step": 18530
    },
    {
      "epoch": 1.4867682437850842,
      "grad_norm": 1.8869303464889526,
      "learning_rate": 1.5179308496381667e-05,
      "loss": 0.5535,
      "step": 18540
    },
    {
      "epoch": 1.48757016840417,
      "grad_norm": 1.852864384651184,
      "learning_rate": 1.517126775663361e-05,
      "loss": 0.5976,
      "step": 18550
    },
    {
      "epoch": 1.4883720930232558,
      "grad_norm": 1.7002038955688477,
      "learning_rate": 1.5163227016885552e-05,
      "loss": 0.5787,
      "step": 18560
    },
    {
      "epoch": 1.4891740176423416,
      "grad_norm": 1.8958873748779297,
      "learning_rate": 1.5155186277137498e-05,
      "loss": 0.5856,
      "step": 18570
    },
    {
      "epoch": 1.4899759422614274,
      "grad_norm": 2.218254804611206,
      "learning_rate": 1.5147145537389441e-05,
      "loss": 0.5663,
      "step": 18580
    },
    {
      "epoch": 1.4907778668805132,
      "grad_norm": 1.875641107559204,
      "learning_rate": 1.5139104797641383e-05,
      "loss": 0.5743,
      "step": 18590
    },
    {
      "epoch": 1.491579791499599,
      "grad_norm": 1.7688612937927246,
      "learning_rate": 1.5131064057893326e-05,
      "loss": 0.5281,
      "step": 18600
    },
    {
      "epoch": 1.4923817161186848,
      "grad_norm": 1.8817594051361084,
      "learning_rate": 1.5123023318145269e-05,
      "loss": 0.6105,
      "step": 18610
    },
    {
      "epoch": 1.4931836407377705,
      "grad_norm": 1.5924252271652222,
      "learning_rate": 1.5114982578397215e-05,
      "loss": 0.5536,
      "step": 18620
    },
    {
      "epoch": 1.4939855653568563,
      "grad_norm": 1.8328248262405396,
      "learning_rate": 1.5106941838649157e-05,
      "loss": 0.5571,
      "step": 18630
    },
    {
      "epoch": 1.4947874899759421,
      "grad_norm": 1.5665395259857178,
      "learning_rate": 1.5098901098901098e-05,
      "loss": 0.5641,
      "step": 18640
    },
    {
      "epoch": 1.4955894145950281,
      "grad_norm": 1.9509060382843018,
      "learning_rate": 1.509086035915304e-05,
      "loss": 0.5255,
      "step": 18650
    },
    {
      "epoch": 1.496391339214114,
      "grad_norm": 1.6845371723175049,
      "learning_rate": 1.5082819619404987e-05,
      "loss": 0.5614,
      "step": 18660
    },
    {
      "epoch": 1.4971932638331997,
      "grad_norm": 1.7451767921447754,
      "learning_rate": 1.507477887965693e-05,
      "loss": 0.5661,
      "step": 18670
    },
    {
      "epoch": 1.4979951884522855,
      "grad_norm": 1.9341520071029663,
      "learning_rate": 1.5066738139908872e-05,
      "loss": 0.4819,
      "step": 18680
    },
    {
      "epoch": 1.4987971130713713,
      "grad_norm": 2.0436959266662598,
      "learning_rate": 1.5058697400160815e-05,
      "loss": 0.5674,
      "step": 18690
    },
    {
      "epoch": 1.499599037690457,
      "grad_norm": 1.8406418561935425,
      "learning_rate": 1.5050656660412757e-05,
      "loss": 0.5853,
      "step": 18700
    },
    {
      "epoch": 1.500400962309543,
      "grad_norm": 1.9605108499526978,
      "learning_rate": 1.5042615920664703e-05,
      "loss": 0.5808,
      "step": 18710
    },
    {
      "epoch": 1.5012028869286287,
      "grad_norm": 1.793606162071228,
      "learning_rate": 1.5034575180916646e-05,
      "loss": 0.5685,
      "step": 18720
    },
    {
      "epoch": 1.5020048115477145,
      "grad_norm": 1.8550715446472168,
      "learning_rate": 1.5026534441168588e-05,
      "loss": 0.5888,
      "step": 18730
    },
    {
      "epoch": 1.5028067361668003,
      "grad_norm": 1.652657389640808,
      "learning_rate": 1.5018493701420531e-05,
      "loss": 0.5482,
      "step": 18740
    },
    {
      "epoch": 1.5036086607858863,
      "grad_norm": 2.1358680725097656,
      "learning_rate": 1.5010452961672473e-05,
      "loss": 0.6124,
      "step": 18750
    },
    {
      "epoch": 1.504410585404972,
      "grad_norm": 1.499570369720459,
      "learning_rate": 1.500241222192442e-05,
      "loss": 0.5034,
      "step": 18760
    },
    {
      "epoch": 1.5052125100240579,
      "grad_norm": 1.9313615560531616,
      "learning_rate": 1.499437148217636e-05,
      "loss": 0.5089,
      "step": 18770
    },
    {
      "epoch": 1.5060144346431437,
      "grad_norm": 2.0275001525878906,
      "learning_rate": 1.4986330742428303e-05,
      "loss": 0.5698,
      "step": 18780
    },
    {
      "epoch": 1.5068163592622295,
      "grad_norm": 1.78191339969635,
      "learning_rate": 1.4978290002680247e-05,
      "loss": 0.5631,
      "step": 18790
    },
    {
      "epoch": 1.5076182838813152,
      "grad_norm": 1.7984371185302734,
      "learning_rate": 1.497024926293219e-05,
      "loss": 0.5607,
      "step": 18800
    },
    {
      "epoch": 1.508420208500401,
      "grad_norm": 1.8524668216705322,
      "learning_rate": 1.4962208523184132e-05,
      "loss": 0.5013,
      "step": 18810
    },
    {
      "epoch": 1.5092221331194868,
      "grad_norm": 1.8981412649154663,
      "learning_rate": 1.4954167783436077e-05,
      "loss": 0.5687,
      "step": 18820
    },
    {
      "epoch": 1.5100240577385726,
      "grad_norm": 1.6731733083724976,
      "learning_rate": 1.494612704368802e-05,
      "loss": 0.5694,
      "step": 18830
    },
    {
      "epoch": 1.5108259823576584,
      "grad_norm": 1.6366217136383057,
      "learning_rate": 1.4938086303939964e-05,
      "loss": 0.5927,
      "step": 18840
    },
    {
      "epoch": 1.5116279069767442,
      "grad_norm": 2.0391836166381836,
      "learning_rate": 1.4930045564191906e-05,
      "loss": 0.5529,
      "step": 18850
    },
    {
      "epoch": 1.51242983159583,
      "grad_norm": 1.618597388267517,
      "learning_rate": 1.4922004824443849e-05,
      "loss": 0.5138,
      "step": 18860
    },
    {
      "epoch": 1.5132317562149158,
      "grad_norm": 1.9646990299224854,
      "learning_rate": 1.4913964084695793e-05,
      "loss": 0.5944,
      "step": 18870
    },
    {
      "epoch": 1.5140336808340016,
      "grad_norm": 2.3988962173461914,
      "learning_rate": 1.4905923344947736e-05,
      "loss": 0.5212,
      "step": 18880
    },
    {
      "epoch": 1.5148356054530874,
      "grad_norm": 1.9424309730529785,
      "learning_rate": 1.489788260519968e-05,
      "loss": 0.5504,
      "step": 18890
    },
    {
      "epoch": 1.5156375300721732,
      "grad_norm": 1.9764176607131958,
      "learning_rate": 1.4889841865451621e-05,
      "loss": 0.5462,
      "step": 18900
    },
    {
      "epoch": 1.516439454691259,
      "grad_norm": 1.9277586936950684,
      "learning_rate": 1.4881801125703565e-05,
      "loss": 0.6532,
      "step": 18910
    },
    {
      "epoch": 1.5172413793103448,
      "grad_norm": 1.8456828594207764,
      "learning_rate": 1.4873760385955508e-05,
      "loss": 0.5466,
      "step": 18920
    },
    {
      "epoch": 1.5180433039294305,
      "grad_norm": 1.8480384349822998,
      "learning_rate": 1.486571964620745e-05,
      "loss": 0.6063,
      "step": 18930
    },
    {
      "epoch": 1.5188452285485163,
      "grad_norm": 1.9605411291122437,
      "learning_rate": 1.4857678906459395e-05,
      "loss": 0.472,
      "step": 18940
    },
    {
      "epoch": 1.5196471531676021,
      "grad_norm": 1.8105729818344116,
      "learning_rate": 1.4849638166711337e-05,
      "loss": 0.5991,
      "step": 18950
    },
    {
      "epoch": 1.520449077786688,
      "grad_norm": 1.9322431087493896,
      "learning_rate": 1.4841597426963282e-05,
      "loss": 0.5706,
      "step": 18960
    },
    {
      "epoch": 1.5212510024057737,
      "grad_norm": 1.6282775402069092,
      "learning_rate": 1.4833556687215224e-05,
      "loss": 0.5738,
      "step": 18970
    },
    {
      "epoch": 1.5220529270248595,
      "grad_norm": 1.7786065340042114,
      "learning_rate": 1.4825515947467168e-05,
      "loss": 0.5695,
      "step": 18980
    },
    {
      "epoch": 1.5228548516439455,
      "grad_norm": 1.6372755765914917,
      "learning_rate": 1.4817475207719111e-05,
      "loss": 0.5707,
      "step": 18990
    },
    {
      "epoch": 1.5236567762630313,
      "grad_norm": 1.8798247575759888,
      "learning_rate": 1.4809434467971054e-05,
      "loss": 0.4994,
      "step": 19000
    },
    {
      "epoch": 1.524458700882117,
      "grad_norm": 1.9691815376281738,
      "learning_rate": 1.4801393728222998e-05,
      "loss": 0.5298,
      "step": 19010
    },
    {
      "epoch": 1.525260625501203,
      "grad_norm": 1.7573927640914917,
      "learning_rate": 1.4793352988474939e-05,
      "loss": 0.4945,
      "step": 19020
    },
    {
      "epoch": 1.5260625501202887,
      "grad_norm": 1.890764832496643,
      "learning_rate": 1.4785312248726883e-05,
      "loss": 0.566,
      "step": 19030
    },
    {
      "epoch": 1.5268644747393745,
      "grad_norm": 1.8208510875701904,
      "learning_rate": 1.4777271508978826e-05,
      "loss": 0.5929,
      "step": 19040
    },
    {
      "epoch": 1.5276663993584603,
      "grad_norm": 1.5746797323226929,
      "learning_rate": 1.476923076923077e-05,
      "loss": 0.577,
      "step": 19050
    },
    {
      "epoch": 1.528468323977546,
      "grad_norm": 1.9931353330612183,
      "learning_rate": 1.4761190029482713e-05,
      "loss": 0.51,
      "step": 19060
    },
    {
      "epoch": 1.529270248596632,
      "grad_norm": 1.811923623085022,
      "learning_rate": 1.4753149289734655e-05,
      "loss": 0.5947,
      "step": 19070
    },
    {
      "epoch": 1.5300721732157179,
      "grad_norm": 2.014923572540283,
      "learning_rate": 1.47451085499866e-05,
      "loss": 0.5954,
      "step": 19080
    },
    {
      "epoch": 1.5308740978348037,
      "grad_norm": 2.088006019592285,
      "learning_rate": 1.4737067810238542e-05,
      "loss": 0.5592,
      "step": 19090
    },
    {
      "epoch": 1.5316760224538895,
      "grad_norm": 1.5300703048706055,
      "learning_rate": 1.4729027070490486e-05,
      "loss": 0.5685,
      "step": 19100
    },
    {
      "epoch": 1.5324779470729752,
      "grad_norm": 1.626927137374878,
      "learning_rate": 1.4720986330742429e-05,
      "loss": 0.5236,
      "step": 19110
    },
    {
      "epoch": 1.533279871692061,
      "grad_norm": 1.9288644790649414,
      "learning_rate": 1.4712945590994372e-05,
      "loss": 0.6061,
      "step": 19120
    },
    {
      "epoch": 1.5340817963111468,
      "grad_norm": 1.733763337135315,
      "learning_rate": 1.4704904851246316e-05,
      "loss": 0.5932,
      "step": 19130
    },
    {
      "epoch": 1.5348837209302326,
      "grad_norm": 1.815740942955017,
      "learning_rate": 1.4696864111498258e-05,
      "loss": 0.5534,
      "step": 19140
    },
    {
      "epoch": 1.5356856455493184,
      "grad_norm": 1.5228068828582764,
      "learning_rate": 1.4688823371750201e-05,
      "loss": 0.54,
      "step": 19150
    },
    {
      "epoch": 1.5364875701684042,
      "grad_norm": 1.570468544960022,
      "learning_rate": 1.4680782632002144e-05,
      "loss": 0.5656,
      "step": 19160
    },
    {
      "epoch": 1.53728949478749,
      "grad_norm": 1.9900052547454834,
      "learning_rate": 1.4672741892254088e-05,
      "loss": 0.5721,
      "step": 19170
    },
    {
      "epoch": 1.5380914194065758,
      "grad_norm": 2.0562591552734375,
      "learning_rate": 1.466470115250603e-05,
      "loss": 0.5345,
      "step": 19180
    },
    {
      "epoch": 1.5388933440256616,
      "grad_norm": 1.6379294395446777,
      "learning_rate": 1.4656660412757973e-05,
      "loss": 0.5619,
      "step": 19190
    },
    {
      "epoch": 1.5396952686447474,
      "grad_norm": 1.6916812658309937,
      "learning_rate": 1.4648619673009917e-05,
      "loss": 0.548,
      "step": 19200
    },
    {
      "epoch": 1.5404971932638332,
      "grad_norm": 1.5129369497299194,
      "learning_rate": 1.464057893326186e-05,
      "loss": 0.5196,
      "step": 19210
    },
    {
      "epoch": 1.541299117882919,
      "grad_norm": 1.8589812517166138,
      "learning_rate": 1.4632538193513804e-05,
      "loss": 0.5255,
      "step": 19220
    },
    {
      "epoch": 1.5421010425020047,
      "grad_norm": 1.8556663990020752,
      "learning_rate": 1.4624497453765747e-05,
      "loss": 0.5183,
      "step": 19230
    },
    {
      "epoch": 1.5429029671210905,
      "grad_norm": 1.8909834623336792,
      "learning_rate": 1.4616456714017691e-05,
      "loss": 0.5667,
      "step": 19240
    },
    {
      "epoch": 1.5437048917401763,
      "grad_norm": 1.6128392219543457,
      "learning_rate": 1.4608415974269634e-05,
      "loss": 0.5404,
      "step": 19250
    },
    {
      "epoch": 1.5445068163592621,
      "grad_norm": 1.6218957901000977,
      "learning_rate": 1.4600375234521576e-05,
      "loss": 0.5774,
      "step": 19260
    },
    {
      "epoch": 1.545308740978348,
      "grad_norm": 1.7478382587432861,
      "learning_rate": 1.459233449477352e-05,
      "loss": 0.6022,
      "step": 19270
    },
    {
      "epoch": 1.5461106655974337,
      "grad_norm": 1.752071738243103,
      "learning_rate": 1.4584293755025462e-05,
      "loss": 0.54,
      "step": 19280
    },
    {
      "epoch": 1.5469125902165195,
      "grad_norm": 1.7999322414398193,
      "learning_rate": 1.4576253015277406e-05,
      "loss": 0.5694,
      "step": 19290
    },
    {
      "epoch": 1.5477145148356053,
      "grad_norm": 1.705605387687683,
      "learning_rate": 1.4568212275529348e-05,
      "loss": 0.5661,
      "step": 19300
    },
    {
      "epoch": 1.5485164394546913,
      "grad_norm": 1.759629487991333,
      "learning_rate": 1.4560171535781291e-05,
      "loss": 0.5276,
      "step": 19310
    },
    {
      "epoch": 1.549318364073777,
      "grad_norm": 1.6910074949264526,
      "learning_rate": 1.4552130796033235e-05,
      "loss": 0.5518,
      "step": 19320
    },
    {
      "epoch": 1.550120288692863,
      "grad_norm": 1.9588323831558228,
      "learning_rate": 1.4544090056285178e-05,
      "loss": 0.5308,
      "step": 19330
    },
    {
      "epoch": 1.5509222133119487,
      "grad_norm": 2.3659908771514893,
      "learning_rate": 1.4536049316537122e-05,
      "loss": 0.518,
      "step": 19340
    },
    {
      "epoch": 1.5517241379310345,
      "grad_norm": 1.6908549070358276,
      "learning_rate": 1.4528008576789065e-05,
      "loss": 0.5664,
      "step": 19350
    },
    {
      "epoch": 1.5525260625501203,
      "grad_norm": 1.7318400144577026,
      "learning_rate": 1.4519967837041009e-05,
      "loss": 0.5337,
      "step": 19360
    },
    {
      "epoch": 1.553327987169206,
      "grad_norm": 1.9815210103988647,
      "learning_rate": 1.4511927097292952e-05,
      "loss": 0.5645,
      "step": 19370
    },
    {
      "epoch": 1.5541299117882919,
      "grad_norm": 2.111180305480957,
      "learning_rate": 1.4503886357544894e-05,
      "loss": 0.5315,
      "step": 19380
    },
    {
      "epoch": 1.5549318364073779,
      "grad_norm": 1.949674367904663,
      "learning_rate": 1.4495845617796839e-05,
      "loss": 0.5711,
      "step": 19390
    },
    {
      "epoch": 1.5557337610264637,
      "grad_norm": 1.6781928539276123,
      "learning_rate": 1.4487804878048781e-05,
      "loss": 0.5892,
      "step": 19400
    },
    {
      "epoch": 1.5565356856455494,
      "grad_norm": 2.02919340133667,
      "learning_rate": 1.4479764138300724e-05,
      "loss": 0.6314,
      "step": 19410
    },
    {
      "epoch": 1.5573376102646352,
      "grad_norm": 1.7976516485214233,
      "learning_rate": 1.4471723398552666e-05,
      "loss": 0.4854,
      "step": 19420
    },
    {
      "epoch": 1.558139534883721,
      "grad_norm": 1.9443100690841675,
      "learning_rate": 1.446368265880461e-05,
      "loss": 0.5888,
      "step": 19430
    },
    {
      "epoch": 1.5589414595028068,
      "grad_norm": 1.6599667072296143,
      "learning_rate": 1.4455641919056553e-05,
      "loss": 0.5445,
      "step": 19440
    },
    {
      "epoch": 1.5597433841218926,
      "grad_norm": 1.800796627998352,
      "learning_rate": 1.4447601179308496e-05,
      "loss": 0.5105,
      "step": 19450
    },
    {
      "epoch": 1.5605453087409784,
      "grad_norm": 1.6410341262817383,
      "learning_rate": 1.443956043956044e-05,
      "loss": 0.5579,
      "step": 19460
    },
    {
      "epoch": 1.5613472333600642,
      "grad_norm": 1.8522037267684937,
      "learning_rate": 1.4431519699812383e-05,
      "loss": 0.5322,
      "step": 19470
    },
    {
      "epoch": 1.56214915797915,
      "grad_norm": 1.782522439956665,
      "learning_rate": 1.4423478960064327e-05,
      "loss": 0.5482,
      "step": 19480
    },
    {
      "epoch": 1.5629510825982358,
      "grad_norm": 1.773241639137268,
      "learning_rate": 1.441543822031627e-05,
      "loss": 0.5063,
      "step": 19490
    },
    {
      "epoch": 1.5637530072173216,
      "grad_norm": 1.5475519895553589,
      "learning_rate": 1.4407397480568212e-05,
      "loss": 0.55,
      "step": 19500
    },
    {
      "epoch": 1.5645549318364074,
      "grad_norm": 1.5400981903076172,
      "learning_rate": 1.4399356740820157e-05,
      "loss": 0.5757,
      "step": 19510
    },
    {
      "epoch": 1.5653568564554932,
      "grad_norm": 1.741898775100708,
      "learning_rate": 1.4391316001072099e-05,
      "loss": 0.5564,
      "step": 19520
    },
    {
      "epoch": 1.566158781074579,
      "grad_norm": 1.605961561203003,
      "learning_rate": 1.4383275261324043e-05,
      "loss": 0.5557,
      "step": 19530
    },
    {
      "epoch": 1.5669607056936647,
      "grad_norm": 1.9754023551940918,
      "learning_rate": 1.4375234521575984e-05,
      "loss": 0.5497,
      "step": 19540
    },
    {
      "epoch": 1.5677626303127505,
      "grad_norm": 1.7884447574615479,
      "learning_rate": 1.4367193781827929e-05,
      "loss": 0.5717,
      "step": 19550
    },
    {
      "epoch": 1.5685645549318363,
      "grad_norm": 1.9308630228042603,
      "learning_rate": 1.4359153042079871e-05,
      "loss": 0.5106,
      "step": 19560
    },
    {
      "epoch": 1.5693664795509221,
      "grad_norm": 1.6533279418945312,
      "learning_rate": 1.4351112302331814e-05,
      "loss": 0.5223,
      "step": 19570
    },
    {
      "epoch": 1.570168404170008,
      "grad_norm": 1.7793503999710083,
      "learning_rate": 1.4343071562583758e-05,
      "loss": 0.4875,
      "step": 19580
    },
    {
      "epoch": 1.5709703287890937,
      "grad_norm": 1.6551131010055542,
      "learning_rate": 1.43350308228357e-05,
      "loss": 0.5496,
      "step": 19590
    },
    {
      "epoch": 1.5717722534081795,
      "grad_norm": 1.726157546043396,
      "learning_rate": 1.4326990083087645e-05,
      "loss": 0.5383,
      "step": 19600
    },
    {
      "epoch": 1.5725741780272653,
      "grad_norm": 1.6884007453918457,
      "learning_rate": 1.4318949343339588e-05,
      "loss": 0.5385,
      "step": 19610
    },
    {
      "epoch": 1.573376102646351,
      "grad_norm": 1.7275362014770508,
      "learning_rate": 1.4310908603591532e-05,
      "loss": 0.5796,
      "step": 19620
    },
    {
      "epoch": 1.574178027265437,
      "grad_norm": 1.8200727701187134,
      "learning_rate": 1.4302867863843474e-05,
      "loss": 0.5473,
      "step": 19630
    },
    {
      "epoch": 1.5749799518845229,
      "grad_norm": 1.6816778182983398,
      "learning_rate": 1.4294827124095417e-05,
      "loss": 0.5234,
      "step": 19640
    },
    {
      "epoch": 1.5757818765036087,
      "grad_norm": 1.908403992652893,
      "learning_rate": 1.4286786384347361e-05,
      "loss": 0.5697,
      "step": 19650
    },
    {
      "epoch": 1.5765838011226945,
      "grad_norm": 1.7691357135772705,
      "learning_rate": 1.4278745644599304e-05,
      "loss": 0.5307,
      "step": 19660
    },
    {
      "epoch": 1.5773857257417803,
      "grad_norm": 2.1238157749176025,
      "learning_rate": 1.4270704904851247e-05,
      "loss": 0.5496,
      "step": 19670
    },
    {
      "epoch": 1.578187650360866,
      "grad_norm": 1.8260691165924072,
      "learning_rate": 1.4262664165103189e-05,
      "loss": 0.5129,
      "step": 19680
    },
    {
      "epoch": 1.5789895749799518,
      "grad_norm": 1.7055244445800781,
      "learning_rate": 1.4254623425355133e-05,
      "loss": 0.5722,
      "step": 19690
    },
    {
      "epoch": 1.5797914995990376,
      "grad_norm": 1.8605543375015259,
      "learning_rate": 1.4246582685607076e-05,
      "loss": 0.6079,
      "step": 19700
    },
    {
      "epoch": 1.5805934242181237,
      "grad_norm": 1.965238094329834,
      "learning_rate": 1.4238541945859019e-05,
      "loss": 0.4762,
      "step": 19710
    },
    {
      "epoch": 1.5813953488372094,
      "grad_norm": 2.1202969551086426,
      "learning_rate": 1.4230501206110963e-05,
      "loss": 0.5193,
      "step": 19720
    },
    {
      "epoch": 1.5821972734562952,
      "grad_norm": 1.793108344078064,
      "learning_rate": 1.4222460466362905e-05,
      "loss": 0.6263,
      "step": 19730
    },
    {
      "epoch": 1.582999198075381,
      "grad_norm": 1.7319872379302979,
      "learning_rate": 1.421441972661485e-05,
      "loss": 0.558,
      "step": 19740
    },
    {
      "epoch": 1.5838011226944668,
      "grad_norm": 1.7471120357513428,
      "learning_rate": 1.4206378986866792e-05,
      "loss": 0.5336,
      "step": 19750
    },
    {
      "epoch": 1.5846030473135526,
      "grad_norm": 1.755840539932251,
      "learning_rate": 1.4198338247118735e-05,
      "loss": 0.5532,
      "step": 19760
    },
    {
      "epoch": 1.5854049719326384,
      "grad_norm": 1.87366783618927,
      "learning_rate": 1.419029750737068e-05,
      "loss": 0.5618,
      "step": 19770
    },
    {
      "epoch": 1.5862068965517242,
      "grad_norm": 1.7868196964263916,
      "learning_rate": 1.4182256767622622e-05,
      "loss": 0.5575,
      "step": 19780
    },
    {
      "epoch": 1.58700882117081,
      "grad_norm": 1.8254764080047607,
      "learning_rate": 1.4174216027874564e-05,
      "loss": 0.6162,
      "step": 19790
    },
    {
      "epoch": 1.5878107457898958,
      "grad_norm": 1.7838095426559448,
      "learning_rate": 1.4166175288126507e-05,
      "loss": 0.578,
      "step": 19800
    },
    {
      "epoch": 1.5886126704089816,
      "grad_norm": 1.7693790197372437,
      "learning_rate": 1.4158134548378451e-05,
      "loss": 0.5289,
      "step": 19810
    },
    {
      "epoch": 1.5894145950280674,
      "grad_norm": 1.7712973356246948,
      "learning_rate": 1.4150093808630394e-05,
      "loss": 0.4599,
      "step": 19820
    },
    {
      "epoch": 1.5902165196471532,
      "grad_norm": 1.845293641090393,
      "learning_rate": 1.4142053068882337e-05,
      "loss": 0.4997,
      "step": 19830
    },
    {
      "epoch": 1.591018444266239,
      "grad_norm": 1.7563631534576416,
      "learning_rate": 1.413401232913428e-05,
      "loss": 0.5694,
      "step": 19840
    },
    {
      "epoch": 1.5918203688853247,
      "grad_norm": 1.6806961297988892,
      "learning_rate": 1.4125971589386223e-05,
      "loss": 0.5433,
      "step": 19850
    },
    {
      "epoch": 1.5926222935044105,
      "grad_norm": 1.9555268287658691,
      "learning_rate": 1.4117930849638168e-05,
      "loss": 0.5255,
      "step": 19860
    },
    {
      "epoch": 1.5934242181234963,
      "grad_norm": 2.1152184009552,
      "learning_rate": 1.410989010989011e-05,
      "loss": 0.5745,
      "step": 19870
    },
    {
      "epoch": 1.5942261427425821,
      "grad_norm": 1.9120545387268066,
      "learning_rate": 1.4101849370142055e-05,
      "loss": 0.5454,
      "step": 19880
    },
    {
      "epoch": 1.595028067361668,
      "grad_norm": 1.587185263633728,
      "learning_rate": 1.4093808630393997e-05,
      "loss": 0.5857,
      "step": 19890
    },
    {
      "epoch": 1.5958299919807537,
      "grad_norm": 1.5732477903366089,
      "learning_rate": 1.408576789064594e-05,
      "loss": 0.6232,
      "step": 19900
    },
    {
      "epoch": 1.5966319165998395,
      "grad_norm": 1.9845151901245117,
      "learning_rate": 1.4077727150897884e-05,
      "loss": 0.5261,
      "step": 19910
    },
    {
      "epoch": 1.5974338412189253,
      "grad_norm": 1.6763349771499634,
      "learning_rate": 1.4069686411149825e-05,
      "loss": 0.5572,
      "step": 19920
    },
    {
      "epoch": 1.598235765838011,
      "grad_norm": 1.7262822389602661,
      "learning_rate": 1.406164567140177e-05,
      "loss": 0.6168,
      "step": 19930
    },
    {
      "epoch": 1.5990376904570969,
      "grad_norm": 1.9612699747085571,
      "learning_rate": 1.4053604931653712e-05,
      "loss": 0.5143,
      "step": 19940
    },
    {
      "epoch": 1.5998396150761829,
      "grad_norm": 1.6414127349853516,
      "learning_rate": 1.4045564191905654e-05,
      "loss": 0.5703,
      "step": 19950
    },
    {
      "epoch": 1.6006415396952687,
      "grad_norm": 1.7772727012634277,
      "learning_rate": 1.4037523452157599e-05,
      "loss": 0.5571,
      "step": 19960
    },
    {
      "epoch": 1.6014434643143545,
      "grad_norm": 1.6387100219726562,
      "learning_rate": 1.4029482712409541e-05,
      "loss": 0.5545,
      "step": 19970
    },
    {
      "epoch": 1.6022453889334403,
      "grad_norm": 1.7815767526626587,
      "learning_rate": 1.4021441972661486e-05,
      "loss": 0.6207,
      "step": 19980
    },
    {
      "epoch": 1.603047313552526,
      "grad_norm": 1.8571739196777344,
      "learning_rate": 1.4013401232913428e-05,
      "loss": 0.5899,
      "step": 19990
    },
    {
      "epoch": 1.6038492381716118,
      "grad_norm": 1.8513559103012085,
      "learning_rate": 1.4005360493165373e-05,
      "loss": 0.5251,
      "step": 20000
    },
    {
      "epoch": 1.6046511627906976,
      "grad_norm": 1.7238383293151855,
      "learning_rate": 1.3997319753417315e-05,
      "loss": 0.5827,
      "step": 20010
    },
    {
      "epoch": 1.6054530874097834,
      "grad_norm": 1.5279035568237305,
      "learning_rate": 1.3989279013669258e-05,
      "loss": 0.489,
      "step": 20020
    },
    {
      "epoch": 1.6062550120288694,
      "grad_norm": 1.8649245500564575,
      "learning_rate": 1.3981238273921202e-05,
      "loss": 0.5751,
      "step": 20030
    },
    {
      "epoch": 1.6070569366479552,
      "grad_norm": 1.921940565109253,
      "learning_rate": 1.3973197534173145e-05,
      "loss": 0.5516,
      "step": 20040
    },
    {
      "epoch": 1.607858861267041,
      "grad_norm": 1.7550688982009888,
      "learning_rate": 1.3965156794425087e-05,
      "loss": 0.5597,
      "step": 20050
    },
    {
      "epoch": 1.6086607858861268,
      "grad_norm": 1.974790334701538,
      "learning_rate": 1.395711605467703e-05,
      "loss": 0.5881,
      "step": 20060
    },
    {
      "epoch": 1.6094627105052126,
      "grad_norm": 1.8000190258026123,
      "learning_rate": 1.3949075314928974e-05,
      "loss": 0.5001,
      "step": 20070
    },
    {
      "epoch": 1.6102646351242984,
      "grad_norm": 1.696603775024414,
      "learning_rate": 1.3941034575180917e-05,
      "loss": 0.5093,
      "step": 20080
    },
    {
      "epoch": 1.6110665597433842,
      "grad_norm": 1.7557902336120605,
      "learning_rate": 1.393299383543286e-05,
      "loss": 0.5581,
      "step": 20090
    },
    {
      "epoch": 1.61186848436247,
      "grad_norm": 1.7178616523742676,
      "learning_rate": 1.3924953095684804e-05,
      "loss": 0.5579,
      "step": 20100
    },
    {
      "epoch": 1.6126704089815558,
      "grad_norm": 2.1431548595428467,
      "learning_rate": 1.3916912355936746e-05,
      "loss": 0.5812,
      "step": 20110
    },
    {
      "epoch": 1.6134723336006416,
      "grad_norm": 2.0150094032287598,
      "learning_rate": 1.390887161618869e-05,
      "loss": 0.5499,
      "step": 20120
    },
    {
      "epoch": 1.6142742582197274,
      "grad_norm": 1.4967933893203735,
      "learning_rate": 1.3900830876440633e-05,
      "loss": 0.5792,
      "step": 20130
    },
    {
      "epoch": 1.6150761828388132,
      "grad_norm": 2.0069711208343506,
      "learning_rate": 1.3892790136692576e-05,
      "loss": 0.5287,
      "step": 20140
    },
    {
      "epoch": 1.615878107457899,
      "grad_norm": 1.7595497369766235,
      "learning_rate": 1.388474939694452e-05,
      "loss": 0.5948,
      "step": 20150
    },
    {
      "epoch": 1.6166800320769847,
      "grad_norm": 1.7004380226135254,
      "learning_rate": 1.3876708657196463e-05,
      "loss": 0.5446,
      "step": 20160
    },
    {
      "epoch": 1.6174819566960705,
      "grad_norm": 1.7506674528121948,
      "learning_rate": 1.3868667917448407e-05,
      "loss": 0.5933,
      "step": 20170
    },
    {
      "epoch": 1.6182838813151563,
      "grad_norm": 1.7511199712753296,
      "learning_rate": 1.3860627177700348e-05,
      "loss": 0.5742,
      "step": 20180
    },
    {
      "epoch": 1.6190858059342421,
      "grad_norm": 1.7937086820602417,
      "learning_rate": 1.3852586437952292e-05,
      "loss": 0.5077,
      "step": 20190
    },
    {
      "epoch": 1.619887730553328,
      "grad_norm": 1.7114288806915283,
      "learning_rate": 1.3844545698204235e-05,
      "loss": 0.5901,
      "step": 20200
    },
    {
      "epoch": 1.6206896551724137,
      "grad_norm": 1.884314775466919,
      "learning_rate": 1.3836504958456177e-05,
      "loss": 0.5112,
      "step": 20210
    },
    {
      "epoch": 1.6214915797914995,
      "grad_norm": 2.099026679992676,
      "learning_rate": 1.3828464218708121e-05,
      "loss": 0.5176,
      "step": 20220
    },
    {
      "epoch": 1.6222935044105853,
      "grad_norm": 1.8124595880508423,
      "learning_rate": 1.3820423478960064e-05,
      "loss": 0.5241,
      "step": 20230
    },
    {
      "epoch": 1.623095429029671,
      "grad_norm": 1.9809205532073975,
      "learning_rate": 1.3812382739212008e-05,
      "loss": 0.6136,
      "step": 20240
    },
    {
      "epoch": 1.6238973536487569,
      "grad_norm": 1.7859759330749512,
      "learning_rate": 1.3804341999463951e-05,
      "loss": 0.4949,
      "step": 20250
    },
    {
      "epoch": 1.6246992782678427,
      "grad_norm": 1.617207407951355,
      "learning_rate": 1.3796301259715895e-05,
      "loss": 0.5299,
      "step": 20260
    },
    {
      "epoch": 1.6255012028869287,
      "grad_norm": 1.7868711948394775,
      "learning_rate": 1.3788260519967838e-05,
      "loss": 0.5714,
      "step": 20270
    },
    {
      "epoch": 1.6263031275060145,
      "grad_norm": 1.7915539741516113,
      "learning_rate": 1.378021978021978e-05,
      "loss": 0.5527,
      "step": 20280
    },
    {
      "epoch": 1.6271050521251003,
      "grad_norm": 1.7937514781951904,
      "learning_rate": 1.3772179040471725e-05,
      "loss": 0.5003,
      "step": 20290
    },
    {
      "epoch": 1.627906976744186,
      "grad_norm": 1.8836044073104858,
      "learning_rate": 1.3764138300723667e-05,
      "loss": 0.525,
      "step": 20300
    },
    {
      "epoch": 1.6287089013632718,
      "grad_norm": 1.8833017349243164,
      "learning_rate": 1.375609756097561e-05,
      "loss": 0.6218,
      "step": 20310
    },
    {
      "epoch": 1.6295108259823576,
      "grad_norm": 1.6581908464431763,
      "learning_rate": 1.3748056821227553e-05,
      "loss": 0.5669,
      "step": 20320
    },
    {
      "epoch": 1.6303127506014434,
      "grad_norm": 2.258521795272827,
      "learning_rate": 1.3740016081479497e-05,
      "loss": 0.5514,
      "step": 20330
    },
    {
      "epoch": 1.6311146752205292,
      "grad_norm": 1.7346166372299194,
      "learning_rate": 1.373197534173144e-05,
      "loss": 0.5886,
      "step": 20340
    },
    {
      "epoch": 1.6319165998396152,
      "grad_norm": 1.9415700435638428,
      "learning_rate": 1.3723934601983382e-05,
      "loss": 0.5906,
      "step": 20350
    },
    {
      "epoch": 1.632718524458701,
      "grad_norm": 2.268770217895508,
      "learning_rate": 1.3715893862235326e-05,
      "loss": 0.5663,
      "step": 20360
    },
    {
      "epoch": 1.6335204490777868,
      "grad_norm": 1.8097882270812988,
      "learning_rate": 1.3707853122487269e-05,
      "loss": 0.5484,
      "step": 20370
    },
    {
      "epoch": 1.6343223736968726,
      "grad_norm": 2.2048118114471436,
      "learning_rate": 1.3699812382739213e-05,
      "loss": 0.5841,
      "step": 20380
    },
    {
      "epoch": 1.6351242983159584,
      "grad_norm": 1.962799072265625,
      "learning_rate": 1.3691771642991156e-05,
      "loss": 0.5688,
      "step": 20390
    },
    {
      "epoch": 1.6359262229350442,
      "grad_norm": 2.4771130084991455,
      "learning_rate": 1.3683730903243098e-05,
      "loss": 0.5555,
      "step": 20400
    },
    {
      "epoch": 1.63672814755413,
      "grad_norm": 1.762997031211853,
      "learning_rate": 1.3676494237469848e-05,
      "loss": 0.4997,
      "step": 20410
    },
    {
      "epoch": 1.6375300721732158,
      "grad_norm": 1.6036546230316162,
      "learning_rate": 1.366845349772179e-05,
      "loss": 0.5794,
      "step": 20420
    },
    {
      "epoch": 1.6383319967923016,
      "grad_norm": 1.672245740890503,
      "learning_rate": 1.3660412757973733e-05,
      "loss": 0.5512,
      "step": 20430
    },
    {
      "epoch": 1.6391339214113874,
      "grad_norm": 1.6288890838623047,
      "learning_rate": 1.3652372018225677e-05,
      "loss": 0.5361,
      "step": 20440
    },
    {
      "epoch": 1.6399358460304732,
      "grad_norm": 1.6155860424041748,
      "learning_rate": 1.364433127847762e-05,
      "loss": 0.5886,
      "step": 20450
    },
    {
      "epoch": 1.640737770649559,
      "grad_norm": 1.6439629793167114,
      "learning_rate": 1.3636290538729564e-05,
      "loss": 0.5733,
      "step": 20460
    },
    {
      "epoch": 1.6415396952686447,
      "grad_norm": 1.9489631652832031,
      "learning_rate": 1.3628249798981507e-05,
      "loss": 0.5945,
      "step": 20470
    },
    {
      "epoch": 1.6423416198877305,
      "grad_norm": 1.9868547916412354,
      "learning_rate": 1.3620209059233451e-05,
      "loss": 0.6106,
      "step": 20480
    },
    {
      "epoch": 1.6431435445068163,
      "grad_norm": 1.824113130569458,
      "learning_rate": 1.3612168319485394e-05,
      "loss": 0.5364,
      "step": 20490
    },
    {
      "epoch": 1.6439454691259021,
      "grad_norm": 1.7673872709274292,
      "learning_rate": 1.3604127579737335e-05,
      "loss": 0.5912,
      "step": 20500
    },
    {
      "epoch": 1.644747393744988,
      "grad_norm": 1.6960939168930054,
      "learning_rate": 1.3596086839989279e-05,
      "loss": 0.537,
      "step": 20510
    },
    {
      "epoch": 1.6455493183640737,
      "grad_norm": 1.8106426000595093,
      "learning_rate": 1.3588046100241222e-05,
      "loss": 0.525,
      "step": 20520
    },
    {
      "epoch": 1.6463512429831595,
      "grad_norm": 1.9877054691314697,
      "learning_rate": 1.3580005360493166e-05,
      "loss": 0.5575,
      "step": 20530
    },
    {
      "epoch": 1.6471531676022453,
      "grad_norm": 1.654228687286377,
      "learning_rate": 1.3571964620745108e-05,
      "loss": 0.6122,
      "step": 20540
    },
    {
      "epoch": 1.647955092221331,
      "grad_norm": 1.6760952472686768,
      "learning_rate": 1.3563923880997053e-05,
      "loss": 0.5386,
      "step": 20550
    },
    {
      "epoch": 1.6487570168404169,
      "grad_norm": 2.1979780197143555,
      "learning_rate": 1.3555883141248995e-05,
      "loss": 0.6055,
      "step": 20560
    },
    {
      "epoch": 1.6495589414595027,
      "grad_norm": 1.8475878238677979,
      "learning_rate": 1.3548646475475743e-05,
      "loss": 0.6099,
      "step": 20570
    },
    {
      "epoch": 1.6503608660785885,
      "grad_norm": 1.7631269693374634,
      "learning_rate": 1.3540605735727687e-05,
      "loss": 0.5675,
      "step": 20580
    },
    {
      "epoch": 1.6511627906976745,
      "grad_norm": 1.728656530380249,
      "learning_rate": 1.353256499597963e-05,
      "loss": 0.5246,
      "step": 20590
    },
    {
      "epoch": 1.6519647153167603,
      "grad_norm": 1.8572367429733276,
      "learning_rate": 1.3524524256231573e-05,
      "loss": 0.52,
      "step": 20600
    },
    {
      "epoch": 1.652766639935846,
      "grad_norm": 1.808702826499939,
      "learning_rate": 1.3516483516483517e-05,
      "loss": 0.5598,
      "step": 20610
    },
    {
      "epoch": 1.6535685645549318,
      "grad_norm": 1.9107286930084229,
      "learning_rate": 1.350844277673546e-05,
      "loss": 0.5234,
      "step": 20620
    },
    {
      "epoch": 1.6543704891740176,
      "grad_norm": 1.6695871353149414,
      "learning_rate": 1.3500402036987404e-05,
      "loss": 0.5379,
      "step": 20630
    },
    {
      "epoch": 1.6551724137931034,
      "grad_norm": 1.6246610879898071,
      "learning_rate": 1.3492361297239346e-05,
      "loss": 0.5978,
      "step": 20640
    },
    {
      "epoch": 1.6559743384121892,
      "grad_norm": 1.8036906719207764,
      "learning_rate": 1.348432055749129e-05,
      "loss": 0.5302,
      "step": 20650
    },
    {
      "epoch": 1.656776263031275,
      "grad_norm": 1.885579228401184,
      "learning_rate": 1.3476279817743233e-05,
      "loss": 0.5494,
      "step": 20660
    },
    {
      "epoch": 1.657578187650361,
      "grad_norm": 2.16865611076355,
      "learning_rate": 1.3468239077995176e-05,
      "loss": 0.5555,
      "step": 20670
    },
    {
      "epoch": 1.6583801122694468,
      "grad_norm": 1.6957788467407227,
      "learning_rate": 1.346019833824712e-05,
      "loss": 0.5874,
      "step": 20680
    },
    {
      "epoch": 1.6591820368885326,
      "grad_norm": 1.8303285837173462,
      "learning_rate": 1.3452157598499061e-05,
      "loss": 0.5826,
      "step": 20690
    },
    {
      "epoch": 1.6599839615076184,
      "grad_norm": 1.960626482963562,
      "learning_rate": 1.3444116858751005e-05,
      "loss": 0.5762,
      "step": 20700
    },
    {
      "epoch": 1.6607858861267042,
      "grad_norm": 1.692848801612854,
      "learning_rate": 1.3436076119002948e-05,
      "loss": 0.4946,
      "step": 20710
    },
    {
      "epoch": 1.66158781074579,
      "grad_norm": 1.7603241205215454,
      "learning_rate": 1.342803537925489e-05,
      "loss": 0.5382,
      "step": 20720
    },
    {
      "epoch": 1.6623897353648758,
      "grad_norm": 1.723981261253357,
      "learning_rate": 1.3419994639506835e-05,
      "loss": 0.5732,
      "step": 20730
    },
    {
      "epoch": 1.6631916599839616,
      "grad_norm": 1.7115271091461182,
      "learning_rate": 1.3411953899758777e-05,
      "loss": 0.5566,
      "step": 20740
    },
    {
      "epoch": 1.6639935846030474,
      "grad_norm": 1.9019807577133179,
      "learning_rate": 1.3403913160010722e-05,
      "loss": 0.5336,
      "step": 20750
    },
    {
      "epoch": 1.6647955092221332,
      "grad_norm": 1.8269747495651245,
      "learning_rate": 1.3395872420262664e-05,
      "loss": 0.497,
      "step": 20760
    },
    {
      "epoch": 1.665597433841219,
      "grad_norm": 1.8627867698669434,
      "learning_rate": 1.3387831680514609e-05,
      "loss": 0.521,
      "step": 20770
    },
    {
      "epoch": 1.6663993584603047,
      "grad_norm": 1.956720232963562,
      "learning_rate": 1.3379790940766551e-05,
      "loss": 0.561,
      "step": 20780
    },
    {
      "epoch": 1.6672012830793905,
      "grad_norm": 1.851728081703186,
      "learning_rate": 1.3371750201018494e-05,
      "loss": 0.585,
      "step": 20790
    },
    {
      "epoch": 1.6680032076984763,
      "grad_norm": 1.9222171306610107,
      "learning_rate": 1.3363709461270438e-05,
      "loss": 0.5309,
      "step": 20800
    },
    {
      "epoch": 1.6688051323175621,
      "grad_norm": 1.9479514360427856,
      "learning_rate": 1.335566872152238e-05,
      "loss": 0.5281,
      "step": 20810
    },
    {
      "epoch": 1.669607056936648,
      "grad_norm": 2.0539278984069824,
      "learning_rate": 1.3347627981774323e-05,
      "loss": 0.5423,
      "step": 20820
    },
    {
      "epoch": 1.6704089815557337,
      "grad_norm": 1.3512417078018188,
      "learning_rate": 1.3339587242026266e-05,
      "loss": 0.5484,
      "step": 20830
    },
    {
      "epoch": 1.6712109061748195,
      "grad_norm": 1.5877962112426758,
      "learning_rate": 1.333154650227821e-05,
      "loss": 0.5293,
      "step": 20840
    },
    {
      "epoch": 1.6720128307939053,
      "grad_norm": 1.7412521839141846,
      "learning_rate": 1.3323505762530153e-05,
      "loss": 0.6062,
      "step": 20850
    },
    {
      "epoch": 1.672814755412991,
      "grad_norm": 1.5940366983413696,
      "learning_rate": 1.3315465022782095e-05,
      "loss": 0.5407,
      "step": 20860
    },
    {
      "epoch": 1.6736166800320769,
      "grad_norm": 2.3751235008239746,
      "learning_rate": 1.330742428303404e-05,
      "loss": 0.5722,
      "step": 20870
    },
    {
      "epoch": 1.6744186046511627,
      "grad_norm": 1.7879643440246582,
      "learning_rate": 1.3299383543285982e-05,
      "loss": 0.6444,
      "step": 20880
    },
    {
      "epoch": 1.6752205292702484,
      "grad_norm": 1.6256097555160522,
      "learning_rate": 1.3291342803537927e-05,
      "loss": 0.5182,
      "step": 20890
    },
    {
      "epoch": 1.6760224538893342,
      "grad_norm": 1.67836332321167,
      "learning_rate": 1.3283302063789869e-05,
      "loss": 0.5874,
      "step": 20900
    },
    {
      "epoch": 1.6768243785084203,
      "grad_norm": 1.8537349700927734,
      "learning_rate": 1.3275261324041812e-05,
      "loss": 0.4771,
      "step": 20910
    },
    {
      "epoch": 1.677626303127506,
      "grad_norm": 1.6890478134155273,
      "learning_rate": 1.3267220584293756e-05,
      "loss": 0.55,
      "step": 20920
    },
    {
      "epoch": 1.6784282277465918,
      "grad_norm": 1.965450406074524,
      "learning_rate": 1.3259179844545699e-05,
      "loss": 0.5503,
      "step": 20930
    },
    {
      "epoch": 1.6792301523656776,
      "grad_norm": 1.7711411714553833,
      "learning_rate": 1.3251139104797643e-05,
      "loss": 0.562,
      "step": 20940
    },
    {
      "epoch": 1.6800320769847634,
      "grad_norm": 1.9033645391464233,
      "learning_rate": 1.3243098365049584e-05,
      "loss": 0.5566,
      "step": 20950
    },
    {
      "epoch": 1.6808340016038492,
      "grad_norm": 1.712197184562683,
      "learning_rate": 1.3235057625301528e-05,
      "loss": 0.556,
      "step": 20960
    },
    {
      "epoch": 1.681635926222935,
      "grad_norm": 1.7827134132385254,
      "learning_rate": 1.322701688555347e-05,
      "loss": 0.5968,
      "step": 20970
    },
    {
      "epoch": 1.6824378508420208,
      "grad_norm": 1.7360903024673462,
      "learning_rate": 1.3218976145805413e-05,
      "loss": 0.5945,
      "step": 20980
    },
    {
      "epoch": 1.6832397754611068,
      "grad_norm": 1.7188552618026733,
      "learning_rate": 1.3210935406057358e-05,
      "loss": 0.5018,
      "step": 20990
    },
    {
      "epoch": 1.6840417000801926,
      "grad_norm": 1.6502325534820557,
      "learning_rate": 1.32028946663093e-05,
      "loss": 0.5615,
      "step": 21000
    },
    {
      "epoch": 1.6848436246992784,
      "grad_norm": 1.8242971897125244,
      "learning_rate": 1.3194853926561244e-05,
      "loss": 0.5019,
      "step": 21010
    },
    {
      "epoch": 1.6856455493183642,
      "grad_norm": 1.9149481058120728,
      "learning_rate": 1.3186813186813187e-05,
      "loss": 0.5722,
      "step": 21020
    },
    {
      "epoch": 1.68644747393745,
      "grad_norm": 1.603631615638733,
      "learning_rate": 1.3178772447065131e-05,
      "loss": 0.53,
      "step": 21030
    },
    {
      "epoch": 1.6872493985565358,
      "grad_norm": 1.6241095066070557,
      "learning_rate": 1.3170731707317074e-05,
      "loss": 0.4872,
      "step": 21040
    },
    {
      "epoch": 1.6880513231756216,
      "grad_norm": 1.9064475297927856,
      "learning_rate": 1.3162690967569017e-05,
      "loss": 0.5586,
      "step": 21050
    },
    {
      "epoch": 1.6888532477947074,
      "grad_norm": 1.7656867504119873,
      "learning_rate": 1.315465022782096e-05,
      "loss": 0.586,
      "step": 21060
    },
    {
      "epoch": 1.6896551724137931,
      "grad_norm": 1.682755947113037,
      "learning_rate": 1.3146609488072903e-05,
      "loss": 0.5689,
      "step": 21070
    },
    {
      "epoch": 1.690457097032879,
      "grad_norm": 1.4866763353347778,
      "learning_rate": 1.3138568748324846e-05,
      "loss": 0.5451,
      "step": 21080
    },
    {
      "epoch": 1.6912590216519647,
      "grad_norm": 1.56606125831604,
      "learning_rate": 1.3130528008576789e-05,
      "loss": 0.5331,
      "step": 21090
    },
    {
      "epoch": 1.6920609462710505,
      "grad_norm": 1.7488147020339966,
      "learning_rate": 1.3122487268828733e-05,
      "loss": 0.5457,
      "step": 21100
    },
    {
      "epoch": 1.6928628708901363,
      "grad_norm": 2.022814989089966,
      "learning_rate": 1.3114446529080676e-05,
      "loss": 0.5289,
      "step": 21110
    },
    {
      "epoch": 1.693664795509222,
      "grad_norm": 1.870336890220642,
      "learning_rate": 1.3106405789332618e-05,
      "loss": 0.5053,
      "step": 21120
    },
    {
      "epoch": 1.694466720128308,
      "grad_norm": 1.7527499198913574,
      "learning_rate": 1.3098365049584562e-05,
      "loss": 0.5001,
      "step": 21130
    },
    {
      "epoch": 1.6952686447473937,
      "grad_norm": 1.981483817100525,
      "learning_rate": 1.3090324309836505e-05,
      "loss": 0.525,
      "step": 21140
    },
    {
      "epoch": 1.6960705693664795,
      "grad_norm": 1.7979270219802856,
      "learning_rate": 1.308228357008845e-05,
      "loss": 0.5088,
      "step": 21150
    },
    {
      "epoch": 1.6968724939855653,
      "grad_norm": 1.5456526279449463,
      "learning_rate": 1.3074242830340392e-05,
      "loss": 0.5668,
      "step": 21160
    },
    {
      "epoch": 1.697674418604651,
      "grad_norm": 2.035219669342041,
      "learning_rate": 1.3066202090592334e-05,
      "loss": 0.5224,
      "step": 21170
    },
    {
      "epoch": 1.6984763432237369,
      "grad_norm": 1.736063838005066,
      "learning_rate": 1.3058161350844279e-05,
      "loss": 0.5374,
      "step": 21180
    },
    {
      "epoch": 1.6992782678428227,
      "grad_norm": 1.8416204452514648,
      "learning_rate": 1.3050120611096221e-05,
      "loss": 0.5916,
      "step": 21190
    },
    {
      "epoch": 1.7000801924619084,
      "grad_norm": 1.9343353509902954,
      "learning_rate": 1.3042079871348166e-05,
      "loss": 0.5288,
      "step": 21200
    },
    {
      "epoch": 1.7008821170809942,
      "grad_norm": 1.82772958278656,
      "learning_rate": 1.3034039131600107e-05,
      "loss": 0.5504,
      "step": 21210
    },
    {
      "epoch": 1.70168404170008,
      "grad_norm": 1.8838995695114136,
      "learning_rate": 1.3025998391852051e-05,
      "loss": 0.534,
      "step": 21220
    },
    {
      "epoch": 1.702485966319166,
      "grad_norm": 1.5935605764389038,
      "learning_rate": 1.3017957652103993e-05,
      "loss": 0.5225,
      "step": 21230
    },
    {
      "epoch": 1.7032878909382518,
      "grad_norm": 1.8902052640914917,
      "learning_rate": 1.3009916912355936e-05,
      "loss": 0.5315,
      "step": 21240
    },
    {
      "epoch": 1.7040898155573376,
      "grad_norm": 2.0150692462921143,
      "learning_rate": 1.300187617260788e-05,
      "loss": 0.5131,
      "step": 21250
    },
    {
      "epoch": 1.7048917401764234,
      "grad_norm": 2.1778171062469482,
      "learning_rate": 1.2993835432859823e-05,
      "loss": 0.521,
      "step": 21260
    },
    {
      "epoch": 1.7056936647955092,
      "grad_norm": 1.7866989374160767,
      "learning_rate": 1.2985794693111767e-05,
      "loss": 0.5286,
      "step": 21270
    },
    {
      "epoch": 1.706495589414595,
      "grad_norm": 1.9732539653778076,
      "learning_rate": 1.297775395336371e-05,
      "loss": 0.5503,
      "step": 21280
    },
    {
      "epoch": 1.7072975140336808,
      "grad_norm": 1.983016014099121,
      "learning_rate": 1.2969713213615654e-05,
      "loss": 0.5833,
      "step": 21290
    },
    {
      "epoch": 1.7080994386527666,
      "grad_norm": 1.8787506818771362,
      "learning_rate": 1.2961672473867597e-05,
      "loss": 0.5701,
      "step": 21300
    },
    {
      "epoch": 1.7089013632718526,
      "grad_norm": 1.8153802156448364,
      "learning_rate": 1.295363173411954e-05,
      "loss": 0.5924,
      "step": 21310
    },
    {
      "epoch": 1.7097032878909384,
      "grad_norm": 1.7383091449737549,
      "learning_rate": 1.2945590994371484e-05,
      "loss": 0.5413,
      "step": 21320
    },
    {
      "epoch": 1.7105052125100242,
      "grad_norm": 2.0211284160614014,
      "learning_rate": 1.2937550254623424e-05,
      "loss": 0.5314,
      "step": 21330
    },
    {
      "epoch": 1.71130713712911,
      "grad_norm": 2.0130045413970947,
      "learning_rate": 1.2929509514875369e-05,
      "loss": 0.5877,
      "step": 21340
    },
    {
      "epoch": 1.7121090617481958,
      "grad_norm": 1.6991524696350098,
      "learning_rate": 1.2921468775127311e-05,
      "loss": 0.4802,
      "step": 21350
    },
    {
      "epoch": 1.7129109863672816,
      "grad_norm": 1.5161256790161133,
      "learning_rate": 1.2913428035379254e-05,
      "loss": 0.5358,
      "step": 21360
    },
    {
      "epoch": 1.7137129109863674,
      "grad_norm": 1.6634455919265747,
      "learning_rate": 1.2905387295631198e-05,
      "loss": 0.519,
      "step": 21370
    },
    {
      "epoch": 1.7145148356054531,
      "grad_norm": 1.3958665132522583,
      "learning_rate": 1.2897346555883141e-05,
      "loss": 0.5359,
      "step": 21380
    },
    {
      "epoch": 1.715316760224539,
      "grad_norm": 1.7410918474197388,
      "learning_rate": 1.2889305816135085e-05,
      "loss": 0.5482,
      "step": 21390
    },
    {
      "epoch": 1.7161186848436247,
      "grad_norm": 1.893524169921875,
      "learning_rate": 1.2881265076387028e-05,
      "loss": 0.5639,
      "step": 21400
    },
    {
      "epoch": 1.7169206094627105,
      "grad_norm": 2.288999080657959,
      "learning_rate": 1.2873224336638972e-05,
      "loss": 0.5105,
      "step": 21410
    },
    {
      "epoch": 1.7177225340817963,
      "grad_norm": 1.7519423961639404,
      "learning_rate": 1.2865183596890915e-05,
      "loss": 0.5637,
      "step": 21420
    },
    {
      "epoch": 1.718524458700882,
      "grad_norm": 1.7770105600357056,
      "learning_rate": 1.2857142857142857e-05,
      "loss": 0.5504,
      "step": 21430
    },
    {
      "epoch": 1.719326383319968,
      "grad_norm": 1.752453088760376,
      "learning_rate": 1.2849102117394802e-05,
      "loss": 0.5757,
      "step": 21440
    },
    {
      "epoch": 1.7201283079390537,
      "grad_norm": 1.8696061372756958,
      "learning_rate": 1.2841061377646744e-05,
      "loss": 0.5036,
      "step": 21450
    },
    {
      "epoch": 1.7209302325581395,
      "grad_norm": 1.5043001174926758,
      "learning_rate": 1.2833020637898687e-05,
      "loss": 0.5817,
      "step": 21460
    },
    {
      "epoch": 1.7217321571772253,
      "grad_norm": 1.6427282094955444,
      "learning_rate": 1.282497989815063e-05,
      "loss": 0.5496,
      "step": 21470
    },
    {
      "epoch": 1.722534081796311,
      "grad_norm": 2.036874771118164,
      "learning_rate": 1.2816939158402574e-05,
      "loss": 0.552,
      "step": 21480
    },
    {
      "epoch": 1.7233360064153969,
      "grad_norm": 1.925912857055664,
      "learning_rate": 1.2808898418654516e-05,
      "loss": 0.57,
      "step": 21490
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 1.8013859987258911,
      "learning_rate": 1.2800857678906459e-05,
      "loss": 0.5953,
      "step": 21500
    },
    {
      "epoch": 1.7249398556535684,
      "grad_norm": 1.829695701599121,
      "learning_rate": 1.2792816939158403e-05,
      "loss": 0.5816,
      "step": 21510
    },
    {
      "epoch": 1.7257417802726542,
      "grad_norm": 1.7961235046386719,
      "learning_rate": 1.2784776199410346e-05,
      "loss": 0.4804,
      "step": 21520
    },
    {
      "epoch": 1.72654370489174,
      "grad_norm": 1.9055266380310059,
      "learning_rate": 1.277673545966229e-05,
      "loss": 0.5469,
      "step": 21530
    },
    {
      "epoch": 1.7273456295108258,
      "grad_norm": 1.665135383605957,
      "learning_rate": 1.2768694719914233e-05,
      "loss": 0.5371,
      "step": 21540
    },
    {
      "epoch": 1.7281475541299118,
      "grad_norm": 1.8679174184799194,
      "learning_rate": 1.2760653980166175e-05,
      "loss": 0.5155,
      "step": 21550
    },
    {
      "epoch": 1.7289494787489976,
      "grad_norm": 1.865152359008789,
      "learning_rate": 1.275261324041812e-05,
      "loss": 0.4803,
      "step": 21560
    },
    {
      "epoch": 1.7297514033680834,
      "grad_norm": 1.8037445545196533,
      "learning_rate": 1.2744572500670062e-05,
      "loss": 0.5007,
      "step": 21570
    },
    {
      "epoch": 1.7305533279871692,
      "grad_norm": 1.7232415676116943,
      "learning_rate": 1.2736531760922006e-05,
      "loss": 0.5543,
      "step": 21580
    },
    {
      "epoch": 1.731355252606255,
      "grad_norm": 1.8824200630187988,
      "learning_rate": 1.2728491021173947e-05,
      "loss": 0.5877,
      "step": 21590
    },
    {
      "epoch": 1.7321571772253408,
      "grad_norm": 1.738403081893921,
      "learning_rate": 1.2720450281425892e-05,
      "loss": 0.5387,
      "step": 21600
    },
    {
      "epoch": 1.7329591018444266,
      "grad_norm": 1.6981474161148071,
      "learning_rate": 1.2712409541677834e-05,
      "loss": 0.5817,
      "step": 21610
    },
    {
      "epoch": 1.7337610264635124,
      "grad_norm": 1.6922134160995483,
      "learning_rate": 1.2704368801929777e-05,
      "loss": 0.5298,
      "step": 21620
    },
    {
      "epoch": 1.7345629510825984,
      "grad_norm": 1.7866404056549072,
      "learning_rate": 1.2696328062181721e-05,
      "loss": 0.5949,
      "step": 21630
    },
    {
      "epoch": 1.7353648757016842,
      "grad_norm": 1.737612247467041,
      "learning_rate": 1.2688287322433664e-05,
      "loss": 0.5563,
      "step": 21640
    },
    {
      "epoch": 1.73616680032077,
      "grad_norm": 1.6682707071304321,
      "learning_rate": 1.2680246582685608e-05,
      "loss": 0.5618,
      "step": 21650
    },
    {
      "epoch": 1.7369687249398558,
      "grad_norm": 1.6880701780319214,
      "learning_rate": 1.267220584293755e-05,
      "loss": 0.5617,
      "step": 21660
    },
    {
      "epoch": 1.7377706495589416,
      "grad_norm": 1.8315404653549194,
      "learning_rate": 1.2664165103189495e-05,
      "loss": 0.5706,
      "step": 21670
    },
    {
      "epoch": 1.7385725741780274,
      "grad_norm": 1.5325614213943481,
      "learning_rate": 1.2656124363441437e-05,
      "loss": 0.5234,
      "step": 21680
    },
    {
      "epoch": 1.7393744987971131,
      "grad_norm": 2.128203868865967,
      "learning_rate": 1.264808362369338e-05,
      "loss": 0.5588,
      "step": 21690
    },
    {
      "epoch": 1.740176423416199,
      "grad_norm": 1.7191407680511475,
      "learning_rate": 1.2640042883945324e-05,
      "loss": 0.528,
      "step": 21700
    },
    {
      "epoch": 1.7409783480352847,
      "grad_norm": 1.9696546792984009,
      "learning_rate": 1.2632002144197267e-05,
      "loss": 0.5963,
      "step": 21710
    },
    {
      "epoch": 1.7417802726543705,
      "grad_norm": 2.01720929145813,
      "learning_rate": 1.262396140444921e-05,
      "loss": 0.5172,
      "step": 21720
    },
    {
      "epoch": 1.7425821972734563,
      "grad_norm": 2.114964723587036,
      "learning_rate": 1.2615920664701152e-05,
      "loss": 0.5314,
      "step": 21730
    },
    {
      "epoch": 1.743384121892542,
      "grad_norm": 2.104536294937134,
      "learning_rate": 1.2607879924953096e-05,
      "loss": 0.5592,
      "step": 21740
    },
    {
      "epoch": 1.744186046511628,
      "grad_norm": 1.820099949836731,
      "learning_rate": 1.2599839185205039e-05,
      "loss": 0.5529,
      "step": 21750
    },
    {
      "epoch": 1.7449879711307137,
      "grad_norm": 2.0939838886260986,
      "learning_rate": 1.2591798445456982e-05,
      "loss": 0.5763,
      "step": 21760
    },
    {
      "epoch": 1.7457898957497995,
      "grad_norm": 1.9067332744598389,
      "learning_rate": 1.2583757705708926e-05,
      "loss": 0.5193,
      "step": 21770
    },
    {
      "epoch": 1.7465918203688853,
      "grad_norm": 1.7562861442565918,
      "learning_rate": 1.2575716965960868e-05,
      "loss": 0.5712,
      "step": 21780
    },
    {
      "epoch": 1.747393744987971,
      "grad_norm": 2.469907283782959,
      "learning_rate": 1.2567676226212813e-05,
      "loss": 0.5608,
      "step": 21790
    },
    {
      "epoch": 1.7481956696070569,
      "grad_norm": 2.110405206680298,
      "learning_rate": 1.2559635486464755e-05,
      "loss": 0.5404,
      "step": 21800
    },
    {
      "epoch": 1.7489975942261426,
      "grad_norm": 1.9025721549987793,
      "learning_rate": 1.2551594746716698e-05,
      "loss": 0.5284,
      "step": 21810
    },
    {
      "epoch": 1.7497995188452284,
      "grad_norm": 1.8748096227645874,
      "learning_rate": 1.2543554006968642e-05,
      "loss": 0.5719,
      "step": 21820
    },
    {
      "epoch": 1.7506014434643142,
      "grad_norm": 1.8310489654541016,
      "learning_rate": 1.2535513267220585e-05,
      "loss": 0.5325,
      "step": 21830
    },
    {
      "epoch": 1.7514033680834,
      "grad_norm": 1.6457687616348267,
      "learning_rate": 1.2527472527472529e-05,
      "loss": 0.5348,
      "step": 21840
    },
    {
      "epoch": 1.7522052927024858,
      "grad_norm": 1.6771820783615112,
      "learning_rate": 1.251943178772447e-05,
      "loss": 0.5203,
      "step": 21850
    },
    {
      "epoch": 1.7530072173215716,
      "grad_norm": 1.9735376834869385,
      "learning_rate": 1.2511391047976414e-05,
      "loss": 0.5141,
      "step": 21860
    },
    {
      "epoch": 1.7538091419406576,
      "grad_norm": 1.9793834686279297,
      "learning_rate": 1.2503350308228357e-05,
      "loss": 0.5558,
      "step": 21870
    },
    {
      "epoch": 1.7546110665597434,
      "grad_norm": 1.7553904056549072,
      "learning_rate": 1.24953095684803e-05,
      "loss": 0.5108,
      "step": 21880
    },
    {
      "epoch": 1.7554129911788292,
      "grad_norm": 2.365591049194336,
      "learning_rate": 1.2487268828732244e-05,
      "loss": 0.5509,
      "step": 21890
    },
    {
      "epoch": 1.756214915797915,
      "grad_norm": 2.722726583480835,
      "learning_rate": 1.2479228088984186e-05,
      "loss": 0.5942,
      "step": 21900
    },
    {
      "epoch": 1.7570168404170008,
      "grad_norm": 2.0490894317626953,
      "learning_rate": 1.247118734923613e-05,
      "loss": 0.5304,
      "step": 21910
    },
    {
      "epoch": 1.7578187650360866,
      "grad_norm": 1.626800537109375,
      "learning_rate": 1.2463146609488073e-05,
      "loss": 0.5288,
      "step": 21920
    },
    {
      "epoch": 1.7586206896551724,
      "grad_norm": 1.9398761987686157,
      "learning_rate": 1.2455105869740018e-05,
      "loss": 0.5618,
      "step": 21930
    },
    {
      "epoch": 1.7594226142742582,
      "grad_norm": 1.8393621444702148,
      "learning_rate": 1.244706512999196e-05,
      "loss": 0.5906,
      "step": 21940
    },
    {
      "epoch": 1.7602245388933442,
      "grad_norm": 1.9734493494033813,
      "learning_rate": 1.2439024390243903e-05,
      "loss": 0.5582,
      "step": 21950
    },
    {
      "epoch": 1.76102646351243,
      "grad_norm": 1.6857123374938965,
      "learning_rate": 1.2430983650495847e-05,
      "loss": 0.5059,
      "step": 21960
    },
    {
      "epoch": 1.7618283881315158,
      "grad_norm": 1.5037156343460083,
      "learning_rate": 1.242294291074779e-05,
      "loss": 0.5354,
      "step": 21970
    },
    {
      "epoch": 1.7626303127506016,
      "grad_norm": 2.1777899265289307,
      "learning_rate": 1.2414902170999732e-05,
      "loss": 0.5463,
      "step": 21980
    },
    {
      "epoch": 1.7634322373696873,
      "grad_norm": 1.9081577062606812,
      "learning_rate": 1.2406861431251675e-05,
      "loss": 0.5266,
      "step": 21990
    },
    {
      "epoch": 1.7642341619887731,
      "grad_norm": 1.923768162727356,
      "learning_rate": 1.2398820691503617e-05,
      "loss": 0.5826,
      "step": 22000
    },
    {
      "epoch": 1.765036086607859,
      "grad_norm": 1.8722877502441406,
      "learning_rate": 1.2390779951755562e-05,
      "loss": 0.527,
      "step": 22010
    },
    {
      "epoch": 1.7658380112269447,
      "grad_norm": 1.6429200172424316,
      "learning_rate": 1.2382739212007504e-05,
      "loss": 0.5346,
      "step": 22020
    },
    {
      "epoch": 1.7666399358460305,
      "grad_norm": 2.1489744186401367,
      "learning_rate": 1.2374698472259449e-05,
      "loss": 0.5962,
      "step": 22030
    },
    {
      "epoch": 1.7674418604651163,
      "grad_norm": 1.6783688068389893,
      "learning_rate": 1.2366657732511391e-05,
      "loss": 0.5283,
      "step": 22040
    },
    {
      "epoch": 1.768243785084202,
      "grad_norm": 1.7857398986816406,
      "learning_rate": 1.2358616992763335e-05,
      "loss": 0.5187,
      "step": 22050
    },
    {
      "epoch": 1.769045709703288,
      "grad_norm": 1.5336495637893677,
      "learning_rate": 1.2350576253015278e-05,
      "loss": 0.554,
      "step": 22060
    },
    {
      "epoch": 1.7698476343223737,
      "grad_norm": 1.7222115993499756,
      "learning_rate": 1.234253551326722e-05,
      "loss": 0.5408,
      "step": 22070
    },
    {
      "epoch": 1.7706495589414595,
      "grad_norm": 1.8731509447097778,
      "learning_rate": 1.2334494773519165e-05,
      "loss": 0.5547,
      "step": 22080
    },
    {
      "epoch": 1.7714514835605453,
      "grad_norm": 1.8285516500473022,
      "learning_rate": 1.2326454033771108e-05,
      "loss": 0.5579,
      "step": 22090
    },
    {
      "epoch": 1.772253408179631,
      "grad_norm": 1.6783051490783691,
      "learning_rate": 1.231841329402305e-05,
      "loss": 0.5132,
      "step": 22100
    },
    {
      "epoch": 1.7730553327987169,
      "grad_norm": 1.8487447500228882,
      "learning_rate": 1.2310372554274993e-05,
      "loss": 0.5362,
      "step": 22110
    },
    {
      "epoch": 1.7738572574178026,
      "grad_norm": 1.8798179626464844,
      "learning_rate": 1.2302331814526937e-05,
      "loss": 0.5543,
      "step": 22120
    },
    {
      "epoch": 1.7746591820368884,
      "grad_norm": 1.9163051843643188,
      "learning_rate": 1.229429107477888e-05,
      "loss": 0.5505,
      "step": 22130
    },
    {
      "epoch": 1.7754611066559742,
      "grad_norm": 1.5083930492401123,
      "learning_rate": 1.2286250335030822e-05,
      "loss": 0.536,
      "step": 22140
    },
    {
      "epoch": 1.77626303127506,
      "grad_norm": 1.737351655960083,
      "learning_rate": 1.2278209595282766e-05,
      "loss": 0.564,
      "step": 22150
    },
    {
      "epoch": 1.7770649558941458,
      "grad_norm": 2.233161687850952,
      "learning_rate": 1.2270168855534709e-05,
      "loss": 0.5234,
      "step": 22160
    },
    {
      "epoch": 1.7778668805132316,
      "grad_norm": 1.6829962730407715,
      "learning_rate": 1.2262128115786653e-05,
      "loss": 0.5811,
      "step": 22170
    },
    {
      "epoch": 1.7786688051323174,
      "grad_norm": 1.8470348119735718,
      "learning_rate": 1.2254087376038596e-05,
      "loss": 0.5255,
      "step": 22180
    },
    {
      "epoch": 1.7794707297514034,
      "grad_norm": 2.132235050201416,
      "learning_rate": 1.2246046636290539e-05,
      "loss": 0.5491,
      "step": 22190
    },
    {
      "epoch": 1.7802726543704892,
      "grad_norm": 1.9180165529251099,
      "learning_rate": 1.2238005896542483e-05,
      "loss": 0.4865,
      "step": 22200
    },
    {
      "epoch": 1.781074578989575,
      "grad_norm": 1.7618327140808105,
      "learning_rate": 1.2229965156794425e-05,
      "loss": 0.5296,
      "step": 22210
    },
    {
      "epoch": 1.7818765036086608,
      "grad_norm": 1.577021598815918,
      "learning_rate": 1.222192441704637e-05,
      "loss": 0.4999,
      "step": 22220
    },
    {
      "epoch": 1.7826784282277466,
      "grad_norm": 1.8886184692382812,
      "learning_rate": 1.221388367729831e-05,
      "loss": 0.5654,
      "step": 22230
    },
    {
      "epoch": 1.7834803528468324,
      "grad_norm": 1.5193785429000854,
      "learning_rate": 1.2205842937550255e-05,
      "loss": 0.5085,
      "step": 22240
    },
    {
      "epoch": 1.7842822774659182,
      "grad_norm": 2.1128382682800293,
      "learning_rate": 1.2197802197802198e-05,
      "loss": 0.5178,
      "step": 22250
    },
    {
      "epoch": 1.785084202085004,
      "grad_norm": 1.469580888748169,
      "learning_rate": 1.218976145805414e-05,
      "loss": 0.5066,
      "step": 22260
    },
    {
      "epoch": 1.78588612670409,
      "grad_norm": 2.025954484939575,
      "learning_rate": 1.2181720718306084e-05,
      "loss": 0.5788,
      "step": 22270
    },
    {
      "epoch": 1.7866880513231758,
      "grad_norm": 1.962808609008789,
      "learning_rate": 1.2173679978558027e-05,
      "loss": 0.5478,
      "step": 22280
    },
    {
      "epoch": 1.7874899759422616,
      "grad_norm": 1.9018367528915405,
      "learning_rate": 1.2165639238809971e-05,
      "loss": 0.5623,
      "step": 22290
    },
    {
      "epoch": 1.7882919005613473,
      "grad_norm": 1.7811379432678223,
      "learning_rate": 1.2157598499061914e-05,
      "loss": 0.5458,
      "step": 22300
    },
    {
      "epoch": 1.7890938251804331,
      "grad_norm": 1.7288947105407715,
      "learning_rate": 1.2149557759313858e-05,
      "loss": 0.5356,
      "step": 22310
    },
    {
      "epoch": 1.789895749799519,
      "grad_norm": 1.7958077192306519,
      "learning_rate": 1.21415170195658e-05,
      "loss": 0.5242,
      "step": 22320
    },
    {
      "epoch": 1.7906976744186047,
      "grad_norm": 1.9957457780838013,
      "learning_rate": 1.2133476279817743e-05,
      "loss": 0.5422,
      "step": 22330
    },
    {
      "epoch": 1.7914995990376905,
      "grad_norm": 1.997849941253662,
      "learning_rate": 1.2125435540069688e-05,
      "loss": 0.5596,
      "step": 22340
    },
    {
      "epoch": 1.7923015236567763,
      "grad_norm": 1.7615811824798584,
      "learning_rate": 1.211739480032163e-05,
      "loss": 0.4999,
      "step": 22350
    },
    {
      "epoch": 1.793103448275862,
      "grad_norm": 2.136504650115967,
      "learning_rate": 1.2109354060573573e-05,
      "loss": 0.5423,
      "step": 22360
    },
    {
      "epoch": 1.793905372894948,
      "grad_norm": 1.7140183448791504,
      "learning_rate": 1.2101313320825515e-05,
      "loss": 0.5587,
      "step": 22370
    },
    {
      "epoch": 1.7947072975140337,
      "grad_norm": 1.5206767320632935,
      "learning_rate": 1.209327258107746e-05,
      "loss": 0.5573,
      "step": 22380
    },
    {
      "epoch": 1.7955092221331195,
      "grad_norm": 1.8442195653915405,
      "learning_rate": 1.2085231841329402e-05,
      "loss": 0.5559,
      "step": 22390
    },
    {
      "epoch": 1.7963111467522053,
      "grad_norm": 1.6229500770568848,
      "learning_rate": 1.2077191101581345e-05,
      "loss": 0.5582,
      "step": 22400
    },
    {
      "epoch": 1.797113071371291,
      "grad_norm": 1.748998999595642,
      "learning_rate": 1.206915036183329e-05,
      "loss": 0.5183,
      "step": 22410
    },
    {
      "epoch": 1.7979149959903769,
      "grad_norm": 1.6613514423370361,
      "learning_rate": 1.2061109622085232e-05,
      "loss": 0.5208,
      "step": 22420
    },
    {
      "epoch": 1.7987169206094626,
      "grad_norm": 2.0816869735717773,
      "learning_rate": 1.2053068882337176e-05,
      "loss": 0.5559,
      "step": 22430
    },
    {
      "epoch": 1.7995188452285484,
      "grad_norm": 1.7325422763824463,
      "learning_rate": 1.2045028142589119e-05,
      "loss": 0.4938,
      "step": 22440
    },
    {
      "epoch": 1.8003207698476342,
      "grad_norm": 1.6751196384429932,
      "learning_rate": 1.2036987402841061e-05,
      "loss": 0.5422,
      "step": 22450
    },
    {
      "epoch": 1.80112269446672,
      "grad_norm": 1.6223077774047852,
      "learning_rate": 1.2028946663093006e-05,
      "loss": 0.5218,
      "step": 22460
    },
    {
      "epoch": 1.8019246190858058,
      "grad_norm": 1.7604864835739136,
      "learning_rate": 1.2020905923344948e-05,
      "loss": 0.499,
      "step": 22470
    },
    {
      "epoch": 1.8027265437048916,
      "grad_norm": 2.0882465839385986,
      "learning_rate": 1.2012865183596892e-05,
      "loss": 0.5133,
      "step": 22480
    },
    {
      "epoch": 1.8035284683239774,
      "grad_norm": 2.1168665885925293,
      "learning_rate": 1.2004824443848833e-05,
      "loss": 0.5566,
      "step": 22490
    },
    {
      "epoch": 1.8043303929430632,
      "grad_norm": 1.994035243988037,
      "learning_rate": 1.1996783704100778e-05,
      "loss": 0.5506,
      "step": 22500
    },
    {
      "epoch": 1.8051323175621492,
      "grad_norm": 1.8042707443237305,
      "learning_rate": 1.198874296435272e-05,
      "loss": 0.5583,
      "step": 22510
    },
    {
      "epoch": 1.805934242181235,
      "grad_norm": 1.9085098505020142,
      "learning_rate": 1.1980702224604663e-05,
      "loss": 0.53,
      "step": 22520
    },
    {
      "epoch": 1.8067361668003208,
      "grad_norm": 2.0280463695526123,
      "learning_rate": 1.1972661484856607e-05,
      "loss": 0.5821,
      "step": 22530
    },
    {
      "epoch": 1.8075380914194066,
      "grad_norm": 1.739442229270935,
      "learning_rate": 1.196462074510855e-05,
      "loss": 0.5074,
      "step": 22540
    },
    {
      "epoch": 1.8083400160384924,
      "grad_norm": 2.020604372024536,
      "learning_rate": 1.1956580005360494e-05,
      "loss": 0.5756,
      "step": 22550
    },
    {
      "epoch": 1.8091419406575782,
      "grad_norm": 1.7914345264434814,
      "learning_rate": 1.1948539265612437e-05,
      "loss": 0.5901,
      "step": 22560
    },
    {
      "epoch": 1.809943865276664,
      "grad_norm": 1.8113127946853638,
      "learning_rate": 1.1940498525864381e-05,
      "loss": 0.5797,
      "step": 22570
    },
    {
      "epoch": 1.8107457898957497,
      "grad_norm": 1.740007758140564,
      "learning_rate": 1.1932457786116324e-05,
      "loss": 0.5612,
      "step": 22580
    },
    {
      "epoch": 1.8115477145148358,
      "grad_norm": 1.9324196577072144,
      "learning_rate": 1.1924417046368266e-05,
      "loss": 0.5474,
      "step": 22590
    },
    {
      "epoch": 1.8123496391339216,
      "grad_norm": 1.8198992013931274,
      "learning_rate": 1.191637630662021e-05,
      "loss": 0.4963,
      "step": 22600
    },
    {
      "epoch": 1.8131515637530073,
      "grad_norm": 1.942276120185852,
      "learning_rate": 1.1908335566872153e-05,
      "loss": 0.5499,
      "step": 22610
    },
    {
      "epoch": 1.8139534883720931,
      "grad_norm": 2.183530330657959,
      "learning_rate": 1.1900294827124096e-05,
      "loss": 0.5736,
      "step": 22620
    },
    {
      "epoch": 1.814755412991179,
      "grad_norm": 1.7369673252105713,
      "learning_rate": 1.1892254087376038e-05,
      "loss": 0.5313,
      "step": 22630
    },
    {
      "epoch": 1.8155573376102647,
      "grad_norm": 2.062568426132202,
      "learning_rate": 1.188421334762798e-05,
      "loss": 0.5528,
      "step": 22640
    },
    {
      "epoch": 1.8163592622293505,
      "grad_norm": 2.010809898376465,
      "learning_rate": 1.1876172607879925e-05,
      "loss": 0.5837,
      "step": 22650
    },
    {
      "epoch": 1.8171611868484363,
      "grad_norm": 1.6300808191299438,
      "learning_rate": 1.1868131868131868e-05,
      "loss": 0.5358,
      "step": 22660
    },
    {
      "epoch": 1.817963111467522,
      "grad_norm": 1.9746390581130981,
      "learning_rate": 1.1860091128383812e-05,
      "loss": 0.6138,
      "step": 22670
    },
    {
      "epoch": 1.818765036086608,
      "grad_norm": 2.1835172176361084,
      "learning_rate": 1.1852050388635755e-05,
      "loss": 0.5602,
      "step": 22680
    },
    {
      "epoch": 1.8195669607056937,
      "grad_norm": 1.6352757215499878,
      "learning_rate": 1.1844009648887699e-05,
      "loss": 0.4997,
      "step": 22690
    },
    {
      "epoch": 1.8203688853247795,
      "grad_norm": 1.7104219198226929,
      "learning_rate": 1.1835968909139641e-05,
      "loss": 0.5382,
      "step": 22700
    },
    {
      "epoch": 1.8211708099438653,
      "grad_norm": 1.7189890146255493,
      "learning_rate": 1.1827928169391584e-05,
      "loss": 0.595,
      "step": 22710
    },
    {
      "epoch": 1.821972734562951,
      "grad_norm": 1.8585695028305054,
      "learning_rate": 1.1819887429643528e-05,
      "loss": 0.5347,
      "step": 22720
    },
    {
      "epoch": 1.8227746591820368,
      "grad_norm": 1.7104352712631226,
      "learning_rate": 1.1811846689895471e-05,
      "loss": 0.5681,
      "step": 22730
    },
    {
      "epoch": 1.8235765838011226,
      "grad_norm": 1.7474523782730103,
      "learning_rate": 1.1803805950147414e-05,
      "loss": 0.5327,
      "step": 22740
    },
    {
      "epoch": 1.8243785084202084,
      "grad_norm": 1.6975972652435303,
      "learning_rate": 1.1795765210399356e-05,
      "loss": 0.5378,
      "step": 22750
    },
    {
      "epoch": 1.8251804330392942,
      "grad_norm": 1.805259108543396,
      "learning_rate": 1.17877244706513e-05,
      "loss": 0.5475,
      "step": 22760
    },
    {
      "epoch": 1.82598235765838,
      "grad_norm": 1.4471125602722168,
      "learning_rate": 1.1779683730903243e-05,
      "loss": 0.5428,
      "step": 22770
    },
    {
      "epoch": 1.8267842822774658,
      "grad_norm": 1.8156150579452515,
      "learning_rate": 1.1771642991155186e-05,
      "loss": 0.5013,
      "step": 22780
    },
    {
      "epoch": 1.8275862068965516,
      "grad_norm": 1.7041715383529663,
      "learning_rate": 1.176360225140713e-05,
      "loss": 0.4828,
      "step": 22790
    },
    {
      "epoch": 1.8283881315156374,
      "grad_norm": 1.9661122560501099,
      "learning_rate": 1.1755561511659073e-05,
      "loss": 0.5422,
      "step": 22800
    },
    {
      "epoch": 1.8291900561347232,
      "grad_norm": 1.867335319519043,
      "learning_rate": 1.1747520771911017e-05,
      "loss": 0.5602,
      "step": 22810
    },
    {
      "epoch": 1.829991980753809,
      "grad_norm": 1.8524479866027832,
      "learning_rate": 1.173948003216296e-05,
      "loss": 0.4773,
      "step": 22820
    },
    {
      "epoch": 1.830793905372895,
      "grad_norm": 1.9455486536026,
      "learning_rate": 1.1731439292414902e-05,
      "loss": 0.5497,
      "step": 22830
    },
    {
      "epoch": 1.8315958299919808,
      "grad_norm": 1.6245402097702026,
      "learning_rate": 1.1723398552666846e-05,
      "loss": 0.5263,
      "step": 22840
    },
    {
      "epoch": 1.8323977546110666,
      "grad_norm": 1.9760899543762207,
      "learning_rate": 1.1716161886893594e-05,
      "loss": 0.4967,
      "step": 22850
    },
    {
      "epoch": 1.8331996792301524,
      "grad_norm": 1.797544002532959,
      "learning_rate": 1.1708121147145538e-05,
      "loss": 0.6051,
      "step": 22860
    },
    {
      "epoch": 1.8340016038492382,
      "grad_norm": 2.037524700164795,
      "learning_rate": 1.1700080407397481e-05,
      "loss": 0.5846,
      "step": 22870
    },
    {
      "epoch": 1.834803528468324,
      "grad_norm": 1.7248444557189941,
      "learning_rate": 1.1692039667649424e-05,
      "loss": 0.589,
      "step": 22880
    },
    {
      "epoch": 1.8356054530874097,
      "grad_norm": 1.7292314767837524,
      "learning_rate": 1.1683998927901368e-05,
      "loss": 0.5711,
      "step": 22890
    },
    {
      "epoch": 1.8364073777064955,
      "grad_norm": 1.8386223316192627,
      "learning_rate": 1.167595818815331e-05,
      "loss": 0.5775,
      "step": 22900
    },
    {
      "epoch": 1.8372093023255816,
      "grad_norm": 2.0450923442840576,
      "learning_rate": 1.1667917448405255e-05,
      "loss": 0.5308,
      "step": 22910
    },
    {
      "epoch": 1.8380112269446673,
      "grad_norm": 1.9276692867279053,
      "learning_rate": 1.1659876708657197e-05,
      "loss": 0.5388,
      "step": 22920
    },
    {
      "epoch": 1.8388131515637531,
      "grad_norm": 1.7185882329940796,
      "learning_rate": 1.165183596890914e-05,
      "loss": 0.5698,
      "step": 22930
    },
    {
      "epoch": 1.839615076182839,
      "grad_norm": 1.9779857397079468,
      "learning_rate": 1.1643795229161083e-05,
      "loss": 0.5413,
      "step": 22940
    },
    {
      "epoch": 1.8404170008019247,
      "grad_norm": 1.6586756706237793,
      "learning_rate": 1.1635754489413025e-05,
      "loss": 0.572,
      "step": 22950
    },
    {
      "epoch": 1.8412189254210105,
      "grad_norm": 1.7218674421310425,
      "learning_rate": 1.162771374966497e-05,
      "loss": 0.6001,
      "step": 22960
    },
    {
      "epoch": 1.8420208500400963,
      "grad_norm": 1.5828509330749512,
      "learning_rate": 1.1619673009916912e-05,
      "loss": 0.5306,
      "step": 22970
    },
    {
      "epoch": 1.842822774659182,
      "grad_norm": 1.7124282121658325,
      "learning_rate": 1.1611632270168856e-05,
      "loss": 0.5741,
      "step": 22980
    },
    {
      "epoch": 1.8436246992782679,
      "grad_norm": 1.8319413661956787,
      "learning_rate": 1.1603591530420799e-05,
      "loss": 0.5247,
      "step": 22990
    },
    {
      "epoch": 1.8444266238973537,
      "grad_norm": 2.0284767150878906,
      "learning_rate": 1.1595550790672742e-05,
      "loss": 0.5672,
      "step": 23000
    },
    {
      "epoch": 1.8452285485164395,
      "grad_norm": 1.9143445491790771,
      "learning_rate": 1.1587510050924686e-05,
      "loss": 0.5878,
      "step": 23010
    },
    {
      "epoch": 1.8460304731355253,
      "grad_norm": 1.8768401145935059,
      "learning_rate": 1.1579469311176628e-05,
      "loss": 0.6409,
      "step": 23020
    },
    {
      "epoch": 1.846832397754611,
      "grad_norm": 1.9025756120681763,
      "learning_rate": 1.1571428571428573e-05,
      "loss": 0.5546,
      "step": 23030
    },
    {
      "epoch": 1.8476343223736968,
      "grad_norm": 2.1431424617767334,
      "learning_rate": 1.1563387831680515e-05,
      "loss": 0.5359,
      "step": 23040
    },
    {
      "epoch": 1.8484362469927826,
      "grad_norm": 1.7130436897277832,
      "learning_rate": 1.155534709193246e-05,
      "loss": 0.5827,
      "step": 23050
    },
    {
      "epoch": 1.8492381716118684,
      "grad_norm": 2.176724433898926,
      "learning_rate": 1.1547306352184402e-05,
      "loss": 0.5877,
      "step": 23060
    },
    {
      "epoch": 1.8500400962309542,
      "grad_norm": 1.750473141670227,
      "learning_rate": 1.1539265612436343e-05,
      "loss": 0.5805,
      "step": 23070
    },
    {
      "epoch": 1.85084202085004,
      "grad_norm": 1.6790053844451904,
      "learning_rate": 1.1531224872688287e-05,
      "loss": 0.4941,
      "step": 23080
    },
    {
      "epoch": 1.8516439454691258,
      "grad_norm": 1.7563081979751587,
      "learning_rate": 1.152318413294023e-05,
      "loss": 0.5276,
      "step": 23090
    },
    {
      "epoch": 1.8524458700882116,
      "grad_norm": 2.009730815887451,
      "learning_rate": 1.1515143393192174e-05,
      "loss": 0.5204,
      "step": 23100
    },
    {
      "epoch": 1.8532477947072974,
      "grad_norm": 1.8945130109786987,
      "learning_rate": 1.1507102653444117e-05,
      "loss": 0.5885,
      "step": 23110
    },
    {
      "epoch": 1.8540497193263832,
      "grad_norm": 2.0570995807647705,
      "learning_rate": 1.149906191369606e-05,
      "loss": 0.5572,
      "step": 23120
    },
    {
      "epoch": 1.854851643945469,
      "grad_norm": 1.7725974321365356,
      "learning_rate": 1.1491021173948004e-05,
      "loss": 0.5278,
      "step": 23130
    },
    {
      "epoch": 1.8556535685645548,
      "grad_norm": 1.96853506565094,
      "learning_rate": 1.1482980434199946e-05,
      "loss": 0.5147,
      "step": 23140
    },
    {
      "epoch": 1.8564554931836408,
      "grad_norm": 1.8880162239074707,
      "learning_rate": 1.147493969445189e-05,
      "loss": 0.5382,
      "step": 23150
    },
    {
      "epoch": 1.8572574178027266,
      "grad_norm": 1.8016538619995117,
      "learning_rate": 1.1466898954703833e-05,
      "loss": 0.569,
      "step": 23160
    },
    {
      "epoch": 1.8580593424218124,
      "grad_norm": 2.4913132190704346,
      "learning_rate": 1.1458858214955778e-05,
      "loss": 0.5653,
      "step": 23170
    },
    {
      "epoch": 1.8588612670408982,
      "grad_norm": 1.877016544342041,
      "learning_rate": 1.145081747520772e-05,
      "loss": 0.5135,
      "step": 23180
    },
    {
      "epoch": 1.859663191659984,
      "grad_norm": 1.7577507495880127,
      "learning_rate": 1.1442776735459663e-05,
      "loss": 0.5272,
      "step": 23190
    },
    {
      "epoch": 1.8604651162790697,
      "grad_norm": 2.0289416313171387,
      "learning_rate": 1.1434735995711605e-05,
      "loss": 0.6124,
      "step": 23200
    },
    {
      "epoch": 1.8612670408981555,
      "grad_norm": 1.9620755910873413,
      "learning_rate": 1.1426695255963548e-05,
      "loss": 0.5481,
      "step": 23210
    },
    {
      "epoch": 1.8620689655172413,
      "grad_norm": 1.7229012250900269,
      "learning_rate": 1.1418654516215492e-05,
      "loss": 0.5595,
      "step": 23220
    },
    {
      "epoch": 1.8628708901363273,
      "grad_norm": 2.1770358085632324,
      "learning_rate": 1.1410613776467435e-05,
      "loss": 0.5696,
      "step": 23230
    },
    {
      "epoch": 1.8636728147554131,
      "grad_norm": 1.7549524307250977,
      "learning_rate": 1.1402573036719379e-05,
      "loss": 0.6029,
      "step": 23240
    },
    {
      "epoch": 1.864474739374499,
      "grad_norm": 1.9453849792480469,
      "learning_rate": 1.1394532296971322e-05,
      "loss": 0.5461,
      "step": 23250
    },
    {
      "epoch": 1.8652766639935847,
      "grad_norm": 1.8651427030563354,
      "learning_rate": 1.1386491557223264e-05,
      "loss": 0.592,
      "step": 23260
    },
    {
      "epoch": 1.8660785886126705,
      "grad_norm": 2.09458589553833,
      "learning_rate": 1.1378450817475209e-05,
      "loss": 0.609,
      "step": 23270
    },
    {
      "epoch": 1.8668805132317563,
      "grad_norm": 1.8981108665466309,
      "learning_rate": 1.1370410077727151e-05,
      "loss": 0.5316,
      "step": 23280
    },
    {
      "epoch": 1.867682437850842,
      "grad_norm": 1.8542119264602661,
      "learning_rate": 1.1362369337979095e-05,
      "loss": 0.5421,
      "step": 23290
    },
    {
      "epoch": 1.8684843624699279,
      "grad_norm": 1.708135724067688,
      "learning_rate": 1.1354328598231038e-05,
      "loss": 0.5354,
      "step": 23300
    },
    {
      "epoch": 1.8692862870890137,
      "grad_norm": 1.6740182638168335,
      "learning_rate": 1.134628785848298e-05,
      "loss": 0.5228,
      "step": 23310
    },
    {
      "epoch": 1.8700882117080995,
      "grad_norm": 1.6874345541000366,
      "learning_rate": 1.1338247118734923e-05,
      "loss": 0.5486,
      "step": 23320
    },
    {
      "epoch": 1.8708901363271853,
      "grad_norm": 1.8038678169250488,
      "learning_rate": 1.1330206378986866e-05,
      "loss": 0.5044,
      "step": 23330
    },
    {
      "epoch": 1.871692060946271,
      "grad_norm": 1.715614914894104,
      "learning_rate": 1.132216563923881e-05,
      "loss": 0.5684,
      "step": 23340
    },
    {
      "epoch": 1.8724939855653568,
      "grad_norm": 1.8968127965927124,
      "learning_rate": 1.1314124899490753e-05,
      "loss": 0.5743,
      "step": 23350
    },
    {
      "epoch": 1.8732959101844426,
      "grad_norm": 1.851401448249817,
      "learning_rate": 1.1306084159742697e-05,
      "loss": 0.5554,
      "step": 23360
    },
    {
      "epoch": 1.8740978348035284,
      "grad_norm": 1.578471064567566,
      "learning_rate": 1.129804341999464e-05,
      "loss": 0.5368,
      "step": 23370
    },
    {
      "epoch": 1.8748997594226142,
      "grad_norm": 1.8309611082077026,
      "learning_rate": 1.1290002680246582e-05,
      "loss": 0.5562,
      "step": 23380
    },
    {
      "epoch": 1.8757016840417,
      "grad_norm": 1.5835410356521606,
      "learning_rate": 1.1281961940498526e-05,
      "loss": 0.6,
      "step": 23390
    },
    {
      "epoch": 1.8765036086607858,
      "grad_norm": 1.764894962310791,
      "learning_rate": 1.1273921200750469e-05,
      "loss": 0.573,
      "step": 23400
    },
    {
      "epoch": 1.8773055332798716,
      "grad_norm": 1.7011903524398804,
      "learning_rate": 1.1265880461002413e-05,
      "loss": 0.6032,
      "step": 23410
    },
    {
      "epoch": 1.8781074578989574,
      "grad_norm": 1.7771823406219482,
      "learning_rate": 1.1257839721254356e-05,
      "loss": 0.5675,
      "step": 23420
    },
    {
      "epoch": 1.8789093825180432,
      "grad_norm": 1.6071568727493286,
      "learning_rate": 1.12497989815063e-05,
      "loss": 0.4879,
      "step": 23430
    },
    {
      "epoch": 1.879711307137129,
      "grad_norm": 1.708163857460022,
      "learning_rate": 1.1241758241758243e-05,
      "loss": 0.5869,
      "step": 23440
    },
    {
      "epoch": 1.8805132317562148,
      "grad_norm": 1.8639628887176514,
      "learning_rate": 1.1233717502010184e-05,
      "loss": 0.5765,
      "step": 23450
    },
    {
      "epoch": 1.8813151563753006,
      "grad_norm": 1.5663104057312012,
      "learning_rate": 1.1225676762262128e-05,
      "loss": 0.5738,
      "step": 23460
    },
    {
      "epoch": 1.8821170809943866,
      "grad_norm": 1.8393964767456055,
      "learning_rate": 1.121763602251407e-05,
      "loss": 0.5219,
      "step": 23470
    },
    {
      "epoch": 1.8829190056134724,
      "grad_norm": 1.8008023500442505,
      "learning_rate": 1.1209595282766015e-05,
      "loss": 0.5729,
      "step": 23480
    },
    {
      "epoch": 1.8837209302325582,
      "grad_norm": 1.8557286262512207,
      "learning_rate": 1.1201554543017958e-05,
      "loss": 0.5596,
      "step": 23490
    },
    {
      "epoch": 1.884522854851644,
      "grad_norm": 1.9271122217178345,
      "learning_rate": 1.1193513803269902e-05,
      "loss": 0.5324,
      "step": 23500
    },
    {
      "epoch": 1.8853247794707297,
      "grad_norm": 1.72890305519104,
      "learning_rate": 1.1185473063521844e-05,
      "loss": 0.5283,
      "step": 23510
    },
    {
      "epoch": 1.8861267040898155,
      "grad_norm": 1.997606873512268,
      "learning_rate": 1.1177432323773787e-05,
      "loss": 0.5719,
      "step": 23520
    },
    {
      "epoch": 1.8869286287089013,
      "grad_norm": 1.8395947217941284,
      "learning_rate": 1.1169391584025731e-05,
      "loss": 0.5556,
      "step": 23530
    },
    {
      "epoch": 1.8877305533279871,
      "grad_norm": 2.21224308013916,
      "learning_rate": 1.1161350844277674e-05,
      "loss": 0.5495,
      "step": 23540
    },
    {
      "epoch": 1.8885324779470731,
      "grad_norm": 1.9147708415985107,
      "learning_rate": 1.1153310104529618e-05,
      "loss": 0.5448,
      "step": 23550
    },
    {
      "epoch": 1.889334402566159,
      "grad_norm": 1.8166900873184204,
      "learning_rate": 1.114526936478156e-05,
      "loss": 0.6001,
      "step": 23560
    },
    {
      "epoch": 1.8901363271852447,
      "grad_norm": 1.710323691368103,
      "learning_rate": 1.1137228625033503e-05,
      "loss": 0.5763,
      "step": 23570
    },
    {
      "epoch": 1.8909382518043305,
      "grad_norm": 1.7390505075454712,
      "learning_rate": 1.1129187885285446e-05,
      "loss": 0.5403,
      "step": 23580
    },
    {
      "epoch": 1.8917401764234163,
      "grad_norm": 1.8337117433547974,
      "learning_rate": 1.1121147145537389e-05,
      "loss": 0.5952,
      "step": 23590
    },
    {
      "epoch": 1.892542101042502,
      "grad_norm": 2.0378243923187256,
      "learning_rate": 1.1113106405789333e-05,
      "loss": 0.5382,
      "step": 23600
    },
    {
      "epoch": 1.8933440256615879,
      "grad_norm": 1.6748777627944946,
      "learning_rate": 1.1105065666041275e-05,
      "loss": 0.5417,
      "step": 23610
    },
    {
      "epoch": 1.8941459502806737,
      "grad_norm": 1.774533987045288,
      "learning_rate": 1.109702492629322e-05,
      "loss": 0.5052,
      "step": 23620
    },
    {
      "epoch": 1.8949478748997595,
      "grad_norm": 1.808840274810791,
      "learning_rate": 1.1088984186545162e-05,
      "loss": 0.5114,
      "step": 23630
    },
    {
      "epoch": 1.8957497995188453,
      "grad_norm": 1.7989468574523926,
      "learning_rate": 1.1080943446797105e-05,
      "loss": 0.5761,
      "step": 23640
    },
    {
      "epoch": 1.896551724137931,
      "grad_norm": 1.9211262464523315,
      "learning_rate": 1.107290270704905e-05,
      "loss": 0.49,
      "step": 23650
    },
    {
      "epoch": 1.8973536487570168,
      "grad_norm": 1.7528935670852661,
      "learning_rate": 1.1064861967300992e-05,
      "loss": 0.55,
      "step": 23660
    },
    {
      "epoch": 1.8981555733761026,
      "grad_norm": 1.906895399093628,
      "learning_rate": 1.1056821227552936e-05,
      "loss": 0.5429,
      "step": 23670
    },
    {
      "epoch": 1.8989574979951884,
      "grad_norm": 1.7906361818313599,
      "learning_rate": 1.1048780487804879e-05,
      "loss": 0.5155,
      "step": 23680
    },
    {
      "epoch": 1.8997594226142742,
      "grad_norm": 1.9175794124603271,
      "learning_rate": 1.1040739748056823e-05,
      "loss": 0.5556,
      "step": 23690
    },
    {
      "epoch": 1.90056134723336,
      "grad_norm": 2.1667280197143555,
      "learning_rate": 1.1032699008308766e-05,
      "loss": 0.5287,
      "step": 23700
    },
    {
      "epoch": 1.9013632718524458,
      "grad_norm": 1.8180115222930908,
      "learning_rate": 1.1024658268560706e-05,
      "loss": 0.5308,
      "step": 23710
    },
    {
      "epoch": 1.9021651964715316,
      "grad_norm": 1.9424173831939697,
      "learning_rate": 1.101661752881265e-05,
      "loss": 0.592,
      "step": 23720
    },
    {
      "epoch": 1.9029671210906174,
      "grad_norm": 1.5830117464065552,
      "learning_rate": 1.1008576789064593e-05,
      "loss": 0.627,
      "step": 23730
    },
    {
      "epoch": 1.9037690457097032,
      "grad_norm": 1.6186574697494507,
      "learning_rate": 1.1000536049316538e-05,
      "loss": 0.5807,
      "step": 23740
    },
    {
      "epoch": 1.904570970328789,
      "grad_norm": 1.6014106273651123,
      "learning_rate": 1.099249530956848e-05,
      "loss": 0.5171,
      "step": 23750
    },
    {
      "epoch": 1.9053728949478748,
      "grad_norm": 1.7305866479873657,
      "learning_rate": 1.0984454569820423e-05,
      "loss": 0.5472,
      "step": 23760
    },
    {
      "epoch": 1.9061748195669606,
      "grad_norm": 1.8708620071411133,
      "learning_rate": 1.0976413830072367e-05,
      "loss": 0.5757,
      "step": 23770
    },
    {
      "epoch": 1.9069767441860463,
      "grad_norm": 1.987783432006836,
      "learning_rate": 1.096837309032431e-05,
      "loss": 0.5541,
      "step": 23780
    },
    {
      "epoch": 1.9077786688051324,
      "grad_norm": 1.7760440111160278,
      "learning_rate": 1.0960332350576254e-05,
      "loss": 0.4935,
      "step": 23790
    },
    {
      "epoch": 1.9085805934242182,
      "grad_norm": 2.0821213722229004,
      "learning_rate": 1.0952291610828197e-05,
      "loss": 0.574,
      "step": 23800
    },
    {
      "epoch": 1.909382518043304,
      "grad_norm": 1.5972849130630493,
      "learning_rate": 1.0944250871080141e-05,
      "loss": 0.5643,
      "step": 23810
    },
    {
      "epoch": 1.9101844426623897,
      "grad_norm": 1.7808347940444946,
      "learning_rate": 1.0936210131332084e-05,
      "loss": 0.5556,
      "step": 23820
    },
    {
      "epoch": 1.9109863672814755,
      "grad_norm": 1.9449743032455444,
      "learning_rate": 1.0928169391584026e-05,
      "loss": 0.5431,
      "step": 23830
    },
    {
      "epoch": 1.9117882919005613,
      "grad_norm": 1.9552608728408813,
      "learning_rate": 1.0920128651835969e-05,
      "loss": 0.5195,
      "step": 23840
    },
    {
      "epoch": 1.9125902165196471,
      "grad_norm": 1.7612128257751465,
      "learning_rate": 1.0912087912087911e-05,
      "loss": 0.5079,
      "step": 23850
    },
    {
      "epoch": 1.913392141138733,
      "grad_norm": 2.0102133750915527,
      "learning_rate": 1.0904047172339856e-05,
      "loss": 0.54,
      "step": 23860
    },
    {
      "epoch": 1.914194065757819,
      "grad_norm": 1.9549866914749146,
      "learning_rate": 1.0896006432591798e-05,
      "loss": 0.5559,
      "step": 23870
    },
    {
      "epoch": 1.9149959903769047,
      "grad_norm": 1.8180402517318726,
      "learning_rate": 1.0887965692843742e-05,
      "loss": 0.5127,
      "step": 23880
    },
    {
      "epoch": 1.9157979149959905,
      "grad_norm": 1.5384516716003418,
      "learning_rate": 1.0879924953095685e-05,
      "loss": 0.5462,
      "step": 23890
    },
    {
      "epoch": 1.9165998396150763,
      "grad_norm": 1.8569623231887817,
      "learning_rate": 1.0871884213347628e-05,
      "loss": 0.5614,
      "step": 23900
    },
    {
      "epoch": 1.917401764234162,
      "grad_norm": 2.4236204624176025,
      "learning_rate": 1.0863843473599572e-05,
      "loss": 0.5685,
      "step": 23910
    },
    {
      "epoch": 1.9182036888532479,
      "grad_norm": 1.6609842777252197,
      "learning_rate": 1.0855802733851515e-05,
      "loss": 0.5005,
      "step": 23920
    },
    {
      "epoch": 1.9190056134723337,
      "grad_norm": 2.045747995376587,
      "learning_rate": 1.0847761994103459e-05,
      "loss": 0.5143,
      "step": 23930
    },
    {
      "epoch": 1.9198075380914195,
      "grad_norm": 1.9209691286087036,
      "learning_rate": 1.0839721254355401e-05,
      "loss": 0.4957,
      "step": 23940
    },
    {
      "epoch": 1.9206094627105053,
      "grad_norm": 1.8750807046890259,
      "learning_rate": 1.0831680514607344e-05,
      "loss": 0.6053,
      "step": 23950
    },
    {
      "epoch": 1.921411387329591,
      "grad_norm": 1.634163737297058,
      "learning_rate": 1.0823639774859287e-05,
      "loss": 0.5626,
      "step": 23960
    },
    {
      "epoch": 1.9222133119486768,
      "grad_norm": 1.9276819229125977,
      "learning_rate": 1.081559903511123e-05,
      "loss": 0.5103,
      "step": 23970
    },
    {
      "epoch": 1.9230152365677626,
      "grad_norm": 1.7974995374679565,
      "learning_rate": 1.0807558295363174e-05,
      "loss": 0.5128,
      "step": 23980
    },
    {
      "epoch": 1.9238171611868484,
      "grad_norm": 1.7316551208496094,
      "learning_rate": 1.0799517555615116e-05,
      "loss": 0.54,
      "step": 23990
    },
    {
      "epoch": 1.9246190858059342,
      "grad_norm": 2.1393280029296875,
      "learning_rate": 1.079147681586706e-05,
      "loss": 0.5555,
      "step": 24000
    },
    {
      "epoch": 1.92542101042502,
      "grad_norm": 1.831298589706421,
      "learning_rate": 1.0783436076119003e-05,
      "loss": 0.5667,
      "step": 24010
    },
    {
      "epoch": 1.9262229350441058,
      "grad_norm": 2.038820266723633,
      "learning_rate": 1.0775395336370946e-05,
      "loss": 0.5821,
      "step": 24020
    },
    {
      "epoch": 1.9270248596631916,
      "grad_norm": 1.9713817834854126,
      "learning_rate": 1.076735459662289e-05,
      "loss": 0.6023,
      "step": 24030
    },
    {
      "epoch": 1.9278267842822774,
      "grad_norm": 1.7592707872390747,
      "learning_rate": 1.0759313856874832e-05,
      "loss": 0.5476,
      "step": 24040
    },
    {
      "epoch": 1.9286287089013632,
      "grad_norm": 1.8786239624023438,
      "learning_rate": 1.0751273117126777e-05,
      "loss": 0.6203,
      "step": 24050
    },
    {
      "epoch": 1.929430633520449,
      "grad_norm": 1.9784903526306152,
      "learning_rate": 1.074323237737872e-05,
      "loss": 0.5669,
      "step": 24060
    },
    {
      "epoch": 1.9302325581395348,
      "grad_norm": 2.1102235317230225,
      "learning_rate": 1.0735191637630664e-05,
      "loss": 0.5693,
      "step": 24070
    },
    {
      "epoch": 1.9310344827586206,
      "grad_norm": 1.8808470964431763,
      "learning_rate": 1.0727150897882606e-05,
      "loss": 0.5426,
      "step": 24080
    },
    {
      "epoch": 1.9318364073777063,
      "grad_norm": 1.6236804723739624,
      "learning_rate": 1.0719110158134547e-05,
      "loss": 0.5469,
      "step": 24090
    },
    {
      "epoch": 1.9326383319967921,
      "grad_norm": 1.7971662282943726,
      "learning_rate": 1.0711069418386491e-05,
      "loss": 0.4756,
      "step": 24100
    },
    {
      "epoch": 1.9334402566158782,
      "grad_norm": 1.781376600265503,
      "learning_rate": 1.0703028678638434e-05,
      "loss": 0.5549,
      "step": 24110
    },
    {
      "epoch": 1.934242181234964,
      "grad_norm": 1.9540178775787354,
      "learning_rate": 1.0694987938890378e-05,
      "loss": 0.555,
      "step": 24120
    },
    {
      "epoch": 1.9350441058540497,
      "grad_norm": 1.607869267463684,
      "learning_rate": 1.0686947199142321e-05,
      "loss": 0.5138,
      "step": 24130
    },
    {
      "epoch": 1.9358460304731355,
      "grad_norm": 1.719957709312439,
      "learning_rate": 1.0678906459394265e-05,
      "loss": 0.4565,
      "step": 24140
    },
    {
      "epoch": 1.9366479550922213,
      "grad_norm": 1.9384050369262695,
      "learning_rate": 1.0670865719646208e-05,
      "loss": 0.5534,
      "step": 24150
    },
    {
      "epoch": 1.937449879711307,
      "grad_norm": 1.7355117797851562,
      "learning_rate": 1.066282497989815e-05,
      "loss": 0.5598,
      "step": 24160
    },
    {
      "epoch": 1.938251804330393,
      "grad_norm": 1.7891733646392822,
      "learning_rate": 1.0654784240150095e-05,
      "loss": 0.5786,
      "step": 24170
    },
    {
      "epoch": 1.9390537289494787,
      "grad_norm": 1.8744341135025024,
      "learning_rate": 1.0646743500402037e-05,
      "loss": 0.5757,
      "step": 24180
    },
    {
      "epoch": 1.9398556535685647,
      "grad_norm": 1.9202923774719238,
      "learning_rate": 1.0638702760653982e-05,
      "loss": 0.5916,
      "step": 24190
    },
    {
      "epoch": 1.9406575781876505,
      "grad_norm": 1.8689801692962646,
      "learning_rate": 1.0630662020905924e-05,
      "loss": 0.5678,
      "step": 24200
    },
    {
      "epoch": 1.9414595028067363,
      "grad_norm": 2.0269711017608643,
      "learning_rate": 1.0622621281157867e-05,
      "loss": 0.5348,
      "step": 24210
    },
    {
      "epoch": 1.942261427425822,
      "grad_norm": 1.95577871799469,
      "learning_rate": 1.061458054140981e-05,
      "loss": 0.5359,
      "step": 24220
    },
    {
      "epoch": 1.9430633520449079,
      "grad_norm": 2.3424465656280518,
      "learning_rate": 1.0606539801661752e-05,
      "loss": 0.5358,
      "step": 24230
    },
    {
      "epoch": 1.9438652766639937,
      "grad_norm": 1.762428879737854,
      "learning_rate": 1.0598499061913696e-05,
      "loss": 0.5339,
      "step": 24240
    },
    {
      "epoch": 1.9446672012830795,
      "grad_norm": 1.9265300035476685,
      "learning_rate": 1.0590458322165639e-05,
      "loss": 0.5569,
      "step": 24250
    },
    {
      "epoch": 1.9454691259021653,
      "grad_norm": 1.9258023500442505,
      "learning_rate": 1.0582417582417583e-05,
      "loss": 0.5582,
      "step": 24260
    },
    {
      "epoch": 1.946271050521251,
      "grad_norm": 1.9051604270935059,
      "learning_rate": 1.0574376842669526e-05,
      "loss": 0.5486,
      "step": 24270
    },
    {
      "epoch": 1.9470729751403368,
      "grad_norm": 1.9573192596435547,
      "learning_rate": 1.0566336102921468e-05,
      "loss": 0.5721,
      "step": 24280
    },
    {
      "epoch": 1.9478748997594226,
      "grad_norm": 1.685020923614502,
      "learning_rate": 1.0558295363173413e-05,
      "loss": 0.5811,
      "step": 24290
    },
    {
      "epoch": 1.9486768243785084,
      "grad_norm": 1.9315998554229736,
      "learning_rate": 1.0550254623425355e-05,
      "loss": 0.5545,
      "step": 24300
    },
    {
      "epoch": 1.9494787489975942,
      "grad_norm": 1.5707547664642334,
      "learning_rate": 1.05422138836773e-05,
      "loss": 0.5229,
      "step": 24310
    },
    {
      "epoch": 1.95028067361668,
      "grad_norm": 2.01088547706604,
      "learning_rate": 1.0534173143929242e-05,
      "loss": 0.5345,
      "step": 24320
    },
    {
      "epoch": 1.9510825982357658,
      "grad_norm": 1.8116185665130615,
      "learning_rate": 1.0526132404181186e-05,
      "loss": 0.5287,
      "step": 24330
    },
    {
      "epoch": 1.9518845228548516,
      "grad_norm": 1.9812066555023193,
      "learning_rate": 1.0518091664433129e-05,
      "loss": 0.5041,
      "step": 24340
    },
    {
      "epoch": 1.9526864474739374,
      "grad_norm": 1.7271052598953247,
      "learning_rate": 1.051005092468507e-05,
      "loss": 0.5209,
      "step": 24350
    },
    {
      "epoch": 1.9534883720930232,
      "grad_norm": 1.6217436790466309,
      "learning_rate": 1.0502010184937014e-05,
      "loss": 0.5427,
      "step": 24360
    },
    {
      "epoch": 1.954290296712109,
      "grad_norm": 1.9198230504989624,
      "learning_rate": 1.0493969445188957e-05,
      "loss": 0.4919,
      "step": 24370
    },
    {
      "epoch": 1.9550922213311948,
      "grad_norm": 1.7549195289611816,
      "learning_rate": 1.0485928705440901e-05,
      "loss": 0.4958,
      "step": 24380
    },
    {
      "epoch": 1.9558941459502805,
      "grad_norm": 1.6051020622253418,
      "learning_rate": 1.0477887965692844e-05,
      "loss": 0.5507,
      "step": 24390
    },
    {
      "epoch": 1.9566960705693663,
      "grad_norm": 1.5076251029968262,
      "learning_rate": 1.0469847225944786e-05,
      "loss": 0.5229,
      "step": 24400
    },
    {
      "epoch": 1.9574979951884521,
      "grad_norm": 2.1373918056488037,
      "learning_rate": 1.046180648619673e-05,
      "loss": 0.5388,
      "step": 24410
    },
    {
      "epoch": 1.958299919807538,
      "grad_norm": 1.7835925817489624,
      "learning_rate": 1.0453765746448673e-05,
      "loss": 0.5498,
      "step": 24420
    },
    {
      "epoch": 1.959101844426624,
      "grad_norm": 2.029000759124756,
      "learning_rate": 1.0445725006700617e-05,
      "loss": 0.5913,
      "step": 24430
    },
    {
      "epoch": 1.9599037690457097,
      "grad_norm": 1.9393413066864014,
      "learning_rate": 1.043768426695256e-05,
      "loss": 0.5143,
      "step": 24440
    },
    {
      "epoch": 1.9607056936647955,
      "grad_norm": 1.8557884693145752,
      "learning_rate": 1.0429643527204504e-05,
      "loss": 0.5414,
      "step": 24450
    },
    {
      "epoch": 1.9615076182838813,
      "grad_norm": 1.9330228567123413,
      "learning_rate": 1.0421602787456447e-05,
      "loss": 0.5536,
      "step": 24460
    },
    {
      "epoch": 1.962309542902967,
      "grad_norm": 1.8508484363555908,
      "learning_rate": 1.041356204770839e-05,
      "loss": 0.4746,
      "step": 24470
    },
    {
      "epoch": 1.963111467522053,
      "grad_norm": 1.8211383819580078,
      "learning_rate": 1.0405521307960332e-05,
      "loss": 0.5005,
      "step": 24480
    },
    {
      "epoch": 1.9639133921411387,
      "grad_norm": 1.8643255233764648,
      "learning_rate": 1.0397480568212275e-05,
      "loss": 0.5755,
      "step": 24490
    },
    {
      "epoch": 1.9647153167602245,
      "grad_norm": 2.023493766784668,
      "learning_rate": 1.0389439828464219e-05,
      "loss": 0.5441,
      "step": 24500
    },
    {
      "epoch": 1.9655172413793105,
      "grad_norm": 1.7147969007492065,
      "learning_rate": 1.0381399088716162e-05,
      "loss": 0.6257,
      "step": 24510
    },
    {
      "epoch": 1.9663191659983963,
      "grad_norm": 2.1842916011810303,
      "learning_rate": 1.0373358348968106e-05,
      "loss": 0.5284,
      "step": 24520
    },
    {
      "epoch": 1.967121090617482,
      "grad_norm": 2.1792521476745605,
      "learning_rate": 1.0365317609220048e-05,
      "loss": 0.5504,
      "step": 24530
    },
    {
      "epoch": 1.9679230152365679,
      "grad_norm": 1.7110003232955933,
      "learning_rate": 1.0357276869471991e-05,
      "loss": 0.6119,
      "step": 24540
    },
    {
      "epoch": 1.9687249398556537,
      "grad_norm": 2.040886402130127,
      "learning_rate": 1.0349236129723935e-05,
      "loss": 0.5743,
      "step": 24550
    },
    {
      "epoch": 1.9695268644747395,
      "grad_norm": 1.927566647529602,
      "learning_rate": 1.0341195389975878e-05,
      "loss": 0.56,
      "step": 24560
    },
    {
      "epoch": 1.9703287890938253,
      "grad_norm": 2.175776720046997,
      "learning_rate": 1.0333154650227822e-05,
      "loss": 0.5328,
      "step": 24570
    },
    {
      "epoch": 1.971130713712911,
      "grad_norm": 1.6878576278686523,
      "learning_rate": 1.0325113910479765e-05,
      "loss": 0.5395,
      "step": 24580
    },
    {
      "epoch": 1.9719326383319968,
      "grad_norm": 1.841945767402649,
      "learning_rate": 1.0317073170731707e-05,
      "loss": 0.5214,
      "step": 24590
    },
    {
      "epoch": 1.9727345629510826,
      "grad_norm": 1.9626803398132324,
      "learning_rate": 1.030903243098365e-05,
      "loss": 0.5596,
      "step": 24600
    },
    {
      "epoch": 1.9735364875701684,
      "grad_norm": 2.0241756439208984,
      "learning_rate": 1.0300991691235593e-05,
      "loss": 0.5583,
      "step": 24610
    },
    {
      "epoch": 1.9743384121892542,
      "grad_norm": 1.7996233701705933,
      "learning_rate": 1.0292950951487537e-05,
      "loss": 0.5046,
      "step": 24620
    },
    {
      "epoch": 1.97514033680834,
      "grad_norm": 1.9904955625534058,
      "learning_rate": 1.028491021173948e-05,
      "loss": 0.5347,
      "step": 24630
    },
    {
      "epoch": 1.9759422614274258,
      "grad_norm": 1.7549470663070679,
      "learning_rate": 1.0276869471991424e-05,
      "loss": 0.5492,
      "step": 24640
    },
    {
      "epoch": 1.9767441860465116,
      "grad_norm": 2.16727614402771,
      "learning_rate": 1.0268828732243366e-05,
      "loss": 0.5645,
      "step": 24650
    },
    {
      "epoch": 1.9775461106655974,
      "grad_norm": 1.7030220031738281,
      "learning_rate": 1.0260787992495309e-05,
      "loss": 0.5971,
      "step": 24660
    },
    {
      "epoch": 1.9783480352846832,
      "grad_norm": 1.7466884851455688,
      "learning_rate": 1.0252747252747253e-05,
      "loss": 0.5706,
      "step": 24670
    },
    {
      "epoch": 1.979149959903769,
      "grad_norm": 1.7364981174468994,
      "learning_rate": 1.0244706512999196e-05,
      "loss": 0.5748,
      "step": 24680
    },
    {
      "epoch": 1.9799518845228548,
      "grad_norm": 1.9112135171890259,
      "learning_rate": 1.023666577325114e-05,
      "loss": 0.5415,
      "step": 24690
    },
    {
      "epoch": 1.9807538091419405,
      "grad_norm": 1.88375985622406,
      "learning_rate": 1.0228625033503083e-05,
      "loss": 0.4804,
      "step": 24700
    },
    {
      "epoch": 1.9815557337610263,
      "grad_norm": 1.9508721828460693,
      "learning_rate": 1.0220584293755027e-05,
      "loss": 0.5394,
      "step": 24710
    },
    {
      "epoch": 1.9823576583801121,
      "grad_norm": 1.640718698501587,
      "learning_rate": 1.021254355400697e-05,
      "loss": 0.5353,
      "step": 24720
    },
    {
      "epoch": 1.983159582999198,
      "grad_norm": 2.0522046089172363,
      "learning_rate": 1.020450281425891e-05,
      "loss": 0.5783,
      "step": 24730
    },
    {
      "epoch": 1.9839615076182837,
      "grad_norm": 1.7544565200805664,
      "learning_rate": 1.0196462074510855e-05,
      "loss": 0.5468,
      "step": 24740
    },
    {
      "epoch": 1.9847634322373697,
      "grad_norm": 1.8814619779586792,
      "learning_rate": 1.0188421334762797e-05,
      "loss": 0.529,
      "step": 24750
    },
    {
      "epoch": 1.9855653568564555,
      "grad_norm": 2.2419772148132324,
      "learning_rate": 1.0180380595014742e-05,
      "loss": 0.5681,
      "step": 24760
    },
    {
      "epoch": 1.9863672814755413,
      "grad_norm": 1.5982555150985718,
      "learning_rate": 1.0172339855266684e-05,
      "loss": 0.5602,
      "step": 24770
    },
    {
      "epoch": 1.987169206094627,
      "grad_norm": 2.186317205429077,
      "learning_rate": 1.0164299115518629e-05,
      "loss": 0.5242,
      "step": 24780
    },
    {
      "epoch": 1.987971130713713,
      "grad_norm": 2.2882204055786133,
      "learning_rate": 1.0156258375770571e-05,
      "loss": 0.5432,
      "step": 24790
    },
    {
      "epoch": 1.9887730553327987,
      "grad_norm": 1.8440760374069214,
      "learning_rate": 1.0148217636022514e-05,
      "loss": 0.5096,
      "step": 24800
    },
    {
      "epoch": 1.9895749799518845,
      "grad_norm": 1.697956919670105,
      "learning_rate": 1.0140176896274458e-05,
      "loss": 0.5798,
      "step": 24810
    },
    {
      "epoch": 1.9903769045709703,
      "grad_norm": 1.7204711437225342,
      "learning_rate": 1.01321361565264e-05,
      "loss": 0.5397,
      "step": 24820
    },
    {
      "epoch": 1.9911788291900563,
      "grad_norm": 1.9163129329681396,
      "learning_rate": 1.0124095416778345e-05,
      "loss": 0.5748,
      "step": 24830
    },
    {
      "epoch": 1.991980753809142,
      "grad_norm": 1.9184441566467285,
      "learning_rate": 1.0116054677030288e-05,
      "loss": 0.5374,
      "step": 24840
    },
    {
      "epoch": 1.9927826784282279,
      "grad_norm": 1.963163137435913,
      "learning_rate": 1.010801393728223e-05,
      "loss": 0.5452,
      "step": 24850
    },
    {
      "epoch": 1.9935846030473137,
      "grad_norm": 1.6627558469772339,
      "learning_rate": 1.0099973197534173e-05,
      "loss": 0.5637,
      "step": 24860
    },
    {
      "epoch": 1.9943865276663995,
      "grad_norm": 1.8411463499069214,
      "learning_rate": 1.0091932457786115e-05,
      "loss": 0.5822,
      "step": 24870
    },
    {
      "epoch": 1.9951884522854852,
      "grad_norm": 1.7830601930618286,
      "learning_rate": 1.008389171803806e-05,
      "loss": 0.5137,
      "step": 24880
    },
    {
      "epoch": 1.995990376904571,
      "grad_norm": 1.8464735746383667,
      "learning_rate": 1.0075850978290002e-05,
      "loss": 0.4885,
      "step": 24890
    },
    {
      "epoch": 1.9967923015236568,
      "grad_norm": 2.210249662399292,
      "learning_rate": 1.0067810238541947e-05,
      "loss": 0.6111,
      "step": 24900
    },
    {
      "epoch": 1.9975942261427426,
      "grad_norm": 2.3808176517486572,
      "learning_rate": 1.005976949879389e-05,
      "loss": 0.5831,
      "step": 24910
    },
    {
      "epoch": 1.9983961507618284,
      "grad_norm": 2.04911208152771,
      "learning_rate": 1.0051728759045832e-05,
      "loss": 0.5394,
      "step": 24920
    },
    {
      "epoch": 1.9991980753809142,
      "grad_norm": 1.851192593574524,
      "learning_rate": 1.0043688019297776e-05,
      "loss": 0.5407,
      "step": 24930
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.913995623588562,
      "learning_rate": 1.0035647279549719e-05,
      "loss": 0.5295,
      "step": 24940
    },
    {
      "epoch": 2.000801924619086,
      "grad_norm": 1.5739052295684814,
      "learning_rate": 1.0027606539801663e-05,
      "loss": 0.502,
      "step": 24950
    },
    {
      "epoch": 2.0016038492381716,
      "grad_norm": 1.5425243377685547,
      "learning_rate": 1.0019565800053606e-05,
      "loss": 0.5178,
      "step": 24960
    },
    {
      "epoch": 2.0024057738572574,
      "grad_norm": 1.729718804359436,
      "learning_rate": 1.001152506030555e-05,
      "loss": 0.5124,
      "step": 24970
    },
    {
      "epoch": 2.003207698476343,
      "grad_norm": 1.8536416292190552,
      "learning_rate": 1.0003484320557492e-05,
      "loss": 0.562,
      "step": 24980
    },
    {
      "epoch": 2.004009623095429,
      "grad_norm": 2.1270883083343506,
      "learning_rate": 9.995443580809433e-06,
      "loss": 0.5611,
      "step": 24990
    },
    {
      "epoch": 2.0048115477145148,
      "grad_norm": 1.7983453273773193,
      "learning_rate": 9.987402841061378e-06,
      "loss": 0.5278,
      "step": 25000
    },
    {
      "epoch": 2.0056134723336005,
      "grad_norm": 2.022455930709839,
      "learning_rate": 9.97936210131332e-06,
      "loss": 0.5569,
      "step": 25010
    },
    {
      "epoch": 2.0064153969526863,
      "grad_norm": 1.838059425354004,
      "learning_rate": 9.971321361565264e-06,
      "loss": 0.5094,
      "step": 25020
    },
    {
      "epoch": 2.007217321571772,
      "grad_norm": 2.0090041160583496,
      "learning_rate": 9.963280621817207e-06,
      "loss": 0.538,
      "step": 25030
    },
    {
      "epoch": 2.008019246190858,
      "grad_norm": 1.9025803804397583,
      "learning_rate": 9.95523988206915e-06,
      "loss": 0.5326,
      "step": 25040
    },
    {
      "epoch": 2.0088211708099437,
      "grad_norm": 1.9076776504516602,
      "learning_rate": 9.947199142321094e-06,
      "loss": 0.4847,
      "step": 25050
    },
    {
      "epoch": 2.0096230954290295,
      "grad_norm": 2.031604290008545,
      "learning_rate": 9.939158402573037e-06,
      "loss": 0.5721,
      "step": 25060
    },
    {
      "epoch": 2.0104250200481153,
      "grad_norm": 1.8895074129104614,
      "learning_rate": 9.931117662824981e-06,
      "loss": 0.5389,
      "step": 25070
    },
    {
      "epoch": 2.011226944667201,
      "grad_norm": 2.232125759124756,
      "learning_rate": 9.923076923076923e-06,
      "loss": 0.5283,
      "step": 25080
    },
    {
      "epoch": 2.012028869286287,
      "grad_norm": 1.819069743156433,
      "learning_rate": 9.915036183328868e-06,
      "loss": 0.6129,
      "step": 25090
    },
    {
      "epoch": 2.0128307939053727,
      "grad_norm": 1.9212251901626587,
      "learning_rate": 9.90699544358081e-06,
      "loss": 0.5311,
      "step": 25100
    },
    {
      "epoch": 2.013632718524459,
      "grad_norm": 1.9793117046356201,
      "learning_rate": 9.898954703832753e-06,
      "loss": 0.5357,
      "step": 25110
    },
    {
      "epoch": 2.0144346431435447,
      "grad_norm": 1.6515365839004517,
      "learning_rate": 9.890913964084696e-06,
      "loss": 0.4967,
      "step": 25120
    },
    {
      "epoch": 2.0152365677626305,
      "grad_norm": 1.6548347473144531,
      "learning_rate": 9.882873224336638e-06,
      "loss": 0.5158,
      "step": 25130
    },
    {
      "epoch": 2.0160384923817163,
      "grad_norm": 1.855186104774475,
      "learning_rate": 9.874832484588582e-06,
      "loss": 0.5246,
      "step": 25140
    },
    {
      "epoch": 2.016840417000802,
      "grad_norm": 2.192873239517212,
      "learning_rate": 9.866791744840525e-06,
      "loss": 0.6198,
      "step": 25150
    },
    {
      "epoch": 2.017642341619888,
      "grad_norm": 2.2567310333251953,
      "learning_rate": 9.85875100509247e-06,
      "loss": 0.5324,
      "step": 25160
    },
    {
      "epoch": 2.0184442662389737,
      "grad_norm": 1.9314957857131958,
      "learning_rate": 9.850710265344412e-06,
      "loss": 0.5187,
      "step": 25170
    },
    {
      "epoch": 2.0192461908580595,
      "grad_norm": 1.915031909942627,
      "learning_rate": 9.842669525596355e-06,
      "loss": 0.5909,
      "step": 25180
    },
    {
      "epoch": 2.0200481154771452,
      "grad_norm": 1.9267383813858032,
      "learning_rate": 9.834628785848299e-06,
      "loss": 0.5319,
      "step": 25190
    },
    {
      "epoch": 2.020850040096231,
      "grad_norm": 1.9191725254058838,
      "learning_rate": 9.826588046100241e-06,
      "loss": 0.5373,
      "step": 25200
    },
    {
      "epoch": 2.021651964715317,
      "grad_norm": 2.0080273151397705,
      "learning_rate": 9.818547306352186e-06,
      "loss": 0.5336,
      "step": 25210
    },
    {
      "epoch": 2.0224538893344026,
      "grad_norm": 1.871055006980896,
      "learning_rate": 9.810506566604128e-06,
      "loss": 0.5457,
      "step": 25220
    },
    {
      "epoch": 2.0232558139534884,
      "grad_norm": 1.8081914186477661,
      "learning_rate": 9.802465826856071e-06,
      "loss": 0.56,
      "step": 25230
    },
    {
      "epoch": 2.024057738572574,
      "grad_norm": 1.8504176139831543,
      "learning_rate": 9.794425087108015e-06,
      "loss": 0.556,
      "step": 25240
    },
    {
      "epoch": 2.02485966319166,
      "grad_norm": 1.8287162780761719,
      "learning_rate": 9.786384347359956e-06,
      "loss": 0.5163,
      "step": 25250
    },
    {
      "epoch": 2.025661587810746,
      "grad_norm": 2.317064046859741,
      "learning_rate": 9.7783436076119e-06,
      "loss": 0.5832,
      "step": 25260
    },
    {
      "epoch": 2.0264635124298316,
      "grad_norm": 1.87588369846344,
      "learning_rate": 9.770302867863843e-06,
      "loss": 0.5528,
      "step": 25270
    },
    {
      "epoch": 2.0272654370489174,
      "grad_norm": 2.0808815956115723,
      "learning_rate": 9.762262128115787e-06,
      "loss": 0.5058,
      "step": 25280
    },
    {
      "epoch": 2.028067361668003,
      "grad_norm": 2.0108821392059326,
      "learning_rate": 9.75422138836773e-06,
      "loss": 0.5478,
      "step": 25290
    },
    {
      "epoch": 2.028869286287089,
      "grad_norm": 1.906446933746338,
      "learning_rate": 9.746180648619672e-06,
      "loss": 0.5425,
      "step": 25300
    },
    {
      "epoch": 2.0296712109061747,
      "grad_norm": 1.9188549518585205,
      "learning_rate": 9.738139908871617e-06,
      "loss": 0.5846,
      "step": 25310
    },
    {
      "epoch": 2.0304731355252605,
      "grad_norm": 1.6670640707015991,
      "learning_rate": 9.73009916912356e-06,
      "loss": 0.4843,
      "step": 25320
    },
    {
      "epoch": 2.0312750601443463,
      "grad_norm": 2.063913106918335,
      "learning_rate": 9.722058429375504e-06,
      "loss": 0.5958,
      "step": 25330
    },
    {
      "epoch": 2.032076984763432,
      "grad_norm": 1.9557147026062012,
      "learning_rate": 9.714017689627446e-06,
      "loss": 0.5734,
      "step": 25340
    },
    {
      "epoch": 2.032878909382518,
      "grad_norm": 1.9875568151474,
      "learning_rate": 9.70597694987939e-06,
      "loss": 0.5744,
      "step": 25350
    },
    {
      "epoch": 2.0336808340016037,
      "grad_norm": 1.9466664791107178,
      "learning_rate": 9.697936210131333e-06,
      "loss": 0.5312,
      "step": 25360
    },
    {
      "epoch": 2.0344827586206895,
      "grad_norm": 1.8782522678375244,
      "learning_rate": 9.689895470383274e-06,
      "loss": 0.5848,
      "step": 25370
    },
    {
      "epoch": 2.0352846832397753,
      "grad_norm": 1.7202119827270508,
      "learning_rate": 9.681854730635218e-06,
      "loss": 0.4774,
      "step": 25380
    },
    {
      "epoch": 2.036086607858861,
      "grad_norm": 2.0626323223114014,
      "learning_rate": 9.673813990887161e-06,
      "loss": 0.4956,
      "step": 25390
    },
    {
      "epoch": 2.036888532477947,
      "grad_norm": 1.8278330564498901,
      "learning_rate": 9.665773251139105e-06,
      "loss": 0.4876,
      "step": 25400
    },
    {
      "epoch": 2.0376904570970327,
      "grad_norm": 1.757952332496643,
      "learning_rate": 9.657732511391048e-06,
      "loss": 0.4767,
      "step": 25410
    },
    {
      "epoch": 2.038492381716119,
      "grad_norm": 1.842411994934082,
      "learning_rate": 9.649691771642992e-06,
      "loss": 0.5283,
      "step": 25420
    },
    {
      "epoch": 2.0392943063352047,
      "grad_norm": 1.883243203163147,
      "learning_rate": 9.641651031894935e-06,
      "loss": 0.5229,
      "step": 25430
    },
    {
      "epoch": 2.0400962309542905,
      "grad_norm": 2.0618155002593994,
      "learning_rate": 9.633610292146877e-06,
      "loss": 0.5256,
      "step": 25440
    },
    {
      "epoch": 2.0408981555733763,
      "grad_norm": 1.8936781883239746,
      "learning_rate": 9.625569552398822e-06,
      "loss": 0.591,
      "step": 25450
    },
    {
      "epoch": 2.041700080192462,
      "grad_norm": 1.7511603832244873,
      "learning_rate": 9.617528812650764e-06,
      "loss": 0.5349,
      "step": 25460
    },
    {
      "epoch": 2.042502004811548,
      "grad_norm": 1.721121907234192,
      "learning_rate": 9.609488072902708e-06,
      "loss": 0.5966,
      "step": 25470
    },
    {
      "epoch": 2.0433039294306337,
      "grad_norm": 1.7389039993286133,
      "learning_rate": 9.601447333154651e-06,
      "loss": 0.5448,
      "step": 25480
    },
    {
      "epoch": 2.0441058540497195,
      "grad_norm": 2.0406038761138916,
      "learning_rate": 9.593406593406594e-06,
      "loss": 0.5024,
      "step": 25490
    },
    {
      "epoch": 2.0449077786688052,
      "grad_norm": 1.780319333076477,
      "learning_rate": 9.585365853658536e-06,
      "loss": 0.5661,
      "step": 25500
    },
    {
      "epoch": 2.045709703287891,
      "grad_norm": 1.8024011850357056,
      "learning_rate": 9.577325113910479e-06,
      "loss": 0.5458,
      "step": 25510
    },
    {
      "epoch": 2.046511627906977,
      "grad_norm": 2.106905698776245,
      "learning_rate": 9.569284374162423e-06,
      "loss": 0.5451,
      "step": 25520
    },
    {
      "epoch": 2.0473135525260626,
      "grad_norm": 1.6070951223373413,
      "learning_rate": 9.561243634414366e-06,
      "loss": 0.5338,
      "step": 25530
    },
    {
      "epoch": 2.0481154771451484,
      "grad_norm": 2.165365695953369,
      "learning_rate": 9.55320289466631e-06,
      "loss": 0.5437,
      "step": 25540
    },
    {
      "epoch": 2.048917401764234,
      "grad_norm": 1.8990678787231445,
      "learning_rate": 9.545162154918253e-06,
      "loss": 0.5845,
      "step": 25550
    },
    {
      "epoch": 2.04971932638332,
      "grad_norm": 2.1501801013946533,
      "learning_rate": 9.537121415170195e-06,
      "loss": 0.5044,
      "step": 25560
    },
    {
      "epoch": 2.050521251002406,
      "grad_norm": 1.7685617208480835,
      "learning_rate": 9.52908067542214e-06,
      "loss": 0.523,
      "step": 25570
    },
    {
      "epoch": 2.0513231756214916,
      "grad_norm": 2.009565591812134,
      "learning_rate": 9.521039935674082e-06,
      "loss": 0.5246,
      "step": 25580
    },
    {
      "epoch": 2.0521251002405774,
      "grad_norm": 1.829712152481079,
      "learning_rate": 9.512999195926026e-06,
      "loss": 0.5732,
      "step": 25590
    },
    {
      "epoch": 2.052927024859663,
      "grad_norm": 1.828237771987915,
      "learning_rate": 9.504958456177969e-06,
      "loss": 0.5258,
      "step": 25600
    },
    {
      "epoch": 2.053728949478749,
      "grad_norm": 1.888617992401123,
      "learning_rate": 9.496917716429913e-06,
      "loss": 0.5405,
      "step": 25610
    },
    {
      "epoch": 2.0545308740978347,
      "grad_norm": 1.9831109046936035,
      "learning_rate": 9.488876976681856e-06,
      "loss": 0.5631,
      "step": 25620
    },
    {
      "epoch": 2.0553327987169205,
      "grad_norm": 1.740384817123413,
      "learning_rate": 9.480836236933797e-06,
      "loss": 0.5988,
      "step": 25630
    },
    {
      "epoch": 2.0561347233360063,
      "grad_norm": 1.607844352722168,
      "learning_rate": 9.472795497185741e-06,
      "loss": 0.5262,
      "step": 25640
    },
    {
      "epoch": 2.056936647955092,
      "grad_norm": 1.865390419960022,
      "learning_rate": 9.464754757437684e-06,
      "loss": 0.5391,
      "step": 25650
    },
    {
      "epoch": 2.057738572574178,
      "grad_norm": 1.951954960823059,
      "learning_rate": 9.456714017689628e-06,
      "loss": 0.5339,
      "step": 25660
    },
    {
      "epoch": 2.0585404971932637,
      "grad_norm": 1.9531528949737549,
      "learning_rate": 9.44867327794157e-06,
      "loss": 0.5506,
      "step": 25670
    },
    {
      "epoch": 2.0593424218123495,
      "grad_norm": 1.6080176830291748,
      "learning_rate": 9.440632538193513e-06,
      "loss": 0.532,
      "step": 25680
    },
    {
      "epoch": 2.0601443464314353,
      "grad_norm": 2.525395631790161,
      "learning_rate": 9.432591798445457e-06,
      "loss": 0.4903,
      "step": 25690
    },
    {
      "epoch": 2.060946271050521,
      "grad_norm": 1.792650580406189,
      "learning_rate": 9.4245510586974e-06,
      "loss": 0.5937,
      "step": 25700
    },
    {
      "epoch": 2.061748195669607,
      "grad_norm": 1.9313350915908813,
      "learning_rate": 9.416510318949344e-06,
      "loss": 0.5201,
      "step": 25710
    },
    {
      "epoch": 2.0625501202886927,
      "grad_norm": 1.8489766120910645,
      "learning_rate": 9.408469579201287e-06,
      "loss": 0.6023,
      "step": 25720
    },
    {
      "epoch": 2.0633520449077785,
      "grad_norm": 1.5646355152130127,
      "learning_rate": 9.400428839453231e-06,
      "loss": 0.5253,
      "step": 25730
    },
    {
      "epoch": 2.0641539695268643,
      "grad_norm": 1.8089969158172607,
      "learning_rate": 9.392388099705174e-06,
      "loss": 0.5005,
      "step": 25740
    },
    {
      "epoch": 2.0649558941459505,
      "grad_norm": 1.757819414138794,
      "learning_rate": 9.384347359957116e-06,
      "loss": 0.6018,
      "step": 25750
    },
    {
      "epoch": 2.0657578187650363,
      "grad_norm": 2.003793478012085,
      "learning_rate": 9.376306620209059e-06,
      "loss": 0.5175,
      "step": 25760
    },
    {
      "epoch": 2.066559743384122,
      "grad_norm": 1.899280071258545,
      "learning_rate": 9.368265880461002e-06,
      "loss": 0.5454,
      "step": 25770
    },
    {
      "epoch": 2.067361668003208,
      "grad_norm": 2.0956203937530518,
      "learning_rate": 9.360225140712946e-06,
      "loss": 0.5834,
      "step": 25780
    },
    {
      "epoch": 2.0681635926222937,
      "grad_norm": 2.3006250858306885,
      "learning_rate": 9.352184400964888e-06,
      "loss": 0.5499,
      "step": 25790
    },
    {
      "epoch": 2.0689655172413794,
      "grad_norm": 2.005310297012329,
      "learning_rate": 9.344143661216833e-06,
      "loss": 0.5617,
      "step": 25800
    },
    {
      "epoch": 2.0697674418604652,
      "grad_norm": 1.739096999168396,
      "learning_rate": 9.336102921468775e-06,
      "loss": 0.5035,
      "step": 25810
    },
    {
      "epoch": 2.070569366479551,
      "grad_norm": 1.875439167022705,
      "learning_rate": 9.328062181720718e-06,
      "loss": 0.4953,
      "step": 25820
    },
    {
      "epoch": 2.071371291098637,
      "grad_norm": 2.0364885330200195,
      "learning_rate": 9.320021441972662e-06,
      "loss": 0.583,
      "step": 25830
    },
    {
      "epoch": 2.0721732157177226,
      "grad_norm": 1.73484468460083,
      "learning_rate": 9.311980702224605e-06,
      "loss": 0.5065,
      "step": 25840
    },
    {
      "epoch": 2.0729751403368084,
      "grad_norm": 1.9392491579055786,
      "learning_rate": 9.303939962476549e-06,
      "loss": 0.6149,
      "step": 25850
    },
    {
      "epoch": 2.073777064955894,
      "grad_norm": 1.6589441299438477,
      "learning_rate": 9.295899222728492e-06,
      "loss": 0.5641,
      "step": 25860
    },
    {
      "epoch": 2.07457898957498,
      "grad_norm": 1.6149359941482544,
      "learning_rate": 9.287858482980434e-06,
      "loss": 0.532,
      "step": 25870
    },
    {
      "epoch": 2.075380914194066,
      "grad_norm": 1.643160343170166,
      "learning_rate": 9.279817743232379e-06,
      "loss": 0.4845,
      "step": 25880
    },
    {
      "epoch": 2.0761828388131516,
      "grad_norm": 2.1128997802734375,
      "learning_rate": 9.27177700348432e-06,
      "loss": 0.586,
      "step": 25890
    },
    {
      "epoch": 2.0769847634322374,
      "grad_norm": 1.5121827125549316,
      "learning_rate": 9.263736263736264e-06,
      "loss": 0.5047,
      "step": 25900
    },
    {
      "epoch": 2.077786688051323,
      "grad_norm": 2.158236026763916,
      "learning_rate": 9.255695523988206e-06,
      "loss": 0.5262,
      "step": 25910
    },
    {
      "epoch": 2.078588612670409,
      "grad_norm": 1.9652451276779175,
      "learning_rate": 9.24765478424015e-06,
      "loss": 0.5753,
      "step": 25920
    },
    {
      "epoch": 2.0793905372894947,
      "grad_norm": 1.6014018058776855,
      "learning_rate": 9.239614044492093e-06,
      "loss": 0.5316,
      "step": 25930
    },
    {
      "epoch": 2.0801924619085805,
      "grad_norm": 1.9759823083877563,
      "learning_rate": 9.231573304744036e-06,
      "loss": 0.5232,
      "step": 25940
    },
    {
      "epoch": 2.0809943865276663,
      "grad_norm": 1.999956727027893,
      "learning_rate": 9.22353256499598e-06,
      "loss": 0.5017,
      "step": 25950
    },
    {
      "epoch": 2.081796311146752,
      "grad_norm": 1.6060458421707153,
      "learning_rate": 9.215491825247923e-06,
      "loss": 0.4867,
      "step": 25960
    },
    {
      "epoch": 2.082598235765838,
      "grad_norm": 2.0678133964538574,
      "learning_rate": 9.207451085499867e-06,
      "loss": 0.5846,
      "step": 25970
    },
    {
      "epoch": 2.0834001603849237,
      "grad_norm": 1.7946455478668213,
      "learning_rate": 9.19941034575181e-06,
      "loss": 0.5507,
      "step": 25980
    },
    {
      "epoch": 2.0842020850040095,
      "grad_norm": 2.2139813899993896,
      "learning_rate": 9.191369606003754e-06,
      "loss": 0.5892,
      "step": 25990
    },
    {
      "epoch": 2.0850040096230953,
      "grad_norm": 1.8895915746688843,
      "learning_rate": 9.183328866255697e-06,
      "loss": 0.5723,
      "step": 26000
    },
    {
      "epoch": 2.085805934242181,
      "grad_norm": 1.9269273281097412,
      "learning_rate": 9.175288126507639e-06,
      "loss": 0.5868,
      "step": 26010
    },
    {
      "epoch": 2.086607858861267,
      "grad_norm": 2.131950616836548,
      "learning_rate": 9.167247386759582e-06,
      "loss": 0.6038,
      "step": 26020
    },
    {
      "epoch": 2.0874097834803527,
      "grad_norm": 1.7538114786148071,
      "learning_rate": 9.159206647011524e-06,
      "loss": 0.5653,
      "step": 26030
    },
    {
      "epoch": 2.0882117080994385,
      "grad_norm": 1.63905668258667,
      "learning_rate": 9.151165907263469e-06,
      "loss": 0.4815,
      "step": 26040
    },
    {
      "epoch": 2.0890136327185242,
      "grad_norm": 1.849825143814087,
      "learning_rate": 9.143125167515411e-06,
      "loss": 0.5663,
      "step": 26050
    },
    {
      "epoch": 2.0898155573376105,
      "grad_norm": 1.5774015188217163,
      "learning_rate": 9.135084427767355e-06,
      "loss": 0.4973,
      "step": 26060
    },
    {
      "epoch": 2.0906174819566963,
      "grad_norm": 1.9501866102218628,
      "learning_rate": 9.127043688019298e-06,
      "loss": 0.5484,
      "step": 26070
    },
    {
      "epoch": 2.091419406575782,
      "grad_norm": 2.249978542327881,
      "learning_rate": 9.11900294827124e-06,
      "loss": 0.5715,
      "step": 26080
    },
    {
      "epoch": 2.092221331194868,
      "grad_norm": 1.787113904953003,
      "learning_rate": 9.110962208523185e-06,
      "loss": 0.5259,
      "step": 26090
    },
    {
      "epoch": 2.0930232558139537,
      "grad_norm": 1.80519437789917,
      "learning_rate": 9.102921468775128e-06,
      "loss": 0.5364,
      "step": 26100
    },
    {
      "epoch": 2.0938251804330394,
      "grad_norm": 2.204251766204834,
      "learning_rate": 9.094880729027072e-06,
      "loss": 0.5517,
      "step": 26110
    },
    {
      "epoch": 2.0946271050521252,
      "grad_norm": 1.8192217350006104,
      "learning_rate": 9.086839989279014e-06,
      "loss": 0.528,
      "step": 26120
    },
    {
      "epoch": 2.095429029671211,
      "grad_norm": 1.783150315284729,
      "learning_rate": 9.078799249530957e-06,
      "loss": 0.5785,
      "step": 26130
    },
    {
      "epoch": 2.096230954290297,
      "grad_norm": 1.6735492944717407,
      "learning_rate": 9.0707585097829e-06,
      "loss": 0.5383,
      "step": 26140
    },
    {
      "epoch": 2.0970328789093826,
      "grad_norm": 1.7580560445785522,
      "learning_rate": 9.062717770034842e-06,
      "loss": 0.4922,
      "step": 26150
    },
    {
      "epoch": 2.0978348035284684,
      "grad_norm": 1.7916641235351562,
      "learning_rate": 9.054677030286787e-06,
      "loss": 0.5237,
      "step": 26160
    },
    {
      "epoch": 2.098636728147554,
      "grad_norm": 2.1449315547943115,
      "learning_rate": 9.046636290538729e-06,
      "loss": 0.4785,
      "step": 26170
    },
    {
      "epoch": 2.09943865276664,
      "grad_norm": 2.300060510635376,
      "learning_rate": 9.038595550790673e-06,
      "loss": 0.5051,
      "step": 26180
    },
    {
      "epoch": 2.100240577385726,
      "grad_norm": 2.37892746925354,
      "learning_rate": 9.030554811042616e-06,
      "loss": 0.5408,
      "step": 26190
    },
    {
      "epoch": 2.1010425020048116,
      "grad_norm": 1.75773024559021,
      "learning_rate": 9.022514071294559e-06,
      "loss": 0.5267,
      "step": 26200
    },
    {
      "epoch": 2.1018444266238974,
      "grad_norm": 2.1031887531280518,
      "learning_rate": 9.014473331546503e-06,
      "loss": 0.5287,
      "step": 26210
    },
    {
      "epoch": 2.102646351242983,
      "grad_norm": 1.9036771059036255,
      "learning_rate": 9.006432591798445e-06,
      "loss": 0.5628,
      "step": 26220
    },
    {
      "epoch": 2.103448275862069,
      "grad_norm": 1.8190668821334839,
      "learning_rate": 8.99839185205039e-06,
      "loss": 0.5377,
      "step": 26230
    },
    {
      "epoch": 2.1042502004811547,
      "grad_norm": 1.9651275873184204,
      "learning_rate": 8.990351112302332e-06,
      "loss": 0.5515,
      "step": 26240
    },
    {
      "epoch": 2.1050521251002405,
      "grad_norm": 1.9118613004684448,
      "learning_rate": 8.982310372554277e-06,
      "loss": 0.5291,
      "step": 26250
    },
    {
      "epoch": 2.1058540497193263,
      "grad_norm": 2.4369072914123535,
      "learning_rate": 8.97426963280622e-06,
      "loss": 0.6269,
      "step": 26260
    },
    {
      "epoch": 2.106655974338412,
      "grad_norm": 1.749930500984192,
      "learning_rate": 8.96622889305816e-06,
      "loss": 0.5439,
      "step": 26270
    },
    {
      "epoch": 2.107457898957498,
      "grad_norm": 1.8617900609970093,
      "learning_rate": 8.958188153310104e-06,
      "loss": 0.5662,
      "step": 26280
    },
    {
      "epoch": 2.1082598235765837,
      "grad_norm": 1.7587666511535645,
      "learning_rate": 8.950147413562047e-06,
      "loss": 0.5428,
      "step": 26290
    },
    {
      "epoch": 2.1090617481956695,
      "grad_norm": 1.8920223712921143,
      "learning_rate": 8.942106673813991e-06,
      "loss": 0.5242,
      "step": 26300
    },
    {
      "epoch": 2.1098636728147553,
      "grad_norm": 1.9766998291015625,
      "learning_rate": 8.934065934065934e-06,
      "loss": 0.4809,
      "step": 26310
    },
    {
      "epoch": 2.110665597433841,
      "grad_norm": 2.066577672958374,
      "learning_rate": 8.926025194317877e-06,
      "loss": 0.5506,
      "step": 26320
    },
    {
      "epoch": 2.111467522052927,
      "grad_norm": 1.7590008974075317,
      "learning_rate": 8.91798445456982e-06,
      "loss": 0.6007,
      "step": 26330
    },
    {
      "epoch": 2.1122694466720127,
      "grad_norm": 1.9317249059677124,
      "learning_rate": 8.909943714821763e-06,
      "loss": 0.5785,
      "step": 26340
    },
    {
      "epoch": 2.1130713712910985,
      "grad_norm": 1.8322399854660034,
      "learning_rate": 8.901902975073708e-06,
      "loss": 0.5757,
      "step": 26350
    },
    {
      "epoch": 2.1138732959101842,
      "grad_norm": 1.6268259286880493,
      "learning_rate": 8.89386223532565e-06,
      "loss": 0.529,
      "step": 26360
    },
    {
      "epoch": 2.11467522052927,
      "grad_norm": 1.967252254486084,
      "learning_rate": 8.885821495577595e-06,
      "loss": 0.5746,
      "step": 26370
    },
    {
      "epoch": 2.115477145148356,
      "grad_norm": 1.7334139347076416,
      "learning_rate": 8.877780755829537e-06,
      "loss": 0.5015,
      "step": 26380
    },
    {
      "epoch": 2.116279069767442,
      "grad_norm": 1.8233884572982788,
      "learning_rate": 8.86974001608148e-06,
      "loss": 0.52,
      "step": 26390
    },
    {
      "epoch": 2.117080994386528,
      "grad_norm": 1.8984718322753906,
      "learning_rate": 8.861699276333422e-06,
      "loss": 0.5349,
      "step": 26400
    },
    {
      "epoch": 2.1178829190056137,
      "grad_norm": 1.8463900089263916,
      "learning_rate": 8.853658536585365e-06,
      "loss": 0.5767,
      "step": 26410
    },
    {
      "epoch": 2.1186848436246994,
      "grad_norm": 1.8377636671066284,
      "learning_rate": 8.84561779683731e-06,
      "loss": 0.5301,
      "step": 26420
    },
    {
      "epoch": 2.1194867682437852,
      "grad_norm": 1.8584119081497192,
      "learning_rate": 8.837577057089252e-06,
      "loss": 0.5265,
      "step": 26430
    },
    {
      "epoch": 2.120288692862871,
      "grad_norm": 1.959907054901123,
      "learning_rate": 8.829536317341196e-06,
      "loss": 0.6004,
      "step": 26440
    },
    {
      "epoch": 2.121090617481957,
      "grad_norm": 1.7450525760650635,
      "learning_rate": 8.821495577593139e-06,
      "loss": 0.5735,
      "step": 26450
    },
    {
      "epoch": 2.1218925421010426,
      "grad_norm": 2.0079519748687744,
      "learning_rate": 8.813454837845081e-06,
      "loss": 0.5973,
      "step": 26460
    },
    {
      "epoch": 2.1226944667201284,
      "grad_norm": 2.0555458068847656,
      "learning_rate": 8.805414098097026e-06,
      "loss": 0.5351,
      "step": 26470
    },
    {
      "epoch": 2.123496391339214,
      "grad_norm": 1.4908363819122314,
      "learning_rate": 8.797373358348968e-06,
      "loss": 0.5315,
      "step": 26480
    },
    {
      "epoch": 2.1242983159583,
      "grad_norm": 1.7970517873764038,
      "learning_rate": 8.789332618600913e-06,
      "loss": 0.5208,
      "step": 26490
    },
    {
      "epoch": 2.125100240577386,
      "grad_norm": 1.5715080499649048,
      "learning_rate": 8.781291878852855e-06,
      "loss": 0.5239,
      "step": 26500
    },
    {
      "epoch": 2.1259021651964716,
      "grad_norm": 1.9493111371994019,
      "learning_rate": 8.773251139104798e-06,
      "loss": 0.5413,
      "step": 26510
    },
    {
      "epoch": 2.1267040898155574,
      "grad_norm": 2.154991388320923,
      "learning_rate": 8.765210399356742e-06,
      "loss": 0.5408,
      "step": 26520
    },
    {
      "epoch": 2.127506014434643,
      "grad_norm": 1.822042465209961,
      "learning_rate": 8.757169659608683e-06,
      "loss": 0.552,
      "step": 26530
    },
    {
      "epoch": 2.128307939053729,
      "grad_norm": 1.847663402557373,
      "learning_rate": 8.749128919860627e-06,
      "loss": 0.5164,
      "step": 26540
    },
    {
      "epoch": 2.1291098636728147,
      "grad_norm": 1.7321579456329346,
      "learning_rate": 8.74108818011257e-06,
      "loss": 0.5578,
      "step": 26550
    },
    {
      "epoch": 2.1299117882919005,
      "grad_norm": 1.8819994926452637,
      "learning_rate": 8.733047440364514e-06,
      "loss": 0.567,
      "step": 26560
    },
    {
      "epoch": 2.1307137129109863,
      "grad_norm": 1.7878544330596924,
      "learning_rate": 8.725006700616457e-06,
      "loss": 0.5273,
      "step": 26570
    },
    {
      "epoch": 2.131515637530072,
      "grad_norm": 2.2267556190490723,
      "learning_rate": 8.7169659608684e-06,
      "loss": 0.5798,
      "step": 26580
    },
    {
      "epoch": 2.132317562149158,
      "grad_norm": 1.8780202865600586,
      "learning_rate": 8.708925221120344e-06,
      "loss": 0.502,
      "step": 26590
    },
    {
      "epoch": 2.1331194867682437,
      "grad_norm": 1.8837553262710571,
      "learning_rate": 8.700884481372286e-06,
      "loss": 0.5744,
      "step": 26600
    },
    {
      "epoch": 2.1339214113873295,
      "grad_norm": 2.2081329822540283,
      "learning_rate": 8.69284374162423e-06,
      "loss": 0.5075,
      "step": 26610
    },
    {
      "epoch": 2.1347233360064153,
      "grad_norm": 1.763828158378601,
      "learning_rate": 8.684803001876173e-06,
      "loss": 0.5461,
      "step": 26620
    },
    {
      "epoch": 2.135525260625501,
      "grad_norm": 2.0461323261260986,
      "learning_rate": 8.676762262128117e-06,
      "loss": 0.5121,
      "step": 26630
    },
    {
      "epoch": 2.136327185244587,
      "grad_norm": 2.0517444610595703,
      "learning_rate": 8.66872152238006e-06,
      "loss": 0.5445,
      "step": 26640
    },
    {
      "epoch": 2.1371291098636727,
      "grad_norm": 2.1028809547424316,
      "learning_rate": 8.660680782632003e-06,
      "loss": 0.5394,
      "step": 26650
    },
    {
      "epoch": 2.1379310344827585,
      "grad_norm": 1.9004919528961182,
      "learning_rate": 8.652640042883945e-06,
      "loss": 0.5439,
      "step": 26660
    },
    {
      "epoch": 2.1387329591018442,
      "grad_norm": 1.854187250137329,
      "learning_rate": 8.644599303135888e-06,
      "loss": 0.5737,
      "step": 26670
    },
    {
      "epoch": 2.13953488372093,
      "grad_norm": 1.6172378063201904,
      "learning_rate": 8.636558563387832e-06,
      "loss": 0.5027,
      "step": 26680
    },
    {
      "epoch": 2.140336808340016,
      "grad_norm": 1.8486822843551636,
      "learning_rate": 8.628517823639775e-06,
      "loss": 0.5737,
      "step": 26690
    },
    {
      "epoch": 2.141138732959102,
      "grad_norm": 1.629401445388794,
      "learning_rate": 8.620477083891719e-06,
      "loss": 0.5544,
      "step": 26700
    },
    {
      "epoch": 2.141940657578188,
      "grad_norm": 1.7223337888717651,
      "learning_rate": 8.612436344143661e-06,
      "loss": 0.4815,
      "step": 26710
    },
    {
      "epoch": 2.1427425821972736,
      "grad_norm": 2.01794171333313,
      "learning_rate": 8.604395604395604e-06,
      "loss": 0.5426,
      "step": 26720
    },
    {
      "epoch": 2.1435445068163594,
      "grad_norm": 2.0143349170684814,
      "learning_rate": 8.596354864647548e-06,
      "loss": 0.5373,
      "step": 26730
    },
    {
      "epoch": 2.1443464314354452,
      "grad_norm": 1.8591209650039673,
      "learning_rate": 8.588314124899491e-06,
      "loss": 0.5832,
      "step": 26740
    },
    {
      "epoch": 2.145148356054531,
      "grad_norm": 2.151897668838501,
      "learning_rate": 8.580273385151435e-06,
      "loss": 0.5277,
      "step": 26750
    },
    {
      "epoch": 2.145950280673617,
      "grad_norm": 1.9511948823928833,
      "learning_rate": 8.572232645403378e-06,
      "loss": 0.5539,
      "step": 26760
    },
    {
      "epoch": 2.1467522052927026,
      "grad_norm": 1.7924954891204834,
      "learning_rate": 8.56419190565532e-06,
      "loss": 0.5379,
      "step": 26770
    },
    {
      "epoch": 2.1475541299117884,
      "grad_norm": 1.7668743133544922,
      "learning_rate": 8.556151165907263e-06,
      "loss": 0.5704,
      "step": 26780
    },
    {
      "epoch": 2.148356054530874,
      "grad_norm": 1.8843494653701782,
      "learning_rate": 8.548110426159206e-06,
      "loss": 0.6564,
      "step": 26790
    },
    {
      "epoch": 2.14915797914996,
      "grad_norm": 2.274541139602661,
      "learning_rate": 8.54006968641115e-06,
      "loss": 0.4792,
      "step": 26800
    },
    {
      "epoch": 2.1499599037690458,
      "grad_norm": 2.0403575897216797,
      "learning_rate": 8.532028946663093e-06,
      "loss": 0.5177,
      "step": 26810
    },
    {
      "epoch": 2.1507618283881316,
      "grad_norm": 1.7091991901397705,
      "learning_rate": 8.523988206915037e-06,
      "loss": 0.5064,
      "step": 26820
    },
    {
      "epoch": 2.1515637530072174,
      "grad_norm": 1.9479140043258667,
      "learning_rate": 8.51594746716698e-06,
      "loss": 0.5443,
      "step": 26830
    },
    {
      "epoch": 2.152365677626303,
      "grad_norm": 1.7947015762329102,
      "learning_rate": 8.507906727418922e-06,
      "loss": 0.4932,
      "step": 26840
    },
    {
      "epoch": 2.153167602245389,
      "grad_norm": 1.7789894342422485,
      "learning_rate": 8.499865987670866e-06,
      "loss": 0.5565,
      "step": 26850
    },
    {
      "epoch": 2.1539695268644747,
      "grad_norm": 1.8319765329360962,
      "learning_rate": 8.491825247922809e-06,
      "loss": 0.5281,
      "step": 26860
    },
    {
      "epoch": 2.1547714514835605,
      "grad_norm": 1.7779773473739624,
      "learning_rate": 8.483784508174753e-06,
      "loss": 0.6475,
      "step": 26870
    },
    {
      "epoch": 2.1555733761026463,
      "grad_norm": 1.8090929985046387,
      "learning_rate": 8.475743768426696e-06,
      "loss": 0.5743,
      "step": 26880
    },
    {
      "epoch": 2.156375300721732,
      "grad_norm": 1.6938276290893555,
      "learning_rate": 8.46770302867864e-06,
      "loss": 0.5024,
      "step": 26890
    },
    {
      "epoch": 2.157177225340818,
      "grad_norm": 1.6939172744750977,
      "learning_rate": 8.459662288930583e-06,
      "loss": 0.5194,
      "step": 26900
    },
    {
      "epoch": 2.1579791499599037,
      "grad_norm": 1.8571704626083374,
      "learning_rate": 8.451621549182524e-06,
      "loss": 0.5507,
      "step": 26910
    },
    {
      "epoch": 2.1587810745789895,
      "grad_norm": 1.7825192213058472,
      "learning_rate": 8.443580809434468e-06,
      "loss": 0.5207,
      "step": 26920
    },
    {
      "epoch": 2.1595829991980753,
      "grad_norm": 1.7721704244613647,
      "learning_rate": 8.43554006968641e-06,
      "loss": 0.5565,
      "step": 26930
    },
    {
      "epoch": 2.160384923817161,
      "grad_norm": 2.2219603061676025,
      "learning_rate": 8.427499329938355e-06,
      "loss": 0.5529,
      "step": 26940
    },
    {
      "epoch": 2.161186848436247,
      "grad_norm": 2.5121586322784424,
      "learning_rate": 8.419458590190297e-06,
      "loss": 0.5425,
      "step": 26950
    },
    {
      "epoch": 2.1619887730553327,
      "grad_norm": 1.618952989578247,
      "learning_rate": 8.41141785044224e-06,
      "loss": 0.5799,
      "step": 26960
    },
    {
      "epoch": 2.1627906976744184,
      "grad_norm": 1.818922758102417,
      "learning_rate": 8.403377110694184e-06,
      "loss": 0.5462,
      "step": 26970
    },
    {
      "epoch": 2.1635926222935042,
      "grad_norm": 2.041114091873169,
      "learning_rate": 8.395336370946127e-06,
      "loss": 0.5669,
      "step": 26980
    },
    {
      "epoch": 2.16439454691259,
      "grad_norm": 1.8317365646362305,
      "learning_rate": 8.387295631198071e-06,
      "loss": 0.5068,
      "step": 26990
    },
    {
      "epoch": 2.165196471531676,
      "grad_norm": 2.0750229358673096,
      "learning_rate": 8.379254891450014e-06,
      "loss": 0.6369,
      "step": 27000
    },
    {
      "epoch": 2.165998396150762,
      "grad_norm": 1.5563147068023682,
      "learning_rate": 8.371214151701958e-06,
      "loss": 0.533,
      "step": 27010
    },
    {
      "epoch": 2.1668003207698474,
      "grad_norm": 1.6675361394882202,
      "learning_rate": 8.3631734119539e-06,
      "loss": 0.5723,
      "step": 27020
    },
    {
      "epoch": 2.1676022453889336,
      "grad_norm": 1.69933021068573,
      "learning_rate": 8.355132672205843e-06,
      "loss": 0.4731,
      "step": 27030
    },
    {
      "epoch": 2.1684041700080194,
      "grad_norm": 1.8956336975097656,
      "learning_rate": 8.347091932457786e-06,
      "loss": 0.5494,
      "step": 27040
    },
    {
      "epoch": 2.1692060946271052,
      "grad_norm": 1.879258394241333,
      "learning_rate": 8.339051192709728e-06,
      "loss": 0.5653,
      "step": 27050
    },
    {
      "epoch": 2.170008019246191,
      "grad_norm": 1.9938974380493164,
      "learning_rate": 8.331814526936478e-06,
      "loss": 0.5555,
      "step": 27060
    },
    {
      "epoch": 2.170809943865277,
      "grad_norm": 1.887939214706421,
      "learning_rate": 8.323773787188422e-06,
      "loss": 0.4936,
      "step": 27070
    },
    {
      "epoch": 2.1716118684843626,
      "grad_norm": 1.8862228393554688,
      "learning_rate": 8.315733047440365e-06,
      "loss": 0.5666,
      "step": 27080
    },
    {
      "epoch": 2.1724137931034484,
      "grad_norm": 2.1096599102020264,
      "learning_rate": 8.307692307692309e-06,
      "loss": 0.5244,
      "step": 27090
    },
    {
      "epoch": 2.173215717722534,
      "grad_norm": 1.7922084331512451,
      "learning_rate": 8.299651567944252e-06,
      "loss": 0.5281,
      "step": 27100
    },
    {
      "epoch": 2.17401764234162,
      "grad_norm": 1.6919307708740234,
      "learning_rate": 8.291610828196194e-06,
      "loss": 0.6026,
      "step": 27110
    },
    {
      "epoch": 2.1748195669607058,
      "grad_norm": 1.7674040794372559,
      "learning_rate": 8.283570088448137e-06,
      "loss": 0.4843,
      "step": 27120
    },
    {
      "epoch": 2.1756214915797916,
      "grad_norm": 1.9645594358444214,
      "learning_rate": 8.27552934870008e-06,
      "loss": 0.5514,
      "step": 27130
    },
    {
      "epoch": 2.1764234161988774,
      "grad_norm": 2.0609724521636963,
      "learning_rate": 8.267488608952024e-06,
      "loss": 0.5507,
      "step": 27140
    },
    {
      "epoch": 2.177225340817963,
      "grad_norm": 1.6700236797332764,
      "learning_rate": 8.259447869203966e-06,
      "loss": 0.5434,
      "step": 27150
    },
    {
      "epoch": 2.178027265437049,
      "grad_norm": 1.744260549545288,
      "learning_rate": 8.25140712945591e-06,
      "loss": 0.5317,
      "step": 27160
    },
    {
      "epoch": 2.1788291900561347,
      "grad_norm": 1.832756757736206,
      "learning_rate": 8.243366389707853e-06,
      "loss": 0.5177,
      "step": 27170
    },
    {
      "epoch": 2.1796311146752205,
      "grad_norm": 1.728906512260437,
      "learning_rate": 8.235325649959798e-06,
      "loss": 0.507,
      "step": 27180
    },
    {
      "epoch": 2.1804330392943063,
      "grad_norm": 2.3095765113830566,
      "learning_rate": 8.22728491021174e-06,
      "loss": 0.5895,
      "step": 27190
    },
    {
      "epoch": 2.181234963913392,
      "grad_norm": 2.057260513305664,
      "learning_rate": 8.219244170463683e-06,
      "loss": 0.5594,
      "step": 27200
    },
    {
      "epoch": 2.182036888532478,
      "grad_norm": 2.0886292457580566,
      "learning_rate": 8.211203430715627e-06,
      "loss": 0.5463,
      "step": 27210
    },
    {
      "epoch": 2.1828388131515637,
      "grad_norm": 1.931402564048767,
      "learning_rate": 8.20316269096757e-06,
      "loss": 0.5736,
      "step": 27220
    },
    {
      "epoch": 2.1836407377706495,
      "grad_norm": 2.336118698120117,
      "learning_rate": 8.195121951219512e-06,
      "loss": 0.5526,
      "step": 27230
    },
    {
      "epoch": 2.1844426623897353,
      "grad_norm": 2.1296985149383545,
      "learning_rate": 8.187081211471455e-06,
      "loss": 0.5783,
      "step": 27240
    },
    {
      "epoch": 2.185244587008821,
      "grad_norm": 1.9448139667510986,
      "learning_rate": 8.179040471723397e-06,
      "loss": 0.6015,
      "step": 27250
    },
    {
      "epoch": 2.186046511627907,
      "grad_norm": 2.1296780109405518,
      "learning_rate": 8.170999731975342e-06,
      "loss": 0.5402,
      "step": 27260
    },
    {
      "epoch": 2.1868484362469927,
      "grad_norm": 1.9172872304916382,
      "learning_rate": 8.162958992227284e-06,
      "loss": 0.5449,
      "step": 27270
    },
    {
      "epoch": 2.1876503608660784,
      "grad_norm": 2.2211313247680664,
      "learning_rate": 8.154918252479229e-06,
      "loss": 0.5389,
      "step": 27280
    },
    {
      "epoch": 2.1884522854851642,
      "grad_norm": 2.07092022895813,
      "learning_rate": 8.146877512731171e-06,
      "loss": 0.4924,
      "step": 27290
    },
    {
      "epoch": 2.18925421010425,
      "grad_norm": 1.7842776775360107,
      "learning_rate": 8.138836772983115e-06,
      "loss": 0.5489,
      "step": 27300
    },
    {
      "epoch": 2.190056134723336,
      "grad_norm": 1.8081117868423462,
      "learning_rate": 8.130796033235058e-06,
      "loss": 0.5413,
      "step": 27310
    },
    {
      "epoch": 2.1908580593424216,
      "grad_norm": 1.7971247434616089,
      "learning_rate": 8.122755293487e-06,
      "loss": 0.5373,
      "step": 27320
    },
    {
      "epoch": 2.1916599839615074,
      "grad_norm": 1.867294430732727,
      "learning_rate": 8.114714553738945e-06,
      "loss": 0.5576,
      "step": 27330
    },
    {
      "epoch": 2.1924619085805936,
      "grad_norm": 1.8047473430633545,
      "learning_rate": 8.106673813990888e-06,
      "loss": 0.5367,
      "step": 27340
    },
    {
      "epoch": 2.1932638331996794,
      "grad_norm": 1.8532438278198242,
      "learning_rate": 8.098633074242832e-06,
      "loss": 0.5294,
      "step": 27350
    },
    {
      "epoch": 2.1940657578187652,
      "grad_norm": 1.78879976272583,
      "learning_rate": 8.090592334494773e-06,
      "loss": 0.5255,
      "step": 27360
    },
    {
      "epoch": 2.194867682437851,
      "grad_norm": 2.390329360961914,
      "learning_rate": 8.082551594746717e-06,
      "loss": 0.52,
      "step": 27370
    },
    {
      "epoch": 2.195669607056937,
      "grad_norm": 1.8835903406143188,
      "learning_rate": 8.07451085499866e-06,
      "loss": 0.5902,
      "step": 27380
    },
    {
      "epoch": 2.1964715316760226,
      "grad_norm": 1.6422230005264282,
      "learning_rate": 8.066470115250602e-06,
      "loss": 0.5399,
      "step": 27390
    },
    {
      "epoch": 2.1972734562951084,
      "grad_norm": 1.7704405784606934,
      "learning_rate": 8.058429375502546e-06,
      "loss": 0.5305,
      "step": 27400
    },
    {
      "epoch": 2.198075380914194,
      "grad_norm": 1.7835804224014282,
      "learning_rate": 8.050388635754489e-06,
      "loss": 0.6153,
      "step": 27410
    },
    {
      "epoch": 2.19887730553328,
      "grad_norm": 1.901126503944397,
      "learning_rate": 8.042347896006433e-06,
      "loss": 0.5537,
      "step": 27420
    },
    {
      "epoch": 2.1996792301523658,
      "grad_norm": 2.5438504219055176,
      "learning_rate": 8.034307156258376e-06,
      "loss": 0.5048,
      "step": 27430
    },
    {
      "epoch": 2.2004811547714516,
      "grad_norm": 1.5902736186981201,
      "learning_rate": 8.026266416510319e-06,
      "loss": 0.5339,
      "step": 27440
    },
    {
      "epoch": 2.2012830793905374,
      "grad_norm": 2.07473087310791,
      "learning_rate": 8.018225676762263e-06,
      "loss": 0.5142,
      "step": 27450
    },
    {
      "epoch": 2.202085004009623,
      "grad_norm": 1.855778455734253,
      "learning_rate": 8.010184937014205e-06,
      "loss": 0.5155,
      "step": 27460
    },
    {
      "epoch": 2.202886928628709,
      "grad_norm": 2.096035957336426,
      "learning_rate": 8.00214419726615e-06,
      "loss": 0.5679,
      "step": 27470
    },
    {
      "epoch": 2.2036888532477947,
      "grad_norm": 2.0179431438446045,
      "learning_rate": 7.994103457518092e-06,
      "loss": 0.5429,
      "step": 27480
    },
    {
      "epoch": 2.2044907778668805,
      "grad_norm": 2.10733962059021,
      "learning_rate": 7.986062717770035e-06,
      "loss": 0.5747,
      "step": 27490
    },
    {
      "epoch": 2.2052927024859663,
      "grad_norm": 1.988525152206421,
      "learning_rate": 7.978021978021978e-06,
      "loss": 0.5731,
      "step": 27500
    },
    {
      "epoch": 2.206094627105052,
      "grad_norm": 2.0897090435028076,
      "learning_rate": 7.96998123827392e-06,
      "loss": 0.5475,
      "step": 27510
    },
    {
      "epoch": 2.206896551724138,
      "grad_norm": 1.8537958860397339,
      "learning_rate": 7.961940498525864e-06,
      "loss": 0.5199,
      "step": 27520
    },
    {
      "epoch": 2.2076984763432237,
      "grad_norm": 1.8547559976577759,
      "learning_rate": 7.953899758777807e-06,
      "loss": 0.5577,
      "step": 27530
    },
    {
      "epoch": 2.2085004009623095,
      "grad_norm": 2.1014552116394043,
      "learning_rate": 7.945859019029751e-06,
      "loss": 0.5452,
      "step": 27540
    },
    {
      "epoch": 2.2093023255813953,
      "grad_norm": 1.9008774757385254,
      "learning_rate": 7.937818279281694e-06,
      "loss": 0.5655,
      "step": 27550
    },
    {
      "epoch": 2.210104250200481,
      "grad_norm": 1.7551559209823608,
      "learning_rate": 7.929777539533638e-06,
      "loss": 0.5467,
      "step": 27560
    },
    {
      "epoch": 2.210906174819567,
      "grad_norm": 1.897849678993225,
      "learning_rate": 7.92173679978558e-06,
      "loss": 0.5384,
      "step": 27570
    },
    {
      "epoch": 2.2117080994386527,
      "grad_norm": 1.8371672630310059,
      "learning_rate": 7.913696060037523e-06,
      "loss": 0.5054,
      "step": 27580
    },
    {
      "epoch": 2.2125100240577384,
      "grad_norm": 1.7928074598312378,
      "learning_rate": 7.905655320289468e-06,
      "loss": 0.5061,
      "step": 27590
    },
    {
      "epoch": 2.2133119486768242,
      "grad_norm": 2.2697527408599854,
      "learning_rate": 7.89761458054141e-06,
      "loss": 0.6384,
      "step": 27600
    },
    {
      "epoch": 2.21411387329591,
      "grad_norm": 1.646348237991333,
      "learning_rate": 7.889573840793355e-06,
      "loss": 0.5116,
      "step": 27610
    },
    {
      "epoch": 2.214915797914996,
      "grad_norm": 1.8264044523239136,
      "learning_rate": 7.881533101045295e-06,
      "loss": 0.5882,
      "step": 27620
    },
    {
      "epoch": 2.2157177225340816,
      "grad_norm": 1.983769178390503,
      "learning_rate": 7.87349236129724e-06,
      "loss": 0.5463,
      "step": 27630
    },
    {
      "epoch": 2.2165196471531674,
      "grad_norm": 1.891523838043213,
      "learning_rate": 7.865451621549182e-06,
      "loss": 0.5219,
      "step": 27640
    },
    {
      "epoch": 2.2173215717722536,
      "grad_norm": 2.3890702724456787,
      "learning_rate": 7.857410881801125e-06,
      "loss": 0.5415,
      "step": 27650
    },
    {
      "epoch": 2.218123496391339,
      "grad_norm": 1.86484956741333,
      "learning_rate": 7.84937014205307e-06,
      "loss": 0.576,
      "step": 27660
    },
    {
      "epoch": 2.2189254210104252,
      "grad_norm": 2.0503432750701904,
      "learning_rate": 7.841329402305012e-06,
      "loss": 0.5454,
      "step": 27670
    },
    {
      "epoch": 2.219727345629511,
      "grad_norm": 2.0081942081451416,
      "learning_rate": 7.833288662556956e-06,
      "loss": 0.5097,
      "step": 27680
    },
    {
      "epoch": 2.220529270248597,
      "grad_norm": 1.8668618202209473,
      "learning_rate": 7.825247922808899e-06,
      "loss": 0.5873,
      "step": 27690
    },
    {
      "epoch": 2.2213311948676826,
      "grad_norm": 2.225367546081543,
      "learning_rate": 7.817207183060841e-06,
      "loss": 0.5391,
      "step": 27700
    },
    {
      "epoch": 2.2221331194867684,
      "grad_norm": 2.3068675994873047,
      "learning_rate": 7.809166443312786e-06,
      "loss": 0.5358,
      "step": 27710
    },
    {
      "epoch": 2.222935044105854,
      "grad_norm": 1.9912726879119873,
      "learning_rate": 7.801125703564728e-06,
      "loss": 0.5925,
      "step": 27720
    },
    {
      "epoch": 2.22373696872494,
      "grad_norm": 1.845102071762085,
      "learning_rate": 7.793084963816672e-06,
      "loss": 0.5185,
      "step": 27730
    },
    {
      "epoch": 2.2245388933440258,
      "grad_norm": 2.10762619972229,
      "learning_rate": 7.78584829804342e-06,
      "loss": 0.5945,
      "step": 27740
    },
    {
      "epoch": 2.2253408179631116,
      "grad_norm": 1.7674050331115723,
      "learning_rate": 7.777807558295363e-06,
      "loss": 0.5089,
      "step": 27750
    },
    {
      "epoch": 2.2261427425821974,
      "grad_norm": 2.1293797492980957,
      "learning_rate": 7.769766818547307e-06,
      "loss": 0.528,
      "step": 27760
    },
    {
      "epoch": 2.226944667201283,
      "grad_norm": 2.0412023067474365,
      "learning_rate": 7.76172607879925e-06,
      "loss": 0.484,
      "step": 27770
    },
    {
      "epoch": 2.227746591820369,
      "grad_norm": 1.9300570487976074,
      "learning_rate": 7.753685339051194e-06,
      "loss": 0.55,
      "step": 27780
    },
    {
      "epoch": 2.2285485164394547,
      "grad_norm": 2.150094509124756,
      "learning_rate": 7.745644599303137e-06,
      "loss": 0.6192,
      "step": 27790
    },
    {
      "epoch": 2.2293504410585405,
      "grad_norm": 1.7603039741516113,
      "learning_rate": 7.73760385955508e-06,
      "loss": 0.549,
      "step": 27800
    },
    {
      "epoch": 2.2301523656776263,
      "grad_norm": 2.033038377761841,
      "learning_rate": 7.729563119807022e-06,
      "loss": 0.5228,
      "step": 27810
    },
    {
      "epoch": 2.230954290296712,
      "grad_norm": 2.0727341175079346,
      "learning_rate": 7.721522380058964e-06,
      "loss": 0.5296,
      "step": 27820
    },
    {
      "epoch": 2.231756214915798,
      "grad_norm": 1.938582420349121,
      "learning_rate": 7.713481640310909e-06,
      "loss": 0.4808,
      "step": 27830
    },
    {
      "epoch": 2.2325581395348837,
      "grad_norm": 2.063872814178467,
      "learning_rate": 7.705440900562851e-06,
      "loss": 0.5108,
      "step": 27840
    },
    {
      "epoch": 2.2333600641539695,
      "grad_norm": 1.9387693405151367,
      "learning_rate": 7.697400160814796e-06,
      "loss": 0.5359,
      "step": 27850
    },
    {
      "epoch": 2.2341619887730553,
      "grad_norm": 1.82741379737854,
      "learning_rate": 7.689359421066738e-06,
      "loss": 0.5489,
      "step": 27860
    },
    {
      "epoch": 2.234963913392141,
      "grad_norm": 2.0421082973480225,
      "learning_rate": 7.68131868131868e-06,
      "loss": 0.5217,
      "step": 27870
    },
    {
      "epoch": 2.235765838011227,
      "grad_norm": 2.0119550228118896,
      "learning_rate": 7.673277941570625e-06,
      "loss": 0.5068,
      "step": 27880
    },
    {
      "epoch": 2.2365677626303127,
      "grad_norm": 1.8697376251220703,
      "learning_rate": 7.665237201822568e-06,
      "loss": 0.5737,
      "step": 27890
    },
    {
      "epoch": 2.2373696872493984,
      "grad_norm": 2.065852165222168,
      "learning_rate": 7.657196462074512e-06,
      "loss": 0.5683,
      "step": 27900
    },
    {
      "epoch": 2.2381716118684842,
      "grad_norm": 1.959166169166565,
      "learning_rate": 7.649155722326455e-06,
      "loss": 0.6209,
      "step": 27910
    },
    {
      "epoch": 2.23897353648757,
      "grad_norm": 1.6499427556991577,
      "learning_rate": 7.641114982578397e-06,
      "loss": 0.511,
      "step": 27920
    },
    {
      "epoch": 2.239775461106656,
      "grad_norm": 2.3105759620666504,
      "learning_rate": 7.633074242830342e-06,
      "loss": 0.5706,
      "step": 27930
    },
    {
      "epoch": 2.2405773857257416,
      "grad_norm": 1.996541976928711,
      "learning_rate": 7.625033503082283e-06,
      "loss": 0.5667,
      "step": 27940
    },
    {
      "epoch": 2.2413793103448274,
      "grad_norm": 1.813873052597046,
      "learning_rate": 7.6169927633342275e-06,
      "loss": 0.5256,
      "step": 27950
    },
    {
      "epoch": 2.242181234963913,
      "grad_norm": 1.931588888168335,
      "learning_rate": 7.60895202358617e-06,
      "loss": 0.5797,
      "step": 27960
    },
    {
      "epoch": 2.242983159582999,
      "grad_norm": 1.9449807405471802,
      "learning_rate": 7.600911283838114e-06,
      "loss": 0.5639,
      "step": 27970
    },
    {
      "epoch": 2.2437850842020852,
      "grad_norm": 2.213669776916504,
      "learning_rate": 7.592870544090056e-06,
      "loss": 0.5407,
      "step": 27980
    },
    {
      "epoch": 2.244587008821171,
      "grad_norm": 1.6731057167053223,
      "learning_rate": 7.584829804341999e-06,
      "loss": 0.5067,
      "step": 27990
    },
    {
      "epoch": 2.245388933440257,
      "grad_norm": 1.902962327003479,
      "learning_rate": 7.576789064593943e-06,
      "loss": 0.6012,
      "step": 28000
    },
    {
      "epoch": 2.2461908580593426,
      "grad_norm": 1.8242626190185547,
      "learning_rate": 7.568748324845886e-06,
      "loss": 0.5537,
      "step": 28010
    },
    {
      "epoch": 2.2469927826784284,
      "grad_norm": 1.8654557466506958,
      "learning_rate": 7.56070758509783e-06,
      "loss": 0.494,
      "step": 28020
    },
    {
      "epoch": 2.247794707297514,
      "grad_norm": 2.0009982585906982,
      "learning_rate": 7.5526668453497725e-06,
      "loss": 0.5428,
      "step": 28030
    },
    {
      "epoch": 2.2485966319166,
      "grad_norm": 2.198586940765381,
      "learning_rate": 7.544626105601716e-06,
      "loss": 0.5269,
      "step": 28040
    },
    {
      "epoch": 2.2493985565356858,
      "grad_norm": 1.8613219261169434,
      "learning_rate": 7.536585365853659e-06,
      "loss": 0.5211,
      "step": 28050
    },
    {
      "epoch": 2.2502004811547716,
      "grad_norm": 1.8071106672286987,
      "learning_rate": 7.528544626105601e-06,
      "loss": 0.555,
      "step": 28060
    },
    {
      "epoch": 2.2510024057738574,
      "grad_norm": 2.045797348022461,
      "learning_rate": 7.5205038863575455e-06,
      "loss": 0.5624,
      "step": 28070
    },
    {
      "epoch": 2.251804330392943,
      "grad_norm": 2.1859347820281982,
      "learning_rate": 7.512463146609488e-06,
      "loss": 0.5372,
      "step": 28080
    },
    {
      "epoch": 2.252606255012029,
      "grad_norm": 1.9054639339447021,
      "learning_rate": 7.504422406861432e-06,
      "loss": 0.5435,
      "step": 28090
    },
    {
      "epoch": 2.2534081796311147,
      "grad_norm": 1.884459376335144,
      "learning_rate": 7.496381667113374e-06,
      "loss": 0.5558,
      "step": 28100
    },
    {
      "epoch": 2.2542101042502005,
      "grad_norm": 1.6403870582580566,
      "learning_rate": 7.4883409273653175e-06,
      "loss": 0.5754,
      "step": 28110
    },
    {
      "epoch": 2.2550120288692863,
      "grad_norm": 1.7891732454299927,
      "learning_rate": 7.480300187617261e-06,
      "loss": 0.5462,
      "step": 28120
    },
    {
      "epoch": 2.255813953488372,
      "grad_norm": 1.737002968788147,
      "learning_rate": 7.4722594478692044e-06,
      "loss": 0.5623,
      "step": 28130
    },
    {
      "epoch": 2.256615878107458,
      "grad_norm": 1.8172886371612549,
      "learning_rate": 7.464218708121148e-06,
      "loss": 0.5229,
      "step": 28140
    },
    {
      "epoch": 2.2574178027265437,
      "grad_norm": 1.6727999448776245,
      "learning_rate": 7.4561779683730905e-06,
      "loss": 0.5365,
      "step": 28150
    },
    {
      "epoch": 2.2582197273456295,
      "grad_norm": 1.825332522392273,
      "learning_rate": 7.448137228625033e-06,
      "loss": 0.5204,
      "step": 28160
    },
    {
      "epoch": 2.2590216519647153,
      "grad_norm": 1.9703505039215088,
      "learning_rate": 7.4400964888769765e-06,
      "loss": 0.4987,
      "step": 28170
    },
    {
      "epoch": 2.259823576583801,
      "grad_norm": 1.6415538787841797,
      "learning_rate": 7.43205574912892e-06,
      "loss": 0.5503,
      "step": 28180
    },
    {
      "epoch": 2.260625501202887,
      "grad_norm": 1.9904487133026123,
      "learning_rate": 7.424015009380863e-06,
      "loss": 0.5602,
      "step": 28190
    },
    {
      "epoch": 2.2614274258219726,
      "grad_norm": 1.715309500694275,
      "learning_rate": 7.415974269632807e-06,
      "loss": 0.573,
      "step": 28200
    },
    {
      "epoch": 2.2622293504410584,
      "grad_norm": 2.1945884227752686,
      "learning_rate": 7.4079335298847494e-06,
      "loss": 0.5561,
      "step": 28210
    },
    {
      "epoch": 2.2630312750601442,
      "grad_norm": 1.993585228919983,
      "learning_rate": 7.399892790136693e-06,
      "loss": 0.4994,
      "step": 28220
    },
    {
      "epoch": 2.26383319967923,
      "grad_norm": 1.824885368347168,
      "learning_rate": 7.3918520503886355e-06,
      "loss": 0.5367,
      "step": 28230
    },
    {
      "epoch": 2.264635124298316,
      "grad_norm": 1.555053949356079,
      "learning_rate": 7.383811310640579e-06,
      "loss": 0.5352,
      "step": 28240
    },
    {
      "epoch": 2.2654370489174016,
      "grad_norm": 2.16401743888855,
      "learning_rate": 7.375770570892522e-06,
      "loss": 0.5391,
      "step": 28250
    },
    {
      "epoch": 2.2662389735364874,
      "grad_norm": 1.700805902481079,
      "learning_rate": 7.367729831144466e-06,
      "loss": 0.5894,
      "step": 28260
    },
    {
      "epoch": 2.267040898155573,
      "grad_norm": 1.9622970819473267,
      "learning_rate": 7.359689091396409e-06,
      "loss": 0.5055,
      "step": 28270
    },
    {
      "epoch": 2.267842822774659,
      "grad_norm": 1.6997758150100708,
      "learning_rate": 7.351648351648352e-06,
      "loss": 0.5052,
      "step": 28280
    },
    {
      "epoch": 2.268644747393745,
      "grad_norm": 2.1198389530181885,
      "learning_rate": 7.3436076119002944e-06,
      "loss": 0.5515,
      "step": 28290
    },
    {
      "epoch": 2.2694466720128306,
      "grad_norm": 1.6513350009918213,
      "learning_rate": 7.335566872152238e-06,
      "loss": 0.5436,
      "step": 28300
    },
    {
      "epoch": 2.270248596631917,
      "grad_norm": 1.9040268659591675,
      "learning_rate": 7.327526132404181e-06,
      "loss": 0.5554,
      "step": 28310
    },
    {
      "epoch": 2.2710505212510026,
      "grad_norm": 2.1138339042663574,
      "learning_rate": 7.319485392656125e-06,
      "loss": 0.5243,
      "step": 28320
    },
    {
      "epoch": 2.2718524458700884,
      "grad_norm": 1.6984308958053589,
      "learning_rate": 7.311444652908068e-06,
      "loss": 0.5255,
      "step": 28330
    },
    {
      "epoch": 2.272654370489174,
      "grad_norm": 2.02738094329834,
      "learning_rate": 7.303403913160011e-06,
      "loss": 0.5224,
      "step": 28340
    },
    {
      "epoch": 2.27345629510826,
      "grad_norm": 2.157886505126953,
      "learning_rate": 7.295363173411954e-06,
      "loss": 0.535,
      "step": 28350
    },
    {
      "epoch": 2.2742582197273458,
      "grad_norm": 2.018929958343506,
      "learning_rate": 7.287322433663897e-06,
      "loss": 0.5828,
      "step": 28360
    },
    {
      "epoch": 2.2750601443464316,
      "grad_norm": 1.6134734153747559,
      "learning_rate": 7.27928169391584e-06,
      "loss": 0.5518,
      "step": 28370
    },
    {
      "epoch": 2.2758620689655173,
      "grad_norm": 1.6664296388626099,
      "learning_rate": 7.271240954167784e-06,
      "loss": 0.4811,
      "step": 28380
    },
    {
      "epoch": 2.276663993584603,
      "grad_norm": 2.019418478012085,
      "learning_rate": 7.263200214419727e-06,
      "loss": 0.5911,
      "step": 28390
    },
    {
      "epoch": 2.277465918203689,
      "grad_norm": 2.048494815826416,
      "learning_rate": 7.25515947467167e-06,
      "loss": 0.571,
      "step": 28400
    },
    {
      "epoch": 2.2782678428227747,
      "grad_norm": 1.9422439336776733,
      "learning_rate": 7.247118734923613e-06,
      "loss": 0.4784,
      "step": 28410
    },
    {
      "epoch": 2.2790697674418605,
      "grad_norm": 1.8482862710952759,
      "learning_rate": 7.239077995175556e-06,
      "loss": 0.5066,
      "step": 28420
    },
    {
      "epoch": 2.2798716920609463,
      "grad_norm": 1.886457920074463,
      "learning_rate": 7.231037255427499e-06,
      "loss": 0.543,
      "step": 28430
    },
    {
      "epoch": 2.280673616680032,
      "grad_norm": 2.0200212001800537,
      "learning_rate": 7.222996515679443e-06,
      "loss": 0.548,
      "step": 28440
    },
    {
      "epoch": 2.281475541299118,
      "grad_norm": 1.861714482307434,
      "learning_rate": 7.214955775931386e-06,
      "loss": 0.5129,
      "step": 28450
    },
    {
      "epoch": 2.2822774659182037,
      "grad_norm": 1.9954833984375,
      "learning_rate": 7.20691503618333e-06,
      "loss": 0.5083,
      "step": 28460
    },
    {
      "epoch": 2.2830793905372895,
      "grad_norm": 1.902992606163025,
      "learning_rate": 7.198874296435272e-06,
      "loss": 0.5638,
      "step": 28470
    },
    {
      "epoch": 2.2838813151563753,
      "grad_norm": 1.9366387128829956,
      "learning_rate": 7.190833556687216e-06,
      "loss": 0.5894,
      "step": 28480
    },
    {
      "epoch": 2.284683239775461,
      "grad_norm": 2.108598232269287,
      "learning_rate": 7.182792816939158e-06,
      "loss": 0.5557,
      "step": 28490
    },
    {
      "epoch": 2.285485164394547,
      "grad_norm": 1.7432036399841309,
      "learning_rate": 7.174752077191102e-06,
      "loss": 0.5244,
      "step": 28500
    },
    {
      "epoch": 2.2862870890136326,
      "grad_norm": 1.6429541110992432,
      "learning_rate": 7.166711337443045e-06,
      "loss": 0.4923,
      "step": 28510
    },
    {
      "epoch": 2.2870890136327184,
      "grad_norm": 1.8344650268554688,
      "learning_rate": 7.1586705976949885e-06,
      "loss": 0.5691,
      "step": 28520
    },
    {
      "epoch": 2.2878909382518042,
      "grad_norm": 2.007809638977051,
      "learning_rate": 7.150629857946931e-06,
      "loss": 0.5273,
      "step": 28530
    },
    {
      "epoch": 2.28869286287089,
      "grad_norm": 1.7513006925582886,
      "learning_rate": 7.142589118198875e-06,
      "loss": 0.5094,
      "step": 28540
    },
    {
      "epoch": 2.289494787489976,
      "grad_norm": 2.0614380836486816,
      "learning_rate": 7.134548378450817e-06,
      "loss": 0.5382,
      "step": 28550
    },
    {
      "epoch": 2.2902967121090616,
      "grad_norm": 1.9421623945236206,
      "learning_rate": 7.126507638702761e-06,
      "loss": 0.5437,
      "step": 28560
    },
    {
      "epoch": 2.2910986367281474,
      "grad_norm": 2.176107406616211,
      "learning_rate": 7.118466898954704e-06,
      "loss": 0.5462,
      "step": 28570
    },
    {
      "epoch": 2.291900561347233,
      "grad_norm": 2.008819818496704,
      "learning_rate": 7.1104261592066475e-06,
      "loss": 0.5347,
      "step": 28580
    },
    {
      "epoch": 2.292702485966319,
      "grad_norm": 1.7123587131500244,
      "learning_rate": 7.102385419458591e-06,
      "loss": 0.568,
      "step": 28590
    },
    {
      "epoch": 2.293504410585405,
      "grad_norm": 1.9888566732406616,
      "learning_rate": 7.0943446797105336e-06,
      "loss": 0.4873,
      "step": 28600
    },
    {
      "epoch": 2.2943063352044906,
      "grad_norm": 2.0557239055633545,
      "learning_rate": 7.086303939962476e-06,
      "loss": 0.4861,
      "step": 28610
    },
    {
      "epoch": 2.295108259823577,
      "grad_norm": 1.6467891931533813,
      "learning_rate": 7.07826320021442e-06,
      "loss": 0.5516,
      "step": 28620
    },
    {
      "epoch": 2.295910184442662,
      "grad_norm": 1.9193840026855469,
      "learning_rate": 7.070222460466363e-06,
      "loss": 0.5479,
      "step": 28630
    },
    {
      "epoch": 2.2967121090617484,
      "grad_norm": 1.997902274131775,
      "learning_rate": 7.0621817207183065e-06,
      "loss": 0.5277,
      "step": 28640
    },
    {
      "epoch": 2.297514033680834,
      "grad_norm": 1.8200478553771973,
      "learning_rate": 7.05414098097025e-06,
      "loss": 0.4879,
      "step": 28650
    },
    {
      "epoch": 2.29831595829992,
      "grad_norm": 1.977737307548523,
      "learning_rate": 7.0461002412221925e-06,
      "loss": 0.5105,
      "step": 28660
    },
    {
      "epoch": 2.2991178829190058,
      "grad_norm": 1.736466646194458,
      "learning_rate": 7.038059501474136e-06,
      "loss": 0.4888,
      "step": 28670
    },
    {
      "epoch": 2.2999198075380916,
      "grad_norm": 1.9452285766601562,
      "learning_rate": 7.0300187617260786e-06,
      "loss": 0.5897,
      "step": 28680
    },
    {
      "epoch": 2.3007217321571773,
      "grad_norm": 1.6949491500854492,
      "learning_rate": 7.021978021978022e-06,
      "loss": 0.5063,
      "step": 28690
    },
    {
      "epoch": 2.301523656776263,
      "grad_norm": 2.046029567718506,
      "learning_rate": 7.0139372822299654e-06,
      "loss": 0.5988,
      "step": 28700
    },
    {
      "epoch": 2.302325581395349,
      "grad_norm": 1.7176741361618042,
      "learning_rate": 7.005896542481909e-06,
      "loss": 0.5943,
      "step": 28710
    },
    {
      "epoch": 2.3031275060144347,
      "grad_norm": 2.0159218311309814,
      "learning_rate": 6.9978558027338515e-06,
      "loss": 0.5333,
      "step": 28720
    },
    {
      "epoch": 2.3039294306335205,
      "grad_norm": 1.6787723302841187,
      "learning_rate": 6.989815062985795e-06,
      "loss": 0.5365,
      "step": 28730
    },
    {
      "epoch": 2.3047313552526063,
      "grad_norm": 1.7590227127075195,
      "learning_rate": 6.9817743232377375e-06,
      "loss": 0.54,
      "step": 28740
    },
    {
      "epoch": 2.305533279871692,
      "grad_norm": 1.92874014377594,
      "learning_rate": 6.973733583489681e-06,
      "loss": 0.4992,
      "step": 28750
    },
    {
      "epoch": 2.306335204490778,
      "grad_norm": 2.0755152702331543,
      "learning_rate": 6.965692843741624e-06,
      "loss": 0.4979,
      "step": 28760
    },
    {
      "epoch": 2.3071371291098637,
      "grad_norm": 1.7023608684539795,
      "learning_rate": 6.957652103993568e-06,
      "loss": 0.5148,
      "step": 28770
    },
    {
      "epoch": 2.3079390537289495,
      "grad_norm": 1.7723948955535889,
      "learning_rate": 6.949611364245511e-06,
      "loss": 0.5241,
      "step": 28780
    },
    {
      "epoch": 2.3087409783480353,
      "grad_norm": 1.9771533012390137,
      "learning_rate": 6.941570624497454e-06,
      "loss": 0.5584,
      "step": 28790
    },
    {
      "epoch": 2.309542902967121,
      "grad_norm": 1.931056022644043,
      "learning_rate": 6.933529884749397e-06,
      "loss": 0.5708,
      "step": 28800
    },
    {
      "epoch": 2.310344827586207,
      "grad_norm": 2.012641191482544,
      "learning_rate": 6.92548914500134e-06,
      "loss": 0.5327,
      "step": 28810
    },
    {
      "epoch": 2.3111467522052926,
      "grad_norm": 2.101193428039551,
      "learning_rate": 6.917448405253283e-06,
      "loss": 0.5165,
      "step": 28820
    },
    {
      "epoch": 2.3119486768243784,
      "grad_norm": 1.92228364944458,
      "learning_rate": 6.909407665505227e-06,
      "loss": 0.5294,
      "step": 28830
    },
    {
      "epoch": 2.3127506014434642,
      "grad_norm": 2.345059871673584,
      "learning_rate": 6.90136692575717e-06,
      "loss": 0.5592,
      "step": 28840
    },
    {
      "epoch": 2.31355252606255,
      "grad_norm": 1.5961050987243652,
      "learning_rate": 6.893326186009113e-06,
      "loss": 0.5982,
      "step": 28850
    },
    {
      "epoch": 2.314354450681636,
      "grad_norm": 1.7688041925430298,
      "learning_rate": 6.885285446261056e-06,
      "loss": 0.5281,
      "step": 28860
    },
    {
      "epoch": 2.3151563753007216,
      "grad_norm": 2.119696617126465,
      "learning_rate": 6.877244706512999e-06,
      "loss": 0.5208,
      "step": 28870
    },
    {
      "epoch": 2.3159582999198074,
      "grad_norm": 1.970217227935791,
      "learning_rate": 6.869203966764942e-06,
      "loss": 0.5308,
      "step": 28880
    },
    {
      "epoch": 2.316760224538893,
      "grad_norm": 2.1691465377807617,
      "learning_rate": 6.861163227016886e-06,
      "loss": 0.5903,
      "step": 28890
    },
    {
      "epoch": 2.317562149157979,
      "grad_norm": 2.103715658187866,
      "learning_rate": 6.853122487268829e-06,
      "loss": 0.5412,
      "step": 28900
    },
    {
      "epoch": 2.3183640737770648,
      "grad_norm": 1.934612512588501,
      "learning_rate": 6.845081747520773e-06,
      "loss": 0.5077,
      "step": 28910
    },
    {
      "epoch": 2.3191659983961506,
      "grad_norm": 2.023686170578003,
      "learning_rate": 6.837041007772715e-06,
      "loss": 0.5322,
      "step": 28920
    },
    {
      "epoch": 2.319967923015237,
      "grad_norm": 1.9118897914886475,
      "learning_rate": 6.829000268024658e-06,
      "loss": 0.5115,
      "step": 28930
    },
    {
      "epoch": 2.320769847634322,
      "grad_norm": 2.0559215545654297,
      "learning_rate": 6.820959528276601e-06,
      "loss": 0.5169,
      "step": 28940
    },
    {
      "epoch": 2.3215717722534084,
      "grad_norm": 2.1284797191619873,
      "learning_rate": 6.812918788528545e-06,
      "loss": 0.5449,
      "step": 28950
    },
    {
      "epoch": 2.322373696872494,
      "grad_norm": 2.150696039199829,
      "learning_rate": 6.804878048780488e-06,
      "loss": 0.5164,
      "step": 28960
    },
    {
      "epoch": 2.32317562149158,
      "grad_norm": 1.7891266345977783,
      "learning_rate": 6.796837309032432e-06,
      "loss": 0.5219,
      "step": 28970
    },
    {
      "epoch": 2.3239775461106658,
      "grad_norm": 1.861504077911377,
      "learning_rate": 6.788796569284374e-06,
      "loss": 0.5571,
      "step": 28980
    },
    {
      "epoch": 2.3247794707297516,
      "grad_norm": 2.0143027305603027,
      "learning_rate": 6.780755829536318e-06,
      "loss": 0.602,
      "step": 28990
    },
    {
      "epoch": 2.3255813953488373,
      "grad_norm": 1.823717713356018,
      "learning_rate": 6.77271508978826e-06,
      "loss": 0.4819,
      "step": 29000
    },
    {
      "epoch": 2.326383319967923,
      "grad_norm": 1.6725101470947266,
      "learning_rate": 6.764674350040204e-06,
      "loss": 0.5611,
      "step": 29010
    },
    {
      "epoch": 2.327185244587009,
      "grad_norm": 2.2269771099090576,
      "learning_rate": 6.756633610292147e-06,
      "loss": 0.556,
      "step": 29020
    },
    {
      "epoch": 2.3279871692060947,
      "grad_norm": 1.8703893423080444,
      "learning_rate": 6.748592870544091e-06,
      "loss": 0.5473,
      "step": 29030
    },
    {
      "epoch": 2.3287890938251805,
      "grad_norm": 1.9933680295944214,
      "learning_rate": 6.740552130796033e-06,
      "loss": 0.5095,
      "step": 29040
    },
    {
      "epoch": 2.3295910184442663,
      "grad_norm": 1.7477682828903198,
      "learning_rate": 6.732511391047977e-06,
      "loss": 0.4953,
      "step": 29050
    },
    {
      "epoch": 2.330392943063352,
      "grad_norm": 1.8520253896713257,
      "learning_rate": 6.724470651299919e-06,
      "loss": 0.5543,
      "step": 29060
    },
    {
      "epoch": 2.331194867682438,
      "grad_norm": 2.096839666366577,
      "learning_rate": 6.716429911551863e-06,
      "loss": 0.496,
      "step": 29070
    },
    {
      "epoch": 2.3319967923015237,
      "grad_norm": 2.0267295837402344,
      "learning_rate": 6.708389171803806e-06,
      "loss": 0.557,
      "step": 29080
    },
    {
      "epoch": 2.3327987169206095,
      "grad_norm": 1.969738245010376,
      "learning_rate": 6.7003484320557496e-06,
      "loss": 0.5645,
      "step": 29090
    },
    {
      "epoch": 2.3336006415396953,
      "grad_norm": 2.2517426013946533,
      "learning_rate": 6.692307692307693e-06,
      "loss": 0.5244,
      "step": 29100
    },
    {
      "epoch": 2.334402566158781,
      "grad_norm": 2.237852096557617,
      "learning_rate": 6.684266952559636e-06,
      "loss": 0.5394,
      "step": 29110
    },
    {
      "epoch": 2.335204490777867,
      "grad_norm": 1.9032657146453857,
      "learning_rate": 6.676226212811579e-06,
      "loss": 0.5459,
      "step": 29120
    },
    {
      "epoch": 2.3360064153969526,
      "grad_norm": 1.9284623861312866,
      "learning_rate": 6.668185473063522e-06,
      "loss": 0.592,
      "step": 29130
    },
    {
      "epoch": 2.3368083400160384,
      "grad_norm": 1.8862290382385254,
      "learning_rate": 6.660144733315465e-06,
      "loss": 0.5462,
      "step": 29140
    },
    {
      "epoch": 2.3376102646351242,
      "grad_norm": 2.0157272815704346,
      "learning_rate": 6.6521039935674085e-06,
      "loss": 0.5316,
      "step": 29150
    },
    {
      "epoch": 2.33841218925421,
      "grad_norm": 2.083089828491211,
      "learning_rate": 6.644063253819352e-06,
      "loss": 0.528,
      "step": 29160
    },
    {
      "epoch": 2.339214113873296,
      "grad_norm": 1.7395037412643433,
      "learning_rate": 6.6360225140712946e-06,
      "loss": 0.5599,
      "step": 29170
    },
    {
      "epoch": 2.3400160384923816,
      "grad_norm": 1.755391001701355,
      "learning_rate": 6.627981774323238e-06,
      "loss": 0.5076,
      "step": 29180
    },
    {
      "epoch": 2.3408179631114674,
      "grad_norm": 1.922343134880066,
      "learning_rate": 6.619941034575181e-06,
      "loss": 0.571,
      "step": 29190
    },
    {
      "epoch": 2.341619887730553,
      "grad_norm": 2.1196393966674805,
      "learning_rate": 6.611900294827124e-06,
      "loss": 0.502,
      "step": 29200
    },
    {
      "epoch": 2.342421812349639,
      "grad_norm": 2.1669015884399414,
      "learning_rate": 6.6038595550790675e-06,
      "loss": 0.5461,
      "step": 29210
    },
    {
      "epoch": 2.3432237369687248,
      "grad_norm": 2.1182503700256348,
      "learning_rate": 6.595818815331011e-06,
      "loss": 0.5485,
      "step": 29220
    },
    {
      "epoch": 2.3440256615878106,
      "grad_norm": 1.8111845254898071,
      "learning_rate": 6.587778075582954e-06,
      "loss": 0.5747,
      "step": 29230
    },
    {
      "epoch": 2.344827586206897,
      "grad_norm": 2.2547922134399414,
      "learning_rate": 6.579737335834897e-06,
      "loss": 0.5449,
      "step": 29240
    },
    {
      "epoch": 2.345629510825982,
      "grad_norm": 1.8442471027374268,
      "learning_rate": 6.5716965960868396e-06,
      "loss": 0.5295,
      "step": 29250
    },
    {
      "epoch": 2.3464314354450684,
      "grad_norm": 1.8998790979385376,
      "learning_rate": 6.563655856338783e-06,
      "loss": 0.5679,
      "step": 29260
    },
    {
      "epoch": 2.3472333600641537,
      "grad_norm": 1.859561562538147,
      "learning_rate": 6.5556151165907264e-06,
      "loss": 0.4864,
      "step": 29270
    },
    {
      "epoch": 2.34803528468324,
      "grad_norm": 1.8199150562286377,
      "learning_rate": 6.54757437684267e-06,
      "loss": 0.5581,
      "step": 29280
    },
    {
      "epoch": 2.3488372093023258,
      "grad_norm": 2.2112324237823486,
      "learning_rate": 6.539533637094613e-06,
      "loss": 0.5448,
      "step": 29290
    },
    {
      "epoch": 2.3496391339214115,
      "grad_norm": 2.0179734230041504,
      "learning_rate": 6.531492897346556e-06,
      "loss": 0.536,
      "step": 29300
    },
    {
      "epoch": 2.3504410585404973,
      "grad_norm": 2.0856575965881348,
      "learning_rate": 6.523452157598499e-06,
      "loss": 0.5947,
      "step": 29310
    },
    {
      "epoch": 2.351242983159583,
      "grad_norm": 1.6976898908615112,
      "learning_rate": 6.515411417850442e-06,
      "loss": 0.5453,
      "step": 29320
    },
    {
      "epoch": 2.352044907778669,
      "grad_norm": 1.788155436515808,
      "learning_rate": 6.507370678102385e-06,
      "loss": 0.5142,
      "step": 29330
    },
    {
      "epoch": 2.3528468323977547,
      "grad_norm": 1.7857848405838013,
      "learning_rate": 6.499329938354329e-06,
      "loss": 0.5421,
      "step": 29340
    },
    {
      "epoch": 2.3536487570168405,
      "grad_norm": 1.8274908065795898,
      "learning_rate": 6.491289198606272e-06,
      "loss": 0.5245,
      "step": 29350
    },
    {
      "epoch": 2.3544506816359263,
      "grad_norm": 1.997612714767456,
      "learning_rate": 6.483248458858215e-06,
      "loss": 0.5282,
      "step": 29360
    },
    {
      "epoch": 2.355252606255012,
      "grad_norm": 2.6609420776367188,
      "learning_rate": 6.475207719110158e-06,
      "loss": 0.6032,
      "step": 29370
    },
    {
      "epoch": 2.356054530874098,
      "grad_norm": 1.7731316089630127,
      "learning_rate": 6.467166979362101e-06,
      "loss": 0.4871,
      "step": 29380
    },
    {
      "epoch": 2.3568564554931837,
      "grad_norm": 1.8789846897125244,
      "learning_rate": 6.459126239614044e-06,
      "loss": 0.5787,
      "step": 29390
    },
    {
      "epoch": 2.3576583801122695,
      "grad_norm": 1.7521675825119019,
      "learning_rate": 6.451085499865988e-06,
      "loss": 0.4987,
      "step": 29400
    },
    {
      "epoch": 2.3584603047313553,
      "grad_norm": 1.8826922178268433,
      "learning_rate": 6.443044760117931e-06,
      "loss": 0.5717,
      "step": 29410
    },
    {
      "epoch": 2.359262229350441,
      "grad_norm": 1.8082407712936401,
      "learning_rate": 6.435004020369875e-06,
      "loss": 0.5549,
      "step": 29420
    },
    {
      "epoch": 2.360064153969527,
      "grad_norm": 1.9531118869781494,
      "learning_rate": 6.426963280621817e-06,
      "loss": 0.5489,
      "step": 29430
    },
    {
      "epoch": 2.3608660785886126,
      "grad_norm": 2.1346120834350586,
      "learning_rate": 6.418922540873761e-06,
      "loss": 0.5339,
      "step": 29440
    },
    {
      "epoch": 2.3616680032076984,
      "grad_norm": 1.7694834470748901,
      "learning_rate": 6.410881801125703e-06,
      "loss": 0.5421,
      "step": 29450
    },
    {
      "epoch": 2.362469927826784,
      "grad_norm": 2.0161831378936768,
      "learning_rate": 6.402841061377647e-06,
      "loss": 0.5513,
      "step": 29460
    },
    {
      "epoch": 2.36327185244587,
      "grad_norm": 2.1693499088287354,
      "learning_rate": 6.39480032162959e-06,
      "loss": 0.5569,
      "step": 29470
    },
    {
      "epoch": 2.364073777064956,
      "grad_norm": 1.9915435314178467,
      "learning_rate": 6.386759581881534e-06,
      "loss": 0.5491,
      "step": 29480
    },
    {
      "epoch": 2.3648757016840416,
      "grad_norm": 2.104095458984375,
      "learning_rate": 6.378718842133476e-06,
      "loss": 0.4866,
      "step": 29490
    },
    {
      "epoch": 2.3656776263031274,
      "grad_norm": 2.279792070388794,
      "learning_rate": 6.37067810238542e-06,
      "loss": 0.5287,
      "step": 29500
    },
    {
      "epoch": 2.366479550922213,
      "grad_norm": 2.086430788040161,
      "learning_rate": 6.362637362637362e-06,
      "loss": 0.5396,
      "step": 29510
    },
    {
      "epoch": 2.367281475541299,
      "grad_norm": 1.963326096534729,
      "learning_rate": 6.354596622889306e-06,
      "loss": 0.5557,
      "step": 29520
    },
    {
      "epoch": 2.3680834001603848,
      "grad_norm": 2.0223278999328613,
      "learning_rate": 6.346555883141249e-06,
      "loss": 0.5035,
      "step": 29530
    },
    {
      "epoch": 2.3688853247794706,
      "grad_norm": 1.899091362953186,
      "learning_rate": 6.338515143393193e-06,
      "loss": 0.5271,
      "step": 29540
    },
    {
      "epoch": 2.3696872493985564,
      "grad_norm": 2.0358726978302,
      "learning_rate": 6.330474403645136e-06,
      "loss": 0.5126,
      "step": 29550
    },
    {
      "epoch": 2.370489174017642,
      "grad_norm": 1.878527045249939,
      "learning_rate": 6.322433663897079e-06,
      "loss": 0.5868,
      "step": 29560
    },
    {
      "epoch": 2.3712910986367284,
      "grad_norm": 1.7405215501785278,
      "learning_rate": 6.314392924149022e-06,
      "loss": 0.5447,
      "step": 29570
    },
    {
      "epoch": 2.3720930232558137,
      "grad_norm": 1.871283769607544,
      "learning_rate": 6.306352184400965e-06,
      "loss": 0.48,
      "step": 29580
    },
    {
      "epoch": 2.3728949478749,
      "grad_norm": 1.9195502996444702,
      "learning_rate": 6.298311444652908e-06,
      "loss": 0.5303,
      "step": 29590
    },
    {
      "epoch": 2.3736968724939858,
      "grad_norm": 1.8264902830123901,
      "learning_rate": 6.290270704904852e-06,
      "loss": 0.546,
      "step": 29600
    },
    {
      "epoch": 2.3744987971130715,
      "grad_norm": 1.9757617712020874,
      "learning_rate": 6.282229965156795e-06,
      "loss": 0.5024,
      "step": 29610
    },
    {
      "epoch": 2.3753007217321573,
      "grad_norm": 1.9721976518630981,
      "learning_rate": 6.274189225408738e-06,
      "loss": 0.5005,
      "step": 29620
    },
    {
      "epoch": 2.376102646351243,
      "grad_norm": 2.066882848739624,
      "learning_rate": 6.266148485660681e-06,
      "loss": 0.5496,
      "step": 29630
    },
    {
      "epoch": 2.376904570970329,
      "grad_norm": 1.7370854616165161,
      "learning_rate": 6.258107745912624e-06,
      "loss": 0.521,
      "step": 29640
    },
    {
      "epoch": 2.3777064955894147,
      "grad_norm": 1.9996708631515503,
      "learning_rate": 6.250067006164567e-06,
      "loss": 0.4924,
      "step": 29650
    },
    {
      "epoch": 2.3785084202085005,
      "grad_norm": 2.115870237350464,
      "learning_rate": 6.2420262664165106e-06,
      "loss": 0.4928,
      "step": 29660
    },
    {
      "epoch": 2.3793103448275863,
      "grad_norm": 2.1087539196014404,
      "learning_rate": 6.233985526668454e-06,
      "loss": 0.582,
      "step": 29670
    },
    {
      "epoch": 2.380112269446672,
      "grad_norm": 2.011200189590454,
      "learning_rate": 6.225944786920397e-06,
      "loss": 0.5137,
      "step": 29680
    },
    {
      "epoch": 2.380914194065758,
      "grad_norm": 1.8815200328826904,
      "learning_rate": 6.21790404717234e-06,
      "loss": 0.5406,
      "step": 29690
    },
    {
      "epoch": 2.3817161186848437,
      "grad_norm": 1.7042880058288574,
      "learning_rate": 6.209863307424283e-06,
      "loss": 0.5361,
      "step": 29700
    },
    {
      "epoch": 2.3825180433039295,
      "grad_norm": 1.8675018548965454,
      "learning_rate": 6.201822567676226e-06,
      "loss": 0.5138,
      "step": 29710
    },
    {
      "epoch": 2.3833199679230153,
      "grad_norm": 2.366507053375244,
      "learning_rate": 6.1937818279281695e-06,
      "loss": 0.5868,
      "step": 29720
    },
    {
      "epoch": 2.384121892542101,
      "grad_norm": 1.9591647386550903,
      "learning_rate": 6.185741088180113e-06,
      "loss": 0.546,
      "step": 29730
    },
    {
      "epoch": 2.384923817161187,
      "grad_norm": 2.2108497619628906,
      "learning_rate": 6.177700348432056e-06,
      "loss": 0.5255,
      "step": 29740
    },
    {
      "epoch": 2.3857257417802726,
      "grad_norm": 1.854013204574585,
      "learning_rate": 6.169659608683999e-06,
      "loss": 0.5327,
      "step": 29750
    },
    {
      "epoch": 2.3865276663993584,
      "grad_norm": 1.9629920721054077,
      "learning_rate": 6.1616188689359425e-06,
      "loss": 0.5797,
      "step": 29760
    },
    {
      "epoch": 2.387329591018444,
      "grad_norm": 1.8275511264801025,
      "learning_rate": 6.153578129187885e-06,
      "loss": 0.5433,
      "step": 29770
    },
    {
      "epoch": 2.38813151563753,
      "grad_norm": 2.1263890266418457,
      "learning_rate": 6.1455373894398285e-06,
      "loss": 0.5929,
      "step": 29780
    },
    {
      "epoch": 2.388933440256616,
      "grad_norm": 2.039503574371338,
      "learning_rate": 6.137496649691772e-06,
      "loss": 0.5081,
      "step": 29790
    },
    {
      "epoch": 2.3897353648757016,
      "grad_norm": 1.9424067735671997,
      "learning_rate": 6.129455909943715e-06,
      "loss": 0.5214,
      "step": 29800
    },
    {
      "epoch": 2.3905372894947874,
      "grad_norm": 1.8531593084335327,
      "learning_rate": 6.121415170195658e-06,
      "loss": 0.5031,
      "step": 29810
    },
    {
      "epoch": 2.391339214113873,
      "grad_norm": 2.4237897396087646,
      "learning_rate": 6.113374430447601e-06,
      "loss": 0.5273,
      "step": 29820
    },
    {
      "epoch": 2.392141138732959,
      "grad_norm": 1.9298503398895264,
      "learning_rate": 6.105333690699544e-06,
      "loss": 0.5458,
      "step": 29830
    },
    {
      "epoch": 2.3929430633520448,
      "grad_norm": 2.5961697101593018,
      "learning_rate": 6.0972929509514875e-06,
      "loss": 0.5084,
      "step": 29840
    },
    {
      "epoch": 2.3937449879711306,
      "grad_norm": 1.760464072227478,
      "learning_rate": 6.089252211203431e-06,
      "loss": 0.5071,
      "step": 29850
    },
    {
      "epoch": 2.3945469125902163,
      "grad_norm": 1.840469241142273,
      "learning_rate": 6.081211471455374e-06,
      "loss": 0.4658,
      "step": 29860
    },
    {
      "epoch": 2.395348837209302,
      "grad_norm": 1.8931007385253906,
      "learning_rate": 6.073170731707318e-06,
      "loss": 0.53,
      "step": 29870
    },
    {
      "epoch": 2.3961507618283884,
      "grad_norm": 1.496627688407898,
      "learning_rate": 6.06512999195926e-06,
      "loss": 0.598,
      "step": 29880
    },
    {
      "epoch": 2.3969526864474737,
      "grad_norm": 1.9636693000793457,
      "learning_rate": 6.057089252211204e-06,
      "loss": 0.5363,
      "step": 29890
    },
    {
      "epoch": 2.39775461106656,
      "grad_norm": 1.9533259868621826,
      "learning_rate": 6.049048512463146e-06,
      "loss": 0.5786,
      "step": 29900
    },
    {
      "epoch": 2.3985565356856453,
      "grad_norm": 1.9517278671264648,
      "learning_rate": 6.04100777271509e-06,
      "loss": 0.5378,
      "step": 29910
    },
    {
      "epoch": 2.3993584603047315,
      "grad_norm": 1.9541243314743042,
      "learning_rate": 6.032967032967033e-06,
      "loss": 0.5823,
      "step": 29920
    },
    {
      "epoch": 2.4001603849238173,
      "grad_norm": 1.6108589172363281,
      "learning_rate": 6.024926293218977e-06,
      "loss": 0.6194,
      "step": 29930
    },
    {
      "epoch": 2.400962309542903,
      "grad_norm": 2.0714941024780273,
      "learning_rate": 6.016885553470919e-06,
      "loss": 0.5405,
      "step": 29940
    },
    {
      "epoch": 2.401764234161989,
      "grad_norm": 2.0968120098114014,
      "learning_rate": 6.008844813722863e-06,
      "loss": 0.567,
      "step": 29950
    },
    {
      "epoch": 2.4025661587810747,
      "grad_norm": 2.112717390060425,
      "learning_rate": 6.000804073974805e-06,
      "loss": 0.5225,
      "step": 29960
    },
    {
      "epoch": 2.4033680834001605,
      "grad_norm": 1.8257685899734497,
      "learning_rate": 5.992763334226749e-06,
      "loss": 0.5393,
      "step": 29970
    },
    {
      "epoch": 2.4041700080192463,
      "grad_norm": 1.690788984298706,
      "learning_rate": 5.984722594478692e-06,
      "loss": 0.5817,
      "step": 29980
    },
    {
      "epoch": 2.404971932638332,
      "grad_norm": 2.275678873062134,
      "learning_rate": 5.976681854730636e-06,
      "loss": 0.547,
      "step": 29990
    },
    {
      "epoch": 2.405773857257418,
      "grad_norm": 2.196181535720825,
      "learning_rate": 5.968641114982578e-06,
      "loss": 0.5316,
      "step": 30000
    },
    {
      "epoch": 2.4065757818765037,
      "grad_norm": 1.8931492567062378,
      "learning_rate": 5.960600375234522e-06,
      "loss": 0.556,
      "step": 30010
    },
    {
      "epoch": 2.4073777064955895,
      "grad_norm": 1.954509973526001,
      "learning_rate": 5.952559635486464e-06,
      "loss": 0.5142,
      "step": 30020
    },
    {
      "epoch": 2.4081796311146753,
      "grad_norm": 1.8612282276153564,
      "learning_rate": 5.944518895738408e-06,
      "loss": 0.6134,
      "step": 30030
    },
    {
      "epoch": 2.408981555733761,
      "grad_norm": 1.9546006917953491,
      "learning_rate": 5.936478155990351e-06,
      "loss": 0.5884,
      "step": 30040
    },
    {
      "epoch": 2.409783480352847,
      "grad_norm": 1.7084033489227295,
      "learning_rate": 5.928437416242295e-06,
      "loss": 0.5228,
      "step": 30050
    },
    {
      "epoch": 2.4105854049719326,
      "grad_norm": 1.7953770160675049,
      "learning_rate": 5.920396676494238e-06,
      "loss": 0.5079,
      "step": 30060
    },
    {
      "epoch": 2.4113873295910184,
      "grad_norm": 2.3777260780334473,
      "learning_rate": 5.912355936746181e-06,
      "loss": 0.4808,
      "step": 30070
    },
    {
      "epoch": 2.412189254210104,
      "grad_norm": 2.003797769546509,
      "learning_rate": 5.904315196998124e-06,
      "loss": 0.5109,
      "step": 30080
    },
    {
      "epoch": 2.41299117882919,
      "grad_norm": 2.3581185340881348,
      "learning_rate": 5.896274457250067e-06,
      "loss": 0.5849,
      "step": 30090
    },
    {
      "epoch": 2.413793103448276,
      "grad_norm": 1.935935139656067,
      "learning_rate": 5.88823371750201e-06,
      "loss": 0.5355,
      "step": 30100
    },
    {
      "epoch": 2.4145950280673616,
      "grad_norm": 2.039436101913452,
      "learning_rate": 5.880192977753954e-06,
      "loss": 0.5697,
      "step": 30110
    },
    {
      "epoch": 2.4153969526864474,
      "grad_norm": 1.7000529766082764,
      "learning_rate": 5.872152238005897e-06,
      "loss": 0.5786,
      "step": 30120
    },
    {
      "epoch": 2.416198877305533,
      "grad_norm": 1.9064786434173584,
      "learning_rate": 5.86411149825784e-06,
      "loss": 0.521,
      "step": 30130
    },
    {
      "epoch": 2.417000801924619,
      "grad_norm": 2.1874566078186035,
      "learning_rate": 5.856070758509783e-06,
      "loss": 0.5137,
      "step": 30140
    },
    {
      "epoch": 2.4178027265437048,
      "grad_norm": 1.8087716102600098,
      "learning_rate": 5.848030018761726e-06,
      "loss": 0.5037,
      "step": 30150
    },
    {
      "epoch": 2.4186046511627906,
      "grad_norm": 1.7709897756576538,
      "learning_rate": 5.839989279013669e-06,
      "loss": 0.5079,
      "step": 30160
    },
    {
      "epoch": 2.4194065757818763,
      "grad_norm": 2.1118428707122803,
      "learning_rate": 5.831948539265613e-06,
      "loss": 0.5371,
      "step": 30170
    },
    {
      "epoch": 2.420208500400962,
      "grad_norm": 1.9654401540756226,
      "learning_rate": 5.823907799517556e-06,
      "loss": 0.5361,
      "step": 30180
    },
    {
      "epoch": 2.421010425020048,
      "grad_norm": 1.9683010578155518,
      "learning_rate": 5.8158670597694995e-06,
      "loss": 0.5066,
      "step": 30190
    },
    {
      "epoch": 2.4218123496391337,
      "grad_norm": 1.864161729812622,
      "learning_rate": 5.807826320021442e-06,
      "loss": 0.5596,
      "step": 30200
    },
    {
      "epoch": 2.42261427425822,
      "grad_norm": 2.139986515045166,
      "learning_rate": 5.7997855802733855e-06,
      "loss": 0.5104,
      "step": 30210
    },
    {
      "epoch": 2.4234161988773053,
      "grad_norm": 1.8523497581481934,
      "learning_rate": 5.791744840525328e-06,
      "loss": 0.5068,
      "step": 30220
    },
    {
      "epoch": 2.4242181234963915,
      "grad_norm": 1.9276514053344727,
      "learning_rate": 5.7837041007772716e-06,
      "loss": 0.5013,
      "step": 30230
    },
    {
      "epoch": 2.4250200481154773,
      "grad_norm": 1.652036190032959,
      "learning_rate": 5.775663361029215e-06,
      "loss": 0.5428,
      "step": 30240
    },
    {
      "epoch": 2.425821972734563,
      "grad_norm": 1.8742356300354004,
      "learning_rate": 5.7676226212811585e-06,
      "loss": 0.5213,
      "step": 30250
    },
    {
      "epoch": 2.426623897353649,
      "grad_norm": 2.05349063873291,
      "learning_rate": 5.759581881533101e-06,
      "loss": 0.539,
      "step": 30260
    },
    {
      "epoch": 2.4274258219727347,
      "grad_norm": 1.6254395246505737,
      "learning_rate": 5.7515411417850445e-06,
      "loss": 0.4925,
      "step": 30270
    },
    {
      "epoch": 2.4282277465918205,
      "grad_norm": 1.994815468788147,
      "learning_rate": 5.743500402036987e-06,
      "loss": 0.5573,
      "step": 30280
    },
    {
      "epoch": 2.4290296712109063,
      "grad_norm": 1.9824267625808716,
      "learning_rate": 5.7354596622889305e-06,
      "loss": 0.5409,
      "step": 30290
    },
    {
      "epoch": 2.429831595829992,
      "grad_norm": 2.3959388732910156,
      "learning_rate": 5.727418922540874e-06,
      "loss": 0.5494,
      "step": 30300
    },
    {
      "epoch": 2.430633520449078,
      "grad_norm": 1.9621042013168335,
      "learning_rate": 5.719378182792817e-06,
      "loss": 0.559,
      "step": 30310
    },
    {
      "epoch": 2.4314354450681637,
      "grad_norm": 2.2323672771453857,
      "learning_rate": 5.71133744304476e-06,
      "loss": 0.5524,
      "step": 30320
    },
    {
      "epoch": 2.4322373696872495,
      "grad_norm": 1.82514226436615,
      "learning_rate": 5.7032967032967035e-06,
      "loss": 0.535,
      "step": 30330
    },
    {
      "epoch": 2.4330392943063353,
      "grad_norm": 1.8988041877746582,
      "learning_rate": 5.695255963548646e-06,
      "loss": 0.6028,
      "step": 30340
    },
    {
      "epoch": 2.433841218925421,
      "grad_norm": 2.0618929862976074,
      "learning_rate": 5.6872152238005895e-06,
      "loss": 0.5318,
      "step": 30350
    },
    {
      "epoch": 2.434643143544507,
      "grad_norm": 1.8174957036972046,
      "learning_rate": 5.679174484052533e-06,
      "loss": 0.5452,
      "step": 30360
    },
    {
      "epoch": 2.4354450681635926,
      "grad_norm": 1.9730839729309082,
      "learning_rate": 5.671133744304476e-06,
      "loss": 0.565,
      "step": 30370
    },
    {
      "epoch": 2.4362469927826784,
      "grad_norm": 1.845341444015503,
      "learning_rate": 5.66309300455642e-06,
      "loss": 0.6136,
      "step": 30380
    },
    {
      "epoch": 2.437048917401764,
      "grad_norm": 1.8622040748596191,
      "learning_rate": 5.6550522648083624e-06,
      "loss": 0.509,
      "step": 30390
    },
    {
      "epoch": 2.43785084202085,
      "grad_norm": 2.149693250656128,
      "learning_rate": 5.647011525060306e-06,
      "loss": 0.5766,
      "step": 30400
    },
    {
      "epoch": 2.438652766639936,
      "grad_norm": 1.9216346740722656,
      "learning_rate": 5.6389707853122485e-06,
      "loss": 0.5298,
      "step": 30410
    },
    {
      "epoch": 2.4394546912590216,
      "grad_norm": 1.9563374519348145,
      "learning_rate": 5.630930045564192e-06,
      "loss": 0.5142,
      "step": 30420
    },
    {
      "epoch": 2.4402566158781074,
      "grad_norm": 1.831126093864441,
      "learning_rate": 5.622889305816135e-06,
      "loss": 0.6106,
      "step": 30430
    },
    {
      "epoch": 2.441058540497193,
      "grad_norm": 1.8467082977294922,
      "learning_rate": 5.614848566068079e-06,
      "loss": 0.5564,
      "step": 30440
    },
    {
      "epoch": 2.441860465116279,
      "grad_norm": 2.441692352294922,
      "learning_rate": 5.606807826320021e-06,
      "loss": 0.5957,
      "step": 30450
    },
    {
      "epoch": 2.4426623897353648,
      "grad_norm": 2.004627227783203,
      "learning_rate": 5.598767086571965e-06,
      "loss": 0.563,
      "step": 30460
    },
    {
      "epoch": 2.4434643143544506,
      "grad_norm": 2.2103536128997803,
      "learning_rate": 5.5907263468239074e-06,
      "loss": 0.544,
      "step": 30470
    },
    {
      "epoch": 2.4442662389735363,
      "grad_norm": 1.8629631996154785,
      "learning_rate": 5.582685607075851e-06,
      "loss": 0.491,
      "step": 30480
    },
    {
      "epoch": 2.445068163592622,
      "grad_norm": 2.3363585472106934,
      "learning_rate": 5.574644867327794e-06,
      "loss": 0.5398,
      "step": 30490
    },
    {
      "epoch": 2.445870088211708,
      "grad_norm": 2.2672178745269775,
      "learning_rate": 5.566604127579738e-06,
      "loss": 0.548,
      "step": 30500
    },
    {
      "epoch": 2.4466720128307937,
      "grad_norm": 1.9049426317214966,
      "learning_rate": 5.558563387831681e-06,
      "loss": 0.5434,
      "step": 30510
    },
    {
      "epoch": 2.44747393744988,
      "grad_norm": 1.9907441139221191,
      "learning_rate": 5.550522648083624e-06,
      "loss": 0.5353,
      "step": 30520
    },
    {
      "epoch": 2.4482758620689653,
      "grad_norm": 1.870638132095337,
      "learning_rate": 5.542481908335567e-06,
      "loss": 0.5813,
      "step": 30530
    },
    {
      "epoch": 2.4490777866880515,
      "grad_norm": 1.8781846761703491,
      "learning_rate": 5.53444116858751e-06,
      "loss": 0.5076,
      "step": 30540
    },
    {
      "epoch": 2.449879711307137,
      "grad_norm": 2.4011080265045166,
      "learning_rate": 5.526400428839453e-06,
      "loss": 0.5584,
      "step": 30550
    },
    {
      "epoch": 2.450681635926223,
      "grad_norm": 2.0444693565368652,
      "learning_rate": 5.518359689091397e-06,
      "loss": 0.5122,
      "step": 30560
    },
    {
      "epoch": 2.451483560545309,
      "grad_norm": 2.1432008743286133,
      "learning_rate": 5.51031894934334e-06,
      "loss": 0.5101,
      "step": 30570
    },
    {
      "epoch": 2.4522854851643947,
      "grad_norm": 1.9102739095687866,
      "learning_rate": 5.502278209595283e-06,
      "loss": 0.5263,
      "step": 30580
    },
    {
      "epoch": 2.4530874097834805,
      "grad_norm": 1.9859538078308105,
      "learning_rate": 5.494237469847226e-06,
      "loss": 0.5573,
      "step": 30590
    },
    {
      "epoch": 2.4538893344025663,
      "grad_norm": 2.1413955688476562,
      "learning_rate": 5.486196730099169e-06,
      "loss": 0.5036,
      "step": 30600
    },
    {
      "epoch": 2.454691259021652,
      "grad_norm": 2.3836681842803955,
      "learning_rate": 5.478155990351112e-06,
      "loss": 0.5618,
      "step": 30610
    },
    {
      "epoch": 2.455493183640738,
      "grad_norm": 1.932774305343628,
      "learning_rate": 5.470115250603056e-06,
      "loss": 0.5261,
      "step": 30620
    },
    {
      "epoch": 2.4562951082598237,
      "grad_norm": 1.857726812362671,
      "learning_rate": 5.462074510854999e-06,
      "loss": 0.5637,
      "step": 30630
    },
    {
      "epoch": 2.4570970328789095,
      "grad_norm": 2.2074856758117676,
      "learning_rate": 5.454033771106942e-06,
      "loss": 0.5732,
      "step": 30640
    },
    {
      "epoch": 2.4578989574979953,
      "grad_norm": 1.7926783561706543,
      "learning_rate": 5.445993031358885e-06,
      "loss": 0.5337,
      "step": 30650
    },
    {
      "epoch": 2.458700882117081,
      "grad_norm": 2.1751692295074463,
      "learning_rate": 5.437952291610828e-06,
      "loss": 0.6011,
      "step": 30660
    },
    {
      "epoch": 2.459502806736167,
      "grad_norm": 1.8176743984222412,
      "learning_rate": 5.429911551862771e-06,
      "loss": 0.591,
      "step": 30670
    },
    {
      "epoch": 2.4603047313552526,
      "grad_norm": 1.7670879364013672,
      "learning_rate": 5.421870812114715e-06,
      "loss": 0.6042,
      "step": 30680
    },
    {
      "epoch": 2.4611066559743384,
      "grad_norm": 1.658638596534729,
      "learning_rate": 5.413830072366658e-06,
      "loss": 0.5304,
      "step": 30690
    },
    {
      "epoch": 2.461908580593424,
      "grad_norm": 1.8789732456207275,
      "learning_rate": 5.4057893326186015e-06,
      "loss": 0.5156,
      "step": 30700
    },
    {
      "epoch": 2.46271050521251,
      "grad_norm": 1.657739520072937,
      "learning_rate": 5.397748592870544e-06,
      "loss": 0.6048,
      "step": 30710
    },
    {
      "epoch": 2.463512429831596,
      "grad_norm": 1.869614839553833,
      "learning_rate": 5.3897078531224876e-06,
      "loss": 0.5441,
      "step": 30720
    },
    {
      "epoch": 2.4643143544506816,
      "grad_norm": 1.9918700456619263,
      "learning_rate": 5.38166711337443e-06,
      "loss": 0.4873,
      "step": 30730
    },
    {
      "epoch": 2.4651162790697674,
      "grad_norm": 1.9632161855697632,
      "learning_rate": 5.373626373626374e-06,
      "loss": 0.5306,
      "step": 30740
    },
    {
      "epoch": 2.465918203688853,
      "grad_norm": 1.892526626586914,
      "learning_rate": 5.365585633878317e-06,
      "loss": 0.5725,
      "step": 30750
    },
    {
      "epoch": 2.466720128307939,
      "grad_norm": 2.52290415763855,
      "learning_rate": 5.3575448941302605e-06,
      "loss": 0.5795,
      "step": 30760
    },
    {
      "epoch": 2.4675220529270248,
      "grad_norm": 1.9238600730895996,
      "learning_rate": 5.349504154382203e-06,
      "loss": 0.4951,
      "step": 30770
    },
    {
      "epoch": 2.4683239775461105,
      "grad_norm": 1.8919402360916138,
      "learning_rate": 5.3414634146341465e-06,
      "loss": 0.5029,
      "step": 30780
    },
    {
      "epoch": 2.4691259021651963,
      "grad_norm": 1.6404623985290527,
      "learning_rate": 5.333422674886089e-06,
      "loss": 0.5307,
      "step": 30790
    },
    {
      "epoch": 2.469927826784282,
      "grad_norm": 2.2151339054107666,
      "learning_rate": 5.325381935138033e-06,
      "loss": 0.5195,
      "step": 30800
    },
    {
      "epoch": 2.470729751403368,
      "grad_norm": 2.220923662185669,
      "learning_rate": 5.317341195389976e-06,
      "loss": 0.5658,
      "step": 30810
    },
    {
      "epoch": 2.4715316760224537,
      "grad_norm": 2.0833027362823486,
      "learning_rate": 5.3093004556419195e-06,
      "loss": 0.5098,
      "step": 30820
    },
    {
      "epoch": 2.4723336006415395,
      "grad_norm": 1.8376675844192505,
      "learning_rate": 5.301259715893863e-06,
      "loss": 0.5239,
      "step": 30830
    },
    {
      "epoch": 2.4731355252606253,
      "grad_norm": 1.9041062593460083,
      "learning_rate": 5.2932189761458055e-06,
      "loss": 0.5458,
      "step": 30840
    },
    {
      "epoch": 2.4739374498797115,
      "grad_norm": 1.8865686655044556,
      "learning_rate": 5.285178236397749e-06,
      "loss": 0.5158,
      "step": 30850
    },
    {
      "epoch": 2.474739374498797,
      "grad_norm": 2.2760062217712402,
      "learning_rate": 5.2771374966496915e-06,
      "loss": 0.5055,
      "step": 30860
    },
    {
      "epoch": 2.475541299117883,
      "grad_norm": 2.0698435306549072,
      "learning_rate": 5.269096756901635e-06,
      "loss": 0.5144,
      "step": 30870
    },
    {
      "epoch": 2.476343223736969,
      "grad_norm": 1.9732019901275635,
      "learning_rate": 5.2610560171535784e-06,
      "loss": 0.5377,
      "step": 30880
    },
    {
      "epoch": 2.4771451483560547,
      "grad_norm": 1.9113253355026245,
      "learning_rate": 5.253015277405522e-06,
      "loss": 0.4951,
      "step": 30890
    },
    {
      "epoch": 2.4779470729751405,
      "grad_norm": 2.2809178829193115,
      "learning_rate": 5.2449745376574645e-06,
      "loss": 0.6374,
      "step": 30900
    },
    {
      "epoch": 2.4787489975942263,
      "grad_norm": 2.0896551609039307,
      "learning_rate": 5.236933797909408e-06,
      "loss": 0.5113,
      "step": 30910
    },
    {
      "epoch": 2.479550922213312,
      "grad_norm": 1.8948005437850952,
      "learning_rate": 5.2288930581613505e-06,
      "loss": 0.5147,
      "step": 30920
    },
    {
      "epoch": 2.480352846832398,
      "grad_norm": 1.8672502040863037,
      "learning_rate": 5.220852318413294e-06,
      "loss": 0.517,
      "step": 30930
    },
    {
      "epoch": 2.4811547714514837,
      "grad_norm": 2.100205183029175,
      "learning_rate": 5.212811578665237e-06,
      "loss": 0.5503,
      "step": 30940
    },
    {
      "epoch": 2.4819566960705695,
      "grad_norm": 1.873561978340149,
      "learning_rate": 5.204770838917181e-06,
      "loss": 0.523,
      "step": 30950
    },
    {
      "epoch": 2.4827586206896552,
      "grad_norm": 1.7136969566345215,
      "learning_rate": 5.1967300991691234e-06,
      "loss": 0.5629,
      "step": 30960
    },
    {
      "epoch": 2.483560545308741,
      "grad_norm": 1.7440489530563354,
      "learning_rate": 5.188689359421067e-06,
      "loss": 0.4996,
      "step": 30970
    },
    {
      "epoch": 2.484362469927827,
      "grad_norm": 1.9562066793441772,
      "learning_rate": 5.18064861967301e-06,
      "loss": 0.5403,
      "step": 30980
    },
    {
      "epoch": 2.4851643945469126,
      "grad_norm": 2.1803531646728516,
      "learning_rate": 5.172607879924953e-06,
      "loss": 0.5225,
      "step": 30990
    },
    {
      "epoch": 2.4859663191659984,
      "grad_norm": 1.8698458671569824,
      "learning_rate": 5.164567140176896e-06,
      "loss": 0.5199,
      "step": 31000
    },
    {
      "epoch": 2.486768243785084,
      "grad_norm": 1.7915277481079102,
      "learning_rate": 5.15652640042884e-06,
      "loss": 0.4908,
      "step": 31010
    },
    {
      "epoch": 2.48757016840417,
      "grad_norm": 2.092139720916748,
      "learning_rate": 5.148485660680783e-06,
      "loss": 0.5165,
      "step": 31020
    },
    {
      "epoch": 2.488372093023256,
      "grad_norm": 1.9610158205032349,
      "learning_rate": 5.140444920932726e-06,
      "loss": 0.482,
      "step": 31030
    },
    {
      "epoch": 2.4891740176423416,
      "grad_norm": 1.997398853302002,
      "learning_rate": 5.132404181184669e-06,
      "loss": 0.5317,
      "step": 31040
    },
    {
      "epoch": 2.4899759422614274,
      "grad_norm": 1.938119649887085,
      "learning_rate": 5.124363441436612e-06,
      "loss": 0.5466,
      "step": 31050
    },
    {
      "epoch": 2.490777866880513,
      "grad_norm": 1.906619906425476,
      "learning_rate": 5.116322701688555e-06,
      "loss": 0.5655,
      "step": 31060
    },
    {
      "epoch": 2.491579791499599,
      "grad_norm": 1.8049207925796509,
      "learning_rate": 5.108281961940499e-06,
      "loss": 0.5424,
      "step": 31070
    },
    {
      "epoch": 2.4923817161186848,
      "grad_norm": 1.8603721857070923,
      "learning_rate": 5.100241222192442e-06,
      "loss": 0.5671,
      "step": 31080
    },
    {
      "epoch": 2.4931836407377705,
      "grad_norm": 2.5110249519348145,
      "learning_rate": 5.092200482444385e-06,
      "loss": 0.5342,
      "step": 31090
    },
    {
      "epoch": 2.4939855653568563,
      "grad_norm": 2.0389676094055176,
      "learning_rate": 5.084159742696328e-06,
      "loss": 0.5428,
      "step": 31100
    },
    {
      "epoch": 2.494787489975942,
      "grad_norm": 1.780758261680603,
      "learning_rate": 5.076119002948271e-06,
      "loss": 0.5308,
      "step": 31110
    },
    {
      "epoch": 2.495589414595028,
      "grad_norm": 1.7234445810317993,
      "learning_rate": 5.068078263200214e-06,
      "loss": 0.5447,
      "step": 31120
    },
    {
      "epoch": 2.4963913392141137,
      "grad_norm": 1.6933434009552002,
      "learning_rate": 5.060037523452158e-06,
      "loss": 0.5275,
      "step": 31130
    },
    {
      "epoch": 2.4971932638331995,
      "grad_norm": 1.7763078212738037,
      "learning_rate": 5.051996783704101e-06,
      "loss": 0.5595,
      "step": 31140
    },
    {
      "epoch": 2.4979951884522853,
      "grad_norm": 1.8109233379364014,
      "learning_rate": 5.043956043956045e-06,
      "loss": 0.5392,
      "step": 31150
    },
    {
      "epoch": 2.4987971130713715,
      "grad_norm": 2.215796709060669,
      "learning_rate": 5.035915304207987e-06,
      "loss": 0.5285,
      "step": 31160
    },
    {
      "epoch": 2.499599037690457,
      "grad_norm": 1.8811166286468506,
      "learning_rate": 5.027874564459931e-06,
      "loss": 0.549,
      "step": 31170
    },
    {
      "epoch": 2.500400962309543,
      "grad_norm": 1.8259601593017578,
      "learning_rate": 5.019833824711873e-06,
      "loss": 0.5502,
      "step": 31180
    },
    {
      "epoch": 2.5012028869286285,
      "grad_norm": 1.600245475769043,
      "learning_rate": 5.011793084963817e-06,
      "loss": 0.5192,
      "step": 31190
    },
    {
      "epoch": 2.5020048115477147,
      "grad_norm": 1.8484328985214233,
      "learning_rate": 5.00375234521576e-06,
      "loss": 0.5102,
      "step": 31200
    },
    {
      "epoch": 2.5028067361668,
      "grad_norm": 1.9219138622283936,
      "learning_rate": 4.995711605467704e-06,
      "loss": 0.5551,
      "step": 31210
    },
    {
      "epoch": 2.5036086607858863,
      "grad_norm": 1.8096808195114136,
      "learning_rate": 4.987670865719646e-06,
      "loss": 0.5328,
      "step": 31220
    },
    {
      "epoch": 2.504410585404972,
      "grad_norm": 2.0783326625823975,
      "learning_rate": 4.97963012597159e-06,
      "loss": 0.5455,
      "step": 31230
    },
    {
      "epoch": 2.505212510024058,
      "grad_norm": 2.0276598930358887,
      "learning_rate": 4.971589386223532e-06,
      "loss": 0.5328,
      "step": 31240
    },
    {
      "epoch": 2.5060144346431437,
      "grad_norm": 2.0256428718566895,
      "learning_rate": 4.963548646475476e-06,
      "loss": 0.5727,
      "step": 31250
    },
    {
      "epoch": 2.5068163592622295,
      "grad_norm": 2.0388102531433105,
      "learning_rate": 4.955507906727419e-06,
      "loss": 0.5427,
      "step": 31260
    },
    {
      "epoch": 2.5076182838813152,
      "grad_norm": 2.036141872406006,
      "learning_rate": 4.9474671669793625e-06,
      "loss": 0.4906,
      "step": 31270
    },
    {
      "epoch": 2.508420208500401,
      "grad_norm": 1.9109747409820557,
      "learning_rate": 4.939426427231305e-06,
      "loss": 0.5253,
      "step": 31280
    },
    {
      "epoch": 2.509222133119487,
      "grad_norm": 1.6994282007217407,
      "learning_rate": 4.931385687483249e-06,
      "loss": 0.5137,
      "step": 31290
    },
    {
      "epoch": 2.5100240577385726,
      "grad_norm": 2.135183811187744,
      "learning_rate": 4.923344947735192e-06,
      "loss": 0.5514,
      "step": 31300
    },
    {
      "epoch": 2.5108259823576584,
      "grad_norm": 2.0883500576019287,
      "learning_rate": 4.915304207987135e-06,
      "loss": 0.5594,
      "step": 31310
    },
    {
      "epoch": 2.511627906976744,
      "grad_norm": 1.895491600036621,
      "learning_rate": 4.907263468239078e-06,
      "loss": 0.5563,
      "step": 31320
    },
    {
      "epoch": 2.51242983159583,
      "grad_norm": 2.417387008666992,
      "learning_rate": 4.8992227284910215e-06,
      "loss": 0.5332,
      "step": 31330
    },
    {
      "epoch": 2.513231756214916,
      "grad_norm": 1.995843529701233,
      "learning_rate": 4.891181988742965e-06,
      "loss": 0.527,
      "step": 31340
    },
    {
      "epoch": 2.5140336808340016,
      "grad_norm": 2.1968352794647217,
      "learning_rate": 4.8831412489949075e-06,
      "loss": 0.561,
      "step": 31350
    },
    {
      "epoch": 2.5148356054530874,
      "grad_norm": 2.1646459102630615,
      "learning_rate": 4.875100509246851e-06,
      "loss": 0.5172,
      "step": 31360
    },
    {
      "epoch": 2.515637530072173,
      "grad_norm": 2.0455594062805176,
      "learning_rate": 4.867059769498794e-06,
      "loss": 0.5814,
      "step": 31370
    },
    {
      "epoch": 2.516439454691259,
      "grad_norm": 2.02221941947937,
      "learning_rate": 4.859019029750737e-06,
      "loss": 0.5338,
      "step": 31380
    },
    {
      "epoch": 2.5172413793103448,
      "grad_norm": 1.8708845376968384,
      "learning_rate": 4.8509782900026805e-06,
      "loss": 0.5356,
      "step": 31390
    },
    {
      "epoch": 2.5180433039294305,
      "grad_norm": 1.848278522491455,
      "learning_rate": 4.842937550254624e-06,
      "loss": 0.4876,
      "step": 31400
    }
  ],
  "logging_steps": 10,
  "max_steps": 37410,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.106004146268406e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
