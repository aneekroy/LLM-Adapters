{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9758114533801945,
  "eval_steps": 500,
  "global_step": 3600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008269588587967749,
      "grad_norm": 1.756215214729309,
      "learning_rate": 2.7e-06,
      "loss": 3.3558,
      "step": 10
    },
    {
      "epoch": 0.016539177175935497,
      "grad_norm": 1.752993106842041,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 3.3034,
      "step": 20
    },
    {
      "epoch": 0.024808765763903246,
      "grad_norm": 1.7655502557754517,
      "learning_rate": 8.7e-06,
      "loss": 3.2912,
      "step": 30
    },
    {
      "epoch": 0.033078354351870995,
      "grad_norm": 1.3982210159301758,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 3.2783,
      "step": 40
    },
    {
      "epoch": 0.04134794293983874,
      "grad_norm": 1.6441909074783325,
      "learning_rate": 1.47e-05,
      "loss": 3.2416,
      "step": 50
    },
    {
      "epoch": 0.04961753152780649,
      "grad_norm": 1.5710030794143677,
      "learning_rate": 1.77e-05,
      "loss": 3.1066,
      "step": 60
    },
    {
      "epoch": 0.05788712011577424,
      "grad_norm": 1.9287712574005127,
      "learning_rate": 2.07e-05,
      "loss": 3.0285,
      "step": 70
    },
    {
      "epoch": 0.06615670870374199,
      "grad_norm": 2.0362753868103027,
      "learning_rate": 2.37e-05,
      "loss": 2.9188,
      "step": 80
    },
    {
      "epoch": 0.07442629729170974,
      "grad_norm": 2.1672632694244385,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.7159,
      "step": 90
    },
    {
      "epoch": 0.08269588587967748,
      "grad_norm": 2.4566586017608643,
      "learning_rate": 2.97e-05,
      "loss": 2.4579,
      "step": 100
    },
    {
      "epoch": 0.09096547446764523,
      "grad_norm": 2.895225763320923,
      "learning_rate": 2.9923512747875356e-05,
      "loss": 2.2011,
      "step": 110
    },
    {
      "epoch": 0.09923506305561298,
      "grad_norm": 3.7146999835968018,
      "learning_rate": 2.9838526912181304e-05,
      "loss": 1.9867,
      "step": 120
    },
    {
      "epoch": 0.10750465164358074,
      "grad_norm": 2.5795164108276367,
      "learning_rate": 2.9753541076487252e-05,
      "loss": 1.6824,
      "step": 130
    },
    {
      "epoch": 0.11577424023154848,
      "grad_norm": 2.636284112930298,
      "learning_rate": 2.96685552407932e-05,
      "loss": 1.5286,
      "step": 140
    },
    {
      "epoch": 0.12404382881951623,
      "grad_norm": 2.8281455039978027,
      "learning_rate": 2.9583569405099153e-05,
      "loss": 1.3347,
      "step": 150
    },
    {
      "epoch": 0.13231341740748398,
      "grad_norm": 2.061628580093384,
      "learning_rate": 2.9498583569405098e-05,
      "loss": 1.2909,
      "step": 160
    },
    {
      "epoch": 0.14058300599545173,
      "grad_norm": 1.5762178897857666,
      "learning_rate": 2.941359773371105e-05,
      "loss": 1.311,
      "step": 170
    },
    {
      "epoch": 0.14885259458341948,
      "grad_norm": 1.7008037567138672,
      "learning_rate": 2.9328611898016998e-05,
      "loss": 1.263,
      "step": 180
    },
    {
      "epoch": 0.15712218317138724,
      "grad_norm": 1.8857333660125732,
      "learning_rate": 2.9243626062322946e-05,
      "loss": 1.3214,
      "step": 190
    },
    {
      "epoch": 0.16539177175935496,
      "grad_norm": 1.4929569959640503,
      "learning_rate": 2.9158640226628895e-05,
      "loss": 1.3044,
      "step": 200
    },
    {
      "epoch": 0.1736613603473227,
      "grad_norm": 1.3408827781677246,
      "learning_rate": 2.9073654390934846e-05,
      "loss": 1.3358,
      "step": 210
    },
    {
      "epoch": 0.18193094893529047,
      "grad_norm": 1.6168785095214844,
      "learning_rate": 2.8988668555240795e-05,
      "loss": 1.2749,
      "step": 220
    },
    {
      "epoch": 0.19020053752325822,
      "grad_norm": 1.6387059688568115,
      "learning_rate": 2.8903682719546743e-05,
      "loss": 1.2767,
      "step": 230
    },
    {
      "epoch": 0.19847012611122597,
      "grad_norm": 1.7577881813049316,
      "learning_rate": 2.881869688385269e-05,
      "loss": 1.3062,
      "step": 240
    },
    {
      "epoch": 0.20673971469919372,
      "grad_norm": 1.4629862308502197,
      "learning_rate": 2.873371104815864e-05,
      "loss": 1.2996,
      "step": 250
    },
    {
      "epoch": 0.21500930328716147,
      "grad_norm": 1.6897460222244263,
      "learning_rate": 2.8648725212464592e-05,
      "loss": 1.2563,
      "step": 260
    },
    {
      "epoch": 0.22327889187512923,
      "grad_norm": 2.6333374977111816,
      "learning_rate": 2.8563739376770537e-05,
      "loss": 1.2537,
      "step": 270
    },
    {
      "epoch": 0.23154848046309695,
      "grad_norm": 1.8025819063186646,
      "learning_rate": 2.847875354107649e-05,
      "loss": 1.2138,
      "step": 280
    },
    {
      "epoch": 0.2398180690510647,
      "grad_norm": 1.747097373008728,
      "learning_rate": 2.8393767705382437e-05,
      "loss": 1.2877,
      "step": 290
    },
    {
      "epoch": 0.24808765763903246,
      "grad_norm": 2.179395914077759,
      "learning_rate": 2.8308781869688385e-05,
      "loss": 1.2934,
      "step": 300
    },
    {
      "epoch": 0.2563572462270002,
      "grad_norm": 1.7090775966644287,
      "learning_rate": 2.8223796033994334e-05,
      "loss": 1.2923,
      "step": 310
    },
    {
      "epoch": 0.26462683481496796,
      "grad_norm": 1.250209927558899,
      "learning_rate": 2.8138810198300286e-05,
      "loss": 1.3089,
      "step": 320
    },
    {
      "epoch": 0.2728964234029357,
      "grad_norm": 1.9530397653579712,
      "learning_rate": 2.8053824362606234e-05,
      "loss": 1.2658,
      "step": 330
    },
    {
      "epoch": 0.28116601199090346,
      "grad_norm": 1.7466472387313843,
      "learning_rate": 2.796883852691218e-05,
      "loss": 1.241,
      "step": 340
    },
    {
      "epoch": 0.2894356005788712,
      "grad_norm": 1.7264188528060913,
      "learning_rate": 2.788385269121813e-05,
      "loss": 1.2234,
      "step": 350
    },
    {
      "epoch": 0.29770518916683897,
      "grad_norm": 1.4299172163009644,
      "learning_rate": 2.779886685552408e-05,
      "loss": 1.2115,
      "step": 360
    },
    {
      "epoch": 0.3059747777548067,
      "grad_norm": 1.5544111728668213,
      "learning_rate": 2.771388101983003e-05,
      "loss": 1.3117,
      "step": 370
    },
    {
      "epoch": 0.3142443663427745,
      "grad_norm": 1.3608521223068237,
      "learning_rate": 2.7628895184135976e-05,
      "loss": 1.2128,
      "step": 380
    },
    {
      "epoch": 0.3225139549307422,
      "grad_norm": 1.503814935684204,
      "learning_rate": 2.7543909348441928e-05,
      "loss": 1.2539,
      "step": 390
    },
    {
      "epoch": 0.3307835435187099,
      "grad_norm": 1.3669956922531128,
      "learning_rate": 2.7458923512747876e-05,
      "loss": 1.2122,
      "step": 400
    },
    {
      "epoch": 0.3390531321066777,
      "grad_norm": 2.168065071105957,
      "learning_rate": 2.7373937677053825e-05,
      "loss": 1.286,
      "step": 410
    },
    {
      "epoch": 0.3473227206946454,
      "grad_norm": 1.4657477140426636,
      "learning_rate": 2.7288951841359773e-05,
      "loss": 1.2139,
      "step": 420
    },
    {
      "epoch": 0.3555923092826132,
      "grad_norm": 1.7266279458999634,
      "learning_rate": 2.7203966005665725e-05,
      "loss": 1.2031,
      "step": 430
    },
    {
      "epoch": 0.36386189787058093,
      "grad_norm": 1.5935134887695312,
      "learning_rate": 2.7118980169971673e-05,
      "loss": 1.3131,
      "step": 440
    },
    {
      "epoch": 0.3721314864585487,
      "grad_norm": 1.7817572355270386,
      "learning_rate": 2.7033994334277618e-05,
      "loss": 1.2198,
      "step": 450
    },
    {
      "epoch": 0.38040107504651643,
      "grad_norm": 1.4046440124511719,
      "learning_rate": 2.694900849858357e-05,
      "loss": 1.2599,
      "step": 460
    },
    {
      "epoch": 0.38867066363448416,
      "grad_norm": 1.6538200378417969,
      "learning_rate": 2.686402266288952e-05,
      "loss": 1.2413,
      "step": 470
    },
    {
      "epoch": 0.39694025222245194,
      "grad_norm": 1.6341058015823364,
      "learning_rate": 2.677903682719547e-05,
      "loss": 1.3059,
      "step": 480
    },
    {
      "epoch": 0.40520984081041966,
      "grad_norm": 1.4044157266616821,
      "learning_rate": 2.6694050991501415e-05,
      "loss": 1.2562,
      "step": 490
    },
    {
      "epoch": 0.41347942939838744,
      "grad_norm": 1.6024563312530518,
      "learning_rate": 2.6609065155807367e-05,
      "loss": 1.2497,
      "step": 500
    },
    {
      "epoch": 0.42174901798635517,
      "grad_norm": 1.8860623836517334,
      "learning_rate": 2.6524079320113315e-05,
      "loss": 1.1691,
      "step": 510
    },
    {
      "epoch": 0.43001860657432295,
      "grad_norm": 2.097654342651367,
      "learning_rate": 2.6439093484419264e-05,
      "loss": 1.2596,
      "step": 520
    },
    {
      "epoch": 0.4382881951622907,
      "grad_norm": 1.7185099124908447,
      "learning_rate": 2.6354107648725212e-05,
      "loss": 1.2697,
      "step": 530
    },
    {
      "epoch": 0.44655778375025845,
      "grad_norm": 1.778671383857727,
      "learning_rate": 2.626912181303116e-05,
      "loss": 1.2515,
      "step": 540
    },
    {
      "epoch": 0.4548273723382262,
      "grad_norm": 1.9038722515106201,
      "learning_rate": 2.6184135977337112e-05,
      "loss": 1.2791,
      "step": 550
    },
    {
      "epoch": 0.4630969609261939,
      "grad_norm": 1.7633650302886963,
      "learning_rate": 2.6099150141643057e-05,
      "loss": 1.2253,
      "step": 560
    },
    {
      "epoch": 0.4713665495141617,
      "grad_norm": 1.7450698614120483,
      "learning_rate": 2.601416430594901e-05,
      "loss": 1.3355,
      "step": 570
    },
    {
      "epoch": 0.4796361381021294,
      "grad_norm": 1.862736701965332,
      "learning_rate": 2.5929178470254958e-05,
      "loss": 1.264,
      "step": 580
    },
    {
      "epoch": 0.4879057266900972,
      "grad_norm": 1.6469048261642456,
      "learning_rate": 2.584419263456091e-05,
      "loss": 1.1889,
      "step": 590
    },
    {
      "epoch": 0.4961753152780649,
      "grad_norm": 1.997885823249817,
      "learning_rate": 2.5759206798866854e-05,
      "loss": 1.1862,
      "step": 600
    },
    {
      "epoch": 0.5044449038660327,
      "grad_norm": 1.817481279373169,
      "learning_rate": 2.5674220963172806e-05,
      "loss": 1.2557,
      "step": 610
    },
    {
      "epoch": 0.5127144924540004,
      "grad_norm": 1.7298475503921509,
      "learning_rate": 2.5589235127478755e-05,
      "loss": 1.2665,
      "step": 620
    },
    {
      "epoch": 0.5209840810419681,
      "grad_norm": 1.4411191940307617,
      "learning_rate": 2.5504249291784706e-05,
      "loss": 1.1753,
      "step": 630
    },
    {
      "epoch": 0.5292536696299359,
      "grad_norm": 1.8844999074935913,
      "learning_rate": 2.541926345609065e-05,
      "loss": 1.2526,
      "step": 640
    },
    {
      "epoch": 0.5375232582179037,
      "grad_norm": 1.7845945358276367,
      "learning_rate": 2.53342776203966e-05,
      "loss": 1.191,
      "step": 650
    },
    {
      "epoch": 0.5457928468058714,
      "grad_norm": 1.3972216844558716,
      "learning_rate": 2.524929178470255e-05,
      "loss": 1.278,
      "step": 660
    },
    {
      "epoch": 0.5540624353938391,
      "grad_norm": 1.462584137916565,
      "learning_rate": 2.5164305949008497e-05,
      "loss": 1.2218,
      "step": 670
    },
    {
      "epoch": 0.5623320239818069,
      "grad_norm": 1.4898312091827393,
      "learning_rate": 2.507932011331445e-05,
      "loss": 1.286,
      "step": 680
    },
    {
      "epoch": 0.5706016125697747,
      "grad_norm": 1.7024825811386108,
      "learning_rate": 2.4994334277620397e-05,
      "loss": 1.2636,
      "step": 690
    },
    {
      "epoch": 0.5788712011577424,
      "grad_norm": 1.8356941938400269,
      "learning_rate": 2.490934844192635e-05,
      "loss": 1.2348,
      "step": 700
    },
    {
      "epoch": 0.5871407897457102,
      "grad_norm": 1.406341552734375,
      "learning_rate": 2.4824362606232294e-05,
      "loss": 1.2553,
      "step": 710
    },
    {
      "epoch": 0.5954103783336779,
      "grad_norm": 1.4195282459259033,
      "learning_rate": 2.4739376770538245e-05,
      "loss": 1.2291,
      "step": 720
    },
    {
      "epoch": 0.6036799669216456,
      "grad_norm": 1.5470885038375854,
      "learning_rate": 2.4654390934844194e-05,
      "loss": 1.1759,
      "step": 730
    },
    {
      "epoch": 0.6119495555096134,
      "grad_norm": 1.437906265258789,
      "learning_rate": 2.4569405099150142e-05,
      "loss": 1.2348,
      "step": 740
    },
    {
      "epoch": 0.6202191440975812,
      "grad_norm": 1.556028962135315,
      "learning_rate": 2.448441926345609e-05,
      "loss": 1.2765,
      "step": 750
    },
    {
      "epoch": 0.628488732685549,
      "grad_norm": 1.877280354499817,
      "learning_rate": 2.439943342776204e-05,
      "loss": 1.31,
      "step": 760
    },
    {
      "epoch": 0.6367583212735166,
      "grad_norm": 1.621369481086731,
      "learning_rate": 2.431444759206799e-05,
      "loss": 1.1872,
      "step": 770
    },
    {
      "epoch": 0.6450279098614844,
      "grad_norm": 1.9835141897201538,
      "learning_rate": 2.4229461756373936e-05,
      "loss": 1.1648,
      "step": 780
    },
    {
      "epoch": 0.6532974984494522,
      "grad_norm": 1.6437077522277832,
      "learning_rate": 2.4144475920679888e-05,
      "loss": 1.251,
      "step": 790
    },
    {
      "epoch": 0.6615670870374198,
      "grad_norm": 1.9001357555389404,
      "learning_rate": 2.4059490084985836e-05,
      "loss": 1.2541,
      "step": 800
    },
    {
      "epoch": 0.6698366756253876,
      "grad_norm": 1.9237158298492432,
      "learning_rate": 2.3974504249291788e-05,
      "loss": 1.2668,
      "step": 810
    },
    {
      "epoch": 0.6781062642133554,
      "grad_norm": 1.5256850719451904,
      "learning_rate": 2.3889518413597733e-05,
      "loss": 1.2121,
      "step": 820
    },
    {
      "epoch": 0.6863758528013232,
      "grad_norm": 1.7874656915664673,
      "learning_rate": 2.3804532577903685e-05,
      "loss": 1.2238,
      "step": 830
    },
    {
      "epoch": 0.6946454413892909,
      "grad_norm": 1.6926606893539429,
      "learning_rate": 2.3719546742209633e-05,
      "loss": 1.1833,
      "step": 840
    },
    {
      "epoch": 0.7029150299772586,
      "grad_norm": 1.5584828853607178,
      "learning_rate": 2.363456090651558e-05,
      "loss": 1.2504,
      "step": 850
    },
    {
      "epoch": 0.7111846185652264,
      "grad_norm": 1.4960335493087769,
      "learning_rate": 2.354957507082153e-05,
      "loss": 1.2366,
      "step": 860
    },
    {
      "epoch": 0.7194542071531941,
      "grad_norm": 1.515151858329773,
      "learning_rate": 2.3464589235127478e-05,
      "loss": 1.2633,
      "step": 870
    },
    {
      "epoch": 0.7277237957411619,
      "grad_norm": 1.5491009950637817,
      "learning_rate": 2.337960339943343e-05,
      "loss": 1.2564,
      "step": 880
    },
    {
      "epoch": 0.7359933843291296,
      "grad_norm": 1.749428153038025,
      "learning_rate": 2.3294617563739375e-05,
      "loss": 1.1558,
      "step": 890
    },
    {
      "epoch": 0.7442629729170974,
      "grad_norm": 1.9399073123931885,
      "learning_rate": 2.3209631728045327e-05,
      "loss": 1.2374,
      "step": 900
    },
    {
      "epoch": 0.7525325615050651,
      "grad_norm": 1.4301351308822632,
      "learning_rate": 2.3124645892351275e-05,
      "loss": 1.1954,
      "step": 910
    },
    {
      "epoch": 0.7608021500930329,
      "grad_norm": 1.9956438541412354,
      "learning_rate": 2.3039660056657227e-05,
      "loss": 1.2491,
      "step": 920
    },
    {
      "epoch": 0.7690717386810006,
      "grad_norm": 1.9098044633865356,
      "learning_rate": 2.2954674220963172e-05,
      "loss": 1.2084,
      "step": 930
    },
    {
      "epoch": 0.7773413272689683,
      "grad_norm": 4.143068790435791,
      "learning_rate": 2.286968838526912e-05,
      "loss": 1.2688,
      "step": 940
    },
    {
      "epoch": 0.7856109158569361,
      "grad_norm": 1.4811300039291382,
      "learning_rate": 2.2784702549575072e-05,
      "loss": 1.2443,
      "step": 950
    },
    {
      "epoch": 0.7938805044449039,
      "grad_norm": 1.675923466682434,
      "learning_rate": 2.269971671388102e-05,
      "loss": 1.3046,
      "step": 960
    },
    {
      "epoch": 0.8021500930328717,
      "grad_norm": 1.5884199142456055,
      "learning_rate": 2.261473087818697e-05,
      "loss": 1.2292,
      "step": 970
    },
    {
      "epoch": 0.8104196816208393,
      "grad_norm": 1.5051164627075195,
      "learning_rate": 2.2529745042492917e-05,
      "loss": 1.2586,
      "step": 980
    },
    {
      "epoch": 0.8186892702088071,
      "grad_norm": 1.782020092010498,
      "learning_rate": 2.244475920679887e-05,
      "loss": 1.2376,
      "step": 990
    },
    {
      "epoch": 0.8269588587967749,
      "grad_norm": 1.7476862668991089,
      "learning_rate": 2.2359773371104814e-05,
      "loss": 1.1613,
      "step": 1000
    },
    {
      "epoch": 0.8352284473847426,
      "grad_norm": 1.9436711072921753,
      "learning_rate": 2.2274787535410766e-05,
      "loss": 1.1755,
      "step": 1010
    },
    {
      "epoch": 0.8434980359727103,
      "grad_norm": 1.5141191482543945,
      "learning_rate": 2.2189801699716714e-05,
      "loss": 1.2314,
      "step": 1020
    },
    {
      "epoch": 0.8517676245606781,
      "grad_norm": 1.5229395627975464,
      "learning_rate": 2.2104815864022666e-05,
      "loss": 1.2687,
      "step": 1030
    },
    {
      "epoch": 0.8600372131486459,
      "grad_norm": 1.9138407707214355,
      "learning_rate": 2.201983002832861e-05,
      "loss": 1.1691,
      "step": 1040
    },
    {
      "epoch": 0.8683068017366136,
      "grad_norm": 2.8024072647094727,
      "learning_rate": 2.193484419263456e-05,
      "loss": 1.2247,
      "step": 1050
    },
    {
      "epoch": 0.8765763903245813,
      "grad_norm": 1.5703880786895752,
      "learning_rate": 2.184985835694051e-05,
      "loss": 1.1985,
      "step": 1060
    },
    {
      "epoch": 0.8848459789125491,
      "grad_norm": 1.7017922401428223,
      "learning_rate": 2.176487252124646e-05,
      "loss": 1.2381,
      "step": 1070
    },
    {
      "epoch": 0.8931155675005169,
      "grad_norm": 1.5464481115341187,
      "learning_rate": 2.1679886685552408e-05,
      "loss": 1.1625,
      "step": 1080
    },
    {
      "epoch": 0.9013851560884846,
      "grad_norm": 2.0196099281311035,
      "learning_rate": 2.1594900849858357e-05,
      "loss": 1.2105,
      "step": 1090
    },
    {
      "epoch": 0.9096547446764524,
      "grad_norm": 1.592670202255249,
      "learning_rate": 2.150991501416431e-05,
      "loss": 1.2346,
      "step": 1100
    },
    {
      "epoch": 0.9179243332644201,
      "grad_norm": 1.6970192193984985,
      "learning_rate": 2.1424929178470253e-05,
      "loss": 1.2559,
      "step": 1110
    },
    {
      "epoch": 0.9261939218523878,
      "grad_norm": 1.988303303718567,
      "learning_rate": 2.1339943342776205e-05,
      "loss": 1.2085,
      "step": 1120
    },
    {
      "epoch": 0.9344635104403556,
      "grad_norm": 1.683928370475769,
      "learning_rate": 2.1254957507082154e-05,
      "loss": 1.2739,
      "step": 1130
    },
    {
      "epoch": 0.9427330990283234,
      "grad_norm": 1.5975086688995361,
      "learning_rate": 2.1169971671388102e-05,
      "loss": 1.2228,
      "step": 1140
    },
    {
      "epoch": 0.9510026876162911,
      "grad_norm": 1.938913106918335,
      "learning_rate": 2.108498583569405e-05,
      "loss": 1.2304,
      "step": 1150
    },
    {
      "epoch": 0.9592722762042588,
      "grad_norm": 1.4729105234146118,
      "learning_rate": 2.1e-05,
      "loss": 1.2267,
      "step": 1160
    },
    {
      "epoch": 0.9675418647922266,
      "grad_norm": 1.5145294666290283,
      "learning_rate": 2.091501416430595e-05,
      "loss": 1.2349,
      "step": 1170
    },
    {
      "epoch": 0.9758114533801944,
      "grad_norm": 1.4880317449569702,
      "learning_rate": 2.08300283286119e-05,
      "loss": 1.1597,
      "step": 1180
    },
    {
      "epoch": 0.984081041968162,
      "grad_norm": 1.7671252489089966,
      "learning_rate": 2.0745042492917847e-05,
      "loss": 1.1892,
      "step": 1190
    },
    {
      "epoch": 0.9923506305561298,
      "grad_norm": 1.8098033666610718,
      "learning_rate": 2.0660056657223796e-05,
      "loss": 1.2681,
      "step": 1200
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.6395375728607178,
      "learning_rate": 2.0575070821529748e-05,
      "loss": 1.2419,
      "step": 1210
    },
    {
      "epoch": 1.0082695885879678,
      "grad_norm": 1.933415174484253,
      "learning_rate": 2.0490084985835693e-05,
      "loss": 1.2109,
      "step": 1220
    },
    {
      "epoch": 1.0165391771759356,
      "grad_norm": 2.1812565326690674,
      "learning_rate": 2.0405099150141644e-05,
      "loss": 1.2607,
      "step": 1230
    },
    {
      "epoch": 1.0248087657639033,
      "grad_norm": 2.1511988639831543,
      "learning_rate": 2.0320113314447593e-05,
      "loss": 1.1897,
      "step": 1240
    },
    {
      "epoch": 1.033078354351871,
      "grad_norm": 1.7363823652267456,
      "learning_rate": 2.023512747875354e-05,
      "loss": 1.2091,
      "step": 1250
    },
    {
      "epoch": 1.0413479429398387,
      "grad_norm": 1.6482384204864502,
      "learning_rate": 2.015014164305949e-05,
      "loss": 1.2388,
      "step": 1260
    },
    {
      "epoch": 1.0496175315278065,
      "grad_norm": 2.138465642929077,
      "learning_rate": 2.0065155807365438e-05,
      "loss": 1.2254,
      "step": 1270
    },
    {
      "epoch": 1.0578871201157742,
      "grad_norm": 1.5426571369171143,
      "learning_rate": 1.998016997167139e-05,
      "loss": 1.2301,
      "step": 1280
    },
    {
      "epoch": 1.066156708703742,
      "grad_norm": 1.647979497909546,
      "learning_rate": 1.9895184135977338e-05,
      "loss": 1.2082,
      "step": 1290
    },
    {
      "epoch": 1.0744262972917098,
      "grad_norm": 1.9111578464508057,
      "learning_rate": 1.9810198300283287e-05,
      "loss": 1.2873,
      "step": 1300
    },
    {
      "epoch": 1.0826958858796776,
      "grad_norm": 2.0739035606384277,
      "learning_rate": 1.9725212464589235e-05,
      "loss": 1.225,
      "step": 1310
    },
    {
      "epoch": 1.0909654744676451,
      "grad_norm": 1.768000841140747,
      "learning_rate": 1.9640226628895187e-05,
      "loss": 1.2159,
      "step": 1320
    },
    {
      "epoch": 1.099235063055613,
      "grad_norm": 1.559824824333191,
      "learning_rate": 1.9555240793201132e-05,
      "loss": 1.1834,
      "step": 1330
    },
    {
      "epoch": 1.1075046516435807,
      "grad_norm": 1.8283014297485352,
      "learning_rate": 1.9470254957507084e-05,
      "loss": 1.2496,
      "step": 1340
    },
    {
      "epoch": 1.1157742402315485,
      "grad_norm": 1.9733119010925293,
      "learning_rate": 1.9385269121813032e-05,
      "loss": 1.2726,
      "step": 1350
    },
    {
      "epoch": 1.1240438288195163,
      "grad_norm": 1.6147011518478394,
      "learning_rate": 1.930028328611898e-05,
      "loss": 1.2387,
      "step": 1360
    },
    {
      "epoch": 1.132313417407484,
      "grad_norm": 1.6239899396896362,
      "learning_rate": 1.921529745042493e-05,
      "loss": 1.238,
      "step": 1370
    },
    {
      "epoch": 1.1405830059954518,
      "grad_norm": 2.299485921859741,
      "learning_rate": 1.9130311614730877e-05,
      "loss": 1.1937,
      "step": 1380
    },
    {
      "epoch": 1.1488525945834196,
      "grad_norm": 1.7911968231201172,
      "learning_rate": 1.904532577903683e-05,
      "loss": 1.2808,
      "step": 1390
    },
    {
      "epoch": 1.1571221831713872,
      "grad_norm": 1.7181757688522339,
      "learning_rate": 1.8960339943342777e-05,
      "loss": 1.2548,
      "step": 1400
    },
    {
      "epoch": 1.165391771759355,
      "grad_norm": 1.4222149848937988,
      "learning_rate": 1.8875354107648726e-05,
      "loss": 1.2389,
      "step": 1410
    },
    {
      "epoch": 1.1736613603473227,
      "grad_norm": 1.8359787464141846,
      "learning_rate": 1.8790368271954674e-05,
      "loss": 1.2713,
      "step": 1420
    },
    {
      "epoch": 1.1819309489352905,
      "grad_norm": 2.156942844390869,
      "learning_rate": 1.8705382436260626e-05,
      "loss": 1.1943,
      "step": 1430
    },
    {
      "epoch": 1.1902005375232583,
      "grad_norm": 1.6063833236694336,
      "learning_rate": 1.862039660056657e-05,
      "loss": 1.1513,
      "step": 1440
    },
    {
      "epoch": 1.198470126111226,
      "grad_norm": 1.8199366331100464,
      "learning_rate": 1.853541076487252e-05,
      "loss": 1.1897,
      "step": 1450
    },
    {
      "epoch": 1.2067397146991938,
      "grad_norm": 2.1016931533813477,
      "learning_rate": 1.845042492917847e-05,
      "loss": 1.2676,
      "step": 1460
    },
    {
      "epoch": 1.2150093032871614,
      "grad_norm": 1.8648042678833008,
      "learning_rate": 1.836543909348442e-05,
      "loss": 1.2432,
      "step": 1470
    },
    {
      "epoch": 1.2232788918751292,
      "grad_norm": 1.8039196729660034,
      "learning_rate": 1.8280453257790368e-05,
      "loss": 1.2373,
      "step": 1480
    },
    {
      "epoch": 1.231548480463097,
      "grad_norm": 1.919260025024414,
      "learning_rate": 1.8195467422096316e-05,
      "loss": 1.2318,
      "step": 1490
    },
    {
      "epoch": 1.2398180690510647,
      "grad_norm": 1.766959547996521,
      "learning_rate": 1.8110481586402268e-05,
      "loss": 1.2211,
      "step": 1500
    },
    {
      "epoch": 1.2480876576390325,
      "grad_norm": 1.8121453523635864,
      "learning_rate": 1.8025495750708217e-05,
      "loss": 1.1636,
      "step": 1510
    },
    {
      "epoch": 1.256357246227,
      "grad_norm": 1.6076911687850952,
      "learning_rate": 1.7940509915014165e-05,
      "loss": 1.1701,
      "step": 1520
    },
    {
      "epoch": 1.264626834814968,
      "grad_norm": 1.5267524719238281,
      "learning_rate": 1.7855524079320113e-05,
      "loss": 1.2588,
      "step": 1530
    },
    {
      "epoch": 1.2728964234029356,
      "grad_norm": 1.5404313802719116,
      "learning_rate": 1.7770538243626065e-05,
      "loss": 1.2564,
      "step": 1540
    },
    {
      "epoch": 1.2811660119909034,
      "grad_norm": 1.53412926197052,
      "learning_rate": 1.768555240793201e-05,
      "loss": 1.1709,
      "step": 1550
    },
    {
      "epoch": 1.2894356005788712,
      "grad_norm": 1.6339588165283203,
      "learning_rate": 1.760056657223796e-05,
      "loss": 1.2209,
      "step": 1560
    },
    {
      "epoch": 1.297705189166839,
      "grad_norm": 2.0418529510498047,
      "learning_rate": 1.751558073654391e-05,
      "loss": 1.2225,
      "step": 1570
    },
    {
      "epoch": 1.3059747777548067,
      "grad_norm": 1.8880177736282349,
      "learning_rate": 1.743059490084986e-05,
      "loss": 1.2776,
      "step": 1580
    },
    {
      "epoch": 1.3142443663427745,
      "grad_norm": 1.6993001699447632,
      "learning_rate": 1.7345609065155807e-05,
      "loss": 1.2001,
      "step": 1590
    },
    {
      "epoch": 1.3225139549307423,
      "grad_norm": 1.3940047025680542,
      "learning_rate": 1.7260623229461756e-05,
      "loss": 1.2719,
      "step": 1600
    },
    {
      "epoch": 1.3307835435187099,
      "grad_norm": 2.1579604148864746,
      "learning_rate": 1.7175637393767707e-05,
      "loss": 1.2346,
      "step": 1610
    },
    {
      "epoch": 1.3390531321066776,
      "grad_norm": 1.737793207168579,
      "learning_rate": 1.7090651558073656e-05,
      "loss": 1.1324,
      "step": 1620
    },
    {
      "epoch": 1.3473227206946454,
      "grad_norm": 2.009244918823242,
      "learning_rate": 1.7005665722379604e-05,
      "loss": 1.2088,
      "step": 1630
    },
    {
      "epoch": 1.3555923092826132,
      "grad_norm": 1.711058497428894,
      "learning_rate": 1.6920679886685553e-05,
      "loss": 1.1762,
      "step": 1640
    },
    {
      "epoch": 1.363861897870581,
      "grad_norm": 1.6632784605026245,
      "learning_rate": 1.68356940509915e-05,
      "loss": 1.2652,
      "step": 1650
    },
    {
      "epoch": 1.3721314864585488,
      "grad_norm": 1.3322538137435913,
      "learning_rate": 1.6750708215297453e-05,
      "loss": 1.2115,
      "step": 1660
    },
    {
      "epoch": 1.3804010750465165,
      "grad_norm": 1.7984195947647095,
      "learning_rate": 1.6665722379603398e-05,
      "loss": 1.2322,
      "step": 1670
    },
    {
      "epoch": 1.388670663634484,
      "grad_norm": 1.653132677078247,
      "learning_rate": 1.658073654390935e-05,
      "loss": 1.2057,
      "step": 1680
    },
    {
      "epoch": 1.3969402522224519,
      "grad_norm": 1.752362608909607,
      "learning_rate": 1.6495750708215298e-05,
      "loss": 1.2297,
      "step": 1690
    },
    {
      "epoch": 1.4052098408104197,
      "grad_norm": 1.6714107990264893,
      "learning_rate": 1.6410764872521246e-05,
      "loss": 1.2176,
      "step": 1700
    },
    {
      "epoch": 1.4134794293983874,
      "grad_norm": 1.8007019758224487,
      "learning_rate": 1.6325779036827195e-05,
      "loss": 1.1668,
      "step": 1710
    },
    {
      "epoch": 1.4217490179863552,
      "grad_norm": 1.7239274978637695,
      "learning_rate": 1.6240793201133147e-05,
      "loss": 1.3031,
      "step": 1720
    },
    {
      "epoch": 1.430018606574323,
      "grad_norm": 1.562807559967041,
      "learning_rate": 1.6155807365439095e-05,
      "loss": 1.2127,
      "step": 1730
    },
    {
      "epoch": 1.4382881951622908,
      "grad_norm": 1.639582872390747,
      "learning_rate": 1.6070821529745043e-05,
      "loss": 1.1703,
      "step": 1740
    },
    {
      "epoch": 1.4465577837502583,
      "grad_norm": 2.131653070449829,
      "learning_rate": 1.5985835694050992e-05,
      "loss": 1.1803,
      "step": 1750
    },
    {
      "epoch": 1.4548273723382261,
      "grad_norm": 1.7146421670913696,
      "learning_rate": 1.590084985835694e-05,
      "loss": 1.2224,
      "step": 1760
    },
    {
      "epoch": 1.463096960926194,
      "grad_norm": 1.8027311563491821,
      "learning_rate": 1.5815864022662892e-05,
      "loss": 1.1629,
      "step": 1770
    },
    {
      "epoch": 1.4713665495141617,
      "grad_norm": 1.9190764427185059,
      "learning_rate": 1.5730878186968837e-05,
      "loss": 1.2159,
      "step": 1780
    },
    {
      "epoch": 1.4796361381021295,
      "grad_norm": 1.9047948122024536,
      "learning_rate": 1.564589235127479e-05,
      "loss": 1.1819,
      "step": 1790
    },
    {
      "epoch": 1.4879057266900972,
      "grad_norm": 1.7790945768356323,
      "learning_rate": 1.5560906515580737e-05,
      "loss": 1.2071,
      "step": 1800
    },
    {
      "epoch": 1.496175315278065,
      "grad_norm": 2.8908729553222656,
      "learning_rate": 1.5475920679886686e-05,
      "loss": 1.2331,
      "step": 1810
    },
    {
      "epoch": 1.5044449038660326,
      "grad_norm": 1.490013599395752,
      "learning_rate": 1.5390934844192634e-05,
      "loss": 1.2128,
      "step": 1820
    },
    {
      "epoch": 1.5127144924540004,
      "grad_norm": 1.7229286432266235,
      "learning_rate": 1.5305949008498586e-05,
      "loss": 1.192,
      "step": 1830
    },
    {
      "epoch": 1.5209840810419681,
      "grad_norm": 1.5648951530456543,
      "learning_rate": 1.5220963172804533e-05,
      "loss": 1.206,
      "step": 1840
    },
    {
      "epoch": 1.529253669629936,
      "grad_norm": 1.9439661502838135,
      "learning_rate": 1.5135977337110481e-05,
      "loss": 1.2563,
      "step": 1850
    },
    {
      "epoch": 1.5375232582179037,
      "grad_norm": 1.656138300895691,
      "learning_rate": 1.5050991501416431e-05,
      "loss": 1.2105,
      "step": 1860
    },
    {
      "epoch": 1.5457928468058713,
      "grad_norm": 1.91109299659729,
      "learning_rate": 1.4966005665722381e-05,
      "loss": 1.2433,
      "step": 1870
    },
    {
      "epoch": 1.5540624353938393,
      "grad_norm": 2.0734164714813232,
      "learning_rate": 1.488101983002833e-05,
      "loss": 1.1336,
      "step": 1880
    },
    {
      "epoch": 1.5623320239818068,
      "grad_norm": 1.7146575450897217,
      "learning_rate": 1.4796033994334278e-05,
      "loss": 1.2499,
      "step": 1890
    },
    {
      "epoch": 1.5706016125697748,
      "grad_norm": 1.3921111822128296,
      "learning_rate": 1.4711048158640226e-05,
      "loss": 1.1622,
      "step": 1900
    },
    {
      "epoch": 1.5788712011577424,
      "grad_norm": 1.869240641593933,
      "learning_rate": 1.4626062322946175e-05,
      "loss": 1.2463,
      "step": 1910
    },
    {
      "epoch": 1.5871407897457102,
      "grad_norm": 1.5721851587295532,
      "learning_rate": 1.4541076487252125e-05,
      "loss": 1.1756,
      "step": 1920
    },
    {
      "epoch": 1.595410378333678,
      "grad_norm": 1.6331114768981934,
      "learning_rate": 1.4456090651558073e-05,
      "loss": 1.2571,
      "step": 1930
    },
    {
      "epoch": 1.6036799669216455,
      "grad_norm": 1.8625869750976562,
      "learning_rate": 1.4371104815864023e-05,
      "loss": 1.1904,
      "step": 1940
    },
    {
      "epoch": 1.6119495555096135,
      "grad_norm": 1.6535367965698242,
      "learning_rate": 1.4286118980169972e-05,
      "loss": 1.2526,
      "step": 1950
    },
    {
      "epoch": 1.620219144097581,
      "grad_norm": 2.2070257663726807,
      "learning_rate": 1.4201133144475922e-05,
      "loss": 1.2521,
      "step": 1960
    },
    {
      "epoch": 1.628488732685549,
      "grad_norm": 1.603054165840149,
      "learning_rate": 1.411614730878187e-05,
      "loss": 1.2246,
      "step": 1970
    },
    {
      "epoch": 1.6367583212735166,
      "grad_norm": 1.9885493516921997,
      "learning_rate": 1.403116147308782e-05,
      "loss": 1.1861,
      "step": 1980
    },
    {
      "epoch": 1.6450279098614844,
      "grad_norm": 1.8240267038345337,
      "learning_rate": 1.3946175637393769e-05,
      "loss": 1.1904,
      "step": 1990
    },
    {
      "epoch": 1.6532974984494522,
      "grad_norm": 2.0635201930999756,
      "learning_rate": 1.3861189801699717e-05,
      "loss": 1.1904,
      "step": 2000
    },
    {
      "epoch": 1.6615670870374197,
      "grad_norm": 1.6415845155715942,
      "learning_rate": 1.3776203966005665e-05,
      "loss": 1.2934,
      "step": 2010
    },
    {
      "epoch": 1.6698366756253877,
      "grad_norm": 1.6729450225830078,
      "learning_rate": 1.3691218130311614e-05,
      "loss": 1.1827,
      "step": 2020
    },
    {
      "epoch": 1.6781062642133553,
      "grad_norm": 1.717220425605774,
      "learning_rate": 1.3606232294617564e-05,
      "loss": 1.1654,
      "step": 2030
    },
    {
      "epoch": 1.6863758528013233,
      "grad_norm": 1.8770403861999512,
      "learning_rate": 1.3521246458923512e-05,
      "loss": 1.1682,
      "step": 2040
    },
    {
      "epoch": 1.6946454413892909,
      "grad_norm": 1.7336089611053467,
      "learning_rate": 1.3436260623229462e-05,
      "loss": 1.2076,
      "step": 2050
    },
    {
      "epoch": 1.7029150299772586,
      "grad_norm": 1.8807499408721924,
      "learning_rate": 1.3351274787535411e-05,
      "loss": 1.203,
      "step": 2060
    },
    {
      "epoch": 1.7111846185652264,
      "grad_norm": 1.5915155410766602,
      "learning_rate": 1.3266288951841361e-05,
      "loss": 1.2286,
      "step": 2070
    },
    {
      "epoch": 1.719454207153194,
      "grad_norm": 1.4237486124038696,
      "learning_rate": 1.318130311614731e-05,
      "loss": 1.1721,
      "step": 2080
    },
    {
      "epoch": 1.727723795741162,
      "grad_norm": 1.638949990272522,
      "learning_rate": 1.309631728045326e-05,
      "loss": 1.2555,
      "step": 2090
    },
    {
      "epoch": 1.7359933843291295,
      "grad_norm": 1.865402102470398,
      "learning_rate": 1.3011331444759206e-05,
      "loss": 1.2384,
      "step": 2100
    },
    {
      "epoch": 1.7442629729170975,
      "grad_norm": 1.5525370836257935,
      "learning_rate": 1.2926345609065156e-05,
      "loss": 1.2469,
      "step": 2110
    },
    {
      "epoch": 1.752532561505065,
      "grad_norm": 1.814950704574585,
      "learning_rate": 1.2841359773371105e-05,
      "loss": 1.1905,
      "step": 2120
    },
    {
      "epoch": 1.7608021500930329,
      "grad_norm": 1.8632404804229736,
      "learning_rate": 1.2756373937677053e-05,
      "loss": 1.2069,
      "step": 2130
    },
    {
      "epoch": 1.7690717386810006,
      "grad_norm": 1.790205955505371,
      "learning_rate": 1.2671388101983003e-05,
      "loss": 1.2407,
      "step": 2140
    },
    {
      "epoch": 1.7773413272689682,
      "grad_norm": 1.8130823373794556,
      "learning_rate": 1.2586402266288952e-05,
      "loss": 1.1813,
      "step": 2150
    },
    {
      "epoch": 1.7856109158569362,
      "grad_norm": 2.062981128692627,
      "learning_rate": 1.2501416430594902e-05,
      "loss": 1.197,
      "step": 2160
    },
    {
      "epoch": 1.7938805044449038,
      "grad_norm": 2.242422342300415,
      "learning_rate": 1.241643059490085e-05,
      "loss": 1.1903,
      "step": 2170
    },
    {
      "epoch": 1.8021500930328718,
      "grad_norm": 1.8906910419464111,
      "learning_rate": 1.23314447592068e-05,
      "loss": 1.206,
      "step": 2180
    },
    {
      "epoch": 1.8104196816208393,
      "grad_norm": 1.5400751829147339,
      "learning_rate": 1.2246458923512749e-05,
      "loss": 1.2571,
      "step": 2190
    },
    {
      "epoch": 1.818689270208807,
      "grad_norm": 1.7088428735733032,
      "learning_rate": 1.2161473087818697e-05,
      "loss": 1.2824,
      "step": 2200
    },
    {
      "epoch": 1.8269588587967749,
      "grad_norm": 1.9669874906539917,
      "learning_rate": 1.2076487252124645e-05,
      "loss": 1.2127,
      "step": 2210
    },
    {
      "epoch": 1.8352284473847424,
      "grad_norm": 2.3330063819885254,
      "learning_rate": 1.1991501416430595e-05,
      "loss": 1.1439,
      "step": 2220
    },
    {
      "epoch": 1.8434980359727104,
      "grad_norm": 1.4940038919448853,
      "learning_rate": 1.1906515580736544e-05,
      "loss": 1.2021,
      "step": 2230
    },
    {
      "epoch": 1.851767624560678,
      "grad_norm": 1.5432305335998535,
      "learning_rate": 1.1821529745042492e-05,
      "loss": 1.223,
      "step": 2240
    },
    {
      "epoch": 1.860037213148646,
      "grad_norm": 1.7886534929275513,
      "learning_rate": 1.1736543909348442e-05,
      "loss": 1.1688,
      "step": 2250
    },
    {
      "epoch": 1.8683068017366136,
      "grad_norm": 1.6868641376495361,
      "learning_rate": 1.165155807365439e-05,
      "loss": 1.1638,
      "step": 2260
    },
    {
      "epoch": 1.8765763903245813,
      "grad_norm": 1.911975622177124,
      "learning_rate": 1.1566572237960341e-05,
      "loss": 1.2201,
      "step": 2270
    },
    {
      "epoch": 1.8848459789125491,
      "grad_norm": 1.7790031433105469,
      "learning_rate": 1.148158640226629e-05,
      "loss": 1.2081,
      "step": 2280
    },
    {
      "epoch": 1.893115567500517,
      "grad_norm": 1.7157018184661865,
      "learning_rate": 1.139660056657224e-05,
      "loss": 1.128,
      "step": 2290
    },
    {
      "epoch": 1.9013851560884847,
      "grad_norm": 2.0118377208709717,
      "learning_rate": 1.1311614730878186e-05,
      "loss": 1.2134,
      "step": 2300
    },
    {
      "epoch": 1.9096547446764522,
      "grad_norm": 1.653088092803955,
      "learning_rate": 1.1226628895184136e-05,
      "loss": 1.1215,
      "step": 2310
    },
    {
      "epoch": 1.9179243332644202,
      "grad_norm": 1.9389797449111938,
      "learning_rate": 1.1141643059490085e-05,
      "loss": 1.2296,
      "step": 2320
    },
    {
      "epoch": 1.9261939218523878,
      "grad_norm": 1.5426229238510132,
      "learning_rate": 1.1056657223796035e-05,
      "loss": 1.1602,
      "step": 2330
    },
    {
      "epoch": 1.9344635104403556,
      "grad_norm": 1.9051172733306885,
      "learning_rate": 1.0971671388101983e-05,
      "loss": 1.1786,
      "step": 2340
    },
    {
      "epoch": 1.9427330990283234,
      "grad_norm": 1.8626322746276855,
      "learning_rate": 1.0886685552407931e-05,
      "loss": 1.2179,
      "step": 2350
    },
    {
      "epoch": 1.9510026876162911,
      "grad_norm": 2.046229124069214,
      "learning_rate": 1.0801699716713882e-05,
      "loss": 1.2564,
      "step": 2360
    },
    {
      "epoch": 1.959272276204259,
      "grad_norm": 1.7243698835372925,
      "learning_rate": 1.071671388101983e-05,
      "loss": 1.2311,
      "step": 2370
    },
    {
      "epoch": 1.9675418647922265,
      "grad_norm": 2.0456719398498535,
      "learning_rate": 1.063172804532578e-05,
      "loss": 1.2366,
      "step": 2380
    },
    {
      "epoch": 1.9758114533801945,
      "grad_norm": 2.1611168384552,
      "learning_rate": 1.0546742209631728e-05,
      "loss": 1.2191,
      "step": 2390
    },
    {
      "epoch": 1.984081041968162,
      "grad_norm": 1.7754509449005127,
      "learning_rate": 1.0461756373937679e-05,
      "loss": 1.1766,
      "step": 2400
    },
    {
      "epoch": 1.9923506305561298,
      "grad_norm": 1.4466800689697266,
      "learning_rate": 1.0376770538243625e-05,
      "loss": 1.227,
      "step": 2410
    },
    {
      "epoch": 2.0,
      "grad_norm": 4.575964450836182,
      "learning_rate": 1.0291784702549575e-05,
      "loss": 1.2338,
      "step": 2420
    },
    {
      "epoch": 2.0082695885879676,
      "grad_norm": 1.9763872623443604,
      "learning_rate": 1.0206798866855524e-05,
      "loss": 1.2203,
      "step": 2430
    },
    {
      "epoch": 2.0165391771759356,
      "grad_norm": 2.202791452407837,
      "learning_rate": 1.0121813031161474e-05,
      "loss": 1.2155,
      "step": 2440
    },
    {
      "epoch": 2.024808765763903,
      "grad_norm": 2.2168502807617188,
      "learning_rate": 1.0036827195467422e-05,
      "loss": 1.2115,
      "step": 2450
    },
    {
      "epoch": 2.033078354351871,
      "grad_norm": 1.8227676153182983,
      "learning_rate": 9.95184135977337e-06,
      "loss": 1.2401,
      "step": 2460
    },
    {
      "epoch": 2.0413479429398387,
      "grad_norm": 2.546567678451538,
      "learning_rate": 9.86685552407932e-06,
      "loss": 1.2387,
      "step": 2470
    },
    {
      "epoch": 2.0496175315278067,
      "grad_norm": 2.582693576812744,
      "learning_rate": 9.78186968838527e-06,
      "loss": 1.2533,
      "step": 2480
    },
    {
      "epoch": 2.0578871201157742,
      "grad_norm": 1.8363505601882935,
      "learning_rate": 9.69688385269122e-06,
      "loss": 1.1447,
      "step": 2490
    },
    {
      "epoch": 2.066156708703742,
      "grad_norm": 2.013913154602051,
      "learning_rate": 9.611898016997168e-06,
      "loss": 1.1471,
      "step": 2500
    },
    {
      "epoch": 2.07442629729171,
      "grad_norm": 2.118820905685425,
      "learning_rate": 9.526912181303116e-06,
      "loss": 1.1904,
      "step": 2510
    },
    {
      "epoch": 2.0826958858796774,
      "grad_norm": 2.1672794818878174,
      "learning_rate": 9.441926345609064e-06,
      "loss": 1.2237,
      "step": 2520
    },
    {
      "epoch": 2.0909654744676454,
      "grad_norm": 1.9740591049194336,
      "learning_rate": 9.356940509915015e-06,
      "loss": 1.1806,
      "step": 2530
    },
    {
      "epoch": 2.099235063055613,
      "grad_norm": 1.6090584993362427,
      "learning_rate": 9.271954674220963e-06,
      "loss": 1.1796,
      "step": 2540
    },
    {
      "epoch": 2.107504651643581,
      "grad_norm": 1.94342839717865,
      "learning_rate": 9.186968838526913e-06,
      "loss": 1.1676,
      "step": 2550
    },
    {
      "epoch": 2.1157742402315485,
      "grad_norm": 1.7232615947723389,
      "learning_rate": 9.101983002832861e-06,
      "loss": 1.2301,
      "step": 2560
    },
    {
      "epoch": 2.124043828819516,
      "grad_norm": 2.046475410461426,
      "learning_rate": 9.016997167138812e-06,
      "loss": 1.2724,
      "step": 2570
    },
    {
      "epoch": 2.132313417407484,
      "grad_norm": 1.721255898475647,
      "learning_rate": 8.93201133144476e-06,
      "loss": 1.2858,
      "step": 2580
    },
    {
      "epoch": 2.1405830059954516,
      "grad_norm": 2.374070882797241,
      "learning_rate": 8.847025495750708e-06,
      "loss": 1.1568,
      "step": 2590
    },
    {
      "epoch": 2.1488525945834196,
      "grad_norm": 1.8042951822280884,
      "learning_rate": 8.762039660056658e-06,
      "loss": 1.2585,
      "step": 2600
    },
    {
      "epoch": 2.157122183171387,
      "grad_norm": 1.9439647197723389,
      "learning_rate": 8.677053824362605e-06,
      "loss": 1.1863,
      "step": 2610
    },
    {
      "epoch": 2.165391771759355,
      "grad_norm": 1.7905464172363281,
      "learning_rate": 8.592067988668555e-06,
      "loss": 1.2333,
      "step": 2620
    },
    {
      "epoch": 2.1736613603473227,
      "grad_norm": 1.7896891832351685,
      "learning_rate": 8.507082152974504e-06,
      "loss": 1.1716,
      "step": 2630
    },
    {
      "epoch": 2.1819309489352903,
      "grad_norm": 1.7220370769500732,
      "learning_rate": 8.422096317280454e-06,
      "loss": 1.2155,
      "step": 2640
    },
    {
      "epoch": 2.1902005375232583,
      "grad_norm": 1.744160532951355,
      "learning_rate": 8.337110481586402e-06,
      "loss": 1.1589,
      "step": 2650
    },
    {
      "epoch": 2.198470126111226,
      "grad_norm": 2.4898080825805664,
      "learning_rate": 8.252124645892352e-06,
      "loss": 1.2051,
      "step": 2660
    },
    {
      "epoch": 2.206739714699194,
      "grad_norm": 1.7670865058898926,
      "learning_rate": 8.1671388101983e-06,
      "loss": 1.1845,
      "step": 2670
    },
    {
      "epoch": 2.2150093032871614,
      "grad_norm": 1.9672374725341797,
      "learning_rate": 8.08215297450425e-06,
      "loss": 1.1746,
      "step": 2680
    },
    {
      "epoch": 2.2232788918751294,
      "grad_norm": 1.6658029556274414,
      "learning_rate": 7.9971671388102e-06,
      "loss": 1.2437,
      "step": 2690
    },
    {
      "epoch": 2.231548480463097,
      "grad_norm": 2.06300687789917,
      "learning_rate": 7.912181303116148e-06,
      "loss": 1.1653,
      "step": 2700
    },
    {
      "epoch": 2.2398180690510645,
      "grad_norm": 2.5978941917419434,
      "learning_rate": 7.827195467422096e-06,
      "loss": 1.2455,
      "step": 2710
    },
    {
      "epoch": 2.2480876576390325,
      "grad_norm": 2.8660449981689453,
      "learning_rate": 7.742209631728044e-06,
      "loss": 1.2182,
      "step": 2720
    },
    {
      "epoch": 2.256357246227,
      "grad_norm": 1.7581579685211182,
      "learning_rate": 7.657223796033994e-06,
      "loss": 1.2036,
      "step": 2730
    },
    {
      "epoch": 2.264626834814968,
      "grad_norm": 2.114532470703125,
      "learning_rate": 7.572237960339944e-06,
      "loss": 1.162,
      "step": 2740
    },
    {
      "epoch": 2.2728964234029356,
      "grad_norm": 1.8526941537857056,
      "learning_rate": 7.487252124645892e-06,
      "loss": 1.169,
      "step": 2750
    },
    {
      "epoch": 2.2811660119909036,
      "grad_norm": 1.7581309080123901,
      "learning_rate": 7.402266288951841e-06,
      "loss": 1.2223,
      "step": 2760
    },
    {
      "epoch": 2.289435600578871,
      "grad_norm": 1.6723483800888062,
      "learning_rate": 7.317280453257791e-06,
      "loss": 1.2,
      "step": 2770
    },
    {
      "epoch": 2.297705189166839,
      "grad_norm": 2.085315704345703,
      "learning_rate": 7.232294617563739e-06,
      "loss": 1.2255,
      "step": 2780
    },
    {
      "epoch": 2.3059747777548067,
      "grad_norm": 1.6619259119033813,
      "learning_rate": 7.147308781869688e-06,
      "loss": 1.1532,
      "step": 2790
    },
    {
      "epoch": 2.3142443663427743,
      "grad_norm": 2.0333821773529053,
      "learning_rate": 7.0623229461756375e-06,
      "loss": 1.1183,
      "step": 2800
    },
    {
      "epoch": 2.3225139549307423,
      "grad_norm": 1.5272356271743774,
      "learning_rate": 6.977337110481587e-06,
      "loss": 1.2095,
      "step": 2810
    },
    {
      "epoch": 2.33078354351871,
      "grad_norm": 1.959203839302063,
      "learning_rate": 6.892351274787536e-06,
      "loss": 1.1316,
      "step": 2820
    },
    {
      "epoch": 2.339053132106678,
      "grad_norm": 2.3357982635498047,
      "learning_rate": 6.807365439093484e-06,
      "loss": 1.2482,
      "step": 2830
    },
    {
      "epoch": 2.3473227206946454,
      "grad_norm": 1.9557772874832153,
      "learning_rate": 6.722379603399434e-06,
      "loss": 1.236,
      "step": 2840
    },
    {
      "epoch": 2.3555923092826134,
      "grad_norm": 2.6579489707946777,
      "learning_rate": 6.637393767705383e-06,
      "loss": 1.1817,
      "step": 2850
    },
    {
      "epoch": 2.363861897870581,
      "grad_norm": 1.6537288427352905,
      "learning_rate": 6.552407932011331e-06,
      "loss": 1.1664,
      "step": 2860
    },
    {
      "epoch": 2.3721314864585485,
      "grad_norm": 1.9053529500961304,
      "learning_rate": 6.4674220963172806e-06,
      "loss": 1.1774,
      "step": 2870
    },
    {
      "epoch": 2.3804010750465165,
      "grad_norm": 2.0489463806152344,
      "learning_rate": 6.382436260623229e-06,
      "loss": 1.2281,
      "step": 2880
    },
    {
      "epoch": 2.388670663634484,
      "grad_norm": 1.9112259149551392,
      "learning_rate": 6.297450424929178e-06,
      "loss": 1.1888,
      "step": 2890
    },
    {
      "epoch": 2.396940252222452,
      "grad_norm": 1.9015276432037354,
      "learning_rate": 6.2124645892351275e-06,
      "loss": 1.1625,
      "step": 2900
    },
    {
      "epoch": 2.4052098408104197,
      "grad_norm": 1.8918744325637817,
      "learning_rate": 6.127478753541077e-06,
      "loss": 1.2092,
      "step": 2910
    },
    {
      "epoch": 2.4134794293983877,
      "grad_norm": 1.703669786453247,
      "learning_rate": 6.042492917847026e-06,
      "loss": 1.1708,
      "step": 2920
    },
    {
      "epoch": 2.4217490179863552,
      "grad_norm": 1.9872866868972778,
      "learning_rate": 5.957507082152975e-06,
      "loss": 1.218,
      "step": 2930
    },
    {
      "epoch": 2.430018606574323,
      "grad_norm": 2.149488925933838,
      "learning_rate": 5.872521246458924e-06,
      "loss": 1.1851,
      "step": 2940
    },
    {
      "epoch": 2.438288195162291,
      "grad_norm": 1.817059874534607,
      "learning_rate": 5.787535410764873e-06,
      "loss": 1.1965,
      "step": 2950
    },
    {
      "epoch": 2.4465577837502583,
      "grad_norm": 2.095409870147705,
      "learning_rate": 5.702549575070822e-06,
      "loss": 1.2255,
      "step": 2960
    },
    {
      "epoch": 2.4548273723382263,
      "grad_norm": 1.9309271574020386,
      "learning_rate": 5.6175637393767705e-06,
      "loss": 1.2458,
      "step": 2970
    },
    {
      "epoch": 2.463096960926194,
      "grad_norm": 1.8910001516342163,
      "learning_rate": 5.53257790368272e-06,
      "loss": 1.2511,
      "step": 2980
    },
    {
      "epoch": 2.471366549514162,
      "grad_norm": 2.189788341522217,
      "learning_rate": 5.447592067988668e-06,
      "loss": 1.2208,
      "step": 2990
    },
    {
      "epoch": 2.4796361381021295,
      "grad_norm": 2.021899700164795,
      "learning_rate": 5.362606232294617e-06,
      "loss": 1.2396,
      "step": 3000
    },
    {
      "epoch": 2.487905726690097,
      "grad_norm": 1.7297967672348022,
      "learning_rate": 5.277620396600567e-06,
      "loss": 1.2565,
      "step": 3010
    },
    {
      "epoch": 2.496175315278065,
      "grad_norm": 1.879725456237793,
      "learning_rate": 5.192634560906516e-06,
      "loss": 1.2163,
      "step": 3020
    },
    {
      "epoch": 2.5044449038660326,
      "grad_norm": 1.8889371156692505,
      "learning_rate": 5.107648725212465e-06,
      "loss": 1.2714,
      "step": 3030
    },
    {
      "epoch": 2.512714492454,
      "grad_norm": 1.8619478940963745,
      "learning_rate": 5.0226628895184135e-06,
      "loss": 1.1849,
      "step": 3040
    },
    {
      "epoch": 2.520984081041968,
      "grad_norm": 1.8680696487426758,
      "learning_rate": 4.937677053824363e-06,
      "loss": 1.2671,
      "step": 3050
    },
    {
      "epoch": 2.529253669629936,
      "grad_norm": 1.7352509498596191,
      "learning_rate": 4.852691218130312e-06,
      "loss": 1.2405,
      "step": 3060
    },
    {
      "epoch": 2.5375232582179037,
      "grad_norm": 1.7222084999084473,
      "learning_rate": 4.767705382436261e-06,
      "loss": 1.1487,
      "step": 3070
    },
    {
      "epoch": 2.5457928468058713,
      "grad_norm": 1.7918256521224976,
      "learning_rate": 4.6827195467422105e-06,
      "loss": 1.1866,
      "step": 3080
    },
    {
      "epoch": 2.5540624353938393,
      "grad_norm": 2.207385301589966,
      "learning_rate": 4.597733711048158e-06,
      "loss": 1.231,
      "step": 3090
    },
    {
      "epoch": 2.562332023981807,
      "grad_norm": 1.6856311559677124,
      "learning_rate": 4.512747875354107e-06,
      "loss": 1.1808,
      "step": 3100
    },
    {
      "epoch": 2.570601612569775,
      "grad_norm": 1.909403681755066,
      "learning_rate": 4.427762039660057e-06,
      "loss": 1.2162,
      "step": 3110
    },
    {
      "epoch": 2.5788712011577424,
      "grad_norm": 1.9691675901412964,
      "learning_rate": 4.342776203966006e-06,
      "loss": 1.1987,
      "step": 3120
    },
    {
      "epoch": 2.5871407897457104,
      "grad_norm": 2.015225410461426,
      "learning_rate": 4.257790368271955e-06,
      "loss": 1.1876,
      "step": 3130
    },
    {
      "epoch": 2.595410378333678,
      "grad_norm": 1.8239184617996216,
      "learning_rate": 4.1728045325779035e-06,
      "loss": 1.2398,
      "step": 3140
    },
    {
      "epoch": 2.6036799669216455,
      "grad_norm": 1.7749358415603638,
      "learning_rate": 4.087818696883853e-06,
      "loss": 1.1445,
      "step": 3150
    },
    {
      "epoch": 2.6119495555096135,
      "grad_norm": 2.030076265335083,
      "learning_rate": 4.002832861189802e-06,
      "loss": 1.1422,
      "step": 3160
    },
    {
      "epoch": 2.620219144097581,
      "grad_norm": 1.586416244506836,
      "learning_rate": 3.917847025495751e-06,
      "loss": 1.1663,
      "step": 3170
    },
    {
      "epoch": 2.628488732685549,
      "grad_norm": 1.7210750579833984,
      "learning_rate": 3.8328611898017005e-06,
      "loss": 1.2014,
      "step": 3180
    },
    {
      "epoch": 2.6367583212735166,
      "grad_norm": 2.1672744750976562,
      "learning_rate": 3.747875354107649e-06,
      "loss": 1.2107,
      "step": 3190
    },
    {
      "epoch": 2.6450279098614846,
      "grad_norm": 1.946649432182312,
      "learning_rate": 3.6628895184135977e-06,
      "loss": 1.2489,
      "step": 3200
    },
    {
      "epoch": 2.653297498449452,
      "grad_norm": 2.467484951019287,
      "learning_rate": 3.577903682719547e-06,
      "loss": 1.143,
      "step": 3210
    },
    {
      "epoch": 2.6615670870374197,
      "grad_norm": 2.1687841415405273,
      "learning_rate": 3.4929178470254962e-06,
      "loss": 1.2668,
      "step": 3220
    },
    {
      "epoch": 2.6698366756253877,
      "grad_norm": 2.4397876262664795,
      "learning_rate": 3.4079320113314446e-06,
      "loss": 1.1909,
      "step": 3230
    },
    {
      "epoch": 2.6781062642133553,
      "grad_norm": 1.7506232261657715,
      "learning_rate": 3.322946175637394e-06,
      "loss": 1.2104,
      "step": 3240
    },
    {
      "epoch": 2.6863758528013233,
      "grad_norm": 1.950901985168457,
      "learning_rate": 3.2379603399433427e-06,
      "loss": 1.2157,
      "step": 3250
    },
    {
      "epoch": 2.694645441389291,
      "grad_norm": 2.250687599182129,
      "learning_rate": 3.152974504249292e-06,
      "loss": 1.2795,
      "step": 3260
    },
    {
      "epoch": 2.702915029977259,
      "grad_norm": 2.1358838081359863,
      "learning_rate": 3.067988668555241e-06,
      "loss": 1.1683,
      "step": 3270
    },
    {
      "epoch": 2.7111846185652264,
      "grad_norm": 1.9450998306274414,
      "learning_rate": 2.98300283286119e-06,
      "loss": 1.1334,
      "step": 3280
    },
    {
      "epoch": 2.719454207153194,
      "grad_norm": 1.7255396842956543,
      "learning_rate": 2.898016997167139e-06,
      "loss": 1.2201,
      "step": 3290
    },
    {
      "epoch": 2.727723795741162,
      "grad_norm": 2.1106693744659424,
      "learning_rate": 2.8130311614730877e-06,
      "loss": 1.2394,
      "step": 3300
    },
    {
      "epoch": 2.7359933843291295,
      "grad_norm": 1.7739542722702026,
      "learning_rate": 2.728045325779037e-06,
      "loss": 1.1426,
      "step": 3310
    },
    {
      "epoch": 2.7442629729170975,
      "grad_norm": 2.051715612411499,
      "learning_rate": 2.643059490084986e-06,
      "loss": 1.2354,
      "step": 3320
    },
    {
      "epoch": 2.752532561505065,
      "grad_norm": 2.328799247741699,
      "learning_rate": 2.558073654390935e-06,
      "loss": 1.2159,
      "step": 3330
    },
    {
      "epoch": 2.760802150093033,
      "grad_norm": 1.9920265674591064,
      "learning_rate": 2.4730878186968842e-06,
      "loss": 1.2667,
      "step": 3340
    },
    {
      "epoch": 2.7690717386810006,
      "grad_norm": 2.213667154312134,
      "learning_rate": 2.3881019830028326e-06,
      "loss": 1.1645,
      "step": 3350
    },
    {
      "epoch": 2.777341327268968,
      "grad_norm": 1.957419753074646,
      "learning_rate": 2.303116147308782e-06,
      "loss": 1.16,
      "step": 3360
    },
    {
      "epoch": 2.785610915856936,
      "grad_norm": 1.9142310619354248,
      "learning_rate": 2.218130311614731e-06,
      "loss": 1.2238,
      "step": 3370
    },
    {
      "epoch": 2.7938805044449038,
      "grad_norm": 2.1737639904022217,
      "learning_rate": 2.13314447592068e-06,
      "loss": 1.2292,
      "step": 3380
    },
    {
      "epoch": 2.8021500930328718,
      "grad_norm": 2.2084732055664062,
      "learning_rate": 2.048158640226629e-06,
      "loss": 1.2322,
      "step": 3390
    },
    {
      "epoch": 2.8104196816208393,
      "grad_norm": 2.0375149250030518,
      "learning_rate": 1.9631728045325776e-06,
      "loss": 1.1958,
      "step": 3400
    },
    {
      "epoch": 2.8186892702088073,
      "grad_norm": 1.8081676959991455,
      "learning_rate": 1.878186968838527e-06,
      "loss": 1.1861,
      "step": 3410
    },
    {
      "epoch": 2.826958858796775,
      "grad_norm": 2.4300856590270996,
      "learning_rate": 1.7932011331444759e-06,
      "loss": 1.2146,
      "step": 3420
    },
    {
      "epoch": 2.8352284473847424,
      "grad_norm": 1.6910979747772217,
      "learning_rate": 1.708215297450425e-06,
      "loss": 1.1773,
      "step": 3430
    },
    {
      "epoch": 2.8434980359727104,
      "grad_norm": 1.9835994243621826,
      "learning_rate": 1.6232294617563742e-06,
      "loss": 1.2379,
      "step": 3440
    },
    {
      "epoch": 2.851767624560678,
      "grad_norm": 2.226795196533203,
      "learning_rate": 1.538243626062323e-06,
      "loss": 1.2184,
      "step": 3450
    },
    {
      "epoch": 2.860037213148646,
      "grad_norm": 2.1995327472686768,
      "learning_rate": 1.453257790368272e-06,
      "loss": 1.2032,
      "step": 3460
    },
    {
      "epoch": 2.8683068017366136,
      "grad_norm": 2.3609673976898193,
      "learning_rate": 1.3682719546742209e-06,
      "loss": 1.187,
      "step": 3470
    },
    {
      "epoch": 2.8765763903245816,
      "grad_norm": 1.9133023023605347,
      "learning_rate": 1.28328611898017e-06,
      "loss": 1.1981,
      "step": 3480
    },
    {
      "epoch": 2.884845978912549,
      "grad_norm": 2.2922427654266357,
      "learning_rate": 1.1983002832861192e-06,
      "loss": 1.2478,
      "step": 3490
    },
    {
      "epoch": 2.8931155675005167,
      "grad_norm": 2.0167531967163086,
      "learning_rate": 1.113314447592068e-06,
      "loss": 1.2329,
      "step": 3500
    },
    {
      "epoch": 2.9013851560884847,
      "grad_norm": 1.8998889923095703,
      "learning_rate": 1.028328611898017e-06,
      "loss": 1.13,
      "step": 3510
    },
    {
      "epoch": 2.9096547446764522,
      "grad_norm": 2.130659818649292,
      "learning_rate": 9.433427762039659e-07,
      "loss": 1.1837,
      "step": 3520
    },
    {
      "epoch": 2.9179243332644202,
      "grad_norm": 2.47963547706604,
      "learning_rate": 8.583569405099151e-07,
      "loss": 1.2564,
      "step": 3530
    },
    {
      "epoch": 2.926193921852388,
      "grad_norm": 2.111050605773926,
      "learning_rate": 7.73371104815864e-07,
      "loss": 1.2012,
      "step": 3540
    },
    {
      "epoch": 2.934463510440356,
      "grad_norm": 1.8260276317596436,
      "learning_rate": 6.883852691218131e-07,
      "loss": 1.1818,
      "step": 3550
    },
    {
      "epoch": 2.9427330990283234,
      "grad_norm": 2.116024971008301,
      "learning_rate": 6.033994334277621e-07,
      "loss": 1.1092,
      "step": 3560
    },
    {
      "epoch": 2.951002687616291,
      "grad_norm": 1.9087313413619995,
      "learning_rate": 5.18413597733711e-07,
      "loss": 1.1795,
      "step": 3570
    },
    {
      "epoch": 2.959272276204259,
      "grad_norm": 1.948176622390747,
      "learning_rate": 4.3342776203966006e-07,
      "loss": 1.3069,
      "step": 3580
    },
    {
      "epoch": 2.9675418647922265,
      "grad_norm": 2.1610121726989746,
      "learning_rate": 3.484419263456091e-07,
      "loss": 1.1879,
      "step": 3590
    },
    {
      "epoch": 2.9758114533801945,
      "grad_norm": 2.360114574432373,
      "learning_rate": 2.634560906515581e-07,
      "loss": 1.2711,
      "step": 3600
    }
  ],
  "logging_steps": 10,
  "max_steps": 3630,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8198321048911872.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
