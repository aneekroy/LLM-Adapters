{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.951416244607016,
  "eval_steps": 500,
  "global_step": 19800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0025010942287250672,
      "grad_norm": 1.848596453666687,
      "learning_rate": 2.7e-06,
      "loss": 3.0612,
      "step": 10
    },
    {
      "epoch": 0.0050021884574501345,
      "grad_norm": 1.6933575868606567,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 3.0936,
      "step": 20
    },
    {
      "epoch": 0.007503282686175202,
      "grad_norm": 1.5116249322891235,
      "learning_rate": 8.7e-06,
      "loss": 3.1926,
      "step": 30
    },
    {
      "epoch": 0.010004376914900269,
      "grad_norm": 1.0510804653167725,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 2.9335,
      "step": 40
    },
    {
      "epoch": 0.012505471143625336,
      "grad_norm": 1.0742143392562866,
      "learning_rate": 1.47e-05,
      "loss": 2.8287,
      "step": 50
    },
    {
      "epoch": 0.015006565372350403,
      "grad_norm": 1.2045135498046875,
      "learning_rate": 1.77e-05,
      "loss": 2.7912,
      "step": 60
    },
    {
      "epoch": 0.01750765960107547,
      "grad_norm": 1.7735519409179688,
      "learning_rate": 2.07e-05,
      "loss": 2.6635,
      "step": 70
    },
    {
      "epoch": 0.020008753829800538,
      "grad_norm": 1.5546375513076782,
      "learning_rate": 2.37e-05,
      "loss": 2.7079,
      "step": 80
    },
    {
      "epoch": 0.022509848058525603,
      "grad_norm": 1.4147676229476929,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.5402,
      "step": 90
    },
    {
      "epoch": 0.025010942287250672,
      "grad_norm": 1.8263378143310547,
      "learning_rate": 2.97e-05,
      "loss": 2.3415,
      "step": 100
    },
    {
      "epoch": 0.027512036515975738,
      "grad_norm": 2.11407470703125,
      "learning_rate": 2.9986428750942448e-05,
      "loss": 2.1522,
      "step": 110
    },
    {
      "epoch": 0.030013130744700807,
      "grad_norm": 1.9380638599395752,
      "learning_rate": 2.9971349585322948e-05,
      "loss": 1.8594,
      "step": 120
    },
    {
      "epoch": 0.032514224973425876,
      "grad_norm": 2.952100992202759,
      "learning_rate": 2.9956270419703444e-05,
      "loss": 1.7218,
      "step": 130
    },
    {
      "epoch": 0.03501531920215094,
      "grad_norm": 2.6348774433135986,
      "learning_rate": 2.994119125408394e-05,
      "loss": 1.4965,
      "step": 140
    },
    {
      "epoch": 0.03751641343087601,
      "grad_norm": 3.6457362174987793,
      "learning_rate": 2.992611208846444e-05,
      "loss": 1.295,
      "step": 150
    },
    {
      "epoch": 0.040017507659601076,
      "grad_norm": 1.606382131576538,
      "learning_rate": 2.9911032922844937e-05,
      "loss": 1.2809,
      "step": 160
    },
    {
      "epoch": 0.042518601888326145,
      "grad_norm": 1.8128122091293335,
      "learning_rate": 2.9895953757225433e-05,
      "loss": 1.2651,
      "step": 170
    },
    {
      "epoch": 0.04501969611705121,
      "grad_norm": 1.4140881299972534,
      "learning_rate": 2.988087459160593e-05,
      "loss": 1.2441,
      "step": 180
    },
    {
      "epoch": 0.047520790345776276,
      "grad_norm": 1.719605803489685,
      "learning_rate": 2.986579542598643e-05,
      "loss": 1.2277,
      "step": 190
    },
    {
      "epoch": 0.050021884574501345,
      "grad_norm": 1.7955849170684814,
      "learning_rate": 2.985071626036693e-05,
      "loss": 1.1978,
      "step": 200
    },
    {
      "epoch": 0.052522978803226414,
      "grad_norm": 1.8814716339111328,
      "learning_rate": 2.9835637094747422e-05,
      "loss": 1.1944,
      "step": 210
    },
    {
      "epoch": 0.055024073031951476,
      "grad_norm": 1.1452209949493408,
      "learning_rate": 2.9820557929127922e-05,
      "loss": 1.1485,
      "step": 220
    },
    {
      "epoch": 0.057525167260676545,
      "grad_norm": 1.706874966621399,
      "learning_rate": 2.9805478763508422e-05,
      "loss": 1.1825,
      "step": 230
    },
    {
      "epoch": 0.060026261489401614,
      "grad_norm": 1.890178918838501,
      "learning_rate": 2.9790399597888918e-05,
      "loss": 1.1806,
      "step": 240
    },
    {
      "epoch": 0.06252735571812668,
      "grad_norm": 2.3192524909973145,
      "learning_rate": 2.9775320432269415e-05,
      "loss": 1.1849,
      "step": 250
    },
    {
      "epoch": 0.06502844994685175,
      "grad_norm": 1.4285073280334473,
      "learning_rate": 2.976024126664991e-05,
      "loss": 1.1515,
      "step": 260
    },
    {
      "epoch": 0.06752954417557681,
      "grad_norm": 1.683820128440857,
      "learning_rate": 2.974516210103041e-05,
      "loss": 1.2404,
      "step": 270
    },
    {
      "epoch": 0.07003063840430188,
      "grad_norm": 1.6671974658966064,
      "learning_rate": 2.973008293541091e-05,
      "loss": 1.1126,
      "step": 280
    },
    {
      "epoch": 0.07253173263302695,
      "grad_norm": 2.724445104598999,
      "learning_rate": 2.9715003769791404e-05,
      "loss": 1.1921,
      "step": 290
    },
    {
      "epoch": 0.07503282686175201,
      "grad_norm": 1.6673007011413574,
      "learning_rate": 2.9699924604171904e-05,
      "loss": 1.2083,
      "step": 300
    },
    {
      "epoch": 0.07753392109047709,
      "grad_norm": 2.035677671432495,
      "learning_rate": 2.96848454385524e-05,
      "loss": 1.1809,
      "step": 310
    },
    {
      "epoch": 0.08003501531920215,
      "grad_norm": 1.6175267696380615,
      "learning_rate": 2.96697662729329e-05,
      "loss": 1.1418,
      "step": 320
    },
    {
      "epoch": 0.08253610954792721,
      "grad_norm": 1.745935320854187,
      "learning_rate": 2.9654687107313396e-05,
      "loss": 1.1708,
      "step": 330
    },
    {
      "epoch": 0.08503720377665229,
      "grad_norm": 1.8633334636688232,
      "learning_rate": 2.9639607941693893e-05,
      "loss": 1.113,
      "step": 340
    },
    {
      "epoch": 0.08753829800537735,
      "grad_norm": 2.5076026916503906,
      "learning_rate": 2.9624528776074392e-05,
      "loss": 1.2098,
      "step": 350
    },
    {
      "epoch": 0.09003939223410241,
      "grad_norm": 1.793565273284912,
      "learning_rate": 2.960944961045489e-05,
      "loss": 1.1757,
      "step": 360
    },
    {
      "epoch": 0.09254048646282749,
      "grad_norm": 1.4937478303909302,
      "learning_rate": 2.9594370444835385e-05,
      "loss": 1.1997,
      "step": 370
    },
    {
      "epoch": 0.09504158069155255,
      "grad_norm": 1.8020058870315552,
      "learning_rate": 2.9579291279215885e-05,
      "loss": 1.1717,
      "step": 380
    },
    {
      "epoch": 0.09754267492027763,
      "grad_norm": 1.3781818151474,
      "learning_rate": 2.956421211359638e-05,
      "loss": 1.1643,
      "step": 390
    },
    {
      "epoch": 0.10004376914900269,
      "grad_norm": 1.4701604843139648,
      "learning_rate": 2.9549132947976878e-05,
      "loss": 1.194,
      "step": 400
    },
    {
      "epoch": 0.10254486337772775,
      "grad_norm": 2.2095859050750732,
      "learning_rate": 2.9534053782357378e-05,
      "loss": 1.1423,
      "step": 410
    },
    {
      "epoch": 0.10504595760645283,
      "grad_norm": 3.4272079467773438,
      "learning_rate": 2.9518974616737874e-05,
      "loss": 1.101,
      "step": 420
    },
    {
      "epoch": 0.10754705183517789,
      "grad_norm": 2.324846029281616,
      "learning_rate": 2.9503895451118374e-05,
      "loss": 1.1451,
      "step": 430
    },
    {
      "epoch": 0.11004814606390295,
      "grad_norm": 1.9975972175598145,
      "learning_rate": 2.9488816285498867e-05,
      "loss": 1.1377,
      "step": 440
    },
    {
      "epoch": 0.11254924029262803,
      "grad_norm": 2.198505401611328,
      "learning_rate": 2.9473737119879367e-05,
      "loss": 1.2032,
      "step": 450
    },
    {
      "epoch": 0.11505033452135309,
      "grad_norm": 1.919996738433838,
      "learning_rate": 2.9458657954259867e-05,
      "loss": 1.1279,
      "step": 460
    },
    {
      "epoch": 0.11755142875007817,
      "grad_norm": 1.7160115242004395,
      "learning_rate": 2.9443578788640363e-05,
      "loss": 1.189,
      "step": 470
    },
    {
      "epoch": 0.12005252297880323,
      "grad_norm": 1.680362343788147,
      "learning_rate": 2.942849962302086e-05,
      "loss": 1.0912,
      "step": 480
    },
    {
      "epoch": 0.12255361720752829,
      "grad_norm": 1.3594170808792114,
      "learning_rate": 2.941342045740136e-05,
      "loss": 1.1042,
      "step": 490
    },
    {
      "epoch": 0.12505471143625335,
      "grad_norm": 1.4367023706436157,
      "learning_rate": 2.9398341291781856e-05,
      "loss": 1.1647,
      "step": 500
    },
    {
      "epoch": 0.12755580566497843,
      "grad_norm": 1.69464111328125,
      "learning_rate": 2.9383262126162355e-05,
      "loss": 1.1898,
      "step": 510
    },
    {
      "epoch": 0.1300568998937035,
      "grad_norm": 2.0418496131896973,
      "learning_rate": 2.936818296054285e-05,
      "loss": 1.1091,
      "step": 520
    },
    {
      "epoch": 0.13255799412242855,
      "grad_norm": 1.3661160469055176,
      "learning_rate": 2.9353103794923348e-05,
      "loss": 1.085,
      "step": 530
    },
    {
      "epoch": 0.13505908835115363,
      "grad_norm": 1.8476754426956177,
      "learning_rate": 2.9338024629303848e-05,
      "loss": 1.1332,
      "step": 540
    },
    {
      "epoch": 0.1375601825798787,
      "grad_norm": 1.550046443939209,
      "learning_rate": 2.932294546368434e-05,
      "loss": 1.1601,
      "step": 550
    },
    {
      "epoch": 0.14006127680860375,
      "grad_norm": 1.628635048866272,
      "learning_rate": 2.930786629806484e-05,
      "loss": 1.0723,
      "step": 560
    },
    {
      "epoch": 0.14256237103732883,
      "grad_norm": 1.830184817314148,
      "learning_rate": 2.9292787132445337e-05,
      "loss": 1.1367,
      "step": 570
    },
    {
      "epoch": 0.1450634652660539,
      "grad_norm": 1.6407710313796997,
      "learning_rate": 2.9277707966825837e-05,
      "loss": 1.2227,
      "step": 580
    },
    {
      "epoch": 0.14756455949477898,
      "grad_norm": 1.8274807929992676,
      "learning_rate": 2.9262628801206334e-05,
      "loss": 1.1292,
      "step": 590
    },
    {
      "epoch": 0.15006565372350403,
      "grad_norm": 1.8724586963653564,
      "learning_rate": 2.924754963558683e-05,
      "loss": 1.2335,
      "step": 600
    },
    {
      "epoch": 0.1525667479522291,
      "grad_norm": 1.5080660581588745,
      "learning_rate": 2.923247046996733e-05,
      "loss": 1.1984,
      "step": 610
    },
    {
      "epoch": 0.15506784218095418,
      "grad_norm": 1.6450859308242798,
      "learning_rate": 2.921739130434783e-05,
      "loss": 1.1544,
      "step": 620
    },
    {
      "epoch": 0.15756893640967923,
      "grad_norm": 1.8589484691619873,
      "learning_rate": 2.9202312138728323e-05,
      "loss": 1.1655,
      "step": 630
    },
    {
      "epoch": 0.1600700306384043,
      "grad_norm": 1.8443593978881836,
      "learning_rate": 2.9187232973108822e-05,
      "loss": 1.1639,
      "step": 640
    },
    {
      "epoch": 0.16257112486712938,
      "grad_norm": 1.5906100273132324,
      "learning_rate": 2.917215380748932e-05,
      "loss": 1.1839,
      "step": 650
    },
    {
      "epoch": 0.16507221909585443,
      "grad_norm": 1.5940055847167969,
      "learning_rate": 2.915707464186982e-05,
      "loss": 1.1808,
      "step": 660
    },
    {
      "epoch": 0.1675733133245795,
      "grad_norm": 2.154905319213867,
      "learning_rate": 2.9141995476250315e-05,
      "loss": 1.2173,
      "step": 670
    },
    {
      "epoch": 0.17007440755330458,
      "grad_norm": 1.8702776432037354,
      "learning_rate": 2.912691631063081e-05,
      "loss": 1.186,
      "step": 680
    },
    {
      "epoch": 0.17257550178202963,
      "grad_norm": 1.6613702774047852,
      "learning_rate": 2.911183714501131e-05,
      "loss": 1.2094,
      "step": 690
    },
    {
      "epoch": 0.1750765960107547,
      "grad_norm": 1.7934913635253906,
      "learning_rate": 2.9096757979391808e-05,
      "loss": 1.1461,
      "step": 700
    },
    {
      "epoch": 0.17757769023947978,
      "grad_norm": 2.024348735809326,
      "learning_rate": 2.9081678813772304e-05,
      "loss": 1.1383,
      "step": 710
    },
    {
      "epoch": 0.18007878446820483,
      "grad_norm": 1.4893604516983032,
      "learning_rate": 2.9066599648152804e-05,
      "loss": 1.2215,
      "step": 720
    },
    {
      "epoch": 0.1825798786969299,
      "grad_norm": 1.8731176853179932,
      "learning_rate": 2.90515204825333e-05,
      "loss": 1.1286,
      "step": 730
    },
    {
      "epoch": 0.18508097292565498,
      "grad_norm": 1.6747506856918335,
      "learning_rate": 2.9036441316913797e-05,
      "loss": 1.1635,
      "step": 740
    },
    {
      "epoch": 0.18758206715438003,
      "grad_norm": 1.8837946653366089,
      "learning_rate": 2.9021362151294297e-05,
      "loss": 1.1559,
      "step": 750
    },
    {
      "epoch": 0.1900831613831051,
      "grad_norm": 1.696988821029663,
      "learning_rate": 2.9006282985674793e-05,
      "loss": 1.1389,
      "step": 760
    },
    {
      "epoch": 0.19258425561183018,
      "grad_norm": 1.979087233543396,
      "learning_rate": 2.8991203820055293e-05,
      "loss": 1.1078,
      "step": 770
    },
    {
      "epoch": 0.19508534984055526,
      "grad_norm": 1.315451741218567,
      "learning_rate": 2.8976124654435786e-05,
      "loss": 1.1651,
      "step": 780
    },
    {
      "epoch": 0.1975864440692803,
      "grad_norm": 1.4434847831726074,
      "learning_rate": 2.8961045488816286e-05,
      "loss": 1.1189,
      "step": 790
    },
    {
      "epoch": 0.20008753829800538,
      "grad_norm": 2.675504684448242,
      "learning_rate": 2.8945966323196785e-05,
      "loss": 1.103,
      "step": 800
    },
    {
      "epoch": 0.20258863252673046,
      "grad_norm": 1.882973313331604,
      "learning_rate": 2.8930887157577282e-05,
      "loss": 1.1121,
      "step": 810
    },
    {
      "epoch": 0.2050897267554555,
      "grad_norm": 1.4398425817489624,
      "learning_rate": 2.891580799195778e-05,
      "loss": 1.1,
      "step": 820
    },
    {
      "epoch": 0.20759082098418058,
      "grad_norm": 2.04418683052063,
      "learning_rate": 2.8900728826338275e-05,
      "loss": 1.1575,
      "step": 830
    },
    {
      "epoch": 0.21009191521290566,
      "grad_norm": 2.1827335357666016,
      "learning_rate": 2.8885649660718775e-05,
      "loss": 1.1543,
      "step": 840
    },
    {
      "epoch": 0.2125930094416307,
      "grad_norm": 1.4456671476364136,
      "learning_rate": 2.8870570495099274e-05,
      "loss": 1.0774,
      "step": 850
    },
    {
      "epoch": 0.21509410367035578,
      "grad_norm": 1.376316785812378,
      "learning_rate": 2.8855491329479767e-05,
      "loss": 1.1437,
      "step": 860
    },
    {
      "epoch": 0.21759519789908086,
      "grad_norm": 1.599084734916687,
      "learning_rate": 2.8840412163860267e-05,
      "loss": 1.0994,
      "step": 870
    },
    {
      "epoch": 0.2200962921278059,
      "grad_norm": 1.7607324123382568,
      "learning_rate": 2.8825332998240767e-05,
      "loss": 1.1333,
      "step": 880
    },
    {
      "epoch": 0.22259738635653098,
      "grad_norm": 1.2867921590805054,
      "learning_rate": 2.8810253832621263e-05,
      "loss": 1.1471,
      "step": 890
    },
    {
      "epoch": 0.22509848058525606,
      "grad_norm": 1.4518766403198242,
      "learning_rate": 2.879517466700176e-05,
      "loss": 1.1377,
      "step": 900
    },
    {
      "epoch": 0.2275995748139811,
      "grad_norm": 2.007554531097412,
      "learning_rate": 2.8780095501382256e-05,
      "loss": 1.1182,
      "step": 910
    },
    {
      "epoch": 0.23010066904270618,
      "grad_norm": 1.823134422302246,
      "learning_rate": 2.8765016335762756e-05,
      "loss": 1.1569,
      "step": 920
    },
    {
      "epoch": 0.23260176327143126,
      "grad_norm": 1.384933590888977,
      "learning_rate": 2.8749937170143253e-05,
      "loss": 1.0723,
      "step": 930
    },
    {
      "epoch": 0.23510285750015633,
      "grad_norm": 1.6844124794006348,
      "learning_rate": 2.873485800452375e-05,
      "loss": 1.1376,
      "step": 940
    },
    {
      "epoch": 0.23760395172888138,
      "grad_norm": 1.3041632175445557,
      "learning_rate": 2.871977883890425e-05,
      "loss": 1.1826,
      "step": 950
    },
    {
      "epoch": 0.24010504595760646,
      "grad_norm": 1.701310157775879,
      "learning_rate": 2.8704699673284745e-05,
      "loss": 1.1246,
      "step": 960
    },
    {
      "epoch": 0.24260614018633153,
      "grad_norm": 1.8935996294021606,
      "learning_rate": 2.868962050766524e-05,
      "loss": 1.1658,
      "step": 970
    },
    {
      "epoch": 0.24510723441505658,
      "grad_norm": 1.728711724281311,
      "learning_rate": 2.867454134204574e-05,
      "loss": 1.1393,
      "step": 980
    },
    {
      "epoch": 0.24760832864378166,
      "grad_norm": 1.9096014499664307,
      "learning_rate": 2.8659462176426238e-05,
      "loss": 1.1448,
      "step": 990
    },
    {
      "epoch": 0.2501094228725067,
      "grad_norm": 1.604227066040039,
      "learning_rate": 2.8644383010806738e-05,
      "loss": 1.1239,
      "step": 1000
    },
    {
      "epoch": 0.2526105171012318,
      "grad_norm": 2.2731189727783203,
      "learning_rate": 2.8629303845187234e-05,
      "loss": 1.1759,
      "step": 1010
    },
    {
      "epoch": 0.25511161132995686,
      "grad_norm": 1.5375797748565674,
      "learning_rate": 2.861422467956773e-05,
      "loss": 1.135,
      "step": 1020
    },
    {
      "epoch": 0.25761270555868193,
      "grad_norm": 1.7005794048309326,
      "learning_rate": 2.859914551394823e-05,
      "loss": 1.123,
      "step": 1030
    },
    {
      "epoch": 0.260113799787407,
      "grad_norm": 1.7119026184082031,
      "learning_rate": 2.8584066348328727e-05,
      "loss": 1.2317,
      "step": 1040
    },
    {
      "epoch": 0.2626148940161321,
      "grad_norm": 1.891147494316101,
      "learning_rate": 2.8568987182709223e-05,
      "loss": 1.19,
      "step": 1050
    },
    {
      "epoch": 0.2651159882448571,
      "grad_norm": 1.7078877687454224,
      "learning_rate": 2.8553908017089723e-05,
      "loss": 1.1689,
      "step": 1060
    },
    {
      "epoch": 0.2676170824735822,
      "grad_norm": 1.4349803924560547,
      "learning_rate": 2.853882885147022e-05,
      "loss": 1.1055,
      "step": 1070
    },
    {
      "epoch": 0.27011817670230726,
      "grad_norm": 3.279419422149658,
      "learning_rate": 2.852374968585072e-05,
      "loss": 1.1375,
      "step": 1080
    },
    {
      "epoch": 0.27261927093103233,
      "grad_norm": 1.5585888624191284,
      "learning_rate": 2.8508670520231212e-05,
      "loss": 1.1906,
      "step": 1090
    },
    {
      "epoch": 0.2751203651597574,
      "grad_norm": 1.4195480346679688,
      "learning_rate": 2.8493591354611712e-05,
      "loss": 1.1724,
      "step": 1100
    },
    {
      "epoch": 0.2776214593884825,
      "grad_norm": 1.4870742559432983,
      "learning_rate": 2.8478512188992212e-05,
      "loss": 1.2366,
      "step": 1110
    },
    {
      "epoch": 0.2801225536172075,
      "grad_norm": 1.103791356086731,
      "learning_rate": 2.8463433023372705e-05,
      "loss": 1.1658,
      "step": 1120
    },
    {
      "epoch": 0.2826236478459326,
      "grad_norm": 1.535593032836914,
      "learning_rate": 2.8448353857753205e-05,
      "loss": 1.1074,
      "step": 1130
    },
    {
      "epoch": 0.28512474207465766,
      "grad_norm": 1.616231083869934,
      "learning_rate": 2.8433274692133704e-05,
      "loss": 1.1441,
      "step": 1140
    },
    {
      "epoch": 0.28762583630338273,
      "grad_norm": 1.9008370637893677,
      "learning_rate": 2.84181955265142e-05,
      "loss": 1.0959,
      "step": 1150
    },
    {
      "epoch": 0.2901269305321078,
      "grad_norm": 2.056619882583618,
      "learning_rate": 2.8403116360894697e-05,
      "loss": 1.0758,
      "step": 1160
    },
    {
      "epoch": 0.2926280247608329,
      "grad_norm": 2.4966485500335693,
      "learning_rate": 2.8388037195275194e-05,
      "loss": 1.1417,
      "step": 1170
    },
    {
      "epoch": 0.29512911898955796,
      "grad_norm": 1.9251575469970703,
      "learning_rate": 2.8372958029655693e-05,
      "loss": 1.1494,
      "step": 1180
    },
    {
      "epoch": 0.297630213218283,
      "grad_norm": 1.632237195968628,
      "learning_rate": 2.8357878864036193e-05,
      "loss": 1.0833,
      "step": 1190
    },
    {
      "epoch": 0.30013130744700806,
      "grad_norm": 1.882112741470337,
      "learning_rate": 2.8342799698416686e-05,
      "loss": 1.1206,
      "step": 1200
    },
    {
      "epoch": 0.30263240167573313,
      "grad_norm": 1.7767338752746582,
      "learning_rate": 2.8327720532797186e-05,
      "loss": 1.1337,
      "step": 1210
    },
    {
      "epoch": 0.3051334959044582,
      "grad_norm": 1.3734437227249146,
      "learning_rate": 2.8312641367177683e-05,
      "loss": 1.1099,
      "step": 1220
    },
    {
      "epoch": 0.3076345901331833,
      "grad_norm": 1.7928900718688965,
      "learning_rate": 2.8297562201558182e-05,
      "loss": 1.1597,
      "step": 1230
    },
    {
      "epoch": 0.31013568436190836,
      "grad_norm": 1.8394631147384644,
      "learning_rate": 2.828248303593868e-05,
      "loss": 1.1224,
      "step": 1240
    },
    {
      "epoch": 0.3126367785906334,
      "grad_norm": 1.8773338794708252,
      "learning_rate": 2.8267403870319175e-05,
      "loss": 1.1792,
      "step": 1250
    },
    {
      "epoch": 0.31513787281935846,
      "grad_norm": 1.3407665491104126,
      "learning_rate": 2.8252324704699675e-05,
      "loss": 1.1623,
      "step": 1260
    },
    {
      "epoch": 0.31763896704808353,
      "grad_norm": 1.3997554779052734,
      "learning_rate": 2.8237245539080175e-05,
      "loss": 1.1521,
      "step": 1270
    },
    {
      "epoch": 0.3201400612768086,
      "grad_norm": 1.4878393411636353,
      "learning_rate": 2.8222166373460668e-05,
      "loss": 1.1067,
      "step": 1280
    },
    {
      "epoch": 0.3226411555055337,
      "grad_norm": 1.7643177509307861,
      "learning_rate": 2.8207087207841168e-05,
      "loss": 1.09,
      "step": 1290
    },
    {
      "epoch": 0.32514224973425876,
      "grad_norm": 1.7037804126739502,
      "learning_rate": 2.8192008042221664e-05,
      "loss": 1.0717,
      "step": 1300
    },
    {
      "epoch": 0.3276433439629838,
      "grad_norm": 1.699029564857483,
      "learning_rate": 2.817692887660216e-05,
      "loss": 1.1387,
      "step": 1310
    },
    {
      "epoch": 0.33014443819170886,
      "grad_norm": 1.5059728622436523,
      "learning_rate": 2.816184971098266e-05,
      "loss": 1.1614,
      "step": 1320
    },
    {
      "epoch": 0.33264553242043393,
      "grad_norm": 1.5802433490753174,
      "learning_rate": 2.8146770545363157e-05,
      "loss": 1.0635,
      "step": 1330
    },
    {
      "epoch": 0.335146626649159,
      "grad_norm": 2.2479591369628906,
      "learning_rate": 2.8131691379743657e-05,
      "loss": 1.1271,
      "step": 1340
    },
    {
      "epoch": 0.3376477208778841,
      "grad_norm": 2.0404531955718994,
      "learning_rate": 2.811661221412415e-05,
      "loss": 1.1269,
      "step": 1350
    },
    {
      "epoch": 0.34014881510660916,
      "grad_norm": 2.10026216506958,
      "learning_rate": 2.810153304850465e-05,
      "loss": 1.1442,
      "step": 1360
    },
    {
      "epoch": 0.34264990933533424,
      "grad_norm": 1.6341272592544556,
      "learning_rate": 2.808645388288515e-05,
      "loss": 1.1124,
      "step": 1370
    },
    {
      "epoch": 0.34515100356405926,
      "grad_norm": 1.324702501296997,
      "learning_rate": 2.8071374717265646e-05,
      "loss": 1.086,
      "step": 1380
    },
    {
      "epoch": 0.34765209779278433,
      "grad_norm": 1.5027506351470947,
      "learning_rate": 2.8056295551646142e-05,
      "loss": 1.0506,
      "step": 1390
    },
    {
      "epoch": 0.3501531920215094,
      "grad_norm": 1.7635719776153564,
      "learning_rate": 2.8041216386026642e-05,
      "loss": 1.1336,
      "step": 1400
    },
    {
      "epoch": 0.3526542862502345,
      "grad_norm": 1.7539360523223877,
      "learning_rate": 2.8026137220407138e-05,
      "loss": 1.1023,
      "step": 1410
    },
    {
      "epoch": 0.35515538047895956,
      "grad_norm": 1.3364341259002686,
      "learning_rate": 2.8011058054787638e-05,
      "loss": 1.1762,
      "step": 1420
    },
    {
      "epoch": 0.35765647470768464,
      "grad_norm": 1.6095643043518066,
      "learning_rate": 2.799597888916813e-05,
      "loss": 1.1276,
      "step": 1430
    },
    {
      "epoch": 0.36015756893640966,
      "grad_norm": 1.4259562492370605,
      "learning_rate": 2.798089972354863e-05,
      "loss": 1.0932,
      "step": 1440
    },
    {
      "epoch": 0.36265866316513473,
      "grad_norm": 2.007652759552002,
      "learning_rate": 2.796582055792913e-05,
      "loss": 1.1567,
      "step": 1450
    },
    {
      "epoch": 0.3651597573938598,
      "grad_norm": 2.2532260417938232,
      "learning_rate": 2.7950741392309627e-05,
      "loss": 1.1864,
      "step": 1460
    },
    {
      "epoch": 0.3676608516225849,
      "grad_norm": 1.900686502456665,
      "learning_rate": 2.7935662226690124e-05,
      "loss": 1.0776,
      "step": 1470
    },
    {
      "epoch": 0.37016194585130996,
      "grad_norm": 2.0438308715820312,
      "learning_rate": 2.792058306107062e-05,
      "loss": 1.1588,
      "step": 1480
    },
    {
      "epoch": 0.37266304008003504,
      "grad_norm": 1.5802737474441528,
      "learning_rate": 2.790550389545112e-05,
      "loss": 1.1422,
      "step": 1490
    },
    {
      "epoch": 0.37516413430876006,
      "grad_norm": 1.577536702156067,
      "learning_rate": 2.7890424729831616e-05,
      "loss": 1.1142,
      "step": 1500
    },
    {
      "epoch": 0.37766522853748513,
      "grad_norm": 1.5185354948043823,
      "learning_rate": 2.7875345564212113e-05,
      "loss": 1.1663,
      "step": 1510
    },
    {
      "epoch": 0.3801663227662102,
      "grad_norm": 2.102351188659668,
      "learning_rate": 2.7860266398592612e-05,
      "loss": 1.1499,
      "step": 1520
    },
    {
      "epoch": 0.3826674169949353,
      "grad_norm": 1.8270939588546753,
      "learning_rate": 2.7845187232973112e-05,
      "loss": 1.1631,
      "step": 1530
    },
    {
      "epoch": 0.38516851122366036,
      "grad_norm": 1.5392462015151978,
      "learning_rate": 2.7830108067353605e-05,
      "loss": 1.1471,
      "step": 1540
    },
    {
      "epoch": 0.38766960545238544,
      "grad_norm": 2.1667330265045166,
      "learning_rate": 2.7815028901734105e-05,
      "loss": 1.1192,
      "step": 1550
    },
    {
      "epoch": 0.3901706996811105,
      "grad_norm": 1.9712071418762207,
      "learning_rate": 2.77999497361146e-05,
      "loss": 1.1901,
      "step": 1560
    },
    {
      "epoch": 0.39267179390983553,
      "grad_norm": 1.4020981788635254,
      "learning_rate": 2.77848705704951e-05,
      "loss": 1.1474,
      "step": 1570
    },
    {
      "epoch": 0.3951728881385606,
      "grad_norm": 1.5248111486434937,
      "learning_rate": 2.7769791404875598e-05,
      "loss": 1.0992,
      "step": 1580
    },
    {
      "epoch": 0.3976739823672857,
      "grad_norm": 1.7587851285934448,
      "learning_rate": 2.7754712239256094e-05,
      "loss": 1.1092,
      "step": 1590
    },
    {
      "epoch": 0.40017507659601076,
      "grad_norm": 1.6108475923538208,
      "learning_rate": 2.7739633073636594e-05,
      "loss": 1.1179,
      "step": 1600
    },
    {
      "epoch": 0.40267617082473584,
      "grad_norm": 1.6008535623550415,
      "learning_rate": 2.772455390801709e-05,
      "loss": 1.0785,
      "step": 1610
    },
    {
      "epoch": 0.4051772650534609,
      "grad_norm": 1.549229621887207,
      "learning_rate": 2.7709474742397587e-05,
      "loss": 1.144,
      "step": 1620
    },
    {
      "epoch": 0.40767835928218593,
      "grad_norm": 1.4627406597137451,
      "learning_rate": 2.7694395576778087e-05,
      "loss": 1.0886,
      "step": 1630
    },
    {
      "epoch": 0.410179453510911,
      "grad_norm": 2.442307472229004,
      "learning_rate": 2.7679316411158583e-05,
      "loss": 1.1638,
      "step": 1640
    },
    {
      "epoch": 0.4126805477396361,
      "grad_norm": 2.4209492206573486,
      "learning_rate": 2.7664237245539083e-05,
      "loss": 1.0933,
      "step": 1650
    },
    {
      "epoch": 0.41518164196836116,
      "grad_norm": 1.6445653438568115,
      "learning_rate": 2.764915807991958e-05,
      "loss": 1.0779,
      "step": 1660
    },
    {
      "epoch": 0.41768273619708624,
      "grad_norm": 1.407103180885315,
      "learning_rate": 2.7634078914300076e-05,
      "loss": 1.1823,
      "step": 1670
    },
    {
      "epoch": 0.4201838304258113,
      "grad_norm": 1.6926558017730713,
      "learning_rate": 2.7618999748680575e-05,
      "loss": 1.0777,
      "step": 1680
    },
    {
      "epoch": 0.4226849246545364,
      "grad_norm": 1.3577938079833984,
      "learning_rate": 2.760392058306107e-05,
      "loss": 1.182,
      "step": 1690
    },
    {
      "epoch": 0.4251860188832614,
      "grad_norm": 1.375274658203125,
      "learning_rate": 2.7588841417441568e-05,
      "loss": 1.1182,
      "step": 1700
    },
    {
      "epoch": 0.4276871131119865,
      "grad_norm": 1.4472147226333618,
      "learning_rate": 2.7573762251822068e-05,
      "loss": 1.1946,
      "step": 1710
    },
    {
      "epoch": 0.43018820734071156,
      "grad_norm": 2.195971727371216,
      "learning_rate": 2.7558683086202564e-05,
      "loss": 1.1208,
      "step": 1720
    },
    {
      "epoch": 0.43268930156943664,
      "grad_norm": 1.6491434574127197,
      "learning_rate": 2.754360392058306e-05,
      "loss": 1.0549,
      "step": 1730
    },
    {
      "epoch": 0.4351903957981617,
      "grad_norm": 1.3535946607589722,
      "learning_rate": 2.7528524754963557e-05,
      "loss": 1.1232,
      "step": 1740
    },
    {
      "epoch": 0.4376914900268868,
      "grad_norm": 1.667986273765564,
      "learning_rate": 2.7513445589344057e-05,
      "loss": 1.1493,
      "step": 1750
    },
    {
      "epoch": 0.4401925842556118,
      "grad_norm": 1.3825726509094238,
      "learning_rate": 2.7498366423724557e-05,
      "loss": 1.1567,
      "step": 1760
    },
    {
      "epoch": 0.4426936784843369,
      "grad_norm": 1.7095228433609009,
      "learning_rate": 2.748328725810505e-05,
      "loss": 1.1282,
      "step": 1770
    },
    {
      "epoch": 0.44519477271306196,
      "grad_norm": 1.233008623123169,
      "learning_rate": 2.746820809248555e-05,
      "loss": 1.1367,
      "step": 1780
    },
    {
      "epoch": 0.44769586694178704,
      "grad_norm": 1.294081687927246,
      "learning_rate": 2.745312892686605e-05,
      "loss": 1.0621,
      "step": 1790
    },
    {
      "epoch": 0.4501969611705121,
      "grad_norm": 1.5960073471069336,
      "learning_rate": 2.7438049761246546e-05,
      "loss": 1.0588,
      "step": 1800
    },
    {
      "epoch": 0.4526980553992372,
      "grad_norm": 1.5538126230239868,
      "learning_rate": 2.7422970595627042e-05,
      "loss": 1.0917,
      "step": 1810
    },
    {
      "epoch": 0.4551991496279622,
      "grad_norm": 1.2372978925704956,
      "learning_rate": 2.740789143000754e-05,
      "loss": 1.1663,
      "step": 1820
    },
    {
      "epoch": 0.4577002438566873,
      "grad_norm": 1.169033169746399,
      "learning_rate": 2.739281226438804e-05,
      "loss": 1.1138,
      "step": 1830
    },
    {
      "epoch": 0.46020133808541236,
      "grad_norm": 1.2943665981292725,
      "learning_rate": 2.737773309876854e-05,
      "loss": 1.1538,
      "step": 1840
    },
    {
      "epoch": 0.46270243231413744,
      "grad_norm": 1.334973931312561,
      "learning_rate": 2.736265393314903e-05,
      "loss": 1.1681,
      "step": 1850
    },
    {
      "epoch": 0.4652035265428625,
      "grad_norm": 1.4643182754516602,
      "learning_rate": 2.734757476752953e-05,
      "loss": 1.1253,
      "step": 1860
    },
    {
      "epoch": 0.4677046207715876,
      "grad_norm": 1.2854238748550415,
      "learning_rate": 2.7332495601910028e-05,
      "loss": 1.1585,
      "step": 1870
    },
    {
      "epoch": 0.47020571500031266,
      "grad_norm": 1.8386867046356201,
      "learning_rate": 2.7317416436290524e-05,
      "loss": 1.207,
      "step": 1880
    },
    {
      "epoch": 0.4727068092290377,
      "grad_norm": 1.5964794158935547,
      "learning_rate": 2.7302337270671024e-05,
      "loss": 1.0089,
      "step": 1890
    },
    {
      "epoch": 0.47520790345776276,
      "grad_norm": 2.266140937805176,
      "learning_rate": 2.728725810505152e-05,
      "loss": 1.0594,
      "step": 1900
    },
    {
      "epoch": 0.47770899768648784,
      "grad_norm": 1.8675427436828613,
      "learning_rate": 2.727217893943202e-05,
      "loss": 1.0805,
      "step": 1910
    },
    {
      "epoch": 0.4802100919152129,
      "grad_norm": 1.6459556818008423,
      "learning_rate": 2.7257099773812517e-05,
      "loss": 1.1319,
      "step": 1920
    },
    {
      "epoch": 0.482711186143938,
      "grad_norm": 1.835208535194397,
      "learning_rate": 2.7242020608193013e-05,
      "loss": 1.142,
      "step": 1930
    },
    {
      "epoch": 0.48521228037266306,
      "grad_norm": 1.2962068319320679,
      "learning_rate": 2.7226941442573513e-05,
      "loss": 1.0923,
      "step": 1940
    },
    {
      "epoch": 0.4877133746013881,
      "grad_norm": 2.441880941390991,
      "learning_rate": 2.721186227695401e-05,
      "loss": 1.0692,
      "step": 1950
    },
    {
      "epoch": 0.49021446883011316,
      "grad_norm": 1.3872227668762207,
      "learning_rate": 2.7196783111334506e-05,
      "loss": 1.1135,
      "step": 1960
    },
    {
      "epoch": 0.49271556305883824,
      "grad_norm": 1.9595210552215576,
      "learning_rate": 2.7181703945715005e-05,
      "loss": 1.1152,
      "step": 1970
    },
    {
      "epoch": 0.4952166572875633,
      "grad_norm": 2.2024424076080322,
      "learning_rate": 2.7166624780095502e-05,
      "loss": 1.1642,
      "step": 1980
    },
    {
      "epoch": 0.4977177515162884,
      "grad_norm": 1.9074814319610596,
      "learning_rate": 2.7151545614476e-05,
      "loss": 1.1327,
      "step": 1990
    },
    {
      "epoch": 0.5002188457450134,
      "grad_norm": 1.5400612354278564,
      "learning_rate": 2.7136466448856495e-05,
      "loss": 1.1187,
      "step": 2000
    },
    {
      "epoch": 0.5027199399737385,
      "grad_norm": 1.373790979385376,
      "learning_rate": 2.7121387283236995e-05,
      "loss": 1.1076,
      "step": 2010
    },
    {
      "epoch": 0.5052210342024636,
      "grad_norm": 1.5109164714813232,
      "learning_rate": 2.7106308117617494e-05,
      "loss": 1.1137,
      "step": 2020
    },
    {
      "epoch": 0.5077221284311887,
      "grad_norm": 1.4427807331085205,
      "learning_rate": 2.709122895199799e-05,
      "loss": 1.0768,
      "step": 2030
    },
    {
      "epoch": 0.5102232226599137,
      "grad_norm": 1.9982526302337646,
      "learning_rate": 2.7076149786378487e-05,
      "loss": 1.1029,
      "step": 2040
    },
    {
      "epoch": 0.5127243168886387,
      "grad_norm": 1.7481822967529297,
      "learning_rate": 2.7061070620758987e-05,
      "loss": 1.1008,
      "step": 2050
    },
    {
      "epoch": 0.5152254111173639,
      "grad_norm": 2.2379846572875977,
      "learning_rate": 2.7045991455139483e-05,
      "loss": 1.0592,
      "step": 2060
    },
    {
      "epoch": 0.5177265053460889,
      "grad_norm": 1.7269457578659058,
      "learning_rate": 2.703091228951998e-05,
      "loss": 1.0727,
      "step": 2070
    },
    {
      "epoch": 0.520227599574814,
      "grad_norm": 1.651289701461792,
      "learning_rate": 2.7015833123900476e-05,
      "loss": 1.1528,
      "step": 2080
    },
    {
      "epoch": 0.522728693803539,
      "grad_norm": 1.8218990564346313,
      "learning_rate": 2.7000753958280976e-05,
      "loss": 1.0802,
      "step": 2090
    },
    {
      "epoch": 0.5252297880322642,
      "grad_norm": 2.2749361991882324,
      "learning_rate": 2.6985674792661476e-05,
      "loss": 1.0455,
      "step": 2100
    },
    {
      "epoch": 0.5277308822609892,
      "grad_norm": 1.9649810791015625,
      "learning_rate": 2.697059562704197e-05,
      "loss": 1.1251,
      "step": 2110
    },
    {
      "epoch": 0.5302319764897142,
      "grad_norm": 1.8836174011230469,
      "learning_rate": 2.695551646142247e-05,
      "loss": 1.1044,
      "step": 2120
    },
    {
      "epoch": 0.5327330707184393,
      "grad_norm": 1.2562686204910278,
      "learning_rate": 2.6940437295802965e-05,
      "loss": 1.1455,
      "step": 2130
    },
    {
      "epoch": 0.5352341649471644,
      "grad_norm": 2.7410690784454346,
      "learning_rate": 2.6925358130183465e-05,
      "loss": 1.2439,
      "step": 2140
    },
    {
      "epoch": 0.5377352591758895,
      "grad_norm": 1.6581770181655884,
      "learning_rate": 2.691027896456396e-05,
      "loss": 1.1371,
      "step": 2150
    },
    {
      "epoch": 0.5402363534046145,
      "grad_norm": 1.3732805252075195,
      "learning_rate": 2.6895199798944458e-05,
      "loss": 1.1159,
      "step": 2160
    },
    {
      "epoch": 0.5427374476333395,
      "grad_norm": 1.555700421333313,
      "learning_rate": 2.6880120633324958e-05,
      "loss": 1.1185,
      "step": 2170
    },
    {
      "epoch": 0.5452385418620647,
      "grad_norm": 1.8633759021759033,
      "learning_rate": 2.6865041467705454e-05,
      "loss": 1.1546,
      "step": 2180
    },
    {
      "epoch": 0.5477396360907897,
      "grad_norm": 1.6936981678009033,
      "learning_rate": 2.684996230208595e-05,
      "loss": 1.1339,
      "step": 2190
    },
    {
      "epoch": 0.5502407303195148,
      "grad_norm": 1.7073029279708862,
      "learning_rate": 2.683488313646645e-05,
      "loss": 1.1058,
      "step": 2200
    },
    {
      "epoch": 0.5527418245482398,
      "grad_norm": 1.3626681566238403,
      "learning_rate": 2.6819803970846947e-05,
      "loss": 1.235,
      "step": 2210
    },
    {
      "epoch": 0.555242918776965,
      "grad_norm": 1.7281533479690552,
      "learning_rate": 2.6804724805227446e-05,
      "loss": 1.1312,
      "step": 2220
    },
    {
      "epoch": 0.55774401300569,
      "grad_norm": 1.8503175973892212,
      "learning_rate": 2.6789645639607943e-05,
      "loss": 1.1532,
      "step": 2230
    },
    {
      "epoch": 0.560245107234415,
      "grad_norm": 1.5095576047897339,
      "learning_rate": 2.677456647398844e-05,
      "loss": 1.1763,
      "step": 2240
    },
    {
      "epoch": 0.5627462014631401,
      "grad_norm": 1.4875584840774536,
      "learning_rate": 2.675948730836894e-05,
      "loss": 1.0855,
      "step": 2250
    },
    {
      "epoch": 0.5652472956918652,
      "grad_norm": 1.495204210281372,
      "learning_rate": 2.6744408142749432e-05,
      "loss": 1.121,
      "step": 2260
    },
    {
      "epoch": 0.5677483899205903,
      "grad_norm": 1.402991533279419,
      "learning_rate": 2.6729328977129932e-05,
      "loss": 1.1245,
      "step": 2270
    },
    {
      "epoch": 0.5702494841493153,
      "grad_norm": 1.5486454963684082,
      "learning_rate": 2.6714249811510432e-05,
      "loss": 1.0929,
      "step": 2280
    },
    {
      "epoch": 0.5727505783780404,
      "grad_norm": 2.3199329376220703,
      "learning_rate": 2.6699170645890928e-05,
      "loss": 1.114,
      "step": 2290
    },
    {
      "epoch": 0.5752516726067655,
      "grad_norm": 1.7906527519226074,
      "learning_rate": 2.6684091480271425e-05,
      "loss": 1.1451,
      "step": 2300
    },
    {
      "epoch": 0.5777527668354905,
      "grad_norm": 2.003047466278076,
      "learning_rate": 2.6669012314651924e-05,
      "loss": 1.1293,
      "step": 2310
    },
    {
      "epoch": 0.5802538610642156,
      "grad_norm": 1.871260404586792,
      "learning_rate": 2.665393314903242e-05,
      "loss": 1.0818,
      "step": 2320
    },
    {
      "epoch": 0.5827549552929406,
      "grad_norm": 1.5403863191604614,
      "learning_rate": 2.663885398341292e-05,
      "loss": 1.1394,
      "step": 2330
    },
    {
      "epoch": 0.5852560495216658,
      "grad_norm": 1.5707935094833374,
      "learning_rate": 2.6623774817793414e-05,
      "loss": 1.1362,
      "step": 2340
    },
    {
      "epoch": 0.5877571437503908,
      "grad_norm": 1.5281596183776855,
      "learning_rate": 2.6608695652173913e-05,
      "loss": 1.1173,
      "step": 2350
    },
    {
      "epoch": 0.5902582379791159,
      "grad_norm": 1.785630226135254,
      "learning_rate": 2.6593616486554413e-05,
      "loss": 1.1409,
      "step": 2360
    },
    {
      "epoch": 0.5927593322078409,
      "grad_norm": 1.5636978149414062,
      "learning_rate": 2.657853732093491e-05,
      "loss": 1.1607,
      "step": 2370
    },
    {
      "epoch": 0.595260426436566,
      "grad_norm": 1.6163196563720703,
      "learning_rate": 2.6563458155315406e-05,
      "loss": 1.1493,
      "step": 2380
    },
    {
      "epoch": 0.5977615206652911,
      "grad_norm": 1.353387475013733,
      "learning_rate": 2.6548378989695902e-05,
      "loss": 1.1098,
      "step": 2390
    },
    {
      "epoch": 0.6002626148940161,
      "grad_norm": 1.7143573760986328,
      "learning_rate": 2.6533299824076402e-05,
      "loss": 1.132,
      "step": 2400
    },
    {
      "epoch": 0.6027637091227412,
      "grad_norm": 2.5345115661621094,
      "learning_rate": 2.6518220658456902e-05,
      "loss": 1.1106,
      "step": 2410
    },
    {
      "epoch": 0.6052648033514663,
      "grad_norm": 2.271721124649048,
      "learning_rate": 2.6503141492837395e-05,
      "loss": 1.0935,
      "step": 2420
    },
    {
      "epoch": 0.6077658975801913,
      "grad_norm": 1.4332129955291748,
      "learning_rate": 2.6488062327217895e-05,
      "loss": 1.1319,
      "step": 2430
    },
    {
      "epoch": 0.6102669918089164,
      "grad_norm": 1.697709560394287,
      "learning_rate": 2.647298316159839e-05,
      "loss": 1.15,
      "step": 2440
    },
    {
      "epoch": 0.6127680860376414,
      "grad_norm": 1.5763161182403564,
      "learning_rate": 2.6457903995978888e-05,
      "loss": 1.1744,
      "step": 2450
    },
    {
      "epoch": 0.6152691802663666,
      "grad_norm": 1.4565751552581787,
      "learning_rate": 2.6442824830359388e-05,
      "loss": 1.1783,
      "step": 2460
    },
    {
      "epoch": 0.6177702744950916,
      "grad_norm": 1.7165358066558838,
      "learning_rate": 2.6427745664739884e-05,
      "loss": 1.1294,
      "step": 2470
    },
    {
      "epoch": 0.6202713687238167,
      "grad_norm": 1.952874779701233,
      "learning_rate": 2.6412666499120384e-05,
      "loss": 1.0596,
      "step": 2480
    },
    {
      "epoch": 0.6227724629525417,
      "grad_norm": 1.6039042472839355,
      "learning_rate": 2.639758733350088e-05,
      "loss": 1.0614,
      "step": 2490
    },
    {
      "epoch": 0.6252735571812668,
      "grad_norm": 1.9868437051773071,
      "learning_rate": 2.6382508167881377e-05,
      "loss": 1.1197,
      "step": 2500
    },
    {
      "epoch": 0.6277746514099919,
      "grad_norm": 1.8012776374816895,
      "learning_rate": 2.6367429002261876e-05,
      "loss": 1.1456,
      "step": 2510
    },
    {
      "epoch": 0.6302757456387169,
      "grad_norm": 1.5462411642074585,
      "learning_rate": 2.6352349836642373e-05,
      "loss": 1.0588,
      "step": 2520
    },
    {
      "epoch": 0.632776839867442,
      "grad_norm": 1.6498494148254395,
      "learning_rate": 2.633727067102287e-05,
      "loss": 1.1281,
      "step": 2530
    },
    {
      "epoch": 0.6352779340961671,
      "grad_norm": 1.6844325065612793,
      "learning_rate": 2.632219150540337e-05,
      "loss": 1.161,
      "step": 2540
    },
    {
      "epoch": 0.6377790283248922,
      "grad_norm": 1.5723176002502441,
      "learning_rate": 2.6307112339783866e-05,
      "loss": 1.0871,
      "step": 2550
    },
    {
      "epoch": 0.6402801225536172,
      "grad_norm": 1.7596272230148315,
      "learning_rate": 2.6292033174164365e-05,
      "loss": 1.1341,
      "step": 2560
    },
    {
      "epoch": 0.6427812167823422,
      "grad_norm": 1.6088130474090576,
      "learning_rate": 2.6276954008544862e-05,
      "loss": 1.0677,
      "step": 2570
    },
    {
      "epoch": 0.6452823110110674,
      "grad_norm": 1.5572032928466797,
      "learning_rate": 2.6261874842925358e-05,
      "loss": 1.1333,
      "step": 2580
    },
    {
      "epoch": 0.6477834052397924,
      "grad_norm": 1.7860950231552124,
      "learning_rate": 2.6246795677305858e-05,
      "loss": 1.1856,
      "step": 2590
    },
    {
      "epoch": 0.6502844994685175,
      "grad_norm": 1.955729365348816,
      "learning_rate": 2.6231716511686354e-05,
      "loss": 1.1292,
      "step": 2600
    },
    {
      "epoch": 0.6527855936972425,
      "grad_norm": 2.053138494491577,
      "learning_rate": 2.621663734606685e-05,
      "loss": 1.1165,
      "step": 2610
    },
    {
      "epoch": 0.6552866879259676,
      "grad_norm": 1.5504590272903442,
      "learning_rate": 2.620155818044735e-05,
      "loss": 1.1272,
      "step": 2620
    },
    {
      "epoch": 0.6577877821546927,
      "grad_norm": 2.677643299102783,
      "learning_rate": 2.6186479014827847e-05,
      "loss": 1.1245,
      "step": 2630
    },
    {
      "epoch": 0.6602888763834177,
      "grad_norm": 2.3277745246887207,
      "learning_rate": 2.6171399849208343e-05,
      "loss": 1.085,
      "step": 2640
    },
    {
      "epoch": 0.6627899706121428,
      "grad_norm": 1.4406393766403198,
      "learning_rate": 2.615632068358884e-05,
      "loss": 1.1587,
      "step": 2650
    },
    {
      "epoch": 0.6652910648408679,
      "grad_norm": 1.2045598030090332,
      "learning_rate": 2.614124151796934e-05,
      "loss": 1.1717,
      "step": 2660
    },
    {
      "epoch": 0.667792159069593,
      "grad_norm": 2.0063982009887695,
      "learning_rate": 2.612616235234984e-05,
      "loss": 1.1539,
      "step": 2670
    },
    {
      "epoch": 0.670293253298318,
      "grad_norm": 1.9657390117645264,
      "learning_rate": 2.6111083186730333e-05,
      "loss": 1.1804,
      "step": 2680
    },
    {
      "epoch": 0.672794347527043,
      "grad_norm": 2.3780500888824463,
      "learning_rate": 2.6096004021110832e-05,
      "loss": 1.1184,
      "step": 2690
    },
    {
      "epoch": 0.6752954417557682,
      "grad_norm": 1.5939441919326782,
      "learning_rate": 2.608092485549133e-05,
      "loss": 1.1834,
      "step": 2700
    },
    {
      "epoch": 0.6777965359844932,
      "grad_norm": 2.371412515640259,
      "learning_rate": 2.606584568987183e-05,
      "loss": 1.1486,
      "step": 2710
    },
    {
      "epoch": 0.6802976302132183,
      "grad_norm": 1.9981110095977783,
      "learning_rate": 2.6050766524252325e-05,
      "loss": 1.0593,
      "step": 2720
    },
    {
      "epoch": 0.6827987244419433,
      "grad_norm": 2.064995288848877,
      "learning_rate": 2.603568735863282e-05,
      "loss": 1.0593,
      "step": 2730
    },
    {
      "epoch": 0.6852998186706685,
      "grad_norm": 1.566508412361145,
      "learning_rate": 2.602060819301332e-05,
      "loss": 1.1283,
      "step": 2740
    },
    {
      "epoch": 0.6878009128993935,
      "grad_norm": 1.7244716882705688,
      "learning_rate": 2.600552902739382e-05,
      "loss": 1.1239,
      "step": 2750
    },
    {
      "epoch": 0.6903020071281185,
      "grad_norm": 1.4732736349105835,
      "learning_rate": 2.5990449861774314e-05,
      "loss": 1.1736,
      "step": 2760
    },
    {
      "epoch": 0.6928031013568436,
      "grad_norm": 2.060614824295044,
      "learning_rate": 2.5975370696154814e-05,
      "loss": 1.191,
      "step": 2770
    },
    {
      "epoch": 0.6953041955855687,
      "grad_norm": 1.4663667678833008,
      "learning_rate": 2.596029153053531e-05,
      "loss": 1.0891,
      "step": 2780
    },
    {
      "epoch": 0.6978052898142938,
      "grad_norm": 1.7121244668960571,
      "learning_rate": 2.594521236491581e-05,
      "loss": 1.1585,
      "step": 2790
    },
    {
      "epoch": 0.7003063840430188,
      "grad_norm": 1.3322365283966064,
      "learning_rate": 2.5930133199296306e-05,
      "loss": 1.0462,
      "step": 2800
    },
    {
      "epoch": 0.7028074782717438,
      "grad_norm": 2.080899238586426,
      "learning_rate": 2.5915054033676803e-05,
      "loss": 1.0688,
      "step": 2810
    },
    {
      "epoch": 0.705308572500469,
      "grad_norm": 2.4984984397888184,
      "learning_rate": 2.5899974868057303e-05,
      "loss": 1.1263,
      "step": 2820
    },
    {
      "epoch": 0.707809666729194,
      "grad_norm": 2.6952297687530518,
      "learning_rate": 2.58848957024378e-05,
      "loss": 1.0685,
      "step": 2830
    },
    {
      "epoch": 0.7103107609579191,
      "grad_norm": 1.6698102951049805,
      "learning_rate": 2.5869816536818296e-05,
      "loss": 1.1186,
      "step": 2840
    },
    {
      "epoch": 0.7128118551866441,
      "grad_norm": 1.474769115447998,
      "learning_rate": 2.5854737371198795e-05,
      "loss": 1.1436,
      "step": 2850
    },
    {
      "epoch": 0.7153129494153693,
      "grad_norm": 2.211724042892456,
      "learning_rate": 2.5839658205579292e-05,
      "loss": 1.1165,
      "step": 2860
    },
    {
      "epoch": 0.7178140436440943,
      "grad_norm": 1.792270541191101,
      "learning_rate": 2.5824579039959788e-05,
      "loss": 1.0869,
      "step": 2870
    },
    {
      "epoch": 0.7203151378728193,
      "grad_norm": 1.3559179306030273,
      "learning_rate": 2.5809499874340288e-05,
      "loss": 1.0525,
      "step": 2880
    },
    {
      "epoch": 0.7228162321015444,
      "grad_norm": 2.0150065422058105,
      "learning_rate": 2.5794420708720784e-05,
      "loss": 1.1393,
      "step": 2890
    },
    {
      "epoch": 0.7253173263302695,
      "grad_norm": 1.549986720085144,
      "learning_rate": 2.5779341543101284e-05,
      "loss": 1.0806,
      "step": 2900
    },
    {
      "epoch": 0.7278184205589946,
      "grad_norm": 2.1467413902282715,
      "learning_rate": 2.5764262377481777e-05,
      "loss": 1.132,
      "step": 2910
    },
    {
      "epoch": 0.7303195147877196,
      "grad_norm": 1.7393152713775635,
      "learning_rate": 2.5749183211862277e-05,
      "loss": 1.1087,
      "step": 2920
    },
    {
      "epoch": 0.7328206090164447,
      "grad_norm": 1.6054960489273071,
      "learning_rate": 2.5734104046242777e-05,
      "loss": 1.084,
      "step": 2930
    },
    {
      "epoch": 0.7353217032451698,
      "grad_norm": 1.568809986114502,
      "learning_rate": 2.5719024880623273e-05,
      "loss": 1.0561,
      "step": 2940
    },
    {
      "epoch": 0.7378227974738948,
      "grad_norm": 1.5661070346832275,
      "learning_rate": 2.570394571500377e-05,
      "loss": 1.1614,
      "step": 2950
    },
    {
      "epoch": 0.7403238917026199,
      "grad_norm": 1.869737982749939,
      "learning_rate": 2.5688866549384266e-05,
      "loss": 1.083,
      "step": 2960
    },
    {
      "epoch": 0.7428249859313449,
      "grad_norm": 1.5745176076889038,
      "learning_rate": 2.5673787383764766e-05,
      "loss": 1.1095,
      "step": 2970
    },
    {
      "epoch": 0.7453260801600701,
      "grad_norm": 2.226139545440674,
      "learning_rate": 2.5658708218145266e-05,
      "loss": 1.068,
      "step": 2980
    },
    {
      "epoch": 0.7478271743887951,
      "grad_norm": 1.8613172769546509,
      "learning_rate": 2.564362905252576e-05,
      "loss": 1.0994,
      "step": 2990
    },
    {
      "epoch": 0.7503282686175201,
      "grad_norm": 2.212529420852661,
      "learning_rate": 2.562854988690626e-05,
      "loss": 1.0625,
      "step": 3000
    },
    {
      "epoch": 0.7528293628462452,
      "grad_norm": 1.5310980081558228,
      "learning_rate": 2.561347072128676e-05,
      "loss": 1.191,
      "step": 3010
    },
    {
      "epoch": 0.7553304570749703,
      "grad_norm": 2.180192708969116,
      "learning_rate": 2.559839155566725e-05,
      "loss": 1.1288,
      "step": 3020
    },
    {
      "epoch": 0.7578315513036954,
      "grad_norm": 2.444472312927246,
      "learning_rate": 2.558331239004775e-05,
      "loss": 1.0843,
      "step": 3030
    },
    {
      "epoch": 0.7603326455324204,
      "grad_norm": 2.025624990463257,
      "learning_rate": 2.5568233224428248e-05,
      "loss": 1.0475,
      "step": 3040
    },
    {
      "epoch": 0.7628337397611455,
      "grad_norm": 1.4861489534378052,
      "learning_rate": 2.5553154058808747e-05,
      "loss": 1.1505,
      "step": 3050
    },
    {
      "epoch": 0.7653348339898706,
      "grad_norm": 1.4204511642456055,
      "learning_rate": 2.5538074893189244e-05,
      "loss": 1.0998,
      "step": 3060
    },
    {
      "epoch": 0.7678359282185956,
      "grad_norm": 2.248335123062134,
      "learning_rate": 2.552299572756974e-05,
      "loss": 1.1621,
      "step": 3070
    },
    {
      "epoch": 0.7703370224473207,
      "grad_norm": 2.026792526245117,
      "learning_rate": 2.550791656195024e-05,
      "loss": 1.1053,
      "step": 3080
    },
    {
      "epoch": 0.7728381166760457,
      "grad_norm": 1.78585684299469,
      "learning_rate": 2.5492837396330737e-05,
      "loss": 1.1059,
      "step": 3090
    },
    {
      "epoch": 0.7753392109047709,
      "grad_norm": 1.3583879470825195,
      "learning_rate": 2.5477758230711233e-05,
      "loss": 1.084,
      "step": 3100
    },
    {
      "epoch": 0.7778403051334959,
      "grad_norm": 1.7980753183364868,
      "learning_rate": 2.5462679065091733e-05,
      "loss": 1.0607,
      "step": 3110
    },
    {
      "epoch": 0.780341399362221,
      "grad_norm": 1.3996378183364868,
      "learning_rate": 2.544759989947223e-05,
      "loss": 1.0969,
      "step": 3120
    },
    {
      "epoch": 0.782842493590946,
      "grad_norm": 1.5342302322387695,
      "learning_rate": 2.543252073385273e-05,
      "loss": 1.0501,
      "step": 3130
    },
    {
      "epoch": 0.7853435878196711,
      "grad_norm": 1.7545788288116455,
      "learning_rate": 2.5417441568233225e-05,
      "loss": 1.0479,
      "step": 3140
    },
    {
      "epoch": 0.7878446820483962,
      "grad_norm": 1.3334664106369019,
      "learning_rate": 2.5402362402613722e-05,
      "loss": 1.073,
      "step": 3150
    },
    {
      "epoch": 0.7903457762771212,
      "grad_norm": 2.01153564453125,
      "learning_rate": 2.538728323699422e-05,
      "loss": 1.1261,
      "step": 3160
    },
    {
      "epoch": 0.7928468705058463,
      "grad_norm": 1.551644206047058,
      "learning_rate": 2.5372204071374718e-05,
      "loss": 1.0962,
      "step": 3170
    },
    {
      "epoch": 0.7953479647345714,
      "grad_norm": 1.6434050798416138,
      "learning_rate": 2.5357124905755214e-05,
      "loss": 1.0977,
      "step": 3180
    },
    {
      "epoch": 0.7978490589632964,
      "grad_norm": 1.757383108139038,
      "learning_rate": 2.5342045740135714e-05,
      "loss": 1.0976,
      "step": 3190
    },
    {
      "epoch": 0.8003501531920215,
      "grad_norm": 1.8683820962905884,
      "learning_rate": 2.532696657451621e-05,
      "loss": 1.1471,
      "step": 3200
    },
    {
      "epoch": 0.8028512474207465,
      "grad_norm": 2.1904220581054688,
      "learning_rate": 2.5311887408896707e-05,
      "loss": 1.1039,
      "step": 3210
    },
    {
      "epoch": 0.8053523416494717,
      "grad_norm": 1.6726408004760742,
      "learning_rate": 2.5296808243277204e-05,
      "loss": 1.1434,
      "step": 3220
    },
    {
      "epoch": 0.8078534358781967,
      "grad_norm": 1.7765958309173584,
      "learning_rate": 2.5281729077657703e-05,
      "loss": 1.1406,
      "step": 3230
    },
    {
      "epoch": 0.8103545301069218,
      "grad_norm": 1.8495137691497803,
      "learning_rate": 2.5266649912038203e-05,
      "loss": 1.121,
      "step": 3240
    },
    {
      "epoch": 0.8128556243356468,
      "grad_norm": 1.5036654472351074,
      "learning_rate": 2.5251570746418696e-05,
      "loss": 1.0898,
      "step": 3250
    },
    {
      "epoch": 0.8153567185643719,
      "grad_norm": 2.049516201019287,
      "learning_rate": 2.5236491580799196e-05,
      "loss": 1.1261,
      "step": 3260
    },
    {
      "epoch": 0.817857812793097,
      "grad_norm": 1.8576658964157104,
      "learning_rate": 2.5221412415179696e-05,
      "loss": 1.064,
      "step": 3270
    },
    {
      "epoch": 0.820358907021822,
      "grad_norm": 1.4904319047927856,
      "learning_rate": 2.5206333249560192e-05,
      "loss": 1.0696,
      "step": 3280
    },
    {
      "epoch": 0.8228600012505471,
      "grad_norm": 2.1224617958068848,
      "learning_rate": 2.519125408394069e-05,
      "loss": 1.0928,
      "step": 3290
    },
    {
      "epoch": 0.8253610954792722,
      "grad_norm": 2.213702917098999,
      "learning_rate": 2.5176174918321185e-05,
      "loss": 1.1229,
      "step": 3300
    },
    {
      "epoch": 0.8278621897079973,
      "grad_norm": 1.4286963939666748,
      "learning_rate": 2.5161095752701685e-05,
      "loss": 1.1245,
      "step": 3310
    },
    {
      "epoch": 0.8303632839367223,
      "grad_norm": 1.6621419191360474,
      "learning_rate": 2.5146016587082185e-05,
      "loss": 1.1292,
      "step": 3320
    },
    {
      "epoch": 0.8328643781654473,
      "grad_norm": 1.707265019416809,
      "learning_rate": 2.5130937421462678e-05,
      "loss": 1.1339,
      "step": 3330
    },
    {
      "epoch": 0.8353654723941725,
      "grad_norm": 1.4314159154891968,
      "learning_rate": 2.5115858255843178e-05,
      "loss": 1.0626,
      "step": 3340
    },
    {
      "epoch": 0.8378665666228975,
      "grad_norm": 2.406371593475342,
      "learning_rate": 2.5100779090223674e-05,
      "loss": 1.1261,
      "step": 3350
    },
    {
      "epoch": 0.8403676608516226,
      "grad_norm": 2.3257768154144287,
      "learning_rate": 2.5085699924604174e-05,
      "loss": 1.0948,
      "step": 3360
    },
    {
      "epoch": 0.8428687550803476,
      "grad_norm": 1.9904453754425049,
      "learning_rate": 2.507062075898467e-05,
      "loss": 1.1389,
      "step": 3370
    },
    {
      "epoch": 0.8453698493090728,
      "grad_norm": 1.434919834136963,
      "learning_rate": 2.5055541593365167e-05,
      "loss": 1.1723,
      "step": 3380
    },
    {
      "epoch": 0.8478709435377978,
      "grad_norm": 1.6850807666778564,
      "learning_rate": 2.5040462427745666e-05,
      "loss": 1.1442,
      "step": 3390
    },
    {
      "epoch": 0.8503720377665228,
      "grad_norm": 2.0101089477539062,
      "learning_rate": 2.5025383262126163e-05,
      "loss": 1.0808,
      "step": 3400
    },
    {
      "epoch": 0.852873131995248,
      "grad_norm": 1.62128484249115,
      "learning_rate": 2.501030409650666e-05,
      "loss": 1.142,
      "step": 3410
    },
    {
      "epoch": 0.855374226223973,
      "grad_norm": 1.748645305633545,
      "learning_rate": 2.499522493088716e-05,
      "loss": 1.0478,
      "step": 3420
    },
    {
      "epoch": 0.8578753204526981,
      "grad_norm": 1.7883528470993042,
      "learning_rate": 2.4980145765267655e-05,
      "loss": 1.1432,
      "step": 3430
    },
    {
      "epoch": 0.8603764146814231,
      "grad_norm": 1.8001056909561157,
      "learning_rate": 2.4965066599648152e-05,
      "loss": 1.0289,
      "step": 3440
    },
    {
      "epoch": 0.8628775089101481,
      "grad_norm": 1.522335171699524,
      "learning_rate": 2.494998743402865e-05,
      "loss": 1.1392,
      "step": 3450
    },
    {
      "epoch": 0.8653786031388733,
      "grad_norm": 1.9244581460952759,
      "learning_rate": 2.4934908268409148e-05,
      "loss": 1.039,
      "step": 3460
    },
    {
      "epoch": 0.8678796973675983,
      "grad_norm": 1.6990773677825928,
      "learning_rate": 2.4919829102789648e-05,
      "loss": 1.1329,
      "step": 3470
    },
    {
      "epoch": 0.8703807915963234,
      "grad_norm": 2.194751262664795,
      "learning_rate": 2.490474993717014e-05,
      "loss": 1.0456,
      "step": 3480
    },
    {
      "epoch": 0.8728818858250484,
      "grad_norm": 2.0638644695281982,
      "learning_rate": 2.488967077155064e-05,
      "loss": 1.081,
      "step": 3490
    },
    {
      "epoch": 0.8753829800537736,
      "grad_norm": 1.4458279609680176,
      "learning_rate": 2.487459160593114e-05,
      "loss": 1.0838,
      "step": 3500
    },
    {
      "epoch": 0.8778840742824986,
      "grad_norm": 1.5226595401763916,
      "learning_rate": 2.4859512440311637e-05,
      "loss": 1.1242,
      "step": 3510
    },
    {
      "epoch": 0.8803851685112236,
      "grad_norm": 1.6489771604537964,
      "learning_rate": 2.4844433274692133e-05,
      "loss": 1.0799,
      "step": 3520
    },
    {
      "epoch": 0.8828862627399487,
      "grad_norm": 1.5780328512191772,
      "learning_rate": 2.4829354109072633e-05,
      "loss": 1.1313,
      "step": 3530
    },
    {
      "epoch": 0.8853873569686738,
      "grad_norm": 1.455593466758728,
      "learning_rate": 2.481427494345313e-05,
      "loss": 1.141,
      "step": 3540
    },
    {
      "epoch": 0.8878884511973989,
      "grad_norm": 1.8260585069656372,
      "learning_rate": 2.479919577783363e-05,
      "loss": 1.111,
      "step": 3550
    },
    {
      "epoch": 0.8903895454261239,
      "grad_norm": 2.034029722213745,
      "learning_rate": 2.4784116612214122e-05,
      "loss": 1.1643,
      "step": 3560
    },
    {
      "epoch": 0.892890639654849,
      "grad_norm": 2.279482126235962,
      "learning_rate": 2.4769037446594622e-05,
      "loss": 1.1294,
      "step": 3570
    },
    {
      "epoch": 0.8953917338835741,
      "grad_norm": 1.7593744993209839,
      "learning_rate": 2.4753958280975122e-05,
      "loss": 1.1803,
      "step": 3580
    },
    {
      "epoch": 0.8978928281122991,
      "grad_norm": 1.585546612739563,
      "learning_rate": 2.4738879115355615e-05,
      "loss": 1.0557,
      "step": 3590
    },
    {
      "epoch": 0.9003939223410242,
      "grad_norm": 1.609125018119812,
      "learning_rate": 2.4723799949736115e-05,
      "loss": 1.141,
      "step": 3600
    },
    {
      "epoch": 0.9028950165697492,
      "grad_norm": 2.708277463912964,
      "learning_rate": 2.470872078411661e-05,
      "loss": 1.1037,
      "step": 3610
    },
    {
      "epoch": 0.9053961107984744,
      "grad_norm": 1.566876769065857,
      "learning_rate": 2.469364161849711e-05,
      "loss": 1.1278,
      "step": 3620
    },
    {
      "epoch": 0.9078972050271994,
      "grad_norm": 1.9406766891479492,
      "learning_rate": 2.4678562452877608e-05,
      "loss": 1.0744,
      "step": 3630
    },
    {
      "epoch": 0.9103982992559244,
      "grad_norm": 1.7104383707046509,
      "learning_rate": 2.4663483287258104e-05,
      "loss": 1.0594,
      "step": 3640
    },
    {
      "epoch": 0.9128993934846495,
      "grad_norm": 1.4710326194763184,
      "learning_rate": 2.4648404121638604e-05,
      "loss": 1.0476,
      "step": 3650
    },
    {
      "epoch": 0.9154004877133746,
      "grad_norm": 1.4968916177749634,
      "learning_rate": 2.4633324956019104e-05,
      "loss": 1.1021,
      "step": 3660
    },
    {
      "epoch": 0.9179015819420997,
      "grad_norm": 2.677046060562134,
      "learning_rate": 2.4618245790399597e-05,
      "loss": 1.1151,
      "step": 3670
    },
    {
      "epoch": 0.9204026761708247,
      "grad_norm": 1.5659700632095337,
      "learning_rate": 2.4603166624780096e-05,
      "loss": 1.1215,
      "step": 3680
    },
    {
      "epoch": 0.9229037703995498,
      "grad_norm": 2.025851011276245,
      "learning_rate": 2.4588087459160593e-05,
      "loss": 1.1049,
      "step": 3690
    },
    {
      "epoch": 0.9254048646282749,
      "grad_norm": 1.8590985536575317,
      "learning_rate": 2.4573008293541093e-05,
      "loss": 1.0347,
      "step": 3700
    },
    {
      "epoch": 0.9279059588569999,
      "grad_norm": 1.4021273851394653,
      "learning_rate": 2.455792912792159e-05,
      "loss": 1.092,
      "step": 3710
    },
    {
      "epoch": 0.930407053085725,
      "grad_norm": 1.924744725227356,
      "learning_rate": 2.4542849962302085e-05,
      "loss": 1.0845,
      "step": 3720
    },
    {
      "epoch": 0.93290814731445,
      "grad_norm": 1.6326359510421753,
      "learning_rate": 2.4527770796682585e-05,
      "loss": 1.1394,
      "step": 3730
    },
    {
      "epoch": 0.9354092415431752,
      "grad_norm": 1.6210600137710571,
      "learning_rate": 2.4512691631063082e-05,
      "loss": 1.0465,
      "step": 3740
    },
    {
      "epoch": 0.9379103357719002,
      "grad_norm": 1.6661696434020996,
      "learning_rate": 2.4497612465443578e-05,
      "loss": 1.1525,
      "step": 3750
    },
    {
      "epoch": 0.9404114300006253,
      "grad_norm": 1.463739275932312,
      "learning_rate": 2.4482533299824078e-05,
      "loss": 1.0766,
      "step": 3760
    },
    {
      "epoch": 0.9429125242293503,
      "grad_norm": 1.4737056493759155,
      "learning_rate": 2.4467454134204574e-05,
      "loss": 1.0526,
      "step": 3770
    },
    {
      "epoch": 0.9454136184580754,
      "grad_norm": 1.6527843475341797,
      "learning_rate": 2.445237496858507e-05,
      "loss": 1.0537,
      "step": 3780
    },
    {
      "epoch": 0.9479147126868005,
      "grad_norm": 1.8099708557128906,
      "learning_rate": 2.443729580296557e-05,
      "loss": 1.0654,
      "step": 3790
    },
    {
      "epoch": 0.9504158069155255,
      "grad_norm": 1.6153182983398438,
      "learning_rate": 2.4422216637346067e-05,
      "loss": 1.0586,
      "step": 3800
    },
    {
      "epoch": 0.9529169011442506,
      "grad_norm": 1.8555574417114258,
      "learning_rate": 2.4407137471726567e-05,
      "loss": 1.0793,
      "step": 3810
    },
    {
      "epoch": 0.9554179953729757,
      "grad_norm": 1.5591671466827393,
      "learning_rate": 2.439205830610706e-05,
      "loss": 1.1427,
      "step": 3820
    },
    {
      "epoch": 0.9579190896017007,
      "grad_norm": 1.5412040948867798,
      "learning_rate": 2.437697914048756e-05,
      "loss": 1.1518,
      "step": 3830
    },
    {
      "epoch": 0.9604201838304258,
      "grad_norm": 1.6440671682357788,
      "learning_rate": 2.436189997486806e-05,
      "loss": 1.0271,
      "step": 3840
    },
    {
      "epoch": 0.9629212780591508,
      "grad_norm": 1.431597113609314,
      "learning_rate": 2.4346820809248556e-05,
      "loss": 1.1203,
      "step": 3850
    },
    {
      "epoch": 0.965422372287876,
      "grad_norm": 1.4590930938720703,
      "learning_rate": 2.4331741643629052e-05,
      "loss": 1.0763,
      "step": 3860
    },
    {
      "epoch": 0.967923466516601,
      "grad_norm": 1.840682029724121,
      "learning_rate": 2.431666247800955e-05,
      "loss": 1.0388,
      "step": 3870
    },
    {
      "epoch": 0.9704245607453261,
      "grad_norm": 1.4900850057601929,
      "learning_rate": 2.430158331239005e-05,
      "loss": 1.1129,
      "step": 3880
    },
    {
      "epoch": 0.9729256549740511,
      "grad_norm": 1.836771845817566,
      "learning_rate": 2.428650414677055e-05,
      "loss": 1.0969,
      "step": 3890
    },
    {
      "epoch": 0.9754267492027762,
      "grad_norm": 1.654354214668274,
      "learning_rate": 2.427142498115104e-05,
      "loss": 1.1648,
      "step": 3900
    },
    {
      "epoch": 0.9779278434315013,
      "grad_norm": 2.0022823810577393,
      "learning_rate": 2.425634581553154e-05,
      "loss": 1.1882,
      "step": 3910
    },
    {
      "epoch": 0.9804289376602263,
      "grad_norm": 2.0210816860198975,
      "learning_rate": 2.424126664991204e-05,
      "loss": 1.1211,
      "step": 3920
    },
    {
      "epoch": 0.9829300318889514,
      "grad_norm": 1.9232174158096313,
      "learning_rate": 2.4226187484292537e-05,
      "loss": 1.1108,
      "step": 3930
    },
    {
      "epoch": 0.9854311261176765,
      "grad_norm": 1.3983384370803833,
      "learning_rate": 2.4211108318673034e-05,
      "loss": 1.1023,
      "step": 3940
    },
    {
      "epoch": 0.9879322203464016,
      "grad_norm": 1.5736140012741089,
      "learning_rate": 2.419602915305353e-05,
      "loss": 1.1617,
      "step": 3950
    },
    {
      "epoch": 0.9904333145751266,
      "grad_norm": 2.4785542488098145,
      "learning_rate": 2.418094998743403e-05,
      "loss": 1.0832,
      "step": 3960
    },
    {
      "epoch": 0.9929344088038516,
      "grad_norm": 2.2966468334198,
      "learning_rate": 2.4165870821814526e-05,
      "loss": 1.0692,
      "step": 3970
    },
    {
      "epoch": 0.9954355030325768,
      "grad_norm": 1.892641305923462,
      "learning_rate": 2.4150791656195023e-05,
      "loss": 1.1764,
      "step": 3980
    },
    {
      "epoch": 0.9979365972613018,
      "grad_norm": 1.7148102521896362,
      "learning_rate": 2.4135712490575523e-05,
      "loss": 1.1376,
      "step": 3990
    },
    {
      "epoch": 1.0002501094228724,
      "grad_norm": 1.9487828016281128,
      "learning_rate": 2.412063332495602e-05,
      "loss": 1.0985,
      "step": 4000
    },
    {
      "epoch": 1.0027512036515975,
      "grad_norm": 1.4823006391525269,
      "learning_rate": 2.4105554159336516e-05,
      "loss": 1.1258,
      "step": 4010
    },
    {
      "epoch": 1.0052522978803227,
      "grad_norm": 2.283611536026001,
      "learning_rate": 2.4090474993717015e-05,
      "loss": 1.0718,
      "step": 4020
    },
    {
      "epoch": 1.0077533921090478,
      "grad_norm": 1.8782814741134644,
      "learning_rate": 2.4075395828097512e-05,
      "loss": 1.1225,
      "step": 4030
    },
    {
      "epoch": 1.0102544863377727,
      "grad_norm": 1.5839446783065796,
      "learning_rate": 2.406031666247801e-05,
      "loss": 1.1066,
      "step": 4040
    },
    {
      "epoch": 1.0127555805664978,
      "grad_norm": 1.7647043466567993,
      "learning_rate": 2.4045237496858508e-05,
      "loss": 1.1275,
      "step": 4050
    },
    {
      "epoch": 1.015256674795223,
      "grad_norm": 1.5500943660736084,
      "learning_rate": 2.4030158331239004e-05,
      "loss": 1.0643,
      "step": 4060
    },
    {
      "epoch": 1.0177577690239479,
      "grad_norm": 2.100677967071533,
      "learning_rate": 2.4015079165619504e-05,
      "loss": 1.1149,
      "step": 4070
    },
    {
      "epoch": 1.020258863252673,
      "grad_norm": 1.8619378805160522,
      "learning_rate": 2.4e-05,
      "loss": 1.0966,
      "step": 4080
    },
    {
      "epoch": 1.0227599574813981,
      "grad_norm": 1.6511666774749756,
      "learning_rate": 2.3984920834380497e-05,
      "loss": 1.0757,
      "step": 4090
    },
    {
      "epoch": 1.0252610517101233,
      "grad_norm": 2.1327123641967773,
      "learning_rate": 2.3969841668760997e-05,
      "loss": 1.2386,
      "step": 4100
    },
    {
      "epoch": 1.0277621459388482,
      "grad_norm": 1.7374048233032227,
      "learning_rate": 2.3954762503141493e-05,
      "loss": 1.0777,
      "step": 4110
    },
    {
      "epoch": 1.0302632401675733,
      "grad_norm": 1.9781277179718018,
      "learning_rate": 2.3939683337521993e-05,
      "loss": 1.1526,
      "step": 4120
    },
    {
      "epoch": 1.0327643343962984,
      "grad_norm": 1.51950204372406,
      "learning_rate": 2.3924604171902486e-05,
      "loss": 1.0735,
      "step": 4130
    },
    {
      "epoch": 1.0352654286250234,
      "grad_norm": 2.0342202186584473,
      "learning_rate": 2.3909525006282986e-05,
      "loss": 1.0627,
      "step": 4140
    },
    {
      "epoch": 1.0377665228537485,
      "grad_norm": 1.76340913772583,
      "learning_rate": 2.3894445840663486e-05,
      "loss": 1.1394,
      "step": 4150
    },
    {
      "epoch": 1.0402676170824736,
      "grad_norm": 1.7913991212844849,
      "learning_rate": 2.387936667504398e-05,
      "loss": 1.0909,
      "step": 4160
    },
    {
      "epoch": 1.0427687113111987,
      "grad_norm": 2.4806900024414062,
      "learning_rate": 2.386428750942448e-05,
      "loss": 1.1136,
      "step": 4170
    },
    {
      "epoch": 1.0452698055399237,
      "grad_norm": 1.7144775390625,
      "learning_rate": 2.384920834380498e-05,
      "loss": 1.0916,
      "step": 4180
    },
    {
      "epoch": 1.0477708997686488,
      "grad_norm": 1.8710561990737915,
      "learning_rate": 2.3834129178185475e-05,
      "loss": 1.1205,
      "step": 4190
    },
    {
      "epoch": 1.050271993997374,
      "grad_norm": 2.2040414810180664,
      "learning_rate": 2.381905001256597e-05,
      "loss": 1.1567,
      "step": 4200
    },
    {
      "epoch": 1.0527730882260988,
      "grad_norm": 1.820971131324768,
      "learning_rate": 2.3803970846946468e-05,
      "loss": 1.1324,
      "step": 4210
    },
    {
      "epoch": 1.055274182454824,
      "grad_norm": 1.9276947975158691,
      "learning_rate": 2.3790399597888918e-05,
      "loss": 1.1071,
      "step": 4220
    },
    {
      "epoch": 1.057775276683549,
      "grad_norm": 2.0504117012023926,
      "learning_rate": 2.3775320432269415e-05,
      "loss": 1.1267,
      "step": 4230
    },
    {
      "epoch": 1.0602763709122742,
      "grad_norm": 1.6924772262573242,
      "learning_rate": 2.3760241266649914e-05,
      "loss": 1.1034,
      "step": 4240
    },
    {
      "epoch": 1.0627774651409991,
      "grad_norm": 1.888902187347412,
      "learning_rate": 2.3745162101030407e-05,
      "loss": 1.1411,
      "step": 4250
    },
    {
      "epoch": 1.0652785593697243,
      "grad_norm": 1.7976319789886475,
      "learning_rate": 2.3730082935410907e-05,
      "loss": 1.0921,
      "step": 4260
    },
    {
      "epoch": 1.0677796535984494,
      "grad_norm": 1.5826854705810547,
      "learning_rate": 2.3715003769791407e-05,
      "loss": 1.0711,
      "step": 4270
    },
    {
      "epoch": 1.0702807478271743,
      "grad_norm": 2.1951487064361572,
      "learning_rate": 2.3699924604171903e-05,
      "loss": 1.117,
      "step": 4280
    },
    {
      "epoch": 1.0727818420558994,
      "grad_norm": 1.7175236940383911,
      "learning_rate": 2.36848454385524e-05,
      "loss": 1.0807,
      "step": 4290
    },
    {
      "epoch": 1.0752829362846246,
      "grad_norm": 2.286088466644287,
      "learning_rate": 2.3669766272932896e-05,
      "loss": 1.0885,
      "step": 4300
    },
    {
      "epoch": 1.0777840305133495,
      "grad_norm": 2.19415545463562,
      "learning_rate": 2.3654687107313396e-05,
      "loss": 1.1224,
      "step": 4310
    },
    {
      "epoch": 1.0802851247420746,
      "grad_norm": 1.6394134759902954,
      "learning_rate": 2.3639607941693896e-05,
      "loss": 1.1322,
      "step": 4320
    },
    {
      "epoch": 1.0827862189707997,
      "grad_norm": 2.1367738246917725,
      "learning_rate": 2.362452877607439e-05,
      "loss": 1.0949,
      "step": 4330
    },
    {
      "epoch": 1.0852873131995249,
      "grad_norm": 1.7075300216674805,
      "learning_rate": 2.360944961045489e-05,
      "loss": 1.1565,
      "step": 4340
    },
    {
      "epoch": 1.0877884074282498,
      "grad_norm": 1.6971402168273926,
      "learning_rate": 2.359437044483539e-05,
      "loss": 1.0642,
      "step": 4350
    },
    {
      "epoch": 1.090289501656975,
      "grad_norm": 1.719961166381836,
      "learning_rate": 2.3579291279215885e-05,
      "loss": 1.0871,
      "step": 4360
    },
    {
      "epoch": 1.0927905958857,
      "grad_norm": 2.0395395755767822,
      "learning_rate": 2.356421211359638e-05,
      "loss": 1.0259,
      "step": 4370
    },
    {
      "epoch": 1.0952916901144252,
      "grad_norm": 2.1421499252319336,
      "learning_rate": 2.3549132947976878e-05,
      "loss": 1.0402,
      "step": 4380
    },
    {
      "epoch": 1.09779278434315,
      "grad_norm": 1.538109302520752,
      "learning_rate": 2.3534053782357378e-05,
      "loss": 1.1348,
      "step": 4390
    },
    {
      "epoch": 1.1002938785718752,
      "grad_norm": 2.317117691040039,
      "learning_rate": 2.3518974616737877e-05,
      "loss": 1.0939,
      "step": 4400
    },
    {
      "epoch": 1.1027949728006003,
      "grad_norm": 1.6373908519744873,
      "learning_rate": 2.350389545111837e-05,
      "loss": 1.1059,
      "step": 4410
    },
    {
      "epoch": 1.1052960670293253,
      "grad_norm": 1.8679777383804321,
      "learning_rate": 2.348881628549887e-05,
      "loss": 1.096,
      "step": 4420
    },
    {
      "epoch": 1.1077971612580504,
      "grad_norm": 2.2475719451904297,
      "learning_rate": 2.3473737119879367e-05,
      "loss": 1.1334,
      "step": 4430
    },
    {
      "epoch": 1.1102982554867755,
      "grad_norm": 1.7728513479232788,
      "learning_rate": 2.3458657954259863e-05,
      "loss": 1.0349,
      "step": 4440
    },
    {
      "epoch": 1.1127993497155004,
      "grad_norm": 1.5759170055389404,
      "learning_rate": 2.3443578788640363e-05,
      "loss": 1.1281,
      "step": 4450
    },
    {
      "epoch": 1.1153004439442256,
      "grad_norm": 1.772171139717102,
      "learning_rate": 2.342849962302086e-05,
      "loss": 1.0972,
      "step": 4460
    },
    {
      "epoch": 1.1178015381729507,
      "grad_norm": 1.5054042339324951,
      "learning_rate": 2.341342045740136e-05,
      "loss": 1.0214,
      "step": 4470
    },
    {
      "epoch": 1.1203026324016758,
      "grad_norm": 1.819055199623108,
      "learning_rate": 2.3398341291781856e-05,
      "loss": 1.1147,
      "step": 4480
    },
    {
      "epoch": 1.1228037266304007,
      "grad_norm": 1.6925078630447388,
      "learning_rate": 2.3383262126162352e-05,
      "loss": 1.1315,
      "step": 4490
    },
    {
      "epoch": 1.1253048208591259,
      "grad_norm": 2.040154457092285,
      "learning_rate": 2.3368182960542852e-05,
      "loss": 1.1552,
      "step": 4500
    },
    {
      "epoch": 1.127805915087851,
      "grad_norm": 1.6684582233428955,
      "learning_rate": 2.3353103794923348e-05,
      "loss": 1.0608,
      "step": 4510
    },
    {
      "epoch": 1.130307009316576,
      "grad_norm": 1.3955820798873901,
      "learning_rate": 2.3338024629303845e-05,
      "loss": 1.0384,
      "step": 4520
    },
    {
      "epoch": 1.132808103545301,
      "grad_norm": 1.503438115119934,
      "learning_rate": 2.3322945463684344e-05,
      "loss": 1.1333,
      "step": 4530
    },
    {
      "epoch": 1.1353091977740262,
      "grad_norm": 2.1045005321502686,
      "learning_rate": 2.330786629806484e-05,
      "loss": 1.1258,
      "step": 4540
    },
    {
      "epoch": 1.1378102920027513,
      "grad_norm": 1.7678974866867065,
      "learning_rate": 2.329278713244534e-05,
      "loss": 1.072,
      "step": 4550
    },
    {
      "epoch": 1.1403113862314762,
      "grad_norm": 1.7512487173080444,
      "learning_rate": 2.3277707966825834e-05,
      "loss": 1.0957,
      "step": 4560
    },
    {
      "epoch": 1.1428124804602013,
      "grad_norm": 1.9223111867904663,
      "learning_rate": 2.3262628801206333e-05,
      "loss": 1.0945,
      "step": 4570
    },
    {
      "epoch": 1.1453135746889265,
      "grad_norm": 1.7776435613632202,
      "learning_rate": 2.3247549635586833e-05,
      "loss": 1.0335,
      "step": 4580
    },
    {
      "epoch": 1.1478146689176514,
      "grad_norm": 1.4782198667526245,
      "learning_rate": 2.323247046996733e-05,
      "loss": 1.0463,
      "step": 4590
    },
    {
      "epoch": 1.1503157631463765,
      "grad_norm": 1.8534784317016602,
      "learning_rate": 2.3217391304347826e-05,
      "loss": 1.1428,
      "step": 4600
    },
    {
      "epoch": 1.1528168573751016,
      "grad_norm": 1.518959879875183,
      "learning_rate": 2.3202312138728326e-05,
      "loss": 1.1062,
      "step": 4610
    },
    {
      "epoch": 1.1553179516038266,
      "grad_norm": 2.0931854248046875,
      "learning_rate": 2.3187232973108822e-05,
      "loss": 1.0878,
      "step": 4620
    },
    {
      "epoch": 1.1578190458325517,
      "grad_norm": 1.2746188640594482,
      "learning_rate": 2.317215380748932e-05,
      "loss": 1.0537,
      "step": 4630
    },
    {
      "epoch": 1.1603201400612768,
      "grad_norm": 1.6061075925827026,
      "learning_rate": 2.3157074641869815e-05,
      "loss": 1.1011,
      "step": 4640
    },
    {
      "epoch": 1.162821234290002,
      "grad_norm": 1.8231297731399536,
      "learning_rate": 2.3141995476250315e-05,
      "loss": 1.1503,
      "step": 4650
    },
    {
      "epoch": 1.1653223285187269,
      "grad_norm": 1.437915325164795,
      "learning_rate": 2.3126916310630815e-05,
      "loss": 1.0685,
      "step": 4660
    },
    {
      "epoch": 1.167823422747452,
      "grad_norm": 1.81037175655365,
      "learning_rate": 2.3111837145011308e-05,
      "loss": 1.0971,
      "step": 4670
    },
    {
      "epoch": 1.1703245169761771,
      "grad_norm": 1.6678649187088013,
      "learning_rate": 2.3096757979391808e-05,
      "loss": 1.0841,
      "step": 4680
    },
    {
      "epoch": 1.1728256112049023,
      "grad_norm": 1.8297464847564697,
      "learning_rate": 2.3081678813772304e-05,
      "loss": 1.1095,
      "step": 4690
    },
    {
      "epoch": 1.1753267054336272,
      "grad_norm": 2.5712990760803223,
      "learning_rate": 2.3066599648152804e-05,
      "loss": 1.0565,
      "step": 4700
    },
    {
      "epoch": 1.1778277996623523,
      "grad_norm": 1.8151426315307617,
      "learning_rate": 2.30515204825333e-05,
      "loss": 1.0799,
      "step": 4710
    },
    {
      "epoch": 1.1803288938910774,
      "grad_norm": 1.6665370464324951,
      "learning_rate": 2.3036441316913797e-05,
      "loss": 1.1397,
      "step": 4720
    },
    {
      "epoch": 1.1828299881198023,
      "grad_norm": 1.6564191579818726,
      "learning_rate": 2.3021362151294296e-05,
      "loss": 1.1411,
      "step": 4730
    },
    {
      "epoch": 1.1853310823485275,
      "grad_norm": 2.4966516494750977,
      "learning_rate": 2.3006282985674796e-05,
      "loss": 1.0923,
      "step": 4740
    },
    {
      "epoch": 1.1878321765772526,
      "grad_norm": 1.949816346168518,
      "learning_rate": 2.299120382005529e-05,
      "loss": 1.065,
      "step": 4750
    },
    {
      "epoch": 1.1903332708059775,
      "grad_norm": 1.7016150951385498,
      "learning_rate": 2.297612465443579e-05,
      "loss": 1.0836,
      "step": 4760
    },
    {
      "epoch": 1.1928343650347026,
      "grad_norm": 1.972440242767334,
      "learning_rate": 2.2961045488816286e-05,
      "loss": 1.1121,
      "step": 4770
    },
    {
      "epoch": 1.1953354592634278,
      "grad_norm": 2.0053067207336426,
      "learning_rate": 2.2945966323196785e-05,
      "loss": 1.1639,
      "step": 4780
    },
    {
      "epoch": 1.197836553492153,
      "grad_norm": 1.9512889385223389,
      "learning_rate": 2.2930887157577282e-05,
      "loss": 1.0795,
      "step": 4790
    },
    {
      "epoch": 1.2003376477208778,
      "grad_norm": 1.9155319929122925,
      "learning_rate": 2.2915807991957778e-05,
      "loss": 1.1875,
      "step": 4800
    },
    {
      "epoch": 1.202838741949603,
      "grad_norm": 1.7939947843551636,
      "learning_rate": 2.2900728826338278e-05,
      "loss": 1.1506,
      "step": 4810
    },
    {
      "epoch": 1.205339836178328,
      "grad_norm": 2.0163581371307373,
      "learning_rate": 2.288564966071877e-05,
      "loss": 1.1673,
      "step": 4820
    },
    {
      "epoch": 1.2078409304070532,
      "grad_norm": 1.347440481185913,
      "learning_rate": 2.287057049509927e-05,
      "loss": 1.1713,
      "step": 4830
    },
    {
      "epoch": 1.2103420246357781,
      "grad_norm": 1.5964547395706177,
      "learning_rate": 2.285549132947977e-05,
      "loss": 1.0604,
      "step": 4840
    },
    {
      "epoch": 1.2128431188645032,
      "grad_norm": 2.317992925643921,
      "learning_rate": 2.2840412163860267e-05,
      "loss": 1.0791,
      "step": 4850
    },
    {
      "epoch": 1.2153442130932284,
      "grad_norm": 1.9038329124450684,
      "learning_rate": 2.2825332998240763e-05,
      "loss": 1.1006,
      "step": 4860
    },
    {
      "epoch": 1.2178453073219533,
      "grad_norm": 1.7156599760055542,
      "learning_rate": 2.2810253832621263e-05,
      "loss": 1.1205,
      "step": 4870
    },
    {
      "epoch": 1.2203464015506784,
      "grad_norm": 1.5687570571899414,
      "learning_rate": 2.279517466700176e-05,
      "loss": 1.0725,
      "step": 4880
    },
    {
      "epoch": 1.2228474957794035,
      "grad_norm": 1.5653637647628784,
      "learning_rate": 2.278009550138226e-05,
      "loss": 1.0644,
      "step": 4890
    },
    {
      "epoch": 1.2253485900081285,
      "grad_norm": 1.9528570175170898,
      "learning_rate": 2.2765016335762753e-05,
      "loss": 1.131,
      "step": 4900
    },
    {
      "epoch": 1.2278496842368536,
      "grad_norm": 1.6633979082107544,
      "learning_rate": 2.2749937170143252e-05,
      "loss": 1.0423,
      "step": 4910
    },
    {
      "epoch": 1.2303507784655787,
      "grad_norm": 1.5249059200286865,
      "learning_rate": 2.2734858004523752e-05,
      "loss": 1.1462,
      "step": 4920
    },
    {
      "epoch": 1.2328518726943039,
      "grad_norm": 1.7617621421813965,
      "learning_rate": 2.271977883890425e-05,
      "loss": 1.0801,
      "step": 4930
    },
    {
      "epoch": 1.2353529669230288,
      "grad_norm": 1.6420207023620605,
      "learning_rate": 2.2704699673284745e-05,
      "loss": 1.1835,
      "step": 4940
    },
    {
      "epoch": 1.237854061151754,
      "grad_norm": 2.0953075885772705,
      "learning_rate": 2.268962050766524e-05,
      "loss": 1.1134,
      "step": 4950
    },
    {
      "epoch": 1.240355155380479,
      "grad_norm": 1.9380590915679932,
      "learning_rate": 2.267454134204574e-05,
      "loss": 1.1339,
      "step": 4960
    },
    {
      "epoch": 1.242856249609204,
      "grad_norm": 2.664229393005371,
      "learning_rate": 2.265946217642624e-05,
      "loss": 1.0573,
      "step": 4970
    },
    {
      "epoch": 1.245357343837929,
      "grad_norm": 1.911124587059021,
      "learning_rate": 2.2644383010806734e-05,
      "loss": 1.1041,
      "step": 4980
    },
    {
      "epoch": 1.2478584380666542,
      "grad_norm": 1.6766233444213867,
      "learning_rate": 2.2629303845187234e-05,
      "loss": 1.1378,
      "step": 4990
    },
    {
      "epoch": 1.2503595322953793,
      "grad_norm": 1.448423981666565,
      "learning_rate": 2.2614224679567734e-05,
      "loss": 1.0436,
      "step": 5000
    },
    {
      "epoch": 1.2528606265241042,
      "grad_norm": 1.6389023065567017,
      "learning_rate": 2.2599145513948227e-05,
      "loss": 1.0789,
      "step": 5010
    },
    {
      "epoch": 1.2553617207528294,
      "grad_norm": 1.4488991498947144,
      "learning_rate": 2.2584066348328727e-05,
      "loss": 1.1434,
      "step": 5020
    },
    {
      "epoch": 1.2578628149815545,
      "grad_norm": 1.7979674339294434,
      "learning_rate": 2.2568987182709223e-05,
      "loss": 1.0929,
      "step": 5030
    },
    {
      "epoch": 1.2603639092102794,
      "grad_norm": 1.8535362482070923,
      "learning_rate": 2.2553908017089723e-05,
      "loss": 1.1105,
      "step": 5040
    },
    {
      "epoch": 1.2628650034390045,
      "grad_norm": 1.488288164138794,
      "learning_rate": 2.253882885147022e-05,
      "loss": 1.0558,
      "step": 5050
    },
    {
      "epoch": 1.2653660976677297,
      "grad_norm": 2.6699085235595703,
      "learning_rate": 2.2523749685850716e-05,
      "loss": 1.0174,
      "step": 5060
    },
    {
      "epoch": 1.2678671918964546,
      "grad_norm": 1.7915815114974976,
      "learning_rate": 2.2508670520231215e-05,
      "loss": 1.0855,
      "step": 5070
    },
    {
      "epoch": 1.2703682861251797,
      "grad_norm": 2.1644558906555176,
      "learning_rate": 2.2493591354611712e-05,
      "loss": 1.1549,
      "step": 5080
    },
    {
      "epoch": 1.2728693803539048,
      "grad_norm": 1.658125638961792,
      "learning_rate": 2.2478512188992208e-05,
      "loss": 1.0921,
      "step": 5090
    },
    {
      "epoch": 1.27537047458263,
      "grad_norm": 2.084462881088257,
      "learning_rate": 2.2463433023372708e-05,
      "loss": 1.0607,
      "step": 5100
    },
    {
      "epoch": 1.2778715688113549,
      "grad_norm": 1.9484212398529053,
      "learning_rate": 2.2448353857753204e-05,
      "loss": 1.1679,
      "step": 5110
    },
    {
      "epoch": 1.28037266304008,
      "grad_norm": 2.3881547451019287,
      "learning_rate": 2.2433274692133704e-05,
      "loss": 1.1185,
      "step": 5120
    },
    {
      "epoch": 1.2828737572688051,
      "grad_norm": 2.2741193771362305,
      "learning_rate": 2.24181955265142e-05,
      "loss": 1.1011,
      "step": 5130
    },
    {
      "epoch": 1.2853748514975303,
      "grad_norm": 1.833390235900879,
      "learning_rate": 2.2403116360894697e-05,
      "loss": 1.1577,
      "step": 5140
    },
    {
      "epoch": 1.2878759457262552,
      "grad_norm": 2.19278883934021,
      "learning_rate": 2.2388037195275197e-05,
      "loss": 1.0575,
      "step": 5150
    },
    {
      "epoch": 1.2903770399549803,
      "grad_norm": 1.5912197828292847,
      "learning_rate": 2.2372958029655693e-05,
      "loss": 1.1332,
      "step": 5160
    },
    {
      "epoch": 1.2928781341837055,
      "grad_norm": 1.9639616012573242,
      "learning_rate": 2.235787886403619e-05,
      "loss": 1.0451,
      "step": 5170
    },
    {
      "epoch": 1.2953792284124304,
      "grad_norm": 1.7947784662246704,
      "learning_rate": 2.234279969841669e-05,
      "loss": 1.0447,
      "step": 5180
    },
    {
      "epoch": 1.2978803226411555,
      "grad_norm": 2.0214526653289795,
      "learning_rate": 2.2327720532797186e-05,
      "loss": 1.1232,
      "step": 5190
    },
    {
      "epoch": 1.3003814168698806,
      "grad_norm": 1.7475214004516602,
      "learning_rate": 2.2312641367177682e-05,
      "loss": 1.0458,
      "step": 5200
    },
    {
      "epoch": 1.3028825110986055,
      "grad_norm": 2.2205185890197754,
      "learning_rate": 2.229756220155818e-05,
      "loss": 1.0801,
      "step": 5210
    },
    {
      "epoch": 1.3053836053273307,
      "grad_norm": 1.5527228116989136,
      "learning_rate": 2.228248303593868e-05,
      "loss": 1.1551,
      "step": 5220
    },
    {
      "epoch": 1.3078846995560558,
      "grad_norm": 1.6481198072433472,
      "learning_rate": 2.226740387031918e-05,
      "loss": 1.0605,
      "step": 5230
    },
    {
      "epoch": 1.3103857937847807,
      "grad_norm": 1.6741960048675537,
      "learning_rate": 2.225232470469967e-05,
      "loss": 1.0636,
      "step": 5240
    },
    {
      "epoch": 1.3128868880135058,
      "grad_norm": 2.022016763687134,
      "learning_rate": 2.223724553908017e-05,
      "loss": 1.101,
      "step": 5250
    },
    {
      "epoch": 1.315387982242231,
      "grad_norm": 2.5511326789855957,
      "learning_rate": 2.222216637346067e-05,
      "loss": 1.0759,
      "step": 5260
    },
    {
      "epoch": 1.317889076470956,
      "grad_norm": 1.5875526666641235,
      "learning_rate": 2.2207087207841167e-05,
      "loss": 1.1036,
      "step": 5270
    },
    {
      "epoch": 1.3203901706996812,
      "grad_norm": 2.052664041519165,
      "learning_rate": 2.2192008042221664e-05,
      "loss": 1.1068,
      "step": 5280
    },
    {
      "epoch": 1.3228912649284061,
      "grad_norm": 1.5762840509414673,
      "learning_rate": 2.217692887660216e-05,
      "loss": 1.1016,
      "step": 5290
    },
    {
      "epoch": 1.3253923591571313,
      "grad_norm": 1.4012079238891602,
      "learning_rate": 2.216184971098266e-05,
      "loss": 1.1327,
      "step": 5300
    },
    {
      "epoch": 1.3278934533858564,
      "grad_norm": 2.091986656188965,
      "learning_rate": 2.214677054536316e-05,
      "loss": 1.0388,
      "step": 5310
    },
    {
      "epoch": 1.3303945476145813,
      "grad_norm": 1.9297032356262207,
      "learning_rate": 2.2131691379743653e-05,
      "loss": 1.1366,
      "step": 5320
    },
    {
      "epoch": 1.3328956418433064,
      "grad_norm": 1.4878170490264893,
      "learning_rate": 2.2116612214124153e-05,
      "loss": 1.0545,
      "step": 5330
    },
    {
      "epoch": 1.3353967360720316,
      "grad_norm": 1.402549147605896,
      "learning_rate": 2.210153304850465e-05,
      "loss": 1.0724,
      "step": 5340
    },
    {
      "epoch": 1.3378978303007565,
      "grad_norm": 1.7244147062301636,
      "learning_rate": 2.208645388288515e-05,
      "loss": 1.139,
      "step": 5350
    },
    {
      "epoch": 1.3403989245294816,
      "grad_norm": 2.237025022506714,
      "learning_rate": 2.2071374717265645e-05,
      "loss": 1.1106,
      "step": 5360
    },
    {
      "epoch": 1.3429000187582067,
      "grad_norm": 1.7011995315551758,
      "learning_rate": 2.2056295551646142e-05,
      "loss": 1.0867,
      "step": 5370
    },
    {
      "epoch": 1.3454011129869317,
      "grad_norm": 1.7413511276245117,
      "learning_rate": 2.204121638602664e-05,
      "loss": 1.0505,
      "step": 5380
    },
    {
      "epoch": 1.3479022072156568,
      "grad_norm": 1.9927910566329956,
      "learning_rate": 2.2026137220407138e-05,
      "loss": 1.0499,
      "step": 5390
    },
    {
      "epoch": 1.350403301444382,
      "grad_norm": 1.473456621170044,
      "learning_rate": 2.2011058054787634e-05,
      "loss": 1.0475,
      "step": 5400
    },
    {
      "epoch": 1.352904395673107,
      "grad_norm": 2.064166784286499,
      "learning_rate": 2.1995978889168134e-05,
      "loss": 1.0651,
      "step": 5410
    },
    {
      "epoch": 1.3554054899018322,
      "grad_norm": 2.095428705215454,
      "learning_rate": 2.198089972354863e-05,
      "loss": 1.1311,
      "step": 5420
    },
    {
      "epoch": 1.357906584130557,
      "grad_norm": 1.5886762142181396,
      "learning_rate": 2.1965820557929127e-05,
      "loss": 1.0866,
      "step": 5430
    },
    {
      "epoch": 1.3604076783592822,
      "grad_norm": 2.2439305782318115,
      "learning_rate": 2.1950741392309627e-05,
      "loss": 1.1024,
      "step": 5440
    },
    {
      "epoch": 1.3629087725880074,
      "grad_norm": 1.6943163871765137,
      "learning_rate": 2.1935662226690123e-05,
      "loss": 1.0883,
      "step": 5450
    },
    {
      "epoch": 1.3654098668167323,
      "grad_norm": 1.6693284511566162,
      "learning_rate": 2.1920583061070623e-05,
      "loss": 1.1157,
      "step": 5460
    },
    {
      "epoch": 1.3679109610454574,
      "grad_norm": 1.6025186777114868,
      "learning_rate": 2.1905503895451116e-05,
      "loss": 1.0569,
      "step": 5470
    },
    {
      "epoch": 1.3704120552741825,
      "grad_norm": 1.596246600151062,
      "learning_rate": 2.1890424729831616e-05,
      "loss": 1.0748,
      "step": 5480
    },
    {
      "epoch": 1.3729131495029074,
      "grad_norm": 1.7194063663482666,
      "learning_rate": 2.1875345564212116e-05,
      "loss": 1.0898,
      "step": 5490
    },
    {
      "epoch": 1.3754142437316326,
      "grad_norm": 1.6447534561157227,
      "learning_rate": 2.1860266398592612e-05,
      "loss": 1.0822,
      "step": 5500
    },
    {
      "epoch": 1.3779153379603577,
      "grad_norm": 1.535420536994934,
      "learning_rate": 2.184518723297311e-05,
      "loss": 1.0978,
      "step": 5510
    },
    {
      "epoch": 1.3804164321890826,
      "grad_norm": 1.4620054960250854,
      "learning_rate": 2.183010806735361e-05,
      "loss": 1.0812,
      "step": 5520
    },
    {
      "epoch": 1.3829175264178077,
      "grad_norm": 1.7210348844528198,
      "learning_rate": 2.1815028901734105e-05,
      "loss": 1.1482,
      "step": 5530
    },
    {
      "epoch": 1.3854186206465329,
      "grad_norm": 1.8145360946655273,
      "learning_rate": 2.1799949736114605e-05,
      "loss": 1.1164,
      "step": 5540
    },
    {
      "epoch": 1.387919714875258,
      "grad_norm": 1.6043668985366821,
      "learning_rate": 2.1784870570495098e-05,
      "loss": 1.129,
      "step": 5550
    },
    {
      "epoch": 1.390420809103983,
      "grad_norm": 2.3060131072998047,
      "learning_rate": 2.1769791404875598e-05,
      "loss": 1.1526,
      "step": 5560
    },
    {
      "epoch": 1.392921903332708,
      "grad_norm": 1.7585091590881348,
      "learning_rate": 2.1754712239256097e-05,
      "loss": 1.1132,
      "step": 5570
    },
    {
      "epoch": 1.3954229975614332,
      "grad_norm": 2.233297348022461,
      "learning_rate": 2.173963307363659e-05,
      "loss": 1.0649,
      "step": 5580
    },
    {
      "epoch": 1.3979240917901583,
      "grad_norm": 1.8509052991867065,
      "learning_rate": 2.172455390801709e-05,
      "loss": 1.0698,
      "step": 5590
    },
    {
      "epoch": 1.4004251860188832,
      "grad_norm": 2.9615085124969482,
      "learning_rate": 2.1709474742397587e-05,
      "loss": 1.1663,
      "step": 5600
    },
    {
      "epoch": 1.4029262802476083,
      "grad_norm": 1.9582810401916504,
      "learning_rate": 2.1694395576778086e-05,
      "loss": 1.1069,
      "step": 5610
    },
    {
      "epoch": 1.4054273744763335,
      "grad_norm": 2.0877366065979004,
      "learning_rate": 2.1679316411158583e-05,
      "loss": 1.0617,
      "step": 5620
    },
    {
      "epoch": 1.4079284687050584,
      "grad_norm": 1.2741283178329468,
      "learning_rate": 2.166423724553908e-05,
      "loss": 1.0394,
      "step": 5630
    },
    {
      "epoch": 1.4104295629337835,
      "grad_norm": 1.9839476346969604,
      "learning_rate": 2.164915807991958e-05,
      "loss": 1.1508,
      "step": 5640
    },
    {
      "epoch": 1.4129306571625087,
      "grad_norm": 1.906613826751709,
      "learning_rate": 2.163407891430008e-05,
      "loss": 1.112,
      "step": 5650
    },
    {
      "epoch": 1.4154317513912336,
      "grad_norm": 1.6434682607650757,
      "learning_rate": 2.1620507665242526e-05,
      "loss": 1.1053,
      "step": 5660
    },
    {
      "epoch": 1.4179328456199587,
      "grad_norm": 1.4211316108703613,
      "learning_rate": 2.160542849962302e-05,
      "loss": 1.1226,
      "step": 5670
    },
    {
      "epoch": 1.4204339398486838,
      "grad_norm": 1.8152297735214233,
      "learning_rate": 2.159034933400352e-05,
      "loss": 1.1214,
      "step": 5680
    },
    {
      "epoch": 1.4229350340774087,
      "grad_norm": 1.9708220958709717,
      "learning_rate": 2.157527016838402e-05,
      "loss": 1.1357,
      "step": 5690
    },
    {
      "epoch": 1.4254361283061339,
      "grad_norm": 1.9990148544311523,
      "learning_rate": 2.1560191002764515e-05,
      "loss": 1.0242,
      "step": 5700
    },
    {
      "epoch": 1.427937222534859,
      "grad_norm": 2.0322983264923096,
      "learning_rate": 2.154511183714501e-05,
      "loss": 1.1262,
      "step": 5710
    },
    {
      "epoch": 1.4304383167635841,
      "grad_norm": 1.845685362815857,
      "learning_rate": 2.1530032671525508e-05,
      "loss": 1.0582,
      "step": 5720
    },
    {
      "epoch": 1.4329394109923093,
      "grad_norm": 2.1096057891845703,
      "learning_rate": 2.1514953505906008e-05,
      "loss": 1.1552,
      "step": 5730
    },
    {
      "epoch": 1.4354405052210342,
      "grad_norm": 2.210186719894409,
      "learning_rate": 2.1499874340286507e-05,
      "loss": 1.145,
      "step": 5740
    },
    {
      "epoch": 1.4379415994497593,
      "grad_norm": 2.34734845161438,
      "learning_rate": 2.1484795174667e-05,
      "loss": 1.0685,
      "step": 5750
    },
    {
      "epoch": 1.4404426936784844,
      "grad_norm": 2.1188950538635254,
      "learning_rate": 2.14697160090475e-05,
      "loss": 1.0531,
      "step": 5760
    },
    {
      "epoch": 1.4429437879072093,
      "grad_norm": 1.8390039205551147,
      "learning_rate": 2.1454636843427997e-05,
      "loss": 1.0409,
      "step": 5770
    },
    {
      "epoch": 1.4454448821359345,
      "grad_norm": 1.7167396545410156,
      "learning_rate": 2.1439557677808497e-05,
      "loss": 1.0668,
      "step": 5780
    },
    {
      "epoch": 1.4479459763646596,
      "grad_norm": 1.691935420036316,
      "learning_rate": 2.1424478512188993e-05,
      "loss": 1.0759,
      "step": 5790
    },
    {
      "epoch": 1.4504470705933845,
      "grad_norm": 1.8529902696609497,
      "learning_rate": 2.140939934656949e-05,
      "loss": 1.0573,
      "step": 5800
    },
    {
      "epoch": 1.4529481648221096,
      "grad_norm": 1.6685572862625122,
      "learning_rate": 2.139432018094999e-05,
      "loss": 1.1097,
      "step": 5810
    },
    {
      "epoch": 1.4554492590508348,
      "grad_norm": 1.7197009325027466,
      "learning_rate": 2.1379241015330486e-05,
      "loss": 1.1229,
      "step": 5820
    },
    {
      "epoch": 1.4579503532795597,
      "grad_norm": 3.265486240386963,
      "learning_rate": 2.1364161849710982e-05,
      "loss": 1.0809,
      "step": 5830
    },
    {
      "epoch": 1.4604514475082848,
      "grad_norm": 1.9697110652923584,
      "learning_rate": 2.1349082684091482e-05,
      "loss": 1.1526,
      "step": 5840
    },
    {
      "epoch": 1.46295254173701,
      "grad_norm": 1.888929009437561,
      "learning_rate": 2.1334003518471978e-05,
      "loss": 1.0696,
      "step": 5850
    },
    {
      "epoch": 1.465453635965735,
      "grad_norm": 1.6717336177825928,
      "learning_rate": 2.1318924352852475e-05,
      "loss": 1.0498,
      "step": 5860
    },
    {
      "epoch": 1.4679547301944602,
      "grad_norm": 1.6725058555603027,
      "learning_rate": 2.1303845187232974e-05,
      "loss": 1.1324,
      "step": 5870
    },
    {
      "epoch": 1.4704558244231851,
      "grad_norm": 1.5868881940841675,
      "learning_rate": 2.128876602161347e-05,
      "loss": 1.1607,
      "step": 5880
    },
    {
      "epoch": 1.4729569186519103,
      "grad_norm": 1.8812124729156494,
      "learning_rate": 2.127368685599397e-05,
      "loss": 1.0444,
      "step": 5890
    },
    {
      "epoch": 1.4754580128806354,
      "grad_norm": 1.9942164421081543,
      "learning_rate": 2.1258607690374464e-05,
      "loss": 1.1253,
      "step": 5900
    },
    {
      "epoch": 1.4779591071093603,
      "grad_norm": 1.6524978876113892,
      "learning_rate": 2.1243528524754964e-05,
      "loss": 1.1217,
      "step": 5910
    },
    {
      "epoch": 1.4804602013380854,
      "grad_norm": 1.6769921779632568,
      "learning_rate": 2.1228449359135463e-05,
      "loss": 1.1049,
      "step": 5920
    },
    {
      "epoch": 1.4829612955668106,
      "grad_norm": 1.8217577934265137,
      "learning_rate": 2.121337019351596e-05,
      "loss": 1.1115,
      "step": 5930
    },
    {
      "epoch": 1.4854623897955355,
      "grad_norm": 2.418161392211914,
      "learning_rate": 2.1198291027896456e-05,
      "loss": 1.1334,
      "step": 5940
    },
    {
      "epoch": 1.4879634840242606,
      "grad_norm": 1.931963562965393,
      "learning_rate": 2.1183211862276956e-05,
      "loss": 1.0652,
      "step": 5950
    },
    {
      "epoch": 1.4904645782529857,
      "grad_norm": 1.6360150575637817,
      "learning_rate": 2.1168132696657452e-05,
      "loss": 1.0733,
      "step": 5960
    },
    {
      "epoch": 1.4929656724817106,
      "grad_norm": 2.008028268814087,
      "learning_rate": 2.1153053531037952e-05,
      "loss": 1.1457,
      "step": 5970
    },
    {
      "epoch": 1.4954667667104358,
      "grad_norm": 1.51488196849823,
      "learning_rate": 2.1137974365418445e-05,
      "loss": 1.1049,
      "step": 5980
    },
    {
      "epoch": 1.497967860939161,
      "grad_norm": 2.1846840381622314,
      "learning_rate": 2.1122895199798945e-05,
      "loss": 1.1163,
      "step": 5990
    },
    {
      "epoch": 1.5004689551678858,
      "grad_norm": 1.6073510646820068,
      "learning_rate": 2.1107816034179445e-05,
      "loss": 1.1314,
      "step": 6000
    },
    {
      "epoch": 1.5029700493966112,
      "grad_norm": 1.2361215353012085,
      "learning_rate": 2.1092736868559938e-05,
      "loss": 1.1745,
      "step": 6010
    },
    {
      "epoch": 1.505471143625336,
      "grad_norm": 1.866775393486023,
      "learning_rate": 2.1077657702940438e-05,
      "loss": 1.1415,
      "step": 6020
    },
    {
      "epoch": 1.5079722378540612,
      "grad_norm": 1.7355494499206543,
      "learning_rate": 2.1062578537320934e-05,
      "loss": 1.0884,
      "step": 6030
    },
    {
      "epoch": 1.5104733320827863,
      "grad_norm": 1.7889817953109741,
      "learning_rate": 2.1047499371701434e-05,
      "loss": 1.1359,
      "step": 6040
    },
    {
      "epoch": 1.5129744263115112,
      "grad_norm": 2.3602569103240967,
      "learning_rate": 2.103242020608193e-05,
      "loss": 1.0659,
      "step": 6050
    },
    {
      "epoch": 1.5154755205402364,
      "grad_norm": 2.0133354663848877,
      "learning_rate": 2.1017341040462427e-05,
      "loss": 0.9785,
      "step": 6060
    },
    {
      "epoch": 1.5179766147689615,
      "grad_norm": 1.8099113702774048,
      "learning_rate": 2.1002261874842927e-05,
      "loss": 1.1031,
      "step": 6070
    },
    {
      "epoch": 1.5204777089976864,
      "grad_norm": 1.5240658521652222,
      "learning_rate": 2.0987182709223426e-05,
      "loss": 1.136,
      "step": 6080
    },
    {
      "epoch": 1.5229788032264115,
      "grad_norm": 2.010571002960205,
      "learning_rate": 2.097210354360392e-05,
      "loss": 1.1554,
      "step": 6090
    },
    {
      "epoch": 1.5254798974551367,
      "grad_norm": 1.839481234550476,
      "learning_rate": 2.095702437798442e-05,
      "loss": 1.0191,
      "step": 6100
    },
    {
      "epoch": 1.5279809916838616,
      "grad_norm": 1.6184161901474,
      "learning_rate": 2.0941945212364916e-05,
      "loss": 1.1358,
      "step": 6110
    },
    {
      "epoch": 1.5304820859125867,
      "grad_norm": 2.1577179431915283,
      "learning_rate": 2.0926866046745415e-05,
      "loss": 1.129,
      "step": 6120
    },
    {
      "epoch": 1.5329831801413119,
      "grad_norm": 2.5082597732543945,
      "learning_rate": 2.0911786881125912e-05,
      "loss": 1.0568,
      "step": 6130
    },
    {
      "epoch": 1.5354842743700368,
      "grad_norm": 1.7160239219665527,
      "learning_rate": 2.0896707715506408e-05,
      "loss": 1.2058,
      "step": 6140
    },
    {
      "epoch": 1.5379853685987621,
      "grad_norm": 1.6691950559616089,
      "learning_rate": 2.0881628549886908e-05,
      "loss": 1.1081,
      "step": 6150
    },
    {
      "epoch": 1.540486462827487,
      "grad_norm": 2.0825328826904297,
      "learning_rate": 2.0866549384267405e-05,
      "loss": 1.0532,
      "step": 6160
    },
    {
      "epoch": 1.542987557056212,
      "grad_norm": 1.7785789966583252,
      "learning_rate": 2.08514702186479e-05,
      "loss": 1.0942,
      "step": 6170
    },
    {
      "epoch": 1.5454886512849373,
      "grad_norm": 1.8688238859176636,
      "learning_rate": 2.08363910530284e-05,
      "loss": 1.0397,
      "step": 6180
    },
    {
      "epoch": 1.5479897455136622,
      "grad_norm": 2.4856460094451904,
      "learning_rate": 2.0821311887408897e-05,
      "loss": 1.0715,
      "step": 6190
    },
    {
      "epoch": 1.5504908397423873,
      "grad_norm": 1.500153660774231,
      "learning_rate": 2.0806232721789394e-05,
      "loss": 1.1023,
      "step": 6200
    },
    {
      "epoch": 1.5529919339711125,
      "grad_norm": 1.7954601049423218,
      "learning_rate": 2.0791153556169893e-05,
      "loss": 1.1557,
      "step": 6210
    },
    {
      "epoch": 1.5554930281998374,
      "grad_norm": 2.5281243324279785,
      "learning_rate": 2.077607439055039e-05,
      "loss": 1.1131,
      "step": 6220
    },
    {
      "epoch": 1.5579941224285625,
      "grad_norm": 1.8097423315048218,
      "learning_rate": 2.076099522493089e-05,
      "loss": 1.1189,
      "step": 6230
    },
    {
      "epoch": 1.5604952166572876,
      "grad_norm": 2.589840888977051,
      "learning_rate": 2.0745916059311383e-05,
      "loss": 1.1121,
      "step": 6240
    },
    {
      "epoch": 1.5629963108860125,
      "grad_norm": 1.7457246780395508,
      "learning_rate": 2.0730836893691882e-05,
      "loss": 1.0768,
      "step": 6250
    },
    {
      "epoch": 1.5654974051147377,
      "grad_norm": 1.675788402557373,
      "learning_rate": 2.0715757728072382e-05,
      "loss": 1.0982,
      "step": 6260
    },
    {
      "epoch": 1.5679984993434628,
      "grad_norm": 1.8162742853164673,
      "learning_rate": 2.070067856245288e-05,
      "loss": 1.0089,
      "step": 6270
    },
    {
      "epoch": 1.5704995935721877,
      "grad_norm": 2.061340093612671,
      "learning_rate": 2.0685599396833375e-05,
      "loss": 1.1034,
      "step": 6280
    },
    {
      "epoch": 1.5730006878009128,
      "grad_norm": 2.1373848915100098,
      "learning_rate": 2.067052023121387e-05,
      "loss": 1.0726,
      "step": 6290
    },
    {
      "epoch": 1.575501782029638,
      "grad_norm": 1.523217797279358,
      "learning_rate": 2.065544106559437e-05,
      "loss": 1.1036,
      "step": 6300
    },
    {
      "epoch": 1.5780028762583629,
      "grad_norm": 1.8537325859069824,
      "learning_rate": 2.064036189997487e-05,
      "loss": 1.0657,
      "step": 6310
    },
    {
      "epoch": 1.5805039704870882,
      "grad_norm": 1.6659129858016968,
      "learning_rate": 2.0625282734355364e-05,
      "loss": 1.1777,
      "step": 6320
    },
    {
      "epoch": 1.5830050647158131,
      "grad_norm": 2.013108491897583,
      "learning_rate": 2.0610203568735864e-05,
      "loss": 1.1256,
      "step": 6330
    },
    {
      "epoch": 1.5855061589445383,
      "grad_norm": 1.8809082508087158,
      "learning_rate": 2.0595124403116364e-05,
      "loss": 1.1298,
      "step": 6340
    },
    {
      "epoch": 1.5880072531732634,
      "grad_norm": 1.7319809198379517,
      "learning_rate": 2.058004523749686e-05,
      "loss": 1.0761,
      "step": 6350
    },
    {
      "epoch": 1.5905083474019883,
      "grad_norm": 1.9252593517303467,
      "learning_rate": 2.0564966071877357e-05,
      "loss": 1.0738,
      "step": 6360
    },
    {
      "epoch": 1.5930094416307135,
      "grad_norm": 1.8102275133132935,
      "learning_rate": 2.0549886906257853e-05,
      "loss": 1.0845,
      "step": 6370
    },
    {
      "epoch": 1.5955105358594386,
      "grad_norm": 1.7995758056640625,
      "learning_rate": 2.0534807740638353e-05,
      "loss": 1.0626,
      "step": 6380
    },
    {
      "epoch": 1.5980116300881635,
      "grad_norm": 2.414339303970337,
      "learning_rate": 2.051972857501885e-05,
      "loss": 1.1703,
      "step": 6390
    },
    {
      "epoch": 1.6005127243168886,
      "grad_norm": 1.5183346271514893,
      "learning_rate": 2.0504649409399346e-05,
      "loss": 1.1632,
      "step": 6400
    },
    {
      "epoch": 1.6030138185456138,
      "grad_norm": 1.9857523441314697,
      "learning_rate": 2.0489570243779845e-05,
      "loss": 1.1332,
      "step": 6410
    },
    {
      "epoch": 1.6055149127743387,
      "grad_norm": 1.7178312540054321,
      "learning_rate": 2.0474491078160342e-05,
      "loss": 1.0379,
      "step": 6420
    },
    {
      "epoch": 1.6080160070030638,
      "grad_norm": 2.189429521560669,
      "learning_rate": 2.045941191254084e-05,
      "loss": 1.1288,
      "step": 6430
    },
    {
      "epoch": 1.610517101231789,
      "grad_norm": 1.7593809366226196,
      "learning_rate": 2.0444332746921338e-05,
      "loss": 1.2215,
      "step": 6440
    },
    {
      "epoch": 1.6130181954605138,
      "grad_norm": 1.8454234600067139,
      "learning_rate": 2.0429253581301835e-05,
      "loss": 1.0798,
      "step": 6450
    },
    {
      "epoch": 1.6155192896892392,
      "grad_norm": 2.4960691928863525,
      "learning_rate": 2.0414174415682334e-05,
      "loss": 1.1299,
      "step": 6460
    },
    {
      "epoch": 1.618020383917964,
      "grad_norm": 2.0374464988708496,
      "learning_rate": 2.039909525006283e-05,
      "loss": 1.1495,
      "step": 6470
    },
    {
      "epoch": 1.6205214781466892,
      "grad_norm": 1.4228867292404175,
      "learning_rate": 2.0384016084443327e-05,
      "loss": 1.0389,
      "step": 6480
    },
    {
      "epoch": 1.6230225723754144,
      "grad_norm": 2.4529097080230713,
      "learning_rate": 2.0368936918823827e-05,
      "loss": 1.1257,
      "step": 6490
    },
    {
      "epoch": 1.6255236666041393,
      "grad_norm": 1.5667191743850708,
      "learning_rate": 2.0353857753204323e-05,
      "loss": 1.1119,
      "step": 6500
    },
    {
      "epoch": 1.6280247608328644,
      "grad_norm": 1.9752075672149658,
      "learning_rate": 2.033877858758482e-05,
      "loss": 1.139,
      "step": 6510
    },
    {
      "epoch": 1.6305258550615895,
      "grad_norm": 1.8634973764419556,
      "learning_rate": 2.032369942196532e-05,
      "loss": 1.0854,
      "step": 6520
    },
    {
      "epoch": 1.6330269492903144,
      "grad_norm": 2.0551328659057617,
      "learning_rate": 2.0308620256345816e-05,
      "loss": 1.1362,
      "step": 6530
    },
    {
      "epoch": 1.6355280435190396,
      "grad_norm": 2.195145606994629,
      "learning_rate": 2.0293541090726316e-05,
      "loss": 1.0279,
      "step": 6540
    },
    {
      "epoch": 1.6380291377477647,
      "grad_norm": 1.7374625205993652,
      "learning_rate": 2.027846192510681e-05,
      "loss": 1.0957,
      "step": 6550
    },
    {
      "epoch": 1.6405302319764896,
      "grad_norm": 2.30572509765625,
      "learning_rate": 2.026338275948731e-05,
      "loss": 1.0931,
      "step": 6560
    },
    {
      "epoch": 1.6430313262052147,
      "grad_norm": 2.0935161113739014,
      "learning_rate": 2.024830359386781e-05,
      "loss": 1.0826,
      "step": 6570
    },
    {
      "epoch": 1.6455324204339399,
      "grad_norm": 1.9352247714996338,
      "learning_rate": 2.02332244282483e-05,
      "loss": 1.1151,
      "step": 6580
    },
    {
      "epoch": 1.6480335146626648,
      "grad_norm": 2.046339511871338,
      "learning_rate": 2.02181452626288e-05,
      "loss": 1.073,
      "step": 6590
    },
    {
      "epoch": 1.6505346088913901,
      "grad_norm": 2.4848697185516357,
      "learning_rate": 2.02030660970093e-05,
      "loss": 1.0749,
      "step": 6600
    },
    {
      "epoch": 1.653035703120115,
      "grad_norm": 1.6378135681152344,
      "learning_rate": 2.0187986931389798e-05,
      "loss": 1.1308,
      "step": 6610
    },
    {
      "epoch": 1.65553679734884,
      "grad_norm": 2.3612728118896484,
      "learning_rate": 2.0172907765770294e-05,
      "loss": 1.0817,
      "step": 6620
    },
    {
      "epoch": 1.6580378915775653,
      "grad_norm": 1.584659457206726,
      "learning_rate": 2.015782860015079e-05,
      "loss": 1.0602,
      "step": 6630
    },
    {
      "epoch": 1.6605389858062902,
      "grad_norm": 1.585896372795105,
      "learning_rate": 2.014274943453129e-05,
      "loss": 1.0346,
      "step": 6640
    },
    {
      "epoch": 1.6630400800350154,
      "grad_norm": 1.9850538969039917,
      "learning_rate": 2.012767026891179e-05,
      "loss": 1.1738,
      "step": 6650
    },
    {
      "epoch": 1.6655411742637405,
      "grad_norm": 2.3204233646392822,
      "learning_rate": 2.0112591103292283e-05,
      "loss": 1.065,
      "step": 6660
    },
    {
      "epoch": 1.6680422684924654,
      "grad_norm": 1.8976973295211792,
      "learning_rate": 2.0097511937672783e-05,
      "loss": 1.035,
      "step": 6670
    },
    {
      "epoch": 1.6705433627211905,
      "grad_norm": 1.5432004928588867,
      "learning_rate": 2.008243277205328e-05,
      "loss": 1.1286,
      "step": 6680
    },
    {
      "epoch": 1.6730444569499157,
      "grad_norm": 1.7961212396621704,
      "learning_rate": 2.006735360643378e-05,
      "loss": 1.0662,
      "step": 6690
    },
    {
      "epoch": 1.6755455511786406,
      "grad_norm": 1.7594314813613892,
      "learning_rate": 2.0052274440814276e-05,
      "loss": 1.0919,
      "step": 6700
    },
    {
      "epoch": 1.6780466454073657,
      "grad_norm": 2.555499792098999,
      "learning_rate": 2.0037195275194772e-05,
      "loss": 1.106,
      "step": 6710
    },
    {
      "epoch": 1.6805477396360908,
      "grad_norm": 2.041701316833496,
      "learning_rate": 2.0022116109575272e-05,
      "loss": 1.0974,
      "step": 6720
    },
    {
      "epoch": 1.6830488338648157,
      "grad_norm": 1.8022809028625488,
      "learning_rate": 2.000703694395577e-05,
      "loss": 1.0776,
      "step": 6730
    },
    {
      "epoch": 1.6855499280935409,
      "grad_norm": 1.6832835674285889,
      "learning_rate": 1.9991957778336265e-05,
      "loss": 1.0499,
      "step": 6740
    },
    {
      "epoch": 1.688051022322266,
      "grad_norm": 1.8088804483413696,
      "learning_rate": 1.9976878612716764e-05,
      "loss": 1.0899,
      "step": 6750
    },
    {
      "epoch": 1.690552116550991,
      "grad_norm": 1.9178636074066162,
      "learning_rate": 1.996179944709726e-05,
      "loss": 1.1299,
      "step": 6760
    },
    {
      "epoch": 1.6930532107797163,
      "grad_norm": 1.624981164932251,
      "learning_rate": 1.9946720281477757e-05,
      "loss": 1.0558,
      "step": 6770
    },
    {
      "epoch": 1.6955543050084412,
      "grad_norm": 2.4287283420562744,
      "learning_rate": 1.9931641115858257e-05,
      "loss": 1.0885,
      "step": 6780
    },
    {
      "epoch": 1.6980553992371663,
      "grad_norm": 1.7815966606140137,
      "learning_rate": 1.9916561950238753e-05,
      "loss": 1.0825,
      "step": 6790
    },
    {
      "epoch": 1.7005564934658914,
      "grad_norm": 1.7631422281265259,
      "learning_rate": 1.9901482784619253e-05,
      "loss": 1.0976,
      "step": 6800
    },
    {
      "epoch": 1.7030575876946163,
      "grad_norm": 1.8082550764083862,
      "learning_rate": 1.9886403618999746e-05,
      "loss": 1.1413,
      "step": 6810
    },
    {
      "epoch": 1.7055586819233415,
      "grad_norm": 2.110130786895752,
      "learning_rate": 1.9871324453380246e-05,
      "loss": 1.1087,
      "step": 6820
    },
    {
      "epoch": 1.7080597761520666,
      "grad_norm": 1.9222763776779175,
      "learning_rate": 1.9856245287760746e-05,
      "loss": 1.0613,
      "step": 6830
    },
    {
      "epoch": 1.7105608703807915,
      "grad_norm": 2.005277156829834,
      "learning_rate": 1.9841166122141242e-05,
      "loss": 1.142,
      "step": 6840
    },
    {
      "epoch": 1.7130619646095167,
      "grad_norm": 1.5195904970169067,
      "learning_rate": 1.982608695652174e-05,
      "loss": 1.1432,
      "step": 6850
    },
    {
      "epoch": 1.7155630588382418,
      "grad_norm": 1.7658212184906006,
      "learning_rate": 1.981100779090224e-05,
      "loss": 1.0352,
      "step": 6860
    },
    {
      "epoch": 1.7180641530669667,
      "grad_norm": 1.62001371383667,
      "learning_rate": 1.9795928625282735e-05,
      "loss": 1.0883,
      "step": 6870
    },
    {
      "epoch": 1.7205652472956918,
      "grad_norm": 2.1832985877990723,
      "learning_rate": 1.9780849459663235e-05,
      "loss": 1.0637,
      "step": 6880
    },
    {
      "epoch": 1.723066341524417,
      "grad_norm": 1.5212687253952026,
      "learning_rate": 1.9765770294043728e-05,
      "loss": 1.11,
      "step": 6890
    },
    {
      "epoch": 1.7255674357531419,
      "grad_norm": 1.9495368003845215,
      "learning_rate": 1.9750691128424228e-05,
      "loss": 1.1065,
      "step": 6900
    },
    {
      "epoch": 1.7280685299818672,
      "grad_norm": 2.2035372257232666,
      "learning_rate": 1.9735611962804727e-05,
      "loss": 1.0567,
      "step": 6910
    },
    {
      "epoch": 1.7305696242105921,
      "grad_norm": 1.5265843868255615,
      "learning_rate": 1.9720532797185224e-05,
      "loss": 1.093,
      "step": 6920
    },
    {
      "epoch": 1.733070718439317,
      "grad_norm": 1.6135847568511963,
      "learning_rate": 1.970545363156572e-05,
      "loss": 1.1,
      "step": 6930
    },
    {
      "epoch": 1.7355718126680424,
      "grad_norm": 1.7504202127456665,
      "learning_rate": 1.9690374465946217e-05,
      "loss": 1.0935,
      "step": 6940
    },
    {
      "epoch": 1.7380729068967673,
      "grad_norm": 2.0726351737976074,
      "learning_rate": 1.9675295300326716e-05,
      "loss": 1.1154,
      "step": 6950
    },
    {
      "epoch": 1.7405740011254924,
      "grad_norm": 1.860743761062622,
      "learning_rate": 1.9660216134707213e-05,
      "loss": 1.1248,
      "step": 6960
    },
    {
      "epoch": 1.7430750953542176,
      "grad_norm": 2.2172114849090576,
      "learning_rate": 1.964513696908771e-05,
      "loss": 1.0132,
      "step": 6970
    },
    {
      "epoch": 1.7455761895829425,
      "grad_norm": 2.3814468383789062,
      "learning_rate": 1.963005780346821e-05,
      "loss": 1.105,
      "step": 6980
    },
    {
      "epoch": 1.7480772838116676,
      "grad_norm": 1.8926652669906616,
      "learning_rate": 1.961497863784871e-05,
      "loss": 1.1663,
      "step": 6990
    },
    {
      "epoch": 1.7505783780403927,
      "grad_norm": 1.9202842712402344,
      "learning_rate": 1.9599899472229202e-05,
      "loss": 1.0326,
      "step": 7000
    },
    {
      "epoch": 1.7530794722691176,
      "grad_norm": 2.4944047927856445,
      "learning_rate": 1.9584820306609702e-05,
      "loss": 1.0799,
      "step": 7010
    },
    {
      "epoch": 1.7555805664978428,
      "grad_norm": 1.771168828010559,
      "learning_rate": 1.9569741140990198e-05,
      "loss": 1.1781,
      "step": 7020
    },
    {
      "epoch": 1.758081660726568,
      "grad_norm": 2.569559335708618,
      "learning_rate": 1.9554661975370698e-05,
      "loss": 1.0665,
      "step": 7030
    },
    {
      "epoch": 1.7605827549552928,
      "grad_norm": 2.0544538497924805,
      "learning_rate": 1.9539582809751194e-05,
      "loss": 1.059,
      "step": 7040
    },
    {
      "epoch": 1.7630838491840182,
      "grad_norm": 1.7003408670425415,
      "learning_rate": 1.952450364413169e-05,
      "loss": 1.0716,
      "step": 7050
    },
    {
      "epoch": 1.765584943412743,
      "grad_norm": 2.1013290882110596,
      "learning_rate": 1.950942447851219e-05,
      "loss": 1.1605,
      "step": 7060
    },
    {
      "epoch": 1.768086037641468,
      "grad_norm": 2.034470558166504,
      "learning_rate": 1.9494345312892687e-05,
      "loss": 1.0669,
      "step": 7070
    },
    {
      "epoch": 1.7705871318701933,
      "grad_norm": 1.963034987449646,
      "learning_rate": 1.9479266147273183e-05,
      "loss": 1.064,
      "step": 7080
    },
    {
      "epoch": 1.7730882260989183,
      "grad_norm": 1.8739479780197144,
      "learning_rate": 1.9464186981653683e-05,
      "loss": 1.0417,
      "step": 7090
    },
    {
      "epoch": 1.7755893203276434,
      "grad_norm": 2.2825300693511963,
      "learning_rate": 1.944910781603418e-05,
      "loss": 1.0951,
      "step": 7100
    },
    {
      "epoch": 1.7780904145563685,
      "grad_norm": 1.8307071924209595,
      "learning_rate": 1.943402865041468e-05,
      "loss": 1.1527,
      "step": 7110
    },
    {
      "epoch": 1.7805915087850934,
      "grad_norm": 2.1779184341430664,
      "learning_rate": 1.9418949484795176e-05,
      "loss": 1.0658,
      "step": 7120
    },
    {
      "epoch": 1.7830926030138186,
      "grad_norm": 2.1304492950439453,
      "learning_rate": 1.9403870319175672e-05,
      "loss": 1.0892,
      "step": 7130
    },
    {
      "epoch": 1.7855936972425437,
      "grad_norm": 2.142413377761841,
      "learning_rate": 1.9388791153556172e-05,
      "loss": 1.0655,
      "step": 7140
    },
    {
      "epoch": 1.7880947914712686,
      "grad_norm": 1.8628153800964355,
      "learning_rate": 1.9373711987936665e-05,
      "loss": 1.1688,
      "step": 7150
    },
    {
      "epoch": 1.7905958856999937,
      "grad_norm": 1.8769139051437378,
      "learning_rate": 1.9358632822317165e-05,
      "loss": 1.1251,
      "step": 7160
    },
    {
      "epoch": 1.7930969799287189,
      "grad_norm": 2.1194076538085938,
      "learning_rate": 1.9343553656697665e-05,
      "loss": 1.1312,
      "step": 7170
    },
    {
      "epoch": 1.7955980741574438,
      "grad_norm": 2.144129514694214,
      "learning_rate": 1.932847449107816e-05,
      "loss": 1.1162,
      "step": 7180
    },
    {
      "epoch": 1.798099168386169,
      "grad_norm": 2.0102250576019287,
      "learning_rate": 1.9313395325458658e-05,
      "loss": 1.0462,
      "step": 7190
    },
    {
      "epoch": 1.800600262614894,
      "grad_norm": 1.9011679887771606,
      "learning_rate": 1.9298316159839154e-05,
      "loss": 1.0739,
      "step": 7200
    },
    {
      "epoch": 1.803101356843619,
      "grad_norm": 2.029627561569214,
      "learning_rate": 1.9283236994219654e-05,
      "loss": 1.1603,
      "step": 7210
    },
    {
      "epoch": 1.8056024510723443,
      "grad_norm": 1.5172842741012573,
      "learning_rate": 1.9268157828600154e-05,
      "loss": 1.0749,
      "step": 7220
    },
    {
      "epoch": 1.8081035453010692,
      "grad_norm": 1.900556206703186,
      "learning_rate": 1.9253078662980647e-05,
      "loss": 1.1437,
      "step": 7230
    },
    {
      "epoch": 1.8106046395297943,
      "grad_norm": 2.111271381378174,
      "learning_rate": 1.9237999497361147e-05,
      "loss": 1.0614,
      "step": 7240
    },
    {
      "epoch": 1.8131057337585195,
      "grad_norm": 1.628043293952942,
      "learning_rate": 1.9222920331741646e-05,
      "loss": 1.0913,
      "step": 7250
    },
    {
      "epoch": 1.8156068279872444,
      "grad_norm": 2.1922154426574707,
      "learning_rate": 1.9207841166122143e-05,
      "loss": 1.0956,
      "step": 7260
    },
    {
      "epoch": 1.8181079222159695,
      "grad_norm": 1.9872123003005981,
      "learning_rate": 1.919276200050264e-05,
      "loss": 1.1125,
      "step": 7270
    },
    {
      "epoch": 1.8206090164446946,
      "grad_norm": 1.985129952430725,
      "learning_rate": 1.9177682834883136e-05,
      "loss": 1.1009,
      "step": 7280
    },
    {
      "epoch": 1.8231101106734195,
      "grad_norm": 2.200841188430786,
      "learning_rate": 1.9162603669263635e-05,
      "loss": 1.1087,
      "step": 7290
    },
    {
      "epoch": 1.8256112049021447,
      "grad_norm": 1.6213570833206177,
      "learning_rate": 1.9147524503644135e-05,
      "loss": 1.0987,
      "step": 7300
    },
    {
      "epoch": 1.8281122991308698,
      "grad_norm": 1.8828305006027222,
      "learning_rate": 1.9132445338024628e-05,
      "loss": 1.2362,
      "step": 7310
    },
    {
      "epoch": 1.8306133933595947,
      "grad_norm": 2.0303397178649902,
      "learning_rate": 1.9117366172405128e-05,
      "loss": 1.0894,
      "step": 7320
    },
    {
      "epoch": 1.8331144875883199,
      "grad_norm": 2.057455062866211,
      "learning_rate": 1.9102287006785624e-05,
      "loss": 1.0582,
      "step": 7330
    },
    {
      "epoch": 1.835615581817045,
      "grad_norm": 1.9815346002578735,
      "learning_rate": 1.908720784116612e-05,
      "loss": 1.0758,
      "step": 7340
    },
    {
      "epoch": 1.83811667604577,
      "grad_norm": 1.9935086965560913,
      "learning_rate": 1.907212867554662e-05,
      "loss": 1.038,
      "step": 7350
    },
    {
      "epoch": 1.8406177702744952,
      "grad_norm": 1.9276169538497925,
      "learning_rate": 1.9057049509927117e-05,
      "loss": 1.0878,
      "step": 7360
    },
    {
      "epoch": 1.8431188645032202,
      "grad_norm": 2.502546548843384,
      "learning_rate": 1.9041970344307617e-05,
      "loss": 1.0924,
      "step": 7370
    },
    {
      "epoch": 1.845619958731945,
      "grad_norm": 1.7716279029846191,
      "learning_rate": 1.9026891178688113e-05,
      "loss": 1.0614,
      "step": 7380
    },
    {
      "epoch": 1.8481210529606704,
      "grad_norm": 2.210076332092285,
      "learning_rate": 1.901181201306861e-05,
      "loss": 1.1212,
      "step": 7390
    },
    {
      "epoch": 1.8506221471893953,
      "grad_norm": 1.7182292938232422,
      "learning_rate": 1.899673284744911e-05,
      "loss": 1.0941,
      "step": 7400
    },
    {
      "epoch": 1.8531232414181205,
      "grad_norm": 1.9824893474578857,
      "learning_rate": 1.8981653681829606e-05,
      "loss": 1.0633,
      "step": 7410
    },
    {
      "epoch": 1.8556243356468456,
      "grad_norm": 1.8421485424041748,
      "learning_rate": 1.8966574516210102e-05,
      "loss": 1.0556,
      "step": 7420
    },
    {
      "epoch": 1.8581254298755705,
      "grad_norm": 1.4529461860656738,
      "learning_rate": 1.8951495350590602e-05,
      "loss": 1.0423,
      "step": 7430
    },
    {
      "epoch": 1.8606265241042956,
      "grad_norm": 2.174057960510254,
      "learning_rate": 1.89364161849711e-05,
      "loss": 1.2172,
      "step": 7440
    },
    {
      "epoch": 1.8631276183330208,
      "grad_norm": 1.866808533668518,
      "learning_rate": 1.89213370193516e-05,
      "loss": 1.0402,
      "step": 7450
    },
    {
      "epoch": 1.8656287125617457,
      "grad_norm": 1.9812729358673096,
      "learning_rate": 1.890625785373209e-05,
      "loss": 1.0745,
      "step": 7460
    },
    {
      "epoch": 1.8681298067904708,
      "grad_norm": 1.6258894205093384,
      "learning_rate": 1.889117868811259e-05,
      "loss": 1.052,
      "step": 7470
    },
    {
      "epoch": 1.870630901019196,
      "grad_norm": 1.7432440519332886,
      "learning_rate": 1.887609952249309e-05,
      "loss": 1.17,
      "step": 7480
    },
    {
      "epoch": 1.8731319952479208,
      "grad_norm": 1.6573398113250732,
      "learning_rate": 1.8861020356873588e-05,
      "loss": 1.076,
      "step": 7490
    },
    {
      "epoch": 1.8756330894766462,
      "grad_norm": 1.4280914068222046,
      "learning_rate": 1.8845941191254084e-05,
      "loss": 1.0307,
      "step": 7500
    },
    {
      "epoch": 1.878134183705371,
      "grad_norm": 1.9772460460662842,
      "learning_rate": 1.8830862025634584e-05,
      "loss": 1.1554,
      "step": 7510
    },
    {
      "epoch": 1.880635277934096,
      "grad_norm": 1.7746102809906006,
      "learning_rate": 1.881578286001508e-05,
      "loss": 1.0655,
      "step": 7520
    },
    {
      "epoch": 1.8831363721628214,
      "grad_norm": 2.58205509185791,
      "learning_rate": 1.8800703694395577e-05,
      "loss": 1.1229,
      "step": 7530
    },
    {
      "epoch": 1.8856374663915463,
      "grad_norm": 1.9806652069091797,
      "learning_rate": 1.8785624528776073e-05,
      "loss": 1.1003,
      "step": 7540
    },
    {
      "epoch": 1.8881385606202714,
      "grad_norm": 2.371793746948242,
      "learning_rate": 1.8770545363156573e-05,
      "loss": 1.1093,
      "step": 7550
    },
    {
      "epoch": 1.8906396548489965,
      "grad_norm": 2.2807834148406982,
      "learning_rate": 1.8755466197537073e-05,
      "loss": 1.0514,
      "step": 7560
    },
    {
      "epoch": 1.8931407490777215,
      "grad_norm": 1.9107915163040161,
      "learning_rate": 1.8740387031917566e-05,
      "loss": 1.0897,
      "step": 7570
    },
    {
      "epoch": 1.8956418433064466,
      "grad_norm": 2.5344080924987793,
      "learning_rate": 1.8725307866298065e-05,
      "loss": 1.0421,
      "step": 7580
    },
    {
      "epoch": 1.8981429375351717,
      "grad_norm": 1.875951886177063,
      "learning_rate": 1.8710228700678562e-05,
      "loss": 1.1148,
      "step": 7590
    },
    {
      "epoch": 1.9006440317638966,
      "grad_norm": 1.8322261571884155,
      "learning_rate": 1.869514953505906e-05,
      "loss": 1.0482,
      "step": 7600
    },
    {
      "epoch": 1.9031451259926218,
      "grad_norm": 2.5369441509246826,
      "learning_rate": 1.8680070369439558e-05,
      "loss": 1.0186,
      "step": 7610
    },
    {
      "epoch": 1.9056462202213469,
      "grad_norm": 2.1420257091522217,
      "learning_rate": 1.8664991203820055e-05,
      "loss": 1.0583,
      "step": 7620
    },
    {
      "epoch": 1.9081473144500718,
      "grad_norm": 1.6662486791610718,
      "learning_rate": 1.8649912038200554e-05,
      "loss": 1.0894,
      "step": 7630
    },
    {
      "epoch": 1.910648408678797,
      "grad_norm": 2.325331926345825,
      "learning_rate": 1.863483287258105e-05,
      "loss": 1.0783,
      "step": 7640
    },
    {
      "epoch": 1.913149502907522,
      "grad_norm": 1.619154453277588,
      "learning_rate": 1.8619753706961547e-05,
      "loss": 1.0327,
      "step": 7650
    },
    {
      "epoch": 1.915650597136247,
      "grad_norm": 1.9679641723632812,
      "learning_rate": 1.8604674541342047e-05,
      "loss": 1.0878,
      "step": 7660
    },
    {
      "epoch": 1.9181516913649723,
      "grad_norm": 1.8821545839309692,
      "learning_rate": 1.8589595375722543e-05,
      "loss": 1.0647,
      "step": 7670
    },
    {
      "epoch": 1.9206527855936972,
      "grad_norm": 1.624426245689392,
      "learning_rate": 1.8574516210103043e-05,
      "loss": 1.1377,
      "step": 7680
    },
    {
      "epoch": 1.9231538798224224,
      "grad_norm": 1.857763409614563,
      "learning_rate": 1.855943704448354e-05,
      "loss": 1.2116,
      "step": 7690
    },
    {
      "epoch": 1.9256549740511475,
      "grad_norm": 1.7005393505096436,
      "learning_rate": 1.8544357878864036e-05,
      "loss": 1.1775,
      "step": 7700
    },
    {
      "epoch": 1.9281560682798724,
      "grad_norm": 1.5126913785934448,
      "learning_rate": 1.8529278713244536e-05,
      "loss": 1.0956,
      "step": 7710
    },
    {
      "epoch": 1.9306571625085975,
      "grad_norm": 2.434643268585205,
      "learning_rate": 1.851419954762503e-05,
      "loss": 1.0956,
      "step": 7720
    },
    {
      "epoch": 1.9331582567373227,
      "grad_norm": 2.144991397857666,
      "learning_rate": 1.849912038200553e-05,
      "loss": 1.1107,
      "step": 7730
    },
    {
      "epoch": 1.9356593509660476,
      "grad_norm": 1.9762511253356934,
      "learning_rate": 1.848404121638603e-05,
      "loss": 1.1134,
      "step": 7740
    },
    {
      "epoch": 1.9381604451947727,
      "grad_norm": 1.9117087125778198,
      "learning_rate": 1.8468962050766525e-05,
      "loss": 1.1324,
      "step": 7750
    },
    {
      "epoch": 1.9406615394234978,
      "grad_norm": 2.5337893962860107,
      "learning_rate": 1.845388288514702e-05,
      "loss": 1.1283,
      "step": 7760
    },
    {
      "epoch": 1.9431626336522227,
      "grad_norm": 1.4088352918624878,
      "learning_rate": 1.8438803719527518e-05,
      "loss": 1.0262,
      "step": 7770
    },
    {
      "epoch": 1.9456637278809479,
      "grad_norm": 1.985780119895935,
      "learning_rate": 1.8423724553908018e-05,
      "loss": 1.0471,
      "step": 7780
    },
    {
      "epoch": 1.948164822109673,
      "grad_norm": 1.9787911176681519,
      "learning_rate": 1.8408645388288517e-05,
      "loss": 1.0646,
      "step": 7790
    },
    {
      "epoch": 1.950665916338398,
      "grad_norm": 2.1946053504943848,
      "learning_rate": 1.839356622266901e-05,
      "loss": 1.0831,
      "step": 7800
    },
    {
      "epoch": 1.9531670105671233,
      "grad_norm": 1.7433542013168335,
      "learning_rate": 1.837848705704951e-05,
      "loss": 1.0427,
      "step": 7810
    },
    {
      "epoch": 1.9556681047958482,
      "grad_norm": 1.698953628540039,
      "learning_rate": 1.836340789143001e-05,
      "loss": 1.0401,
      "step": 7820
    },
    {
      "epoch": 1.958169199024573,
      "grad_norm": 1.546944499015808,
      "learning_rate": 1.8348328725810506e-05,
      "loss": 1.0685,
      "step": 7830
    },
    {
      "epoch": 1.9606702932532984,
      "grad_norm": 1.729493260383606,
      "learning_rate": 1.8333249560191003e-05,
      "loss": 1.0869,
      "step": 7840
    },
    {
      "epoch": 1.9631713874820234,
      "grad_norm": 1.9154367446899414,
      "learning_rate": 1.83181703945715e-05,
      "loss": 1.0893,
      "step": 7850
    },
    {
      "epoch": 1.9656724817107485,
      "grad_norm": 2.0488288402557373,
      "learning_rate": 1.8303091228952e-05,
      "loss": 1.0986,
      "step": 7860
    },
    {
      "epoch": 1.9681735759394736,
      "grad_norm": 1.8890072107315063,
      "learning_rate": 1.82880120633325e-05,
      "loss": 0.995,
      "step": 7870
    },
    {
      "epoch": 1.9706746701681985,
      "grad_norm": 1.7092530727386475,
      "learning_rate": 1.8272932897712992e-05,
      "loss": 1.0706,
      "step": 7880
    },
    {
      "epoch": 1.9731757643969237,
      "grad_norm": 1.8417943716049194,
      "learning_rate": 1.8257853732093492e-05,
      "loss": 1.0725,
      "step": 7890
    },
    {
      "epoch": 1.9756768586256488,
      "grad_norm": 1.7854033708572388,
      "learning_rate": 1.8242774566473988e-05,
      "loss": 1.0895,
      "step": 7900
    },
    {
      "epoch": 1.9781779528543737,
      "grad_norm": 1.8835941553115845,
      "learning_rate": 1.8227695400854485e-05,
      "loss": 1.0997,
      "step": 7910
    },
    {
      "epoch": 1.9806790470830988,
      "grad_norm": 2.1950831413269043,
      "learning_rate": 1.8212616235234984e-05,
      "loss": 1.1093,
      "step": 7920
    },
    {
      "epoch": 1.983180141311824,
      "grad_norm": 1.7571932077407837,
      "learning_rate": 1.819753706961548e-05,
      "loss": 1.0801,
      "step": 7930
    },
    {
      "epoch": 1.9856812355405489,
      "grad_norm": 1.8071955442428589,
      "learning_rate": 1.818245790399598e-05,
      "loss": 1.0926,
      "step": 7940
    },
    {
      "epoch": 1.988182329769274,
      "grad_norm": 2.318774700164795,
      "learning_rate": 1.8167378738376477e-05,
      "loss": 1.034,
      "step": 7950
    },
    {
      "epoch": 1.9906834239979991,
      "grad_norm": 1.9471389055252075,
      "learning_rate": 1.8152299572756973e-05,
      "loss": 1.0568,
      "step": 7960
    },
    {
      "epoch": 1.993184518226724,
      "grad_norm": 2.1021151542663574,
      "learning_rate": 1.8137220407137473e-05,
      "loss": 1.1105,
      "step": 7970
    },
    {
      "epoch": 1.9956856124554494,
      "grad_norm": 2.250574827194214,
      "learning_rate": 1.812214124151797e-05,
      "loss": 1.0865,
      "step": 7980
    },
    {
      "epoch": 1.9981867066841743,
      "grad_norm": 1.8213282823562622,
      "learning_rate": 1.8107062075898466e-05,
      "loss": 1.0715,
      "step": 7990
    },
    {
      "epoch": 2.000500218845745,
      "grad_norm": 2.014328718185425,
      "learning_rate": 1.8091982910278966e-05,
      "loss": 1.0789,
      "step": 8000
    },
    {
      "epoch": 2.00300131307447,
      "grad_norm": 1.8028252124786377,
      "learning_rate": 1.8076903744659462e-05,
      "loss": 1.1048,
      "step": 8010
    },
    {
      "epoch": 2.005502407303195,
      "grad_norm": 2.3393635749816895,
      "learning_rate": 1.8061824579039962e-05,
      "loss": 1.0528,
      "step": 8020
    },
    {
      "epoch": 2.0080035015319204,
      "grad_norm": 1.8886587619781494,
      "learning_rate": 1.8046745413420455e-05,
      "loss": 1.0219,
      "step": 8030
    },
    {
      "epoch": 2.0105045957606453,
      "grad_norm": 1.978552222251892,
      "learning_rate": 1.8031666247800955e-05,
      "loss": 1.1123,
      "step": 8040
    },
    {
      "epoch": 2.0130056899893702,
      "grad_norm": 1.657240629196167,
      "learning_rate": 1.8016587082181455e-05,
      "loss": 1.0903,
      "step": 8050
    },
    {
      "epoch": 2.0155067842180956,
      "grad_norm": 1.7170308828353882,
      "learning_rate": 1.8003015833123902e-05,
      "loss": 1.053,
      "step": 8060
    },
    {
      "epoch": 2.0180078784468205,
      "grad_norm": 1.7205266952514648,
      "learning_rate": 1.7987936667504398e-05,
      "loss": 1.1005,
      "step": 8070
    },
    {
      "epoch": 2.0205089726755454,
      "grad_norm": 2.526984453201294,
      "learning_rate": 1.7972857501884895e-05,
      "loss": 1.1003,
      "step": 8080
    },
    {
      "epoch": 2.0230100669042708,
      "grad_norm": 1.5442503690719604,
      "learning_rate": 1.7957778336265394e-05,
      "loss": 1.0909,
      "step": 8090
    },
    {
      "epoch": 2.0255111611329957,
      "grad_norm": 1.9747085571289062,
      "learning_rate": 1.794269917064589e-05,
      "loss": 1.0777,
      "step": 8100
    },
    {
      "epoch": 2.0280122553617206,
      "grad_norm": 2.217729330062866,
      "learning_rate": 1.792762000502639e-05,
      "loss": 1.0472,
      "step": 8110
    },
    {
      "epoch": 2.030513349590446,
      "grad_norm": 1.9480966329574585,
      "learning_rate": 1.7912540839406887e-05,
      "loss": 1.0754,
      "step": 8120
    },
    {
      "epoch": 2.033014443819171,
      "grad_norm": 1.7800170183181763,
      "learning_rate": 1.7897461673787384e-05,
      "loss": 1.0804,
      "step": 8130
    },
    {
      "epoch": 2.0355155380478958,
      "grad_norm": 2.2463748455047607,
      "learning_rate": 1.7882382508167883e-05,
      "loss": 1.0702,
      "step": 8140
    },
    {
      "epoch": 2.038016632276621,
      "grad_norm": 1.895548939704895,
      "learning_rate": 1.7867303342548376e-05,
      "loss": 1.1906,
      "step": 8150
    },
    {
      "epoch": 2.040517726505346,
      "grad_norm": 1.9601945877075195,
      "learning_rate": 1.7852224176928876e-05,
      "loss": 1.0997,
      "step": 8160
    },
    {
      "epoch": 2.0430188207340714,
      "grad_norm": 1.805010437965393,
      "learning_rate": 1.7837145011309376e-05,
      "loss": 1.0906,
      "step": 8170
    },
    {
      "epoch": 2.0455199149627963,
      "grad_norm": 1.5034430027008057,
      "learning_rate": 1.7822065845689872e-05,
      "loss": 1.0744,
      "step": 8180
    },
    {
      "epoch": 2.048021009191521,
      "grad_norm": 2.440251111984253,
      "learning_rate": 1.780698668007037e-05,
      "loss": 1.1145,
      "step": 8190
    },
    {
      "epoch": 2.0505221034202465,
      "grad_norm": 1.694313645362854,
      "learning_rate": 1.7791907514450865e-05,
      "loss": 1.1126,
      "step": 8200
    },
    {
      "epoch": 2.0530231976489715,
      "grad_norm": 1.8972543478012085,
      "learning_rate": 1.7776828348831365e-05,
      "loss": 1.1035,
      "step": 8210
    },
    {
      "epoch": 2.0555242918776964,
      "grad_norm": 2.103243589401245,
      "learning_rate": 1.7761749183211865e-05,
      "loss": 1.0807,
      "step": 8220
    },
    {
      "epoch": 2.0580253861064217,
      "grad_norm": 2.234670877456665,
      "learning_rate": 1.7746670017592358e-05,
      "loss": 1.0279,
      "step": 8230
    },
    {
      "epoch": 2.0605264803351466,
      "grad_norm": 2.6004600524902344,
      "learning_rate": 1.7731590851972858e-05,
      "loss": 1.091,
      "step": 8240
    },
    {
      "epoch": 2.0630275745638715,
      "grad_norm": 1.9566011428833008,
      "learning_rate": 1.7716511686353358e-05,
      "loss": 1.1969,
      "step": 8250
    },
    {
      "epoch": 2.065528668792597,
      "grad_norm": 2.0135276317596436,
      "learning_rate": 1.7701432520733854e-05,
      "loss": 1.1168,
      "step": 8260
    },
    {
      "epoch": 2.068029763021322,
      "grad_norm": 1.8205993175506592,
      "learning_rate": 1.768635335511435e-05,
      "loss": 1.1496,
      "step": 8270
    },
    {
      "epoch": 2.0705308572500467,
      "grad_norm": 2.002781867980957,
      "learning_rate": 1.7671274189494847e-05,
      "loss": 1.0875,
      "step": 8280
    },
    {
      "epoch": 2.073031951478772,
      "grad_norm": 1.8551044464111328,
      "learning_rate": 1.7656195023875347e-05,
      "loss": 1.0957,
      "step": 8290
    },
    {
      "epoch": 2.075533045707497,
      "grad_norm": 1.8654537200927734,
      "learning_rate": 1.7641115858255846e-05,
      "loss": 1.1009,
      "step": 8300
    },
    {
      "epoch": 2.0780341399362223,
      "grad_norm": 1.7707288265228271,
      "learning_rate": 1.762603669263634e-05,
      "loss": 1.1102,
      "step": 8310
    },
    {
      "epoch": 2.0805352341649472,
      "grad_norm": 2.024214267730713,
      "learning_rate": 1.761095752701684e-05,
      "loss": 1.1424,
      "step": 8320
    },
    {
      "epoch": 2.083036328393672,
      "grad_norm": 1.8965061902999878,
      "learning_rate": 1.7595878361397336e-05,
      "loss": 1.0629,
      "step": 8330
    },
    {
      "epoch": 2.0855374226223975,
      "grad_norm": 1.9688174724578857,
      "learning_rate": 1.7580799195777832e-05,
      "loss": 1.1284,
      "step": 8340
    },
    {
      "epoch": 2.0880385168511224,
      "grad_norm": 1.991923451423645,
      "learning_rate": 1.7565720030158332e-05,
      "loss": 1.1231,
      "step": 8350
    },
    {
      "epoch": 2.0905396110798473,
      "grad_norm": 2.074571132659912,
      "learning_rate": 1.7550640864538828e-05,
      "loss": 1.1027,
      "step": 8360
    },
    {
      "epoch": 2.0930407053085727,
      "grad_norm": 2.134291172027588,
      "learning_rate": 1.7535561698919328e-05,
      "loss": 1.0829,
      "step": 8370
    },
    {
      "epoch": 2.0955417995372976,
      "grad_norm": 2.5772626399993896,
      "learning_rate": 1.7520482533299825e-05,
      "loss": 1.0725,
      "step": 8380
    },
    {
      "epoch": 2.0980428937660225,
      "grad_norm": 2.279078245162964,
      "learning_rate": 1.750540336768032e-05,
      "loss": 1.0648,
      "step": 8390
    },
    {
      "epoch": 2.100543987994748,
      "grad_norm": 1.9886679649353027,
      "learning_rate": 1.749032420206082e-05,
      "loss": 1.1219,
      "step": 8400
    },
    {
      "epoch": 2.1030450822234728,
      "grad_norm": 1.7788360118865967,
      "learning_rate": 1.7475245036441317e-05,
      "loss": 1.0922,
      "step": 8410
    },
    {
      "epoch": 2.1055461764521977,
      "grad_norm": 1.9263187646865845,
      "learning_rate": 1.7460165870821814e-05,
      "loss": 1.1766,
      "step": 8420
    },
    {
      "epoch": 2.108047270680923,
      "grad_norm": 1.7849748134613037,
      "learning_rate": 1.7445086705202313e-05,
      "loss": 1.0948,
      "step": 8430
    },
    {
      "epoch": 2.110548364909648,
      "grad_norm": 2.5376694202423096,
      "learning_rate": 1.743000753958281e-05,
      "loss": 1.0656,
      "step": 8440
    },
    {
      "epoch": 2.113049459138373,
      "grad_norm": 2.0965983867645264,
      "learning_rate": 1.741492837396331e-05,
      "loss": 1.099,
      "step": 8450
    },
    {
      "epoch": 2.115550553367098,
      "grad_norm": 2.620180130004883,
      "learning_rate": 1.7399849208343803e-05,
      "loss": 1.0936,
      "step": 8460
    },
    {
      "epoch": 2.118051647595823,
      "grad_norm": 2.303760051727295,
      "learning_rate": 1.7384770042724302e-05,
      "loss": 0.9952,
      "step": 8470
    },
    {
      "epoch": 2.1205527418245484,
      "grad_norm": 1.684340476989746,
      "learning_rate": 1.7369690877104802e-05,
      "loss": 1.0151,
      "step": 8480
    },
    {
      "epoch": 2.1230538360532734,
      "grad_norm": 2.4834232330322266,
      "learning_rate": 1.73546117114853e-05,
      "loss": 1.0514,
      "step": 8490
    },
    {
      "epoch": 2.1255549302819983,
      "grad_norm": 1.943626880645752,
      "learning_rate": 1.7339532545865795e-05,
      "loss": 1.025,
      "step": 8500
    },
    {
      "epoch": 2.1280560245107236,
      "grad_norm": 2.381333589553833,
      "learning_rate": 1.7324453380246295e-05,
      "loss": 1.063,
      "step": 8510
    },
    {
      "epoch": 2.1305571187394485,
      "grad_norm": 2.522416114807129,
      "learning_rate": 1.730937421462679e-05,
      "loss": 0.987,
      "step": 8520
    },
    {
      "epoch": 2.1330582129681734,
      "grad_norm": 2.2302157878875732,
      "learning_rate": 1.7294295049007288e-05,
      "loss": 1.0873,
      "step": 8530
    },
    {
      "epoch": 2.135559307196899,
      "grad_norm": 1.844623327255249,
      "learning_rate": 1.7279215883387784e-05,
      "loss": 1.0784,
      "step": 8540
    },
    {
      "epoch": 2.1380604014256237,
      "grad_norm": 1.987170934677124,
      "learning_rate": 1.7264136717768284e-05,
      "loss": 1.0608,
      "step": 8550
    },
    {
      "epoch": 2.1405614956543486,
      "grad_norm": 2.094599723815918,
      "learning_rate": 1.7249057552148784e-05,
      "loss": 1.1203,
      "step": 8560
    },
    {
      "epoch": 2.143062589883074,
      "grad_norm": 1.7769790887832642,
      "learning_rate": 1.7233978386529277e-05,
      "loss": 1.0174,
      "step": 8570
    },
    {
      "epoch": 2.145563684111799,
      "grad_norm": 1.8843916654586792,
      "learning_rate": 1.7218899220909777e-05,
      "loss": 1.0647,
      "step": 8580
    },
    {
      "epoch": 2.148064778340524,
      "grad_norm": 1.563133955001831,
      "learning_rate": 1.7203820055290273e-05,
      "loss": 1.0235,
      "step": 8590
    },
    {
      "epoch": 2.150565872569249,
      "grad_norm": 1.942138671875,
      "learning_rate": 1.7188740889670773e-05,
      "loss": 1.12,
      "step": 8600
    },
    {
      "epoch": 2.153066966797974,
      "grad_norm": 1.6748631000518799,
      "learning_rate": 1.717366172405127e-05,
      "loss": 1.0989,
      "step": 8610
    },
    {
      "epoch": 2.155568061026699,
      "grad_norm": 2.3355112075805664,
      "learning_rate": 1.7158582558431766e-05,
      "loss": 1.0623,
      "step": 8620
    },
    {
      "epoch": 2.1580691552554243,
      "grad_norm": 2.4234201908111572,
      "learning_rate": 1.7143503392812266e-05,
      "loss": 1.1043,
      "step": 8630
    },
    {
      "epoch": 2.160570249484149,
      "grad_norm": 1.8536169528961182,
      "learning_rate": 1.7128424227192765e-05,
      "loss": 1.0485,
      "step": 8640
    },
    {
      "epoch": 2.1630713437128746,
      "grad_norm": 1.8522902727127075,
      "learning_rate": 1.711334506157326e-05,
      "loss": 1.2412,
      "step": 8650
    },
    {
      "epoch": 2.1655724379415995,
      "grad_norm": 2.18123197555542,
      "learning_rate": 1.7098265895953758e-05,
      "loss": 1.1481,
      "step": 8660
    },
    {
      "epoch": 2.1680735321703244,
      "grad_norm": 1.8254972696304321,
      "learning_rate": 1.7083186730334255e-05,
      "loss": 1.0568,
      "step": 8670
    },
    {
      "epoch": 2.1705746263990497,
      "grad_norm": 1.9924395084381104,
      "learning_rate": 1.7068107564714754e-05,
      "loss": 1.0479,
      "step": 8680
    },
    {
      "epoch": 2.1730757206277747,
      "grad_norm": 2.0040042400360107,
      "learning_rate": 1.705302839909525e-05,
      "loss": 1.0111,
      "step": 8690
    },
    {
      "epoch": 2.1755768148564996,
      "grad_norm": 1.889377236366272,
      "learning_rate": 1.7037949233475747e-05,
      "loss": 1.1131,
      "step": 8700
    },
    {
      "epoch": 2.178077909085225,
      "grad_norm": 1.692763328552246,
      "learning_rate": 1.7022870067856247e-05,
      "loss": 1.1102,
      "step": 8710
    },
    {
      "epoch": 2.18057900331395,
      "grad_norm": 2.1043689250946045,
      "learning_rate": 1.700779090223674e-05,
      "loss": 1.039,
      "step": 8720
    },
    {
      "epoch": 2.1830800975426747,
      "grad_norm": 2.453638792037964,
      "learning_rate": 1.699271173661724e-05,
      "loss": 1.1079,
      "step": 8730
    },
    {
      "epoch": 2.1855811917714,
      "grad_norm": 1.7864166498184204,
      "learning_rate": 1.697763257099774e-05,
      "loss": 1.113,
      "step": 8740
    },
    {
      "epoch": 2.188082286000125,
      "grad_norm": 2.2534170150756836,
      "learning_rate": 1.6962553405378236e-05,
      "loss": 1.0877,
      "step": 8750
    },
    {
      "epoch": 2.1905833802288504,
      "grad_norm": 2.763869285583496,
      "learning_rate": 1.6947474239758733e-05,
      "loss": 1.0511,
      "step": 8760
    },
    {
      "epoch": 2.1930844744575753,
      "grad_norm": 2.194491386413574,
      "learning_rate": 1.6932395074139232e-05,
      "loss": 1.0471,
      "step": 8770
    },
    {
      "epoch": 2.1955855686863,
      "grad_norm": 1.9493310451507568,
      "learning_rate": 1.691731590851973e-05,
      "loss": 1.0755,
      "step": 8780
    },
    {
      "epoch": 2.1980866629150255,
      "grad_norm": 1.8430497646331787,
      "learning_rate": 1.690223674290023e-05,
      "loss": 1.0954,
      "step": 8790
    },
    {
      "epoch": 2.2005877571437504,
      "grad_norm": 2.103027105331421,
      "learning_rate": 1.688715757728072e-05,
      "loss": 1.068,
      "step": 8800
    },
    {
      "epoch": 2.2030888513724753,
      "grad_norm": 2.106199026107788,
      "learning_rate": 1.687207841166122e-05,
      "loss": 1.0906,
      "step": 8810
    },
    {
      "epoch": 2.2055899456012007,
      "grad_norm": 1.8496363162994385,
      "learning_rate": 1.685699924604172e-05,
      "loss": 1.0426,
      "step": 8820
    },
    {
      "epoch": 2.2080910398299256,
      "grad_norm": 2.1459639072418213,
      "learning_rate": 1.6841920080422218e-05,
      "loss": 1.1158,
      "step": 8830
    },
    {
      "epoch": 2.2105921340586505,
      "grad_norm": 1.8049098253250122,
      "learning_rate": 1.6826840914802714e-05,
      "loss": 1.1495,
      "step": 8840
    },
    {
      "epoch": 2.213093228287376,
      "grad_norm": 1.8845216035842896,
      "learning_rate": 1.681176174918321e-05,
      "loss": 1.0694,
      "step": 8850
    },
    {
      "epoch": 2.215594322516101,
      "grad_norm": 1.8791778087615967,
      "learning_rate": 1.679668258356371e-05,
      "loss": 1.079,
      "step": 8860
    },
    {
      "epoch": 2.2180954167448257,
      "grad_norm": 2.2943108081817627,
      "learning_rate": 1.678160341794421e-05,
      "loss": 1.0337,
      "step": 8870
    },
    {
      "epoch": 2.220596510973551,
      "grad_norm": 1.7378573417663574,
      "learning_rate": 1.6766524252324703e-05,
      "loss": 1.1053,
      "step": 8880
    },
    {
      "epoch": 2.223097605202276,
      "grad_norm": 2.29695463180542,
      "learning_rate": 1.6751445086705203e-05,
      "loss": 1.0843,
      "step": 8890
    },
    {
      "epoch": 2.225598699431001,
      "grad_norm": 1.9452723264694214,
      "learning_rate": 1.6736365921085703e-05,
      "loss": 1.0853,
      "step": 8900
    },
    {
      "epoch": 2.228099793659726,
      "grad_norm": 1.883211374282837,
      "learning_rate": 1.6721286755466196e-05,
      "loss": 1.1175,
      "step": 8910
    },
    {
      "epoch": 2.230600887888451,
      "grad_norm": 1.8412690162658691,
      "learning_rate": 1.6706207589846696e-05,
      "loss": 1.0864,
      "step": 8920
    },
    {
      "epoch": 2.2331019821171765,
      "grad_norm": 1.5991164445877075,
      "learning_rate": 1.6691128424227192e-05,
      "loss": 1.0324,
      "step": 8930
    },
    {
      "epoch": 2.2356030763459014,
      "grad_norm": 1.9964584112167358,
      "learning_rate": 1.6676049258607692e-05,
      "loss": 1.0862,
      "step": 8940
    },
    {
      "epoch": 2.2381041705746263,
      "grad_norm": 2.3179333209991455,
      "learning_rate": 1.6660970092988188e-05,
      "loss": 1.1091,
      "step": 8950
    },
    {
      "epoch": 2.2406052648033516,
      "grad_norm": 1.907081127166748,
      "learning_rate": 1.6645890927368685e-05,
      "loss": 1.0318,
      "step": 8960
    },
    {
      "epoch": 2.2431063590320766,
      "grad_norm": 2.507197141647339,
      "learning_rate": 1.6630811761749184e-05,
      "loss": 1.1189,
      "step": 8970
    },
    {
      "epoch": 2.2456074532608015,
      "grad_norm": 2.2089273929595947,
      "learning_rate": 1.661573259612968e-05,
      "loss": 1.0527,
      "step": 8980
    },
    {
      "epoch": 2.248108547489527,
      "grad_norm": 2.030130624771118,
      "learning_rate": 1.6600653430510177e-05,
      "loss": 1.0737,
      "step": 8990
    },
    {
      "epoch": 2.2506096417182517,
      "grad_norm": 2.139472007751465,
      "learning_rate": 1.6585574264890677e-05,
      "loss": 1.0393,
      "step": 9000
    },
    {
      "epoch": 2.2531107359469766,
      "grad_norm": 2.5326621532440186,
      "learning_rate": 1.6570495099271173e-05,
      "loss": 1.1087,
      "step": 9010
    },
    {
      "epoch": 2.255611830175702,
      "grad_norm": 2.1627166271209717,
      "learning_rate": 1.6555415933651673e-05,
      "loss": 1.0224,
      "step": 9020
    },
    {
      "epoch": 2.258112924404427,
      "grad_norm": 1.7373937368392944,
      "learning_rate": 1.654033676803217e-05,
      "loss": 1.0887,
      "step": 9030
    },
    {
      "epoch": 2.260614018633152,
      "grad_norm": 2.1937921047210693,
      "learning_rate": 1.6525257602412666e-05,
      "loss": 1.1546,
      "step": 9040
    },
    {
      "epoch": 2.263115112861877,
      "grad_norm": 1.7727668285369873,
      "learning_rate": 1.6510178436793166e-05,
      "loss": 1.0512,
      "step": 9050
    },
    {
      "epoch": 2.265616207090602,
      "grad_norm": 1.8487697839736938,
      "learning_rate": 1.6495099271173662e-05,
      "loss": 1.1853,
      "step": 9060
    },
    {
      "epoch": 2.268117301319327,
      "grad_norm": 1.8113526105880737,
      "learning_rate": 1.648002010555416e-05,
      "loss": 1.0883,
      "step": 9070
    },
    {
      "epoch": 2.2706183955480523,
      "grad_norm": 2.151240825653076,
      "learning_rate": 1.646494093993466e-05,
      "loss": 1.0642,
      "step": 9080
    },
    {
      "epoch": 2.2731194897767772,
      "grad_norm": 2.6351420879364014,
      "learning_rate": 1.6449861774315155e-05,
      "loss": 1.0578,
      "step": 9090
    },
    {
      "epoch": 2.2756205840055026,
      "grad_norm": 1.9915528297424316,
      "learning_rate": 1.643478260869565e-05,
      "loss": 1.1856,
      "step": 9100
    },
    {
      "epoch": 2.2781216782342275,
      "grad_norm": 2.196972370147705,
      "learning_rate": 1.6419703443076148e-05,
      "loss": 1.0741,
      "step": 9110
    },
    {
      "epoch": 2.2806227724629524,
      "grad_norm": 2.1447339057922363,
      "learning_rate": 1.6404624277456648e-05,
      "loss": 1.0649,
      "step": 9120
    },
    {
      "epoch": 2.2831238666916778,
      "grad_norm": 1.9305601119995117,
      "learning_rate": 1.6389545111837147e-05,
      "loss": 1.0427,
      "step": 9130
    },
    {
      "epoch": 2.2856249609204027,
      "grad_norm": 1.8022971153259277,
      "learning_rate": 1.637446594621764e-05,
      "loss": 1.0933,
      "step": 9140
    },
    {
      "epoch": 2.2881260551491276,
      "grad_norm": 1.965545654296875,
      "learning_rate": 1.635938678059814e-05,
      "loss": 1.0825,
      "step": 9150
    },
    {
      "epoch": 2.290627149377853,
      "grad_norm": 2.2911665439605713,
      "learning_rate": 1.634430761497864e-05,
      "loss": 1.0621,
      "step": 9160
    },
    {
      "epoch": 2.293128243606578,
      "grad_norm": 1.9795905351638794,
      "learning_rate": 1.6329228449359137e-05,
      "loss": 1.1148,
      "step": 9170
    },
    {
      "epoch": 2.2956293378353028,
      "grad_norm": 2.3463191986083984,
      "learning_rate": 1.6314149283739633e-05,
      "loss": 1.0732,
      "step": 9180
    },
    {
      "epoch": 2.298130432064028,
      "grad_norm": 1.81947660446167,
      "learning_rate": 1.629907011812013e-05,
      "loss": 1.1088,
      "step": 9190
    },
    {
      "epoch": 2.300631526292753,
      "grad_norm": 1.8257801532745361,
      "learning_rate": 1.628399095250063e-05,
      "loss": 1.0611,
      "step": 9200
    },
    {
      "epoch": 2.3031326205214784,
      "grad_norm": 2.650514841079712,
      "learning_rate": 1.626891178688113e-05,
      "loss": 1.0529,
      "step": 9210
    },
    {
      "epoch": 2.3056337147502033,
      "grad_norm": 2.3049991130828857,
      "learning_rate": 1.6253832621261622e-05,
      "loss": 1.0768,
      "step": 9220
    },
    {
      "epoch": 2.308134808978928,
      "grad_norm": 1.8607760667800903,
      "learning_rate": 1.6238753455642122e-05,
      "loss": 1.0958,
      "step": 9230
    },
    {
      "epoch": 2.310635903207653,
      "grad_norm": 1.6449027061462402,
      "learning_rate": 1.6223674290022618e-05,
      "loss": 1.0854,
      "step": 9240
    },
    {
      "epoch": 2.3131369974363785,
      "grad_norm": 1.5849378108978271,
      "learning_rate": 1.6208595124403118e-05,
      "loss": 1.0242,
      "step": 9250
    },
    {
      "epoch": 2.3156380916651034,
      "grad_norm": 1.9389396905899048,
      "learning_rate": 1.6193515958783614e-05,
      "loss": 1.0801,
      "step": 9260
    },
    {
      "epoch": 2.3181391858938287,
      "grad_norm": 1.9626940488815308,
      "learning_rate": 1.617843679316411e-05,
      "loss": 1.0207,
      "step": 9270
    },
    {
      "epoch": 2.3206402801225536,
      "grad_norm": 2.265531301498413,
      "learning_rate": 1.616335762754461e-05,
      "loss": 1.0845,
      "step": 9280
    },
    {
      "epoch": 2.3231413743512785,
      "grad_norm": 1.8767337799072266,
      "learning_rate": 1.6148278461925107e-05,
      "loss": 1.1143,
      "step": 9290
    },
    {
      "epoch": 2.325642468580004,
      "grad_norm": 1.805138349533081,
      "learning_rate": 1.6133199296305604e-05,
      "loss": 1.089,
      "step": 9300
    },
    {
      "epoch": 2.328143562808729,
      "grad_norm": 2.1619620323181152,
      "learning_rate": 1.6118120130686103e-05,
      "loss": 1.0696,
      "step": 9310
    },
    {
      "epoch": 2.3306446570374537,
      "grad_norm": 2.4014182090759277,
      "learning_rate": 1.61030409650666e-05,
      "loss": 1.093,
      "step": 9320
    },
    {
      "epoch": 2.333145751266179,
      "grad_norm": 1.8121305704116821,
      "learning_rate": 1.6087961799447096e-05,
      "loss": 1.2129,
      "step": 9330
    },
    {
      "epoch": 2.335646845494904,
      "grad_norm": 1.6602439880371094,
      "learning_rate": 1.6072882633827596e-05,
      "loss": 1.0542,
      "step": 9340
    },
    {
      "epoch": 2.338147939723629,
      "grad_norm": 1.794198989868164,
      "learning_rate": 1.6057803468208092e-05,
      "loss": 1.1387,
      "step": 9350
    },
    {
      "epoch": 2.3406490339523542,
      "grad_norm": 2.910214900970459,
      "learning_rate": 1.6042724302588592e-05,
      "loss": 1.0949,
      "step": 9360
    },
    {
      "epoch": 2.343150128181079,
      "grad_norm": 1.7017098665237427,
      "learning_rate": 1.6027645136969085e-05,
      "loss": 1.0587,
      "step": 9370
    },
    {
      "epoch": 2.3456512224098045,
      "grad_norm": 2.0216071605682373,
      "learning_rate": 1.6012565971349585e-05,
      "loss": 1.166,
      "step": 9380
    },
    {
      "epoch": 2.3481523166385294,
      "grad_norm": 2.5101583003997803,
      "learning_rate": 1.5997486805730085e-05,
      "loss": 1.0325,
      "step": 9390
    },
    {
      "epoch": 2.3506534108672543,
      "grad_norm": 2.7389004230499268,
      "learning_rate": 1.598240764011058e-05,
      "loss": 1.1308,
      "step": 9400
    },
    {
      "epoch": 2.3531545050959797,
      "grad_norm": 1.9128674268722534,
      "learning_rate": 1.5967328474491078e-05,
      "loss": 1.0767,
      "step": 9410
    },
    {
      "epoch": 2.3556555993247046,
      "grad_norm": 1.9585665464401245,
      "learning_rate": 1.5952249308871577e-05,
      "loss": 1.1223,
      "step": 9420
    },
    {
      "epoch": 2.3581566935534295,
      "grad_norm": 1.9285900592803955,
      "learning_rate": 1.5937170143252074e-05,
      "loss": 1.1115,
      "step": 9430
    },
    {
      "epoch": 2.360657787782155,
      "grad_norm": 1.7193270921707153,
      "learning_rate": 1.5922090977632574e-05,
      "loss": 1.1636,
      "step": 9440
    },
    {
      "epoch": 2.3631588820108798,
      "grad_norm": 2.42824387550354,
      "learning_rate": 1.5907011812013067e-05,
      "loss": 1.052,
      "step": 9450
    },
    {
      "epoch": 2.3656599762396047,
      "grad_norm": 2.4249305725097656,
      "learning_rate": 1.5891932646393567e-05,
      "loss": 1.0556,
      "step": 9460
    },
    {
      "epoch": 2.36816107046833,
      "grad_norm": 1.9310667514801025,
      "learning_rate": 1.5876853480774066e-05,
      "loss": 1.0348,
      "step": 9470
    },
    {
      "epoch": 2.370662164697055,
      "grad_norm": 1.8524852991104126,
      "learning_rate": 1.586177431515456e-05,
      "loss": 1.0238,
      "step": 9480
    },
    {
      "epoch": 2.37316325892578,
      "grad_norm": 2.163316249847412,
      "learning_rate": 1.584669514953506e-05,
      "loss": 1.0924,
      "step": 9490
    },
    {
      "epoch": 2.375664353154505,
      "grad_norm": 1.88397216796875,
      "learning_rate": 1.5831615983915556e-05,
      "loss": 1.0409,
      "step": 9500
    },
    {
      "epoch": 2.37816544738323,
      "grad_norm": 2.1601099967956543,
      "learning_rate": 1.5816536818296055e-05,
      "loss": 1.0955,
      "step": 9510
    },
    {
      "epoch": 2.380666541611955,
      "grad_norm": 2.039203405380249,
      "learning_rate": 1.5801457652676552e-05,
      "loss": 1.1246,
      "step": 9520
    },
    {
      "epoch": 2.3831676358406804,
      "grad_norm": 1.6911399364471436,
      "learning_rate": 1.5786378487057048e-05,
      "loss": 1.0451,
      "step": 9530
    },
    {
      "epoch": 2.3856687300694053,
      "grad_norm": 2.1925735473632812,
      "learning_rate": 1.5771299321437548e-05,
      "loss": 1.0785,
      "step": 9540
    },
    {
      "epoch": 2.3881698242981306,
      "grad_norm": 1.7990025281906128,
      "learning_rate": 1.5756220155818048e-05,
      "loss": 1.0771,
      "step": 9550
    },
    {
      "epoch": 2.3906709185268555,
      "grad_norm": 1.8247135877609253,
      "learning_rate": 1.574114099019854e-05,
      "loss": 1.093,
      "step": 9560
    },
    {
      "epoch": 2.3931720127555804,
      "grad_norm": 2.33089280128479,
      "learning_rate": 1.572606182457904e-05,
      "loss": 1.0713,
      "step": 9570
    },
    {
      "epoch": 2.395673106984306,
      "grad_norm": 2.0385055541992188,
      "learning_rate": 1.5710982658959537e-05,
      "loss": 1.1065,
      "step": 9580
    },
    {
      "epoch": 2.3981742012130307,
      "grad_norm": 1.883798360824585,
      "learning_rate": 1.5695903493340037e-05,
      "loss": 1.0392,
      "step": 9590
    },
    {
      "epoch": 2.4006752954417556,
      "grad_norm": 2.142691135406494,
      "learning_rate": 1.5680824327720533e-05,
      "loss": 1.0827,
      "step": 9600
    },
    {
      "epoch": 2.403176389670481,
      "grad_norm": 2.016408920288086,
      "learning_rate": 1.566574516210103e-05,
      "loss": 1.1228,
      "step": 9610
    },
    {
      "epoch": 2.405677483899206,
      "grad_norm": 2.2841343879699707,
      "learning_rate": 1.565066599648153e-05,
      "loss": 1.0699,
      "step": 9620
    },
    {
      "epoch": 2.408178578127931,
      "grad_norm": 2.3490123748779297,
      "learning_rate": 1.5635586830862026e-05,
      "loss": 1.1068,
      "step": 9630
    },
    {
      "epoch": 2.410679672356656,
      "grad_norm": 1.8360199928283691,
      "learning_rate": 1.5620507665242522e-05,
      "loss": 1.0698,
      "step": 9640
    },
    {
      "epoch": 2.413180766585381,
      "grad_norm": 2.0621981620788574,
      "learning_rate": 1.5605428499623022e-05,
      "loss": 0.9862,
      "step": 9650
    },
    {
      "epoch": 2.4156818608141064,
      "grad_norm": 2.042081356048584,
      "learning_rate": 1.559034933400352e-05,
      "loss": 1.1009,
      "step": 9660
    },
    {
      "epoch": 2.4181829550428313,
      "grad_norm": 2.1558823585510254,
      "learning_rate": 1.5575270168384015e-05,
      "loss": 1.0677,
      "step": 9670
    },
    {
      "epoch": 2.4206840492715562,
      "grad_norm": 2.219304323196411,
      "learning_rate": 1.5560191002764515e-05,
      "loss": 1.0889,
      "step": 9680
    },
    {
      "epoch": 2.423185143500281,
      "grad_norm": 1.828887701034546,
      "learning_rate": 1.554511183714501e-05,
      "loss": 1.0352,
      "step": 9690
    },
    {
      "epoch": 2.4256862377290065,
      "grad_norm": 1.9811018705368042,
      "learning_rate": 1.553003267152551e-05,
      "loss": 1.0857,
      "step": 9700
    },
    {
      "epoch": 2.4281873319577314,
      "grad_norm": 1.9725713729858398,
      "learning_rate": 1.5514953505906004e-05,
      "loss": 1.0533,
      "step": 9710
    },
    {
      "epoch": 2.4306884261864568,
      "grad_norm": 1.617741346359253,
      "learning_rate": 1.5499874340286504e-05,
      "loss": 1.0026,
      "step": 9720
    },
    {
      "epoch": 2.4331895204151817,
      "grad_norm": 2.2318222522735596,
      "learning_rate": 1.5484795174667004e-05,
      "loss": 1.2021,
      "step": 9730
    },
    {
      "epoch": 2.4356906146439066,
      "grad_norm": 2.530238628387451,
      "learning_rate": 1.54697160090475e-05,
      "loss": 1.088,
      "step": 9740
    },
    {
      "epoch": 2.438191708872632,
      "grad_norm": 2.2216689586639404,
      "learning_rate": 1.5454636843427997e-05,
      "loss": 1.0465,
      "step": 9750
    },
    {
      "epoch": 2.440692803101357,
      "grad_norm": 2.073164463043213,
      "learning_rate": 1.5439557677808493e-05,
      "loss": 1.1082,
      "step": 9760
    },
    {
      "epoch": 2.4431938973300817,
      "grad_norm": 1.9235303401947021,
      "learning_rate": 1.5424478512188993e-05,
      "loss": 1.0023,
      "step": 9770
    },
    {
      "epoch": 2.445694991558807,
      "grad_norm": 2.302154779434204,
      "learning_rate": 1.5409399346569493e-05,
      "loss": 1.044,
      "step": 9780
    },
    {
      "epoch": 2.448196085787532,
      "grad_norm": 2.378223180770874,
      "learning_rate": 1.5394320180949986e-05,
      "loss": 1.1384,
      "step": 9790
    },
    {
      "epoch": 2.450697180016257,
      "grad_norm": 2.1268577575683594,
      "learning_rate": 1.5379241015330485e-05,
      "loss": 1.0625,
      "step": 9800
    },
    {
      "epoch": 2.4531982742449823,
      "grad_norm": 1.8585524559020996,
      "learning_rate": 1.5364161849710985e-05,
      "loss": 1.1004,
      "step": 9810
    },
    {
      "epoch": 2.455699368473707,
      "grad_norm": 1.84176504611969,
      "learning_rate": 1.534908268409148e-05,
      "loss": 1.0795,
      "step": 9820
    },
    {
      "epoch": 2.4582004627024325,
      "grad_norm": 2.0327343940734863,
      "learning_rate": 1.5334003518471978e-05,
      "loss": 1.037,
      "step": 9830
    },
    {
      "epoch": 2.4607015569311574,
      "grad_norm": 2.771214485168457,
      "learning_rate": 1.5318924352852475e-05,
      "loss": 1.0703,
      "step": 9840
    },
    {
      "epoch": 2.4632026511598824,
      "grad_norm": 2.1989667415618896,
      "learning_rate": 1.5303845187232974e-05,
      "loss": 1.0888,
      "step": 9850
    },
    {
      "epoch": 2.4657037453886077,
      "grad_norm": 1.8412981033325195,
      "learning_rate": 1.528876602161347e-05,
      "loss": 1.0587,
      "step": 9860
    },
    {
      "epoch": 2.4682048396173326,
      "grad_norm": 2.33052659034729,
      "learning_rate": 1.5273686855993967e-05,
      "loss": 1.0935,
      "step": 9870
    },
    {
      "epoch": 2.4707059338460575,
      "grad_norm": 1.9252960681915283,
      "learning_rate": 1.5258607690374467e-05,
      "loss": 1.112,
      "step": 9880
    },
    {
      "epoch": 2.473207028074783,
      "grad_norm": 1.8252419233322144,
      "learning_rate": 1.5243528524754965e-05,
      "loss": 1.0915,
      "step": 9890
    },
    {
      "epoch": 2.475708122303508,
      "grad_norm": 1.724606990814209,
      "learning_rate": 1.522844935913546e-05,
      "loss": 1.1074,
      "step": 9900
    },
    {
      "epoch": 2.4782092165322327,
      "grad_norm": 1.9351855516433716,
      "learning_rate": 1.5213370193515958e-05,
      "loss": 1.1374,
      "step": 9910
    },
    {
      "epoch": 2.480710310760958,
      "grad_norm": 2.6026740074157715,
      "learning_rate": 1.5198291027896458e-05,
      "loss": 1.0935,
      "step": 9920
    },
    {
      "epoch": 2.483211404989683,
      "grad_norm": 2.178664207458496,
      "learning_rate": 1.5183211862276956e-05,
      "loss": 1.1076,
      "step": 9930
    },
    {
      "epoch": 2.485712499218408,
      "grad_norm": 1.9340089559555054,
      "learning_rate": 1.516813269665745e-05,
      "loss": 1.0517,
      "step": 9940
    },
    {
      "epoch": 2.488213593447133,
      "grad_norm": 1.8014293909072876,
      "learning_rate": 1.5153053531037949e-05,
      "loss": 1.0792,
      "step": 9950
    },
    {
      "epoch": 2.490714687675858,
      "grad_norm": 1.8182445764541626,
      "learning_rate": 1.5137974365418448e-05,
      "loss": 1.1049,
      "step": 9960
    },
    {
      "epoch": 2.493215781904583,
      "grad_norm": 1.7234103679656982,
      "learning_rate": 1.5122895199798947e-05,
      "loss": 1.1006,
      "step": 9970
    },
    {
      "epoch": 2.4957168761333084,
      "grad_norm": 2.0357303619384766,
      "learning_rate": 1.5107816034179441e-05,
      "loss": 1.08,
      "step": 9980
    },
    {
      "epoch": 2.4982179703620333,
      "grad_norm": 1.8673714399337769,
      "learning_rate": 1.509273686855994e-05,
      "loss": 1.0621,
      "step": 9990
    },
    {
      "epoch": 2.5007190645907587,
      "grad_norm": 1.8663415908813477,
      "learning_rate": 1.5077657702940438e-05,
      "loss": 1.0907,
      "step": 10000
    },
    {
      "epoch": 2.5032201588194836,
      "grad_norm": 2.043262004852295,
      "learning_rate": 1.5062578537320937e-05,
      "loss": 1.1276,
      "step": 10010
    },
    {
      "epoch": 2.5057212530482085,
      "grad_norm": 1.464645504951477,
      "learning_rate": 1.5047499371701432e-05,
      "loss": 1.0799,
      "step": 10020
    },
    {
      "epoch": 2.5082223472769334,
      "grad_norm": 1.9219846725463867,
      "learning_rate": 1.503242020608193e-05,
      "loss": 1.1181,
      "step": 10030
    },
    {
      "epoch": 2.5107234415056587,
      "grad_norm": 2.149179697036743,
      "learning_rate": 1.5017341040462428e-05,
      "loss": 1.0734,
      "step": 10040
    },
    {
      "epoch": 2.5132245357343836,
      "grad_norm": 1.6374856233596802,
      "learning_rate": 1.5002261874842925e-05,
      "loss": 1.1007,
      "step": 10050
    },
    {
      "epoch": 2.515725629963109,
      "grad_norm": 1.7807146310806274,
      "learning_rate": 1.4987182709223423e-05,
      "loss": 1.1039,
      "step": 10060
    },
    {
      "epoch": 2.518226724191834,
      "grad_norm": 1.9980906248092651,
      "learning_rate": 1.4972103543603921e-05,
      "loss": 1.0175,
      "step": 10070
    },
    {
      "epoch": 2.520727818420559,
      "grad_norm": 2.040637493133545,
      "learning_rate": 1.4957024377984417e-05,
      "loss": 1.0693,
      "step": 10080
    },
    {
      "epoch": 2.523228912649284,
      "grad_norm": 2.709536552429199,
      "learning_rate": 1.4941945212364917e-05,
      "loss": 1.1194,
      "step": 10090
    },
    {
      "epoch": 2.525730006878009,
      "grad_norm": 2.0081403255462646,
      "learning_rate": 1.4926866046745414e-05,
      "loss": 1.1154,
      "step": 10100
    },
    {
      "epoch": 2.5282311011067344,
      "grad_norm": 2.220966100692749,
      "learning_rate": 1.4911786881125912e-05,
      "loss": 1.0805,
      "step": 10110
    },
    {
      "epoch": 2.5307321953354593,
      "grad_norm": 2.248377561569214,
      "learning_rate": 1.4896707715506408e-05,
      "loss": 1.0556,
      "step": 10120
    },
    {
      "epoch": 2.5332332895641843,
      "grad_norm": 2.007676362991333,
      "learning_rate": 1.4881628549886906e-05,
      "loss": 1.0518,
      "step": 10130
    },
    {
      "epoch": 2.535734383792909,
      "grad_norm": 2.4134292602539062,
      "learning_rate": 1.4866549384267404e-05,
      "loss": 1.0478,
      "step": 10140
    },
    {
      "epoch": 2.5382354780216345,
      "grad_norm": 2.2013237476348877,
      "learning_rate": 1.4851470218647902e-05,
      "loss": 1.0597,
      "step": 10150
    },
    {
      "epoch": 2.5407365722503594,
      "grad_norm": 2.1334948539733887,
      "learning_rate": 1.4836391053028399e-05,
      "loss": 1.1912,
      "step": 10160
    },
    {
      "epoch": 2.543237666479085,
      "grad_norm": 2.2138583660125732,
      "learning_rate": 1.4821311887408897e-05,
      "loss": 1.1082,
      "step": 10170
    },
    {
      "epoch": 2.5457387607078097,
      "grad_norm": 2.132200002670288,
      "learning_rate": 1.4806232721789395e-05,
      "loss": 1.0917,
      "step": 10180
    },
    {
      "epoch": 2.5482398549365346,
      "grad_norm": 1.814327597618103,
      "learning_rate": 1.4791153556169893e-05,
      "loss": 1.0543,
      "step": 10190
    },
    {
      "epoch": 2.55074094916526,
      "grad_norm": 1.974644422531128,
      "learning_rate": 1.477607439055039e-05,
      "loss": 1.0945,
      "step": 10200
    },
    {
      "epoch": 2.553242043393985,
      "grad_norm": 2.333514451980591,
      "learning_rate": 1.4760995224930888e-05,
      "loss": 1.0741,
      "step": 10210
    },
    {
      "epoch": 2.5557431376227098,
      "grad_norm": 2.057371139526367,
      "learning_rate": 1.4745916059311386e-05,
      "loss": 1.13,
      "step": 10220
    },
    {
      "epoch": 2.558244231851435,
      "grad_norm": 2.486769199371338,
      "learning_rate": 1.4730836893691884e-05,
      "loss": 1.0888,
      "step": 10230
    },
    {
      "epoch": 2.56074532608016,
      "grad_norm": 2.2404346466064453,
      "learning_rate": 1.471575772807238e-05,
      "loss": 1.0996,
      "step": 10240
    },
    {
      "epoch": 2.563246420308885,
      "grad_norm": 2.1973695755004883,
      "learning_rate": 1.4700678562452877e-05,
      "loss": 1.058,
      "step": 10250
    },
    {
      "epoch": 2.5657475145376103,
      "grad_norm": 2.18296480178833,
      "learning_rate": 1.4685599396833375e-05,
      "loss": 1.031,
      "step": 10260
    },
    {
      "epoch": 2.568248608766335,
      "grad_norm": 1.634588360786438,
      "learning_rate": 1.4670520231213873e-05,
      "loss": 1.1234,
      "step": 10270
    },
    {
      "epoch": 2.5707497029950606,
      "grad_norm": 2.013981580734253,
      "learning_rate": 1.4655441065594371e-05,
      "loss": 1.1564,
      "step": 10280
    },
    {
      "epoch": 2.5732507972237855,
      "grad_norm": 2.3468942642211914,
      "learning_rate": 1.4640361899974868e-05,
      "loss": 1.1403,
      "step": 10290
    },
    {
      "epoch": 2.5757518914525104,
      "grad_norm": 2.1960692405700684,
      "learning_rate": 1.4625282734355366e-05,
      "loss": 1.0302,
      "step": 10300
    },
    {
      "epoch": 2.5782529856812353,
      "grad_norm": 2.41148042678833,
      "learning_rate": 1.4610203568735864e-05,
      "loss": 1.0501,
      "step": 10310
    },
    {
      "epoch": 2.5807540799099606,
      "grad_norm": 2.4344990253448486,
      "learning_rate": 1.4595124403116362e-05,
      "loss": 1.0801,
      "step": 10320
    },
    {
      "epoch": 2.5832551741386856,
      "grad_norm": 2.4187090396881104,
      "learning_rate": 1.4580045237496858e-05,
      "loss": 1.0748,
      "step": 10330
    },
    {
      "epoch": 2.585756268367411,
      "grad_norm": 2.4255216121673584,
      "learning_rate": 1.4564966071877356e-05,
      "loss": 1.1076,
      "step": 10340
    },
    {
      "epoch": 2.588257362596136,
      "grad_norm": 2.1133906841278076,
      "learning_rate": 1.4549886906257855e-05,
      "loss": 1.0341,
      "step": 10350
    },
    {
      "epoch": 2.5907584568248607,
      "grad_norm": 1.6385926008224487,
      "learning_rate": 1.4534807740638353e-05,
      "loss": 1.0274,
      "step": 10360
    },
    {
      "epoch": 2.593259551053586,
      "grad_norm": 2.2578423023223877,
      "learning_rate": 1.4519728575018849e-05,
      "loss": 1.009,
      "step": 10370
    },
    {
      "epoch": 2.595760645282311,
      "grad_norm": 2.698146343231201,
      "learning_rate": 1.4504649409399347e-05,
      "loss": 1.0771,
      "step": 10380
    },
    {
      "epoch": 2.5982617395110363,
      "grad_norm": 2.0318284034729004,
      "learning_rate": 1.4489570243779844e-05,
      "loss": 1.0678,
      "step": 10390
    },
    {
      "epoch": 2.6007628337397612,
      "grad_norm": 1.9403212070465088,
      "learning_rate": 1.4474491078160343e-05,
      "loss": 1.0601,
      "step": 10400
    },
    {
      "epoch": 2.603263927968486,
      "grad_norm": 2.5651612281799316,
      "learning_rate": 1.445941191254084e-05,
      "loss": 1.1668,
      "step": 10410
    },
    {
      "epoch": 2.605765022197211,
      "grad_norm": 2.0135624408721924,
      "learning_rate": 1.4444332746921338e-05,
      "loss": 1.0456,
      "step": 10420
    },
    {
      "epoch": 2.6082661164259364,
      "grad_norm": 2.421757459640503,
      "learning_rate": 1.4429253581301834e-05,
      "loss": 1.08,
      "step": 10430
    },
    {
      "epoch": 2.6107672106546613,
      "grad_norm": 2.400146961212158,
      "learning_rate": 1.4414174415682333e-05,
      "loss": 1.1598,
      "step": 10440
    },
    {
      "epoch": 2.6132683048833867,
      "grad_norm": 2.30853533744812,
      "learning_rate": 1.439909525006283e-05,
      "loss": 1.1511,
      "step": 10450
    },
    {
      "epoch": 2.6157693991121116,
      "grad_norm": 2.4795467853546143,
      "learning_rate": 1.4384016084443327e-05,
      "loss": 1.0647,
      "step": 10460
    },
    {
      "epoch": 2.6182704933408365,
      "grad_norm": 2.140493631362915,
      "learning_rate": 1.4368936918823825e-05,
      "loss": 1.0537,
      "step": 10470
    },
    {
      "epoch": 2.6207715875695614,
      "grad_norm": 2.1465647220611572,
      "learning_rate": 1.4353857753204323e-05,
      "loss": 1.1844,
      "step": 10480
    },
    {
      "epoch": 2.6232726817982868,
      "grad_norm": 2.0691723823547363,
      "learning_rate": 1.4338778587584821e-05,
      "loss": 1.0412,
      "step": 10490
    },
    {
      "epoch": 2.6257737760270117,
      "grad_norm": 1.9358090162277222,
      "learning_rate": 1.4323699421965318e-05,
      "loss": 1.1217,
      "step": 10500
    },
    {
      "epoch": 2.628274870255737,
      "grad_norm": 1.9455384016036987,
      "learning_rate": 1.4308620256345816e-05,
      "loss": 1.0643,
      "step": 10510
    },
    {
      "epoch": 2.630775964484462,
      "grad_norm": 2.337221622467041,
      "learning_rate": 1.4293541090726312e-05,
      "loss": 1.0725,
      "step": 10520
    },
    {
      "epoch": 2.633277058713187,
      "grad_norm": 1.8919256925582886,
      "learning_rate": 1.4278461925106812e-05,
      "loss": 1.1546,
      "step": 10530
    },
    {
      "epoch": 2.635778152941912,
      "grad_norm": 2.9673514366149902,
      "learning_rate": 1.4263382759487309e-05,
      "loss": 1.0682,
      "step": 10540
    },
    {
      "epoch": 2.638279247170637,
      "grad_norm": 2.1395015716552734,
      "learning_rate": 1.4248303593867807e-05,
      "loss": 1.0785,
      "step": 10550
    },
    {
      "epoch": 2.6407803413993625,
      "grad_norm": 1.823432207107544,
      "learning_rate": 1.4233224428248303e-05,
      "loss": 1.0968,
      "step": 10560
    },
    {
      "epoch": 2.6432814356280874,
      "grad_norm": 2.187274932861328,
      "learning_rate": 1.4218145262628803e-05,
      "loss": 1.0679,
      "step": 10570
    },
    {
      "epoch": 2.6457825298568123,
      "grad_norm": 2.02748441696167,
      "learning_rate": 1.42030660970093e-05,
      "loss": 1.1405,
      "step": 10580
    },
    {
      "epoch": 2.648283624085537,
      "grad_norm": 2.355919361114502,
      "learning_rate": 1.4187986931389797e-05,
      "loss": 1.0187,
      "step": 10590
    },
    {
      "epoch": 2.6507847183142625,
      "grad_norm": 2.814345598220825,
      "learning_rate": 1.4172907765770294e-05,
      "loss": 1.0874,
      "step": 10600
    },
    {
      "epoch": 2.6532858125429875,
      "grad_norm": 2.1575257778167725,
      "learning_rate": 1.4157828600150794e-05,
      "loss": 1.1013,
      "step": 10610
    },
    {
      "epoch": 2.655786906771713,
      "grad_norm": 2.4569363594055176,
      "learning_rate": 1.414274943453129e-05,
      "loss": 1.1314,
      "step": 10620
    },
    {
      "epoch": 2.6582880010004377,
      "grad_norm": 2.1323249340057373,
      "learning_rate": 1.4127670268911786e-05,
      "loss": 1.1342,
      "step": 10630
    },
    {
      "epoch": 2.6607890952291626,
      "grad_norm": 2.3919460773468018,
      "learning_rate": 1.4112591103292285e-05,
      "loss": 1.0541,
      "step": 10640
    },
    {
      "epoch": 2.663290189457888,
      "grad_norm": 1.969269871711731,
      "learning_rate": 1.4097511937672781e-05,
      "loss": 1.0467,
      "step": 10650
    },
    {
      "epoch": 2.665791283686613,
      "grad_norm": 2.9822421073913574,
      "learning_rate": 1.408243277205328e-05,
      "loss": 1.083,
      "step": 10660
    },
    {
      "epoch": 2.668292377915338,
      "grad_norm": 2.3241164684295654,
      "learning_rate": 1.4067353606433777e-05,
      "loss": 1.0437,
      "step": 10670
    },
    {
      "epoch": 2.670793472144063,
      "grad_norm": 1.8759021759033203,
      "learning_rate": 1.4052274440814275e-05,
      "loss": 1.0779,
      "step": 10680
    },
    {
      "epoch": 2.673294566372788,
      "grad_norm": 2.5125765800476074,
      "learning_rate": 1.4037195275194772e-05,
      "loss": 1.1108,
      "step": 10690
    },
    {
      "epoch": 2.675795660601513,
      "grad_norm": 1.9224026203155518,
      "learning_rate": 1.4022116109575272e-05,
      "loss": 1.0753,
      "step": 10700
    },
    {
      "epoch": 2.6782967548302383,
      "grad_norm": 2.3063549995422363,
      "learning_rate": 1.4007036943955768e-05,
      "loss": 1.0682,
      "step": 10710
    },
    {
      "epoch": 2.6807978490589632,
      "grad_norm": 2.368089199066162,
      "learning_rate": 1.3991957778336266e-05,
      "loss": 1.1771,
      "step": 10720
    },
    {
      "epoch": 2.6832989432876886,
      "grad_norm": 2.218918800354004,
      "learning_rate": 1.3976878612716763e-05,
      "loss": 1.0933,
      "step": 10730
    },
    {
      "epoch": 2.6858000375164135,
      "grad_norm": 1.9757080078125,
      "learning_rate": 1.3961799447097262e-05,
      "loss": 1.1061,
      "step": 10740
    },
    {
      "epoch": 2.6883011317451384,
      "grad_norm": 2.835115909576416,
      "learning_rate": 1.3946720281477759e-05,
      "loss": 1.1847,
      "step": 10750
    },
    {
      "epoch": 2.6908022259738633,
      "grad_norm": 1.6840815544128418,
      "learning_rate": 1.3933149032420206e-05,
      "loss": 1.0868,
      "step": 10760
    },
    {
      "epoch": 2.6933033202025887,
      "grad_norm": 2.328964948654175,
      "learning_rate": 1.3918069866800704e-05,
      "loss": 1.0248,
      "step": 10770
    },
    {
      "epoch": 2.6958044144313136,
      "grad_norm": 2.213505983352661,
      "learning_rate": 1.3902990701181202e-05,
      "loss": 1.1133,
      "step": 10780
    },
    {
      "epoch": 2.698305508660039,
      "grad_norm": 1.8715702295303345,
      "learning_rate": 1.38879115355617e-05,
      "loss": 1.1144,
      "step": 10790
    },
    {
      "epoch": 2.700806602888764,
      "grad_norm": 2.201059341430664,
      "learning_rate": 1.3872832369942197e-05,
      "loss": 1.15,
      "step": 10800
    },
    {
      "epoch": 2.7033076971174888,
      "grad_norm": 2.1148319244384766,
      "learning_rate": 1.3857753204322695e-05,
      "loss": 1.0479,
      "step": 10810
    },
    {
      "epoch": 2.705808791346214,
      "grad_norm": 2.4930648803710938,
      "learning_rate": 1.3842674038703191e-05,
      "loss": 1.0944,
      "step": 10820
    },
    {
      "epoch": 2.708309885574939,
      "grad_norm": 2.1108717918395996,
      "learning_rate": 1.3827594873083691e-05,
      "loss": 1.0794,
      "step": 10830
    },
    {
      "epoch": 2.7108109798036644,
      "grad_norm": 2.709416627883911,
      "learning_rate": 1.3812515707464187e-05,
      "loss": 1.1298,
      "step": 10840
    },
    {
      "epoch": 2.7133120740323893,
      "grad_norm": 2.040750026702881,
      "learning_rate": 1.3797436541844686e-05,
      "loss": 1.0266,
      "step": 10850
    },
    {
      "epoch": 2.715813168261114,
      "grad_norm": 1.9060940742492676,
      "learning_rate": 1.3782357376225182e-05,
      "loss": 1.1238,
      "step": 10860
    },
    {
      "epoch": 2.718314262489839,
      "grad_norm": 2.249250888824463,
      "learning_rate": 1.376727821060568e-05,
      "loss": 1.1739,
      "step": 10870
    },
    {
      "epoch": 2.7208153567185644,
      "grad_norm": 2.022167444229126,
      "learning_rate": 1.3752199044986178e-05,
      "loss": 1.0276,
      "step": 10880
    },
    {
      "epoch": 2.7233164509472894,
      "grad_norm": 2.0764715671539307,
      "learning_rate": 1.3737119879366675e-05,
      "loss": 1.0341,
      "step": 10890
    },
    {
      "epoch": 2.7258175451760147,
      "grad_norm": 2.790196180343628,
      "learning_rate": 1.3722040713747173e-05,
      "loss": 1.0766,
      "step": 10900
    },
    {
      "epoch": 2.7283186394047396,
      "grad_norm": 1.9362179040908813,
      "learning_rate": 1.370696154812767e-05,
      "loss": 1.1186,
      "step": 10910
    },
    {
      "epoch": 2.7308197336334645,
      "grad_norm": 1.6779836416244507,
      "learning_rate": 1.3691882382508169e-05,
      "loss": 1.0881,
      "step": 10920
    },
    {
      "epoch": 2.7333208278621894,
      "grad_norm": 1.757590413093567,
      "learning_rate": 1.3676803216888665e-05,
      "loss": 1.0359,
      "step": 10930
    },
    {
      "epoch": 2.735821922090915,
      "grad_norm": 2.0354039669036865,
      "learning_rate": 1.3661724051269163e-05,
      "loss": 1.179,
      "step": 10940
    },
    {
      "epoch": 2.7383230163196397,
      "grad_norm": 2.2336935997009277,
      "learning_rate": 1.364664488564966e-05,
      "loss": 1.0714,
      "step": 10950
    },
    {
      "epoch": 2.740824110548365,
      "grad_norm": 2.377288818359375,
      "learning_rate": 1.363156572003016e-05,
      "loss": 1.0774,
      "step": 10960
    },
    {
      "epoch": 2.74332520477709,
      "grad_norm": 2.475529432296753,
      "learning_rate": 1.3616486554410656e-05,
      "loss": 1.0816,
      "step": 10970
    },
    {
      "epoch": 2.745826299005815,
      "grad_norm": 2.1931521892547607,
      "learning_rate": 1.3601407388791154e-05,
      "loss": 1.168,
      "step": 10980
    },
    {
      "epoch": 2.7483273932345402,
      "grad_norm": 2.0569422245025635,
      "learning_rate": 1.358632822317165e-05,
      "loss": 1.0718,
      "step": 10990
    },
    {
      "epoch": 2.750828487463265,
      "grad_norm": 2.070718765258789,
      "learning_rate": 1.357124905755215e-05,
      "loss": 1.1347,
      "step": 11000
    },
    {
      "epoch": 2.7533295816919905,
      "grad_norm": 2.1506423950195312,
      "learning_rate": 1.3556169891932647e-05,
      "loss": 1.0498,
      "step": 11010
    },
    {
      "epoch": 2.7558306759207154,
      "grad_norm": 1.9084259271621704,
      "learning_rate": 1.3541090726313145e-05,
      "loss": 1.1523,
      "step": 11020
    },
    {
      "epoch": 2.7583317701494403,
      "grad_norm": 1.6952303647994995,
      "learning_rate": 1.3526011560693641e-05,
      "loss": 1.0755,
      "step": 11030
    },
    {
      "epoch": 2.760832864378165,
      "grad_norm": 2.683710813522339,
      "learning_rate": 1.351093239507414e-05,
      "loss": 1.0534,
      "step": 11040
    },
    {
      "epoch": 2.7633339586068906,
      "grad_norm": 2.1073973178863525,
      "learning_rate": 1.3495853229454638e-05,
      "loss": 1.0618,
      "step": 11050
    },
    {
      "epoch": 2.7658350528356155,
      "grad_norm": 2.00628924369812,
      "learning_rate": 1.3480774063835134e-05,
      "loss": 1.0273,
      "step": 11060
    },
    {
      "epoch": 2.768336147064341,
      "grad_norm": 1.840247392654419,
      "learning_rate": 1.3465694898215632e-05,
      "loss": 1.087,
      "step": 11070
    },
    {
      "epoch": 2.7708372412930657,
      "grad_norm": 2.4081358909606934,
      "learning_rate": 1.3450615732596129e-05,
      "loss": 1.0604,
      "step": 11080
    },
    {
      "epoch": 2.7733383355217907,
      "grad_norm": 2.166890859603882,
      "learning_rate": 1.3435536566976628e-05,
      "loss": 1.0301,
      "step": 11090
    },
    {
      "epoch": 2.775839429750516,
      "grad_norm": 2.2008743286132812,
      "learning_rate": 1.3420457401357125e-05,
      "loss": 0.9973,
      "step": 11100
    },
    {
      "epoch": 2.778340523979241,
      "grad_norm": 2.9485747814178467,
      "learning_rate": 1.3405378235737623e-05,
      "loss": 1.1927,
      "step": 11110
    },
    {
      "epoch": 2.780841618207966,
      "grad_norm": 2.026700019836426,
      "learning_rate": 1.339029907011812e-05,
      "loss": 1.0242,
      "step": 11120
    },
    {
      "epoch": 2.783342712436691,
      "grad_norm": 2.3726420402526855,
      "learning_rate": 1.3375219904498619e-05,
      "loss": 1.0705,
      "step": 11130
    },
    {
      "epoch": 2.785843806665416,
      "grad_norm": 2.2532434463500977,
      "learning_rate": 1.3360140738879116e-05,
      "loss": 1.074,
      "step": 11140
    },
    {
      "epoch": 2.788344900894141,
      "grad_norm": 2.722918748855591,
      "learning_rate": 1.3345061573259614e-05,
      "loss": 1.0554,
      "step": 11150
    },
    {
      "epoch": 2.7908459951228664,
      "grad_norm": 2.1389529705047607,
      "learning_rate": 1.332998240764011e-05,
      "loss": 1.0992,
      "step": 11160
    },
    {
      "epoch": 2.7933470893515913,
      "grad_norm": 2.2528929710388184,
      "learning_rate": 1.3314903242020608e-05,
      "loss": 1.0538,
      "step": 11170
    },
    {
      "epoch": 2.7958481835803166,
      "grad_norm": 1.7289518117904663,
      "learning_rate": 1.3299824076401106e-05,
      "loss": 1.1974,
      "step": 11180
    },
    {
      "epoch": 2.7983492778090415,
      "grad_norm": 2.5520224571228027,
      "learning_rate": 1.3284744910781604e-05,
      "loss": 1.0288,
      "step": 11190
    },
    {
      "epoch": 2.8008503720377664,
      "grad_norm": 2.215341091156006,
      "learning_rate": 1.3269665745162101e-05,
      "loss": 1.0799,
      "step": 11200
    },
    {
      "epoch": 2.8033514662664913,
      "grad_norm": 2.319462776184082,
      "learning_rate": 1.3254586579542599e-05,
      "loss": 1.0363,
      "step": 11210
    },
    {
      "epoch": 2.8058525604952167,
      "grad_norm": 1.9470951557159424,
      "learning_rate": 1.3239507413923097e-05,
      "loss": 1.1015,
      "step": 11220
    },
    {
      "epoch": 2.8083536547239416,
      "grad_norm": 2.059325695037842,
      "learning_rate": 1.3224428248303595e-05,
      "loss": 1.1266,
      "step": 11230
    },
    {
      "epoch": 2.810854748952667,
      "grad_norm": 1.8247398138046265,
      "learning_rate": 1.3209349082684092e-05,
      "loss": 1.0632,
      "step": 11240
    },
    {
      "epoch": 2.813355843181392,
      "grad_norm": 1.8326467275619507,
      "learning_rate": 1.3194269917064588e-05,
      "loss": 1.0486,
      "step": 11250
    },
    {
      "epoch": 2.815856937410117,
      "grad_norm": 2.198530673980713,
      "learning_rate": 1.3179190751445088e-05,
      "loss": 1.0506,
      "step": 11260
    },
    {
      "epoch": 2.818358031638842,
      "grad_norm": 2.1442959308624268,
      "learning_rate": 1.3164111585825584e-05,
      "loss": 1.0391,
      "step": 11270
    },
    {
      "epoch": 2.820859125867567,
      "grad_norm": 2.0236458778381348,
      "learning_rate": 1.3149032420206082e-05,
      "loss": 1.0919,
      "step": 11280
    },
    {
      "epoch": 2.8233602200962924,
      "grad_norm": 1.8861455917358398,
      "learning_rate": 1.3133953254586579e-05,
      "loss": 1.1212,
      "step": 11290
    },
    {
      "epoch": 2.8258613143250173,
      "grad_norm": 2.20449161529541,
      "learning_rate": 1.3118874088967077e-05,
      "loss": 1.013,
      "step": 11300
    },
    {
      "epoch": 2.828362408553742,
      "grad_norm": 2.054595947265625,
      "learning_rate": 1.3103794923347575e-05,
      "loss": 1.1556,
      "step": 11310
    },
    {
      "epoch": 2.830863502782467,
      "grad_norm": 2.217041492462158,
      "learning_rate": 1.3088715757728073e-05,
      "loss": 1.0917,
      "step": 11320
    },
    {
      "epoch": 2.8333645970111925,
      "grad_norm": 1.9941496849060059,
      "learning_rate": 1.307363659210857e-05,
      "loss": 1.1563,
      "step": 11330
    },
    {
      "epoch": 2.8358656912399174,
      "grad_norm": 1.9312717914581299,
      "learning_rate": 1.3058557426489068e-05,
      "loss": 1.0709,
      "step": 11340
    },
    {
      "epoch": 2.8383667854686427,
      "grad_norm": 1.7570844888687134,
      "learning_rate": 1.3043478260869566e-05,
      "loss": 1.1157,
      "step": 11350
    },
    {
      "epoch": 2.8408678796973676,
      "grad_norm": 2.1438350677490234,
      "learning_rate": 1.3028399095250064e-05,
      "loss": 1.0923,
      "step": 11360
    },
    {
      "epoch": 2.8433689739260926,
      "grad_norm": 1.8958172798156738,
      "learning_rate": 1.301331992963056e-05,
      "loss": 1.1139,
      "step": 11370
    },
    {
      "epoch": 2.8458700681548175,
      "grad_norm": 2.1235361099243164,
      "learning_rate": 1.2998240764011058e-05,
      "loss": 1.1741,
      "step": 11380
    },
    {
      "epoch": 2.848371162383543,
      "grad_norm": 2.1010477542877197,
      "learning_rate": 1.2983161598391557e-05,
      "loss": 1.0674,
      "step": 11390
    },
    {
      "epoch": 2.8508722566122677,
      "grad_norm": 2.5483791828155518,
      "learning_rate": 1.2968082432772055e-05,
      "loss": 1.0835,
      "step": 11400
    },
    {
      "epoch": 2.853373350840993,
      "grad_norm": 2.196709394454956,
      "learning_rate": 1.2953003267152551e-05,
      "loss": 1.0903,
      "step": 11410
    },
    {
      "epoch": 2.855874445069718,
      "grad_norm": 1.9307256937026978,
      "learning_rate": 1.293792410153305e-05,
      "loss": 1.0786,
      "step": 11420
    },
    {
      "epoch": 2.858375539298443,
      "grad_norm": 1.8602896928787231,
      "learning_rate": 1.2922844935913546e-05,
      "loss": 1.1885,
      "step": 11430
    },
    {
      "epoch": 2.8608766335271683,
      "grad_norm": 2.120851755142212,
      "learning_rate": 1.2907765770294044e-05,
      "loss": 1.0492,
      "step": 11440
    },
    {
      "epoch": 2.863377727755893,
      "grad_norm": 2.220431327819824,
      "learning_rate": 1.2892686604674542e-05,
      "loss": 1.0933,
      "step": 11450
    },
    {
      "epoch": 2.8658788219846185,
      "grad_norm": 2.1938161849975586,
      "learning_rate": 1.2877607439055038e-05,
      "loss": 1.0529,
      "step": 11460
    },
    {
      "epoch": 2.8683799162133434,
      "grad_norm": 1.8355127573013306,
      "learning_rate": 1.2862528273435536e-05,
      "loss": 1.0715,
      "step": 11470
    },
    {
      "epoch": 2.8708810104420683,
      "grad_norm": 2.1019253730773926,
      "learning_rate": 1.2847449107816034e-05,
      "loss": 1.0349,
      "step": 11480
    },
    {
      "epoch": 2.8733821046707932,
      "grad_norm": 2.8861517906188965,
      "learning_rate": 1.2832369942196533e-05,
      "loss": 1.0586,
      "step": 11490
    },
    {
      "epoch": 2.8758831988995186,
      "grad_norm": 1.660283088684082,
      "learning_rate": 1.2817290776577029e-05,
      "loss": 0.9583,
      "step": 11500
    },
    {
      "epoch": 2.8783842931282435,
      "grad_norm": 2.0253500938415527,
      "learning_rate": 1.2802211610957527e-05,
      "loss": 1.0942,
      "step": 11510
    },
    {
      "epoch": 2.880885387356969,
      "grad_norm": 2.1483685970306396,
      "learning_rate": 1.2787132445338025e-05,
      "loss": 1.1371,
      "step": 11520
    },
    {
      "epoch": 2.8833864815856938,
      "grad_norm": 1.8426549434661865,
      "learning_rate": 1.2772053279718523e-05,
      "loss": 1.0706,
      "step": 11530
    },
    {
      "epoch": 2.8858875758144187,
      "grad_norm": 2.603541851043701,
      "learning_rate": 1.275697411409902e-05,
      "loss": 1.0837,
      "step": 11540
    },
    {
      "epoch": 2.888388670043144,
      "grad_norm": 2.027318239212036,
      "learning_rate": 1.2741894948479518e-05,
      "loss": 1.1414,
      "step": 11550
    },
    {
      "epoch": 2.890889764271869,
      "grad_norm": 2.368290424346924,
      "learning_rate": 1.2726815782860014e-05,
      "loss": 1.0892,
      "step": 11560
    },
    {
      "epoch": 2.893390858500594,
      "grad_norm": 2.035886287689209,
      "learning_rate": 1.2711736617240514e-05,
      "loss": 1.054,
      "step": 11570
    },
    {
      "epoch": 2.895891952729319,
      "grad_norm": 2.3202195167541504,
      "learning_rate": 1.269665745162101e-05,
      "loss": 1.0037,
      "step": 11580
    },
    {
      "epoch": 2.898393046958044,
      "grad_norm": 2.6774792671203613,
      "learning_rate": 1.2681578286001509e-05,
      "loss": 1.0894,
      "step": 11590
    },
    {
      "epoch": 2.900894141186769,
      "grad_norm": 2.2130284309387207,
      "learning_rate": 1.2666499120382005e-05,
      "loss": 1.0893,
      "step": 11600
    },
    {
      "epoch": 2.9033952354154944,
      "grad_norm": 2.39286732673645,
      "learning_rate": 1.2651419954762505e-05,
      "loss": 1.1585,
      "step": 11610
    },
    {
      "epoch": 2.9058963296442193,
      "grad_norm": 2.075319766998291,
      "learning_rate": 1.2636340789143001e-05,
      "loss": 1.0532,
      "step": 11620
    },
    {
      "epoch": 2.9083974238729446,
      "grad_norm": 2.263883113861084,
      "learning_rate": 1.2621261623523498e-05,
      "loss": 1.0313,
      "step": 11630
    },
    {
      "epoch": 2.9108985181016696,
      "grad_norm": 1.8979884386062622,
      "learning_rate": 1.2606182457903996e-05,
      "loss": 1.0388,
      "step": 11640
    },
    {
      "epoch": 2.9133996123303945,
      "grad_norm": 1.957284927368164,
      "learning_rate": 1.2591103292284494e-05,
      "loss": 1.1104,
      "step": 11650
    },
    {
      "epoch": 2.9159007065591194,
      "grad_norm": 2.6757662296295166,
      "learning_rate": 1.2576024126664992e-05,
      "loss": 1.0617,
      "step": 11660
    },
    {
      "epoch": 2.9184018007878447,
      "grad_norm": 1.941254734992981,
      "learning_rate": 1.2560944961045488e-05,
      "loss": 1.0698,
      "step": 11670
    },
    {
      "epoch": 2.9209028950165696,
      "grad_norm": 2.177523374557495,
      "learning_rate": 1.2545865795425987e-05,
      "loss": 1.0537,
      "step": 11680
    },
    {
      "epoch": 2.923403989245295,
      "grad_norm": 2.506235361099243,
      "learning_rate": 1.2530786629806483e-05,
      "loss": 1.0243,
      "step": 11690
    },
    {
      "epoch": 2.92590508347402,
      "grad_norm": 2.7014541625976562,
      "learning_rate": 1.2515707464186983e-05,
      "loss": 1.1212,
      "step": 11700
    },
    {
      "epoch": 2.928406177702745,
      "grad_norm": 1.9179130792617798,
      "learning_rate": 1.250062829856748e-05,
      "loss": 1.1385,
      "step": 11710
    },
    {
      "epoch": 2.93090727193147,
      "grad_norm": 1.9079725742340088,
      "learning_rate": 1.2485549132947977e-05,
      "loss": 1.0479,
      "step": 11720
    },
    {
      "epoch": 2.933408366160195,
      "grad_norm": 2.102388620376587,
      "learning_rate": 1.2470469967328474e-05,
      "loss": 1.0415,
      "step": 11730
    },
    {
      "epoch": 2.9359094603889204,
      "grad_norm": 2.0053746700286865,
      "learning_rate": 1.2455390801708974e-05,
      "loss": 1.0918,
      "step": 11740
    },
    {
      "epoch": 2.9384105546176453,
      "grad_norm": 2.509657144546509,
      "learning_rate": 1.244031163608947e-05,
      "loss": 1.0957,
      "step": 11750
    },
    {
      "epoch": 2.9409116488463702,
      "grad_norm": 1.7270880937576294,
      "learning_rate": 1.2425232470469968e-05,
      "loss": 1.0361,
      "step": 11760
    },
    {
      "epoch": 2.943412743075095,
      "grad_norm": 2.442788600921631,
      "learning_rate": 1.2410153304850465e-05,
      "loss": 1.06,
      "step": 11770
    },
    {
      "epoch": 2.9459138373038205,
      "grad_norm": 2.1048812866210938,
      "learning_rate": 1.2395074139230964e-05,
      "loss": 1.0948,
      "step": 11780
    },
    {
      "epoch": 2.9484149315325454,
      "grad_norm": 2.219182252883911,
      "learning_rate": 1.237999497361146e-05,
      "loss": 1.0284,
      "step": 11790
    },
    {
      "epoch": 2.9509160257612708,
      "grad_norm": 2.033127546310425,
      "learning_rate": 1.2364915807991959e-05,
      "loss": 1.0527,
      "step": 11800
    },
    {
      "epoch": 2.9534171199899957,
      "grad_norm": 2.5462770462036133,
      "learning_rate": 1.2349836642372455e-05,
      "loss": 1.073,
      "step": 11810
    },
    {
      "epoch": 2.9559182142187206,
      "grad_norm": 2.005558967590332,
      "learning_rate": 1.2334757476752952e-05,
      "loss": 1.0612,
      "step": 11820
    },
    {
      "epoch": 2.9584193084474455,
      "grad_norm": 2.2392642498016357,
      "learning_rate": 1.2319678311133451e-05,
      "loss": 1.0547,
      "step": 11830
    },
    {
      "epoch": 2.960920402676171,
      "grad_norm": 2.6274101734161377,
      "learning_rate": 1.2304599145513948e-05,
      "loss": 1.0599,
      "step": 11840
    },
    {
      "epoch": 2.9634214969048958,
      "grad_norm": 2.619631052017212,
      "learning_rate": 1.2289519979894446e-05,
      "loss": 1.03,
      "step": 11850
    },
    {
      "epoch": 2.965922591133621,
      "grad_norm": 1.8951880931854248,
      "learning_rate": 1.2274440814274942e-05,
      "loss": 1.0581,
      "step": 11860
    },
    {
      "epoch": 2.968423685362346,
      "grad_norm": 1.8210495710372925,
      "learning_rate": 1.2259361648655442e-05,
      "loss": 1.1625,
      "step": 11870
    },
    {
      "epoch": 2.970924779591071,
      "grad_norm": 2.0177290439605713,
      "learning_rate": 1.2244282483035939e-05,
      "loss": 1.0845,
      "step": 11880
    },
    {
      "epoch": 2.9734258738197963,
      "grad_norm": 2.0449283123016357,
      "learning_rate": 1.2229203317416437e-05,
      "loss": 1.0029,
      "step": 11890
    },
    {
      "epoch": 2.975926968048521,
      "grad_norm": 1.9880434274673462,
      "learning_rate": 1.2214124151796933e-05,
      "loss": 1.1281,
      "step": 11900
    },
    {
      "epoch": 2.9784280622772465,
      "grad_norm": 2.205859899520874,
      "learning_rate": 1.2199044986177433e-05,
      "loss": 1.0299,
      "step": 11910
    },
    {
      "epoch": 2.9809291565059715,
      "grad_norm": 3.0533463954925537,
      "learning_rate": 1.218396582055793e-05,
      "loss": 1.0727,
      "step": 11920
    },
    {
      "epoch": 2.9834302507346964,
      "grad_norm": 2.590156078338623,
      "learning_rate": 1.2168886654938428e-05,
      "loss": 1.12,
      "step": 11930
    },
    {
      "epoch": 2.9859313449634213,
      "grad_norm": 2.723520278930664,
      "learning_rate": 1.2153807489318924e-05,
      "loss": 1.0686,
      "step": 11940
    },
    {
      "epoch": 2.9884324391921466,
      "grad_norm": 2.403925895690918,
      "learning_rate": 1.2138728323699422e-05,
      "loss": 1.0442,
      "step": 11950
    },
    {
      "epoch": 2.9909335334208715,
      "grad_norm": 1.7271894216537476,
      "learning_rate": 1.212364915807992e-05,
      "loss": 1.0754,
      "step": 11960
    },
    {
      "epoch": 2.993434627649597,
      "grad_norm": 2.1440725326538086,
      "learning_rate": 1.2108569992460418e-05,
      "loss": 1.1158,
      "step": 11970
    },
    {
      "epoch": 2.995935721878322,
      "grad_norm": 2.1676814556121826,
      "learning_rate": 1.2093490826840915e-05,
      "loss": 1.189,
      "step": 11980
    },
    {
      "epoch": 2.9984368161070467,
      "grad_norm": 2.7517054080963135,
      "learning_rate": 1.2078411661221413e-05,
      "loss": 1.094,
      "step": 11990
    },
    {
      "epoch": 3.0007503282686177,
      "grad_norm": 1.953558325767517,
      "learning_rate": 1.2063332495601911e-05,
      "loss": 1.0994,
      "step": 12000
    },
    {
      "epoch": 3.0032514224973426,
      "grad_norm": 2.0249688625335693,
      "learning_rate": 1.2048253329982407e-05,
      "loss": 1.1123,
      "step": 12010
    },
    {
      "epoch": 3.0057525167260675,
      "grad_norm": 2.2581748962402344,
      "learning_rate": 1.2033174164362905e-05,
      "loss": 1.1171,
      "step": 12020
    },
    {
      "epoch": 3.008253610954793,
      "grad_norm": 2.477701187133789,
      "learning_rate": 1.2018094998743402e-05,
      "loss": 1.0766,
      "step": 12030
    },
    {
      "epoch": 3.0107547051835177,
      "grad_norm": 1.7559375762939453,
      "learning_rate": 1.2003015833123902e-05,
      "loss": 1.0916,
      "step": 12040
    },
    {
      "epoch": 3.0132557994122426,
      "grad_norm": 2.1371378898620605,
      "learning_rate": 1.1987936667504398e-05,
      "loss": 1.132,
      "step": 12050
    },
    {
      "epoch": 3.015756893640968,
      "grad_norm": 2.3623921871185303,
      "learning_rate": 1.1972857501884896e-05,
      "loss": 1.0686,
      "step": 12060
    },
    {
      "epoch": 3.018257987869693,
      "grad_norm": 1.9705922603607178,
      "learning_rate": 1.1957778336265393e-05,
      "loss": 1.0279,
      "step": 12070
    },
    {
      "epoch": 3.0207590820984183,
      "grad_norm": 2.1249194145202637,
      "learning_rate": 1.194269917064589e-05,
      "loss": 1.0552,
      "step": 12080
    },
    {
      "epoch": 3.023260176327143,
      "grad_norm": 2.5302510261535645,
      "learning_rate": 1.1927620005026389e-05,
      "loss": 1.1291,
      "step": 12090
    },
    {
      "epoch": 3.025761270555868,
      "grad_norm": 2.5312275886535645,
      "learning_rate": 1.1912540839406887e-05,
      "loss": 1.0417,
      "step": 12100
    },
    {
      "epoch": 3.0282623647845934,
      "grad_norm": 2.3500349521636963,
      "learning_rate": 1.1897461673787383e-05,
      "loss": 1.1509,
      "step": 12110
    },
    {
      "epoch": 3.0307634590133183,
      "grad_norm": 2.35815691947937,
      "learning_rate": 1.1882382508167882e-05,
      "loss": 1.092,
      "step": 12120
    },
    {
      "epoch": 3.0332645532420432,
      "grad_norm": 2.995236396789551,
      "learning_rate": 1.186730334254838e-05,
      "loss": 1.0051,
      "step": 12130
    },
    {
      "epoch": 3.0357656474707686,
      "grad_norm": 2.4424822330474854,
      "learning_rate": 1.1852224176928878e-05,
      "loss": 1.072,
      "step": 12140
    },
    {
      "epoch": 3.0382667416994935,
      "grad_norm": 1.942474603652954,
      "learning_rate": 1.1837145011309374e-05,
      "loss": 1.0353,
      "step": 12150
    },
    {
      "epoch": 3.0407678359282184,
      "grad_norm": 1.8916714191436768,
      "learning_rate": 1.1822065845689872e-05,
      "loss": 1.1114,
      "step": 12160
    },
    {
      "epoch": 3.0432689301569438,
      "grad_norm": 1.7101285457611084,
      "learning_rate": 1.180698668007037e-05,
      "loss": 1.1328,
      "step": 12170
    },
    {
      "epoch": 3.0457700243856687,
      "grad_norm": 2.354753017425537,
      "learning_rate": 1.1791907514450869e-05,
      "loss": 1.0993,
      "step": 12180
    },
    {
      "epoch": 3.0482711186143936,
      "grad_norm": 1.7749519348144531,
      "learning_rate": 1.1776828348831365e-05,
      "loss": 1.0761,
      "step": 12190
    },
    {
      "epoch": 3.050772212843119,
      "grad_norm": 2.1296112537384033,
      "learning_rate": 1.1761749183211861e-05,
      "loss": 1.0594,
      "step": 12200
    },
    {
      "epoch": 3.053273307071844,
      "grad_norm": 2.2035837173461914,
      "learning_rate": 1.174667001759236e-05,
      "loss": 1.107,
      "step": 12210
    },
    {
      "epoch": 3.055774401300569,
      "grad_norm": 2.076876640319824,
      "learning_rate": 1.1731590851972858e-05,
      "loss": 1.0932,
      "step": 12220
    },
    {
      "epoch": 3.058275495529294,
      "grad_norm": 2.1661744117736816,
      "learning_rate": 1.1716511686353356e-05,
      "loss": 1.0411,
      "step": 12230
    },
    {
      "epoch": 3.060776589758019,
      "grad_norm": 2.373896598815918,
      "learning_rate": 1.1701432520733852e-05,
      "loss": 1.0343,
      "step": 12240
    },
    {
      "epoch": 3.0632776839867444,
      "grad_norm": 1.859763264656067,
      "learning_rate": 1.168635335511435e-05,
      "loss": 1.0947,
      "step": 12250
    },
    {
      "epoch": 3.0657787782154693,
      "grad_norm": 2.4430930614471436,
      "learning_rate": 1.1671274189494848e-05,
      "loss": 1.0706,
      "step": 12260
    },
    {
      "epoch": 3.068279872444194,
      "grad_norm": 2.4277679920196533,
      "learning_rate": 1.1656195023875346e-05,
      "loss": 1.0961,
      "step": 12270
    },
    {
      "epoch": 3.0707809666729196,
      "grad_norm": 2.5336852073669434,
      "learning_rate": 1.1641115858255843e-05,
      "loss": 1.049,
      "step": 12280
    },
    {
      "epoch": 3.0732820609016445,
      "grad_norm": 1.883908987045288,
      "learning_rate": 1.1626036692636341e-05,
      "loss": 1.0455,
      "step": 12290
    },
    {
      "epoch": 3.0757831551303694,
      "grad_norm": 2.0244319438934326,
      "learning_rate": 1.1610957527016839e-05,
      "loss": 1.1725,
      "step": 12300
    },
    {
      "epoch": 3.0782842493590947,
      "grad_norm": 2.003779172897339,
      "learning_rate": 1.1595878361397337e-05,
      "loss": 1.1001,
      "step": 12310
    },
    {
      "epoch": 3.0807853435878196,
      "grad_norm": 2.0814015865325928,
      "learning_rate": 1.1580799195777834e-05,
      "loss": 1.0811,
      "step": 12320
    },
    {
      "epoch": 3.0832864378165445,
      "grad_norm": 2.384054660797119,
      "learning_rate": 1.1565720030158332e-05,
      "loss": 1.1156,
      "step": 12330
    },
    {
      "epoch": 3.08578753204527,
      "grad_norm": 1.9104008674621582,
      "learning_rate": 1.1550640864538828e-05,
      "loss": 1.0967,
      "step": 12340
    },
    {
      "epoch": 3.088288626273995,
      "grad_norm": 2.377570390701294,
      "learning_rate": 1.1535561698919328e-05,
      "loss": 0.9838,
      "step": 12350
    },
    {
      "epoch": 3.0907897205027197,
      "grad_norm": 2.0689773559570312,
      "learning_rate": 1.1520482533299824e-05,
      "loss": 1.0761,
      "step": 12360
    },
    {
      "epoch": 3.093290814731445,
      "grad_norm": 1.8118371963500977,
      "learning_rate": 1.1505403367680322e-05,
      "loss": 1.0453,
      "step": 12370
    },
    {
      "epoch": 3.09579190896017,
      "grad_norm": 2.0648043155670166,
      "learning_rate": 1.1490324202060819e-05,
      "loss": 1.0917,
      "step": 12380
    },
    {
      "epoch": 3.0982930031888953,
      "grad_norm": 1.9555467367172241,
      "learning_rate": 1.1475245036441317e-05,
      "loss": 1.1585,
      "step": 12390
    },
    {
      "epoch": 3.1007940974176202,
      "grad_norm": 2.207110643386841,
      "learning_rate": 1.1460165870821815e-05,
      "loss": 0.948,
      "step": 12400
    },
    {
      "epoch": 3.103295191646345,
      "grad_norm": 2.1952662467956543,
      "learning_rate": 1.1445086705202312e-05,
      "loss": 1.077,
      "step": 12410
    },
    {
      "epoch": 3.1057962858750705,
      "grad_norm": 1.920525312423706,
      "learning_rate": 1.143000753958281e-05,
      "loss": 1.1097,
      "step": 12420
    },
    {
      "epoch": 3.1082973801037954,
      "grad_norm": 1.9246761798858643,
      "learning_rate": 1.1414928373963308e-05,
      "loss": 1.0502,
      "step": 12430
    },
    {
      "epoch": 3.1107984743325203,
      "grad_norm": 2.8862080574035645,
      "learning_rate": 1.1399849208343806e-05,
      "loss": 1.1262,
      "step": 12440
    },
    {
      "epoch": 3.1132995685612457,
      "grad_norm": 1.6431597471237183,
      "learning_rate": 1.1384770042724302e-05,
      "loss": 1.1237,
      "step": 12450
    },
    {
      "epoch": 3.1158006627899706,
      "grad_norm": 1.9656051397323608,
      "learning_rate": 1.13696908771048e-05,
      "loss": 1.0919,
      "step": 12460
    },
    {
      "epoch": 3.1183017570186955,
      "grad_norm": 2.094156503677368,
      "learning_rate": 1.1354611711485297e-05,
      "loss": 1.0578,
      "step": 12470
    },
    {
      "epoch": 3.120802851247421,
      "grad_norm": 2.2544302940368652,
      "learning_rate": 1.1339532545865797e-05,
      "loss": 1.1435,
      "step": 12480
    },
    {
      "epoch": 3.1233039454761458,
      "grad_norm": 1.9812610149383545,
      "learning_rate": 1.1324453380246293e-05,
      "loss": 1.0163,
      "step": 12490
    },
    {
      "epoch": 3.125805039704871,
      "grad_norm": 2.384016275405884,
      "learning_rate": 1.1309374214626791e-05,
      "loss": 1.0554,
      "step": 12500
    },
    {
      "epoch": 3.128306133933596,
      "grad_norm": 2.046696662902832,
      "learning_rate": 1.1294295049007288e-05,
      "loss": 1.0538,
      "step": 12510
    },
    {
      "epoch": 3.130807228162321,
      "grad_norm": 2.1122703552246094,
      "learning_rate": 1.1279215883387787e-05,
      "loss": 1.0754,
      "step": 12520
    },
    {
      "epoch": 3.1333083223910463,
      "grad_norm": 1.7891449928283691,
      "learning_rate": 1.1264136717768284e-05,
      "loss": 1.0737,
      "step": 12530
    },
    {
      "epoch": 3.135809416619771,
      "grad_norm": 2.1354832649230957,
      "learning_rate": 1.1249057552148782e-05,
      "loss": 1.1094,
      "step": 12540
    },
    {
      "epoch": 3.138310510848496,
      "grad_norm": 2.110292911529541,
      "learning_rate": 1.1233978386529278e-05,
      "loss": 1.0298,
      "step": 12550
    },
    {
      "epoch": 3.1408116050772215,
      "grad_norm": 2.9511935710906982,
      "learning_rate": 1.1218899220909778e-05,
      "loss": 1.0814,
      "step": 12560
    },
    {
      "epoch": 3.1433126993059464,
      "grad_norm": 2.1029539108276367,
      "learning_rate": 1.1203820055290275e-05,
      "loss": 1.0999,
      "step": 12570
    },
    {
      "epoch": 3.1458137935346713,
      "grad_norm": 2.0050909519195557,
      "learning_rate": 1.1188740889670771e-05,
      "loss": 1.0936,
      "step": 12580
    },
    {
      "epoch": 3.1483148877633966,
      "grad_norm": 2.2576546669006348,
      "learning_rate": 1.1173661724051269e-05,
      "loss": 1.1169,
      "step": 12590
    },
    {
      "epoch": 3.1508159819921215,
      "grad_norm": 2.0708506107330322,
      "learning_rate": 1.1158582558431766e-05,
      "loss": 1.0675,
      "step": 12600
    },
    {
      "epoch": 3.1533170762208464,
      "grad_norm": 2.1559219360351562,
      "learning_rate": 1.1143503392812265e-05,
      "loss": 1.0674,
      "step": 12610
    },
    {
      "epoch": 3.155818170449572,
      "grad_norm": 2.502216100692749,
      "learning_rate": 1.1128424227192762e-05,
      "loss": 1.1077,
      "step": 12620
    },
    {
      "epoch": 3.1583192646782967,
      "grad_norm": 2.0224297046661377,
      "learning_rate": 1.111334506157326e-05,
      "loss": 1.0162,
      "step": 12630
    },
    {
      "epoch": 3.1608203589070216,
      "grad_norm": 2.2756457328796387,
      "learning_rate": 1.1098265895953756e-05,
      "loss": 1.0444,
      "step": 12640
    },
    {
      "epoch": 3.163321453135747,
      "grad_norm": 2.77846097946167,
      "learning_rate": 1.1083186730334256e-05,
      "loss": 1.0267,
      "step": 12650
    },
    {
      "epoch": 3.165822547364472,
      "grad_norm": 2.626232385635376,
      "learning_rate": 1.1068107564714753e-05,
      "loss": 1.1283,
      "step": 12660
    },
    {
      "epoch": 3.1683236415931972,
      "grad_norm": 2.137125253677368,
      "learning_rate": 1.105302839909525e-05,
      "loss": 1.1674,
      "step": 12670
    },
    {
      "epoch": 3.170824735821922,
      "grad_norm": 2.4207282066345215,
      "learning_rate": 1.1037949233475747e-05,
      "loss": 1.0086,
      "step": 12680
    },
    {
      "epoch": 3.173325830050647,
      "grad_norm": 2.2354929447174072,
      "learning_rate": 1.1022870067856247e-05,
      "loss": 1.0559,
      "step": 12690
    },
    {
      "epoch": 3.1758269242793724,
      "grad_norm": 2.7339115142822266,
      "learning_rate": 1.1007790902236743e-05,
      "loss": 1.0964,
      "step": 12700
    },
    {
      "epoch": 3.1783280185080973,
      "grad_norm": 2.092643976211548,
      "learning_rate": 1.0992711736617241e-05,
      "loss": 1.0915,
      "step": 12710
    },
    {
      "epoch": 3.1808291127368222,
      "grad_norm": 2.0708749294281006,
      "learning_rate": 1.0977632570997738e-05,
      "loss": 1.0887,
      "step": 12720
    },
    {
      "epoch": 3.1833302069655476,
      "grad_norm": 3.1313390731811523,
      "learning_rate": 1.0962553405378236e-05,
      "loss": 1.0432,
      "step": 12730
    },
    {
      "epoch": 3.1858313011942725,
      "grad_norm": 2.1226563453674316,
      "learning_rate": 1.0947474239758734e-05,
      "loss": 1.0215,
      "step": 12740
    },
    {
      "epoch": 3.1883323954229974,
      "grad_norm": 2.2882027626037598,
      "learning_rate": 1.0932395074139232e-05,
      "loss": 1.0511,
      "step": 12750
    },
    {
      "epoch": 3.1908334896517228,
      "grad_norm": 2.3382375240325928,
      "learning_rate": 1.0917315908519729e-05,
      "loss": 1.0395,
      "step": 12760
    },
    {
      "epoch": 3.1933345838804477,
      "grad_norm": 2.8599915504455566,
      "learning_rate": 1.0902236742900225e-05,
      "loss": 1.0393,
      "step": 12770
    },
    {
      "epoch": 3.1958356781091726,
      "grad_norm": 2.3893582820892334,
      "learning_rate": 1.0887157577280725e-05,
      "loss": 1.0692,
      "step": 12780
    },
    {
      "epoch": 3.198336772337898,
      "grad_norm": 1.7385859489440918,
      "learning_rate": 1.0872078411661221e-05,
      "loss": 1.1022,
      "step": 12790
    },
    {
      "epoch": 3.200837866566623,
      "grad_norm": 2.6860485076904297,
      "learning_rate": 1.085699924604172e-05,
      "loss": 1.18,
      "step": 12800
    },
    {
      "epoch": 3.2033389607953477,
      "grad_norm": 2.1016111373901367,
      "learning_rate": 1.0841920080422216e-05,
      "loss": 1.1286,
      "step": 12810
    },
    {
      "epoch": 3.205840055024073,
      "grad_norm": 1.8107595443725586,
      "learning_rate": 1.0826840914802716e-05,
      "loss": 1.0695,
      "step": 12820
    },
    {
      "epoch": 3.208341149252798,
      "grad_norm": 2.2371771335601807,
      "learning_rate": 1.0811761749183212e-05,
      "loss": 1.1703,
      "step": 12830
    },
    {
      "epoch": 3.2108422434815234,
      "grad_norm": 2.697519063949585,
      "learning_rate": 1.079668258356371e-05,
      "loss": 1.0879,
      "step": 12840
    },
    {
      "epoch": 3.2133433377102483,
      "grad_norm": 2.4964890480041504,
      "learning_rate": 1.0781603417944207e-05,
      "loss": 1.075,
      "step": 12850
    },
    {
      "epoch": 3.215844431938973,
      "grad_norm": 1.820715308189392,
      "learning_rate": 1.0766524252324705e-05,
      "loss": 1.0532,
      "step": 12860
    },
    {
      "epoch": 3.2183455261676985,
      "grad_norm": 1.8238383531570435,
      "learning_rate": 1.0751445086705203e-05,
      "loss": 1.1137,
      "step": 12870
    },
    {
      "epoch": 3.2208466203964234,
      "grad_norm": 2.0803866386413574,
      "learning_rate": 1.0736365921085701e-05,
      "loss": 1.0364,
      "step": 12880
    },
    {
      "epoch": 3.2233477146251484,
      "grad_norm": 1.844922661781311,
      "learning_rate": 1.0721286755466197e-05,
      "loss": 1.0812,
      "step": 12890
    },
    {
      "epoch": 3.2258488088538737,
      "grad_norm": 2.4184153079986572,
      "learning_rate": 1.0706207589846695e-05,
      "loss": 1.0659,
      "step": 12900
    },
    {
      "epoch": 3.2283499030825986,
      "grad_norm": 1.874626636505127,
      "learning_rate": 1.0691128424227193e-05,
      "loss": 1.0747,
      "step": 12910
    },
    {
      "epoch": 3.2308509973113235,
      "grad_norm": 2.013422727584839,
      "learning_rate": 1.0676049258607692e-05,
      "loss": 1.0336,
      "step": 12920
    },
    {
      "epoch": 3.233352091540049,
      "grad_norm": 2.700870990753174,
      "learning_rate": 1.0660970092988188e-05,
      "loss": 1.0935,
      "step": 12930
    },
    {
      "epoch": 3.235853185768774,
      "grad_norm": 2.1998088359832764,
      "learning_rate": 1.0645890927368686e-05,
      "loss": 1.0864,
      "step": 12940
    },
    {
      "epoch": 3.238354279997499,
      "grad_norm": 2.3769335746765137,
      "learning_rate": 1.0630811761749184e-05,
      "loss": 1.0726,
      "step": 12950
    },
    {
      "epoch": 3.240855374226224,
      "grad_norm": 2.2713968753814697,
      "learning_rate": 1.061573259612968e-05,
      "loss": 1.0159,
      "step": 12960
    },
    {
      "epoch": 3.243356468454949,
      "grad_norm": 2.8615474700927734,
      "learning_rate": 1.0600653430510179e-05,
      "loss": 1.0419,
      "step": 12970
    },
    {
      "epoch": 3.2458575626836743,
      "grad_norm": 2.462968111038208,
      "learning_rate": 1.0585574264890675e-05,
      "loss": 1.0766,
      "step": 12980
    },
    {
      "epoch": 3.2483586569123992,
      "grad_norm": 2.1574113368988037,
      "learning_rate": 1.0570495099271173e-05,
      "loss": 1.0126,
      "step": 12990
    },
    {
      "epoch": 3.250859751141124,
      "grad_norm": 2.3030269145965576,
      "learning_rate": 1.0555415933651671e-05,
      "loss": 1.0842,
      "step": 13000
    },
    {
      "epoch": 3.2533608453698495,
      "grad_norm": 2.670144557952881,
      "learning_rate": 1.054033676803217e-05,
      "loss": 1.0556,
      "step": 13010
    },
    {
      "epoch": 3.2558619395985744,
      "grad_norm": 2.0836801528930664,
      "learning_rate": 1.0525257602412666e-05,
      "loss": 1.0332,
      "step": 13020
    },
    {
      "epoch": 3.2583630338272993,
      "grad_norm": 2.408348560333252,
      "learning_rate": 1.0510178436793164e-05,
      "loss": 1.0639,
      "step": 13030
    },
    {
      "epoch": 3.2608641280560247,
      "grad_norm": 2.637174367904663,
      "learning_rate": 1.0495099271173662e-05,
      "loss": 1.0681,
      "step": 13040
    },
    {
      "epoch": 3.2633652222847496,
      "grad_norm": 2.290933132171631,
      "learning_rate": 1.048002010555416e-05,
      "loss": 1.081,
      "step": 13050
    },
    {
      "epoch": 3.2658663165134745,
      "grad_norm": 1.978703260421753,
      "learning_rate": 1.0464940939934657e-05,
      "loss": 1.0765,
      "step": 13060
    },
    {
      "epoch": 3.2683674107422,
      "grad_norm": 1.9034396409988403,
      "learning_rate": 1.0449861774315155e-05,
      "loss": 1.0543,
      "step": 13070
    },
    {
      "epoch": 3.2708685049709247,
      "grad_norm": 1.6895006895065308,
      "learning_rate": 1.0434782608695653e-05,
      "loss": 1.0874,
      "step": 13080
    },
    {
      "epoch": 3.2733695991996496,
      "grad_norm": 2.401973247528076,
      "learning_rate": 1.0419703443076151e-05,
      "loss": 1.0737,
      "step": 13090
    },
    {
      "epoch": 3.275870693428375,
      "grad_norm": 2.3648605346679688,
      "learning_rate": 1.0404624277456647e-05,
      "loss": 1.0985,
      "step": 13100
    },
    {
      "epoch": 3.2783717876571,
      "grad_norm": 2.333869218826294,
      "learning_rate": 1.0389545111837146e-05,
      "loss": 1.0477,
      "step": 13110
    },
    {
      "epoch": 3.2808728818858253,
      "grad_norm": 2.0007166862487793,
      "learning_rate": 1.0374465946217642e-05,
      "loss": 1.0981,
      "step": 13120
    },
    {
      "epoch": 3.28337397611455,
      "grad_norm": 2.7969865798950195,
      "learning_rate": 1.0359386780598142e-05,
      "loss": 1.0401,
      "step": 13130
    },
    {
      "epoch": 3.285875070343275,
      "grad_norm": 2.0831921100616455,
      "learning_rate": 1.0344307614978638e-05,
      "loss": 1.0491,
      "step": 13140
    },
    {
      "epoch": 3.2883761645720004,
      "grad_norm": 2.4528753757476807,
      "learning_rate": 1.0329228449359135e-05,
      "loss": 1.1151,
      "step": 13150
    },
    {
      "epoch": 3.2908772588007253,
      "grad_norm": 2.120961904525757,
      "learning_rate": 1.0314149283739633e-05,
      "loss": 1.0348,
      "step": 13160
    },
    {
      "epoch": 3.2933783530294503,
      "grad_norm": 2.6073057651519775,
      "learning_rate": 1.0299070118120131e-05,
      "loss": 1.0843,
      "step": 13170
    },
    {
      "epoch": 3.2958794472581756,
      "grad_norm": 2.171480655670166,
      "learning_rate": 1.0283990952500629e-05,
      "loss": 1.0564,
      "step": 13180
    },
    {
      "epoch": 3.2983805414869005,
      "grad_norm": 2.049823760986328,
      "learning_rate": 1.0268911786881125e-05,
      "loss": 1.12,
      "step": 13190
    },
    {
      "epoch": 3.3008816357156254,
      "grad_norm": 1.8107572793960571,
      "learning_rate": 1.0253832621261624e-05,
      "loss": 1.0884,
      "step": 13200
    },
    {
      "epoch": 3.303382729944351,
      "grad_norm": 1.855776071548462,
      "learning_rate": 1.0238753455642122e-05,
      "loss": 1.0925,
      "step": 13210
    },
    {
      "epoch": 3.3058838241730757,
      "grad_norm": 2.441377878189087,
      "learning_rate": 1.022367429002262e-05,
      "loss": 1.074,
      "step": 13220
    },
    {
      "epoch": 3.3083849184018006,
      "grad_norm": 2.6557421684265137,
      "learning_rate": 1.0208595124403116e-05,
      "loss": 1.1115,
      "step": 13230
    },
    {
      "epoch": 3.310886012630526,
      "grad_norm": 2.072397470474243,
      "learning_rate": 1.0193515958783614e-05,
      "loss": 1.0915,
      "step": 13240
    },
    {
      "epoch": 3.313387106859251,
      "grad_norm": 2.517267942428589,
      "learning_rate": 1.017843679316411e-05,
      "loss": 1.0712,
      "step": 13250
    },
    {
      "epoch": 3.3158882010879758,
      "grad_norm": 2.015941858291626,
      "learning_rate": 1.016335762754461e-05,
      "loss": 1.1158,
      "step": 13260
    },
    {
      "epoch": 3.318389295316701,
      "grad_norm": 2.008270025253296,
      "learning_rate": 1.0148278461925107e-05,
      "loss": 1.0642,
      "step": 13270
    },
    {
      "epoch": 3.320890389545426,
      "grad_norm": 1.9592372179031372,
      "learning_rate": 1.0133199296305605e-05,
      "loss": 1.0524,
      "step": 13280
    },
    {
      "epoch": 3.3233914837741514,
      "grad_norm": 2.4243268966674805,
      "learning_rate": 1.0118120130686101e-05,
      "loss": 1.0784,
      "step": 13290
    },
    {
      "epoch": 3.3258925780028763,
      "grad_norm": 2.4020111560821533,
      "learning_rate": 1.0103040965066601e-05,
      "loss": 1.1352,
      "step": 13300
    },
    {
      "epoch": 3.328393672231601,
      "grad_norm": 2.305629253387451,
      "learning_rate": 1.0087961799447098e-05,
      "loss": 1.0455,
      "step": 13310
    },
    {
      "epoch": 3.3308947664603266,
      "grad_norm": 2.4422287940979004,
      "learning_rate": 1.0072882633827596e-05,
      "loss": 1.0564,
      "step": 13320
    },
    {
      "epoch": 3.3333958606890515,
      "grad_norm": 2.703935384750366,
      "learning_rate": 1.0057803468208092e-05,
      "loss": 1.076,
      "step": 13330
    },
    {
      "epoch": 3.3358969549177764,
      "grad_norm": 2.2639312744140625,
      "learning_rate": 1.004272430258859e-05,
      "loss": 1.0273,
      "step": 13340
    },
    {
      "epoch": 3.3383980491465017,
      "grad_norm": 2.7225217819213867,
      "learning_rate": 1.0027645136969088e-05,
      "loss": 1.0274,
      "step": 13350
    },
    {
      "epoch": 3.3408991433752266,
      "grad_norm": 2.764033555984497,
      "learning_rate": 1.0012565971349585e-05,
      "loss": 1.0521,
      "step": 13360
    },
    {
      "epoch": 3.3434002376039516,
      "grad_norm": 1.9736210107803345,
      "learning_rate": 9.997486805730083e-06,
      "loss": 1.1072,
      "step": 13370
    },
    {
      "epoch": 3.345901331832677,
      "grad_norm": 1.9230425357818604,
      "learning_rate": 9.98240764011058e-06,
      "loss": 1.124,
      "step": 13380
    },
    {
      "epoch": 3.348402426061402,
      "grad_norm": 2.0015861988067627,
      "learning_rate": 9.96732847449108e-06,
      "loss": 1.0809,
      "step": 13390
    },
    {
      "epoch": 3.350903520290127,
      "grad_norm": 2.1035003662109375,
      "learning_rate": 9.952249308871576e-06,
      "loss": 1.1009,
      "step": 13400
    },
    {
      "epoch": 3.353404614518852,
      "grad_norm": 2.2159829139709473,
      "learning_rate": 9.937170143252074e-06,
      "loss": 1.1071,
      "step": 13410
    },
    {
      "epoch": 3.355905708747577,
      "grad_norm": 2.1168956756591797,
      "learning_rate": 9.92209097763257e-06,
      "loss": 1.063,
      "step": 13420
    },
    {
      "epoch": 3.358406802976302,
      "grad_norm": 2.0889556407928467,
      "learning_rate": 9.90701181201307e-06,
      "loss": 1.0568,
      "step": 13430
    },
    {
      "epoch": 3.3609078972050273,
      "grad_norm": 2.2515954971313477,
      "learning_rate": 9.891932646393566e-06,
      "loss": 1.0895,
      "step": 13440
    },
    {
      "epoch": 3.363408991433752,
      "grad_norm": 2.2619874477386475,
      "learning_rate": 9.876853480774065e-06,
      "loss": 1.1292,
      "step": 13450
    },
    {
      "epoch": 3.3659100856624775,
      "grad_norm": 1.969360589981079,
      "learning_rate": 9.861774315154561e-06,
      "loss": 1.0696,
      "step": 13460
    },
    {
      "epoch": 3.3684111798912024,
      "grad_norm": 2.6416683197021484,
      "learning_rate": 9.84669514953506e-06,
      "loss": 1.0974,
      "step": 13470
    },
    {
      "epoch": 3.3709122741199273,
      "grad_norm": 2.82293963432312,
      "learning_rate": 9.831615983915557e-06,
      "loss": 1.0356,
      "step": 13480
    },
    {
      "epoch": 3.3734133683486527,
      "grad_norm": 2.243025302886963,
      "learning_rate": 9.816536818296055e-06,
      "loss": 1.0644,
      "step": 13490
    },
    {
      "epoch": 3.3759144625773776,
      "grad_norm": 2.295836925506592,
      "learning_rate": 9.801457652676552e-06,
      "loss": 1.0974,
      "step": 13500
    },
    {
      "epoch": 3.3784155568061025,
      "grad_norm": 1.9044201374053955,
      "learning_rate": 9.78637848705705e-06,
      "loss": 1.09,
      "step": 13510
    },
    {
      "epoch": 3.380916651034828,
      "grad_norm": 2.598984956741333,
      "learning_rate": 9.771299321437548e-06,
      "loss": 1.0943,
      "step": 13520
    },
    {
      "epoch": 3.3834177452635528,
      "grad_norm": 2.485931158065796,
      "learning_rate": 9.756220155818044e-06,
      "loss": 1.1006,
      "step": 13530
    },
    {
      "epoch": 3.3859188394922777,
      "grad_norm": 2.6536827087402344,
      "learning_rate": 9.741140990198542e-06,
      "loss": 1.1605,
      "step": 13540
    },
    {
      "epoch": 3.388419933721003,
      "grad_norm": 2.091808319091797,
      "learning_rate": 9.726061824579039e-06,
      "loss": 1.0569,
      "step": 13550
    },
    {
      "epoch": 3.390921027949728,
      "grad_norm": 2.3770124912261963,
      "learning_rate": 9.710982658959539e-06,
      "loss": 1.1595,
      "step": 13560
    },
    {
      "epoch": 3.3934221221784533,
      "grad_norm": 1.9096976518630981,
      "learning_rate": 9.695903493340035e-06,
      "loss": 1.0478,
      "step": 13570
    },
    {
      "epoch": 3.395923216407178,
      "grad_norm": 5.281803131103516,
      "learning_rate": 9.680824327720533e-06,
      "loss": 1.1047,
      "step": 13580
    },
    {
      "epoch": 3.398424310635903,
      "grad_norm": 3.1843857765197754,
      "learning_rate": 9.66574516210103e-06,
      "loss": 1.1012,
      "step": 13590
    },
    {
      "epoch": 3.4009254048646285,
      "grad_norm": 1.7990261316299438,
      "learning_rate": 9.65066599648153e-06,
      "loss": 0.9526,
      "step": 13600
    },
    {
      "epoch": 3.4034264990933534,
      "grad_norm": 2.214672088623047,
      "learning_rate": 9.635586830862026e-06,
      "loss": 1.0434,
      "step": 13610
    },
    {
      "epoch": 3.4059275933220783,
      "grad_norm": 2.1168124675750732,
      "learning_rate": 9.620507665242524e-06,
      "loss": 1.0708,
      "step": 13620
    },
    {
      "epoch": 3.4084286875508036,
      "grad_norm": 1.8047999143600464,
      "learning_rate": 9.60542849962302e-06,
      "loss": 1.1134,
      "step": 13630
    },
    {
      "epoch": 3.4109297817795285,
      "grad_norm": 1.7313371896743774,
      "learning_rate": 9.59185725056547e-06,
      "loss": 1.163,
      "step": 13640
    },
    {
      "epoch": 3.4134308760082535,
      "grad_norm": 2.246140480041504,
      "learning_rate": 9.576778084945967e-06,
      "loss": 1.0933,
      "step": 13650
    },
    {
      "epoch": 3.415931970236979,
      "grad_norm": 1.8983027935028076,
      "learning_rate": 9.561698919326464e-06,
      "loss": 1.0608,
      "step": 13660
    },
    {
      "epoch": 3.4184330644657037,
      "grad_norm": 2.5080513954162598,
      "learning_rate": 9.546619753706962e-06,
      "loss": 1.0087,
      "step": 13670
    },
    {
      "epoch": 3.4209341586944286,
      "grad_norm": 2.3170344829559326,
      "learning_rate": 9.531540588087458e-06,
      "loss": 1.0207,
      "step": 13680
    },
    {
      "epoch": 3.423435252923154,
      "grad_norm": 2.3061277866363525,
      "learning_rate": 9.516461422467958e-06,
      "loss": 1.1125,
      "step": 13690
    },
    {
      "epoch": 3.425936347151879,
      "grad_norm": 3.2099218368530273,
      "learning_rate": 9.501382256848454e-06,
      "loss": 1.0462,
      "step": 13700
    },
    {
      "epoch": 3.428437441380604,
      "grad_norm": 1.9946497678756714,
      "learning_rate": 9.486303091228953e-06,
      "loss": 1.1047,
      "step": 13710
    },
    {
      "epoch": 3.430938535609329,
      "grad_norm": 2.214998483657837,
      "learning_rate": 9.471223925609449e-06,
      "loss": 1.0652,
      "step": 13720
    },
    {
      "epoch": 3.433439629838054,
      "grad_norm": 2.3877317905426025,
      "learning_rate": 9.456144759989949e-06,
      "loss": 1.049,
      "step": 13730
    },
    {
      "epoch": 3.4359407240667794,
      "grad_norm": 1.9374316930770874,
      "learning_rate": 9.441065594370445e-06,
      "loss": 1.0239,
      "step": 13740
    },
    {
      "epoch": 3.4384418182955043,
      "grad_norm": 2.2191343307495117,
      "learning_rate": 9.425986428750943e-06,
      "loss": 0.994,
      "step": 13750
    },
    {
      "epoch": 3.4409429125242292,
      "grad_norm": 1.7666075229644775,
      "learning_rate": 9.41090726313144e-06,
      "loss": 1.1128,
      "step": 13760
    },
    {
      "epoch": 3.4434440067529546,
      "grad_norm": 1.963073968887329,
      "learning_rate": 9.395828097511938e-06,
      "loss": 1.0435,
      "step": 13770
    },
    {
      "epoch": 3.4459451009816795,
      "grad_norm": 2.575369358062744,
      "learning_rate": 9.380748931892436e-06,
      "loss": 1.0769,
      "step": 13780
    },
    {
      "epoch": 3.4484461952104044,
      "grad_norm": 2.58620548248291,
      "learning_rate": 9.365669766272932e-06,
      "loss": 1.0575,
      "step": 13790
    },
    {
      "epoch": 3.4509472894391298,
      "grad_norm": 3.5110740661621094,
      "learning_rate": 9.35059060065343e-06,
      "loss": 1.0929,
      "step": 13800
    },
    {
      "epoch": 3.4534483836678547,
      "grad_norm": 2.0437815189361572,
      "learning_rate": 9.335511435033927e-06,
      "loss": 1.0423,
      "step": 13810
    },
    {
      "epoch": 3.4559494778965796,
      "grad_norm": 2.499938726425171,
      "learning_rate": 9.320432269414427e-06,
      "loss": 1.0801,
      "step": 13820
    },
    {
      "epoch": 3.458450572125305,
      "grad_norm": 2.358518362045288,
      "learning_rate": 9.305353103794923e-06,
      "loss": 1.1028,
      "step": 13830
    },
    {
      "epoch": 3.46095166635403,
      "grad_norm": 2.4486777782440186,
      "learning_rate": 9.290273938175421e-06,
      "loss": 1.0652,
      "step": 13840
    },
    {
      "epoch": 3.463452760582755,
      "grad_norm": 2.18206787109375,
      "learning_rate": 9.275194772555918e-06,
      "loss": 1.0704,
      "step": 13850
    },
    {
      "epoch": 3.46595385481148,
      "grad_norm": 2.242952346801758,
      "learning_rate": 9.260115606936418e-06,
      "loss": 1.0752,
      "step": 13860
    },
    {
      "epoch": 3.468454949040205,
      "grad_norm": 2.2912685871124268,
      "learning_rate": 9.245036441316914e-06,
      "loss": 1.0326,
      "step": 13870
    },
    {
      "epoch": 3.47095604326893,
      "grad_norm": 2.542358636856079,
      "learning_rate": 9.229957275697412e-06,
      "loss": 1.1178,
      "step": 13880
    },
    {
      "epoch": 3.4734571374976553,
      "grad_norm": 1.8822064399719238,
      "learning_rate": 9.214878110077908e-06,
      "loss": 1.0728,
      "step": 13890
    },
    {
      "epoch": 3.47595823172638,
      "grad_norm": 2.1337382793426514,
      "learning_rate": 9.199798944458407e-06,
      "loss": 1.0191,
      "step": 13900
    },
    {
      "epoch": 3.4784593259551055,
      "grad_norm": 1.991410732269287,
      "learning_rate": 9.184719778838905e-06,
      "loss": 1.106,
      "step": 13910
    },
    {
      "epoch": 3.4809604201838305,
      "grad_norm": 2.348787546157837,
      "learning_rate": 9.169640613219403e-06,
      "loss": 1.1156,
      "step": 13920
    },
    {
      "epoch": 3.4834615144125554,
      "grad_norm": 2.4061014652252197,
      "learning_rate": 9.1545614475999e-06,
      "loss": 1.099,
      "step": 13930
    },
    {
      "epoch": 3.4859626086412807,
      "grad_norm": 2.6520981788635254,
      "learning_rate": 9.139482281980397e-06,
      "loss": 1.0305,
      "step": 13940
    },
    {
      "epoch": 3.4884637028700056,
      "grad_norm": 2.1353392601013184,
      "learning_rate": 9.124403116360895e-06,
      "loss": 1.0958,
      "step": 13950
    },
    {
      "epoch": 3.4909647970987305,
      "grad_norm": 1.9331247806549072,
      "learning_rate": 9.109323950741394e-06,
      "loss": 1.0394,
      "step": 13960
    },
    {
      "epoch": 3.493465891327456,
      "grad_norm": 2.192122459411621,
      "learning_rate": 9.09424478512189e-06,
      "loss": 1.0714,
      "step": 13970
    },
    {
      "epoch": 3.495966985556181,
      "grad_norm": 1.8462188243865967,
      "learning_rate": 9.079165619502386e-06,
      "loss": 1.0746,
      "step": 13980
    },
    {
      "epoch": 3.4984680797849057,
      "grad_norm": 1.8740900754928589,
      "learning_rate": 9.064086453882886e-06,
      "loss": 1.0809,
      "step": 13990
    },
    {
      "epoch": 3.500969174013631,
      "grad_norm": 2.2309916019439697,
      "learning_rate": 9.049007288263383e-06,
      "loss": 1.1155,
      "step": 14000
    },
    {
      "epoch": 3.503470268242356,
      "grad_norm": 2.3248510360717773,
      "learning_rate": 9.03392812264388e-06,
      "loss": 1.073,
      "step": 14010
    },
    {
      "epoch": 3.5059713624710813,
      "grad_norm": 1.8633568286895752,
      "learning_rate": 9.018848957024377e-06,
      "loss": 1.1025,
      "step": 14020
    },
    {
      "epoch": 3.5084724566998062,
      "grad_norm": 2.026621103286743,
      "learning_rate": 9.003769791404875e-06,
      "loss": 1.0438,
      "step": 14030
    },
    {
      "epoch": 3.510973550928531,
      "grad_norm": 2.4596853256225586,
      "learning_rate": 8.988690625785373e-06,
      "loss": 1.0065,
      "step": 14040
    },
    {
      "epoch": 3.513474645157256,
      "grad_norm": 2.2428958415985107,
      "learning_rate": 8.973611460165872e-06,
      "loss": 1.0938,
      "step": 14050
    },
    {
      "epoch": 3.5159757393859814,
      "grad_norm": 2.0077736377716064,
      "learning_rate": 8.958532294546368e-06,
      "loss": 1.0377,
      "step": 14060
    },
    {
      "epoch": 3.5184768336147063,
      "grad_norm": 2.202789306640625,
      "learning_rate": 8.943453128926866e-06,
      "loss": 1.0766,
      "step": 14070
    },
    {
      "epoch": 3.5209779278434317,
      "grad_norm": 2.6490213871002197,
      "learning_rate": 8.928373963307364e-06,
      "loss": 1.0706,
      "step": 14080
    },
    {
      "epoch": 3.5234790220721566,
      "grad_norm": 2.0146212577819824,
      "learning_rate": 8.913294797687862e-06,
      "loss": 1.0972,
      "step": 14090
    },
    {
      "epoch": 3.5259801163008815,
      "grad_norm": 2.093357563018799,
      "learning_rate": 8.898215632068359e-06,
      "loss": 1.1305,
      "step": 14100
    },
    {
      "epoch": 3.528481210529607,
      "grad_norm": 2.042268991470337,
      "learning_rate": 8.883136466448857e-06,
      "loss": 0.9976,
      "step": 14110
    },
    {
      "epoch": 3.5309823047583317,
      "grad_norm": 1.8882310390472412,
      "learning_rate": 8.868057300829355e-06,
      "loss": 1.1064,
      "step": 14120
    },
    {
      "epoch": 3.533483398987057,
      "grad_norm": 2.1577296257019043,
      "learning_rate": 8.852978135209853e-06,
      "loss": 1.143,
      "step": 14130
    },
    {
      "epoch": 3.535984493215782,
      "grad_norm": 2.717366933822632,
      "learning_rate": 8.83789896959035e-06,
      "loss": 1.0284,
      "step": 14140
    },
    {
      "epoch": 3.538485587444507,
      "grad_norm": 2.4694902896881104,
      "learning_rate": 8.822819803970848e-06,
      "loss": 1.083,
      "step": 14150
    },
    {
      "epoch": 3.540986681673232,
      "grad_norm": 2.195172071456909,
      "learning_rate": 8.807740638351344e-06,
      "loss": 1.0527,
      "step": 14160
    },
    {
      "epoch": 3.543487775901957,
      "grad_norm": 2.2317731380462646,
      "learning_rate": 8.792661472731842e-06,
      "loss": 1.0834,
      "step": 14170
    },
    {
      "epoch": 3.545988870130682,
      "grad_norm": 2.2468550205230713,
      "learning_rate": 8.77758230711234e-06,
      "loss": 1.0727,
      "step": 14180
    },
    {
      "epoch": 3.5484899643594074,
      "grad_norm": 2.336571455001831,
      "learning_rate": 8.762503141492837e-06,
      "loss": 1.0157,
      "step": 14190
    },
    {
      "epoch": 3.5509910585881324,
      "grad_norm": 1.8807382583618164,
      "learning_rate": 8.747423975873335e-06,
      "loss": 1.1284,
      "step": 14200
    },
    {
      "epoch": 3.5534921528168573,
      "grad_norm": 2.1607825756073,
      "learning_rate": 8.732344810253833e-06,
      "loss": 1.0997,
      "step": 14210
    },
    {
      "epoch": 3.555993247045582,
      "grad_norm": 2.1749773025512695,
      "learning_rate": 8.717265644634331e-06,
      "loss": 1.0932,
      "step": 14220
    },
    {
      "epoch": 3.5584943412743075,
      "grad_norm": 2.391723155975342,
      "learning_rate": 8.702186479014827e-06,
      "loss": 1.0647,
      "step": 14230
    },
    {
      "epoch": 3.5609954355030324,
      "grad_norm": 2.2077324390411377,
      "learning_rate": 8.687107313395325e-06,
      "loss": 1.0887,
      "step": 14240
    },
    {
      "epoch": 3.563496529731758,
      "grad_norm": 2.4012482166290283,
      "learning_rate": 8.672028147775824e-06,
      "loss": 1.0893,
      "step": 14250
    },
    {
      "epoch": 3.5659976239604827,
      "grad_norm": 2.077815532684326,
      "learning_rate": 8.656948982156322e-06,
      "loss": 1.0292,
      "step": 14260
    },
    {
      "epoch": 3.5684987181892076,
      "grad_norm": 2.025319814682007,
      "learning_rate": 8.641869816536818e-06,
      "loss": 1.0993,
      "step": 14270
    },
    {
      "epoch": 3.570999812417933,
      "grad_norm": 1.8565351963043213,
      "learning_rate": 8.626790650917316e-06,
      "loss": 1.0332,
      "step": 14280
    },
    {
      "epoch": 3.573500906646658,
      "grad_norm": 2.3150699138641357,
      "learning_rate": 8.611711485297813e-06,
      "loss": 1.0573,
      "step": 14290
    },
    {
      "epoch": 3.5760020008753832,
      "grad_norm": 2.45304536819458,
      "learning_rate": 8.596632319678312e-06,
      "loss": 1.0477,
      "step": 14300
    },
    {
      "epoch": 3.578503095104108,
      "grad_norm": 2.239872932434082,
      "learning_rate": 8.581553154058809e-06,
      "loss": 1.1582,
      "step": 14310
    },
    {
      "epoch": 3.581004189332833,
      "grad_norm": 2.4890801906585693,
      "learning_rate": 8.566473988439307e-06,
      "loss": 1.0438,
      "step": 14320
    },
    {
      "epoch": 3.583505283561558,
      "grad_norm": 2.449770212173462,
      "learning_rate": 8.551394822819803e-06,
      "loss": 1.0917,
      "step": 14330
    },
    {
      "epoch": 3.5860063777902833,
      "grad_norm": 2.917357921600342,
      "learning_rate": 8.536315657200303e-06,
      "loss": 1.1187,
      "step": 14340
    },
    {
      "epoch": 3.588507472019008,
      "grad_norm": 2.9046239852905273,
      "learning_rate": 8.5212364915808e-06,
      "loss": 1.0939,
      "step": 14350
    },
    {
      "epoch": 3.5910085662477336,
      "grad_norm": 1.835844874382019,
      "learning_rate": 8.506157325961296e-06,
      "loss": 1.0063,
      "step": 14360
    },
    {
      "epoch": 3.5935096604764585,
      "grad_norm": 2.309591770172119,
      "learning_rate": 8.491078160341794e-06,
      "loss": 1.1103,
      "step": 14370
    },
    {
      "epoch": 3.5960107547051834,
      "grad_norm": 2.2930150032043457,
      "learning_rate": 8.475998994722292e-06,
      "loss": 1.0791,
      "step": 14380
    },
    {
      "epoch": 3.5985118489339087,
      "grad_norm": 1.9628612995147705,
      "learning_rate": 8.46091982910279e-06,
      "loss": 1.0495,
      "step": 14390
    },
    {
      "epoch": 3.6010129431626337,
      "grad_norm": 1.8431819677352905,
      "learning_rate": 8.445840663483287e-06,
      "loss": 1.0755,
      "step": 14400
    },
    {
      "epoch": 3.6035140373913586,
      "grad_norm": 2.204362630844116,
      "learning_rate": 8.430761497863785e-06,
      "loss": 1.0902,
      "step": 14410
    },
    {
      "epoch": 3.606015131620084,
      "grad_norm": 2.148375988006592,
      "learning_rate": 8.415682332244281e-06,
      "loss": 1.0843,
      "step": 14420
    },
    {
      "epoch": 3.608516225848809,
      "grad_norm": 2.0934154987335205,
      "learning_rate": 8.400603166624781e-06,
      "loss": 1.0087,
      "step": 14430
    },
    {
      "epoch": 3.6110173200775337,
      "grad_norm": 2.687748908996582,
      "learning_rate": 8.385524001005278e-06,
      "loss": 1.0258,
      "step": 14440
    },
    {
      "epoch": 3.613518414306259,
      "grad_norm": 2.469464063644409,
      "learning_rate": 8.370444835385776e-06,
      "loss": 1.1499,
      "step": 14450
    },
    {
      "epoch": 3.616019508534984,
      "grad_norm": 2.479379177093506,
      "learning_rate": 8.355365669766272e-06,
      "loss": 1.0613,
      "step": 14460
    },
    {
      "epoch": 3.6185206027637093,
      "grad_norm": 2.148932933807373,
      "learning_rate": 8.340286504146772e-06,
      "loss": 1.0082,
      "step": 14470
    },
    {
      "epoch": 3.6210216969924343,
      "grad_norm": 2.5355074405670166,
      "learning_rate": 8.325207338527268e-06,
      "loss": 1.0691,
      "step": 14480
    },
    {
      "epoch": 3.623522791221159,
      "grad_norm": 2.2578532695770264,
      "learning_rate": 8.310128172907766e-06,
      "loss": 1.0563,
      "step": 14490
    },
    {
      "epoch": 3.626023885449884,
      "grad_norm": 2.426867961883545,
      "learning_rate": 8.295049007288263e-06,
      "loss": 1.0053,
      "step": 14500
    },
    {
      "epoch": 3.6285249796786094,
      "grad_norm": 2.976757287979126,
      "learning_rate": 8.279969841668763e-06,
      "loss": 1.0508,
      "step": 14510
    },
    {
      "epoch": 3.6310260739073343,
      "grad_norm": 2.2904515266418457,
      "learning_rate": 8.264890676049259e-06,
      "loss": 1.1567,
      "step": 14520
    },
    {
      "epoch": 3.6335271681360597,
      "grad_norm": 2.385932207107544,
      "learning_rate": 8.249811510429757e-06,
      "loss": 1.036,
      "step": 14530
    },
    {
      "epoch": 3.6360282623647846,
      "grad_norm": 2.1019985675811768,
      "learning_rate": 8.234732344810254e-06,
      "loss": 1.0634,
      "step": 14540
    },
    {
      "epoch": 3.6385293565935095,
      "grad_norm": 1.7722227573394775,
      "learning_rate": 8.21965317919075e-06,
      "loss": 1.0425,
      "step": 14550
    },
    {
      "epoch": 3.641030450822235,
      "grad_norm": 2.1264851093292236,
      "learning_rate": 8.20457401357125e-06,
      "loss": 1.0564,
      "step": 14560
    },
    {
      "epoch": 3.6435315450509598,
      "grad_norm": 2.1622567176818848,
      "learning_rate": 8.189494847951746e-06,
      "loss": 1.0722,
      "step": 14570
    },
    {
      "epoch": 3.646032639279685,
      "grad_norm": 2.0061562061309814,
      "learning_rate": 8.174415682332244e-06,
      "loss": 1.1426,
      "step": 14580
    },
    {
      "epoch": 3.64853373350841,
      "grad_norm": 1.8602838516235352,
      "learning_rate": 8.15933651671274e-06,
      "loss": 1.0745,
      "step": 14590
    },
    {
      "epoch": 3.651034827737135,
      "grad_norm": 2.604471206665039,
      "learning_rate": 8.14425735109324e-06,
      "loss": 1.0556,
      "step": 14600
    },
    {
      "epoch": 3.65353592196586,
      "grad_norm": 2.782759428024292,
      "learning_rate": 8.129178185473737e-06,
      "loss": 1.1312,
      "step": 14610
    },
    {
      "epoch": 3.656037016194585,
      "grad_norm": 1.8260877132415771,
      "learning_rate": 8.114099019854235e-06,
      "loss": 1.0235,
      "step": 14620
    },
    {
      "epoch": 3.65853811042331,
      "grad_norm": 2.486588954925537,
      "learning_rate": 8.099019854234732e-06,
      "loss": 1.0951,
      "step": 14630
    },
    {
      "epoch": 3.6610392046520355,
      "grad_norm": 2.0987446308135986,
      "learning_rate": 8.083940688615231e-06,
      "loss": 1.0745,
      "step": 14640
    },
    {
      "epoch": 3.6635402988807604,
      "grad_norm": 2.708954334259033,
      "learning_rate": 8.068861522995728e-06,
      "loss": 1.082,
      "step": 14650
    },
    {
      "epoch": 3.6660413931094853,
      "grad_norm": 1.7666015625,
      "learning_rate": 8.053782357376226e-06,
      "loss": 1.1123,
      "step": 14660
    },
    {
      "epoch": 3.66854248733821,
      "grad_norm": 1.9702240228652954,
      "learning_rate": 8.038703191756722e-06,
      "loss": 1.0203,
      "step": 14670
    },
    {
      "epoch": 3.6710435815669356,
      "grad_norm": 2.1249053478240967,
      "learning_rate": 8.02362402613722e-06,
      "loss": 1.0239,
      "step": 14680
    },
    {
      "epoch": 3.6735446757956605,
      "grad_norm": 2.022521495819092,
      "learning_rate": 8.008544860517719e-06,
      "loss": 1.0716,
      "step": 14690
    },
    {
      "epoch": 3.676045770024386,
      "grad_norm": 1.700411081314087,
      "learning_rate": 7.993465694898217e-06,
      "loss": 1.0625,
      "step": 14700
    },
    {
      "epoch": 3.6785468642531107,
      "grad_norm": 2.431079864501953,
      "learning_rate": 7.978386529278713e-06,
      "loss": 1.0764,
      "step": 14710
    },
    {
      "epoch": 3.6810479584818356,
      "grad_norm": 2.4240567684173584,
      "learning_rate": 7.963307363659211e-06,
      "loss": 1.1007,
      "step": 14720
    },
    {
      "epoch": 3.683549052710561,
      "grad_norm": 1.9918992519378662,
      "learning_rate": 7.94822819803971e-06,
      "loss": 1.1164,
      "step": 14730
    },
    {
      "epoch": 3.686050146939286,
      "grad_norm": 2.5596251487731934,
      "learning_rate": 7.933149032420206e-06,
      "loss": 1.0786,
      "step": 14740
    },
    {
      "epoch": 3.6885512411680113,
      "grad_norm": 2.151940107345581,
      "learning_rate": 7.918069866800704e-06,
      "loss": 1.1052,
      "step": 14750
    },
    {
      "epoch": 3.691052335396736,
      "grad_norm": 2.342287302017212,
      "learning_rate": 7.9029907011812e-06,
      "loss": 1.0767,
      "step": 14760
    },
    {
      "epoch": 3.693553429625461,
      "grad_norm": 2.4056169986724854,
      "learning_rate": 7.8879115355617e-06,
      "loss": 1.0067,
      "step": 14770
    },
    {
      "epoch": 3.696054523854186,
      "grad_norm": 1.935185194015503,
      "learning_rate": 7.872832369942196e-06,
      "loss": 1.1316,
      "step": 14780
    },
    {
      "epoch": 3.6985556180829113,
      "grad_norm": 2.467757225036621,
      "learning_rate": 7.857753204322695e-06,
      "loss": 1.1218,
      "step": 14790
    },
    {
      "epoch": 3.7010567123116362,
      "grad_norm": 1.8825637102127075,
      "learning_rate": 7.842674038703191e-06,
      "loss": 1.0904,
      "step": 14800
    },
    {
      "epoch": 3.7035578065403616,
      "grad_norm": 2.6544415950775146,
      "learning_rate": 7.827594873083689e-06,
      "loss": 1.1583,
      "step": 14810
    },
    {
      "epoch": 3.7060589007690865,
      "grad_norm": 2.3411035537719727,
      "learning_rate": 7.812515707464187e-06,
      "loss": 1.0376,
      "step": 14820
    },
    {
      "epoch": 3.7085599949978114,
      "grad_norm": 2.5191726684570312,
      "learning_rate": 7.797436541844685e-06,
      "loss": 1.1235,
      "step": 14830
    },
    {
      "epoch": 3.7110610892265368,
      "grad_norm": 1.9130775928497314,
      "learning_rate": 7.782357376225182e-06,
      "loss": 1.0636,
      "step": 14840
    },
    {
      "epoch": 3.7135621834552617,
      "grad_norm": 2.061316967010498,
      "learning_rate": 7.76727821060568e-06,
      "loss": 1.123,
      "step": 14850
    },
    {
      "epoch": 3.7160632776839866,
      "grad_norm": 1.95292067527771,
      "learning_rate": 7.752199044986178e-06,
      "loss": 1.0772,
      "step": 14860
    },
    {
      "epoch": 3.718564371912712,
      "grad_norm": 2.2239222526550293,
      "learning_rate": 7.737119879366676e-06,
      "loss": 1.0912,
      "step": 14870
    },
    {
      "epoch": 3.721065466141437,
      "grad_norm": 2.25663161277771,
      "learning_rate": 7.722040713747173e-06,
      "loss": 1.1318,
      "step": 14880
    },
    {
      "epoch": 3.7235665603701618,
      "grad_norm": 2.0075020790100098,
      "learning_rate": 7.70696154812767e-06,
      "loss": 1.0444,
      "step": 14890
    },
    {
      "epoch": 3.726067654598887,
      "grad_norm": 2.2269723415374756,
      "learning_rate": 7.691882382508169e-06,
      "loss": 0.9409,
      "step": 14900
    },
    {
      "epoch": 3.728568748827612,
      "grad_norm": 2.206348419189453,
      "learning_rate": 7.676803216888667e-06,
      "loss": 1.0602,
      "step": 14910
    },
    {
      "epoch": 3.7310698430563374,
      "grad_norm": 2.4474008083343506,
      "learning_rate": 7.661724051269163e-06,
      "loss": 1.0705,
      "step": 14920
    },
    {
      "epoch": 3.7335709372850623,
      "grad_norm": 2.7945001125335693,
      "learning_rate": 7.64664488564966e-06,
      "loss": 0.9873,
      "step": 14930
    },
    {
      "epoch": 3.736072031513787,
      "grad_norm": 1.8971364498138428,
      "learning_rate": 7.631565720030158e-06,
      "loss": 1.1103,
      "step": 14940
    },
    {
      "epoch": 3.738573125742512,
      "grad_norm": 2.216803550720215,
      "learning_rate": 7.616486554410655e-06,
      "loss": 1.0749,
      "step": 14950
    },
    {
      "epoch": 3.7410742199712375,
      "grad_norm": 2.1234290599823,
      "learning_rate": 7.601407388791154e-06,
      "loss": 1.0767,
      "step": 14960
    },
    {
      "epoch": 3.7435753141999624,
      "grad_norm": 2.398714542388916,
      "learning_rate": 7.5863282231716505e-06,
      "loss": 1.0134,
      "step": 14970
    },
    {
      "epoch": 3.7460764084286877,
      "grad_norm": 2.142091751098633,
      "learning_rate": 7.5712490575521494e-06,
      "loss": 1.1343,
      "step": 14980
    },
    {
      "epoch": 3.7485775026574126,
      "grad_norm": 1.9363632202148438,
      "learning_rate": 7.556169891932646e-06,
      "loss": 1.023,
      "step": 14990
    },
    {
      "epoch": 3.7510785968861375,
      "grad_norm": 2.2604455947875977,
      "learning_rate": 7.541090726313145e-06,
      "loss": 0.9825,
      "step": 15000
    },
    {
      "epoch": 3.753579691114863,
      "grad_norm": 2.498175621032715,
      "learning_rate": 7.526011560693641e-06,
      "loss": 1.0483,
      "step": 15010
    },
    {
      "epoch": 3.756080785343588,
      "grad_norm": 2.514991521835327,
      "learning_rate": 7.51093239507414e-06,
      "loss": 1.0359,
      "step": 15020
    },
    {
      "epoch": 3.758581879572313,
      "grad_norm": 2.247429609298706,
      "learning_rate": 7.495853229454637e-06,
      "loss": 1.0734,
      "step": 15030
    },
    {
      "epoch": 3.761082973801038,
      "grad_norm": 2.5232994556427,
      "learning_rate": 7.480774063835135e-06,
      "loss": 1.0503,
      "step": 15040
    },
    {
      "epoch": 3.763584068029763,
      "grad_norm": 2.00394606590271,
      "learning_rate": 7.465694898215632e-06,
      "loss": 1.035,
      "step": 15050
    },
    {
      "epoch": 3.766085162258488,
      "grad_norm": 2.141479730606079,
      "learning_rate": 7.450615732596129e-06,
      "loss": 1.0682,
      "step": 15060
    },
    {
      "epoch": 3.7685862564872132,
      "grad_norm": 2.912364959716797,
      "learning_rate": 7.435536566976627e-06,
      "loss": 1.0624,
      "step": 15070
    },
    {
      "epoch": 3.771087350715938,
      "grad_norm": 2.1232104301452637,
      "learning_rate": 7.420457401357125e-06,
      "loss": 1.0003,
      "step": 15080
    },
    {
      "epoch": 3.7735884449446635,
      "grad_norm": 2.8888888359069824,
      "learning_rate": 7.405378235737623e-06,
      "loss": 0.9871,
      "step": 15090
    },
    {
      "epoch": 3.7760895391733884,
      "grad_norm": 3.1799285411834717,
      "learning_rate": 7.39029907011812e-06,
      "loss": 1.1387,
      "step": 15100
    },
    {
      "epoch": 3.7785906334021133,
      "grad_norm": 2.242940664291382,
      "learning_rate": 7.375219904498618e-06,
      "loss": 1.1143,
      "step": 15110
    },
    {
      "epoch": 3.7810917276308382,
      "grad_norm": 2.5814249515533447,
      "learning_rate": 7.360140738879115e-06,
      "loss": 1.0216,
      "step": 15120
    },
    {
      "epoch": 3.7835928218595636,
      "grad_norm": 2.01155948638916,
      "learning_rate": 7.3450615732596135e-06,
      "loss": 1.0766,
      "step": 15130
    },
    {
      "epoch": 3.7860939160882885,
      "grad_norm": 2.5532138347625732,
      "learning_rate": 7.329982407640111e-06,
      "loss": 1.1023,
      "step": 15140
    },
    {
      "epoch": 3.788595010317014,
      "grad_norm": 2.194382667541504,
      "learning_rate": 7.314903242020609e-06,
      "loss": 1.1142,
      "step": 15150
    },
    {
      "epoch": 3.7910961045457388,
      "grad_norm": 2.4556894302368164,
      "learning_rate": 7.299824076401106e-06,
      "loss": 1.0776,
      "step": 15160
    },
    {
      "epoch": 3.7935971987744637,
      "grad_norm": 1.9792428016662598,
      "learning_rate": 7.284744910781604e-06,
      "loss": 1.119,
      "step": 15170
    },
    {
      "epoch": 3.796098293003189,
      "grad_norm": 2.165235757827759,
      "learning_rate": 7.2696657451621015e-06,
      "loss": 1.0375,
      "step": 15180
    },
    {
      "epoch": 3.798599387231914,
      "grad_norm": 2.4050629138946533,
      "learning_rate": 7.254586579542599e-06,
      "loss": 1.0575,
      "step": 15190
    },
    {
      "epoch": 3.8011004814606393,
      "grad_norm": 2.0731136798858643,
      "learning_rate": 7.239507413923097e-06,
      "loss": 0.9925,
      "step": 15200
    },
    {
      "epoch": 3.803601575689364,
      "grad_norm": 2.334354877471924,
      "learning_rate": 7.224428248303593e-06,
      "loss": 1.0831,
      "step": 15210
    },
    {
      "epoch": 3.806102669918089,
      "grad_norm": 1.585518717765808,
      "learning_rate": 7.2093490826840915e-06,
      "loss": 1.1771,
      "step": 15220
    },
    {
      "epoch": 3.808603764146814,
      "grad_norm": 1.968660593032837,
      "learning_rate": 7.194269917064589e-06,
      "loss": 1.1211,
      "step": 15230
    },
    {
      "epoch": 3.8111048583755394,
      "grad_norm": 2.750408172607422,
      "learning_rate": 7.179190751445087e-06,
      "loss": 1.1123,
      "step": 15240
    },
    {
      "epoch": 3.8136059526042643,
      "grad_norm": 1.8241573572158813,
      "learning_rate": 7.164111585825584e-06,
      "loss": 1.0118,
      "step": 15250
    },
    {
      "epoch": 3.8161070468329896,
      "grad_norm": 2.1534430980682373,
      "learning_rate": 7.149032420206082e-06,
      "loss": 1.0793,
      "step": 15260
    },
    {
      "epoch": 3.8186081410617145,
      "grad_norm": 3.579080820083618,
      "learning_rate": 7.1339532545865795e-06,
      "loss": 1.0336,
      "step": 15270
    },
    {
      "epoch": 3.8211092352904394,
      "grad_norm": 2.734938859939575,
      "learning_rate": 7.118874088967078e-06,
      "loss": 1.1,
      "step": 15280
    },
    {
      "epoch": 3.823610329519165,
      "grad_norm": 1.787672519683838,
      "learning_rate": 7.103794923347575e-06,
      "loss": 1.0458,
      "step": 15290
    },
    {
      "epoch": 3.8261114237478897,
      "grad_norm": 1.8320469856262207,
      "learning_rate": 7.088715757728073e-06,
      "loss": 1.1765,
      "step": 15300
    },
    {
      "epoch": 3.8286125179766146,
      "grad_norm": 2.299858570098877,
      "learning_rate": 7.07363659210857e-06,
      "loss": 1.0894,
      "step": 15310
    },
    {
      "epoch": 3.83111361220534,
      "grad_norm": 2.400216817855835,
      "learning_rate": 7.0585574264890675e-06,
      "loss": 1.0333,
      "step": 15320
    },
    {
      "epoch": 3.833614706434065,
      "grad_norm": 2.4047186374664307,
      "learning_rate": 7.043478260869566e-06,
      "loss": 1.1155,
      "step": 15330
    },
    {
      "epoch": 3.83611580066279,
      "grad_norm": 2.2842588424682617,
      "learning_rate": 7.028399095250063e-06,
      "loss": 1.0888,
      "step": 15340
    },
    {
      "epoch": 3.838616894891515,
      "grad_norm": 2.1268692016601562,
      "learning_rate": 7.013319929630561e-06,
      "loss": 1.0869,
      "step": 15350
    },
    {
      "epoch": 3.84111798912024,
      "grad_norm": 2.5395758152008057,
      "learning_rate": 6.998240764011058e-06,
      "loss": 1.0339,
      "step": 15360
    },
    {
      "epoch": 3.8436190833489654,
      "grad_norm": 2.0399627685546875,
      "learning_rate": 6.983161598391556e-06,
      "loss": 1.1351,
      "step": 15370
    },
    {
      "epoch": 3.8461201775776903,
      "grad_norm": 2.4397802352905273,
      "learning_rate": 6.968082432772054e-06,
      "loss": 1.0576,
      "step": 15380
    },
    {
      "epoch": 3.8486212718064152,
      "grad_norm": 2.7364373207092285,
      "learning_rate": 6.953003267152552e-06,
      "loss": 1.1646,
      "step": 15390
    },
    {
      "epoch": 3.85112236603514,
      "grad_norm": 2.4695961475372314,
      "learning_rate": 6.937924101533048e-06,
      "loss": 1.1045,
      "step": 15400
    },
    {
      "epoch": 3.8536234602638655,
      "grad_norm": 2.360046863555908,
      "learning_rate": 6.922844935913546e-06,
      "loss": 1.1279,
      "step": 15410
    },
    {
      "epoch": 3.8561245544925904,
      "grad_norm": 2.2402284145355225,
      "learning_rate": 6.9077657702940436e-06,
      "loss": 1.028,
      "step": 15420
    },
    {
      "epoch": 3.8586256487213157,
      "grad_norm": 1.8921613693237305,
      "learning_rate": 6.892686604674542e-06,
      "loss": 1.0769,
      "step": 15430
    },
    {
      "epoch": 3.8611267429500407,
      "grad_norm": 1.8405256271362305,
      "learning_rate": 6.877607439055039e-06,
      "loss": 1.0376,
      "step": 15440
    },
    {
      "epoch": 3.8636278371787656,
      "grad_norm": 2.233548402786255,
      "learning_rate": 6.862528273435536e-06,
      "loss": 1.0221,
      "step": 15450
    },
    {
      "epoch": 3.866128931407491,
      "grad_norm": 2.2444241046905518,
      "learning_rate": 6.847449107816034e-06,
      "loss": 1.0561,
      "step": 15460
    },
    {
      "epoch": 3.868630025636216,
      "grad_norm": 2.6752636432647705,
      "learning_rate": 6.832369942196532e-06,
      "loss": 1.0716,
      "step": 15470
    },
    {
      "epoch": 3.871131119864941,
      "grad_norm": 2.389655828475952,
      "learning_rate": 6.81729077657703e-06,
      "loss": 1.089,
      "step": 15480
    },
    {
      "epoch": 3.873632214093666,
      "grad_norm": 2.235308885574341,
      "learning_rate": 6.802211610957527e-06,
      "loss": 1.097,
      "step": 15490
    },
    {
      "epoch": 3.876133308322391,
      "grad_norm": 2.126159906387329,
      "learning_rate": 6.787132445338025e-06,
      "loss": 1.0805,
      "step": 15500
    },
    {
      "epoch": 3.878634402551116,
      "grad_norm": 2.571495294570923,
      "learning_rate": 6.772053279718522e-06,
      "loss": 1.0626,
      "step": 15510
    },
    {
      "epoch": 3.8811354967798413,
      "grad_norm": 2.084106922149658,
      "learning_rate": 6.7569741140990205e-06,
      "loss": 1.0604,
      "step": 15520
    },
    {
      "epoch": 3.883636591008566,
      "grad_norm": 1.9373043775558472,
      "learning_rate": 6.741894948479518e-06,
      "loss": 1.0854,
      "step": 15530
    },
    {
      "epoch": 3.8861376852372915,
      "grad_norm": 2.8575446605682373,
      "learning_rate": 6.726815782860016e-06,
      "loss": 1.0817,
      "step": 15540
    },
    {
      "epoch": 3.8886387794660164,
      "grad_norm": 2.073885679244995,
      "learning_rate": 6.711736617240513e-06,
      "loss": 1.0579,
      "step": 15550
    },
    {
      "epoch": 3.8911398736947413,
      "grad_norm": 2.010226011276245,
      "learning_rate": 6.69665745162101e-06,
      "loss": 1.0697,
      "step": 15560
    },
    {
      "epoch": 3.8936409679234663,
      "grad_norm": 2.0484743118286133,
      "learning_rate": 6.6815782860015085e-06,
      "loss": 1.0963,
      "step": 15570
    },
    {
      "epoch": 3.8961420621521916,
      "grad_norm": 2.323462724685669,
      "learning_rate": 6.666499120382006e-06,
      "loss": 1.1133,
      "step": 15580
    },
    {
      "epoch": 3.8986431563809165,
      "grad_norm": 2.2808618545532227,
      "learning_rate": 6.651419954762503e-06,
      "loss": 1.1024,
      "step": 15590
    },
    {
      "epoch": 3.901144250609642,
      "grad_norm": 2.3380954265594482,
      "learning_rate": 6.636340789143e-06,
      "loss": 1.0552,
      "step": 15600
    },
    {
      "epoch": 3.903645344838367,
      "grad_norm": 2.452721357345581,
      "learning_rate": 6.621261623523498e-06,
      "loss": 1.0405,
      "step": 15610
    },
    {
      "epoch": 3.9061464390670917,
      "grad_norm": 2.2196342945098877,
      "learning_rate": 6.606182457903996e-06,
      "loss": 1.0909,
      "step": 15620
    },
    {
      "epoch": 3.908647533295817,
      "grad_norm": 2.49446177482605,
      "learning_rate": 6.591103292284494e-06,
      "loss": 1.1661,
      "step": 15630
    },
    {
      "epoch": 3.911148627524542,
      "grad_norm": 1.878650188446045,
      "learning_rate": 6.576024126664991e-06,
      "loss": 1.0731,
      "step": 15640
    },
    {
      "epoch": 3.9136497217532673,
      "grad_norm": 2.259072780609131,
      "learning_rate": 6.560944961045489e-06,
      "loss": 1.0049,
      "step": 15650
    },
    {
      "epoch": 3.916150815981992,
      "grad_norm": 2.3929662704467773,
      "learning_rate": 6.545865795425986e-06,
      "loss": 1.1302,
      "step": 15660
    },
    {
      "epoch": 3.918651910210717,
      "grad_norm": 4.037023544311523,
      "learning_rate": 6.5307866298064845e-06,
      "loss": 1.0564,
      "step": 15670
    },
    {
      "epoch": 3.921153004439442,
      "grad_norm": 1.8937386274337769,
      "learning_rate": 6.515707464186982e-06,
      "loss": 1.0819,
      "step": 15680
    },
    {
      "epoch": 3.9236540986681674,
      "grad_norm": 2.284008502960205,
      "learning_rate": 6.500628298567479e-06,
      "loss": 1.0393,
      "step": 15690
    },
    {
      "epoch": 3.9261551928968923,
      "grad_norm": 2.2340478897094727,
      "learning_rate": 6.485549132947977e-06,
      "loss": 1.1015,
      "step": 15700
    },
    {
      "epoch": 3.9286562871256177,
      "grad_norm": 2.1128227710723877,
      "learning_rate": 6.4704699673284744e-06,
      "loss": 1.0313,
      "step": 15710
    },
    {
      "epoch": 3.9311573813543426,
      "grad_norm": 2.61104154586792,
      "learning_rate": 6.4553908017089725e-06,
      "loss": 1.1458,
      "step": 15720
    },
    {
      "epoch": 3.9336584755830675,
      "grad_norm": 2.962315797805786,
      "learning_rate": 6.44031163608947e-06,
      "loss": 1.0905,
      "step": 15730
    },
    {
      "epoch": 3.936159569811793,
      "grad_norm": 2.469010591506958,
      "learning_rate": 6.425232470469968e-06,
      "loss": 1.055,
      "step": 15740
    },
    {
      "epoch": 3.9386606640405177,
      "grad_norm": 2.3553662300109863,
      "learning_rate": 6.410153304850465e-06,
      "loss": 1.0532,
      "step": 15750
    },
    {
      "epoch": 3.9411617582692426,
      "grad_norm": 2.169656991958618,
      "learning_rate": 6.395074139230963e-06,
      "loss": 1.059,
      "step": 15760
    },
    {
      "epoch": 3.943662852497968,
      "grad_norm": 2.501103639602661,
      "learning_rate": 6.3799949736114606e-06,
      "loss": 1.1281,
      "step": 15770
    },
    {
      "epoch": 3.946163946726693,
      "grad_norm": 1.9460467100143433,
      "learning_rate": 6.364915807991958e-06,
      "loss": 1.1295,
      "step": 15780
    },
    {
      "epoch": 3.948665040955418,
      "grad_norm": 2.0767619609832764,
      "learning_rate": 6.349836642372455e-06,
      "loss": 1.0829,
      "step": 15790
    },
    {
      "epoch": 3.951166135184143,
      "grad_norm": 2.069049596786499,
      "learning_rate": 6.334757476752953e-06,
      "loss": 1.0577,
      "step": 15800
    },
    {
      "epoch": 3.953667229412868,
      "grad_norm": 2.5008387565612793,
      "learning_rate": 6.3196783111334505e-06,
      "loss": 1.1157,
      "step": 15810
    },
    {
      "epoch": 3.9561683236415934,
      "grad_norm": 2.5102059841156006,
      "learning_rate": 6.304599145513948e-06,
      "loss": 1.0758,
      "step": 15820
    },
    {
      "epoch": 3.9586694178703183,
      "grad_norm": 2.078958749771118,
      "learning_rate": 6.289519979894446e-06,
      "loss": 1.053,
      "step": 15830
    },
    {
      "epoch": 3.9611705120990433,
      "grad_norm": 2.0700020790100098,
      "learning_rate": 6.274440814274943e-06,
      "loss": 1.1425,
      "step": 15840
    },
    {
      "epoch": 3.963671606327768,
      "grad_norm": 2.074812650680542,
      "learning_rate": 6.259361648655441e-06,
      "loss": 1.1012,
      "step": 15850
    },
    {
      "epoch": 3.9661727005564935,
      "grad_norm": 1.8764874935150146,
      "learning_rate": 6.2442824830359385e-06,
      "loss": 1.0461,
      "step": 15860
    },
    {
      "epoch": 3.9686737947852184,
      "grad_norm": 2.466111421585083,
      "learning_rate": 6.229203317416437e-06,
      "loss": 1.11,
      "step": 15870
    },
    {
      "epoch": 3.9711748890139438,
      "grad_norm": 2.5445449352264404,
      "learning_rate": 6.214124151796934e-06,
      "loss": 1.12,
      "step": 15880
    },
    {
      "epoch": 3.9736759832426687,
      "grad_norm": 1.8577179908752441,
      "learning_rate": 6.199044986177432e-06,
      "loss": 1.1046,
      "step": 15890
    },
    {
      "epoch": 3.9761770774713936,
      "grad_norm": 2.1565945148468018,
      "learning_rate": 6.183965820557929e-06,
      "loss": 1.1129,
      "step": 15900
    },
    {
      "epoch": 3.978678171700119,
      "grad_norm": 2.5224809646606445,
      "learning_rate": 6.168886654938427e-06,
      "loss": 1.017,
      "step": 15910
    },
    {
      "epoch": 3.981179265928844,
      "grad_norm": 2.167705774307251,
      "learning_rate": 6.153807489318925e-06,
      "loss": 1.0917,
      "step": 15920
    },
    {
      "epoch": 3.983680360157569,
      "grad_norm": 2.105403184890747,
      "learning_rate": 6.138728323699423e-06,
      "loss": 1.0501,
      "step": 15930
    },
    {
      "epoch": 3.986181454386294,
      "grad_norm": 1.8000667095184326,
      "learning_rate": 6.12364915807992e-06,
      "loss": 1.0609,
      "step": 15940
    },
    {
      "epoch": 3.988682548615019,
      "grad_norm": 2.294867753982544,
      "learning_rate": 6.108569992460417e-06,
      "loss": 1.0397,
      "step": 15950
    },
    {
      "epoch": 3.991183642843744,
      "grad_norm": 2.514120578765869,
      "learning_rate": 6.093490826840915e-06,
      "loss": 1.0806,
      "step": 15960
    },
    {
      "epoch": 3.9936847370724693,
      "grad_norm": 2.657093048095703,
      "learning_rate": 6.078411661221412e-06,
      "loss": 1.1208,
      "step": 15970
    },
    {
      "epoch": 3.996185831301194,
      "grad_norm": 2.1660869121551514,
      "learning_rate": 6.06333249560191e-06,
      "loss": 1.09,
      "step": 15980
    },
    {
      "epoch": 3.9986869255299196,
      "grad_norm": 1.5998777151107788,
      "learning_rate": 6.048253329982407e-06,
      "loss": 1.0838,
      "step": 15990
    },
    {
      "epoch": 4.00100043769149,
      "grad_norm": 2.118985176086426,
      "learning_rate": 6.033174164362905e-06,
      "loss": 1.0968,
      "step": 16000
    },
    {
      "epoch": 4.003501531920215,
      "grad_norm": 2.037651777267456,
      "learning_rate": 6.018094998743403e-06,
      "loss": 1.0729,
      "step": 16010
    },
    {
      "epoch": 4.00600262614894,
      "grad_norm": 2.391113758087158,
      "learning_rate": 6.003015833123901e-06,
      "loss": 1.1248,
      "step": 16020
    },
    {
      "epoch": 4.008503720377665,
      "grad_norm": 2.689635753631592,
      "learning_rate": 5.987936667504398e-06,
      "loss": 1.1076,
      "step": 16030
    },
    {
      "epoch": 4.01100481460639,
      "grad_norm": 2.023225784301758,
      "learning_rate": 5.972857501884896e-06,
      "loss": 1.0799,
      "step": 16040
    },
    {
      "epoch": 4.013505908835115,
      "grad_norm": 3.0000646114349365,
      "learning_rate": 5.957778336265393e-06,
      "loss": 1.1107,
      "step": 16050
    },
    {
      "epoch": 4.016007003063841,
      "grad_norm": 2.0805158615112305,
      "learning_rate": 5.9426991706458915e-06,
      "loss": 1.1288,
      "step": 16060
    },
    {
      "epoch": 4.018508097292566,
      "grad_norm": 2.209954023361206,
      "learning_rate": 5.927620005026389e-06,
      "loss": 1.1116,
      "step": 16070
    },
    {
      "epoch": 4.021009191521291,
      "grad_norm": 2.413316488265991,
      "learning_rate": 5.912540839406886e-06,
      "loss": 1.0944,
      "step": 16080
    },
    {
      "epoch": 4.023510285750016,
      "grad_norm": 2.7500319480895996,
      "learning_rate": 5.897461673787384e-06,
      "loss": 1.0939,
      "step": 16090
    },
    {
      "epoch": 4.0260113799787405,
      "grad_norm": 1.838454246520996,
      "learning_rate": 5.882382508167881e-06,
      "loss": 1.0383,
      "step": 16100
    },
    {
      "epoch": 4.028512474207465,
      "grad_norm": 2.9061036109924316,
      "learning_rate": 5.8673033425483795e-06,
      "loss": 1.0796,
      "step": 16110
    },
    {
      "epoch": 4.031013568436191,
      "grad_norm": 1.7996681928634644,
      "learning_rate": 5.852224176928877e-06,
      "loss": 1.0522,
      "step": 16120
    },
    {
      "epoch": 4.033514662664916,
      "grad_norm": 2.2087783813476562,
      "learning_rate": 5.837145011309375e-06,
      "loss": 1.0032,
      "step": 16130
    },
    {
      "epoch": 4.036015756893641,
      "grad_norm": 2.4928057193756104,
      "learning_rate": 5.822065845689872e-06,
      "loss": 1.1328,
      "step": 16140
    },
    {
      "epoch": 4.038516851122366,
      "grad_norm": 2.737030506134033,
      "learning_rate": 5.80698668007037e-06,
      "loss": 1.1545,
      "step": 16150
    },
    {
      "epoch": 4.041017945351091,
      "grad_norm": 2.615368366241455,
      "learning_rate": 5.791907514450867e-06,
      "loss": 1.1033,
      "step": 16160
    },
    {
      "epoch": 4.043519039579816,
      "grad_norm": 2.1425647735595703,
      "learning_rate": 5.776828348831365e-06,
      "loss": 1.0851,
      "step": 16170
    },
    {
      "epoch": 4.0460201338085415,
      "grad_norm": 2.4524049758911133,
      "learning_rate": 5.761749183211862e-06,
      "loss": 1.0444,
      "step": 16180
    },
    {
      "epoch": 4.048521228037266,
      "grad_norm": 2.0213613510131836,
      "learning_rate": 5.74667001759236e-06,
      "loss": 1.1063,
      "step": 16190
    },
    {
      "epoch": 4.051022322265991,
      "grad_norm": 3.081082344055176,
      "learning_rate": 5.731590851972857e-06,
      "loss": 1.0473,
      "step": 16200
    },
    {
      "epoch": 4.053523416494716,
      "grad_norm": 3.117910146713257,
      "learning_rate": 5.716511686353355e-06,
      "loss": 1.0718,
      "step": 16210
    },
    {
      "epoch": 4.056024510723441,
      "grad_norm": 2.6863632202148438,
      "learning_rate": 5.701432520733853e-06,
      "loss": 1.1183,
      "step": 16220
    },
    {
      "epoch": 4.058525604952167,
      "grad_norm": 2.4381723403930664,
      "learning_rate": 5.68635335511435e-06,
      "loss": 1.0851,
      "step": 16230
    },
    {
      "epoch": 4.061026699180892,
      "grad_norm": 1.9527156352996826,
      "learning_rate": 5.671274189494848e-06,
      "loss": 1.0717,
      "step": 16240
    },
    {
      "epoch": 4.063527793409617,
      "grad_norm": 2.1639301776885986,
      "learning_rate": 5.6561950238753454e-06,
      "loss": 1.0335,
      "step": 16250
    },
    {
      "epoch": 4.066028887638342,
      "grad_norm": 2.144110679626465,
      "learning_rate": 5.6411158582558436e-06,
      "loss": 1.0803,
      "step": 16260
    },
    {
      "epoch": 4.068529981867067,
      "grad_norm": 2.9187722206115723,
      "learning_rate": 5.626036692636341e-06,
      "loss": 1.0862,
      "step": 16270
    },
    {
      "epoch": 4.0710310760957915,
      "grad_norm": 2.473175048828125,
      "learning_rate": 5.610957527016839e-06,
      "loss": 1.1312,
      "step": 16280
    },
    {
      "epoch": 4.073532170324517,
      "grad_norm": 2.2387115955352783,
      "learning_rate": 5.595878361397336e-06,
      "loss": 1.0593,
      "step": 16290
    },
    {
      "epoch": 4.076033264553242,
      "grad_norm": 2.8275866508483887,
      "learning_rate": 5.580799195777834e-06,
      "loss": 1.0639,
      "step": 16300
    },
    {
      "epoch": 4.078534358781967,
      "grad_norm": 2.3698370456695557,
      "learning_rate": 5.565720030158332e-06,
      "loss": 0.9951,
      "step": 16310
    },
    {
      "epoch": 4.081035453010692,
      "grad_norm": 2.150954008102417,
      "learning_rate": 5.55064086453883e-06,
      "loss": 1.0923,
      "step": 16320
    },
    {
      "epoch": 4.083536547239417,
      "grad_norm": 2.3550543785095215,
      "learning_rate": 5.535561698919327e-06,
      "loss": 1.0487,
      "step": 16330
    },
    {
      "epoch": 4.086037641468143,
      "grad_norm": 2.748490333557129,
      "learning_rate": 5.520482533299824e-06,
      "loss": 1.033,
      "step": 16340
    },
    {
      "epoch": 4.088538735696868,
      "grad_norm": 1.99776291847229,
      "learning_rate": 5.5054033676803215e-06,
      "loss": 1.0862,
      "step": 16350
    },
    {
      "epoch": 4.091039829925593,
      "grad_norm": 2.2610228061676025,
      "learning_rate": 5.490324202060819e-06,
      "loss": 1.1567,
      "step": 16360
    },
    {
      "epoch": 4.0935409241543175,
      "grad_norm": 2.8272135257720947,
      "learning_rate": 5.475245036441317e-06,
      "loss": 1.0356,
      "step": 16370
    },
    {
      "epoch": 4.096042018383042,
      "grad_norm": 2.9000258445739746,
      "learning_rate": 5.460165870821814e-06,
      "loss": 1.0775,
      "step": 16380
    },
    {
      "epoch": 4.098543112611767,
      "grad_norm": 2.6626501083374023,
      "learning_rate": 5.445086705202312e-06,
      "loss": 1.0103,
      "step": 16390
    },
    {
      "epoch": 4.101044206840493,
      "grad_norm": 2.2921395301818848,
      "learning_rate": 5.4300075395828095e-06,
      "loss": 1.0703,
      "step": 16400
    },
    {
      "epoch": 4.103545301069218,
      "grad_norm": 1.9755771160125732,
      "learning_rate": 5.414928373963308e-06,
      "loss": 1.17,
      "step": 16410
    },
    {
      "epoch": 4.106046395297943,
      "grad_norm": 2.410989999771118,
      "learning_rate": 5.399849208343805e-06,
      "loss": 1.0502,
      "step": 16420
    },
    {
      "epoch": 4.108547489526668,
      "grad_norm": 2.620875120162964,
      "learning_rate": 5.384770042724303e-06,
      "loss": 1.1234,
      "step": 16430
    },
    {
      "epoch": 4.111048583755393,
      "grad_norm": 1.7908908128738403,
      "learning_rate": 5.3696908771048e-06,
      "loss": 1.0601,
      "step": 16440
    },
    {
      "epoch": 4.113549677984118,
      "grad_norm": 2.4411940574645996,
      "learning_rate": 5.354611711485298e-06,
      "loss": 1.0593,
      "step": 16450
    },
    {
      "epoch": 4.116050772212843,
      "grad_norm": 2.4048244953155518,
      "learning_rate": 5.339532545865796e-06,
      "loss": 1.0621,
      "step": 16460
    },
    {
      "epoch": 4.118551866441568,
      "grad_norm": 2.160931348800659,
      "learning_rate": 5.324453380246293e-06,
      "loss": 1.1373,
      "step": 16470
    },
    {
      "epoch": 4.121052960670293,
      "grad_norm": 2.4517459869384766,
      "learning_rate": 5.309374214626791e-06,
      "loss": 1.0606,
      "step": 16480
    },
    {
      "epoch": 4.123554054899018,
      "grad_norm": 2.0375020503997803,
      "learning_rate": 5.294295049007288e-06,
      "loss": 1.135,
      "step": 16490
    },
    {
      "epoch": 4.126055149127743,
      "grad_norm": 2.229677438735962,
      "learning_rate": 5.279215883387786e-06,
      "loss": 1.007,
      "step": 16500
    },
    {
      "epoch": 4.128556243356469,
      "grad_norm": 2.8763647079467773,
      "learning_rate": 5.264136717768284e-06,
      "loss": 1.0698,
      "step": 16510
    },
    {
      "epoch": 4.131057337585194,
      "grad_norm": 2.436729907989502,
      "learning_rate": 5.249057552148782e-06,
      "loss": 1.0358,
      "step": 16520
    },
    {
      "epoch": 4.133558431813919,
      "grad_norm": 2.4611268043518066,
      "learning_rate": 5.233978386529279e-06,
      "loss": 0.9977,
      "step": 16530
    },
    {
      "epoch": 4.136059526042644,
      "grad_norm": 1.9874701499938965,
      "learning_rate": 5.218899220909776e-06,
      "loss": 1.0881,
      "step": 16540
    },
    {
      "epoch": 4.1385606202713685,
      "grad_norm": 2.286139965057373,
      "learning_rate": 5.203820055290274e-06,
      "loss": 1.0775,
      "step": 16550
    },
    {
      "epoch": 4.141061714500093,
      "grad_norm": 2.5246903896331787,
      "learning_rate": 5.188740889670772e-06,
      "loss": 1.1332,
      "step": 16560
    },
    {
      "epoch": 4.143562808728819,
      "grad_norm": 2.0643303394317627,
      "learning_rate": 5.173661724051269e-06,
      "loss": 1.0626,
      "step": 16570
    },
    {
      "epoch": 4.146063902957544,
      "grad_norm": 2.187537431716919,
      "learning_rate": 5.158582558431767e-06,
      "loss": 1.1207,
      "step": 16580
    },
    {
      "epoch": 4.148564997186269,
      "grad_norm": 2.5647270679473877,
      "learning_rate": 5.143503392812264e-06,
      "loss": 1.0541,
      "step": 16590
    },
    {
      "epoch": 4.151066091414994,
      "grad_norm": 2.3963634967803955,
      "learning_rate": 5.128424227192762e-06,
      "loss": 1.0929,
      "step": 16600
    },
    {
      "epoch": 4.153567185643719,
      "grad_norm": 2.246852397918701,
      "learning_rate": 5.11334506157326e-06,
      "loss": 1.0331,
      "step": 16610
    },
    {
      "epoch": 4.156068279872445,
      "grad_norm": 2.7864363193511963,
      "learning_rate": 5.098265895953757e-06,
      "loss": 1.0395,
      "step": 16620
    },
    {
      "epoch": 4.15856937410117,
      "grad_norm": 2.205948829650879,
      "learning_rate": 5.083186730334255e-06,
      "loss": 1.0477,
      "step": 16630
    },
    {
      "epoch": 4.1610704683298945,
      "grad_norm": 2.112668514251709,
      "learning_rate": 5.068107564714752e-06,
      "loss": 1.1537,
      "step": 16640
    },
    {
      "epoch": 4.163571562558619,
      "grad_norm": 2.241490125656128,
      "learning_rate": 5.0530283990952505e-06,
      "loss": 1.0567,
      "step": 16650
    },
    {
      "epoch": 4.166072656787344,
      "grad_norm": 2.125511407852173,
      "learning_rate": 5.037949233475748e-06,
      "loss": 1.0343,
      "step": 16660
    },
    {
      "epoch": 4.168573751016069,
      "grad_norm": 2.3168418407440186,
      "learning_rate": 5.022870067856246e-06,
      "loss": 1.1417,
      "step": 16670
    },
    {
      "epoch": 4.171074845244795,
      "grad_norm": 2.0744335651397705,
      "learning_rate": 5.007790902236743e-06,
      "loss": 1.1002,
      "step": 16680
    },
    {
      "epoch": 4.17357593947352,
      "grad_norm": 2.5284886360168457,
      "learning_rate": 4.992711736617241e-06,
      "loss": 1.0489,
      "step": 16690
    },
    {
      "epoch": 4.176077033702245,
      "grad_norm": 2.1139888763427734,
      "learning_rate": 4.9776325709977385e-06,
      "loss": 1.0901,
      "step": 16700
    },
    {
      "epoch": 4.17857812793097,
      "grad_norm": 1.8483554124832153,
      "learning_rate": 4.962553405378237e-06,
      "loss": 1.0129,
      "step": 16710
    },
    {
      "epoch": 4.181079222159695,
      "grad_norm": 2.2092032432556152,
      "learning_rate": 4.947474239758734e-06,
      "loss": 1.0406,
      "step": 16720
    },
    {
      "epoch": 4.1835803163884195,
      "grad_norm": 2.3123886585235596,
      "learning_rate": 4.93239507413923e-06,
      "loss": 1.0732,
      "step": 16730
    },
    {
      "epoch": 4.186081410617145,
      "grad_norm": 2.205432891845703,
      "learning_rate": 4.9173159085197284e-06,
      "loss": 1.0748,
      "step": 16740
    },
    {
      "epoch": 4.18858250484587,
      "grad_norm": 2.396505355834961,
      "learning_rate": 4.902236742900226e-06,
      "loss": 1.0936,
      "step": 16750
    },
    {
      "epoch": 4.191083599074595,
      "grad_norm": 2.225087881088257,
      "learning_rate": 4.887157577280724e-06,
      "loss": 1.1585,
      "step": 16760
    },
    {
      "epoch": 4.19358469330332,
      "grad_norm": 2.5923168659210205,
      "learning_rate": 4.872078411661221e-06,
      "loss": 1.06,
      "step": 16770
    },
    {
      "epoch": 4.196085787532045,
      "grad_norm": 2.3918707370758057,
      "learning_rate": 4.856999246041719e-06,
      "loss": 1.052,
      "step": 16780
    },
    {
      "epoch": 4.19858688176077,
      "grad_norm": 1.9463801383972168,
      "learning_rate": 4.8419200804222165e-06,
      "loss": 1.056,
      "step": 16790
    },
    {
      "epoch": 4.201087975989496,
      "grad_norm": 2.0347046852111816,
      "learning_rate": 4.8268409148027146e-06,
      "loss": 0.9725,
      "step": 16800
    },
    {
      "epoch": 4.203589070218221,
      "grad_norm": 2.6444787979125977,
      "learning_rate": 4.811761749183212e-06,
      "loss": 1.0465,
      "step": 16810
    },
    {
      "epoch": 4.2060901644469455,
      "grad_norm": 3.5239057540893555,
      "learning_rate": 4.79668258356371e-06,
      "loss": 1.0573,
      "step": 16820
    },
    {
      "epoch": 4.20859125867567,
      "grad_norm": 2.5182104110717773,
      "learning_rate": 4.781603417944207e-06,
      "loss": 1.047,
      "step": 16830
    },
    {
      "epoch": 4.211092352904395,
      "grad_norm": 2.996029853820801,
      "learning_rate": 4.766524252324705e-06,
      "loss": 1.0933,
      "step": 16840
    },
    {
      "epoch": 4.213593447133121,
      "grad_norm": 2.353506326675415,
      "learning_rate": 4.751445086705203e-06,
      "loss": 1.0659,
      "step": 16850
    },
    {
      "epoch": 4.216094541361846,
      "grad_norm": 2.070042610168457,
      "learning_rate": 4.7363659210857e-06,
      "loss": 1.0745,
      "step": 16860
    },
    {
      "epoch": 4.218595635590571,
      "grad_norm": 1.9406676292419434,
      "learning_rate": 4.721286755466198e-06,
      "loss": 1.1315,
      "step": 16870
    },
    {
      "epoch": 4.221096729819296,
      "grad_norm": 2.2628610134124756,
      "learning_rate": 4.706207589846695e-06,
      "loss": 1.0464,
      "step": 16880
    },
    {
      "epoch": 4.223597824048021,
      "grad_norm": 2.543203592300415,
      "learning_rate": 4.691128424227193e-06,
      "loss": 1.0445,
      "step": 16890
    },
    {
      "epoch": 4.226098918276746,
      "grad_norm": 2.4506938457489014,
      "learning_rate": 4.676049258607691e-06,
      "loss": 1.0673,
      "step": 16900
    },
    {
      "epoch": 4.2286000125054715,
      "grad_norm": 1.977481484413147,
      "learning_rate": 4.660970092988189e-06,
      "loss": 1.1133,
      "step": 16910
    },
    {
      "epoch": 4.231101106734196,
      "grad_norm": 2.4426510334014893,
      "learning_rate": 4.645890927368685e-06,
      "loss": 1.0479,
      "step": 16920
    },
    {
      "epoch": 4.233602200962921,
      "grad_norm": 2.3878514766693115,
      "learning_rate": 4.630811761749183e-06,
      "loss": 1.066,
      "step": 16930
    },
    {
      "epoch": 4.236103295191646,
      "grad_norm": 2.2681753635406494,
      "learning_rate": 4.6157325961296805e-06,
      "loss": 1.0747,
      "step": 16940
    },
    {
      "epoch": 4.238604389420371,
      "grad_norm": 2.1851415634155273,
      "learning_rate": 4.600653430510179e-06,
      "loss": 1.1031,
      "step": 16950
    },
    {
      "epoch": 4.241105483649097,
      "grad_norm": 2.4227709770202637,
      "learning_rate": 4.585574264890676e-06,
      "loss": 1.0501,
      "step": 16960
    },
    {
      "epoch": 4.243606577877822,
      "grad_norm": 1.9513914585113525,
      "learning_rate": 4.570495099271173e-06,
      "loss": 1.1118,
      "step": 16970
    },
    {
      "epoch": 4.246107672106547,
      "grad_norm": 2.224426031112671,
      "learning_rate": 4.555415933651671e-06,
      "loss": 1.0776,
      "step": 16980
    },
    {
      "epoch": 4.248608766335272,
      "grad_norm": 2.1456124782562256,
      "learning_rate": 4.5403367680321686e-06,
      "loss": 1.0735,
      "step": 16990
    },
    {
      "epoch": 4.2511098605639965,
      "grad_norm": 2.702583074569702,
      "learning_rate": 4.525257602412667e-06,
      "loss": 1.0166,
      "step": 17000
    },
    {
      "epoch": 4.253610954792721,
      "grad_norm": 2.516941785812378,
      "learning_rate": 4.510178436793164e-06,
      "loss": 1.0827,
      "step": 17010
    },
    {
      "epoch": 4.256112049021447,
      "grad_norm": 2.818624973297119,
      "learning_rate": 4.495099271173662e-06,
      "loss": 1.097,
      "step": 17020
    },
    {
      "epoch": 4.258613143250172,
      "grad_norm": 3.2991700172424316,
      "learning_rate": 4.480020105554159e-06,
      "loss": 1.0553,
      "step": 17030
    },
    {
      "epoch": 4.261114237478897,
      "grad_norm": 2.5693602561950684,
      "learning_rate": 4.464940939934657e-06,
      "loss": 1.0389,
      "step": 17040
    },
    {
      "epoch": 4.263615331707622,
      "grad_norm": 2.1339004039764404,
      "learning_rate": 4.449861774315155e-06,
      "loss": 0.9595,
      "step": 17050
    },
    {
      "epoch": 4.266116425936347,
      "grad_norm": 2.098165273666382,
      "learning_rate": 4.434782608695653e-06,
      "loss": 1.0041,
      "step": 17060
    },
    {
      "epoch": 4.268617520165073,
      "grad_norm": 3.0003793239593506,
      "learning_rate": 4.41970344307615e-06,
      "loss": 0.9881,
      "step": 17070
    },
    {
      "epoch": 4.271118614393798,
      "grad_norm": 2.3132684230804443,
      "learning_rate": 4.404624277456648e-06,
      "loss": 1.0745,
      "step": 17080
    },
    {
      "epoch": 4.2736197086225225,
      "grad_norm": 2.370713233947754,
      "learning_rate": 4.3895451118371454e-06,
      "loss": 1.0347,
      "step": 17090
    },
    {
      "epoch": 4.276120802851247,
      "grad_norm": 3.068657159805298,
      "learning_rate": 4.374465946217643e-06,
      "loss": 1.0411,
      "step": 17100
    },
    {
      "epoch": 4.278621897079972,
      "grad_norm": 2.3773605823516846,
      "learning_rate": 4.35938678059814e-06,
      "loss": 1.0291,
      "step": 17110
    },
    {
      "epoch": 4.281122991308697,
      "grad_norm": 2.0198793411254883,
      "learning_rate": 4.344307614978637e-06,
      "loss": 1.0632,
      "step": 17120
    },
    {
      "epoch": 4.283624085537423,
      "grad_norm": 2.7428953647613525,
      "learning_rate": 4.329228449359135e-06,
      "loss": 1.134,
      "step": 17130
    },
    {
      "epoch": 4.286125179766148,
      "grad_norm": 2.465183734893799,
      "learning_rate": 4.314149283739633e-06,
      "loss": 1.0326,
      "step": 17140
    },
    {
      "epoch": 4.288626273994873,
      "grad_norm": 2.40671706199646,
      "learning_rate": 4.299070118120131e-06,
      "loss": 1.0844,
      "step": 17150
    },
    {
      "epoch": 4.291127368223598,
      "grad_norm": 1.9846936464309692,
      "learning_rate": 4.283990952500628e-06,
      "loss": 1.0113,
      "step": 17160
    },
    {
      "epoch": 4.293628462452323,
      "grad_norm": 1.7320200204849243,
      "learning_rate": 4.268911786881126e-06,
      "loss": 1.0761,
      "step": 17170
    },
    {
      "epoch": 4.296129556681048,
      "grad_norm": 2.188795566558838,
      "learning_rate": 4.253832621261623e-06,
      "loss": 1.0775,
      "step": 17180
    },
    {
      "epoch": 4.298630650909773,
      "grad_norm": 1.8516396284103394,
      "learning_rate": 4.2387534556421215e-06,
      "loss": 1.0297,
      "step": 17190
    },
    {
      "epoch": 4.301131745138498,
      "grad_norm": 2.237399101257324,
      "learning_rate": 4.223674290022619e-06,
      "loss": 1.0657,
      "step": 17200
    },
    {
      "epoch": 4.303632839367223,
      "grad_norm": 2.264719247817993,
      "learning_rate": 4.208595124403117e-06,
      "loss": 1.0707,
      "step": 17210
    },
    {
      "epoch": 4.306133933595948,
      "grad_norm": 2.3211209774017334,
      "learning_rate": 4.193515958783614e-06,
      "loss": 1.0684,
      "step": 17220
    },
    {
      "epoch": 4.308635027824673,
      "grad_norm": 2.476893901824951,
      "learning_rate": 4.178436793164111e-06,
      "loss": 1.1529,
      "step": 17230
    },
    {
      "epoch": 4.311136122053398,
      "grad_norm": 2.054311513900757,
      "learning_rate": 4.1633576275446095e-06,
      "loss": 1.0371,
      "step": 17240
    },
    {
      "epoch": 4.313637216282124,
      "grad_norm": 2.6825292110443115,
      "learning_rate": 4.148278461925107e-06,
      "loss": 1.0771,
      "step": 17250
    },
    {
      "epoch": 4.316138310510849,
      "grad_norm": 2.3067681789398193,
      "learning_rate": 4.133199296305605e-06,
      "loss": 1.0844,
      "step": 17260
    },
    {
      "epoch": 4.3186394047395735,
      "grad_norm": 3.5225889682769775,
      "learning_rate": 4.118120130686102e-06,
      "loss": 1.0721,
      "step": 17270
    },
    {
      "epoch": 4.321140498968298,
      "grad_norm": 2.494898796081543,
      "learning_rate": 4.1030409650666e-06,
      "loss": 1.113,
      "step": 17280
    },
    {
      "epoch": 4.323641593197023,
      "grad_norm": 2.593374013900757,
      "learning_rate": 4.0879617994470975e-06,
      "loss": 1.0717,
      "step": 17290
    },
    {
      "epoch": 4.326142687425749,
      "grad_norm": 2.0390052795410156,
      "learning_rate": 4.072882633827595e-06,
      "loss": 1.0635,
      "step": 17300
    },
    {
      "epoch": 4.328643781654474,
      "grad_norm": 2.573124408721924,
      "learning_rate": 4.057803468208092e-06,
      "loss": 1.0739,
      "step": 17310
    },
    {
      "epoch": 4.331144875883199,
      "grad_norm": 2.518481731414795,
      "learning_rate": 4.04272430258859e-06,
      "loss": 1.1001,
      "step": 17320
    },
    {
      "epoch": 4.333645970111924,
      "grad_norm": 2.4987783432006836,
      "learning_rate": 4.0276451369690875e-06,
      "loss": 1.0891,
      "step": 17330
    },
    {
      "epoch": 4.336147064340649,
      "grad_norm": 3.200495719909668,
      "learning_rate": 4.0125659713495856e-06,
      "loss": 1.0232,
      "step": 17340
    },
    {
      "epoch": 4.338648158569374,
      "grad_norm": 2.1781082153320312,
      "learning_rate": 3.997486805730083e-06,
      "loss": 1.0838,
      "step": 17350
    },
    {
      "epoch": 4.3411492527980995,
      "grad_norm": 2.4219281673431396,
      "learning_rate": 3.98240764011058e-06,
      "loss": 1.083,
      "step": 17360
    },
    {
      "epoch": 4.343650347026824,
      "grad_norm": 2.0137906074523926,
      "learning_rate": 3.967328474491078e-06,
      "loss": 1.0862,
      "step": 17370
    },
    {
      "epoch": 4.346151441255549,
      "grad_norm": 2.134868860244751,
      "learning_rate": 3.9522493088715755e-06,
      "loss": 1.0479,
      "step": 17380
    },
    {
      "epoch": 4.348652535484274,
      "grad_norm": 2.002006769180298,
      "learning_rate": 3.937170143252074e-06,
      "loss": 1.0433,
      "step": 17390
    },
    {
      "epoch": 4.351153629712999,
      "grad_norm": 2.9054441452026367,
      "learning_rate": 3.922090977632571e-06,
      "loss": 1.1173,
      "step": 17400
    },
    {
      "epoch": 4.353654723941725,
      "grad_norm": 2.4003283977508545,
      "learning_rate": 3.907011812013069e-06,
      "loss": 1.0679,
      "step": 17410
    },
    {
      "epoch": 4.35615581817045,
      "grad_norm": 1.9817371368408203,
      "learning_rate": 3.891932646393566e-06,
      "loss": 1.0902,
      "step": 17420
    },
    {
      "epoch": 4.358656912399175,
      "grad_norm": 2.7988593578338623,
      "learning_rate": 3.876853480774064e-06,
      "loss": 1.089,
      "step": 17430
    },
    {
      "epoch": 4.3611580066279,
      "grad_norm": 2.2961628437042236,
      "learning_rate": 3.861774315154562e-06,
      "loss": 1.0359,
      "step": 17440
    },
    {
      "epoch": 4.363659100856625,
      "grad_norm": 2.555039405822754,
      "learning_rate": 3.84669514953506e-06,
      "loss": 1.1333,
      "step": 17450
    },
    {
      "epoch": 4.3661601950853495,
      "grad_norm": 2.2625036239624023,
      "learning_rate": 3.831615983915557e-06,
      "loss": 1.0745,
      "step": 17460
    },
    {
      "epoch": 4.368661289314075,
      "grad_norm": 2.5591464042663574,
      "learning_rate": 3.816536818296055e-06,
      "loss": 1.1107,
      "step": 17470
    },
    {
      "epoch": 4.3711623835428,
      "grad_norm": 2.786799192428589,
      "learning_rate": 3.8014576526765524e-06,
      "loss": 1.1531,
      "step": 17480
    },
    {
      "epoch": 4.373663477771525,
      "grad_norm": 2.5577735900878906,
      "learning_rate": 3.7863784870570492e-06,
      "loss": 1.0208,
      "step": 17490
    },
    {
      "epoch": 4.37616457200025,
      "grad_norm": 2.188462495803833,
      "learning_rate": 3.771299321437547e-06,
      "loss": 1.0354,
      "step": 17500
    },
    {
      "epoch": 4.378665666228975,
      "grad_norm": 2.5405797958374023,
      "learning_rate": 3.7562201558180446e-06,
      "loss": 1.0268,
      "step": 17510
    },
    {
      "epoch": 4.381166760457701,
      "grad_norm": 2.53383469581604,
      "learning_rate": 3.7411409901985427e-06,
      "loss": 1.0602,
      "step": 17520
    },
    {
      "epoch": 4.383667854686426,
      "grad_norm": 2.6253788471221924,
      "learning_rate": 3.7260618245790404e-06,
      "loss": 1.1151,
      "step": 17530
    },
    {
      "epoch": 4.3861689489151505,
      "grad_norm": 2.817923069000244,
      "learning_rate": 3.7109826589595377e-06,
      "loss": 1.0214,
      "step": 17540
    },
    {
      "epoch": 4.388670043143875,
      "grad_norm": 2.058468818664551,
      "learning_rate": 3.695903493340035e-06,
      "loss": 1.0437,
      "step": 17550
    },
    {
      "epoch": 4.3911711373726,
      "grad_norm": 2.1721622943878174,
      "learning_rate": 3.6808243277205326e-06,
      "loss": 1.0228,
      "step": 17560
    },
    {
      "epoch": 4.393672231601325,
      "grad_norm": 2.003875732421875,
      "learning_rate": 3.6657451621010303e-06,
      "loss": 1.1086,
      "step": 17570
    },
    {
      "epoch": 4.396173325830051,
      "grad_norm": 2.0200209617614746,
      "learning_rate": 3.650665996481528e-06,
      "loss": 1.1097,
      "step": 17580
    },
    {
      "epoch": 4.398674420058776,
      "grad_norm": 2.6046996116638184,
      "learning_rate": 3.6355868308620257e-06,
      "loss": 1.0849,
      "step": 17590
    },
    {
      "epoch": 4.401175514287501,
      "grad_norm": 2.3428077697753906,
      "learning_rate": 3.6205076652425234e-06,
      "loss": 1.076,
      "step": 17600
    },
    {
      "epoch": 4.403676608516226,
      "grad_norm": 2.338423490524292,
      "learning_rate": 3.605428499623021e-06,
      "loss": 1.1095,
      "step": 17610
    },
    {
      "epoch": 4.406177702744951,
      "grad_norm": 2.1124675273895264,
      "learning_rate": 3.5903493340035188e-06,
      "loss": 1.0113,
      "step": 17620
    },
    {
      "epoch": 4.408678796973676,
      "grad_norm": 3.3443291187286377,
      "learning_rate": 3.5752701683840165e-06,
      "loss": 1.1057,
      "step": 17630
    },
    {
      "epoch": 4.411179891202401,
      "grad_norm": 3.3041059970855713,
      "learning_rate": 3.5601910027645137e-06,
      "loss": 1.0838,
      "step": 17640
    },
    {
      "epoch": 4.413680985431126,
      "grad_norm": 2.344520330429077,
      "learning_rate": 3.5451118371450114e-06,
      "loss": 1.0721,
      "step": 17650
    },
    {
      "epoch": 4.416182079659851,
      "grad_norm": 2.3420662879943848,
      "learning_rate": 3.530032671525509e-06,
      "loss": 1.0786,
      "step": 17660
    },
    {
      "epoch": 4.418683173888576,
      "grad_norm": 2.251006841659546,
      "learning_rate": 3.514953505906007e-06,
      "loss": 1.0616,
      "step": 17670
    },
    {
      "epoch": 4.421184268117301,
      "grad_norm": 2.2272815704345703,
      "learning_rate": 3.499874340286504e-06,
      "loss": 1.0512,
      "step": 17680
    },
    {
      "epoch": 4.423685362346026,
      "grad_norm": 2.8584365844726562,
      "learning_rate": 3.4847951746670017e-06,
      "loss": 1.0791,
      "step": 17690
    },
    {
      "epoch": 4.426186456574752,
      "grad_norm": 2.2885921001434326,
      "learning_rate": 3.4697160090474994e-06,
      "loss": 1.1191,
      "step": 17700
    },
    {
      "epoch": 4.428687550803477,
      "grad_norm": 2.329770088195801,
      "learning_rate": 3.454636843427997e-06,
      "loss": 1.083,
      "step": 17710
    },
    {
      "epoch": 4.431188645032202,
      "grad_norm": 2.6192312240600586,
      "learning_rate": 3.439557677808495e-06,
      "loss": 1.0517,
      "step": 17720
    },
    {
      "epoch": 4.4336897392609265,
      "grad_norm": 1.9460150003433228,
      "learning_rate": 3.424478512188992e-06,
      "loss": 1.106,
      "step": 17730
    },
    {
      "epoch": 4.436190833489651,
      "grad_norm": 2.9891819953918457,
      "learning_rate": 3.4093993465694898e-06,
      "loss": 1.0812,
      "step": 17740
    },
    {
      "epoch": 4.438691927718377,
      "grad_norm": 2.2873096466064453,
      "learning_rate": 3.3943201809499875e-06,
      "loss": 1.046,
      "step": 17750
    },
    {
      "epoch": 4.441193021947102,
      "grad_norm": 2.5815603733062744,
      "learning_rate": 3.379241015330485e-06,
      "loss": 0.9926,
      "step": 17760
    },
    {
      "epoch": 4.443694116175827,
      "grad_norm": 1.8257235288619995,
      "learning_rate": 3.364161849710983e-06,
      "loss": 1.0783,
      "step": 17770
    },
    {
      "epoch": 4.446195210404552,
      "grad_norm": 2.6633810997009277,
      "learning_rate": 3.3490826840914805e-06,
      "loss": 1.0565,
      "step": 17780
    },
    {
      "epoch": 4.448696304633277,
      "grad_norm": 2.2466301918029785,
      "learning_rate": 3.3340035184719782e-06,
      "loss": 1.0491,
      "step": 17790
    },
    {
      "epoch": 4.451197398862002,
      "grad_norm": 2.286362409591675,
      "learning_rate": 3.3189243528524755e-06,
      "loss": 1.0662,
      "step": 17800
    },
    {
      "epoch": 4.4536984930907275,
      "grad_norm": 3.0262489318847656,
      "learning_rate": 3.303845187232973e-06,
      "loss": 1.0677,
      "step": 17810
    },
    {
      "epoch": 4.456199587319452,
      "grad_norm": 2.0200259685516357,
      "learning_rate": 3.288766021613471e-06,
      "loss": 1.0488,
      "step": 17820
    },
    {
      "epoch": 4.458700681548177,
      "grad_norm": 2.8927738666534424,
      "learning_rate": 3.273686855993968e-06,
      "loss": 1.0684,
      "step": 17830
    },
    {
      "epoch": 4.461201775776902,
      "grad_norm": 2.1184072494506836,
      "learning_rate": 3.258607690374466e-06,
      "loss": 1.1224,
      "step": 17840
    },
    {
      "epoch": 4.463702870005627,
      "grad_norm": 2.448745012283325,
      "learning_rate": 3.2435285247549635e-06,
      "loss": 1.114,
      "step": 17850
    },
    {
      "epoch": 4.466203964234353,
      "grad_norm": 2.534379720687866,
      "learning_rate": 3.228449359135461e-06,
      "loss": 1.0922,
      "step": 17860
    },
    {
      "epoch": 4.468705058463078,
      "grad_norm": 2.5909721851348877,
      "learning_rate": 3.213370193515959e-06,
      "loss": 1.0874,
      "step": 17870
    },
    {
      "epoch": 4.471206152691803,
      "grad_norm": 2.5357418060302734,
      "learning_rate": 3.1982910278964566e-06,
      "loss": 1.1062,
      "step": 17880
    },
    {
      "epoch": 4.473707246920528,
      "grad_norm": 2.4085943698883057,
      "learning_rate": 3.1832118622769543e-06,
      "loss": 1.1022,
      "step": 17890
    },
    {
      "epoch": 4.476208341149253,
      "grad_norm": 1.8492077589035034,
      "learning_rate": 3.168132696657452e-06,
      "loss": 1.0367,
      "step": 17900
    },
    {
      "epoch": 4.4787094353779775,
      "grad_norm": 2.099982976913452,
      "learning_rate": 3.1530535310379496e-06,
      "loss": 1.0225,
      "step": 17910
    },
    {
      "epoch": 4.481210529606703,
      "grad_norm": 2.229566812515259,
      "learning_rate": 3.137974365418447e-06,
      "loss": 1.1453,
      "step": 17920
    },
    {
      "epoch": 4.483711623835428,
      "grad_norm": 2.283355712890625,
      "learning_rate": 3.122895199798944e-06,
      "loss": 1.1498,
      "step": 17930
    },
    {
      "epoch": 4.486212718064153,
      "grad_norm": 2.6397695541381836,
      "learning_rate": 3.107816034179442e-06,
      "loss": 1.0794,
      "step": 17940
    },
    {
      "epoch": 4.488713812292878,
      "grad_norm": 2.1577532291412354,
      "learning_rate": 3.0927368685599396e-06,
      "loss": 1.0631,
      "step": 17950
    },
    {
      "epoch": 4.491214906521603,
      "grad_norm": 2.6356987953186035,
      "learning_rate": 3.0776577029404372e-06,
      "loss": 1.0675,
      "step": 17960
    },
    {
      "epoch": 4.493716000750329,
      "grad_norm": 2.3047373294830322,
      "learning_rate": 3.062578537320935e-06,
      "loss": 1.1029,
      "step": 17970
    },
    {
      "epoch": 4.496217094979054,
      "grad_norm": 1.9401466846466064,
      "learning_rate": 3.0474993717014326e-06,
      "loss": 1.0992,
      "step": 17980
    },
    {
      "epoch": 4.4987181892077786,
      "grad_norm": 2.0853958129882812,
      "learning_rate": 3.0324202060819303e-06,
      "loss": 0.9914,
      "step": 17990
    },
    {
      "epoch": 4.5012192834365035,
      "grad_norm": 1.991326093673706,
      "learning_rate": 3.017341040462428e-06,
      "loss": 0.991,
      "step": 18000
    },
    {
      "epoch": 4.503720377665228,
      "grad_norm": 2.6903111934661865,
      "learning_rate": 3.0022618748429257e-06,
      "loss": 1.0841,
      "step": 18010
    },
    {
      "epoch": 4.506221471893953,
      "grad_norm": 2.232105255126953,
      "learning_rate": 2.987182709223423e-06,
      "loss": 1.0756,
      "step": 18020
    },
    {
      "epoch": 4.508722566122678,
      "grad_norm": 2.4787771701812744,
      "learning_rate": 2.9721035436039207e-06,
      "loss": 1.1061,
      "step": 18030
    },
    {
      "epoch": 4.511223660351404,
      "grad_norm": 3.0485360622406006,
      "learning_rate": 2.9570243779844183e-06,
      "loss": 1.0329,
      "step": 18040
    },
    {
      "epoch": 4.513724754580129,
      "grad_norm": 2.073918104171753,
      "learning_rate": 2.941945212364916e-06,
      "loss": 1.1097,
      "step": 18050
    },
    {
      "epoch": 4.516225848808854,
      "grad_norm": 2.1559901237487793,
      "learning_rate": 2.9268660467454133e-06,
      "loss": 1.0587,
      "step": 18060
    },
    {
      "epoch": 4.518726943037579,
      "grad_norm": 1.992977261543274,
      "learning_rate": 2.911786881125911e-06,
      "loss": 1.0837,
      "step": 18070
    },
    {
      "epoch": 4.521228037266304,
      "grad_norm": 2.489062786102295,
      "learning_rate": 2.8967077155064087e-06,
      "loss": 1.2252,
      "step": 18080
    },
    {
      "epoch": 4.523729131495029,
      "grad_norm": 2.3105947971343994,
      "learning_rate": 2.8816285498869064e-06,
      "loss": 1.0834,
      "step": 18090
    },
    {
      "epoch": 4.526230225723754,
      "grad_norm": 2.236448287963867,
      "learning_rate": 2.866549384267404e-06,
      "loss": 0.9883,
      "step": 18100
    },
    {
      "epoch": 4.528731319952479,
      "grad_norm": 2.1228904724121094,
      "learning_rate": 2.8514702186479013e-06,
      "loss": 1.1094,
      "step": 18110
    },
    {
      "epoch": 4.531232414181204,
      "grad_norm": 2.302657127380371,
      "learning_rate": 2.836391053028399e-06,
      "loss": 1.0419,
      "step": 18120
    },
    {
      "epoch": 4.533733508409929,
      "grad_norm": 2.3904869556427,
      "learning_rate": 2.8213118874088967e-06,
      "loss": 1.1528,
      "step": 18130
    },
    {
      "epoch": 4.536234602638654,
      "grad_norm": 2.3123652935028076,
      "learning_rate": 2.8062327217893944e-06,
      "loss": 1.0632,
      "step": 18140
    },
    {
      "epoch": 4.53873569686738,
      "grad_norm": 2.2026467323303223,
      "learning_rate": 2.791153556169892e-06,
      "loss": 1.0503,
      "step": 18150
    },
    {
      "epoch": 4.541236791096105,
      "grad_norm": 2.473484754562378,
      "learning_rate": 2.7760743905503898e-06,
      "loss": 1.0363,
      "step": 18160
    },
    {
      "epoch": 4.54373788532483,
      "grad_norm": 2.371628522872925,
      "learning_rate": 2.7609952249308875e-06,
      "loss": 1.0117,
      "step": 18170
    },
    {
      "epoch": 4.5462389795535545,
      "grad_norm": 2.4779367446899414,
      "learning_rate": 2.745916059311385e-06,
      "loss": 1.0254,
      "step": 18180
    },
    {
      "epoch": 4.548740073782279,
      "grad_norm": 2.6114251613616943,
      "learning_rate": 2.7308368936918824e-06,
      "loss": 1.0585,
      "step": 18190
    },
    {
      "epoch": 4.551241168011005,
      "grad_norm": 2.245309352874756,
      "learning_rate": 2.71575772807238e-06,
      "loss": 1.0487,
      "step": 18200
    },
    {
      "epoch": 4.55374226223973,
      "grad_norm": 2.2193405628204346,
      "learning_rate": 2.702186479014828e-06,
      "loss": 1.1269,
      "step": 18210
    },
    {
      "epoch": 4.556243356468455,
      "grad_norm": 2.276211977005005,
      "learning_rate": 2.6871073133953258e-06,
      "loss": 1.1537,
      "step": 18220
    },
    {
      "epoch": 4.55874445069718,
      "grad_norm": 2.106236696243286,
      "learning_rate": 2.672028147775823e-06,
      "loss": 1.0723,
      "step": 18230
    },
    {
      "epoch": 4.561245544925905,
      "grad_norm": 2.2168569564819336,
      "learning_rate": 2.6569489821563207e-06,
      "loss": 1.1505,
      "step": 18240
    },
    {
      "epoch": 4.56374663915463,
      "grad_norm": 2.123943567276001,
      "learning_rate": 2.6418698165368184e-06,
      "loss": 1.0707,
      "step": 18250
    },
    {
      "epoch": 4.5662477333833555,
      "grad_norm": 2.806464672088623,
      "learning_rate": 2.6267906509173157e-06,
      "loss": 1.0663,
      "step": 18260
    },
    {
      "epoch": 4.5687488276120805,
      "grad_norm": 2.808654546737671,
      "learning_rate": 2.6117114852978134e-06,
      "loss": 0.9925,
      "step": 18270
    },
    {
      "epoch": 4.571249921840805,
      "grad_norm": 2.8177032470703125,
      "learning_rate": 2.596632319678311e-06,
      "loss": 1.0975,
      "step": 18280
    },
    {
      "epoch": 4.57375101606953,
      "grad_norm": 1.8769021034240723,
      "learning_rate": 2.5815531540588087e-06,
      "loss": 1.0196,
      "step": 18290
    },
    {
      "epoch": 4.576252110298255,
      "grad_norm": 1.8005939722061157,
      "learning_rate": 2.5664739884393064e-06,
      "loss": 1.0334,
      "step": 18300
    },
    {
      "epoch": 4.578753204526981,
      "grad_norm": 2.385101318359375,
      "learning_rate": 2.551394822819804e-06,
      "loss": 1.0493,
      "step": 18310
    },
    {
      "epoch": 4.581254298755706,
      "grad_norm": 2.362931966781616,
      "learning_rate": 2.536315657200302e-06,
      "loss": 1.0915,
      "step": 18320
    },
    {
      "epoch": 4.583755392984431,
      "grad_norm": 2.1777894496917725,
      "learning_rate": 2.5212364915807995e-06,
      "loss": 1.1008,
      "step": 18330
    },
    {
      "epoch": 4.586256487213156,
      "grad_norm": 2.6675381660461426,
      "learning_rate": 2.506157325961297e-06,
      "loss": 1.0154,
      "step": 18340
    },
    {
      "epoch": 4.588757581441881,
      "grad_norm": 2.6342782974243164,
      "learning_rate": 2.491078160341795e-06,
      "loss": 1.0394,
      "step": 18350
    },
    {
      "epoch": 4.5912586756706055,
      "grad_norm": 2.3292346000671387,
      "learning_rate": 2.4759989947222917e-06,
      "loss": 1.0327,
      "step": 18360
    },
    {
      "epoch": 4.593759769899331,
      "grad_norm": 2.1357920169830322,
      "learning_rate": 2.4609198291027894e-06,
      "loss": 1.1022,
      "step": 18370
    },
    {
      "epoch": 4.596260864128056,
      "grad_norm": 2.5731194019317627,
      "learning_rate": 2.445840663483287e-06,
      "loss": 1.1051,
      "step": 18380
    },
    {
      "epoch": 4.598761958356781,
      "grad_norm": 2.2859432697296143,
      "learning_rate": 2.430761497863785e-06,
      "loss": 1.1362,
      "step": 18390
    },
    {
      "epoch": 4.601263052585506,
      "grad_norm": 2.173241376876831,
      "learning_rate": 2.4156823322442825e-06,
      "loss": 1.0217,
      "step": 18400
    },
    {
      "epoch": 4.603764146814231,
      "grad_norm": 2.7704262733459473,
      "learning_rate": 2.40060316662478e-06,
      "loss": 1.0947,
      "step": 18410
    },
    {
      "epoch": 4.606265241042957,
      "grad_norm": 2.239993095397949,
      "learning_rate": 2.385524001005278e-06,
      "loss": 1.107,
      "step": 18420
    },
    {
      "epoch": 4.608766335271682,
      "grad_norm": 2.525461435317993,
      "learning_rate": 2.3704448353857755e-06,
      "loss": 0.987,
      "step": 18430
    },
    {
      "epoch": 4.611267429500407,
      "grad_norm": 2.273228406906128,
      "learning_rate": 2.3553656697662732e-06,
      "loss": 1.0645,
      "step": 18440
    },
    {
      "epoch": 4.6137685237291315,
      "grad_norm": 2.5047495365142822,
      "learning_rate": 2.340286504146771e-06,
      "loss": 1.0798,
      "step": 18450
    },
    {
      "epoch": 4.616269617957856,
      "grad_norm": 2.234694480895996,
      "learning_rate": 2.325207338527268e-06,
      "loss": 1.1951,
      "step": 18460
    },
    {
      "epoch": 4.618770712186581,
      "grad_norm": 2.2627651691436768,
      "learning_rate": 2.310128172907766e-06,
      "loss": 1.085,
      "step": 18470
    },
    {
      "epoch": 4.621271806415306,
      "grad_norm": 2.430793046951294,
      "learning_rate": 2.295049007288263e-06,
      "loss": 1.0703,
      "step": 18480
    },
    {
      "epoch": 4.623772900644032,
      "grad_norm": 2.468782901763916,
      "learning_rate": 2.279969841668761e-06,
      "loss": 1.0721,
      "step": 18490
    },
    {
      "epoch": 4.626273994872757,
      "grad_norm": 2.365407943725586,
      "learning_rate": 2.2648906760492585e-06,
      "loss": 1.0909,
      "step": 18500
    },
    {
      "epoch": 4.628775089101482,
      "grad_norm": 2.0739097595214844,
      "learning_rate": 2.2498115104297562e-06,
      "loss": 1.1309,
      "step": 18510
    },
    {
      "epoch": 4.631276183330207,
      "grad_norm": 2.165555953979492,
      "learning_rate": 2.234732344810254e-06,
      "loss": 1.0554,
      "step": 18520
    },
    {
      "epoch": 4.633777277558932,
      "grad_norm": 2.3899035453796387,
      "learning_rate": 2.2196531791907516e-06,
      "loss": 1.0812,
      "step": 18530
    },
    {
      "epoch": 4.6362783717876574,
      "grad_norm": 2.3365609645843506,
      "learning_rate": 2.2045740135712493e-06,
      "loss": 1.0939,
      "step": 18540
    },
    {
      "epoch": 4.638779466016382,
      "grad_norm": 2.553882598876953,
      "learning_rate": 2.1894948479517466e-06,
      "loss": 1.1537,
      "step": 18550
    },
    {
      "epoch": 4.641280560245107,
      "grad_norm": 2.946944236755371,
      "learning_rate": 2.1744156823322442e-06,
      "loss": 1.0479,
      "step": 18560
    },
    {
      "epoch": 4.643781654473832,
      "grad_norm": 2.3878817558288574,
      "learning_rate": 2.159336516712742e-06,
      "loss": 1.0785,
      "step": 18570
    },
    {
      "epoch": 4.646282748702557,
      "grad_norm": 1.8442907333374023,
      "learning_rate": 2.1442573510932396e-06,
      "loss": 1.0731,
      "step": 18580
    },
    {
      "epoch": 4.648783842931282,
      "grad_norm": 2.144564151763916,
      "learning_rate": 2.1291781854737373e-06,
      "loss": 1.07,
      "step": 18590
    },
    {
      "epoch": 4.651284937160008,
      "grad_norm": 1.9725332260131836,
      "learning_rate": 2.114099019854235e-06,
      "loss": 1.0365,
      "step": 18600
    },
    {
      "epoch": 4.653786031388733,
      "grad_norm": 2.245596408843994,
      "learning_rate": 2.0990198542347323e-06,
      "loss": 1.075,
      "step": 18610
    },
    {
      "epoch": 4.656287125617458,
      "grad_norm": 1.9742721319198608,
      "learning_rate": 2.08394068861523e-06,
      "loss": 1.0665,
      "step": 18620
    },
    {
      "epoch": 4.6587882198461825,
      "grad_norm": 2.1492931842803955,
      "learning_rate": 2.0688615229957276e-06,
      "loss": 1.0834,
      "step": 18630
    },
    {
      "epoch": 4.661289314074907,
      "grad_norm": 2.2885634899139404,
      "learning_rate": 2.0537823573762253e-06,
      "loss": 1.108,
      "step": 18640
    },
    {
      "epoch": 4.663790408303633,
      "grad_norm": 1.9373652935028076,
      "learning_rate": 2.0387031917567226e-06,
      "loss": 1.0674,
      "step": 18650
    },
    {
      "epoch": 4.666291502532358,
      "grad_norm": 2.2295851707458496,
      "learning_rate": 2.0236240261372203e-06,
      "loss": 1.1245,
      "step": 18660
    },
    {
      "epoch": 4.668792596761083,
      "grad_norm": 2.169797420501709,
      "learning_rate": 2.008544860517718e-06,
      "loss": 1.1026,
      "step": 18670
    },
    {
      "epoch": 4.671293690989808,
      "grad_norm": 2.4194540977478027,
      "learning_rate": 1.9934656948982157e-06,
      "loss": 1.044,
      "step": 18680
    },
    {
      "epoch": 4.673794785218533,
      "grad_norm": 2.1587343215942383,
      "learning_rate": 1.9783865292787134e-06,
      "loss": 1.0825,
      "step": 18690
    },
    {
      "epoch": 4.676295879447258,
      "grad_norm": 2.8741841316223145,
      "learning_rate": 1.963307363659211e-06,
      "loss": 1.1158,
      "step": 18700
    },
    {
      "epoch": 4.678796973675984,
      "grad_norm": 2.1513803005218506,
      "learning_rate": 1.9482281980397087e-06,
      "loss": 1.0709,
      "step": 18710
    },
    {
      "epoch": 4.6812980679047085,
      "grad_norm": 2.7472078800201416,
      "learning_rate": 1.9331490324202064e-06,
      "loss": 1.0217,
      "step": 18720
    },
    {
      "epoch": 4.683799162133433,
      "grad_norm": 2.29901385307312,
      "learning_rate": 1.918069866800704e-06,
      "loss": 1.1127,
      "step": 18730
    },
    {
      "epoch": 4.686300256362158,
      "grad_norm": 2.110398292541504,
      "learning_rate": 1.9029907011812012e-06,
      "loss": 1.1043,
      "step": 18740
    },
    {
      "epoch": 4.688801350590883,
      "grad_norm": 3.074059247970581,
      "learning_rate": 1.8879115355616989e-06,
      "loss": 1.0264,
      "step": 18750
    },
    {
      "epoch": 4.691302444819609,
      "grad_norm": 2.3022124767303467,
      "learning_rate": 1.8728323699421966e-06,
      "loss": 1.0668,
      "step": 18760
    },
    {
      "epoch": 4.693803539048334,
      "grad_norm": 1.8642133474349976,
      "learning_rate": 1.857753204322694e-06,
      "loss": 0.9841,
      "step": 18770
    },
    {
      "epoch": 4.696304633277059,
      "grad_norm": 2.757458448410034,
      "learning_rate": 1.8426740387031917e-06,
      "loss": 1.05,
      "step": 18780
    },
    {
      "epoch": 4.698805727505784,
      "grad_norm": 2.3990070819854736,
      "learning_rate": 1.8275948730836894e-06,
      "loss": 1.0868,
      "step": 18790
    },
    {
      "epoch": 4.701306821734509,
      "grad_norm": 1.947974681854248,
      "learning_rate": 1.812515707464187e-06,
      "loss": 1.0714,
      "step": 18800
    },
    {
      "epoch": 4.7038079159632336,
      "grad_norm": 2.725058078765869,
      "learning_rate": 1.7974365418446846e-06,
      "loss": 1.1582,
      "step": 18810
    },
    {
      "epoch": 4.706309010191959,
      "grad_norm": 2.5467166900634766,
      "learning_rate": 1.7823573762251823e-06,
      "loss": 1.0677,
      "step": 18820
    },
    {
      "epoch": 4.708810104420684,
      "grad_norm": 1.9679194688796997,
      "learning_rate": 1.76727821060568e-06,
      "loss": 1.0913,
      "step": 18830
    },
    {
      "epoch": 4.711311198649409,
      "grad_norm": 2.387951612472534,
      "learning_rate": 1.7521990449861774e-06,
      "loss": 1.0631,
      "step": 18840
    },
    {
      "epoch": 4.713812292878134,
      "grad_norm": 2.1876649856567383,
      "learning_rate": 1.7371198793666751e-06,
      "loss": 1.0044,
      "step": 18850
    },
    {
      "epoch": 4.716313387106859,
      "grad_norm": 2.5824332237243652,
      "learning_rate": 1.7220407137471726e-06,
      "loss": 1.0595,
      "step": 18860
    },
    {
      "epoch": 4.718814481335585,
      "grad_norm": 2.059495687484741,
      "learning_rate": 1.7069615481276703e-06,
      "loss": 1.0722,
      "step": 18870
    },
    {
      "epoch": 4.72131557556431,
      "grad_norm": 2.090470552444458,
      "learning_rate": 1.691882382508168e-06,
      "loss": 1.1674,
      "step": 18880
    },
    {
      "epoch": 4.723816669793035,
      "grad_norm": 2.86881160736084,
      "learning_rate": 1.6768032168886657e-06,
      "loss": 1.0547,
      "step": 18890
    },
    {
      "epoch": 4.7263177640217595,
      "grad_norm": 2.642784833908081,
      "learning_rate": 1.6617240512691632e-06,
      "loss": 1.1138,
      "step": 18900
    },
    {
      "epoch": 4.728818858250484,
      "grad_norm": 2.195993185043335,
      "learning_rate": 1.6466448856496606e-06,
      "loss": 1.0173,
      "step": 18910
    },
    {
      "epoch": 4.731319952479209,
      "grad_norm": 2.1485416889190674,
      "learning_rate": 1.6315657200301583e-06,
      "loss": 1.0926,
      "step": 18920
    },
    {
      "epoch": 4.733821046707934,
      "grad_norm": 2.0782785415649414,
      "learning_rate": 1.616486554410656e-06,
      "loss": 1.0841,
      "step": 18930
    },
    {
      "epoch": 4.73632214093666,
      "grad_norm": 2.3682215213775635,
      "learning_rate": 1.6014073887911537e-06,
      "loss": 1.08,
      "step": 18940
    },
    {
      "epoch": 4.738823235165385,
      "grad_norm": 1.9236440658569336,
      "learning_rate": 1.5863282231716514e-06,
      "loss": 1.0556,
      "step": 18950
    },
    {
      "epoch": 4.74132432939411,
      "grad_norm": 2.526224136352539,
      "learning_rate": 1.5712490575521487e-06,
      "loss": 1.0818,
      "step": 18960
    },
    {
      "epoch": 4.743825423622835,
      "grad_norm": 2.5025908946990967,
      "learning_rate": 1.5561698919326463e-06,
      "loss": 1.0623,
      "step": 18970
    },
    {
      "epoch": 4.74632651785156,
      "grad_norm": 2.582488775253296,
      "learning_rate": 1.541090726313144e-06,
      "loss": 1.1269,
      "step": 18980
    },
    {
      "epoch": 4.7488276120802855,
      "grad_norm": 2.543806552886963,
      "learning_rate": 1.5260115606936417e-06,
      "loss": 1.1022,
      "step": 18990
    },
    {
      "epoch": 4.75132870630901,
      "grad_norm": 2.05898118019104,
      "learning_rate": 1.5109323950741392e-06,
      "loss": 0.9867,
      "step": 19000
    },
    {
      "epoch": 4.753829800537735,
      "grad_norm": 2.53170108795166,
      "learning_rate": 1.4958532294546369e-06,
      "loss": 1.0236,
      "step": 19010
    },
    {
      "epoch": 4.75633089476646,
      "grad_norm": 2.883436918258667,
      "learning_rate": 1.4807740638351346e-06,
      "loss": 1.0863,
      "step": 19020
    },
    {
      "epoch": 4.758831988995185,
      "grad_norm": 2.2296125888824463,
      "learning_rate": 1.465694898215632e-06,
      "loss": 1.0316,
      "step": 19030
    },
    {
      "epoch": 4.76133308322391,
      "grad_norm": 2.906637191772461,
      "learning_rate": 1.4506157325961297e-06,
      "loss": 0.9965,
      "step": 19040
    },
    {
      "epoch": 4.763834177452636,
      "grad_norm": 1.9806294441223145,
      "learning_rate": 1.4355365669766272e-06,
      "loss": 1.0675,
      "step": 19050
    },
    {
      "epoch": 4.766335271681361,
      "grad_norm": 2.326406955718994,
      "learning_rate": 1.420457401357125e-06,
      "loss": 1.0467,
      "step": 19060
    },
    {
      "epoch": 4.768836365910086,
      "grad_norm": 2.617859125137329,
      "learning_rate": 1.4053782357376226e-06,
      "loss": 1.0597,
      "step": 19070
    },
    {
      "epoch": 4.7713374601388105,
      "grad_norm": 2.1095316410064697,
      "learning_rate": 1.3902990701181203e-06,
      "loss": 1.069,
      "step": 19080
    },
    {
      "epoch": 4.7738385543675355,
      "grad_norm": 2.751662015914917,
      "learning_rate": 1.3752199044986178e-06,
      "loss": 1.1449,
      "step": 19090
    },
    {
      "epoch": 4.776339648596261,
      "grad_norm": 2.1596691608428955,
      "learning_rate": 1.3601407388791153e-06,
      "loss": 0.9953,
      "step": 19100
    },
    {
      "epoch": 4.778840742824986,
      "grad_norm": 2.5976531505584717,
      "learning_rate": 1.345061573259613e-06,
      "loss": 1.0467,
      "step": 19110
    },
    {
      "epoch": 4.781341837053711,
      "grad_norm": 1.8352619409561157,
      "learning_rate": 1.3299824076401106e-06,
      "loss": 1.0186,
      "step": 19120
    },
    {
      "epoch": 4.783842931282436,
      "grad_norm": 1.911249041557312,
      "learning_rate": 1.3149032420206083e-06,
      "loss": 1.0466,
      "step": 19130
    },
    {
      "epoch": 4.786344025511161,
      "grad_norm": 2.334209442138672,
      "learning_rate": 1.299824076401106e-06,
      "loss": 1.0588,
      "step": 19140
    },
    {
      "epoch": 4.788845119739886,
      "grad_norm": 2.6833269596099854,
      "learning_rate": 1.2847449107816035e-06,
      "loss": 1.0632,
      "step": 19150
    },
    {
      "epoch": 4.791346213968612,
      "grad_norm": 2.57384991645813,
      "learning_rate": 1.269665745162101e-06,
      "loss": 1.0254,
      "step": 19160
    },
    {
      "epoch": 4.7938473081973365,
      "grad_norm": 3.131141424179077,
      "learning_rate": 1.2545865795425987e-06,
      "loss": 1.1087,
      "step": 19170
    },
    {
      "epoch": 4.796348402426061,
      "grad_norm": 2.882798194885254,
      "learning_rate": 1.2395074139230963e-06,
      "loss": 1.0718,
      "step": 19180
    },
    {
      "epoch": 4.798849496654786,
      "grad_norm": 2.1993799209594727,
      "learning_rate": 1.2244282483035938e-06,
      "loss": 1.0817,
      "step": 19190
    },
    {
      "epoch": 4.801350590883511,
      "grad_norm": 3.873732566833496,
      "learning_rate": 1.2093490826840915e-06,
      "loss": 1.0859,
      "step": 19200
    },
    {
      "epoch": 4.803851685112237,
      "grad_norm": 2.7528607845306396,
      "learning_rate": 1.1942699170645892e-06,
      "loss": 1.0424,
      "step": 19210
    },
    {
      "epoch": 4.806352779340962,
      "grad_norm": 3.202166795730591,
      "learning_rate": 1.1791907514450867e-06,
      "loss": 1.0791,
      "step": 19220
    },
    {
      "epoch": 4.808853873569687,
      "grad_norm": 2.134160041809082,
      "learning_rate": 1.1641115858255844e-06,
      "loss": 1.0199,
      "step": 19230
    },
    {
      "epoch": 4.811354967798412,
      "grad_norm": 1.9851949214935303,
      "learning_rate": 1.1490324202060818e-06,
      "loss": 1.043,
      "step": 19240
    },
    {
      "epoch": 4.813856062027137,
      "grad_norm": 2.033003330230713,
      "learning_rate": 1.1339532545865795e-06,
      "loss": 1.0742,
      "step": 19250
    },
    {
      "epoch": 4.816357156255862,
      "grad_norm": 2.7964110374450684,
      "learning_rate": 1.1188740889670772e-06,
      "loss": 1.0611,
      "step": 19260
    },
    {
      "epoch": 4.818858250484587,
      "grad_norm": 3.3375139236450195,
      "learning_rate": 1.103794923347575e-06,
      "loss": 1.0515,
      "step": 19270
    },
    {
      "epoch": 4.821359344713312,
      "grad_norm": 2.310908079147339,
      "learning_rate": 1.0887157577280724e-06,
      "loss": 1.0652,
      "step": 19280
    },
    {
      "epoch": 4.823860438942037,
      "grad_norm": 2.0211100578308105,
      "learning_rate": 1.0736365921085699e-06,
      "loss": 1.0519,
      "step": 19290
    },
    {
      "epoch": 4.826361533170762,
      "grad_norm": 2.7704503536224365,
      "learning_rate": 1.0585574264890676e-06,
      "loss": 1.115,
      "step": 19300
    },
    {
      "epoch": 4.828862627399487,
      "grad_norm": 3.4574477672576904,
      "learning_rate": 1.0434782608695653e-06,
      "loss": 1.0148,
      "step": 19310
    },
    {
      "epoch": 4.831363721628213,
      "grad_norm": 2.388766050338745,
      "learning_rate": 1.028399095250063e-06,
      "loss": 1.0697,
      "step": 19320
    },
    {
      "epoch": 4.833864815856938,
      "grad_norm": 2.1746904850006104,
      "learning_rate": 1.0133199296305606e-06,
      "loss": 1.0036,
      "step": 19330
    },
    {
      "epoch": 4.836365910085663,
      "grad_norm": 1.9194715023040771,
      "learning_rate": 9.982407640110581e-07,
      "loss": 1.0816,
      "step": 19340
    },
    {
      "epoch": 4.8388670043143875,
      "grad_norm": 2.434713840484619,
      "learning_rate": 9.831615983915556e-07,
      "loss": 1.0623,
      "step": 19350
    },
    {
      "epoch": 4.8413680985431125,
      "grad_norm": 2.1507554054260254,
      "learning_rate": 9.680824327720533e-07,
      "loss": 1.0012,
      "step": 19360
    },
    {
      "epoch": 4.843869192771837,
      "grad_norm": 1.8542814254760742,
      "learning_rate": 9.53003267152551e-07,
      "loss": 1.1206,
      "step": 19370
    },
    {
      "epoch": 4.846370287000562,
      "grad_norm": 2.1149330139160156,
      "learning_rate": 9.379241015330484e-07,
      "loss": 1.0753,
      "step": 19380
    },
    {
      "epoch": 4.848871381229288,
      "grad_norm": 1.8971349000930786,
      "learning_rate": 9.228449359135462e-07,
      "loss": 1.1254,
      "step": 19390
    },
    {
      "epoch": 4.851372475458013,
      "grad_norm": 2.392185926437378,
      "learning_rate": 9.077657702940437e-07,
      "loss": 1.0602,
      "step": 19400
    },
    {
      "epoch": 4.853873569686738,
      "grad_norm": 2.316568374633789,
      "learning_rate": 8.926866046745414e-07,
      "loss": 1.0531,
      "step": 19410
    },
    {
      "epoch": 4.856374663915463,
      "grad_norm": 2.3527164459228516,
      "learning_rate": 8.77607439055039e-07,
      "loss": 1.0594,
      "step": 19420
    },
    {
      "epoch": 4.858875758144188,
      "grad_norm": 2.7162322998046875,
      "learning_rate": 8.625282734355366e-07,
      "loss": 1.1123,
      "step": 19430
    },
    {
      "epoch": 4.8613768523729135,
      "grad_norm": 2.5935044288635254,
      "learning_rate": 8.474491078160342e-07,
      "loss": 1.0316,
      "step": 19440
    },
    {
      "epoch": 4.863877946601638,
      "grad_norm": 2.615791082382202,
      "learning_rate": 8.323699421965318e-07,
      "loss": 1.1092,
      "step": 19450
    },
    {
      "epoch": 4.866379040830363,
      "grad_norm": 2.5483522415161133,
      "learning_rate": 8.172907765770294e-07,
      "loss": 1.0193,
      "step": 19460
    },
    {
      "epoch": 4.868880135059088,
      "grad_norm": 2.1339073181152344,
      "learning_rate": 8.02211610957527e-07,
      "loss": 1.1464,
      "step": 19470
    },
    {
      "epoch": 4.871381229287813,
      "grad_norm": 2.2842326164245605,
      "learning_rate": 7.871324453380247e-07,
      "loss": 1.1025,
      "step": 19480
    },
    {
      "epoch": 4.873882323516538,
      "grad_norm": 2.051597833633423,
      "learning_rate": 7.720532797185222e-07,
      "loss": 1.0381,
      "step": 19490
    },
    {
      "epoch": 4.876383417745264,
      "grad_norm": 2.272792339324951,
      "learning_rate": 7.569741140990199e-07,
      "loss": 1.0166,
      "step": 19500
    },
    {
      "epoch": 4.878884511973989,
      "grad_norm": 2.889096736907959,
      "learning_rate": 7.418949484795175e-07,
      "loss": 1.0125,
      "step": 19510
    },
    {
      "epoch": 4.881385606202714,
      "grad_norm": 3.299241781234741,
      "learning_rate": 7.26815782860015e-07,
      "loss": 1.0918,
      "step": 19520
    },
    {
      "epoch": 4.883886700431439,
      "grad_norm": 3.752575159072876,
      "learning_rate": 7.117366172405127e-07,
      "loss": 1.0261,
      "step": 19530
    },
    {
      "epoch": 4.8863877946601635,
      "grad_norm": 2.8596715927124023,
      "learning_rate": 6.966574516210103e-07,
      "loss": 1.1295,
      "step": 19540
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 2.525059700012207,
      "learning_rate": 6.81578286001508e-07,
      "loss": 1.114,
      "step": 19550
    },
    {
      "epoch": 4.891389983117614,
      "grad_norm": 2.349404811859131,
      "learning_rate": 6.664991203820055e-07,
      "loss": 1.0793,
      "step": 19560
    },
    {
      "epoch": 4.893891077346339,
      "grad_norm": 1.8981941938400269,
      "learning_rate": 6.514199547625032e-07,
      "loss": 1.1185,
      "step": 19570
    },
    {
      "epoch": 4.896392171575064,
      "grad_norm": 3.248549461364746,
      "learning_rate": 6.363407891430009e-07,
      "loss": 0.9789,
      "step": 19580
    },
    {
      "epoch": 4.898893265803789,
      "grad_norm": 2.045781135559082,
      "learning_rate": 6.212616235234983e-07,
      "loss": 1.027,
      "step": 19590
    },
    {
      "epoch": 4.901394360032514,
      "grad_norm": 2.1395957469940186,
      "learning_rate": 6.06182457903996e-07,
      "loss": 0.9883,
      "step": 19600
    },
    {
      "epoch": 4.90389545426124,
      "grad_norm": 2.2266290187835693,
      "learning_rate": 5.911032922844936e-07,
      "loss": 1.0214,
      "step": 19610
    },
    {
      "epoch": 4.9063965484899645,
      "grad_norm": 2.07448148727417,
      "learning_rate": 5.760241266649912e-07,
      "loss": 1.1488,
      "step": 19620
    },
    {
      "epoch": 4.9088976427186894,
      "grad_norm": 1.891741156578064,
      "learning_rate": 5.609449610454888e-07,
      "loss": 1.0817,
      "step": 19630
    },
    {
      "epoch": 4.911398736947414,
      "grad_norm": 2.462674856185913,
      "learning_rate": 5.458657954259865e-07,
      "loss": 1.0872,
      "step": 19640
    },
    {
      "epoch": 4.913899831176139,
      "grad_norm": 2.120836019515991,
      "learning_rate": 5.307866298064841e-07,
      "loss": 1.0751,
      "step": 19650
    },
    {
      "epoch": 4.916400925404865,
      "grad_norm": 2.288605213165283,
      "learning_rate": 5.157074641869816e-07,
      "loss": 0.9942,
      "step": 19660
    },
    {
      "epoch": 4.91890201963359,
      "grad_norm": 2.3375868797302246,
      "learning_rate": 5.006282985674793e-07,
      "loss": 1.0814,
      "step": 19670
    },
    {
      "epoch": 4.921403113862315,
      "grad_norm": 2.634840250015259,
      "learning_rate": 4.855491329479768e-07,
      "loss": 1.0264,
      "step": 19680
    },
    {
      "epoch": 4.92390420809104,
      "grad_norm": 2.388104200363159,
      "learning_rate": 4.704699673284745e-07,
      "loss": 1.0536,
      "step": 19690
    },
    {
      "epoch": 4.926405302319765,
      "grad_norm": 2.5921289920806885,
      "learning_rate": 4.5539080170897213e-07,
      "loss": 1.0202,
      "step": 19700
    },
    {
      "epoch": 4.92890639654849,
      "grad_norm": 2.2095541954040527,
      "learning_rate": 4.403116360894697e-07,
      "loss": 1.0355,
      "step": 19710
    },
    {
      "epoch": 4.931407490777215,
      "grad_norm": 2.0016226768493652,
      "learning_rate": 4.252324704699673e-07,
      "loss": 1.0313,
      "step": 19720
    },
    {
      "epoch": 4.93390858500594,
      "grad_norm": 2.4315004348754883,
      "learning_rate": 4.1015330485046494e-07,
      "loss": 1.0262,
      "step": 19730
    },
    {
      "epoch": 4.936409679234665,
      "grad_norm": 2.470954656600952,
      "learning_rate": 3.950741392309626e-07,
      "loss": 1.0743,
      "step": 19740
    },
    {
      "epoch": 4.93891077346339,
      "grad_norm": 3.0314784049987793,
      "learning_rate": 3.799949736114602e-07,
      "loss": 1.0912,
      "step": 19750
    },
    {
      "epoch": 4.941411867692115,
      "grad_norm": 2.698273181915283,
      "learning_rate": 3.649158079919578e-07,
      "loss": 1.1349,
      "step": 19760
    },
    {
      "epoch": 4.943912961920841,
      "grad_norm": 2.390608787536621,
      "learning_rate": 3.498366423724554e-07,
      "loss": 1.0451,
      "step": 19770
    },
    {
      "epoch": 4.946414056149566,
      "grad_norm": 2.887941360473633,
      "learning_rate": 3.34757476752953e-07,
      "loss": 1.0561,
      "step": 19780
    },
    {
      "epoch": 4.948915150378291,
      "grad_norm": 2.138223886489868,
      "learning_rate": 3.196783111334506e-07,
      "loss": 1.0062,
      "step": 19790
    },
    {
      "epoch": 4.951416244607016,
      "grad_norm": 2.1401543617248535,
      "learning_rate": 3.0459914551394824e-07,
      "loss": 1.0414,
      "step": 19800
    }
  ],
  "logging_steps": 10,
  "max_steps": 19995,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.207802546382438e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
