{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 24969,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0012015620306398318,
      "grad_norm": 1.6356604099273682,
      "learning_rate": 2.7e-06,
      "loss": 3.4358,
      "step": 10
    },
    {
      "epoch": 0.0024031240612796636,
      "grad_norm": 1.6478530168533325,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 3.508,
      "step": 20
    },
    {
      "epoch": 0.003604686091919495,
      "grad_norm": 1.499620795249939,
      "learning_rate": 8.7e-06,
      "loss": 3.4768,
      "step": 30
    },
    {
      "epoch": 0.004806248122559327,
      "grad_norm": 1.4761016368865967,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 3.4399,
      "step": 40
    },
    {
      "epoch": 0.006007810153199159,
      "grad_norm": 1.5773042440414429,
      "learning_rate": 1.47e-05,
      "loss": 3.3971,
      "step": 50
    },
    {
      "epoch": 0.00720937218383899,
      "grad_norm": 1.5374075174331665,
      "learning_rate": 1.77e-05,
      "loss": 3.3916,
      "step": 60
    },
    {
      "epoch": 0.008410934214478822,
      "grad_norm": 1.9481515884399414,
      "learning_rate": 2.07e-05,
      "loss": 3.1998,
      "step": 70
    },
    {
      "epoch": 0.009612496245118654,
      "grad_norm": 2.063148260116577,
      "learning_rate": 2.37e-05,
      "loss": 3.1257,
      "step": 80
    },
    {
      "epoch": 0.010814058275758487,
      "grad_norm": 2.382978677749634,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.8737,
      "step": 90
    },
    {
      "epoch": 0.012015620306398318,
      "grad_norm": 2.6205005645751953,
      "learning_rate": 2.97e-05,
      "loss": 2.6568,
      "step": 100
    },
    {
      "epoch": 0.01321718233703815,
      "grad_norm": 3.3479576110839844,
      "learning_rate": 2.9989143109895855e-05,
      "loss": 2.3795,
      "step": 110
    },
    {
      "epoch": 0.01441874436767798,
      "grad_norm": 2.8395771980285645,
      "learning_rate": 2.997707989866903e-05,
      "loss": 2.0833,
      "step": 120
    },
    {
      "epoch": 0.015620306398317813,
      "grad_norm": 3.247994899749756,
      "learning_rate": 2.9965016687442198e-05,
      "loss": 1.8169,
      "step": 130
    },
    {
      "epoch": 0.016821868428957644,
      "grad_norm": 2.483731269836426,
      "learning_rate": 2.9952953476215368e-05,
      "loss": 1.687,
      "step": 140
    },
    {
      "epoch": 0.018023430459597478,
      "grad_norm": 3.453052282333374,
      "learning_rate": 2.994089026498854e-05,
      "loss": 1.5726,
      "step": 150
    },
    {
      "epoch": 0.01922499249023731,
      "grad_norm": 2.409130334854126,
      "learning_rate": 2.992882705376171e-05,
      "loss": 1.3906,
      "step": 160
    },
    {
      "epoch": 0.02042655452087714,
      "grad_norm": 1.9381115436553955,
      "learning_rate": 2.9916763842534884e-05,
      "loss": 1.4046,
      "step": 170
    },
    {
      "epoch": 0.021628116551516974,
      "grad_norm": 2.2987771034240723,
      "learning_rate": 2.9904700631308054e-05,
      "loss": 1.3976,
      "step": 180
    },
    {
      "epoch": 0.022829678582156804,
      "grad_norm": 1.884333610534668,
      "learning_rate": 2.9892637420081227e-05,
      "loss": 1.4163,
      "step": 190
    },
    {
      "epoch": 0.024031240612796635,
      "grad_norm": 2.4653539657592773,
      "learning_rate": 2.98805742088544e-05,
      "loss": 1.3762,
      "step": 200
    },
    {
      "epoch": 0.025232802643436466,
      "grad_norm": 1.780578374862671,
      "learning_rate": 2.986851099762757e-05,
      "loss": 1.3996,
      "step": 210
    },
    {
      "epoch": 0.0264343646740763,
      "grad_norm": 2.289854049682617,
      "learning_rate": 2.985644778640074e-05,
      "loss": 1.3904,
      "step": 220
    },
    {
      "epoch": 0.02763592670471613,
      "grad_norm": 1.9651415348052979,
      "learning_rate": 2.9844384575173913e-05,
      "loss": 1.3898,
      "step": 230
    },
    {
      "epoch": 0.02883748873535596,
      "grad_norm": 2.1148476600646973,
      "learning_rate": 2.9832321363947082e-05,
      "loss": 1.3204,
      "step": 240
    },
    {
      "epoch": 0.030039050765995796,
      "grad_norm": 2.7151269912719727,
      "learning_rate": 2.9820258152720255e-05,
      "loss": 1.3663,
      "step": 250
    },
    {
      "epoch": 0.031240612796635626,
      "grad_norm": 2.975978374481201,
      "learning_rate": 2.9808194941493425e-05,
      "loss": 1.3814,
      "step": 260
    },
    {
      "epoch": 0.03244217482727546,
      "grad_norm": 2.8652048110961914,
      "learning_rate": 2.9796131730266598e-05,
      "loss": 1.269,
      "step": 270
    },
    {
      "epoch": 0.03364373685791529,
      "grad_norm": 2.0454177856445312,
      "learning_rate": 2.978406851903977e-05,
      "loss": 1.3665,
      "step": 280
    },
    {
      "epoch": 0.03484529888855512,
      "grad_norm": 2.449887990951538,
      "learning_rate": 2.977200530781294e-05,
      "loss": 1.2644,
      "step": 290
    },
    {
      "epoch": 0.036046860919194956,
      "grad_norm": 2.392212152481079,
      "learning_rate": 2.9759942096586114e-05,
      "loss": 1.3696,
      "step": 300
    },
    {
      "epoch": 0.037248422949834784,
      "grad_norm": 2.532525062561035,
      "learning_rate": 2.974787888535928e-05,
      "loss": 1.3183,
      "step": 310
    },
    {
      "epoch": 0.03844998498047462,
      "grad_norm": 2.4599335193634033,
      "learning_rate": 2.9735815674132454e-05,
      "loss": 1.3077,
      "step": 320
    },
    {
      "epoch": 0.03965154701111445,
      "grad_norm": 2.8134970664978027,
      "learning_rate": 2.9723752462905627e-05,
      "loss": 1.3115,
      "step": 330
    },
    {
      "epoch": 0.04085310904175428,
      "grad_norm": 2.86922025680542,
      "learning_rate": 2.9711689251678797e-05,
      "loss": 1.3665,
      "step": 340
    },
    {
      "epoch": 0.04205467107239411,
      "grad_norm": 2.154834270477295,
      "learning_rate": 2.969962604045197e-05,
      "loss": 1.336,
      "step": 350
    },
    {
      "epoch": 0.04325623310303395,
      "grad_norm": 2.722572088241577,
      "learning_rate": 2.968756282922514e-05,
      "loss": 1.241,
      "step": 360
    },
    {
      "epoch": 0.044457795133673775,
      "grad_norm": 3.5805208683013916,
      "learning_rate": 2.9675499617998313e-05,
      "loss": 1.2526,
      "step": 370
    },
    {
      "epoch": 0.04565935716431361,
      "grad_norm": 3.475782632827759,
      "learning_rate": 2.9663436406771486e-05,
      "loss": 1.2757,
      "step": 380
    },
    {
      "epoch": 0.046860919194953436,
      "grad_norm": 3.9300997257232666,
      "learning_rate": 2.9651373195544652e-05,
      "loss": 1.282,
      "step": 390
    },
    {
      "epoch": 0.04806248122559327,
      "grad_norm": 4.978875160217285,
      "learning_rate": 2.9639309984317825e-05,
      "loss": 1.2299,
      "step": 400
    },
    {
      "epoch": 0.049264043256233105,
      "grad_norm": 2.831456422805786,
      "learning_rate": 2.9627246773091e-05,
      "loss": 1.2578,
      "step": 410
    },
    {
      "epoch": 0.05046560528687293,
      "grad_norm": 4.581047058105469,
      "learning_rate": 2.9615183561864168e-05,
      "loss": 1.2432,
      "step": 420
    },
    {
      "epoch": 0.051667167317512766,
      "grad_norm": 3.067068338394165,
      "learning_rate": 2.960312035063734e-05,
      "loss": 1.2777,
      "step": 430
    },
    {
      "epoch": 0.0528687293481526,
      "grad_norm": 3.9223921298980713,
      "learning_rate": 2.959105713941051e-05,
      "loss": 1.2726,
      "step": 440
    },
    {
      "epoch": 0.05407029137879243,
      "grad_norm": 2.735332727432251,
      "learning_rate": 2.9578993928183684e-05,
      "loss": 1.2983,
      "step": 450
    },
    {
      "epoch": 0.05527185340943226,
      "grad_norm": 3.4082462787628174,
      "learning_rate": 2.9566930716956857e-05,
      "loss": 1.2477,
      "step": 460
    },
    {
      "epoch": 0.056473415440072096,
      "grad_norm": 2.8860857486724854,
      "learning_rate": 2.9554867505730027e-05,
      "loss": 1.2271,
      "step": 470
    },
    {
      "epoch": 0.05767497747071192,
      "grad_norm": 4.297685623168945,
      "learning_rate": 2.9542804294503197e-05,
      "loss": 1.242,
      "step": 480
    },
    {
      "epoch": 0.05887653950135176,
      "grad_norm": 2.971956729888916,
      "learning_rate": 2.9530741083276367e-05,
      "loss": 1.2889,
      "step": 490
    },
    {
      "epoch": 0.06007810153199159,
      "grad_norm": 3.7332370281219482,
      "learning_rate": 2.951867787204954e-05,
      "loss": 1.2079,
      "step": 500
    },
    {
      "epoch": 0.06127966356263142,
      "grad_norm": 2.5436880588531494,
      "learning_rate": 2.9506614660822713e-05,
      "loss": 1.2534,
      "step": 510
    },
    {
      "epoch": 0.06248122559327125,
      "grad_norm": 3.360830783843994,
      "learning_rate": 2.9494551449595883e-05,
      "loss": 1.2328,
      "step": 520
    },
    {
      "epoch": 0.06368278762391108,
      "grad_norm": 3.571169137954712,
      "learning_rate": 2.9482488238369056e-05,
      "loss": 1.2553,
      "step": 530
    },
    {
      "epoch": 0.06488434965455092,
      "grad_norm": 4.080465316772461,
      "learning_rate": 2.9470425027142225e-05,
      "loss": 1.2103,
      "step": 540
    },
    {
      "epoch": 0.06608591168519075,
      "grad_norm": 2.5470800399780273,
      "learning_rate": 2.94583618159154e-05,
      "loss": 1.2024,
      "step": 550
    },
    {
      "epoch": 0.06728747371583058,
      "grad_norm": 3.557190179824829,
      "learning_rate": 2.944629860468857e-05,
      "loss": 1.2455,
      "step": 560
    },
    {
      "epoch": 0.06848903574647042,
      "grad_norm": 4.857729434967041,
      "learning_rate": 2.9434235393461738e-05,
      "loss": 1.289,
      "step": 570
    },
    {
      "epoch": 0.06969059777711024,
      "grad_norm": 5.1125688552856445,
      "learning_rate": 2.942217218223491e-05,
      "loss": 1.2101,
      "step": 580
    },
    {
      "epoch": 0.07089215980775007,
      "grad_norm": 3.097302198410034,
      "learning_rate": 2.9410108971008084e-05,
      "loss": 1.2749,
      "step": 590
    },
    {
      "epoch": 0.07209372183838991,
      "grad_norm": 4.489323139190674,
      "learning_rate": 2.9398045759781254e-05,
      "loss": 1.2716,
      "step": 600
    },
    {
      "epoch": 0.07329528386902974,
      "grad_norm": 4.920882225036621,
      "learning_rate": 2.9385982548554427e-05,
      "loss": 1.1782,
      "step": 610
    },
    {
      "epoch": 0.07449684589966957,
      "grad_norm": 3.8641903400421143,
      "learning_rate": 2.9373919337327597e-05,
      "loss": 1.2189,
      "step": 620
    },
    {
      "epoch": 0.07569840793030941,
      "grad_norm": 3.2635018825531006,
      "learning_rate": 2.936185612610077e-05,
      "loss": 1.2214,
      "step": 630
    },
    {
      "epoch": 0.07689996996094924,
      "grad_norm": 5.973597526550293,
      "learning_rate": 2.934979291487394e-05,
      "loss": 1.2094,
      "step": 640
    },
    {
      "epoch": 0.07810153199158906,
      "grad_norm": 3.089153528213501,
      "learning_rate": 2.933772970364711e-05,
      "loss": 1.1915,
      "step": 650
    },
    {
      "epoch": 0.0793030940222289,
      "grad_norm": 2.898796796798706,
      "learning_rate": 2.9325666492420283e-05,
      "loss": 1.1991,
      "step": 660
    },
    {
      "epoch": 0.08050465605286873,
      "grad_norm": 3.292264461517334,
      "learning_rate": 2.9313603281193453e-05,
      "loss": 1.2453,
      "step": 670
    },
    {
      "epoch": 0.08170621808350856,
      "grad_norm": 2.9236464500427246,
      "learning_rate": 2.9301540069966626e-05,
      "loss": 1.1949,
      "step": 680
    },
    {
      "epoch": 0.0829077801141484,
      "grad_norm": 4.475027561187744,
      "learning_rate": 2.92894768587398e-05,
      "loss": 1.2048,
      "step": 690
    },
    {
      "epoch": 0.08410934214478823,
      "grad_norm": 3.5348002910614014,
      "learning_rate": 2.927741364751297e-05,
      "loss": 1.1688,
      "step": 700
    },
    {
      "epoch": 0.08531090417542805,
      "grad_norm": 2.876878261566162,
      "learning_rate": 2.926535043628614e-05,
      "loss": 1.2231,
      "step": 710
    },
    {
      "epoch": 0.0865124662060679,
      "grad_norm": 3.2883355617523193,
      "learning_rate": 2.925328722505931e-05,
      "loss": 1.2796,
      "step": 720
    },
    {
      "epoch": 0.08771402823670772,
      "grad_norm": 3.182431221008301,
      "learning_rate": 2.924122401383248e-05,
      "loss": 1.2686,
      "step": 730
    },
    {
      "epoch": 0.08891559026734755,
      "grad_norm": 3.4462220668792725,
      "learning_rate": 2.9229160802605654e-05,
      "loss": 1.2478,
      "step": 740
    },
    {
      "epoch": 0.09011715229798738,
      "grad_norm": 3.3695225715637207,
      "learning_rate": 2.9217097591378824e-05,
      "loss": 1.2458,
      "step": 750
    },
    {
      "epoch": 0.09131871432862722,
      "grad_norm": 3.5661749839782715,
      "learning_rate": 2.9205034380151997e-05,
      "loss": 1.2328,
      "step": 760
    },
    {
      "epoch": 0.09252027635926705,
      "grad_norm": 2.9913814067840576,
      "learning_rate": 2.919297116892517e-05,
      "loss": 1.2061,
      "step": 770
    },
    {
      "epoch": 0.09372183838990687,
      "grad_norm": 2.949331045150757,
      "learning_rate": 2.918090795769834e-05,
      "loss": 1.2166,
      "step": 780
    },
    {
      "epoch": 0.09492340042054671,
      "grad_norm": 2.9333202838897705,
      "learning_rate": 2.9168844746471513e-05,
      "loss": 1.1967,
      "step": 790
    },
    {
      "epoch": 0.09612496245118654,
      "grad_norm": 3.382840871810913,
      "learning_rate": 2.9156781535244683e-05,
      "loss": 1.1799,
      "step": 800
    },
    {
      "epoch": 0.09732652448182637,
      "grad_norm": 3.3575687408447266,
      "learning_rate": 2.9144718324017853e-05,
      "loss": 1.2483,
      "step": 810
    },
    {
      "epoch": 0.09852808651246621,
      "grad_norm": 3.4066593647003174,
      "learning_rate": 2.9132655112791026e-05,
      "loss": 1.1892,
      "step": 820
    },
    {
      "epoch": 0.09972964854310604,
      "grad_norm": 3.9565327167510986,
      "learning_rate": 2.9120591901564196e-05,
      "loss": 1.2188,
      "step": 830
    },
    {
      "epoch": 0.10093121057374586,
      "grad_norm": 3.2491378784179688,
      "learning_rate": 2.910852869033737e-05,
      "loss": 1.1815,
      "step": 840
    },
    {
      "epoch": 0.1021327726043857,
      "grad_norm": 4.043277740478516,
      "learning_rate": 2.909646547911054e-05,
      "loss": 1.2275,
      "step": 850
    },
    {
      "epoch": 0.10333433463502553,
      "grad_norm": 4.8370184898376465,
      "learning_rate": 2.908440226788371e-05,
      "loss": 1.1923,
      "step": 860
    },
    {
      "epoch": 0.10453589666566536,
      "grad_norm": 3.4142613410949707,
      "learning_rate": 2.9072339056656885e-05,
      "loss": 1.2439,
      "step": 870
    },
    {
      "epoch": 0.1057374586963052,
      "grad_norm": 4.166114330291748,
      "learning_rate": 2.9060275845430054e-05,
      "loss": 1.2497,
      "step": 880
    },
    {
      "epoch": 0.10693902072694503,
      "grad_norm": 3.095386028289795,
      "learning_rate": 2.9048212634203228e-05,
      "loss": 1.2636,
      "step": 890
    },
    {
      "epoch": 0.10814058275758485,
      "grad_norm": 3.256441116333008,
      "learning_rate": 2.9036149422976397e-05,
      "loss": 1.1719,
      "step": 900
    },
    {
      "epoch": 0.1093421447882247,
      "grad_norm": 3.0590639114379883,
      "learning_rate": 2.9024086211749567e-05,
      "loss": 1.2005,
      "step": 910
    },
    {
      "epoch": 0.11054370681886452,
      "grad_norm": 3.1722512245178223,
      "learning_rate": 2.901202300052274e-05,
      "loss": 1.2189,
      "step": 920
    },
    {
      "epoch": 0.11174526884950435,
      "grad_norm": 4.388335227966309,
      "learning_rate": 2.899995978929591e-05,
      "loss": 1.1695,
      "step": 930
    },
    {
      "epoch": 0.11294683088014419,
      "grad_norm": 5.8408331871032715,
      "learning_rate": 2.8987896578069083e-05,
      "loss": 1.2003,
      "step": 940
    },
    {
      "epoch": 0.11414839291078402,
      "grad_norm": 3.646632671356201,
      "learning_rate": 2.8975833366842256e-05,
      "loss": 1.2215,
      "step": 950
    },
    {
      "epoch": 0.11534995494142385,
      "grad_norm": 3.0727651119232178,
      "learning_rate": 2.8963770155615426e-05,
      "loss": 1.212,
      "step": 960
    },
    {
      "epoch": 0.11655151697206369,
      "grad_norm": 3.4500105381011963,
      "learning_rate": 2.89517069443886e-05,
      "loss": 1.2391,
      "step": 970
    },
    {
      "epoch": 0.11775307900270351,
      "grad_norm": 3.161513090133667,
      "learning_rate": 2.8939643733161766e-05,
      "loss": 1.2325,
      "step": 980
    },
    {
      "epoch": 0.11895464103334334,
      "grad_norm": 3.234539270401001,
      "learning_rate": 2.892758052193494e-05,
      "loss": 1.2082,
      "step": 990
    },
    {
      "epoch": 0.12015620306398318,
      "grad_norm": 2.6466612815856934,
      "learning_rate": 2.8915517310708112e-05,
      "loss": 1.1746,
      "step": 1000
    },
    {
      "epoch": 0.12135776509462301,
      "grad_norm": 2.966912031173706,
      "learning_rate": 2.890345409948128e-05,
      "loss": 1.2131,
      "step": 1010
    },
    {
      "epoch": 0.12255932712526284,
      "grad_norm": 2.5336389541625977,
      "learning_rate": 2.8891390888254455e-05,
      "loss": 1.2083,
      "step": 1020
    },
    {
      "epoch": 0.12376088915590268,
      "grad_norm": 3.3832502365112305,
      "learning_rate": 2.8879327677027624e-05,
      "loss": 1.2468,
      "step": 1030
    },
    {
      "epoch": 0.1249624511865425,
      "grad_norm": 2.8538360595703125,
      "learning_rate": 2.8867264465800798e-05,
      "loss": 1.2255,
      "step": 1040
    },
    {
      "epoch": 0.12616401321718235,
      "grad_norm": 3.0943760871887207,
      "learning_rate": 2.885520125457397e-05,
      "loss": 1.1775,
      "step": 1050
    },
    {
      "epoch": 0.12736557524782216,
      "grad_norm": 2.550607204437256,
      "learning_rate": 2.8843138043347137e-05,
      "loss": 1.1798,
      "step": 1060
    },
    {
      "epoch": 0.128567137278462,
      "grad_norm": 3.6089370250701904,
      "learning_rate": 2.883107483212031e-05,
      "loss": 1.2546,
      "step": 1070
    },
    {
      "epoch": 0.12976869930910184,
      "grad_norm": 2.9161012172698975,
      "learning_rate": 2.8819011620893483e-05,
      "loss": 1.1751,
      "step": 1080
    },
    {
      "epoch": 0.13097026133974166,
      "grad_norm": 2.5826127529144287,
      "learning_rate": 2.8806948409666653e-05,
      "loss": 1.1625,
      "step": 1090
    },
    {
      "epoch": 0.1321718233703815,
      "grad_norm": 2.8647661209106445,
      "learning_rate": 2.8794885198439826e-05,
      "loss": 1.233,
      "step": 1100
    },
    {
      "epoch": 0.13337338540102134,
      "grad_norm": 3.039463520050049,
      "learning_rate": 2.8782821987212996e-05,
      "loss": 1.2081,
      "step": 1110
    },
    {
      "epoch": 0.13457494743166115,
      "grad_norm": 3.1110379695892334,
      "learning_rate": 2.877075877598617e-05,
      "loss": 1.2133,
      "step": 1120
    },
    {
      "epoch": 0.135776509462301,
      "grad_norm": 2.461698532104492,
      "learning_rate": 2.8758695564759342e-05,
      "loss": 1.1667,
      "step": 1130
    },
    {
      "epoch": 0.13697807149294083,
      "grad_norm": 2.9509119987487793,
      "learning_rate": 2.8746632353532512e-05,
      "loss": 1.1349,
      "step": 1140
    },
    {
      "epoch": 0.13817963352358065,
      "grad_norm": 2.6400034427642822,
      "learning_rate": 2.8734569142305682e-05,
      "loss": 1.15,
      "step": 1150
    },
    {
      "epoch": 0.1393811955542205,
      "grad_norm": 3.4070959091186523,
      "learning_rate": 2.872250593107885e-05,
      "loss": 1.2059,
      "step": 1160
    },
    {
      "epoch": 0.14058275758486033,
      "grad_norm": 3.0602986812591553,
      "learning_rate": 2.8710442719852025e-05,
      "loss": 1.2197,
      "step": 1170
    },
    {
      "epoch": 0.14178431961550014,
      "grad_norm": 3.2459914684295654,
      "learning_rate": 2.8698379508625198e-05,
      "loss": 1.2136,
      "step": 1180
    },
    {
      "epoch": 0.14298588164613998,
      "grad_norm": 2.551804304122925,
      "learning_rate": 2.8686316297398367e-05,
      "loss": 1.2203,
      "step": 1190
    },
    {
      "epoch": 0.14418744367677983,
      "grad_norm": 2.8655574321746826,
      "learning_rate": 2.867425308617154e-05,
      "loss": 1.2448,
      "step": 1200
    },
    {
      "epoch": 0.14538900570741964,
      "grad_norm": 2.906076192855835,
      "learning_rate": 2.866218987494471e-05,
      "loss": 1.2177,
      "step": 1210
    },
    {
      "epoch": 0.14659056773805948,
      "grad_norm": 3.2623329162597656,
      "learning_rate": 2.8650126663717883e-05,
      "loss": 1.219,
      "step": 1220
    },
    {
      "epoch": 0.14779212976869932,
      "grad_norm": 2.949843645095825,
      "learning_rate": 2.8638063452491053e-05,
      "loss": 1.1811,
      "step": 1230
    },
    {
      "epoch": 0.14899369179933913,
      "grad_norm": 3.6116480827331543,
      "learning_rate": 2.8626000241264223e-05,
      "loss": 1.1548,
      "step": 1240
    },
    {
      "epoch": 0.15019525382997898,
      "grad_norm": 2.880375623703003,
      "learning_rate": 2.8613937030037396e-05,
      "loss": 1.2069,
      "step": 1250
    },
    {
      "epoch": 0.15139681586061882,
      "grad_norm": 3.7423837184906006,
      "learning_rate": 2.860187381881057e-05,
      "loss": 1.2235,
      "step": 1260
    },
    {
      "epoch": 0.15259837789125863,
      "grad_norm": 2.6154417991638184,
      "learning_rate": 2.858981060758374e-05,
      "loss": 1.2187,
      "step": 1270
    },
    {
      "epoch": 0.15379993992189847,
      "grad_norm": 3.22594952583313,
      "learning_rate": 2.8577747396356912e-05,
      "loss": 1.1502,
      "step": 1280
    },
    {
      "epoch": 0.1550015019525383,
      "grad_norm": 5.788451194763184,
      "learning_rate": 2.8565684185130082e-05,
      "loss": 1.187,
      "step": 1290
    },
    {
      "epoch": 0.15620306398317813,
      "grad_norm": 2.6365041732788086,
      "learning_rate": 2.8553620973903255e-05,
      "loss": 1.1876,
      "step": 1300
    },
    {
      "epoch": 0.15740462601381797,
      "grad_norm": 3.3341829776763916,
      "learning_rate": 2.8541557762676428e-05,
      "loss": 1.1693,
      "step": 1310
    },
    {
      "epoch": 0.1586061880444578,
      "grad_norm": 2.9476466178894043,
      "learning_rate": 2.8529494551449595e-05,
      "loss": 1.226,
      "step": 1320
    },
    {
      "epoch": 0.15980775007509762,
      "grad_norm": 4.849359035491943,
      "learning_rate": 2.8517431340222768e-05,
      "loss": 1.1834,
      "step": 1330
    },
    {
      "epoch": 0.16100931210573746,
      "grad_norm": 2.8508856296539307,
      "learning_rate": 2.8505368128995937e-05,
      "loss": 1.2427,
      "step": 1340
    },
    {
      "epoch": 0.1622108741363773,
      "grad_norm": 2.43009090423584,
      "learning_rate": 2.849330491776911e-05,
      "loss": 1.2137,
      "step": 1350
    },
    {
      "epoch": 0.16341243616701712,
      "grad_norm": 2.40653920173645,
      "learning_rate": 2.8481241706542284e-05,
      "loss": 1.1844,
      "step": 1360
    },
    {
      "epoch": 0.16461399819765696,
      "grad_norm": 3.6909003257751465,
      "learning_rate": 2.8469178495315453e-05,
      "loss": 1.2023,
      "step": 1370
    },
    {
      "epoch": 0.1658155602282968,
      "grad_norm": 2.804927349090576,
      "learning_rate": 2.8457115284088627e-05,
      "loss": 1.1782,
      "step": 1380
    },
    {
      "epoch": 0.1670171222589366,
      "grad_norm": 2.9070169925689697,
      "learning_rate": 2.84450520728618e-05,
      "loss": 1.1538,
      "step": 1390
    },
    {
      "epoch": 0.16821868428957645,
      "grad_norm": 2.8567826747894287,
      "learning_rate": 2.8432988861634966e-05,
      "loss": 1.2235,
      "step": 1400
    },
    {
      "epoch": 0.1694202463202163,
      "grad_norm": 3.2624688148498535,
      "learning_rate": 2.842092565040814e-05,
      "loss": 1.2026,
      "step": 1410
    },
    {
      "epoch": 0.1706218083508561,
      "grad_norm": 2.99491024017334,
      "learning_rate": 2.840886243918131e-05,
      "loss": 1.1846,
      "step": 1420
    },
    {
      "epoch": 0.17182337038149595,
      "grad_norm": 2.366443634033203,
      "learning_rate": 2.8396799227954482e-05,
      "loss": 1.172,
      "step": 1430
    },
    {
      "epoch": 0.1730249324121358,
      "grad_norm": 2.946413516998291,
      "learning_rate": 2.8384736016727655e-05,
      "loss": 1.1419,
      "step": 1440
    },
    {
      "epoch": 0.1742264944427756,
      "grad_norm": 2.9150121212005615,
      "learning_rate": 2.8372672805500825e-05,
      "loss": 1.1848,
      "step": 1450
    },
    {
      "epoch": 0.17542805647341544,
      "grad_norm": 2.7549824714660645,
      "learning_rate": 2.8360609594273998e-05,
      "loss": 1.1581,
      "step": 1460
    },
    {
      "epoch": 0.17662961850405526,
      "grad_norm": 3.1688966751098633,
      "learning_rate": 2.8348546383047168e-05,
      "loss": 1.1594,
      "step": 1470
    },
    {
      "epoch": 0.1778311805346951,
      "grad_norm": 2.4294025897979736,
      "learning_rate": 2.8336483171820338e-05,
      "loss": 1.205,
      "step": 1480
    },
    {
      "epoch": 0.17903274256533494,
      "grad_norm": 2.9080636501312256,
      "learning_rate": 2.832441996059351e-05,
      "loss": 1.2701,
      "step": 1490
    },
    {
      "epoch": 0.18023430459597475,
      "grad_norm": 2.460056781768799,
      "learning_rate": 2.831235674936668e-05,
      "loss": 1.2018,
      "step": 1500
    },
    {
      "epoch": 0.1814358666266146,
      "grad_norm": 3.058758497238159,
      "learning_rate": 2.8300293538139854e-05,
      "loss": 1.1772,
      "step": 1510
    },
    {
      "epoch": 0.18263742865725444,
      "grad_norm": 2.8518383502960205,
      "learning_rate": 2.8288230326913023e-05,
      "loss": 1.161,
      "step": 1520
    },
    {
      "epoch": 0.18383899068789425,
      "grad_norm": 3.5207529067993164,
      "learning_rate": 2.8276167115686196e-05,
      "loss": 1.1792,
      "step": 1530
    },
    {
      "epoch": 0.1850405527185341,
      "grad_norm": 2.749217987060547,
      "learning_rate": 2.826410390445937e-05,
      "loss": 1.2052,
      "step": 1540
    },
    {
      "epoch": 0.18624211474917393,
      "grad_norm": 2.960780382156372,
      "learning_rate": 2.825204069323254e-05,
      "loss": 1.1742,
      "step": 1550
    },
    {
      "epoch": 0.18744367677981374,
      "grad_norm": 2.463855504989624,
      "learning_rate": 2.8239977482005712e-05,
      "loss": 1.0981,
      "step": 1560
    },
    {
      "epoch": 0.18864523881045359,
      "grad_norm": 2.5534300804138184,
      "learning_rate": 2.8227914270778882e-05,
      "loss": 1.1919,
      "step": 1570
    },
    {
      "epoch": 0.18984680084109343,
      "grad_norm": 2.643510580062866,
      "learning_rate": 2.8215851059552052e-05,
      "loss": 1.1903,
      "step": 1580
    },
    {
      "epoch": 0.19104836287173324,
      "grad_norm": 2.3711495399475098,
      "learning_rate": 2.8203787848325225e-05,
      "loss": 1.2049,
      "step": 1590
    },
    {
      "epoch": 0.19224992490237308,
      "grad_norm": 2.6091389656066895,
      "learning_rate": 2.8191724637098395e-05,
      "loss": 1.1897,
      "step": 1600
    },
    {
      "epoch": 0.19345148693301292,
      "grad_norm": 2.267965078353882,
      "learning_rate": 2.8179661425871568e-05,
      "loss": 1.2032,
      "step": 1610
    },
    {
      "epoch": 0.19465304896365274,
      "grad_norm": 3.0370497703552246,
      "learning_rate": 2.816759821464474e-05,
      "loss": 1.1742,
      "step": 1620
    },
    {
      "epoch": 0.19585461099429258,
      "grad_norm": 2.857623815536499,
      "learning_rate": 2.815553500341791e-05,
      "loss": 1.1898,
      "step": 1630
    },
    {
      "epoch": 0.19705617302493242,
      "grad_norm": 2.7295572757720947,
      "learning_rate": 2.8143471792191084e-05,
      "loss": 1.1957,
      "step": 1640
    },
    {
      "epoch": 0.19825773505557223,
      "grad_norm": 2.7341458797454834,
      "learning_rate": 2.813140858096425e-05,
      "loss": 1.2563,
      "step": 1650
    },
    {
      "epoch": 0.19945929708621207,
      "grad_norm": 2.5540027618408203,
      "learning_rate": 2.8119345369737424e-05,
      "loss": 1.2411,
      "step": 1660
    },
    {
      "epoch": 0.20066085911685191,
      "grad_norm": 2.6415350437164307,
      "learning_rate": 2.8107282158510597e-05,
      "loss": 1.2291,
      "step": 1670
    },
    {
      "epoch": 0.20186242114749173,
      "grad_norm": 2.197789430618286,
      "learning_rate": 2.8095218947283766e-05,
      "loss": 1.1639,
      "step": 1680
    },
    {
      "epoch": 0.20306398317813157,
      "grad_norm": 3.4279544353485107,
      "learning_rate": 2.808315573605694e-05,
      "loss": 1.1548,
      "step": 1690
    },
    {
      "epoch": 0.2042655452087714,
      "grad_norm": 3.2731311321258545,
      "learning_rate": 2.807109252483011e-05,
      "loss": 1.1671,
      "step": 1700
    },
    {
      "epoch": 0.20546710723941122,
      "grad_norm": 3.0241477489471436,
      "learning_rate": 2.8059029313603282e-05,
      "loss": 1.2295,
      "step": 1710
    },
    {
      "epoch": 0.20666866927005106,
      "grad_norm": 2.8787293434143066,
      "learning_rate": 2.8046966102376456e-05,
      "loss": 1.1606,
      "step": 1720
    },
    {
      "epoch": 0.2078702313006909,
      "grad_norm": 3.059537410736084,
      "learning_rate": 2.8034902891149625e-05,
      "loss": 1.1597,
      "step": 1730
    },
    {
      "epoch": 0.20907179333133072,
      "grad_norm": 2.4803130626678467,
      "learning_rate": 2.8022839679922795e-05,
      "loss": 1.1717,
      "step": 1740
    },
    {
      "epoch": 0.21027335536197056,
      "grad_norm": 2.743851900100708,
      "learning_rate": 2.8010776468695968e-05,
      "loss": 1.2414,
      "step": 1750
    },
    {
      "epoch": 0.2114749173926104,
      "grad_norm": 2.816011428833008,
      "learning_rate": 2.7998713257469138e-05,
      "loss": 1.1735,
      "step": 1760
    },
    {
      "epoch": 0.21267647942325021,
      "grad_norm": 2.4862968921661377,
      "learning_rate": 2.798665004624231e-05,
      "loss": 1.1807,
      "step": 1770
    },
    {
      "epoch": 0.21387804145389006,
      "grad_norm": 2.628909111022949,
      "learning_rate": 2.797458683501548e-05,
      "loss": 1.2092,
      "step": 1780
    },
    {
      "epoch": 0.2150796034845299,
      "grad_norm": 2.729163885116577,
      "learning_rate": 2.7962523623788654e-05,
      "loss": 1.1511,
      "step": 1790
    },
    {
      "epoch": 0.2162811655151697,
      "grad_norm": 2.8538401126861572,
      "learning_rate": 2.7950460412561827e-05,
      "loss": 1.194,
      "step": 1800
    },
    {
      "epoch": 0.21748272754580955,
      "grad_norm": 3.4018497467041016,
      "learning_rate": 2.7938397201334997e-05,
      "loss": 1.193,
      "step": 1810
    },
    {
      "epoch": 0.2186842895764494,
      "grad_norm": 3.17280912399292,
      "learning_rate": 2.7926333990108167e-05,
      "loss": 1.2676,
      "step": 1820
    },
    {
      "epoch": 0.2198858516070892,
      "grad_norm": 3.207840919494629,
      "learning_rate": 2.7914270778881336e-05,
      "loss": 1.208,
      "step": 1830
    },
    {
      "epoch": 0.22108741363772905,
      "grad_norm": 2.8937325477600098,
      "learning_rate": 2.790220756765451e-05,
      "loss": 1.1933,
      "step": 1840
    },
    {
      "epoch": 0.2222889756683689,
      "grad_norm": 3.1658248901367188,
      "learning_rate": 2.7890144356427683e-05,
      "loss": 1.2114,
      "step": 1850
    },
    {
      "epoch": 0.2234905376990087,
      "grad_norm": 3.345982789993286,
      "learning_rate": 2.7878081145200852e-05,
      "loss": 1.2201,
      "step": 1860
    },
    {
      "epoch": 0.22469209972964854,
      "grad_norm": 2.8375675678253174,
      "learning_rate": 2.7866017933974025e-05,
      "loss": 1.1431,
      "step": 1870
    },
    {
      "epoch": 0.22589366176028838,
      "grad_norm": 3.631347179412842,
      "learning_rate": 2.78539547227472e-05,
      "loss": 1.1145,
      "step": 1880
    },
    {
      "epoch": 0.2270952237909282,
      "grad_norm": 3.0351946353912354,
      "learning_rate": 2.784189151152037e-05,
      "loss": 1.1557,
      "step": 1890
    },
    {
      "epoch": 0.22829678582156804,
      "grad_norm": 3.5695104598999023,
      "learning_rate": 2.7829828300293538e-05,
      "loss": 1.1362,
      "step": 1900
    },
    {
      "epoch": 0.22949834785220788,
      "grad_norm": 2.6372334957122803,
      "learning_rate": 2.7817765089066708e-05,
      "loss": 1.1982,
      "step": 1910
    },
    {
      "epoch": 0.2306999098828477,
      "grad_norm": 3.1410279273986816,
      "learning_rate": 2.780570187783988e-05,
      "loss": 1.1558,
      "step": 1920
    },
    {
      "epoch": 0.23190147191348753,
      "grad_norm": 3.920654058456421,
      "learning_rate": 2.7793638666613054e-05,
      "loss": 1.2074,
      "step": 1930
    },
    {
      "epoch": 0.23310303394412737,
      "grad_norm": 2.59781551361084,
      "learning_rate": 2.7781575455386224e-05,
      "loss": 1.1783,
      "step": 1940
    },
    {
      "epoch": 0.2343045959747672,
      "grad_norm": 4.267780780792236,
      "learning_rate": 2.7769512244159397e-05,
      "loss": 1.1513,
      "step": 1950
    },
    {
      "epoch": 0.23550615800540703,
      "grad_norm": 2.80067777633667,
      "learning_rate": 2.7757449032932567e-05,
      "loss": 1.2198,
      "step": 1960
    },
    {
      "epoch": 0.23670772003604687,
      "grad_norm": 2.755800724029541,
      "learning_rate": 2.774538582170574e-05,
      "loss": 1.1969,
      "step": 1970
    },
    {
      "epoch": 0.23790928206668668,
      "grad_norm": 3.179060220718384,
      "learning_rate": 2.7733322610478913e-05,
      "loss": 1.2191,
      "step": 1980
    },
    {
      "epoch": 0.23911084409732652,
      "grad_norm": 2.535529136657715,
      "learning_rate": 2.772125939925208e-05,
      "loss": 1.1922,
      "step": 1990
    },
    {
      "epoch": 0.24031240612796637,
      "grad_norm": 2.7243857383728027,
      "learning_rate": 2.7709196188025252e-05,
      "loss": 1.1866,
      "step": 2000
    },
    {
      "epoch": 0.24151396815860618,
      "grad_norm": 2.6123416423797607,
      "learning_rate": 2.7697132976798422e-05,
      "loss": 1.207,
      "step": 2010
    },
    {
      "epoch": 0.24271553018924602,
      "grad_norm": 3.054924488067627,
      "learning_rate": 2.7685069765571595e-05,
      "loss": 1.1988,
      "step": 2020
    },
    {
      "epoch": 0.24391709221988586,
      "grad_norm": 1.9436805248260498,
      "learning_rate": 2.767300655434477e-05,
      "loss": 1.1297,
      "step": 2030
    },
    {
      "epoch": 0.24511865425052567,
      "grad_norm": 2.296905994415283,
      "learning_rate": 2.7660943343117938e-05,
      "loss": 1.1874,
      "step": 2040
    },
    {
      "epoch": 0.24632021628116552,
      "grad_norm": 3.0049889087677,
      "learning_rate": 2.764888013189111e-05,
      "loss": 1.1836,
      "step": 2050
    },
    {
      "epoch": 0.24752177831180536,
      "grad_norm": 2.960667848587036,
      "learning_rate": 2.7636816920664285e-05,
      "loss": 1.1857,
      "step": 2060
    },
    {
      "epoch": 0.24872334034244517,
      "grad_norm": 2.313141345977783,
      "learning_rate": 2.762475370943745e-05,
      "loss": 1.1567,
      "step": 2070
    },
    {
      "epoch": 0.249924902373085,
      "grad_norm": 2.4359686374664307,
      "learning_rate": 2.7612690498210624e-05,
      "loss": 1.1559,
      "step": 2080
    },
    {
      "epoch": 0.2511264644037248,
      "grad_norm": 2.9015355110168457,
      "learning_rate": 2.7600627286983794e-05,
      "loss": 1.2312,
      "step": 2090
    },
    {
      "epoch": 0.2523280264343647,
      "grad_norm": 3.0771589279174805,
      "learning_rate": 2.7588564075756967e-05,
      "loss": 1.1576,
      "step": 2100
    },
    {
      "epoch": 0.2535295884650045,
      "grad_norm": 2.688138723373413,
      "learning_rate": 2.757650086453014e-05,
      "loss": 1.1723,
      "step": 2110
    },
    {
      "epoch": 0.2547311504956443,
      "grad_norm": 2.489461660385132,
      "learning_rate": 2.756443765330331e-05,
      "loss": 1.111,
      "step": 2120
    },
    {
      "epoch": 0.2559327125262842,
      "grad_norm": 2.837310314178467,
      "learning_rate": 2.7552374442076483e-05,
      "loss": 1.1916,
      "step": 2130
    },
    {
      "epoch": 0.257134274556924,
      "grad_norm": 2.630139112472534,
      "learning_rate": 2.7540311230849653e-05,
      "loss": 1.2193,
      "step": 2140
    },
    {
      "epoch": 0.2583358365875638,
      "grad_norm": 2.851454257965088,
      "learning_rate": 2.7528248019622826e-05,
      "loss": 1.1639,
      "step": 2150
    },
    {
      "epoch": 0.2595373986182037,
      "grad_norm": 2.4561660289764404,
      "learning_rate": 2.7516184808395996e-05,
      "loss": 1.1967,
      "step": 2160
    },
    {
      "epoch": 0.2607389606488435,
      "grad_norm": 2.4668755531311035,
      "learning_rate": 2.7504121597169165e-05,
      "loss": 1.1454,
      "step": 2170
    },
    {
      "epoch": 0.2619405226794833,
      "grad_norm": 2.3372578620910645,
      "learning_rate": 2.749205838594234e-05,
      "loss": 1.1631,
      "step": 2180
    },
    {
      "epoch": 0.2631420847101232,
      "grad_norm": 3.340115547180176,
      "learning_rate": 2.7479995174715508e-05,
      "loss": 1.1876,
      "step": 2190
    },
    {
      "epoch": 0.264343646740763,
      "grad_norm": 2.5998709201812744,
      "learning_rate": 2.746793196348868e-05,
      "loss": 1.1426,
      "step": 2200
    },
    {
      "epoch": 0.2655452087714028,
      "grad_norm": 2.3356263637542725,
      "learning_rate": 2.7455868752261854e-05,
      "loss": 1.0929,
      "step": 2210
    },
    {
      "epoch": 0.2667467708020427,
      "grad_norm": 2.527843952178955,
      "learning_rate": 2.7443805541035024e-05,
      "loss": 1.2443,
      "step": 2220
    },
    {
      "epoch": 0.2679483328326825,
      "grad_norm": 2.3537957668304443,
      "learning_rate": 2.7431742329808197e-05,
      "loss": 1.1988,
      "step": 2230
    },
    {
      "epoch": 0.2691498948633223,
      "grad_norm": 2.308087110519409,
      "learning_rate": 2.7419679118581367e-05,
      "loss": 1.1461,
      "step": 2240
    },
    {
      "epoch": 0.27035145689396217,
      "grad_norm": 2.4159464836120605,
      "learning_rate": 2.7407615907354537e-05,
      "loss": 1.1433,
      "step": 2250
    },
    {
      "epoch": 0.271553018924602,
      "grad_norm": 3.2510910034179688,
      "learning_rate": 2.739555269612771e-05,
      "loss": 1.1798,
      "step": 2260
    },
    {
      "epoch": 0.2727545809552418,
      "grad_norm": 2.501601219177246,
      "learning_rate": 2.738348948490088e-05,
      "loss": 1.1737,
      "step": 2270
    },
    {
      "epoch": 0.27395614298588167,
      "grad_norm": 3.848269462585449,
      "learning_rate": 2.7371426273674053e-05,
      "loss": 1.1663,
      "step": 2280
    },
    {
      "epoch": 0.2751577050165215,
      "grad_norm": 2.619987964630127,
      "learning_rate": 2.7359363062447226e-05,
      "loss": 1.1828,
      "step": 2290
    },
    {
      "epoch": 0.2763592670471613,
      "grad_norm": 2.764699697494507,
      "learning_rate": 2.7347299851220396e-05,
      "loss": 1.12,
      "step": 2300
    },
    {
      "epoch": 0.27756082907780116,
      "grad_norm": 2.4959328174591064,
      "learning_rate": 2.733523663999357e-05,
      "loss": 1.1197,
      "step": 2310
    },
    {
      "epoch": 0.278762391108441,
      "grad_norm": 2.4761157035827637,
      "learning_rate": 2.7323173428766735e-05,
      "loss": 1.1606,
      "step": 2320
    },
    {
      "epoch": 0.2799639531390808,
      "grad_norm": 2.5800445079803467,
      "learning_rate": 2.731111021753991e-05,
      "loss": 1.1667,
      "step": 2330
    },
    {
      "epoch": 0.28116551516972066,
      "grad_norm": 2.9445812702178955,
      "learning_rate": 2.729904700631308e-05,
      "loss": 1.1761,
      "step": 2340
    },
    {
      "epoch": 0.2823670772003605,
      "grad_norm": 2.5938570499420166,
      "learning_rate": 2.728698379508625e-05,
      "loss": 1.2196,
      "step": 2350
    },
    {
      "epoch": 0.2835686392310003,
      "grad_norm": 2.825988531112671,
      "learning_rate": 2.7274920583859424e-05,
      "loss": 1.1667,
      "step": 2360
    },
    {
      "epoch": 0.28477020126164015,
      "grad_norm": 2.681251287460327,
      "learning_rate": 2.7262857372632594e-05,
      "loss": 1.1393,
      "step": 2370
    },
    {
      "epoch": 0.28597176329227997,
      "grad_norm": 2.6024885177612305,
      "learning_rate": 2.7250794161405767e-05,
      "loss": 1.158,
      "step": 2380
    },
    {
      "epoch": 0.2871733253229198,
      "grad_norm": 2.547234296798706,
      "learning_rate": 2.723873095017894e-05,
      "loss": 1.185,
      "step": 2390
    },
    {
      "epoch": 0.28837488735355965,
      "grad_norm": 3.145181179046631,
      "learning_rate": 2.722666773895211e-05,
      "loss": 1.1806,
      "step": 2400
    },
    {
      "epoch": 0.28957644938419946,
      "grad_norm": 2.2221016883850098,
      "learning_rate": 2.721460452772528e-05,
      "loss": 1.1269,
      "step": 2410
    },
    {
      "epoch": 0.2907780114148393,
      "grad_norm": 2.2296900749206543,
      "learning_rate": 2.7202541316498453e-05,
      "loss": 1.15,
      "step": 2420
    },
    {
      "epoch": 0.29197957344547915,
      "grad_norm": 2.249767303466797,
      "learning_rate": 2.7190478105271623e-05,
      "loss": 1.1685,
      "step": 2430
    },
    {
      "epoch": 0.29318113547611896,
      "grad_norm": 2.6831214427948,
      "learning_rate": 2.7178414894044796e-05,
      "loss": 1.187,
      "step": 2440
    },
    {
      "epoch": 0.2943826975067588,
      "grad_norm": 2.846402883529663,
      "learning_rate": 2.7166351682817966e-05,
      "loss": 1.1756,
      "step": 2450
    },
    {
      "epoch": 0.29558425953739864,
      "grad_norm": 2.581502914428711,
      "learning_rate": 2.715428847159114e-05,
      "loss": 1.1689,
      "step": 2460
    },
    {
      "epoch": 0.29678582156803845,
      "grad_norm": 2.7768871784210205,
      "learning_rate": 2.7142225260364312e-05,
      "loss": 1.1794,
      "step": 2470
    },
    {
      "epoch": 0.29798738359867827,
      "grad_norm": 2.5009520053863525,
      "learning_rate": 2.713016204913748e-05,
      "loss": 1.1488,
      "step": 2480
    },
    {
      "epoch": 0.29918894562931814,
      "grad_norm": 2.4609227180480957,
      "learning_rate": 2.711809883791065e-05,
      "loss": 1.2016,
      "step": 2490
    },
    {
      "epoch": 0.30039050765995795,
      "grad_norm": 3.00813364982605,
      "learning_rate": 2.710603562668382e-05,
      "loss": 1.1596,
      "step": 2500
    },
    {
      "epoch": 0.30159206969059776,
      "grad_norm": 2.564207077026367,
      "learning_rate": 2.7093972415456994e-05,
      "loss": 1.1801,
      "step": 2510
    },
    {
      "epoch": 0.30279363172123763,
      "grad_norm": 3.6703505516052246,
      "learning_rate": 2.7081909204230167e-05,
      "loss": 1.1394,
      "step": 2520
    },
    {
      "epoch": 0.30399519375187745,
      "grad_norm": 2.3859546184539795,
      "learning_rate": 2.7069845993003337e-05,
      "loss": 1.1619,
      "step": 2530
    },
    {
      "epoch": 0.30519675578251726,
      "grad_norm": 2.9602625370025635,
      "learning_rate": 2.705778278177651e-05,
      "loss": 1.1518,
      "step": 2540
    },
    {
      "epoch": 0.30639831781315713,
      "grad_norm": 2.3583881855010986,
      "learning_rate": 2.7045719570549683e-05,
      "loss": 1.1578,
      "step": 2550
    },
    {
      "epoch": 0.30759987984379694,
      "grad_norm": 3.303779125213623,
      "learning_rate": 2.7033656359322853e-05,
      "loss": 1.2497,
      "step": 2560
    },
    {
      "epoch": 0.30880144187443676,
      "grad_norm": 2.5511584281921387,
      "learning_rate": 2.7021593148096026e-05,
      "loss": 1.2175,
      "step": 2570
    },
    {
      "epoch": 0.3100030039050766,
      "grad_norm": 2.2171859741210938,
      "learning_rate": 2.7009529936869193e-05,
      "loss": 1.1957,
      "step": 2580
    },
    {
      "epoch": 0.31120456593571644,
      "grad_norm": 2.8237626552581787,
      "learning_rate": 2.6997466725642366e-05,
      "loss": 1.1572,
      "step": 2590
    },
    {
      "epoch": 0.31240612796635625,
      "grad_norm": 2.755048990249634,
      "learning_rate": 2.698540351441554e-05,
      "loss": 1.1905,
      "step": 2600
    },
    {
      "epoch": 0.3136076899969961,
      "grad_norm": 2.5845956802368164,
      "learning_rate": 2.697334030318871e-05,
      "loss": 1.1824,
      "step": 2610
    },
    {
      "epoch": 0.31480925202763593,
      "grad_norm": 3.0854880809783936,
      "learning_rate": 2.6961277091961882e-05,
      "loss": 1.2115,
      "step": 2620
    },
    {
      "epoch": 0.31601081405827575,
      "grad_norm": 2.554464817047119,
      "learning_rate": 2.694921388073505e-05,
      "loss": 1.1892,
      "step": 2630
    },
    {
      "epoch": 0.3172123760889156,
      "grad_norm": 2.310736656188965,
      "learning_rate": 2.6937150669508225e-05,
      "loss": 1.1222,
      "step": 2640
    },
    {
      "epoch": 0.31841393811955543,
      "grad_norm": 3.66450834274292,
      "learning_rate": 2.6925087458281398e-05,
      "loss": 1.1387,
      "step": 2650
    },
    {
      "epoch": 0.31961550015019524,
      "grad_norm": 2.206603527069092,
      "learning_rate": 2.6913024247054564e-05,
      "loss": 1.109,
      "step": 2660
    },
    {
      "epoch": 0.3208170621808351,
      "grad_norm": 2.1975295543670654,
      "learning_rate": 2.6900961035827737e-05,
      "loss": 1.1893,
      "step": 2670
    },
    {
      "epoch": 0.3220186242114749,
      "grad_norm": 2.3061599731445312,
      "learning_rate": 2.6888897824600907e-05,
      "loss": 1.2132,
      "step": 2680
    },
    {
      "epoch": 0.32322018624211474,
      "grad_norm": 2.5791015625,
      "learning_rate": 2.687683461337408e-05,
      "loss": 1.1155,
      "step": 2690
    },
    {
      "epoch": 0.3244217482727546,
      "grad_norm": 2.503556728363037,
      "learning_rate": 2.6864771402147253e-05,
      "loss": 1.2057,
      "step": 2700
    },
    {
      "epoch": 0.3256233103033944,
      "grad_norm": 3.4364686012268066,
      "learning_rate": 2.6852708190920423e-05,
      "loss": 1.2025,
      "step": 2710
    },
    {
      "epoch": 0.32682487233403423,
      "grad_norm": 2.691314220428467,
      "learning_rate": 2.6840644979693596e-05,
      "loss": 1.1707,
      "step": 2720
    },
    {
      "epoch": 0.3280264343646741,
      "grad_norm": 2.754074811935425,
      "learning_rate": 2.682858176846677e-05,
      "loss": 1.1625,
      "step": 2730
    },
    {
      "epoch": 0.3292279963953139,
      "grad_norm": 2.7829160690307617,
      "learning_rate": 2.681651855723994e-05,
      "loss": 1.195,
      "step": 2740
    },
    {
      "epoch": 0.33042955842595373,
      "grad_norm": 3.2662620544433594,
      "learning_rate": 2.680445534601311e-05,
      "loss": 1.2095,
      "step": 2750
    },
    {
      "epoch": 0.3316311204565936,
      "grad_norm": 2.7985446453094482,
      "learning_rate": 2.679239213478628e-05,
      "loss": 1.2114,
      "step": 2760
    },
    {
      "epoch": 0.3328326824872334,
      "grad_norm": 3.06706166267395,
      "learning_rate": 2.6780328923559452e-05,
      "loss": 1.1116,
      "step": 2770
    },
    {
      "epoch": 0.3340342445178732,
      "grad_norm": 2.8466103076934814,
      "learning_rate": 2.6768265712332625e-05,
      "loss": 1.1242,
      "step": 2780
    },
    {
      "epoch": 0.3352358065485131,
      "grad_norm": 2.49790096282959,
      "learning_rate": 2.6756202501105795e-05,
      "loss": 1.2121,
      "step": 2790
    },
    {
      "epoch": 0.3364373685791529,
      "grad_norm": 2.5057241916656494,
      "learning_rate": 2.6744139289878968e-05,
      "loss": 1.1708,
      "step": 2800
    },
    {
      "epoch": 0.3376389306097927,
      "grad_norm": 2.4394233226776123,
      "learning_rate": 2.6732076078652138e-05,
      "loss": 1.2173,
      "step": 2810
    },
    {
      "epoch": 0.3388404926404326,
      "grad_norm": 2.4308788776397705,
      "learning_rate": 2.672001286742531e-05,
      "loss": 1.086,
      "step": 2820
    },
    {
      "epoch": 0.3400420546710724,
      "grad_norm": 6.097671031951904,
      "learning_rate": 2.670794965619848e-05,
      "loss": 1.1963,
      "step": 2830
    },
    {
      "epoch": 0.3412436167017122,
      "grad_norm": 3.087080478668213,
      "learning_rate": 2.669588644497165e-05,
      "loss": 1.2049,
      "step": 2840
    },
    {
      "epoch": 0.3424451787323521,
      "grad_norm": 3.50563383102417,
      "learning_rate": 2.6683823233744823e-05,
      "loss": 1.1854,
      "step": 2850
    },
    {
      "epoch": 0.3436467407629919,
      "grad_norm": 2.320648431777954,
      "learning_rate": 2.6671760022517993e-05,
      "loss": 1.15,
      "step": 2860
    },
    {
      "epoch": 0.3448483027936317,
      "grad_norm": 2.5600950717926025,
      "learning_rate": 2.6659696811291166e-05,
      "loss": 1.151,
      "step": 2870
    },
    {
      "epoch": 0.3460498648242716,
      "grad_norm": 2.9433765411376953,
      "learning_rate": 2.664763360006434e-05,
      "loss": 1.1429,
      "step": 2880
    },
    {
      "epoch": 0.3472514268549114,
      "grad_norm": 2.299943208694458,
      "learning_rate": 2.663557038883751e-05,
      "loss": 1.1636,
      "step": 2890
    },
    {
      "epoch": 0.3484529888855512,
      "grad_norm": 2.7463300228118896,
      "learning_rate": 2.6623507177610682e-05,
      "loss": 1.1471,
      "step": 2900
    },
    {
      "epoch": 0.3496545509161911,
      "grad_norm": 3.0321006774902344,
      "learning_rate": 2.6611443966383852e-05,
      "loss": 1.187,
      "step": 2910
    },
    {
      "epoch": 0.3508561129468309,
      "grad_norm": 2.6772866249084473,
      "learning_rate": 2.659938075515702e-05,
      "loss": 1.1464,
      "step": 2920
    },
    {
      "epoch": 0.3520576749774707,
      "grad_norm": 2.2189528942108154,
      "learning_rate": 2.6587317543930195e-05,
      "loss": 1.2026,
      "step": 2930
    },
    {
      "epoch": 0.3532592370081105,
      "grad_norm": 2.4382407665252686,
      "learning_rate": 2.6575254332703365e-05,
      "loss": 1.1984,
      "step": 2940
    },
    {
      "epoch": 0.3544607990387504,
      "grad_norm": 2.680574655532837,
      "learning_rate": 2.6563191121476538e-05,
      "loss": 1.1294,
      "step": 2950
    },
    {
      "epoch": 0.3556623610693902,
      "grad_norm": 2.475632429122925,
      "learning_rate": 2.655112791024971e-05,
      "loss": 1.1865,
      "step": 2960
    },
    {
      "epoch": 0.35686392310003,
      "grad_norm": 3.8841450214385986,
      "learning_rate": 2.653906469902288e-05,
      "loss": 1.2068,
      "step": 2970
    },
    {
      "epoch": 0.3580654851306699,
      "grad_norm": 2.2667977809906006,
      "learning_rate": 2.6527001487796054e-05,
      "loss": 1.2069,
      "step": 2980
    },
    {
      "epoch": 0.3592670471613097,
      "grad_norm": 2.710280656814575,
      "learning_rate": 2.6514938276569223e-05,
      "loss": 1.1282,
      "step": 2990
    },
    {
      "epoch": 0.3604686091919495,
      "grad_norm": 2.5306618213653564,
      "learning_rate": 2.6502875065342393e-05,
      "loss": 1.1966,
      "step": 3000
    },
    {
      "epoch": 0.3616701712225894,
      "grad_norm": 2.1139681339263916,
      "learning_rate": 2.6490811854115566e-05,
      "loss": 1.1479,
      "step": 3010
    },
    {
      "epoch": 0.3628717332532292,
      "grad_norm": 2.6616036891937256,
      "learning_rate": 2.6478748642888736e-05,
      "loss": 1.129,
      "step": 3020
    },
    {
      "epoch": 0.364073295283869,
      "grad_norm": 2.770655393600464,
      "learning_rate": 2.646668543166191e-05,
      "loss": 1.1553,
      "step": 3030
    },
    {
      "epoch": 0.36527485731450887,
      "grad_norm": 3.1836113929748535,
      "learning_rate": 2.6454622220435082e-05,
      "loss": 1.1771,
      "step": 3040
    },
    {
      "epoch": 0.3664764193451487,
      "grad_norm": 2.573970079421997,
      "learning_rate": 2.6442559009208252e-05,
      "loss": 1.1723,
      "step": 3050
    },
    {
      "epoch": 0.3676779813757885,
      "grad_norm": 2.490741729736328,
      "learning_rate": 2.6430495797981425e-05,
      "loss": 1.1897,
      "step": 3060
    },
    {
      "epoch": 0.36887954340642837,
      "grad_norm": 1.985977053642273,
      "learning_rate": 2.6418432586754595e-05,
      "loss": 1.1157,
      "step": 3070
    },
    {
      "epoch": 0.3700811054370682,
      "grad_norm": 2.83701753616333,
      "learning_rate": 2.6406369375527765e-05,
      "loss": 1.1837,
      "step": 3080
    },
    {
      "epoch": 0.371282667467708,
      "grad_norm": 2.323976993560791,
      "learning_rate": 2.6394306164300938e-05,
      "loss": 1.2156,
      "step": 3090
    },
    {
      "epoch": 0.37248422949834786,
      "grad_norm": 2.856289863586426,
      "learning_rate": 2.6382242953074108e-05,
      "loss": 1.1827,
      "step": 3100
    },
    {
      "epoch": 0.3736857915289877,
      "grad_norm": 2.788370132446289,
      "learning_rate": 2.637017974184728e-05,
      "loss": 1.1219,
      "step": 3110
    },
    {
      "epoch": 0.3748873535596275,
      "grad_norm": 2.566585063934326,
      "learning_rate": 2.635811653062045e-05,
      "loss": 1.2247,
      "step": 3120
    },
    {
      "epoch": 0.37608891559026736,
      "grad_norm": 2.0449984073638916,
      "learning_rate": 2.6346053319393624e-05,
      "loss": 1.1399,
      "step": 3130
    },
    {
      "epoch": 0.37729047762090717,
      "grad_norm": 3.092155933380127,
      "learning_rate": 2.6333990108166797e-05,
      "loss": 1.1758,
      "step": 3140
    },
    {
      "epoch": 0.378492039651547,
      "grad_norm": 2.5616164207458496,
      "learning_rate": 2.6321926896939967e-05,
      "loss": 1.1953,
      "step": 3150
    },
    {
      "epoch": 0.37969360168218685,
      "grad_norm": 2.4971368312835693,
      "learning_rate": 2.630986368571314e-05,
      "loss": 1.17,
      "step": 3160
    },
    {
      "epoch": 0.38089516371282667,
      "grad_norm": 2.4396374225616455,
      "learning_rate": 2.6297800474486306e-05,
      "loss": 1.2126,
      "step": 3170
    },
    {
      "epoch": 0.3820967257434665,
      "grad_norm": 2.8524017333984375,
      "learning_rate": 2.628573726325948e-05,
      "loss": 1.1375,
      "step": 3180
    },
    {
      "epoch": 0.38329828777410635,
      "grad_norm": 2.739401340484619,
      "learning_rate": 2.6273674052032652e-05,
      "loss": 1.1617,
      "step": 3190
    },
    {
      "epoch": 0.38449984980474616,
      "grad_norm": 2.254767894744873,
      "learning_rate": 2.6261610840805822e-05,
      "loss": 1.1372,
      "step": 3200
    },
    {
      "epoch": 0.385701411835386,
      "grad_norm": 2.6338188648223877,
      "learning_rate": 2.6249547629578995e-05,
      "loss": 1.153,
      "step": 3210
    },
    {
      "epoch": 0.38690297386602585,
      "grad_norm": 2.301142454147339,
      "learning_rate": 2.6237484418352168e-05,
      "loss": 1.1709,
      "step": 3220
    },
    {
      "epoch": 0.38810453589666566,
      "grad_norm": 2.4393510818481445,
      "learning_rate": 2.6225421207125338e-05,
      "loss": 1.1833,
      "step": 3230
    },
    {
      "epoch": 0.3893060979273055,
      "grad_norm": 2.209959030151367,
      "learning_rate": 2.621335799589851e-05,
      "loss": 1.2005,
      "step": 3240
    },
    {
      "epoch": 0.39050765995794534,
      "grad_norm": 3.2370293140411377,
      "learning_rate": 2.6201294784671678e-05,
      "loss": 1.2131,
      "step": 3250
    },
    {
      "epoch": 0.39170922198858515,
      "grad_norm": 2.8269691467285156,
      "learning_rate": 2.618923157344485e-05,
      "loss": 1.1586,
      "step": 3260
    },
    {
      "epoch": 0.39291078401922497,
      "grad_norm": 3.059093475341797,
      "learning_rate": 2.6177168362218024e-05,
      "loss": 1.1606,
      "step": 3270
    },
    {
      "epoch": 0.39411234604986484,
      "grad_norm": 2.470789909362793,
      "learning_rate": 2.6165105150991194e-05,
      "loss": 1.174,
      "step": 3280
    },
    {
      "epoch": 0.39531390808050465,
      "grad_norm": 2.3620760440826416,
      "learning_rate": 2.6153041939764367e-05,
      "loss": 1.1695,
      "step": 3290
    },
    {
      "epoch": 0.39651547011114446,
      "grad_norm": 2.544787645339966,
      "learning_rate": 2.6140978728537536e-05,
      "loss": 1.1223,
      "step": 3300
    },
    {
      "epoch": 0.39771703214178433,
      "grad_norm": 2.97009539604187,
      "learning_rate": 2.612891551731071e-05,
      "loss": 1.1609,
      "step": 3310
    },
    {
      "epoch": 0.39891859417242415,
      "grad_norm": 2.5772500038146973,
      "learning_rate": 2.6116852306083883e-05,
      "loss": 1.1666,
      "step": 3320
    },
    {
      "epoch": 0.40012015620306396,
      "grad_norm": 2.3862617015838623,
      "learning_rate": 2.610478909485705e-05,
      "loss": 1.1757,
      "step": 3330
    },
    {
      "epoch": 0.40132171823370383,
      "grad_norm": 2.4083220958709717,
      "learning_rate": 2.6092725883630222e-05,
      "loss": 1.221,
      "step": 3340
    },
    {
      "epoch": 0.40252328026434364,
      "grad_norm": 2.6823177337646484,
      "learning_rate": 2.6080662672403392e-05,
      "loss": 1.1902,
      "step": 3350
    },
    {
      "epoch": 0.40372484229498345,
      "grad_norm": 2.6381373405456543,
      "learning_rate": 2.6068599461176565e-05,
      "loss": 1.1733,
      "step": 3360
    },
    {
      "epoch": 0.4049264043256233,
      "grad_norm": 3.1154611110687256,
      "learning_rate": 2.6056536249949738e-05,
      "loss": 1.1739,
      "step": 3370
    },
    {
      "epoch": 0.40612796635626314,
      "grad_norm": 2.675461769104004,
      "learning_rate": 2.6044473038722908e-05,
      "loss": 1.1488,
      "step": 3380
    },
    {
      "epoch": 0.40732952838690295,
      "grad_norm": 2.7295000553131104,
      "learning_rate": 2.603240982749608e-05,
      "loss": 1.2084,
      "step": 3390
    },
    {
      "epoch": 0.4085310904175428,
      "grad_norm": 3.016747236251831,
      "learning_rate": 2.6020346616269254e-05,
      "loss": 1.1767,
      "step": 3400
    },
    {
      "epoch": 0.40973265244818263,
      "grad_norm": 2.621757984161377,
      "learning_rate": 2.6008283405042424e-05,
      "loss": 1.1378,
      "step": 3410
    },
    {
      "epoch": 0.41093421447882245,
      "grad_norm": 2.5112743377685547,
      "learning_rate": 2.5996220193815594e-05,
      "loss": 1.1887,
      "step": 3420
    },
    {
      "epoch": 0.4121357765094623,
      "grad_norm": 3.046820878982544,
      "learning_rate": 2.5984156982588763e-05,
      "loss": 1.1201,
      "step": 3430
    },
    {
      "epoch": 0.41333733854010213,
      "grad_norm": 2.809202194213867,
      "learning_rate": 2.5972093771361937e-05,
      "loss": 1.1848,
      "step": 3440
    },
    {
      "epoch": 0.41453890057074194,
      "grad_norm": 2.58705472946167,
      "learning_rate": 2.596003056013511e-05,
      "loss": 1.12,
      "step": 3450
    },
    {
      "epoch": 0.4157404626013818,
      "grad_norm": 2.564190149307251,
      "learning_rate": 2.594796734890828e-05,
      "loss": 1.1835,
      "step": 3460
    },
    {
      "epoch": 0.4169420246320216,
      "grad_norm": 2.5979440212249756,
      "learning_rate": 2.5935904137681453e-05,
      "loss": 1.166,
      "step": 3470
    },
    {
      "epoch": 0.41814358666266144,
      "grad_norm": 2.7621450424194336,
      "learning_rate": 2.5923840926454622e-05,
      "loss": 1.2178,
      "step": 3480
    },
    {
      "epoch": 0.4193451486933013,
      "grad_norm": 2.09158992767334,
      "learning_rate": 2.5911777715227796e-05,
      "loss": 1.1653,
      "step": 3490
    },
    {
      "epoch": 0.4205467107239411,
      "grad_norm": 2.40000057220459,
      "learning_rate": 2.5899714504000965e-05,
      "loss": 1.139,
      "step": 3500
    },
    {
      "epoch": 0.42174827275458093,
      "grad_norm": 2.290113687515259,
      "learning_rate": 2.5887651292774135e-05,
      "loss": 1.1419,
      "step": 3510
    },
    {
      "epoch": 0.4229498347852208,
      "grad_norm": 2.814410924911499,
      "learning_rate": 2.5875588081547308e-05,
      "loss": 1.1635,
      "step": 3520
    },
    {
      "epoch": 0.4241513968158606,
      "grad_norm": 2.767165422439575,
      "learning_rate": 2.586352487032048e-05,
      "loss": 1.2027,
      "step": 3530
    },
    {
      "epoch": 0.42535295884650043,
      "grad_norm": 2.782895088195801,
      "learning_rate": 2.585146165909365e-05,
      "loss": 1.1658,
      "step": 3540
    },
    {
      "epoch": 0.4265545208771403,
      "grad_norm": 2.6985952854156494,
      "learning_rate": 2.5839398447866824e-05,
      "loss": 1.0642,
      "step": 3550
    },
    {
      "epoch": 0.4277560829077801,
      "grad_norm": 3.179030656814575,
      "learning_rate": 2.5827335236639994e-05,
      "loss": 1.1412,
      "step": 3560
    },
    {
      "epoch": 0.4289576449384199,
      "grad_norm": 2.730319023132324,
      "learning_rate": 2.5815272025413167e-05,
      "loss": 1.1303,
      "step": 3570
    },
    {
      "epoch": 0.4301592069690598,
      "grad_norm": 2.4229230880737305,
      "learning_rate": 2.580320881418634e-05,
      "loss": 1.1416,
      "step": 3580
    },
    {
      "epoch": 0.4313607689996996,
      "grad_norm": 2.2854442596435547,
      "learning_rate": 2.5791145602959507e-05,
      "loss": 1.1272,
      "step": 3590
    },
    {
      "epoch": 0.4325623310303394,
      "grad_norm": 2.629950761795044,
      "learning_rate": 2.577908239173268e-05,
      "loss": 1.1948,
      "step": 3600
    },
    {
      "epoch": 0.4337638930609793,
      "grad_norm": 2.1668951511383057,
      "learning_rate": 2.576701918050585e-05,
      "loss": 1.1303,
      "step": 3610
    },
    {
      "epoch": 0.4349654550916191,
      "grad_norm": 2.441757917404175,
      "learning_rate": 2.5754955969279023e-05,
      "loss": 1.1756,
      "step": 3620
    },
    {
      "epoch": 0.4361670171222589,
      "grad_norm": 4.084231853485107,
      "learning_rate": 2.5742892758052196e-05,
      "loss": 1.1087,
      "step": 3630
    },
    {
      "epoch": 0.4373685791528988,
      "grad_norm": 2.581449270248413,
      "learning_rate": 2.5730829546825365e-05,
      "loss": 1.1912,
      "step": 3640
    },
    {
      "epoch": 0.4385701411835386,
      "grad_norm": 3.6685373783111572,
      "learning_rate": 2.571876633559854e-05,
      "loss": 1.1482,
      "step": 3650
    },
    {
      "epoch": 0.4397717032141784,
      "grad_norm": 2.7895071506500244,
      "learning_rate": 2.570670312437171e-05,
      "loss": 1.1894,
      "step": 3660
    },
    {
      "epoch": 0.4409732652448183,
      "grad_norm": 2.4775915145874023,
      "learning_rate": 2.5694639913144878e-05,
      "loss": 1.1644,
      "step": 3670
    },
    {
      "epoch": 0.4421748272754581,
      "grad_norm": 2.5614724159240723,
      "learning_rate": 2.568257670191805e-05,
      "loss": 1.1774,
      "step": 3680
    },
    {
      "epoch": 0.4433763893060979,
      "grad_norm": 2.7605714797973633,
      "learning_rate": 2.567051349069122e-05,
      "loss": 1.1705,
      "step": 3690
    },
    {
      "epoch": 0.4445779513367378,
      "grad_norm": 2.526151180267334,
      "learning_rate": 2.5658450279464394e-05,
      "loss": 1.1202,
      "step": 3700
    },
    {
      "epoch": 0.4457795133673776,
      "grad_norm": 2.6185169219970703,
      "learning_rate": 2.5646387068237567e-05,
      "loss": 1.1612,
      "step": 3710
    },
    {
      "epoch": 0.4469810753980174,
      "grad_norm": 3.695657730102539,
      "learning_rate": 2.5634323857010737e-05,
      "loss": 1.1322,
      "step": 3720
    },
    {
      "epoch": 0.44818263742865727,
      "grad_norm": 2.384298086166382,
      "learning_rate": 2.562226064578391e-05,
      "loss": 1.1198,
      "step": 3730
    },
    {
      "epoch": 0.4493841994592971,
      "grad_norm": 2.5260584354400635,
      "learning_rate": 2.561019743455708e-05,
      "loss": 1.1828,
      "step": 3740
    },
    {
      "epoch": 0.4505857614899369,
      "grad_norm": 2.439030170440674,
      "learning_rate": 2.559813422333025e-05,
      "loss": 1.1642,
      "step": 3750
    },
    {
      "epoch": 0.45178732352057677,
      "grad_norm": 2.440866470336914,
      "learning_rate": 2.5586071012103423e-05,
      "loss": 1.1231,
      "step": 3760
    },
    {
      "epoch": 0.4529888855512166,
      "grad_norm": 2.400704860687256,
      "learning_rate": 2.5574007800876592e-05,
      "loss": 1.1341,
      "step": 3770
    },
    {
      "epoch": 0.4541904475818564,
      "grad_norm": 2.489985704421997,
      "learning_rate": 2.5561944589649766e-05,
      "loss": 1.1368,
      "step": 3780
    },
    {
      "epoch": 0.45539200961249626,
      "grad_norm": 2.31006121635437,
      "learning_rate": 2.5549881378422935e-05,
      "loss": 1.1647,
      "step": 3790
    },
    {
      "epoch": 0.4565935716431361,
      "grad_norm": 2.592113971710205,
      "learning_rate": 2.553781816719611e-05,
      "loss": 1.1609,
      "step": 3800
    },
    {
      "epoch": 0.4577951336737759,
      "grad_norm": 2.7729032039642334,
      "learning_rate": 2.552575495596928e-05,
      "loss": 1.1412,
      "step": 3810
    },
    {
      "epoch": 0.45899669570441576,
      "grad_norm": 3.5841283798217773,
      "learning_rate": 2.551369174474245e-05,
      "loss": 1.1632,
      "step": 3820
    },
    {
      "epoch": 0.46019825773505557,
      "grad_norm": 3.0016796588897705,
      "learning_rate": 2.5501628533515625e-05,
      "loss": 1.1495,
      "step": 3830
    },
    {
      "epoch": 0.4613998197656954,
      "grad_norm": 3.1837971210479736,
      "learning_rate": 2.548956532228879e-05,
      "loss": 1.2384,
      "step": 3840
    },
    {
      "epoch": 0.46260138179633525,
      "grad_norm": 2.3891234397888184,
      "learning_rate": 2.5477502111061964e-05,
      "loss": 1.1426,
      "step": 3850
    },
    {
      "epoch": 0.46380294382697507,
      "grad_norm": 2.5318522453308105,
      "learning_rate": 2.5465438899835137e-05,
      "loss": 1.1628,
      "step": 3860
    },
    {
      "epoch": 0.4650045058576149,
      "grad_norm": 2.782796621322632,
      "learning_rate": 2.5453375688608307e-05,
      "loss": 1.1833,
      "step": 3870
    },
    {
      "epoch": 0.46620606788825475,
      "grad_norm": 2.339977264404297,
      "learning_rate": 2.544131247738148e-05,
      "loss": 1.1563,
      "step": 3880
    },
    {
      "epoch": 0.46740762991889456,
      "grad_norm": 2.512321710586548,
      "learning_rate": 2.5429249266154653e-05,
      "loss": 1.1394,
      "step": 3890
    },
    {
      "epoch": 0.4686091919495344,
      "grad_norm": 2.5941567420959473,
      "learning_rate": 2.5417186054927823e-05,
      "loss": 1.1152,
      "step": 3900
    },
    {
      "epoch": 0.46981075398017424,
      "grad_norm": 2.457409620285034,
      "learning_rate": 2.5405122843700996e-05,
      "loss": 1.1905,
      "step": 3910
    },
    {
      "epoch": 0.47101231601081406,
      "grad_norm": 2.653008460998535,
      "learning_rate": 2.5393059632474162e-05,
      "loss": 1.1549,
      "step": 3920
    },
    {
      "epoch": 0.47221387804145387,
      "grad_norm": 2.6714980602264404,
      "learning_rate": 2.5380996421247336e-05,
      "loss": 1.1663,
      "step": 3930
    },
    {
      "epoch": 0.47341544007209374,
      "grad_norm": 2.2187397480010986,
      "learning_rate": 2.536893321002051e-05,
      "loss": 1.1292,
      "step": 3940
    },
    {
      "epoch": 0.47461700210273355,
      "grad_norm": 2.588024854660034,
      "learning_rate": 2.535686999879368e-05,
      "loss": 1.1308,
      "step": 3950
    },
    {
      "epoch": 0.47581856413337337,
      "grad_norm": 2.4186534881591797,
      "learning_rate": 2.534480678756685e-05,
      "loss": 1.1767,
      "step": 3960
    },
    {
      "epoch": 0.47702012616401324,
      "grad_norm": 2.7175254821777344,
      "learning_rate": 2.533274357634002e-05,
      "loss": 1.1446,
      "step": 3970
    },
    {
      "epoch": 0.47822168819465305,
      "grad_norm": 2.8743996620178223,
      "learning_rate": 2.5320680365113194e-05,
      "loss": 1.1776,
      "step": 3980
    },
    {
      "epoch": 0.47942325022529286,
      "grad_norm": 2.439943313598633,
      "learning_rate": 2.5308617153886368e-05,
      "loss": 1.1654,
      "step": 3990
    },
    {
      "epoch": 0.48062481225593273,
      "grad_norm": 2.798753499984741,
      "learning_rate": 2.5296553942659537e-05,
      "loss": 1.101,
      "step": 4000
    },
    {
      "epoch": 0.48182637428657255,
      "grad_norm": 2.6302642822265625,
      "learning_rate": 2.5284490731432707e-05,
      "loss": 1.212,
      "step": 4010
    },
    {
      "epoch": 0.48302793631721236,
      "grad_norm": 2.362274169921875,
      "learning_rate": 2.527242752020588e-05,
      "loss": 1.1925,
      "step": 4020
    },
    {
      "epoch": 0.4842294983478522,
      "grad_norm": 2.3818252086639404,
      "learning_rate": 2.526036430897905e-05,
      "loss": 1.1824,
      "step": 4030
    },
    {
      "epoch": 0.48543106037849204,
      "grad_norm": 2.644052028656006,
      "learning_rate": 2.5248301097752223e-05,
      "loss": 1.1173,
      "step": 4040
    },
    {
      "epoch": 0.48663262240913185,
      "grad_norm": 2.719715118408203,
      "learning_rate": 2.5236237886525393e-05,
      "loss": 1.1203,
      "step": 4050
    },
    {
      "epoch": 0.4878341844397717,
      "grad_norm": 3.4996893405914307,
      "learning_rate": 2.5224174675298566e-05,
      "loss": 1.1627,
      "step": 4060
    },
    {
      "epoch": 0.48903574647041154,
      "grad_norm": 3.056471109390259,
      "learning_rate": 2.521211146407174e-05,
      "loss": 1.1019,
      "step": 4070
    },
    {
      "epoch": 0.49023730850105135,
      "grad_norm": 2.3430051803588867,
      "learning_rate": 2.520004825284491e-05,
      "loss": 1.182,
      "step": 4080
    },
    {
      "epoch": 0.4914388705316912,
      "grad_norm": 3.0053961277008057,
      "learning_rate": 2.518798504161808e-05,
      "loss": 1.1958,
      "step": 4090
    },
    {
      "epoch": 0.49264043256233103,
      "grad_norm": 2.4208974838256836,
      "learning_rate": 2.517592183039125e-05,
      "loss": 1.142,
      "step": 4100
    },
    {
      "epoch": 0.49384199459297085,
      "grad_norm": 2.5967540740966797,
      "learning_rate": 2.5165064940287106e-05,
      "loss": 1.2002,
      "step": 4110
    },
    {
      "epoch": 0.4950435566236107,
      "grad_norm": 2.027489423751831,
      "learning_rate": 2.5153001729060276e-05,
      "loss": 1.1606,
      "step": 4120
    },
    {
      "epoch": 0.4962451186542505,
      "grad_norm": 2.6000053882598877,
      "learning_rate": 2.514093851783345e-05,
      "loss": 1.1387,
      "step": 4130
    },
    {
      "epoch": 0.49744668068489034,
      "grad_norm": 2.801210641860962,
      "learning_rate": 2.512887530660662e-05,
      "loss": 1.124,
      "step": 4140
    },
    {
      "epoch": 0.4986482427155302,
      "grad_norm": 3.529576301574707,
      "learning_rate": 2.5116812095379792e-05,
      "loss": 1.1177,
      "step": 4150
    },
    {
      "epoch": 0.49984980474617,
      "grad_norm": 2.995048999786377,
      "learning_rate": 2.5104748884152965e-05,
      "loss": 1.1986,
      "step": 4160
    },
    {
      "epoch": 0.5010513667768098,
      "grad_norm": 2.505927562713623,
      "learning_rate": 2.509268567292613e-05,
      "loss": 1.161,
      "step": 4170
    },
    {
      "epoch": 0.5022529288074497,
      "grad_norm": 2.541262626647949,
      "learning_rate": 2.5080622461699305e-05,
      "loss": 1.1876,
      "step": 4180
    },
    {
      "epoch": 0.5034544908380895,
      "grad_norm": 2.7592356204986572,
      "learning_rate": 2.5068559250472474e-05,
      "loss": 1.1824,
      "step": 4190
    },
    {
      "epoch": 0.5046560528687294,
      "grad_norm": 2.351583242416382,
      "learning_rate": 2.5056496039245647e-05,
      "loss": 1.1448,
      "step": 4200
    },
    {
      "epoch": 0.5058576148993692,
      "grad_norm": 2.2015700340270996,
      "learning_rate": 2.504443282801882e-05,
      "loss": 1.1554,
      "step": 4210
    },
    {
      "epoch": 0.507059176930009,
      "grad_norm": 2.7327158451080322,
      "learning_rate": 2.503236961679199e-05,
      "loss": 1.1361,
      "step": 4220
    },
    {
      "epoch": 0.5082607389606488,
      "grad_norm": 3.2227470874786377,
      "learning_rate": 2.5020306405565163e-05,
      "loss": 1.1182,
      "step": 4230
    },
    {
      "epoch": 0.5094623009912886,
      "grad_norm": 2.9800126552581787,
      "learning_rate": 2.5008243194338337e-05,
      "loss": 1.1414,
      "step": 4240
    },
    {
      "epoch": 0.5106638630219285,
      "grad_norm": 3.5223405361175537,
      "learning_rate": 2.4996179983111503e-05,
      "loss": 1.1571,
      "step": 4250
    },
    {
      "epoch": 0.5118654250525684,
      "grad_norm": 2.8903491497039795,
      "learning_rate": 2.4984116771884676e-05,
      "loss": 1.2178,
      "step": 4260
    },
    {
      "epoch": 0.5130669870832082,
      "grad_norm": 2.457393169403076,
      "learning_rate": 2.4972053560657846e-05,
      "loss": 1.1375,
      "step": 4270
    },
    {
      "epoch": 0.514268549113848,
      "grad_norm": 2.7290701866149902,
      "learning_rate": 2.495999034943102e-05,
      "loss": 1.1605,
      "step": 4280
    },
    {
      "epoch": 0.5154701111444878,
      "grad_norm": 3.0812013149261475,
      "learning_rate": 2.4947927138204192e-05,
      "loss": 1.2007,
      "step": 4290
    },
    {
      "epoch": 0.5166716731751276,
      "grad_norm": 2.4477851390838623,
      "learning_rate": 2.4935863926977362e-05,
      "loss": 1.1328,
      "step": 4300
    },
    {
      "epoch": 0.5178732352057674,
      "grad_norm": 2.2844908237457275,
      "learning_rate": 2.4923800715750535e-05,
      "loss": 1.2051,
      "step": 4310
    },
    {
      "epoch": 0.5190747972364074,
      "grad_norm": 2.4927315711975098,
      "learning_rate": 2.4911737504523705e-05,
      "loss": 1.1623,
      "step": 4320
    },
    {
      "epoch": 0.5202763592670472,
      "grad_norm": 2.338674783706665,
      "learning_rate": 2.4899674293296875e-05,
      "loss": 1.1537,
      "step": 4330
    },
    {
      "epoch": 0.521477921297687,
      "grad_norm": 2.3393192291259766,
      "learning_rate": 2.4887611082070048e-05,
      "loss": 1.1143,
      "step": 4340
    },
    {
      "epoch": 0.5226794833283268,
      "grad_norm": 2.619077682495117,
      "learning_rate": 2.4875547870843217e-05,
      "loss": 1.1749,
      "step": 4350
    },
    {
      "epoch": 0.5238810453589666,
      "grad_norm": 2.4565470218658447,
      "learning_rate": 2.486348465961639e-05,
      "loss": 1.1549,
      "step": 4360
    },
    {
      "epoch": 0.5250826073896064,
      "grad_norm": 2.5559957027435303,
      "learning_rate": 2.485142144838956e-05,
      "loss": 1.1565,
      "step": 4370
    },
    {
      "epoch": 0.5262841694202464,
      "grad_norm": 2.411228656768799,
      "learning_rate": 2.4839358237162733e-05,
      "loss": 1.1121,
      "step": 4380
    },
    {
      "epoch": 0.5274857314508862,
      "grad_norm": 2.553987741470337,
      "learning_rate": 2.4827295025935907e-05,
      "loss": 1.1706,
      "step": 4390
    },
    {
      "epoch": 0.528687293481526,
      "grad_norm": 2.41792368888855,
      "learning_rate": 2.4815231814709076e-05,
      "loss": 1.1271,
      "step": 4400
    },
    {
      "epoch": 0.5298888555121658,
      "grad_norm": 2.573080062866211,
      "learning_rate": 2.480316860348225e-05,
      "loss": 1.1519,
      "step": 4410
    },
    {
      "epoch": 0.5310904175428056,
      "grad_norm": 2.6448354721069336,
      "learning_rate": 2.479110539225542e-05,
      "loss": 1.1402,
      "step": 4420
    },
    {
      "epoch": 0.5322919795734454,
      "grad_norm": 2.444575309753418,
      "learning_rate": 2.477904218102859e-05,
      "loss": 1.1645,
      "step": 4430
    },
    {
      "epoch": 0.5334935416040854,
      "grad_norm": 2.589184284210205,
      "learning_rate": 2.4766978969801762e-05,
      "loss": 1.1667,
      "step": 4440
    },
    {
      "epoch": 0.5346951036347252,
      "grad_norm": 2.284700393676758,
      "learning_rate": 2.4754915758574932e-05,
      "loss": 1.1029,
      "step": 4450
    },
    {
      "epoch": 0.535896665665365,
      "grad_norm": 2.175442695617676,
      "learning_rate": 2.4742852547348105e-05,
      "loss": 1.1733,
      "step": 4460
    },
    {
      "epoch": 0.5370982276960048,
      "grad_norm": 3.369086503982544,
      "learning_rate": 2.4730789336121278e-05,
      "loss": 1.172,
      "step": 4470
    },
    {
      "epoch": 0.5382997897266446,
      "grad_norm": 2.491457939147949,
      "learning_rate": 2.4718726124894448e-05,
      "loss": 1.1939,
      "step": 4480
    },
    {
      "epoch": 0.5395013517572844,
      "grad_norm": 2.6749889850616455,
      "learning_rate": 2.470666291366762e-05,
      "loss": 1.1542,
      "step": 4490
    },
    {
      "epoch": 0.5407029137879243,
      "grad_norm": 2.718701124191284,
      "learning_rate": 2.4694599702440787e-05,
      "loss": 1.1606,
      "step": 4500
    },
    {
      "epoch": 0.5419044758185642,
      "grad_norm": 3.3786513805389404,
      "learning_rate": 2.468253649121396e-05,
      "loss": 1.1754,
      "step": 4510
    },
    {
      "epoch": 0.543106037849204,
      "grad_norm": 2.790289878845215,
      "learning_rate": 2.4670473279987134e-05,
      "loss": 1.103,
      "step": 4520
    },
    {
      "epoch": 0.5443075998798438,
      "grad_norm": 2.72505784034729,
      "learning_rate": 2.4658410068760303e-05,
      "loss": 1.1409,
      "step": 4530
    },
    {
      "epoch": 0.5455091619104836,
      "grad_norm": 2.2227959632873535,
      "learning_rate": 2.4646346857533476e-05,
      "loss": 1.1477,
      "step": 4540
    },
    {
      "epoch": 0.5467107239411234,
      "grad_norm": 2.3343920707702637,
      "learning_rate": 2.4634283646306646e-05,
      "loss": 1.1253,
      "step": 4550
    },
    {
      "epoch": 0.5479122859717633,
      "grad_norm": 2.355476140975952,
      "learning_rate": 2.462222043507982e-05,
      "loss": 1.1387,
      "step": 4560
    },
    {
      "epoch": 0.5491138480024031,
      "grad_norm": 2.545687675476074,
      "learning_rate": 2.4610157223852992e-05,
      "loss": 1.1128,
      "step": 4570
    },
    {
      "epoch": 0.550315410033043,
      "grad_norm": 2.936713695526123,
      "learning_rate": 2.4598094012626162e-05,
      "loss": 1.1536,
      "step": 4580
    },
    {
      "epoch": 0.5515169720636828,
      "grad_norm": 2.6547296047210693,
      "learning_rate": 2.4586030801399332e-05,
      "loss": 1.1453,
      "step": 4590
    },
    {
      "epoch": 0.5527185340943226,
      "grad_norm": 2.198305606842041,
      "learning_rate": 2.4573967590172505e-05,
      "loss": 1.169,
      "step": 4600
    },
    {
      "epoch": 0.5539200961249624,
      "grad_norm": 3.041977643966675,
      "learning_rate": 2.4561904378945675e-05,
      "loss": 1.1947,
      "step": 4610
    },
    {
      "epoch": 0.5551216581556023,
      "grad_norm": 2.5743961334228516,
      "learning_rate": 2.4549841167718848e-05,
      "loss": 1.1543,
      "step": 4620
    },
    {
      "epoch": 0.5563232201862421,
      "grad_norm": 3.3884503841400146,
      "learning_rate": 2.4537777956492018e-05,
      "loss": 1.1406,
      "step": 4630
    },
    {
      "epoch": 0.557524782216882,
      "grad_norm": 2.500394821166992,
      "learning_rate": 2.452571474526519e-05,
      "loss": 1.1895,
      "step": 4640
    },
    {
      "epoch": 0.5587263442475218,
      "grad_norm": 2.287407398223877,
      "learning_rate": 2.4513651534038364e-05,
      "loss": 1.1333,
      "step": 4650
    },
    {
      "epoch": 0.5599279062781616,
      "grad_norm": 2.6707680225372314,
      "learning_rate": 2.4501588322811534e-05,
      "loss": 1.0779,
      "step": 4660
    },
    {
      "epoch": 0.5611294683088014,
      "grad_norm": 2.786595582962036,
      "learning_rate": 2.4489525111584703e-05,
      "loss": 1.1939,
      "step": 4670
    },
    {
      "epoch": 0.5623310303394413,
      "grad_norm": 2.475229501724243,
      "learning_rate": 2.4477461900357873e-05,
      "loss": 1.1314,
      "step": 4680
    },
    {
      "epoch": 0.5635325923700811,
      "grad_norm": 2.7979164123535156,
      "learning_rate": 2.4465398689131046e-05,
      "loss": 1.1514,
      "step": 4690
    },
    {
      "epoch": 0.564734154400721,
      "grad_norm": 2.597417116165161,
      "learning_rate": 2.445333547790422e-05,
      "loss": 1.1875,
      "step": 4700
    },
    {
      "epoch": 0.5659357164313608,
      "grad_norm": 2.499419689178467,
      "learning_rate": 2.444127226667739e-05,
      "loss": 1.1429,
      "step": 4710
    },
    {
      "epoch": 0.5671372784620006,
      "grad_norm": 3.3924782276153564,
      "learning_rate": 2.4429209055450562e-05,
      "loss": 1.1274,
      "step": 4720
    },
    {
      "epoch": 0.5683388404926404,
      "grad_norm": 2.1915953159332275,
      "learning_rate": 2.4417145844223736e-05,
      "loss": 1.0988,
      "step": 4730
    },
    {
      "epoch": 0.5695404025232803,
      "grad_norm": 2.556109666824341,
      "learning_rate": 2.4405082632996905e-05,
      "loss": 1.1305,
      "step": 4740
    },
    {
      "epoch": 0.5707419645539201,
      "grad_norm": 2.795656204223633,
      "learning_rate": 2.4393019421770075e-05,
      "loss": 1.1481,
      "step": 4750
    },
    {
      "epoch": 0.5719435265845599,
      "grad_norm": 3.0178792476654053,
      "learning_rate": 2.4380956210543245e-05,
      "loss": 1.1973,
      "step": 4760
    },
    {
      "epoch": 0.5731450886151997,
      "grad_norm": 2.7570087909698486,
      "learning_rate": 2.4368892999316418e-05,
      "loss": 1.204,
      "step": 4770
    },
    {
      "epoch": 0.5743466506458396,
      "grad_norm": 2.5224640369415283,
      "learning_rate": 2.435682978808959e-05,
      "loss": 1.1477,
      "step": 4780
    },
    {
      "epoch": 0.5755482126764794,
      "grad_norm": 3.951967716217041,
      "learning_rate": 2.434476657686276e-05,
      "loss": 1.178,
      "step": 4790
    },
    {
      "epoch": 0.5767497747071193,
      "grad_norm": 2.588291883468628,
      "learning_rate": 2.4332703365635934e-05,
      "loss": 1.1138,
      "step": 4800
    },
    {
      "epoch": 0.5779513367377591,
      "grad_norm": 2.425629138946533,
      "learning_rate": 2.4320640154409104e-05,
      "loss": 1.1093,
      "step": 4810
    },
    {
      "epoch": 0.5791528987683989,
      "grad_norm": 2.976696729660034,
      "learning_rate": 2.4308576943182277e-05,
      "loss": 1.1154,
      "step": 4820
    },
    {
      "epoch": 0.5803544607990387,
      "grad_norm": 3.2839536666870117,
      "learning_rate": 2.429651373195545e-05,
      "loss": 1.0969,
      "step": 4830
    },
    {
      "epoch": 0.5815560228296786,
      "grad_norm": 3.0509984493255615,
      "learning_rate": 2.4284450520728616e-05,
      "loss": 1.1387,
      "step": 4840
    },
    {
      "epoch": 0.5827575848603184,
      "grad_norm": 2.2361979484558105,
      "learning_rate": 2.427238730950179e-05,
      "loss": 1.1597,
      "step": 4850
    },
    {
      "epoch": 0.5839591468909583,
      "grad_norm": 2.366654634475708,
      "learning_rate": 2.426032409827496e-05,
      "loss": 1.1571,
      "step": 4860
    },
    {
      "epoch": 0.5851607089215981,
      "grad_norm": 2.59304141998291,
      "learning_rate": 2.4248260887048132e-05,
      "loss": 1.1253,
      "step": 4870
    },
    {
      "epoch": 0.5863622709522379,
      "grad_norm": 3.2065203189849854,
      "learning_rate": 2.4236197675821305e-05,
      "loss": 1.17,
      "step": 4880
    },
    {
      "epoch": 0.5875638329828777,
      "grad_norm": 3.0151443481445312,
      "learning_rate": 2.4224134464594475e-05,
      "loss": 1.1455,
      "step": 4890
    },
    {
      "epoch": 0.5887653950135175,
      "grad_norm": 2.400662660598755,
      "learning_rate": 2.421207125336765e-05,
      "loss": 1.1378,
      "step": 4900
    },
    {
      "epoch": 0.5899669570441574,
      "grad_norm": 2.700183391571045,
      "learning_rate": 2.420000804214082e-05,
      "loss": 1.1562,
      "step": 4910
    },
    {
      "epoch": 0.5911685190747973,
      "grad_norm": 3.0833218097686768,
      "learning_rate": 2.4187944830913988e-05,
      "loss": 1.0984,
      "step": 4920
    },
    {
      "epoch": 0.5923700811054371,
      "grad_norm": 2.8082447052001953,
      "learning_rate": 2.417588161968716e-05,
      "loss": 1.1581,
      "step": 4930
    },
    {
      "epoch": 0.5935716431360769,
      "grad_norm": 2.8131773471832275,
      "learning_rate": 2.416381840846033e-05,
      "loss": 1.1657,
      "step": 4940
    },
    {
      "epoch": 0.5947732051667167,
      "grad_norm": 2.799870014190674,
      "learning_rate": 2.4151755197233504e-05,
      "loss": 1.1636,
      "step": 4950
    },
    {
      "epoch": 0.5959747671973565,
      "grad_norm": 2.5447628498077393,
      "learning_rate": 2.4139691986006677e-05,
      "loss": 1.1217,
      "step": 4960
    },
    {
      "epoch": 0.5971763292279963,
      "grad_norm": 2.700284242630005,
      "learning_rate": 2.4127628774779847e-05,
      "loss": 1.1334,
      "step": 4970
    },
    {
      "epoch": 0.5983778912586363,
      "grad_norm": 2.6620919704437256,
      "learning_rate": 2.411556556355302e-05,
      "loss": 1.1221,
      "step": 4980
    },
    {
      "epoch": 0.5995794532892761,
      "grad_norm": 2.4893596172332764,
      "learning_rate": 2.410350235232619e-05,
      "loss": 1.1464,
      "step": 4990
    },
    {
      "epoch": 0.6007810153199159,
      "grad_norm": 2.7123429775238037,
      "learning_rate": 2.4092645462222044e-05,
      "loss": 1.2009,
      "step": 5000
    },
    {
      "epoch": 0.6019825773505557,
      "grad_norm": 2.2703514099121094,
      "learning_rate": 2.4080582250995214e-05,
      "loss": 1.1325,
      "step": 5010
    },
    {
      "epoch": 0.6031841393811955,
      "grad_norm": 3.3896377086639404,
      "learning_rate": 2.4068519039768387e-05,
      "loss": 1.1497,
      "step": 5020
    },
    {
      "epoch": 0.6043857014118353,
      "grad_norm": 2.4854581356048584,
      "learning_rate": 2.4056455828541557e-05,
      "loss": 1.1542,
      "step": 5030
    },
    {
      "epoch": 0.6055872634424753,
      "grad_norm": 2.376967191696167,
      "learning_rate": 2.404439261731473e-05,
      "loss": 1.157,
      "step": 5040
    },
    {
      "epoch": 0.6067888254731151,
      "grad_norm": 2.473101854324341,
      "learning_rate": 2.4032329406087903e-05,
      "loss": 1.1554,
      "step": 5050
    },
    {
      "epoch": 0.6079903875037549,
      "grad_norm": 2.7951877117156982,
      "learning_rate": 2.4020266194861073e-05,
      "loss": 1.1452,
      "step": 5060
    },
    {
      "epoch": 0.6091919495343947,
      "grad_norm": 2.1529009342193604,
      "learning_rate": 2.4008202983634246e-05,
      "loss": 1.1454,
      "step": 5070
    },
    {
      "epoch": 0.6103935115650345,
      "grad_norm": 2.4755818843841553,
      "learning_rate": 2.3996139772407412e-05,
      "loss": 1.153,
      "step": 5080
    },
    {
      "epoch": 0.6115950735956743,
      "grad_norm": 2.6557559967041016,
      "learning_rate": 2.3984076561180585e-05,
      "loss": 1.1691,
      "step": 5090
    },
    {
      "epoch": 0.6127966356263143,
      "grad_norm": 2.31557297706604,
      "learning_rate": 2.397201334995376e-05,
      "loss": 1.2023,
      "step": 5100
    },
    {
      "epoch": 0.6139981976569541,
      "grad_norm": 2.2819228172302246,
      "learning_rate": 2.3959950138726928e-05,
      "loss": 1.1573,
      "step": 5110
    },
    {
      "epoch": 0.6151997596875939,
      "grad_norm": 3.8943841457366943,
      "learning_rate": 2.39478869275001e-05,
      "loss": 1.126,
      "step": 5120
    },
    {
      "epoch": 0.6164013217182337,
      "grad_norm": 2.352203130722046,
      "learning_rate": 2.3935823716273275e-05,
      "loss": 1.096,
      "step": 5130
    },
    {
      "epoch": 0.6176028837488735,
      "grad_norm": 2.4595351219177246,
      "learning_rate": 2.3923760505046444e-05,
      "loss": 1.1673,
      "step": 5140
    },
    {
      "epoch": 0.6188044457795133,
      "grad_norm": 2.2002391815185547,
      "learning_rate": 2.3911697293819617e-05,
      "loss": 1.1571,
      "step": 5150
    },
    {
      "epoch": 0.6200060078101532,
      "grad_norm": 2.8571255207061768,
      "learning_rate": 2.3899634082592787e-05,
      "loss": 1.1105,
      "step": 5160
    },
    {
      "epoch": 0.6212075698407931,
      "grad_norm": 3.2751028537750244,
      "learning_rate": 2.3887570871365957e-05,
      "loss": 1.1182,
      "step": 5170
    },
    {
      "epoch": 0.6224091318714329,
      "grad_norm": 2.7479801177978516,
      "learning_rate": 2.387550766013913e-05,
      "loss": 1.1458,
      "step": 5180
    },
    {
      "epoch": 0.6236106939020727,
      "grad_norm": 2.68717360496521,
      "learning_rate": 2.38634444489123e-05,
      "loss": 1.1501,
      "step": 5190
    },
    {
      "epoch": 0.6248122559327125,
      "grad_norm": 2.3024680614471436,
      "learning_rate": 2.3851381237685473e-05,
      "loss": 1.1431,
      "step": 5200
    },
    {
      "epoch": 0.6260138179633523,
      "grad_norm": 2.458082914352417,
      "learning_rate": 2.3839318026458643e-05,
      "loss": 1.1825,
      "step": 5210
    },
    {
      "epoch": 0.6272153799939922,
      "grad_norm": 2.505681037902832,
      "learning_rate": 2.3827254815231816e-05,
      "loss": 1.1774,
      "step": 5220
    },
    {
      "epoch": 0.628416942024632,
      "grad_norm": 2.395306348800659,
      "learning_rate": 2.381519160400499e-05,
      "loss": 1.1362,
      "step": 5230
    },
    {
      "epoch": 0.6296185040552719,
      "grad_norm": 2.4142038822174072,
      "learning_rate": 2.380312839277816e-05,
      "loss": 1.1626,
      "step": 5240
    },
    {
      "epoch": 0.6308200660859117,
      "grad_norm": 3.2651619911193848,
      "learning_rate": 2.379106518155133e-05,
      "loss": 1.1582,
      "step": 5250
    },
    {
      "epoch": 0.6320216281165515,
      "grad_norm": 2.8457653522491455,
      "learning_rate": 2.37790019703245e-05,
      "loss": 1.1532,
      "step": 5260
    },
    {
      "epoch": 0.6332231901471913,
      "grad_norm": 2.687645196914673,
      "learning_rate": 2.376693875909767e-05,
      "loss": 1.1298,
      "step": 5270
    },
    {
      "epoch": 0.6344247521778312,
      "grad_norm": 2.408106565475464,
      "learning_rate": 2.3754875547870844e-05,
      "loss": 1.0988,
      "step": 5280
    },
    {
      "epoch": 0.635626314208471,
      "grad_norm": 2.7752366065979004,
      "learning_rate": 2.3742812336644014e-05,
      "loss": 1.1389,
      "step": 5290
    },
    {
      "epoch": 0.6368278762391109,
      "grad_norm": 2.7382261753082275,
      "learning_rate": 2.3730749125417187e-05,
      "loss": 1.0949,
      "step": 5300
    },
    {
      "epoch": 0.6380294382697507,
      "grad_norm": 2.5384273529052734,
      "learning_rate": 2.371868591419036e-05,
      "loss": 1.1388,
      "step": 5310
    },
    {
      "epoch": 0.6392310003003905,
      "grad_norm": 2.648148536682129,
      "learning_rate": 2.370662270296353e-05,
      "loss": 1.1166,
      "step": 5320
    },
    {
      "epoch": 0.6404325623310303,
      "grad_norm": 2.437873125076294,
      "learning_rate": 2.3694559491736703e-05,
      "loss": 1.1446,
      "step": 5330
    },
    {
      "epoch": 0.6416341243616702,
      "grad_norm": 2.383296012878418,
      "learning_rate": 2.368249628050987e-05,
      "loss": 1.2119,
      "step": 5340
    },
    {
      "epoch": 0.64283568639231,
      "grad_norm": 2.367547035217285,
      "learning_rate": 2.3670433069283043e-05,
      "loss": 1.1658,
      "step": 5350
    },
    {
      "epoch": 0.6440372484229498,
      "grad_norm": 2.492396593093872,
      "learning_rate": 2.3658369858056216e-05,
      "loss": 1.1344,
      "step": 5360
    },
    {
      "epoch": 0.6452388104535897,
      "grad_norm": 2.263925075531006,
      "learning_rate": 2.3646306646829386e-05,
      "loss": 1.1399,
      "step": 5370
    },
    {
      "epoch": 0.6464403724842295,
      "grad_norm": 2.6079580783843994,
      "learning_rate": 2.363424343560256e-05,
      "loss": 1.161,
      "step": 5380
    },
    {
      "epoch": 0.6476419345148693,
      "grad_norm": 2.6511940956115723,
      "learning_rate": 2.362218022437573e-05,
      "loss": 1.1356,
      "step": 5390
    },
    {
      "epoch": 0.6488434965455092,
      "grad_norm": 2.702413320541382,
      "learning_rate": 2.3610117013148902e-05,
      "loss": 1.1301,
      "step": 5400
    },
    {
      "epoch": 0.650045058576149,
      "grad_norm": 2.396157741546631,
      "learning_rate": 2.3598053801922075e-05,
      "loss": 1.1464,
      "step": 5410
    },
    {
      "epoch": 0.6512466206067888,
      "grad_norm": 2.1780097484588623,
      "learning_rate": 2.358599059069524e-05,
      "loss": 1.1688,
      "step": 5420
    },
    {
      "epoch": 0.6524481826374287,
      "grad_norm": 2.2614400386810303,
      "learning_rate": 2.3573927379468414e-05,
      "loss": 1.2278,
      "step": 5430
    },
    {
      "epoch": 0.6536497446680685,
      "grad_norm": 2.5514659881591797,
      "learning_rate": 2.3561864168241587e-05,
      "loss": 1.1643,
      "step": 5440
    },
    {
      "epoch": 0.6548513066987083,
      "grad_norm": 2.5685858726501465,
      "learning_rate": 2.3549800957014757e-05,
      "loss": 1.1782,
      "step": 5450
    },
    {
      "epoch": 0.6560528687293482,
      "grad_norm": 2.4901905059814453,
      "learning_rate": 2.353773774578793e-05,
      "loss": 1.1362,
      "step": 5460
    },
    {
      "epoch": 0.657254430759988,
      "grad_norm": 2.8134779930114746,
      "learning_rate": 2.35256745345611e-05,
      "loss": 1.1119,
      "step": 5470
    },
    {
      "epoch": 0.6584559927906278,
      "grad_norm": 2.942143440246582,
      "learning_rate": 2.3513611323334273e-05,
      "loss": 1.1108,
      "step": 5480
    },
    {
      "epoch": 0.6596575548212676,
      "grad_norm": 2.410102367401123,
      "learning_rate": 2.3501548112107446e-05,
      "loss": 1.1372,
      "step": 5490
    },
    {
      "epoch": 0.6608591168519075,
      "grad_norm": 2.948065757751465,
      "learning_rate": 2.3489484900880613e-05,
      "loss": 1.1268,
      "step": 5500
    },
    {
      "epoch": 0.6620606788825473,
      "grad_norm": 2.538789749145508,
      "learning_rate": 2.3477421689653786e-05,
      "loss": 1.1837,
      "step": 5510
    },
    {
      "epoch": 0.6632622409131872,
      "grad_norm": 2.453547954559326,
      "learning_rate": 2.3465358478426956e-05,
      "loss": 1.1702,
      "step": 5520
    },
    {
      "epoch": 0.664463802943827,
      "grad_norm": 2.3959860801696777,
      "learning_rate": 2.345329526720013e-05,
      "loss": 1.1124,
      "step": 5530
    },
    {
      "epoch": 0.6656653649744668,
      "grad_norm": 2.437869071960449,
      "learning_rate": 2.3441232055973302e-05,
      "loss": 1.15,
      "step": 5540
    },
    {
      "epoch": 0.6668669270051066,
      "grad_norm": 2.415254831314087,
      "learning_rate": 2.342916884474647e-05,
      "loss": 1.1568,
      "step": 5550
    },
    {
      "epoch": 0.6680684890357464,
      "grad_norm": 2.5065534114837646,
      "learning_rate": 2.3417105633519645e-05,
      "loss": 1.1351,
      "step": 5560
    },
    {
      "epoch": 0.6692700510663863,
      "grad_norm": 2.460529327392578,
      "learning_rate": 2.3405042422292815e-05,
      "loss": 1.106,
      "step": 5570
    },
    {
      "epoch": 0.6704716130970262,
      "grad_norm": 2.1950366497039795,
      "learning_rate": 2.3392979211065988e-05,
      "loss": 1.1325,
      "step": 5580
    },
    {
      "epoch": 0.671673175127666,
      "grad_norm": 3.22969388961792,
      "learning_rate": 2.3380915999839157e-05,
      "loss": 1.1573,
      "step": 5590
    },
    {
      "epoch": 0.6728747371583058,
      "grad_norm": 2.736839771270752,
      "learning_rate": 2.3368852788612327e-05,
      "loss": 1.1056,
      "step": 5600
    },
    {
      "epoch": 0.6740762991889456,
      "grad_norm": 2.3927693367004395,
      "learning_rate": 2.33567895773855e-05,
      "loss": 1.1687,
      "step": 5610
    },
    {
      "epoch": 0.6752778612195854,
      "grad_norm": 2.6924898624420166,
      "learning_rate": 2.3344726366158673e-05,
      "loss": 1.112,
      "step": 5620
    },
    {
      "epoch": 0.6764794232502253,
      "grad_norm": 2.6447033882141113,
      "learning_rate": 2.3332663154931843e-05,
      "loss": 1.1915,
      "step": 5630
    },
    {
      "epoch": 0.6776809852808652,
      "grad_norm": 2.1723124980926514,
      "learning_rate": 2.3320599943705016e-05,
      "loss": 1.1885,
      "step": 5640
    },
    {
      "epoch": 0.678882547311505,
      "grad_norm": 2.545649766921997,
      "learning_rate": 2.3308536732478186e-05,
      "loss": 1.1592,
      "step": 5650
    },
    {
      "epoch": 0.6800841093421448,
      "grad_norm": 2.867419719696045,
      "learning_rate": 2.329647352125136e-05,
      "loss": 1.1424,
      "step": 5660
    },
    {
      "epoch": 0.6812856713727846,
      "grad_norm": 2.306494951248169,
      "learning_rate": 2.328441031002453e-05,
      "loss": 1.1138,
      "step": 5670
    },
    {
      "epoch": 0.6824872334034244,
      "grad_norm": 2.92975115776062,
      "learning_rate": 2.32723470987977e-05,
      "loss": 1.1278,
      "step": 5680
    },
    {
      "epoch": 0.6836887954340642,
      "grad_norm": 2.1984479427337646,
      "learning_rate": 2.3260283887570872e-05,
      "loss": 1.1017,
      "step": 5690
    },
    {
      "epoch": 0.6848903574647042,
      "grad_norm": 2.4268791675567627,
      "learning_rate": 2.324822067634404e-05,
      "loss": 1.1183,
      "step": 5700
    },
    {
      "epoch": 0.686091919495344,
      "grad_norm": 2.2125244140625,
      "learning_rate": 2.3236157465117215e-05,
      "loss": 1.0945,
      "step": 5710
    },
    {
      "epoch": 0.6872934815259838,
      "grad_norm": 2.3693125247955322,
      "learning_rate": 2.3224094253890388e-05,
      "loss": 1.0709,
      "step": 5720
    },
    {
      "epoch": 0.6884950435566236,
      "grad_norm": 2.7304298877716064,
      "learning_rate": 2.3212031042663558e-05,
      "loss": 1.1333,
      "step": 5730
    },
    {
      "epoch": 0.6896966055872634,
      "grad_norm": 2.705434799194336,
      "learning_rate": 2.319996783143673e-05,
      "loss": 1.0705,
      "step": 5740
    },
    {
      "epoch": 0.6908981676179032,
      "grad_norm": 4.672322750091553,
      "learning_rate": 2.31879046202099e-05,
      "loss": 1.1401,
      "step": 5750
    },
    {
      "epoch": 0.6920997296485432,
      "grad_norm": 2.724590301513672,
      "learning_rate": 2.317584140898307e-05,
      "loss": 1.09,
      "step": 5760
    },
    {
      "epoch": 0.693301291679183,
      "grad_norm": 2.850940465927124,
      "learning_rate": 2.3163778197756243e-05,
      "loss": 1.1293,
      "step": 5770
    },
    {
      "epoch": 0.6945028537098228,
      "grad_norm": 2.906043291091919,
      "learning_rate": 2.3151714986529413e-05,
      "loss": 1.0913,
      "step": 5780
    },
    {
      "epoch": 0.6957044157404626,
      "grad_norm": 2.5413320064544678,
      "learning_rate": 2.3139651775302586e-05,
      "loss": 1.1296,
      "step": 5790
    },
    {
      "epoch": 0.6969059777711024,
      "grad_norm": 3.874148368835449,
      "learning_rate": 2.312758856407576e-05,
      "loss": 1.1968,
      "step": 5800
    },
    {
      "epoch": 0.6981075398017422,
      "grad_norm": 2.5468802452087402,
      "learning_rate": 2.311552535284893e-05,
      "loss": 1.1624,
      "step": 5810
    },
    {
      "epoch": 0.6993091018323822,
      "grad_norm": 2.9172699451446533,
      "learning_rate": 2.3103462141622102e-05,
      "loss": 1.1625,
      "step": 5820
    },
    {
      "epoch": 0.700510663863022,
      "grad_norm": 2.788724422454834,
      "learning_rate": 2.3091398930395272e-05,
      "loss": 1.1506,
      "step": 5830
    },
    {
      "epoch": 0.7017122258936618,
      "grad_norm": 3.0068845748901367,
      "learning_rate": 2.3079335719168442e-05,
      "loss": 1.1482,
      "step": 5840
    },
    {
      "epoch": 0.7029137879243016,
      "grad_norm": 2.7416892051696777,
      "learning_rate": 2.3067272507941615e-05,
      "loss": 1.132,
      "step": 5850
    },
    {
      "epoch": 0.7041153499549414,
      "grad_norm": 2.411986827850342,
      "learning_rate": 2.3055209296714785e-05,
      "loss": 1.1118,
      "step": 5860
    },
    {
      "epoch": 0.7053169119855812,
      "grad_norm": 2.443315267562866,
      "learning_rate": 2.3043146085487958e-05,
      "loss": 1.1342,
      "step": 5870
    },
    {
      "epoch": 0.706518474016221,
      "grad_norm": 2.6772682666778564,
      "learning_rate": 2.3031082874261127e-05,
      "loss": 1.149,
      "step": 5880
    },
    {
      "epoch": 0.707720036046861,
      "grad_norm": 2.4777750968933105,
      "learning_rate": 2.30190196630343e-05,
      "loss": 1.1317,
      "step": 5890
    },
    {
      "epoch": 0.7089215980775008,
      "grad_norm": 2.974484443664551,
      "learning_rate": 2.3006956451807474e-05,
      "loss": 1.1721,
      "step": 5900
    },
    {
      "epoch": 0.7101231601081406,
      "grad_norm": 2.4420104026794434,
      "learning_rate": 2.2994893240580644e-05,
      "loss": 1.148,
      "step": 5910
    },
    {
      "epoch": 0.7113247221387804,
      "grad_norm": 2.440376043319702,
      "learning_rate": 2.2982830029353813e-05,
      "loss": 1.0756,
      "step": 5920
    },
    {
      "epoch": 0.7125262841694202,
      "grad_norm": 3.101425886154175,
      "learning_rate": 2.2970766818126986e-05,
      "loss": 1.1477,
      "step": 5930
    },
    {
      "epoch": 0.71372784620006,
      "grad_norm": 2.549867630004883,
      "learning_rate": 2.2958703606900156e-05,
      "loss": 1.1474,
      "step": 5940
    },
    {
      "epoch": 0.7149294082307,
      "grad_norm": 2.581353187561035,
      "learning_rate": 2.294664039567333e-05,
      "loss": 1.1552,
      "step": 5950
    },
    {
      "epoch": 0.7161309702613398,
      "grad_norm": 3.0751099586486816,
      "learning_rate": 2.29345771844465e-05,
      "loss": 1.1398,
      "step": 5960
    },
    {
      "epoch": 0.7173325322919796,
      "grad_norm": 2.285836935043335,
      "learning_rate": 2.2922513973219672e-05,
      "loss": 1.1315,
      "step": 5970
    },
    {
      "epoch": 0.7185340943226194,
      "grad_norm": 3.036024570465088,
      "learning_rate": 2.2910450761992845e-05,
      "loss": 1.1177,
      "step": 5980
    },
    {
      "epoch": 0.7197356563532592,
      "grad_norm": 2.4737751483917236,
      "learning_rate": 2.2898387550766015e-05,
      "loss": 1.1403,
      "step": 5990
    },
    {
      "epoch": 0.720937218383899,
      "grad_norm": 2.6176326274871826,
      "learning_rate": 2.2886324339539188e-05,
      "loss": 1.1164,
      "step": 6000
    },
    {
      "epoch": 0.7221387804145389,
      "grad_norm": 2.9427807331085205,
      "learning_rate": 2.2874261128312355e-05,
      "loss": 1.1031,
      "step": 6010
    },
    {
      "epoch": 0.7233403424451788,
      "grad_norm": 2.5448150634765625,
      "learning_rate": 2.2862197917085528e-05,
      "loss": 1.1308,
      "step": 6020
    },
    {
      "epoch": 0.7245419044758186,
      "grad_norm": 3.0048131942749023,
      "learning_rate": 2.28501347058587e-05,
      "loss": 1.125,
      "step": 6030
    },
    {
      "epoch": 0.7257434665064584,
      "grad_norm": 2.704249382019043,
      "learning_rate": 2.283807149463187e-05,
      "loss": 1.134,
      "step": 6040
    },
    {
      "epoch": 0.7269450285370982,
      "grad_norm": 2.149163246154785,
      "learning_rate": 2.2826008283405044e-05,
      "loss": 1.2009,
      "step": 6050
    },
    {
      "epoch": 0.728146590567738,
      "grad_norm": 2.6559293270111084,
      "learning_rate": 2.2813945072178213e-05,
      "loss": 1.1835,
      "step": 6060
    },
    {
      "epoch": 0.7293481525983779,
      "grad_norm": 3.497617721557617,
      "learning_rate": 2.2801881860951387e-05,
      "loss": 1.1713,
      "step": 6070
    },
    {
      "epoch": 0.7305497146290177,
      "grad_norm": 2.2832813262939453,
      "learning_rate": 2.278981864972456e-05,
      "loss": 1.1066,
      "step": 6080
    },
    {
      "epoch": 0.7317512766596576,
      "grad_norm": 2.595144271850586,
      "learning_rate": 2.2777755438497726e-05,
      "loss": 1.1725,
      "step": 6090
    },
    {
      "epoch": 0.7329528386902974,
      "grad_norm": 2.3511197566986084,
      "learning_rate": 2.27656922272709e-05,
      "loss": 1.1955,
      "step": 6100
    },
    {
      "epoch": 0.7341544007209372,
      "grad_norm": 2.685542583465576,
      "learning_rate": 2.2753629016044072e-05,
      "loss": 1.1155,
      "step": 6110
    },
    {
      "epoch": 0.735355962751577,
      "grad_norm": 3.170987129211426,
      "learning_rate": 2.2741565804817242e-05,
      "loss": 1.1319,
      "step": 6120
    },
    {
      "epoch": 0.7365575247822169,
      "grad_norm": 2.3113150596618652,
      "learning_rate": 2.2729502593590415e-05,
      "loss": 1.1871,
      "step": 6130
    },
    {
      "epoch": 0.7377590868128567,
      "grad_norm": 2.3833370208740234,
      "learning_rate": 2.2717439382363585e-05,
      "loss": 1.1744,
      "step": 6140
    },
    {
      "epoch": 0.7389606488434965,
      "grad_norm": 2.5621724128723145,
      "learning_rate": 2.2705376171136758e-05,
      "loss": 1.0936,
      "step": 6150
    },
    {
      "epoch": 0.7401622108741364,
      "grad_norm": 2.5892179012298584,
      "learning_rate": 2.269331295990993e-05,
      "loss": 1.1152,
      "step": 6160
    },
    {
      "epoch": 0.7413637729047762,
      "grad_norm": 2.40556263923645,
      "learning_rate": 2.26812497486831e-05,
      "loss": 1.1399,
      "step": 6170
    },
    {
      "epoch": 0.742565334935416,
      "grad_norm": 3.120389461517334,
      "learning_rate": 2.266918653745627e-05,
      "loss": 1.1809,
      "step": 6180
    },
    {
      "epoch": 0.7437668969660559,
      "grad_norm": 2.365818977355957,
      "learning_rate": 2.265712332622944e-05,
      "loss": 1.1797,
      "step": 6190
    },
    {
      "epoch": 0.7449684589966957,
      "grad_norm": 2.8360860347747803,
      "learning_rate": 2.2645060115002614e-05,
      "loss": 1.1351,
      "step": 6200
    },
    {
      "epoch": 0.7461700210273355,
      "grad_norm": 3.109564781188965,
      "learning_rate": 2.2632996903775787e-05,
      "loss": 1.1459,
      "step": 6210
    },
    {
      "epoch": 0.7473715830579754,
      "grad_norm": 2.4774370193481445,
      "learning_rate": 2.2620933692548956e-05,
      "loss": 1.1317,
      "step": 6220
    },
    {
      "epoch": 0.7485731450886152,
      "grad_norm": 4.421533584594727,
      "learning_rate": 2.260887048132213e-05,
      "loss": 1.1104,
      "step": 6230
    },
    {
      "epoch": 0.749774707119255,
      "grad_norm": 2.6431517601013184,
      "learning_rate": 2.25968072700953e-05,
      "loss": 1.0798,
      "step": 6240
    },
    {
      "epoch": 0.7509762691498949,
      "grad_norm": 2.9069113731384277,
      "learning_rate": 2.2584744058868473e-05,
      "loss": 1.1386,
      "step": 6250
    },
    {
      "epoch": 0.7521778311805347,
      "grad_norm": 2.4818274974823,
      "learning_rate": 2.2572680847641642e-05,
      "loss": 1.1509,
      "step": 6260
    },
    {
      "epoch": 0.7533793932111745,
      "grad_norm": 2.4568982124328613,
      "learning_rate": 2.2560617636414812e-05,
      "loss": 1.175,
      "step": 6270
    },
    {
      "epoch": 0.7545809552418143,
      "grad_norm": 3.859294891357422,
      "learning_rate": 2.2548554425187985e-05,
      "loss": 1.1062,
      "step": 6280
    },
    {
      "epoch": 0.7557825172724542,
      "grad_norm": 3.401517391204834,
      "learning_rate": 2.2536491213961158e-05,
      "loss": 1.1719,
      "step": 6290
    },
    {
      "epoch": 0.756984079303094,
      "grad_norm": 2.754058599472046,
      "learning_rate": 2.2524428002734328e-05,
      "loss": 1.1572,
      "step": 6300
    },
    {
      "epoch": 0.7581856413337339,
      "grad_norm": 2.6672942638397217,
      "learning_rate": 2.25123647915075e-05,
      "loss": 1.149,
      "step": 6310
    },
    {
      "epoch": 0.7593872033643737,
      "grad_norm": 2.5751447677612305,
      "learning_rate": 2.250030158028067e-05,
      "loss": 1.1195,
      "step": 6320
    },
    {
      "epoch": 0.7605887653950135,
      "grad_norm": 3.975034236907959,
      "learning_rate": 2.2488238369053844e-05,
      "loss": 1.1119,
      "step": 6330
    },
    {
      "epoch": 0.7617903274256533,
      "grad_norm": 2.436157703399658,
      "learning_rate": 2.2476175157827014e-05,
      "loss": 1.1234,
      "step": 6340
    },
    {
      "epoch": 0.7629918894562931,
      "grad_norm": 2.174705743789673,
      "learning_rate": 2.2464111946600184e-05,
      "loss": 1.0937,
      "step": 6350
    },
    {
      "epoch": 0.764193451486933,
      "grad_norm": 2.4881813526153564,
      "learning_rate": 2.2452048735373357e-05,
      "loss": 1.1167,
      "step": 6360
    },
    {
      "epoch": 0.7653950135175729,
      "grad_norm": 2.2695438861846924,
      "learning_rate": 2.2439985524146526e-05,
      "loss": 1.1509,
      "step": 6370
    },
    {
      "epoch": 0.7665965755482127,
      "grad_norm": 2.261349678039551,
      "learning_rate": 2.24279223129197e-05,
      "loss": 1.1752,
      "step": 6380
    },
    {
      "epoch": 0.7677981375788525,
      "grad_norm": 2.4667809009552,
      "learning_rate": 2.2415859101692873e-05,
      "loss": 1.1167,
      "step": 6390
    },
    {
      "epoch": 0.7689996996094923,
      "grad_norm": 2.39054799079895,
      "learning_rate": 2.2403795890466042e-05,
      "loss": 1.1407,
      "step": 6400
    },
    {
      "epoch": 0.7702012616401321,
      "grad_norm": 2.668421506881714,
      "learning_rate": 2.2391732679239216e-05,
      "loss": 1.1761,
      "step": 6410
    },
    {
      "epoch": 0.771402823670772,
      "grad_norm": 2.323302984237671,
      "learning_rate": 2.2379669468012385e-05,
      "loss": 1.0911,
      "step": 6420
    },
    {
      "epoch": 0.7726043857014119,
      "grad_norm": 2.6158061027526855,
      "learning_rate": 2.2367606256785555e-05,
      "loss": 1.2118,
      "step": 6430
    },
    {
      "epoch": 0.7738059477320517,
      "grad_norm": 2.6207492351531982,
      "learning_rate": 2.2355543045558728e-05,
      "loss": 1.1292,
      "step": 6440
    },
    {
      "epoch": 0.7750075097626915,
      "grad_norm": 2.478825569152832,
      "learning_rate": 2.2343479834331898e-05,
      "loss": 1.0913,
      "step": 6450
    },
    {
      "epoch": 0.7762090717933313,
      "grad_norm": 3.056776762008667,
      "learning_rate": 2.233141662310507e-05,
      "loss": 1.1452,
      "step": 6460
    },
    {
      "epoch": 0.7774106338239711,
      "grad_norm": 2.787562608718872,
      "learning_rate": 2.2319353411878244e-05,
      "loss": 1.0679,
      "step": 6470
    },
    {
      "epoch": 0.778612195854611,
      "grad_norm": 2.5105297565460205,
      "learning_rate": 2.2307290200651414e-05,
      "loss": 1.1028,
      "step": 6480
    },
    {
      "epoch": 0.7798137578852509,
      "grad_norm": 2.5175013542175293,
      "learning_rate": 2.2295226989424587e-05,
      "loss": 1.1484,
      "step": 6490
    },
    {
      "epoch": 0.7810153199158907,
      "grad_norm": 2.7630417346954346,
      "learning_rate": 2.2283163778197757e-05,
      "loss": 1.1586,
      "step": 6500
    },
    {
      "epoch": 0.7822168819465305,
      "grad_norm": 2.458238363265991,
      "learning_rate": 2.2271100566970927e-05,
      "loss": 1.1735,
      "step": 6510
    },
    {
      "epoch": 0.7834184439771703,
      "grad_norm": 2.3790173530578613,
      "learning_rate": 2.22590373557441e-05,
      "loss": 1.1264,
      "step": 6520
    },
    {
      "epoch": 0.7846200060078101,
      "grad_norm": 2.3849098682403564,
      "learning_rate": 2.224697414451727e-05,
      "loss": 1.1463,
      "step": 6530
    },
    {
      "epoch": 0.7858215680384499,
      "grad_norm": 2.4911558628082275,
      "learning_rate": 2.2234910933290443e-05,
      "loss": 1.1289,
      "step": 6540
    },
    {
      "epoch": 0.7870231300690899,
      "grad_norm": 3.041707754135132,
      "learning_rate": 2.2222847722063612e-05,
      "loss": 1.1024,
      "step": 6550
    },
    {
      "epoch": 0.7882246920997297,
      "grad_norm": 2.5190577507019043,
      "learning_rate": 2.2210784510836785e-05,
      "loss": 1.1642,
      "step": 6560
    },
    {
      "epoch": 0.7894262541303695,
      "grad_norm": 2.6264305114746094,
      "learning_rate": 2.219872129960996e-05,
      "loss": 1.1525,
      "step": 6570
    },
    {
      "epoch": 0.7906278161610093,
      "grad_norm": 2.3951587677001953,
      "learning_rate": 2.218665808838313e-05,
      "loss": 1.1651,
      "step": 6580
    },
    {
      "epoch": 0.7918293781916491,
      "grad_norm": 2.252534866333008,
      "learning_rate": 2.21745948771563e-05,
      "loss": 1.1172,
      "step": 6590
    },
    {
      "epoch": 0.7930309402222889,
      "grad_norm": 2.4472010135650635,
      "learning_rate": 2.216253166592947e-05,
      "loss": 1.1322,
      "step": 6600
    },
    {
      "epoch": 0.7942325022529289,
      "grad_norm": 2.709167003631592,
      "learning_rate": 2.215046845470264e-05,
      "loss": 1.162,
      "step": 6610
    },
    {
      "epoch": 0.7954340642835687,
      "grad_norm": 2.5818262100219727,
      "learning_rate": 2.2138405243475814e-05,
      "loss": 1.1197,
      "step": 6620
    },
    {
      "epoch": 0.7966356263142085,
      "grad_norm": 2.632683753967285,
      "learning_rate": 2.2126342032248984e-05,
      "loss": 1.1206,
      "step": 6630
    },
    {
      "epoch": 0.7978371883448483,
      "grad_norm": 2.9742324352264404,
      "learning_rate": 2.2114278821022157e-05,
      "loss": 1.1433,
      "step": 6640
    },
    {
      "epoch": 0.7990387503754881,
      "grad_norm": 2.988640785217285,
      "learning_rate": 2.210221560979533e-05,
      "loss": 1.134,
      "step": 6650
    },
    {
      "epoch": 0.8002403124061279,
      "grad_norm": 3.7380404472351074,
      "learning_rate": 2.20901523985685e-05,
      "loss": 1.1503,
      "step": 6660
    },
    {
      "epoch": 0.8014418744367678,
      "grad_norm": 3.0424227714538574,
      "learning_rate": 2.2078089187341673e-05,
      "loss": 1.1667,
      "step": 6670
    },
    {
      "epoch": 0.8026434364674077,
      "grad_norm": 2.790614366531372,
      "learning_rate": 2.206602597611484e-05,
      "loss": 1.1951,
      "step": 6680
    },
    {
      "epoch": 0.8038449984980475,
      "grad_norm": 2.828190803527832,
      "learning_rate": 2.2053962764888013e-05,
      "loss": 1.0754,
      "step": 6690
    },
    {
      "epoch": 0.8050465605286873,
      "grad_norm": 2.7257182598114014,
      "learning_rate": 2.2041899553661186e-05,
      "loss": 1.1502,
      "step": 6700
    },
    {
      "epoch": 0.8062481225593271,
      "grad_norm": 2.676926612854004,
      "learning_rate": 2.2029836342434355e-05,
      "loss": 1.1566,
      "step": 6710
    },
    {
      "epoch": 0.8074496845899669,
      "grad_norm": 2.583650588989258,
      "learning_rate": 2.201777313120753e-05,
      "loss": 1.1545,
      "step": 6720
    },
    {
      "epoch": 0.8086512466206068,
      "grad_norm": 2.3625175952911377,
      "learning_rate": 2.2005709919980698e-05,
      "loss": 1.133,
      "step": 6730
    },
    {
      "epoch": 0.8098528086512466,
      "grad_norm": 2.5627384185791016,
      "learning_rate": 2.199364670875387e-05,
      "loss": 1.1094,
      "step": 6740
    },
    {
      "epoch": 0.8110543706818865,
      "grad_norm": 2.502166271209717,
      "learning_rate": 2.1981583497527045e-05,
      "loss": 1.134,
      "step": 6750
    },
    {
      "epoch": 0.8122559327125263,
      "grad_norm": 2.485356569290161,
      "learning_rate": 2.196952028630021e-05,
      "loss": 1.1642,
      "step": 6760
    },
    {
      "epoch": 0.8134574947431661,
      "grad_norm": 3.2920444011688232,
      "learning_rate": 2.1957457075073384e-05,
      "loss": 1.095,
      "step": 6770
    },
    {
      "epoch": 0.8146590567738059,
      "grad_norm": 2.4699838161468506,
      "learning_rate": 2.1945393863846557e-05,
      "loss": 1.101,
      "step": 6780
    },
    {
      "epoch": 0.8158606188044458,
      "grad_norm": 3.0075249671936035,
      "learning_rate": 2.1933330652619727e-05,
      "loss": 1.1442,
      "step": 6790
    },
    {
      "epoch": 0.8170621808350856,
      "grad_norm": 3.0731966495513916,
      "learning_rate": 2.19212674413929e-05,
      "loss": 1.1465,
      "step": 6800
    },
    {
      "epoch": 0.8182637428657255,
      "grad_norm": 3.3657684326171875,
      "learning_rate": 2.190920423016607e-05,
      "loss": 1.1396,
      "step": 6810
    },
    {
      "epoch": 0.8194653048963653,
      "grad_norm": 2.7516801357269287,
      "learning_rate": 2.1897141018939243e-05,
      "loss": 1.1505,
      "step": 6820
    },
    {
      "epoch": 0.8206668669270051,
      "grad_norm": 2.1776821613311768,
      "learning_rate": 2.1885077807712416e-05,
      "loss": 1.1305,
      "step": 6830
    },
    {
      "epoch": 0.8218684289576449,
      "grad_norm": 2.3906099796295166,
      "learning_rate": 2.1873014596485586e-05,
      "loss": 1.1533,
      "step": 6840
    },
    {
      "epoch": 0.8230699909882848,
      "grad_norm": 2.3752708435058594,
      "learning_rate": 2.1860951385258756e-05,
      "loss": 1.1142,
      "step": 6850
    },
    {
      "epoch": 0.8242715530189246,
      "grad_norm": 2.715057373046875,
      "learning_rate": 2.1848888174031925e-05,
      "loss": 1.1579,
      "step": 6860
    },
    {
      "epoch": 0.8254731150495644,
      "grad_norm": 2.5718986988067627,
      "learning_rate": 2.18368249628051e-05,
      "loss": 1.1396,
      "step": 6870
    },
    {
      "epoch": 0.8266746770802043,
      "grad_norm": 2.485377073287964,
      "learning_rate": 2.182476175157827e-05,
      "loss": 1.1484,
      "step": 6880
    },
    {
      "epoch": 0.8278762391108441,
      "grad_norm": 2.4074244499206543,
      "learning_rate": 2.181269854035144e-05,
      "loss": 1.0756,
      "step": 6890
    },
    {
      "epoch": 0.8290778011414839,
      "grad_norm": 3.795280694961548,
      "learning_rate": 2.1800635329124614e-05,
      "loss": 1.1649,
      "step": 6900
    },
    {
      "epoch": 0.8302793631721238,
      "grad_norm": 2.6195006370544434,
      "learning_rate": 2.1788572117897784e-05,
      "loss": 1.1253,
      "step": 6910
    },
    {
      "epoch": 0.8314809252027636,
      "grad_norm": 2.633327007293701,
      "learning_rate": 2.1776508906670957e-05,
      "loss": 1.0736,
      "step": 6920
    },
    {
      "epoch": 0.8326824872334034,
      "grad_norm": 2.410677433013916,
      "learning_rate": 2.1764445695444127e-05,
      "loss": 1.2196,
      "step": 6930
    },
    {
      "epoch": 0.8338840492640432,
      "grad_norm": 2.635446548461914,
      "learning_rate": 2.1752382484217297e-05,
      "loss": 1.1508,
      "step": 6940
    },
    {
      "epoch": 0.8350856112946831,
      "grad_norm": 2.294802665710449,
      "learning_rate": 2.174031927299047e-05,
      "loss": 1.1052,
      "step": 6950
    },
    {
      "epoch": 0.8362871733253229,
      "grad_norm": 2.7482528686523438,
      "learning_rate": 2.1728256061763643e-05,
      "loss": 1.1723,
      "step": 6960
    },
    {
      "epoch": 0.8374887353559628,
      "grad_norm": 2.9213085174560547,
      "learning_rate": 2.1716192850536813e-05,
      "loss": 1.162,
      "step": 6970
    },
    {
      "epoch": 0.8386902973866026,
      "grad_norm": 2.549434185028076,
      "learning_rate": 2.1704129639309986e-05,
      "loss": 1.0813,
      "step": 6980
    },
    {
      "epoch": 0.8398918594172424,
      "grad_norm": 2.809014320373535,
      "learning_rate": 2.1692066428083156e-05,
      "loss": 1.129,
      "step": 6990
    },
    {
      "epoch": 0.8410934214478822,
      "grad_norm": 2.6856069564819336,
      "learning_rate": 2.168000321685633e-05,
      "loss": 1.1346,
      "step": 7000
    },
    {
      "epoch": 0.842294983478522,
      "grad_norm": 3.613865613937378,
      "learning_rate": 2.1667940005629502e-05,
      "loss": 1.1301,
      "step": 7010
    },
    {
      "epoch": 0.8434965455091619,
      "grad_norm": 2.8980188369750977,
      "learning_rate": 2.165587679440267e-05,
      "loss": 1.1538,
      "step": 7020
    },
    {
      "epoch": 0.8446981075398018,
      "grad_norm": 2.6251072883605957,
      "learning_rate": 2.164381358317584e-05,
      "loss": 1.1427,
      "step": 7030
    },
    {
      "epoch": 0.8458996695704416,
      "grad_norm": 2.008434295654297,
      "learning_rate": 2.163175037194901e-05,
      "loss": 1.1039,
      "step": 7040
    },
    {
      "epoch": 0.8471012316010814,
      "grad_norm": 2.3324341773986816,
      "learning_rate": 2.1619687160722184e-05,
      "loss": 1.115,
      "step": 7050
    },
    {
      "epoch": 0.8483027936317212,
      "grad_norm": 2.5659759044647217,
      "learning_rate": 2.1607623949495358e-05,
      "loss": 1.1382,
      "step": 7060
    },
    {
      "epoch": 0.849504355662361,
      "grad_norm": 2.323385000228882,
      "learning_rate": 2.1595560738268527e-05,
      "loss": 1.156,
      "step": 7070
    },
    {
      "epoch": 0.8507059176930009,
      "grad_norm": 2.597486734390259,
      "learning_rate": 2.15834975270417e-05,
      "loss": 1.1372,
      "step": 7080
    },
    {
      "epoch": 0.8519074797236408,
      "grad_norm": 2.653672695159912,
      "learning_rate": 2.1571434315814874e-05,
      "loss": 1.1594,
      "step": 7090
    },
    {
      "epoch": 0.8531090417542806,
      "grad_norm": 2.6090543270111084,
      "learning_rate": 2.155937110458804e-05,
      "loss": 1.1846,
      "step": 7100
    },
    {
      "epoch": 0.8543106037849204,
      "grad_norm": 2.655064105987549,
      "learning_rate": 2.1547307893361213e-05,
      "loss": 1.1384,
      "step": 7110
    },
    {
      "epoch": 0.8555121658155602,
      "grad_norm": 2.649284839630127,
      "learning_rate": 2.1535244682134383e-05,
      "loss": 1.2044,
      "step": 7120
    },
    {
      "epoch": 0.8567137278462,
      "grad_norm": 2.3860023021698,
      "learning_rate": 2.1523181470907556e-05,
      "loss": 1.145,
      "step": 7130
    },
    {
      "epoch": 0.8579152898768398,
      "grad_norm": 2.67128324508667,
      "learning_rate": 2.151111825968073e-05,
      "loss": 1.1017,
      "step": 7140
    },
    {
      "epoch": 0.8591168519074798,
      "grad_norm": 2.715541362762451,
      "learning_rate": 2.14990550484539e-05,
      "loss": 1.1591,
      "step": 7150
    },
    {
      "epoch": 0.8603184139381196,
      "grad_norm": 2.708993673324585,
      "learning_rate": 2.1486991837227072e-05,
      "loss": 1.1214,
      "step": 7160
    },
    {
      "epoch": 0.8615199759687594,
      "grad_norm": 2.7513723373413086,
      "learning_rate": 2.1474928626000242e-05,
      "loss": 1.1844,
      "step": 7170
    },
    {
      "epoch": 0.8627215379993992,
      "grad_norm": 2.7005715370178223,
      "learning_rate": 2.146286541477341e-05,
      "loss": 1.2147,
      "step": 7180
    },
    {
      "epoch": 0.863923100030039,
      "grad_norm": 2.574065923690796,
      "learning_rate": 2.1450802203546585e-05,
      "loss": 1.1603,
      "step": 7190
    },
    {
      "epoch": 0.8651246620606788,
      "grad_norm": 2.264862537384033,
      "learning_rate": 2.1438738992319754e-05,
      "loss": 1.0741,
      "step": 7200
    },
    {
      "epoch": 0.8663262240913188,
      "grad_norm": 2.664086103439331,
      "learning_rate": 2.1426675781092927e-05,
      "loss": 1.1673,
      "step": 7210
    },
    {
      "epoch": 0.8675277861219586,
      "grad_norm": 2.3659164905548096,
      "learning_rate": 2.1414612569866097e-05,
      "loss": 1.1254,
      "step": 7220
    },
    {
      "epoch": 0.8687293481525984,
      "grad_norm": 3.366569995880127,
      "learning_rate": 2.140254935863927e-05,
      "loss": 1.1305,
      "step": 7230
    },
    {
      "epoch": 0.8699309101832382,
      "grad_norm": 2.782184600830078,
      "learning_rate": 2.1390486147412443e-05,
      "loss": 1.1323,
      "step": 7240
    },
    {
      "epoch": 0.871132472213878,
      "grad_norm": 2.533828020095825,
      "learning_rate": 2.1378422936185613e-05,
      "loss": 1.1975,
      "step": 7250
    },
    {
      "epoch": 0.8723340342445178,
      "grad_norm": 2.7432937622070312,
      "learning_rate": 2.1366359724958786e-05,
      "loss": 1.167,
      "step": 7260
    },
    {
      "epoch": 0.8735355962751578,
      "grad_norm": 2.692704677581787,
      "learning_rate": 2.1354296513731956e-05,
      "loss": 1.164,
      "step": 7270
    },
    {
      "epoch": 0.8747371583057976,
      "grad_norm": 2.7004005908966064,
      "learning_rate": 2.1342233302505126e-05,
      "loss": 1.185,
      "step": 7280
    },
    {
      "epoch": 0.8759387203364374,
      "grad_norm": 3.283797025680542,
      "learning_rate": 2.13301700912783e-05,
      "loss": 1.1507,
      "step": 7290
    },
    {
      "epoch": 0.8771402823670772,
      "grad_norm": 2.735473871231079,
      "learning_rate": 2.131810688005147e-05,
      "loss": 1.1696,
      "step": 7300
    },
    {
      "epoch": 0.878341844397717,
      "grad_norm": 2.8751022815704346,
      "learning_rate": 2.1306043668824642e-05,
      "loss": 1.1629,
      "step": 7310
    },
    {
      "epoch": 0.8795434064283568,
      "grad_norm": 2.5504443645477295,
      "learning_rate": 2.1293980457597815e-05,
      "loss": 1.1242,
      "step": 7320
    },
    {
      "epoch": 0.8807449684589967,
      "grad_norm": 2.489100694656372,
      "learning_rate": 2.1281917246370985e-05,
      "loss": 1.1726,
      "step": 7330
    },
    {
      "epoch": 0.8819465304896366,
      "grad_norm": 2.7794439792633057,
      "learning_rate": 2.1269854035144158e-05,
      "loss": 1.1372,
      "step": 7340
    },
    {
      "epoch": 0.8831480925202764,
      "grad_norm": 3.0614709854125977,
      "learning_rate": 2.1257790823917324e-05,
      "loss": 1.2247,
      "step": 7350
    },
    {
      "epoch": 0.8843496545509162,
      "grad_norm": 2.5614874362945557,
      "learning_rate": 2.1245727612690497e-05,
      "loss": 1.2031,
      "step": 7360
    },
    {
      "epoch": 0.885551216581556,
      "grad_norm": 2.3309624195098877,
      "learning_rate": 2.123366440146367e-05,
      "loss": 1.1002,
      "step": 7370
    },
    {
      "epoch": 0.8867527786121958,
      "grad_norm": 2.8851253986358643,
      "learning_rate": 2.122160119023684e-05,
      "loss": 1.1448,
      "step": 7380
    },
    {
      "epoch": 0.8879543406428357,
      "grad_norm": 2.3435606956481934,
      "learning_rate": 2.1209537979010013e-05,
      "loss": 1.1435,
      "step": 7390
    },
    {
      "epoch": 0.8891559026734756,
      "grad_norm": 2.555915117263794,
      "learning_rate": 2.1197474767783183e-05,
      "loss": 1.094,
      "step": 7400
    },
    {
      "epoch": 0.8903574647041154,
      "grad_norm": 2.817153215408325,
      "learning_rate": 2.1185411556556356e-05,
      "loss": 1.147,
      "step": 7410
    },
    {
      "epoch": 0.8915590267347552,
      "grad_norm": 3.323465347290039,
      "learning_rate": 2.117334834532953e-05,
      "loss": 1.1379,
      "step": 7420
    },
    {
      "epoch": 0.892760588765395,
      "grad_norm": 2.3488519191741943,
      "learning_rate": 2.11612851341027e-05,
      "loss": 1.0822,
      "step": 7430
    },
    {
      "epoch": 0.8939621507960348,
      "grad_norm": 2.6494076251983643,
      "learning_rate": 2.114922192287587e-05,
      "loss": 1.1473,
      "step": 7440
    },
    {
      "epoch": 0.8951637128266747,
      "grad_norm": 2.7398996353149414,
      "learning_rate": 2.1137158711649042e-05,
      "loss": 1.1049,
      "step": 7450
    },
    {
      "epoch": 0.8963652748573145,
      "grad_norm": 2.5541300773620605,
      "learning_rate": 2.1125095500422212e-05,
      "loss": 1.1348,
      "step": 7460
    },
    {
      "epoch": 0.8975668368879544,
      "grad_norm": 2.446856737136841,
      "learning_rate": 2.1113032289195385e-05,
      "loss": 1.1151,
      "step": 7470
    },
    {
      "epoch": 0.8987683989185942,
      "grad_norm": 2.2040016651153564,
      "learning_rate": 2.1100969077968555e-05,
      "loss": 1.1592,
      "step": 7480
    },
    {
      "epoch": 0.899969960949234,
      "grad_norm": 2.8657896518707275,
      "learning_rate": 2.1088905866741728e-05,
      "loss": 1.1219,
      "step": 7490
    },
    {
      "epoch": 0.9011715229798738,
      "grad_norm": 2.675189733505249,
      "learning_rate": 2.10768426555149e-05,
      "loss": 1.1639,
      "step": 7500
    },
    {
      "epoch": 0.9023730850105137,
      "grad_norm": 2.5651087760925293,
      "learning_rate": 2.106477944428807e-05,
      "loss": 1.1237,
      "step": 7510
    },
    {
      "epoch": 0.9035746470411535,
      "grad_norm": 2.5807840824127197,
      "learning_rate": 2.105271623306124e-05,
      "loss": 1.1156,
      "step": 7520
    },
    {
      "epoch": 0.9047762090717933,
      "grad_norm": 2.404937744140625,
      "learning_rate": 2.104065302183441e-05,
      "loss": 1.1625,
      "step": 7530
    },
    {
      "epoch": 0.9059777711024332,
      "grad_norm": 2.750743865966797,
      "learning_rate": 2.1028589810607583e-05,
      "loss": 1.1617,
      "step": 7540
    },
    {
      "epoch": 0.907179333133073,
      "grad_norm": 2.3569366931915283,
      "learning_rate": 2.1016526599380756e-05,
      "loss": 1.1657,
      "step": 7550
    },
    {
      "epoch": 0.9083808951637128,
      "grad_norm": 2.536813735961914,
      "learning_rate": 2.1004463388153926e-05,
      "loss": 1.1567,
      "step": 7560
    },
    {
      "epoch": 0.9095824571943527,
      "grad_norm": 3.257499933242798,
      "learning_rate": 2.09924001769271e-05,
      "loss": 1.1534,
      "step": 7570
    },
    {
      "epoch": 0.9107840192249925,
      "grad_norm": 2.566643238067627,
      "learning_rate": 2.0980336965700272e-05,
      "loss": 1.1552,
      "step": 7580
    },
    {
      "epoch": 0.9119855812556323,
      "grad_norm": 2.4859156608581543,
      "learning_rate": 2.0968273754473442e-05,
      "loss": 1.2145,
      "step": 7590
    },
    {
      "epoch": 0.9131871432862722,
      "grad_norm": 2.652719020843506,
      "learning_rate": 2.0956210543246612e-05,
      "loss": 1.1409,
      "step": 7600
    },
    {
      "epoch": 0.914388705316912,
      "grad_norm": 2.5778465270996094,
      "learning_rate": 2.0944147332019782e-05,
      "loss": 1.1181,
      "step": 7610
    },
    {
      "epoch": 0.9155902673475518,
      "grad_norm": 2.742919683456421,
      "learning_rate": 2.0932084120792955e-05,
      "loss": 1.1758,
      "step": 7620
    },
    {
      "epoch": 0.9167918293781917,
      "grad_norm": 3.051391839981079,
      "learning_rate": 2.0920020909566128e-05,
      "loss": 1.1669,
      "step": 7630
    },
    {
      "epoch": 0.9179933914088315,
      "grad_norm": 2.5715394020080566,
      "learning_rate": 2.0907957698339298e-05,
      "loss": 1.1091,
      "step": 7640
    },
    {
      "epoch": 0.9191949534394713,
      "grad_norm": 2.3806982040405273,
      "learning_rate": 2.089589448711247e-05,
      "loss": 1.0791,
      "step": 7650
    },
    {
      "epoch": 0.9203965154701111,
      "grad_norm": 2.505246877670288,
      "learning_rate": 2.088383127588564e-05,
      "loss": 1.1261,
      "step": 7660
    },
    {
      "epoch": 0.921598077500751,
      "grad_norm": 2.3555123805999756,
      "learning_rate": 2.0871768064658814e-05,
      "loss": 1.167,
      "step": 7670
    },
    {
      "epoch": 0.9227996395313908,
      "grad_norm": 2.7110800743103027,
      "learning_rate": 2.0859704853431987e-05,
      "loss": 1.1602,
      "step": 7680
    },
    {
      "epoch": 0.9240012015620307,
      "grad_norm": 2.8762400150299072,
      "learning_rate": 2.0847641642205153e-05,
      "loss": 1.146,
      "step": 7690
    },
    {
      "epoch": 0.9252027635926705,
      "grad_norm": 2.460923433303833,
      "learning_rate": 2.0835578430978326e-05,
      "loss": 1.1913,
      "step": 7700
    },
    {
      "epoch": 0.9264043256233103,
      "grad_norm": 2.8004565238952637,
      "learning_rate": 2.0823515219751496e-05,
      "loss": 1.1554,
      "step": 7710
    },
    {
      "epoch": 0.9276058876539501,
      "grad_norm": 2.7996599674224854,
      "learning_rate": 2.081145200852467e-05,
      "loss": 1.1549,
      "step": 7720
    },
    {
      "epoch": 0.92880744968459,
      "grad_norm": 2.523179292678833,
      "learning_rate": 2.0799388797297842e-05,
      "loss": 1.1264,
      "step": 7730
    },
    {
      "epoch": 0.9300090117152298,
      "grad_norm": 2.4843811988830566,
      "learning_rate": 2.0787325586071012e-05,
      "loss": 1.1602,
      "step": 7740
    },
    {
      "epoch": 0.9312105737458697,
      "grad_norm": 2.7515974044799805,
      "learning_rate": 2.0775262374844185e-05,
      "loss": 1.1491,
      "step": 7750
    },
    {
      "epoch": 0.9324121357765095,
      "grad_norm": 2.178964138031006,
      "learning_rate": 2.076319916361736e-05,
      "loss": 1.0952,
      "step": 7760
    },
    {
      "epoch": 0.9336136978071493,
      "grad_norm": 2.894002914428711,
      "learning_rate": 2.0751135952390525e-05,
      "loss": 1.0916,
      "step": 7770
    },
    {
      "epoch": 0.9348152598377891,
      "grad_norm": 2.7202203273773193,
      "learning_rate": 2.0739072741163698e-05,
      "loss": 1.133,
      "step": 7780
    },
    {
      "epoch": 0.9360168218684289,
      "grad_norm": 2.628939151763916,
      "learning_rate": 2.0727009529936868e-05,
      "loss": 1.1415,
      "step": 7790
    },
    {
      "epoch": 0.9372183838990688,
      "grad_norm": 2.4305951595306396,
      "learning_rate": 2.071494631871004e-05,
      "loss": 1.1252,
      "step": 7800
    },
    {
      "epoch": 0.9384199459297087,
      "grad_norm": 3.7183070182800293,
      "learning_rate": 2.0702883107483214e-05,
      "loss": 1.0953,
      "step": 7810
    },
    {
      "epoch": 0.9396215079603485,
      "grad_norm": 2.644578218460083,
      "learning_rate": 2.0690819896256384e-05,
      "loss": 1.1489,
      "step": 7820
    },
    {
      "epoch": 0.9408230699909883,
      "grad_norm": 2.9045443534851074,
      "learning_rate": 2.0678756685029557e-05,
      "loss": 1.1542,
      "step": 7830
    },
    {
      "epoch": 0.9420246320216281,
      "grad_norm": 2.2777278423309326,
      "learning_rate": 2.0666693473802727e-05,
      "loss": 1.0651,
      "step": 7840
    },
    {
      "epoch": 0.9432261940522679,
      "grad_norm": 3.145153522491455,
      "learning_rate": 2.06546302625759e-05,
      "loss": 1.1816,
      "step": 7850
    },
    {
      "epoch": 0.9444277560829077,
      "grad_norm": 2.270280599594116,
      "learning_rate": 2.064256705134907e-05,
      "loss": 1.1113,
      "step": 7860
    },
    {
      "epoch": 0.9456293181135476,
      "grad_norm": 3.248840570449829,
      "learning_rate": 2.063050384012224e-05,
      "loss": 1.0866,
      "step": 7870
    },
    {
      "epoch": 0.9468308801441875,
      "grad_norm": 2.5905866622924805,
      "learning_rate": 2.0618440628895412e-05,
      "loss": 1.0922,
      "step": 7880
    },
    {
      "epoch": 0.9480324421748273,
      "grad_norm": 2.5259668827056885,
      "learning_rate": 2.0606377417668582e-05,
      "loss": 1.0857,
      "step": 7890
    },
    {
      "epoch": 0.9492340042054671,
      "grad_norm": 3.6742618083953857,
      "learning_rate": 2.0594314206441755e-05,
      "loss": 1.2009,
      "step": 7900
    },
    {
      "epoch": 0.9504355662361069,
      "grad_norm": 2.481639862060547,
      "learning_rate": 2.058225099521493e-05,
      "loss": 1.088,
      "step": 7910
    },
    {
      "epoch": 0.9516371282667467,
      "grad_norm": 2.628365993499756,
      "learning_rate": 2.0570187783988098e-05,
      "loss": 1.1149,
      "step": 7920
    },
    {
      "epoch": 0.9528386902973865,
      "grad_norm": 2.507375478744507,
      "learning_rate": 2.055812457276127e-05,
      "loss": 1.1574,
      "step": 7930
    },
    {
      "epoch": 0.9540402523280265,
      "grad_norm": 3.0421173572540283,
      "learning_rate": 2.054606136153444e-05,
      "loss": 1.1329,
      "step": 7940
    },
    {
      "epoch": 0.9552418143586663,
      "grad_norm": 2.359036445617676,
      "learning_rate": 2.053399815030761e-05,
      "loss": 1.0952,
      "step": 7950
    },
    {
      "epoch": 0.9564433763893061,
      "grad_norm": 2.253100633621216,
      "learning_rate": 2.0521934939080784e-05,
      "loss": 1.1198,
      "step": 7960
    },
    {
      "epoch": 0.9576449384199459,
      "grad_norm": 2.21287202835083,
      "learning_rate": 2.0509871727853954e-05,
      "loss": 1.1404,
      "step": 7970
    },
    {
      "epoch": 0.9588465004505857,
      "grad_norm": 2.605591297149658,
      "learning_rate": 2.0497808516627127e-05,
      "loss": 1.1451,
      "step": 7980
    },
    {
      "epoch": 0.9600480624812255,
      "grad_norm": 2.831880807876587,
      "learning_rate": 2.04857453054003e-05,
      "loss": 1.1736,
      "step": 7990
    },
    {
      "epoch": 0.9612496245118655,
      "grad_norm": NaN,
      "learning_rate": 2.047368209417347e-05,
      "loss": 1.139,
      "step": 8000
    },
    {
      "epoch": 0.9624511865425053,
      "grad_norm": 3.2064311504364014,
      "learning_rate": 2.0462825204069324e-05,
      "loss": 1.1339,
      "step": 8010
    },
    {
      "epoch": 0.9636527485731451,
      "grad_norm": 2.706078290939331,
      "learning_rate": 2.0450761992842494e-05,
      "loss": 1.1542,
      "step": 8020
    },
    {
      "epoch": 0.9648543106037849,
      "grad_norm": 2.527101755142212,
      "learning_rate": 2.0438698781615667e-05,
      "loss": 1.1253,
      "step": 8030
    },
    {
      "epoch": 0.9660558726344247,
      "grad_norm": 2.4743781089782715,
      "learning_rate": 2.0426635570388837e-05,
      "loss": 1.1591,
      "step": 8040
    },
    {
      "epoch": 0.9672574346650645,
      "grad_norm": 2.4749255180358887,
      "learning_rate": 2.041457235916201e-05,
      "loss": 1.1199,
      "step": 8050
    },
    {
      "epoch": 0.9684589966957045,
      "grad_norm": 2.581827402114868,
      "learning_rate": 2.040250914793518e-05,
      "loss": 1.1418,
      "step": 8060
    },
    {
      "epoch": 0.9696605587263443,
      "grad_norm": 2.820852041244507,
      "learning_rate": 2.0390445936708353e-05,
      "loss": 1.1196,
      "step": 8070
    },
    {
      "epoch": 0.9708621207569841,
      "grad_norm": 3.332857847213745,
      "learning_rate": 2.0378382725481526e-05,
      "loss": 1.1669,
      "step": 8080
    },
    {
      "epoch": 0.9720636827876239,
      "grad_norm": 3.722160577774048,
      "learning_rate": 2.0366319514254696e-05,
      "loss": 1.1592,
      "step": 8090
    },
    {
      "epoch": 0.9732652448182637,
      "grad_norm": 2.613147497177124,
      "learning_rate": 2.0354256303027865e-05,
      "loss": 1.2193,
      "step": 8100
    },
    {
      "epoch": 0.9744668068489035,
      "grad_norm": 2.9165406227111816,
      "learning_rate": 2.034219309180104e-05,
      "loss": 1.1046,
      "step": 8110
    },
    {
      "epoch": 0.9756683688795434,
      "grad_norm": 3.004166603088379,
      "learning_rate": 2.0330129880574208e-05,
      "loss": 1.1658,
      "step": 8120
    },
    {
      "epoch": 0.9768699309101833,
      "grad_norm": 2.740947723388672,
      "learning_rate": 2.031806666934738e-05,
      "loss": 1.1251,
      "step": 8130
    },
    {
      "epoch": 0.9780714929408231,
      "grad_norm": 2.292358875274658,
      "learning_rate": 2.030600345812055e-05,
      "loss": 1.0844,
      "step": 8140
    },
    {
      "epoch": 0.9792730549714629,
      "grad_norm": 2.7368009090423584,
      "learning_rate": 2.0293940246893724e-05,
      "loss": 1.1413,
      "step": 8150
    },
    {
      "epoch": 0.9804746170021027,
      "grad_norm": 2.4290103912353516,
      "learning_rate": 2.0281877035666897e-05,
      "loss": 1.1194,
      "step": 8160
    },
    {
      "epoch": 0.9816761790327425,
      "grad_norm": 2.592336893081665,
      "learning_rate": 2.0269813824440067e-05,
      "loss": 1.1585,
      "step": 8170
    },
    {
      "epoch": 0.9828777410633824,
      "grad_norm": 2.734504461288452,
      "learning_rate": 2.025775061321324e-05,
      "loss": 1.1575,
      "step": 8180
    },
    {
      "epoch": 0.9840793030940223,
      "grad_norm": 2.758422613143921,
      "learning_rate": 2.0245687401986407e-05,
      "loss": 1.1084,
      "step": 8190
    },
    {
      "epoch": 0.9852808651246621,
      "grad_norm": 2.1880064010620117,
      "learning_rate": 2.023362419075958e-05,
      "loss": 1.1871,
      "step": 8200
    },
    {
      "epoch": 0.9864824271553019,
      "grad_norm": 2.5389530658721924,
      "learning_rate": 2.0221560979532753e-05,
      "loss": 1.137,
      "step": 8210
    },
    {
      "epoch": 0.9876839891859417,
      "grad_norm": 2.623870611190796,
      "learning_rate": 2.0209497768305923e-05,
      "loss": 1.2134,
      "step": 8220
    },
    {
      "epoch": 0.9888855512165815,
      "grad_norm": 2.310638666152954,
      "learning_rate": 2.0197434557079096e-05,
      "loss": 1.1713,
      "step": 8230
    },
    {
      "epoch": 0.9900871132472214,
      "grad_norm": 2.745413303375244,
      "learning_rate": 2.0185371345852266e-05,
      "loss": 1.1407,
      "step": 8240
    },
    {
      "epoch": 0.9912886752778612,
      "grad_norm": 3.4021031856536865,
      "learning_rate": 2.017330813462544e-05,
      "loss": 1.1046,
      "step": 8250
    },
    {
      "epoch": 0.992490237308501,
      "grad_norm": 2.7862308025360107,
      "learning_rate": 2.0161244923398612e-05,
      "loss": 1.1153,
      "step": 8260
    },
    {
      "epoch": 0.9936917993391409,
      "grad_norm": 2.73512601852417,
      "learning_rate": 2.0149181712171778e-05,
      "loss": 1.1818,
      "step": 8270
    },
    {
      "epoch": 0.9948933613697807,
      "grad_norm": 2.481980562210083,
      "learning_rate": 2.013711850094495e-05,
      "loss": 1.1664,
      "step": 8280
    },
    {
      "epoch": 0.9960949234004205,
      "grad_norm": 3.293112277984619,
      "learning_rate": 2.0125055289718124e-05,
      "loss": 1.1689,
      "step": 8290
    },
    {
      "epoch": 0.9972964854310604,
      "grad_norm": 2.7016468048095703,
      "learning_rate": 2.0112992078491294e-05,
      "loss": 1.127,
      "step": 8300
    },
    {
      "epoch": 0.9984980474617002,
      "grad_norm": 2.666548490524292,
      "learning_rate": 2.0100928867264467e-05,
      "loss": 1.18,
      "step": 8310
    },
    {
      "epoch": 0.99969960949234,
      "grad_norm": 2.565119743347168,
      "learning_rate": 2.0088865656037637e-05,
      "loss": 1.0996,
      "step": 8320
    },
    {
      "epoch": 1.0008410934214478,
      "grad_norm": 2.457174062728882,
      "learning_rate": 2.007680244481081e-05,
      "loss": 1.0709,
      "step": 8330
    },
    {
      "epoch": 1.0020426554520878,
      "grad_norm": 2.4643137454986572,
      "learning_rate": 2.0064739233583983e-05,
      "loss": 1.1516,
      "step": 8340
    },
    {
      "epoch": 1.0032442174827276,
      "grad_norm": 2.945295572280884,
      "learning_rate": 2.005267602235715e-05,
      "loss": 1.1217,
      "step": 8350
    },
    {
      "epoch": 1.0044457795133674,
      "grad_norm": 2.372199058532715,
      "learning_rate": 2.0040612811130323e-05,
      "loss": 1.1276,
      "step": 8360
    },
    {
      "epoch": 1.0056473415440073,
      "grad_norm": 2.34916615486145,
      "learning_rate": 2.0028549599903493e-05,
      "loss": 1.1535,
      "step": 8370
    },
    {
      "epoch": 1.006848903574647,
      "grad_norm": 2.614799976348877,
      "learning_rate": 2.0016486388676666e-05,
      "loss": 1.104,
      "step": 8380
    },
    {
      "epoch": 1.0080504656052869,
      "grad_norm": 2.73007869720459,
      "learning_rate": 2.000442317744984e-05,
      "loss": 1.1475,
      "step": 8390
    },
    {
      "epoch": 1.0092520276359267,
      "grad_norm": 3.197272777557373,
      "learning_rate": 1.999235996622301e-05,
      "loss": 1.1061,
      "step": 8400
    },
    {
      "epoch": 1.0104535896665665,
      "grad_norm": 2.8269917964935303,
      "learning_rate": 1.9980296754996182e-05,
      "loss": 1.1621,
      "step": 8410
    },
    {
      "epoch": 1.0116551516972063,
      "grad_norm": 3.1193904876708984,
      "learning_rate": 1.996823354376935e-05,
      "loss": 1.1357,
      "step": 8420
    },
    {
      "epoch": 1.0128567137278461,
      "grad_norm": 2.664830446243286,
      "learning_rate": 1.9956170332542525e-05,
      "loss": 1.1027,
      "step": 8430
    },
    {
      "epoch": 1.014058275758486,
      "grad_norm": 2.640620708465576,
      "learning_rate": 1.9944107121315694e-05,
      "loss": 1.0952,
      "step": 8440
    },
    {
      "epoch": 1.0152598377891258,
      "grad_norm": 2.6036794185638428,
      "learning_rate": 1.9932043910088864e-05,
      "loss": 1.0988,
      "step": 8450
    },
    {
      "epoch": 1.0164613998197658,
      "grad_norm": 2.9717719554901123,
      "learning_rate": 1.9919980698862037e-05,
      "loss": 1.1448,
      "step": 8460
    },
    {
      "epoch": 1.0176629618504056,
      "grad_norm": 2.807953357696533,
      "learning_rate": 1.990791748763521e-05,
      "loss": 1.1595,
      "step": 8470
    },
    {
      "epoch": 1.0188645238810454,
      "grad_norm": 2.5342800617218018,
      "learning_rate": 1.989585427640838e-05,
      "loss": 1.1325,
      "step": 8480
    },
    {
      "epoch": 1.0200660859116852,
      "grad_norm": 2.857616424560547,
      "learning_rate": 1.9883791065181553e-05,
      "loss": 1.1239,
      "step": 8490
    },
    {
      "epoch": 1.021267647942325,
      "grad_norm": 2.698742151260376,
      "learning_rate": 1.9871727853954723e-05,
      "loss": 1.1412,
      "step": 8500
    },
    {
      "epoch": 1.0224692099729649,
      "grad_norm": 2.6764819622039795,
      "learning_rate": 1.9859664642727896e-05,
      "loss": 1.1558,
      "step": 8510
    },
    {
      "epoch": 1.0236707720036047,
      "grad_norm": 2.6579864025115967,
      "learning_rate": 1.9847601431501066e-05,
      "loss": 1.1263,
      "step": 8520
    },
    {
      "epoch": 1.0248723340342445,
      "grad_norm": 2.501504421234131,
      "learning_rate": 1.9835538220274236e-05,
      "loss": 1.0779,
      "step": 8530
    },
    {
      "epoch": 1.0260738960648843,
      "grad_norm": 3.1378495693206787,
      "learning_rate": 1.982347500904741e-05,
      "loss": 1.1301,
      "step": 8540
    },
    {
      "epoch": 1.0272754580955241,
      "grad_norm": 2.7894437313079834,
      "learning_rate": 1.981141179782058e-05,
      "loss": 1.1271,
      "step": 8550
    },
    {
      "epoch": 1.028477020126164,
      "grad_norm": 2.6777265071868896,
      "learning_rate": 1.979934858659375e-05,
      "loss": 1.0642,
      "step": 8560
    },
    {
      "epoch": 1.0296785821568037,
      "grad_norm": 2.9447360038757324,
      "learning_rate": 1.9787285375366925e-05,
      "loss": 1.0808,
      "step": 8570
    },
    {
      "epoch": 1.0308801441874438,
      "grad_norm": 2.5561294555664062,
      "learning_rate": 1.9775222164140095e-05,
      "loss": 1.1811,
      "step": 8580
    },
    {
      "epoch": 1.0320817062180836,
      "grad_norm": 2.638225555419922,
      "learning_rate": 1.9763158952913268e-05,
      "loss": 1.1764,
      "step": 8590
    },
    {
      "epoch": 1.0332832682487234,
      "grad_norm": 2.3911473751068115,
      "learning_rate": 1.9751095741686437e-05,
      "loss": 1.0619,
      "step": 8600
    },
    {
      "epoch": 1.0344848302793632,
      "grad_norm": 2.5939602851867676,
      "learning_rate": 1.9739032530459607e-05,
      "loss": 1.1167,
      "step": 8610
    },
    {
      "epoch": 1.035686392310003,
      "grad_norm": 2.5901594161987305,
      "learning_rate": 1.972696931923278e-05,
      "loss": 1.1436,
      "step": 8620
    },
    {
      "epoch": 1.0368879543406428,
      "grad_norm": 2.296440362930298,
      "learning_rate": 1.971490610800595e-05,
      "loss": 1.1348,
      "step": 8630
    },
    {
      "epoch": 1.0380895163712827,
      "grad_norm": 2.8947386741638184,
      "learning_rate": 1.9702842896779123e-05,
      "loss": 1.1444,
      "step": 8640
    },
    {
      "epoch": 1.0392910784019225,
      "grad_norm": 2.694937229156494,
      "learning_rate": 1.9690779685552296e-05,
      "loss": 1.2055,
      "step": 8650
    },
    {
      "epoch": 1.0404926404325623,
      "grad_norm": 2.3180243968963623,
      "learning_rate": 1.9678716474325466e-05,
      "loss": 1.0761,
      "step": 8660
    },
    {
      "epoch": 1.041694202463202,
      "grad_norm": 2.576375722885132,
      "learning_rate": 1.966665326309864e-05,
      "loss": 1.1623,
      "step": 8670
    },
    {
      "epoch": 1.042895764493842,
      "grad_norm": 2.524752140045166,
      "learning_rate": 1.965459005187181e-05,
      "loss": 1.1088,
      "step": 8680
    },
    {
      "epoch": 1.0440973265244817,
      "grad_norm": 2.406287670135498,
      "learning_rate": 1.964252684064498e-05,
      "loss": 1.1618,
      "step": 8690
    },
    {
      "epoch": 1.0452988885551218,
      "grad_norm": 2.8467724323272705,
      "learning_rate": 1.9630463629418152e-05,
      "loss": 1.1388,
      "step": 8700
    },
    {
      "epoch": 1.0465004505857616,
      "grad_norm": 2.801888942718506,
      "learning_rate": 1.961840041819132e-05,
      "loss": 1.1154,
      "step": 8710
    },
    {
      "epoch": 1.0477020126164014,
      "grad_norm": 2.8796563148498535,
      "learning_rate": 1.9606337206964495e-05,
      "loss": 1.101,
      "step": 8720
    },
    {
      "epoch": 1.0489035746470412,
      "grad_norm": 2.7718162536621094,
      "learning_rate": 1.9594273995737664e-05,
      "loss": 1.1211,
      "step": 8730
    },
    {
      "epoch": 1.050105136677681,
      "grad_norm": 2.689976453781128,
      "learning_rate": 1.9582210784510838e-05,
      "loss": 1.0518,
      "step": 8740
    },
    {
      "epoch": 1.0513066987083208,
      "grad_norm": 2.6210391521453857,
      "learning_rate": 1.957014757328401e-05,
      "loss": 1.0744,
      "step": 8750
    },
    {
      "epoch": 1.0525082607389606,
      "grad_norm": 2.426211357116699,
      "learning_rate": 1.955808436205718e-05,
      "loss": 1.0767,
      "step": 8760
    },
    {
      "epoch": 1.0537098227696005,
      "grad_norm": 2.4004757404327393,
      "learning_rate": 1.954602115083035e-05,
      "loss": 1.1481,
      "step": 8770
    },
    {
      "epoch": 1.0549113848002403,
      "grad_norm": 2.828169107437134,
      "learning_rate": 1.9533957939603523e-05,
      "loss": 1.1155,
      "step": 8780
    },
    {
      "epoch": 1.05611294683088,
      "grad_norm": 2.3827097415924072,
      "learning_rate": 1.9521894728376693e-05,
      "loss": 1.1348,
      "step": 8790
    },
    {
      "epoch": 1.05731450886152,
      "grad_norm": 2.9214344024658203,
      "learning_rate": 1.9509831517149866e-05,
      "loss": 1.144,
      "step": 8800
    },
    {
      "epoch": 1.0585160708921597,
      "grad_norm": 2.6775572299957275,
      "learning_rate": 1.9497768305923036e-05,
      "loss": 1.1555,
      "step": 8810
    },
    {
      "epoch": 1.0597176329227997,
      "grad_norm": 2.8838610649108887,
      "learning_rate": 1.948570509469621e-05,
      "loss": 1.1425,
      "step": 8820
    },
    {
      "epoch": 1.0609191949534396,
      "grad_norm": 3.5438294410705566,
      "learning_rate": 1.9473641883469382e-05,
      "loss": 1.1111,
      "step": 8830
    },
    {
      "epoch": 1.0621207569840794,
      "grad_norm": 2.8421638011932373,
      "learning_rate": 1.9461578672242552e-05,
      "loss": 1.1214,
      "step": 8840
    },
    {
      "epoch": 1.0633223190147192,
      "grad_norm": 2.54170823097229,
      "learning_rate": 1.9449515461015725e-05,
      "loss": 1.088,
      "step": 8850
    },
    {
      "epoch": 1.064523881045359,
      "grad_norm": 3.0205912590026855,
      "learning_rate": 1.943745224978889e-05,
      "loss": 1.1386,
      "step": 8860
    },
    {
      "epoch": 1.0657254430759988,
      "grad_norm": 2.6829676628112793,
      "learning_rate": 1.9425389038562065e-05,
      "loss": 1.2011,
      "step": 8870
    },
    {
      "epoch": 1.0669270051066386,
      "grad_norm": 2.39819073677063,
      "learning_rate": 1.9413325827335238e-05,
      "loss": 1.1439,
      "step": 8880
    },
    {
      "epoch": 1.0681285671372784,
      "grad_norm": 2.4787802696228027,
      "learning_rate": 1.9401262616108407e-05,
      "loss": 1.1591,
      "step": 8890
    },
    {
      "epoch": 1.0693301291679183,
      "grad_norm": 2.121821403503418,
      "learning_rate": 1.938919940488158e-05,
      "loss": 1.1296,
      "step": 8900
    },
    {
      "epoch": 1.070531691198558,
      "grad_norm": 2.5758683681488037,
      "learning_rate": 1.937713619365475e-05,
      "loss": 1.1449,
      "step": 8910
    },
    {
      "epoch": 1.0717332532291979,
      "grad_norm": 2.6418778896331787,
      "learning_rate": 1.9365072982427924e-05,
      "loss": 1.1107,
      "step": 8920
    },
    {
      "epoch": 1.0729348152598377,
      "grad_norm": 2.475748300552368,
      "learning_rate": 1.9353009771201097e-05,
      "loss": 1.1983,
      "step": 8930
    },
    {
      "epoch": 1.0741363772904777,
      "grad_norm": 2.8069307804107666,
      "learning_rate": 1.9340946559974263e-05,
      "loss": 1.1456,
      "step": 8940
    },
    {
      "epoch": 1.0753379393211175,
      "grad_norm": 2.781625509262085,
      "learning_rate": 1.9328883348747436e-05,
      "loss": 1.1198,
      "step": 8950
    },
    {
      "epoch": 1.0765395013517574,
      "grad_norm": 2.5557749271392822,
      "learning_rate": 1.931682013752061e-05,
      "loss": 1.1095,
      "step": 8960
    },
    {
      "epoch": 1.0777410633823972,
      "grad_norm": 2.5752317905426025,
      "learning_rate": 1.930475692629378e-05,
      "loss": 1.0485,
      "step": 8970
    },
    {
      "epoch": 1.078942625413037,
      "grad_norm": 2.5654520988464355,
      "learning_rate": 1.9292693715066952e-05,
      "loss": 1.1128,
      "step": 8980
    },
    {
      "epoch": 1.0801441874436768,
      "grad_norm": 2.656616687774658,
      "learning_rate": 1.9280630503840122e-05,
      "loss": 1.1301,
      "step": 8990
    },
    {
      "epoch": 1.0813457494743166,
      "grad_norm": 2.9276185035705566,
      "learning_rate": 1.9268567292613295e-05,
      "loss": 1.1185,
      "step": 9000
    },
    {
      "epoch": 1.0825473115049564,
      "grad_norm": 2.535673141479492,
      "learning_rate": 1.9256504081386468e-05,
      "loss": 1.0885,
      "step": 9010
    },
    {
      "epoch": 1.0837488735355962,
      "grad_norm": 2.4624640941619873,
      "learning_rate": 1.9244440870159638e-05,
      "loss": 1.1596,
      "step": 9020
    },
    {
      "epoch": 1.084950435566236,
      "grad_norm": 2.8598122596740723,
      "learning_rate": 1.9232377658932808e-05,
      "loss": 1.1224,
      "step": 9030
    },
    {
      "epoch": 1.0861519975968759,
      "grad_norm": 2.328498601913452,
      "learning_rate": 1.9220314447705977e-05,
      "loss": 1.1098,
      "step": 9040
    },
    {
      "epoch": 1.0873535596275157,
      "grad_norm": 2.771423578262329,
      "learning_rate": 1.920825123647915e-05,
      "loss": 1.1043,
      "step": 9050
    },
    {
      "epoch": 1.0885551216581555,
      "grad_norm": 3.0399742126464844,
      "learning_rate": 1.9196188025252324e-05,
      "loss": 1.1149,
      "step": 9060
    },
    {
      "epoch": 1.0897566836887955,
      "grad_norm": 2.835324764251709,
      "learning_rate": 1.9184124814025493e-05,
      "loss": 1.1583,
      "step": 9070
    },
    {
      "epoch": 1.0909582457194353,
      "grad_norm": 2.9059619903564453,
      "learning_rate": 1.9172061602798667e-05,
      "loss": 1.1279,
      "step": 9080
    },
    {
      "epoch": 1.0921598077500752,
      "grad_norm": 2.921564817428589,
      "learning_rate": 1.9159998391571836e-05,
      "loss": 1.1926,
      "step": 9090
    },
    {
      "epoch": 1.093361369780715,
      "grad_norm": 2.3811662197113037,
      "learning_rate": 1.914793518034501e-05,
      "loss": 1.1529,
      "step": 9100
    },
    {
      "epoch": 1.0945629318113548,
      "grad_norm": 2.695885181427002,
      "learning_rate": 1.913587196911818e-05,
      "loss": 1.1666,
      "step": 9110
    },
    {
      "epoch": 1.0957644938419946,
      "grad_norm": 2.2224886417388916,
      "learning_rate": 1.912380875789135e-05,
      "loss": 1.121,
      "step": 9120
    },
    {
      "epoch": 1.0969660558726344,
      "grad_norm": 2.620720148086548,
      "learning_rate": 1.9111745546664522e-05,
      "loss": 1.1403,
      "step": 9130
    },
    {
      "epoch": 1.0981676179032742,
      "grad_norm": 2.4816782474517822,
      "learning_rate": 1.9099682335437695e-05,
      "loss": 1.0454,
      "step": 9140
    },
    {
      "epoch": 1.099369179933914,
      "grad_norm": 2.474820852279663,
      "learning_rate": 1.9087619124210865e-05,
      "loss": 1.1406,
      "step": 9150
    },
    {
      "epoch": 1.1005707419645538,
      "grad_norm": 3.174823045730591,
      "learning_rate": 1.9075555912984038e-05,
      "loss": 1.1348,
      "step": 9160
    },
    {
      "epoch": 1.1017723039951937,
      "grad_norm": 2.52687931060791,
      "learning_rate": 1.9063492701757208e-05,
      "loss": 1.1648,
      "step": 9170
    },
    {
      "epoch": 1.1029738660258337,
      "grad_norm": 2.5766398906707764,
      "learning_rate": 1.905142949053038e-05,
      "loss": 1.1946,
      "step": 9180
    },
    {
      "epoch": 1.1041754280564735,
      "grad_norm": 2.3928298950195312,
      "learning_rate": 1.903936627930355e-05,
      "loss": 1.0915,
      "step": 9190
    },
    {
      "epoch": 1.1053769900871133,
      "grad_norm": 2.6015982627868652,
      "learning_rate": 1.902730306807672e-05,
      "loss": 1.1632,
      "step": 9200
    },
    {
      "epoch": 1.1065785521177531,
      "grad_norm": 2.489806652069092,
      "learning_rate": 1.9015239856849894e-05,
      "loss": 1.1266,
      "step": 9210
    },
    {
      "epoch": 1.107780114148393,
      "grad_norm": 2.4798221588134766,
      "learning_rate": 1.9003176645623063e-05,
      "loss": 1.1536,
      "step": 9220
    },
    {
      "epoch": 1.1089816761790328,
      "grad_norm": 2.4843921661376953,
      "learning_rate": 1.8991113434396236e-05,
      "loss": 1.16,
      "step": 9230
    },
    {
      "epoch": 1.1101832382096726,
      "grad_norm": 2.2956032752990723,
      "learning_rate": 1.897905022316941e-05,
      "loss": 1.1349,
      "step": 9240
    },
    {
      "epoch": 1.1113848002403124,
      "grad_norm": 2.565183639526367,
      "learning_rate": 1.896698701194258e-05,
      "loss": 1.0655,
      "step": 9250
    },
    {
      "epoch": 1.1125863622709522,
      "grad_norm": 2.416224718093872,
      "learning_rate": 1.8954923800715753e-05,
      "loss": 1.1399,
      "step": 9260
    },
    {
      "epoch": 1.113787924301592,
      "grad_norm": 2.5232372283935547,
      "learning_rate": 1.8942860589488926e-05,
      "loss": 1.2041,
      "step": 9270
    },
    {
      "epoch": 1.1149894863322318,
      "grad_norm": 2.912033796310425,
      "learning_rate": 1.8930797378262092e-05,
      "loss": 1.1246,
      "step": 9280
    },
    {
      "epoch": 1.1161910483628716,
      "grad_norm": 2.9857828617095947,
      "learning_rate": 1.8918734167035265e-05,
      "loss": 1.1128,
      "step": 9290
    },
    {
      "epoch": 1.1173926103935115,
      "grad_norm": 2.46962571144104,
      "learning_rate": 1.8906670955808435e-05,
      "loss": 1.1135,
      "step": 9300
    },
    {
      "epoch": 1.1185941724241515,
      "grad_norm": 3.0423452854156494,
      "learning_rate": 1.8894607744581608e-05,
      "loss": 1.1812,
      "step": 9310
    },
    {
      "epoch": 1.1197957344547913,
      "grad_norm": 2.504849433898926,
      "learning_rate": 1.888254453335478e-05,
      "loss": 1.1325,
      "step": 9320
    },
    {
      "epoch": 1.1209972964854311,
      "grad_norm": 2.4146206378936768,
      "learning_rate": 1.887048132212795e-05,
      "loss": 1.1243,
      "step": 9330
    },
    {
      "epoch": 1.122198858516071,
      "grad_norm": 2.6210620403289795,
      "learning_rate": 1.8858418110901124e-05,
      "loss": 1.1075,
      "step": 9340
    },
    {
      "epoch": 1.1234004205467107,
      "grad_norm": 2.8725223541259766,
      "learning_rate": 1.8846354899674294e-05,
      "loss": 1.1052,
      "step": 9350
    },
    {
      "epoch": 1.1246019825773506,
      "grad_norm": 3.193427085876465,
      "learning_rate": 1.8834291688447464e-05,
      "loss": 1.123,
      "step": 9360
    },
    {
      "epoch": 1.1258035446079904,
      "grad_norm": 2.6845786571502686,
      "learning_rate": 1.8822228477220637e-05,
      "loss": 1.1117,
      "step": 9370
    },
    {
      "epoch": 1.1270051066386302,
      "grad_norm": 2.6783127784729004,
      "learning_rate": 1.8810165265993806e-05,
      "loss": 1.1443,
      "step": 9380
    },
    {
      "epoch": 1.12820666866927,
      "grad_norm": 2.7461605072021484,
      "learning_rate": 1.879810205476698e-05,
      "loss": 1.0798,
      "step": 9390
    },
    {
      "epoch": 1.1294082306999098,
      "grad_norm": 2.5646183490753174,
      "learning_rate": 1.878603884354015e-05,
      "loss": 1.1129,
      "step": 9400
    },
    {
      "epoch": 1.1306097927305496,
      "grad_norm": 2.794671058654785,
      "learning_rate": 1.8773975632313322e-05,
      "loss": 1.1373,
      "step": 9410
    },
    {
      "epoch": 1.1318113547611897,
      "grad_norm": 2.744814872741699,
      "learning_rate": 1.8761912421086496e-05,
      "loss": 1.1351,
      "step": 9420
    },
    {
      "epoch": 1.1330129167918295,
      "grad_norm": 2.5378613471984863,
      "learning_rate": 1.8749849209859665e-05,
      "loss": 1.1564,
      "step": 9430
    },
    {
      "epoch": 1.1342144788224693,
      "grad_norm": 2.50053071975708,
      "learning_rate": 1.873778599863284e-05,
      "loss": 1.1533,
      "step": 9440
    },
    {
      "epoch": 1.135416040853109,
      "grad_norm": 2.778167247772217,
      "learning_rate": 1.8725722787406008e-05,
      "loss": 1.1863,
      "step": 9450
    },
    {
      "epoch": 1.136617602883749,
      "grad_norm": 2.9499053955078125,
      "learning_rate": 1.8713659576179178e-05,
      "loss": 1.1769,
      "step": 9460
    },
    {
      "epoch": 1.1378191649143887,
      "grad_norm": 2.6105599403381348,
      "learning_rate": 1.870159636495235e-05,
      "loss": 1.1239,
      "step": 9470
    },
    {
      "epoch": 1.1390207269450285,
      "grad_norm": 2.3395652770996094,
      "learning_rate": 1.868953315372552e-05,
      "loss": 1.0915,
      "step": 9480
    },
    {
      "epoch": 1.1402222889756684,
      "grad_norm": 3.5621888637542725,
      "learning_rate": 1.8677469942498694e-05,
      "loss": 1.136,
      "step": 9490
    },
    {
      "epoch": 1.1414238510063082,
      "grad_norm": 2.5241105556488037,
      "learning_rate": 1.8665406731271867e-05,
      "loss": 1.0924,
      "step": 9500
    },
    {
      "epoch": 1.142625413036948,
      "grad_norm": 2.966395378112793,
      "learning_rate": 1.8653343520045037e-05,
      "loss": 1.1999,
      "step": 9510
    },
    {
      "epoch": 1.1438269750675878,
      "grad_norm": 2.4833786487579346,
      "learning_rate": 1.864128030881821e-05,
      "loss": 1.1233,
      "step": 9520
    },
    {
      "epoch": 1.1450285370982276,
      "grad_norm": 2.992577075958252,
      "learning_rate": 1.8629217097591376e-05,
      "loss": 1.1202,
      "step": 9530
    },
    {
      "epoch": 1.1462300991288674,
      "grad_norm": 2.3473315238952637,
      "learning_rate": 1.861715388636455e-05,
      "loss": 1.1339,
      "step": 9540
    },
    {
      "epoch": 1.1474316611595075,
      "grad_norm": 2.383009910583496,
      "learning_rate": 1.8605090675137723e-05,
      "loss": 1.0818,
      "step": 9550
    },
    {
      "epoch": 1.1486332231901473,
      "grad_norm": 2.8843917846679688,
      "learning_rate": 1.8593027463910892e-05,
      "loss": 1.1224,
      "step": 9560
    },
    {
      "epoch": 1.149834785220787,
      "grad_norm": 2.479877471923828,
      "learning_rate": 1.8580964252684065e-05,
      "loss": 1.1103,
      "step": 9570
    },
    {
      "epoch": 1.151036347251427,
      "grad_norm": 2.258720874786377,
      "learning_rate": 1.8568901041457235e-05,
      "loss": 1.1493,
      "step": 9580
    },
    {
      "epoch": 1.1522379092820667,
      "grad_norm": 2.729077100753784,
      "learning_rate": 1.855683783023041e-05,
      "loss": 1.0753,
      "step": 9590
    },
    {
      "epoch": 1.1534394713127065,
      "grad_norm": 2.2378127574920654,
      "learning_rate": 1.854477461900358e-05,
      "loss": 1.1978,
      "step": 9600
    },
    {
      "epoch": 1.1546410333433463,
      "grad_norm": 3.0124034881591797,
      "learning_rate": 1.8532711407776748e-05,
      "loss": 1.1353,
      "step": 9610
    },
    {
      "epoch": 1.1558425953739861,
      "grad_norm": 3.3486921787261963,
      "learning_rate": 1.852064819654992e-05,
      "loss": 1.1665,
      "step": 9620
    },
    {
      "epoch": 1.157044157404626,
      "grad_norm": 2.5190086364746094,
      "learning_rate": 1.8508584985323094e-05,
      "loss": 1.1246,
      "step": 9630
    },
    {
      "epoch": 1.1582457194352658,
      "grad_norm": 2.301936388015747,
      "learning_rate": 1.8496521774096264e-05,
      "loss": 1.1071,
      "step": 9640
    },
    {
      "epoch": 1.1594472814659056,
      "grad_norm": 3.4591145515441895,
      "learning_rate": 1.8484458562869437e-05,
      "loss": 1.1042,
      "step": 9650
    },
    {
      "epoch": 1.1606488434965456,
      "grad_norm": 2.59413480758667,
      "learning_rate": 1.8472395351642607e-05,
      "loss": 1.1043,
      "step": 9660
    },
    {
      "epoch": 1.1618504055271854,
      "grad_norm": 2.4999353885650635,
      "learning_rate": 1.846033214041578e-05,
      "loss": 1.1559,
      "step": 9670
    },
    {
      "epoch": 1.1630519675578253,
      "grad_norm": 2.4852969646453857,
      "learning_rate": 1.8448268929188953e-05,
      "loss": 1.1616,
      "step": 9680
    },
    {
      "epoch": 1.164253529588465,
      "grad_norm": 2.742018222808838,
      "learning_rate": 1.8436205717962123e-05,
      "loss": 1.0992,
      "step": 9690
    },
    {
      "epoch": 1.1654550916191049,
      "grad_norm": 2.9051320552825928,
      "learning_rate": 1.8424142506735293e-05,
      "loss": 1.1763,
      "step": 9700
    },
    {
      "epoch": 1.1666566536497447,
      "grad_norm": 2.7973783016204834,
      "learning_rate": 1.8412079295508462e-05,
      "loss": 1.1194,
      "step": 9710
    },
    {
      "epoch": 1.1678582156803845,
      "grad_norm": 2.5124666690826416,
      "learning_rate": 1.8400016084281635e-05,
      "loss": 1.111,
      "step": 9720
    },
    {
      "epoch": 1.1690597777110243,
      "grad_norm": 2.75290584564209,
      "learning_rate": 1.838795287305481e-05,
      "loss": 1.1451,
      "step": 9730
    },
    {
      "epoch": 1.1702613397416641,
      "grad_norm": 3.131373882293701,
      "learning_rate": 1.8375889661827978e-05,
      "loss": 1.1633,
      "step": 9740
    },
    {
      "epoch": 1.171462901772304,
      "grad_norm": 2.428713798522949,
      "learning_rate": 1.836382645060115e-05,
      "loss": 1.1413,
      "step": 9750
    },
    {
      "epoch": 1.1726644638029438,
      "grad_norm": 2.445054531097412,
      "learning_rate": 1.8351763239374325e-05,
      "loss": 1.1215,
      "step": 9760
    },
    {
      "epoch": 1.1738660258335836,
      "grad_norm": 2.5051023960113525,
      "learning_rate": 1.8339700028147494e-05,
      "loss": 1.1332,
      "step": 9770
    },
    {
      "epoch": 1.1750675878642234,
      "grad_norm": 2.778909206390381,
      "learning_rate": 1.8327636816920664e-05,
      "loss": 1.0979,
      "step": 9780
    },
    {
      "epoch": 1.1762691498948634,
      "grad_norm": 2.8877508640289307,
      "learning_rate": 1.8315573605693834e-05,
      "loss": 1.1318,
      "step": 9790
    },
    {
      "epoch": 1.1774707119255032,
      "grad_norm": 2.646587371826172,
      "learning_rate": 1.8303510394467007e-05,
      "loss": 1.1545,
      "step": 9800
    },
    {
      "epoch": 1.178672273956143,
      "grad_norm": 2.3708372116088867,
      "learning_rate": 1.829144718324018e-05,
      "loss": 1.1445,
      "step": 9810
    },
    {
      "epoch": 1.1798738359867829,
      "grad_norm": 2.773468017578125,
      "learning_rate": 1.827938397201335e-05,
      "loss": 1.1043,
      "step": 9820
    },
    {
      "epoch": 1.1810753980174227,
      "grad_norm": 2.5194497108459473,
      "learning_rate": 1.8267320760786523e-05,
      "loss": 1.1001,
      "step": 9830
    },
    {
      "epoch": 1.1822769600480625,
      "grad_norm": 2.5530030727386475,
      "learning_rate": 1.8255257549559693e-05,
      "loss": 1.1181,
      "step": 9840
    },
    {
      "epoch": 1.1834785220787023,
      "grad_norm": 2.6104815006256104,
      "learning_rate": 1.8243194338332866e-05,
      "loss": 1.1634,
      "step": 9850
    },
    {
      "epoch": 1.1846800841093421,
      "grad_norm": 3.380270004272461,
      "learning_rate": 1.823113112710604e-05,
      "loss": 1.1109,
      "step": 9860
    },
    {
      "epoch": 1.185881646139982,
      "grad_norm": 2.3133511543273926,
      "learning_rate": 1.8219067915879205e-05,
      "loss": 1.1136,
      "step": 9870
    },
    {
      "epoch": 1.1870832081706217,
      "grad_norm": 3.1346187591552734,
      "learning_rate": 1.820700470465238e-05,
      "loss": 1.1907,
      "step": 9880
    },
    {
      "epoch": 1.1882847702012616,
      "grad_norm": 3.479447364807129,
      "learning_rate": 1.8194941493425548e-05,
      "loss": 1.1328,
      "step": 9890
    },
    {
      "epoch": 1.1894863322319016,
      "grad_norm": 2.644453525543213,
      "learning_rate": 1.818287828219872e-05,
      "loss": 1.1438,
      "step": 9900
    },
    {
      "epoch": 1.1906878942625414,
      "grad_norm": 2.6994552612304688,
      "learning_rate": 1.8170815070971894e-05,
      "loss": 1.1271,
      "step": 9910
    },
    {
      "epoch": 1.1918894562931812,
      "grad_norm": 3.0221729278564453,
      "learning_rate": 1.8158751859745064e-05,
      "loss": 1.1178,
      "step": 9920
    },
    {
      "epoch": 1.193091018323821,
      "grad_norm": 2.839080333709717,
      "learning_rate": 1.8146688648518237e-05,
      "loss": 1.0932,
      "step": 9930
    },
    {
      "epoch": 1.1942925803544608,
      "grad_norm": 2.6925292015075684,
      "learning_rate": 1.813462543729141e-05,
      "loss": 1.1389,
      "step": 9940
    },
    {
      "epoch": 1.1954941423851007,
      "grad_norm": 2.863330125808716,
      "learning_rate": 1.8122562226064577e-05,
      "loss": 1.1158,
      "step": 9950
    },
    {
      "epoch": 1.1966957044157405,
      "grad_norm": 2.6745452880859375,
      "learning_rate": 1.811049901483775e-05,
      "loss": 1.1431,
      "step": 9960
    },
    {
      "epoch": 1.1978972664463803,
      "grad_norm": 2.7104811668395996,
      "learning_rate": 1.809843580361092e-05,
      "loss": 1.1091,
      "step": 9970
    },
    {
      "epoch": 1.19909882847702,
      "grad_norm": 2.576148271560669,
      "learning_rate": 1.8086372592384093e-05,
      "loss": 1.1565,
      "step": 9980
    },
    {
      "epoch": 1.20030039050766,
      "grad_norm": 2.7626283168792725,
      "learning_rate": 1.8074309381157266e-05,
      "loss": 1.0948,
      "step": 9990
    },
    {
      "epoch": 1.2015019525382997,
      "grad_norm": 2.366489887237549,
      "learning_rate": 1.8062246169930436e-05,
      "loss": 1.1291,
      "step": 10000
    },
    {
      "epoch": 1.2027035145689395,
      "grad_norm": 2.4311580657958984,
      "learning_rate": 1.805018295870361e-05,
      "loss": 1.1433,
      "step": 10010
    },
    {
      "epoch": 1.2039050765995793,
      "grad_norm": 2.463174343109131,
      "learning_rate": 1.803811974747678e-05,
      "loss": 1.1269,
      "step": 10020
    },
    {
      "epoch": 1.2051066386302192,
      "grad_norm": 2.627350330352783,
      "learning_rate": 1.802605653624995e-05,
      "loss": 1.0793,
      "step": 10030
    },
    {
      "epoch": 1.2063082006608592,
      "grad_norm": 2.544891595840454,
      "learning_rate": 1.801399332502312e-05,
      "loss": 1.1283,
      "step": 10040
    },
    {
      "epoch": 1.207509762691499,
      "grad_norm": 2.7824032306671143,
      "learning_rate": 1.800193011379629e-05,
      "loss": 1.1178,
      "step": 10050
    },
    {
      "epoch": 1.2087113247221388,
      "grad_norm": 2.7939674854278564,
      "learning_rate": 1.7989866902569464e-05,
      "loss": 1.0988,
      "step": 10060
    },
    {
      "epoch": 1.2099128867527786,
      "grad_norm": 2.897395372390747,
      "learning_rate": 1.7977803691342634e-05,
      "loss": 1.1331,
      "step": 10070
    },
    {
      "epoch": 1.2111144487834185,
      "grad_norm": 2.940838575363159,
      "learning_rate": 1.7965740480115807e-05,
      "loss": 1.1461,
      "step": 10080
    },
    {
      "epoch": 1.2123160108140583,
      "grad_norm": 2.1736011505126953,
      "learning_rate": 1.795367726888898e-05,
      "loss": 1.1414,
      "step": 10090
    },
    {
      "epoch": 1.213517572844698,
      "grad_norm": 3.040576457977295,
      "learning_rate": 1.794161405766215e-05,
      "loss": 1.1778,
      "step": 10100
    },
    {
      "epoch": 1.214719134875338,
      "grad_norm": 2.6963746547698975,
      "learning_rate": 1.7929550846435323e-05,
      "loss": 1.0605,
      "step": 10110
    },
    {
      "epoch": 1.2159206969059777,
      "grad_norm": 2.2538833618164062,
      "learning_rate": 1.7917487635208493e-05,
      "loss": 1.1091,
      "step": 10120
    },
    {
      "epoch": 1.2171222589366175,
      "grad_norm": 2.912287473678589,
      "learning_rate": 1.7905424423981663e-05,
      "loss": 1.1268,
      "step": 10130
    },
    {
      "epoch": 1.2183238209672576,
      "grad_norm": 2.6774213314056396,
      "learning_rate": 1.7893361212754836e-05,
      "loss": 1.1531,
      "step": 10140
    },
    {
      "epoch": 1.2195253829978974,
      "grad_norm": 3.0237972736358643,
      "learning_rate": 1.7881298001528006e-05,
      "loss": 1.0941,
      "step": 10150
    },
    {
      "epoch": 1.2207269450285372,
      "grad_norm": 2.5960967540740967,
      "learning_rate": 1.786923479030118e-05,
      "loss": 1.0835,
      "step": 10160
    },
    {
      "epoch": 1.221928507059177,
      "grad_norm": 2.419473886489868,
      "learning_rate": 1.7857171579074352e-05,
      "loss": 1.1059,
      "step": 10170
    },
    {
      "epoch": 1.2231300690898168,
      "grad_norm": 2.7052671909332275,
      "learning_rate": 1.784510836784752e-05,
      "loss": 1.0839,
      "step": 10180
    },
    {
      "epoch": 1.2243316311204566,
      "grad_norm": 2.6281557083129883,
      "learning_rate": 1.7833045156620695e-05,
      "loss": 1.1459,
      "step": 10190
    },
    {
      "epoch": 1.2255331931510964,
      "grad_norm": 2.7574613094329834,
      "learning_rate": 1.782098194539386e-05,
      "loss": 1.1217,
      "step": 10200
    },
    {
      "epoch": 1.2267347551817362,
      "grad_norm": 2.923673629760742,
      "learning_rate": 1.7808918734167034e-05,
      "loss": 1.1138,
      "step": 10210
    },
    {
      "epoch": 1.227936317212376,
      "grad_norm": 2.5467255115509033,
      "learning_rate": 1.7796855522940207e-05,
      "loss": 1.0822,
      "step": 10220
    },
    {
      "epoch": 1.2291378792430159,
      "grad_norm": 2.530773162841797,
      "learning_rate": 1.7784792311713377e-05,
      "loss": 1.111,
      "step": 10230
    },
    {
      "epoch": 1.2303394412736557,
      "grad_norm": 2.394350051879883,
      "learning_rate": 1.777272910048655e-05,
      "loss": 1.1143,
      "step": 10240
    },
    {
      "epoch": 1.2315410033042955,
      "grad_norm": 3.0926122665405273,
      "learning_rate": 1.7760665889259723e-05,
      "loss": 1.1432,
      "step": 10250
    },
    {
      "epoch": 1.2327425653349353,
      "grad_norm": 2.6780238151550293,
      "learning_rate": 1.7748602678032893e-05,
      "loss": 1.1145,
      "step": 10260
    },
    {
      "epoch": 1.2339441273655751,
      "grad_norm": 2.4187068939208984,
      "learning_rate": 1.7736539466806066e-05,
      "loss": 1.1469,
      "step": 10270
    },
    {
      "epoch": 1.2351456893962152,
      "grad_norm": 2.401693344116211,
      "learning_rate": 1.7724476255579236e-05,
      "loss": 1.1182,
      "step": 10280
    },
    {
      "epoch": 1.236347251426855,
      "grad_norm": 2.97987699508667,
      "learning_rate": 1.7712413044352406e-05,
      "loss": 1.0947,
      "step": 10290
    },
    {
      "epoch": 1.2375488134574948,
      "grad_norm": 2.607510566711426,
      "learning_rate": 1.770034983312558e-05,
      "loss": 1.1278,
      "step": 10300
    },
    {
      "epoch": 1.2387503754881346,
      "grad_norm": 2.4724056720733643,
      "learning_rate": 1.768828662189875e-05,
      "loss": 1.0818,
      "step": 10310
    },
    {
      "epoch": 1.2399519375187744,
      "grad_norm": 2.4947962760925293,
      "learning_rate": 1.7676223410671922e-05,
      "loss": 1.1397,
      "step": 10320
    },
    {
      "epoch": 1.2411534995494142,
      "grad_norm": 2.778668165206909,
      "learning_rate": 1.766416019944509e-05,
      "loss": 1.0907,
      "step": 10330
    },
    {
      "epoch": 1.242355061580054,
      "grad_norm": 2.9384002685546875,
      "learning_rate": 1.7652096988218265e-05,
      "loss": 1.0931,
      "step": 10340
    },
    {
      "epoch": 1.2435566236106939,
      "grad_norm": 2.8737430572509766,
      "learning_rate": 1.7640033776991438e-05,
      "loss": 1.1943,
      "step": 10350
    },
    {
      "epoch": 1.2447581856413337,
      "grad_norm": 2.9228053092956543,
      "learning_rate": 1.7627970565764608e-05,
      "loss": 1.1361,
      "step": 10360
    },
    {
      "epoch": 1.2459597476719735,
      "grad_norm": 2.5312933921813965,
      "learning_rate": 1.7615907354537777e-05,
      "loss": 1.1785,
      "step": 10370
    },
    {
      "epoch": 1.2471613097026135,
      "grad_norm": 2.684603452682495,
      "learning_rate": 1.7603844143310947e-05,
      "loss": 1.1617,
      "step": 10380
    },
    {
      "epoch": 1.2483628717332533,
      "grad_norm": 2.482400417327881,
      "learning_rate": 1.759178093208412e-05,
      "loss": 1.1429,
      "step": 10390
    },
    {
      "epoch": 1.2495644337638931,
      "grad_norm": 3.017789602279663,
      "learning_rate": 1.7579717720857293e-05,
      "loss": 1.1419,
      "step": 10400
    },
    {
      "epoch": 1.250765995794533,
      "grad_norm": 2.5226244926452637,
      "learning_rate": 1.7567654509630463e-05,
      "loss": 1.1156,
      "step": 10410
    },
    {
      "epoch": 1.2519675578251728,
      "grad_norm": 2.705683469772339,
      "learning_rate": 1.7555591298403636e-05,
      "loss": 1.1221,
      "step": 10420
    },
    {
      "epoch": 1.2531691198558126,
      "grad_norm": 3.2882442474365234,
      "learning_rate": 1.754352808717681e-05,
      "loss": 1.1674,
      "step": 10430
    },
    {
      "epoch": 1.2543706818864524,
      "grad_norm": 3.291767120361328,
      "learning_rate": 1.753146487594998e-05,
      "loss": 1.1147,
      "step": 10440
    },
    {
      "epoch": 1.2555722439170922,
      "grad_norm": 2.5852925777435303,
      "learning_rate": 1.751940166472315e-05,
      "loss": 1.1302,
      "step": 10450
    },
    {
      "epoch": 1.256773805947732,
      "grad_norm": 2.4972071647644043,
      "learning_rate": 1.750733845349632e-05,
      "loss": 1.1298,
      "step": 10460
    },
    {
      "epoch": 1.2579753679783718,
      "grad_norm": 2.9860544204711914,
      "learning_rate": 1.7495275242269492e-05,
      "loss": 1.1168,
      "step": 10470
    },
    {
      "epoch": 1.2591769300090117,
      "grad_norm": 2.574989080429077,
      "learning_rate": 1.7483212031042665e-05,
      "loss": 1.1156,
      "step": 10480
    },
    {
      "epoch": 1.2603784920396515,
      "grad_norm": 2.769526958465576,
      "learning_rate": 1.7471148819815835e-05,
      "loss": 1.1564,
      "step": 10490
    },
    {
      "epoch": 1.2615800540702913,
      "grad_norm": 2.587695598602295,
      "learning_rate": 1.7459085608589008e-05,
      "loss": 1.126,
      "step": 10500
    },
    {
      "epoch": 1.262781616100931,
      "grad_norm": 2.7012693881988525,
      "learning_rate": 1.7447022397362178e-05,
      "loss": 1.0887,
      "step": 10510
    },
    {
      "epoch": 1.263983178131571,
      "grad_norm": 2.3044867515563965,
      "learning_rate": 1.743495918613535e-05,
      "loss": 1.1342,
      "step": 10520
    },
    {
      "epoch": 1.265184740162211,
      "grad_norm": 2.494374990463257,
      "learning_rate": 1.7422895974908524e-05,
      "loss": 1.1416,
      "step": 10530
    },
    {
      "epoch": 1.2663863021928508,
      "grad_norm": 3.3579931259155273,
      "learning_rate": 1.741083276368169e-05,
      "loss": 1.1353,
      "step": 10540
    },
    {
      "epoch": 1.2675878642234906,
      "grad_norm": 2.386317491531372,
      "learning_rate": 1.7398769552454863e-05,
      "loss": 1.0922,
      "step": 10550
    },
    {
      "epoch": 1.2687894262541304,
      "grad_norm": 2.6633849143981934,
      "learning_rate": 1.7386706341228033e-05,
      "loss": 1.1585,
      "step": 10560
    },
    {
      "epoch": 1.2699909882847702,
      "grad_norm": 2.7313971519470215,
      "learning_rate": 1.7374643130001206e-05,
      "loss": 1.0839,
      "step": 10570
    },
    {
      "epoch": 1.27119255031541,
      "grad_norm": 2.2689051628112793,
      "learning_rate": 1.736257991877438e-05,
      "loss": 1.1614,
      "step": 10580
    },
    {
      "epoch": 1.2723941123460498,
      "grad_norm": 2.530855417251587,
      "learning_rate": 1.735051670754755e-05,
      "loss": 1.1539,
      "step": 10590
    },
    {
      "epoch": 1.2735956743766896,
      "grad_norm": 2.7105765342712402,
      "learning_rate": 1.7338453496320722e-05,
      "loss": 1.1102,
      "step": 10600
    },
    {
      "epoch": 1.2747972364073294,
      "grad_norm": 2.5600969791412354,
      "learning_rate": 1.7326390285093895e-05,
      "loss": 1.1265,
      "step": 10610
    },
    {
      "epoch": 1.2759987984379695,
      "grad_norm": 2.619292736053467,
      "learning_rate": 1.7314327073867062e-05,
      "loss": 1.0784,
      "step": 10620
    },
    {
      "epoch": 1.2772003604686093,
      "grad_norm": 3.1919069290161133,
      "learning_rate": 1.7302263862640235e-05,
      "loss": 1.1054,
      "step": 10630
    },
    {
      "epoch": 1.278401922499249,
      "grad_norm": 2.736973762512207,
      "learning_rate": 1.7290200651413405e-05,
      "loss": 1.1835,
      "step": 10640
    },
    {
      "epoch": 1.279603484529889,
      "grad_norm": 2.952831983566284,
      "learning_rate": 1.7278137440186578e-05,
      "loss": 1.1533,
      "step": 10650
    },
    {
      "epoch": 1.2808050465605287,
      "grad_norm": 2.595137357711792,
      "learning_rate": 1.726607422895975e-05,
      "loss": 1.1016,
      "step": 10660
    },
    {
      "epoch": 1.2820066085911686,
      "grad_norm": 2.740215539932251,
      "learning_rate": 1.725401101773292e-05,
      "loss": 1.1459,
      "step": 10670
    },
    {
      "epoch": 1.2832081706218084,
      "grad_norm": 2.786129951477051,
      "learning_rate": 1.7241947806506094e-05,
      "loss": 1.1526,
      "step": 10680
    },
    {
      "epoch": 1.2844097326524482,
      "grad_norm": 2.7337958812713623,
      "learning_rate": 1.7229884595279263e-05,
      "loss": 1.1661,
      "step": 10690
    },
    {
      "epoch": 1.285611294683088,
      "grad_norm": 2.669414520263672,
      "learning_rate": 1.7217821384052437e-05,
      "loss": 1.1267,
      "step": 10700
    },
    {
      "epoch": 1.2868128567137278,
      "grad_norm": 2.601597785949707,
      "learning_rate": 1.7205758172825606e-05,
      "loss": 1.1606,
      "step": 10710
    },
    {
      "epoch": 1.2880144187443676,
      "grad_norm": 2.433917760848999,
      "learning_rate": 1.7193694961598776e-05,
      "loss": 1.0836,
      "step": 10720
    },
    {
      "epoch": 1.2892159807750074,
      "grad_norm": 2.486668348312378,
      "learning_rate": 1.718163175037195e-05,
      "loss": 1.1389,
      "step": 10730
    },
    {
      "epoch": 1.2904175428056472,
      "grad_norm": 2.722788095474243,
      "learning_rate": 1.7169568539145122e-05,
      "loss": 1.1541,
      "step": 10740
    },
    {
      "epoch": 1.291619104836287,
      "grad_norm": 2.957554578781128,
      "learning_rate": 1.7157505327918292e-05,
      "loss": 1.1194,
      "step": 10750
    },
    {
      "epoch": 1.2928206668669269,
      "grad_norm": 3.3640804290771484,
      "learning_rate": 1.7145442116691465e-05,
      "loss": 1.1283,
      "step": 10760
    },
    {
      "epoch": 1.294022228897567,
      "grad_norm": 2.6044046878814697,
      "learning_rate": 1.7133378905464635e-05,
      "loss": 1.1465,
      "step": 10770
    },
    {
      "epoch": 1.2952237909282067,
      "grad_norm": 2.6596999168395996,
      "learning_rate": 1.7121315694237808e-05,
      "loss": 1.1243,
      "step": 10780
    },
    {
      "epoch": 1.2964253529588465,
      "grad_norm": 2.379389762878418,
      "learning_rate": 1.7109252483010978e-05,
      "loss": 1.0915,
      "step": 10790
    },
    {
      "epoch": 1.2976269149894863,
      "grad_norm": 2.8395633697509766,
      "learning_rate": 1.7097189271784148e-05,
      "loss": 1.1514,
      "step": 10800
    },
    {
      "epoch": 1.2988284770201262,
      "grad_norm": 2.596677780151367,
      "learning_rate": 1.708512606055732e-05,
      "loss": 1.1326,
      "step": 10810
    },
    {
      "epoch": 1.300030039050766,
      "grad_norm": 2.757807731628418,
      "learning_rate": 1.707306284933049e-05,
      "loss": 1.1274,
      "step": 10820
    },
    {
      "epoch": 1.3012316010814058,
      "grad_norm": 2.978224754333496,
      "learning_rate": 1.7060999638103664e-05,
      "loss": 1.116,
      "step": 10830
    },
    {
      "epoch": 1.3024331631120456,
      "grad_norm": 2.518561840057373,
      "learning_rate": 1.7048936426876837e-05,
      "loss": 1.0508,
      "step": 10840
    },
    {
      "epoch": 1.3036347251426854,
      "grad_norm": 2.7522027492523193,
      "learning_rate": 1.7036873215650007e-05,
      "loss": 1.0709,
      "step": 10850
    },
    {
      "epoch": 1.3048362871733254,
      "grad_norm": 2.8265676498413086,
      "learning_rate": 1.702601632554586e-05,
      "loss": 1.1657,
      "step": 10860
    },
    {
      "epoch": 1.3060378492039653,
      "grad_norm": 2.6359329223632812,
      "learning_rate": 1.701395311431903e-05,
      "loss": 1.1295,
      "step": 10870
    },
    {
      "epoch": 1.307239411234605,
      "grad_norm": 2.8716092109680176,
      "learning_rate": 1.7001889903092204e-05,
      "loss": 1.1445,
      "step": 10880
    },
    {
      "epoch": 1.3084409732652449,
      "grad_norm": 2.8730006217956543,
      "learning_rate": 1.6989826691865374e-05,
      "loss": 1.1127,
      "step": 10890
    },
    {
      "epoch": 1.3096425352958847,
      "grad_norm": 2.4461669921875,
      "learning_rate": 1.6977763480638547e-05,
      "loss": 1.0952,
      "step": 10900
    },
    {
      "epoch": 1.3108440973265245,
      "grad_norm": 2.5842409133911133,
      "learning_rate": 1.6965700269411717e-05,
      "loss": 1.1798,
      "step": 10910
    },
    {
      "epoch": 1.3120456593571643,
      "grad_norm": 2.5422441959381104,
      "learning_rate": 1.695363705818489e-05,
      "loss": 1.1122,
      "step": 10920
    },
    {
      "epoch": 1.3132472213878041,
      "grad_norm": 2.53517746925354,
      "learning_rate": 1.6941573846958063e-05,
      "loss": 1.129,
      "step": 10930
    },
    {
      "epoch": 1.314448783418444,
      "grad_norm": 3.1331117153167725,
      "learning_rate": 1.6929510635731233e-05,
      "loss": 1.1125,
      "step": 10940
    },
    {
      "epoch": 1.3156503454490838,
      "grad_norm": 2.7041726112365723,
      "learning_rate": 1.6917447424504402e-05,
      "loss": 1.1017,
      "step": 10950
    },
    {
      "epoch": 1.3168519074797236,
      "grad_norm": 2.88090443611145,
      "learning_rate": 1.6905384213277575e-05,
      "loss": 1.1384,
      "step": 10960
    },
    {
      "epoch": 1.3180534695103634,
      "grad_norm": 3.307497978210449,
      "learning_rate": 1.6893321002050745e-05,
      "loss": 1.1487,
      "step": 10970
    },
    {
      "epoch": 1.3192550315410032,
      "grad_norm": 2.9407975673675537,
      "learning_rate": 1.6881257790823918e-05,
      "loss": 1.1006,
      "step": 10980
    },
    {
      "epoch": 1.320456593571643,
      "grad_norm": 2.7494776248931885,
      "learning_rate": 1.6869194579597088e-05,
      "loss": 1.1026,
      "step": 10990
    },
    {
      "epoch": 1.3216581556022828,
      "grad_norm": 2.9484143257141113,
      "learning_rate": 1.685713136837026e-05,
      "loss": 1.1133,
      "step": 11000
    },
    {
      "epoch": 1.3228597176329229,
      "grad_norm": 3.0232315063476562,
      "learning_rate": 1.6845068157143434e-05,
      "loss": 1.1493,
      "step": 11010
    },
    {
      "epoch": 1.3240612796635627,
      "grad_norm": 2.8326668739318848,
      "learning_rate": 1.6833004945916604e-05,
      "loss": 1.1371,
      "step": 11020
    },
    {
      "epoch": 1.3252628416942025,
      "grad_norm": 2.7217588424682617,
      "learning_rate": 1.6820941734689777e-05,
      "loss": 1.131,
      "step": 11030
    },
    {
      "epoch": 1.3264644037248423,
      "grad_norm": 2.466040849685669,
      "learning_rate": 1.6808878523462944e-05,
      "loss": 1.0639,
      "step": 11040
    },
    {
      "epoch": 1.3276659657554821,
      "grad_norm": 2.8753662109375,
      "learning_rate": 1.6796815312236117e-05,
      "loss": 1.1262,
      "step": 11050
    },
    {
      "epoch": 1.328867527786122,
      "grad_norm": 2.8399336338043213,
      "learning_rate": 1.678475210100929e-05,
      "loss": 1.1632,
      "step": 11060
    },
    {
      "epoch": 1.3300690898167618,
      "grad_norm": 2.6520025730133057,
      "learning_rate": 1.677268888978246e-05,
      "loss": 1.1101,
      "step": 11070
    },
    {
      "epoch": 1.3312706518474016,
      "grad_norm": 2.980046272277832,
      "learning_rate": 1.6760625678555633e-05,
      "loss": 1.1157,
      "step": 11080
    },
    {
      "epoch": 1.3324722138780414,
      "grad_norm": 2.6418557167053223,
      "learning_rate": 1.6748562467328802e-05,
      "loss": 1.1112,
      "step": 11090
    },
    {
      "epoch": 1.3336737759086814,
      "grad_norm": 2.501659870147705,
      "learning_rate": 1.6736499256101976e-05,
      "loss": 1.1275,
      "step": 11100
    },
    {
      "epoch": 1.3348753379393212,
      "grad_norm": 2.4262311458587646,
      "learning_rate": 1.672443604487515e-05,
      "loss": 1.1772,
      "step": 11110
    },
    {
      "epoch": 1.336076899969961,
      "grad_norm": 2.737577199935913,
      "learning_rate": 1.6712372833648315e-05,
      "loss": 1.1615,
      "step": 11120
    },
    {
      "epoch": 1.3372784620006009,
      "grad_norm": 2.723907709121704,
      "learning_rate": 1.6700309622421488e-05,
      "loss": 1.1447,
      "step": 11130
    },
    {
      "epoch": 1.3384800240312407,
      "grad_norm": 2.7010979652404785,
      "learning_rate": 1.668824641119466e-05,
      "loss": 1.1668,
      "step": 11140
    },
    {
      "epoch": 1.3396815860618805,
      "grad_norm": 2.773145914077759,
      "learning_rate": 1.667618319996783e-05,
      "loss": 1.1302,
      "step": 11150
    },
    {
      "epoch": 1.3408831480925203,
      "grad_norm": 2.5884525775909424,
      "learning_rate": 1.6664119988741004e-05,
      "loss": 1.0845,
      "step": 11160
    },
    {
      "epoch": 1.34208471012316,
      "grad_norm": 3.266110420227051,
      "learning_rate": 1.6652056777514174e-05,
      "loss": 1.1706,
      "step": 11170
    },
    {
      "epoch": 1.3432862721538,
      "grad_norm": 2.3880374431610107,
      "learning_rate": 1.6639993566287347e-05,
      "loss": 1.1497,
      "step": 11180
    },
    {
      "epoch": 1.3444878341844397,
      "grad_norm": 2.442629814147949,
      "learning_rate": 1.662793035506052e-05,
      "loss": 1.0891,
      "step": 11190
    },
    {
      "epoch": 1.3456893962150795,
      "grad_norm": 2.7365753650665283,
      "learning_rate": 1.6615867143833687e-05,
      "loss": 1.1511,
      "step": 11200
    },
    {
      "epoch": 1.3468909582457194,
      "grad_norm": 2.6808717250823975,
      "learning_rate": 1.660380393260686e-05,
      "loss": 1.1106,
      "step": 11210
    },
    {
      "epoch": 1.3480925202763592,
      "grad_norm": 2.6024906635284424,
      "learning_rate": 1.659174072138003e-05,
      "loss": 1.1715,
      "step": 11220
    },
    {
      "epoch": 1.349294082306999,
      "grad_norm": 2.526949405670166,
      "learning_rate": 1.6579677510153203e-05,
      "loss": 1.1497,
      "step": 11230
    },
    {
      "epoch": 1.3504956443376388,
      "grad_norm": 2.4813501834869385,
      "learning_rate": 1.6567614298926376e-05,
      "loss": 1.0679,
      "step": 11240
    },
    {
      "epoch": 1.3516972063682788,
      "grad_norm": 2.3612000942230225,
      "learning_rate": 1.6555551087699546e-05,
      "loss": 1.1204,
      "step": 11250
    },
    {
      "epoch": 1.3528987683989186,
      "grad_norm": 2.859198808670044,
      "learning_rate": 1.654348787647272e-05,
      "loss": 1.1258,
      "step": 11260
    },
    {
      "epoch": 1.3541003304295585,
      "grad_norm": 2.709778070449829,
      "learning_rate": 1.653142466524589e-05,
      "loss": 1.1365,
      "step": 11270
    },
    {
      "epoch": 1.3553018924601983,
      "grad_norm": 2.4905624389648438,
      "learning_rate": 1.651936145401906e-05,
      "loss": 1.1042,
      "step": 11280
    },
    {
      "epoch": 1.356503454490838,
      "grad_norm": 3.7208309173583984,
      "learning_rate": 1.650729824279223e-05,
      "loss": 1.1114,
      "step": 11290
    },
    {
      "epoch": 1.357705016521478,
      "grad_norm": 2.824780225753784,
      "learning_rate": 1.64952350315654e-05,
      "loss": 1.156,
      "step": 11300
    },
    {
      "epoch": 1.3589065785521177,
      "grad_norm": 2.609941005706787,
      "learning_rate": 1.6483171820338574e-05,
      "loss": 1.0709,
      "step": 11310
    },
    {
      "epoch": 1.3601081405827575,
      "grad_norm": 3.203636646270752,
      "learning_rate": 1.6471108609111747e-05,
      "loss": 1.1082,
      "step": 11320
    },
    {
      "epoch": 1.3613097026133973,
      "grad_norm": 2.661813735961914,
      "learning_rate": 1.6459045397884917e-05,
      "loss": 1.1225,
      "step": 11330
    },
    {
      "epoch": 1.3625112646440374,
      "grad_norm": 2.5487327575683594,
      "learning_rate": 1.644698218665809e-05,
      "loss": 1.1691,
      "step": 11340
    },
    {
      "epoch": 1.3637128266746772,
      "grad_norm": 2.618924140930176,
      "learning_rate": 1.643491897543126e-05,
      "loss": 1.1197,
      "step": 11350
    },
    {
      "epoch": 1.364914388705317,
      "grad_norm": 2.8548026084899902,
      "learning_rate": 1.6422855764204433e-05,
      "loss": 1.1246,
      "step": 11360
    },
    {
      "epoch": 1.3661159507359568,
      "grad_norm": 3.0928666591644287,
      "learning_rate": 1.6410792552977603e-05,
      "loss": 1.1797,
      "step": 11370
    },
    {
      "epoch": 1.3673175127665966,
      "grad_norm": 2.7932093143463135,
      "learning_rate": 1.6398729341750773e-05,
      "loss": 1.131,
      "step": 11380
    },
    {
      "epoch": 1.3685190747972364,
      "grad_norm": 2.8721611499786377,
      "learning_rate": 1.6386666130523946e-05,
      "loss": 1.0625,
      "step": 11390
    },
    {
      "epoch": 1.3697206368278763,
      "grad_norm": 2.7520809173583984,
      "learning_rate": 1.6374602919297115e-05,
      "loss": 1.156,
      "step": 11400
    },
    {
      "epoch": 1.370922198858516,
      "grad_norm": 2.817314624786377,
      "learning_rate": 1.636253970807029e-05,
      "loss": 1.1203,
      "step": 11410
    },
    {
      "epoch": 1.3721237608891559,
      "grad_norm": 2.676278591156006,
      "learning_rate": 1.6350476496843462e-05,
      "loss": 1.145,
      "step": 11420
    },
    {
      "epoch": 1.3733253229197957,
      "grad_norm": 2.8790738582611084,
      "learning_rate": 1.633841328561663e-05,
      "loss": 1.1231,
      "step": 11430
    },
    {
      "epoch": 1.3745268849504355,
      "grad_norm": 2.7020339965820312,
      "learning_rate": 1.6326350074389805e-05,
      "loss": 1.129,
      "step": 11440
    },
    {
      "epoch": 1.3757284469810753,
      "grad_norm": 2.631075620651245,
      "learning_rate": 1.6314286863162974e-05,
      "loss": 1.1049,
      "step": 11450
    },
    {
      "epoch": 1.3769300090117151,
      "grad_norm": 2.718146800994873,
      "learning_rate": 1.6302223651936144e-05,
      "loss": 1.1016,
      "step": 11460
    },
    {
      "epoch": 1.378131571042355,
      "grad_norm": 3.2379467487335205,
      "learning_rate": 1.6290160440709317e-05,
      "loss": 1.1437,
      "step": 11470
    },
    {
      "epoch": 1.3793331330729948,
      "grad_norm": 2.6552834510803223,
      "learning_rate": 1.6278097229482487e-05,
      "loss": 1.1566,
      "step": 11480
    },
    {
      "epoch": 1.3805346951036348,
      "grad_norm": 2.814174175262451,
      "learning_rate": 1.626603401825566e-05,
      "loss": 1.129,
      "step": 11490
    },
    {
      "epoch": 1.3817362571342746,
      "grad_norm": 2.3934834003448486,
      "learning_rate": 1.6253970807028833e-05,
      "loss": 1.1183,
      "step": 11500
    },
    {
      "epoch": 1.3829378191649144,
      "grad_norm": 2.6576192378997803,
      "learning_rate": 1.6241907595802003e-05,
      "loss": 1.1552,
      "step": 11510
    },
    {
      "epoch": 1.3841393811955542,
      "grad_norm": 2.8357162475585938,
      "learning_rate": 1.6229844384575176e-05,
      "loss": 1.1568,
      "step": 11520
    },
    {
      "epoch": 1.385340943226194,
      "grad_norm": 2.4678151607513428,
      "learning_rate": 1.6217781173348346e-05,
      "loss": 1.1519,
      "step": 11530
    },
    {
      "epoch": 1.3865425052568339,
      "grad_norm": 2.7283246517181396,
      "learning_rate": 1.6205717962121516e-05,
      "loss": 1.1629,
      "step": 11540
    },
    {
      "epoch": 1.3877440672874737,
      "grad_norm": 2.5875017642974854,
      "learning_rate": 1.619365475089469e-05,
      "loss": 1.1517,
      "step": 11550
    },
    {
      "epoch": 1.3889456293181135,
      "grad_norm": 2.4782540798187256,
      "learning_rate": 1.618159153966786e-05,
      "loss": 1.0993,
      "step": 11560
    },
    {
      "epoch": 1.3901471913487533,
      "grad_norm": 2.644097328186035,
      "learning_rate": 1.616952832844103e-05,
      "loss": 1.1348,
      "step": 11570
    },
    {
      "epoch": 1.3913487533793933,
      "grad_norm": 2.61492657661438,
      "learning_rate": 1.61574651172142e-05,
      "loss": 1.1365,
      "step": 11580
    },
    {
      "epoch": 1.3925503154100332,
      "grad_norm": 2.5492031574249268,
      "learning_rate": 1.6145401905987375e-05,
      "loss": 1.0741,
      "step": 11590
    },
    {
      "epoch": 1.393751877440673,
      "grad_norm": 3.100792169570923,
      "learning_rate": 1.6133338694760548e-05,
      "loss": 1.1397,
      "step": 11600
    },
    {
      "epoch": 1.3949534394713128,
      "grad_norm": 2.877500057220459,
      "learning_rate": 1.6121275483533717e-05,
      "loss": 1.0657,
      "step": 11610
    },
    {
      "epoch": 1.3961550015019526,
      "grad_norm": 3.062544107437134,
      "learning_rate": 1.6109212272306887e-05,
      "loss": 1.1538,
      "step": 11620
    },
    {
      "epoch": 1.3973565635325924,
      "grad_norm": 2.6135737895965576,
      "learning_rate": 1.609714906108006e-05,
      "loss": 1.1186,
      "step": 11630
    },
    {
      "epoch": 1.3985581255632322,
      "grad_norm": 2.4602627754211426,
      "learning_rate": 1.608508584985323e-05,
      "loss": 1.0988,
      "step": 11640
    },
    {
      "epoch": 1.399759687593872,
      "grad_norm": 2.553938388824463,
      "learning_rate": 1.6073022638626403e-05,
      "loss": 1.0938,
      "step": 11650
    },
    {
      "epoch": 1.4009612496245119,
      "grad_norm": 3.13604474067688,
      "learning_rate": 1.6060959427399573e-05,
      "loss": 1.1046,
      "step": 11660
    },
    {
      "epoch": 1.4021628116551517,
      "grad_norm": 2.622894287109375,
      "learning_rate": 1.6048896216172746e-05,
      "loss": 1.085,
      "step": 11670
    },
    {
      "epoch": 1.4033643736857915,
      "grad_norm": 2.2053732872009277,
      "learning_rate": 1.603683300494592e-05,
      "loss": 1.0827,
      "step": 11680
    },
    {
      "epoch": 1.4045659357164313,
      "grad_norm": 2.4113056659698486,
      "learning_rate": 1.602476979371909e-05,
      "loss": 1.1273,
      "step": 11690
    },
    {
      "epoch": 1.405767497747071,
      "grad_norm": 2.7448432445526123,
      "learning_rate": 1.6012706582492262e-05,
      "loss": 1.1145,
      "step": 11700
    },
    {
      "epoch": 1.406969059777711,
      "grad_norm": 2.6369504928588867,
      "learning_rate": 1.600064337126543e-05,
      "loss": 1.1174,
      "step": 11710
    },
    {
      "epoch": 1.4081706218083507,
      "grad_norm": 2.971128225326538,
      "learning_rate": 1.59885801600386e-05,
      "loss": 1.092,
      "step": 11720
    },
    {
      "epoch": 1.4093721838389908,
      "grad_norm": 2.7329580783843994,
      "learning_rate": 1.5976516948811775e-05,
      "loss": 1.0849,
      "step": 11730
    },
    {
      "epoch": 1.4105737458696306,
      "grad_norm": 2.4404895305633545,
      "learning_rate": 1.5964453737584944e-05,
      "loss": 1.0987,
      "step": 11740
    },
    {
      "epoch": 1.4117753079002704,
      "grad_norm": 2.5912904739379883,
      "learning_rate": 1.5952390526358118e-05,
      "loss": 1.1576,
      "step": 11750
    },
    {
      "epoch": 1.4129768699309102,
      "grad_norm": 3.2656447887420654,
      "learning_rate": 1.5940327315131287e-05,
      "loss": 1.1228,
      "step": 11760
    },
    {
      "epoch": 1.41417843196155,
      "grad_norm": 2.573805570602417,
      "learning_rate": 1.592826410390446e-05,
      "loss": 1.1541,
      "step": 11770
    },
    {
      "epoch": 1.4153799939921898,
      "grad_norm": 2.555384635925293,
      "learning_rate": 1.5916200892677634e-05,
      "loss": 1.0926,
      "step": 11780
    },
    {
      "epoch": 1.4165815560228296,
      "grad_norm": 2.7658205032348633,
      "learning_rate": 1.59041376814508e-05,
      "loss": 1.1205,
      "step": 11790
    },
    {
      "epoch": 1.4177831180534695,
      "grad_norm": 3.0175065994262695,
      "learning_rate": 1.5892074470223973e-05,
      "loss": 1.1241,
      "step": 11800
    },
    {
      "epoch": 1.4189846800841093,
      "grad_norm": 2.734518527984619,
      "learning_rate": 1.5880011258997146e-05,
      "loss": 1.1255,
      "step": 11810
    },
    {
      "epoch": 1.4201862421147493,
      "grad_norm": 2.497652769088745,
      "learning_rate": 1.5867948047770316e-05,
      "loss": 1.1247,
      "step": 11820
    },
    {
      "epoch": 1.4213878041453891,
      "grad_norm": 2.9836173057556152,
      "learning_rate": 1.585588483654349e-05,
      "loss": 1.1069,
      "step": 11830
    },
    {
      "epoch": 1.422589366176029,
      "grad_norm": 3.0451767444610596,
      "learning_rate": 1.584382162531666e-05,
      "loss": 1.1155,
      "step": 11840
    },
    {
      "epoch": 1.4237909282066687,
      "grad_norm": 3.0895345211029053,
      "learning_rate": 1.5831758414089832e-05,
      "loss": 1.1591,
      "step": 11850
    },
    {
      "epoch": 1.4249924902373086,
      "grad_norm": 3.3182077407836914,
      "learning_rate": 1.5819695202863005e-05,
      "loss": 1.1358,
      "step": 11860
    },
    {
      "epoch": 1.4261940522679484,
      "grad_norm": 2.4676272869110107,
      "learning_rate": 1.5807631991636175e-05,
      "loss": 1.1446,
      "step": 11870
    },
    {
      "epoch": 1.4273956142985882,
      "grad_norm": 3.1287806034088135,
      "learning_rate": 1.5795568780409345e-05,
      "loss": 1.0613,
      "step": 11880
    },
    {
      "epoch": 1.428597176329228,
      "grad_norm": 2.58247709274292,
      "learning_rate": 1.5783505569182514e-05,
      "loss": 1.124,
      "step": 11890
    },
    {
      "epoch": 1.4297987383598678,
      "grad_norm": 2.847074031829834,
      "learning_rate": 1.5771442357955687e-05,
      "loss": 1.1023,
      "step": 11900
    },
    {
      "epoch": 1.4310003003905076,
      "grad_norm": 2.8126423358917236,
      "learning_rate": 1.575937914672886e-05,
      "loss": 1.1167,
      "step": 11910
    },
    {
      "epoch": 1.4322018624211474,
      "grad_norm": 2.6199429035186768,
      "learning_rate": 1.574731593550203e-05,
      "loss": 1.0836,
      "step": 11920
    },
    {
      "epoch": 1.4334034244517873,
      "grad_norm": 2.6052968502044678,
      "learning_rate": 1.5735252724275204e-05,
      "loss": 1.1318,
      "step": 11930
    },
    {
      "epoch": 1.434604986482427,
      "grad_norm": 3.1860177516937256,
      "learning_rate": 1.5723189513048373e-05,
      "loss": 1.1358,
      "step": 11940
    },
    {
      "epoch": 1.4358065485130669,
      "grad_norm": 2.9200522899627686,
      "learning_rate": 1.5711126301821546e-05,
      "loss": 1.0939,
      "step": 11950
    },
    {
      "epoch": 1.4370081105437067,
      "grad_norm": 2.7430849075317383,
      "learning_rate": 1.5699063090594716e-05,
      "loss": 1.1222,
      "step": 11960
    },
    {
      "epoch": 1.4382096725743467,
      "grad_norm": 3.0239593982696533,
      "learning_rate": 1.5686999879367886e-05,
      "loss": 1.1345,
      "step": 11970
    },
    {
      "epoch": 1.4394112346049865,
      "grad_norm": 2.9680299758911133,
      "learning_rate": 1.567493666814106e-05,
      "loss": 1.1694,
      "step": 11980
    },
    {
      "epoch": 1.4406127966356264,
      "grad_norm": 2.7509965896606445,
      "learning_rate": 1.5662873456914232e-05,
      "loss": 1.1049,
      "step": 11990
    },
    {
      "epoch": 1.4418143586662662,
      "grad_norm": 3.036421060562134,
      "learning_rate": 1.5650810245687402e-05,
      "loss": 1.1158,
      "step": 12000
    },
    {
      "epoch": 1.443015920696906,
      "grad_norm": 2.6016128063201904,
      "learning_rate": 1.5638747034460575e-05,
      "loss": 1.1504,
      "step": 12010
    },
    {
      "epoch": 1.4442174827275458,
      "grad_norm": 2.313816547393799,
      "learning_rate": 1.5626683823233745e-05,
      "loss": 1.0816,
      "step": 12020
    },
    {
      "epoch": 1.4454190447581856,
      "grad_norm": 3.611588716506958,
      "learning_rate": 1.5614620612006918e-05,
      "loss": 1.1094,
      "step": 12030
    },
    {
      "epoch": 1.4466206067888254,
      "grad_norm": 2.4333720207214355,
      "learning_rate": 1.5602557400780088e-05,
      "loss": 1.1048,
      "step": 12040
    },
    {
      "epoch": 1.4478221688194652,
      "grad_norm": 2.64644455909729,
      "learning_rate": 1.5590494189553257e-05,
      "loss": 1.0838,
      "step": 12050
    },
    {
      "epoch": 1.4490237308501053,
      "grad_norm": 2.800039768218994,
      "learning_rate": 1.557843097832643e-05,
      "loss": 1.0828,
      "step": 12060
    },
    {
      "epoch": 1.450225292880745,
      "grad_norm": 2.557469129562378,
      "learning_rate": 1.55663677670996e-05,
      "loss": 1.1502,
      "step": 12070
    },
    {
      "epoch": 1.451426854911385,
      "grad_norm": 2.803555965423584,
      "learning_rate": 1.5554304555872773e-05,
      "loss": 1.145,
      "step": 12080
    },
    {
      "epoch": 1.4526284169420247,
      "grad_norm": 2.440441131591797,
      "learning_rate": 1.5542241344645947e-05,
      "loss": 1.1037,
      "step": 12090
    },
    {
      "epoch": 1.4538299789726645,
      "grad_norm": 2.875213384628296,
      "learning_rate": 1.5530178133419116e-05,
      "loss": 1.1463,
      "step": 12100
    },
    {
      "epoch": 1.4550315410033043,
      "grad_norm": 2.724687099456787,
      "learning_rate": 1.551811492219229e-05,
      "loss": 1.0902,
      "step": 12110
    },
    {
      "epoch": 1.4562331030339442,
      "grad_norm": 2.8703420162200928,
      "learning_rate": 1.5506051710965463e-05,
      "loss": 1.1089,
      "step": 12120
    },
    {
      "epoch": 1.457434665064584,
      "grad_norm": 2.7359883785247803,
      "learning_rate": 1.549398849973863e-05,
      "loss": 1.1168,
      "step": 12130
    },
    {
      "epoch": 1.4586362270952238,
      "grad_norm": 2.5307412147521973,
      "learning_rate": 1.5481925288511802e-05,
      "loss": 1.0921,
      "step": 12140
    },
    {
      "epoch": 1.4598377891258636,
      "grad_norm": 3.0870652198791504,
      "learning_rate": 1.5469862077284972e-05,
      "loss": 1.1057,
      "step": 12150
    },
    {
      "epoch": 1.4610393511565034,
      "grad_norm": 3.2646079063415527,
      "learning_rate": 1.5457798866058145e-05,
      "loss": 1.1296,
      "step": 12160
    },
    {
      "epoch": 1.4622409131871432,
      "grad_norm": 2.7022652626037598,
      "learning_rate": 1.5445735654831318e-05,
      "loss": 1.1249,
      "step": 12170
    },
    {
      "epoch": 1.463442475217783,
      "grad_norm": 2.8720428943634033,
      "learning_rate": 1.5433672443604488e-05,
      "loss": 1.1362,
      "step": 12180
    },
    {
      "epoch": 1.4646440372484228,
      "grad_norm": 3.0668699741363525,
      "learning_rate": 1.542160923237766e-05,
      "loss": 1.1095,
      "step": 12190
    },
    {
      "epoch": 1.4658455992790627,
      "grad_norm": 2.4417574405670166,
      "learning_rate": 1.540954602115083e-05,
      "loss": 1.0748,
      "step": 12200
    },
    {
      "epoch": 1.4670471613097027,
      "grad_norm": 2.519059181213379,
      "learning_rate": 1.5397482809924e-05,
      "loss": 1.119,
      "step": 12210
    },
    {
      "epoch": 1.4682487233403425,
      "grad_norm": 2.8989126682281494,
      "learning_rate": 1.5385419598697174e-05,
      "loss": 1.1453,
      "step": 12220
    },
    {
      "epoch": 1.4694502853709823,
      "grad_norm": 2.7167367935180664,
      "learning_rate": 1.5373356387470343e-05,
      "loss": 1.1451,
      "step": 12230
    },
    {
      "epoch": 1.4706518474016221,
      "grad_norm": 2.6963565349578857,
      "learning_rate": 1.5361293176243516e-05,
      "loss": 1.0904,
      "step": 12240
    },
    {
      "epoch": 1.471853409432262,
      "grad_norm": 2.9467272758483887,
      "learning_rate": 1.5349229965016686e-05,
      "loss": 1.1612,
      "step": 12250
    },
    {
      "epoch": 1.4730549714629018,
      "grad_norm": 2.6051154136657715,
      "learning_rate": 1.533716675378986e-05,
      "loss": 1.1058,
      "step": 12260
    },
    {
      "epoch": 1.4742565334935416,
      "grad_norm": 3.223097562789917,
      "learning_rate": 1.5325103542563033e-05,
      "loss": 1.1384,
      "step": 12270
    },
    {
      "epoch": 1.4754580955241814,
      "grad_norm": 2.8972625732421875,
      "learning_rate": 1.5313040331336202e-05,
      "loss": 1.136,
      "step": 12280
    },
    {
      "epoch": 1.4766596575548212,
      "grad_norm": 2.869551181793213,
      "learning_rate": 1.5300977120109375e-05,
      "loss": 1.1232,
      "step": 12290
    },
    {
      "epoch": 1.477861219585461,
      "grad_norm": 2.904041290283203,
      "learning_rate": 1.5288913908882545e-05,
      "loss": 1.0844,
      "step": 12300
    },
    {
      "epoch": 1.479062781616101,
      "grad_norm": 2.8479082584381104,
      "learning_rate": 1.5276850697655715e-05,
      "loss": 1.1227,
      "step": 12310
    },
    {
      "epoch": 1.4802643436467409,
      "grad_norm": 2.8530280590057373,
      "learning_rate": 1.5264787486428888e-05,
      "loss": 1.1175,
      "step": 12320
    },
    {
      "epoch": 1.4814659056773807,
      "grad_norm": 2.631197452545166,
      "learning_rate": 1.525272427520206e-05,
      "loss": 1.1233,
      "step": 12330
    },
    {
      "epoch": 1.4826674677080205,
      "grad_norm": 3.0440006256103516,
      "learning_rate": 1.5240661063975231e-05,
      "loss": 1.1334,
      "step": 12340
    },
    {
      "epoch": 1.4838690297386603,
      "grad_norm": 2.8696985244750977,
      "learning_rate": 1.5228597852748402e-05,
      "loss": 1.1463,
      "step": 12350
    },
    {
      "epoch": 1.4850705917693001,
      "grad_norm": 3.279757261276245,
      "learning_rate": 1.5216534641521574e-05,
      "loss": 1.0651,
      "step": 12360
    },
    {
      "epoch": 1.48627215379994,
      "grad_norm": 2.885361671447754,
      "learning_rate": 1.5204471430294747e-05,
      "loss": 1.145,
      "step": 12370
    },
    {
      "epoch": 1.4874737158305797,
      "grad_norm": 2.4282963275909424,
      "learning_rate": 1.5192408219067915e-05,
      "loss": 1.0909,
      "step": 12380
    },
    {
      "epoch": 1.4886752778612196,
      "grad_norm": 3.2648849487304688,
      "learning_rate": 1.5180345007841086e-05,
      "loss": 1.1185,
      "step": 12390
    },
    {
      "epoch": 1.4898768398918594,
      "grad_norm": 2.5424845218658447,
      "learning_rate": 1.5168281796614258e-05,
      "loss": 1.0979,
      "step": 12400
    },
    {
      "epoch": 1.4910784019224992,
      "grad_norm": 2.5679543018341064,
      "learning_rate": 1.5156218585387431e-05,
      "loss": 1.0877,
      "step": 12410
    },
    {
      "epoch": 1.492279963953139,
      "grad_norm": 2.5663065910339355,
      "learning_rate": 1.5144155374160602e-05,
      "loss": 1.0874,
      "step": 12420
    },
    {
      "epoch": 1.4934815259837788,
      "grad_norm": 3.3478612899780273,
      "learning_rate": 1.5132092162933774e-05,
      "loss": 1.1364,
      "step": 12430
    },
    {
      "epoch": 1.4946830880144186,
      "grad_norm": 2.9524502754211426,
      "learning_rate": 1.5120028951706945e-05,
      "loss": 1.1062,
      "step": 12440
    },
    {
      "epoch": 1.4958846500450584,
      "grad_norm": 2.6794724464416504,
      "learning_rate": 1.5107965740480117e-05,
      "loss": 1.1149,
      "step": 12450
    },
    {
      "epoch": 1.4970862120756985,
      "grad_norm": 2.8211328983306885,
      "learning_rate": 1.5095902529253286e-05,
      "loss": 1.1636,
      "step": 12460
    },
    {
      "epoch": 1.4982877741063383,
      "grad_norm": 2.6647167205810547,
      "learning_rate": 1.5083839318026458e-05,
      "loss": 1.151,
      "step": 12470
    },
    {
      "epoch": 1.499489336136978,
      "grad_norm": 2.887117385864258,
      "learning_rate": 1.507177610679963e-05,
      "loss": 1.0959,
      "step": 12480
    },
    {
      "epoch": 1.500690898167618,
      "grad_norm": 2.7945477962493896,
      "learning_rate": 1.50597128955728e-05,
      "loss": 1.1152,
      "step": 12490
    },
    {
      "epoch": 1.5018924601982577,
      "grad_norm": 3.189642906188965,
      "learning_rate": 1.5047649684345974e-05,
      "loss": 1.1937,
      "step": 12500
    },
    {
      "epoch": 1.5030940222288975,
      "grad_norm": 2.727647066116333,
      "learning_rate": 1.5035586473119145e-05,
      "loss": 1.1211,
      "step": 12510
    },
    {
      "epoch": 1.5042955842595374,
      "grad_norm": 2.8715100288391113,
      "learning_rate": 1.5023523261892317e-05,
      "loss": 1.1441,
      "step": 12520
    },
    {
      "epoch": 1.5054971462901774,
      "grad_norm": 2.5133838653564453,
      "learning_rate": 1.5011460050665488e-05,
      "loss": 1.0988,
      "step": 12530
    },
    {
      "epoch": 1.5066987083208172,
      "grad_norm": 2.9920194149017334,
      "learning_rate": 1.4999396839438658e-05,
      "loss": 1.1417,
      "step": 12540
    },
    {
      "epoch": 1.507900270351457,
      "grad_norm": 2.8538010120391846,
      "learning_rate": 1.4987333628211831e-05,
      "loss": 1.1503,
      "step": 12550
    },
    {
      "epoch": 1.5091018323820968,
      "grad_norm": 3.3445003032684326,
      "learning_rate": 1.4975270416985003e-05,
      "loss": 1.1371,
      "step": 12560
    },
    {
      "epoch": 1.5103033944127366,
      "grad_norm": 2.670717716217041,
      "learning_rate": 1.4963207205758172e-05,
      "loss": 1.0902,
      "step": 12570
    },
    {
      "epoch": 1.5115049564433765,
      "grad_norm": 2.9993655681610107,
      "learning_rate": 1.4951143994531344e-05,
      "loss": 1.0666,
      "step": 12580
    },
    {
      "epoch": 1.5127065184740163,
      "grad_norm": 2.5948662757873535,
      "learning_rate": 1.4939080783304517e-05,
      "loss": 1.1374,
      "step": 12590
    },
    {
      "epoch": 1.513908080504656,
      "grad_norm": 2.9374921321868896,
      "learning_rate": 1.4927017572077688e-05,
      "loss": 1.1488,
      "step": 12600
    },
    {
      "epoch": 1.515109642535296,
      "grad_norm": 2.9078736305236816,
      "learning_rate": 1.4914954360850858e-05,
      "loss": 1.1248,
      "step": 12610
    },
    {
      "epoch": 1.5163112045659357,
      "grad_norm": 2.5948562622070312,
      "learning_rate": 1.490289114962403e-05,
      "loss": 1.1722,
      "step": 12620
    },
    {
      "epoch": 1.5175127665965755,
      "grad_norm": 2.8077173233032227,
      "learning_rate": 1.4890827938397201e-05,
      "loss": 1.107,
      "step": 12630
    },
    {
      "epoch": 1.5187143286272153,
      "grad_norm": 2.7457072734832764,
      "learning_rate": 1.4878764727170374e-05,
      "loss": 1.1285,
      "step": 12640
    },
    {
      "epoch": 1.5199158906578552,
      "grad_norm": 2.826625347137451,
      "learning_rate": 1.4866701515943544e-05,
      "loss": 1.1389,
      "step": 12650
    },
    {
      "epoch": 1.521117452688495,
      "grad_norm": 2.5907695293426514,
      "learning_rate": 1.4854638304716715e-05,
      "loss": 1.1452,
      "step": 12660
    },
    {
      "epoch": 1.5223190147191348,
      "grad_norm": 2.671394109725952,
      "learning_rate": 1.4842575093489887e-05,
      "loss": 1.0977,
      "step": 12670
    },
    {
      "epoch": 1.5235205767497746,
      "grad_norm": 2.5632941722869873,
      "learning_rate": 1.483051188226306e-05,
      "loss": 1.1503,
      "step": 12680
    },
    {
      "epoch": 1.5247221387804144,
      "grad_norm": 2.780240535736084,
      "learning_rate": 1.4818448671036231e-05,
      "loss": 1.0885,
      "step": 12690
    },
    {
      "epoch": 1.5259237008110542,
      "grad_norm": 2.560854196548462,
      "learning_rate": 1.4806385459809401e-05,
      "loss": 1.1193,
      "step": 12700
    },
    {
      "epoch": 1.5271252628416943,
      "grad_norm": 3.1606643199920654,
      "learning_rate": 1.4794322248582573e-05,
      "loss": 1.0922,
      "step": 12710
    },
    {
      "epoch": 1.528326824872334,
      "grad_norm": 3.148538589477539,
      "learning_rate": 1.4782259037355744e-05,
      "loss": 1.0573,
      "step": 12720
    },
    {
      "epoch": 1.5295283869029739,
      "grad_norm": 2.863950729370117,
      "learning_rate": 1.4770195826128917e-05,
      "loss": 1.1824,
      "step": 12730
    },
    {
      "epoch": 1.5307299489336137,
      "grad_norm": 2.482658863067627,
      "learning_rate": 1.4758132614902087e-05,
      "loss": 1.1147,
      "step": 12740
    },
    {
      "epoch": 1.5319315109642535,
      "grad_norm": 2.8017120361328125,
      "learning_rate": 1.4746069403675258e-05,
      "loss": 1.1062,
      "step": 12750
    },
    {
      "epoch": 1.5331330729948933,
      "grad_norm": 2.9786765575408936,
      "learning_rate": 1.473400619244843e-05,
      "loss": 1.0822,
      "step": 12760
    },
    {
      "epoch": 1.5343346350255334,
      "grad_norm": 2.747804880142212,
      "learning_rate": 1.4721942981221603e-05,
      "loss": 1.1282,
      "step": 12770
    },
    {
      "epoch": 1.5355361970561732,
      "grad_norm": 2.889317512512207,
      "learning_rate": 1.4709879769994773e-05,
      "loss": 1.1421,
      "step": 12780
    },
    {
      "epoch": 1.536737759086813,
      "grad_norm": 3.027390480041504,
      "learning_rate": 1.4697816558767944e-05,
      "loss": 1.1069,
      "step": 12790
    },
    {
      "epoch": 1.5379393211174528,
      "grad_norm": 2.8214328289031982,
      "learning_rate": 1.4685753347541115e-05,
      "loss": 1.1194,
      "step": 12800
    },
    {
      "epoch": 1.5391408831480926,
      "grad_norm": 3.743290901184082,
      "learning_rate": 1.4673690136314289e-05,
      "loss": 1.116,
      "step": 12810
    },
    {
      "epoch": 1.5403424451787324,
      "grad_norm": 2.898001194000244,
      "learning_rate": 1.4661626925087458e-05,
      "loss": 1.1565,
      "step": 12820
    },
    {
      "epoch": 1.5415440072093722,
      "grad_norm": 2.7541754245758057,
      "learning_rate": 1.464956371386063e-05,
      "loss": 1.1284,
      "step": 12830
    },
    {
      "epoch": 1.542745569240012,
      "grad_norm": 3.2209246158599854,
      "learning_rate": 1.4637500502633801e-05,
      "loss": 1.1161,
      "step": 12840
    },
    {
      "epoch": 1.5439471312706519,
      "grad_norm": 2.804764747619629,
      "learning_rate": 1.4625437291406973e-05,
      "loss": 1.0678,
      "step": 12850
    },
    {
      "epoch": 1.5451486933012917,
      "grad_norm": 2.7385356426239014,
      "learning_rate": 1.4613374080180144e-05,
      "loss": 1.1098,
      "step": 12860
    },
    {
      "epoch": 1.5463502553319315,
      "grad_norm": 2.8814196586608887,
      "learning_rate": 1.4601310868953316e-05,
      "loss": 1.1635,
      "step": 12870
    },
    {
      "epoch": 1.5475518173625713,
      "grad_norm": 2.346903085708618,
      "learning_rate": 1.4589247657726487e-05,
      "loss": 1.0811,
      "step": 12880
    },
    {
      "epoch": 1.5487533793932111,
      "grad_norm": 4.097607612609863,
      "learning_rate": 1.4577184446499658e-05,
      "loss": 1.1738,
      "step": 12890
    },
    {
      "epoch": 1.549954941423851,
      "grad_norm": 3.062182664871216,
      "learning_rate": 1.4565121235272832e-05,
      "loss": 1.149,
      "step": 12900
    },
    {
      "epoch": 1.5511565034544907,
      "grad_norm": 2.818589210510254,
      "learning_rate": 1.4553058024046001e-05,
      "loss": 1.1706,
      "step": 12910
    },
    {
      "epoch": 1.5523580654851306,
      "grad_norm": 2.623525381088257,
      "learning_rate": 1.4540994812819173e-05,
      "loss": 1.1163,
      "step": 12920
    },
    {
      "epoch": 1.5535596275157704,
      "grad_norm": 2.4302847385406494,
      "learning_rate": 1.4528931601592344e-05,
      "loss": 1.0978,
      "step": 12930
    },
    {
      "epoch": 1.5547611895464102,
      "grad_norm": 4.188512802124023,
      "learning_rate": 1.4516868390365516e-05,
      "loss": 1.105,
      "step": 12940
    },
    {
      "epoch": 1.5559627515770502,
      "grad_norm": 2.863788366317749,
      "learning_rate": 1.4504805179138687e-05,
      "loss": 1.0923,
      "step": 12950
    },
    {
      "epoch": 1.55716431360769,
      "grad_norm": 2.768562078475952,
      "learning_rate": 1.4492741967911859e-05,
      "loss": 1.1007,
      "step": 12960
    },
    {
      "epoch": 1.5583658756383298,
      "grad_norm": 2.999818801879883,
      "learning_rate": 1.448067875668503e-05,
      "loss": 1.0928,
      "step": 12970
    },
    {
      "epoch": 1.5595674376689697,
      "grad_norm": 3.0274484157562256,
      "learning_rate": 1.4468615545458201e-05,
      "loss": 1.1013,
      "step": 12980
    },
    {
      "epoch": 1.5607689996996095,
      "grad_norm": 2.7981131076812744,
      "learning_rate": 1.4456552334231373e-05,
      "loss": 1.1521,
      "step": 12990
    },
    {
      "epoch": 1.5619705617302493,
      "grad_norm": 2.4357709884643555,
      "learning_rate": 1.4444489123004544e-05,
      "loss": 1.1156,
      "step": 13000
    },
    {
      "epoch": 1.5631721237608893,
      "grad_norm": 2.7827253341674805,
      "learning_rate": 1.4432425911777716e-05,
      "loss": 1.1417,
      "step": 13010
    },
    {
      "epoch": 1.5643736857915291,
      "grad_norm": 3.296912431716919,
      "learning_rate": 1.4420362700550887e-05,
      "loss": 1.0504,
      "step": 13020
    },
    {
      "epoch": 1.565575247822169,
      "grad_norm": 2.920100688934326,
      "learning_rate": 1.4408299489324057e-05,
      "loss": 1.1499,
      "step": 13030
    },
    {
      "epoch": 1.5667768098528088,
      "grad_norm": 2.768476963043213,
      "learning_rate": 1.439623627809723e-05,
      "loss": 1.0672,
      "step": 13040
    },
    {
      "epoch": 1.5679783718834486,
      "grad_norm": 2.7405152320861816,
      "learning_rate": 1.4384173066870402e-05,
      "loss": 1.1414,
      "step": 13050
    },
    {
      "epoch": 1.5691799339140884,
      "grad_norm": 2.6152002811431885,
      "learning_rate": 1.4372109855643573e-05,
      "loss": 1.11,
      "step": 13060
    },
    {
      "epoch": 1.5703814959447282,
      "grad_norm": 2.671271800994873,
      "learning_rate": 1.4360046644416743e-05,
      "loss": 1.0827,
      "step": 13070
    },
    {
      "epoch": 1.571583057975368,
      "grad_norm": 2.8548648357391357,
      "learning_rate": 1.4347983433189916e-05,
      "loss": 1.0755,
      "step": 13080
    },
    {
      "epoch": 1.5727846200060078,
      "grad_norm": 2.724050760269165,
      "learning_rate": 1.4335920221963087e-05,
      "loss": 1.1411,
      "step": 13090
    },
    {
      "epoch": 1.5739861820366476,
      "grad_norm": 2.624302625656128,
      "learning_rate": 1.4323857010736259e-05,
      "loss": 1.1098,
      "step": 13100
    },
    {
      "epoch": 1.5751877440672875,
      "grad_norm": 2.8799357414245605,
      "learning_rate": 1.431179379950943e-05,
      "loss": 1.1265,
      "step": 13110
    },
    {
      "epoch": 1.5763893060979273,
      "grad_norm": 2.896097421646118,
      "learning_rate": 1.42997305882826e-05,
      "loss": 1.1214,
      "step": 13120
    },
    {
      "epoch": 1.577590868128567,
      "grad_norm": 2.7153873443603516,
      "learning_rate": 1.4287667377055773e-05,
      "loss": 1.115,
      "step": 13130
    },
    {
      "epoch": 1.578792430159207,
      "grad_norm": 2.8218090534210205,
      "learning_rate": 1.4275604165828944e-05,
      "loss": 1.1043,
      "step": 13140
    },
    {
      "epoch": 1.5799939921898467,
      "grad_norm": 2.8328449726104736,
      "learning_rate": 1.4263540954602116e-05,
      "loss": 1.1682,
      "step": 13150
    },
    {
      "epoch": 1.5811955542204865,
      "grad_norm": 2.879711151123047,
      "learning_rate": 1.4251477743375286e-05,
      "loss": 1.0891,
      "step": 13160
    },
    {
      "epoch": 1.5823971162511263,
      "grad_norm": 2.502096652984619,
      "learning_rate": 1.4239414532148459e-05,
      "loss": 1.1611,
      "step": 13170
    },
    {
      "epoch": 1.5835986782817661,
      "grad_norm": 2.382418394088745,
      "learning_rate": 1.422735132092163e-05,
      "loss": 1.0761,
      "step": 13180
    },
    {
      "epoch": 1.5848002403124062,
      "grad_norm": 2.653859853744507,
      "learning_rate": 1.4215288109694802e-05,
      "loss": 1.0689,
      "step": 13190
    },
    {
      "epoch": 1.586001802343046,
      "grad_norm": 2.6381561756134033,
      "learning_rate": 1.4203224898467971e-05,
      "loss": 1.102,
      "step": 13200
    },
    {
      "epoch": 1.5872033643736858,
      "grad_norm": 2.7599196434020996,
      "learning_rate": 1.4191161687241143e-05,
      "loss": 1.0469,
      "step": 13210
    },
    {
      "epoch": 1.5884049264043256,
      "grad_norm": 2.730989456176758,
      "learning_rate": 1.4179098476014316e-05,
      "loss": 1.0978,
      "step": 13220
    },
    {
      "epoch": 1.5896064884349654,
      "grad_norm": 2.737332582473755,
      "learning_rate": 1.4167035264787487e-05,
      "loss": 1.1093,
      "step": 13230
    },
    {
      "epoch": 1.5908080504656053,
      "grad_norm": 2.962250232696533,
      "learning_rate": 1.4154972053560657e-05,
      "loss": 1.12,
      "step": 13240
    },
    {
      "epoch": 1.592009612496245,
      "grad_norm": 2.5763866901397705,
      "learning_rate": 1.4142908842333829e-05,
      "loss": 1.1445,
      "step": 13250
    },
    {
      "epoch": 1.593211174526885,
      "grad_norm": 2.613748788833618,
      "learning_rate": 1.4130845631107002e-05,
      "loss": 1.1142,
      "step": 13260
    },
    {
      "epoch": 1.594412736557525,
      "grad_norm": 3.0479109287261963,
      "learning_rate": 1.4118782419880173e-05,
      "loss": 1.1085,
      "step": 13270
    },
    {
      "epoch": 1.5956142985881647,
      "grad_norm": 2.936344861984253,
      "learning_rate": 1.4106719208653343e-05,
      "loss": 1.0875,
      "step": 13280
    },
    {
      "epoch": 1.5968158606188045,
      "grad_norm": 2.764500379562378,
      "learning_rate": 1.4094655997426514e-05,
      "loss": 1.1007,
      "step": 13290
    },
    {
      "epoch": 1.5980174226494444,
      "grad_norm": 2.4217662811279297,
      "learning_rate": 1.4082592786199688e-05,
      "loss": 1.1025,
      "step": 13300
    },
    {
      "epoch": 1.5992189846800842,
      "grad_norm": 2.981151580810547,
      "learning_rate": 1.4070529574972859e-05,
      "loss": 1.1418,
      "step": 13310
    },
    {
      "epoch": 1.600420546710724,
      "grad_norm": 2.519685983657837,
      "learning_rate": 1.405846636374603e-05,
      "loss": 1.1486,
      "step": 13320
    },
    {
      "epoch": 1.6016221087413638,
      "grad_norm": 2.631951093673706,
      "learning_rate": 1.40464031525192e-05,
      "loss": 1.1329,
      "step": 13330
    },
    {
      "epoch": 1.6028236707720036,
      "grad_norm": 2.9714198112487793,
      "learning_rate": 1.4034339941292372e-05,
      "loss": 1.0748,
      "step": 13340
    },
    {
      "epoch": 1.6040252328026434,
      "grad_norm": 2.629009246826172,
      "learning_rate": 1.4022276730065545e-05,
      "loss": 1.1169,
      "step": 13350
    },
    {
      "epoch": 1.6052267948332832,
      "grad_norm": 2.8251447677612305,
      "learning_rate": 1.4010213518838716e-05,
      "loss": 1.1293,
      "step": 13360
    },
    {
      "epoch": 1.606428356863923,
      "grad_norm": 3.1269960403442383,
      "learning_rate": 1.3998150307611886e-05,
      "loss": 1.1205,
      "step": 13370
    },
    {
      "epoch": 1.6076299188945629,
      "grad_norm": 3.210965156555176,
      "learning_rate": 1.3986087096385057e-05,
      "loss": 1.1318,
      "step": 13380
    },
    {
      "epoch": 1.6088314809252027,
      "grad_norm": 2.566749334335327,
      "learning_rate": 1.397402388515823e-05,
      "loss": 1.0973,
      "step": 13390
    },
    {
      "epoch": 1.6100330429558425,
      "grad_norm": 2.777805805206299,
      "learning_rate": 1.3961960673931402e-05,
      "loss": 1.123,
      "step": 13400
    },
    {
      "epoch": 1.6112346049864823,
      "grad_norm": 2.929656744003296,
      "learning_rate": 1.3949897462704572e-05,
      "loss": 1.0979,
      "step": 13410
    },
    {
      "epoch": 1.6124361670171221,
      "grad_norm": 3.158869743347168,
      "learning_rate": 1.3937834251477743e-05,
      "loss": 1.1015,
      "step": 13420
    },
    {
      "epoch": 1.613637729047762,
      "grad_norm": 3.4473695755004883,
      "learning_rate": 1.3925771040250915e-05,
      "loss": 1.1423,
      "step": 13430
    },
    {
      "epoch": 1.614839291078402,
      "grad_norm": 2.8116753101348877,
      "learning_rate": 1.3913707829024088e-05,
      "loss": 1.1228,
      "step": 13440
    },
    {
      "epoch": 1.6160408531090418,
      "grad_norm": 2.4195809364318848,
      "learning_rate": 1.3901644617797257e-05,
      "loss": 1.0906,
      "step": 13450
    },
    {
      "epoch": 1.6172424151396816,
      "grad_norm": 2.5298609733581543,
      "learning_rate": 1.3889581406570429e-05,
      "loss": 1.1329,
      "step": 13460
    },
    {
      "epoch": 1.6184439771703214,
      "grad_norm": 2.808133363723755,
      "learning_rate": 1.38775181953436e-05,
      "loss": 1.1245,
      "step": 13470
    },
    {
      "epoch": 1.6196455392009612,
      "grad_norm": 3.2089855670928955,
      "learning_rate": 1.3865454984116773e-05,
      "loss": 1.117,
      "step": 13480
    },
    {
      "epoch": 1.620847101231601,
      "grad_norm": 2.70819354057312,
      "learning_rate": 1.3853391772889943e-05,
      "loss": 1.1338,
      "step": 13490
    },
    {
      "epoch": 1.622048663262241,
      "grad_norm": 2.621650457382202,
      "learning_rate": 1.3841328561663115e-05,
      "loss": 1.12,
      "step": 13500
    },
    {
      "epoch": 1.6232502252928809,
      "grad_norm": 3.063567638397217,
      "learning_rate": 1.3829265350436286e-05,
      "loss": 1.0885,
      "step": 13510
    },
    {
      "epoch": 1.6244517873235207,
      "grad_norm": 2.554208755493164,
      "learning_rate": 1.3817202139209458e-05,
      "loss": 1.1124,
      "step": 13520
    },
    {
      "epoch": 1.6256533493541605,
      "grad_norm": 2.4323275089263916,
      "learning_rate": 1.380513892798263e-05,
      "loss": 1.0629,
      "step": 13530
    },
    {
      "epoch": 1.6268549113848003,
      "grad_norm": 2.9847140312194824,
      "learning_rate": 1.37930757167558e-05,
      "loss": 1.1321,
      "step": 13540
    },
    {
      "epoch": 1.6280564734154401,
      "grad_norm": 3.348921537399292,
      "learning_rate": 1.3781012505528972e-05,
      "loss": 1.1865,
      "step": 13550
    },
    {
      "epoch": 1.62925803544608,
      "grad_norm": 2.9026496410369873,
      "learning_rate": 1.3768949294302143e-05,
      "loss": 1.0755,
      "step": 13560
    },
    {
      "epoch": 1.6304595974767198,
      "grad_norm": 3.165621757507324,
      "learning_rate": 1.3756886083075316e-05,
      "loss": 1.1271,
      "step": 13570
    },
    {
      "epoch": 1.6316611595073596,
      "grad_norm": 3.4168829917907715,
      "learning_rate": 1.3744822871848486e-05,
      "loss": 1.1231,
      "step": 13580
    },
    {
      "epoch": 1.6328627215379994,
      "grad_norm": 2.930211305618286,
      "learning_rate": 1.3732759660621658e-05,
      "loss": 1.127,
      "step": 13590
    },
    {
      "epoch": 1.6340642835686392,
      "grad_norm": 2.646083354949951,
      "learning_rate": 1.3720696449394829e-05,
      "loss": 1.0591,
      "step": 13600
    },
    {
      "epoch": 1.635265845599279,
      "grad_norm": 2.9721858501434326,
      "learning_rate": 1.3708633238168e-05,
      "loss": 1.1241,
      "step": 13610
    },
    {
      "epoch": 1.6364674076299188,
      "grad_norm": 2.675175428390503,
      "learning_rate": 1.3696570026941172e-05,
      "loss": 1.173,
      "step": 13620
    },
    {
      "epoch": 1.6376689696605586,
      "grad_norm": 3.250122547149658,
      "learning_rate": 1.3684506815714343e-05,
      "loss": 1.1654,
      "step": 13630
    },
    {
      "epoch": 1.6388705316911985,
      "grad_norm": 3.0979275703430176,
      "learning_rate": 1.3672443604487515e-05,
      "loss": 1.0709,
      "step": 13640
    },
    {
      "epoch": 1.6400720937218383,
      "grad_norm": 2.575645923614502,
      "learning_rate": 1.3660380393260686e-05,
      "loss": 1.1064,
      "step": 13650
    },
    {
      "epoch": 1.641273655752478,
      "grad_norm": 3.3043627738952637,
      "learning_rate": 1.3648317182033858e-05,
      "loss": 1.1499,
      "step": 13660
    },
    {
      "epoch": 1.642475217783118,
      "grad_norm": 2.3761706352233887,
      "learning_rate": 1.363625397080703e-05,
      "loss": 1.1272,
      "step": 13670
    },
    {
      "epoch": 1.643676779813758,
      "grad_norm": 2.554978370666504,
      "learning_rate": 1.36241907595802e-05,
      "loss": 1.1402,
      "step": 13680
    },
    {
      "epoch": 1.6448783418443977,
      "grad_norm": 2.6826696395874023,
      "learning_rate": 1.3612127548353372e-05,
      "loss": 1.114,
      "step": 13690
    },
    {
      "epoch": 1.6460799038750376,
      "grad_norm": 2.726881504058838,
      "learning_rate": 1.3600064337126542e-05,
      "loss": 1.0999,
      "step": 13700
    },
    {
      "epoch": 1.6472814659056774,
      "grad_norm": 2.5490527153015137,
      "learning_rate": 1.3588001125899715e-05,
      "loss": 1.0836,
      "step": 13710
    },
    {
      "epoch": 1.6484830279363172,
      "grad_norm": 2.5569844245910645,
      "learning_rate": 1.3575937914672886e-05,
      "loss": 1.0491,
      "step": 13720
    },
    {
      "epoch": 1.649684589966957,
      "grad_norm": 2.798830032348633,
      "learning_rate": 1.3563874703446058e-05,
      "loss": 1.1392,
      "step": 13730
    },
    {
      "epoch": 1.650886151997597,
      "grad_norm": 2.4986743927001953,
      "learning_rate": 1.355181149221923e-05,
      "loss": 1.1353,
      "step": 13740
    },
    {
      "epoch": 1.6520877140282368,
      "grad_norm": 3.090555429458618,
      "learning_rate": 1.35397482809924e-05,
      "loss": 1.0759,
      "step": 13750
    },
    {
      "epoch": 1.6532892760588767,
      "grad_norm": 2.3497800827026367,
      "learning_rate": 1.3527685069765572e-05,
      "loss": 1.127,
      "step": 13760
    },
    {
      "epoch": 1.6544908380895165,
      "grad_norm": 2.4715628623962402,
      "learning_rate": 1.3515621858538744e-05,
      "loss": 1.0994,
      "step": 13770
    },
    {
      "epoch": 1.6556924001201563,
      "grad_norm": 2.6739919185638428,
      "learning_rate": 1.3503558647311915e-05,
      "loss": 1.1271,
      "step": 13780
    },
    {
      "epoch": 1.656893962150796,
      "grad_norm": 2.605501413345337,
      "learning_rate": 1.3491495436085086e-05,
      "loss": 1.1005,
      "step": 13790
    },
    {
      "epoch": 1.658095524181436,
      "grad_norm": 2.932318687438965,
      "learning_rate": 1.3479432224858258e-05,
      "loss": 1.1363,
      "step": 13800
    },
    {
      "epoch": 1.6592970862120757,
      "grad_norm": 3.2249600887298584,
      "learning_rate": 1.346736901363143e-05,
      "loss": 1.1723,
      "step": 13810
    },
    {
      "epoch": 1.6604986482427155,
      "grad_norm": 2.9073235988616943,
      "learning_rate": 1.34553058024046e-05,
      "loss": 1.1148,
      "step": 13820
    },
    {
      "epoch": 1.6617002102733553,
      "grad_norm": 3.0577149391174316,
      "learning_rate": 1.344324259117777e-05,
      "loss": 1.0996,
      "step": 13830
    },
    {
      "epoch": 1.6629017723039952,
      "grad_norm": 2.6509387493133545,
      "learning_rate": 1.3431179379950944e-05,
      "loss": 1.0958,
      "step": 13840
    },
    {
      "epoch": 1.664103334334635,
      "grad_norm": 2.7197306156158447,
      "learning_rate": 1.3419116168724115e-05,
      "loss": 1.0657,
      "step": 13850
    },
    {
      "epoch": 1.6653048963652748,
      "grad_norm": 2.6593782901763916,
      "learning_rate": 1.3407052957497287e-05,
      "loss": 1.0736,
      "step": 13860
    },
    {
      "epoch": 1.6665064583959146,
      "grad_norm": 2.641895055770874,
      "learning_rate": 1.3394989746270456e-05,
      "loss": 1.1174,
      "step": 13870
    },
    {
      "epoch": 1.6677080204265544,
      "grad_norm": 3.1377055644989014,
      "learning_rate": 1.338292653504363e-05,
      "loss": 1.175,
      "step": 13880
    },
    {
      "epoch": 1.6689095824571942,
      "grad_norm": 2.912031412124634,
      "learning_rate": 1.3370863323816801e-05,
      "loss": 1.1091,
      "step": 13890
    },
    {
      "epoch": 1.670111144487834,
      "grad_norm": 2.4167354106903076,
      "learning_rate": 1.3358800112589972e-05,
      "loss": 1.1269,
      "step": 13900
    },
    {
      "epoch": 1.6713127065184739,
      "grad_norm": 3.151607036590576,
      "learning_rate": 1.3346736901363142e-05,
      "loss": 1.1363,
      "step": 13910
    },
    {
      "epoch": 1.672514268549114,
      "grad_norm": 2.9415109157562256,
      "learning_rate": 1.3334673690136313e-05,
      "loss": 1.1079,
      "step": 13920
    },
    {
      "epoch": 1.6737158305797537,
      "grad_norm": 2.936810255050659,
      "learning_rate": 1.3322610478909487e-05,
      "loss": 1.1099,
      "step": 13930
    },
    {
      "epoch": 1.6749173926103935,
      "grad_norm": 3.6124236583709717,
      "learning_rate": 1.3310547267682658e-05,
      "loss": 1.0718,
      "step": 13940
    },
    {
      "epoch": 1.6761189546410333,
      "grad_norm": 2.6854333877563477,
      "learning_rate": 1.329848405645583e-05,
      "loss": 1.0626,
      "step": 13950
    },
    {
      "epoch": 1.6773205166716731,
      "grad_norm": 2.9044246673583984,
      "learning_rate": 1.3286420845229e-05,
      "loss": 1.1036,
      "step": 13960
    },
    {
      "epoch": 1.678522078702313,
      "grad_norm": 2.8832151889801025,
      "learning_rate": 1.3274357634002172e-05,
      "loss": 1.0967,
      "step": 13970
    },
    {
      "epoch": 1.679723640732953,
      "grad_norm": 2.500521421432495,
      "learning_rate": 1.3262294422775344e-05,
      "loss": 1.0845,
      "step": 13980
    },
    {
      "epoch": 1.6809252027635928,
      "grad_norm": 3.0923590660095215,
      "learning_rate": 1.3250231211548515e-05,
      "loss": 1.1633,
      "step": 13990
    },
    {
      "epoch": 1.6821267647942326,
      "grad_norm": 2.837061643600464,
      "learning_rate": 1.3238168000321685e-05,
      "loss": 1.0954,
      "step": 14000
    },
    {
      "epoch": 1.6833283268248724,
      "grad_norm": 2.5516703128814697,
      "learning_rate": 1.3226104789094856e-05,
      "loss": 1.158,
      "step": 14010
    },
    {
      "epoch": 1.6845298888555122,
      "grad_norm": 3.1523537635803223,
      "learning_rate": 1.321404157786803e-05,
      "loss": 1.076,
      "step": 14020
    },
    {
      "epoch": 1.685731450886152,
      "grad_norm": 2.6358580589294434,
      "learning_rate": 1.3201978366641201e-05,
      "loss": 1.1146,
      "step": 14030
    },
    {
      "epoch": 1.6869330129167919,
      "grad_norm": 2.9491400718688965,
      "learning_rate": 1.318991515541437e-05,
      "loss": 1.0991,
      "step": 14040
    },
    {
      "epoch": 1.6881345749474317,
      "grad_norm": 2.5989668369293213,
      "learning_rate": 1.3177851944187542e-05,
      "loss": 1.1505,
      "step": 14050
    },
    {
      "epoch": 1.6893361369780715,
      "grad_norm": 2.4847726821899414,
      "learning_rate": 1.3165788732960715e-05,
      "loss": 1.0892,
      "step": 14060
    },
    {
      "epoch": 1.6905376990087113,
      "grad_norm": 2.9988956451416016,
      "learning_rate": 1.3153725521733887e-05,
      "loss": 1.0771,
      "step": 14070
    },
    {
      "epoch": 1.6917392610393511,
      "grad_norm": 2.657487392425537,
      "learning_rate": 1.3141662310507057e-05,
      "loss": 1.1354,
      "step": 14080
    },
    {
      "epoch": 1.692940823069991,
      "grad_norm": 3.425808906555176,
      "learning_rate": 1.3129599099280228e-05,
      "loss": 1.1513,
      "step": 14090
    },
    {
      "epoch": 1.6941423851006308,
      "grad_norm": 2.5758321285247803,
      "learning_rate": 1.31175358880534e-05,
      "loss": 1.1543,
      "step": 14100
    },
    {
      "epoch": 1.6953439471312706,
      "grad_norm": 2.8085219860076904,
      "learning_rate": 1.3105472676826573e-05,
      "loss": 1.0967,
      "step": 14110
    },
    {
      "epoch": 1.6965455091619104,
      "grad_norm": 2.8046493530273438,
      "learning_rate": 1.3093409465599742e-05,
      "loss": 1.1365,
      "step": 14120
    },
    {
      "epoch": 1.6977470711925502,
      "grad_norm": 2.86147403717041,
      "learning_rate": 1.3081346254372914e-05,
      "loss": 1.1257,
      "step": 14130
    },
    {
      "epoch": 1.69894863322319,
      "grad_norm": 2.6590137481689453,
      "learning_rate": 1.3069283043146085e-05,
      "loss": 1.0746,
      "step": 14140
    },
    {
      "epoch": 1.7001501952538298,
      "grad_norm": 2.3988776206970215,
      "learning_rate": 1.3057219831919258e-05,
      "loss": 1.109,
      "step": 14150
    },
    {
      "epoch": 1.7013517572844699,
      "grad_norm": 3.032273054122925,
      "learning_rate": 1.304515662069243e-05,
      "loss": 1.0971,
      "step": 14160
    },
    {
      "epoch": 1.7025533193151097,
      "grad_norm": 2.812978982925415,
      "learning_rate": 1.30330934094656e-05,
      "loss": 1.1472,
      "step": 14170
    },
    {
      "epoch": 1.7037548813457495,
      "grad_norm": 2.5980262756347656,
      "learning_rate": 1.3021030198238771e-05,
      "loss": 1.0754,
      "step": 14180
    },
    {
      "epoch": 1.7049564433763893,
      "grad_norm": 2.623476028442383,
      "learning_rate": 1.3008966987011942e-05,
      "loss": 1.1056,
      "step": 14190
    },
    {
      "epoch": 1.706158005407029,
      "grad_norm": 2.8476414680480957,
      "learning_rate": 1.2996903775785116e-05,
      "loss": 1.1374,
      "step": 14200
    },
    {
      "epoch": 1.707359567437669,
      "grad_norm": 3.0174152851104736,
      "learning_rate": 1.2984840564558285e-05,
      "loss": 1.0592,
      "step": 14210
    },
    {
      "epoch": 1.708561129468309,
      "grad_norm": 3.0167834758758545,
      "learning_rate": 1.2972777353331457e-05,
      "loss": 1.1063,
      "step": 14220
    },
    {
      "epoch": 1.7097626914989488,
      "grad_norm": 2.607131242752075,
      "learning_rate": 1.2960714142104628e-05,
      "loss": 1.1392,
      "step": 14230
    },
    {
      "epoch": 1.7109642535295886,
      "grad_norm": 2.7496063709259033,
      "learning_rate": 1.2948650930877801e-05,
      "loss": 1.0795,
      "step": 14240
    },
    {
      "epoch": 1.7121658155602284,
      "grad_norm": 2.7073781490325928,
      "learning_rate": 1.2936587719650971e-05,
      "loss": 1.1206,
      "step": 14250
    },
    {
      "epoch": 1.7133673775908682,
      "grad_norm": 2.8357439041137695,
      "learning_rate": 1.2924524508424142e-05,
      "loss": 1.104,
      "step": 14260
    },
    {
      "epoch": 1.714568939621508,
      "grad_norm": 3.0295541286468506,
      "learning_rate": 1.2912461297197314e-05,
      "loss": 1.0694,
      "step": 14270
    },
    {
      "epoch": 1.7157705016521478,
      "grad_norm": 2.812084436416626,
      "learning_rate": 1.2900398085970485e-05,
      "loss": 1.1281,
      "step": 14280
    },
    {
      "epoch": 1.7169720636827877,
      "grad_norm": 2.8497378826141357,
      "learning_rate": 1.2888334874743657e-05,
      "loss": 1.0805,
      "step": 14290
    },
    {
      "epoch": 1.7181736257134275,
      "grad_norm": 3.695237874984741,
      "learning_rate": 1.2876271663516828e-05,
      "loss": 1.0976,
      "step": 14300
    },
    {
      "epoch": 1.7193751877440673,
      "grad_norm": 2.6867032051086426,
      "learning_rate": 1.286420845229e-05,
      "loss": 1.0703,
      "step": 14310
    },
    {
      "epoch": 1.720576749774707,
      "grad_norm": 2.900184154510498,
      "learning_rate": 1.2852145241063171e-05,
      "loss": 1.067,
      "step": 14320
    },
    {
      "epoch": 1.721778311805347,
      "grad_norm": 2.6599197387695312,
      "learning_rate": 1.2840082029836343e-05,
      "loss": 1.0761,
      "step": 14330
    },
    {
      "epoch": 1.7229798738359867,
      "grad_norm": 2.8462955951690674,
      "learning_rate": 1.2828018818609514e-05,
      "loss": 1.0811,
      "step": 14340
    },
    {
      "epoch": 1.7241814358666265,
      "grad_norm": 2.6989638805389404,
      "learning_rate": 1.2815955607382685e-05,
      "loss": 1.0727,
      "step": 14350
    },
    {
      "epoch": 1.7253829978972663,
      "grad_norm": 2.961456298828125,
      "learning_rate": 1.2803892396155857e-05,
      "loss": 1.1043,
      "step": 14360
    },
    {
      "epoch": 1.7265845599279062,
      "grad_norm": 2.7392404079437256,
      "learning_rate": 1.279182918492903e-05,
      "loss": 1.0753,
      "step": 14370
    },
    {
      "epoch": 1.727786121958546,
      "grad_norm": 2.8789634704589844,
      "learning_rate": 1.27797659737022e-05,
      "loss": 1.125,
      "step": 14380
    },
    {
      "epoch": 1.7289876839891858,
      "grad_norm": 2.597527503967285,
      "learning_rate": 1.2767702762475371e-05,
      "loss": 1.0866,
      "step": 14390
    },
    {
      "epoch": 1.7301892460198258,
      "grad_norm": 3.3185248374938965,
      "learning_rate": 1.2755639551248543e-05,
      "loss": 1.1295,
      "step": 14400
    },
    {
      "epoch": 1.7313908080504656,
      "grad_norm": 3.115976333618164,
      "learning_rate": 1.2743576340021714e-05,
      "loss": 1.1271,
      "step": 14410
    },
    {
      "epoch": 1.7325923700811054,
      "grad_norm": 2.7803094387054443,
      "learning_rate": 1.2731513128794886e-05,
      "loss": 1.1192,
      "step": 14420
    },
    {
      "epoch": 1.7337939321117453,
      "grad_norm": 2.7995951175689697,
      "learning_rate": 1.2719449917568057e-05,
      "loss": 1.1087,
      "step": 14430
    },
    {
      "epoch": 1.734995494142385,
      "grad_norm": 2.8718948364257812,
      "learning_rate": 1.2707386706341228e-05,
      "loss": 1.0798,
      "step": 14440
    },
    {
      "epoch": 1.7361970561730249,
      "grad_norm": 3.2286932468414307,
      "learning_rate": 1.26953234951144e-05,
      "loss": 1.0871,
      "step": 14450
    },
    {
      "epoch": 1.737398618203665,
      "grad_norm": 2.794957399368286,
      "learning_rate": 1.2683260283887571e-05,
      "loss": 1.1401,
      "step": 14460
    },
    {
      "epoch": 1.7386001802343047,
      "grad_norm": 2.7301414012908936,
      "learning_rate": 1.2671197072660743e-05,
      "loss": 1.0435,
      "step": 14470
    },
    {
      "epoch": 1.7398017422649446,
      "grad_norm": 2.769010543823242,
      "learning_rate": 1.2659133861433914e-05,
      "loss": 1.1132,
      "step": 14480
    },
    {
      "epoch": 1.7410033042955844,
      "grad_norm": 2.8917791843414307,
      "learning_rate": 1.2647070650207086e-05,
      "loss": 1.1294,
      "step": 14490
    },
    {
      "epoch": 1.7422048663262242,
      "grad_norm": 2.68019437789917,
      "learning_rate": 1.2635007438980255e-05,
      "loss": 1.0771,
      "step": 14500
    },
    {
      "epoch": 1.743406428356864,
      "grad_norm": 3.2476937770843506,
      "learning_rate": 1.2622944227753429e-05,
      "loss": 1.0717,
      "step": 14510
    },
    {
      "epoch": 1.7446079903875038,
      "grad_norm": 2.9040374755859375,
      "learning_rate": 1.26108810165266e-05,
      "loss": 1.1371,
      "step": 14520
    },
    {
      "epoch": 1.7458095524181436,
      "grad_norm": 3.154585361480713,
      "learning_rate": 1.2598817805299771e-05,
      "loss": 1.1567,
      "step": 14530
    },
    {
      "epoch": 1.7470111144487834,
      "grad_norm": 2.828089952468872,
      "learning_rate": 1.2586754594072941e-05,
      "loss": 1.0932,
      "step": 14540
    },
    {
      "epoch": 1.7482126764794232,
      "grad_norm": 2.8708724975585938,
      "learning_rate": 1.2574691382846114e-05,
      "loss": 1.12,
      "step": 14550
    },
    {
      "epoch": 1.749414238510063,
      "grad_norm": 2.997471332550049,
      "learning_rate": 1.2562628171619286e-05,
      "loss": 1.1055,
      "step": 14560
    },
    {
      "epoch": 1.7506158005407029,
      "grad_norm": 2.9262855052948,
      "learning_rate": 1.2550564960392457e-05,
      "loss": 1.0736,
      "step": 14570
    },
    {
      "epoch": 1.7518173625713427,
      "grad_norm": 2.745206117630005,
      "learning_rate": 1.2538501749165629e-05,
      "loss": 1.0288,
      "step": 14580
    },
    {
      "epoch": 1.7530189246019825,
      "grad_norm": 2.863481044769287,
      "learning_rate": 1.2526438537938798e-05,
      "loss": 1.0676,
      "step": 14590
    },
    {
      "epoch": 1.7542204866326223,
      "grad_norm": 2.766179323196411,
      "learning_rate": 1.2514375326711971e-05,
      "loss": 1.1129,
      "step": 14600
    },
    {
      "epoch": 1.7554220486632621,
      "grad_norm": 3.1777822971343994,
      "learning_rate": 1.2502312115485143e-05,
      "loss": 1.1324,
      "step": 14610
    },
    {
      "epoch": 1.756623610693902,
      "grad_norm": 3.197956085205078,
      "learning_rate": 1.2490248904258314e-05,
      "loss": 1.1081,
      "step": 14620
    },
    {
      "epoch": 1.7578251727245418,
      "grad_norm": 3.2736096382141113,
      "learning_rate": 1.2478185693031484e-05,
      "loss": 1.1277,
      "step": 14630
    },
    {
      "epoch": 1.7590267347551818,
      "grad_norm": 3.025806427001953,
      "learning_rate": 1.2466122481804657e-05,
      "loss": 1.1039,
      "step": 14640
    },
    {
      "epoch": 1.7602282967858216,
      "grad_norm": 3.4069442749023438,
      "learning_rate": 1.2454059270577829e-05,
      "loss": 1.1306,
      "step": 14650
    },
    {
      "epoch": 1.7614298588164614,
      "grad_norm": 2.89910626411438,
      "learning_rate": 1.2441996059351e-05,
      "loss": 1.0833,
      "step": 14660
    },
    {
      "epoch": 1.7626314208471012,
      "grad_norm": 3.518203020095825,
      "learning_rate": 1.242993284812417e-05,
      "loss": 1.0875,
      "step": 14670
    },
    {
      "epoch": 1.763832982877741,
      "grad_norm": 2.4610848426818848,
      "learning_rate": 1.2417869636897341e-05,
      "loss": 1.1688,
      "step": 14680
    },
    {
      "epoch": 1.7650345449083809,
      "grad_norm": 2.617189407348633,
      "learning_rate": 1.2405806425670514e-05,
      "loss": 1.1374,
      "step": 14690
    },
    {
      "epoch": 1.7662361069390209,
      "grad_norm": 2.8157460689544678,
      "learning_rate": 1.2393743214443686e-05,
      "loss": 1.1686,
      "step": 14700
    },
    {
      "epoch": 1.7674376689696607,
      "grad_norm": 3.3738279342651367,
      "learning_rate": 1.2381680003216856e-05,
      "loss": 1.1193,
      "step": 14710
    },
    {
      "epoch": 1.7686392310003005,
      "grad_norm": 3.236607313156128,
      "learning_rate": 1.2369616791990027e-05,
      "loss": 1.1454,
      "step": 14720
    },
    {
      "epoch": 1.7698407930309403,
      "grad_norm": 2.799445867538452,
      "learning_rate": 1.23575535807632e-05,
      "loss": 1.1088,
      "step": 14730
    },
    {
      "epoch": 1.7710423550615801,
      "grad_norm": 2.852010726928711,
      "learning_rate": 1.2345490369536372e-05,
      "loss": 1.1013,
      "step": 14740
    },
    {
      "epoch": 1.77224391709222,
      "grad_norm": 2.8602421283721924,
      "learning_rate": 1.2333427158309541e-05,
      "loss": 1.1036,
      "step": 14750
    },
    {
      "epoch": 1.7734454791228598,
      "grad_norm": 3.167921781539917,
      "learning_rate": 1.2321363947082713e-05,
      "loss": 1.0744,
      "step": 14760
    },
    {
      "epoch": 1.7746470411534996,
      "grad_norm": 2.7038190364837646,
      "learning_rate": 1.2309300735855884e-05,
      "loss": 1.0838,
      "step": 14770
    },
    {
      "epoch": 1.7758486031841394,
      "grad_norm": 2.8145735263824463,
      "learning_rate": 1.2297237524629057e-05,
      "loss": 1.0766,
      "step": 14780
    },
    {
      "epoch": 1.7770501652147792,
      "grad_norm": 2.8569042682647705,
      "learning_rate": 1.2285174313402229e-05,
      "loss": 1.1421,
      "step": 14790
    },
    {
      "epoch": 1.778251727245419,
      "grad_norm": 2.7782015800476074,
      "learning_rate": 1.2273111102175399e-05,
      "loss": 1.0658,
      "step": 14800
    },
    {
      "epoch": 1.7794532892760588,
      "grad_norm": 2.9003067016601562,
      "learning_rate": 1.226104789094857e-05,
      "loss": 1.1786,
      "step": 14810
    },
    {
      "epoch": 1.7806548513066986,
      "grad_norm": 2.6498167514801025,
      "learning_rate": 1.2248984679721743e-05,
      "loss": 1.1053,
      "step": 14820
    },
    {
      "epoch": 1.7818564133373385,
      "grad_norm": 2.7879221439361572,
      "learning_rate": 1.2236921468494915e-05,
      "loss": 1.1245,
      "step": 14830
    },
    {
      "epoch": 1.7830579753679783,
      "grad_norm": 2.6080760955810547,
      "learning_rate": 1.2224858257268084e-05,
      "loss": 1.1005,
      "step": 14840
    },
    {
      "epoch": 1.784259537398618,
      "grad_norm": 2.7479209899902344,
      "learning_rate": 1.2212795046041256e-05,
      "loss": 1.0998,
      "step": 14850
    },
    {
      "epoch": 1.785461099429258,
      "grad_norm": 2.8594014644622803,
      "learning_rate": 1.2200731834814429e-05,
      "loss": 1.1203,
      "step": 14860
    },
    {
      "epoch": 1.7866626614598977,
      "grad_norm": 2.863253116607666,
      "learning_rate": 1.21886686235876e-05,
      "loss": 1.0824,
      "step": 14870
    },
    {
      "epoch": 1.7878642234905378,
      "grad_norm": 2.740344524383545,
      "learning_rate": 1.217660541236077e-05,
      "loss": 1.1106,
      "step": 14880
    },
    {
      "epoch": 1.7890657855211776,
      "grad_norm": 2.4603424072265625,
      "learning_rate": 1.2164542201133942e-05,
      "loss": 1.0648,
      "step": 14890
    },
    {
      "epoch": 1.7902673475518174,
      "grad_norm": 2.502607583999634,
      "learning_rate": 1.2152478989907113e-05,
      "loss": 1.104,
      "step": 14900
    },
    {
      "epoch": 1.7914689095824572,
      "grad_norm": 2.7419791221618652,
      "learning_rate": 1.2140415778680286e-05,
      "loss": 1.0873,
      "step": 14910
    },
    {
      "epoch": 1.792670471613097,
      "grad_norm": 2.681208610534668,
      "learning_rate": 1.2128352567453456e-05,
      "loss": 1.1223,
      "step": 14920
    },
    {
      "epoch": 1.7938720336437368,
      "grad_norm": 2.5839929580688477,
      "learning_rate": 1.211749567734931e-05,
      "loss": 1.0874,
      "step": 14930
    },
    {
      "epoch": 1.7950735956743769,
      "grad_norm": 2.7420036792755127,
      "learning_rate": 1.2105432466122482e-05,
      "loss": 1.1035,
      "step": 14940
    },
    {
      "epoch": 1.7962751577050167,
      "grad_norm": 2.76589298248291,
      "learning_rate": 1.2093369254895655e-05,
      "loss": 1.151,
      "step": 14950
    },
    {
      "epoch": 1.7974767197356565,
      "grad_norm": 2.9434914588928223,
      "learning_rate": 1.2081306043668825e-05,
      "loss": 1.1175,
      "step": 14960
    },
    {
      "epoch": 1.7986782817662963,
      "grad_norm": 2.876433849334717,
      "learning_rate": 1.2069242832441996e-05,
      "loss": 1.0487,
      "step": 14970
    },
    {
      "epoch": 1.799879843796936,
      "grad_norm": 3.0262327194213867,
      "learning_rate": 1.2057179621215168e-05,
      "loss": 1.1305,
      "step": 14980
    },
    {
      "epoch": 1.801081405827576,
      "grad_norm": 2.6810131072998047,
      "learning_rate": 1.2045116409988339e-05,
      "loss": 1.12,
      "step": 14990
    },
    {
      "epoch": 1.8022829678582157,
      "grad_norm": 2.55770206451416,
      "learning_rate": 1.203305319876151e-05,
      "loss": 1.0922,
      "step": 15000
    },
    {
      "epoch": 1.8034845298888555,
      "grad_norm": 2.943786382675171,
      "learning_rate": 1.2020989987534682e-05,
      "loss": 1.0686,
      "step": 15010
    },
    {
      "epoch": 1.8046860919194954,
      "grad_norm": 2.6871836185455322,
      "learning_rate": 1.2008926776307853e-05,
      "loss": 1.0661,
      "step": 15020
    },
    {
      "epoch": 1.8058876539501352,
      "grad_norm": 2.7418620586395264,
      "learning_rate": 1.1996863565081025e-05,
      "loss": 1.0853,
      "step": 15030
    },
    {
      "epoch": 1.807089215980775,
      "grad_norm": 3.0869672298431396,
      "learning_rate": 1.1984800353854196e-05,
      "loss": 1.0902,
      "step": 15040
    },
    {
      "epoch": 1.8082907780114148,
      "grad_norm": 3.255523204803467,
      "learning_rate": 1.1972737142627368e-05,
      "loss": 1.1258,
      "step": 15050
    },
    {
      "epoch": 1.8094923400420546,
      "grad_norm": 2.8723466396331787,
      "learning_rate": 1.1960673931400539e-05,
      "loss": 1.0972,
      "step": 15060
    },
    {
      "epoch": 1.8106939020726944,
      "grad_norm": 2.2615902423858643,
      "learning_rate": 1.194861072017371e-05,
      "loss": 1.1006,
      "step": 15070
    },
    {
      "epoch": 1.8118954641033342,
      "grad_norm": 2.512082099914551,
      "learning_rate": 1.1936547508946882e-05,
      "loss": 1.1089,
      "step": 15080
    },
    {
      "epoch": 1.813097026133974,
      "grad_norm": 2.8147449493408203,
      "learning_rate": 1.1924484297720053e-05,
      "loss": 1.1042,
      "step": 15090
    },
    {
      "epoch": 1.8142985881646139,
      "grad_norm": 3.1074674129486084,
      "learning_rate": 1.1912421086493225e-05,
      "loss": 1.0909,
      "step": 15100
    },
    {
      "epoch": 1.8155001501952537,
      "grad_norm": 3.0187795162200928,
      "learning_rate": 1.1900357875266396e-05,
      "loss": 1.1567,
      "step": 15110
    },
    {
      "epoch": 1.8167017122258937,
      "grad_norm": 2.804513454437256,
      "learning_rate": 1.1888294664039568e-05,
      "loss": 1.1197,
      "step": 15120
    },
    {
      "epoch": 1.8179032742565335,
      "grad_norm": 2.939424991607666,
      "learning_rate": 1.187623145281274e-05,
      "loss": 1.127,
      "step": 15130
    },
    {
      "epoch": 1.8191048362871733,
      "grad_norm": 2.9034242630004883,
      "learning_rate": 1.186416824158591e-05,
      "loss": 1.1189,
      "step": 15140
    },
    {
      "epoch": 1.8203063983178132,
      "grad_norm": 2.934790849685669,
      "learning_rate": 1.1852105030359082e-05,
      "loss": 1.1082,
      "step": 15150
    },
    {
      "epoch": 1.821507960348453,
      "grad_norm": 2.770630121231079,
      "learning_rate": 1.1840041819132254e-05,
      "loss": 1.0764,
      "step": 15160
    },
    {
      "epoch": 1.8227095223790928,
      "grad_norm": 2.8904900550842285,
      "learning_rate": 1.1827978607905425e-05,
      "loss": 1.0908,
      "step": 15170
    },
    {
      "epoch": 1.8239110844097326,
      "grad_norm": 2.7439935207366943,
      "learning_rate": 1.1815915396678596e-05,
      "loss": 1.1073,
      "step": 15180
    },
    {
      "epoch": 1.8251126464403726,
      "grad_norm": 2.830331802368164,
      "learning_rate": 1.1803852185451768e-05,
      "loss": 1.1124,
      "step": 15190
    },
    {
      "epoch": 1.8263142084710124,
      "grad_norm": 2.7875144481658936,
      "learning_rate": 1.179178897422494e-05,
      "loss": 1.1449,
      "step": 15200
    },
    {
      "epoch": 1.8275157705016523,
      "grad_norm": 2.8925747871398926,
      "learning_rate": 1.1779725762998109e-05,
      "loss": 1.0787,
      "step": 15210
    },
    {
      "epoch": 1.828717332532292,
      "grad_norm": 2.765932559967041,
      "learning_rate": 1.1768868872893965e-05,
      "loss": 1.0945,
      "step": 15220
    },
    {
      "epoch": 1.8299188945629319,
      "grad_norm": 2.7500219345092773,
      "learning_rate": 1.1756805661667137e-05,
      "loss": 1.1512,
      "step": 15230
    },
    {
      "epoch": 1.8311204565935717,
      "grad_norm": 2.8426265716552734,
      "learning_rate": 1.1744742450440306e-05,
      "loss": 1.1677,
      "step": 15240
    },
    {
      "epoch": 1.8323220186242115,
      "grad_norm": 2.7508461475372314,
      "learning_rate": 1.1732679239213478e-05,
      "loss": 1.089,
      "step": 15250
    },
    {
      "epoch": 1.8335235806548513,
      "grad_norm": 2.8898754119873047,
      "learning_rate": 1.1720616027986651e-05,
      "loss": 1.1252,
      "step": 15260
    },
    {
      "epoch": 1.8347251426854911,
      "grad_norm": 3.036637306213379,
      "learning_rate": 1.1708552816759822e-05,
      "loss": 1.1734,
      "step": 15270
    },
    {
      "epoch": 1.835926704716131,
      "grad_norm": 2.595470666885376,
      "learning_rate": 1.1696489605532994e-05,
      "loss": 1.1453,
      "step": 15280
    },
    {
      "epoch": 1.8371282667467708,
      "grad_norm": 3.0644896030426025,
      "learning_rate": 1.1684426394306164e-05,
      "loss": 1.1678,
      "step": 15290
    },
    {
      "epoch": 1.8383298287774106,
      "grad_norm": 2.8849050998687744,
      "learning_rate": 1.1672363183079337e-05,
      "loss": 1.0909,
      "step": 15300
    },
    {
      "epoch": 1.8395313908080504,
      "grad_norm": 3.065314531326294,
      "learning_rate": 1.1660299971852508e-05,
      "loss": 1.1329,
      "step": 15310
    },
    {
      "epoch": 1.8407329528386902,
      "grad_norm": 3.4492850303649902,
      "learning_rate": 1.164823676062568e-05,
      "loss": 1.1287,
      "step": 15320
    },
    {
      "epoch": 1.84193451486933,
      "grad_norm": 2.70270037651062,
      "learning_rate": 1.163617354939885e-05,
      "loss": 1.0461,
      "step": 15330
    },
    {
      "epoch": 1.8431360768999698,
      "grad_norm": 3.303175926208496,
      "learning_rate": 1.162411033817202e-05,
      "loss": 1.0934,
      "step": 15340
    },
    {
      "epoch": 1.8443376389306096,
      "grad_norm": 3.051143169403076,
      "learning_rate": 1.1612047126945194e-05,
      "loss": 1.1358,
      "step": 15350
    },
    {
      "epoch": 1.8455392009612495,
      "grad_norm": 3.1895675659179688,
      "learning_rate": 1.1599983915718365e-05,
      "loss": 1.1228,
      "step": 15360
    },
    {
      "epoch": 1.8467407629918895,
      "grad_norm": 2.8445587158203125,
      "learning_rate": 1.1587920704491535e-05,
      "loss": 1.0997,
      "step": 15370
    },
    {
      "epoch": 1.8479423250225293,
      "grad_norm": 2.7920093536376953,
      "learning_rate": 1.1575857493264707e-05,
      "loss": 1.1106,
      "step": 15380
    },
    {
      "epoch": 1.8491438870531691,
      "grad_norm": 2.897324800491333,
      "learning_rate": 1.156379428203788e-05,
      "loss": 1.1391,
      "step": 15390
    },
    {
      "epoch": 1.850345449083809,
      "grad_norm": 2.845087766647339,
      "learning_rate": 1.1551731070811051e-05,
      "loss": 1.1286,
      "step": 15400
    },
    {
      "epoch": 1.8515470111144487,
      "grad_norm": 2.6107709407806396,
      "learning_rate": 1.1539667859584221e-05,
      "loss": 1.1456,
      "step": 15410
    },
    {
      "epoch": 1.8527485731450886,
      "grad_norm": 2.8796939849853516,
      "learning_rate": 1.1527604648357392e-05,
      "loss": 1.1289,
      "step": 15420
    },
    {
      "epoch": 1.8539501351757286,
      "grad_norm": 2.6360466480255127,
      "learning_rate": 1.1515541437130564e-05,
      "loss": 1.0891,
      "step": 15430
    },
    {
      "epoch": 1.8551516972063684,
      "grad_norm": 2.7793564796447754,
      "learning_rate": 1.1503478225903737e-05,
      "loss": 1.1195,
      "step": 15440
    },
    {
      "epoch": 1.8563532592370082,
      "grad_norm": 2.83780837059021,
      "learning_rate": 1.1491415014676907e-05,
      "loss": 1.0964,
      "step": 15450
    },
    {
      "epoch": 1.857554821267648,
      "grad_norm": 2.6612977981567383,
      "learning_rate": 1.1479351803450078e-05,
      "loss": 1.1564,
      "step": 15460
    },
    {
      "epoch": 1.8587563832982879,
      "grad_norm": 2.8129489421844482,
      "learning_rate": 1.146728859222325e-05,
      "loss": 1.1344,
      "step": 15470
    },
    {
      "epoch": 1.8599579453289277,
      "grad_norm": 3.077255964279175,
      "learning_rate": 1.1455225380996423e-05,
      "loss": 1.0993,
      "step": 15480
    },
    {
      "epoch": 1.8611595073595675,
      "grad_norm": 2.9234495162963867,
      "learning_rate": 1.1443162169769594e-05,
      "loss": 1.1434,
      "step": 15490
    },
    {
      "epoch": 1.8623610693902073,
      "grad_norm": 2.6361184120178223,
      "learning_rate": 1.1431098958542764e-05,
      "loss": 1.1238,
      "step": 15500
    },
    {
      "epoch": 1.863562631420847,
      "grad_norm": 3.1760616302490234,
      "learning_rate": 1.1419035747315935e-05,
      "loss": 1.1316,
      "step": 15510
    },
    {
      "epoch": 1.864764193451487,
      "grad_norm": 2.504492998123169,
      "learning_rate": 1.1406972536089107e-05,
      "loss": 1.1057,
      "step": 15520
    },
    {
      "epoch": 1.8659657554821267,
      "grad_norm": 3.070638418197632,
      "learning_rate": 1.139490932486228e-05,
      "loss": 1.1005,
      "step": 15530
    },
    {
      "epoch": 1.8671673175127665,
      "grad_norm": 2.7754125595092773,
      "learning_rate": 1.138284611363545e-05,
      "loss": 1.0859,
      "step": 15540
    },
    {
      "epoch": 1.8683688795434064,
      "grad_norm": 3.1174962520599365,
      "learning_rate": 1.1370782902408621e-05,
      "loss": 1.0452,
      "step": 15550
    },
    {
      "epoch": 1.8695704415740462,
      "grad_norm": 2.658019542694092,
      "learning_rate": 1.1358719691181792e-05,
      "loss": 1.0756,
      "step": 15560
    },
    {
      "epoch": 1.870772003604686,
      "grad_norm": 2.9431519508361816,
      "learning_rate": 1.1346656479954966e-05,
      "loss": 1.1039,
      "step": 15570
    },
    {
      "epoch": 1.8719735656353258,
      "grad_norm": 2.982794761657715,
      "learning_rate": 1.1334593268728135e-05,
      "loss": 1.1006,
      "step": 15580
    },
    {
      "epoch": 1.8731751276659656,
      "grad_norm": 3.1167547702789307,
      "learning_rate": 1.1322530057501307e-05,
      "loss": 1.1233,
      "step": 15590
    },
    {
      "epoch": 1.8743766896966054,
      "grad_norm": 2.899101495742798,
      "learning_rate": 1.1310466846274478e-05,
      "loss": 1.1413,
      "step": 15600
    },
    {
      "epoch": 1.8755782517272455,
      "grad_norm": 3.6859450340270996,
      "learning_rate": 1.129840363504765e-05,
      "loss": 1.1185,
      "step": 15610
    },
    {
      "epoch": 1.8767798137578853,
      "grad_norm": 3.014242172241211,
      "learning_rate": 1.1286340423820821e-05,
      "loss": 1.1491,
      "step": 15620
    },
    {
      "epoch": 1.877981375788525,
      "grad_norm": 2.8322460651397705,
      "learning_rate": 1.1274277212593993e-05,
      "loss": 1.0621,
      "step": 15630
    },
    {
      "epoch": 1.879182937819165,
      "grad_norm": 3.0361390113830566,
      "learning_rate": 1.1262214001367164e-05,
      "loss": 1.0423,
      "step": 15640
    },
    {
      "epoch": 1.8803844998498047,
      "grad_norm": 2.902569055557251,
      "learning_rate": 1.1250150790140335e-05,
      "loss": 1.1253,
      "step": 15650
    },
    {
      "epoch": 1.8815860618804445,
      "grad_norm": 2.9784457683563232,
      "learning_rate": 1.1238087578913507e-05,
      "loss": 1.0825,
      "step": 15660
    },
    {
      "epoch": 1.8827876239110846,
      "grad_norm": 3.226654529571533,
      "learning_rate": 1.1226024367686678e-05,
      "loss": 1.1319,
      "step": 15670
    },
    {
      "epoch": 1.8839891859417244,
      "grad_norm": 2.9446699619293213,
      "learning_rate": 1.121396115645985e-05,
      "loss": 1.1599,
      "step": 15680
    },
    {
      "epoch": 1.8851907479723642,
      "grad_norm": 2.90450382232666,
      "learning_rate": 1.1201897945233021e-05,
      "loss": 1.1415,
      "step": 15690
    },
    {
      "epoch": 1.886392310003004,
      "grad_norm": 2.664691686630249,
      "learning_rate": 1.1189834734006193e-05,
      "loss": 1.1119,
      "step": 15700
    },
    {
      "epoch": 1.8875938720336438,
      "grad_norm": 2.660480499267578,
      "learning_rate": 1.1177771522779364e-05,
      "loss": 1.1276,
      "step": 15710
    },
    {
      "epoch": 1.8887954340642836,
      "grad_norm": 3.059915542602539,
      "learning_rate": 1.1165708311552536e-05,
      "loss": 1.102,
      "step": 15720
    },
    {
      "epoch": 1.8899969960949234,
      "grad_norm": 4.02586555480957,
      "learning_rate": 1.1153645100325707e-05,
      "loss": 1.1026,
      "step": 15730
    },
    {
      "epoch": 1.8911985581255633,
      "grad_norm": 3.1444225311279297,
      "learning_rate": 1.1141581889098878e-05,
      "loss": 1.1185,
      "step": 15740
    },
    {
      "epoch": 1.892400120156203,
      "grad_norm": 3.6480209827423096,
      "learning_rate": 1.112951867787205e-05,
      "loss": 1.1424,
      "step": 15750
    },
    {
      "epoch": 1.8936016821868429,
      "grad_norm": 2.705322265625,
      "learning_rate": 1.1117455466645221e-05,
      "loss": 1.0696,
      "step": 15760
    },
    {
      "epoch": 1.8948032442174827,
      "grad_norm": 2.8907666206359863,
      "learning_rate": 1.1105392255418393e-05,
      "loss": 1.108,
      "step": 15770
    },
    {
      "epoch": 1.8960048062481225,
      "grad_norm": 3.2391326427459717,
      "learning_rate": 1.1093329044191564e-05,
      "loss": 1.1175,
      "step": 15780
    },
    {
      "epoch": 1.8972063682787623,
      "grad_norm": 2.5359225273132324,
      "learning_rate": 1.1081265832964736e-05,
      "loss": 1.1369,
      "step": 15790
    },
    {
      "epoch": 1.8984079303094021,
      "grad_norm": 3.1112146377563477,
      "learning_rate": 1.1069202621737907e-05,
      "loss": 1.069,
      "step": 15800
    },
    {
      "epoch": 1.899609492340042,
      "grad_norm": 2.862910032272339,
      "learning_rate": 1.1057139410511079e-05,
      "loss": 1.118,
      "step": 15810
    },
    {
      "epoch": 1.9008110543706818,
      "grad_norm": 2.6183369159698486,
      "learning_rate": 1.104507619928425e-05,
      "loss": 1.0968,
      "step": 15820
    },
    {
      "epoch": 1.9020126164013216,
      "grad_norm": 2.7524561882019043,
      "learning_rate": 1.103301298805742e-05,
      "loss": 1.0772,
      "step": 15830
    },
    {
      "epoch": 1.9032141784319614,
      "grad_norm": 2.833345413208008,
      "learning_rate": 1.1020949776830593e-05,
      "loss": 1.0842,
      "step": 15840
    },
    {
      "epoch": 1.9044157404626014,
      "grad_norm": 2.781982421875,
      "learning_rate": 1.1008886565603764e-05,
      "loss": 1.1015,
      "step": 15850
    },
    {
      "epoch": 1.9056173024932412,
      "grad_norm": 3.478159189224243,
      "learning_rate": 1.0996823354376936e-05,
      "loss": 1.0742,
      "step": 15860
    },
    {
      "epoch": 1.906818864523881,
      "grad_norm": 3.0183048248291016,
      "learning_rate": 1.0984760143150105e-05,
      "loss": 1.0783,
      "step": 15870
    },
    {
      "epoch": 1.9080204265545209,
      "grad_norm": 2.977834939956665,
      "learning_rate": 1.0972696931923279e-05,
      "loss": 1.0725,
      "step": 15880
    },
    {
      "epoch": 1.9092219885851607,
      "grad_norm": 2.8727288246154785,
      "learning_rate": 1.096063372069645e-05,
      "loss": 1.0899,
      "step": 15890
    },
    {
      "epoch": 1.9104235506158005,
      "grad_norm": 2.8672752380371094,
      "learning_rate": 1.0948570509469621e-05,
      "loss": 1.1095,
      "step": 15900
    },
    {
      "epoch": 1.9116251126464405,
      "grad_norm": 3.1840364933013916,
      "learning_rate": 1.0936507298242793e-05,
      "loss": 1.0868,
      "step": 15910
    },
    {
      "epoch": 1.9128266746770803,
      "grad_norm": 2.9606261253356934,
      "learning_rate": 1.0924444087015963e-05,
      "loss": 1.1096,
      "step": 15920
    },
    {
      "epoch": 1.9140282367077202,
      "grad_norm": 3.0050930976867676,
      "learning_rate": 1.0912380875789136e-05,
      "loss": 1.0759,
      "step": 15930
    },
    {
      "epoch": 1.91522979873836,
      "grad_norm": 2.9109609127044678,
      "learning_rate": 1.0900317664562307e-05,
      "loss": 1.0834,
      "step": 15940
    },
    {
      "epoch": 1.9164313607689998,
      "grad_norm": 2.931952714920044,
      "learning_rate": 1.0888254453335479e-05,
      "loss": 1.1031,
      "step": 15950
    },
    {
      "epoch": 1.9176329227996396,
      "grad_norm": 2.8890647888183594,
      "learning_rate": 1.0876191242108648e-05,
      "loss": 1.1201,
      "step": 15960
    },
    {
      "epoch": 1.9188344848302794,
      "grad_norm": 2.9195504188537598,
      "learning_rate": 1.0864128030881822e-05,
      "loss": 1.115,
      "step": 15970
    },
    {
      "epoch": 1.9200360468609192,
      "grad_norm": 3.3140461444854736,
      "learning_rate": 1.0852064819654993e-05,
      "loss": 1.11,
      "step": 15980
    },
    {
      "epoch": 1.921237608891559,
      "grad_norm": 2.578575849533081,
      "learning_rate": 1.0840001608428164e-05,
      "loss": 1.0922,
      "step": 15990
    },
    {
      "epoch": 1.9224391709221988,
      "grad_norm": 2.9054715633392334,
      "learning_rate": 1.0827938397201334e-05,
      "loss": 1.1223,
      "step": 16000
    },
    {
      "epoch": 1.9236407329528387,
      "grad_norm": 2.764845132827759,
      "learning_rate": 1.0815875185974506e-05,
      "loss": 1.1341,
      "step": 16010
    },
    {
      "epoch": 1.9248422949834785,
      "grad_norm": 2.772242307662964,
      "learning_rate": 1.0803811974747679e-05,
      "loss": 1.1338,
      "step": 16020
    },
    {
      "epoch": 1.9260438570141183,
      "grad_norm": 3.0030429363250732,
      "learning_rate": 1.079174876352085e-05,
      "loss": 1.1247,
      "step": 16030
    },
    {
      "epoch": 1.927245419044758,
      "grad_norm": 2.939311981201172,
      "learning_rate": 1.077968555229402e-05,
      "loss": 1.1263,
      "step": 16040
    },
    {
      "epoch": 1.928446981075398,
      "grad_norm": 2.880075216293335,
      "learning_rate": 1.0767622341067191e-05,
      "loss": 1.0686,
      "step": 16050
    },
    {
      "epoch": 1.9296485431060377,
      "grad_norm": 3.776834487915039,
      "learning_rate": 1.0755559129840365e-05,
      "loss": 1.1133,
      "step": 16060
    },
    {
      "epoch": 1.9308501051366775,
      "grad_norm": 2.8699803352355957,
      "learning_rate": 1.0743495918613536e-05,
      "loss": 1.0749,
      "step": 16070
    },
    {
      "epoch": 1.9320516671673174,
      "grad_norm": 3.0478649139404297,
      "learning_rate": 1.0731432707386706e-05,
      "loss": 1.1256,
      "step": 16080
    },
    {
      "epoch": 1.9332532291979574,
      "grad_norm": 2.841740846633911,
      "learning_rate": 1.0719369496159877e-05,
      "loss": 1.0614,
      "step": 16090
    },
    {
      "epoch": 1.9344547912285972,
      "grad_norm": 2.6649258136749268,
      "learning_rate": 1.0707306284933049e-05,
      "loss": 1.1031,
      "step": 16100
    },
    {
      "epoch": 1.935656353259237,
      "grad_norm": 2.761329174041748,
      "learning_rate": 1.0695243073706222e-05,
      "loss": 1.0854,
      "step": 16110
    },
    {
      "epoch": 1.9368579152898768,
      "grad_norm": 2.7711074352264404,
      "learning_rate": 1.0683179862479393e-05,
      "loss": 1.0953,
      "step": 16120
    },
    {
      "epoch": 1.9380594773205166,
      "grad_norm": 3.113908290863037,
      "learning_rate": 1.0671116651252563e-05,
      "loss": 1.1019,
      "step": 16130
    },
    {
      "epoch": 1.9392610393511565,
      "grad_norm": 3.25856876373291,
      "learning_rate": 1.0659053440025734e-05,
      "loss": 1.1892,
      "step": 16140
    },
    {
      "epoch": 1.9404626013817965,
      "grad_norm": 2.4150781631469727,
      "learning_rate": 1.0646990228798908e-05,
      "loss": 1.1025,
      "step": 16150
    },
    {
      "epoch": 1.9416641634124363,
      "grad_norm": 3.3375720977783203,
      "learning_rate": 1.0634927017572079e-05,
      "loss": 1.0755,
      "step": 16160
    },
    {
      "epoch": 1.9428657254430761,
      "grad_norm": 3.591082811355591,
      "learning_rate": 1.0622863806345249e-05,
      "loss": 1.1111,
      "step": 16170
    },
    {
      "epoch": 1.944067287473716,
      "grad_norm": 3.2751779556274414,
      "learning_rate": 1.061080059511842e-05,
      "loss": 1.0836,
      "step": 16180
    },
    {
      "epoch": 1.9452688495043557,
      "grad_norm": 2.9355640411376953,
      "learning_rate": 1.0598737383891592e-05,
      "loss": 1.0731,
      "step": 16190
    },
    {
      "epoch": 1.9464704115349956,
      "grad_norm": 3.1960551738739014,
      "learning_rate": 1.0586674172664765e-05,
      "loss": 1.119,
      "step": 16200
    },
    {
      "epoch": 1.9476719735656354,
      "grad_norm": 2.9307806491851807,
      "learning_rate": 1.0574610961437934e-05,
      "loss": 1.1519,
      "step": 16210
    },
    {
      "epoch": 1.9488735355962752,
      "grad_norm": 2.772674083709717,
      "learning_rate": 1.0562547750211106e-05,
      "loss": 1.1363,
      "step": 16220
    },
    {
      "epoch": 1.950075097626915,
      "grad_norm": 3.4186201095581055,
      "learning_rate": 1.0550484538984277e-05,
      "loss": 1.1452,
      "step": 16230
    },
    {
      "epoch": 1.9512766596575548,
      "grad_norm": 2.5888946056365967,
      "learning_rate": 1.053842132775745e-05,
      "loss": 1.0898,
      "step": 16240
    },
    {
      "epoch": 1.9524782216881946,
      "grad_norm": 2.5674514770507812,
      "learning_rate": 1.052635811653062e-05,
      "loss": 1.0702,
      "step": 16250
    },
    {
      "epoch": 1.9536797837188344,
      "grad_norm": 2.6910669803619385,
      "learning_rate": 1.0514294905303792e-05,
      "loss": 1.0935,
      "step": 16260
    },
    {
      "epoch": 1.9548813457494743,
      "grad_norm": 2.771521806716919,
      "learning_rate": 1.0502231694076963e-05,
      "loss": 1.0679,
      "step": 16270
    },
    {
      "epoch": 1.956082907780114,
      "grad_norm": 2.820141077041626,
      "learning_rate": 1.0490168482850136e-05,
      "loss": 1.1251,
      "step": 16280
    },
    {
      "epoch": 1.9572844698107539,
      "grad_norm": 2.697119951248169,
      "learning_rate": 1.0478105271623306e-05,
      "loss": 1.0914,
      "step": 16290
    },
    {
      "epoch": 1.9584860318413937,
      "grad_norm": 3.7266643047332764,
      "learning_rate": 1.0466042060396477e-05,
      "loss": 1.104,
      "step": 16300
    },
    {
      "epoch": 1.9596875938720335,
      "grad_norm": 2.890990734100342,
      "learning_rate": 1.0453978849169649e-05,
      "loss": 1.1244,
      "step": 16310
    },
    {
      "epoch": 1.9608891559026733,
      "grad_norm": 3.503268003463745,
      "learning_rate": 1.044191563794282e-05,
      "loss": 1.1074,
      "step": 16320
    },
    {
      "epoch": 1.9620907179333134,
      "grad_norm": 2.6462128162384033,
      "learning_rate": 1.0429852426715993e-05,
      "loss": 1.0936,
      "step": 16330
    },
    {
      "epoch": 1.9632922799639532,
      "grad_norm": 2.666914701461792,
      "learning_rate": 1.0417789215489163e-05,
      "loss": 1.1336,
      "step": 16340
    },
    {
      "epoch": 1.964493841994593,
      "grad_norm": 3.018734931945801,
      "learning_rate": 1.0405726004262335e-05,
      "loss": 1.1125,
      "step": 16350
    },
    {
      "epoch": 1.9656954040252328,
      "grad_norm": 2.988618850708008,
      "learning_rate": 1.0393662793035506e-05,
      "loss": 1.045,
      "step": 16360
    },
    {
      "epoch": 1.9668969660558726,
      "grad_norm": 2.9957151412963867,
      "learning_rate": 1.038159958180868e-05,
      "loss": 1.1343,
      "step": 16370
    },
    {
      "epoch": 1.9680985280865124,
      "grad_norm": 2.834824562072754,
      "learning_rate": 1.0369536370581849e-05,
      "loss": 1.1189,
      "step": 16380
    },
    {
      "epoch": 1.9693000901171525,
      "grad_norm": 2.831989049911499,
      "learning_rate": 1.035747315935502e-05,
      "loss": 1.1182,
      "step": 16390
    },
    {
      "epoch": 1.9705016521477923,
      "grad_norm": 2.805915594100952,
      "learning_rate": 1.0345409948128192e-05,
      "loss": 1.0873,
      "step": 16400
    },
    {
      "epoch": 1.971703214178432,
      "grad_norm": 2.8318607807159424,
      "learning_rate": 1.0333346736901363e-05,
      "loss": 1.1356,
      "step": 16410
    },
    {
      "epoch": 1.972904776209072,
      "grad_norm": 2.6894469261169434,
      "learning_rate": 1.0321283525674535e-05,
      "loss": 1.1112,
      "step": 16420
    },
    {
      "epoch": 1.9741063382397117,
      "grad_norm": 2.9541690349578857,
      "learning_rate": 1.0309220314447706e-05,
      "loss": 1.0691,
      "step": 16430
    },
    {
      "epoch": 1.9753079002703515,
      "grad_norm": 2.8670668601989746,
      "learning_rate": 1.0297157103220878e-05,
      "loss": 1.1015,
      "step": 16440
    },
    {
      "epoch": 1.9765094623009913,
      "grad_norm": 3.0003573894500732,
      "learning_rate": 1.0285093891994049e-05,
      "loss": 1.0808,
      "step": 16450
    },
    {
      "epoch": 1.9777110243316312,
      "grad_norm": 4.085958003997803,
      "learning_rate": 1.027303068076722e-05,
      "loss": 1.1889,
      "step": 16460
    },
    {
      "epoch": 1.978912586362271,
      "grad_norm": 2.802501678466797,
      "learning_rate": 1.0260967469540392e-05,
      "loss": 1.0936,
      "step": 16470
    },
    {
      "epoch": 1.9801141483929108,
      "grad_norm": 3.1444613933563232,
      "learning_rate": 1.0248904258313563e-05,
      "loss": 1.124,
      "step": 16480
    },
    {
      "epoch": 1.9813157104235506,
      "grad_norm": 3.4045493602752686,
      "learning_rate": 1.0236841047086735e-05,
      "loss": 1.0656,
      "step": 16490
    },
    {
      "epoch": 1.9825172724541904,
      "grad_norm": 3.2946712970733643,
      "learning_rate": 1.0224777835859906e-05,
      "loss": 1.1033,
      "step": 16500
    },
    {
      "epoch": 1.9837188344848302,
      "grad_norm": 3.103276014328003,
      "learning_rate": 1.0212714624633078e-05,
      "loss": 1.0773,
      "step": 16510
    },
    {
      "epoch": 1.98492039651547,
      "grad_norm": 3.040001630783081,
      "learning_rate": 1.0200651413406249e-05,
      "loss": 1.1139,
      "step": 16520
    },
    {
      "epoch": 1.9861219585461098,
      "grad_norm": 2.791308879852295,
      "learning_rate": 1.018858820217942e-05,
      "loss": 1.0821,
      "step": 16530
    },
    {
      "epoch": 1.9873235205767497,
      "grad_norm": 3.7371764183044434,
      "learning_rate": 1.0176524990952592e-05,
      "loss": 1.1306,
      "step": 16540
    },
    {
      "epoch": 1.9885250826073895,
      "grad_norm": 3.2209205627441406,
      "learning_rate": 1.0164461779725763e-05,
      "loss": 1.0961,
      "step": 16550
    },
    {
      "epoch": 1.9897266446380293,
      "grad_norm": 2.7212955951690674,
      "learning_rate": 1.0152398568498935e-05,
      "loss": 1.1402,
      "step": 16560
    },
    {
      "epoch": 1.9909282066686693,
      "grad_norm": 2.854785442352295,
      "learning_rate": 1.0140335357272106e-05,
      "loss": 1.0893,
      "step": 16570
    },
    {
      "epoch": 1.9921297686993091,
      "grad_norm": 2.975830316543579,
      "learning_rate": 1.0128272146045278e-05,
      "loss": 1.1251,
      "step": 16580
    },
    {
      "epoch": 1.993331330729949,
      "grad_norm": 2.707780122756958,
      "learning_rate": 1.0116208934818448e-05,
      "loss": 1.0889,
      "step": 16590
    },
    {
      "epoch": 1.9945328927605888,
      "grad_norm": 2.9143216609954834,
      "learning_rate": 1.010414572359162e-05,
      "loss": 1.1117,
      "step": 16600
    },
    {
      "epoch": 1.9957344547912286,
      "grad_norm": 2.5910775661468506,
      "learning_rate": 1.0092082512364792e-05,
      "loss": 1.0783,
      "step": 16610
    },
    {
      "epoch": 1.9969360168218684,
      "grad_norm": 2.8692636489868164,
      "learning_rate": 1.0080019301137964e-05,
      "loss": 1.1019,
      "step": 16620
    },
    {
      "epoch": 1.9981375788525084,
      "grad_norm": 2.7387828826904297,
      "learning_rate": 1.0067956089911133e-05,
      "loss": 1.094,
      "step": 16630
    },
    {
      "epoch": 1.9993391408831482,
      "grad_norm": 2.9568052291870117,
      "learning_rate": 1.0055892878684306e-05,
      "loss": 1.0511,
      "step": 16640
    },
    {
      "epoch": 2.0004806248122557,
      "grad_norm": 3.219620943069458,
      "learning_rate": 1.0043829667457478e-05,
      "loss": 1.0639,
      "step": 16650
    },
    {
      "epoch": 2.0016821868428956,
      "grad_norm": 3.7475786209106445,
      "learning_rate": 1.003176645623065e-05,
      "loss": 1.0231,
      "step": 16660
    },
    {
      "epoch": 2.0028837488735354,
      "grad_norm": 3.0399677753448486,
      "learning_rate": 1.0019703245003819e-05,
      "loss": 1.0938,
      "step": 16670
    },
    {
      "epoch": 2.0040853109041756,
      "grad_norm": 3.001067876815796,
      "learning_rate": 1.000764003377699e-05,
      "loss": 1.1546,
      "step": 16680
    },
    {
      "epoch": 2.0052868729348154,
      "grad_norm": 3.0315639972686768,
      "learning_rate": 9.995576822550164e-06,
      "loss": 1.0746,
      "step": 16690
    },
    {
      "epoch": 2.0064884349654553,
      "grad_norm": 3.035609245300293,
      "learning_rate": 9.983513611323335e-06,
      "loss": 1.0441,
      "step": 16700
    },
    {
      "epoch": 2.007689996996095,
      "grad_norm": 3.011693000793457,
      "learning_rate": 9.971450400096507e-06,
      "loss": 1.1419,
      "step": 16710
    },
    {
      "epoch": 2.008891559026735,
      "grad_norm": 3.1578879356384277,
      "learning_rate": 9.959387188869676e-06,
      "loss": 1.0734,
      "step": 16720
    },
    {
      "epoch": 2.0100931210573747,
      "grad_norm": 2.756446123123169,
      "learning_rate": 9.94732397764285e-06,
      "loss": 1.0945,
      "step": 16730
    },
    {
      "epoch": 2.0112946830880145,
      "grad_norm": 2.8078510761260986,
      "learning_rate": 9.93526076641602e-06,
      "loss": 1.1069,
      "step": 16740
    },
    {
      "epoch": 2.0124962451186543,
      "grad_norm": 2.6962032318115234,
      "learning_rate": 9.923197555189192e-06,
      "loss": 1.09,
      "step": 16750
    },
    {
      "epoch": 2.013697807149294,
      "grad_norm": 3.201458215713501,
      "learning_rate": 9.911134343962362e-06,
      "loss": 1.0858,
      "step": 16760
    },
    {
      "epoch": 2.014899369179934,
      "grad_norm": 2.667069673538208,
      "learning_rate": 9.899071132735535e-06,
      "loss": 1.0626,
      "step": 16770
    },
    {
      "epoch": 2.0161009312105738,
      "grad_norm": 2.9874284267425537,
      "learning_rate": 9.887007921508707e-06,
      "loss": 1.1252,
      "step": 16780
    },
    {
      "epoch": 2.0173024932412136,
      "grad_norm": 2.7961926460266113,
      "learning_rate": 9.874944710281878e-06,
      "loss": 1.083,
      "step": 16790
    },
    {
      "epoch": 2.0185040552718534,
      "grad_norm": 2.8429317474365234,
      "learning_rate": 9.862881499055048e-06,
      "loss": 1.1128,
      "step": 16800
    },
    {
      "epoch": 2.019705617302493,
      "grad_norm": 2.775108814239502,
      "learning_rate": 9.85081828782822e-06,
      "loss": 1.0551,
      "step": 16810
    },
    {
      "epoch": 2.020907179333133,
      "grad_norm": 3.4425742626190186,
      "learning_rate": 9.838755076601392e-06,
      "loss": 1.1283,
      "step": 16820
    },
    {
      "epoch": 2.022108741363773,
      "grad_norm": 3.387361764907837,
      "learning_rate": 9.826691865374564e-06,
      "loss": 1.1505,
      "step": 16830
    },
    {
      "epoch": 2.0233103033944126,
      "grad_norm": 2.921731948852539,
      "learning_rate": 9.814628654147734e-06,
      "loss": 1.0477,
      "step": 16840
    },
    {
      "epoch": 2.0245118654250525,
      "grad_norm": 2.9522814750671387,
      "learning_rate": 9.802565442920905e-06,
      "loss": 1.1363,
      "step": 16850
    },
    {
      "epoch": 2.0257134274556923,
      "grad_norm": 3.197028875350952,
      "learning_rate": 9.790502231694078e-06,
      "loss": 1.0678,
      "step": 16860
    },
    {
      "epoch": 2.026914989486332,
      "grad_norm": 4.085653305053711,
      "learning_rate": 9.77843902046725e-06,
      "loss": 1.1997,
      "step": 16870
    },
    {
      "epoch": 2.028116551516972,
      "grad_norm": 3.3505091667175293,
      "learning_rate": 9.76637580924042e-06,
      "loss": 1.0614,
      "step": 16880
    },
    {
      "epoch": 2.0293181135476117,
      "grad_norm": 3.1307883262634277,
      "learning_rate": 9.75431259801359e-06,
      "loss": 1.1135,
      "step": 16890
    },
    {
      "epoch": 2.0305196755782515,
      "grad_norm": 2.909670829772949,
      "learning_rate": 9.742249386786762e-06,
      "loss": 1.134,
      "step": 16900
    },
    {
      "epoch": 2.031721237608892,
      "grad_norm": 2.8021414279937744,
      "learning_rate": 9.730186175559935e-06,
      "loss": 1.102,
      "step": 16910
    },
    {
      "epoch": 2.0329227996395316,
      "grad_norm": 2.7228875160217285,
      "learning_rate": 9.718122964333107e-06,
      "loss": 1.0673,
      "step": 16920
    },
    {
      "epoch": 2.0341243616701714,
      "grad_norm": 2.9417099952697754,
      "learning_rate": 9.706059753106277e-06,
      "loss": 1.0454,
      "step": 16930
    },
    {
      "epoch": 2.0353259237008112,
      "grad_norm": 2.941394090652466,
      "learning_rate": 9.693996541879448e-06,
      "loss": 1.0908,
      "step": 16940
    },
    {
      "epoch": 2.036527485731451,
      "grad_norm": 3.17948842048645,
      "learning_rate": 9.681933330652621e-06,
      "loss": 1.0705,
      "step": 16950
    },
    {
      "epoch": 2.037729047762091,
      "grad_norm": 3.1741280555725098,
      "learning_rate": 9.669870119425793e-06,
      "loss": 1.0542,
      "step": 16960
    },
    {
      "epoch": 2.0389306097927307,
      "grad_norm": 3.3023715019226074,
      "learning_rate": 9.657806908198962e-06,
      "loss": 1.0592,
      "step": 16970
    },
    {
      "epoch": 2.0401321718233705,
      "grad_norm": 3.0020363330841064,
      "learning_rate": 9.645743696972134e-06,
      "loss": 1.1094,
      "step": 16980
    },
    {
      "epoch": 2.0413337338540103,
      "grad_norm": 3.3084299564361572,
      "learning_rate": 9.633680485745305e-06,
      "loss": 1.1189,
      "step": 16990
    },
    {
      "epoch": 2.04253529588465,
      "grad_norm": 3.40694260597229,
      "learning_rate": 9.621617274518478e-06,
      "loss": 1.0976,
      "step": 17000
    },
    {
      "epoch": 2.04373685791529,
      "grad_norm": 3.05115008354187,
      "learning_rate": 9.609554063291648e-06,
      "loss": 1.1034,
      "step": 17010
    },
    {
      "epoch": 2.0449384199459297,
      "grad_norm": 3.0174062252044678,
      "learning_rate": 9.59749085206482e-06,
      "loss": 1.1518,
      "step": 17020
    },
    {
      "epoch": 2.0461399819765695,
      "grad_norm": 3.3065357208251953,
      "learning_rate": 9.585427640837991e-06,
      "loss": 1.0861,
      "step": 17030
    },
    {
      "epoch": 2.0473415440072094,
      "grad_norm": 2.746835947036743,
      "learning_rate": 9.573364429611164e-06,
      "loss": 1.0809,
      "step": 17040
    },
    {
      "epoch": 2.048543106037849,
      "grad_norm": 3.3798608779907227,
      "learning_rate": 9.561301218384334e-06,
      "loss": 1.0729,
      "step": 17050
    },
    {
      "epoch": 2.049744668068489,
      "grad_norm": 3.181321382522583,
      "learning_rate": 9.549238007157505e-06,
      "loss": 1.0709,
      "step": 17060
    },
    {
      "epoch": 2.050946230099129,
      "grad_norm": 2.8408315181732178,
      "learning_rate": 9.537174795930677e-06,
      "loss": 1.0976,
      "step": 17070
    },
    {
      "epoch": 2.0521477921297686,
      "grad_norm": 3.16947078704834,
      "learning_rate": 9.525111584703848e-06,
      "loss": 1.1127,
      "step": 17080
    },
    {
      "epoch": 2.0533493541604084,
      "grad_norm": 2.9917047023773193,
      "learning_rate": 9.51304837347702e-06,
      "loss": 1.1064,
      "step": 17090
    },
    {
      "epoch": 2.0545509161910482,
      "grad_norm": 3.4687018394470215,
      "learning_rate": 9.500985162250191e-06,
      "loss": 1.0873,
      "step": 17100
    },
    {
      "epoch": 2.055752478221688,
      "grad_norm": 3.13853120803833,
      "learning_rate": 9.488921951023362e-06,
      "loss": 1.0688,
      "step": 17110
    },
    {
      "epoch": 2.056954040252328,
      "grad_norm": 3.067312479019165,
      "learning_rate": 9.476858739796534e-06,
      "loss": 1.0724,
      "step": 17120
    },
    {
      "epoch": 2.0581556022829677,
      "grad_norm": 2.957613706588745,
      "learning_rate": 9.464795528569707e-06,
      "loss": 1.0471,
      "step": 17130
    },
    {
      "epoch": 2.0593571643136075,
      "grad_norm": 3.2582008838653564,
      "learning_rate": 9.452732317342877e-06,
      "loss": 1.0376,
      "step": 17140
    },
    {
      "epoch": 2.0605587263442473,
      "grad_norm": 2.9080727100372314,
      "learning_rate": 9.440669106116048e-06,
      "loss": 1.0383,
      "step": 17150
    },
    {
      "epoch": 2.0617602883748876,
      "grad_norm": 3.4288315773010254,
      "learning_rate": 9.42860589488922e-06,
      "loss": 1.1066,
      "step": 17160
    },
    {
      "epoch": 2.0629618504055274,
      "grad_norm": 3.116729259490967,
      "learning_rate": 9.416542683662391e-06,
      "loss": 1.083,
      "step": 17170
    },
    {
      "epoch": 2.064163412436167,
      "grad_norm": 3.2210443019866943,
      "learning_rate": 9.404479472435563e-06,
      "loss": 1.1091,
      "step": 17180
    },
    {
      "epoch": 2.065364974466807,
      "grad_norm": 3.1489336490631104,
      "learning_rate": 9.392416261208734e-06,
      "loss": 1.1243,
      "step": 17190
    },
    {
      "epoch": 2.066566536497447,
      "grad_norm": 3.597325086593628,
      "learning_rate": 9.380353049981905e-06,
      "loss": 1.0712,
      "step": 17200
    },
    {
      "epoch": 2.0677680985280866,
      "grad_norm": 3.13067626953125,
      "learning_rate": 9.368289838755077e-06,
      "loss": 1.1041,
      "step": 17210
    },
    {
      "epoch": 2.0689696605587264,
      "grad_norm": 3.2703537940979004,
      "learning_rate": 9.356226627528248e-06,
      "loss": 1.0788,
      "step": 17220
    },
    {
      "epoch": 2.0701712225893663,
      "grad_norm": 3.241881847381592,
      "learning_rate": 9.34416341630142e-06,
      "loss": 1.0819,
      "step": 17230
    },
    {
      "epoch": 2.071372784620006,
      "grad_norm": 3.0537993907928467,
      "learning_rate": 9.332100205074591e-06,
      "loss": 1.0859,
      "step": 17240
    },
    {
      "epoch": 2.072574346650646,
      "grad_norm": 3.066330671310425,
      "learning_rate": 9.320036993847763e-06,
      "loss": 1.0778,
      "step": 17250
    },
    {
      "epoch": 2.0737759086812857,
      "grad_norm": 3.185352087020874,
      "learning_rate": 9.307973782620934e-06,
      "loss": 1.0462,
      "step": 17260
    },
    {
      "epoch": 2.0749774707119255,
      "grad_norm": 3.4165289402008057,
      "learning_rate": 9.295910571394106e-06,
      "loss": 1.1348,
      "step": 17270
    },
    {
      "epoch": 2.0761790327425653,
      "grad_norm": 3.516268014907837,
      "learning_rate": 9.283847360167277e-06,
      "loss": 1.1209,
      "step": 17280
    },
    {
      "epoch": 2.077380594773205,
      "grad_norm": 3.198676109313965,
      "learning_rate": 9.271784148940448e-06,
      "loss": 1.1241,
      "step": 17290
    },
    {
      "epoch": 2.078582156803845,
      "grad_norm": 3.102076768875122,
      "learning_rate": 9.259720937713618e-06,
      "loss": 1.0548,
      "step": 17300
    },
    {
      "epoch": 2.0797837188344848,
      "grad_norm": 3.3436553478240967,
      "learning_rate": 9.247657726486791e-06,
      "loss": 1.0562,
      "step": 17310
    },
    {
      "epoch": 2.0809852808651246,
      "grad_norm": 3.29359769821167,
      "learning_rate": 9.235594515259963e-06,
      "loss": 1.0523,
      "step": 17320
    },
    {
      "epoch": 2.0821868428957644,
      "grad_norm": 2.980224370956421,
      "learning_rate": 9.223531304033134e-06,
      "loss": 1.1026,
      "step": 17330
    },
    {
      "epoch": 2.083388404926404,
      "grad_norm": 3.097954750061035,
      "learning_rate": 9.211468092806306e-06,
      "loss": 1.1427,
      "step": 17340
    },
    {
      "epoch": 2.084589966957044,
      "grad_norm": 3.6185672283172607,
      "learning_rate": 9.199404881579477e-06,
      "loss": 1.0704,
      "step": 17350
    },
    {
      "epoch": 2.085791528987684,
      "grad_norm": 4.033557415008545,
      "learning_rate": 9.187341670352648e-06,
      "loss": 1.0707,
      "step": 17360
    },
    {
      "epoch": 2.0869930910183236,
      "grad_norm": 4.872500419616699,
      "learning_rate": 9.17527845912582e-06,
      "loss": 1.0751,
      "step": 17370
    },
    {
      "epoch": 2.0881946530489635,
      "grad_norm": 3.034990072250366,
      "learning_rate": 9.163215247898991e-06,
      "loss": 1.0729,
      "step": 17380
    },
    {
      "epoch": 2.0893962150796033,
      "grad_norm": 3.07326340675354,
      "learning_rate": 9.151152036672161e-06,
      "loss": 1.0594,
      "step": 17390
    },
    {
      "epoch": 2.0905977771102435,
      "grad_norm": 3.2427353858947754,
      "learning_rate": 9.139088825445334e-06,
      "loss": 1.1338,
      "step": 17400
    },
    {
      "epoch": 2.0917993391408833,
      "grad_norm": 3.1006762981414795,
      "learning_rate": 9.127025614218506e-06,
      "loss": 1.0605,
      "step": 17410
    },
    {
      "epoch": 2.093000901171523,
      "grad_norm": 3.1517140865325928,
      "learning_rate": 9.114962402991677e-06,
      "loss": 1.0968,
      "step": 17420
    },
    {
      "epoch": 2.094202463202163,
      "grad_norm": 3.3327667713165283,
      "learning_rate": 9.102899191764847e-06,
      "loss": 1.1586,
      "step": 17430
    },
    {
      "epoch": 2.095404025232803,
      "grad_norm": 3.530585765838623,
      "learning_rate": 9.09083598053802e-06,
      "loss": 1.1223,
      "step": 17440
    },
    {
      "epoch": 2.0966055872634426,
      "grad_norm": 3.4345996379852295,
      "learning_rate": 9.078772769311191e-06,
      "loss": 1.0903,
      "step": 17450
    },
    {
      "epoch": 2.0978071492940824,
      "grad_norm": 3.46756911277771,
      "learning_rate": 9.066709558084363e-06,
      "loss": 1.0913,
      "step": 17460
    },
    {
      "epoch": 2.099008711324722,
      "grad_norm": 3.19765567779541,
      "learning_rate": 9.054646346857533e-06,
      "loss": 1.0993,
      "step": 17470
    },
    {
      "epoch": 2.100210273355362,
      "grad_norm": 3.2841603755950928,
      "learning_rate": 9.042583135630704e-06,
      "loss": 1.0308,
      "step": 17480
    },
    {
      "epoch": 2.101411835386002,
      "grad_norm": 3.9193315505981445,
      "learning_rate": 9.030519924403877e-06,
      "loss": 1.0651,
      "step": 17490
    },
    {
      "epoch": 2.1026133974166417,
      "grad_norm": 3.0183489322662354,
      "learning_rate": 9.018456713177049e-06,
      "loss": 1.1157,
      "step": 17500
    },
    {
      "epoch": 2.1038149594472815,
      "grad_norm": 3.5603485107421875,
      "learning_rate": 9.006393501950218e-06,
      "loss": 1.1053,
      "step": 17510
    },
    {
      "epoch": 2.1050165214779213,
      "grad_norm": 3.2440860271453857,
      "learning_rate": 8.99433029072339e-06,
      "loss": 1.0756,
      "step": 17520
    },
    {
      "epoch": 2.106218083508561,
      "grad_norm": 3.2830002307891846,
      "learning_rate": 8.982267079496563e-06,
      "loss": 1.1013,
      "step": 17530
    },
    {
      "epoch": 2.107419645539201,
      "grad_norm": 3.4122021198272705,
      "learning_rate": 8.970203868269734e-06,
      "loss": 1.0898,
      "step": 17540
    },
    {
      "epoch": 2.1086212075698407,
      "grad_norm": 3.2520699501037598,
      "learning_rate": 8.958140657042906e-06,
      "loss": 1.1206,
      "step": 17550
    },
    {
      "epoch": 2.1098227696004805,
      "grad_norm": 3.189897060394287,
      "learning_rate": 8.946077445816076e-06,
      "loss": 1.0113,
      "step": 17560
    },
    {
      "epoch": 2.1110243316311204,
      "grad_norm": 3.900036334991455,
      "learning_rate": 8.934014234589247e-06,
      "loss": 1.1161,
      "step": 17570
    },
    {
      "epoch": 2.11222589366176,
      "grad_norm": 3.7290706634521484,
      "learning_rate": 8.92195102336242e-06,
      "loss": 1.0441,
      "step": 17580
    },
    {
      "epoch": 2.1134274556924,
      "grad_norm": 3.8059866428375244,
      "learning_rate": 8.909887812135592e-06,
      "loss": 1.0503,
      "step": 17590
    },
    {
      "epoch": 2.11462901772304,
      "grad_norm": 3.019613265991211,
      "learning_rate": 8.897824600908761e-06,
      "loss": 1.0636,
      "step": 17600
    },
    {
      "epoch": 2.1158305797536796,
      "grad_norm": 3.389974355697632,
      "learning_rate": 8.885761389681933e-06,
      "loss": 1.0879,
      "step": 17610
    },
    {
      "epoch": 2.1170321417843194,
      "grad_norm": 3.3784890174865723,
      "learning_rate": 8.873698178455106e-06,
      "loss": 1.031,
      "step": 17620
    },
    {
      "epoch": 2.1182337038149592,
      "grad_norm": 3.853055715560913,
      "learning_rate": 8.861634967228277e-06,
      "loss": 1.047,
      "step": 17630
    },
    {
      "epoch": 2.1194352658455995,
      "grad_norm": 3.4383857250213623,
      "learning_rate": 8.849571756001447e-06,
      "loss": 1.0977,
      "step": 17640
    },
    {
      "epoch": 2.1206368278762393,
      "grad_norm": 3.296607732772827,
      "learning_rate": 8.837508544774619e-06,
      "loss": 1.0533,
      "step": 17650
    },
    {
      "epoch": 2.121838389906879,
      "grad_norm": 3.1827847957611084,
      "learning_rate": 8.82544533354779e-06,
      "loss": 1.0259,
      "step": 17660
    },
    {
      "epoch": 2.123039951937519,
      "grad_norm": 3.095262289047241,
      "learning_rate": 8.813382122320963e-06,
      "loss": 1.0945,
      "step": 17670
    },
    {
      "epoch": 2.1242415139681587,
      "grad_norm": 2.975504159927368,
      "learning_rate": 8.801318911094133e-06,
      "loss": 1.0998,
      "step": 17680
    },
    {
      "epoch": 2.1254430759987986,
      "grad_norm": 3.9610536098480225,
      "learning_rate": 8.789255699867304e-06,
      "loss": 1.0722,
      "step": 17690
    },
    {
      "epoch": 2.1266446380294384,
      "grad_norm": 2.8836469650268555,
      "learning_rate": 8.777192488640476e-06,
      "loss": 1.05,
      "step": 17700
    },
    {
      "epoch": 2.127846200060078,
      "grad_norm": 3.3393449783325195,
      "learning_rate": 8.765129277413649e-06,
      "loss": 1.1023,
      "step": 17710
    },
    {
      "epoch": 2.129047762090718,
      "grad_norm": 2.8174803256988525,
      "learning_rate": 8.753066066186819e-06,
      "loss": 1.0518,
      "step": 17720
    },
    {
      "epoch": 2.130249324121358,
      "grad_norm": 3.4656760692596436,
      "learning_rate": 8.74100285495999e-06,
      "loss": 1.051,
      "step": 17730
    },
    {
      "epoch": 2.1314508861519976,
      "grad_norm": 3.513989210128784,
      "learning_rate": 8.728939643733162e-06,
      "loss": 1.0371,
      "step": 17740
    },
    {
      "epoch": 2.1326524481826374,
      "grad_norm": 3.1408793926239014,
      "learning_rate": 8.716876432506333e-06,
      "loss": 1.121,
      "step": 17750
    },
    {
      "epoch": 2.1338540102132773,
      "grad_norm": 3.3748035430908203,
      "learning_rate": 8.704813221279506e-06,
      "loss": 1.0866,
      "step": 17760
    },
    {
      "epoch": 2.135055572243917,
      "grad_norm": 3.2301464080810547,
      "learning_rate": 8.692750010052676e-06,
      "loss": 1.1059,
      "step": 17770
    },
    {
      "epoch": 2.136257134274557,
      "grad_norm": 3.1893599033355713,
      "learning_rate": 8.680686798825847e-06,
      "loss": 1.1137,
      "step": 17780
    },
    {
      "epoch": 2.1374586963051967,
      "grad_norm": 4.224078178405762,
      "learning_rate": 8.668623587599019e-06,
      "loss": 1.108,
      "step": 17790
    },
    {
      "epoch": 2.1386602583358365,
      "grad_norm": 3.1863579750061035,
      "learning_rate": 8.656560376372192e-06,
      "loss": 1.1628,
      "step": 17800
    },
    {
      "epoch": 2.1398618203664763,
      "grad_norm": 3.4635956287384033,
      "learning_rate": 8.644497165145362e-06,
      "loss": 1.0571,
      "step": 17810
    },
    {
      "epoch": 2.141063382397116,
      "grad_norm": 2.971128225326538,
      "learning_rate": 8.632433953918533e-06,
      "loss": 1.0357,
      "step": 17820
    },
    {
      "epoch": 2.142264944427756,
      "grad_norm": 3.9155614376068115,
      "learning_rate": 8.620370742691705e-06,
      "loss": 1.0854,
      "step": 17830
    },
    {
      "epoch": 2.1434665064583958,
      "grad_norm": 3.1494665145874023,
      "learning_rate": 8.608307531464878e-06,
      "loss": 1.0892,
      "step": 17840
    },
    {
      "epoch": 2.1446680684890356,
      "grad_norm": 3.568605422973633,
      "learning_rate": 8.596244320238047e-06,
      "loss": 1.0309,
      "step": 17850
    },
    {
      "epoch": 2.1458696305196754,
      "grad_norm": 3.3918793201446533,
      "learning_rate": 8.584181109011219e-06,
      "loss": 1.0898,
      "step": 17860
    },
    {
      "epoch": 2.1470711925503156,
      "grad_norm": 3.271942377090454,
      "learning_rate": 8.57211789778439e-06,
      "loss": 1.0292,
      "step": 17870
    },
    {
      "epoch": 2.1482727545809555,
      "grad_norm": 3.6660778522491455,
      "learning_rate": 8.560054686557562e-06,
      "loss": 1.038,
      "step": 17880
    },
    {
      "epoch": 2.1494743166115953,
      "grad_norm": 3.3002490997314453,
      "learning_rate": 8.547991475330733e-06,
      "loss": 1.0944,
      "step": 17890
    },
    {
      "epoch": 2.150675878642235,
      "grad_norm": 3.2023375034332275,
      "learning_rate": 8.535928264103905e-06,
      "loss": 1.0902,
      "step": 17900
    },
    {
      "epoch": 2.151877440672875,
      "grad_norm": 3.3999602794647217,
      "learning_rate": 8.523865052877076e-06,
      "loss": 1.0321,
      "step": 17910
    },
    {
      "epoch": 2.1530790027035147,
      "grad_norm": 3.856832981109619,
      "learning_rate": 8.511801841650247e-06,
      "loss": 1.0243,
      "step": 17920
    },
    {
      "epoch": 2.1542805647341545,
      "grad_norm": 3.3974409103393555,
      "learning_rate": 8.499738630423419e-06,
      "loss": 0.9984,
      "step": 17930
    },
    {
      "epoch": 2.1554821267647943,
      "grad_norm": 3.3613076210021973,
      "learning_rate": 8.48767541919659e-06,
      "loss": 1.0445,
      "step": 17940
    },
    {
      "epoch": 2.156683688795434,
      "grad_norm": 3.518082857131958,
      "learning_rate": 8.475612207969762e-06,
      "loss": 1.0779,
      "step": 17950
    },
    {
      "epoch": 2.157885250826074,
      "grad_norm": 3.1055331230163574,
      "learning_rate": 8.463548996742933e-06,
      "loss": 1.0798,
      "step": 17960
    },
    {
      "epoch": 2.1590868128567138,
      "grad_norm": 3.8116610050201416,
      "learning_rate": 8.451485785516105e-06,
      "loss": 1.0958,
      "step": 17970
    },
    {
      "epoch": 2.1602883748873536,
      "grad_norm": 3.312572956085205,
      "learning_rate": 8.439422574289276e-06,
      "loss": 1.0834,
      "step": 17980
    },
    {
      "epoch": 2.1614899369179934,
      "grad_norm": 3.431084394454956,
      "learning_rate": 8.427359363062448e-06,
      "loss": 1.0495,
      "step": 17990
    },
    {
      "epoch": 2.162691498948633,
      "grad_norm": 3.7201907634735107,
      "learning_rate": 8.415296151835619e-06,
      "loss": 1.0332,
      "step": 18000
    },
    {
      "epoch": 2.163893060979273,
      "grad_norm": 3.0408241748809814,
      "learning_rate": 8.40323294060879e-06,
      "loss": 1.0712,
      "step": 18010
    },
    {
      "epoch": 2.165094623009913,
      "grad_norm": 3.5062828063964844,
      "learning_rate": 8.391169729381962e-06,
      "loss": 1.0204,
      "step": 18020
    },
    {
      "epoch": 2.1662961850405527,
      "grad_norm": 3.5982375144958496,
      "learning_rate": 8.379106518155133e-06,
      "loss": 1.0458,
      "step": 18030
    },
    {
      "epoch": 2.1674977470711925,
      "grad_norm": 3.224978446960449,
      "learning_rate": 8.367043306928305e-06,
      "loss": 1.0588,
      "step": 18040
    },
    {
      "epoch": 2.1686993091018323,
      "grad_norm": 3.3914523124694824,
      "learning_rate": 8.354980095701476e-06,
      "loss": 1.0625,
      "step": 18050
    },
    {
      "epoch": 2.169900871132472,
      "grad_norm": 3.78334903717041,
      "learning_rate": 8.342916884474646e-06,
      "loss": 1.0584,
      "step": 18060
    },
    {
      "epoch": 2.171102433163112,
      "grad_norm": 3.411731481552124,
      "learning_rate": 8.330853673247819e-06,
      "loss": 1.043,
      "step": 18070
    },
    {
      "epoch": 2.1723039951937517,
      "grad_norm": 3.2909066677093506,
      "learning_rate": 8.31879046202099e-06,
      "loss": 1.1304,
      "step": 18080
    },
    {
      "epoch": 2.1735055572243915,
      "grad_norm": 3.6419107913970947,
      "learning_rate": 8.306727250794162e-06,
      "loss": 1.0715,
      "step": 18090
    },
    {
      "epoch": 2.1747071192550314,
      "grad_norm": 3.127105712890625,
      "learning_rate": 8.294664039567332e-06,
      "loss": 1.0482,
      "step": 18100
    },
    {
      "epoch": 2.175908681285671,
      "grad_norm": 3.125659942626953,
      "learning_rate": 8.282600828340505e-06,
      "loss": 1.0531,
      "step": 18110
    },
    {
      "epoch": 2.177110243316311,
      "grad_norm": 2.6757261753082275,
      "learning_rate": 8.270537617113676e-06,
      "loss": 1.0341,
      "step": 18120
    },
    {
      "epoch": 2.1783118053469512,
      "grad_norm": 3.754626750946045,
      "learning_rate": 8.258474405886848e-06,
      "loss": 1.0777,
      "step": 18130
    },
    {
      "epoch": 2.179513367377591,
      "grad_norm": 3.4557294845581055,
      "learning_rate": 8.246411194660017e-06,
      "loss": 1.0759,
      "step": 18140
    },
    {
      "epoch": 2.180714929408231,
      "grad_norm": 3.707885503768921,
      "learning_rate": 8.234347983433189e-06,
      "loss": 1.0442,
      "step": 18150
    },
    {
      "epoch": 2.1819164914388707,
      "grad_norm": 3.5487284660339355,
      "learning_rate": 8.222284772206362e-06,
      "loss": 1.087,
      "step": 18160
    },
    {
      "epoch": 2.1831180534695105,
      "grad_norm": 3.6464052200317383,
      "learning_rate": 8.210221560979534e-06,
      "loss": 1.0561,
      "step": 18170
    },
    {
      "epoch": 2.1843196155001503,
      "grad_norm": 3.408841133117676,
      "learning_rate": 8.198158349752705e-06,
      "loss": 1.1005,
      "step": 18180
    },
    {
      "epoch": 2.18552117753079,
      "grad_norm": 3.2747066020965576,
      "learning_rate": 8.186095138525875e-06,
      "loss": 1.1036,
      "step": 18190
    },
    {
      "epoch": 2.18672273956143,
      "grad_norm": 2.857353925704956,
      "learning_rate": 8.174031927299048e-06,
      "loss": 1.1274,
      "step": 18200
    },
    {
      "epoch": 2.1879243015920697,
      "grad_norm": 2.985328435897827,
      "learning_rate": 8.16196871607222e-06,
      "loss": 1.0664,
      "step": 18210
    },
    {
      "epoch": 2.1891258636227096,
      "grad_norm": 3.6363818645477295,
      "learning_rate": 8.14990550484539e-06,
      "loss": 1.1257,
      "step": 18220
    },
    {
      "epoch": 2.1903274256533494,
      "grad_norm": 3.5886988639831543,
      "learning_rate": 8.13784229361856e-06,
      "loss": 0.9979,
      "step": 18230
    },
    {
      "epoch": 2.191528987683989,
      "grad_norm": 4.108978748321533,
      "learning_rate": 8.125779082391732e-06,
      "loss": 1.0604,
      "step": 18240
    },
    {
      "epoch": 2.192730549714629,
      "grad_norm": 3.4086461067199707,
      "learning_rate": 8.113715871164905e-06,
      "loss": 1.067,
      "step": 18250
    },
    {
      "epoch": 2.193932111745269,
      "grad_norm": 3.206688165664673,
      "learning_rate": 8.101652659938076e-06,
      "loss": 1.0322,
      "step": 18260
    },
    {
      "epoch": 2.1951336737759086,
      "grad_norm": 3.594257116317749,
      "learning_rate": 8.089589448711246e-06,
      "loss": 1.119,
      "step": 18270
    },
    {
      "epoch": 2.1963352358065484,
      "grad_norm": 3.512244939804077,
      "learning_rate": 8.077526237484418e-06,
      "loss": 1.063,
      "step": 18280
    },
    {
      "epoch": 2.1975367978371882,
      "grad_norm": 3.3588979244232178,
      "learning_rate": 8.06546302625759e-06,
      "loss": 1.1079,
      "step": 18290
    },
    {
      "epoch": 2.198738359867828,
      "grad_norm": 3.084254264831543,
      "learning_rate": 8.053399815030762e-06,
      "loss": 0.9926,
      "step": 18300
    },
    {
      "epoch": 2.199939921898468,
      "grad_norm": 3.3062403202056885,
      "learning_rate": 8.041336603803932e-06,
      "loss": 1.1181,
      "step": 18310
    },
    {
      "epoch": 2.2011414839291077,
      "grad_norm": 3.183109760284424,
      "learning_rate": 8.029273392577103e-06,
      "loss": 1.0813,
      "step": 18320
    },
    {
      "epoch": 2.2023430459597475,
      "grad_norm": 2.798311471939087,
      "learning_rate": 8.017210181350277e-06,
      "loss": 1.1034,
      "step": 18330
    },
    {
      "epoch": 2.2035446079903873,
      "grad_norm": 3.2084879875183105,
      "learning_rate": 8.005146970123448e-06,
      "loss": 1.0268,
      "step": 18340
    },
    {
      "epoch": 2.204746170021027,
      "grad_norm": 3.416079044342041,
      "learning_rate": 7.993083758896618e-06,
      "loss": 1.0177,
      "step": 18350
    },
    {
      "epoch": 2.2059477320516674,
      "grad_norm": 3.013739585876465,
      "learning_rate": 7.98102054766979e-06,
      "loss": 1.0639,
      "step": 18360
    },
    {
      "epoch": 2.207149294082307,
      "grad_norm": 3.061415433883667,
      "learning_rate": 7.96895733644296e-06,
      "loss": 0.9951,
      "step": 18370
    },
    {
      "epoch": 2.208350856112947,
      "grad_norm": 3.3824353218078613,
      "learning_rate": 7.956894125216134e-06,
      "loss": 1.0633,
      "step": 18380
    },
    {
      "epoch": 2.209552418143587,
      "grad_norm": 3.295597553253174,
      "learning_rate": 7.944830913989305e-06,
      "loss": 1.0783,
      "step": 18390
    },
    {
      "epoch": 2.2107539801742266,
      "grad_norm": 3.350322961807251,
      "learning_rate": 7.932767702762475e-06,
      "loss": 1.1121,
      "step": 18400
    },
    {
      "epoch": 2.2119555422048665,
      "grad_norm": 2.9464242458343506,
      "learning_rate": 7.920704491535646e-06,
      "loss": 1.0839,
      "step": 18410
    },
    {
      "epoch": 2.2131571042355063,
      "grad_norm": 2.9069511890411377,
      "learning_rate": 7.90864128030882e-06,
      "loss": 1.0422,
      "step": 18420
    },
    {
      "epoch": 2.214358666266146,
      "grad_norm": 3.2642855644226074,
      "learning_rate": 7.896578069081991e-06,
      "loss": 1.0389,
      "step": 18430
    },
    {
      "epoch": 2.215560228296786,
      "grad_norm": 3.6868937015533447,
      "learning_rate": 7.88451485785516e-06,
      "loss": 1.0442,
      "step": 18440
    },
    {
      "epoch": 2.2167617903274257,
      "grad_norm": 3.733715057373047,
      "learning_rate": 7.872451646628332e-06,
      "loss": 1.0891,
      "step": 18450
    },
    {
      "epoch": 2.2179633523580655,
      "grad_norm": 4.4596662521362305,
      "learning_rate": 7.860388435401504e-06,
      "loss": 1.0156,
      "step": 18460
    },
    {
      "epoch": 2.2191649143887053,
      "grad_norm": 3.758080005645752,
      "learning_rate": 7.848325224174677e-06,
      "loss": 1.073,
      "step": 18470
    },
    {
      "epoch": 2.220366476419345,
      "grad_norm": 2.888594388961792,
      "learning_rate": 7.836262012947846e-06,
      "loss": 1.0171,
      "step": 18480
    },
    {
      "epoch": 2.221568038449985,
      "grad_norm": 3.2533586025238037,
      "learning_rate": 7.824198801721018e-06,
      "loss": 1.0838,
      "step": 18490
    },
    {
      "epoch": 2.2227696004806248,
      "grad_norm": 4.338935852050781,
      "learning_rate": 7.81213559049419e-06,
      "loss": 1.1083,
      "step": 18500
    },
    {
      "epoch": 2.2239711625112646,
      "grad_norm": 3.7055070400238037,
      "learning_rate": 7.800072379267363e-06,
      "loss": 1.0551,
      "step": 18510
    },
    {
      "epoch": 2.2251727245419044,
      "grad_norm": 3.369852304458618,
      "learning_rate": 7.788009168040532e-06,
      "loss": 1.0407,
      "step": 18520
    },
    {
      "epoch": 2.226374286572544,
      "grad_norm": 3.8009321689605713,
      "learning_rate": 7.775945956813704e-06,
      "loss": 1.065,
      "step": 18530
    },
    {
      "epoch": 2.227575848603184,
      "grad_norm": 3.4198665618896484,
      "learning_rate": 7.763882745586875e-06,
      "loss": 1.0902,
      "step": 18540
    },
    {
      "epoch": 2.228777410633824,
      "grad_norm": 2.880744218826294,
      "learning_rate": 7.751819534360047e-06,
      "loss": 1.0705,
      "step": 18550
    },
    {
      "epoch": 2.2299789726644637,
      "grad_norm": 3.769608497619629,
      "learning_rate": 7.739756323133218e-06,
      "loss": 1.056,
      "step": 18560
    },
    {
      "epoch": 2.2311805346951035,
      "grad_norm": 3.473703145980835,
      "learning_rate": 7.72769311190639e-06,
      "loss": 1.0173,
      "step": 18570
    },
    {
      "epoch": 2.2323820967257433,
      "grad_norm": 3.3248002529144287,
      "learning_rate": 7.715629900679561e-06,
      "loss": 1.0049,
      "step": 18580
    },
    {
      "epoch": 2.233583658756383,
      "grad_norm": 3.244640588760376,
      "learning_rate": 7.703566689452732e-06,
      "loss": 1.008,
      "step": 18590
    },
    {
      "epoch": 2.234785220787023,
      "grad_norm": 3.227550506591797,
      "learning_rate": 7.691503478225905e-06,
      "loss": 1.0229,
      "step": 18600
    },
    {
      "epoch": 2.235986782817663,
      "grad_norm": 3.4358408451080322,
      "learning_rate": 7.679440266999075e-06,
      "loss": 1.0553,
      "step": 18610
    },
    {
      "epoch": 2.237188344848303,
      "grad_norm": 3.156620979309082,
      "learning_rate": 7.667377055772247e-06,
      "loss": 1.036,
      "step": 18620
    },
    {
      "epoch": 2.238389906878943,
      "grad_norm": 3.5797667503356934,
      "learning_rate": 7.655313844545418e-06,
      "loss": 1.0094,
      "step": 18630
    },
    {
      "epoch": 2.2395914689095826,
      "grad_norm": 3.1477785110473633,
      "learning_rate": 7.64325063331859e-06,
      "loss": 1.0784,
      "step": 18640
    },
    {
      "epoch": 2.2407930309402224,
      "grad_norm": 3.617093086242676,
      "learning_rate": 7.631187422091761e-06,
      "loss": 1.0151,
      "step": 18650
    },
    {
      "epoch": 2.2419945929708622,
      "grad_norm": 3.546316146850586,
      "learning_rate": 7.619124210864932e-06,
      "loss": 1.0453,
      "step": 18660
    },
    {
      "epoch": 2.243196155001502,
      "grad_norm": 3.156531572341919,
      "learning_rate": 7.607060999638104e-06,
      "loss": 1.0441,
      "step": 18670
    },
    {
      "epoch": 2.244397717032142,
      "grad_norm": 3.748532295227051,
      "learning_rate": 7.594997788411276e-06,
      "loss": 1.1049,
      "step": 18680
    },
    {
      "epoch": 2.2455992790627817,
      "grad_norm": 2.988100528717041,
      "learning_rate": 7.582934577184446e-06,
      "loss": 1.0622,
      "step": 18690
    },
    {
      "epoch": 2.2468008410934215,
      "grad_norm": 3.3084163665771484,
      "learning_rate": 7.570871365957618e-06,
      "loss": 1.0502,
      "step": 18700
    },
    {
      "epoch": 2.2480024031240613,
      "grad_norm": 3.5238876342773438,
      "learning_rate": 7.55880815473079e-06,
      "loss": 1.0649,
      "step": 18710
    },
    {
      "epoch": 2.249203965154701,
      "grad_norm": 3.5617072582244873,
      "learning_rate": 7.546744943503962e-06,
      "loss": 1.027,
      "step": 18720
    },
    {
      "epoch": 2.250405527185341,
      "grad_norm": 3.214219093322754,
      "learning_rate": 7.534681732277132e-06,
      "loss": 1.0874,
      "step": 18730
    },
    {
      "epoch": 2.2516070892159807,
      "grad_norm": 3.2783148288726807,
      "learning_rate": 7.522618521050303e-06,
      "loss": 1.0363,
      "step": 18740
    },
    {
      "epoch": 2.2528086512466206,
      "grad_norm": 2.958599805831909,
      "learning_rate": 7.510555309823475e-06,
      "loss": 1.0047,
      "step": 18750
    },
    {
      "epoch": 2.2540102132772604,
      "grad_norm": 2.9130783081054688,
      "learning_rate": 7.498492098596646e-06,
      "loss": 1.0792,
      "step": 18760
    },
    {
      "epoch": 2.2552117753079,
      "grad_norm": 3.2998485565185547,
      "learning_rate": 7.486428887369818e-06,
      "loss": 1.0742,
      "step": 18770
    },
    {
      "epoch": 2.25641333733854,
      "grad_norm": 3.197072744369507,
      "learning_rate": 7.474365676142989e-06,
      "loss": 1.0741,
      "step": 18780
    },
    {
      "epoch": 2.25761489936918,
      "grad_norm": 3.1417171955108643,
      "learning_rate": 7.462302464916161e-06,
      "loss": 1.0669,
      "step": 18790
    },
    {
      "epoch": 2.2588164613998196,
      "grad_norm": 2.9334733486175537,
      "learning_rate": 7.450239253689333e-06,
      "loss": 1.0338,
      "step": 18800
    },
    {
      "epoch": 2.2600180234304594,
      "grad_norm": 3.798534393310547,
      "learning_rate": 7.438176042462504e-06,
      "loss": 1.0605,
      "step": 18810
    },
    {
      "epoch": 2.2612195854610992,
      "grad_norm": 3.160073757171631,
      "learning_rate": 7.4261128312356755e-06,
      "loss": 1.1326,
      "step": 18820
    },
    {
      "epoch": 2.2624211474917395,
      "grad_norm": 3.6111412048339844,
      "learning_rate": 7.414049620008846e-06,
      "loss": 1.0467,
      "step": 18830
    },
    {
      "epoch": 2.2636227095223793,
      "grad_norm": 3.0815961360931396,
      "learning_rate": 7.401986408782018e-06,
      "loss": 1.0601,
      "step": 18840
    },
    {
      "epoch": 2.264824271553019,
      "grad_norm": 3.298401355743408,
      "learning_rate": 7.389923197555189e-06,
      "loss": 1.033,
      "step": 18850
    },
    {
      "epoch": 2.266025833583659,
      "grad_norm": 2.9003212451934814,
      "learning_rate": 7.377859986328361e-06,
      "loss": 1.005,
      "step": 18860
    },
    {
      "epoch": 2.2672273956142988,
      "grad_norm": 3.101860523223877,
      "learning_rate": 7.365796775101532e-06,
      "loss": 1.027,
      "step": 18870
    },
    {
      "epoch": 2.2684289576449386,
      "grad_norm": 3.473275661468506,
      "learning_rate": 7.353733563874704e-06,
      "loss": 1.0829,
      "step": 18880
    },
    {
      "epoch": 2.2696305196755784,
      "grad_norm": 3.2822422981262207,
      "learning_rate": 7.341670352647875e-06,
      "loss": 1.0316,
      "step": 18890
    },
    {
      "epoch": 2.270832081706218,
      "grad_norm": 4.195730686187744,
      "learning_rate": 7.329607141421047e-06,
      "loss": 1.0091,
      "step": 18900
    },
    {
      "epoch": 2.272033643736858,
      "grad_norm": 3.75240421295166,
      "learning_rate": 7.317543930194218e-06,
      "loss": 1.0376,
      "step": 18910
    },
    {
      "epoch": 2.273235205767498,
      "grad_norm": 3.5950303077697754,
      "learning_rate": 7.305480718967389e-06,
      "loss": 1.0481,
      "step": 18920
    },
    {
      "epoch": 2.2744367677981376,
      "grad_norm": 3.409518241882324,
      "learning_rate": 7.2934175077405605e-06,
      "loss": 1.0172,
      "step": 18930
    },
    {
      "epoch": 2.2756383298287775,
      "grad_norm": 2.9290659427642822,
      "learning_rate": 7.281354296513732e-06,
      "loss": 1.0057,
      "step": 18940
    },
    {
      "epoch": 2.2768398918594173,
      "grad_norm": 3.126253604888916,
      "learning_rate": 7.269291085286903e-06,
      "loss": 1.0516,
      "step": 18950
    },
    {
      "epoch": 2.278041453890057,
      "grad_norm": 3.4856204986572266,
      "learning_rate": 7.257227874060075e-06,
      "loss": 1.025,
      "step": 18960
    },
    {
      "epoch": 2.279243015920697,
      "grad_norm": 3.5589241981506348,
      "learning_rate": 7.245164662833246e-06,
      "loss": 1.044,
      "step": 18970
    },
    {
      "epoch": 2.2804445779513367,
      "grad_norm": 3.6150434017181396,
      "learning_rate": 7.233101451606418e-06,
      "loss": 1.0189,
      "step": 18980
    },
    {
      "epoch": 2.2816461399819765,
      "grad_norm": 3.0120368003845215,
      "learning_rate": 7.221038240379589e-06,
      "loss": 1.0579,
      "step": 18990
    },
    {
      "epoch": 2.2828477020126163,
      "grad_norm": 3.0239250659942627,
      "learning_rate": 7.2089750291527606e-06,
      "loss": 1.0565,
      "step": 19000
    },
    {
      "epoch": 2.284049264043256,
      "grad_norm": 3.385568618774414,
      "learning_rate": 7.196911817925933e-06,
      "loss": 1.0614,
      "step": 19010
    },
    {
      "epoch": 2.285250826073896,
      "grad_norm": 3.136894464492798,
      "learning_rate": 7.1848486066991035e-06,
      "loss": 1.0663,
      "step": 19020
    },
    {
      "epoch": 2.2864523881045358,
      "grad_norm": 3.265800952911377,
      "learning_rate": 7.172785395472275e-06,
      "loss": 1.0521,
      "step": 19030
    },
    {
      "epoch": 2.2876539501351756,
      "grad_norm": 3.5732269287109375,
      "learning_rate": 7.160722184245446e-06,
      "loss": 1.0962,
      "step": 19040
    },
    {
      "epoch": 2.2888555121658154,
      "grad_norm": 2.827986240386963,
      "learning_rate": 7.148658973018618e-06,
      "loss": 1.0042,
      "step": 19050
    },
    {
      "epoch": 2.290057074196455,
      "grad_norm": 3.340379238128662,
      "learning_rate": 7.136595761791789e-06,
      "loss": 1.0547,
      "step": 19060
    },
    {
      "epoch": 2.291258636227095,
      "grad_norm": 3.6550495624542236,
      "learning_rate": 7.124532550564961e-06,
      "loss": 1.0984,
      "step": 19070
    },
    {
      "epoch": 2.292460198257735,
      "grad_norm": 3.106766700744629,
      "learning_rate": 7.112469339338132e-06,
      "loss": 1.0948,
      "step": 19080
    },
    {
      "epoch": 2.2936617602883747,
      "grad_norm": 3.374267339706421,
      "learning_rate": 7.1004061281113035e-06,
      "loss": 1.0483,
      "step": 19090
    },
    {
      "epoch": 2.294863322319015,
      "grad_norm": 3.3116512298583984,
      "learning_rate": 7.088342916884475e-06,
      "loss": 1.0476,
      "step": 19100
    },
    {
      "epoch": 2.2960648843496547,
      "grad_norm": 3.2141430377960205,
      "learning_rate": 7.076279705657646e-06,
      "loss": 1.0367,
      "step": 19110
    },
    {
      "epoch": 2.2972664463802945,
      "grad_norm": 3.145233154296875,
      "learning_rate": 7.064216494430817e-06,
      "loss": 1.082,
      "step": 19120
    },
    {
      "epoch": 2.2984680084109343,
      "grad_norm": 3.518254041671753,
      "learning_rate": 7.052153283203989e-06,
      "loss": 0.9842,
      "step": 19130
    },
    {
      "epoch": 2.299669570441574,
      "grad_norm": 3.109553575515747,
      "learning_rate": 7.04009007197716e-06,
      "loss": 1.11,
      "step": 19140
    },
    {
      "epoch": 2.300871132472214,
      "grad_norm": 3.5146796703338623,
      "learning_rate": 7.028026860750332e-06,
      "loss": 1.0305,
      "step": 19150
    },
    {
      "epoch": 2.302072694502854,
      "grad_norm": 3.7448549270629883,
      "learning_rate": 7.015963649523503e-06,
      "loss": 1.0597,
      "step": 19160
    },
    {
      "epoch": 2.3032742565334936,
      "grad_norm": 3.4761297702789307,
      "learning_rate": 7.003900438296675e-06,
      "loss": 1.0748,
      "step": 19170
    },
    {
      "epoch": 2.3044758185641334,
      "grad_norm": 3.149245023727417,
      "learning_rate": 6.991837227069846e-06,
      "loss": 1.0895,
      "step": 19180
    },
    {
      "epoch": 2.3056773805947732,
      "grad_norm": 2.9805901050567627,
      "learning_rate": 6.979774015843018e-06,
      "loss": 1.0671,
      "step": 19190
    },
    {
      "epoch": 2.306878942625413,
      "grad_norm": 3.3869266510009766,
      "learning_rate": 6.9677108046161885e-06,
      "loss": 1.0762,
      "step": 19200
    },
    {
      "epoch": 2.308080504656053,
      "grad_norm": 3.4851479530334473,
      "learning_rate": 6.95564759338936e-06,
      "loss": 1.0373,
      "step": 19210
    },
    {
      "epoch": 2.3092820666866927,
      "grad_norm": 3.253406524658203,
      "learning_rate": 6.943584382162532e-06,
      "loss": 1.0555,
      "step": 19220
    },
    {
      "epoch": 2.3104836287173325,
      "grad_norm": 2.774123191833496,
      "learning_rate": 6.931521170935703e-06,
      "loss": 1.0804,
      "step": 19230
    },
    {
      "epoch": 2.3116851907479723,
      "grad_norm": 3.003464460372925,
      "learning_rate": 6.919457959708875e-06,
      "loss": 1.1148,
      "step": 19240
    },
    {
      "epoch": 2.312886752778612,
      "grad_norm": 2.9861536026000977,
      "learning_rate": 6.907394748482046e-06,
      "loss": 1.0229,
      "step": 19250
    },
    {
      "epoch": 2.314088314809252,
      "grad_norm": 3.8242106437683105,
      "learning_rate": 6.895331537255218e-06,
      "loss": 1.0245,
      "step": 19260
    },
    {
      "epoch": 2.3152898768398917,
      "grad_norm": 3.435950756072998,
      "learning_rate": 6.883268326028389e-06,
      "loss": 1.0512,
      "step": 19270
    },
    {
      "epoch": 2.3164914388705315,
      "grad_norm": 3.094299077987671,
      "learning_rate": 6.872411435924243e-06,
      "loss": 0.9963,
      "step": 19280
    },
    {
      "epoch": 2.3176930009011714,
      "grad_norm": 2.904984474182129,
      "learning_rate": 6.8603482246974145e-06,
      "loss": 1.069,
      "step": 19290
    },
    {
      "epoch": 2.318894562931811,
      "grad_norm": 3.3168840408325195,
      "learning_rate": 6.848285013470586e-06,
      "loss": 1.1614,
      "step": 19300
    },
    {
      "epoch": 2.3200961249624514,
      "grad_norm": 3.1745288372039795,
      "learning_rate": 6.836221802243757e-06,
      "loss": 1.1114,
      "step": 19310
    },
    {
      "epoch": 2.3212976869930912,
      "grad_norm": 3.0180251598358154,
      "learning_rate": 6.824158591016929e-06,
      "loss": 1.1001,
      "step": 19320
    },
    {
      "epoch": 2.322499249023731,
      "grad_norm": 3.081151247024536,
      "learning_rate": 6.8120953797901e-06,
      "loss": 1.0947,
      "step": 19330
    },
    {
      "epoch": 2.323700811054371,
      "grad_norm": 3.1435940265655518,
      "learning_rate": 6.800032168563271e-06,
      "loss": 1.0609,
      "step": 19340
    },
    {
      "epoch": 2.3249023730850107,
      "grad_norm": 3.182036876678467,
      "learning_rate": 6.787968957336443e-06,
      "loss": 0.992,
      "step": 19350
    },
    {
      "epoch": 2.3261039351156505,
      "grad_norm": 3.182713031768799,
      "learning_rate": 6.775905746109615e-06,
      "loss": 0.9775,
      "step": 19360
    },
    {
      "epoch": 2.3273054971462903,
      "grad_norm": 5.242903709411621,
      "learning_rate": 6.763842534882786e-06,
      "loss": 1.1055,
      "step": 19370
    },
    {
      "epoch": 2.32850705917693,
      "grad_norm": 4.038592338562012,
      "learning_rate": 6.7517793236559575e-06,
      "loss": 1.0636,
      "step": 19380
    },
    {
      "epoch": 2.32970862120757,
      "grad_norm": 3.056756019592285,
      "learning_rate": 6.739716112429129e-06,
      "loss": 1.0166,
      "step": 19390
    },
    {
      "epoch": 2.3309101832382098,
      "grad_norm": 3.331803798675537,
      "learning_rate": 6.7276529012023e-06,
      "loss": 1.0904,
      "step": 19400
    },
    {
      "epoch": 2.3321117452688496,
      "grad_norm": 3.3243274688720703,
      "learning_rate": 6.715589689975472e-06,
      "loss": 1.0322,
      "step": 19410
    },
    {
      "epoch": 2.3333133072994894,
      "grad_norm": 3.2131879329681396,
      "learning_rate": 6.703526478748643e-06,
      "loss": 1.0134,
      "step": 19420
    },
    {
      "epoch": 2.334514869330129,
      "grad_norm": 3.318000555038452,
      "learning_rate": 6.691463267521815e-06,
      "loss": 1.1017,
      "step": 19430
    },
    {
      "epoch": 2.335716431360769,
      "grad_norm": 3.12263822555542,
      "learning_rate": 6.679400056294986e-06,
      "loss": 1.0896,
      "step": 19440
    },
    {
      "epoch": 2.336917993391409,
      "grad_norm": 3.533888578414917,
      "learning_rate": 6.667336845068157e-06,
      "loss": 1.0565,
      "step": 19450
    },
    {
      "epoch": 2.3381195554220486,
      "grad_norm": 3.1664891242980957,
      "learning_rate": 6.655273633841329e-06,
      "loss": 1.03,
      "step": 19460
    },
    {
      "epoch": 2.3393211174526884,
      "grad_norm": 3.1805405616760254,
      "learning_rate": 6.6432104226145e-06,
      "loss": 1.076,
      "step": 19470
    },
    {
      "epoch": 2.3405226794833283,
      "grad_norm": 2.919686794281006,
      "learning_rate": 6.631147211387672e-06,
      "loss": 1.012,
      "step": 19480
    },
    {
      "epoch": 2.341724241513968,
      "grad_norm": 3.281059741973877,
      "learning_rate": 6.6190840001608425e-06,
      "loss": 1.0313,
      "step": 19490
    },
    {
      "epoch": 2.342925803544608,
      "grad_norm": 3.7634167671203613,
      "learning_rate": 6.607020788934015e-06,
      "loss": 1.0713,
      "step": 19500
    },
    {
      "epoch": 2.3441273655752477,
      "grad_norm": 3.0069146156311035,
      "learning_rate": 6.594957577707185e-06,
      "loss": 1.0119,
      "step": 19510
    },
    {
      "epoch": 2.3453289276058875,
      "grad_norm": 3.430227518081665,
      "learning_rate": 6.582894366480358e-06,
      "loss": 1.0745,
      "step": 19520
    },
    {
      "epoch": 2.3465304896365273,
      "grad_norm": 3.439189910888672,
      "learning_rate": 6.570831155253528e-06,
      "loss": 1.1274,
      "step": 19530
    },
    {
      "epoch": 2.347732051667167,
      "grad_norm": 2.7835991382598877,
      "learning_rate": 6.5587679440267e-06,
      "loss": 1.0844,
      "step": 19540
    },
    {
      "epoch": 2.348933613697807,
      "grad_norm": 3.3131046295166016,
      "learning_rate": 6.546704732799871e-06,
      "loss": 1.0787,
      "step": 19550
    },
    {
      "epoch": 2.3501351757284468,
      "grad_norm": 2.990651845932007,
      "learning_rate": 6.534641521573043e-06,
      "loss": 1.0502,
      "step": 19560
    },
    {
      "epoch": 2.3513367377590866,
      "grad_norm": 3.293227434158325,
      "learning_rate": 6.522578310346215e-06,
      "loss": 1.0701,
      "step": 19570
    },
    {
      "epoch": 2.352538299789727,
      "grad_norm": 3.4259021282196045,
      "learning_rate": 6.5105150991193855e-06,
      "loss": 1.0547,
      "step": 19580
    },
    {
      "epoch": 2.3537398618203667,
      "grad_norm": 3.433353900909424,
      "learning_rate": 6.498451887892558e-06,
      "loss": 1.0531,
      "step": 19590
    },
    {
      "epoch": 2.3549414238510065,
      "grad_norm": 3.0698153972625732,
      "learning_rate": 6.486388676665728e-06,
      "loss": 1.0744,
      "step": 19600
    },
    {
      "epoch": 2.3561429858816463,
      "grad_norm": 3.1355812549591064,
      "learning_rate": 6.474325465438901e-06,
      "loss": 1.0583,
      "step": 19610
    },
    {
      "epoch": 2.357344547912286,
      "grad_norm": 3.4702062606811523,
      "learning_rate": 6.462262254212071e-06,
      "loss": 1.0751,
      "step": 19620
    },
    {
      "epoch": 2.358546109942926,
      "grad_norm": 3.3245534896850586,
      "learning_rate": 6.450199042985243e-06,
      "loss": 1.0739,
      "step": 19630
    },
    {
      "epoch": 2.3597476719735657,
      "grad_norm": 3.2089290618896484,
      "learning_rate": 6.438135831758414e-06,
      "loss": 1.0927,
      "step": 19640
    },
    {
      "epoch": 2.3609492340042055,
      "grad_norm": 3.2250845432281494,
      "learning_rate": 6.4260726205315856e-06,
      "loss": 1.0495,
      "step": 19650
    },
    {
      "epoch": 2.3621507960348453,
      "grad_norm": 4.052084922790527,
      "learning_rate": 6.414009409304757e-06,
      "loss": 1.0295,
      "step": 19660
    },
    {
      "epoch": 2.363352358065485,
      "grad_norm": 3.157991409301758,
      "learning_rate": 6.4019461980779284e-06,
      "loss": 1.0408,
      "step": 19670
    },
    {
      "epoch": 2.364553920096125,
      "grad_norm": 3.2964518070220947,
      "learning_rate": 6.3898829868511e-06,
      "loss": 1.0565,
      "step": 19680
    },
    {
      "epoch": 2.365755482126765,
      "grad_norm": 4.089906692504883,
      "learning_rate": 6.377819775624271e-06,
      "loss": 1.1134,
      "step": 19690
    },
    {
      "epoch": 2.3669570441574046,
      "grad_norm": 3.2510180473327637,
      "learning_rate": 6.365756564397443e-06,
      "loss": 1.1126,
      "step": 19700
    },
    {
      "epoch": 2.3681586061880444,
      "grad_norm": 3.0563597679138184,
      "learning_rate": 6.353693353170614e-06,
      "loss": 1.083,
      "step": 19710
    },
    {
      "epoch": 2.3693601682186842,
      "grad_norm": 3.2195444107055664,
      "learning_rate": 6.341630141943786e-06,
      "loss": 1.0909,
      "step": 19720
    },
    {
      "epoch": 2.370561730249324,
      "grad_norm": 3.0485153198242188,
      "learning_rate": 6.329566930716957e-06,
      "loss": 1.0706,
      "step": 19730
    },
    {
      "epoch": 2.371763292279964,
      "grad_norm": 3.173940420150757,
      "learning_rate": 6.317503719490128e-06,
      "loss": 1.0579,
      "step": 19740
    },
    {
      "epoch": 2.3729648543106037,
      "grad_norm": 3.245408535003662,
      "learning_rate": 6.3054405082633e-06,
      "loss": 1.085,
      "step": 19750
    },
    {
      "epoch": 2.3741664163412435,
      "grad_norm": 3.233628749847412,
      "learning_rate": 6.2933772970364706e-06,
      "loss": 1.0566,
      "step": 19760
    },
    {
      "epoch": 2.3753679783718833,
      "grad_norm": 3.0689079761505127,
      "learning_rate": 6.281314085809643e-06,
      "loss": 1.0155,
      "step": 19770
    },
    {
      "epoch": 2.376569540402523,
      "grad_norm": 3.0824978351593018,
      "learning_rate": 6.269250874582814e-06,
      "loss": 1.0476,
      "step": 19780
    },
    {
      "epoch": 2.377771102433163,
      "grad_norm": 2.897385835647583,
      "learning_rate": 6.257187663355986e-06,
      "loss": 1.023,
      "step": 19790
    },
    {
      "epoch": 2.378972664463803,
      "grad_norm": 3.143731117248535,
      "learning_rate": 6.245124452129157e-06,
      "loss": 1.0991,
      "step": 19800
    },
    {
      "epoch": 2.380174226494443,
      "grad_norm": 3.175361394882202,
      "learning_rate": 6.233061240902329e-06,
      "loss": 1.0424,
      "step": 19810
    },
    {
      "epoch": 2.381375788525083,
      "grad_norm": 3.3017866611480713,
      "learning_rate": 6.2209980296755e-06,
      "loss": 1.0396,
      "step": 19820
    },
    {
      "epoch": 2.3825773505557226,
      "grad_norm": 3.2444357872009277,
      "learning_rate": 6.208934818448671e-06,
      "loss": 1.0292,
      "step": 19830
    },
    {
      "epoch": 2.3837789125863624,
      "grad_norm": 3.733022451400757,
      "learning_rate": 6.196871607221843e-06,
      "loss": 1.0271,
      "step": 19840
    },
    {
      "epoch": 2.3849804746170022,
      "grad_norm": 3.9111313819885254,
      "learning_rate": 6.1848083959950135e-06,
      "loss": 1.09,
      "step": 19850
    },
    {
      "epoch": 2.386182036647642,
      "grad_norm": 2.9121687412261963,
      "learning_rate": 6.172745184768186e-06,
      "loss": 1.0726,
      "step": 19860
    },
    {
      "epoch": 2.387383598678282,
      "grad_norm": 2.948657512664795,
      "learning_rate": 6.160681973541356e-06,
      "loss": 1.0793,
      "step": 19870
    },
    {
      "epoch": 2.3885851607089217,
      "grad_norm": 3.270394802093506,
      "learning_rate": 6.148618762314529e-06,
      "loss": 0.9972,
      "step": 19880
    },
    {
      "epoch": 2.3897867227395615,
      "grad_norm": 3.3789377212524414,
      "learning_rate": 6.136555551087699e-06,
      "loss": 1.04,
      "step": 19890
    },
    {
      "epoch": 2.3909882847702013,
      "grad_norm": 3.3437857627868652,
      "learning_rate": 6.124492339860872e-06,
      "loss": 1.0893,
      "step": 19900
    },
    {
      "epoch": 2.392189846800841,
      "grad_norm": 3.182783365249634,
      "learning_rate": 6.112429128634042e-06,
      "loss": 1.1057,
      "step": 19910
    },
    {
      "epoch": 2.393391408831481,
      "grad_norm": 3.412684202194214,
      "learning_rate": 6.1003659174072145e-06,
      "loss": 1.0436,
      "step": 19920
    },
    {
      "epoch": 2.3945929708621208,
      "grad_norm": 3.1360905170440674,
      "learning_rate": 6.088302706180385e-06,
      "loss": 1.0472,
      "step": 19930
    },
    {
      "epoch": 2.3957945328927606,
      "grad_norm": 3.0339643955230713,
      "learning_rate": 6.0762394949535565e-06,
      "loss": 1.0871,
      "step": 19940
    },
    {
      "epoch": 2.3969960949234004,
      "grad_norm": 2.845120668411255,
      "learning_rate": 6.064176283726728e-06,
      "loss": 1.0857,
      "step": 19950
    },
    {
      "epoch": 2.39819765695404,
      "grad_norm": 3.199873924255371,
      "learning_rate": 6.052113072499899e-06,
      "loss": 1.0777,
      "step": 19960
    },
    {
      "epoch": 2.39939921898468,
      "grad_norm": 3.361966371536255,
      "learning_rate": 6.040049861273072e-06,
      "loss": 1.0785,
      "step": 19970
    },
    {
      "epoch": 2.40060078101532,
      "grad_norm": 3.2870116233825684,
      "learning_rate": 6.027986650046242e-06,
      "loss": 1.0257,
      "step": 19980
    },
    {
      "epoch": 2.4018023430459596,
      "grad_norm": 3.1340534687042236,
      "learning_rate": 6.0159234388194146e-06,
      "loss": 1.0265,
      "step": 19990
    },
    {
      "epoch": 2.4030039050765994,
      "grad_norm": 2.9605495929718018,
      "learning_rate": 6.003860227592585e-06,
      "loss": 1.0185,
      "step": 20000
    },
    {
      "epoch": 2.4042054671072393,
      "grad_norm": 3.3521111011505127,
      "learning_rate": 5.9917970163657574e-06,
      "loss": 1.0445,
      "step": 20010
    },
    {
      "epoch": 2.405407029137879,
      "grad_norm": 3.1710188388824463,
      "learning_rate": 5.979733805138928e-06,
      "loss": 1.0346,
      "step": 20020
    },
    {
      "epoch": 2.406608591168519,
      "grad_norm": 3.4255967140197754,
      "learning_rate": 5.9676705939120995e-06,
      "loss": 1.0924,
      "step": 20030
    },
    {
      "epoch": 2.4078101531991587,
      "grad_norm": 3.5952413082122803,
      "learning_rate": 5.955607382685271e-06,
      "loss": 1.0609,
      "step": 20040
    },
    {
      "epoch": 2.4090117152297985,
      "grad_norm": 2.9315948486328125,
      "learning_rate": 5.943544171458442e-06,
      "loss": 1.0216,
      "step": 20050
    },
    {
      "epoch": 2.4102132772604383,
      "grad_norm": 3.240309476852417,
      "learning_rate": 5.931480960231614e-06,
      "loss": 1.0565,
      "step": 20060
    },
    {
      "epoch": 2.4114148392910786,
      "grad_norm": 2.949923276901245,
      "learning_rate": 5.919417749004785e-06,
      "loss": 1.0251,
      "step": 20070
    },
    {
      "epoch": 2.4126164013217184,
      "grad_norm": 2.919907569885254,
      "learning_rate": 5.907354537777957e-06,
      "loss": 1.1231,
      "step": 20080
    },
    {
      "epoch": 2.413817963352358,
      "grad_norm": 3.16696834564209,
      "learning_rate": 5.895291326551128e-06,
      "loss": 1.0111,
      "step": 20090
    },
    {
      "epoch": 2.415019525382998,
      "grad_norm": 3.3851654529571533,
      "learning_rate": 5.8832281153242996e-06,
      "loss": 1.1084,
      "step": 20100
    },
    {
      "epoch": 2.416221087413638,
      "grad_norm": 3.2462565898895264,
      "learning_rate": 5.871164904097471e-06,
      "loss": 0.9962,
      "step": 20110
    },
    {
      "epoch": 2.4174226494442776,
      "grad_norm": 3.2078888416290283,
      "learning_rate": 5.859101692870642e-06,
      "loss": 1.0403,
      "step": 20120
    },
    {
      "epoch": 2.4186242114749175,
      "grad_norm": 3.3598625659942627,
      "learning_rate": 5.847038481643814e-06,
      "loss": 0.9937,
      "step": 20130
    },
    {
      "epoch": 2.4198257735055573,
      "grad_norm": 3.3235251903533936,
      "learning_rate": 5.8349752704169845e-06,
      "loss": 1.0429,
      "step": 20140
    },
    {
      "epoch": 2.421027335536197,
      "grad_norm": 3.6782538890838623,
      "learning_rate": 5.822912059190157e-06,
      "loss": 1.0651,
      "step": 20150
    },
    {
      "epoch": 2.422228897566837,
      "grad_norm": 3.034550428390503,
      "learning_rate": 5.810848847963327e-06,
      "loss": 1.0655,
      "step": 20160
    },
    {
      "epoch": 2.4234304595974767,
      "grad_norm": 3.3920016288757324,
      "learning_rate": 5.7987856367365e-06,
      "loss": 1.0888,
      "step": 20170
    },
    {
      "epoch": 2.4246320216281165,
      "grad_norm": 2.895669460296631,
      "learning_rate": 5.786722425509671e-06,
      "loss": 1.0161,
      "step": 20180
    },
    {
      "epoch": 2.4258335836587563,
      "grad_norm": 2.9986021518707275,
      "learning_rate": 5.7746592142828425e-06,
      "loss": 1.086,
      "step": 20190
    },
    {
      "epoch": 2.427035145689396,
      "grad_norm": 4.082935333251953,
      "learning_rate": 5.762596003056014e-06,
      "loss": 1.0474,
      "step": 20200
    },
    {
      "epoch": 2.428236707720036,
      "grad_norm": 3.546374797821045,
      "learning_rate": 5.750532791829185e-06,
      "loss": 1.0646,
      "step": 20210
    },
    {
      "epoch": 2.429438269750676,
      "grad_norm": 3.114098072052002,
      "learning_rate": 5.738469580602357e-06,
      "loss": 1.0409,
      "step": 20220
    },
    {
      "epoch": 2.4306398317813156,
      "grad_norm": 2.9358134269714355,
      "learning_rate": 5.7264063693755275e-06,
      "loss": 1.0478,
      "step": 20230
    },
    {
      "epoch": 2.4318413938119554,
      "grad_norm": 3.0304503440856934,
      "learning_rate": 5.7143431581487e-06,
      "loss": 1.0542,
      "step": 20240
    },
    {
      "epoch": 2.4330429558425952,
      "grad_norm": 3.1021366119384766,
      "learning_rate": 5.70227994692187e-06,
      "loss": 1.0474,
      "step": 20250
    },
    {
      "epoch": 2.434244517873235,
      "grad_norm": 3.036498546600342,
      "learning_rate": 5.690216735695043e-06,
      "loss": 1.0179,
      "step": 20260
    },
    {
      "epoch": 2.435446079903875,
      "grad_norm": 3.2793383598327637,
      "learning_rate": 5.678153524468213e-06,
      "loss": 1.0192,
      "step": 20270
    },
    {
      "epoch": 2.436647641934515,
      "grad_norm": 3.236509084701538,
      "learning_rate": 5.6660903132413855e-06,
      "loss": 1.0389,
      "step": 20280
    },
    {
      "epoch": 2.437849203965155,
      "grad_norm": 2.9465508460998535,
      "learning_rate": 5.654027102014556e-06,
      "loss": 1.0793,
      "step": 20290
    },
    {
      "epoch": 2.4390507659957947,
      "grad_norm": 3.095473051071167,
      "learning_rate": 5.641963890787728e-06,
      "loss": 1.0408,
      "step": 20300
    },
    {
      "epoch": 2.4402523280264345,
      "grad_norm": 3.1106760501861572,
      "learning_rate": 5.629900679560899e-06,
      "loss": 1.0489,
      "step": 20310
    },
    {
      "epoch": 2.4414538900570744,
      "grad_norm": 3.1778368949890137,
      "learning_rate": 5.61783746833407e-06,
      "loss": 1.0677,
      "step": 20320
    },
    {
      "epoch": 2.442655452087714,
      "grad_norm": 3.630659818649292,
      "learning_rate": 5.605774257107242e-06,
      "loss": 1.1381,
      "step": 20330
    },
    {
      "epoch": 2.443857014118354,
      "grad_norm": 3.0327208042144775,
      "learning_rate": 5.593711045880413e-06,
      "loss": 1.0607,
      "step": 20340
    },
    {
      "epoch": 2.445058576148994,
      "grad_norm": 3.421189069747925,
      "learning_rate": 5.581647834653585e-06,
      "loss": 1.0308,
      "step": 20350
    },
    {
      "epoch": 2.4462601381796336,
      "grad_norm": 3.3606626987457275,
      "learning_rate": 5.569584623426756e-06,
      "loss": 1.0543,
      "step": 20360
    },
    {
      "epoch": 2.4474617002102734,
      "grad_norm": 3.1891849040985107,
      "learning_rate": 5.557521412199928e-06,
      "loss": 1.031,
      "step": 20370
    },
    {
      "epoch": 2.4486632622409132,
      "grad_norm": 3.0221762657165527,
      "learning_rate": 5.545458200973099e-06,
      "loss": 1.0946,
      "step": 20380
    },
    {
      "epoch": 2.449864824271553,
      "grad_norm": 4.328399181365967,
      "learning_rate": 5.533394989746271e-06,
      "loss": 1.057,
      "step": 20390
    },
    {
      "epoch": 2.451066386302193,
      "grad_norm": 3.8155102729797363,
      "learning_rate": 5.521331778519442e-06,
      "loss": 1.0464,
      "step": 20400
    },
    {
      "epoch": 2.4522679483328327,
      "grad_norm": 3.3310458660125732,
      "learning_rate": 5.509268567292613e-06,
      "loss": 1.0643,
      "step": 20410
    },
    {
      "epoch": 2.4534695103634725,
      "grad_norm": 3.6078343391418457,
      "learning_rate": 5.497205356065785e-06,
      "loss": 1.0489,
      "step": 20420
    },
    {
      "epoch": 2.4546710723941123,
      "grad_norm": 2.913503408432007,
      "learning_rate": 5.485142144838956e-06,
      "loss": 1.0368,
      "step": 20430
    },
    {
      "epoch": 2.455872634424752,
      "grad_norm": 3.8168251514434814,
      "learning_rate": 5.473078933612128e-06,
      "loss": 1.0475,
      "step": 20440
    },
    {
      "epoch": 2.457074196455392,
      "grad_norm": 3.038018226623535,
      "learning_rate": 5.461015722385299e-06,
      "loss": 1.0717,
      "step": 20450
    },
    {
      "epoch": 2.4582757584860317,
      "grad_norm": 3.19775652885437,
      "learning_rate": 5.448952511158471e-06,
      "loss": 1.009,
      "step": 20460
    },
    {
      "epoch": 2.4594773205166716,
      "grad_norm": 2.7987799644470215,
      "learning_rate": 5.436889299931642e-06,
      "loss": 1.0464,
      "step": 20470
    },
    {
      "epoch": 2.4606788825473114,
      "grad_norm": 3.4962894916534424,
      "learning_rate": 5.4248260887048135e-06,
      "loss": 1.0418,
      "step": 20480
    },
    {
      "epoch": 2.461880444577951,
      "grad_norm": 2.7567877769470215,
      "learning_rate": 5.412762877477985e-06,
      "loss": 1.0383,
      "step": 20490
    },
    {
      "epoch": 2.463082006608591,
      "grad_norm": 3.3650131225585938,
      "learning_rate": 5.400699666251156e-06,
      "loss": 1.0275,
      "step": 20500
    },
    {
      "epoch": 2.464283568639231,
      "grad_norm": 3.2769615650177,
      "learning_rate": 5.388636455024328e-06,
      "loss": 1.0558,
      "step": 20510
    },
    {
      "epoch": 2.4654851306698706,
      "grad_norm": 3.5480170249938965,
      "learning_rate": 5.376573243797498e-06,
      "loss": 1.031,
      "step": 20520
    },
    {
      "epoch": 2.4666866927005104,
      "grad_norm": 3.2911276817321777,
      "learning_rate": 5.364510032570671e-06,
      "loss": 1.1437,
      "step": 20530
    },
    {
      "epoch": 2.4678882547311503,
      "grad_norm": 2.9208645820617676,
      "learning_rate": 5.352446821343841e-06,
      "loss": 1.1038,
      "step": 20540
    },
    {
      "epoch": 2.4690898167617905,
      "grad_norm": 2.798879384994507,
      "learning_rate": 5.3403836101170136e-06,
      "loss": 1.0398,
      "step": 20550
    },
    {
      "epoch": 2.4702913787924303,
      "grad_norm": 3.1699955463409424,
      "learning_rate": 5.328320398890184e-06,
      "loss": 1.0313,
      "step": 20560
    },
    {
      "epoch": 2.47149294082307,
      "grad_norm": 3.33886456489563,
      "learning_rate": 5.3162571876633564e-06,
      "loss": 1.0129,
      "step": 20570
    },
    {
      "epoch": 2.47269450285371,
      "grad_norm": 3.3541882038116455,
      "learning_rate": 5.304193976436527e-06,
      "loss": 1.0397,
      "step": 20580
    },
    {
      "epoch": 2.4738960648843498,
      "grad_norm": 3.304306745529175,
      "learning_rate": 5.292130765209699e-06,
      "loss": 1.0102,
      "step": 20590
    },
    {
      "epoch": 2.4750976269149896,
      "grad_norm": 3.2581069469451904,
      "learning_rate": 5.280067553982871e-06,
      "loss": 1.055,
      "step": 20600
    },
    {
      "epoch": 2.4762991889456294,
      "grad_norm": 3.09995698928833,
      "learning_rate": 5.268004342756041e-06,
      "loss": 1.0186,
      "step": 20610
    },
    {
      "epoch": 2.477500750976269,
      "grad_norm": 2.924652338027954,
      "learning_rate": 5.255941131529214e-06,
      "loss": 1.0295,
      "step": 20620
    },
    {
      "epoch": 2.478702313006909,
      "grad_norm": 3.1319780349731445,
      "learning_rate": 5.243877920302384e-06,
      "loss": 1.013,
      "step": 20630
    },
    {
      "epoch": 2.479903875037549,
      "grad_norm": 3.75150990486145,
      "learning_rate": 5.2318147090755565e-06,
      "loss": 1.0727,
      "step": 20640
    },
    {
      "epoch": 2.4811054370681886,
      "grad_norm": 3.174083709716797,
      "learning_rate": 5.219751497848727e-06,
      "loss": 1.1431,
      "step": 20650
    },
    {
      "epoch": 2.4823069990988285,
      "grad_norm": 2.77750825881958,
      "learning_rate": 5.207688286621899e-06,
      "loss": 1.0185,
      "step": 20660
    },
    {
      "epoch": 2.4835085611294683,
      "grad_norm": 3.40157151222229,
      "learning_rate": 5.19562507539507e-06,
      "loss": 1.0152,
      "step": 20670
    },
    {
      "epoch": 2.484710123160108,
      "grad_norm": 3.5346341133117676,
      "learning_rate": 5.183561864168242e-06,
      "loss": 1.0414,
      "step": 20680
    },
    {
      "epoch": 2.485911685190748,
      "grad_norm": 3.243942975997925,
      "learning_rate": 5.171498652941413e-06,
      "loss": 1.0295,
      "step": 20690
    },
    {
      "epoch": 2.4871132472213877,
      "grad_norm": 3.5872297286987305,
      "learning_rate": 5.159435441714584e-06,
      "loss": 1.0277,
      "step": 20700
    },
    {
      "epoch": 2.4883148092520275,
      "grad_norm": 2.9651098251342773,
      "learning_rate": 5.147372230487756e-06,
      "loss": 1.0532,
      "step": 20710
    },
    {
      "epoch": 2.4895163712826673,
      "grad_norm": 2.839134693145752,
      "learning_rate": 5.135309019260927e-06,
      "loss": 1.0272,
      "step": 20720
    },
    {
      "epoch": 2.490717933313307,
      "grad_norm": 2.90399432182312,
      "learning_rate": 5.123245808034099e-06,
      "loss": 0.9925,
      "step": 20730
    },
    {
      "epoch": 2.491919495343947,
      "grad_norm": 3.5850062370300293,
      "learning_rate": 5.11118259680727e-06,
      "loss": 1.1197,
      "step": 20740
    },
    {
      "epoch": 2.493121057374587,
      "grad_norm": 3.3646862506866455,
      "learning_rate": 5.0991193855804415e-06,
      "loss": 1.0441,
      "step": 20750
    },
    {
      "epoch": 2.494322619405227,
      "grad_norm": 3.1046104431152344,
      "learning_rate": 5.087056174353613e-06,
      "loss": 1.065,
      "step": 20760
    },
    {
      "epoch": 2.495524181435867,
      "grad_norm": 3.133847713470459,
      "learning_rate": 5.074992963126784e-06,
      "loss": 1.0672,
      "step": 20770
    },
    {
      "epoch": 2.4967257434665067,
      "grad_norm": 3.3568365573883057,
      "learning_rate": 5.062929751899956e-06,
      "loss": 1.077,
      "step": 20780
    },
    {
      "epoch": 2.4979273054971465,
      "grad_norm": 3.0310757160186768,
      "learning_rate": 5.050866540673127e-06,
      "loss": 1.0381,
      "step": 20790
    },
    {
      "epoch": 2.4991288675277863,
      "grad_norm": 3.144681930541992,
      "learning_rate": 5.038803329446299e-06,
      "loss": 1.0551,
      "step": 20800
    },
    {
      "epoch": 2.500330429558426,
      "grad_norm": 3.472043037414551,
      "learning_rate": 5.02674011821947e-06,
      "loss": 1.038,
      "step": 20810
    },
    {
      "epoch": 2.501531991589066,
      "grad_norm": 2.933290481567383,
      "learning_rate": 5.014676906992642e-06,
      "loss": 1.0567,
      "step": 20820
    },
    {
      "epoch": 2.5027335536197057,
      "grad_norm": 3.03393816947937,
      "learning_rate": 5.002613695765813e-06,
      "loss": 1.0378,
      "step": 20830
    },
    {
      "epoch": 2.5039351156503455,
      "grad_norm": 3.4525146484375,
      "learning_rate": 4.9905504845389845e-06,
      "loss": 0.9854,
      "step": 20840
    },
    {
      "epoch": 2.5051366776809854,
      "grad_norm": 2.9462056159973145,
      "learning_rate": 4.978487273312156e-06,
      "loss": 1.0669,
      "step": 20850
    },
    {
      "epoch": 2.506338239711625,
      "grad_norm": 3.62933349609375,
      "learning_rate": 4.966424062085327e-06,
      "loss": 1.1001,
      "step": 20860
    },
    {
      "epoch": 2.507539801742265,
      "grad_norm": 2.8826630115509033,
      "learning_rate": 4.954360850858499e-06,
      "loss": 1.0011,
      "step": 20870
    },
    {
      "epoch": 2.508741363772905,
      "grad_norm": 3.5304441452026367,
      "learning_rate": 4.94229763963167e-06,
      "loss": 1.0657,
      "step": 20880
    },
    {
      "epoch": 2.5099429258035446,
      "grad_norm": 3.1132051944732666,
      "learning_rate": 4.930234428404842e-06,
      "loss": 1.0308,
      "step": 20890
    },
    {
      "epoch": 2.5111444878341844,
      "grad_norm": 3.063511848449707,
      "learning_rate": 4.918171217178012e-06,
      "loss": 1.0764,
      "step": 20900
    },
    {
      "epoch": 2.5123460498648242,
      "grad_norm": 2.921255111694336,
      "learning_rate": 4.906108005951185e-06,
      "loss": 1.0478,
      "step": 20910
    },
    {
      "epoch": 2.513547611895464,
      "grad_norm": 2.978221893310547,
      "learning_rate": 4.894044794724355e-06,
      "loss": 1.0394,
      "step": 20920
    },
    {
      "epoch": 2.514749173926104,
      "grad_norm": 3.424657106399536,
      "learning_rate": 4.8819815834975275e-06,
      "loss": 1.0766,
      "step": 20930
    },
    {
      "epoch": 2.5159507359567437,
      "grad_norm": 3.4936885833740234,
      "learning_rate": 4.869918372270698e-06,
      "loss": 1.0254,
      "step": 20940
    },
    {
      "epoch": 2.5171522979873835,
      "grad_norm": 3.547114610671997,
      "learning_rate": 4.85785516104387e-06,
      "loss": 1.0258,
      "step": 20950
    },
    {
      "epoch": 2.5183538600180233,
      "grad_norm": 3.0889134407043457,
      "learning_rate": 4.845791949817041e-06,
      "loss": 1.0934,
      "step": 20960
    },
    {
      "epoch": 2.519555422048663,
      "grad_norm": 3.27887225151062,
      "learning_rate": 4.833728738590213e-06,
      "loss": 0.9986,
      "step": 20970
    },
    {
      "epoch": 2.520756984079303,
      "grad_norm": 3.1854562759399414,
      "learning_rate": 4.821665527363384e-06,
      "loss": 1.0949,
      "step": 20980
    },
    {
      "epoch": 2.5219585461099427,
      "grad_norm": 3.072017192840576,
      "learning_rate": 4.809602316136556e-06,
      "loss": 1.0709,
      "step": 20990
    },
    {
      "epoch": 2.5231601081405826,
      "grad_norm": 3.3289713859558105,
      "learning_rate": 4.797539104909727e-06,
      "loss": 1.0109,
      "step": 21000
    },
    {
      "epoch": 2.5243616701712224,
      "grad_norm": 2.8385422229766846,
      "learning_rate": 4.785475893682898e-06,
      "loss": 1.0185,
      "step": 21010
    },
    {
      "epoch": 2.525563232201862,
      "grad_norm": 3.1605563163757324,
      "learning_rate": 4.7734126824560704e-06,
      "loss": 1.051,
      "step": 21020
    },
    {
      "epoch": 2.526764794232502,
      "grad_norm": 3.268263816833496,
      "learning_rate": 4.761349471229241e-06,
      "loss": 1.0496,
      "step": 21030
    },
    {
      "epoch": 2.527966356263142,
      "grad_norm": 3.339508295059204,
      "learning_rate": 4.749286260002413e-06,
      "loss": 0.9973,
      "step": 21040
    },
    {
      "epoch": 2.529167918293782,
      "grad_norm": 3.3935482501983643,
      "learning_rate": 4.737223048775584e-06,
      "loss": 1.0124,
      "step": 21050
    },
    {
      "epoch": 2.530369480324422,
      "grad_norm": 3.7891130447387695,
      "learning_rate": 4.725159837548756e-06,
      "loss": 1.0809,
      "step": 21060
    },
    {
      "epoch": 2.5315710423550617,
      "grad_norm": 3.3940651416778564,
      "learning_rate": 4.713096626321927e-06,
      "loss": 1.013,
      "step": 21070
    },
    {
      "epoch": 2.5327726043857015,
      "grad_norm": 2.9344868659973145,
      "learning_rate": 4.701033415095099e-06,
      "loss": 1.1119,
      "step": 21080
    },
    {
      "epoch": 2.5339741664163413,
      "grad_norm": 3.724207639694214,
      "learning_rate": 4.68897020386827e-06,
      "loss": 1.0355,
      "step": 21090
    },
    {
      "epoch": 2.535175728446981,
      "grad_norm": 3.4073565006256104,
      "learning_rate": 4.676906992641441e-06,
      "loss": 1.0307,
      "step": 21100
    },
    {
      "epoch": 2.536377290477621,
      "grad_norm": 2.9414525032043457,
      "learning_rate": 4.6648437814146126e-06,
      "loss": 1.059,
      "step": 21110
    },
    {
      "epoch": 2.5375788525082608,
      "grad_norm": 3.252047061920166,
      "learning_rate": 4.652780570187784e-06,
      "loss": 1.0645,
      "step": 21120
    },
    {
      "epoch": 2.5387804145389006,
      "grad_norm": 3.025963068008423,
      "learning_rate": 4.6407173589609554e-06,
      "loss": 1.0301,
      "step": 21130
    },
    {
      "epoch": 2.5399819765695404,
      "grad_norm": 2.8746683597564697,
      "learning_rate": 4.628654147734127e-06,
      "loss": 1.0537,
      "step": 21140
    },
    {
      "epoch": 2.54118353860018,
      "grad_norm": 3.4088196754455566,
      "learning_rate": 4.616590936507298e-06,
      "loss": 1.0347,
      "step": 21150
    },
    {
      "epoch": 2.54238510063082,
      "grad_norm": 3.514268636703491,
      "learning_rate": 4.60452772528047e-06,
      "loss": 1.0988,
      "step": 21160
    },
    {
      "epoch": 2.54358666266146,
      "grad_norm": 3.1169559955596924,
      "learning_rate": 4.592464514053641e-06,
      "loss": 1.0766,
      "step": 21170
    },
    {
      "epoch": 2.5447882246920996,
      "grad_norm": 3.3224124908447266,
      "learning_rate": 4.580401302826813e-06,
      "loss": 1.1074,
      "step": 21180
    },
    {
      "epoch": 2.5459897867227395,
      "grad_norm": 3.9262359142303467,
      "learning_rate": 4.568338091599983e-06,
      "loss": 1.0711,
      "step": 21190
    },
    {
      "epoch": 2.5471913487533793,
      "grad_norm": 3.1332268714904785,
      "learning_rate": 4.5562748803731555e-06,
      "loss": 1.0654,
      "step": 21200
    },
    {
      "epoch": 2.548392910784019,
      "grad_norm": 3.656097650527954,
      "learning_rate": 4.544211669146326e-06,
      "loss": 1.0353,
      "step": 21210
    },
    {
      "epoch": 2.549594472814659,
      "grad_norm": 3.154569387435913,
      "learning_rate": 4.532148457919498e-06,
      "loss": 1.053,
      "step": 21220
    },
    {
      "epoch": 2.550796034845299,
      "grad_norm": 3.2843544483184814,
      "learning_rate": 4.52008524669267e-06,
      "loss": 1.0416,
      "step": 21230
    },
    {
      "epoch": 2.551997596875939,
      "grad_norm": 2.8848931789398193,
      "learning_rate": 4.508022035465841e-06,
      "loss": 1.0575,
      "step": 21240
    },
    {
      "epoch": 2.553199158906579,
      "grad_norm": 2.7854163646698,
      "learning_rate": 4.495958824239013e-06,
      "loss": 1.041,
      "step": 21250
    },
    {
      "epoch": 2.5544007209372186,
      "grad_norm": 3.208341121673584,
      "learning_rate": 4.483895613012184e-06,
      "loss": 1.0699,
      "step": 21260
    },
    {
      "epoch": 2.5556022829678584,
      "grad_norm": 3.202113151550293,
      "learning_rate": 4.471832401785356e-06,
      "loss": 1.1007,
      "step": 21270
    },
    {
      "epoch": 2.556803844998498,
      "grad_norm": 3.565570116043091,
      "learning_rate": 4.459769190558527e-06,
      "loss": 1.031,
      "step": 21280
    },
    {
      "epoch": 2.558005407029138,
      "grad_norm": 3.069683074951172,
      "learning_rate": 4.4477059793316985e-06,
      "loss": 1.1196,
      "step": 21290
    },
    {
      "epoch": 2.559206969059778,
      "grad_norm": 3.2507119178771973,
      "learning_rate": 4.436849089227553e-06,
      "loss": 1.0911,
      "step": 21300
    },
    {
      "epoch": 2.5604085310904177,
      "grad_norm": 2.9985103607177734,
      "learning_rate": 4.4247858780007236e-06,
      "loss": 1.0469,
      "step": 21310
    },
    {
      "epoch": 2.5616100931210575,
      "grad_norm": 3.020176887512207,
      "learning_rate": 4.412722666773895e-06,
      "loss": 1.0025,
      "step": 21320
    },
    {
      "epoch": 2.5628116551516973,
      "grad_norm": 3.9732303619384766,
      "learning_rate": 4.4006594555470664e-06,
      "loss": 1.053,
      "step": 21330
    },
    {
      "epoch": 2.564013217182337,
      "grad_norm": 3.374621868133545,
      "learning_rate": 4.388596244320238e-06,
      "loss": 1.1042,
      "step": 21340
    },
    {
      "epoch": 2.565214779212977,
      "grad_norm": 3.8245387077331543,
      "learning_rate": 4.376533033093409e-06,
      "loss": 1.0323,
      "step": 21350
    },
    {
      "epoch": 2.5664163412436167,
      "grad_norm": 4.36053991317749,
      "learning_rate": 4.364469821866581e-06,
      "loss": 1.104,
      "step": 21360
    },
    {
      "epoch": 2.5676179032742565,
      "grad_norm": 3.412189245223999,
      "learning_rate": 4.352406610639753e-06,
      "loss": 1.0541,
      "step": 21370
    },
    {
      "epoch": 2.5688194653048964,
      "grad_norm": 3.392745018005371,
      "learning_rate": 4.340343399412924e-06,
      "loss": 1.0976,
      "step": 21380
    },
    {
      "epoch": 2.570021027335536,
      "grad_norm": 3.7149465084075928,
      "learning_rate": 4.328280188186096e-06,
      "loss": 1.0396,
      "step": 21390
    },
    {
      "epoch": 2.571222589366176,
      "grad_norm": 2.9301440715789795,
      "learning_rate": 4.3162169769592665e-06,
      "loss": 1.044,
      "step": 21400
    },
    {
      "epoch": 2.572424151396816,
      "grad_norm": 2.856553077697754,
      "learning_rate": 4.304153765732439e-06,
      "loss": 1.0258,
      "step": 21410
    },
    {
      "epoch": 2.5736257134274556,
      "grad_norm": 3.2313671112060547,
      "learning_rate": 4.292090554505609e-06,
      "loss": 1.0635,
      "step": 21420
    },
    {
      "epoch": 2.5748272754580954,
      "grad_norm": 3.445706605911255,
      "learning_rate": 4.280027343278781e-06,
      "loss": 1.0504,
      "step": 21430
    },
    {
      "epoch": 2.5760288374887352,
      "grad_norm": 3.460986614227295,
      "learning_rate": 4.267964132051952e-06,
      "loss": 1.16,
      "step": 21440
    },
    {
      "epoch": 2.577230399519375,
      "grad_norm": 3.0935308933258057,
      "learning_rate": 4.255900920825124e-06,
      "loss": 1.0921,
      "step": 21450
    },
    {
      "epoch": 2.578431961550015,
      "grad_norm": 3.4033010005950928,
      "learning_rate": 4.243837709598295e-06,
      "loss": 1.0617,
      "step": 21460
    },
    {
      "epoch": 2.5796335235806547,
      "grad_norm": 3.260867118835449,
      "learning_rate": 4.231774498371467e-06,
      "loss": 1.0219,
      "step": 21470
    },
    {
      "epoch": 2.5808350856112945,
      "grad_norm": 3.145472526550293,
      "learning_rate": 4.219711287144638e-06,
      "loss": 1.1124,
      "step": 21480
    },
    {
      "epoch": 2.5820366476419343,
      "grad_norm": 4.455163478851318,
      "learning_rate": 4.2076480759178095e-06,
      "loss": 1.1019,
      "step": 21490
    },
    {
      "epoch": 2.583238209672574,
      "grad_norm": 3.0962061882019043,
      "learning_rate": 4.195584864690981e-06,
      "loss": 1.0698,
      "step": 21500
    },
    {
      "epoch": 2.584439771703214,
      "grad_norm": 2.902256727218628,
      "learning_rate": 4.183521653464152e-06,
      "loss": 1.0172,
      "step": 21510
    },
    {
      "epoch": 2.5856413337338537,
      "grad_norm": 4.392050743103027,
      "learning_rate": 4.171458442237323e-06,
      "loss": 1.1003,
      "step": 21520
    },
    {
      "epoch": 2.586842895764494,
      "grad_norm": 3.5447375774383545,
      "learning_rate": 4.159395231010495e-06,
      "loss": 1.0163,
      "step": 21530
    },
    {
      "epoch": 2.588044457795134,
      "grad_norm": 3.2902770042419434,
      "learning_rate": 4.147332019783666e-06,
      "loss": 1.0566,
      "step": 21540
    },
    {
      "epoch": 2.5892460198257736,
      "grad_norm": 2.9632601737976074,
      "learning_rate": 4.135268808556838e-06,
      "loss": 1.0597,
      "step": 21550
    },
    {
      "epoch": 2.5904475818564134,
      "grad_norm": 3.536588668823242,
      "learning_rate": 4.124411918452692e-06,
      "loss": 1.0405,
      "step": 21560
    },
    {
      "epoch": 2.5916491438870533,
      "grad_norm": 3.7034058570861816,
      "learning_rate": 4.112348707225864e-06,
      "loss": 0.9997,
      "step": 21570
    },
    {
      "epoch": 2.592850705917693,
      "grad_norm": 3.1686060428619385,
      "learning_rate": 4.100285495999035e-06,
      "loss": 1.0085,
      "step": 21580
    },
    {
      "epoch": 2.594052267948333,
      "grad_norm": 2.8910233974456787,
      "learning_rate": 4.088222284772207e-06,
      "loss": 1.0243,
      "step": 21590
    },
    {
      "epoch": 2.5952538299789727,
      "grad_norm": 2.866335153579712,
      "learning_rate": 4.0761590735453775e-06,
      "loss": 1.0143,
      "step": 21600
    },
    {
      "epoch": 2.5964553920096125,
      "grad_norm": 3.4556260108947754,
      "learning_rate": 4.06409586231855e-06,
      "loss": 1.0612,
      "step": 21610
    },
    {
      "epoch": 2.5976569540402523,
      "grad_norm": 3.137665033340454,
      "learning_rate": 4.05203265109172e-06,
      "loss": 1.0169,
      "step": 21620
    },
    {
      "epoch": 2.598858516070892,
      "grad_norm": 3.274944543838501,
      "learning_rate": 4.039969439864893e-06,
      "loss": 1.0753,
      "step": 21630
    },
    {
      "epoch": 2.600060078101532,
      "grad_norm": 3.6611409187316895,
      "learning_rate": 4.027906228638063e-06,
      "loss": 1.0204,
      "step": 21640
    },
    {
      "epoch": 2.6012616401321718,
      "grad_norm": 3.5898516178131104,
      "learning_rate": 4.015843017411235e-06,
      "loss": 1.0359,
      "step": 21650
    },
    {
      "epoch": 2.6024632021628116,
      "grad_norm": 3.2148473262786865,
      "learning_rate": 4.003779806184406e-06,
      "loss": 1.0133,
      "step": 21660
    },
    {
      "epoch": 2.6036647641934514,
      "grad_norm": 3.500810146331787,
      "learning_rate": 3.991716594957578e-06,
      "loss": 1.0287,
      "step": 21670
    },
    {
      "epoch": 2.604866326224091,
      "grad_norm": 3.102142095565796,
      "learning_rate": 3.979653383730749e-06,
      "loss": 1.0855,
      "step": 21680
    },
    {
      "epoch": 2.606067888254731,
      "grad_norm": 3.870985746383667,
      "learning_rate": 3.9675901725039205e-06,
      "loss": 1.0668,
      "step": 21690
    },
    {
      "epoch": 2.607269450285371,
      "grad_norm": 2.9323742389678955,
      "learning_rate": 3.955526961277092e-06,
      "loss": 1.04,
      "step": 21700
    },
    {
      "epoch": 2.608471012316011,
      "grad_norm": 4.356187343597412,
      "learning_rate": 3.943463750050263e-06,
      "loss": 1.0626,
      "step": 21710
    },
    {
      "epoch": 2.609672574346651,
      "grad_norm": 2.7842538356781006,
      "learning_rate": 3.931400538823436e-06,
      "loss": 1.0491,
      "step": 21720
    },
    {
      "epoch": 2.6108741363772907,
      "grad_norm": 3.1830427646636963,
      "learning_rate": 3.919337327596606e-06,
      "loss": 1.0577,
      "step": 21730
    },
    {
      "epoch": 2.6120756984079305,
      "grad_norm": 3.409611225128174,
      "learning_rate": 3.907274116369778e-06,
      "loss": 1.0835,
      "step": 21740
    },
    {
      "epoch": 2.6132772604385703,
      "grad_norm": 2.953234910964966,
      "learning_rate": 3.895210905142949e-06,
      "loss": 1.0307,
      "step": 21750
    },
    {
      "epoch": 2.61447882246921,
      "grad_norm": 3.4124228954315186,
      "learning_rate": 3.883147693916121e-06,
      "loss": 1.0529,
      "step": 21760
    },
    {
      "epoch": 2.61568038449985,
      "grad_norm": 3.120276689529419,
      "learning_rate": 3.871084482689292e-06,
      "loss": 1.0433,
      "step": 21770
    },
    {
      "epoch": 2.6168819465304898,
      "grad_norm": 3.8643739223480225,
      "learning_rate": 3.8590212714624635e-06,
      "loss": 1.1002,
      "step": 21780
    },
    {
      "epoch": 2.6180835085611296,
      "grad_norm": 3.147454023361206,
      "learning_rate": 3.846958060235635e-06,
      "loss": 1.054,
      "step": 21790
    },
    {
      "epoch": 2.6192850705917694,
      "grad_norm": 3.262403726577759,
      "learning_rate": 3.834894849008806e-06,
      "loss": 1.0588,
      "step": 21800
    },
    {
      "epoch": 2.620486632622409,
      "grad_norm": 3.203051805496216,
      "learning_rate": 3.822831637781978e-06,
      "loss": 1.0167,
      "step": 21810
    },
    {
      "epoch": 2.621688194653049,
      "grad_norm": 3.2115585803985596,
      "learning_rate": 3.8107684265551492e-06,
      "loss": 1.059,
      "step": 21820
    },
    {
      "epoch": 2.622889756683689,
      "grad_norm": 2.8185131549835205,
      "learning_rate": 3.7987052153283203e-06,
      "loss": 1.0696,
      "step": 21830
    },
    {
      "epoch": 2.6240913187143287,
      "grad_norm": 3.3256099224090576,
      "learning_rate": 3.786642004101492e-06,
      "loss": 1.0672,
      "step": 21840
    },
    {
      "epoch": 2.6252928807449685,
      "grad_norm": 3.7890985012054443,
      "learning_rate": 3.774578792874663e-06,
      "loss": 1.0665,
      "step": 21850
    },
    {
      "epoch": 2.6264944427756083,
      "grad_norm": 3.0935909748077393,
      "learning_rate": 3.762515581647835e-06,
      "loss": 1.0545,
      "step": 21860
    },
    {
      "epoch": 2.627696004806248,
      "grad_norm": 3.3438382148742676,
      "learning_rate": 3.750452370421006e-06,
      "loss": 0.9834,
      "step": 21870
    },
    {
      "epoch": 2.628897566836888,
      "grad_norm": 3.613868236541748,
      "learning_rate": 3.738389159194178e-06,
      "loss": 1.0399,
      "step": 21880
    },
    {
      "epoch": 2.6300991288675277,
      "grad_norm": 3.454376459121704,
      "learning_rate": 3.726325947967349e-06,
      "loss": 1.0262,
      "step": 21890
    },
    {
      "epoch": 2.6313006908981675,
      "grad_norm": 3.053102970123291,
      "learning_rate": 3.7142627367405203e-06,
      "loss": 1.0507,
      "step": 21900
    },
    {
      "epoch": 2.6325022529288074,
      "grad_norm": 2.998255491256714,
      "learning_rate": 3.7021995255136918e-06,
      "loss": 1.0526,
      "step": 21910
    },
    {
      "epoch": 2.633703814959447,
      "grad_norm": 3.2780580520629883,
      "learning_rate": 3.6901363142868632e-06,
      "loss": 1.0673,
      "step": 21920
    },
    {
      "epoch": 2.634905376990087,
      "grad_norm": 3.492875814437866,
      "learning_rate": 3.6780731030600347e-06,
      "loss": 0.9914,
      "step": 21930
    },
    {
      "epoch": 2.636106939020727,
      "grad_norm": 3.276700973510742,
      "learning_rate": 3.666009891833206e-06,
      "loss": 1.0399,
      "step": 21940
    },
    {
      "epoch": 2.6373085010513666,
      "grad_norm": 3.115828514099121,
      "learning_rate": 3.6539466806063775e-06,
      "loss": 1.0528,
      "step": 21950
    },
    {
      "epoch": 2.6385100630820064,
      "grad_norm": 2.9947476387023926,
      "learning_rate": 3.641883469379549e-06,
      "loss": 1.0316,
      "step": 21960
    },
    {
      "epoch": 2.6397116251126462,
      "grad_norm": 3.244992971420288,
      "learning_rate": 3.6298202581527204e-06,
      "loss": 1.0577,
      "step": 21970
    },
    {
      "epoch": 2.640913187143286,
      "grad_norm": 3.3156635761260986,
      "learning_rate": 3.6177570469258914e-06,
      "loss": 0.983,
      "step": 21980
    },
    {
      "epoch": 2.642114749173926,
      "grad_norm": 3.445603132247925,
      "learning_rate": 3.605693835699063e-06,
      "loss": 1.0157,
      "step": 21990
    },
    {
      "epoch": 2.6433163112045657,
      "grad_norm": 3.6216158866882324,
      "learning_rate": 3.5936306244722343e-06,
      "loss": 0.9832,
      "step": 22000
    },
    {
      "epoch": 2.644517873235206,
      "grad_norm": 3.463397979736328,
      "learning_rate": 3.5815674132454058e-06,
      "loss": 1.0268,
      "step": 22010
    },
    {
      "epoch": 2.6457194352658457,
      "grad_norm": 3.0857656002044678,
      "learning_rate": 3.569504202018577e-06,
      "loss": 1.0731,
      "step": 22020
    },
    {
      "epoch": 2.6469209972964856,
      "grad_norm": 3.776663303375244,
      "learning_rate": 3.557440990791749e-06,
      "loss": 1.1115,
      "step": 22030
    },
    {
      "epoch": 2.6481225593271254,
      "grad_norm": 3.188939332962036,
      "learning_rate": 3.5453777795649205e-06,
      "loss": 0.9998,
      "step": 22040
    },
    {
      "epoch": 2.649324121357765,
      "grad_norm": 3.6898226737976074,
      "learning_rate": 3.533314568338092e-06,
      "loss": 1.0818,
      "step": 22050
    },
    {
      "epoch": 2.650525683388405,
      "grad_norm": 3.014392614364624,
      "learning_rate": 3.5212513571112634e-06,
      "loss": 1.0361,
      "step": 22060
    },
    {
      "epoch": 2.651727245419045,
      "grad_norm": 3.863582134246826,
      "learning_rate": 3.509188145884435e-06,
      "loss": 1.0715,
      "step": 22070
    },
    {
      "epoch": 2.6529288074496846,
      "grad_norm": 3.121750831604004,
      "learning_rate": 3.497124934657606e-06,
      "loss": 1.084,
      "step": 22080
    },
    {
      "epoch": 2.6541303694803244,
      "grad_norm": 3.3223228454589844,
      "learning_rate": 3.4850617234307773e-06,
      "loss": 1.0015,
      "step": 22090
    },
    {
      "epoch": 2.6553319315109642,
      "grad_norm": 3.0140228271484375,
      "learning_rate": 3.4729985122039487e-06,
      "loss": 1.0345,
      "step": 22100
    },
    {
      "epoch": 2.656533493541604,
      "grad_norm": 3.035672187805176,
      "learning_rate": 3.46093530097712e-06,
      "loss": 1.0643,
      "step": 22110
    },
    {
      "epoch": 2.657735055572244,
      "grad_norm": 3.2983930110931396,
      "learning_rate": 3.4488720897502916e-06,
      "loss": 1.0539,
      "step": 22120
    },
    {
      "epoch": 2.6589366176028837,
      "grad_norm": 3.469717264175415,
      "learning_rate": 3.436808878523463e-06,
      "loss": 1.0504,
      "step": 22130
    },
    {
      "epoch": 2.6601381796335235,
      "grad_norm": 3.4932053089141846,
      "learning_rate": 3.4247456672966345e-06,
      "loss": 1.0563,
      "step": 22140
    },
    {
      "epoch": 2.6613397416641633,
      "grad_norm": 3.1224446296691895,
      "learning_rate": 3.412682456069806e-06,
      "loss": 1.0525,
      "step": 22150
    },
    {
      "epoch": 2.662541303694803,
      "grad_norm": 2.996992588043213,
      "learning_rate": 3.4006192448429774e-06,
      "loss": 1.0538,
      "step": 22160
    },
    {
      "epoch": 2.663742865725443,
      "grad_norm": 3.319162607192993,
      "learning_rate": 3.388556033616149e-06,
      "loss": 1.048,
      "step": 22170
    },
    {
      "epoch": 2.6649444277560828,
      "grad_norm": 3.562723159790039,
      "learning_rate": 3.37649282238932e-06,
      "loss": 1.0588,
      "step": 22180
    },
    {
      "epoch": 2.666145989786723,
      "grad_norm": 3.355653762817383,
      "learning_rate": 3.3644296111624913e-06,
      "loss": 1.0311,
      "step": 22190
    },
    {
      "epoch": 2.667347551817363,
      "grad_norm": 3.3636274337768555,
      "learning_rate": 3.3523663999356627e-06,
      "loss": 1.0845,
      "step": 22200
    },
    {
      "epoch": 2.6685491138480026,
      "grad_norm": 4.429311275482178,
      "learning_rate": 3.340303188708834e-06,
      "loss": 1.0089,
      "step": 22210
    },
    {
      "epoch": 2.6697506758786425,
      "grad_norm": 3.3576443195343018,
      "learning_rate": 3.3282399774820056e-06,
      "loss": 1.0116,
      "step": 22220
    },
    {
      "epoch": 2.6709522379092823,
      "grad_norm": 3.123915910720825,
      "learning_rate": 3.316176766255177e-06,
      "loss": 0.9895,
      "step": 22230
    },
    {
      "epoch": 2.672153799939922,
      "grad_norm": 3.1912639141082764,
      "learning_rate": 3.304113555028349e-06,
      "loss": 1.0348,
      "step": 22240
    },
    {
      "epoch": 2.673355361970562,
      "grad_norm": 2.7790732383728027,
      "learning_rate": 3.2920503438015204e-06,
      "loss": 1.0708,
      "step": 22250
    },
    {
      "epoch": 2.6745569240012017,
      "grad_norm": 3.3449549674987793,
      "learning_rate": 3.279987132574692e-06,
      "loss": 1.0555,
      "step": 22260
    },
    {
      "epoch": 2.6757584860318415,
      "grad_norm": 3.410287618637085,
      "learning_rate": 3.2679239213478632e-06,
      "loss": 1.039,
      "step": 22270
    },
    {
      "epoch": 2.6769600480624813,
      "grad_norm": 2.8715407848358154,
      "learning_rate": 3.2558607101210343e-06,
      "loss": 1.0864,
      "step": 22280
    },
    {
      "epoch": 2.678161610093121,
      "grad_norm": 3.4635519981384277,
      "learning_rate": 3.2437974988942057e-06,
      "loss": 1.0567,
      "step": 22290
    },
    {
      "epoch": 2.679363172123761,
      "grad_norm": 2.8092474937438965,
      "learning_rate": 3.231734287667377e-06,
      "loss": 1.0432,
      "step": 22300
    },
    {
      "epoch": 2.6805647341544008,
      "grad_norm": 2.9894847869873047,
      "learning_rate": 3.2196710764405486e-06,
      "loss": 1.0758,
      "step": 22310
    },
    {
      "epoch": 2.6817662961850406,
      "grad_norm": 3.307771921157837,
      "learning_rate": 3.20760786521372e-06,
      "loss": 1.0446,
      "step": 22320
    },
    {
      "epoch": 2.6829678582156804,
      "grad_norm": 3.3322176933288574,
      "learning_rate": 3.1955446539868915e-06,
      "loss": 1.0766,
      "step": 22330
    },
    {
      "epoch": 2.68416942024632,
      "grad_norm": 2.987008571624756,
      "learning_rate": 3.183481442760063e-06,
      "loss": 1.0094,
      "step": 22340
    },
    {
      "epoch": 2.68537098227696,
      "grad_norm": 3.128570318222046,
      "learning_rate": 3.1714182315332343e-06,
      "loss": 1.0329,
      "step": 22350
    },
    {
      "epoch": 2.6865725443076,
      "grad_norm": 3.358154296875,
      "learning_rate": 3.1593550203064058e-06,
      "loss": 1.08,
      "step": 22360
    },
    {
      "epoch": 2.6877741063382397,
      "grad_norm": 3.682981252670288,
      "learning_rate": 3.147291809079577e-06,
      "loss": 1.0461,
      "step": 22370
    },
    {
      "epoch": 2.6889756683688795,
      "grad_norm": 3.0608763694763184,
      "learning_rate": 3.1352285978527482e-06,
      "loss": 1.0495,
      "step": 22380
    },
    {
      "epoch": 2.6901772303995193,
      "grad_norm": 2.8919708728790283,
      "learning_rate": 3.1231653866259197e-06,
      "loss": 1.0392,
      "step": 22390
    },
    {
      "epoch": 2.691378792430159,
      "grad_norm": 3.6967954635620117,
      "learning_rate": 3.111102175399091e-06,
      "loss": 1.0682,
      "step": 22400
    },
    {
      "epoch": 2.692580354460799,
      "grad_norm": 3.0803346633911133,
      "learning_rate": 3.0990389641722626e-06,
      "loss": 1.068,
      "step": 22410
    },
    {
      "epoch": 2.6937819164914387,
      "grad_norm": 3.1491739749908447,
      "learning_rate": 3.086975752945434e-06,
      "loss": 0.9942,
      "step": 22420
    },
    {
      "epoch": 2.6949834785220785,
      "grad_norm": 3.26625919342041,
      "learning_rate": 3.0749125417186054e-06,
      "loss": 1.0322,
      "step": 22430
    },
    {
      "epoch": 2.6961850405527183,
      "grad_norm": 3.2343709468841553,
      "learning_rate": 3.062849330491777e-06,
      "loss": 1.0811,
      "step": 22440
    },
    {
      "epoch": 2.697386602583358,
      "grad_norm": 3.014134168624878,
      "learning_rate": 3.0507861192649487e-06,
      "loss": 1.0842,
      "step": 22450
    },
    {
      "epoch": 2.698588164613998,
      "grad_norm": 3.1987688541412354,
      "learning_rate": 3.03872290803812e-06,
      "loss": 1.0218,
      "step": 22460
    },
    {
      "epoch": 2.699789726644638,
      "grad_norm": 3.723930597305298,
      "learning_rate": 3.026659696811291e-06,
      "loss": 1.0793,
      "step": 22470
    },
    {
      "epoch": 2.7009912886752776,
      "grad_norm": 3.5396642684936523,
      "learning_rate": 3.0145964855844626e-06,
      "loss": 1.0731,
      "step": 22480
    },
    {
      "epoch": 2.702192850705918,
      "grad_norm": 3.112401247024536,
      "learning_rate": 3.002533274357634e-06,
      "loss": 1.0464,
      "step": 22490
    },
    {
      "epoch": 2.7033944127365577,
      "grad_norm": 3.622187376022339,
      "learning_rate": 2.9904700631308055e-06,
      "loss": 1.0212,
      "step": 22500
    },
    {
      "epoch": 2.7045959747671975,
      "grad_norm": 3.145427942276001,
      "learning_rate": 2.978406851903977e-06,
      "loss": 1.1042,
      "step": 22510
    },
    {
      "epoch": 2.7057975367978373,
      "grad_norm": 3.1762545108795166,
      "learning_rate": 2.9663436406771484e-06,
      "loss": 1.054,
      "step": 22520
    },
    {
      "epoch": 2.706999098828477,
      "grad_norm": 3.0169291496276855,
      "learning_rate": 2.95428042945032e-06,
      "loss": 1.0,
      "step": 22530
    },
    {
      "epoch": 2.708200660859117,
      "grad_norm": 2.971210241317749,
      "learning_rate": 2.9422172182234913e-06,
      "loss": 1.1032,
      "step": 22540
    },
    {
      "epoch": 2.7094022228897567,
      "grad_norm": 3.2479162216186523,
      "learning_rate": 2.9301540069966627e-06,
      "loss": 1.0502,
      "step": 22550
    },
    {
      "epoch": 2.7106037849203966,
      "grad_norm": 2.998136520385742,
      "learning_rate": 2.918090795769834e-06,
      "loss": 1.0027,
      "step": 22560
    },
    {
      "epoch": 2.7118053469510364,
      "grad_norm": 3.345208168029785,
      "learning_rate": 2.906027584543005e-06,
      "loss": 1.0511,
      "step": 22570
    },
    {
      "epoch": 2.713006908981676,
      "grad_norm": 3.2308926582336426,
      "learning_rate": 2.8939643733161766e-06,
      "loss": 1.0735,
      "step": 22580
    },
    {
      "epoch": 2.714208471012316,
      "grad_norm": 3.084031581878662,
      "learning_rate": 2.881901162089348e-06,
      "loss": 1.0411,
      "step": 22590
    },
    {
      "epoch": 2.715410033042956,
      "grad_norm": 3.338519811630249,
      "learning_rate": 2.8698379508625195e-06,
      "loss": 1.0169,
      "step": 22600
    },
    {
      "epoch": 2.7166115950735956,
      "grad_norm": 3.573390245437622,
      "learning_rate": 2.857774739635691e-06,
      "loss": 1.073,
      "step": 22610
    },
    {
      "epoch": 2.7178131571042354,
      "grad_norm": 3.5165371894836426,
      "learning_rate": 2.8457115284088624e-06,
      "loss": 1.0838,
      "step": 22620
    },
    {
      "epoch": 2.7190147191348752,
      "grad_norm": 3.4745373725891113,
      "learning_rate": 2.833648317182034e-06,
      "loss": 1.0633,
      "step": 22630
    },
    {
      "epoch": 2.720216281165515,
      "grad_norm": 3.32737398147583,
      "learning_rate": 2.8215851059552053e-06,
      "loss": 1.0482,
      "step": 22640
    },
    {
      "epoch": 2.721417843196155,
      "grad_norm": 3.1439692974090576,
      "learning_rate": 2.8095218947283767e-06,
      "loss": 1.0007,
      "step": 22650
    },
    {
      "epoch": 2.7226194052267947,
      "grad_norm": 3.5152955055236816,
      "learning_rate": 2.797458683501548e-06,
      "loss": 1.0347,
      "step": 22660
    },
    {
      "epoch": 2.7238209672574345,
      "grad_norm": 3.522902250289917,
      "learning_rate": 2.7853954722747196e-06,
      "loss": 1.0684,
      "step": 22670
    },
    {
      "epoch": 2.7250225292880748,
      "grad_norm": 3.3424646854400635,
      "learning_rate": 2.773332261047891e-06,
      "loss": 1.0403,
      "step": 22680
    },
    {
      "epoch": 2.7262240913187146,
      "grad_norm": 3.225015163421631,
      "learning_rate": 2.7612690498210625e-06,
      "loss": 1.062,
      "step": 22690
    },
    {
      "epoch": 2.7274256533493544,
      "grad_norm": 3.0986311435699463,
      "learning_rate": 2.749205838594234e-06,
      "loss": 1.0174,
      "step": 22700
    },
    {
      "epoch": 2.728627215379994,
      "grad_norm": 3.0415213108062744,
      "learning_rate": 2.7371426273674054e-06,
      "loss": 1.0372,
      "step": 22710
    },
    {
      "epoch": 2.729828777410634,
      "grad_norm": 2.9096357822418213,
      "learning_rate": 2.725079416140577e-06,
      "loss": 1.0239,
      "step": 22720
    },
    {
      "epoch": 2.731030339441274,
      "grad_norm": 3.6115612983703613,
      "learning_rate": 2.7130162049137483e-06,
      "loss": 1.0423,
      "step": 22730
    },
    {
      "epoch": 2.7322319014719136,
      "grad_norm": 3.471275806427002,
      "learning_rate": 2.7009529936869197e-06,
      "loss": 1.0401,
      "step": 22740
    },
    {
      "epoch": 2.7334334635025535,
      "grad_norm": 3.7130115032196045,
      "learning_rate": 2.688889782460091e-06,
      "loss": 1.0637,
      "step": 22750
    },
    {
      "epoch": 2.7346350255331933,
      "grad_norm": 4.757441997528076,
      "learning_rate": 2.676826571233262e-06,
      "loss": 1.1033,
      "step": 22760
    },
    {
      "epoch": 2.735836587563833,
      "grad_norm": 3.6237123012542725,
      "learning_rate": 2.6647633600064336e-06,
      "loss": 0.9643,
      "step": 22770
    },
    {
      "epoch": 2.737038149594473,
      "grad_norm": 3.8414382934570312,
      "learning_rate": 2.652700148779605e-06,
      "loss": 1.0616,
      "step": 22780
    },
    {
      "epoch": 2.7382397116251127,
      "grad_norm": 3.303936243057251,
      "learning_rate": 2.6406369375527765e-06,
      "loss": 1.0615,
      "step": 22790
    },
    {
      "epoch": 2.7394412736557525,
      "grad_norm": 3.1602962017059326,
      "learning_rate": 2.628573726325948e-06,
      "loss": 1.0814,
      "step": 22800
    },
    {
      "epoch": 2.7406428356863923,
      "grad_norm": 4.135040283203125,
      "learning_rate": 2.6165105150991194e-06,
      "loss": 1.0508,
      "step": 22810
    },
    {
      "epoch": 2.741844397717032,
      "grad_norm": 3.058870553970337,
      "learning_rate": 2.604447303872291e-06,
      "loss": 1.0445,
      "step": 22820
    },
    {
      "epoch": 2.743045959747672,
      "grad_norm": 4.15173864364624,
      "learning_rate": 2.5923840926454622e-06,
      "loss": 0.994,
      "step": 22830
    },
    {
      "epoch": 2.7442475217783118,
      "grad_norm": 3.487431287765503,
      "learning_rate": 2.5803208814186337e-06,
      "loss": 1.0285,
      "step": 22840
    },
    {
      "epoch": 2.7454490838089516,
      "grad_norm": 3.1212949752807617,
      "learning_rate": 2.568257670191805e-06,
      "loss": 1.0468,
      "step": 22850
    },
    {
      "epoch": 2.7466506458395914,
      "grad_norm": 3.4929540157318115,
      "learning_rate": 2.5561944589649766e-06,
      "loss": 1.0601,
      "step": 22860
    },
    {
      "epoch": 2.747852207870231,
      "grad_norm": 2.8556692600250244,
      "learning_rate": 2.544131247738148e-06,
      "loss": 1.0219,
      "step": 22870
    },
    {
      "epoch": 2.749053769900871,
      "grad_norm": 3.1346797943115234,
      "learning_rate": 2.5320680365113194e-06,
      "loss": 1.0739,
      "step": 22880
    },
    {
      "epoch": 2.750255331931511,
      "grad_norm": 3.0175890922546387,
      "learning_rate": 2.520004825284491e-06,
      "loss": 1.0882,
      "step": 22890
    },
    {
      "epoch": 2.7514568939621507,
      "grad_norm": 3.568290948867798,
      "learning_rate": 2.5079416140576623e-06,
      "loss": 1.101,
      "step": 22900
    },
    {
      "epoch": 2.7526584559927905,
      "grad_norm": 3.4445018768310547,
      "learning_rate": 2.4958784028308338e-06,
      "loss": 1.0118,
      "step": 22910
    },
    {
      "epoch": 2.7538600180234303,
      "grad_norm": 3.3684804439544678,
      "learning_rate": 2.483815191604005e-06,
      "loss": 1.0489,
      "step": 22920
    },
    {
      "epoch": 2.75506158005407,
      "grad_norm": 2.8419172763824463,
      "learning_rate": 2.4717519803771766e-06,
      "loss": 1.1069,
      "step": 22930
    },
    {
      "epoch": 2.75626314208471,
      "grad_norm": 3.1125268936157227,
      "learning_rate": 2.459688769150348e-06,
      "loss": 1.0592,
      "step": 22940
    },
    {
      "epoch": 2.7574647041153497,
      "grad_norm": 3.568297863006592,
      "learning_rate": 2.4476255579235195e-06,
      "loss": 1.0135,
      "step": 22950
    },
    {
      "epoch": 2.7586662661459895,
      "grad_norm": 3.5795540809631348,
      "learning_rate": 2.4355623466966905e-06,
      "loss": 1.0791,
      "step": 22960
    },
    {
      "epoch": 2.7598678281766293,
      "grad_norm": 3.471238374710083,
      "learning_rate": 2.423499135469862e-06,
      "loss": 1.0107,
      "step": 22970
    },
    {
      "epoch": 2.7610693902072696,
      "grad_norm": 2.9097883701324463,
      "learning_rate": 2.4114359242430334e-06,
      "loss": 1.028,
      "step": 22980
    },
    {
      "epoch": 2.7622709522379094,
      "grad_norm": 3.0581133365631104,
      "learning_rate": 2.399372713016205e-06,
      "loss": 1.0632,
      "step": 22990
    },
    {
      "epoch": 2.7634725142685492,
      "grad_norm": 2.8944802284240723,
      "learning_rate": 2.3873095017893763e-06,
      "loss": 1.0672,
      "step": 23000
    },
    {
      "epoch": 2.764674076299189,
      "grad_norm": 3.850553512573242,
      "learning_rate": 2.3752462905625478e-06,
      "loss": 1.0382,
      "step": 23010
    },
    {
      "epoch": 2.765875638329829,
      "grad_norm": 3.598635196685791,
      "learning_rate": 2.363183079335719e-06,
      "loss": 1.0659,
      "step": 23020
    },
    {
      "epoch": 2.7670772003604687,
      "grad_norm": 3.171175241470337,
      "learning_rate": 2.3511198681088906e-06,
      "loss": 1.0335,
      "step": 23030
    },
    {
      "epoch": 2.7682787623911085,
      "grad_norm": 3.087958574295044,
      "learning_rate": 2.339056656882062e-06,
      "loss": 1.0885,
      "step": 23040
    },
    {
      "epoch": 2.7694803244217483,
      "grad_norm": 3.1883161067962646,
      "learning_rate": 2.3269934456552335e-06,
      "loss": 1.0456,
      "step": 23050
    },
    {
      "epoch": 2.770681886452388,
      "grad_norm": 3.682936668395996,
      "learning_rate": 2.3149302344284045e-06,
      "loss": 1.0743,
      "step": 23060
    },
    {
      "epoch": 2.771883448483028,
      "grad_norm": 3.009044885635376,
      "learning_rate": 2.3028670232015764e-06,
      "loss": 1.0108,
      "step": 23070
    },
    {
      "epoch": 2.7730850105136677,
      "grad_norm": 3.448052406311035,
      "learning_rate": 2.290803811974748e-06,
      "loss": 1.0877,
      "step": 23080
    },
    {
      "epoch": 2.7742865725443075,
      "grad_norm": 3.1012253761291504,
      "learning_rate": 2.2787406007479193e-06,
      "loss": 1.107,
      "step": 23090
    },
    {
      "epoch": 2.7754881345749474,
      "grad_norm": 3.4618160724639893,
      "learning_rate": 2.2666773895210907e-06,
      "loss": 1.0574,
      "step": 23100
    },
    {
      "epoch": 2.776689696605587,
      "grad_norm": 3.1802000999450684,
      "learning_rate": 2.254614178294262e-06,
      "loss": 1.0736,
      "step": 23110
    },
    {
      "epoch": 2.777891258636227,
      "grad_norm": 3.0963029861450195,
      "learning_rate": 2.2425509670674336e-06,
      "loss": 1.0504,
      "step": 23120
    },
    {
      "epoch": 2.779092820666867,
      "grad_norm": 3.2793169021606445,
      "learning_rate": 2.230487755840605e-06,
      "loss": 1.0832,
      "step": 23130
    },
    {
      "epoch": 2.7802943826975066,
      "grad_norm": 3.0859742164611816,
      "learning_rate": 2.2184245446137765e-06,
      "loss": 1.0783,
      "step": 23140
    },
    {
      "epoch": 2.7814959447281464,
      "grad_norm": 3.758134365081787,
      "learning_rate": 2.2063613333869475e-06,
      "loss": 1.081,
      "step": 23150
    },
    {
      "epoch": 2.7826975067587867,
      "grad_norm": 3.5695478916168213,
      "learning_rate": 2.194298122160119e-06,
      "loss": 1.013,
      "step": 23160
    },
    {
      "epoch": 2.7838990687894265,
      "grad_norm": 3.344576120376587,
      "learning_rate": 2.1822349109332904e-06,
      "loss": 1.0443,
      "step": 23170
    },
    {
      "epoch": 2.7851006308200663,
      "grad_norm": 3.126692295074463,
      "learning_rate": 2.170171699706462e-06,
      "loss": 1.0664,
      "step": 23180
    },
    {
      "epoch": 2.786302192850706,
      "grad_norm": 3.4208834171295166,
      "learning_rate": 2.1581084884796333e-06,
      "loss": 1.0677,
      "step": 23190
    },
    {
      "epoch": 2.787503754881346,
      "grad_norm": 3.4881792068481445,
      "learning_rate": 2.1460452772528047e-06,
      "loss": 1.0325,
      "step": 23200
    },
    {
      "epoch": 2.7887053169119858,
      "grad_norm": 3.0318257808685303,
      "learning_rate": 2.133982066025976e-06,
      "loss": 1.0174,
      "step": 23210
    },
    {
      "epoch": 2.7899068789426256,
      "grad_norm": 3.0691490173339844,
      "learning_rate": 2.1219188547991476e-06,
      "loss": 1.0182,
      "step": 23220
    },
    {
      "epoch": 2.7911084409732654,
      "grad_norm": 3.708919048309326,
      "learning_rate": 2.109855643572319e-06,
      "loss": 1.1382,
      "step": 23230
    },
    {
      "epoch": 2.792310003003905,
      "grad_norm": 3.788928747177124,
      "learning_rate": 2.0977924323454905e-06,
      "loss": 1.1278,
      "step": 23240
    },
    {
      "epoch": 2.793511565034545,
      "grad_norm": 3.4012017250061035,
      "learning_rate": 2.0857292211186615e-06,
      "loss": 1.0596,
      "step": 23250
    },
    {
      "epoch": 2.794713127065185,
      "grad_norm": 3.4515771865844727,
      "learning_rate": 2.073666009891833e-06,
      "loss": 1.0377,
      "step": 23260
    },
    {
      "epoch": 2.7959146890958246,
      "grad_norm": 3.368271827697754,
      "learning_rate": 2.0616027986650044e-06,
      "loss": 1.0478,
      "step": 23270
    },
    {
      "epoch": 2.7971162511264644,
      "grad_norm": 2.9587626457214355,
      "learning_rate": 2.0495395874381762e-06,
      "loss": 1.0376,
      "step": 23280
    },
    {
      "epoch": 2.7983178131571043,
      "grad_norm": 3.64996337890625,
      "learning_rate": 2.0374763762113477e-06,
      "loss": 1.0127,
      "step": 23290
    },
    {
      "epoch": 2.799519375187744,
      "grad_norm": 4.342430114746094,
      "learning_rate": 2.025413164984519e-06,
      "loss": 1.0804,
      "step": 23300
    },
    {
      "epoch": 2.800720937218384,
      "grad_norm": 3.9691262245178223,
      "learning_rate": 2.0133499537576906e-06,
      "loss": 1.0637,
      "step": 23310
    },
    {
      "epoch": 2.8019224992490237,
      "grad_norm": 3.136634588241577,
      "learning_rate": 2.001286742530862e-06,
      "loss": 1.0739,
      "step": 23320
    },
    {
      "epoch": 2.8031240612796635,
      "grad_norm": 3.3927533626556396,
      "learning_rate": 1.9892235313040334e-06,
      "loss": 1.0158,
      "step": 23330
    },
    {
      "epoch": 2.8043256233103033,
      "grad_norm": 3.465080499649048,
      "learning_rate": 1.977160320077205e-06,
      "loss": 1.0014,
      "step": 23340
    },
    {
      "epoch": 2.805527185340943,
      "grad_norm": 3.3071351051330566,
      "learning_rate": 1.965097108850376e-06,
      "loss": 1.067,
      "step": 23350
    },
    {
      "epoch": 2.806728747371583,
      "grad_norm": 3.1014959812164307,
      "learning_rate": 1.9530338976235473e-06,
      "loss": 1.041,
      "step": 23360
    },
    {
      "epoch": 2.8079303094022228,
      "grad_norm": 3.041321277618408,
      "learning_rate": 1.9409706863967188e-06,
      "loss": 1.0546,
      "step": 23370
    },
    {
      "epoch": 2.8091318714328626,
      "grad_norm": 4.010103225708008,
      "learning_rate": 1.9289074751698902e-06,
      "loss": 1.0295,
      "step": 23380
    },
    {
      "epoch": 2.8103334334635024,
      "grad_norm": 3.102224826812744,
      "learning_rate": 1.9168442639430617e-06,
      "loss": 1.0343,
      "step": 23390
    },
    {
      "epoch": 2.811534995494142,
      "grad_norm": 3.221544027328491,
      "learning_rate": 1.904781052716233e-06,
      "loss": 1.0785,
      "step": 23400
    },
    {
      "epoch": 2.812736557524782,
      "grad_norm": 2.980074644088745,
      "learning_rate": 1.8927178414894045e-06,
      "loss": 1.0088,
      "step": 23410
    },
    {
      "epoch": 2.813938119555422,
      "grad_norm": 3.486396551132202,
      "learning_rate": 1.8806546302625758e-06,
      "loss": 1.0796,
      "step": 23420
    },
    {
      "epoch": 2.8151396815860616,
      "grad_norm": 3.0154900550842285,
      "learning_rate": 1.8685914190357472e-06,
      "loss": 1.0074,
      "step": 23430
    },
    {
      "epoch": 2.8163412436167015,
      "grad_norm": 3.804025650024414,
      "learning_rate": 1.8565282078089189e-06,
      "loss": 1.1046,
      "step": 23440
    },
    {
      "epoch": 2.8175428056473413,
      "grad_norm": 3.6916096210479736,
      "learning_rate": 1.8444649965820903e-06,
      "loss": 1.0573,
      "step": 23450
    },
    {
      "epoch": 2.8187443676779815,
      "grad_norm": 3.481600046157837,
      "learning_rate": 1.8324017853552618e-06,
      "loss": 1.0494,
      "step": 23460
    },
    {
      "epoch": 2.8199459297086213,
      "grad_norm": 3.2975878715515137,
      "learning_rate": 1.820338574128433e-06,
      "loss": 1.0327,
      "step": 23470
    },
    {
      "epoch": 2.821147491739261,
      "grad_norm": 3.26954984664917,
      "learning_rate": 1.8082753629016044e-06,
      "loss": 1.0368,
      "step": 23480
    },
    {
      "epoch": 2.822349053769901,
      "grad_norm": 2.9634766578674316,
      "learning_rate": 1.7962121516747759e-06,
      "loss": 1.0737,
      "step": 23490
    },
    {
      "epoch": 2.823550615800541,
      "grad_norm": 2.9977948665618896,
      "learning_rate": 1.7841489404479473e-06,
      "loss": 1.0376,
      "step": 23500
    },
    {
      "epoch": 2.8247521778311806,
      "grad_norm": 3.1466708183288574,
      "learning_rate": 1.7720857292211187e-06,
      "loss": 0.9923,
      "step": 23510
    },
    {
      "epoch": 2.8259537398618204,
      "grad_norm": 3.1968307495117188,
      "learning_rate": 1.76002251799429e-06,
      "loss": 1.0943,
      "step": 23520
    },
    {
      "epoch": 2.8271553018924602,
      "grad_norm": 3.5739011764526367,
      "learning_rate": 1.7479593067674614e-06,
      "loss": 1.0623,
      "step": 23530
    },
    {
      "epoch": 2.8283568639231,
      "grad_norm": 3.3157315254211426,
      "learning_rate": 1.735896095540633e-06,
      "loss": 1.0725,
      "step": 23540
    },
    {
      "epoch": 2.82955842595374,
      "grad_norm": 2.9157514572143555,
      "learning_rate": 1.7238328843138045e-06,
      "loss": 1.0234,
      "step": 23550
    },
    {
      "epoch": 2.8307599879843797,
      "grad_norm": 3.3744568824768066,
      "learning_rate": 1.711769673086976e-06,
      "loss": 1.035,
      "step": 23560
    },
    {
      "epoch": 2.8319615500150195,
      "grad_norm": 3.0925679206848145,
      "learning_rate": 1.6997064618601472e-06,
      "loss": 1.0536,
      "step": 23570
    },
    {
      "epoch": 2.8331631120456593,
      "grad_norm": 3.313220739364624,
      "learning_rate": 1.6876432506333186e-06,
      "loss": 1.0318,
      "step": 23580
    },
    {
      "epoch": 2.834364674076299,
      "grad_norm": 3.7774319648742676,
      "learning_rate": 1.67558003940649e-06,
      "loss": 1.0534,
      "step": 23590
    },
    {
      "epoch": 2.835566236106939,
      "grad_norm": 3.0069985389709473,
      "learning_rate": 1.6635168281796615e-06,
      "loss": 1.068,
      "step": 23600
    },
    {
      "epoch": 2.8367677981375787,
      "grad_norm": 3.025325298309326,
      "learning_rate": 1.651453616952833e-06,
      "loss": 1.0082,
      "step": 23610
    },
    {
      "epoch": 2.8379693601682185,
      "grad_norm": 3.4890308380126953,
      "learning_rate": 1.6393904057260042e-06,
      "loss": 1.0179,
      "step": 23620
    },
    {
      "epoch": 2.8391709221988584,
      "grad_norm": 2.9154419898986816,
      "learning_rate": 1.6273271944991756e-06,
      "loss": 1.0311,
      "step": 23630
    },
    {
      "epoch": 2.8403724842294986,
      "grad_norm": 4.0528459548950195,
      "learning_rate": 1.6152639832723473e-06,
      "loss": 1.0032,
      "step": 23640
    },
    {
      "epoch": 2.8415740462601384,
      "grad_norm": 3.577289581298828,
      "learning_rate": 1.6032007720455187e-06,
      "loss": 1.0333,
      "step": 23650
    },
    {
      "epoch": 2.8427756082907782,
      "grad_norm": 2.940369129180908,
      "learning_rate": 1.59113756081869e-06,
      "loss": 1.0752,
      "step": 23660
    },
    {
      "epoch": 2.843977170321418,
      "grad_norm": 3.6467792987823486,
      "learning_rate": 1.5790743495918614e-06,
      "loss": 1.0626,
      "step": 23670
    },
    {
      "epoch": 2.845178732352058,
      "grad_norm": 3.1246225833892822,
      "learning_rate": 1.5670111383650328e-06,
      "loss": 1.0592,
      "step": 23680
    },
    {
      "epoch": 2.8463802943826977,
      "grad_norm": 3.0048716068267822,
      "learning_rate": 1.5549479271382043e-06,
      "loss": 1.0748,
      "step": 23690
    },
    {
      "epoch": 2.8475818564133375,
      "grad_norm": 3.4321072101593018,
      "learning_rate": 1.5428847159113757e-06,
      "loss": 1.035,
      "step": 23700
    },
    {
      "epoch": 2.8487834184439773,
      "grad_norm": 3.7036194801330566,
      "learning_rate": 1.530821504684547e-06,
      "loss": 1.0292,
      "step": 23710
    },
    {
      "epoch": 2.849984980474617,
      "grad_norm": 3.330305337905884,
      "learning_rate": 1.5187582934577184e-06,
      "loss": 1.0642,
      "step": 23720
    },
    {
      "epoch": 2.851186542505257,
      "grad_norm": 3.3638765811920166,
      "learning_rate": 1.5066950822308898e-06,
      "loss": 1.1222,
      "step": 23730
    },
    {
      "epoch": 2.8523881045358968,
      "grad_norm": 3.5122601985931396,
      "learning_rate": 1.4946318710040613e-06,
      "loss": 1.0805,
      "step": 23740
    },
    {
      "epoch": 2.8535896665665366,
      "grad_norm": 3.043248176574707,
      "learning_rate": 1.482568659777233e-06,
      "loss": 1.0675,
      "step": 23750
    },
    {
      "epoch": 2.8547912285971764,
      "grad_norm": 3.6033413410186768,
      "learning_rate": 1.4705054485504041e-06,
      "loss": 1.0585,
      "step": 23760
    },
    {
      "epoch": 2.855992790627816,
      "grad_norm": 3.2838597297668457,
      "learning_rate": 1.4584422373235756e-06,
      "loss": 1.0607,
      "step": 23770
    },
    {
      "epoch": 2.857194352658456,
      "grad_norm": 3.212141752243042,
      "learning_rate": 1.446379026096747e-06,
      "loss": 1.0528,
      "step": 23780
    },
    {
      "epoch": 2.858395914689096,
      "grad_norm": 3.1400206089019775,
      "learning_rate": 1.4343158148699185e-06,
      "loss": 1.0634,
      "step": 23790
    },
    {
      "epoch": 2.8595974767197356,
      "grad_norm": 3.084627628326416,
      "learning_rate": 1.42225260364309e-06,
      "loss": 0.9971,
      "step": 23800
    },
    {
      "epoch": 2.8607990387503754,
      "grad_norm": 3.6069700717926025,
      "learning_rate": 1.4101893924162611e-06,
      "loss": 1.0786,
      "step": 23810
    },
    {
      "epoch": 2.8620006007810153,
      "grad_norm": 3.0730843544006348,
      "learning_rate": 1.3981261811894326e-06,
      "loss": 1.0602,
      "step": 23820
    },
    {
      "epoch": 2.863202162811655,
      "grad_norm": 3.4924283027648926,
      "learning_rate": 1.386062969962604e-06,
      "loss": 1.0397,
      "step": 23830
    },
    {
      "epoch": 2.864403724842295,
      "grad_norm": 3.1598613262176514,
      "learning_rate": 1.3739997587357755e-06,
      "loss": 1.0609,
      "step": 23840
    },
    {
      "epoch": 2.8656052868729347,
      "grad_norm": 3.8195648193359375,
      "learning_rate": 1.361936547508947e-06,
      "loss": 1.0352,
      "step": 23850
    },
    {
      "epoch": 2.8668068489035745,
      "grad_norm": 4.255918979644775,
      "learning_rate": 1.3498733362821183e-06,
      "loss": 1.0781,
      "step": 23860
    },
    {
      "epoch": 2.8680084109342143,
      "grad_norm": 3.249260663986206,
      "learning_rate": 1.3378101250552898e-06,
      "loss": 1.0875,
      "step": 23870
    },
    {
      "epoch": 2.869209972964854,
      "grad_norm": 3.579648494720459,
      "learning_rate": 1.3257469138284612e-06,
      "loss": 1.0315,
      "step": 23880
    },
    {
      "epoch": 2.870411534995494,
      "grad_norm": 3.601522207260132,
      "learning_rate": 1.3136837026016327e-06,
      "loss": 1.0169,
      "step": 23890
    },
    {
      "epoch": 2.8716130970261338,
      "grad_norm": 3.3252577781677246,
      "learning_rate": 1.301620491374804e-06,
      "loss": 1.0222,
      "step": 23900
    },
    {
      "epoch": 2.8728146590567736,
      "grad_norm": 3.090167760848999,
      "learning_rate": 1.2895572801479753e-06,
      "loss": 1.0465,
      "step": 23910
    },
    {
      "epoch": 2.8740162210874134,
      "grad_norm": 3.442850351333618,
      "learning_rate": 1.2774940689211468e-06,
      "loss": 1.0606,
      "step": 23920
    },
    {
      "epoch": 2.875217783118053,
      "grad_norm": 3.9618396759033203,
      "learning_rate": 1.2654308576943182e-06,
      "loss": 1.0532,
      "step": 23930
    },
    {
      "epoch": 2.8764193451486935,
      "grad_norm": 3.0633888244628906,
      "learning_rate": 1.2533676464674896e-06,
      "loss": 1.0318,
      "step": 23940
    },
    {
      "epoch": 2.8776209071793333,
      "grad_norm": 3.2912752628326416,
      "learning_rate": 1.241304435240661e-06,
      "loss": 1.1067,
      "step": 23950
    },
    {
      "epoch": 2.878822469209973,
      "grad_norm": 3.2766242027282715,
      "learning_rate": 1.2292412240138325e-06,
      "loss": 1.0445,
      "step": 23960
    },
    {
      "epoch": 2.880024031240613,
      "grad_norm": 3.126277446746826,
      "learning_rate": 1.217178012787004e-06,
      "loss": 1.0146,
      "step": 23970
    },
    {
      "epoch": 2.8812255932712527,
      "grad_norm": 3.0978708267211914,
      "learning_rate": 1.2051148015601754e-06,
      "loss": 1.0961,
      "step": 23980
    },
    {
      "epoch": 2.8824271553018925,
      "grad_norm": 3.287722587585449,
      "learning_rate": 1.1930515903333469e-06,
      "loss": 1.0051,
      "step": 23990
    },
    {
      "epoch": 2.8836287173325323,
      "grad_norm": 3.3401846885681152,
      "learning_rate": 1.1809883791065183e-06,
      "loss": 1.0647,
      "step": 24000
    },
    {
      "epoch": 2.884830279363172,
      "grad_norm": 3.36499285697937,
      "learning_rate": 1.1689251678796895e-06,
      "loss": 1.041,
      "step": 24010
    },
    {
      "epoch": 2.886031841393812,
      "grad_norm": 3.428302764892578,
      "learning_rate": 1.156861956652861e-06,
      "loss": 1.0966,
      "step": 24020
    },
    {
      "epoch": 2.887233403424452,
      "grad_norm": 3.676285982131958,
      "learning_rate": 1.1447987454260324e-06,
      "loss": 1.0686,
      "step": 24030
    },
    {
      "epoch": 2.8884349654550916,
      "grad_norm": 3.2566134929656982,
      "learning_rate": 1.1327355341992038e-06,
      "loss": 1.049,
      "step": 24040
    },
    {
      "epoch": 2.8896365274857314,
      "grad_norm": 3.3721415996551514,
      "learning_rate": 1.120672322972375e-06,
      "loss": 1.0908,
      "step": 24050
    },
    {
      "epoch": 2.8908380895163712,
      "grad_norm": 3.3379368782043457,
      "learning_rate": 1.1086091117455467e-06,
      "loss": 0.9818,
      "step": 24060
    },
    {
      "epoch": 2.892039651547011,
      "grad_norm": 3.0692312717437744,
      "learning_rate": 1.0965459005187182e-06,
      "loss": 1.0442,
      "step": 24070
    },
    {
      "epoch": 2.893241213577651,
      "grad_norm": 3.1649045944213867,
      "learning_rate": 1.0844826892918896e-06,
      "loss": 1.0397,
      "step": 24080
    },
    {
      "epoch": 2.8944427756082907,
      "grad_norm": 3.5426721572875977,
      "learning_rate": 1.072419478065061e-06,
      "loss": 1.0558,
      "step": 24090
    },
    {
      "epoch": 2.8956443376389305,
      "grad_norm": 3.381516218185425,
      "learning_rate": 1.0603562668382323e-06,
      "loss": 1.0546,
      "step": 24100
    },
    {
      "epoch": 2.8968458996695703,
      "grad_norm": 3.2772583961486816,
      "learning_rate": 1.0482930556114037e-06,
      "loss": 1.0433,
      "step": 24110
    },
    {
      "epoch": 2.8980474617002105,
      "grad_norm": 3.4736642837524414,
      "learning_rate": 1.0362298443845752e-06,
      "loss": 0.9928,
      "step": 24120
    },
    {
      "epoch": 2.8992490237308504,
      "grad_norm": 3.156041383743286,
      "learning_rate": 1.0241666331577466e-06,
      "loss": 1.0268,
      "step": 24130
    },
    {
      "epoch": 2.90045058576149,
      "grad_norm": 4.5527262687683105,
      "learning_rate": 1.012103421930918e-06,
      "loss": 1.0573,
      "step": 24140
    },
    {
      "epoch": 2.90165214779213,
      "grad_norm": 3.681901693344116,
      "learning_rate": 1.0000402107040893e-06,
      "loss": 0.9837,
      "step": 24150
    },
    {
      "epoch": 2.90285370982277,
      "grad_norm": 3.210440158843994,
      "learning_rate": 9.87976999477261e-07,
      "loss": 1.0624,
      "step": 24160
    },
    {
      "epoch": 2.9040552718534096,
      "grad_norm": 3.738954544067383,
      "learning_rate": 9.759137882504324e-07,
      "loss": 0.9831,
      "step": 24170
    },
    {
      "epoch": 2.9052568338840494,
      "grad_norm": 3.044989585876465,
      "learning_rate": 9.638505770236038e-07,
      "loss": 1.1009,
      "step": 24180
    },
    {
      "epoch": 2.9064583959146892,
      "grad_norm": 3.7554218769073486,
      "learning_rate": 9.517873657967751e-07,
      "loss": 1.0132,
      "step": 24190
    },
    {
      "epoch": 2.907659957945329,
      "grad_norm": 3.746248960494995,
      "learning_rate": 9.397241545699466e-07,
      "loss": 0.9929,
      "step": 24200
    },
    {
      "epoch": 2.908861519975969,
      "grad_norm": 3.220146894454956,
      "learning_rate": 9.276609433431179e-07,
      "loss": 1.0818,
      "step": 24210
    },
    {
      "epoch": 2.9100630820066087,
      "grad_norm": 3.133192777633667,
      "learning_rate": 9.155977321162894e-07,
      "loss": 1.0225,
      "step": 24220
    },
    {
      "epoch": 2.9112646440372485,
      "grad_norm": 2.992431402206421,
      "learning_rate": 9.035345208894608e-07,
      "loss": 1.0028,
      "step": 24230
    },
    {
      "epoch": 2.9124662060678883,
      "grad_norm": 3.0957372188568115,
      "learning_rate": 8.914713096626322e-07,
      "loss": 1.0301,
      "step": 24240
    },
    {
      "epoch": 2.913667768098528,
      "grad_norm": 3.5141193866729736,
      "learning_rate": 8.794080984358037e-07,
      "loss": 1.0691,
      "step": 24250
    },
    {
      "epoch": 2.914869330129168,
      "grad_norm": 2.923245906829834,
      "learning_rate": 8.67344887208975e-07,
      "loss": 1.0052,
      "step": 24260
    },
    {
      "epoch": 2.9160708921598077,
      "grad_norm": 3.045252561569214,
      "learning_rate": 8.552816759821465e-07,
      "loss": 1.0381,
      "step": 24270
    },
    {
      "epoch": 2.9172724541904476,
      "grad_norm": 3.385463237762451,
      "learning_rate": 8.432184647553179e-07,
      "loss": 1.1131,
      "step": 24280
    },
    {
      "epoch": 2.9184740162210874,
      "grad_norm": 3.1244730949401855,
      "learning_rate": 8.311552535284893e-07,
      "loss": 1.0462,
      "step": 24290
    },
    {
      "epoch": 2.919675578251727,
      "grad_norm": 3.1005730628967285,
      "learning_rate": 8.190920423016608e-07,
      "loss": 1.0612,
      "step": 24300
    },
    {
      "epoch": 2.920877140282367,
      "grad_norm": 3.648313522338867,
      "learning_rate": 8.070288310748321e-07,
      "loss": 1.0731,
      "step": 24310
    },
    {
      "epoch": 2.922078702313007,
      "grad_norm": 2.9084112644195557,
      "learning_rate": 7.949656198480036e-07,
      "loss": 1.0421,
      "step": 24320
    },
    {
      "epoch": 2.9232802643436466,
      "grad_norm": 3.2507705688476562,
      "learning_rate": 7.829024086211749e-07,
      "loss": 1.0006,
      "step": 24330
    },
    {
      "epoch": 2.9244818263742864,
      "grad_norm": 3.7412872314453125,
      "learning_rate": 7.708391973943463e-07,
      "loss": 1.0357,
      "step": 24340
    },
    {
      "epoch": 2.9256833884049263,
      "grad_norm": 3.1213958263397217,
      "learning_rate": 7.587759861675179e-07,
      "loss": 1.0236,
      "step": 24350
    },
    {
      "epoch": 2.926884950435566,
      "grad_norm": 2.914301633834839,
      "learning_rate": 7.467127749406892e-07,
      "loss": 1.0185,
      "step": 24360
    },
    {
      "epoch": 2.928086512466206,
      "grad_norm": 2.888965368270874,
      "learning_rate": 7.346495637138607e-07,
      "loss": 1.0148,
      "step": 24370
    },
    {
      "epoch": 2.9292880744968457,
      "grad_norm": 3.005418300628662,
      "learning_rate": 7.22586352487032e-07,
      "loss": 1.0002,
      "step": 24380
    },
    {
      "epoch": 2.9304896365274855,
      "grad_norm": 3.151660919189453,
      "learning_rate": 7.105231412602034e-07,
      "loss": 1.0651,
      "step": 24390
    },
    {
      "epoch": 2.9316911985581253,
      "grad_norm": 3.5254571437835693,
      "learning_rate": 6.98459930033375e-07,
      "loss": 1.0375,
      "step": 24400
    },
    {
      "epoch": 2.932892760588765,
      "grad_norm": 3.156951904296875,
      "learning_rate": 6.863967188065463e-07,
      "loss": 1.0498,
      "step": 24410
    },
    {
      "epoch": 2.9340943226194054,
      "grad_norm": 3.2657105922698975,
      "learning_rate": 6.743335075797178e-07,
      "loss": 1.0543,
      "step": 24420
    },
    {
      "epoch": 2.935295884650045,
      "grad_norm": 3.0072543621063232,
      "learning_rate": 6.622702963528891e-07,
      "loss": 1.0434,
      "step": 24430
    },
    {
      "epoch": 2.936497446680685,
      "grad_norm": 3.598799467086792,
      "learning_rate": 6.502070851260605e-07,
      "loss": 1.0673,
      "step": 24440
    },
    {
      "epoch": 2.937699008711325,
      "grad_norm": 3.440263271331787,
      "learning_rate": 6.381438738992321e-07,
      "loss": 1.1186,
      "step": 24450
    },
    {
      "epoch": 2.9389005707419646,
      "grad_norm": 3.447991132736206,
      "learning_rate": 6.260806626724034e-07,
      "loss": 1.0127,
      "step": 24460
    },
    {
      "epoch": 2.9401021327726045,
      "grad_norm": 3.4682698249816895,
      "learning_rate": 6.140174514455749e-07,
      "loss": 1.0547,
      "step": 24470
    },
    {
      "epoch": 2.9413036948032443,
      "grad_norm": 3.298367500305176,
      "learning_rate": 6.019542402187462e-07,
      "loss": 1.0701,
      "step": 24480
    },
    {
      "epoch": 2.942505256833884,
      "grad_norm": 3.507066488265991,
      "learning_rate": 5.898910289919176e-07,
      "loss": 1.0499,
      "step": 24490
    },
    {
      "epoch": 2.943706818864524,
      "grad_norm": 3.2029483318328857,
      "learning_rate": 5.778278177650891e-07,
      "loss": 1.0634,
      "step": 24500
    },
    {
      "epoch": 2.9449083808951637,
      "grad_norm": 2.913386344909668,
      "learning_rate": 5.657646065382605e-07,
      "loss": 1.1267,
      "step": 24510
    },
    {
      "epoch": 2.9461099429258035,
      "grad_norm": 3.794032335281372,
      "learning_rate": 5.53701395311432e-07,
      "loss": 1.0585,
      "step": 24520
    },
    {
      "epoch": 2.9473115049564433,
      "grad_norm": 3.575181245803833,
      "learning_rate": 5.416381840846033e-07,
      "loss": 1.0206,
      "step": 24530
    },
    {
      "epoch": 2.948513066987083,
      "grad_norm": 3.5688440799713135,
      "learning_rate": 5.295749728577747e-07,
      "loss": 1.0637,
      "step": 24540
    },
    {
      "epoch": 2.949714629017723,
      "grad_norm": 2.8980507850646973,
      "learning_rate": 5.175117616309462e-07,
      "loss": 1.034,
      "step": 24550
    },
    {
      "epoch": 2.950916191048363,
      "grad_norm": 3.045017719268799,
      "learning_rate": 5.054485504041176e-07,
      "loss": 1.0581,
      "step": 24560
    },
    {
      "epoch": 2.9521177530790026,
      "grad_norm": 3.4713826179504395,
      "learning_rate": 4.933853391772891e-07,
      "loss": 1.0477,
      "step": 24570
    },
    {
      "epoch": 2.9533193151096424,
      "grad_norm": 3.643965244293213,
      "learning_rate": 4.813221279504604e-07,
      "loss": 1.0025,
      "step": 24580
    },
    {
      "epoch": 2.954520877140282,
      "grad_norm": 3.265733003616333,
      "learning_rate": 4.6925891672363183e-07,
      "loss": 1.0406,
      "step": 24590
    },
    {
      "epoch": 2.955722439170922,
      "grad_norm": 3.095825672149658,
      "learning_rate": 4.571957054968033e-07,
      "loss": 1.065,
      "step": 24600
    },
    {
      "epoch": 2.9569240012015623,
      "grad_norm": 3.8412530422210693,
      "learning_rate": 4.4513249426997466e-07,
      "loss": 1.111,
      "step": 24610
    },
    {
      "epoch": 2.958125563232202,
      "grad_norm": 3.090414047241211,
      "learning_rate": 4.330692830431461e-07,
      "loss": 1.043,
      "step": 24620
    },
    {
      "epoch": 2.959327125262842,
      "grad_norm": 3.042034387588501,
      "learning_rate": 4.210060718163175e-07,
      "loss": 1.0661,
      "step": 24630
    },
    {
      "epoch": 2.9605286872934817,
      "grad_norm": 3.188507318496704,
      "learning_rate": 4.0894286058948893e-07,
      "loss": 1.0629,
      "step": 24640
    },
    {
      "epoch": 2.9617302493241215,
      "grad_norm": 3.2900121212005615,
      "learning_rate": 3.968796493626604e-07,
      "loss": 1.0541,
      "step": 24650
    },
    {
      "epoch": 2.9629318113547614,
      "grad_norm": 3.3059191703796387,
      "learning_rate": 3.8481643813583176e-07,
      "loss": 1.119,
      "step": 24660
    },
    {
      "epoch": 2.964133373385401,
      "grad_norm": 3.0419626235961914,
      "learning_rate": 3.727532269090032e-07,
      "loss": 1.0228,
      "step": 24670
    },
    {
      "epoch": 2.965334935416041,
      "grad_norm": 3.2152559757232666,
      "learning_rate": 3.606900156821746e-07,
      "loss": 1.0384,
      "step": 24680
    },
    {
      "epoch": 2.966536497446681,
      "grad_norm": 3.002877950668335,
      "learning_rate": 3.48626804455346e-07,
      "loss": 1.0387,
      "step": 24690
    },
    {
      "epoch": 2.9677380594773206,
      "grad_norm": 3.4246280193328857,
      "learning_rate": 3.3656359322851747e-07,
      "loss": 1.0399,
      "step": 24700
    },
    {
      "epoch": 2.9689396215079604,
      "grad_norm": 3.6091349124908447,
      "learning_rate": 3.2450038200168886e-07,
      "loss": 1.1292,
      "step": 24710
    },
    {
      "epoch": 2.9701411835386002,
      "grad_norm": 3.519932746887207,
      "learning_rate": 3.124371707748603e-07,
      "loss": 1.0439,
      "step": 24720
    },
    {
      "epoch": 2.97134274556924,
      "grad_norm": 2.8108911514282227,
      "learning_rate": 3.003739595480317e-07,
      "loss": 1.0413,
      "step": 24730
    },
    {
      "epoch": 2.97254430759988,
      "grad_norm": 2.726414680480957,
      "learning_rate": 2.883107483212031e-07,
      "loss": 1.0085,
      "step": 24740
    },
    {
      "epoch": 2.9737458696305197,
      "grad_norm": 3.0853183269500732,
      "learning_rate": 2.762475370943745e-07,
      "loss": 1.0319,
      "step": 24750
    },
    {
      "epoch": 2.9749474316611595,
      "grad_norm": 3.5884201526641846,
      "learning_rate": 2.6418432586754596e-07,
      "loss": 1.0196,
      "step": 24760
    },
    {
      "epoch": 2.9761489936917993,
      "grad_norm": 3.4383385181427,
      "learning_rate": 2.521211146407174e-07,
      "loss": 1.025,
      "step": 24770
    },
    {
      "epoch": 2.977350555722439,
      "grad_norm": 3.204343318939209,
      "learning_rate": 2.400579034138888e-07,
      "loss": 1.0347,
      "step": 24780
    },
    {
      "epoch": 2.978552117753079,
      "grad_norm": 3.213378667831421,
      "learning_rate": 2.279946921870602e-07,
      "loss": 1.0735,
      "step": 24790
    },
    {
      "epoch": 2.9797536797837187,
      "grad_norm": 3.4388742446899414,
      "learning_rate": 2.1593148096023162e-07,
      "loss": 1.0617,
      "step": 24800
    },
    {
      "epoch": 2.9809552418143586,
      "grad_norm": 3.3471367359161377,
      "learning_rate": 2.0386826973340303e-07,
      "loss": 1.1292,
      "step": 24810
    },
    {
      "epoch": 2.9821568038449984,
      "grad_norm": 3.8779520988464355,
      "learning_rate": 1.9180505850657445e-07,
      "loss": 1.1188,
      "step": 24820
    },
    {
      "epoch": 2.983358365875638,
      "grad_norm": 3.323080539703369,
      "learning_rate": 1.797418472797459e-07,
      "loss": 1.0217,
      "step": 24830
    },
    {
      "epoch": 2.984559927906278,
      "grad_norm": 2.926565170288086,
      "learning_rate": 1.6767863605291728e-07,
      "loss": 1.0305,
      "step": 24840
    },
    {
      "epoch": 2.985761489936918,
      "grad_norm": 3.042177677154541,
      "learning_rate": 1.556154248260887e-07,
      "loss": 1.0601,
      "step": 24850
    },
    {
      "epoch": 2.9869630519675576,
      "grad_norm": 3.371907949447632,
      "learning_rate": 1.4355221359926013e-07,
      "loss": 1.0573,
      "step": 24860
    },
    {
      "epoch": 2.9881646139981974,
      "grad_norm": 3.410357713699341,
      "learning_rate": 1.3148900237243155e-07,
      "loss": 0.9949,
      "step": 24870
    },
    {
      "epoch": 2.9893661760288373,
      "grad_norm": 3.5367281436920166,
      "learning_rate": 1.1942579114560296e-07,
      "loss": 1.0565,
      "step": 24880
    },
    {
      "epoch": 2.990567738059477,
      "grad_norm": 3.6123385429382324,
      "learning_rate": 1.0736257991877439e-07,
      "loss": 1.0689,
      "step": 24890
    },
    {
      "epoch": 2.991769300090117,
      "grad_norm": 3.4173836708068848,
      "learning_rate": 9.529936869194579e-08,
      "loss": 1.0642,
      "step": 24900
    },
    {
      "epoch": 2.992970862120757,
      "grad_norm": 3.9985105991363525,
      "learning_rate": 8.323615746511722e-08,
      "loss": 1.1036,
      "step": 24910
    },
    {
      "epoch": 2.994172424151397,
      "grad_norm": 2.9594972133636475,
      "learning_rate": 7.117294623828863e-08,
      "loss": 1.0475,
      "step": 24920
    },
    {
      "epoch": 2.9953739861820368,
      "grad_norm": 3.384648323059082,
      "learning_rate": 5.9109735011460054e-08,
      "loss": 1.0331,
      "step": 24930
    },
    {
      "epoch": 2.9965755482126766,
      "grad_norm": 3.1622586250305176,
      "learning_rate": 4.704652378463147e-08,
      "loss": 1.126,
      "step": 24940
    },
    {
      "epoch": 2.9977771102433164,
      "grad_norm": 3.184781074523926,
      "learning_rate": 3.498331255780289e-08,
      "loss": 1.0246,
      "step": 24950
    },
    {
      "epoch": 2.998978672273956,
      "grad_norm": 3.047919273376465,
      "learning_rate": 2.2920101330974308e-08,
      "loss": 1.0086,
      "step": 24960
    }
  ],
  "logging_steps": 10,
  "max_steps": 24969,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.905621546591846e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
