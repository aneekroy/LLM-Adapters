{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.064640831458538,
  "eval_steps": 500,
  "global_step": 16800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006337336417503723,
      "grad_norm": 1.9117872714996338,
      "learning_rate": 2.7e-06,
      "loss": 3.7576,
      "step": 10
    },
    {
      "epoch": 0.0012674672835007446,
      "grad_norm": 1.9629536867141724,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 3.7461,
      "step": 20
    },
    {
      "epoch": 0.0019012009252511169,
      "grad_norm": 1.8843337297439575,
      "learning_rate": 8.7e-06,
      "loss": 3.67,
      "step": 30
    },
    {
      "epoch": 0.002534934567001489,
      "grad_norm": 1.6954119205474854,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 3.7126,
      "step": 40
    },
    {
      "epoch": 0.0031686682087518616,
      "grad_norm": 1.913343071937561,
      "learning_rate": 1.47e-05,
      "loss": 3.5163,
      "step": 50
    },
    {
      "epoch": 0.0038024018505022337,
      "grad_norm": 1.8398901224136353,
      "learning_rate": 1.77e-05,
      "loss": 3.4468,
      "step": 60
    },
    {
      "epoch": 0.004436135492252607,
      "grad_norm": 2.3952956199645996,
      "learning_rate": 2.07e-05,
      "loss": 3.4141,
      "step": 70
    },
    {
      "epoch": 0.005069869134002978,
      "grad_norm": 2.5747063159942627,
      "learning_rate": 2.37e-05,
      "loss": 3.1859,
      "step": 80
    },
    {
      "epoch": 0.005703602775753351,
      "grad_norm": 2.613248586654663,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.9878,
      "step": 90
    },
    {
      "epoch": 0.006337336417503723,
      "grad_norm": 3.0131685733795166,
      "learning_rate": 2.97e-05,
      "loss": 2.6839,
      "step": 100
    },
    {
      "epoch": 0.006971070059254096,
      "grad_norm": 3.628626585006714,
      "learning_rate": 2.999428450465707e-05,
      "loss": 2.3129,
      "step": 110
    },
    {
      "epoch": 0.007604803701004467,
      "grad_norm": 3.504011869430542,
      "learning_rate": 2.9987933954276038e-05,
      "loss": 1.9783,
      "step": 120
    },
    {
      "epoch": 0.00823853734275484,
      "grad_norm": 3.0373315811157227,
      "learning_rate": 2.9981583403895007e-05,
      "loss": 1.6821,
      "step": 130
    },
    {
      "epoch": 0.008872270984505213,
      "grad_norm": 2.904947519302368,
      "learning_rate": 2.997523285351397e-05,
      "loss": 1.4791,
      "step": 140
    },
    {
      "epoch": 0.009506004626255584,
      "grad_norm": 4.103780269622803,
      "learning_rate": 2.9968882303132937e-05,
      "loss": 1.3516,
      "step": 150
    },
    {
      "epoch": 0.010139738268005957,
      "grad_norm": 2.2443034648895264,
      "learning_rate": 2.9962531752751907e-05,
      "loss": 1.2661,
      "step": 160
    },
    {
      "epoch": 0.010773471909756329,
      "grad_norm": 2.082948923110962,
      "learning_rate": 2.9956181202370873e-05,
      "loss": 1.2268,
      "step": 170
    },
    {
      "epoch": 0.011407205551506702,
      "grad_norm": 2.696936845779419,
      "learning_rate": 2.994983065198984e-05,
      "loss": 1.194,
      "step": 180
    },
    {
      "epoch": 0.012040939193257074,
      "grad_norm": 2.160275459289551,
      "learning_rate": 2.9943480101608806e-05,
      "loss": 1.1856,
      "step": 190
    },
    {
      "epoch": 0.012674672835007447,
      "grad_norm": 1.7468996047973633,
      "learning_rate": 2.9937129551227772e-05,
      "loss": 1.1432,
      "step": 200
    },
    {
      "epoch": 0.013308406476757819,
      "grad_norm": 1.9496897459030151,
      "learning_rate": 2.9930779000846742e-05,
      "loss": 1.1428,
      "step": 210
    },
    {
      "epoch": 0.013942140118508192,
      "grad_norm": 1.8727227449417114,
      "learning_rate": 2.992442845046571e-05,
      "loss": 1.11,
      "step": 220
    },
    {
      "epoch": 0.014575873760258564,
      "grad_norm": 1.8571088314056396,
      "learning_rate": 2.9918077900084675e-05,
      "loss": 1.1374,
      "step": 230
    },
    {
      "epoch": 0.015209607402008935,
      "grad_norm": 2.1544878482818604,
      "learning_rate": 2.991172734970364e-05,
      "loss": 1.116,
      "step": 240
    },
    {
      "epoch": 0.01584334104375931,
      "grad_norm": 1.8210844993591309,
      "learning_rate": 2.9905376799322608e-05,
      "loss": 1.1418,
      "step": 250
    },
    {
      "epoch": 0.01647707468550968,
      "grad_norm": 2.422865390777588,
      "learning_rate": 2.9899026248941574e-05,
      "loss": 1.1154,
      "step": 260
    },
    {
      "epoch": 0.017110808327260054,
      "grad_norm": 2.2297041416168213,
      "learning_rate": 2.9892675698560544e-05,
      "loss": 1.1005,
      "step": 270
    },
    {
      "epoch": 0.017744541969010427,
      "grad_norm": 2.2503745555877686,
      "learning_rate": 2.988632514817951e-05,
      "loss": 1.1489,
      "step": 280
    },
    {
      "epoch": 0.018378275610760796,
      "grad_norm": 2.0805060863494873,
      "learning_rate": 2.9879974597798474e-05,
      "loss": 1.062,
      "step": 290
    },
    {
      "epoch": 0.019012009252511168,
      "grad_norm": 2.4523537158966064,
      "learning_rate": 2.9873624047417443e-05,
      "loss": 1.1196,
      "step": 300
    },
    {
      "epoch": 0.01964574289426154,
      "grad_norm": 2.0582127571105957,
      "learning_rate": 2.986727349703641e-05,
      "loss": 1.1173,
      "step": 310
    },
    {
      "epoch": 0.020279476536011913,
      "grad_norm": 1.9782936573028564,
      "learning_rate": 2.9860922946655376e-05,
      "loss": 1.081,
      "step": 320
    },
    {
      "epoch": 0.020913210177762286,
      "grad_norm": 1.9162189960479736,
      "learning_rate": 2.9854572396274346e-05,
      "loss": 1.1322,
      "step": 330
    },
    {
      "epoch": 0.021546943819512658,
      "grad_norm": 2.451296091079712,
      "learning_rate": 2.9848221845893313e-05,
      "loss": 1.101,
      "step": 340
    },
    {
      "epoch": 0.02218067746126303,
      "grad_norm": 2.363271474838257,
      "learning_rate": 2.984187129551228e-05,
      "loss": 1.0976,
      "step": 350
    },
    {
      "epoch": 0.022814411103013403,
      "grad_norm": 2.8077797889709473,
      "learning_rate": 2.9835520745131245e-05,
      "loss": 1.1167,
      "step": 360
    },
    {
      "epoch": 0.023448144744763776,
      "grad_norm": 1.8548303842544556,
      "learning_rate": 2.9829170194750212e-05,
      "loss": 1.0899,
      "step": 370
    },
    {
      "epoch": 0.024081878386514148,
      "grad_norm": 2.128476858139038,
      "learning_rate": 2.982281964436918e-05,
      "loss": 1.0929,
      "step": 380
    },
    {
      "epoch": 0.02471561202826452,
      "grad_norm": 1.7355762720108032,
      "learning_rate": 2.9816469093988148e-05,
      "loss": 1.0846,
      "step": 390
    },
    {
      "epoch": 0.025349345670014893,
      "grad_norm": 1.9736723899841309,
      "learning_rate": 2.981011854360711e-05,
      "loss": 1.0459,
      "step": 400
    },
    {
      "epoch": 0.025983079311765266,
      "grad_norm": 2.735546112060547,
      "learning_rate": 2.980376799322608e-05,
      "loss": 1.0525,
      "step": 410
    },
    {
      "epoch": 0.026616812953515638,
      "grad_norm": 1.8927018642425537,
      "learning_rate": 2.9797417442845047e-05,
      "loss": 1.0875,
      "step": 420
    },
    {
      "epoch": 0.02725054659526601,
      "grad_norm": 2.2376179695129395,
      "learning_rate": 2.9791066892464014e-05,
      "loss": 1.1144,
      "step": 430
    },
    {
      "epoch": 0.027884280237016383,
      "grad_norm": 2.68056321144104,
      "learning_rate": 2.9784716342082984e-05,
      "loss": 1.0873,
      "step": 440
    },
    {
      "epoch": 0.028518013878766756,
      "grad_norm": 2.5218887329101562,
      "learning_rate": 2.9778365791701947e-05,
      "loss": 1.1174,
      "step": 450
    },
    {
      "epoch": 0.029151747520517128,
      "grad_norm": 2.2011311054229736,
      "learning_rate": 2.9772015241320913e-05,
      "loss": 1.0863,
      "step": 460
    },
    {
      "epoch": 0.0297854811622675,
      "grad_norm": 2.162109613418579,
      "learning_rate": 2.9765664690939883e-05,
      "loss": 1.0938,
      "step": 470
    },
    {
      "epoch": 0.03041921480401787,
      "grad_norm": 1.846325397491455,
      "learning_rate": 2.975931414055885e-05,
      "loss": 1.0804,
      "step": 480
    },
    {
      "epoch": 0.031052948445768242,
      "grad_norm": 1.5710378885269165,
      "learning_rate": 2.9752963590177816e-05,
      "loss": 1.0729,
      "step": 490
    },
    {
      "epoch": 0.03168668208751862,
      "grad_norm": 2.069298505783081,
      "learning_rate": 2.9746613039796782e-05,
      "loss": 1.0757,
      "step": 500
    },
    {
      "epoch": 0.03232041572926899,
      "grad_norm": 2.619649887084961,
      "learning_rate": 2.974026248941575e-05,
      "loss": 1.0518,
      "step": 510
    },
    {
      "epoch": 0.03295414937101936,
      "grad_norm": 2.3792941570281982,
      "learning_rate": 2.973391193903472e-05,
      "loss": 1.08,
      "step": 520
    },
    {
      "epoch": 0.033587883012769736,
      "grad_norm": 1.7744858264923096,
      "learning_rate": 2.9727561388653685e-05,
      "loss": 1.0544,
      "step": 530
    },
    {
      "epoch": 0.03422161665452011,
      "grad_norm": 2.064081907272339,
      "learning_rate": 2.972121083827265e-05,
      "loss": 1.096,
      "step": 540
    },
    {
      "epoch": 0.03485535029627048,
      "grad_norm": 2.3793511390686035,
      "learning_rate": 2.9714860287891618e-05,
      "loss": 1.0673,
      "step": 550
    },
    {
      "epoch": 0.03548908393802085,
      "grad_norm": 2.490527629852295,
      "learning_rate": 2.9708509737510584e-05,
      "loss": 1.0479,
      "step": 560
    },
    {
      "epoch": 0.036122817579771226,
      "grad_norm": 1.9669108390808105,
      "learning_rate": 2.970215918712955e-05,
      "loss": 1.0919,
      "step": 570
    },
    {
      "epoch": 0.03675655122152159,
      "grad_norm": 2.5692787170410156,
      "learning_rate": 2.969580863674852e-05,
      "loss": 1.0651,
      "step": 580
    },
    {
      "epoch": 0.037390284863271964,
      "grad_norm": 1.9911679029464722,
      "learning_rate": 2.9689458086367487e-05,
      "loss": 1.0446,
      "step": 590
    },
    {
      "epoch": 0.038024018505022336,
      "grad_norm": 2.396453380584717,
      "learning_rate": 2.9683107535986453e-05,
      "loss": 1.0775,
      "step": 600
    },
    {
      "epoch": 0.03865775214677271,
      "grad_norm": 2.606339931488037,
      "learning_rate": 2.967675698560542e-05,
      "loss": 1.0804,
      "step": 610
    },
    {
      "epoch": 0.03929148578852308,
      "grad_norm": 2.1065726280212402,
      "learning_rate": 2.9670406435224386e-05,
      "loss": 1.0934,
      "step": 620
    },
    {
      "epoch": 0.039925219430273454,
      "grad_norm": 1.9628839492797852,
      "learning_rate": 2.9664055884843353e-05,
      "loss": 1.032,
      "step": 630
    },
    {
      "epoch": 0.040558953072023826,
      "grad_norm": 2.4476349353790283,
      "learning_rate": 2.9657705334462322e-05,
      "loss": 1.052,
      "step": 640
    },
    {
      "epoch": 0.0411926867137742,
      "grad_norm": 2.4301021099090576,
      "learning_rate": 2.965135478408129e-05,
      "loss": 1.0703,
      "step": 650
    },
    {
      "epoch": 0.04182642035552457,
      "grad_norm": 1.9761780500411987,
      "learning_rate": 2.9645004233700255e-05,
      "loss": 1.0648,
      "step": 660
    },
    {
      "epoch": 0.042460153997274944,
      "grad_norm": 2.5984835624694824,
      "learning_rate": 2.963865368331922e-05,
      "loss": 1.0509,
      "step": 670
    },
    {
      "epoch": 0.043093887639025316,
      "grad_norm": 2.4108059406280518,
      "learning_rate": 2.9632303132938188e-05,
      "loss": 1.0826,
      "step": 680
    },
    {
      "epoch": 0.04372762128077569,
      "grad_norm": 2.240973472595215,
      "learning_rate": 2.9625952582557158e-05,
      "loss": 1.0712,
      "step": 690
    },
    {
      "epoch": 0.04436135492252606,
      "grad_norm": 2.962956190109253,
      "learning_rate": 2.9619602032176124e-05,
      "loss": 1.0677,
      "step": 700
    },
    {
      "epoch": 0.044995088564276434,
      "grad_norm": 2.5757505893707275,
      "learning_rate": 2.9613251481795087e-05,
      "loss": 1.0226,
      "step": 710
    },
    {
      "epoch": 0.045628822206026806,
      "grad_norm": 1.820131540298462,
      "learning_rate": 2.9606900931414057e-05,
      "loss": 1.002,
      "step": 720
    },
    {
      "epoch": 0.04626255584777718,
      "grad_norm": 2.5515904426574707,
      "learning_rate": 2.9600550381033024e-05,
      "loss": 1.0122,
      "step": 730
    },
    {
      "epoch": 0.04689628948952755,
      "grad_norm": 2.3855221271514893,
      "learning_rate": 2.959419983065199e-05,
      "loss": 1.0609,
      "step": 740
    },
    {
      "epoch": 0.047530023131277924,
      "grad_norm": 2.1727123260498047,
      "learning_rate": 2.958784928027096e-05,
      "loss": 1.0659,
      "step": 750
    },
    {
      "epoch": 0.048163756773028296,
      "grad_norm": 2.3179781436920166,
      "learning_rate": 2.9581498729889923e-05,
      "loss": 1.0875,
      "step": 760
    },
    {
      "epoch": 0.04879749041477867,
      "grad_norm": 2.2498719692230225,
      "learning_rate": 2.957514817950889e-05,
      "loss": 1.0629,
      "step": 770
    },
    {
      "epoch": 0.04943122405652904,
      "grad_norm": 2.6589295864105225,
      "learning_rate": 2.956879762912786e-05,
      "loss": 1.0185,
      "step": 780
    },
    {
      "epoch": 0.050064957698279414,
      "grad_norm": 3.0515198707580566,
      "learning_rate": 2.9562447078746826e-05,
      "loss": 1.0486,
      "step": 790
    },
    {
      "epoch": 0.050698691340029786,
      "grad_norm": 2.1008763313293457,
      "learning_rate": 2.9556096528365792e-05,
      "loss": 1.0436,
      "step": 800
    },
    {
      "epoch": 0.05133242498178016,
      "grad_norm": 2.712459087371826,
      "learning_rate": 2.954974597798476e-05,
      "loss": 1.0802,
      "step": 810
    },
    {
      "epoch": 0.05196615862353053,
      "grad_norm": 1.7878366708755493,
      "learning_rate": 2.9543395427603725e-05,
      "loss": 1.0396,
      "step": 820
    },
    {
      "epoch": 0.052599892265280904,
      "grad_norm": 2.8222696781158447,
      "learning_rate": 2.9537044877222695e-05,
      "loss": 1.0783,
      "step": 830
    },
    {
      "epoch": 0.053233625907031276,
      "grad_norm": 2.4172685146331787,
      "learning_rate": 2.953069432684166e-05,
      "loss": 1.0266,
      "step": 840
    },
    {
      "epoch": 0.05386735954878165,
      "grad_norm": 2.337292194366455,
      "learning_rate": 2.9524343776460627e-05,
      "loss": 1.0719,
      "step": 850
    },
    {
      "epoch": 0.05450109319053202,
      "grad_norm": 1.97150719165802,
      "learning_rate": 2.9517993226079597e-05,
      "loss": 1.0655,
      "step": 860
    },
    {
      "epoch": 0.055134826832282394,
      "grad_norm": 2.312532901763916,
      "learning_rate": 2.951164267569856e-05,
      "loss": 0.9907,
      "step": 870
    },
    {
      "epoch": 0.055768560474032766,
      "grad_norm": 2.0114686489105225,
      "learning_rate": 2.9505292125317527e-05,
      "loss": 1.0653,
      "step": 880
    },
    {
      "epoch": 0.05640229411578314,
      "grad_norm": 3.0803422927856445,
      "learning_rate": 2.9498941574936497e-05,
      "loss": 1.0396,
      "step": 890
    },
    {
      "epoch": 0.05703602775753351,
      "grad_norm": 2.489912748336792,
      "learning_rate": 2.9492591024555463e-05,
      "loss": 1.0297,
      "step": 900
    },
    {
      "epoch": 0.057669761399283884,
      "grad_norm": 1.7714165449142456,
      "learning_rate": 2.948624047417443e-05,
      "loss": 1.0403,
      "step": 910
    },
    {
      "epoch": 0.058303495041034256,
      "grad_norm": 2.1479690074920654,
      "learning_rate": 2.9479889923793396e-05,
      "loss": 1.0398,
      "step": 920
    },
    {
      "epoch": 0.05893722868278463,
      "grad_norm": 2.6930530071258545,
      "learning_rate": 2.9473539373412362e-05,
      "loss": 1.0291,
      "step": 930
    },
    {
      "epoch": 0.059570962324535,
      "grad_norm": 2.432041883468628,
      "learning_rate": 2.946718882303133e-05,
      "loss": 1.0329,
      "step": 940
    },
    {
      "epoch": 0.060204695966285374,
      "grad_norm": 2.5477395057678223,
      "learning_rate": 2.94608382726503e-05,
      "loss": 1.0672,
      "step": 950
    },
    {
      "epoch": 0.06083842960803574,
      "grad_norm": 2.7343316078186035,
      "learning_rate": 2.9454487722269265e-05,
      "loss": 1.0453,
      "step": 960
    },
    {
      "epoch": 0.06147216324978611,
      "grad_norm": 2.1107900142669678,
      "learning_rate": 2.944813717188823e-05,
      "loss": 1.0618,
      "step": 970
    },
    {
      "epoch": 0.062105896891536484,
      "grad_norm": 2.133950710296631,
      "learning_rate": 2.9441786621507198e-05,
      "loss": 1.0506,
      "step": 980
    },
    {
      "epoch": 0.06273963053328686,
      "grad_norm": 3.080186128616333,
      "learning_rate": 2.9435436071126164e-05,
      "loss": 1.0593,
      "step": 990
    },
    {
      "epoch": 0.06337336417503724,
      "grad_norm": 2.077913284301758,
      "learning_rate": 2.9429085520745134e-05,
      "loss": 1.0267,
      "step": 1000
    },
    {
      "epoch": 0.06400709781678761,
      "grad_norm": 2.3823487758636475,
      "learning_rate": 2.94227349703641e-05,
      "loss": 1.0376,
      "step": 1010
    },
    {
      "epoch": 0.06464083145853798,
      "grad_norm": 2.1436007022857666,
      "learning_rate": 2.9416384419983064e-05,
      "loss": 1.0524,
      "step": 1020
    },
    {
      "epoch": 0.06527456510028835,
      "grad_norm": 1.9627431631088257,
      "learning_rate": 2.9410033869602033e-05,
      "loss": 1.0126,
      "step": 1030
    },
    {
      "epoch": 0.06590829874203873,
      "grad_norm": 2.6993441581726074,
      "learning_rate": 2.9403683319221e-05,
      "loss": 1.0569,
      "step": 1040
    },
    {
      "epoch": 0.0665420323837891,
      "grad_norm": 2.4357666969299316,
      "learning_rate": 2.9397332768839966e-05,
      "loss": 1.0464,
      "step": 1050
    },
    {
      "epoch": 0.06717576602553947,
      "grad_norm": 1.8371834754943848,
      "learning_rate": 2.9390982218458936e-05,
      "loss": 1.0293,
      "step": 1060
    },
    {
      "epoch": 0.06780949966728984,
      "grad_norm": 2.702312469482422,
      "learning_rate": 2.93846316680779e-05,
      "loss": 1.0385,
      "step": 1070
    },
    {
      "epoch": 0.06844323330904022,
      "grad_norm": 2.127427339553833,
      "learning_rate": 2.9378281117696865e-05,
      "loss": 1.0152,
      "step": 1080
    },
    {
      "epoch": 0.06907696695079059,
      "grad_norm": 2.0491347312927246,
      "learning_rate": 2.9371930567315835e-05,
      "loss": 1.011,
      "step": 1090
    },
    {
      "epoch": 0.06971070059254096,
      "grad_norm": 2.197349786758423,
      "learning_rate": 2.93655800169348e-05,
      "loss": 1.0406,
      "step": 1100
    },
    {
      "epoch": 0.07034443423429133,
      "grad_norm": 2.3717641830444336,
      "learning_rate": 2.9359229466553768e-05,
      "loss": 1.0062,
      "step": 1110
    },
    {
      "epoch": 0.0709781678760417,
      "grad_norm": 3.063239574432373,
      "learning_rate": 2.9352878916172738e-05,
      "loss": 1.036,
      "step": 1120
    },
    {
      "epoch": 0.07161190151779208,
      "grad_norm": 1.8244127035140991,
      "learning_rate": 2.93465283657917e-05,
      "loss": 0.9955,
      "step": 1130
    },
    {
      "epoch": 0.07224563515954245,
      "grad_norm": 2.032191276550293,
      "learning_rate": 2.934017781541067e-05,
      "loss": 1.0243,
      "step": 1140
    },
    {
      "epoch": 0.07287936880129281,
      "grad_norm": 2.129345178604126,
      "learning_rate": 2.9333827265029637e-05,
      "loss": 1.0795,
      "step": 1150
    },
    {
      "epoch": 0.07351310244304318,
      "grad_norm": 1.9811575412750244,
      "learning_rate": 2.9327476714648604e-05,
      "loss": 1.0478,
      "step": 1160
    },
    {
      "epoch": 0.07414683608479355,
      "grad_norm": 2.0752980709075928,
      "learning_rate": 2.9321126164267573e-05,
      "loss": 1.0186,
      "step": 1170
    },
    {
      "epoch": 0.07478056972654393,
      "grad_norm": 2.1868174076080322,
      "learning_rate": 2.9314775613886536e-05,
      "loss": 0.9826,
      "step": 1180
    },
    {
      "epoch": 0.0754143033682943,
      "grad_norm": 2.469831705093384,
      "learning_rate": 2.9308425063505503e-05,
      "loss": 1.0274,
      "step": 1190
    },
    {
      "epoch": 0.07604803701004467,
      "grad_norm": 2.5827648639678955,
      "learning_rate": 2.9302074513124473e-05,
      "loss": 1.0128,
      "step": 1200
    },
    {
      "epoch": 0.07668177065179504,
      "grad_norm": 2.326472043991089,
      "learning_rate": 2.929572396274344e-05,
      "loss": 0.9925,
      "step": 1210
    },
    {
      "epoch": 0.07731550429354542,
      "grad_norm": 2.7983322143554688,
      "learning_rate": 2.9289373412362406e-05,
      "loss": 1.0568,
      "step": 1220
    },
    {
      "epoch": 0.07794923793529579,
      "grad_norm": 2.0109000205993652,
      "learning_rate": 2.9283022861981372e-05,
      "loss": 1.0303,
      "step": 1230
    },
    {
      "epoch": 0.07858297157704616,
      "grad_norm": 2.0318307876586914,
      "learning_rate": 2.927667231160034e-05,
      "loss": 1.0425,
      "step": 1240
    },
    {
      "epoch": 0.07921670521879653,
      "grad_norm": 2.2090444564819336,
      "learning_rate": 2.9270321761219305e-05,
      "loss": 1.0086,
      "step": 1250
    },
    {
      "epoch": 0.07985043886054691,
      "grad_norm": 2.1038131713867188,
      "learning_rate": 2.9263971210838275e-05,
      "loss": 1.0508,
      "step": 1260
    },
    {
      "epoch": 0.08048417250229728,
      "grad_norm": 2.262449026107788,
      "learning_rate": 2.925762066045724e-05,
      "loss": 1.1044,
      "step": 1270
    },
    {
      "epoch": 0.08111790614404765,
      "grad_norm": 2.298631429672241,
      "learning_rate": 2.9251270110076208e-05,
      "loss": 0.9896,
      "step": 1280
    },
    {
      "epoch": 0.08175163978579802,
      "grad_norm": 2.6024832725524902,
      "learning_rate": 2.9244919559695174e-05,
      "loss": 1.0513,
      "step": 1290
    },
    {
      "epoch": 0.0823853734275484,
      "grad_norm": 2.634636640548706,
      "learning_rate": 2.923856900931414e-05,
      "loss": 1.0158,
      "step": 1300
    },
    {
      "epoch": 0.08301910706929877,
      "grad_norm": 2.281294584274292,
      "learning_rate": 2.923221845893311e-05,
      "loss": 1.0296,
      "step": 1310
    },
    {
      "epoch": 0.08365284071104914,
      "grad_norm": 2.2298431396484375,
      "learning_rate": 2.9225867908552077e-05,
      "loss": 1.0166,
      "step": 1320
    },
    {
      "epoch": 0.08428657435279951,
      "grad_norm": 2.2944416999816895,
      "learning_rate": 2.921951735817104e-05,
      "loss": 1.0485,
      "step": 1330
    },
    {
      "epoch": 0.08492030799454989,
      "grad_norm": 2.9307198524475098,
      "learning_rate": 2.921316680779001e-05,
      "loss": 1.0507,
      "step": 1340
    },
    {
      "epoch": 0.08555404163630026,
      "grad_norm": 2.504673719406128,
      "learning_rate": 2.9206816257408976e-05,
      "loss": 0.9956,
      "step": 1350
    },
    {
      "epoch": 0.08618777527805063,
      "grad_norm": 2.2404143810272217,
      "learning_rate": 2.9200465707027942e-05,
      "loss": 0.9851,
      "step": 1360
    },
    {
      "epoch": 0.086821508919801,
      "grad_norm": 2.597153425216675,
      "learning_rate": 2.9194115156646912e-05,
      "loss": 0.9986,
      "step": 1370
    },
    {
      "epoch": 0.08745524256155138,
      "grad_norm": 1.9478813409805298,
      "learning_rate": 2.918776460626588e-05,
      "loss": 1.0381,
      "step": 1380
    },
    {
      "epoch": 0.08808897620330175,
      "grad_norm": 1.919011116027832,
      "learning_rate": 2.918141405588484e-05,
      "loss": 1.0492,
      "step": 1390
    },
    {
      "epoch": 0.08872270984505212,
      "grad_norm": 2.6251440048217773,
      "learning_rate": 2.917506350550381e-05,
      "loss": 1.0348,
      "step": 1400
    },
    {
      "epoch": 0.0893564434868025,
      "grad_norm": 2.4770138263702393,
      "learning_rate": 2.9168712955122778e-05,
      "loss": 1.0616,
      "step": 1410
    },
    {
      "epoch": 0.08999017712855287,
      "grad_norm": 2.1502511501312256,
      "learning_rate": 2.9162362404741744e-05,
      "loss": 1.0489,
      "step": 1420
    },
    {
      "epoch": 0.09062391077030324,
      "grad_norm": 2.9368698596954346,
      "learning_rate": 2.9156011854360714e-05,
      "loss": 1.1177,
      "step": 1430
    },
    {
      "epoch": 0.09125764441205361,
      "grad_norm": 2.6200106143951416,
      "learning_rate": 2.9149661303979677e-05,
      "loss": 1.0038,
      "step": 1440
    },
    {
      "epoch": 0.09189137805380398,
      "grad_norm": 2.2277140617370605,
      "learning_rate": 2.9143310753598647e-05,
      "loss": 0.9964,
      "step": 1450
    },
    {
      "epoch": 0.09252511169555436,
      "grad_norm": 2.1982579231262207,
      "learning_rate": 2.9136960203217613e-05,
      "loss": 0.996,
      "step": 1460
    },
    {
      "epoch": 0.09315884533730473,
      "grad_norm": 2.203191041946411,
      "learning_rate": 2.913060965283658e-05,
      "loss": 1.0319,
      "step": 1470
    },
    {
      "epoch": 0.0937925789790551,
      "grad_norm": 2.5706727504730225,
      "learning_rate": 2.912425910245555e-05,
      "loss": 1.0235,
      "step": 1480
    },
    {
      "epoch": 0.09442631262080547,
      "grad_norm": 2.6622090339660645,
      "learning_rate": 2.9117908552074513e-05,
      "loss": 1.0149,
      "step": 1490
    },
    {
      "epoch": 0.09506004626255585,
      "grad_norm": 2.948490858078003,
      "learning_rate": 2.911155800169348e-05,
      "loss": 0.9811,
      "step": 1500
    },
    {
      "epoch": 0.09569377990430622,
      "grad_norm": 2.039179563522339,
      "learning_rate": 2.910520745131245e-05,
      "loss": 0.9769,
      "step": 1510
    },
    {
      "epoch": 0.09632751354605659,
      "grad_norm": 2.604874849319458,
      "learning_rate": 2.9098856900931415e-05,
      "loss": 1.0101,
      "step": 1520
    },
    {
      "epoch": 0.09696124718780696,
      "grad_norm": 2.2408905029296875,
      "learning_rate": 2.9092506350550382e-05,
      "loss": 1.0278,
      "step": 1530
    },
    {
      "epoch": 0.09759498082955734,
      "grad_norm": 2.109778881072998,
      "learning_rate": 2.9086155800169348e-05,
      "loss": 1.0007,
      "step": 1540
    },
    {
      "epoch": 0.09822871447130771,
      "grad_norm": 2.0859289169311523,
      "learning_rate": 2.9079805249788315e-05,
      "loss": 0.9822,
      "step": 1550
    },
    {
      "epoch": 0.09886244811305808,
      "grad_norm": 3.751253843307495,
      "learning_rate": 2.907345469940728e-05,
      "loss": 1.0413,
      "step": 1560
    },
    {
      "epoch": 0.09949618175480845,
      "grad_norm": 2.790252685546875,
      "learning_rate": 2.906710414902625e-05,
      "loss": 1.0408,
      "step": 1570
    },
    {
      "epoch": 0.10012991539655883,
      "grad_norm": 1.9749186038970947,
      "learning_rate": 2.9060753598645217e-05,
      "loss": 1.0281,
      "step": 1580
    },
    {
      "epoch": 0.1007636490383092,
      "grad_norm": 2.2959342002868652,
      "learning_rate": 2.9054403048264184e-05,
      "loss": 1.0248,
      "step": 1590
    },
    {
      "epoch": 0.10139738268005957,
      "grad_norm": 2.1232380867004395,
      "learning_rate": 2.904805249788315e-05,
      "loss": 1.0823,
      "step": 1600
    },
    {
      "epoch": 0.10203111632180994,
      "grad_norm": 2.3029565811157227,
      "learning_rate": 2.9041701947502117e-05,
      "loss": 1.0314,
      "step": 1610
    },
    {
      "epoch": 0.10266484996356032,
      "grad_norm": 2.7153608798980713,
      "learning_rate": 2.9035351397121086e-05,
      "loss": 1.0769,
      "step": 1620
    },
    {
      "epoch": 0.10329858360531069,
      "grad_norm": 2.5098183155059814,
      "learning_rate": 2.9029000846740053e-05,
      "loss": 1.0129,
      "step": 1630
    },
    {
      "epoch": 0.10393231724706106,
      "grad_norm": 2.3358700275421143,
      "learning_rate": 2.902265029635902e-05,
      "loss": 1.0009,
      "step": 1640
    },
    {
      "epoch": 0.10456605088881143,
      "grad_norm": 2.184455394744873,
      "learning_rate": 2.9016299745977986e-05,
      "loss": 1.0316,
      "step": 1650
    },
    {
      "epoch": 0.10519978453056181,
      "grad_norm": 2.5969958305358887,
      "learning_rate": 2.9009949195596952e-05,
      "loss": 1.0711,
      "step": 1660
    },
    {
      "epoch": 0.10583351817231218,
      "grad_norm": 2.6478588581085205,
      "learning_rate": 2.900359864521592e-05,
      "loss": 0.9916,
      "step": 1670
    },
    {
      "epoch": 0.10646725181406255,
      "grad_norm": 2.3887476921081543,
      "learning_rate": 2.899724809483489e-05,
      "loss": 0.9919,
      "step": 1680
    },
    {
      "epoch": 0.10710098545581292,
      "grad_norm": 2.0973358154296875,
      "learning_rate": 2.8990897544453855e-05,
      "loss": 1.0487,
      "step": 1690
    },
    {
      "epoch": 0.1077347190975633,
      "grad_norm": 2.454986095428467,
      "learning_rate": 2.8984546994072818e-05,
      "loss": 0.9949,
      "step": 1700
    },
    {
      "epoch": 0.10836845273931367,
      "grad_norm": 3.4651479721069336,
      "learning_rate": 2.8978196443691788e-05,
      "loss": 0.9914,
      "step": 1710
    },
    {
      "epoch": 0.10900218638106404,
      "grad_norm": 1.9965912103652954,
      "learning_rate": 2.8971845893310754e-05,
      "loss": 1.024,
      "step": 1720
    },
    {
      "epoch": 0.10963592002281441,
      "grad_norm": 2.4559950828552246,
      "learning_rate": 2.896549534292972e-05,
      "loss": 1.0404,
      "step": 1730
    },
    {
      "epoch": 0.11026965366456479,
      "grad_norm": 2.047884941101074,
      "learning_rate": 2.895914479254869e-05,
      "loss": 1.0199,
      "step": 1740
    },
    {
      "epoch": 0.11090338730631516,
      "grad_norm": 2.149149179458618,
      "learning_rate": 2.8952794242167653e-05,
      "loss": 1.0024,
      "step": 1750
    },
    {
      "epoch": 0.11153712094806553,
      "grad_norm": 2.2531871795654297,
      "learning_rate": 2.8946443691786623e-05,
      "loss": 1.0232,
      "step": 1760
    },
    {
      "epoch": 0.1121708545898159,
      "grad_norm": 2.634312629699707,
      "learning_rate": 2.894009314140559e-05,
      "loss": 0.9989,
      "step": 1770
    },
    {
      "epoch": 0.11280458823156628,
      "grad_norm": 2.1846442222595215,
      "learning_rate": 2.8933742591024556e-05,
      "loss": 1.0066,
      "step": 1780
    },
    {
      "epoch": 0.11343832187331665,
      "grad_norm": 2.563927173614502,
      "learning_rate": 2.8927392040643526e-05,
      "loss": 0.9853,
      "step": 1790
    },
    {
      "epoch": 0.11407205551506702,
      "grad_norm": 2.242816209793091,
      "learning_rate": 2.892104149026249e-05,
      "loss": 1.0335,
      "step": 1800
    },
    {
      "epoch": 0.1147057891568174,
      "grad_norm": 2.1806468963623047,
      "learning_rate": 2.8914690939881455e-05,
      "loss": 0.9899,
      "step": 1810
    },
    {
      "epoch": 0.11533952279856777,
      "grad_norm": 2.064244508743286,
      "learning_rate": 2.8908340389500425e-05,
      "loss": 1.0283,
      "step": 1820
    },
    {
      "epoch": 0.11597325644031814,
      "grad_norm": 2.2729651927948,
      "learning_rate": 2.890198983911939e-05,
      "loss": 1.0092,
      "step": 1830
    },
    {
      "epoch": 0.11660699008206851,
      "grad_norm": 2.1567416191101074,
      "learning_rate": 2.8895639288738358e-05,
      "loss": 1.0146,
      "step": 1840
    },
    {
      "epoch": 0.11724072372381888,
      "grad_norm": 2.5029704570770264,
      "learning_rate": 2.8889288738357324e-05,
      "loss": 1.0459,
      "step": 1850
    },
    {
      "epoch": 0.11787445736556926,
      "grad_norm": 2.025200128555298,
      "learning_rate": 2.888293818797629e-05,
      "loss": 1.0257,
      "step": 1860
    },
    {
      "epoch": 0.11850819100731963,
      "grad_norm": 2.440765142440796,
      "learning_rate": 2.8876587637595257e-05,
      "loss": 0.9872,
      "step": 1870
    },
    {
      "epoch": 0.11914192464907,
      "grad_norm": 2.143096923828125,
      "learning_rate": 2.8870237087214227e-05,
      "loss": 0.9973,
      "step": 1880
    },
    {
      "epoch": 0.11977565829082037,
      "grad_norm": 2.647839307785034,
      "learning_rate": 2.8863886536833193e-05,
      "loss": 0.9916,
      "step": 1890
    },
    {
      "epoch": 0.12040939193257075,
      "grad_norm": 2.5948615074157715,
      "learning_rate": 2.885753598645216e-05,
      "loss": 0.998,
      "step": 1900
    },
    {
      "epoch": 0.1210431255743211,
      "grad_norm": 2.1083855628967285,
      "learning_rate": 2.8851185436071126e-05,
      "loss": 1.0509,
      "step": 1910
    },
    {
      "epoch": 0.12167685921607148,
      "grad_norm": 2.15505051612854,
      "learning_rate": 2.8844834885690093e-05,
      "loss": 0.9716,
      "step": 1920
    },
    {
      "epoch": 0.12231059285782185,
      "grad_norm": 4.598371982574463,
      "learning_rate": 2.8838484335309063e-05,
      "loss": 1.0167,
      "step": 1930
    },
    {
      "epoch": 0.12294432649957222,
      "grad_norm": 2.1678571701049805,
      "learning_rate": 2.883213378492803e-05,
      "loss": 0.996,
      "step": 1940
    },
    {
      "epoch": 0.1235780601413226,
      "grad_norm": 2.668134927749634,
      "learning_rate": 2.8825783234546995e-05,
      "loss": 1.0141,
      "step": 1950
    },
    {
      "epoch": 0.12421179378307297,
      "grad_norm": 3.168182611465454,
      "learning_rate": 2.8819432684165962e-05,
      "loss": 1.0368,
      "step": 1960
    },
    {
      "epoch": 0.12484552742482334,
      "grad_norm": 2.756594181060791,
      "learning_rate": 2.8813082133784928e-05,
      "loss": 1.0259,
      "step": 1970
    },
    {
      "epoch": 0.12547926106657373,
      "grad_norm": 2.764883279800415,
      "learning_rate": 2.8806731583403895e-05,
      "loss": 1.0121,
      "step": 1980
    },
    {
      "epoch": 0.1261129947083241,
      "grad_norm": 2.166102647781372,
      "learning_rate": 2.8800381033022865e-05,
      "loss": 1.0042,
      "step": 1990
    },
    {
      "epoch": 0.12674672835007447,
      "grad_norm": 2.974255323410034,
      "learning_rate": 2.879403048264183e-05,
      "loss": 1.0289,
      "step": 2000
    },
    {
      "epoch": 0.12738046199182484,
      "grad_norm": 2.151545524597168,
      "learning_rate": 2.8787679932260794e-05,
      "loss": 0.9816,
      "step": 2010
    },
    {
      "epoch": 0.12801419563357522,
      "grad_norm": 2.3204457759857178,
      "learning_rate": 2.8781329381879764e-05,
      "loss": 1.0045,
      "step": 2020
    },
    {
      "epoch": 0.1286479292753256,
      "grad_norm": 2.5284411907196045,
      "learning_rate": 2.877497883149873e-05,
      "loss": 1.0361,
      "step": 2030
    },
    {
      "epoch": 0.12928166291707596,
      "grad_norm": 2.2219316959381104,
      "learning_rate": 2.8768628281117697e-05,
      "loss": 1.0237,
      "step": 2040
    },
    {
      "epoch": 0.12991539655882633,
      "grad_norm": 2.8189005851745605,
      "learning_rate": 2.8762277730736666e-05,
      "loss": 0.9858,
      "step": 2050
    },
    {
      "epoch": 0.1305491302005767,
      "grad_norm": 2.0883400440216064,
      "learning_rate": 2.875592718035563e-05,
      "loss": 0.9891,
      "step": 2060
    },
    {
      "epoch": 0.13118286384232708,
      "grad_norm": 2.5032310485839844,
      "learning_rate": 2.87495766299746e-05,
      "loss": 1.008,
      "step": 2070
    },
    {
      "epoch": 0.13181659748407745,
      "grad_norm": 2.3152377605438232,
      "learning_rate": 2.8743226079593566e-05,
      "loss": 1.0075,
      "step": 2080
    },
    {
      "epoch": 0.13245033112582782,
      "grad_norm": 2.412395715713501,
      "learning_rate": 2.8736875529212532e-05,
      "loss": 1.0127,
      "step": 2090
    },
    {
      "epoch": 0.1330840647675782,
      "grad_norm": 2.468770742416382,
      "learning_rate": 2.8730524978831502e-05,
      "loss": 0.9891,
      "step": 2100
    },
    {
      "epoch": 0.13371779840932857,
      "grad_norm": 2.0966873168945312,
      "learning_rate": 2.8724174428450465e-05,
      "loss": 1.0279,
      "step": 2110
    },
    {
      "epoch": 0.13435153205107894,
      "grad_norm": 2.8442087173461914,
      "learning_rate": 2.871782387806943e-05,
      "loss": 1.0486,
      "step": 2120
    },
    {
      "epoch": 0.13498526569282931,
      "grad_norm": 1.9599123001098633,
      "learning_rate": 2.87114733276884e-05,
      "loss": 1.0183,
      "step": 2130
    },
    {
      "epoch": 0.1356189993345797,
      "grad_norm": 2.537285566329956,
      "learning_rate": 2.8705122777307368e-05,
      "loss": 0.999,
      "step": 2140
    },
    {
      "epoch": 0.13625273297633006,
      "grad_norm": 2.4373931884765625,
      "learning_rate": 2.8698772226926334e-05,
      "loss": 0.9662,
      "step": 2150
    },
    {
      "epoch": 0.13688646661808043,
      "grad_norm": 2.9809112548828125,
      "learning_rate": 2.8692421676545304e-05,
      "loss": 1.0395,
      "step": 2160
    },
    {
      "epoch": 0.1375202002598308,
      "grad_norm": 2.494171380996704,
      "learning_rate": 2.8686071126164267e-05,
      "loss": 1.0263,
      "step": 2170
    },
    {
      "epoch": 0.13815393390158118,
      "grad_norm": 2.4323501586914062,
      "learning_rate": 2.8679720575783233e-05,
      "loss": 1.0352,
      "step": 2180
    },
    {
      "epoch": 0.13878766754333155,
      "grad_norm": 2.0321784019470215,
      "learning_rate": 2.8673370025402203e-05,
      "loss": 0.9996,
      "step": 2190
    },
    {
      "epoch": 0.13942140118508192,
      "grad_norm": 2.5907537937164307,
      "learning_rate": 2.866701947502117e-05,
      "loss": 0.9712,
      "step": 2200
    },
    {
      "epoch": 0.1400551348268323,
      "grad_norm": 1.7985048294067383,
      "learning_rate": 2.8660668924640136e-05,
      "loss": 1.0333,
      "step": 2210
    },
    {
      "epoch": 0.14068886846858267,
      "grad_norm": 2.4061989784240723,
      "learning_rate": 2.8654318374259102e-05,
      "loss": 0.9777,
      "step": 2220
    },
    {
      "epoch": 0.14132260211033304,
      "grad_norm": 2.4231984615325928,
      "learning_rate": 2.864796782387807e-05,
      "loss": 0.998,
      "step": 2230
    },
    {
      "epoch": 0.1419563357520834,
      "grad_norm": 2.828348159790039,
      "learning_rate": 2.864161727349704e-05,
      "loss": 1.0593,
      "step": 2240
    },
    {
      "epoch": 0.14259006939383378,
      "grad_norm": 2.3762269020080566,
      "learning_rate": 2.8635266723116005e-05,
      "loss": 0.9813,
      "step": 2250
    },
    {
      "epoch": 0.14322380303558416,
      "grad_norm": 2.5080089569091797,
      "learning_rate": 2.862891617273497e-05,
      "loss": 1.0072,
      "step": 2260
    },
    {
      "epoch": 0.14385753667733453,
      "grad_norm": 2.520904302597046,
      "learning_rate": 2.8622565622353938e-05,
      "loss": 1.0457,
      "step": 2270
    },
    {
      "epoch": 0.1444912703190849,
      "grad_norm": 2.2867467403411865,
      "learning_rate": 2.8616215071972904e-05,
      "loss": 1.022,
      "step": 2280
    },
    {
      "epoch": 0.14512500396083525,
      "grad_norm": 2.3986265659332275,
      "learning_rate": 2.860986452159187e-05,
      "loss": 0.9993,
      "step": 2290
    },
    {
      "epoch": 0.14575873760258562,
      "grad_norm": 2.9832329750061035,
      "learning_rate": 2.860351397121084e-05,
      "loss": 0.9922,
      "step": 2300
    },
    {
      "epoch": 0.146392471244336,
      "grad_norm": 2.488196849822998,
      "learning_rate": 2.8597163420829807e-05,
      "loss": 0.9599,
      "step": 2310
    },
    {
      "epoch": 0.14702620488608636,
      "grad_norm": 2.4126174449920654,
      "learning_rate": 2.859081287044877e-05,
      "loss": 1.0038,
      "step": 2320
    },
    {
      "epoch": 0.14765993852783674,
      "grad_norm": 1.8689850568771362,
      "learning_rate": 2.858446232006774e-05,
      "loss": 0.9911,
      "step": 2330
    },
    {
      "epoch": 0.1482936721695871,
      "grad_norm": 2.317544937133789,
      "learning_rate": 2.8578111769686706e-05,
      "loss": 0.9634,
      "step": 2340
    },
    {
      "epoch": 0.14892740581133748,
      "grad_norm": 2.3033387660980225,
      "learning_rate": 2.8571761219305673e-05,
      "loss": 0.9945,
      "step": 2350
    },
    {
      "epoch": 0.14956113945308785,
      "grad_norm": 2.3790338039398193,
      "learning_rate": 2.8565410668924643e-05,
      "loss": 1.0006,
      "step": 2360
    },
    {
      "epoch": 0.15019487309483823,
      "grad_norm": 2.5691072940826416,
      "learning_rate": 2.8559060118543606e-05,
      "loss": 1.0085,
      "step": 2370
    },
    {
      "epoch": 0.1508286067365886,
      "grad_norm": 2.044067621231079,
      "learning_rate": 2.8552709568162575e-05,
      "loss": 1.0085,
      "step": 2380
    },
    {
      "epoch": 0.15146234037833897,
      "grad_norm": 2.2551167011260986,
      "learning_rate": 2.8546359017781542e-05,
      "loss": 0.9896,
      "step": 2390
    },
    {
      "epoch": 0.15209607402008934,
      "grad_norm": 2.716008424758911,
      "learning_rate": 2.854000846740051e-05,
      "loss": 0.9939,
      "step": 2400
    },
    {
      "epoch": 0.15272980766183972,
      "grad_norm": 2.8033671379089355,
      "learning_rate": 2.8533657917019478e-05,
      "loss": 1.0023,
      "step": 2410
    },
    {
      "epoch": 0.1533635413035901,
      "grad_norm": 2.4409759044647217,
      "learning_rate": 2.8527307366638445e-05,
      "loss": 1.009,
      "step": 2420
    },
    {
      "epoch": 0.15399727494534046,
      "grad_norm": 2.585149049758911,
      "learning_rate": 2.8520956816257408e-05,
      "loss": 1.0013,
      "step": 2430
    },
    {
      "epoch": 0.15463100858709083,
      "grad_norm": 2.4594602584838867,
      "learning_rate": 2.8514606265876377e-05,
      "loss": 0.9934,
      "step": 2440
    },
    {
      "epoch": 0.1552647422288412,
      "grad_norm": 3.008371353149414,
      "learning_rate": 2.8508255715495344e-05,
      "loss": 0.9783,
      "step": 2450
    },
    {
      "epoch": 0.15589847587059158,
      "grad_norm": 3.271615505218506,
      "learning_rate": 2.850190516511431e-05,
      "loss": 0.9913,
      "step": 2460
    },
    {
      "epoch": 0.15653220951234195,
      "grad_norm": 2.6596269607543945,
      "learning_rate": 2.849555461473328e-05,
      "loss": 1.0043,
      "step": 2470
    },
    {
      "epoch": 0.15716594315409232,
      "grad_norm": 2.271651268005371,
      "learning_rate": 2.8489204064352243e-05,
      "loss": 0.9946,
      "step": 2480
    },
    {
      "epoch": 0.1577996767958427,
      "grad_norm": 2.5608878135681152,
      "learning_rate": 2.848285351397121e-05,
      "loss": 0.9893,
      "step": 2490
    },
    {
      "epoch": 0.15843341043759307,
      "grad_norm": 2.437818765640259,
      "learning_rate": 2.847650296359018e-05,
      "loss": 1.002,
      "step": 2500
    },
    {
      "epoch": 0.15906714407934344,
      "grad_norm": 2.201176643371582,
      "learning_rate": 2.8470152413209146e-05,
      "loss": 0.9878,
      "step": 2510
    },
    {
      "epoch": 0.15970087772109381,
      "grad_norm": 2.8983874320983887,
      "learning_rate": 2.8463801862828112e-05,
      "loss": 1.0275,
      "step": 2520
    },
    {
      "epoch": 0.1603346113628442,
      "grad_norm": 2.3371946811676025,
      "learning_rate": 2.845745131244708e-05,
      "loss": 0.9735,
      "step": 2530
    },
    {
      "epoch": 0.16096834500459456,
      "grad_norm": 2.373133659362793,
      "learning_rate": 2.8451100762066045e-05,
      "loss": 1.0149,
      "step": 2540
    },
    {
      "epoch": 0.16160207864634493,
      "grad_norm": 2.5242488384246826,
      "learning_rate": 2.8444750211685015e-05,
      "loss": 1.0475,
      "step": 2550
    },
    {
      "epoch": 0.1622358122880953,
      "grad_norm": 2.3034913539886475,
      "learning_rate": 2.843839966130398e-05,
      "loss": 1.0262,
      "step": 2560
    },
    {
      "epoch": 0.16286954592984568,
      "grad_norm": 2.071619987487793,
      "learning_rate": 2.8432049110922948e-05,
      "loss": 1.0021,
      "step": 2570
    },
    {
      "epoch": 0.16350327957159605,
      "grad_norm": 2.419959306716919,
      "learning_rate": 2.8425698560541914e-05,
      "loss": 1.0014,
      "step": 2580
    },
    {
      "epoch": 0.16413701321334642,
      "grad_norm": 2.6636717319488525,
      "learning_rate": 2.841934801016088e-05,
      "loss": 1.0191,
      "step": 2590
    },
    {
      "epoch": 0.1647707468550968,
      "grad_norm": 2.4050989151000977,
      "learning_rate": 2.8412997459779847e-05,
      "loss": 1.0679,
      "step": 2600
    },
    {
      "epoch": 0.16540448049684717,
      "grad_norm": 2.136690855026245,
      "learning_rate": 2.8406646909398817e-05,
      "loss": 1.0221,
      "step": 2610
    },
    {
      "epoch": 0.16603821413859754,
      "grad_norm": 2.5630693435668945,
      "learning_rate": 2.8400296359017783e-05,
      "loss": 1.0207,
      "step": 2620
    },
    {
      "epoch": 0.1666719477803479,
      "grad_norm": 2.1396989822387695,
      "learning_rate": 2.8393945808636746e-05,
      "loss": 0.9896,
      "step": 2630
    },
    {
      "epoch": 0.16730568142209828,
      "grad_norm": 2.2794673442840576,
      "learning_rate": 2.8387595258255716e-05,
      "loss": 1.0022,
      "step": 2640
    },
    {
      "epoch": 0.16793941506384866,
      "grad_norm": 2.2908451557159424,
      "learning_rate": 2.8381244707874683e-05,
      "loss": 1.0341,
      "step": 2650
    },
    {
      "epoch": 0.16857314870559903,
      "grad_norm": 2.1314034461975098,
      "learning_rate": 2.837489415749365e-05,
      "loss": 1.0227,
      "step": 2660
    },
    {
      "epoch": 0.1692068823473494,
      "grad_norm": 2.249807596206665,
      "learning_rate": 2.836854360711262e-05,
      "loss": 0.9588,
      "step": 2670
    },
    {
      "epoch": 0.16984061598909977,
      "grad_norm": 1.9574499130249023,
      "learning_rate": 2.8362193056731585e-05,
      "loss": 0.9869,
      "step": 2680
    },
    {
      "epoch": 0.17047434963085015,
      "grad_norm": 2.1540513038635254,
      "learning_rate": 2.835584250635055e-05,
      "loss": 0.9518,
      "step": 2690
    },
    {
      "epoch": 0.17110808327260052,
      "grad_norm": 2.1685128211975098,
      "learning_rate": 2.8349491955969518e-05,
      "loss": 0.9666,
      "step": 2700
    },
    {
      "epoch": 0.1717418169143509,
      "grad_norm": 2.468830108642578,
      "learning_rate": 2.8343141405588485e-05,
      "loss": 1.0279,
      "step": 2710
    },
    {
      "epoch": 0.17237555055610126,
      "grad_norm": 3.2103271484375,
      "learning_rate": 2.8336790855207454e-05,
      "loss": 0.9793,
      "step": 2720
    },
    {
      "epoch": 0.17300928419785164,
      "grad_norm": 1.941463589668274,
      "learning_rate": 2.833044030482642e-05,
      "loss": 0.966,
      "step": 2730
    },
    {
      "epoch": 0.173643017839602,
      "grad_norm": 2.5897631645202637,
      "learning_rate": 2.8324089754445384e-05,
      "loss": 1.0308,
      "step": 2740
    },
    {
      "epoch": 0.17427675148135238,
      "grad_norm": 2.4741148948669434,
      "learning_rate": 2.8317739204064354e-05,
      "loss": 0.9767,
      "step": 2750
    },
    {
      "epoch": 0.17491048512310275,
      "grad_norm": 2.750504493713379,
      "learning_rate": 2.831138865368332e-05,
      "loss": 0.9974,
      "step": 2760
    },
    {
      "epoch": 0.17554421876485313,
      "grad_norm": 2.4614410400390625,
      "learning_rate": 2.8305038103302286e-05,
      "loss": 0.9948,
      "step": 2770
    },
    {
      "epoch": 0.1761779524066035,
      "grad_norm": 2.1061413288116455,
      "learning_rate": 2.8298687552921256e-05,
      "loss": 1.0109,
      "step": 2780
    },
    {
      "epoch": 0.17681168604835387,
      "grad_norm": 2.574578285217285,
      "learning_rate": 2.829233700254022e-05,
      "loss": 1.0324,
      "step": 2790
    },
    {
      "epoch": 0.17744541969010424,
      "grad_norm": 2.2413649559020996,
      "learning_rate": 2.8285986452159186e-05,
      "loss": 0.9795,
      "step": 2800
    },
    {
      "epoch": 0.17807915333185462,
      "grad_norm": 2.017630100250244,
      "learning_rate": 2.8279635901778156e-05,
      "loss": 1.0059,
      "step": 2810
    },
    {
      "epoch": 0.178712886973605,
      "grad_norm": 2.2543983459472656,
      "learning_rate": 2.8273285351397122e-05,
      "loss": 1.0059,
      "step": 2820
    },
    {
      "epoch": 0.17934662061535536,
      "grad_norm": 2.3321497440338135,
      "learning_rate": 2.826693480101609e-05,
      "loss": 0.995,
      "step": 2830
    },
    {
      "epoch": 0.17998035425710573,
      "grad_norm": 2.396773338317871,
      "learning_rate": 2.8260584250635055e-05,
      "loss": 1.0221,
      "step": 2840
    },
    {
      "epoch": 0.1806140878988561,
      "grad_norm": 2.4513938426971436,
      "learning_rate": 2.825423370025402e-05,
      "loss": 1.0184,
      "step": 2850
    },
    {
      "epoch": 0.18124782154060648,
      "grad_norm": 2.146777391433716,
      "learning_rate": 2.824788314987299e-05,
      "loss": 0.9651,
      "step": 2860
    },
    {
      "epoch": 0.18188155518235685,
      "grad_norm": 2.3244080543518066,
      "learning_rate": 2.8241532599491958e-05,
      "loss": 1.0044,
      "step": 2870
    },
    {
      "epoch": 0.18251528882410722,
      "grad_norm": 2.3014485836029053,
      "learning_rate": 2.8235182049110924e-05,
      "loss": 0.9987,
      "step": 2880
    },
    {
      "epoch": 0.1831490224658576,
      "grad_norm": 2.248777151107788,
      "learning_rate": 2.822883149872989e-05,
      "loss": 0.9934,
      "step": 2890
    },
    {
      "epoch": 0.18378275610760797,
      "grad_norm": 3.2415568828582764,
      "learning_rate": 2.8222480948348857e-05,
      "loss": 1.0313,
      "step": 2900
    },
    {
      "epoch": 0.18441648974935834,
      "grad_norm": 1.8893219232559204,
      "learning_rate": 2.8216130397967823e-05,
      "loss": 0.9856,
      "step": 2910
    },
    {
      "epoch": 0.18505022339110871,
      "grad_norm": 2.34016489982605,
      "learning_rate": 2.8209779847586793e-05,
      "loss": 0.9955,
      "step": 2920
    },
    {
      "epoch": 0.1856839570328591,
      "grad_norm": 2.4143567085266113,
      "learning_rate": 2.820342929720576e-05,
      "loss": 0.9977,
      "step": 2930
    },
    {
      "epoch": 0.18631769067460946,
      "grad_norm": 2.3814735412597656,
      "learning_rate": 2.8197078746824726e-05,
      "loss": 0.9554,
      "step": 2940
    },
    {
      "epoch": 0.18695142431635983,
      "grad_norm": 2.8147990703582764,
      "learning_rate": 2.8190728196443692e-05,
      "loss": 1.0353,
      "step": 2950
    },
    {
      "epoch": 0.1875851579581102,
      "grad_norm": 2.538766384124756,
      "learning_rate": 2.818437764606266e-05,
      "loss": 1.0264,
      "step": 2960
    },
    {
      "epoch": 0.18821889159986058,
      "grad_norm": 2.517272472381592,
      "learning_rate": 2.8178027095681625e-05,
      "loss": 0.9957,
      "step": 2970
    },
    {
      "epoch": 0.18885262524161095,
      "grad_norm": 2.5263116359710693,
      "learning_rate": 2.8171676545300595e-05,
      "loss": 0.9857,
      "step": 2980
    },
    {
      "epoch": 0.18948635888336132,
      "grad_norm": 2.447692632675171,
      "learning_rate": 2.816532599491956e-05,
      "loss": 0.9601,
      "step": 2990
    },
    {
      "epoch": 0.1901200925251117,
      "grad_norm": 2.7507495880126953,
      "learning_rate": 2.8158975444538524e-05,
      "loss": 1.0229,
      "step": 3000
    },
    {
      "epoch": 0.19075382616686207,
      "grad_norm": 2.2534189224243164,
      "learning_rate": 2.8152624894157494e-05,
      "loss": 1.0086,
      "step": 3010
    },
    {
      "epoch": 0.19138755980861244,
      "grad_norm": 3.743133068084717,
      "learning_rate": 2.814627434377646e-05,
      "loss": 1.0142,
      "step": 3020
    },
    {
      "epoch": 0.1920212934503628,
      "grad_norm": 2.549394369125366,
      "learning_rate": 2.813992379339543e-05,
      "loss": 1.0342,
      "step": 3030
    },
    {
      "epoch": 0.19265502709211318,
      "grad_norm": 2.3656203746795654,
      "learning_rate": 2.8133573243014397e-05,
      "loss": 0.9906,
      "step": 3040
    },
    {
      "epoch": 0.19328876073386356,
      "grad_norm": 2.131513833999634,
      "learning_rate": 2.812722269263336e-05,
      "loss": 1.0262,
      "step": 3050
    },
    {
      "epoch": 0.19392249437561393,
      "grad_norm": 2.7792608737945557,
      "learning_rate": 2.812087214225233e-05,
      "loss": 1.0306,
      "step": 3060
    },
    {
      "epoch": 0.1945562280173643,
      "grad_norm": 2.1477484703063965,
      "learning_rate": 2.8114521591871296e-05,
      "loss": 0.9718,
      "step": 3070
    },
    {
      "epoch": 0.19518996165911467,
      "grad_norm": 2.735358476638794,
      "learning_rate": 2.8108171041490263e-05,
      "loss": 1.0227,
      "step": 3080
    },
    {
      "epoch": 0.19582369530086505,
      "grad_norm": 2.2428958415985107,
      "learning_rate": 2.8101820491109232e-05,
      "loss": 0.9778,
      "step": 3090
    },
    {
      "epoch": 0.19645742894261542,
      "grad_norm": 3.004837989807129,
      "learning_rate": 2.8095469940728195e-05,
      "loss": 0.9515,
      "step": 3100
    },
    {
      "epoch": 0.1970911625843658,
      "grad_norm": 2.3609790802001953,
      "learning_rate": 2.8089119390347162e-05,
      "loss": 0.9797,
      "step": 3110
    },
    {
      "epoch": 0.19772489622611616,
      "grad_norm": 2.4560415744781494,
      "learning_rate": 2.8082768839966132e-05,
      "loss": 0.9679,
      "step": 3120
    },
    {
      "epoch": 0.19835862986786654,
      "grad_norm": 2.575709581375122,
      "learning_rate": 2.8076418289585098e-05,
      "loss": 1.0379,
      "step": 3130
    },
    {
      "epoch": 0.1989923635096169,
      "grad_norm": 2.2849783897399902,
      "learning_rate": 2.8070067739204065e-05,
      "loss": 0.977,
      "step": 3140
    },
    {
      "epoch": 0.19962609715136728,
      "grad_norm": 2.7122859954833984,
      "learning_rate": 2.806371718882303e-05,
      "loss": 0.9917,
      "step": 3150
    },
    {
      "epoch": 0.20025983079311765,
      "grad_norm": 2.5333452224731445,
      "learning_rate": 2.8057366638441997e-05,
      "loss": 0.98,
      "step": 3160
    },
    {
      "epoch": 0.20089356443486803,
      "grad_norm": 2.4066567420959473,
      "learning_rate": 2.8051016088060967e-05,
      "loss": 0.9778,
      "step": 3170
    },
    {
      "epoch": 0.2015272980766184,
      "grad_norm": 2.2017722129821777,
      "learning_rate": 2.8044665537679934e-05,
      "loss": 1.0101,
      "step": 3180
    },
    {
      "epoch": 0.20216103171836877,
      "grad_norm": 2.584683656692505,
      "learning_rate": 2.80383149872989e-05,
      "loss": 1.0066,
      "step": 3190
    },
    {
      "epoch": 0.20279476536011914,
      "grad_norm": 2.2302286624908447,
      "learning_rate": 2.803196443691787e-05,
      "loss": 0.987,
      "step": 3200
    },
    {
      "epoch": 0.20342849900186952,
      "grad_norm": 2.757089376449585,
      "learning_rate": 2.8025613886536833e-05,
      "loss": 0.9825,
      "step": 3210
    },
    {
      "epoch": 0.2040622326436199,
      "grad_norm": 2.398127317428589,
      "learning_rate": 2.80192633361558e-05,
      "loss": 0.9809,
      "step": 3220
    },
    {
      "epoch": 0.20469596628537026,
      "grad_norm": 3.42431378364563,
      "learning_rate": 2.801291278577477e-05,
      "loss": 0.9835,
      "step": 3230
    },
    {
      "epoch": 0.20532969992712063,
      "grad_norm": 2.54882550239563,
      "learning_rate": 2.8006562235393736e-05,
      "loss": 0.9655,
      "step": 3240
    },
    {
      "epoch": 0.205963433568871,
      "grad_norm": 2.351565361022949,
      "learning_rate": 2.8000211685012702e-05,
      "loss": 0.9603,
      "step": 3250
    },
    {
      "epoch": 0.20659716721062138,
      "grad_norm": 2.176176071166992,
      "learning_rate": 2.799386113463167e-05,
      "loss": 1.0099,
      "step": 3260
    },
    {
      "epoch": 0.20723090085237175,
      "grad_norm": 2.003251314163208,
      "learning_rate": 2.7987510584250635e-05,
      "loss": 0.9253,
      "step": 3270
    },
    {
      "epoch": 0.20786463449412212,
      "grad_norm": 3.2449004650115967,
      "learning_rate": 2.79811600338696e-05,
      "loss": 1.0147,
      "step": 3280
    },
    {
      "epoch": 0.2084983681358725,
      "grad_norm": 2.5526928901672363,
      "learning_rate": 2.797480948348857e-05,
      "loss": 1.0299,
      "step": 3290
    },
    {
      "epoch": 0.20913210177762287,
      "grad_norm": 2.267590045928955,
      "learning_rate": 2.7968458933107538e-05,
      "loss": 0.9953,
      "step": 3300
    },
    {
      "epoch": 0.20976583541937324,
      "grad_norm": 2.325261116027832,
      "learning_rate": 2.79621083827265e-05,
      "loss": 1.0463,
      "step": 3310
    },
    {
      "epoch": 0.21039956906112361,
      "grad_norm": 2.0832226276397705,
      "learning_rate": 2.795575783234547e-05,
      "loss": 1.0082,
      "step": 3320
    },
    {
      "epoch": 0.211033302702874,
      "grad_norm": 2.2474513053894043,
      "learning_rate": 2.7949407281964437e-05,
      "loss": 1.0051,
      "step": 3330
    },
    {
      "epoch": 0.21166703634462436,
      "grad_norm": 2.580388307571411,
      "learning_rate": 2.7943056731583407e-05,
      "loss": 0.9694,
      "step": 3340
    },
    {
      "epoch": 0.21230076998637473,
      "grad_norm": 2.2547616958618164,
      "learning_rate": 2.7936706181202373e-05,
      "loss": 0.949,
      "step": 3350
    },
    {
      "epoch": 0.2129345036281251,
      "grad_norm": 2.289921283721924,
      "learning_rate": 2.7930355630821336e-05,
      "loss": 0.9763,
      "step": 3360
    },
    {
      "epoch": 0.21356823726987548,
      "grad_norm": 2.0584774017333984,
      "learning_rate": 2.7924005080440306e-05,
      "loss": 0.9511,
      "step": 3370
    },
    {
      "epoch": 0.21420197091162585,
      "grad_norm": 2.243021011352539,
      "learning_rate": 2.7917654530059272e-05,
      "loss": 1.0013,
      "step": 3380
    },
    {
      "epoch": 0.21483570455337622,
      "grad_norm": 2.065838098526001,
      "learning_rate": 2.7911939034716343e-05,
      "loss": 0.9588,
      "step": 3390
    },
    {
      "epoch": 0.2154694381951266,
      "grad_norm": 2.286329984664917,
      "learning_rate": 2.790558848433531e-05,
      "loss": 0.9613,
      "step": 3400
    },
    {
      "epoch": 0.21610317183687697,
      "grad_norm": 2.2905731201171875,
      "learning_rate": 2.789923793395428e-05,
      "loss": 0.9587,
      "step": 3410
    },
    {
      "epoch": 0.21673690547862734,
      "grad_norm": 2.242253541946411,
      "learning_rate": 2.7892887383573242e-05,
      "loss": 1.0009,
      "step": 3420
    },
    {
      "epoch": 0.2173706391203777,
      "grad_norm": 2.5485353469848633,
      "learning_rate": 2.788653683319221e-05,
      "loss": 1.0111,
      "step": 3430
    },
    {
      "epoch": 0.21800437276212808,
      "grad_norm": 2.196955680847168,
      "learning_rate": 2.788018628281118e-05,
      "loss": 0.9847,
      "step": 3440
    },
    {
      "epoch": 0.21863810640387846,
      "grad_norm": 2.63417911529541,
      "learning_rate": 2.7873835732430145e-05,
      "loss": 0.9927,
      "step": 3450
    },
    {
      "epoch": 0.21927184004562883,
      "grad_norm": 3.367152690887451,
      "learning_rate": 2.786748518204911e-05,
      "loss": 1.0217,
      "step": 3460
    },
    {
      "epoch": 0.2199055736873792,
      "grad_norm": 2.2749593257904053,
      "learning_rate": 2.7861134631668078e-05,
      "loss": 0.9858,
      "step": 3470
    },
    {
      "epoch": 0.22053930732912957,
      "grad_norm": 3.132128953933716,
      "learning_rate": 2.7854784081287044e-05,
      "loss": 1.009,
      "step": 3480
    },
    {
      "epoch": 0.22117304097087995,
      "grad_norm": 2.0321030616760254,
      "learning_rate": 2.7848433530906014e-05,
      "loss": 1.0178,
      "step": 3490
    },
    {
      "epoch": 0.22180677461263032,
      "grad_norm": 2.4559614658355713,
      "learning_rate": 2.784208298052498e-05,
      "loss": 1.0137,
      "step": 3500
    },
    {
      "epoch": 0.2224405082543807,
      "grad_norm": 2.2069590091705322,
      "learning_rate": 2.7835732430143947e-05,
      "loss": 1.0019,
      "step": 3510
    },
    {
      "epoch": 0.22307424189613106,
      "grad_norm": 2.7632484436035156,
      "learning_rate": 2.7829381879762917e-05,
      "loss": 0.9874,
      "step": 3520
    },
    {
      "epoch": 0.22370797553788144,
      "grad_norm": 2.1527812480926514,
      "learning_rate": 2.782303132938188e-05,
      "loss": 1.0239,
      "step": 3530
    },
    {
      "epoch": 0.2243417091796318,
      "grad_norm": 2.3577280044555664,
      "learning_rate": 2.7816680779000846e-05,
      "loss": 0.944,
      "step": 3540
    },
    {
      "epoch": 0.22497544282138218,
      "grad_norm": 2.614461660385132,
      "learning_rate": 2.7810330228619816e-05,
      "loss": 0.9846,
      "step": 3550
    },
    {
      "epoch": 0.22560917646313255,
      "grad_norm": 2.4917800426483154,
      "learning_rate": 2.7803979678238782e-05,
      "loss": 0.9775,
      "step": 3560
    },
    {
      "epoch": 0.22624291010488293,
      "grad_norm": 2.3714282512664795,
      "learning_rate": 2.779762912785775e-05,
      "loss": 0.9953,
      "step": 3570
    },
    {
      "epoch": 0.2268766437466333,
      "grad_norm": 2.378512144088745,
      "learning_rate": 2.7791278577476715e-05,
      "loss": 0.996,
      "step": 3580
    },
    {
      "epoch": 0.22751037738838367,
      "grad_norm": 2.708212375640869,
      "learning_rate": 2.778492802709568e-05,
      "loss": 0.9986,
      "step": 3590
    },
    {
      "epoch": 0.22814411103013404,
      "grad_norm": 2.2659640312194824,
      "learning_rate": 2.7778577476714648e-05,
      "loss": 1.0228,
      "step": 3600
    },
    {
      "epoch": 0.22877784467188442,
      "grad_norm": 2.2860915660858154,
      "learning_rate": 2.7772226926333618e-05,
      "loss": 1.0092,
      "step": 3610
    },
    {
      "epoch": 0.2294115783136348,
      "grad_norm": 2.7061307430267334,
      "learning_rate": 2.7765876375952584e-05,
      "loss": 0.9896,
      "step": 3620
    },
    {
      "epoch": 0.23004531195538516,
      "grad_norm": 2.4743921756744385,
      "learning_rate": 2.7759525825571547e-05,
      "loss": 1.0129,
      "step": 3630
    },
    {
      "epoch": 0.23067904559713553,
      "grad_norm": 2.55769419670105,
      "learning_rate": 2.7753175275190517e-05,
      "loss": 0.9738,
      "step": 3640
    },
    {
      "epoch": 0.2313127792388859,
      "grad_norm": 2.038259983062744,
      "learning_rate": 2.7746824724809483e-05,
      "loss": 0.9899,
      "step": 3650
    },
    {
      "epoch": 0.23194651288063628,
      "grad_norm": 1.9794466495513916,
      "learning_rate": 2.7740474174428453e-05,
      "loss": 0.996,
      "step": 3660
    },
    {
      "epoch": 0.23258024652238665,
      "grad_norm": 2.4352834224700928,
      "learning_rate": 2.773412362404742e-05,
      "loss": 0.9977,
      "step": 3670
    },
    {
      "epoch": 0.23321398016413702,
      "grad_norm": 2.225895404815674,
      "learning_rate": 2.7727773073666383e-05,
      "loss": 0.9869,
      "step": 3680
    },
    {
      "epoch": 0.2338477138058874,
      "grad_norm": 2.2178707122802734,
      "learning_rate": 2.7721422523285353e-05,
      "loss": 1.006,
      "step": 3690
    },
    {
      "epoch": 0.23448144744763777,
      "grad_norm": 2.580091714859009,
      "learning_rate": 2.771507197290432e-05,
      "loss": 1.0195,
      "step": 3700
    },
    {
      "epoch": 0.23511518108938814,
      "grad_norm": 2.942695379257202,
      "learning_rate": 2.7708721422523285e-05,
      "loss": 0.9667,
      "step": 3710
    },
    {
      "epoch": 0.23574891473113851,
      "grad_norm": 2.540663957595825,
      "learning_rate": 2.7702370872142255e-05,
      "loss": 1.0495,
      "step": 3720
    },
    {
      "epoch": 0.2363826483728889,
      "grad_norm": 2.4146764278411865,
      "learning_rate": 2.7696020321761218e-05,
      "loss": 0.9695,
      "step": 3730
    },
    {
      "epoch": 0.23701638201463926,
      "grad_norm": 3.0230660438537598,
      "learning_rate": 2.7689669771380185e-05,
      "loss": 1.0102,
      "step": 3740
    },
    {
      "epoch": 0.23765011565638963,
      "grad_norm": 2.2321553230285645,
      "learning_rate": 2.7683319220999155e-05,
      "loss": 1.012,
      "step": 3750
    },
    {
      "epoch": 0.23828384929814,
      "grad_norm": 2.6013576984405518,
      "learning_rate": 2.767696867061812e-05,
      "loss": 1.0133,
      "step": 3760
    },
    {
      "epoch": 0.23891758293989038,
      "grad_norm": 2.1758313179016113,
      "learning_rate": 2.7670618120237087e-05,
      "loss": 1.0417,
      "step": 3770
    },
    {
      "epoch": 0.23955131658164075,
      "grad_norm": 2.350978136062622,
      "learning_rate": 2.7664267569856057e-05,
      "loss": 0.9669,
      "step": 3780
    },
    {
      "epoch": 0.24018505022339112,
      "grad_norm": 2.0350513458251953,
      "learning_rate": 2.765791701947502e-05,
      "loss": 0.9998,
      "step": 3790
    },
    {
      "epoch": 0.2408187838651415,
      "grad_norm": 2.114159345626831,
      "learning_rate": 2.765156646909399e-05,
      "loss": 0.9942,
      "step": 3800
    },
    {
      "epoch": 0.24145251750689187,
      "grad_norm": 2.6152825355529785,
      "learning_rate": 2.7645215918712956e-05,
      "loss": 1.0224,
      "step": 3810
    },
    {
      "epoch": 0.2420862511486422,
      "grad_norm": 2.397021532058716,
      "learning_rate": 2.7638865368331923e-05,
      "loss": 0.9871,
      "step": 3820
    },
    {
      "epoch": 0.24271998479039258,
      "grad_norm": 2.438300848007202,
      "learning_rate": 2.7632514817950893e-05,
      "loss": 0.9843,
      "step": 3830
    },
    {
      "epoch": 0.24335371843214296,
      "grad_norm": 2.3559112548828125,
      "learning_rate": 2.7626164267569856e-05,
      "loss": 1.003,
      "step": 3840
    },
    {
      "epoch": 0.24398745207389333,
      "grad_norm": 1.9954297542572021,
      "learning_rate": 2.7619813717188822e-05,
      "loss": 0.9888,
      "step": 3850
    },
    {
      "epoch": 0.2446211857156437,
      "grad_norm": 2.163553237915039,
      "learning_rate": 2.7613463166807792e-05,
      "loss": 0.9946,
      "step": 3860
    },
    {
      "epoch": 0.24525491935739407,
      "grad_norm": 2.172375202178955,
      "learning_rate": 2.760711261642676e-05,
      "loss": 0.96,
      "step": 3870
    },
    {
      "epoch": 0.24588865299914445,
      "grad_norm": 2.065352439880371,
      "learning_rate": 2.7600762066045725e-05,
      "loss": 1.0211,
      "step": 3880
    },
    {
      "epoch": 0.24652238664089482,
      "grad_norm": 2.4614787101745605,
      "learning_rate": 2.759441151566469e-05,
      "loss": 1.015,
      "step": 3890
    },
    {
      "epoch": 0.2471561202826452,
      "grad_norm": 2.4206457138061523,
      "learning_rate": 2.7588060965283658e-05,
      "loss": 0.9713,
      "step": 3900
    },
    {
      "epoch": 0.24778985392439556,
      "grad_norm": 2.903296709060669,
      "learning_rate": 2.7581710414902624e-05,
      "loss": 1.058,
      "step": 3910
    },
    {
      "epoch": 0.24842358756614594,
      "grad_norm": 2.8012197017669678,
      "learning_rate": 2.7575359864521594e-05,
      "loss": 1.0066,
      "step": 3920
    },
    {
      "epoch": 0.2490573212078963,
      "grad_norm": 2.4113030433654785,
      "learning_rate": 2.756900931414056e-05,
      "loss": 1.011,
      "step": 3930
    },
    {
      "epoch": 0.24969105484964668,
      "grad_norm": 2.0686609745025635,
      "learning_rate": 2.7562658763759523e-05,
      "loss": 0.9598,
      "step": 3940
    },
    {
      "epoch": 0.25032478849139705,
      "grad_norm": 2.1372992992401123,
      "learning_rate": 2.7556308213378493e-05,
      "loss": 0.977,
      "step": 3950
    },
    {
      "epoch": 0.25095852213314745,
      "grad_norm": 2.7274155616760254,
      "learning_rate": 2.754995766299746e-05,
      "loss": 0.9789,
      "step": 3960
    },
    {
      "epoch": 0.2515922557748978,
      "grad_norm": 1.9421849250793457,
      "learning_rate": 2.754360711261643e-05,
      "loss": 0.9793,
      "step": 3970
    },
    {
      "epoch": 0.2522259894166482,
      "grad_norm": 2.4166696071624756,
      "learning_rate": 2.7537256562235396e-05,
      "loss": 0.9985,
      "step": 3980
    },
    {
      "epoch": 0.25285972305839854,
      "grad_norm": 2.3929672241210938,
      "learning_rate": 2.753090601185436e-05,
      "loss": 0.9716,
      "step": 3990
    },
    {
      "epoch": 0.25349345670014894,
      "grad_norm": 2.6565544605255127,
      "learning_rate": 2.752455546147333e-05,
      "loss": 1.0006,
      "step": 4000
    },
    {
      "epoch": 0.2541271903418993,
      "grad_norm": 2.314592123031616,
      "learning_rate": 2.7518204911092295e-05,
      "loss": 0.9646,
      "step": 4010
    },
    {
      "epoch": 0.2547609239836497,
      "grad_norm": 2.0283875465393066,
      "learning_rate": 2.751185436071126e-05,
      "loss": 1.0107,
      "step": 4020
    },
    {
      "epoch": 0.25539465762540003,
      "grad_norm": 2.2830986976623535,
      "learning_rate": 2.750550381033023e-05,
      "loss": 0.9726,
      "step": 4030
    },
    {
      "epoch": 0.25602839126715043,
      "grad_norm": 2.1753225326538086,
      "learning_rate": 2.7499788314987302e-05,
      "loss": 0.9824,
      "step": 4040
    },
    {
      "epoch": 0.2566621249089008,
      "grad_norm": 2.0353281497955322,
      "learning_rate": 2.7493437764606265e-05,
      "loss": 0.9802,
      "step": 4050
    },
    {
      "epoch": 0.2572958585506512,
      "grad_norm": 2.668802261352539,
      "learning_rate": 2.748708721422523e-05,
      "loss": 0.9845,
      "step": 4060
    },
    {
      "epoch": 0.2579295921924015,
      "grad_norm": 2.2359142303466797,
      "learning_rate": 2.74807366638442e-05,
      "loss": 0.956,
      "step": 4070
    },
    {
      "epoch": 0.2585633258341519,
      "grad_norm": 2.1922872066497803,
      "learning_rate": 2.7474386113463168e-05,
      "loss": 0.976,
      "step": 4080
    },
    {
      "epoch": 0.25919705947590227,
      "grad_norm": 2.848691940307617,
      "learning_rate": 2.7468035563082134e-05,
      "loss": 0.9798,
      "step": 4090
    },
    {
      "epoch": 0.25983079311765267,
      "grad_norm": 2.465880870819092,
      "learning_rate": 2.74616850127011e-05,
      "loss": 0.9962,
      "step": 4100
    },
    {
      "epoch": 0.260464526759403,
      "grad_norm": 4.078268051147461,
      "learning_rate": 2.7455334462320067e-05,
      "loss": 0.985,
      "step": 4110
    },
    {
      "epoch": 0.2610982604011534,
      "grad_norm": 2.8106865882873535,
      "learning_rate": 2.7448983911939037e-05,
      "loss": 0.9667,
      "step": 4120
    },
    {
      "epoch": 0.26173199404290376,
      "grad_norm": 2.193296194076538,
      "learning_rate": 2.7442633361558003e-05,
      "loss": 1.0304,
      "step": 4130
    },
    {
      "epoch": 0.26236572768465416,
      "grad_norm": 2.304377317428589,
      "learning_rate": 2.743628281117697e-05,
      "loss": 0.9638,
      "step": 4140
    },
    {
      "epoch": 0.2629994613264045,
      "grad_norm": 2.5293869972229004,
      "learning_rate": 2.742993226079594e-05,
      "loss": 1.006,
      "step": 4150
    },
    {
      "epoch": 0.2636331949681549,
      "grad_norm": 2.482271432876587,
      "learning_rate": 2.7423581710414902e-05,
      "loss": 0.9648,
      "step": 4160
    },
    {
      "epoch": 0.26426692860990525,
      "grad_norm": 2.6014137268066406,
      "learning_rate": 2.741723116003387e-05,
      "loss": 1.0116,
      "step": 4170
    },
    {
      "epoch": 0.26490066225165565,
      "grad_norm": 2.0156843662261963,
      "learning_rate": 2.741088060965284e-05,
      "loss": 1.0412,
      "step": 4180
    },
    {
      "epoch": 0.265534395893406,
      "grad_norm": 1.868124008178711,
      "learning_rate": 2.7404530059271805e-05,
      "loss": 1.0266,
      "step": 4190
    },
    {
      "epoch": 0.2661681295351564,
      "grad_norm": 2.5964508056640625,
      "learning_rate": 2.739817950889077e-05,
      "loss": 1.0054,
      "step": 4200
    },
    {
      "epoch": 0.26680186317690674,
      "grad_norm": 2.343414545059204,
      "learning_rate": 2.7391828958509738e-05,
      "loss": 0.9873,
      "step": 4210
    },
    {
      "epoch": 0.26743559681865714,
      "grad_norm": 2.08260440826416,
      "learning_rate": 2.7385478408128704e-05,
      "loss": 0.9921,
      "step": 4220
    },
    {
      "epoch": 0.2680693304604075,
      "grad_norm": 3.1561474800109863,
      "learning_rate": 2.737912785774767e-05,
      "loss": 1.0004,
      "step": 4230
    },
    {
      "epoch": 0.2687030641021579,
      "grad_norm": 2.497260808944702,
      "learning_rate": 2.737277730736664e-05,
      "loss": 0.9862,
      "step": 4240
    },
    {
      "epoch": 0.26933679774390823,
      "grad_norm": 2.533987045288086,
      "learning_rate": 2.7366426756985607e-05,
      "loss": 0.9578,
      "step": 4250
    },
    {
      "epoch": 0.26997053138565863,
      "grad_norm": 2.3282663822174072,
      "learning_rate": 2.7360076206604573e-05,
      "loss": 1.0222,
      "step": 4260
    },
    {
      "epoch": 0.270604265027409,
      "grad_norm": 2.4627931118011475,
      "learning_rate": 2.735372565622354e-05,
      "loss": 0.9728,
      "step": 4270
    },
    {
      "epoch": 0.2712379986691594,
      "grad_norm": 2.3791842460632324,
      "learning_rate": 2.7347375105842506e-05,
      "loss": 0.9916,
      "step": 4280
    },
    {
      "epoch": 0.2718717323109097,
      "grad_norm": 2.3863041400909424,
      "learning_rate": 2.7341024555461476e-05,
      "loss": 0.9716,
      "step": 4290
    },
    {
      "epoch": 0.2725054659526601,
      "grad_norm": 2.5714287757873535,
      "learning_rate": 2.7334674005080443e-05,
      "loss": 0.9829,
      "step": 4300
    },
    {
      "epoch": 0.27313919959441046,
      "grad_norm": 1.8971589803695679,
      "learning_rate": 2.7328323454699406e-05,
      "loss": 0.9531,
      "step": 4310
    },
    {
      "epoch": 0.27377293323616086,
      "grad_norm": 2.3652894496917725,
      "learning_rate": 2.7321972904318375e-05,
      "loss": 1.0232,
      "step": 4320
    },
    {
      "epoch": 0.2744066668779112,
      "grad_norm": 2.3350908756256104,
      "learning_rate": 2.7315622353937342e-05,
      "loss": 0.9446,
      "step": 4330
    },
    {
      "epoch": 0.2750404005196616,
      "grad_norm": 2.4711291790008545,
      "learning_rate": 2.7309271803556308e-05,
      "loss": 0.9859,
      "step": 4340
    },
    {
      "epoch": 0.27567413416141195,
      "grad_norm": 1.8905357122421265,
      "learning_rate": 2.7302921253175278e-05,
      "loss": 0.9802,
      "step": 4350
    },
    {
      "epoch": 0.27630786780316235,
      "grad_norm": 2.152503490447998,
      "learning_rate": 2.729657070279424e-05,
      "loss": 1.0172,
      "step": 4360
    },
    {
      "epoch": 0.2769416014449127,
      "grad_norm": 2.069955825805664,
      "learning_rate": 2.7290220152413208e-05,
      "loss": 0.9637,
      "step": 4370
    },
    {
      "epoch": 0.2775753350866631,
      "grad_norm": 2.9231443405151367,
      "learning_rate": 2.7283869602032177e-05,
      "loss": 0.9655,
      "step": 4380
    },
    {
      "epoch": 0.27820906872841344,
      "grad_norm": 2.2110438346862793,
      "learning_rate": 2.7277519051651144e-05,
      "loss": 0.9853,
      "step": 4390
    },
    {
      "epoch": 0.27884280237016384,
      "grad_norm": 2.6840262413024902,
      "learning_rate": 2.727116850127011e-05,
      "loss": 0.9893,
      "step": 4400
    },
    {
      "epoch": 0.2794765360119142,
      "grad_norm": 2.8068745136260986,
      "learning_rate": 2.726481795088908e-05,
      "loss": 0.9812,
      "step": 4410
    },
    {
      "epoch": 0.2801102696536646,
      "grad_norm": 2.00561785697937,
      "learning_rate": 2.7258467400508043e-05,
      "loss": 0.9701,
      "step": 4420
    },
    {
      "epoch": 0.28074400329541493,
      "grad_norm": 2.1066250801086426,
      "learning_rate": 2.7252116850127013e-05,
      "loss": 0.9593,
      "step": 4430
    },
    {
      "epoch": 0.28137773693716533,
      "grad_norm": 2.3824000358581543,
      "learning_rate": 2.724576629974598e-05,
      "loss": 1.0155,
      "step": 4440
    },
    {
      "epoch": 0.2820114705789157,
      "grad_norm": 2.494593858718872,
      "learning_rate": 2.7239415749364946e-05,
      "loss": 0.9902,
      "step": 4450
    },
    {
      "epoch": 0.2826452042206661,
      "grad_norm": 4.455418109893799,
      "learning_rate": 2.7233065198983916e-05,
      "loss": 0.9773,
      "step": 4460
    },
    {
      "epoch": 0.2832789378624164,
      "grad_norm": 2.374818801879883,
      "learning_rate": 2.722671464860288e-05,
      "loss": 0.9757,
      "step": 4470
    },
    {
      "epoch": 0.2839126715041668,
      "grad_norm": 2.3214917182922363,
      "learning_rate": 2.7220364098221845e-05,
      "loss": 0.9903,
      "step": 4480
    },
    {
      "epoch": 0.28454640514591717,
      "grad_norm": 2.768280029296875,
      "learning_rate": 2.7214013547840815e-05,
      "loss": 1.0136,
      "step": 4490
    },
    {
      "epoch": 0.28518013878766757,
      "grad_norm": 2.5287153720855713,
      "learning_rate": 2.720766299745978e-05,
      "loss": 0.9847,
      "step": 4500
    },
    {
      "epoch": 0.2858138724294179,
      "grad_norm": 2.5321927070617676,
      "learning_rate": 2.7201312447078748e-05,
      "loss": 0.9684,
      "step": 4510
    },
    {
      "epoch": 0.2864476060711683,
      "grad_norm": 2.255659580230713,
      "learning_rate": 2.7194961896697714e-05,
      "loss": 0.977,
      "step": 4520
    },
    {
      "epoch": 0.28708133971291866,
      "grad_norm": 2.6872975826263428,
      "learning_rate": 2.718861134631668e-05,
      "loss": 1.0042,
      "step": 4530
    },
    {
      "epoch": 0.28771507335466906,
      "grad_norm": 2.101151943206787,
      "learning_rate": 2.7182260795935647e-05,
      "loss": 0.9569,
      "step": 4540
    },
    {
      "epoch": 0.2883488069964194,
      "grad_norm": 2.4000866413116455,
      "learning_rate": 2.7175910245554617e-05,
      "loss": 0.9892,
      "step": 4550
    },
    {
      "epoch": 0.2889825406381698,
      "grad_norm": 2.7204229831695557,
      "learning_rate": 2.7169559695173583e-05,
      "loss": 0.9937,
      "step": 4560
    },
    {
      "epoch": 0.28961627427992015,
      "grad_norm": 2.3743245601654053,
      "learning_rate": 2.7163209144792546e-05,
      "loss": 0.9737,
      "step": 4570
    },
    {
      "epoch": 0.2902500079216705,
      "grad_norm": 2.0039381980895996,
      "learning_rate": 2.7156858594411516e-05,
      "loss": 0.9856,
      "step": 4580
    },
    {
      "epoch": 0.2908837415634209,
      "grad_norm": 2.4298601150512695,
      "learning_rate": 2.7150508044030482e-05,
      "loss": 0.9554,
      "step": 4590
    },
    {
      "epoch": 0.29151747520517124,
      "grad_norm": 2.1136341094970703,
      "learning_rate": 2.7144157493649452e-05,
      "loss": 0.9599,
      "step": 4600
    },
    {
      "epoch": 0.29215120884692164,
      "grad_norm": 2.083838701248169,
      "learning_rate": 2.713780694326842e-05,
      "loss": 0.9749,
      "step": 4610
    },
    {
      "epoch": 0.292784942488672,
      "grad_norm": 2.115694284439087,
      "learning_rate": 2.7131456392887382e-05,
      "loss": 1.0076,
      "step": 4620
    },
    {
      "epoch": 0.2934186761304224,
      "grad_norm": 2.1778616905212402,
      "learning_rate": 2.712510584250635e-05,
      "loss": 1.0089,
      "step": 4630
    },
    {
      "epoch": 0.29405240977217273,
      "grad_norm": 2.1838760375976562,
      "learning_rate": 2.7118755292125318e-05,
      "loss": 0.9915,
      "step": 4640
    },
    {
      "epoch": 0.29468614341392313,
      "grad_norm": 2.033905267715454,
      "learning_rate": 2.7112404741744284e-05,
      "loss": 0.985,
      "step": 4650
    },
    {
      "epoch": 0.2953198770556735,
      "grad_norm": 2.114168405532837,
      "learning_rate": 2.7106054191363254e-05,
      "loss": 1.0211,
      "step": 4660
    },
    {
      "epoch": 0.2959536106974239,
      "grad_norm": 2.4453744888305664,
      "learning_rate": 2.709970364098222e-05,
      "loss": 1.0002,
      "step": 4670
    },
    {
      "epoch": 0.2965873443391742,
      "grad_norm": 2.2126893997192383,
      "learning_rate": 2.7093353090601184e-05,
      "loss": 0.9819,
      "step": 4680
    },
    {
      "epoch": 0.2972210779809246,
      "grad_norm": 2.129840135574341,
      "learning_rate": 2.7087002540220154e-05,
      "loss": 0.9784,
      "step": 4690
    },
    {
      "epoch": 0.29785481162267496,
      "grad_norm": 2.2191386222839355,
      "learning_rate": 2.708065198983912e-05,
      "loss": 0.9817,
      "step": 4700
    },
    {
      "epoch": 0.29848854526442536,
      "grad_norm": 2.50726318359375,
      "learning_rate": 2.7074301439458086e-05,
      "loss": 0.9759,
      "step": 4710
    },
    {
      "epoch": 0.2991222789061757,
      "grad_norm": 2.397005558013916,
      "learning_rate": 2.7067950889077056e-05,
      "loss": 0.9714,
      "step": 4720
    },
    {
      "epoch": 0.2997560125479261,
      "grad_norm": 2.583728075027466,
      "learning_rate": 2.706160033869602e-05,
      "loss": 0.9765,
      "step": 4730
    },
    {
      "epoch": 0.30038974618967645,
      "grad_norm": 2.479814052581787,
      "learning_rate": 2.705524978831499e-05,
      "loss": 0.9664,
      "step": 4740
    },
    {
      "epoch": 0.30102347983142685,
      "grad_norm": 2.2237017154693604,
      "learning_rate": 2.7048899237933955e-05,
      "loss": 0.968,
      "step": 4750
    },
    {
      "epoch": 0.3016572134731772,
      "grad_norm": 2.219372034072876,
      "learning_rate": 2.7042548687552922e-05,
      "loss": 0.9674,
      "step": 4760
    },
    {
      "epoch": 0.3022909471149276,
      "grad_norm": 2.5179896354675293,
      "learning_rate": 2.7036198137171892e-05,
      "loss": 1.0244,
      "step": 4770
    },
    {
      "epoch": 0.30292468075667794,
      "grad_norm": 2.0227856636047363,
      "learning_rate": 2.7029847586790855e-05,
      "loss": 0.9809,
      "step": 4780
    },
    {
      "epoch": 0.30355841439842834,
      "grad_norm": 2.179152488708496,
      "learning_rate": 2.702349703640982e-05,
      "loss": 0.9986,
      "step": 4790
    },
    {
      "epoch": 0.3041921480401787,
      "grad_norm": 1.9788833856582642,
      "learning_rate": 2.701714648602879e-05,
      "loss": 1.0,
      "step": 4800
    },
    {
      "epoch": 0.3048258816819291,
      "grad_norm": 2.0417261123657227,
      "learning_rate": 2.7010795935647757e-05,
      "loss": 0.9503,
      "step": 4810
    },
    {
      "epoch": 0.30545961532367943,
      "grad_norm": 2.280271053314209,
      "learning_rate": 2.7004445385266724e-05,
      "loss": 0.9776,
      "step": 4820
    },
    {
      "epoch": 0.30609334896542983,
      "grad_norm": 2.8256092071533203,
      "learning_rate": 2.699809483488569e-05,
      "loss": 0.9979,
      "step": 4830
    },
    {
      "epoch": 0.3067270826071802,
      "grad_norm": 2.0850412845611572,
      "learning_rate": 2.6991744284504657e-05,
      "loss": 0.9639,
      "step": 4840
    },
    {
      "epoch": 0.3073608162489306,
      "grad_norm": 2.646437883377075,
      "learning_rate": 2.6985393734123623e-05,
      "loss": 1.005,
      "step": 4850
    },
    {
      "epoch": 0.3079945498906809,
      "grad_norm": 2.949681282043457,
      "learning_rate": 2.6979043183742593e-05,
      "loss": 0.9974,
      "step": 4860
    },
    {
      "epoch": 0.3086282835324313,
      "grad_norm": 2.350914478302002,
      "learning_rate": 2.697269263336156e-05,
      "loss": 1.0057,
      "step": 4870
    },
    {
      "epoch": 0.30926201717418167,
      "grad_norm": 2.490952730178833,
      "learning_rate": 2.6966342082980522e-05,
      "loss": 1.0388,
      "step": 4880
    },
    {
      "epoch": 0.30989575081593207,
      "grad_norm": 2.136080741882324,
      "learning_rate": 2.6959991532599492e-05,
      "loss": 0.9756,
      "step": 4890
    },
    {
      "epoch": 0.3105294844576824,
      "grad_norm": 2.738816261291504,
      "learning_rate": 2.695364098221846e-05,
      "loss": 0.9866,
      "step": 4900
    },
    {
      "epoch": 0.3111632180994328,
      "grad_norm": 2.271932363510132,
      "learning_rate": 2.694729043183743e-05,
      "loss": 0.9722,
      "step": 4910
    },
    {
      "epoch": 0.31179695174118316,
      "grad_norm": 2.4065098762512207,
      "learning_rate": 2.6940939881456395e-05,
      "loss": 0.9933,
      "step": 4920
    },
    {
      "epoch": 0.31243068538293356,
      "grad_norm": 2.6142311096191406,
      "learning_rate": 2.693458933107536e-05,
      "loss": 0.9756,
      "step": 4930
    },
    {
      "epoch": 0.3130644190246839,
      "grad_norm": 2.231361150741577,
      "learning_rate": 2.6928238780694328e-05,
      "loss": 1.0374,
      "step": 4940
    },
    {
      "epoch": 0.3136981526664343,
      "grad_norm": 2.5703837871551514,
      "learning_rate": 2.6921888230313294e-05,
      "loss": 0.9856,
      "step": 4950
    },
    {
      "epoch": 0.31433188630818465,
      "grad_norm": 2.273010730743408,
      "learning_rate": 2.691553767993226e-05,
      "loss": 0.9473,
      "step": 4960
    },
    {
      "epoch": 0.31496561994993505,
      "grad_norm": 2.100154161453247,
      "learning_rate": 2.690918712955123e-05,
      "loss": 0.9999,
      "step": 4970
    },
    {
      "epoch": 0.3155993535916854,
      "grad_norm": 2.221238613128662,
      "learning_rate": 2.6902836579170197e-05,
      "loss": 0.9881,
      "step": 4980
    },
    {
      "epoch": 0.3162330872334358,
      "grad_norm": 2.129283905029297,
      "learning_rate": 2.689648602878916e-05,
      "loss": 0.9997,
      "step": 4990
    },
    {
      "epoch": 0.31686682087518614,
      "grad_norm": 2.210181474685669,
      "learning_rate": 2.689013547840813e-05,
      "loss": 0.954,
      "step": 5000
    },
    {
      "epoch": 0.31750055451693654,
      "grad_norm": 2.6119396686553955,
      "learning_rate": 2.6883784928027096e-05,
      "loss": 0.951,
      "step": 5010
    },
    {
      "epoch": 0.3181342881586869,
      "grad_norm": 2.083984375,
      "learning_rate": 2.6877434377646063e-05,
      "loss": 0.9688,
      "step": 5020
    },
    {
      "epoch": 0.3187680218004373,
      "grad_norm": 2.7602462768554688,
      "learning_rate": 2.6871083827265032e-05,
      "loss": 0.9952,
      "step": 5030
    },
    {
      "epoch": 0.31940175544218763,
      "grad_norm": 2.387117385864258,
      "learning_rate": 2.6864733276883995e-05,
      "loss": 0.9989,
      "step": 5040
    },
    {
      "epoch": 0.32003548908393803,
      "grad_norm": 2.4308533668518066,
      "learning_rate": 2.6858382726502965e-05,
      "loss": 1.0259,
      "step": 5050
    },
    {
      "epoch": 0.3206692227256884,
      "grad_norm": 2.4051294326782227,
      "learning_rate": 2.685203217612193e-05,
      "loss": 0.9788,
      "step": 5060
    },
    {
      "epoch": 0.3213029563674388,
      "grad_norm": 2.3584930896759033,
      "learning_rate": 2.6845681625740898e-05,
      "loss": 0.9575,
      "step": 5070
    },
    {
      "epoch": 0.3219366900091891,
      "grad_norm": 2.0702648162841797,
      "learning_rate": 2.6839331075359868e-05,
      "loss": 0.9455,
      "step": 5080
    },
    {
      "epoch": 0.3225704236509395,
      "grad_norm": 2.706589937210083,
      "learning_rate": 2.683298052497883e-05,
      "loss": 0.985,
      "step": 5090
    },
    {
      "epoch": 0.32320415729268986,
      "grad_norm": 2.910306215286255,
      "learning_rate": 2.6826629974597797e-05,
      "loss": 0.9601,
      "step": 5100
    },
    {
      "epoch": 0.32383789093444026,
      "grad_norm": 2.5598220825195312,
      "learning_rate": 2.6820279424216767e-05,
      "loss": 1.0027,
      "step": 5110
    },
    {
      "epoch": 0.3244716245761906,
      "grad_norm": 2.0851387977600098,
      "learning_rate": 2.6813928873835734e-05,
      "loss": 0.9824,
      "step": 5120
    },
    {
      "epoch": 0.325105358217941,
      "grad_norm": 2.4126203060150146,
      "learning_rate": 2.68075783234547e-05,
      "loss": 0.9983,
      "step": 5130
    },
    {
      "epoch": 0.32573909185969135,
      "grad_norm": 2.2933716773986816,
      "learning_rate": 2.6801227773073666e-05,
      "loss": 0.9611,
      "step": 5140
    },
    {
      "epoch": 0.32637282550144175,
      "grad_norm": 2.0498604774475098,
      "learning_rate": 2.6794877222692633e-05,
      "loss": 0.979,
      "step": 5150
    },
    {
      "epoch": 0.3270065591431921,
      "grad_norm": 2.3225576877593994,
      "learning_rate": 2.67885266723116e-05,
      "loss": 0.9921,
      "step": 5160
    },
    {
      "epoch": 0.3276402927849425,
      "grad_norm": 2.2657036781311035,
      "learning_rate": 2.678217612193057e-05,
      "loss": 1.0007,
      "step": 5170
    },
    {
      "epoch": 0.32827402642669284,
      "grad_norm": 3.0919318199157715,
      "learning_rate": 2.6775825571549536e-05,
      "loss": 0.9798,
      "step": 5180
    },
    {
      "epoch": 0.32890776006844324,
      "grad_norm": 2.391502618789673,
      "learning_rate": 2.6769475021168502e-05,
      "loss": 0.9617,
      "step": 5190
    },
    {
      "epoch": 0.3295414937101936,
      "grad_norm": 2.5995659828186035,
      "learning_rate": 2.676312447078747e-05,
      "loss": 1.0121,
      "step": 5200
    },
    {
      "epoch": 0.330175227351944,
      "grad_norm": 2.2573728561401367,
      "learning_rate": 2.6756773920406435e-05,
      "loss": 0.9946,
      "step": 5210
    },
    {
      "epoch": 0.33080896099369433,
      "grad_norm": 2.1184000968933105,
      "learning_rate": 2.6750423370025405e-05,
      "loss": 1.0086,
      "step": 5220
    },
    {
      "epoch": 0.33144269463544473,
      "grad_norm": 2.099074125289917,
      "learning_rate": 2.674407281964437e-05,
      "loss": 0.933,
      "step": 5230
    },
    {
      "epoch": 0.3320764282771951,
      "grad_norm": 2.507707118988037,
      "learning_rate": 2.6737722269263337e-05,
      "loss": 0.9689,
      "step": 5240
    },
    {
      "epoch": 0.3327101619189455,
      "grad_norm": 2.0602450370788574,
      "learning_rate": 2.6731371718882304e-05,
      "loss": 0.9531,
      "step": 5250
    },
    {
      "epoch": 0.3333438955606958,
      "grad_norm": 2.2188332080841064,
      "learning_rate": 2.672502116850127e-05,
      "loss": 1.0134,
      "step": 5260
    },
    {
      "epoch": 0.3339776292024462,
      "grad_norm": 2.154158353805542,
      "learning_rate": 2.6718670618120237e-05,
      "loss": 0.9935,
      "step": 5270
    },
    {
      "epoch": 0.33461136284419657,
      "grad_norm": 2.358680486679077,
      "learning_rate": 2.6712320067739207e-05,
      "loss": 0.9904,
      "step": 5280
    },
    {
      "epoch": 0.33524509648594697,
      "grad_norm": 2.3714752197265625,
      "learning_rate": 2.6705969517358173e-05,
      "loss": 0.9702,
      "step": 5290
    },
    {
      "epoch": 0.3358788301276973,
      "grad_norm": 2.183443307876587,
      "learning_rate": 2.6699618966977136e-05,
      "loss": 0.9814,
      "step": 5300
    },
    {
      "epoch": 0.3365125637694477,
      "grad_norm": 2.585148334503174,
      "learning_rate": 2.6693268416596106e-05,
      "loss": 0.9602,
      "step": 5310
    },
    {
      "epoch": 0.33714629741119806,
      "grad_norm": 2.1341023445129395,
      "learning_rate": 2.6686917866215072e-05,
      "loss": 0.9803,
      "step": 5320
    },
    {
      "epoch": 0.33778003105294846,
      "grad_norm": 2.952950954437256,
      "learning_rate": 2.668056731583404e-05,
      "loss": 0.9912,
      "step": 5330
    },
    {
      "epoch": 0.3384137646946988,
      "grad_norm": 1.9648027420043945,
      "learning_rate": 2.667421676545301e-05,
      "loss": 0.9821,
      "step": 5340
    },
    {
      "epoch": 0.3390474983364492,
      "grad_norm": 2.412372589111328,
      "learning_rate": 2.666786621507197e-05,
      "loss": 0.9329,
      "step": 5350
    },
    {
      "epoch": 0.33968123197819955,
      "grad_norm": 2.1162233352661133,
      "learning_rate": 2.666151566469094e-05,
      "loss": 0.9524,
      "step": 5360
    },
    {
      "epoch": 0.34031496561994995,
      "grad_norm": 2.759885787963867,
      "learning_rate": 2.6655165114309908e-05,
      "loss": 0.9729,
      "step": 5370
    },
    {
      "epoch": 0.3409486992617003,
      "grad_norm": 2.712097406387329,
      "learning_rate": 2.6648814563928874e-05,
      "loss": 1.0169,
      "step": 5380
    },
    {
      "epoch": 0.3415824329034507,
      "grad_norm": 1.9833990335464478,
      "learning_rate": 2.6642464013547844e-05,
      "loss": 0.9521,
      "step": 5390
    },
    {
      "epoch": 0.34221616654520104,
      "grad_norm": 2.1856935024261475,
      "learning_rate": 2.6636113463166807e-05,
      "loss": 0.9816,
      "step": 5400
    },
    {
      "epoch": 0.34284990018695144,
      "grad_norm": 2.630599021911621,
      "learning_rate": 2.6629762912785774e-05,
      "loss": 0.9782,
      "step": 5410
    },
    {
      "epoch": 0.3434836338287018,
      "grad_norm": 2.236259698867798,
      "learning_rate": 2.6623412362404743e-05,
      "loss": 1.0193,
      "step": 5420
    },
    {
      "epoch": 0.3441173674704522,
      "grad_norm": 2.3419270515441895,
      "learning_rate": 2.661706181202371e-05,
      "loss": 0.9384,
      "step": 5430
    },
    {
      "epoch": 0.34475110111220253,
      "grad_norm": 1.9995434284210205,
      "learning_rate": 2.6610711261642676e-05,
      "loss": 0.9782,
      "step": 5440
    },
    {
      "epoch": 0.34538483475395293,
      "grad_norm": 2.515993356704712,
      "learning_rate": 2.6604360711261646e-05,
      "loss": 0.9895,
      "step": 5450
    },
    {
      "epoch": 0.3460185683957033,
      "grad_norm": 2.5864388942718506,
      "learning_rate": 2.659801016088061e-05,
      "loss": 1.0147,
      "step": 5460
    },
    {
      "epoch": 0.3466523020374537,
      "grad_norm": 2.553481101989746,
      "learning_rate": 2.6591659610499575e-05,
      "loss": 0.9985,
      "step": 5470
    },
    {
      "epoch": 0.347286035679204,
      "grad_norm": 2.193030595779419,
      "learning_rate": 2.6585309060118545e-05,
      "loss": 0.9896,
      "step": 5480
    },
    {
      "epoch": 0.3479197693209544,
      "grad_norm": 2.0924606323242188,
      "learning_rate": 2.6578958509737512e-05,
      "loss": 0.9425,
      "step": 5490
    },
    {
      "epoch": 0.34855350296270476,
      "grad_norm": 2.1207547187805176,
      "learning_rate": 2.6572607959356478e-05,
      "loss": 1.0401,
      "step": 5500
    },
    {
      "epoch": 0.34918723660445516,
      "grad_norm": 2.2998645305633545,
      "learning_rate": 2.6566257408975445e-05,
      "loss": 0.9838,
      "step": 5510
    },
    {
      "epoch": 0.3498209702462055,
      "grad_norm": 2.48590350151062,
      "learning_rate": 2.655990685859441e-05,
      "loss": 1.0326,
      "step": 5520
    },
    {
      "epoch": 0.3504547038879559,
      "grad_norm": 2.5302016735076904,
      "learning_rate": 2.655355630821338e-05,
      "loss": 0.9616,
      "step": 5530
    },
    {
      "epoch": 0.35108843752970625,
      "grad_norm": 2.4432337284088135,
      "learning_rate": 2.6547205757832347e-05,
      "loss": 0.9928,
      "step": 5540
    },
    {
      "epoch": 0.35172217117145665,
      "grad_norm": 2.252192258834839,
      "learning_rate": 2.6540855207451314e-05,
      "loss": 0.9654,
      "step": 5550
    },
    {
      "epoch": 0.352355904813207,
      "grad_norm": 2.0626137256622314,
      "learning_rate": 2.653450465707028e-05,
      "loss": 1.0056,
      "step": 5560
    },
    {
      "epoch": 0.3529896384549574,
      "grad_norm": 2.3848471641540527,
      "learning_rate": 2.6528154106689247e-05,
      "loss": 1.0032,
      "step": 5570
    },
    {
      "epoch": 0.35362337209670774,
      "grad_norm": 2.2800145149230957,
      "learning_rate": 2.6521803556308213e-05,
      "loss": 0.9807,
      "step": 5580
    },
    {
      "epoch": 0.35425710573845814,
      "grad_norm": 3.3787589073181152,
      "learning_rate": 2.6515453005927183e-05,
      "loss": 0.9884,
      "step": 5590
    },
    {
      "epoch": 0.3548908393802085,
      "grad_norm": 2.0658695697784424,
      "learning_rate": 2.650910245554615e-05,
      "loss": 0.9838,
      "step": 5600
    },
    {
      "epoch": 0.3555245730219589,
      "grad_norm": 2.1459081172943115,
      "learning_rate": 2.6502751905165112e-05,
      "loss": 0.9714,
      "step": 5610
    },
    {
      "epoch": 0.35615830666370923,
      "grad_norm": 2.1199002265930176,
      "learning_rate": 2.6496401354784082e-05,
      "loss": 0.9642,
      "step": 5620
    },
    {
      "epoch": 0.35679204030545963,
      "grad_norm": 2.487696647644043,
      "learning_rate": 2.649005080440305e-05,
      "loss": 1.0063,
      "step": 5630
    },
    {
      "epoch": 0.35742577394721,
      "grad_norm": 2.2940542697906494,
      "learning_rate": 2.6483700254022015e-05,
      "loss": 1.0081,
      "step": 5640
    },
    {
      "epoch": 0.3580595075889604,
      "grad_norm": 2.1990561485290527,
      "learning_rate": 2.6477349703640985e-05,
      "loss": 0.9849,
      "step": 5650
    },
    {
      "epoch": 0.3586932412307107,
      "grad_norm": 2.6023218631744385,
      "learning_rate": 2.6470999153259948e-05,
      "loss": 0.959,
      "step": 5660
    },
    {
      "epoch": 0.3593269748724611,
      "grad_norm": 2.2632346153259277,
      "learning_rate": 2.6464648602878914e-05,
      "loss": 0.9185,
      "step": 5670
    },
    {
      "epoch": 0.35996070851421147,
      "grad_norm": 2.2142679691314697,
      "learning_rate": 2.6458298052497884e-05,
      "loss": 0.96,
      "step": 5680
    },
    {
      "epoch": 0.36059444215596187,
      "grad_norm": 2.4688761234283447,
      "learning_rate": 2.645194750211685e-05,
      "loss": 1.0063,
      "step": 5690
    },
    {
      "epoch": 0.3612281757977122,
      "grad_norm": 3.3397998809814453,
      "learning_rate": 2.644559695173582e-05,
      "loss": 0.9828,
      "step": 5700
    },
    {
      "epoch": 0.3618619094394626,
      "grad_norm": 2.9049816131591797,
      "learning_rate": 2.6439246401354787e-05,
      "loss": 1.0085,
      "step": 5710
    },
    {
      "epoch": 0.36249564308121296,
      "grad_norm": 2.587655782699585,
      "learning_rate": 2.643289585097375e-05,
      "loss": 0.9738,
      "step": 5720
    },
    {
      "epoch": 0.36312937672296336,
      "grad_norm": 2.3533780574798584,
      "learning_rate": 2.642654530059272e-05,
      "loss": 0.9912,
      "step": 5730
    },
    {
      "epoch": 0.3637631103647137,
      "grad_norm": 2.5258591175079346,
      "learning_rate": 2.6420194750211686e-05,
      "loss": 0.9583,
      "step": 5740
    },
    {
      "epoch": 0.3643968440064641,
      "grad_norm": 2.355693817138672,
      "learning_rate": 2.6413844199830652e-05,
      "loss": 0.9961,
      "step": 5750
    },
    {
      "epoch": 0.36503057764821445,
      "grad_norm": 2.1473240852355957,
      "learning_rate": 2.6407493649449622e-05,
      "loss": 0.9625,
      "step": 5760
    },
    {
      "epoch": 0.36566431128996485,
      "grad_norm": 2.167174816131592,
      "learning_rate": 2.6401143099068585e-05,
      "loss": 0.9823,
      "step": 5770
    },
    {
      "epoch": 0.3662980449317152,
      "grad_norm": 2.056626796722412,
      "learning_rate": 2.639479254868755e-05,
      "loss": 1.0084,
      "step": 5780
    },
    {
      "epoch": 0.3669317785734656,
      "grad_norm": 2.263904094696045,
      "learning_rate": 2.638844199830652e-05,
      "loss": 0.9556,
      "step": 5790
    },
    {
      "epoch": 0.36756551221521594,
      "grad_norm": 2.5295770168304443,
      "learning_rate": 2.6382091447925488e-05,
      "loss": 0.9504,
      "step": 5800
    },
    {
      "epoch": 0.36819924585696634,
      "grad_norm": 2.4721922874450684,
      "learning_rate": 2.6375740897544454e-05,
      "loss": 0.9703,
      "step": 5810
    },
    {
      "epoch": 0.3688329794987167,
      "grad_norm": 2.698338747024536,
      "learning_rate": 2.636939034716342e-05,
      "loss": 1.0085,
      "step": 5820
    },
    {
      "epoch": 0.3694667131404671,
      "grad_norm": 2.630523204803467,
      "learning_rate": 2.6363039796782387e-05,
      "loss": 0.9833,
      "step": 5830
    },
    {
      "epoch": 0.37010044678221743,
      "grad_norm": 2.4700939655303955,
      "learning_rate": 2.6356689246401357e-05,
      "loss": 1.0091,
      "step": 5840
    },
    {
      "epoch": 0.37073418042396783,
      "grad_norm": 2.226928949356079,
      "learning_rate": 2.6350338696020323e-05,
      "loss": 1.0396,
      "step": 5850
    },
    {
      "epoch": 0.3713679140657182,
      "grad_norm": 2.416705846786499,
      "learning_rate": 2.634398814563929e-05,
      "loss": 0.9469,
      "step": 5860
    },
    {
      "epoch": 0.3720016477074686,
      "grad_norm": 2.2262980937957764,
      "learning_rate": 2.6337637595258256e-05,
      "loss": 0.975,
      "step": 5870
    },
    {
      "epoch": 0.3726353813492189,
      "grad_norm": 2.1909263134002686,
      "learning_rate": 2.6331287044877223e-05,
      "loss": 0.9787,
      "step": 5880
    },
    {
      "epoch": 0.3732691149909693,
      "grad_norm": 2.7040016651153564,
      "learning_rate": 2.632493649449619e-05,
      "loss": 0.945,
      "step": 5890
    },
    {
      "epoch": 0.37390284863271966,
      "grad_norm": 2.4241106510162354,
      "learning_rate": 2.631858594411516e-05,
      "loss": 0.9885,
      "step": 5900
    },
    {
      "epoch": 0.37453658227447006,
      "grad_norm": 2.5355191230773926,
      "learning_rate": 2.6312235393734125e-05,
      "loss": 0.9555,
      "step": 5910
    },
    {
      "epoch": 0.3751703159162204,
      "grad_norm": 2.4981067180633545,
      "learning_rate": 2.630588484335309e-05,
      "loss": 0.9707,
      "step": 5920
    },
    {
      "epoch": 0.3758040495579708,
      "grad_norm": 2.410797595977783,
      "learning_rate": 2.6299534292972058e-05,
      "loss": 0.9498,
      "step": 5930
    },
    {
      "epoch": 0.37643778319972115,
      "grad_norm": 2.2229530811309814,
      "learning_rate": 2.6293183742591025e-05,
      "loss": 0.9622,
      "step": 5940
    },
    {
      "epoch": 0.37707151684147155,
      "grad_norm": 3.1553850173950195,
      "learning_rate": 2.628683319220999e-05,
      "loss": 1.0272,
      "step": 5950
    },
    {
      "epoch": 0.3777052504832219,
      "grad_norm": 2.428607225418091,
      "learning_rate": 2.628048264182896e-05,
      "loss": 0.9522,
      "step": 5960
    },
    {
      "epoch": 0.3783389841249723,
      "grad_norm": 2.739508628845215,
      "learning_rate": 2.6274132091447927e-05,
      "loss": 0.9749,
      "step": 5970
    },
    {
      "epoch": 0.37897271776672264,
      "grad_norm": 2.2851216793060303,
      "learning_rate": 2.626778154106689e-05,
      "loss": 0.9795,
      "step": 5980
    },
    {
      "epoch": 0.37960645140847304,
      "grad_norm": 2.146599531173706,
      "learning_rate": 2.626143099068586e-05,
      "loss": 0.978,
      "step": 5990
    },
    {
      "epoch": 0.3802401850502234,
      "grad_norm": 2.687757968902588,
      "learning_rate": 2.6255080440304827e-05,
      "loss": 0.9615,
      "step": 6000
    },
    {
      "epoch": 0.3808739186919738,
      "grad_norm": 2.401304006576538,
      "learning_rate": 2.6248729889923796e-05,
      "loss": 0.9319,
      "step": 6010
    },
    {
      "epoch": 0.38150765233372413,
      "grad_norm": 2.6312096118927,
      "learning_rate": 2.6242379339542763e-05,
      "loss": 0.9924,
      "step": 6020
    },
    {
      "epoch": 0.38214138597547453,
      "grad_norm": 2.520496368408203,
      "learning_rate": 2.6236028789161726e-05,
      "loss": 1.0026,
      "step": 6030
    },
    {
      "epoch": 0.3827751196172249,
      "grad_norm": 2.4763574600219727,
      "learning_rate": 2.6229678238780696e-05,
      "loss": 0.9557,
      "step": 6040
    },
    {
      "epoch": 0.3834088532589753,
      "grad_norm": 2.0288970470428467,
      "learning_rate": 2.6223327688399662e-05,
      "loss": 0.9498,
      "step": 6050
    },
    {
      "epoch": 0.3840425869007256,
      "grad_norm": 2.3276567459106445,
      "learning_rate": 2.621697713801863e-05,
      "loss": 0.9589,
      "step": 6060
    },
    {
      "epoch": 0.384676320542476,
      "grad_norm": 3.0233349800109863,
      "learning_rate": 2.62106265876376e-05,
      "loss": 1.0085,
      "step": 6070
    },
    {
      "epoch": 0.38531005418422637,
      "grad_norm": 2.2996909618377686,
      "learning_rate": 2.620427603725656e-05,
      "loss": 1.008,
      "step": 6080
    },
    {
      "epoch": 0.38594378782597677,
      "grad_norm": 2.214984178543091,
      "learning_rate": 2.6197925486875528e-05,
      "loss": 0.9464,
      "step": 6090
    },
    {
      "epoch": 0.3865775214677271,
      "grad_norm": 2.122485637664795,
      "learning_rate": 2.6191574936494498e-05,
      "loss": 1.0011,
      "step": 6100
    },
    {
      "epoch": 0.38721125510947746,
      "grad_norm": 2.1969258785247803,
      "learning_rate": 2.6185224386113464e-05,
      "loss": 0.9677,
      "step": 6110
    },
    {
      "epoch": 0.38784498875122786,
      "grad_norm": 2.2543444633483887,
      "learning_rate": 2.617887383573243e-05,
      "loss": 1.004,
      "step": 6120
    },
    {
      "epoch": 0.3884787223929782,
      "grad_norm": 2.36507248878479,
      "learning_rate": 2.6172523285351397e-05,
      "loss": 1.0133,
      "step": 6130
    },
    {
      "epoch": 0.3891124560347286,
      "grad_norm": 2.3757476806640625,
      "learning_rate": 2.6166172734970363e-05,
      "loss": 0.996,
      "step": 6140
    },
    {
      "epoch": 0.38974618967647895,
      "grad_norm": 2.559335470199585,
      "learning_rate": 2.6159822184589333e-05,
      "loss": 0.969,
      "step": 6150
    },
    {
      "epoch": 0.39037992331822935,
      "grad_norm": 2.4770450592041016,
      "learning_rate": 2.61534716342083e-05,
      "loss": 0.97,
      "step": 6160
    },
    {
      "epoch": 0.3910136569599797,
      "grad_norm": 2.0932118892669678,
      "learning_rate": 2.6147121083827266e-05,
      "loss": 0.9679,
      "step": 6170
    },
    {
      "epoch": 0.3916473906017301,
      "grad_norm": 2.3895809650421143,
      "learning_rate": 2.6140770533446232e-05,
      "loss": 0.9621,
      "step": 6180
    },
    {
      "epoch": 0.39228112424348044,
      "grad_norm": 2.157581090927124,
      "learning_rate": 2.61344199830652e-05,
      "loss": 0.9749,
      "step": 6190
    },
    {
      "epoch": 0.39291485788523084,
      "grad_norm": 2.647714138031006,
      "learning_rate": 2.6128069432684165e-05,
      "loss": 0.9582,
      "step": 6200
    },
    {
      "epoch": 0.3935485915269812,
      "grad_norm": 2.549042224884033,
      "learning_rate": 2.6121718882303135e-05,
      "loss": 0.9924,
      "step": 6210
    },
    {
      "epoch": 0.3941823251687316,
      "grad_norm": 2.05676531791687,
      "learning_rate": 2.61153683319221e-05,
      "loss": 0.9777,
      "step": 6220
    },
    {
      "epoch": 0.39481605881048193,
      "grad_norm": 2.597990036010742,
      "learning_rate": 2.6109017781541068e-05,
      "loss": 0.9879,
      "step": 6230
    },
    {
      "epoch": 0.39544979245223233,
      "grad_norm": 2.5945181846618652,
      "learning_rate": 2.6102667231160034e-05,
      "loss": 1.0187,
      "step": 6240
    },
    {
      "epoch": 0.3960835260939827,
      "grad_norm": 2.2339704036712646,
      "learning_rate": 2.6096316680779e-05,
      "loss": 0.9442,
      "step": 6250
    },
    {
      "epoch": 0.3967172597357331,
      "grad_norm": 2.1201157569885254,
      "learning_rate": 2.6089966130397967e-05,
      "loss": 0.9685,
      "step": 6260
    },
    {
      "epoch": 0.3973509933774834,
      "grad_norm": 2.1350791454315186,
      "learning_rate": 2.6083615580016937e-05,
      "loss": 0.9331,
      "step": 6270
    },
    {
      "epoch": 0.3979847270192338,
      "grad_norm": 2.2396140098571777,
      "learning_rate": 2.6077265029635903e-05,
      "loss": 0.9776,
      "step": 6280
    },
    {
      "epoch": 0.39861846066098416,
      "grad_norm": 1.993811011314392,
      "learning_rate": 2.6070914479254867e-05,
      "loss": 0.9339,
      "step": 6290
    },
    {
      "epoch": 0.39925219430273456,
      "grad_norm": 2.112387180328369,
      "learning_rate": 2.6064563928873836e-05,
      "loss": 0.9657,
      "step": 6300
    },
    {
      "epoch": 0.3998859279444849,
      "grad_norm": 2.3971803188323975,
      "learning_rate": 2.6058213378492803e-05,
      "loss": 0.9827,
      "step": 6310
    },
    {
      "epoch": 0.4005196615862353,
      "grad_norm": 2.601036548614502,
      "learning_rate": 2.6051862828111773e-05,
      "loss": 0.9569,
      "step": 6320
    },
    {
      "epoch": 0.40115339522798565,
      "grad_norm": 2.7897446155548096,
      "learning_rate": 2.604551227773074e-05,
      "loss": 0.961,
      "step": 6330
    },
    {
      "epoch": 0.40178712886973605,
      "grad_norm": 2.0838623046875,
      "learning_rate": 2.6039161727349702e-05,
      "loss": 0.9432,
      "step": 6340
    },
    {
      "epoch": 0.4024208625114864,
      "grad_norm": 2.9719061851501465,
      "learning_rate": 2.6032811176968672e-05,
      "loss": 1.0083,
      "step": 6350
    },
    {
      "epoch": 0.4030545961532368,
      "grad_norm": 2.1705613136291504,
      "learning_rate": 2.6026460626587638e-05,
      "loss": 0.9412,
      "step": 6360
    },
    {
      "epoch": 0.40368832979498714,
      "grad_norm": 2.8509955406188965,
      "learning_rate": 2.6020110076206605e-05,
      "loss": 0.9794,
      "step": 6370
    },
    {
      "epoch": 0.40432206343673754,
      "grad_norm": 2.5521292686462402,
      "learning_rate": 2.6013759525825575e-05,
      "loss": 0.961,
      "step": 6380
    },
    {
      "epoch": 0.4049557970784879,
      "grad_norm": 2.393019914627075,
      "learning_rate": 2.6007408975444538e-05,
      "loss": 0.9613,
      "step": 6390
    },
    {
      "epoch": 0.4055895307202383,
      "grad_norm": 2.1284282207489014,
      "learning_rate": 2.6001058425063504e-05,
      "loss": 0.9934,
      "step": 6400
    },
    {
      "epoch": 0.40622326436198863,
      "grad_norm": 2.5337812900543213,
      "learning_rate": 2.5994707874682474e-05,
      "loss": 0.9654,
      "step": 6410
    },
    {
      "epoch": 0.40685699800373903,
      "grad_norm": 2.4096899032592773,
      "learning_rate": 2.598835732430144e-05,
      "loss": 0.9831,
      "step": 6420
    },
    {
      "epoch": 0.4074907316454894,
      "grad_norm": 2.4304840564727783,
      "learning_rate": 2.5982006773920407e-05,
      "loss": 0.9521,
      "step": 6430
    },
    {
      "epoch": 0.4081244652872398,
      "grad_norm": 2.3095219135284424,
      "learning_rate": 2.5975656223539373e-05,
      "loss": 0.9566,
      "step": 6440
    },
    {
      "epoch": 0.4087581989289901,
      "grad_norm": 2.433082103729248,
      "learning_rate": 2.596930567315834e-05,
      "loss": 0.9241,
      "step": 6450
    },
    {
      "epoch": 0.4093919325707405,
      "grad_norm": 2.465911388397217,
      "learning_rate": 2.596295512277731e-05,
      "loss": 1.0152,
      "step": 6460
    },
    {
      "epoch": 0.41002566621249087,
      "grad_norm": 2.175802707672119,
      "learning_rate": 2.5956604572396276e-05,
      "loss": 0.9414,
      "step": 6470
    },
    {
      "epoch": 0.41065939985424127,
      "grad_norm": 2.319173812866211,
      "learning_rate": 2.5950254022015242e-05,
      "loss": 0.9612,
      "step": 6480
    },
    {
      "epoch": 0.4112931334959916,
      "grad_norm": 2.46710467338562,
      "learning_rate": 2.5943903471634212e-05,
      "loss": 0.9803,
      "step": 6490
    },
    {
      "epoch": 0.411926867137742,
      "grad_norm": 2.2159438133239746,
      "learning_rate": 2.5937552921253175e-05,
      "loss": 0.9694,
      "step": 6500
    },
    {
      "epoch": 0.41256060077949236,
      "grad_norm": 2.4501302242279053,
      "learning_rate": 2.593120237087214e-05,
      "loss": 0.978,
      "step": 6510
    },
    {
      "epoch": 0.41319433442124276,
      "grad_norm": 2.518714666366577,
      "learning_rate": 2.592485182049111e-05,
      "loss": 1.0192,
      "step": 6520
    },
    {
      "epoch": 0.4138280680629931,
      "grad_norm": 2.1535067558288574,
      "learning_rate": 2.5918501270110078e-05,
      "loss": 0.9884,
      "step": 6530
    },
    {
      "epoch": 0.4144618017047435,
      "grad_norm": 2.346169948577881,
      "learning_rate": 2.5912150719729044e-05,
      "loss": 1.013,
      "step": 6540
    },
    {
      "epoch": 0.41509553534649385,
      "grad_norm": 2.1694605350494385,
      "learning_rate": 2.590580016934801e-05,
      "loss": 0.9822,
      "step": 6550
    },
    {
      "epoch": 0.41572926898824425,
      "grad_norm": 2.454122304916382,
      "learning_rate": 2.5899449618966977e-05,
      "loss": 0.9843,
      "step": 6560
    },
    {
      "epoch": 0.4163630026299946,
      "grad_norm": 2.5382235050201416,
      "learning_rate": 2.5893099068585943e-05,
      "loss": 0.9603,
      "step": 6570
    },
    {
      "epoch": 0.416996736271745,
      "grad_norm": 2.427191972732544,
      "learning_rate": 2.5886748518204913e-05,
      "loss": 0.9498,
      "step": 6580
    },
    {
      "epoch": 0.41763046991349534,
      "grad_norm": 2.3373491764068604,
      "learning_rate": 2.588039796782388e-05,
      "loss": 0.9277,
      "step": 6590
    },
    {
      "epoch": 0.41826420355524574,
      "grad_norm": 2.2922303676605225,
      "learning_rate": 2.5874047417442843e-05,
      "loss": 0.9877,
      "step": 6600
    },
    {
      "epoch": 0.4188979371969961,
      "grad_norm": 2.391180992126465,
      "learning_rate": 2.5867696867061813e-05,
      "loss": 1.0343,
      "step": 6610
    },
    {
      "epoch": 0.4195316708387465,
      "grad_norm": 2.578624725341797,
      "learning_rate": 2.586134631668078e-05,
      "loss": 0.99,
      "step": 6620
    },
    {
      "epoch": 0.42016540448049683,
      "grad_norm": 2.345576047897339,
      "learning_rate": 2.585499576629975e-05,
      "loss": 0.9571,
      "step": 6630
    },
    {
      "epoch": 0.42079913812224723,
      "grad_norm": 2.491687774658203,
      "learning_rate": 2.5848645215918715e-05,
      "loss": 0.9955,
      "step": 6640
    },
    {
      "epoch": 0.4214328717639976,
      "grad_norm": 2.6161715984344482,
      "learning_rate": 2.5842294665537678e-05,
      "loss": 1.0031,
      "step": 6650
    },
    {
      "epoch": 0.422066605405748,
      "grad_norm": 2.871776819229126,
      "learning_rate": 2.5835944115156648e-05,
      "loss": 0.9342,
      "step": 6660
    },
    {
      "epoch": 0.4227003390474983,
      "grad_norm": 2.3553054332733154,
      "learning_rate": 2.5829593564775614e-05,
      "loss": 0.9985,
      "step": 6670
    },
    {
      "epoch": 0.4233340726892487,
      "grad_norm": 1.9586098194122314,
      "learning_rate": 2.582324301439458e-05,
      "loss": 0.9887,
      "step": 6680
    },
    {
      "epoch": 0.42396780633099906,
      "grad_norm": 2.67582106590271,
      "learning_rate": 2.581689246401355e-05,
      "loss": 0.9863,
      "step": 6690
    },
    {
      "epoch": 0.42460153997274946,
      "grad_norm": 1.9338499307632446,
      "learning_rate": 2.5810541913632514e-05,
      "loss": 0.9735,
      "step": 6700
    },
    {
      "epoch": 0.4252352736144998,
      "grad_norm": 3.021202564239502,
      "learning_rate": 2.580419136325148e-05,
      "loss": 0.987,
      "step": 6710
    },
    {
      "epoch": 0.4258690072562502,
      "grad_norm": 2.0804057121276855,
      "learning_rate": 2.579784081287045e-05,
      "loss": 0.9997,
      "step": 6720
    },
    {
      "epoch": 0.42650274089800055,
      "grad_norm": 2.444840669631958,
      "learning_rate": 2.5791490262489416e-05,
      "loss": 1.0022,
      "step": 6730
    },
    {
      "epoch": 0.42713647453975095,
      "grad_norm": 3.01973032951355,
      "learning_rate": 2.5785139712108383e-05,
      "loss": 0.9684,
      "step": 6740
    },
    {
      "epoch": 0.4277702081815013,
      "grad_norm": 2.1451611518859863,
      "learning_rate": 2.5778789161727353e-05,
      "loss": 0.9829,
      "step": 6750
    },
    {
      "epoch": 0.4284039418232517,
      "grad_norm": 2.2611725330352783,
      "learning_rate": 2.5772438611346316e-05,
      "loss": 0.9221,
      "step": 6760
    },
    {
      "epoch": 0.42903767546500204,
      "grad_norm": 2.618743658065796,
      "learning_rate": 2.5766088060965282e-05,
      "loss": 0.947,
      "step": 6770
    },
    {
      "epoch": 0.42967140910675244,
      "grad_norm": 2.2316160202026367,
      "learning_rate": 2.5759737510584252e-05,
      "loss": 0.9787,
      "step": 6780
    },
    {
      "epoch": 0.4303051427485028,
      "grad_norm": 2.2427353858947754,
      "learning_rate": 2.575338696020322e-05,
      "loss": 0.9779,
      "step": 6790
    },
    {
      "epoch": 0.4309388763902532,
      "grad_norm": 2.293799638748169,
      "learning_rate": 2.5747036409822188e-05,
      "loss": 1.0179,
      "step": 6800
    },
    {
      "epoch": 0.43157261003200353,
      "grad_norm": 2.1884677410125732,
      "learning_rate": 2.574068585944115e-05,
      "loss": 0.9622,
      "step": 6810
    },
    {
      "epoch": 0.43220634367375393,
      "grad_norm": 2.8132271766662598,
      "learning_rate": 2.5734335309060118e-05,
      "loss": 0.9908,
      "step": 6820
    },
    {
      "epoch": 0.4328400773155043,
      "grad_norm": 2.325813055038452,
      "learning_rate": 2.5727984758679087e-05,
      "loss": 0.9436,
      "step": 6830
    },
    {
      "epoch": 0.4334738109572547,
      "grad_norm": 2.4362828731536865,
      "learning_rate": 2.5721634208298054e-05,
      "loss": 0.9699,
      "step": 6840
    },
    {
      "epoch": 0.434107544599005,
      "grad_norm": 3.6379172801971436,
      "learning_rate": 2.571528365791702e-05,
      "loss": 0.938,
      "step": 6850
    },
    {
      "epoch": 0.4347412782407554,
      "grad_norm": 2.4353818893432617,
      "learning_rate": 2.5708933107535987e-05,
      "loss": 1.0163,
      "step": 6860
    },
    {
      "epoch": 0.43537501188250577,
      "grad_norm": 2.2664411067962646,
      "learning_rate": 2.5702582557154953e-05,
      "loss": 0.9392,
      "step": 6870
    },
    {
      "epoch": 0.43600874552425617,
      "grad_norm": 2.0883779525756836,
      "learning_rate": 2.569623200677392e-05,
      "loss": 0.9893,
      "step": 6880
    },
    {
      "epoch": 0.4366424791660065,
      "grad_norm": 2.5431132316589355,
      "learning_rate": 2.568988145639289e-05,
      "loss": 0.9891,
      "step": 6890
    },
    {
      "epoch": 0.4372762128077569,
      "grad_norm": 2.162748098373413,
      "learning_rate": 2.5683530906011856e-05,
      "loss": 0.9902,
      "step": 6900
    },
    {
      "epoch": 0.43790994644950726,
      "grad_norm": 2.2197248935699463,
      "learning_rate": 2.567718035563082e-05,
      "loss": 0.961,
      "step": 6910
    },
    {
      "epoch": 0.43854368009125766,
      "grad_norm": 2.2747271060943604,
      "learning_rate": 2.567082980524979e-05,
      "loss": 0.9875,
      "step": 6920
    },
    {
      "epoch": 0.439177413733008,
      "grad_norm": 2.5151398181915283,
      "learning_rate": 2.5664479254868755e-05,
      "loss": 0.9537,
      "step": 6930
    },
    {
      "epoch": 0.4398111473747584,
      "grad_norm": 2.4137887954711914,
      "learning_rate": 2.5658128704487725e-05,
      "loss": 0.961,
      "step": 6940
    },
    {
      "epoch": 0.44044488101650875,
      "grad_norm": 2.3919456005096436,
      "learning_rate": 2.565177815410669e-05,
      "loss": 0.9725,
      "step": 6950
    },
    {
      "epoch": 0.44107861465825915,
      "grad_norm": 2.467475175857544,
      "learning_rate": 2.5645427603725658e-05,
      "loss": 0.9774,
      "step": 6960
    },
    {
      "epoch": 0.4417123483000095,
      "grad_norm": 2.1430552005767822,
      "learning_rate": 2.5639077053344624e-05,
      "loss": 0.9741,
      "step": 6970
    },
    {
      "epoch": 0.4423460819417599,
      "grad_norm": 3.255181312561035,
      "learning_rate": 2.563272650296359e-05,
      "loss": 0.9307,
      "step": 6980
    },
    {
      "epoch": 0.44297981558351024,
      "grad_norm": 2.4745771884918213,
      "learning_rate": 2.5626375952582557e-05,
      "loss": 0.9471,
      "step": 6990
    },
    {
      "epoch": 0.44361354922526064,
      "grad_norm": 2.369776964187622,
      "learning_rate": 2.5620025402201527e-05,
      "loss": 0.9625,
      "step": 7000
    },
    {
      "epoch": 0.444247282867011,
      "grad_norm": 2.1720941066741943,
      "learning_rate": 2.5613674851820493e-05,
      "loss": 0.9822,
      "step": 7010
    },
    {
      "epoch": 0.4448810165087614,
      "grad_norm": 2.176588296890259,
      "learning_rate": 2.5607324301439456e-05,
      "loss": 0.9667,
      "step": 7020
    },
    {
      "epoch": 0.44551475015051173,
      "grad_norm": 2.2355473041534424,
      "learning_rate": 2.5600973751058426e-05,
      "loss": 0.9313,
      "step": 7030
    },
    {
      "epoch": 0.44614848379226213,
      "grad_norm": 2.2668392658233643,
      "learning_rate": 2.5594623200677393e-05,
      "loss": 0.9398,
      "step": 7040
    },
    {
      "epoch": 0.4467822174340125,
      "grad_norm": 3.0307724475860596,
      "learning_rate": 2.558827265029636e-05,
      "loss": 1.0063,
      "step": 7050
    },
    {
      "epoch": 0.4474159510757629,
      "grad_norm": 2.4516139030456543,
      "learning_rate": 2.558192209991533e-05,
      "loss": 0.9475,
      "step": 7060
    },
    {
      "epoch": 0.4480496847175132,
      "grad_norm": 2.8560214042663574,
      "learning_rate": 2.5575571549534292e-05,
      "loss": 0.9692,
      "step": 7070
    },
    {
      "epoch": 0.4486834183592636,
      "grad_norm": 2.1933791637420654,
      "learning_rate": 2.5569220999153258e-05,
      "loss": 0.9592,
      "step": 7080
    },
    {
      "epoch": 0.44931715200101396,
      "grad_norm": 2.3344075679779053,
      "learning_rate": 2.5562870448772228e-05,
      "loss": 0.9577,
      "step": 7090
    },
    {
      "epoch": 0.44995088564276436,
      "grad_norm": 2.5050251483917236,
      "learning_rate": 2.5556519898391195e-05,
      "loss": 1.0,
      "step": 7100
    },
    {
      "epoch": 0.4505846192845147,
      "grad_norm": 2.468620777130127,
      "learning_rate": 2.5550169348010164e-05,
      "loss": 1.0221,
      "step": 7110
    },
    {
      "epoch": 0.4512183529262651,
      "grad_norm": 2.447788715362549,
      "learning_rate": 2.5543818797629127e-05,
      "loss": 0.9633,
      "step": 7120
    },
    {
      "epoch": 0.45185208656801545,
      "grad_norm": 2.597842216491699,
      "learning_rate": 2.5537468247248094e-05,
      "loss": 0.9643,
      "step": 7130
    },
    {
      "epoch": 0.45248582020976585,
      "grad_norm": 2.759779214859009,
      "learning_rate": 2.5531117696867064e-05,
      "loss": 0.9535,
      "step": 7140
    },
    {
      "epoch": 0.4531195538515162,
      "grad_norm": 2.048201322555542,
      "learning_rate": 2.552476714648603e-05,
      "loss": 0.9769,
      "step": 7150
    },
    {
      "epoch": 0.4537532874932666,
      "grad_norm": 2.487860918045044,
      "learning_rate": 2.5518416596104996e-05,
      "loss": 1.0023,
      "step": 7160
    },
    {
      "epoch": 0.45438702113501694,
      "grad_norm": 2.09230375289917,
      "learning_rate": 2.5512066045723963e-05,
      "loss": 0.9629,
      "step": 7170
    },
    {
      "epoch": 0.45502075477676734,
      "grad_norm": 2.4592320919036865,
      "learning_rate": 2.550571549534293e-05,
      "loss": 0.9935,
      "step": 7180
    },
    {
      "epoch": 0.4556544884185177,
      "grad_norm": 2.6403534412384033,
      "learning_rate": 2.5499364944961896e-05,
      "loss": 0.9673,
      "step": 7190
    },
    {
      "epoch": 0.4562882220602681,
      "grad_norm": 2.436964988708496,
      "learning_rate": 2.5493014394580866e-05,
      "loss": 0.9382,
      "step": 7200
    },
    {
      "epoch": 0.45692195570201843,
      "grad_norm": 1.992918610572815,
      "learning_rate": 2.5486663844199832e-05,
      "loss": 0.9103,
      "step": 7210
    },
    {
      "epoch": 0.45755568934376883,
      "grad_norm": 2.3629024028778076,
      "learning_rate": 2.54803132938188e-05,
      "loss": 0.9615,
      "step": 7220
    },
    {
      "epoch": 0.4581894229855192,
      "grad_norm": 2.4839351177215576,
      "learning_rate": 2.5473962743437765e-05,
      "loss": 0.9894,
      "step": 7230
    },
    {
      "epoch": 0.4588231566272696,
      "grad_norm": 2.1495468616485596,
      "learning_rate": 2.546761219305673e-05,
      "loss": 0.958,
      "step": 7240
    },
    {
      "epoch": 0.4594568902690199,
      "grad_norm": 2.5453438758850098,
      "learning_rate": 2.54612616426757e-05,
      "loss": 0.9354,
      "step": 7250
    },
    {
      "epoch": 0.4600906239107703,
      "grad_norm": 2.376894235610962,
      "learning_rate": 2.5454911092294668e-05,
      "loss": 0.975,
      "step": 7260
    },
    {
      "epoch": 0.46072435755252067,
      "grad_norm": 2.2018747329711914,
      "learning_rate": 2.5448560541913634e-05,
      "loss": 0.9356,
      "step": 7270
    },
    {
      "epoch": 0.46135809119427107,
      "grad_norm": 2.404893398284912,
      "learning_rate": 2.54422099915326e-05,
      "loss": 0.9391,
      "step": 7280
    },
    {
      "epoch": 0.4619918248360214,
      "grad_norm": 2.307643413543701,
      "learning_rate": 2.5435859441151567e-05,
      "loss": 0.9704,
      "step": 7290
    },
    {
      "epoch": 0.4626255584777718,
      "grad_norm": 2.791118621826172,
      "learning_rate": 2.5429508890770533e-05,
      "loss": 0.9649,
      "step": 7300
    },
    {
      "epoch": 0.46325929211952216,
      "grad_norm": 2.181204319000244,
      "learning_rate": 2.5423158340389503e-05,
      "loss": 0.9281,
      "step": 7310
    },
    {
      "epoch": 0.46389302576127256,
      "grad_norm": 2.2379744052886963,
      "learning_rate": 2.541680779000847e-05,
      "loss": 0.9236,
      "step": 7320
    },
    {
      "epoch": 0.4645267594030229,
      "grad_norm": 3.10076904296875,
      "learning_rate": 2.5410457239627433e-05,
      "loss": 0.9951,
      "step": 7330
    },
    {
      "epoch": 0.4651604930447733,
      "grad_norm": 2.4599859714508057,
      "learning_rate": 2.5404106689246402e-05,
      "loss": 0.9666,
      "step": 7340
    },
    {
      "epoch": 0.46579422668652365,
      "grad_norm": 2.7183239459991455,
      "learning_rate": 2.539775613886537e-05,
      "loss": 0.9732,
      "step": 7350
    },
    {
      "epoch": 0.46642796032827405,
      "grad_norm": 2.3969128131866455,
      "learning_rate": 2.5391405588484335e-05,
      "loss": 0.9805,
      "step": 7360
    },
    {
      "epoch": 0.4670616939700244,
      "grad_norm": 2.4102840423583984,
      "learning_rate": 2.5385055038103305e-05,
      "loss": 0.9826,
      "step": 7370
    },
    {
      "epoch": 0.4676954276117748,
      "grad_norm": 1.9870262145996094,
      "learning_rate": 2.5378704487722268e-05,
      "loss": 0.9601,
      "step": 7380
    },
    {
      "epoch": 0.46832916125352514,
      "grad_norm": 2.388395071029663,
      "learning_rate": 2.5372353937341234e-05,
      "loss": 1.0026,
      "step": 7390
    },
    {
      "epoch": 0.46896289489527554,
      "grad_norm": 2.2882473468780518,
      "learning_rate": 2.5366003386960204e-05,
      "loss": 0.9485,
      "step": 7400
    },
    {
      "epoch": 0.4695966285370259,
      "grad_norm": 2.758145332336426,
      "learning_rate": 2.535965283657917e-05,
      "loss": 0.9512,
      "step": 7410
    },
    {
      "epoch": 0.4702303621787763,
      "grad_norm": 3.6730618476867676,
      "learning_rate": 2.535330228619814e-05,
      "loss": 0.9154,
      "step": 7420
    },
    {
      "epoch": 0.47086409582052663,
      "grad_norm": 2.664313316345215,
      "learning_rate": 2.5346951735817104e-05,
      "loss": 0.9885,
      "step": 7430
    },
    {
      "epoch": 0.47149782946227703,
      "grad_norm": 2.2438526153564453,
      "learning_rate": 2.534060118543607e-05,
      "loss": 0.9607,
      "step": 7440
    },
    {
      "epoch": 0.4721315631040274,
      "grad_norm": 2.23822021484375,
      "learning_rate": 2.533425063505504e-05,
      "loss": 0.9529,
      "step": 7450
    },
    {
      "epoch": 0.4727652967457778,
      "grad_norm": 2.8653714656829834,
      "learning_rate": 2.5327900084674006e-05,
      "loss": 0.9962,
      "step": 7460
    },
    {
      "epoch": 0.4733990303875281,
      "grad_norm": 2.7577919960021973,
      "learning_rate": 2.5321549534292973e-05,
      "loss": 0.9689,
      "step": 7470
    },
    {
      "epoch": 0.4740327640292785,
      "grad_norm": 2.4108340740203857,
      "learning_rate": 2.5315198983911942e-05,
      "loss": 0.9782,
      "step": 7480
    },
    {
      "epoch": 0.47466649767102886,
      "grad_norm": 2.227461576461792,
      "learning_rate": 2.5308848433530906e-05,
      "loss": 0.9712,
      "step": 7490
    },
    {
      "epoch": 0.47530023131277926,
      "grad_norm": 2.2530245780944824,
      "learning_rate": 2.5302497883149872e-05,
      "loss": 0.9351,
      "step": 7500
    },
    {
      "epoch": 0.4759339649545296,
      "grad_norm": 2.07965350151062,
      "learning_rate": 2.5296147332768842e-05,
      "loss": 0.9399,
      "step": 7510
    },
    {
      "epoch": 0.47656769859628,
      "grad_norm": 2.722790479660034,
      "learning_rate": 2.5289796782387808e-05,
      "loss": 0.9667,
      "step": 7520
    },
    {
      "epoch": 0.47720143223803035,
      "grad_norm": 2.2876527309417725,
      "learning_rate": 2.5283446232006775e-05,
      "loss": 0.9734,
      "step": 7530
    },
    {
      "epoch": 0.47783516587978075,
      "grad_norm": 2.294724225997925,
      "learning_rate": 2.527709568162574e-05,
      "loss": 0.9745,
      "step": 7540
    },
    {
      "epoch": 0.4784688995215311,
      "grad_norm": 1.8140308856964111,
      "learning_rate": 2.5270745131244707e-05,
      "loss": 0.9449,
      "step": 7550
    },
    {
      "epoch": 0.4791026331632815,
      "grad_norm": 2.3347582817077637,
      "learning_rate": 2.5264394580863677e-05,
      "loss": 0.9941,
      "step": 7560
    },
    {
      "epoch": 0.47973636680503184,
      "grad_norm": 2.488996982574463,
      "learning_rate": 2.5258044030482644e-05,
      "loss": 0.9596,
      "step": 7570
    },
    {
      "epoch": 0.48037010044678224,
      "grad_norm": 2.126427173614502,
      "learning_rate": 2.525169348010161e-05,
      "loss": 0.9526,
      "step": 7580
    },
    {
      "epoch": 0.4810038340885326,
      "grad_norm": 2.846975326538086,
      "learning_rate": 2.5245342929720577e-05,
      "loss": 0.9836,
      "step": 7590
    },
    {
      "epoch": 0.481637567730283,
      "grad_norm": 2.7056961059570312,
      "learning_rate": 2.5238992379339543e-05,
      "loss": 0.93,
      "step": 7600
    },
    {
      "epoch": 0.48227130137203333,
      "grad_norm": 2.1373064517974854,
      "learning_rate": 2.523264182895851e-05,
      "loss": 0.9911,
      "step": 7610
    },
    {
      "epoch": 0.48290503501378373,
      "grad_norm": 2.484464168548584,
      "learning_rate": 2.522629127857748e-05,
      "loss": 0.9887,
      "step": 7620
    },
    {
      "epoch": 0.4835387686555341,
      "grad_norm": 2.280266284942627,
      "learning_rate": 2.5219940728196446e-05,
      "loss": 0.9369,
      "step": 7630
    },
    {
      "epoch": 0.4841725022972844,
      "grad_norm": 2.5229735374450684,
      "learning_rate": 2.521359017781541e-05,
      "loss": 0.9901,
      "step": 7640
    },
    {
      "epoch": 0.4848062359390348,
      "grad_norm": 2.6133909225463867,
      "learning_rate": 2.520723962743438e-05,
      "loss": 0.9706,
      "step": 7650
    },
    {
      "epoch": 0.48543996958078517,
      "grad_norm": 2.1142420768737793,
      "learning_rate": 2.5200889077053345e-05,
      "loss": 0.9743,
      "step": 7660
    },
    {
      "epoch": 0.48607370322253557,
      "grad_norm": 2.3085877895355225,
      "learning_rate": 2.519453852667231e-05,
      "loss": 0.9625,
      "step": 7670
    },
    {
      "epoch": 0.4867074368642859,
      "grad_norm": 2.118098497390747,
      "learning_rate": 2.518818797629128e-05,
      "loss": 0.9793,
      "step": 7680
    },
    {
      "epoch": 0.4873411705060363,
      "grad_norm": 2.4793708324432373,
      "learning_rate": 2.5181837425910244e-05,
      "loss": 0.96,
      "step": 7690
    },
    {
      "epoch": 0.48797490414778666,
      "grad_norm": 2.143747568130493,
      "learning_rate": 2.517548687552921e-05,
      "loss": 0.9683,
      "step": 7700
    },
    {
      "epoch": 0.48860863778953706,
      "grad_norm": 2.1645255088806152,
      "learning_rate": 2.516913632514818e-05,
      "loss": 0.9765,
      "step": 7710
    },
    {
      "epoch": 0.4892423714312874,
      "grad_norm": 2.44954514503479,
      "learning_rate": 2.5162785774767147e-05,
      "loss": 0.9745,
      "step": 7720
    },
    {
      "epoch": 0.4898761050730378,
      "grad_norm": 2.3186774253845215,
      "learning_rate": 2.5156435224386117e-05,
      "loss": 0.8965,
      "step": 7730
    },
    {
      "epoch": 0.49050983871478815,
      "grad_norm": 1.956725001335144,
      "learning_rate": 2.5150084674005083e-05,
      "loss": 0.955,
      "step": 7740
    },
    {
      "epoch": 0.49114357235653855,
      "grad_norm": 2.2733850479125977,
      "learning_rate": 2.5143734123624046e-05,
      "loss": 0.9424,
      "step": 7750
    },
    {
      "epoch": 0.4917773059982889,
      "grad_norm": 2.6566216945648193,
      "learning_rate": 2.5137383573243016e-05,
      "loss": 1.0037,
      "step": 7760
    },
    {
      "epoch": 0.4924110396400393,
      "grad_norm": 2.3368887901306152,
      "learning_rate": 2.5131033022861982e-05,
      "loss": 0.9331,
      "step": 7770
    },
    {
      "epoch": 0.49304477328178964,
      "grad_norm": 2.3497154712677,
      "learning_rate": 2.512468247248095e-05,
      "loss": 0.9811,
      "step": 7780
    },
    {
      "epoch": 0.49367850692354004,
      "grad_norm": 2.45331072807312,
      "learning_rate": 2.511833192209992e-05,
      "loss": 0.9428,
      "step": 7790
    },
    {
      "epoch": 0.4943122405652904,
      "grad_norm": 2.7217934131622314,
      "learning_rate": 2.511198137171888e-05,
      "loss": 1.0153,
      "step": 7800
    },
    {
      "epoch": 0.4949459742070408,
      "grad_norm": 2.7175683975219727,
      "learning_rate": 2.5105630821337848e-05,
      "loss": 0.9927,
      "step": 7810
    },
    {
      "epoch": 0.49557970784879113,
      "grad_norm": 2.264707326889038,
      "learning_rate": 2.5099280270956818e-05,
      "loss": 0.9461,
      "step": 7820
    },
    {
      "epoch": 0.49621344149054153,
      "grad_norm": 2.502983331680298,
      "learning_rate": 2.5092929720575784e-05,
      "loss": 0.96,
      "step": 7830
    },
    {
      "epoch": 0.4968471751322919,
      "grad_norm": 2.0394890308380127,
      "learning_rate": 2.508657917019475e-05,
      "loss": 0.9971,
      "step": 7840
    },
    {
      "epoch": 0.4974809087740423,
      "grad_norm": 2.4115982055664062,
      "learning_rate": 2.5080228619813717e-05,
      "loss": 0.9443,
      "step": 7850
    },
    {
      "epoch": 0.4981146424157926,
      "grad_norm": 2.531599521636963,
      "learning_rate": 2.5073878069432684e-05,
      "loss": 1.0002,
      "step": 7860
    },
    {
      "epoch": 0.498748376057543,
      "grad_norm": 2.2721164226531982,
      "learning_rate": 2.506752751905165e-05,
      "loss": 0.9746,
      "step": 7870
    },
    {
      "epoch": 0.49938210969929336,
      "grad_norm": 2.5633745193481445,
      "learning_rate": 2.506117696867062e-05,
      "loss": 0.991,
      "step": 7880
    },
    {
      "epoch": 0.5000158433410438,
      "grad_norm": 2.361595630645752,
      "learning_rate": 2.5054826418289586e-05,
      "loss": 0.9838,
      "step": 7890
    },
    {
      "epoch": 0.5006495769827941,
      "grad_norm": 2.6697824001312256,
      "learning_rate": 2.5048475867908553e-05,
      "loss": 0.9829,
      "step": 7900
    },
    {
      "epoch": 0.5012833106245445,
      "grad_norm": 2.759368419647217,
      "learning_rate": 2.504212531752752e-05,
      "loss": 0.9915,
      "step": 7910
    },
    {
      "epoch": 0.5019170442662949,
      "grad_norm": 2.570490837097168,
      "learning_rate": 2.5035774767146486e-05,
      "loss": 0.988,
      "step": 7920
    },
    {
      "epoch": 0.5025507779080453,
      "grad_norm": 2.2399826049804688,
      "learning_rate": 2.5029424216765455e-05,
      "loss": 0.9732,
      "step": 7930
    },
    {
      "epoch": 0.5031845115497956,
      "grad_norm": 1.9201583862304688,
      "learning_rate": 2.5023073666384422e-05,
      "loss": 0.96,
      "step": 7940
    },
    {
      "epoch": 0.5038182451915459,
      "grad_norm": 2.467085123062134,
      "learning_rate": 2.5016723116003385e-05,
      "loss": 0.9565,
      "step": 7950
    },
    {
      "epoch": 0.5044519788332964,
      "grad_norm": 2.386364221572876,
      "learning_rate": 2.5010372565622355e-05,
      "loss": 0.9409,
      "step": 7960
    },
    {
      "epoch": 0.5050857124750467,
      "grad_norm": 2.049872636795044,
      "learning_rate": 2.500402201524132e-05,
      "loss": 0.9286,
      "step": 7970
    },
    {
      "epoch": 0.5057194461167971,
      "grad_norm": 2.298725128173828,
      "learning_rate": 2.4997671464860288e-05,
      "loss": 0.9495,
      "step": 7980
    },
    {
      "epoch": 0.5063531797585474,
      "grad_norm": 2.1721746921539307,
      "learning_rate": 2.4991320914479257e-05,
      "loss": 0.9346,
      "step": 7990
    },
    {
      "epoch": 0.5069869134002979,
      "grad_norm": 2.4207539558410645,
      "learning_rate": 2.4984970364098224e-05,
      "loss": 0.9815,
      "step": 8000
    },
    {
      "epoch": 0.5076206470420482,
      "grad_norm": 2.425496816635132,
      "learning_rate": 2.4978619813717187e-05,
      "loss": 0.9737,
      "step": 8010
    },
    {
      "epoch": 0.5082543806837986,
      "grad_norm": 2.2896180152893066,
      "learning_rate": 2.4972269263336157e-05,
      "loss": 0.9618,
      "step": 8020
    },
    {
      "epoch": 0.5088881143255489,
      "grad_norm": 2.5054945945739746,
      "learning_rate": 2.4965918712955123e-05,
      "loss": 0.9673,
      "step": 8030
    },
    {
      "epoch": 0.5095218479672994,
      "grad_norm": 2.1220502853393555,
      "learning_rate": 2.4959568162574093e-05,
      "loss": 0.9693,
      "step": 8040
    },
    {
      "epoch": 0.5101555816090497,
      "grad_norm": 2.4411680698394775,
      "learning_rate": 2.495321761219306e-05,
      "loss": 0.9646,
      "step": 8050
    },
    {
      "epoch": 0.5107893152508001,
      "grad_norm": 2.310612916946411,
      "learning_rate": 2.4946867061812022e-05,
      "loss": 0.9918,
      "step": 8060
    },
    {
      "epoch": 0.5114230488925504,
      "grad_norm": 2.341562271118164,
      "learning_rate": 2.4940516511430992e-05,
      "loss": 1.0027,
      "step": 8070
    },
    {
      "epoch": 0.5120567825343009,
      "grad_norm": 2.3291146755218506,
      "learning_rate": 2.493416596104996e-05,
      "loss": 0.9315,
      "step": 8080
    },
    {
      "epoch": 0.5126905161760512,
      "grad_norm": 2.7770841121673584,
      "learning_rate": 2.4927815410668925e-05,
      "loss": 0.9936,
      "step": 8090
    },
    {
      "epoch": 0.5133242498178016,
      "grad_norm": 2.059237241744995,
      "learning_rate": 2.4921464860287895e-05,
      "loss": 0.9605,
      "step": 8100
    },
    {
      "epoch": 0.5139579834595519,
      "grad_norm": 2.47184419631958,
      "learning_rate": 2.4915114309906858e-05,
      "loss": 0.9507,
      "step": 8110
    },
    {
      "epoch": 0.5145917171013024,
      "grad_norm": 2.4429354667663574,
      "learning_rate": 2.4908763759525824e-05,
      "loss": 0.9571,
      "step": 8120
    },
    {
      "epoch": 0.5152254507430527,
      "grad_norm": 2.4851975440979004,
      "learning_rate": 2.4902413209144794e-05,
      "loss": 0.9812,
      "step": 8130
    },
    {
      "epoch": 0.515859184384803,
      "grad_norm": 2.8480918407440186,
      "learning_rate": 2.489606265876376e-05,
      "loss": 0.9888,
      "step": 8140
    },
    {
      "epoch": 0.5164929180265534,
      "grad_norm": 2.9939262866973877,
      "learning_rate": 2.4889712108382727e-05,
      "loss": 0.9711,
      "step": 8150
    },
    {
      "epoch": 0.5171266516683038,
      "grad_norm": 2.3272111415863037,
      "learning_rate": 2.4883361558001693e-05,
      "loss": 0.9745,
      "step": 8160
    },
    {
      "epoch": 0.5177603853100542,
      "grad_norm": 2.908283233642578,
      "learning_rate": 2.487701100762066e-05,
      "loss": 0.9373,
      "step": 8170
    },
    {
      "epoch": 0.5183941189518045,
      "grad_norm": 2.2969250679016113,
      "learning_rate": 2.4870660457239626e-05,
      "loss": 0.8974,
      "step": 8180
    },
    {
      "epoch": 0.5190278525935549,
      "grad_norm": 2.2454993724823,
      "learning_rate": 2.4864309906858596e-05,
      "loss": 0.9509,
      "step": 8190
    },
    {
      "epoch": 0.5196615862353053,
      "grad_norm": 2.6299126148223877,
      "learning_rate": 2.4857959356477562e-05,
      "loss": 0.9869,
      "step": 8200
    },
    {
      "epoch": 0.5202953198770557,
      "grad_norm": 2.667656660079956,
      "learning_rate": 2.485160880609653e-05,
      "loss": 0.96,
      "step": 8210
    },
    {
      "epoch": 0.520929053518806,
      "grad_norm": 2.4179742336273193,
      "learning_rate": 2.4845258255715495e-05,
      "loss": 0.9411,
      "step": 8220
    },
    {
      "epoch": 0.5215627871605564,
      "grad_norm": 1.9466849565505981,
      "learning_rate": 2.4838907705334462e-05,
      "loss": 0.9093,
      "step": 8230
    },
    {
      "epoch": 0.5221965208023068,
      "grad_norm": 2.5564002990722656,
      "learning_rate": 2.483255715495343e-05,
      "loss": 0.9557,
      "step": 8240
    },
    {
      "epoch": 0.5228302544440572,
      "grad_norm": 2.6963958740234375,
      "learning_rate": 2.4826206604572398e-05,
      "loss": 0.972,
      "step": 8250
    },
    {
      "epoch": 0.5234639880858075,
      "grad_norm": 2.355520486831665,
      "learning_rate": 2.4819856054191364e-05,
      "loss": 0.9771,
      "step": 8260
    },
    {
      "epoch": 0.5240977217275579,
      "grad_norm": 2.6609554290771484,
      "learning_rate": 2.481350550381033e-05,
      "loss": 0.9892,
      "step": 8270
    },
    {
      "epoch": 0.5247314553693083,
      "grad_norm": 2.3215320110321045,
      "learning_rate": 2.4807154953429297e-05,
      "loss": 0.9637,
      "step": 8280
    },
    {
      "epoch": 0.5253651890110587,
      "grad_norm": 2.3937759399414062,
      "learning_rate": 2.4800804403048264e-05,
      "loss": 0.9814,
      "step": 8290
    },
    {
      "epoch": 0.525998922652809,
      "grad_norm": 2.793794870376587,
      "learning_rate": 2.4794453852667234e-05,
      "loss": 0.9675,
      "step": 8300
    },
    {
      "epoch": 0.5266326562945594,
      "grad_norm": 2.6650187969207764,
      "learning_rate": 2.47881033022862e-05,
      "loss": 0.9523,
      "step": 8310
    },
    {
      "epoch": 0.5272663899363098,
      "grad_norm": 2.366338014602661,
      "learning_rate": 2.4781752751905163e-05,
      "loss": 1.0336,
      "step": 8320
    },
    {
      "epoch": 0.5279001235780602,
      "grad_norm": 2.4870901107788086,
      "learning_rate": 2.4775402201524133e-05,
      "loss": 0.9359,
      "step": 8330
    },
    {
      "epoch": 0.5285338572198105,
      "grad_norm": 2.4459757804870605,
      "learning_rate": 2.47690516511431e-05,
      "loss": 0.9405,
      "step": 8340
    },
    {
      "epoch": 0.5291675908615608,
      "grad_norm": 2.408177375793457,
      "learning_rate": 2.476270110076207e-05,
      "loss": 0.9196,
      "step": 8350
    },
    {
      "epoch": 0.5298013245033113,
      "grad_norm": 2.4682908058166504,
      "learning_rate": 2.4756350550381035e-05,
      "loss": 0.9347,
      "step": 8360
    },
    {
      "epoch": 0.5304350581450616,
      "grad_norm": 2.088839292526245,
      "learning_rate": 2.475e-05,
      "loss": 0.9655,
      "step": 8370
    },
    {
      "epoch": 0.531068791786812,
      "grad_norm": 2.7868592739105225,
      "learning_rate": 2.474364944961897e-05,
      "loss": 1.0278,
      "step": 8380
    },
    {
      "epoch": 0.5317025254285623,
      "grad_norm": 2.6103553771972656,
      "learning_rate": 2.4737298899237935e-05,
      "loss": 0.9872,
      "step": 8390
    },
    {
      "epoch": 0.5323362590703128,
      "grad_norm": 3.1155056953430176,
      "learning_rate": 2.47309483488569e-05,
      "loss": 1.0129,
      "step": 8400
    },
    {
      "epoch": 0.5329699927120631,
      "grad_norm": 2.047562599182129,
      "learning_rate": 2.472459779847587e-05,
      "loss": 0.9279,
      "step": 8410
    },
    {
      "epoch": 0.5336037263538135,
      "grad_norm": 2.039130210876465,
      "learning_rate": 2.4718247248094834e-05,
      "loss": 1.0093,
      "step": 8420
    },
    {
      "epoch": 0.5342374599955638,
      "grad_norm": 2.3977725505828857,
      "learning_rate": 2.47118966977138e-05,
      "loss": 0.9623,
      "step": 8430
    },
    {
      "epoch": 0.5348711936373143,
      "grad_norm": 2.2456552982330322,
      "learning_rate": 2.470554614733277e-05,
      "loss": 0.9484,
      "step": 8440
    },
    {
      "epoch": 0.5355049272790646,
      "grad_norm": 2.257558822631836,
      "learning_rate": 2.4699195596951737e-05,
      "loss": 0.956,
      "step": 8450
    },
    {
      "epoch": 0.536138660920815,
      "grad_norm": 2.4910051822662354,
      "learning_rate": 2.4692845046570703e-05,
      "loss": 0.9963,
      "step": 8460
    },
    {
      "epoch": 0.5367723945625653,
      "grad_norm": 2.1713638305664062,
      "learning_rate": 2.468649449618967e-05,
      "loss": 0.9692,
      "step": 8470
    },
    {
      "epoch": 0.5374061282043158,
      "grad_norm": 2.3248255252838135,
      "learning_rate": 2.4680143945808636e-05,
      "loss": 0.9606,
      "step": 8480
    },
    {
      "epoch": 0.5380398618460661,
      "grad_norm": 2.2841947078704834,
      "learning_rate": 2.4673793395427602e-05,
      "loss": 0.9265,
      "step": 8490
    },
    {
      "epoch": 0.5386735954878165,
      "grad_norm": 2.258441686630249,
      "learning_rate": 2.4667442845046572e-05,
      "loss": 0.9221,
      "step": 8500
    },
    {
      "epoch": 0.5393073291295668,
      "grad_norm": 2.240354537963867,
      "learning_rate": 2.466109229466554e-05,
      "loss": 0.9299,
      "step": 8510
    },
    {
      "epoch": 0.5399410627713173,
      "grad_norm": 2.1794896125793457,
      "learning_rate": 2.465474174428451e-05,
      "loss": 0.9523,
      "step": 8520
    },
    {
      "epoch": 0.5405747964130676,
      "grad_norm": 2.233027458190918,
      "learning_rate": 2.464839119390347e-05,
      "loss": 0.9564,
      "step": 8530
    },
    {
      "epoch": 0.541208530054818,
      "grad_norm": 2.5071630477905273,
      "learning_rate": 2.4642040643522438e-05,
      "loss": 0.9424,
      "step": 8540
    },
    {
      "epoch": 0.5418422636965683,
      "grad_norm": 2.314746618270874,
      "learning_rate": 2.4635690093141408e-05,
      "loss": 0.9633,
      "step": 8550
    },
    {
      "epoch": 0.5424759973383187,
      "grad_norm": 2.5734763145446777,
      "learning_rate": 2.4629339542760374e-05,
      "loss": 0.9828,
      "step": 8560
    },
    {
      "epoch": 0.5431097309800691,
      "grad_norm": 2.5566670894622803,
      "learning_rate": 2.462298899237934e-05,
      "loss": 0.9851,
      "step": 8570
    },
    {
      "epoch": 0.5437434646218194,
      "grad_norm": 2.4229986667633057,
      "learning_rate": 2.4616638441998307e-05,
      "loss": 0.9264,
      "step": 8580
    },
    {
      "epoch": 0.5443771982635698,
      "grad_norm": 2.130317211151123,
      "learning_rate": 2.4610287891617273e-05,
      "loss": 0.9646,
      "step": 8590
    },
    {
      "epoch": 0.5450109319053202,
      "grad_norm": 2.4479756355285645,
      "learning_rate": 2.460393734123624e-05,
      "loss": 0.9555,
      "step": 8600
    },
    {
      "epoch": 0.5456446655470706,
      "grad_norm": 2.264420747756958,
      "learning_rate": 2.459758679085521e-05,
      "loss": 0.9507,
      "step": 8610
    },
    {
      "epoch": 0.5462783991888209,
      "grad_norm": 2.3813369274139404,
      "learning_rate": 2.4591236240474176e-05,
      "loss": 0.9515,
      "step": 8620
    },
    {
      "epoch": 0.5469121328305713,
      "grad_norm": 2.219444990158081,
      "learning_rate": 2.458488569009314e-05,
      "loss": 0.9655,
      "step": 8630
    },
    {
      "epoch": 0.5475458664723217,
      "grad_norm": 2.4741971492767334,
      "learning_rate": 2.457853513971211e-05,
      "loss": 0.989,
      "step": 8640
    },
    {
      "epoch": 0.5481796001140721,
      "grad_norm": 2.2437832355499268,
      "learning_rate": 2.4572184589331075e-05,
      "loss": 0.9801,
      "step": 8650
    },
    {
      "epoch": 0.5488133337558224,
      "grad_norm": 2.4867453575134277,
      "learning_rate": 2.4565834038950045e-05,
      "loss": 0.9699,
      "step": 8660
    },
    {
      "epoch": 0.5494470673975728,
      "grad_norm": 2.542290687561035,
      "learning_rate": 2.455948348856901e-05,
      "loss": 0.9758,
      "step": 8670
    },
    {
      "epoch": 0.5500808010393232,
      "grad_norm": 2.5071024894714355,
      "learning_rate": 2.4553132938187975e-05,
      "loss": 0.9905,
      "step": 8680
    },
    {
      "epoch": 0.5507145346810736,
      "grad_norm": 2.49065899848938,
      "learning_rate": 2.4546782387806944e-05,
      "loss": 0.9229,
      "step": 8690
    },
    {
      "epoch": 0.5513482683228239,
      "grad_norm": 2.564775228500366,
      "learning_rate": 2.454043183742591e-05,
      "loss": 0.9909,
      "step": 8700
    },
    {
      "epoch": 0.5519820019645743,
      "grad_norm": 2.39894437789917,
      "learning_rate": 2.453471634208298e-05,
      "loss": 1.0164,
      "step": 8710
    },
    {
      "epoch": 0.5526157356063247,
      "grad_norm": 2.55578351020813,
      "learning_rate": 2.4528365791701948e-05,
      "loss": 0.9644,
      "step": 8720
    },
    {
      "epoch": 0.553249469248075,
      "grad_norm": 3.0906171798706055,
      "learning_rate": 2.4522015241320918e-05,
      "loss": 0.9632,
      "step": 8730
    },
    {
      "epoch": 0.5538832028898254,
      "grad_norm": 2.3429367542266846,
      "learning_rate": 2.451566469093988e-05,
      "loss": 0.9605,
      "step": 8740
    },
    {
      "epoch": 0.5545169365315757,
      "grad_norm": 2.471719980239868,
      "learning_rate": 2.4509314140558847e-05,
      "loss": 0.9352,
      "step": 8750
    },
    {
      "epoch": 0.5551506701733262,
      "grad_norm": 2.2043917179107666,
      "learning_rate": 2.4502963590177817e-05,
      "loss": 0.9552,
      "step": 8760
    },
    {
      "epoch": 0.5557844038150765,
      "grad_norm": 2.7611799240112305,
      "learning_rate": 2.4496613039796783e-05,
      "loss": 0.9709,
      "step": 8770
    },
    {
      "epoch": 0.5564181374568269,
      "grad_norm": 2.8329553604125977,
      "learning_rate": 2.449026248941575e-05,
      "loss": 1.0254,
      "step": 8780
    },
    {
      "epoch": 0.5570518710985772,
      "grad_norm": 2.2273521423339844,
      "learning_rate": 2.4483911939034716e-05,
      "loss": 0.9753,
      "step": 8790
    },
    {
      "epoch": 0.5576856047403277,
      "grad_norm": 2.349017858505249,
      "learning_rate": 2.4477561388653683e-05,
      "loss": 0.958,
      "step": 8800
    },
    {
      "epoch": 0.558319338382078,
      "grad_norm": 2.7014811038970947,
      "learning_rate": 2.447121083827265e-05,
      "loss": 0.9591,
      "step": 8810
    },
    {
      "epoch": 0.5589530720238284,
      "grad_norm": 2.302557945251465,
      "learning_rate": 2.446486028789162e-05,
      "loss": 0.9669,
      "step": 8820
    },
    {
      "epoch": 0.5595868056655787,
      "grad_norm": 2.468170404434204,
      "learning_rate": 2.4458509737510585e-05,
      "loss": 0.9304,
      "step": 8830
    },
    {
      "epoch": 0.5602205393073292,
      "grad_norm": 2.399590015411377,
      "learning_rate": 2.4452159187129552e-05,
      "loss": 0.9342,
      "step": 8840
    },
    {
      "epoch": 0.5608542729490795,
      "grad_norm": 2.356537103652954,
      "learning_rate": 2.4445808636748518e-05,
      "loss": 0.9884,
      "step": 8850
    },
    {
      "epoch": 0.5614880065908299,
      "grad_norm": 2.56160831451416,
      "learning_rate": 2.4439458086367485e-05,
      "loss": 0.9526,
      "step": 8860
    },
    {
      "epoch": 0.5621217402325802,
      "grad_norm": 2.6135756969451904,
      "learning_rate": 2.4433107535986454e-05,
      "loss": 0.9277,
      "step": 8870
    },
    {
      "epoch": 0.5627554738743307,
      "grad_norm": 2.641507148742676,
      "learning_rate": 2.442675698560542e-05,
      "loss": 0.9595,
      "step": 8880
    },
    {
      "epoch": 0.563389207516081,
      "grad_norm": 2.318776845932007,
      "learning_rate": 2.4420406435224387e-05,
      "loss": 0.9471,
      "step": 8890
    },
    {
      "epoch": 0.5640229411578314,
      "grad_norm": 2.480666160583496,
      "learning_rate": 2.4414055884843354e-05,
      "loss": 0.964,
      "step": 8900
    },
    {
      "epoch": 0.5646566747995817,
      "grad_norm": 2.2129340171813965,
      "learning_rate": 2.440770533446232e-05,
      "loss": 0.9675,
      "step": 8910
    },
    {
      "epoch": 0.5652904084413322,
      "grad_norm": 5.935197830200195,
      "learning_rate": 2.4401354784081287e-05,
      "loss": 0.9611,
      "step": 8920
    },
    {
      "epoch": 0.5659241420830825,
      "grad_norm": 2.3162424564361572,
      "learning_rate": 2.4395004233700256e-05,
      "loss": 0.961,
      "step": 8930
    },
    {
      "epoch": 0.5665578757248328,
      "grad_norm": 2.51128888130188,
      "learning_rate": 2.4388653683319223e-05,
      "loss": 0.9732,
      "step": 8940
    },
    {
      "epoch": 0.5671916093665832,
      "grad_norm": 2.442354440689087,
      "learning_rate": 2.4382303132938186e-05,
      "loss": 0.9599,
      "step": 8950
    },
    {
      "epoch": 0.5678253430083336,
      "grad_norm": 3.0407021045684814,
      "learning_rate": 2.4375952582557156e-05,
      "loss": 0.9435,
      "step": 8960
    },
    {
      "epoch": 0.568459076650084,
      "grad_norm": 2.1753056049346924,
      "learning_rate": 2.4369602032176122e-05,
      "loss": 0.9045,
      "step": 8970
    },
    {
      "epoch": 0.5690928102918343,
      "grad_norm": 2.4869801998138428,
      "learning_rate": 2.4363251481795092e-05,
      "loss": 0.9793,
      "step": 8980
    },
    {
      "epoch": 0.5697265439335847,
      "grad_norm": 2.260756731033325,
      "learning_rate": 2.4356900931414058e-05,
      "loss": 0.9725,
      "step": 8990
    },
    {
      "epoch": 0.5703602775753351,
      "grad_norm": 2.560659646987915,
      "learning_rate": 2.435055038103302e-05,
      "loss": 0.9369,
      "step": 9000
    },
    {
      "epoch": 0.5709940112170855,
      "grad_norm": 2.471491575241089,
      "learning_rate": 2.434419983065199e-05,
      "loss": 0.9418,
      "step": 9010
    },
    {
      "epoch": 0.5716277448588358,
      "grad_norm": 2.6285452842712402,
      "learning_rate": 2.4337849280270958e-05,
      "loss": 0.9649,
      "step": 9020
    },
    {
      "epoch": 0.5722614785005862,
      "grad_norm": 2.345120906829834,
      "learning_rate": 2.4331498729889924e-05,
      "loss": 0.9267,
      "step": 9030
    },
    {
      "epoch": 0.5728952121423366,
      "grad_norm": 2.1166346073150635,
      "learning_rate": 2.4325148179508894e-05,
      "loss": 0.9611,
      "step": 9040
    },
    {
      "epoch": 0.573528945784087,
      "grad_norm": 2.6751108169555664,
      "learning_rate": 2.4318797629127857e-05,
      "loss": 0.9039,
      "step": 9050
    },
    {
      "epoch": 0.5741626794258373,
      "grad_norm": 2.6563658714294434,
      "learning_rate": 2.4312447078746823e-05,
      "loss": 0.9488,
      "step": 9060
    },
    {
      "epoch": 0.5747964130675877,
      "grad_norm": 2.176976203918457,
      "learning_rate": 2.4306096528365793e-05,
      "loss": 0.92,
      "step": 9070
    },
    {
      "epoch": 0.5754301467093381,
      "grad_norm": 2.5865519046783447,
      "learning_rate": 2.429974597798476e-05,
      "loss": 0.948,
      "step": 9080
    },
    {
      "epoch": 0.5760638803510885,
      "grad_norm": 3.123345136642456,
      "learning_rate": 2.4293395427603726e-05,
      "loss": 0.9432,
      "step": 9090
    },
    {
      "epoch": 0.5766976139928388,
      "grad_norm": 3.229445695877075,
      "learning_rate": 2.4287044877222692e-05,
      "loss": 1.0045,
      "step": 9100
    },
    {
      "epoch": 0.5773313476345892,
      "grad_norm": 2.3789899349212646,
      "learning_rate": 2.428069432684166e-05,
      "loss": 0.9502,
      "step": 9110
    },
    {
      "epoch": 0.5779650812763396,
      "grad_norm": 2.4674746990203857,
      "learning_rate": 2.4274343776460625e-05,
      "loss": 0.9842,
      "step": 9120
    },
    {
      "epoch": 0.57859881491809,
      "grad_norm": 2.4387288093566895,
      "learning_rate": 2.4267993226079595e-05,
      "loss": 0.921,
      "step": 9130
    },
    {
      "epoch": 0.5792325485598403,
      "grad_norm": 2.257199764251709,
      "learning_rate": 2.426164267569856e-05,
      "loss": 0.9545,
      "step": 9140
    },
    {
      "epoch": 0.5798662822015906,
      "grad_norm": 2.4085850715637207,
      "learning_rate": 2.425529212531753e-05,
      "loss": 0.9636,
      "step": 9150
    },
    {
      "epoch": 0.580500015843341,
      "grad_norm": 2.4960992336273193,
      "learning_rate": 2.4248941574936494e-05,
      "loss": 0.9445,
      "step": 9160
    },
    {
      "epoch": 0.5811337494850914,
      "grad_norm": 2.1886539459228516,
      "learning_rate": 2.424259102455546e-05,
      "loss": 0.8978,
      "step": 9170
    },
    {
      "epoch": 0.5817674831268418,
      "grad_norm": 2.43503737449646,
      "learning_rate": 2.423624047417443e-05,
      "loss": 0.9764,
      "step": 9180
    },
    {
      "epoch": 0.5824012167685921,
      "grad_norm": 2.5666019916534424,
      "learning_rate": 2.4229889923793397e-05,
      "loss": 0.9023,
      "step": 9190
    },
    {
      "epoch": 0.5830349504103425,
      "grad_norm": 2.7266244888305664,
      "learning_rate": 2.4223539373412363e-05,
      "loss": 0.9728,
      "step": 9200
    },
    {
      "epoch": 0.5836686840520929,
      "grad_norm": 2.3994224071502686,
      "learning_rate": 2.421718882303133e-05,
      "loss": 0.9435,
      "step": 9210
    },
    {
      "epoch": 0.5843024176938433,
      "grad_norm": 2.686389684677124,
      "learning_rate": 2.4210838272650296e-05,
      "loss": 0.9452,
      "step": 9220
    },
    {
      "epoch": 0.5849361513355936,
      "grad_norm": 2.8345890045166016,
      "learning_rate": 2.4204487722269263e-05,
      "loss": 0.9676,
      "step": 9230
    },
    {
      "epoch": 0.585569884977344,
      "grad_norm": 2.653242826461792,
      "learning_rate": 2.4198137171888233e-05,
      "loss": 0.9128,
      "step": 9240
    },
    {
      "epoch": 0.5862036186190944,
      "grad_norm": 2.6246750354766846,
      "learning_rate": 2.41917866215072e-05,
      "loss": 0.9697,
      "step": 9250
    },
    {
      "epoch": 0.5868373522608448,
      "grad_norm": 2.7675154209136963,
      "learning_rate": 2.4185436071126162e-05,
      "loss": 0.9168,
      "step": 9260
    },
    {
      "epoch": 0.5874710859025951,
      "grad_norm": 2.287811517715454,
      "learning_rate": 2.4179085520745132e-05,
      "loss": 0.9412,
      "step": 9270
    },
    {
      "epoch": 0.5881048195443455,
      "grad_norm": 2.24045467376709,
      "learning_rate": 2.4172734970364098e-05,
      "loss": 0.9231,
      "step": 9280
    },
    {
      "epoch": 0.5887385531860959,
      "grad_norm": 2.640718698501587,
      "learning_rate": 2.4166384419983068e-05,
      "loss": 0.9776,
      "step": 9290
    },
    {
      "epoch": 0.5893722868278463,
      "grad_norm": 2.47019624710083,
      "learning_rate": 2.4160033869602034e-05,
      "loss": 0.9863,
      "step": 9300
    },
    {
      "epoch": 0.5900060204695966,
      "grad_norm": 6.139875888824463,
      "learning_rate": 2.4153683319220997e-05,
      "loss": 0.9759,
      "step": 9310
    },
    {
      "epoch": 0.590639754111347,
      "grad_norm": 2.1215081214904785,
      "learning_rate": 2.4147332768839967e-05,
      "loss": 1.0115,
      "step": 9320
    },
    {
      "epoch": 0.5912734877530974,
      "grad_norm": 2.5364112854003906,
      "learning_rate": 2.4140982218458934e-05,
      "loss": 0.9544,
      "step": 9330
    },
    {
      "epoch": 0.5919072213948477,
      "grad_norm": 2.314020872116089,
      "learning_rate": 2.41346316680779e-05,
      "loss": 1.0053,
      "step": 9340
    },
    {
      "epoch": 0.5925409550365981,
      "grad_norm": 2.4064745903015137,
      "learning_rate": 2.412828111769687e-05,
      "loss": 0.9367,
      "step": 9350
    },
    {
      "epoch": 0.5931746886783484,
      "grad_norm": 2.5856144428253174,
      "learning_rate": 2.4121930567315833e-05,
      "loss": 0.966,
      "step": 9360
    },
    {
      "epoch": 0.5938084223200989,
      "grad_norm": 2.4691810607910156,
      "learning_rate": 2.41155800169348e-05,
      "loss": 0.9368,
      "step": 9370
    },
    {
      "epoch": 0.5944421559618492,
      "grad_norm": 2.605900526046753,
      "learning_rate": 2.410922946655377e-05,
      "loss": 0.9332,
      "step": 9380
    },
    {
      "epoch": 0.5950758896035996,
      "grad_norm": 2.184323310852051,
      "learning_rate": 2.4102878916172736e-05,
      "loss": 0.9554,
      "step": 9390
    },
    {
      "epoch": 0.5957096232453499,
      "grad_norm": 2.5592281818389893,
      "learning_rate": 2.4096528365791702e-05,
      "loss": 0.9484,
      "step": 9400
    },
    {
      "epoch": 0.5963433568871004,
      "grad_norm": 2.6007955074310303,
      "learning_rate": 2.4090177815410672e-05,
      "loss": 0.9611,
      "step": 9410
    },
    {
      "epoch": 0.5969770905288507,
      "grad_norm": 2.0717215538024902,
      "learning_rate": 2.4083827265029635e-05,
      "loss": 0.9318,
      "step": 9420
    },
    {
      "epoch": 0.5976108241706011,
      "grad_norm": 2.7570013999938965,
      "learning_rate": 2.40774767146486e-05,
      "loss": 0.9908,
      "step": 9430
    },
    {
      "epoch": 0.5982445578123514,
      "grad_norm": 2.4762003421783447,
      "learning_rate": 2.407112616426757e-05,
      "loss": 0.9478,
      "step": 9440
    },
    {
      "epoch": 0.5988782914541019,
      "grad_norm": 2.371011972427368,
      "learning_rate": 2.4064775613886538e-05,
      "loss": 0.9466,
      "step": 9450
    },
    {
      "epoch": 0.5995120250958522,
      "grad_norm": 2.8074452877044678,
      "learning_rate": 2.4058425063505507e-05,
      "loss": 0.9824,
      "step": 9460
    },
    {
      "epoch": 0.6001457587376026,
      "grad_norm": 2.425466299057007,
      "learning_rate": 2.405207451312447e-05,
      "loss": 0.9629,
      "step": 9470
    },
    {
      "epoch": 0.6007794923793529,
      "grad_norm": 2.6814398765563965,
      "learning_rate": 2.4045723962743437e-05,
      "loss": 0.9261,
      "step": 9480
    },
    {
      "epoch": 0.6014132260211034,
      "grad_norm": 2.535936117172241,
      "learning_rate": 2.4039373412362407e-05,
      "loss": 0.9413,
      "step": 9490
    },
    {
      "epoch": 0.6020469596628537,
      "grad_norm": 2.6451144218444824,
      "learning_rate": 2.4033022861981373e-05,
      "loss": 0.9558,
      "step": 9500
    },
    {
      "epoch": 0.602680693304604,
      "grad_norm": 3.3258724212646484,
      "learning_rate": 2.402667231160034e-05,
      "loss": 0.9451,
      "step": 9510
    },
    {
      "epoch": 0.6033144269463544,
      "grad_norm": 2.7634193897247314,
      "learning_rate": 2.4020321761219306e-05,
      "loss": 0.8918,
      "step": 9520
    },
    {
      "epoch": 0.6039481605881049,
      "grad_norm": 2.3382320404052734,
      "learning_rate": 2.4013971210838272e-05,
      "loss": 0.9797,
      "step": 9530
    },
    {
      "epoch": 0.6045818942298552,
      "grad_norm": 2.126988410949707,
      "learning_rate": 2.400762066045724e-05,
      "loss": 0.9552,
      "step": 9540
    },
    {
      "epoch": 0.6052156278716055,
      "grad_norm": 2.858214855194092,
      "learning_rate": 2.400127011007621e-05,
      "loss": 0.9283,
      "step": 9550
    },
    {
      "epoch": 0.6058493615133559,
      "grad_norm": 2.755748748779297,
      "learning_rate": 2.3994919559695175e-05,
      "loss": 0.9193,
      "step": 9560
    },
    {
      "epoch": 0.6064830951551063,
      "grad_norm": 2.8426260948181152,
      "learning_rate": 2.3988569009314138e-05,
      "loss": 0.9151,
      "step": 9570
    },
    {
      "epoch": 0.6071168287968567,
      "grad_norm": 2.162794828414917,
      "learning_rate": 2.3982218458933108e-05,
      "loss": 0.9368,
      "step": 9580
    },
    {
      "epoch": 0.607750562438607,
      "grad_norm": 2.5935282707214355,
      "learning_rate": 2.3975867908552074e-05,
      "loss": 0.9352,
      "step": 9590
    },
    {
      "epoch": 0.6083842960803574,
      "grad_norm": 2.9454729557037354,
      "learning_rate": 2.396951735817104e-05,
      "loss": 0.9458,
      "step": 9600
    },
    {
      "epoch": 0.6090180297221078,
      "grad_norm": 3.3275527954101562,
      "learning_rate": 2.396316680779001e-05,
      "loss": 1.0187,
      "step": 9610
    },
    {
      "epoch": 0.6096517633638582,
      "grad_norm": 2.6062564849853516,
      "learning_rate": 2.3956816257408974e-05,
      "loss": 0.9722,
      "step": 9620
    },
    {
      "epoch": 0.6102854970056085,
      "grad_norm": 1.9506381750106812,
      "learning_rate": 2.3950465707027943e-05,
      "loss": 0.9142,
      "step": 9630
    },
    {
      "epoch": 0.6109192306473589,
      "grad_norm": 2.500009298324585,
      "learning_rate": 2.394411515664691e-05,
      "loss": 0.9508,
      "step": 9640
    },
    {
      "epoch": 0.6115529642891093,
      "grad_norm": 2.6865975856781006,
      "learning_rate": 2.3937764606265876e-05,
      "loss": 0.9263,
      "step": 9650
    },
    {
      "epoch": 0.6121866979308597,
      "grad_norm": 2.2587108612060547,
      "learning_rate": 2.3931414055884846e-05,
      "loss": 0.9421,
      "step": 9660
    },
    {
      "epoch": 0.61282043157261,
      "grad_norm": 2.6219139099121094,
      "learning_rate": 2.3925063505503813e-05,
      "loss": 0.9479,
      "step": 9670
    },
    {
      "epoch": 0.6134541652143604,
      "grad_norm": 3.6941184997558594,
      "learning_rate": 2.3918712955122776e-05,
      "loss": 0.9544,
      "step": 9680
    },
    {
      "epoch": 0.6140878988561108,
      "grad_norm": 2.0799014568328857,
      "learning_rate": 2.3912362404741745e-05,
      "loss": 0.9503,
      "step": 9690
    },
    {
      "epoch": 0.6147216324978612,
      "grad_norm": 2.5601611137390137,
      "learning_rate": 2.3906011854360712e-05,
      "loss": 0.935,
      "step": 9700
    },
    {
      "epoch": 0.6153553661396115,
      "grad_norm": 2.697531223297119,
      "learning_rate": 2.3899661303979678e-05,
      "loss": 0.9212,
      "step": 9710
    },
    {
      "epoch": 0.6159890997813618,
      "grad_norm": 2.0188803672790527,
      "learning_rate": 2.3893310753598648e-05,
      "loss": 0.9003,
      "step": 9720
    },
    {
      "epoch": 0.6166228334231123,
      "grad_norm": 2.6894783973693848,
      "learning_rate": 2.388696020321761e-05,
      "loss": 0.9333,
      "step": 9730
    },
    {
      "epoch": 0.6172565670648626,
      "grad_norm": 3.603621482849121,
      "learning_rate": 2.3880609652836578e-05,
      "loss": 0.9718,
      "step": 9740
    },
    {
      "epoch": 0.617890300706613,
      "grad_norm": 2.2646639347076416,
      "learning_rate": 2.3874259102455547e-05,
      "loss": 0.9352,
      "step": 9750
    },
    {
      "epoch": 0.6185240343483633,
      "grad_norm": 2.4769721031188965,
      "learning_rate": 2.3867908552074514e-05,
      "loss": 0.8933,
      "step": 9760
    },
    {
      "epoch": 0.6191577679901138,
      "grad_norm": 2.1453776359558105,
      "learning_rate": 2.3861558001693484e-05,
      "loss": 0.9541,
      "step": 9770
    },
    {
      "epoch": 0.6197915016318641,
      "grad_norm": 2.4837100505828857,
      "learning_rate": 2.3855207451312447e-05,
      "loss": 0.9539,
      "step": 9780
    },
    {
      "epoch": 0.6204252352736145,
      "grad_norm": 2.5629825592041016,
      "learning_rate": 2.3848856900931413e-05,
      "loss": 0.9999,
      "step": 9790
    },
    {
      "epoch": 0.6210589689153648,
      "grad_norm": 2.5315186977386475,
      "learning_rate": 2.3842506350550383e-05,
      "loss": 0.9511,
      "step": 9800
    },
    {
      "epoch": 0.6216927025571153,
      "grad_norm": 2.2314462661743164,
      "learning_rate": 2.383615580016935e-05,
      "loss": 0.9314,
      "step": 9810
    },
    {
      "epoch": 0.6223264361988656,
      "grad_norm": 2.6850526332855225,
      "learning_rate": 2.3829805249788316e-05,
      "loss": 0.9648,
      "step": 9820
    },
    {
      "epoch": 0.622960169840616,
      "grad_norm": 2.469541549682617,
      "learning_rate": 2.3823454699407282e-05,
      "loss": 0.9149,
      "step": 9830
    },
    {
      "epoch": 0.6235939034823663,
      "grad_norm": 2.6978392601013184,
      "learning_rate": 2.381710414902625e-05,
      "loss": 0.9204,
      "step": 9840
    },
    {
      "epoch": 0.6242276371241168,
      "grad_norm": 2.297720193862915,
      "learning_rate": 2.3810753598645215e-05,
      "loss": 0.9068,
      "step": 9850
    },
    {
      "epoch": 0.6248613707658671,
      "grad_norm": 2.3910019397735596,
      "learning_rate": 2.3804403048264185e-05,
      "loss": 0.9437,
      "step": 9860
    },
    {
      "epoch": 0.6254951044076175,
      "grad_norm": 2.6271259784698486,
      "learning_rate": 2.379805249788315e-05,
      "loss": 0.9029,
      "step": 9870
    },
    {
      "epoch": 0.6261288380493678,
      "grad_norm": 3.0717432498931885,
      "learning_rate": 2.3791701947502114e-05,
      "loss": 1.0094,
      "step": 9880
    },
    {
      "epoch": 0.6267625716911183,
      "grad_norm": 2.6822967529296875,
      "learning_rate": 2.3785351397121084e-05,
      "loss": 0.9071,
      "step": 9890
    },
    {
      "epoch": 0.6273963053328686,
      "grad_norm": 2.7336204051971436,
      "learning_rate": 2.377900084674005e-05,
      "loss": 0.9177,
      "step": 9900
    },
    {
      "epoch": 0.628030038974619,
      "grad_norm": 3.177675247192383,
      "learning_rate": 2.3772650296359017e-05,
      "loss": 0.9388,
      "step": 9910
    },
    {
      "epoch": 0.6286637726163693,
      "grad_norm": 2.5609583854675293,
      "learning_rate": 2.3766299745977987e-05,
      "loss": 0.9453,
      "step": 9920
    },
    {
      "epoch": 0.6292975062581198,
      "grad_norm": 2.173006534576416,
      "learning_rate": 2.3759949195596953e-05,
      "loss": 0.9492,
      "step": 9930
    },
    {
      "epoch": 0.6299312398998701,
      "grad_norm": 2.6579909324645996,
      "learning_rate": 2.375359864521592e-05,
      "loss": 0.9026,
      "step": 9940
    },
    {
      "epoch": 0.6305649735416204,
      "grad_norm": 2.7209458351135254,
      "learning_rate": 2.3747248094834886e-05,
      "loss": 0.9007,
      "step": 9950
    },
    {
      "epoch": 0.6311987071833708,
      "grad_norm": 2.345783233642578,
      "learning_rate": 2.3740897544453853e-05,
      "loss": 0.9198,
      "step": 9960
    },
    {
      "epoch": 0.6318324408251212,
      "grad_norm": 2.953462839126587,
      "learning_rate": 2.3734546994072822e-05,
      "loss": 0.9217,
      "step": 9970
    },
    {
      "epoch": 0.6324661744668716,
      "grad_norm": 2.7734620571136475,
      "learning_rate": 2.372819644369179e-05,
      "loss": 0.9247,
      "step": 9980
    },
    {
      "epoch": 0.6330999081086219,
      "grad_norm": 2.6670899391174316,
      "learning_rate": 2.3721845893310752e-05,
      "loss": 0.9548,
      "step": 9990
    },
    {
      "epoch": 0.6337336417503723,
      "grad_norm": 2.360053777694702,
      "learning_rate": 2.371549534292972e-05,
      "loss": 0.9249,
      "step": 10000
    },
    {
      "epoch": 0.6343673753921227,
      "grad_norm": 3.006300449371338,
      "learning_rate": 2.3709144792548688e-05,
      "loss": 0.9488,
      "step": 10010
    },
    {
      "epoch": 0.6350011090338731,
      "grad_norm": 2.7210474014282227,
      "learning_rate": 2.3702794242167654e-05,
      "loss": 0.9343,
      "step": 10020
    },
    {
      "epoch": 0.6356348426756234,
      "grad_norm": 2.5297038555145264,
      "learning_rate": 2.3696443691786624e-05,
      "loss": 0.9488,
      "step": 10030
    },
    {
      "epoch": 0.6362685763173738,
      "grad_norm": 3.0011544227600098,
      "learning_rate": 2.3690093141405587e-05,
      "loss": 0.9498,
      "step": 10040
    },
    {
      "epoch": 0.6369023099591242,
      "grad_norm": 3.118892192840576,
      "learning_rate": 2.3683742591024554e-05,
      "loss": 0.9867,
      "step": 10050
    },
    {
      "epoch": 0.6375360436008746,
      "grad_norm": 2.2093753814697266,
      "learning_rate": 2.3677392040643524e-05,
      "loss": 0.9283,
      "step": 10060
    },
    {
      "epoch": 0.6381697772426249,
      "grad_norm": 2.9644699096679688,
      "learning_rate": 2.367104149026249e-05,
      "loss": 0.9281,
      "step": 10070
    },
    {
      "epoch": 0.6388035108843753,
      "grad_norm": 2.250732898712158,
      "learning_rate": 2.366469093988146e-05,
      "loss": 0.9396,
      "step": 10080
    },
    {
      "epoch": 0.6394372445261257,
      "grad_norm": 2.5887458324432373,
      "learning_rate": 2.3658340389500423e-05,
      "loss": 0.9436,
      "step": 10090
    },
    {
      "epoch": 0.6400709781678761,
      "grad_norm": 2.676168441772461,
      "learning_rate": 2.365198983911939e-05,
      "loss": 0.9286,
      "step": 10100
    },
    {
      "epoch": 0.6407047118096264,
      "grad_norm": 2.3780431747436523,
      "learning_rate": 2.364563928873836e-05,
      "loss": 0.9105,
      "step": 10110
    },
    {
      "epoch": 0.6413384454513767,
      "grad_norm": 2.314903974533081,
      "learning_rate": 2.3639288738357326e-05,
      "loss": 0.9368,
      "step": 10120
    },
    {
      "epoch": 0.6419721790931272,
      "grad_norm": 2.4001986980438232,
      "learning_rate": 2.3632938187976292e-05,
      "loss": 0.9182,
      "step": 10130
    },
    {
      "epoch": 0.6426059127348775,
      "grad_norm": 2.857478380203247,
      "learning_rate": 2.362658763759526e-05,
      "loss": 0.9705,
      "step": 10140
    },
    {
      "epoch": 0.6432396463766279,
      "grad_norm": 2.348262310028076,
      "learning_rate": 2.3620237087214225e-05,
      "loss": 0.9188,
      "step": 10150
    },
    {
      "epoch": 0.6438733800183782,
      "grad_norm": 2.8464136123657227,
      "learning_rate": 2.361388653683319e-05,
      "loss": 0.9645,
      "step": 10160
    },
    {
      "epoch": 0.6445071136601287,
      "grad_norm": 2.6954705715179443,
      "learning_rate": 2.360753598645216e-05,
      "loss": 0.9534,
      "step": 10170
    },
    {
      "epoch": 0.645140847301879,
      "grad_norm": 2.731018543243408,
      "learning_rate": 2.3601185436071127e-05,
      "loss": 0.9679,
      "step": 10180
    },
    {
      "epoch": 0.6457745809436294,
      "grad_norm": 2.4714765548706055,
      "learning_rate": 2.3594834885690094e-05,
      "loss": 0.9033,
      "step": 10190
    },
    {
      "epoch": 0.6464083145853797,
      "grad_norm": 2.6243956089019775,
      "learning_rate": 2.358848433530906e-05,
      "loss": 0.9285,
      "step": 10200
    },
    {
      "epoch": 0.6470420482271302,
      "grad_norm": 3.1798925399780273,
      "learning_rate": 2.3582133784928027e-05,
      "loss": 0.9112,
      "step": 10210
    },
    {
      "epoch": 0.6476757818688805,
      "grad_norm": 2.929896116256714,
      "learning_rate": 2.3575783234546993e-05,
      "loss": 0.9156,
      "step": 10220
    },
    {
      "epoch": 0.6483095155106309,
      "grad_norm": 2.8251278400421143,
      "learning_rate": 2.3569432684165963e-05,
      "loss": 0.9027,
      "step": 10230
    },
    {
      "epoch": 0.6489432491523812,
      "grad_norm": 3.9813013076782227,
      "learning_rate": 2.356308213378493e-05,
      "loss": 0.9102,
      "step": 10240
    },
    {
      "epoch": 0.6495769827941317,
      "grad_norm": 2.88022518157959,
      "learning_rate": 2.3556731583403896e-05,
      "loss": 0.8874,
      "step": 10250
    },
    {
      "epoch": 0.650210716435882,
      "grad_norm": 3.0530788898468018,
      "learning_rate": 2.3550381033022862e-05,
      "loss": 0.9463,
      "step": 10260
    },
    {
      "epoch": 0.6508444500776324,
      "grad_norm": 2.847869634628296,
      "learning_rate": 2.354403048264183e-05,
      "loss": 0.9419,
      "step": 10270
    },
    {
      "epoch": 0.6514781837193827,
      "grad_norm": 2.6842551231384277,
      "learning_rate": 2.35376799322608e-05,
      "loss": 0.916,
      "step": 10280
    },
    {
      "epoch": 0.6521119173611332,
      "grad_norm": 2.7051808834075928,
      "learning_rate": 2.3531329381879765e-05,
      "loss": 0.9124,
      "step": 10290
    },
    {
      "epoch": 0.6527456510028835,
      "grad_norm": 2.8838863372802734,
      "learning_rate": 2.3524978831498728e-05,
      "loss": 0.9493,
      "step": 10300
    },
    {
      "epoch": 0.6533793846446339,
      "grad_norm": 2.8712515830993652,
      "learning_rate": 2.3518628281117698e-05,
      "loss": 0.9406,
      "step": 10310
    },
    {
      "epoch": 0.6540131182863842,
      "grad_norm": 3.0717456340789795,
      "learning_rate": 2.3512277730736664e-05,
      "loss": 0.9186,
      "step": 10320
    },
    {
      "epoch": 0.6546468519281347,
      "grad_norm": 3.305572986602783,
      "learning_rate": 2.350592718035563e-05,
      "loss": 0.8732,
      "step": 10330
    },
    {
      "epoch": 0.655280585569885,
      "grad_norm": 2.6345016956329346,
      "learning_rate": 2.34995766299746e-05,
      "loss": 0.9014,
      "step": 10340
    },
    {
      "epoch": 0.6559143192116353,
      "grad_norm": 2.774024724960327,
      "learning_rate": 2.3493226079593563e-05,
      "loss": 0.9452,
      "step": 10350
    },
    {
      "epoch": 0.6565480528533857,
      "grad_norm": 2.5123300552368164,
      "learning_rate": 2.348687552921253e-05,
      "loss": 0.9112,
      "step": 10360
    },
    {
      "epoch": 0.6571817864951361,
      "grad_norm": 2.3397152423858643,
      "learning_rate": 2.34805249788315e-05,
      "loss": 0.8959,
      "step": 10370
    },
    {
      "epoch": 0.6578155201368865,
      "grad_norm": 2.8064517974853516,
      "learning_rate": 2.3474174428450466e-05,
      "loss": 0.8671,
      "step": 10380
    },
    {
      "epoch": 0.6584492537786368,
      "grad_norm": 3.03979229927063,
      "learning_rate": 2.3467823878069436e-05,
      "loss": 0.9057,
      "step": 10390
    },
    {
      "epoch": 0.6590829874203872,
      "grad_norm": 2.686368465423584,
      "learning_rate": 2.3461473327688402e-05,
      "loss": 0.9061,
      "step": 10400
    },
    {
      "epoch": 0.6597167210621376,
      "grad_norm": 3.2508182525634766,
      "learning_rate": 2.3455122777307365e-05,
      "loss": 0.9258,
      "step": 10410
    },
    {
      "epoch": 0.660350454703888,
      "grad_norm": 2.428299903869629,
      "learning_rate": 2.3448772226926335e-05,
      "loss": 0.9178,
      "step": 10420
    },
    {
      "epoch": 0.6609841883456383,
      "grad_norm": 3.343003749847412,
      "learning_rate": 2.34424216765453e-05,
      "loss": 0.9308,
      "step": 10430
    },
    {
      "epoch": 0.6616179219873887,
      "grad_norm": 3.5411925315856934,
      "learning_rate": 2.3436071126164268e-05,
      "loss": 0.9091,
      "step": 10440
    },
    {
      "epoch": 0.6622516556291391,
      "grad_norm": 2.6989705562591553,
      "learning_rate": 2.3429720575783238e-05,
      "loss": 0.9202,
      "step": 10450
    },
    {
      "epoch": 0.6628853892708895,
      "grad_norm": 3.07995867729187,
      "learning_rate": 2.34233700254022e-05,
      "loss": 0.9419,
      "step": 10460
    },
    {
      "epoch": 0.6635191229126398,
      "grad_norm": 2.901003122329712,
      "learning_rate": 2.3417019475021167e-05,
      "loss": 0.8604,
      "step": 10470
    },
    {
      "epoch": 0.6641528565543902,
      "grad_norm": 2.6294686794281006,
      "learning_rate": 2.3410668924640137e-05,
      "loss": 0.9109,
      "step": 10480
    },
    {
      "epoch": 0.6647865901961406,
      "grad_norm": 2.763125419616699,
      "learning_rate": 2.3404318374259104e-05,
      "loss": 0.9216,
      "step": 10490
    },
    {
      "epoch": 0.665420323837891,
      "grad_norm": 3.172152280807495,
      "learning_rate": 2.339796782387807e-05,
      "loss": 0.9102,
      "step": 10500
    },
    {
      "epoch": 0.6660540574796413,
      "grad_norm": 3.22257661819458,
      "learning_rate": 2.3391617273497036e-05,
      "loss": 0.8977,
      "step": 10510
    },
    {
      "epoch": 0.6666877911213916,
      "grad_norm": 2.869058132171631,
      "learning_rate": 2.3385266723116003e-05,
      "loss": 0.8761,
      "step": 10520
    },
    {
      "epoch": 0.6673215247631421,
      "grad_norm": 2.648646593093872,
      "learning_rate": 2.337891617273497e-05,
      "loss": 0.9067,
      "step": 10530
    },
    {
      "epoch": 0.6679552584048924,
      "grad_norm": 2.341895580291748,
      "learning_rate": 2.337256562235394e-05,
      "loss": 0.8921,
      "step": 10540
    },
    {
      "epoch": 0.6685889920466428,
      "grad_norm": 2.8787131309509277,
      "learning_rate": 2.3366215071972906e-05,
      "loss": 0.9096,
      "step": 10550
    },
    {
      "epoch": 0.6692227256883931,
      "grad_norm": 2.672619104385376,
      "learning_rate": 2.3359864521591872e-05,
      "loss": 0.8858,
      "step": 10560
    },
    {
      "epoch": 0.6698564593301436,
      "grad_norm": 2.6924679279327393,
      "learning_rate": 2.335351397121084e-05,
      "loss": 0.8981,
      "step": 10570
    },
    {
      "epoch": 0.6704901929718939,
      "grad_norm": 3.0086922645568848,
      "learning_rate": 2.3347163420829805e-05,
      "loss": 0.9086,
      "step": 10580
    },
    {
      "epoch": 0.6711239266136443,
      "grad_norm": 2.637820243835449,
      "learning_rate": 2.3340812870448775e-05,
      "loss": 0.9227,
      "step": 10590
    },
    {
      "epoch": 0.6717576602553946,
      "grad_norm": 2.972930431365967,
      "learning_rate": 2.333446232006774e-05,
      "loss": 0.895,
      "step": 10600
    },
    {
      "epoch": 0.6723913938971451,
      "grad_norm": 2.823545217514038,
      "learning_rate": 2.3328111769686704e-05,
      "loss": 0.9011,
      "step": 10610
    },
    {
      "epoch": 0.6730251275388954,
      "grad_norm": 2.5134923458099365,
      "learning_rate": 2.3321761219305674e-05,
      "loss": 0.9251,
      "step": 10620
    },
    {
      "epoch": 0.6736588611806458,
      "grad_norm": 2.339024543762207,
      "learning_rate": 2.331541066892464e-05,
      "loss": 0.8989,
      "step": 10630
    },
    {
      "epoch": 0.6742925948223961,
      "grad_norm": 2.344633102416992,
      "learning_rate": 2.3309060118543607e-05,
      "loss": 0.9169,
      "step": 10640
    },
    {
      "epoch": 0.6749263284641466,
      "grad_norm": 2.665719985961914,
      "learning_rate": 2.3302709568162577e-05,
      "loss": 0.8758,
      "step": 10650
    },
    {
      "epoch": 0.6755600621058969,
      "grad_norm": 2.7732512950897217,
      "learning_rate": 2.3296359017781543e-05,
      "loss": 0.9271,
      "step": 10660
    },
    {
      "epoch": 0.6761937957476473,
      "grad_norm": 2.7568600177764893,
      "learning_rate": 2.3290008467400506e-05,
      "loss": 0.896,
      "step": 10670
    },
    {
      "epoch": 0.6768275293893976,
      "grad_norm": 2.5581796169281006,
      "learning_rate": 2.3283657917019476e-05,
      "loss": 0.9112,
      "step": 10680
    },
    {
      "epoch": 0.677461263031148,
      "grad_norm": 2.9102089405059814,
      "learning_rate": 2.3277307366638442e-05,
      "loss": 0.9223,
      "step": 10690
    },
    {
      "epoch": 0.6780949966728984,
      "grad_norm": 3.00411319732666,
      "learning_rate": 2.3270956816257412e-05,
      "loss": 0.879,
      "step": 10700
    },
    {
      "epoch": 0.6787287303146488,
      "grad_norm": 3.3148491382598877,
      "learning_rate": 2.326460626587638e-05,
      "loss": 0.9119,
      "step": 10710
    },
    {
      "epoch": 0.6793624639563991,
      "grad_norm": 2.6149353981018066,
      "learning_rate": 2.325825571549534e-05,
      "loss": 0.8818,
      "step": 10720
    },
    {
      "epoch": 0.6799961975981494,
      "grad_norm": 2.787827253341675,
      "learning_rate": 2.325190516511431e-05,
      "loss": 0.9134,
      "step": 10730
    },
    {
      "epoch": 0.6806299312398999,
      "grad_norm": 2.312587022781372,
      "learning_rate": 2.3245554614733278e-05,
      "loss": 0.8685,
      "step": 10740
    },
    {
      "epoch": 0.6812636648816502,
      "grad_norm": 3.3929402828216553,
      "learning_rate": 2.3239204064352244e-05,
      "loss": 0.9143,
      "step": 10750
    },
    {
      "epoch": 0.6818973985234006,
      "grad_norm": 2.9479191303253174,
      "learning_rate": 2.3232853513971214e-05,
      "loss": 0.9011,
      "step": 10760
    },
    {
      "epoch": 0.6825311321651509,
      "grad_norm": 2.700450897216797,
      "learning_rate": 2.3226502963590177e-05,
      "loss": 0.9303,
      "step": 10770
    },
    {
      "epoch": 0.6831648658069014,
      "grad_norm": 2.6712090969085693,
      "learning_rate": 2.3220152413209144e-05,
      "loss": 0.8967,
      "step": 10780
    },
    {
      "epoch": 0.6837985994486517,
      "grad_norm": 2.721543312072754,
      "learning_rate": 2.3213801862828113e-05,
      "loss": 0.8924,
      "step": 10790
    },
    {
      "epoch": 0.6844323330904021,
      "grad_norm": 2.9241979122161865,
      "learning_rate": 2.320745131244708e-05,
      "loss": 0.9323,
      "step": 10800
    },
    {
      "epoch": 0.6850660667321524,
      "grad_norm": 2.729719638824463,
      "learning_rate": 2.3201100762066046e-05,
      "loss": 0.8984,
      "step": 10810
    },
    {
      "epoch": 0.6856998003739029,
      "grad_norm": 2.7382543087005615,
      "learning_rate": 2.3194750211685013e-05,
      "loss": 0.8867,
      "step": 10820
    },
    {
      "epoch": 0.6863335340156532,
      "grad_norm": 3.4753711223602295,
      "learning_rate": 2.318839966130398e-05,
      "loss": 0.8721,
      "step": 10830
    },
    {
      "epoch": 0.6869672676574036,
      "grad_norm": 2.6310958862304688,
      "learning_rate": 2.3182049110922946e-05,
      "loss": 0.8966,
      "step": 10840
    },
    {
      "epoch": 0.6876010012991539,
      "grad_norm": 3.0650229454040527,
      "learning_rate": 2.3175698560541915e-05,
      "loss": 0.9391,
      "step": 10850
    },
    {
      "epoch": 0.6882347349409044,
      "grad_norm": 3.2741494178771973,
      "learning_rate": 2.3169348010160882e-05,
      "loss": 0.9104,
      "step": 10860
    },
    {
      "epoch": 0.6888684685826547,
      "grad_norm": 2.612020492553711,
      "learning_rate": 2.3162997459779848e-05,
      "loss": 0.8819,
      "step": 10870
    },
    {
      "epoch": 0.6895022022244051,
      "grad_norm": 2.812664270401001,
      "learning_rate": 2.3156646909398815e-05,
      "loss": 0.9037,
      "step": 10880
    },
    {
      "epoch": 0.6901359358661554,
      "grad_norm": 2.726696014404297,
      "learning_rate": 2.315029635901778e-05,
      "loss": 0.9293,
      "step": 10890
    },
    {
      "epoch": 0.6907696695079059,
      "grad_norm": 2.367039918899536,
      "learning_rate": 2.314394580863675e-05,
      "loss": 0.919,
      "step": 10900
    },
    {
      "epoch": 0.6914034031496562,
      "grad_norm": 2.454620122909546,
      "learning_rate": 2.3137595258255717e-05,
      "loss": 0.8982,
      "step": 10910
    },
    {
      "epoch": 0.6920371367914065,
      "grad_norm": 2.831268072128296,
      "learning_rate": 2.3131244707874684e-05,
      "loss": 0.8687,
      "step": 10920
    },
    {
      "epoch": 0.6926708704331569,
      "grad_norm": 2.8648037910461426,
      "learning_rate": 2.312489415749365e-05,
      "loss": 0.9344,
      "step": 10930
    },
    {
      "epoch": 0.6933046040749073,
      "grad_norm": 2.675287961959839,
      "learning_rate": 2.3118543607112617e-05,
      "loss": 0.9088,
      "step": 10940
    },
    {
      "epoch": 0.6939383377166577,
      "grad_norm": 2.368258476257324,
      "learning_rate": 2.3112193056731583e-05,
      "loss": 0.8605,
      "step": 10950
    },
    {
      "epoch": 0.694572071358408,
      "grad_norm": 3.305015802383423,
      "learning_rate": 2.3105842506350553e-05,
      "loss": 0.9084,
      "step": 10960
    },
    {
      "epoch": 0.6952058050001584,
      "grad_norm": 2.578406572341919,
      "learning_rate": 2.309949195596952e-05,
      "loss": 0.9029,
      "step": 10970
    },
    {
      "epoch": 0.6958395386419088,
      "grad_norm": 2.9109609127044678,
      "learning_rate": 2.3093141405588482e-05,
      "loss": 0.8721,
      "step": 10980
    },
    {
      "epoch": 0.6964732722836592,
      "grad_norm": 2.3781394958496094,
      "learning_rate": 2.3086790855207452e-05,
      "loss": 0.8601,
      "step": 10990
    },
    {
      "epoch": 0.6971070059254095,
      "grad_norm": 2.4462196826934814,
      "learning_rate": 2.308044030482642e-05,
      "loss": 0.8758,
      "step": 11000
    },
    {
      "epoch": 0.6977407395671599,
      "grad_norm": 2.601332664489746,
      "learning_rate": 2.3074089754445385e-05,
      "loss": 0.9348,
      "step": 11010
    },
    {
      "epoch": 0.6983744732089103,
      "grad_norm": 2.72023868560791,
      "learning_rate": 2.3067739204064355e-05,
      "loss": 0.8834,
      "step": 11020
    },
    {
      "epoch": 0.6990082068506607,
      "grad_norm": 3.4715285301208496,
      "learning_rate": 2.3061388653683318e-05,
      "loss": 0.9089,
      "step": 11030
    },
    {
      "epoch": 0.699641940492411,
      "grad_norm": 2.7907772064208984,
      "learning_rate": 2.3055038103302288e-05,
      "loss": 0.884,
      "step": 11040
    },
    {
      "epoch": 0.7002756741341614,
      "grad_norm": 2.7843475341796875,
      "learning_rate": 2.3048687552921254e-05,
      "loss": 0.8843,
      "step": 11050
    },
    {
      "epoch": 0.7009094077759118,
      "grad_norm": 3.3096320629119873,
      "learning_rate": 2.304233700254022e-05,
      "loss": 0.8569,
      "step": 11060
    },
    {
      "epoch": 0.7015431414176622,
      "grad_norm": 2.4272050857543945,
      "learning_rate": 2.303598645215919e-05,
      "loss": 0.8574,
      "step": 11070
    },
    {
      "epoch": 0.7021768750594125,
      "grad_norm": 2.7075891494750977,
      "learning_rate": 2.3029635901778153e-05,
      "loss": 0.9298,
      "step": 11080
    },
    {
      "epoch": 0.7028106087011629,
      "grad_norm": 2.5774388313293457,
      "learning_rate": 2.302328535139712e-05,
      "loss": 0.8846,
      "step": 11090
    },
    {
      "epoch": 0.7034443423429133,
      "grad_norm": 2.2494261264801025,
      "learning_rate": 2.301693480101609e-05,
      "loss": 0.8439,
      "step": 11100
    },
    {
      "epoch": 0.7040780759846637,
      "grad_norm": 2.643949031829834,
      "learning_rate": 2.3010584250635056e-05,
      "loss": 0.8922,
      "step": 11110
    },
    {
      "epoch": 0.704711809626414,
      "grad_norm": 2.3081560134887695,
      "learning_rate": 2.3004868755292126e-05,
      "loss": 0.9226,
      "step": 11120
    },
    {
      "epoch": 0.7053455432681643,
      "grad_norm": 3.0476882457733154,
      "learning_rate": 2.2998518204911093e-05,
      "loss": 0.9106,
      "step": 11130
    },
    {
      "epoch": 0.7059792769099148,
      "grad_norm": 3.016237497329712,
      "learning_rate": 2.299216765453006e-05,
      "loss": 0.9174,
      "step": 11140
    },
    {
      "epoch": 0.7066130105516651,
      "grad_norm": 2.286888837814331,
      "learning_rate": 2.2985817104149026e-05,
      "loss": 0.9112,
      "step": 11150
    },
    {
      "epoch": 0.7072467441934155,
      "grad_norm": 2.496037006378174,
      "learning_rate": 2.2979466553767992e-05,
      "loss": 0.8402,
      "step": 11160
    },
    {
      "epoch": 0.7078804778351658,
      "grad_norm": 2.72318696975708,
      "learning_rate": 2.2973116003386962e-05,
      "loss": 0.8763,
      "step": 11170
    },
    {
      "epoch": 0.7085142114769163,
      "grad_norm": 2.3716604709625244,
      "learning_rate": 2.296676545300593e-05,
      "loss": 0.9129,
      "step": 11180
    },
    {
      "epoch": 0.7091479451186666,
      "grad_norm": 3.000404119491577,
      "learning_rate": 2.2960414902624895e-05,
      "loss": 0.916,
      "step": 11190
    },
    {
      "epoch": 0.709781678760417,
      "grad_norm": 2.4323890209198,
      "learning_rate": 2.295406435224386e-05,
      "loss": 0.8683,
      "step": 11200
    },
    {
      "epoch": 0.7104154124021673,
      "grad_norm": 2.332850694656372,
      "learning_rate": 2.2947713801862828e-05,
      "loss": 0.8764,
      "step": 11210
    },
    {
      "epoch": 0.7110491460439178,
      "grad_norm": 2.746978282928467,
      "learning_rate": 2.2941363251481797e-05,
      "loss": 0.8951,
      "step": 11220
    },
    {
      "epoch": 0.7116828796856681,
      "grad_norm": 2.3636317253112793,
      "learning_rate": 2.2935012701100764e-05,
      "loss": 0.8805,
      "step": 11230
    },
    {
      "epoch": 0.7123166133274185,
      "grad_norm": 2.7804360389709473,
      "learning_rate": 2.2928662150719727e-05,
      "loss": 0.8509,
      "step": 11240
    },
    {
      "epoch": 0.7129503469691688,
      "grad_norm": 2.5125837326049805,
      "learning_rate": 2.2922311600338697e-05,
      "loss": 0.8424,
      "step": 11250
    },
    {
      "epoch": 0.7135840806109193,
      "grad_norm": 2.574906826019287,
      "learning_rate": 2.2915961049957663e-05,
      "loss": 0.8885,
      "step": 11260
    },
    {
      "epoch": 0.7142178142526696,
      "grad_norm": 2.5433454513549805,
      "learning_rate": 2.290961049957663e-05,
      "loss": 0.8964,
      "step": 11270
    },
    {
      "epoch": 0.71485154789442,
      "grad_norm": 3.0278000831604004,
      "learning_rate": 2.29032599491956e-05,
      "loss": 0.9307,
      "step": 11280
    },
    {
      "epoch": 0.7154852815361703,
      "grad_norm": 2.723392963409424,
      "learning_rate": 2.2896909398814566e-05,
      "loss": 0.8782,
      "step": 11290
    },
    {
      "epoch": 0.7161190151779208,
      "grad_norm": 3.7283589839935303,
      "learning_rate": 2.289055884843353e-05,
      "loss": 0.8868,
      "step": 11300
    },
    {
      "epoch": 0.7167527488196711,
      "grad_norm": 2.531069278717041,
      "learning_rate": 2.28842082980525e-05,
      "loss": 0.8258,
      "step": 11310
    },
    {
      "epoch": 0.7173864824614214,
      "grad_norm": 3.033503770828247,
      "learning_rate": 2.2877857747671465e-05,
      "loss": 0.8863,
      "step": 11320
    },
    {
      "epoch": 0.7180202161031718,
      "grad_norm": 2.486292839050293,
      "learning_rate": 2.2871507197290435e-05,
      "loss": 0.8706,
      "step": 11330
    },
    {
      "epoch": 0.7186539497449222,
      "grad_norm": 2.707362413406372,
      "learning_rate": 2.28651566469094e-05,
      "loss": 0.8678,
      "step": 11340
    },
    {
      "epoch": 0.7192876833866726,
      "grad_norm": 2.932262420654297,
      "learning_rate": 2.2858806096528364e-05,
      "loss": 0.8907,
      "step": 11350
    },
    {
      "epoch": 0.7199214170284229,
      "grad_norm": 2.8879029750823975,
      "learning_rate": 2.2852455546147334e-05,
      "loss": 0.9146,
      "step": 11360
    },
    {
      "epoch": 0.7205551506701733,
      "grad_norm": 2.2523298263549805,
      "learning_rate": 2.28461049957663e-05,
      "loss": 0.8596,
      "step": 11370
    },
    {
      "epoch": 0.7211888843119237,
      "grad_norm": 2.618563652038574,
      "learning_rate": 2.2839754445385267e-05,
      "loss": 0.8481,
      "step": 11380
    },
    {
      "epoch": 0.7218226179536741,
      "grad_norm": 2.689938545227051,
      "learning_rate": 2.2833403895004237e-05,
      "loss": 0.8797,
      "step": 11390
    },
    {
      "epoch": 0.7224563515954244,
      "grad_norm": 2.373206377029419,
      "learning_rate": 2.28270533446232e-05,
      "loss": 0.9271,
      "step": 11400
    },
    {
      "epoch": 0.7230900852371748,
      "grad_norm": 2.5451743602752686,
      "learning_rate": 2.2820702794242166e-05,
      "loss": 0.8609,
      "step": 11410
    },
    {
      "epoch": 0.7237238188789252,
      "grad_norm": 2.369440793991089,
      "learning_rate": 2.2814352243861136e-05,
      "loss": 0.8667,
      "step": 11420
    },
    {
      "epoch": 0.7243575525206756,
      "grad_norm": 2.7511088848114014,
      "learning_rate": 2.2808001693480103e-05,
      "loss": 0.8864,
      "step": 11430
    },
    {
      "epoch": 0.7249912861624259,
      "grad_norm": 2.5427684783935547,
      "learning_rate": 2.280165114309907e-05,
      "loss": 0.8692,
      "step": 11440
    },
    {
      "epoch": 0.7256250198041763,
      "grad_norm": 2.4423906803131104,
      "learning_rate": 2.2795300592718035e-05,
      "loss": 0.8415,
      "step": 11450
    },
    {
      "epoch": 0.7262587534459267,
      "grad_norm": 2.6075615882873535,
      "learning_rate": 2.2788950042337002e-05,
      "loss": 0.9333,
      "step": 11460
    },
    {
      "epoch": 0.7268924870876771,
      "grad_norm": 3.1445517539978027,
      "learning_rate": 2.278259949195597e-05,
      "loss": 0.912,
      "step": 11470
    },
    {
      "epoch": 0.7275262207294274,
      "grad_norm": 2.5151641368865967,
      "learning_rate": 2.2776248941574938e-05,
      "loss": 0.8766,
      "step": 11480
    },
    {
      "epoch": 0.7281599543711778,
      "grad_norm": 2.8760788440704346,
      "learning_rate": 2.2769898391193905e-05,
      "loss": 0.9149,
      "step": 11490
    },
    {
      "epoch": 0.7287936880129282,
      "grad_norm": 2.9003918170928955,
      "learning_rate": 2.276354784081287e-05,
      "loss": 0.881,
      "step": 11500
    },
    {
      "epoch": 0.7294274216546786,
      "grad_norm": 2.431610345840454,
      "learning_rate": 2.2757197290431837e-05,
      "loss": 0.8651,
      "step": 11510
    },
    {
      "epoch": 0.7300611552964289,
      "grad_norm": 2.914886951446533,
      "learning_rate": 2.2750846740050804e-05,
      "loss": 0.8892,
      "step": 11520
    },
    {
      "epoch": 0.7306948889381792,
      "grad_norm": 2.566147565841675,
      "learning_rate": 2.2744496189669774e-05,
      "loss": 0.8826,
      "step": 11530
    },
    {
      "epoch": 0.7313286225799297,
      "grad_norm": 2.5795938968658447,
      "learning_rate": 2.273814563928874e-05,
      "loss": 0.9165,
      "step": 11540
    },
    {
      "epoch": 0.73196235622168,
      "grad_norm": 2.162437677383423,
      "learning_rate": 2.2731795088907707e-05,
      "loss": 0.8746,
      "step": 11550
    },
    {
      "epoch": 0.7325960898634304,
      "grad_norm": 2.495314121246338,
      "learning_rate": 2.2725444538526673e-05,
      "loss": 0.9113,
      "step": 11560
    },
    {
      "epoch": 0.7332298235051807,
      "grad_norm": 2.498319387435913,
      "learning_rate": 2.271909398814564e-05,
      "loss": 0.8647,
      "step": 11570
    },
    {
      "epoch": 0.7338635571469312,
      "grad_norm": 2.3378353118896484,
      "learning_rate": 2.2712743437764606e-05,
      "loss": 0.854,
      "step": 11580
    },
    {
      "epoch": 0.7344972907886815,
      "grad_norm": 3.1113674640655518,
      "learning_rate": 2.2706392887383576e-05,
      "loss": 0.8861,
      "step": 11590
    },
    {
      "epoch": 0.7351310244304319,
      "grad_norm": 2.745837450027466,
      "learning_rate": 2.2700042337002542e-05,
      "loss": 0.8759,
      "step": 11600
    },
    {
      "epoch": 0.7357647580721822,
      "grad_norm": 2.3565683364868164,
      "learning_rate": 2.2693691786621505e-05,
      "loss": 0.8615,
      "step": 11610
    },
    {
      "epoch": 0.7363984917139327,
      "grad_norm": 3.68918514251709,
      "learning_rate": 2.2687341236240475e-05,
      "loss": 0.8654,
      "step": 11620
    },
    {
      "epoch": 0.737032225355683,
      "grad_norm": 2.7015891075134277,
      "learning_rate": 2.268099068585944e-05,
      "loss": 0.8951,
      "step": 11630
    },
    {
      "epoch": 0.7376659589974334,
      "grad_norm": 2.5554611682891846,
      "learning_rate": 2.2674640135478408e-05,
      "loss": 0.8858,
      "step": 11640
    },
    {
      "epoch": 0.7382996926391837,
      "grad_norm": 2.66017746925354,
      "learning_rate": 2.2668289585097378e-05,
      "loss": 0.9296,
      "step": 11650
    },
    {
      "epoch": 0.7389334262809342,
      "grad_norm": 2.633955955505371,
      "learning_rate": 2.266193903471634e-05,
      "loss": 0.9018,
      "step": 11660
    },
    {
      "epoch": 0.7395671599226845,
      "grad_norm": 3.0893666744232178,
      "learning_rate": 2.265558848433531e-05,
      "loss": 0.8756,
      "step": 11670
    },
    {
      "epoch": 0.7402008935644349,
      "grad_norm": 2.529947280883789,
      "learning_rate": 2.2649237933954277e-05,
      "loss": 0.9133,
      "step": 11680
    },
    {
      "epoch": 0.7408346272061852,
      "grad_norm": 3.226402997970581,
      "learning_rate": 2.2642887383573243e-05,
      "loss": 0.8985,
      "step": 11690
    },
    {
      "epoch": 0.7414683608479357,
      "grad_norm": 2.901736259460449,
      "learning_rate": 2.2636536833192213e-05,
      "loss": 0.9202,
      "step": 11700
    },
    {
      "epoch": 0.742102094489686,
      "grad_norm": 3.1939687728881836,
      "learning_rate": 2.2630186282811176e-05,
      "loss": 0.8774,
      "step": 11710
    },
    {
      "epoch": 0.7427358281314363,
      "grad_norm": 2.1364517211914062,
      "learning_rate": 2.2623835732430143e-05,
      "loss": 0.9423,
      "step": 11720
    },
    {
      "epoch": 0.7433695617731867,
      "grad_norm": 2.455350399017334,
      "learning_rate": 2.2617485182049112e-05,
      "loss": 0.9302,
      "step": 11730
    },
    {
      "epoch": 0.7440032954149371,
      "grad_norm": 2.5191171169281006,
      "learning_rate": 2.261113463166808e-05,
      "loss": 0.9084,
      "step": 11740
    },
    {
      "epoch": 0.7446370290566875,
      "grad_norm": 2.5917229652404785,
      "learning_rate": 2.2604784081287045e-05,
      "loss": 0.9331,
      "step": 11750
    },
    {
      "epoch": 0.7452707626984378,
      "grad_norm": 2.761322021484375,
      "learning_rate": 2.259843353090601e-05,
      "loss": 0.894,
      "step": 11760
    },
    {
      "epoch": 0.7459044963401882,
      "grad_norm": 2.8212873935699463,
      "learning_rate": 2.2592082980524978e-05,
      "loss": 0.9054,
      "step": 11770
    },
    {
      "epoch": 0.7465382299819386,
      "grad_norm": 2.4036409854888916,
      "learning_rate": 2.2585732430143944e-05,
      "loss": 0.8636,
      "step": 11780
    },
    {
      "epoch": 0.747171963623689,
      "grad_norm": 2.576643466949463,
      "learning_rate": 2.2579381879762914e-05,
      "loss": 0.9039,
      "step": 11790
    },
    {
      "epoch": 0.7478056972654393,
      "grad_norm": 2.6737866401672363,
      "learning_rate": 2.257303132938188e-05,
      "loss": 0.8885,
      "step": 11800
    },
    {
      "epoch": 0.7484394309071897,
      "grad_norm": 2.9098410606384277,
      "learning_rate": 2.256668077900085e-05,
      "loss": 0.9211,
      "step": 11810
    },
    {
      "epoch": 0.7490731645489401,
      "grad_norm": 2.3220114707946777,
      "learning_rate": 2.2560330228619814e-05,
      "loss": 0.8356,
      "step": 11820
    },
    {
      "epoch": 0.7497068981906905,
      "grad_norm": 2.430539131164551,
      "learning_rate": 2.255397967823878e-05,
      "loss": 0.8668,
      "step": 11830
    },
    {
      "epoch": 0.7503406318324408,
      "grad_norm": 2.569017171859741,
      "learning_rate": 2.254762912785775e-05,
      "loss": 0.8361,
      "step": 11840
    },
    {
      "epoch": 0.7509743654741912,
      "grad_norm": 2.6462411880493164,
      "learning_rate": 2.2541278577476716e-05,
      "loss": 0.9202,
      "step": 11850
    },
    {
      "epoch": 0.7516080991159416,
      "grad_norm": 2.708401918411255,
      "learning_rate": 2.2534928027095683e-05,
      "loss": 0.8776,
      "step": 11860
    },
    {
      "epoch": 0.752241832757692,
      "grad_norm": 2.968353271484375,
      "learning_rate": 2.252857747671465e-05,
      "loss": 0.869,
      "step": 11870
    },
    {
      "epoch": 0.7528755663994423,
      "grad_norm": 2.632642984390259,
      "learning_rate": 2.2522226926333616e-05,
      "loss": 0.8998,
      "step": 11880
    },
    {
      "epoch": 0.7535093000411927,
      "grad_norm": 2.674168109893799,
      "learning_rate": 2.2515876375952582e-05,
      "loss": 0.8591,
      "step": 11890
    },
    {
      "epoch": 0.7541430336829431,
      "grad_norm": 2.3604800701141357,
      "learning_rate": 2.2509525825571552e-05,
      "loss": 0.9083,
      "step": 11900
    },
    {
      "epoch": 0.7547767673246935,
      "grad_norm": 2.653898239135742,
      "learning_rate": 2.2503175275190518e-05,
      "loss": 0.9229,
      "step": 11910
    },
    {
      "epoch": 0.7554105009664438,
      "grad_norm": 2.5257041454315186,
      "learning_rate": 2.249682472480948e-05,
      "loss": 0.8899,
      "step": 11920
    },
    {
      "epoch": 0.7560442346081941,
      "grad_norm": 2.627610445022583,
      "learning_rate": 2.249047417442845e-05,
      "loss": 0.8879,
      "step": 11930
    },
    {
      "epoch": 0.7566779682499446,
      "grad_norm": 2.847301483154297,
      "learning_rate": 2.2484123624047417e-05,
      "loss": 0.9163,
      "step": 11940
    },
    {
      "epoch": 0.7573117018916949,
      "grad_norm": 2.4903602600097656,
      "learning_rate": 2.2477773073666384e-05,
      "loss": 0.8452,
      "step": 11950
    },
    {
      "epoch": 0.7579454355334453,
      "grad_norm": 2.7806508541107178,
      "learning_rate": 2.2471422523285354e-05,
      "loss": 0.8932,
      "step": 11960
    },
    {
      "epoch": 0.7585791691751956,
      "grad_norm": 2.513731002807617,
      "learning_rate": 2.2465071972904317e-05,
      "loss": 0.9117,
      "step": 11970
    },
    {
      "epoch": 0.7592129028169461,
      "grad_norm": 2.8140463829040527,
      "learning_rate": 2.2458721422523287e-05,
      "loss": 0.8751,
      "step": 11980
    },
    {
      "epoch": 0.7598466364586964,
      "grad_norm": 2.67763090133667,
      "learning_rate": 2.2452370872142253e-05,
      "loss": 0.8934,
      "step": 11990
    },
    {
      "epoch": 0.7604803701004468,
      "grad_norm": 2.690945863723755,
      "learning_rate": 2.244602032176122e-05,
      "loss": 0.8866,
      "step": 12000
    },
    {
      "epoch": 0.7611141037421971,
      "grad_norm": 2.2616376876831055,
      "learning_rate": 2.243966977138019e-05,
      "loss": 0.8957,
      "step": 12010
    },
    {
      "epoch": 0.7617478373839476,
      "grad_norm": 3.2670459747314453,
      "learning_rate": 2.2433319220999152e-05,
      "loss": 0.8812,
      "step": 12020
    },
    {
      "epoch": 0.7623815710256979,
      "grad_norm": 3.1054654121398926,
      "learning_rate": 2.242696867061812e-05,
      "loss": 0.8816,
      "step": 12030
    },
    {
      "epoch": 0.7630153046674483,
      "grad_norm": 3.0049591064453125,
      "learning_rate": 2.242061812023709e-05,
      "loss": 0.877,
      "step": 12040
    },
    {
      "epoch": 0.7636490383091986,
      "grad_norm": 2.846219301223755,
      "learning_rate": 2.2414267569856055e-05,
      "loss": 0.8769,
      "step": 12050
    },
    {
      "epoch": 0.7642827719509491,
      "grad_norm": 2.5906336307525635,
      "learning_rate": 2.240791701947502e-05,
      "loss": 0.8857,
      "step": 12060
    },
    {
      "epoch": 0.7649165055926994,
      "grad_norm": 3.009115695953369,
      "learning_rate": 2.240156646909399e-05,
      "loss": 0.8995,
      "step": 12070
    },
    {
      "epoch": 0.7655502392344498,
      "grad_norm": 2.697626829147339,
      "learning_rate": 2.2395215918712954e-05,
      "loss": 0.8815,
      "step": 12080
    },
    {
      "epoch": 0.7661839728762001,
      "grad_norm": 2.483557939529419,
      "learning_rate": 2.238886536833192e-05,
      "loss": 0.9155,
      "step": 12090
    },
    {
      "epoch": 0.7668177065179506,
      "grad_norm": 2.527679204940796,
      "learning_rate": 2.238251481795089e-05,
      "loss": 0.8821,
      "step": 12100
    },
    {
      "epoch": 0.7674514401597009,
      "grad_norm": 2.5689666271209717,
      "learning_rate": 2.2376164267569857e-05,
      "loss": 0.8434,
      "step": 12110
    },
    {
      "epoch": 0.7680851738014512,
      "grad_norm": 2.4715137481689453,
      "learning_rate": 2.2369813717188827e-05,
      "loss": 0.8845,
      "step": 12120
    },
    {
      "epoch": 0.7687189074432016,
      "grad_norm": 3.100144147872925,
      "learning_rate": 2.236346316680779e-05,
      "loss": 0.8966,
      "step": 12130
    },
    {
      "epoch": 0.769352641084952,
      "grad_norm": 2.7340478897094727,
      "learning_rate": 2.2357112616426756e-05,
      "loss": 0.8572,
      "step": 12140
    },
    {
      "epoch": 0.7699863747267024,
      "grad_norm": 2.774057149887085,
      "learning_rate": 2.2350762066045726e-05,
      "loss": 0.8632,
      "step": 12150
    },
    {
      "epoch": 0.7706201083684527,
      "grad_norm": 2.8506851196289062,
      "learning_rate": 2.2344411515664692e-05,
      "loss": 0.8765,
      "step": 12160
    },
    {
      "epoch": 0.7712538420102031,
      "grad_norm": 2.364302396774292,
      "learning_rate": 2.233806096528366e-05,
      "loss": 0.8769,
      "step": 12170
    },
    {
      "epoch": 0.7718875756519535,
      "grad_norm": 2.50726580619812,
      "learning_rate": 2.2331710414902625e-05,
      "loss": 0.9206,
      "step": 12180
    },
    {
      "epoch": 0.7725213092937039,
      "grad_norm": 2.5364248752593994,
      "learning_rate": 2.2325359864521592e-05,
      "loss": 0.9021,
      "step": 12190
    },
    {
      "epoch": 0.7731550429354542,
      "grad_norm": 2.5426113605499268,
      "learning_rate": 2.2319009314140558e-05,
      "loss": 0.8984,
      "step": 12200
    },
    {
      "epoch": 0.7737887765772046,
      "grad_norm": 3.039476156234741,
      "learning_rate": 2.2312658763759528e-05,
      "loss": 0.9107,
      "step": 12210
    },
    {
      "epoch": 0.7744225102189549,
      "grad_norm": 3.534250497817993,
      "learning_rate": 2.2306308213378494e-05,
      "loss": 0.8863,
      "step": 12220
    },
    {
      "epoch": 0.7750562438607054,
      "grad_norm": 2.633138418197632,
      "learning_rate": 2.2299957662997457e-05,
      "loss": 0.8634,
      "step": 12230
    },
    {
      "epoch": 0.7756899775024557,
      "grad_norm": 2.393345832824707,
      "learning_rate": 2.2293607112616427e-05,
      "loss": 0.858,
      "step": 12240
    },
    {
      "epoch": 0.7763237111442061,
      "grad_norm": 3.1426808834075928,
      "learning_rate": 2.2287256562235394e-05,
      "loss": 0.8552,
      "step": 12250
    },
    {
      "epoch": 0.7769574447859564,
      "grad_norm": 2.3800923824310303,
      "learning_rate": 2.228090601185436e-05,
      "loss": 0.8807,
      "step": 12260
    },
    {
      "epoch": 0.7775911784277069,
      "grad_norm": 2.6557836532592773,
      "learning_rate": 2.227455546147333e-05,
      "loss": 0.8474,
      "step": 12270
    },
    {
      "epoch": 0.7782249120694572,
      "grad_norm": 2.85386323928833,
      "learning_rate": 2.2268204911092293e-05,
      "loss": 0.9008,
      "step": 12280
    },
    {
      "epoch": 0.7788586457112076,
      "grad_norm": 3.2128889560699463,
      "learning_rate": 2.2261854360711263e-05,
      "loss": 0.8916,
      "step": 12290
    },
    {
      "epoch": 0.7794923793529579,
      "grad_norm": 2.520719528198242,
      "learning_rate": 2.225550381033023e-05,
      "loss": 0.8624,
      "step": 12300
    },
    {
      "epoch": 0.7801261129947084,
      "grad_norm": 2.913175106048584,
      "learning_rate": 2.2249153259949196e-05,
      "loss": 0.8723,
      "step": 12310
    },
    {
      "epoch": 0.7807598466364587,
      "grad_norm": 2.6526832580566406,
      "learning_rate": 2.2242802709568165e-05,
      "loss": 0.914,
      "step": 12320
    },
    {
      "epoch": 0.781393580278209,
      "grad_norm": 2.6920199394226074,
      "learning_rate": 2.2236452159187132e-05,
      "loss": 0.8818,
      "step": 12330
    },
    {
      "epoch": 0.7820273139199594,
      "grad_norm": 2.4588782787323,
      "learning_rate": 2.2230101608806095e-05,
      "loss": 0.9137,
      "step": 12340
    },
    {
      "epoch": 0.7826610475617098,
      "grad_norm": 2.3281853199005127,
      "learning_rate": 2.2223751058425065e-05,
      "loss": 0.8515,
      "step": 12350
    },
    {
      "epoch": 0.7832947812034602,
      "grad_norm": 2.1753413677215576,
      "learning_rate": 2.221740050804403e-05,
      "loss": 0.9041,
      "step": 12360
    },
    {
      "epoch": 0.7839285148452105,
      "grad_norm": 2.214839458465576,
      "learning_rate": 2.2211049957662998e-05,
      "loss": 0.858,
      "step": 12370
    },
    {
      "epoch": 0.7845622484869609,
      "grad_norm": 2.517937421798706,
      "learning_rate": 2.2204699407281967e-05,
      "loss": 0.8705,
      "step": 12380
    },
    {
      "epoch": 0.7851959821287113,
      "grad_norm": 2.3792080879211426,
      "learning_rate": 2.219834885690093e-05,
      "loss": 0.9096,
      "step": 12390
    },
    {
      "epoch": 0.7858297157704617,
      "grad_norm": 2.7714734077453613,
      "learning_rate": 2.2191998306519897e-05,
      "loss": 0.8925,
      "step": 12400
    },
    {
      "epoch": 0.786463449412212,
      "grad_norm": 2.66195011138916,
      "learning_rate": 2.2185647756138867e-05,
      "loss": 0.8365,
      "step": 12410
    },
    {
      "epoch": 0.7870971830539624,
      "grad_norm": 2.673104763031006,
      "learning_rate": 2.2179297205757833e-05,
      "loss": 0.8724,
      "step": 12420
    },
    {
      "epoch": 0.7877309166957128,
      "grad_norm": 2.836909770965576,
      "learning_rate": 2.2172946655376803e-05,
      "loss": 0.8852,
      "step": 12430
    },
    {
      "epoch": 0.7883646503374632,
      "grad_norm": 2.5866730213165283,
      "learning_rate": 2.2166596104995766e-05,
      "loss": 0.8997,
      "step": 12440
    },
    {
      "epoch": 0.7889983839792135,
      "grad_norm": 2.3596248626708984,
      "learning_rate": 2.2160245554614732e-05,
      "loss": 0.8529,
      "step": 12450
    },
    {
      "epoch": 0.7896321176209639,
      "grad_norm": 2.602994680404663,
      "learning_rate": 2.2153895004233702e-05,
      "loss": 0.8724,
      "step": 12460
    },
    {
      "epoch": 0.7902658512627143,
      "grad_norm": 2.5479021072387695,
      "learning_rate": 2.214754445385267e-05,
      "loss": 0.8729,
      "step": 12470
    },
    {
      "epoch": 0.7908995849044647,
      "grad_norm": 2.7566978931427,
      "learning_rate": 2.2141193903471635e-05,
      "loss": 0.9155,
      "step": 12480
    },
    {
      "epoch": 0.791533318546215,
      "grad_norm": 2.934816360473633,
      "learning_rate": 2.21348433530906e-05,
      "loss": 0.8632,
      "step": 12490
    },
    {
      "epoch": 0.7921670521879653,
      "grad_norm": 2.772369623184204,
      "learning_rate": 2.2128492802709568e-05,
      "loss": 0.8755,
      "step": 12500
    },
    {
      "epoch": 0.7928007858297158,
      "grad_norm": 2.28131103515625,
      "learning_rate": 2.2122142252328534e-05,
      "loss": 0.8594,
      "step": 12510
    },
    {
      "epoch": 0.7934345194714661,
      "grad_norm": 2.8458595275878906,
      "learning_rate": 2.2115791701947504e-05,
      "loss": 0.8962,
      "step": 12520
    },
    {
      "epoch": 0.7940682531132165,
      "grad_norm": 2.624633550643921,
      "learning_rate": 2.210944115156647e-05,
      "loss": 0.9035,
      "step": 12530
    },
    {
      "epoch": 0.7947019867549668,
      "grad_norm": 2.282276153564453,
      "learning_rate": 2.2103090601185434e-05,
      "loss": 0.891,
      "step": 12540
    },
    {
      "epoch": 0.7953357203967173,
      "grad_norm": 2.3910818099975586,
      "learning_rate": 2.2096740050804403e-05,
      "loss": 0.8338,
      "step": 12550
    },
    {
      "epoch": 0.7959694540384676,
      "grad_norm": 2.310664415359497,
      "learning_rate": 2.209038950042337e-05,
      "loss": 0.8696,
      "step": 12560
    },
    {
      "epoch": 0.796603187680218,
      "grad_norm": 2.4116828441619873,
      "learning_rate": 2.2084038950042336e-05,
      "loss": 0.8982,
      "step": 12570
    },
    {
      "epoch": 0.7972369213219683,
      "grad_norm": 2.6020753383636475,
      "learning_rate": 2.2077688399661306e-05,
      "loss": 0.8823,
      "step": 12580
    },
    {
      "epoch": 0.7978706549637188,
      "grad_norm": 2.909609317779541,
      "learning_rate": 2.2071337849280273e-05,
      "loss": 0.8579,
      "step": 12590
    },
    {
      "epoch": 0.7985043886054691,
      "grad_norm": 2.616763114929199,
      "learning_rate": 2.206498729889924e-05,
      "loss": 0.8536,
      "step": 12600
    },
    {
      "epoch": 0.7991381222472195,
      "grad_norm": 2.706928014755249,
      "learning_rate": 2.2058636748518205e-05,
      "loss": 0.943,
      "step": 12610
    },
    {
      "epoch": 0.7997718558889698,
      "grad_norm": 3.14524507522583,
      "learning_rate": 2.2052286198137172e-05,
      "loss": 0.8753,
      "step": 12620
    },
    {
      "epoch": 0.8004055895307203,
      "grad_norm": 2.878547430038452,
      "learning_rate": 2.204593564775614e-05,
      "loss": 0.8545,
      "step": 12630
    },
    {
      "epoch": 0.8010393231724706,
      "grad_norm": 2.5283634662628174,
      "learning_rate": 2.2039585097375108e-05,
      "loss": 0.8467,
      "step": 12640
    },
    {
      "epoch": 0.801673056814221,
      "grad_norm": 2.9784443378448486,
      "learning_rate": 2.203323454699407e-05,
      "loss": 0.8443,
      "step": 12650
    },
    {
      "epoch": 0.8023067904559713,
      "grad_norm": 2.253800630569458,
      "learning_rate": 2.202688399661304e-05,
      "loss": 0.8863,
      "step": 12660
    },
    {
      "epoch": 0.8029405240977218,
      "grad_norm": 3.1222524642944336,
      "learning_rate": 2.2020533446232007e-05,
      "loss": 0.8716,
      "step": 12670
    },
    {
      "epoch": 0.8035742577394721,
      "grad_norm": 2.3476035594940186,
      "learning_rate": 2.2014182895850974e-05,
      "loss": 0.8957,
      "step": 12680
    },
    {
      "epoch": 0.8042079913812225,
      "grad_norm": 2.4197700023651123,
      "learning_rate": 2.2007832345469944e-05,
      "loss": 0.8655,
      "step": 12690
    },
    {
      "epoch": 0.8048417250229728,
      "grad_norm": 2.2097203731536865,
      "learning_rate": 2.2001481795088907e-05,
      "loss": 0.8553,
      "step": 12700
    },
    {
      "epoch": 0.8054754586647233,
      "grad_norm": 2.6053497791290283,
      "learning_rate": 2.1995131244707873e-05,
      "loss": 0.9113,
      "step": 12710
    },
    {
      "epoch": 0.8061091923064736,
      "grad_norm": 2.890845537185669,
      "learning_rate": 2.1988780694326843e-05,
      "loss": 0.8416,
      "step": 12720
    },
    {
      "epoch": 0.8067429259482239,
      "grad_norm": 2.497286558151245,
      "learning_rate": 2.198243014394581e-05,
      "loss": 0.8551,
      "step": 12730
    },
    {
      "epoch": 0.8073766595899743,
      "grad_norm": 2.685933828353882,
      "learning_rate": 2.1976079593564776e-05,
      "loss": 0.8782,
      "step": 12740
    },
    {
      "epoch": 0.8080103932317247,
      "grad_norm": 3.1706693172454834,
      "learning_rate": 2.1969729043183742e-05,
      "loss": 0.891,
      "step": 12750
    },
    {
      "epoch": 0.8086441268734751,
      "grad_norm": 2.97446608543396,
      "learning_rate": 2.196337849280271e-05,
      "loss": 0.8795,
      "step": 12760
    },
    {
      "epoch": 0.8092778605152254,
      "grad_norm": 2.386746644973755,
      "learning_rate": 2.195702794242168e-05,
      "loss": 0.8874,
      "step": 12770
    },
    {
      "epoch": 0.8099115941569758,
      "grad_norm": 3.0892152786254883,
      "learning_rate": 2.1950677392040645e-05,
      "loss": 0.9164,
      "step": 12780
    },
    {
      "epoch": 0.8105453277987262,
      "grad_norm": 2.9609169960021973,
      "learning_rate": 2.194432684165961e-05,
      "loss": 0.8844,
      "step": 12790
    },
    {
      "epoch": 0.8111790614404766,
      "grad_norm": 2.416473388671875,
      "learning_rate": 2.1937976291278578e-05,
      "loss": 0.8671,
      "step": 12800
    },
    {
      "epoch": 0.8118127950822269,
      "grad_norm": 3.1337649822235107,
      "learning_rate": 2.1931625740897544e-05,
      "loss": 0.9004,
      "step": 12810
    },
    {
      "epoch": 0.8124465287239773,
      "grad_norm": 2.6872901916503906,
      "learning_rate": 2.192527519051651e-05,
      "loss": 0.9021,
      "step": 12820
    },
    {
      "epoch": 0.8130802623657277,
      "grad_norm": 2.553256034851074,
      "learning_rate": 2.191892464013548e-05,
      "loss": 0.9136,
      "step": 12830
    },
    {
      "epoch": 0.8137139960074781,
      "grad_norm": 2.4788806438446045,
      "learning_rate": 2.1912574089754447e-05,
      "loss": 0.8966,
      "step": 12840
    },
    {
      "epoch": 0.8143477296492284,
      "grad_norm": 3.9600212574005127,
      "learning_rate": 2.1906223539373413e-05,
      "loss": 0.844,
      "step": 12850
    },
    {
      "epoch": 0.8149814632909788,
      "grad_norm": 2.7628936767578125,
      "learning_rate": 2.189987298899238e-05,
      "loss": 0.8687,
      "step": 12860
    },
    {
      "epoch": 0.8156151969327292,
      "grad_norm": 2.924243688583374,
      "learning_rate": 2.1893522438611346e-05,
      "loss": 0.8704,
      "step": 12870
    },
    {
      "epoch": 0.8162489305744796,
      "grad_norm": 2.8180248737335205,
      "learning_rate": 2.1887171888230312e-05,
      "loss": 0.8917,
      "step": 12880
    },
    {
      "epoch": 0.8168826642162299,
      "grad_norm": 2.5122156143188477,
      "learning_rate": 2.1880821337849282e-05,
      "loss": 0.8633,
      "step": 12890
    },
    {
      "epoch": 0.8175163978579802,
      "grad_norm": 2.7609663009643555,
      "learning_rate": 2.187447078746825e-05,
      "loss": 0.8686,
      "step": 12900
    },
    {
      "epoch": 0.8181501314997307,
      "grad_norm": 2.5462048053741455,
      "learning_rate": 2.1868120237087215e-05,
      "loss": 0.8854,
      "step": 12910
    },
    {
      "epoch": 0.818783865141481,
      "grad_norm": 2.608659505844116,
      "learning_rate": 2.186176968670618e-05,
      "loss": 0.8561,
      "step": 12920
    },
    {
      "epoch": 0.8194175987832314,
      "grad_norm": 2.612313985824585,
      "learning_rate": 2.1855419136325148e-05,
      "loss": 0.8667,
      "step": 12930
    },
    {
      "epoch": 0.8200513324249817,
      "grad_norm": 2.4242007732391357,
      "learning_rate": 2.1849068585944118e-05,
      "loss": 0.8598,
      "step": 12940
    },
    {
      "epoch": 0.8206850660667322,
      "grad_norm": 2.3006083965301514,
      "learning_rate": 2.1842718035563084e-05,
      "loss": 0.882,
      "step": 12950
    },
    {
      "epoch": 0.8213187997084825,
      "grad_norm": 2.4361860752105713,
      "learning_rate": 2.1836367485182047e-05,
      "loss": 0.8723,
      "step": 12960
    },
    {
      "epoch": 0.8219525333502329,
      "grad_norm": 2.368040084838867,
      "learning_rate": 2.1830016934801017e-05,
      "loss": 0.912,
      "step": 12970
    },
    {
      "epoch": 0.8225862669919832,
      "grad_norm": 3.1332263946533203,
      "learning_rate": 2.1823666384419983e-05,
      "loss": 0.917,
      "step": 12980
    },
    {
      "epoch": 0.8232200006337337,
      "grad_norm": 3.071446180343628,
      "learning_rate": 2.181731583403895e-05,
      "loss": 0.8799,
      "step": 12990
    },
    {
      "epoch": 0.823853734275484,
      "grad_norm": 3.242931842803955,
      "learning_rate": 2.181096528365792e-05,
      "loss": 0.9029,
      "step": 13000
    },
    {
      "epoch": 0.8244874679172344,
      "grad_norm": 1.9718576669692993,
      "learning_rate": 2.1804614733276883e-05,
      "loss": 0.8607,
      "step": 13010
    },
    {
      "epoch": 0.8251212015589847,
      "grad_norm": 2.3983287811279297,
      "learning_rate": 2.179826418289585e-05,
      "loss": 0.8141,
      "step": 13020
    },
    {
      "epoch": 0.8257549352007352,
      "grad_norm": 3.0049631595611572,
      "learning_rate": 2.179191363251482e-05,
      "loss": 0.8533,
      "step": 13030
    },
    {
      "epoch": 0.8263886688424855,
      "grad_norm": 2.5620384216308594,
      "learning_rate": 2.1785563082133785e-05,
      "loss": 0.8643,
      "step": 13040
    },
    {
      "epoch": 0.8270224024842359,
      "grad_norm": 2.448477029800415,
      "learning_rate": 2.1779212531752752e-05,
      "loss": 0.8804,
      "step": 13050
    },
    {
      "epoch": 0.8276561361259862,
      "grad_norm": 2.937744379043579,
      "learning_rate": 2.1772861981371718e-05,
      "loss": 0.8867,
      "step": 13060
    },
    {
      "epoch": 0.8282898697677367,
      "grad_norm": 3.1764822006225586,
      "learning_rate": 2.1766511430990685e-05,
      "loss": 0.901,
      "step": 13070
    },
    {
      "epoch": 0.828923603409487,
      "grad_norm": 2.7163829803466797,
      "learning_rate": 2.1760160880609655e-05,
      "loss": 0.9145,
      "step": 13080
    },
    {
      "epoch": 0.8295573370512374,
      "grad_norm": 2.492793560028076,
      "learning_rate": 2.175381033022862e-05,
      "loss": 0.8583,
      "step": 13090
    },
    {
      "epoch": 0.8301910706929877,
      "grad_norm": 2.485731840133667,
      "learning_rate": 2.1747459779847587e-05,
      "loss": 0.8972,
      "step": 13100
    },
    {
      "epoch": 0.8308248043347382,
      "grad_norm": 2.30157208442688,
      "learning_rate": 2.1741109229466557e-05,
      "loss": 0.8829,
      "step": 13110
    },
    {
      "epoch": 0.8314585379764885,
      "grad_norm": 3.5487449169158936,
      "learning_rate": 2.173475867908552e-05,
      "loss": 0.886,
      "step": 13120
    },
    {
      "epoch": 0.8320922716182388,
      "grad_norm": 2.290863513946533,
      "learning_rate": 2.1728408128704487e-05,
      "loss": 0.8728,
      "step": 13130
    },
    {
      "epoch": 0.8327260052599892,
      "grad_norm": 2.6882479190826416,
      "learning_rate": 2.1722057578323456e-05,
      "loss": 0.8536,
      "step": 13140
    },
    {
      "epoch": 0.8333597389017396,
      "grad_norm": 2.541968822479248,
      "learning_rate": 2.1715707027942423e-05,
      "loss": 0.882,
      "step": 13150
    },
    {
      "epoch": 0.83399347254349,
      "grad_norm": 2.4528403282165527,
      "learning_rate": 2.170935647756139e-05,
      "loss": 0.921,
      "step": 13160
    },
    {
      "epoch": 0.8346272061852403,
      "grad_norm": 2.6062867641448975,
      "learning_rate": 2.1703005927180356e-05,
      "loss": 0.8762,
      "step": 13170
    },
    {
      "epoch": 0.8352609398269907,
      "grad_norm": 2.775116443634033,
      "learning_rate": 2.1696655376799322e-05,
      "loss": 0.857,
      "step": 13180
    },
    {
      "epoch": 0.8358946734687411,
      "grad_norm": 4.059709548950195,
      "learning_rate": 2.169030482641829e-05,
      "loss": 0.878,
      "step": 13190
    },
    {
      "epoch": 0.8365284071104915,
      "grad_norm": 2.4108364582061768,
      "learning_rate": 2.168395427603726e-05,
      "loss": 0.8447,
      "step": 13200
    },
    {
      "epoch": 0.8371621407522418,
      "grad_norm": 2.4512157440185547,
      "learning_rate": 2.1677603725656225e-05,
      "loss": 0.8876,
      "step": 13210
    },
    {
      "epoch": 0.8377958743939922,
      "grad_norm": 2.454357147216797,
      "learning_rate": 2.167125317527519e-05,
      "loss": 0.8833,
      "step": 13220
    },
    {
      "epoch": 0.8384296080357426,
      "grad_norm": 2.307054042816162,
      "learning_rate": 2.1664902624894158e-05,
      "loss": 0.8363,
      "step": 13230
    },
    {
      "epoch": 0.839063341677493,
      "grad_norm": 2.806272268295288,
      "learning_rate": 2.1658552074513124e-05,
      "loss": 0.9336,
      "step": 13240
    },
    {
      "epoch": 0.8396970753192433,
      "grad_norm": 2.6884236335754395,
      "learning_rate": 2.1652201524132094e-05,
      "loss": 0.8549,
      "step": 13250
    },
    {
      "epoch": 0.8403308089609937,
      "grad_norm": 2.7010796070098877,
      "learning_rate": 2.164585097375106e-05,
      "loss": 0.8456,
      "step": 13260
    },
    {
      "epoch": 0.8409645426027441,
      "grad_norm": 2.7394349575042725,
      "learning_rate": 2.1639500423370023e-05,
      "loss": 0.8484,
      "step": 13270
    },
    {
      "epoch": 0.8415982762444945,
      "grad_norm": 2.4845588207244873,
      "learning_rate": 2.1633149872988993e-05,
      "loss": 0.9176,
      "step": 13280
    },
    {
      "epoch": 0.8422320098862448,
      "grad_norm": 2.4342076778411865,
      "learning_rate": 2.162679932260796e-05,
      "loss": 0.8639,
      "step": 13290
    },
    {
      "epoch": 0.8428657435279951,
      "grad_norm": 2.4570305347442627,
      "learning_rate": 2.1620448772226926e-05,
      "loss": 0.8664,
      "step": 13300
    },
    {
      "epoch": 0.8434994771697456,
      "grad_norm": 2.6971347332000732,
      "learning_rate": 2.1614098221845896e-05,
      "loss": 0.8574,
      "step": 13310
    },
    {
      "epoch": 0.844133210811496,
      "grad_norm": 2.6852242946624756,
      "learning_rate": 2.160774767146486e-05,
      "loss": 0.8878,
      "step": 13320
    },
    {
      "epoch": 0.8447669444532463,
      "grad_norm": 3.048811912536621,
      "learning_rate": 2.1601397121083825e-05,
      "loss": 0.9086,
      "step": 13330
    },
    {
      "epoch": 0.8454006780949966,
      "grad_norm": 3.3206746578216553,
      "learning_rate": 2.1595046570702795e-05,
      "loss": 0.8878,
      "step": 13340
    },
    {
      "epoch": 0.8460344117367471,
      "grad_norm": 2.288797378540039,
      "learning_rate": 2.158869602032176e-05,
      "loss": 0.8636,
      "step": 13350
    },
    {
      "epoch": 0.8466681453784974,
      "grad_norm": 2.6346867084503174,
      "learning_rate": 2.1582345469940728e-05,
      "loss": 0.9134,
      "step": 13360
    },
    {
      "epoch": 0.8473018790202478,
      "grad_norm": 2.3158915042877197,
      "learning_rate": 2.1575994919559698e-05,
      "loss": 0.8838,
      "step": 13370
    },
    {
      "epoch": 0.8479356126619981,
      "grad_norm": 2.255794048309326,
      "learning_rate": 2.156964436917866e-05,
      "loss": 0.8847,
      "step": 13380
    },
    {
      "epoch": 0.8485693463037486,
      "grad_norm": 2.199345588684082,
      "learning_rate": 2.156329381879763e-05,
      "loss": 0.8521,
      "step": 13390
    },
    {
      "epoch": 0.8492030799454989,
      "grad_norm": 2.3251471519470215,
      "learning_rate": 2.1556943268416597e-05,
      "loss": 0.8769,
      "step": 13400
    },
    {
      "epoch": 0.8498368135872493,
      "grad_norm": 2.3044190406799316,
      "learning_rate": 2.1550592718035564e-05,
      "loss": 0.8696,
      "step": 13410
    },
    {
      "epoch": 0.8504705472289996,
      "grad_norm": 2.651409387588501,
      "learning_rate": 2.1544242167654533e-05,
      "loss": 0.8902,
      "step": 13420
    },
    {
      "epoch": 0.8511042808707501,
      "grad_norm": 2.353825807571411,
      "learning_rate": 2.1537891617273496e-05,
      "loss": 0.8746,
      "step": 13430
    },
    {
      "epoch": 0.8517380145125004,
      "grad_norm": 2.430565118789673,
      "learning_rate": 2.1531541066892463e-05,
      "loss": 0.8583,
      "step": 13440
    },
    {
      "epoch": 0.8523717481542508,
      "grad_norm": 2.587409496307373,
      "learning_rate": 2.1525190516511433e-05,
      "loss": 0.8857,
      "step": 13450
    },
    {
      "epoch": 0.8530054817960011,
      "grad_norm": 2.4291839599609375,
      "learning_rate": 2.15188399661304e-05,
      "loss": 0.8292,
      "step": 13460
    },
    {
      "epoch": 0.8536392154377516,
      "grad_norm": 2.35835862159729,
      "learning_rate": 2.1512489415749365e-05,
      "loss": 0.9033,
      "step": 13470
    },
    {
      "epoch": 0.8542729490795019,
      "grad_norm": 2.6959002017974854,
      "learning_rate": 2.1506138865368332e-05,
      "loss": 0.8877,
      "step": 13480
    },
    {
      "epoch": 0.8549066827212523,
      "grad_norm": 2.436370849609375,
      "learning_rate": 2.14997883149873e-05,
      "loss": 0.8771,
      "step": 13490
    },
    {
      "epoch": 0.8555404163630026,
      "grad_norm": 2.4627225399017334,
      "learning_rate": 2.1493437764606265e-05,
      "loss": 0.8581,
      "step": 13500
    },
    {
      "epoch": 0.856174150004753,
      "grad_norm": 2.5257678031921387,
      "learning_rate": 2.1487087214225235e-05,
      "loss": 0.8679,
      "step": 13510
    },
    {
      "epoch": 0.8568078836465034,
      "grad_norm": 3.23891282081604,
      "learning_rate": 2.14807366638442e-05,
      "loss": 0.8945,
      "step": 13520
    },
    {
      "epoch": 0.8574416172882537,
      "grad_norm": 2.879664421081543,
      "learning_rate": 2.1474386113463167e-05,
      "loss": 0.8786,
      "step": 13530
    },
    {
      "epoch": 0.8580753509300041,
      "grad_norm": 2.537043333053589,
      "learning_rate": 2.1468035563082134e-05,
      "loss": 0.9207,
      "step": 13540
    },
    {
      "epoch": 0.8587090845717545,
      "grad_norm": 2.890453815460205,
      "learning_rate": 2.14616850127011e-05,
      "loss": 0.8696,
      "step": 13550
    },
    {
      "epoch": 0.8593428182135049,
      "grad_norm": 2.7178702354431152,
      "learning_rate": 2.145533446232007e-05,
      "loss": 0.9276,
      "step": 13560
    },
    {
      "epoch": 0.8599765518552552,
      "grad_norm": 2.5844357013702393,
      "learning_rate": 2.1448983911939037e-05,
      "loss": 0.8921,
      "step": 13570
    },
    {
      "epoch": 0.8606102854970056,
      "grad_norm": 2.468672752380371,
      "learning_rate": 2.1442633361558e-05,
      "loss": 0.9145,
      "step": 13580
    },
    {
      "epoch": 0.861244019138756,
      "grad_norm": 2.384101390838623,
      "learning_rate": 2.143628281117697e-05,
      "loss": 0.8566,
      "step": 13590
    },
    {
      "epoch": 0.8618777527805064,
      "grad_norm": 2.4920260906219482,
      "learning_rate": 2.1429932260795936e-05,
      "loss": 0.8811,
      "step": 13600
    },
    {
      "epoch": 0.8625114864222567,
      "grad_norm": 2.4117307662963867,
      "learning_rate": 2.1423581710414902e-05,
      "loss": 0.8469,
      "step": 13610
    },
    {
      "epoch": 0.8631452200640071,
      "grad_norm": 2.6276893615722656,
      "learning_rate": 2.1417231160033872e-05,
      "loss": 0.8681,
      "step": 13620
    },
    {
      "epoch": 0.8637789537057575,
      "grad_norm": 2.5244317054748535,
      "learning_rate": 2.141088060965284e-05,
      "loss": 0.8722,
      "step": 13630
    },
    {
      "epoch": 0.8644126873475079,
      "grad_norm": 2.847834348678589,
      "learning_rate": 2.14045300592718e-05,
      "loss": 0.8328,
      "step": 13640
    },
    {
      "epoch": 0.8650464209892582,
      "grad_norm": 2.293896198272705,
      "learning_rate": 2.139817950889077e-05,
      "loss": 0.8834,
      "step": 13650
    },
    {
      "epoch": 0.8656801546310086,
      "grad_norm": 2.2472548484802246,
      "learning_rate": 2.1391828958509738e-05,
      "loss": 0.8176,
      "step": 13660
    },
    {
      "epoch": 0.866313888272759,
      "grad_norm": 2.16127347946167,
      "learning_rate": 2.1385478408128704e-05,
      "loss": 0.8929,
      "step": 13670
    },
    {
      "epoch": 0.8669476219145094,
      "grad_norm": 2.3802995681762695,
      "learning_rate": 2.1379127857747674e-05,
      "loss": 0.89,
      "step": 13680
    },
    {
      "epoch": 0.8675813555562597,
      "grad_norm": 2.5778732299804688,
      "learning_rate": 2.1372777307366637e-05,
      "loss": 0.8838,
      "step": 13690
    },
    {
      "epoch": 0.86821508919801,
      "grad_norm": 2.0263118743896484,
      "learning_rate": 2.1366426756985607e-05,
      "loss": 0.8956,
      "step": 13700
    },
    {
      "epoch": 0.8688488228397605,
      "grad_norm": 2.7622334957122803,
      "learning_rate": 2.1360076206604573e-05,
      "loss": 0.8865,
      "step": 13710
    },
    {
      "epoch": 0.8694825564815108,
      "grad_norm": 2.3950839042663574,
      "learning_rate": 2.135372565622354e-05,
      "loss": 0.8786,
      "step": 13720
    },
    {
      "epoch": 0.8701162901232612,
      "grad_norm": 2.3674139976501465,
      "learning_rate": 2.134737510584251e-05,
      "loss": 0.874,
      "step": 13730
    },
    {
      "epoch": 0.8707500237650115,
      "grad_norm": 2.693100929260254,
      "learning_rate": 2.1341024555461473e-05,
      "loss": 0.9155,
      "step": 13740
    },
    {
      "epoch": 0.8713837574067619,
      "grad_norm": 2.3737471103668213,
      "learning_rate": 2.133467400508044e-05,
      "loss": 0.8689,
      "step": 13750
    },
    {
      "epoch": 0.8720174910485123,
      "grad_norm": 2.7096705436706543,
      "learning_rate": 2.132832345469941e-05,
      "loss": 0.8704,
      "step": 13760
    },
    {
      "epoch": 0.8726512246902627,
      "grad_norm": 2.3538684844970703,
      "learning_rate": 2.1321972904318375e-05,
      "loss": 0.8918,
      "step": 13770
    },
    {
      "epoch": 0.873284958332013,
      "grad_norm": 2.4297263622283936,
      "learning_rate": 2.131562235393734e-05,
      "loss": 0.8996,
      "step": 13780
    },
    {
      "epoch": 0.8739186919737634,
      "grad_norm": 3.2211902141571045,
      "learning_rate": 2.1309271803556308e-05,
      "loss": 0.8185,
      "step": 13790
    },
    {
      "epoch": 0.8745524256155138,
      "grad_norm": 2.1893880367279053,
      "learning_rate": 2.1302921253175275e-05,
      "loss": 0.8814,
      "step": 13800
    },
    {
      "epoch": 0.8751861592572642,
      "grad_norm": 2.372863292694092,
      "learning_rate": 2.129657070279424e-05,
      "loss": 0.8781,
      "step": 13810
    },
    {
      "epoch": 0.8758198928990145,
      "grad_norm": 2.2863547801971436,
      "learning_rate": 2.129022015241321e-05,
      "loss": 0.8727,
      "step": 13820
    },
    {
      "epoch": 0.8764536265407649,
      "grad_norm": 2.5442423820495605,
      "learning_rate": 2.1283869602032177e-05,
      "loss": 0.8963,
      "step": 13830
    },
    {
      "epoch": 0.8770873601825153,
      "grad_norm": 2.6264052391052246,
      "learning_rate": 2.1277519051651144e-05,
      "loss": 0.8505,
      "step": 13840
    },
    {
      "epoch": 0.8777210938242657,
      "grad_norm": 2.5679686069488525,
      "learning_rate": 2.127116850127011e-05,
      "loss": 0.8566,
      "step": 13850
    },
    {
      "epoch": 0.878354827466016,
      "grad_norm": 3.1977009773254395,
      "learning_rate": 2.1264817950889076e-05,
      "loss": 0.8616,
      "step": 13860
    },
    {
      "epoch": 0.8789885611077664,
      "grad_norm": 2.4535973072052,
      "learning_rate": 2.1258467400508046e-05,
      "loss": 0.8754,
      "step": 13870
    },
    {
      "epoch": 0.8796222947495168,
      "grad_norm": 2.4727208614349365,
      "learning_rate": 2.1252116850127013e-05,
      "loss": 0.8685,
      "step": 13880
    },
    {
      "epoch": 0.8802560283912672,
      "grad_norm": 2.5037426948547363,
      "learning_rate": 2.124576629974598e-05,
      "loss": 0.8994,
      "step": 13890
    },
    {
      "epoch": 0.8808897620330175,
      "grad_norm": 2.4167654514312744,
      "learning_rate": 2.1239415749364946e-05,
      "loss": 0.8502,
      "step": 13900
    },
    {
      "epoch": 0.8815234956747678,
      "grad_norm": 2.752596378326416,
      "learning_rate": 2.1233065198983912e-05,
      "loss": 0.8631,
      "step": 13910
    },
    {
      "epoch": 0.8821572293165183,
      "grad_norm": 2.8935704231262207,
      "learning_rate": 2.122671464860288e-05,
      "loss": 0.857,
      "step": 13920
    },
    {
      "epoch": 0.8827909629582686,
      "grad_norm": 2.723877191543579,
      "learning_rate": 2.1220364098221848e-05,
      "loss": 0.8914,
      "step": 13930
    },
    {
      "epoch": 0.883424696600019,
      "grad_norm": 2.8387231826782227,
      "learning_rate": 2.1214013547840815e-05,
      "loss": 0.879,
      "step": 13940
    },
    {
      "epoch": 0.8840584302417693,
      "grad_norm": 2.60616135597229,
      "learning_rate": 2.1207662997459778e-05,
      "loss": 0.8769,
      "step": 13950
    },
    {
      "epoch": 0.8846921638835198,
      "grad_norm": 2.5252621173858643,
      "learning_rate": 2.1201312447078748e-05,
      "loss": 0.854,
      "step": 13960
    },
    {
      "epoch": 0.8853258975252701,
      "grad_norm": 2.7542030811309814,
      "learning_rate": 2.1194961896697714e-05,
      "loss": 0.8347,
      "step": 13970
    },
    {
      "epoch": 0.8859596311670205,
      "grad_norm": 2.3920412063598633,
      "learning_rate": 2.118861134631668e-05,
      "loss": 0.8473,
      "step": 13980
    },
    {
      "epoch": 0.8865933648087708,
      "grad_norm": 2.907040596008301,
      "learning_rate": 2.118226079593565e-05,
      "loss": 0.8494,
      "step": 13990
    },
    {
      "epoch": 0.8872270984505213,
      "grad_norm": 2.2726895809173584,
      "learning_rate": 2.1175910245554613e-05,
      "loss": 0.8569,
      "step": 14000
    },
    {
      "epoch": 0.8878608320922716,
      "grad_norm": 3.1485068798065186,
      "learning_rate": 2.1169559695173583e-05,
      "loss": 0.9187,
      "step": 14010
    },
    {
      "epoch": 0.888494565734022,
      "grad_norm": 2.7925190925598145,
      "learning_rate": 2.116320914479255e-05,
      "loss": 0.8959,
      "step": 14020
    },
    {
      "epoch": 0.8891282993757723,
      "grad_norm": 2.41477632522583,
      "learning_rate": 2.1156858594411516e-05,
      "loss": 0.8585,
      "step": 14030
    },
    {
      "epoch": 0.8897620330175228,
      "grad_norm": 2.1461901664733887,
      "learning_rate": 2.1150508044030486e-05,
      "loss": 0.8871,
      "step": 14040
    },
    {
      "epoch": 0.8903957666592731,
      "grad_norm": 2.502722978591919,
      "learning_rate": 2.114415749364945e-05,
      "loss": 0.8963,
      "step": 14050
    },
    {
      "epoch": 0.8910295003010235,
      "grad_norm": 2.279609203338623,
      "learning_rate": 2.1137806943268415e-05,
      "loss": 0.8582,
      "step": 14060
    },
    {
      "epoch": 0.8916632339427738,
      "grad_norm": 2.2925477027893066,
      "learning_rate": 2.1131456392887385e-05,
      "loss": 0.8627,
      "step": 14070
    },
    {
      "epoch": 0.8922969675845243,
      "grad_norm": 2.7372395992279053,
      "learning_rate": 2.112510584250635e-05,
      "loss": 0.9259,
      "step": 14080
    },
    {
      "epoch": 0.8929307012262746,
      "grad_norm": 2.339829444885254,
      "learning_rate": 2.1118755292125318e-05,
      "loss": 0.8165,
      "step": 14090
    },
    {
      "epoch": 0.893564434868025,
      "grad_norm": 2.1894354820251465,
      "learning_rate": 2.1112404741744288e-05,
      "loss": 0.8686,
      "step": 14100
    },
    {
      "epoch": 0.8941981685097753,
      "grad_norm": 2.57075834274292,
      "learning_rate": 2.110605419136325e-05,
      "loss": 0.858,
      "step": 14110
    },
    {
      "epoch": 0.8948319021515257,
      "grad_norm": 2.095625877380371,
      "learning_rate": 2.1099703640982217e-05,
      "loss": 0.8629,
      "step": 14120
    },
    {
      "epoch": 0.8954656357932761,
      "grad_norm": 2.7350053787231445,
      "learning_rate": 2.1093353090601187e-05,
      "loss": 0.8528,
      "step": 14130
    },
    {
      "epoch": 0.8960993694350264,
      "grad_norm": 2.559835433959961,
      "learning_rate": 2.1087002540220153e-05,
      "loss": 0.9164,
      "step": 14140
    },
    {
      "epoch": 0.8967331030767768,
      "grad_norm": 2.710005521774292,
      "learning_rate": 2.108065198983912e-05,
      "loss": 0.8735,
      "step": 14150
    },
    {
      "epoch": 0.8973668367185272,
      "grad_norm": 3.192967414855957,
      "learning_rate": 2.1074301439458086e-05,
      "loss": 0.8628,
      "step": 14160
    },
    {
      "epoch": 0.8980005703602776,
      "grad_norm": 2.74300217628479,
      "learning_rate": 2.1067950889077053e-05,
      "loss": 0.8861,
      "step": 14170
    },
    {
      "epoch": 0.8986343040020279,
      "grad_norm": 2.2420387268066406,
      "learning_rate": 2.1061600338696022e-05,
      "loss": 0.8819,
      "step": 14180
    },
    {
      "epoch": 0.8992680376437783,
      "grad_norm": 2.614213228225708,
      "learning_rate": 2.105524978831499e-05,
      "loss": 0.8534,
      "step": 14190
    },
    {
      "epoch": 0.8999017712855287,
      "grad_norm": 2.972785472869873,
      "learning_rate": 2.1048899237933955e-05,
      "loss": 0.8794,
      "step": 14200
    },
    {
      "epoch": 0.9005355049272791,
      "grad_norm": 2.4139487743377686,
      "learning_rate": 2.1042548687552922e-05,
      "loss": 0.893,
      "step": 14210
    },
    {
      "epoch": 0.9011692385690294,
      "grad_norm": 2.2429096698760986,
      "learning_rate": 2.1036198137171888e-05,
      "loss": 0.8378,
      "step": 14220
    },
    {
      "epoch": 0.9018029722107798,
      "grad_norm": 2.8096301555633545,
      "learning_rate": 2.1029847586790855e-05,
      "loss": 0.8198,
      "step": 14230
    },
    {
      "epoch": 0.9024367058525302,
      "grad_norm": 2.6099541187286377,
      "learning_rate": 2.1023497036409824e-05,
      "loss": 0.8734,
      "step": 14240
    },
    {
      "epoch": 0.9030704394942806,
      "grad_norm": 2.5153512954711914,
      "learning_rate": 2.101714648602879e-05,
      "loss": 0.8778,
      "step": 14250
    },
    {
      "epoch": 0.9037041731360309,
      "grad_norm": 2.8747057914733887,
      "learning_rate": 2.1010795935647754e-05,
      "loss": 0.8962,
      "step": 14260
    },
    {
      "epoch": 0.9043379067777813,
      "grad_norm": 2.5032167434692383,
      "learning_rate": 2.1004445385266724e-05,
      "loss": 0.8774,
      "step": 14270
    },
    {
      "epoch": 0.9049716404195317,
      "grad_norm": 2.14949107170105,
      "learning_rate": 2.099809483488569e-05,
      "loss": 0.8807,
      "step": 14280
    },
    {
      "epoch": 0.905605374061282,
      "grad_norm": 2.3438291549682617,
      "learning_rate": 2.0991744284504657e-05,
      "loss": 0.8624,
      "step": 14290
    },
    {
      "epoch": 0.9062391077030324,
      "grad_norm": 2.3456084728240967,
      "learning_rate": 2.0985393734123626e-05,
      "loss": 0.9103,
      "step": 14300
    },
    {
      "epoch": 0.9068728413447827,
      "grad_norm": 2.4847991466522217,
      "learning_rate": 2.097904318374259e-05,
      "loss": 0.871,
      "step": 14310
    },
    {
      "epoch": 0.9075065749865332,
      "grad_norm": 2.5888214111328125,
      "learning_rate": 2.097269263336156e-05,
      "loss": 0.8639,
      "step": 14320
    },
    {
      "epoch": 0.9081403086282835,
      "grad_norm": 2.3263790607452393,
      "learning_rate": 2.0966342082980526e-05,
      "loss": 0.8804,
      "step": 14330
    },
    {
      "epoch": 0.9087740422700339,
      "grad_norm": 2.5214955806732178,
      "learning_rate": 2.0959991532599492e-05,
      "loss": 0.868,
      "step": 14340
    },
    {
      "epoch": 0.9094077759117842,
      "grad_norm": 2.1318461894989014,
      "learning_rate": 2.0953640982218462e-05,
      "loss": 0.8924,
      "step": 14350
    },
    {
      "epoch": 0.9100415095535347,
      "grad_norm": 2.6743383407592773,
      "learning_rate": 2.094729043183743e-05,
      "loss": 0.8454,
      "step": 14360
    },
    {
      "epoch": 0.910675243195285,
      "grad_norm": 2.736497163772583,
      "learning_rate": 2.094093988145639e-05,
      "loss": 0.8716,
      "step": 14370
    },
    {
      "epoch": 0.9113089768370354,
      "grad_norm": 2.9955356121063232,
      "learning_rate": 2.093458933107536e-05,
      "loss": 0.8188,
      "step": 14380
    },
    {
      "epoch": 0.9119427104787857,
      "grad_norm": 2.684967279434204,
      "learning_rate": 2.0928238780694328e-05,
      "loss": 0.8664,
      "step": 14390
    },
    {
      "epoch": 0.9125764441205362,
      "grad_norm": 2.632713556289673,
      "learning_rate": 2.0921888230313294e-05,
      "loss": 0.8538,
      "step": 14400
    },
    {
      "epoch": 0.9132101777622865,
      "grad_norm": 2.612499237060547,
      "learning_rate": 2.0915537679932264e-05,
      "loss": 0.8846,
      "step": 14410
    },
    {
      "epoch": 0.9138439114040369,
      "grad_norm": 2.233461618423462,
      "learning_rate": 2.0909187129551227e-05,
      "loss": 0.8382,
      "step": 14420
    },
    {
      "epoch": 0.9144776450457872,
      "grad_norm": 2.9423882961273193,
      "learning_rate": 2.0902836579170193e-05,
      "loss": 0.9039,
      "step": 14430
    },
    {
      "epoch": 0.9151113786875377,
      "grad_norm": 2.4837646484375,
      "learning_rate": 2.0896486028789163e-05,
      "loss": 0.8616,
      "step": 14440
    },
    {
      "epoch": 0.915745112329288,
      "grad_norm": 2.527708053588867,
      "learning_rate": 2.089013547840813e-05,
      "loss": 0.8249,
      "step": 14450
    },
    {
      "epoch": 0.9163788459710384,
      "grad_norm": 2.3286991119384766,
      "learning_rate": 2.0883784928027096e-05,
      "loss": 0.8725,
      "step": 14460
    },
    {
      "epoch": 0.9170125796127887,
      "grad_norm": 2.36860990524292,
      "learning_rate": 2.0877434377646062e-05,
      "loss": 0.8588,
      "step": 14470
    },
    {
      "epoch": 0.9176463132545392,
      "grad_norm": 2.852073907852173,
      "learning_rate": 2.087108382726503e-05,
      "loss": 0.8224,
      "step": 14480
    },
    {
      "epoch": 0.9182800468962895,
      "grad_norm": 3.2535648345947266,
      "learning_rate": 2.0864733276884e-05,
      "loss": 0.8909,
      "step": 14490
    },
    {
      "epoch": 0.9189137805380398,
      "grad_norm": 3.2409298419952393,
      "learning_rate": 2.0858382726502965e-05,
      "loss": 0.8586,
      "step": 14500
    },
    {
      "epoch": 0.9195475141797902,
      "grad_norm": 2.567211389541626,
      "learning_rate": 2.085203217612193e-05,
      "loss": 0.8701,
      "step": 14510
    },
    {
      "epoch": 0.9201812478215406,
      "grad_norm": 2.8753435611724854,
      "learning_rate": 2.0845681625740898e-05,
      "loss": 0.9078,
      "step": 14520
    },
    {
      "epoch": 0.920814981463291,
      "grad_norm": 2.498081922531128,
      "learning_rate": 2.0839331075359864e-05,
      "loss": 0.8225,
      "step": 14530
    },
    {
      "epoch": 0.9214487151050413,
      "grad_norm": 2.4203972816467285,
      "learning_rate": 2.083298052497883e-05,
      "loss": 0.896,
      "step": 14540
    },
    {
      "epoch": 0.9220824487467917,
      "grad_norm": 2.695007562637329,
      "learning_rate": 2.08266299745978e-05,
      "loss": 0.8604,
      "step": 14550
    },
    {
      "epoch": 0.9227161823885421,
      "grad_norm": 2.766587734222412,
      "learning_rate": 2.0820279424216767e-05,
      "loss": 0.8807,
      "step": 14560
    },
    {
      "epoch": 0.9233499160302925,
      "grad_norm": 2.131244421005249,
      "learning_rate": 2.081392887383573e-05,
      "loss": 0.8612,
      "step": 14570
    },
    {
      "epoch": 0.9239836496720428,
      "grad_norm": 2.814016819000244,
      "learning_rate": 2.08075783234547e-05,
      "loss": 0.8263,
      "step": 14580
    },
    {
      "epoch": 0.9246173833137932,
      "grad_norm": 3.009038209915161,
      "learning_rate": 2.0801227773073666e-05,
      "loss": 0.8721,
      "step": 14590
    },
    {
      "epoch": 0.9252511169555436,
      "grad_norm": 2.691915988922119,
      "learning_rate": 2.0794877222692633e-05,
      "loss": 0.9592,
      "step": 14600
    },
    {
      "epoch": 0.925884850597294,
      "grad_norm": 2.8347458839416504,
      "learning_rate": 2.0788526672311603e-05,
      "loss": 0.8898,
      "step": 14610
    },
    {
      "epoch": 0.9265185842390443,
      "grad_norm": 2.301841974258423,
      "learning_rate": 2.078217612193057e-05,
      "loss": 0.879,
      "step": 14620
    },
    {
      "epoch": 0.9271523178807947,
      "grad_norm": 2.4412460327148438,
      "learning_rate": 2.0775825571549535e-05,
      "loss": 0.841,
      "step": 14630
    },
    {
      "epoch": 0.9277860515225451,
      "grad_norm": 3.3830370903015137,
      "learning_rate": 2.0769475021168502e-05,
      "loss": 0.9042,
      "step": 14640
    },
    {
      "epoch": 0.9284197851642955,
      "grad_norm": 2.3997483253479004,
      "learning_rate": 2.0763124470787468e-05,
      "loss": 0.8512,
      "step": 14650
    },
    {
      "epoch": 0.9290535188060458,
      "grad_norm": 2.539452075958252,
      "learning_rate": 2.0756773920406438e-05,
      "loss": 0.8849,
      "step": 14660
    },
    {
      "epoch": 0.9296872524477962,
      "grad_norm": 2.6081721782684326,
      "learning_rate": 2.0750423370025404e-05,
      "loss": 0.8719,
      "step": 14670
    },
    {
      "epoch": 0.9303209860895466,
      "grad_norm": 2.444533586502075,
      "learning_rate": 2.0744072819644368e-05,
      "loss": 0.8706,
      "step": 14680
    },
    {
      "epoch": 0.930954719731297,
      "grad_norm": 2.5986897945404053,
      "learning_rate": 2.0737722269263337e-05,
      "loss": 0.8597,
      "step": 14690
    },
    {
      "epoch": 0.9315884533730473,
      "grad_norm": 2.6561858654022217,
      "learning_rate": 2.0731371718882304e-05,
      "loss": 0.8629,
      "step": 14700
    },
    {
      "epoch": 0.9322221870147976,
      "grad_norm": 2.3377206325531006,
      "learning_rate": 2.072502116850127e-05,
      "loss": 0.9112,
      "step": 14710
    },
    {
      "epoch": 0.9328559206565481,
      "grad_norm": 2.395174503326416,
      "learning_rate": 2.071867061812024e-05,
      "loss": 0.8599,
      "step": 14720
    },
    {
      "epoch": 0.9334896542982984,
      "grad_norm": 2.5609233379364014,
      "learning_rate": 2.0712320067739203e-05,
      "loss": 0.8582,
      "step": 14730
    },
    {
      "epoch": 0.9341233879400488,
      "grad_norm": 2.049445390701294,
      "learning_rate": 2.070596951735817e-05,
      "loss": 0.8735,
      "step": 14740
    },
    {
      "epoch": 0.9347571215817991,
      "grad_norm": 2.3222410678863525,
      "learning_rate": 2.069961896697714e-05,
      "loss": 0.8348,
      "step": 14750
    },
    {
      "epoch": 0.9353908552235496,
      "grad_norm": 2.484666347503662,
      "learning_rate": 2.0693268416596106e-05,
      "loss": 0.8857,
      "step": 14760
    },
    {
      "epoch": 0.9360245888652999,
      "grad_norm": 2.5750062465667725,
      "learning_rate": 2.0686917866215072e-05,
      "loss": 0.8835,
      "step": 14770
    },
    {
      "epoch": 0.9366583225070503,
      "grad_norm": 2.3875198364257812,
      "learning_rate": 2.068056731583404e-05,
      "loss": 0.8499,
      "step": 14780
    },
    {
      "epoch": 0.9372920561488006,
      "grad_norm": 2.586940050125122,
      "learning_rate": 2.0674216765453005e-05,
      "loss": 0.8461,
      "step": 14790
    },
    {
      "epoch": 0.9379257897905511,
      "grad_norm": 2.339401960372925,
      "learning_rate": 2.0667866215071975e-05,
      "loss": 0.8826,
      "step": 14800
    },
    {
      "epoch": 0.9385595234323014,
      "grad_norm": 2.671529531478882,
      "learning_rate": 2.066151566469094e-05,
      "loss": 0.8734,
      "step": 14810
    },
    {
      "epoch": 0.9391932570740518,
      "grad_norm": 2.286008596420288,
      "learning_rate": 2.0655165114309908e-05,
      "loss": 0.8883,
      "step": 14820
    },
    {
      "epoch": 0.9398269907158021,
      "grad_norm": 2.4221835136413574,
      "learning_rate": 2.0648814563928874e-05,
      "loss": 0.8792,
      "step": 14830
    },
    {
      "epoch": 0.9404607243575526,
      "grad_norm": 2.6714398860931396,
      "learning_rate": 2.064246401354784e-05,
      "loss": 0.8529,
      "step": 14840
    },
    {
      "epoch": 0.9410944579993029,
      "grad_norm": 2.8200600147247314,
      "learning_rate": 2.0636113463166807e-05,
      "loss": 0.8885,
      "step": 14850
    },
    {
      "epoch": 0.9417281916410533,
      "grad_norm": 2.4842560291290283,
      "learning_rate": 2.0629762912785777e-05,
      "loss": 0.9278,
      "step": 14860
    },
    {
      "epoch": 0.9423619252828036,
      "grad_norm": 3.5271599292755127,
      "learning_rate": 2.0623412362404743e-05,
      "loss": 0.8492,
      "step": 14870
    },
    {
      "epoch": 0.9429956589245541,
      "grad_norm": 2.208570957183838,
      "learning_rate": 2.061706181202371e-05,
      "loss": 0.8889,
      "step": 14880
    },
    {
      "epoch": 0.9436293925663044,
      "grad_norm": 2.4343254566192627,
      "learning_rate": 2.0610711261642676e-05,
      "loss": 0.8737,
      "step": 14890
    },
    {
      "epoch": 0.9442631262080547,
      "grad_norm": 2.605602741241455,
      "learning_rate": 2.0604360711261642e-05,
      "loss": 0.8484,
      "step": 14900
    },
    {
      "epoch": 0.9448968598498051,
      "grad_norm": 2.408208131790161,
      "learning_rate": 2.059801016088061e-05,
      "loss": 0.8669,
      "step": 14910
    },
    {
      "epoch": 0.9455305934915555,
      "grad_norm": 2.164874792098999,
      "learning_rate": 2.059165961049958e-05,
      "loss": 0.8732,
      "step": 14920
    },
    {
      "epoch": 0.9461643271333059,
      "grad_norm": 2.2660984992980957,
      "learning_rate": 2.0585309060118545e-05,
      "loss": 0.8615,
      "step": 14930
    },
    {
      "epoch": 0.9467980607750562,
      "grad_norm": 2.8095366954803467,
      "learning_rate": 2.057895850973751e-05,
      "loss": 0.8927,
      "step": 14940
    },
    {
      "epoch": 0.9474317944168066,
      "grad_norm": 2.587510585784912,
      "learning_rate": 2.0572607959356478e-05,
      "loss": 0.8961,
      "step": 14950
    },
    {
      "epoch": 0.948065528058557,
      "grad_norm": 2.3466150760650635,
      "learning_rate": 2.0566257408975444e-05,
      "loss": 0.8472,
      "step": 14960
    },
    {
      "epoch": 0.9486992617003074,
      "grad_norm": 2.4450926780700684,
      "learning_rate": 2.0559906858594414e-05,
      "loss": 0.8801,
      "step": 14970
    },
    {
      "epoch": 0.9493329953420577,
      "grad_norm": 2.7271716594696045,
      "learning_rate": 2.055355630821338e-05,
      "loss": 0.8825,
      "step": 14980
    },
    {
      "epoch": 0.9499667289838081,
      "grad_norm": 2.712165594100952,
      "learning_rate": 2.0547205757832344e-05,
      "loss": 0.8684,
      "step": 14990
    },
    {
      "epoch": 0.9506004626255585,
      "grad_norm": 2.710509777069092,
      "learning_rate": 2.0540855207451314e-05,
      "loss": 0.9051,
      "step": 15000
    },
    {
      "epoch": 0.9512341962673089,
      "grad_norm": 3.632862091064453,
      "learning_rate": 2.053450465707028e-05,
      "loss": 0.8984,
      "step": 15010
    },
    {
      "epoch": 0.9518679299090592,
      "grad_norm": 2.6096932888031006,
      "learning_rate": 2.0528154106689246e-05,
      "loss": 0.8504,
      "step": 15020
    },
    {
      "epoch": 0.9525016635508096,
      "grad_norm": 3.0851762294769287,
      "learning_rate": 2.0521803556308216e-05,
      "loss": 0.894,
      "step": 15030
    },
    {
      "epoch": 0.95313539719256,
      "grad_norm": 2.95367693901062,
      "learning_rate": 2.051545300592718e-05,
      "loss": 0.8818,
      "step": 15040
    },
    {
      "epoch": 0.9537691308343104,
      "grad_norm": 2.68985652923584,
      "learning_rate": 2.0509102455546146e-05,
      "loss": 0.8822,
      "step": 15050
    },
    {
      "epoch": 0.9544028644760607,
      "grad_norm": 2.3700075149536133,
      "learning_rate": 2.0502751905165115e-05,
      "loss": 0.8521,
      "step": 15060
    },
    {
      "epoch": 0.955036598117811,
      "grad_norm": 2.49988055229187,
      "learning_rate": 2.0496401354784082e-05,
      "loss": 0.8599,
      "step": 15070
    },
    {
      "epoch": 0.9556703317595615,
      "grad_norm": 2.3011393547058105,
      "learning_rate": 2.049005080440305e-05,
      "loss": 0.8381,
      "step": 15080
    },
    {
      "epoch": 0.9563040654013119,
      "grad_norm": 2.0119988918304443,
      "learning_rate": 2.048433530906012e-05,
      "loss": 0.8637,
      "step": 15090
    },
    {
      "epoch": 0.9569377990430622,
      "grad_norm": 2.187485933303833,
      "learning_rate": 2.0477984758679085e-05,
      "loss": 0.8653,
      "step": 15100
    },
    {
      "epoch": 0.9575715326848125,
      "grad_norm": 2.99790096282959,
      "learning_rate": 2.047163420829805e-05,
      "loss": 0.8583,
      "step": 15110
    },
    {
      "epoch": 0.958205266326563,
      "grad_norm": 2.6936228275299072,
      "learning_rate": 2.046528365791702e-05,
      "loss": 0.8771,
      "step": 15120
    },
    {
      "epoch": 0.9588389999683133,
      "grad_norm": 2.5065643787384033,
      "learning_rate": 2.0458933107535988e-05,
      "loss": 0.89,
      "step": 15130
    },
    {
      "epoch": 0.9594727336100637,
      "grad_norm": 2.505967855453491,
      "learning_rate": 2.0452582557154954e-05,
      "loss": 0.9155,
      "step": 15140
    },
    {
      "epoch": 0.960106467251814,
      "grad_norm": 2.3820481300354004,
      "learning_rate": 2.044623200677392e-05,
      "loss": 0.8641,
      "step": 15150
    },
    {
      "epoch": 0.9607402008935645,
      "grad_norm": 3.044936180114746,
      "learning_rate": 2.0439881456392887e-05,
      "loss": 0.8434,
      "step": 15160
    },
    {
      "epoch": 0.9613739345353148,
      "grad_norm": 2.4917218685150146,
      "learning_rate": 2.0433530906011854e-05,
      "loss": 0.8387,
      "step": 15170
    },
    {
      "epoch": 0.9620076681770652,
      "grad_norm": 3.0331599712371826,
      "learning_rate": 2.0427180355630823e-05,
      "loss": 0.8217,
      "step": 15180
    },
    {
      "epoch": 0.9626414018188155,
      "grad_norm": 3.026888847351074,
      "learning_rate": 2.042082980524979e-05,
      "loss": 0.835,
      "step": 15190
    },
    {
      "epoch": 0.963275135460566,
      "grad_norm": 2.694187879562378,
      "learning_rate": 2.0414479254868753e-05,
      "loss": 0.8904,
      "step": 15200
    },
    {
      "epoch": 0.9639088691023163,
      "grad_norm": 2.3510208129882812,
      "learning_rate": 2.0408128704487723e-05,
      "loss": 0.8662,
      "step": 15210
    },
    {
      "epoch": 0.9645426027440667,
      "grad_norm": 2.6223385334014893,
      "learning_rate": 2.040177815410669e-05,
      "loss": 0.8506,
      "step": 15220
    },
    {
      "epoch": 0.965176336385817,
      "grad_norm": 2.162346601486206,
      "learning_rate": 2.0395427603725656e-05,
      "loss": 0.882,
      "step": 15230
    },
    {
      "epoch": 0.9658100700275675,
      "grad_norm": 2.4513185024261475,
      "learning_rate": 2.0389077053344625e-05,
      "loss": 0.909,
      "step": 15240
    },
    {
      "epoch": 0.9664438036693178,
      "grad_norm": 3.206282138824463,
      "learning_rate": 2.0382726502963592e-05,
      "loss": 0.8827,
      "step": 15250
    },
    {
      "epoch": 0.9670775373110682,
      "grad_norm": 2.726104736328125,
      "learning_rate": 2.0376375952582558e-05,
      "loss": 0.8786,
      "step": 15260
    },
    {
      "epoch": 0.9677112709528185,
      "grad_norm": 2.658155679702759,
      "learning_rate": 2.0370025402201525e-05,
      "loss": 0.873,
      "step": 15270
    },
    {
      "epoch": 0.9683450045945688,
      "grad_norm": 2.4895553588867188,
      "learning_rate": 2.036367485182049e-05,
      "loss": 0.896,
      "step": 15280
    },
    {
      "epoch": 0.9689787382363193,
      "grad_norm": 2.5433578491210938,
      "learning_rate": 2.035732430143946e-05,
      "loss": 0.8935,
      "step": 15290
    },
    {
      "epoch": 0.9696124718780696,
      "grad_norm": 2.1555144786834717,
      "learning_rate": 2.0350973751058427e-05,
      "loss": 0.8739,
      "step": 15300
    },
    {
      "epoch": 0.97024620551982,
      "grad_norm": 2.538215398788452,
      "learning_rate": 2.034462320067739e-05,
      "loss": 0.8695,
      "step": 15310
    },
    {
      "epoch": 0.9708799391615703,
      "grad_norm": 2.2878780364990234,
      "learning_rate": 2.033827265029636e-05,
      "loss": 0.8747,
      "step": 15320
    },
    {
      "epoch": 0.9715136728033208,
      "grad_norm": 2.302175760269165,
      "learning_rate": 2.0331922099915327e-05,
      "loss": 0.8401,
      "step": 15330
    },
    {
      "epoch": 0.9721474064450711,
      "grad_norm": 2.255842924118042,
      "learning_rate": 2.0325571549534293e-05,
      "loss": 0.8471,
      "step": 15340
    },
    {
      "epoch": 0.9727811400868215,
      "grad_norm": 2.505828380584717,
      "learning_rate": 2.0319220999153263e-05,
      "loss": 0.8851,
      "step": 15350
    },
    {
      "epoch": 0.9734148737285718,
      "grad_norm": 2.6507742404937744,
      "learning_rate": 2.0312870448772226e-05,
      "loss": 0.8901,
      "step": 15360
    },
    {
      "epoch": 0.9740486073703223,
      "grad_norm": 2.6544458866119385,
      "learning_rate": 2.0306519898391192e-05,
      "loss": 0.8009,
      "step": 15370
    },
    {
      "epoch": 0.9746823410120726,
      "grad_norm": 2.445568799972534,
      "learning_rate": 2.0300169348010162e-05,
      "loss": 0.8641,
      "step": 15380
    },
    {
      "epoch": 0.975316074653823,
      "grad_norm": 2.560854434967041,
      "learning_rate": 2.029381879762913e-05,
      "loss": 0.8367,
      "step": 15390
    },
    {
      "epoch": 0.9759498082955733,
      "grad_norm": 2.42287015914917,
      "learning_rate": 2.0287468247248095e-05,
      "loss": 0.8513,
      "step": 15400
    },
    {
      "epoch": 0.9765835419373238,
      "grad_norm": 2.5177502632141113,
      "learning_rate": 2.028111769686706e-05,
      "loss": 0.8778,
      "step": 15410
    },
    {
      "epoch": 0.9772172755790741,
      "grad_norm": 2.231316328048706,
      "learning_rate": 2.0274767146486028e-05,
      "loss": 0.8683,
      "step": 15420
    },
    {
      "epoch": 0.9778510092208245,
      "grad_norm": 2.6936850547790527,
      "learning_rate": 2.0268416596104998e-05,
      "loss": 0.8525,
      "step": 15430
    },
    {
      "epoch": 0.9784847428625748,
      "grad_norm": 2.6311330795288086,
      "learning_rate": 2.0262066045723964e-05,
      "loss": 0.863,
      "step": 15440
    },
    {
      "epoch": 0.9791184765043253,
      "grad_norm": 2.400651454925537,
      "learning_rate": 2.025571549534293e-05,
      "loss": 0.8526,
      "step": 15450
    },
    {
      "epoch": 0.9797522101460756,
      "grad_norm": 2.466874837875366,
      "learning_rate": 2.0249364944961897e-05,
      "loss": 0.8581,
      "step": 15460
    },
    {
      "epoch": 0.980385943787826,
      "grad_norm": 2.8673195838928223,
      "learning_rate": 2.0243014394580863e-05,
      "loss": 0.9221,
      "step": 15470
    },
    {
      "epoch": 0.9810196774295763,
      "grad_norm": 3.124176263809204,
      "learning_rate": 2.023666384419983e-05,
      "loss": 0.8638,
      "step": 15480
    },
    {
      "epoch": 0.9816534110713268,
      "grad_norm": 2.4516215324401855,
      "learning_rate": 2.02303132938188e-05,
      "loss": 0.8559,
      "step": 15490
    },
    {
      "epoch": 0.9822871447130771,
      "grad_norm": 2.392122507095337,
      "learning_rate": 2.0223962743437766e-05,
      "loss": 0.8719,
      "step": 15500
    },
    {
      "epoch": 0.9829208783548274,
      "grad_norm": 2.411200761795044,
      "learning_rate": 2.0217612193056732e-05,
      "loss": 0.8719,
      "step": 15510
    },
    {
      "epoch": 0.9835546119965778,
      "grad_norm": 2.7952919006347656,
      "learning_rate": 2.02112616426757e-05,
      "loss": 0.883,
      "step": 15520
    },
    {
      "epoch": 0.9841883456383282,
      "grad_norm": 2.775820255279541,
      "learning_rate": 2.0204911092294665e-05,
      "loss": 0.8715,
      "step": 15530
    },
    {
      "epoch": 0.9848220792800786,
      "grad_norm": 2.8677566051483154,
      "learning_rate": 2.0198560541913632e-05,
      "loss": 0.8686,
      "step": 15540
    },
    {
      "epoch": 0.9854558129218289,
      "grad_norm": 3.075369358062744,
      "learning_rate": 2.01922099915326e-05,
      "loss": 0.8812,
      "step": 15550
    },
    {
      "epoch": 0.9860895465635793,
      "grad_norm": 2.4495086669921875,
      "learning_rate": 2.0185859441151568e-05,
      "loss": 0.8357,
      "step": 15560
    },
    {
      "epoch": 0.9867232802053297,
      "grad_norm": 2.692326545715332,
      "learning_rate": 2.0179508890770534e-05,
      "loss": 0.8936,
      "step": 15570
    },
    {
      "epoch": 0.9873570138470801,
      "grad_norm": 2.415086030960083,
      "learning_rate": 2.01731583403895e-05,
      "loss": 0.9026,
      "step": 15580
    },
    {
      "epoch": 0.9879907474888304,
      "grad_norm": 2.3063783645629883,
      "learning_rate": 2.0166807790008467e-05,
      "loss": 0.8609,
      "step": 15590
    },
    {
      "epoch": 0.9886244811305808,
      "grad_norm": 2.295275926589966,
      "learning_rate": 2.0160457239627437e-05,
      "loss": 0.8378,
      "step": 15600
    },
    {
      "epoch": 0.9892582147723312,
      "grad_norm": 2.639955520629883,
      "learning_rate": 2.0154106689246403e-05,
      "loss": 0.8883,
      "step": 15610
    },
    {
      "epoch": 0.9898919484140816,
      "grad_norm": 2.4646201133728027,
      "learning_rate": 2.0147756138865367e-05,
      "loss": 0.9025,
      "step": 15620
    },
    {
      "epoch": 0.9905256820558319,
      "grad_norm": 2.663939952850342,
      "learning_rate": 2.0141405588484336e-05,
      "loss": 0.842,
      "step": 15630
    },
    {
      "epoch": 0.9911594156975823,
      "grad_norm": 2.4661216735839844,
      "learning_rate": 2.0135055038103303e-05,
      "loss": 0.8337,
      "step": 15640
    },
    {
      "epoch": 0.9917931493393327,
      "grad_norm": 2.309645652770996,
      "learning_rate": 2.012870448772227e-05,
      "loss": 0.8566,
      "step": 15650
    },
    {
      "epoch": 0.9924268829810831,
      "grad_norm": 2.5306613445281982,
      "learning_rate": 2.012235393734124e-05,
      "loss": 0.8823,
      "step": 15660
    },
    {
      "epoch": 0.9930606166228334,
      "grad_norm": 1.964311957359314,
      "learning_rate": 2.0116003386960202e-05,
      "loss": 0.8876,
      "step": 15670
    },
    {
      "epoch": 0.9936943502645837,
      "grad_norm": 2.641120195388794,
      "learning_rate": 2.010965283657917e-05,
      "loss": 0.8791,
      "step": 15680
    },
    {
      "epoch": 0.9943280839063342,
      "grad_norm": 2.6402933597564697,
      "learning_rate": 2.0103302286198138e-05,
      "loss": 0.8973,
      "step": 15690
    },
    {
      "epoch": 0.9949618175480845,
      "grad_norm": 2.8615903854370117,
      "learning_rate": 2.0096951735817105e-05,
      "loss": 0.8612,
      "step": 15700
    },
    {
      "epoch": 0.9955955511898349,
      "grad_norm": 2.7606301307678223,
      "learning_rate": 2.009060118543607e-05,
      "loss": 0.9033,
      "step": 15710
    },
    {
      "epoch": 0.9962292848315852,
      "grad_norm": 2.2864296436309814,
      "learning_rate": 2.0084250635055038e-05,
      "loss": 0.8381,
      "step": 15720
    },
    {
      "epoch": 0.9968630184733357,
      "grad_norm": 2.898097276687622,
      "learning_rate": 2.0077900084674004e-05,
      "loss": 0.8979,
      "step": 15730
    },
    {
      "epoch": 0.997496752115086,
      "grad_norm": 2.329012393951416,
      "learning_rate": 2.0071549534292974e-05,
      "loss": 0.8583,
      "step": 15740
    },
    {
      "epoch": 0.9981304857568364,
      "grad_norm": 2.687086343765259,
      "learning_rate": 2.006519898391194e-05,
      "loss": 0.8893,
      "step": 15750
    },
    {
      "epoch": 0.9987642193985867,
      "grad_norm": 2.601032018661499,
      "learning_rate": 2.0058848433530907e-05,
      "loss": 0.8557,
      "step": 15760
    },
    {
      "epoch": 0.9993979530403372,
      "grad_norm": 3.1976780891418457,
      "learning_rate": 2.0052497883149876e-05,
      "loss": 0.8985,
      "step": 15770
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.129493236541748,
      "learning_rate": 2.004614733276884e-05,
      "loss": 0.8377,
      "step": 15780
    },
    {
      "epoch": 1.0006337336417503,
      "grad_norm": 2.1803390979766846,
      "learning_rate": 2.0039796782387806e-05,
      "loss": 0.8759,
      "step": 15790
    },
    {
      "epoch": 1.0012674672835007,
      "grad_norm": 2.3581480979919434,
      "learning_rate": 2.0033446232006776e-05,
      "loss": 0.8352,
      "step": 15800
    },
    {
      "epoch": 1.001901200925251,
      "grad_norm": 2.4728405475616455,
      "learning_rate": 2.0027095681625742e-05,
      "loss": 0.8558,
      "step": 15810
    },
    {
      "epoch": 1.0025349345670014,
      "grad_norm": 2.296816349029541,
      "learning_rate": 2.002074513124471e-05,
      "loss": 0.88,
      "step": 15820
    },
    {
      "epoch": 1.003168668208752,
      "grad_norm": 2.98172926902771,
      "learning_rate": 2.0014394580863675e-05,
      "loss": 0.8683,
      "step": 15830
    },
    {
      "epoch": 1.0038024018505023,
      "grad_norm": 2.286242723464966,
      "learning_rate": 2.000804403048264e-05,
      "loss": 0.8854,
      "step": 15840
    },
    {
      "epoch": 1.0044361354922526,
      "grad_norm": 3.4088854789733887,
      "learning_rate": 2.0001693480101608e-05,
      "loss": 0.8884,
      "step": 15850
    },
    {
      "epoch": 1.005069869134003,
      "grad_norm": 2.670619249343872,
      "learning_rate": 1.9995342929720578e-05,
      "loss": 0.8637,
      "step": 15860
    },
    {
      "epoch": 1.0057036027757533,
      "grad_norm": 2.427828788757324,
      "learning_rate": 1.9988992379339544e-05,
      "loss": 0.8607,
      "step": 15870
    },
    {
      "epoch": 1.0063373364175037,
      "grad_norm": 2.7728662490844727,
      "learning_rate": 1.998264182895851e-05,
      "loss": 0.866,
      "step": 15880
    },
    {
      "epoch": 1.006971070059254,
      "grad_norm": 2.250821113586426,
      "learning_rate": 1.9976291278577477e-05,
      "loss": 0.8551,
      "step": 15890
    },
    {
      "epoch": 1.0076048037010044,
      "grad_norm": 2.3570730686187744,
      "learning_rate": 1.9969940728196443e-05,
      "loss": 0.8966,
      "step": 15900
    },
    {
      "epoch": 1.008238537342755,
      "grad_norm": 2.3378310203552246,
      "learning_rate": 1.9963590177815413e-05,
      "loss": 0.8709,
      "step": 15910
    },
    {
      "epoch": 1.0088722709845053,
      "grad_norm": 2.378261089324951,
      "learning_rate": 1.995723962743438e-05,
      "loss": 0.883,
      "step": 15920
    },
    {
      "epoch": 1.0095060046262556,
      "grad_norm": 2.4943177700042725,
      "learning_rate": 1.9950889077053343e-05,
      "loss": 0.8917,
      "step": 15930
    },
    {
      "epoch": 1.010139738268006,
      "grad_norm": 2.5638809204101562,
      "learning_rate": 1.9944538526672312e-05,
      "loss": 0.8397,
      "step": 15940
    },
    {
      "epoch": 1.0107734719097563,
      "grad_norm": 2.6158382892608643,
      "learning_rate": 1.993818797629128e-05,
      "loss": 0.8722,
      "step": 15950
    },
    {
      "epoch": 1.0114072055515066,
      "grad_norm": 2.7740511894226074,
      "learning_rate": 1.9931837425910245e-05,
      "loss": 0.8181,
      "step": 15960
    },
    {
      "epoch": 1.012040939193257,
      "grad_norm": 2.800326108932495,
      "learning_rate": 1.9925486875529215e-05,
      "loss": 0.8871,
      "step": 15970
    },
    {
      "epoch": 1.0126746728350073,
      "grad_norm": 2.4141054153442383,
      "learning_rate": 1.9919136325148178e-05,
      "loss": 0.8714,
      "step": 15980
    },
    {
      "epoch": 1.013308406476758,
      "grad_norm": 3.007450819015503,
      "learning_rate": 1.9912785774767145e-05,
      "loss": 0.9273,
      "step": 15990
    },
    {
      "epoch": 1.0139421401185083,
      "grad_norm": 2.6646335124969482,
      "learning_rate": 1.9906435224386114e-05,
      "loss": 0.8629,
      "step": 16000
    },
    {
      "epoch": 1.0145758737602586,
      "grad_norm": 2.451300621032715,
      "learning_rate": 1.990008467400508e-05,
      "loss": 0.8676,
      "step": 16010
    },
    {
      "epoch": 1.015209607402009,
      "grad_norm": 2.3866002559661865,
      "learning_rate": 1.9893734123624047e-05,
      "loss": 0.8606,
      "step": 16020
    },
    {
      "epoch": 1.0158433410437593,
      "grad_norm": 2.9598777294158936,
      "learning_rate": 1.9887383573243017e-05,
      "loss": 0.8517,
      "step": 16030
    },
    {
      "epoch": 1.0164770746855096,
      "grad_norm": 2.4374582767486572,
      "learning_rate": 1.988103302286198e-05,
      "loss": 0.8751,
      "step": 16040
    },
    {
      "epoch": 1.01711080832726,
      "grad_norm": 2.178262710571289,
      "learning_rate": 1.987468247248095e-05,
      "loss": 0.8745,
      "step": 16050
    },
    {
      "epoch": 1.0177445419690103,
      "grad_norm": 2.6731536388397217,
      "learning_rate": 1.9868331922099916e-05,
      "loss": 0.8895,
      "step": 16060
    },
    {
      "epoch": 1.0183782756107609,
      "grad_norm": 2.452091932296753,
      "learning_rate": 1.9861981371718883e-05,
      "loss": 0.8885,
      "step": 16070
    },
    {
      "epoch": 1.0190120092525112,
      "grad_norm": 2.285182476043701,
      "learning_rate": 1.9855630821337853e-05,
      "loss": 0.8874,
      "step": 16080
    },
    {
      "epoch": 1.0196457428942616,
      "grad_norm": 2.6067702770233154,
      "learning_rate": 1.9849280270956816e-05,
      "loss": 0.8822,
      "step": 16090
    },
    {
      "epoch": 1.020279476536012,
      "grad_norm": 2.5393550395965576,
      "learning_rate": 1.9842929720575782e-05,
      "loss": 0.8586,
      "step": 16100
    },
    {
      "epoch": 1.0209132101777623,
      "grad_norm": 3.0290029048919678,
      "learning_rate": 1.9836579170194752e-05,
      "loss": 0.8713,
      "step": 16110
    },
    {
      "epoch": 1.0215469438195126,
      "grad_norm": 2.461786985397339,
      "learning_rate": 1.983022861981372e-05,
      "loss": 0.8633,
      "step": 16120
    },
    {
      "epoch": 1.022180677461263,
      "grad_norm": 2.3880152702331543,
      "learning_rate": 1.9823878069432685e-05,
      "loss": 0.8374,
      "step": 16130
    },
    {
      "epoch": 1.0228144111030133,
      "grad_norm": 2.765738010406494,
      "learning_rate": 1.981752751905165e-05,
      "loss": 0.8804,
      "step": 16140
    },
    {
      "epoch": 1.0234481447447639,
      "grad_norm": 3.054748296737671,
      "learning_rate": 1.9811176968670618e-05,
      "loss": 0.8653,
      "step": 16150
    },
    {
      "epoch": 1.0240818783865142,
      "grad_norm": 2.893095016479492,
      "learning_rate": 1.9804826418289584e-05,
      "loss": 0.886,
      "step": 16160
    },
    {
      "epoch": 1.0247156120282646,
      "grad_norm": 2.5597310066223145,
      "learning_rate": 1.9798475867908554e-05,
      "loss": 0.8657,
      "step": 16170
    },
    {
      "epoch": 1.025349345670015,
      "grad_norm": 2.507194995880127,
      "learning_rate": 1.979212531752752e-05,
      "loss": 0.8813,
      "step": 16180
    },
    {
      "epoch": 1.0259830793117652,
      "grad_norm": 2.3967602252960205,
      "learning_rate": 1.9785774767146487e-05,
      "loss": 0.8694,
      "step": 16190
    },
    {
      "epoch": 1.0266168129535156,
      "grad_norm": 2.410937547683716,
      "learning_rate": 1.9779424216765453e-05,
      "loss": 0.889,
      "step": 16200
    },
    {
      "epoch": 1.027250546595266,
      "grad_norm": 2.7730066776275635,
      "learning_rate": 1.977307366638442e-05,
      "loss": 0.8853,
      "step": 16210
    },
    {
      "epoch": 1.0278842802370163,
      "grad_norm": 2.665311098098755,
      "learning_rate": 1.976672311600339e-05,
      "loss": 0.8666,
      "step": 16220
    },
    {
      "epoch": 1.0285180138787668,
      "grad_norm": 2.68996524810791,
      "learning_rate": 1.9760372565622356e-05,
      "loss": 0.8801,
      "step": 16230
    },
    {
      "epoch": 1.0291517475205172,
      "grad_norm": 2.2773544788360596,
      "learning_rate": 1.975402201524132e-05,
      "loss": 0.8948,
      "step": 16240
    },
    {
      "epoch": 1.0297854811622675,
      "grad_norm": 2.771078109741211,
      "learning_rate": 1.974767146486029e-05,
      "loss": 0.9066,
      "step": 16250
    },
    {
      "epoch": 1.0304192148040179,
      "grad_norm": 2.4437310695648193,
      "learning_rate": 1.9741320914479255e-05,
      "loss": 0.8449,
      "step": 16260
    },
    {
      "epoch": 1.0310529484457682,
      "grad_norm": 2.2938568592071533,
      "learning_rate": 1.973497036409822e-05,
      "loss": 0.8186,
      "step": 16270
    },
    {
      "epoch": 1.0316866820875186,
      "grad_norm": 2.4876201152801514,
      "learning_rate": 1.972861981371719e-05,
      "loss": 0.8798,
      "step": 16280
    },
    {
      "epoch": 1.032320415729269,
      "grad_norm": 2.5379176139831543,
      "learning_rate": 1.9722269263336158e-05,
      "loss": 0.9195,
      "step": 16290
    },
    {
      "epoch": 1.0329541493710193,
      "grad_norm": 2.791412830352783,
      "learning_rate": 1.971591871295512e-05,
      "loss": 0.8848,
      "step": 16300
    },
    {
      "epoch": 1.0335878830127698,
      "grad_norm": 2.8081443309783936,
      "learning_rate": 1.970956816257409e-05,
      "loss": 0.9028,
      "step": 16310
    },
    {
      "epoch": 1.0342216166545202,
      "grad_norm": 2.5783441066741943,
      "learning_rate": 1.9703217612193057e-05,
      "loss": 0.8711,
      "step": 16320
    },
    {
      "epoch": 1.0348553502962705,
      "grad_norm": 2.6712257862091064,
      "learning_rate": 1.9696867061812023e-05,
      "loss": 0.8375,
      "step": 16330
    },
    {
      "epoch": 1.0354890839380209,
      "grad_norm": 2.5964436531066895,
      "learning_rate": 1.9690516511430993e-05,
      "loss": 0.8871,
      "step": 16340
    },
    {
      "epoch": 1.0361228175797712,
      "grad_norm": 2.4651334285736084,
      "learning_rate": 1.9684165961049956e-05,
      "loss": 0.8851,
      "step": 16350
    },
    {
      "epoch": 1.0367565512215215,
      "grad_norm": 2.528292179107666,
      "learning_rate": 1.9677815410668926e-05,
      "loss": 0.8984,
      "step": 16360
    },
    {
      "epoch": 1.037390284863272,
      "grad_norm": 2.5426888465881348,
      "learning_rate": 1.9671464860287893e-05,
      "loss": 0.872,
      "step": 16370
    },
    {
      "epoch": 1.0380240185050222,
      "grad_norm": 2.558929204940796,
      "learning_rate": 1.966511430990686e-05,
      "loss": 0.8631,
      "step": 16380
    },
    {
      "epoch": 1.0386577521467728,
      "grad_norm": 3.068251848220825,
      "learning_rate": 1.965876375952583e-05,
      "loss": 0.8895,
      "step": 16390
    },
    {
      "epoch": 1.0392914857885232,
      "grad_norm": 2.7002878189086914,
      "learning_rate": 1.9652413209144792e-05,
      "loss": 0.8645,
      "step": 16400
    },
    {
      "epoch": 1.0399252194302735,
      "grad_norm": 2.8268380165100098,
      "learning_rate": 1.9646062658763758e-05,
      "loss": 0.8466,
      "step": 16410
    },
    {
      "epoch": 1.0405589530720238,
      "grad_norm": 2.427044630050659,
      "learning_rate": 1.9639712108382728e-05,
      "loss": 0.8469,
      "step": 16420
    },
    {
      "epoch": 1.0411926867137742,
      "grad_norm": 2.712270975112915,
      "learning_rate": 1.9633361558001695e-05,
      "loss": 0.9328,
      "step": 16430
    },
    {
      "epoch": 1.0418264203555245,
      "grad_norm": 2.384371757507324,
      "learning_rate": 1.962701100762066e-05,
      "loss": 0.8405,
      "step": 16440
    },
    {
      "epoch": 1.0424601539972749,
      "grad_norm": 2.3064467906951904,
      "learning_rate": 1.9620660457239627e-05,
      "loss": 0.8796,
      "step": 16450
    },
    {
      "epoch": 1.0430938876390252,
      "grad_norm": 3.117154121398926,
      "learning_rate": 1.9614309906858594e-05,
      "loss": 0.8569,
      "step": 16460
    },
    {
      "epoch": 1.0437276212807758,
      "grad_norm": 2.681518077850342,
      "learning_rate": 1.960795935647756e-05,
      "loss": 0.8467,
      "step": 16470
    },
    {
      "epoch": 1.0443613549225261,
      "grad_norm": 2.6589419841766357,
      "learning_rate": 1.960160880609653e-05,
      "loss": 0.8981,
      "step": 16480
    },
    {
      "epoch": 1.0449950885642765,
      "grad_norm": 2.5468082427978516,
      "learning_rate": 1.9595258255715496e-05,
      "loss": 0.8999,
      "step": 16490
    },
    {
      "epoch": 1.0456288222060268,
      "grad_norm": 2.3322079181671143,
      "learning_rate": 1.958890770533446e-05,
      "loss": 0.8551,
      "step": 16500
    },
    {
      "epoch": 1.0462625558477772,
      "grad_norm": 2.5373101234436035,
      "learning_rate": 1.958255715495343e-05,
      "loss": 0.9258,
      "step": 16510
    },
    {
      "epoch": 1.0468962894895275,
      "grad_norm": 2.9862022399902344,
      "learning_rate": 1.9576206604572396e-05,
      "loss": 0.8388,
      "step": 16520
    },
    {
      "epoch": 1.0475300231312779,
      "grad_norm": 2.5495927333831787,
      "learning_rate": 1.9569856054191366e-05,
      "loss": 0.9098,
      "step": 16530
    },
    {
      "epoch": 1.0481637567730282,
      "grad_norm": 2.364136219024658,
      "learning_rate": 1.9563505503810332e-05,
      "loss": 0.8236,
      "step": 16540
    },
    {
      "epoch": 1.0487974904147788,
      "grad_norm": 2.3461313247680664,
      "learning_rate": 1.95571549534293e-05,
      "loss": 0.9095,
      "step": 16550
    },
    {
      "epoch": 1.049431224056529,
      "grad_norm": 2.747528314590454,
      "learning_rate": 1.9550804403048265e-05,
      "loss": 0.8693,
      "step": 16560
    },
    {
      "epoch": 1.0500649576982795,
      "grad_norm": 2.5940258502960205,
      "learning_rate": 1.954445385266723e-05,
      "loss": 0.9074,
      "step": 16570
    },
    {
      "epoch": 1.0506986913400298,
      "grad_norm": 2.911616563796997,
      "learning_rate": 1.9538103302286198e-05,
      "loss": 0.8764,
      "step": 16580
    },
    {
      "epoch": 1.0513324249817801,
      "grad_norm": 2.8236565589904785,
      "learning_rate": 1.9531752751905168e-05,
      "loss": 0.8737,
      "step": 16590
    },
    {
      "epoch": 1.0519661586235305,
      "grad_norm": 2.619009494781494,
      "learning_rate": 1.9525402201524134e-05,
      "loss": 0.8629,
      "step": 16600
    },
    {
      "epoch": 1.0525998922652808,
      "grad_norm": 2.4357542991638184,
      "learning_rate": 1.9519051651143097e-05,
      "loss": 0.9052,
      "step": 16610
    },
    {
      "epoch": 1.0532336259070312,
      "grad_norm": 2.5646255016326904,
      "learning_rate": 1.9512701100762067e-05,
      "loss": 0.8702,
      "step": 16620
    },
    {
      "epoch": 1.0538673595487817,
      "grad_norm": 2.262667417526245,
      "learning_rate": 1.9506350550381033e-05,
      "loss": 0.7974,
      "step": 16630
    },
    {
      "epoch": 1.054501093190532,
      "grad_norm": 2.3088486194610596,
      "learning_rate": 1.95e-05,
      "loss": 0.9184,
      "step": 16640
    },
    {
      "epoch": 1.0551348268322824,
      "grad_norm": 3.6780405044555664,
      "learning_rate": 1.949364944961897e-05,
      "loss": 0.8883,
      "step": 16650
    },
    {
      "epoch": 1.0557685604740328,
      "grad_norm": 2.8897109031677246,
      "learning_rate": 1.9487298899237933e-05,
      "loss": 0.857,
      "step": 16660
    },
    {
      "epoch": 1.0564022941157831,
      "grad_norm": 2.471345901489258,
      "learning_rate": 1.9480948348856902e-05,
      "loss": 0.8612,
      "step": 16670
    },
    {
      "epoch": 1.0570360277575335,
      "grad_norm": 2.8213489055633545,
      "learning_rate": 1.947459779847587e-05,
      "loss": 0.8968,
      "step": 16680
    },
    {
      "epoch": 1.0576697613992838,
      "grad_norm": 2.368732213973999,
      "learning_rate": 1.9468247248094835e-05,
      "loss": 0.8544,
      "step": 16690
    },
    {
      "epoch": 1.0583034950410342,
      "grad_norm": 4.443994522094727,
      "learning_rate": 1.9461896697713805e-05,
      "loss": 0.8621,
      "step": 16700
    },
    {
      "epoch": 1.0589372286827847,
      "grad_norm": 2.608466148376465,
      "learning_rate": 1.9455546147332768e-05,
      "loss": 0.889,
      "step": 16710
    },
    {
      "epoch": 1.059570962324535,
      "grad_norm": 2.5331149101257324,
      "learning_rate": 1.9449195596951734e-05,
      "loss": 0.8535,
      "step": 16720
    },
    {
      "epoch": 1.0602046959662854,
      "grad_norm": 2.6327850818634033,
      "learning_rate": 1.9442845046570704e-05,
      "loss": 0.8855,
      "step": 16730
    },
    {
      "epoch": 1.0608384296080358,
      "grad_norm": 2.5378453731536865,
      "learning_rate": 1.943649449618967e-05,
      "loss": 0.8657,
      "step": 16740
    },
    {
      "epoch": 1.061472163249786,
      "grad_norm": 2.4014482498168945,
      "learning_rate": 1.9430143945808637e-05,
      "loss": 0.9126,
      "step": 16750
    },
    {
      "epoch": 1.0621058968915364,
      "grad_norm": 2.5663225650787354,
      "learning_rate": 1.9423793395427604e-05,
      "loss": 0.8435,
      "step": 16760
    },
    {
      "epoch": 1.0627396305332868,
      "grad_norm": 2.469902276992798,
      "learning_rate": 1.941744284504657e-05,
      "loss": 0.8549,
      "step": 16770
    },
    {
      "epoch": 1.0633733641750371,
      "grad_norm": 2.5894534587860107,
      "learning_rate": 1.9411092294665536e-05,
      "loss": 0.883,
      "step": 16780
    },
    {
      "epoch": 1.0640070978167877,
      "grad_norm": 2.3223960399627686,
      "learning_rate": 1.9404741744284506e-05,
      "loss": 0.859,
      "step": 16790
    },
    {
      "epoch": 1.064640831458538,
      "grad_norm": 2.4826314449310303,
      "learning_rate": 1.9398391193903473e-05,
      "loss": 0.9055,
      "step": 16800
    }
  ],
  "logging_steps": 10,
  "max_steps": 47340,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.543911445508915e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
