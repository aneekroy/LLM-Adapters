{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9911594156975823,
  "eval_steps": 500,
  "global_step": 47200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006337336417503723,
      "grad_norm": 1.9117872714996338,
      "learning_rate": 2.7e-06,
      "loss": 3.7576,
      "step": 10
    },
    {
      "epoch": 0.0012674672835007446,
      "grad_norm": 1.9629536867141724,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 3.7461,
      "step": 20
    },
    {
      "epoch": 0.0019012009252511169,
      "grad_norm": 1.8843337297439575,
      "learning_rate": 8.7e-06,
      "loss": 3.67,
      "step": 30
    },
    {
      "epoch": 0.002534934567001489,
      "grad_norm": 1.6954119205474854,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 3.7126,
      "step": 40
    },
    {
      "epoch": 0.0031686682087518616,
      "grad_norm": 1.913343071937561,
      "learning_rate": 1.47e-05,
      "loss": 3.5163,
      "step": 50
    },
    {
      "epoch": 0.0038024018505022337,
      "grad_norm": 1.8398901224136353,
      "learning_rate": 1.77e-05,
      "loss": 3.4468,
      "step": 60
    },
    {
      "epoch": 0.004436135492252607,
      "grad_norm": 2.3952956199645996,
      "learning_rate": 2.07e-05,
      "loss": 3.4141,
      "step": 70
    },
    {
      "epoch": 0.005069869134002978,
      "grad_norm": 2.5747063159942627,
      "learning_rate": 2.37e-05,
      "loss": 3.1859,
      "step": 80
    },
    {
      "epoch": 0.005703602775753351,
      "grad_norm": 2.613248586654663,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.9878,
      "step": 90
    },
    {
      "epoch": 0.006337336417503723,
      "grad_norm": 3.0131685733795166,
      "learning_rate": 2.97e-05,
      "loss": 2.6839,
      "step": 100
    },
    {
      "epoch": 0.006971070059254096,
      "grad_norm": 3.628626585006714,
      "learning_rate": 2.999428450465707e-05,
      "loss": 2.3129,
      "step": 110
    },
    {
      "epoch": 0.007604803701004467,
      "grad_norm": 3.504011869430542,
      "learning_rate": 2.9987933954276038e-05,
      "loss": 1.9783,
      "step": 120
    },
    {
      "epoch": 0.00823853734275484,
      "grad_norm": 3.0373315811157227,
      "learning_rate": 2.9981583403895007e-05,
      "loss": 1.6821,
      "step": 130
    },
    {
      "epoch": 0.008872270984505213,
      "grad_norm": 2.904947519302368,
      "learning_rate": 2.997523285351397e-05,
      "loss": 1.4791,
      "step": 140
    },
    {
      "epoch": 0.009506004626255584,
      "grad_norm": 4.103780269622803,
      "learning_rate": 2.9968882303132937e-05,
      "loss": 1.3516,
      "step": 150
    },
    {
      "epoch": 0.010139738268005957,
      "grad_norm": 2.2443034648895264,
      "learning_rate": 2.9962531752751907e-05,
      "loss": 1.2661,
      "step": 160
    },
    {
      "epoch": 0.010773471909756329,
      "grad_norm": 2.082948923110962,
      "learning_rate": 2.9956181202370873e-05,
      "loss": 1.2268,
      "step": 170
    },
    {
      "epoch": 0.011407205551506702,
      "grad_norm": 2.696936845779419,
      "learning_rate": 2.994983065198984e-05,
      "loss": 1.194,
      "step": 180
    },
    {
      "epoch": 0.012040939193257074,
      "grad_norm": 2.160275459289551,
      "learning_rate": 2.9943480101608806e-05,
      "loss": 1.1856,
      "step": 190
    },
    {
      "epoch": 0.012674672835007447,
      "grad_norm": 1.7468996047973633,
      "learning_rate": 2.9937129551227772e-05,
      "loss": 1.1432,
      "step": 200
    },
    {
      "epoch": 0.013308406476757819,
      "grad_norm": 1.9496897459030151,
      "learning_rate": 2.9930779000846742e-05,
      "loss": 1.1428,
      "step": 210
    },
    {
      "epoch": 0.013942140118508192,
      "grad_norm": 1.8727227449417114,
      "learning_rate": 2.992442845046571e-05,
      "loss": 1.11,
      "step": 220
    },
    {
      "epoch": 0.014575873760258564,
      "grad_norm": 1.8571088314056396,
      "learning_rate": 2.9918077900084675e-05,
      "loss": 1.1374,
      "step": 230
    },
    {
      "epoch": 0.015209607402008935,
      "grad_norm": 2.1544878482818604,
      "learning_rate": 2.991172734970364e-05,
      "loss": 1.116,
      "step": 240
    },
    {
      "epoch": 0.01584334104375931,
      "grad_norm": 1.8210844993591309,
      "learning_rate": 2.9905376799322608e-05,
      "loss": 1.1418,
      "step": 250
    },
    {
      "epoch": 0.01647707468550968,
      "grad_norm": 2.422865390777588,
      "learning_rate": 2.9899026248941574e-05,
      "loss": 1.1154,
      "step": 260
    },
    {
      "epoch": 0.017110808327260054,
      "grad_norm": 2.2297041416168213,
      "learning_rate": 2.9892675698560544e-05,
      "loss": 1.1005,
      "step": 270
    },
    {
      "epoch": 0.017744541969010427,
      "grad_norm": 2.2503745555877686,
      "learning_rate": 2.988632514817951e-05,
      "loss": 1.1489,
      "step": 280
    },
    {
      "epoch": 0.018378275610760796,
      "grad_norm": 2.0805060863494873,
      "learning_rate": 2.9879974597798474e-05,
      "loss": 1.062,
      "step": 290
    },
    {
      "epoch": 0.019012009252511168,
      "grad_norm": 2.4523537158966064,
      "learning_rate": 2.9873624047417443e-05,
      "loss": 1.1196,
      "step": 300
    },
    {
      "epoch": 0.01964574289426154,
      "grad_norm": 2.0582127571105957,
      "learning_rate": 2.986727349703641e-05,
      "loss": 1.1173,
      "step": 310
    },
    {
      "epoch": 0.020279476536011913,
      "grad_norm": 1.9782936573028564,
      "learning_rate": 2.9860922946655376e-05,
      "loss": 1.081,
      "step": 320
    },
    {
      "epoch": 0.020913210177762286,
      "grad_norm": 1.9162189960479736,
      "learning_rate": 2.9854572396274346e-05,
      "loss": 1.1322,
      "step": 330
    },
    {
      "epoch": 0.021546943819512658,
      "grad_norm": 2.451296091079712,
      "learning_rate": 2.9848221845893313e-05,
      "loss": 1.101,
      "step": 340
    },
    {
      "epoch": 0.02218067746126303,
      "grad_norm": 2.363271474838257,
      "learning_rate": 2.984187129551228e-05,
      "loss": 1.0976,
      "step": 350
    },
    {
      "epoch": 0.022814411103013403,
      "grad_norm": 2.8077797889709473,
      "learning_rate": 2.9835520745131245e-05,
      "loss": 1.1167,
      "step": 360
    },
    {
      "epoch": 0.023448144744763776,
      "grad_norm": 1.8548303842544556,
      "learning_rate": 2.9829170194750212e-05,
      "loss": 1.0899,
      "step": 370
    },
    {
      "epoch": 0.024081878386514148,
      "grad_norm": 2.128476858139038,
      "learning_rate": 2.982281964436918e-05,
      "loss": 1.0929,
      "step": 380
    },
    {
      "epoch": 0.02471561202826452,
      "grad_norm": 1.7355762720108032,
      "learning_rate": 2.9816469093988148e-05,
      "loss": 1.0846,
      "step": 390
    },
    {
      "epoch": 0.025349345670014893,
      "grad_norm": 1.9736723899841309,
      "learning_rate": 2.981011854360711e-05,
      "loss": 1.0459,
      "step": 400
    },
    {
      "epoch": 0.025983079311765266,
      "grad_norm": 2.735546112060547,
      "learning_rate": 2.980376799322608e-05,
      "loss": 1.0525,
      "step": 410
    },
    {
      "epoch": 0.026616812953515638,
      "grad_norm": 1.8927018642425537,
      "learning_rate": 2.9797417442845047e-05,
      "loss": 1.0875,
      "step": 420
    },
    {
      "epoch": 0.02725054659526601,
      "grad_norm": 2.2376179695129395,
      "learning_rate": 2.9791066892464014e-05,
      "loss": 1.1144,
      "step": 430
    },
    {
      "epoch": 0.027884280237016383,
      "grad_norm": 2.68056321144104,
      "learning_rate": 2.9784716342082984e-05,
      "loss": 1.0873,
      "step": 440
    },
    {
      "epoch": 0.028518013878766756,
      "grad_norm": 2.5218887329101562,
      "learning_rate": 2.9778365791701947e-05,
      "loss": 1.1174,
      "step": 450
    },
    {
      "epoch": 0.029151747520517128,
      "grad_norm": 2.2011311054229736,
      "learning_rate": 2.9772015241320913e-05,
      "loss": 1.0863,
      "step": 460
    },
    {
      "epoch": 0.0297854811622675,
      "grad_norm": 2.162109613418579,
      "learning_rate": 2.9765664690939883e-05,
      "loss": 1.0938,
      "step": 470
    },
    {
      "epoch": 0.03041921480401787,
      "grad_norm": 1.846325397491455,
      "learning_rate": 2.975931414055885e-05,
      "loss": 1.0804,
      "step": 480
    },
    {
      "epoch": 0.031052948445768242,
      "grad_norm": 1.5710378885269165,
      "learning_rate": 2.9752963590177816e-05,
      "loss": 1.0729,
      "step": 490
    },
    {
      "epoch": 0.03168668208751862,
      "grad_norm": 2.069298505783081,
      "learning_rate": 2.9746613039796782e-05,
      "loss": 1.0757,
      "step": 500
    },
    {
      "epoch": 0.03232041572926899,
      "grad_norm": 2.619649887084961,
      "learning_rate": 2.974026248941575e-05,
      "loss": 1.0518,
      "step": 510
    },
    {
      "epoch": 0.03295414937101936,
      "grad_norm": 2.3792941570281982,
      "learning_rate": 2.973391193903472e-05,
      "loss": 1.08,
      "step": 520
    },
    {
      "epoch": 0.033587883012769736,
      "grad_norm": 1.7744858264923096,
      "learning_rate": 2.9727561388653685e-05,
      "loss": 1.0544,
      "step": 530
    },
    {
      "epoch": 0.03422161665452011,
      "grad_norm": 2.064081907272339,
      "learning_rate": 2.972121083827265e-05,
      "loss": 1.096,
      "step": 540
    },
    {
      "epoch": 0.03485535029627048,
      "grad_norm": 2.3793511390686035,
      "learning_rate": 2.9714860287891618e-05,
      "loss": 1.0673,
      "step": 550
    },
    {
      "epoch": 0.03548908393802085,
      "grad_norm": 2.490527629852295,
      "learning_rate": 2.9708509737510584e-05,
      "loss": 1.0479,
      "step": 560
    },
    {
      "epoch": 0.036122817579771226,
      "grad_norm": 1.9669108390808105,
      "learning_rate": 2.970215918712955e-05,
      "loss": 1.0919,
      "step": 570
    },
    {
      "epoch": 0.03675655122152159,
      "grad_norm": 2.5692787170410156,
      "learning_rate": 2.969580863674852e-05,
      "loss": 1.0651,
      "step": 580
    },
    {
      "epoch": 0.037390284863271964,
      "grad_norm": 1.9911679029464722,
      "learning_rate": 2.9689458086367487e-05,
      "loss": 1.0446,
      "step": 590
    },
    {
      "epoch": 0.038024018505022336,
      "grad_norm": 2.396453380584717,
      "learning_rate": 2.9683107535986453e-05,
      "loss": 1.0775,
      "step": 600
    },
    {
      "epoch": 0.03865775214677271,
      "grad_norm": 2.606339931488037,
      "learning_rate": 2.967675698560542e-05,
      "loss": 1.0804,
      "step": 610
    },
    {
      "epoch": 0.03929148578852308,
      "grad_norm": 2.1065726280212402,
      "learning_rate": 2.9670406435224386e-05,
      "loss": 1.0934,
      "step": 620
    },
    {
      "epoch": 0.039925219430273454,
      "grad_norm": 1.9628839492797852,
      "learning_rate": 2.9664055884843353e-05,
      "loss": 1.032,
      "step": 630
    },
    {
      "epoch": 0.040558953072023826,
      "grad_norm": 2.4476349353790283,
      "learning_rate": 2.9657705334462322e-05,
      "loss": 1.052,
      "step": 640
    },
    {
      "epoch": 0.0411926867137742,
      "grad_norm": 2.4301021099090576,
      "learning_rate": 2.965135478408129e-05,
      "loss": 1.0703,
      "step": 650
    },
    {
      "epoch": 0.04182642035552457,
      "grad_norm": 1.9761780500411987,
      "learning_rate": 2.9645004233700255e-05,
      "loss": 1.0648,
      "step": 660
    },
    {
      "epoch": 0.042460153997274944,
      "grad_norm": 2.5984835624694824,
      "learning_rate": 2.963865368331922e-05,
      "loss": 1.0509,
      "step": 670
    },
    {
      "epoch": 0.043093887639025316,
      "grad_norm": 2.4108059406280518,
      "learning_rate": 2.9632303132938188e-05,
      "loss": 1.0826,
      "step": 680
    },
    {
      "epoch": 0.04372762128077569,
      "grad_norm": 2.240973472595215,
      "learning_rate": 2.9625952582557158e-05,
      "loss": 1.0712,
      "step": 690
    },
    {
      "epoch": 0.04436135492252606,
      "grad_norm": 2.962956190109253,
      "learning_rate": 2.9619602032176124e-05,
      "loss": 1.0677,
      "step": 700
    },
    {
      "epoch": 0.044995088564276434,
      "grad_norm": 2.5757505893707275,
      "learning_rate": 2.9613251481795087e-05,
      "loss": 1.0226,
      "step": 710
    },
    {
      "epoch": 0.045628822206026806,
      "grad_norm": 1.820131540298462,
      "learning_rate": 2.9606900931414057e-05,
      "loss": 1.002,
      "step": 720
    },
    {
      "epoch": 0.04626255584777718,
      "grad_norm": 2.5515904426574707,
      "learning_rate": 2.9600550381033024e-05,
      "loss": 1.0122,
      "step": 730
    },
    {
      "epoch": 0.04689628948952755,
      "grad_norm": 2.3855221271514893,
      "learning_rate": 2.959419983065199e-05,
      "loss": 1.0609,
      "step": 740
    },
    {
      "epoch": 0.047530023131277924,
      "grad_norm": 2.1727123260498047,
      "learning_rate": 2.958784928027096e-05,
      "loss": 1.0659,
      "step": 750
    },
    {
      "epoch": 0.048163756773028296,
      "grad_norm": 2.3179781436920166,
      "learning_rate": 2.9581498729889923e-05,
      "loss": 1.0875,
      "step": 760
    },
    {
      "epoch": 0.04879749041477867,
      "grad_norm": 2.2498719692230225,
      "learning_rate": 2.957514817950889e-05,
      "loss": 1.0629,
      "step": 770
    },
    {
      "epoch": 0.04943122405652904,
      "grad_norm": 2.6589295864105225,
      "learning_rate": 2.956879762912786e-05,
      "loss": 1.0185,
      "step": 780
    },
    {
      "epoch": 0.050064957698279414,
      "grad_norm": 3.0515198707580566,
      "learning_rate": 2.9562447078746826e-05,
      "loss": 1.0486,
      "step": 790
    },
    {
      "epoch": 0.050698691340029786,
      "grad_norm": 2.1008763313293457,
      "learning_rate": 2.9556096528365792e-05,
      "loss": 1.0436,
      "step": 800
    },
    {
      "epoch": 0.05133242498178016,
      "grad_norm": 2.712459087371826,
      "learning_rate": 2.954974597798476e-05,
      "loss": 1.0802,
      "step": 810
    },
    {
      "epoch": 0.05196615862353053,
      "grad_norm": 1.7878366708755493,
      "learning_rate": 2.9543395427603725e-05,
      "loss": 1.0396,
      "step": 820
    },
    {
      "epoch": 0.052599892265280904,
      "grad_norm": 2.8222696781158447,
      "learning_rate": 2.9537044877222695e-05,
      "loss": 1.0783,
      "step": 830
    },
    {
      "epoch": 0.053233625907031276,
      "grad_norm": 2.4172685146331787,
      "learning_rate": 2.953069432684166e-05,
      "loss": 1.0266,
      "step": 840
    },
    {
      "epoch": 0.05386735954878165,
      "grad_norm": 2.337292194366455,
      "learning_rate": 2.9524343776460627e-05,
      "loss": 1.0719,
      "step": 850
    },
    {
      "epoch": 0.05450109319053202,
      "grad_norm": 1.97150719165802,
      "learning_rate": 2.9517993226079597e-05,
      "loss": 1.0655,
      "step": 860
    },
    {
      "epoch": 0.055134826832282394,
      "grad_norm": 2.312532901763916,
      "learning_rate": 2.951164267569856e-05,
      "loss": 0.9907,
      "step": 870
    },
    {
      "epoch": 0.055768560474032766,
      "grad_norm": 2.0114686489105225,
      "learning_rate": 2.9505292125317527e-05,
      "loss": 1.0653,
      "step": 880
    },
    {
      "epoch": 0.05640229411578314,
      "grad_norm": 3.0803422927856445,
      "learning_rate": 2.9498941574936497e-05,
      "loss": 1.0396,
      "step": 890
    },
    {
      "epoch": 0.05703602775753351,
      "grad_norm": 2.489912748336792,
      "learning_rate": 2.9492591024555463e-05,
      "loss": 1.0297,
      "step": 900
    },
    {
      "epoch": 0.057669761399283884,
      "grad_norm": 1.7714165449142456,
      "learning_rate": 2.948624047417443e-05,
      "loss": 1.0403,
      "step": 910
    },
    {
      "epoch": 0.058303495041034256,
      "grad_norm": 2.1479690074920654,
      "learning_rate": 2.9479889923793396e-05,
      "loss": 1.0398,
      "step": 920
    },
    {
      "epoch": 0.05893722868278463,
      "grad_norm": 2.6930530071258545,
      "learning_rate": 2.9473539373412362e-05,
      "loss": 1.0291,
      "step": 930
    },
    {
      "epoch": 0.059570962324535,
      "grad_norm": 2.432041883468628,
      "learning_rate": 2.946718882303133e-05,
      "loss": 1.0329,
      "step": 940
    },
    {
      "epoch": 0.060204695966285374,
      "grad_norm": 2.5477395057678223,
      "learning_rate": 2.94608382726503e-05,
      "loss": 1.0672,
      "step": 950
    },
    {
      "epoch": 0.06083842960803574,
      "grad_norm": 2.7343316078186035,
      "learning_rate": 2.9454487722269265e-05,
      "loss": 1.0453,
      "step": 960
    },
    {
      "epoch": 0.06147216324978611,
      "grad_norm": 2.1107900142669678,
      "learning_rate": 2.944813717188823e-05,
      "loss": 1.0618,
      "step": 970
    },
    {
      "epoch": 0.062105896891536484,
      "grad_norm": 2.133950710296631,
      "learning_rate": 2.9441786621507198e-05,
      "loss": 1.0506,
      "step": 980
    },
    {
      "epoch": 0.06273963053328686,
      "grad_norm": 3.080186128616333,
      "learning_rate": 2.9435436071126164e-05,
      "loss": 1.0593,
      "step": 990
    },
    {
      "epoch": 0.06337336417503724,
      "grad_norm": 2.077913284301758,
      "learning_rate": 2.9429085520745134e-05,
      "loss": 1.0267,
      "step": 1000
    },
    {
      "epoch": 0.06400709781678761,
      "grad_norm": 2.3823487758636475,
      "learning_rate": 2.94227349703641e-05,
      "loss": 1.0376,
      "step": 1010
    },
    {
      "epoch": 0.06464083145853798,
      "grad_norm": 2.1436007022857666,
      "learning_rate": 2.9416384419983064e-05,
      "loss": 1.0524,
      "step": 1020
    },
    {
      "epoch": 0.06527456510028835,
      "grad_norm": 1.9627431631088257,
      "learning_rate": 2.9410033869602033e-05,
      "loss": 1.0126,
      "step": 1030
    },
    {
      "epoch": 0.06590829874203873,
      "grad_norm": 2.6993441581726074,
      "learning_rate": 2.9403683319221e-05,
      "loss": 1.0569,
      "step": 1040
    },
    {
      "epoch": 0.0665420323837891,
      "grad_norm": 2.4357666969299316,
      "learning_rate": 2.9397332768839966e-05,
      "loss": 1.0464,
      "step": 1050
    },
    {
      "epoch": 0.06717576602553947,
      "grad_norm": 1.8371834754943848,
      "learning_rate": 2.9390982218458936e-05,
      "loss": 1.0293,
      "step": 1060
    },
    {
      "epoch": 0.06780949966728984,
      "grad_norm": 2.702312469482422,
      "learning_rate": 2.93846316680779e-05,
      "loss": 1.0385,
      "step": 1070
    },
    {
      "epoch": 0.06844323330904022,
      "grad_norm": 2.127427339553833,
      "learning_rate": 2.9378281117696865e-05,
      "loss": 1.0152,
      "step": 1080
    },
    {
      "epoch": 0.06907696695079059,
      "grad_norm": 2.0491347312927246,
      "learning_rate": 2.9371930567315835e-05,
      "loss": 1.011,
      "step": 1090
    },
    {
      "epoch": 0.06971070059254096,
      "grad_norm": 2.197349786758423,
      "learning_rate": 2.93655800169348e-05,
      "loss": 1.0406,
      "step": 1100
    },
    {
      "epoch": 0.07034443423429133,
      "grad_norm": 2.3717641830444336,
      "learning_rate": 2.9359229466553768e-05,
      "loss": 1.0062,
      "step": 1110
    },
    {
      "epoch": 0.0709781678760417,
      "grad_norm": 3.063239574432373,
      "learning_rate": 2.9352878916172738e-05,
      "loss": 1.036,
      "step": 1120
    },
    {
      "epoch": 0.07161190151779208,
      "grad_norm": 1.8244127035140991,
      "learning_rate": 2.93465283657917e-05,
      "loss": 0.9955,
      "step": 1130
    },
    {
      "epoch": 0.07224563515954245,
      "grad_norm": 2.032191276550293,
      "learning_rate": 2.934017781541067e-05,
      "loss": 1.0243,
      "step": 1140
    },
    {
      "epoch": 0.07287936880129281,
      "grad_norm": 2.129345178604126,
      "learning_rate": 2.9333827265029637e-05,
      "loss": 1.0795,
      "step": 1150
    },
    {
      "epoch": 0.07351310244304318,
      "grad_norm": 1.9811575412750244,
      "learning_rate": 2.9327476714648604e-05,
      "loss": 1.0478,
      "step": 1160
    },
    {
      "epoch": 0.07414683608479355,
      "grad_norm": 2.0752980709075928,
      "learning_rate": 2.9321126164267573e-05,
      "loss": 1.0186,
      "step": 1170
    },
    {
      "epoch": 0.07478056972654393,
      "grad_norm": 2.1868174076080322,
      "learning_rate": 2.9314775613886536e-05,
      "loss": 0.9826,
      "step": 1180
    },
    {
      "epoch": 0.0754143033682943,
      "grad_norm": 2.469831705093384,
      "learning_rate": 2.9308425063505503e-05,
      "loss": 1.0274,
      "step": 1190
    },
    {
      "epoch": 0.07604803701004467,
      "grad_norm": 2.5827648639678955,
      "learning_rate": 2.9302074513124473e-05,
      "loss": 1.0128,
      "step": 1200
    },
    {
      "epoch": 0.07668177065179504,
      "grad_norm": 2.326472043991089,
      "learning_rate": 2.929572396274344e-05,
      "loss": 0.9925,
      "step": 1210
    },
    {
      "epoch": 0.07731550429354542,
      "grad_norm": 2.7983322143554688,
      "learning_rate": 2.9289373412362406e-05,
      "loss": 1.0568,
      "step": 1220
    },
    {
      "epoch": 0.07794923793529579,
      "grad_norm": 2.0109000205993652,
      "learning_rate": 2.9283022861981372e-05,
      "loss": 1.0303,
      "step": 1230
    },
    {
      "epoch": 0.07858297157704616,
      "grad_norm": 2.0318307876586914,
      "learning_rate": 2.927667231160034e-05,
      "loss": 1.0425,
      "step": 1240
    },
    {
      "epoch": 0.07921670521879653,
      "grad_norm": 2.2090444564819336,
      "learning_rate": 2.9270321761219305e-05,
      "loss": 1.0086,
      "step": 1250
    },
    {
      "epoch": 0.07985043886054691,
      "grad_norm": 2.1038131713867188,
      "learning_rate": 2.9263971210838275e-05,
      "loss": 1.0508,
      "step": 1260
    },
    {
      "epoch": 0.08048417250229728,
      "grad_norm": 2.262449026107788,
      "learning_rate": 2.925762066045724e-05,
      "loss": 1.1044,
      "step": 1270
    },
    {
      "epoch": 0.08111790614404765,
      "grad_norm": 2.298631429672241,
      "learning_rate": 2.9251270110076208e-05,
      "loss": 0.9896,
      "step": 1280
    },
    {
      "epoch": 0.08175163978579802,
      "grad_norm": 2.6024832725524902,
      "learning_rate": 2.9244919559695174e-05,
      "loss": 1.0513,
      "step": 1290
    },
    {
      "epoch": 0.0823853734275484,
      "grad_norm": 2.634636640548706,
      "learning_rate": 2.923856900931414e-05,
      "loss": 1.0158,
      "step": 1300
    },
    {
      "epoch": 0.08301910706929877,
      "grad_norm": 2.281294584274292,
      "learning_rate": 2.923221845893311e-05,
      "loss": 1.0296,
      "step": 1310
    },
    {
      "epoch": 0.08365284071104914,
      "grad_norm": 2.2298431396484375,
      "learning_rate": 2.9225867908552077e-05,
      "loss": 1.0166,
      "step": 1320
    },
    {
      "epoch": 0.08428657435279951,
      "grad_norm": 2.2944416999816895,
      "learning_rate": 2.921951735817104e-05,
      "loss": 1.0485,
      "step": 1330
    },
    {
      "epoch": 0.08492030799454989,
      "grad_norm": 2.9307198524475098,
      "learning_rate": 2.921316680779001e-05,
      "loss": 1.0507,
      "step": 1340
    },
    {
      "epoch": 0.08555404163630026,
      "grad_norm": 2.504673719406128,
      "learning_rate": 2.9206816257408976e-05,
      "loss": 0.9956,
      "step": 1350
    },
    {
      "epoch": 0.08618777527805063,
      "grad_norm": 2.2404143810272217,
      "learning_rate": 2.9200465707027942e-05,
      "loss": 0.9851,
      "step": 1360
    },
    {
      "epoch": 0.086821508919801,
      "grad_norm": 2.597153425216675,
      "learning_rate": 2.9194115156646912e-05,
      "loss": 0.9986,
      "step": 1370
    },
    {
      "epoch": 0.08745524256155138,
      "grad_norm": 1.9478813409805298,
      "learning_rate": 2.918776460626588e-05,
      "loss": 1.0381,
      "step": 1380
    },
    {
      "epoch": 0.08808897620330175,
      "grad_norm": 1.919011116027832,
      "learning_rate": 2.918141405588484e-05,
      "loss": 1.0492,
      "step": 1390
    },
    {
      "epoch": 0.08872270984505212,
      "grad_norm": 2.6251440048217773,
      "learning_rate": 2.917506350550381e-05,
      "loss": 1.0348,
      "step": 1400
    },
    {
      "epoch": 0.0893564434868025,
      "grad_norm": 2.4770138263702393,
      "learning_rate": 2.9168712955122778e-05,
      "loss": 1.0616,
      "step": 1410
    },
    {
      "epoch": 0.08999017712855287,
      "grad_norm": 2.1502511501312256,
      "learning_rate": 2.9162362404741744e-05,
      "loss": 1.0489,
      "step": 1420
    },
    {
      "epoch": 0.09062391077030324,
      "grad_norm": 2.9368698596954346,
      "learning_rate": 2.9156011854360714e-05,
      "loss": 1.1177,
      "step": 1430
    },
    {
      "epoch": 0.09125764441205361,
      "grad_norm": 2.6200106143951416,
      "learning_rate": 2.9149661303979677e-05,
      "loss": 1.0038,
      "step": 1440
    },
    {
      "epoch": 0.09189137805380398,
      "grad_norm": 2.2277140617370605,
      "learning_rate": 2.9143310753598647e-05,
      "loss": 0.9964,
      "step": 1450
    },
    {
      "epoch": 0.09252511169555436,
      "grad_norm": 2.1982579231262207,
      "learning_rate": 2.9136960203217613e-05,
      "loss": 0.996,
      "step": 1460
    },
    {
      "epoch": 0.09315884533730473,
      "grad_norm": 2.203191041946411,
      "learning_rate": 2.913060965283658e-05,
      "loss": 1.0319,
      "step": 1470
    },
    {
      "epoch": 0.0937925789790551,
      "grad_norm": 2.5706727504730225,
      "learning_rate": 2.912425910245555e-05,
      "loss": 1.0235,
      "step": 1480
    },
    {
      "epoch": 0.09442631262080547,
      "grad_norm": 2.6622090339660645,
      "learning_rate": 2.9117908552074513e-05,
      "loss": 1.0149,
      "step": 1490
    },
    {
      "epoch": 0.09506004626255585,
      "grad_norm": 2.948490858078003,
      "learning_rate": 2.911155800169348e-05,
      "loss": 0.9811,
      "step": 1500
    },
    {
      "epoch": 0.09569377990430622,
      "grad_norm": 2.039179563522339,
      "learning_rate": 2.910520745131245e-05,
      "loss": 0.9769,
      "step": 1510
    },
    {
      "epoch": 0.09632751354605659,
      "grad_norm": 2.604874849319458,
      "learning_rate": 2.9098856900931415e-05,
      "loss": 1.0101,
      "step": 1520
    },
    {
      "epoch": 0.09696124718780696,
      "grad_norm": 2.2408905029296875,
      "learning_rate": 2.9092506350550382e-05,
      "loss": 1.0278,
      "step": 1530
    },
    {
      "epoch": 0.09759498082955734,
      "grad_norm": 2.109778881072998,
      "learning_rate": 2.9086155800169348e-05,
      "loss": 1.0007,
      "step": 1540
    },
    {
      "epoch": 0.09822871447130771,
      "grad_norm": 2.0859289169311523,
      "learning_rate": 2.9079805249788315e-05,
      "loss": 0.9822,
      "step": 1550
    },
    {
      "epoch": 0.09886244811305808,
      "grad_norm": 3.751253843307495,
      "learning_rate": 2.907345469940728e-05,
      "loss": 1.0413,
      "step": 1560
    },
    {
      "epoch": 0.09949618175480845,
      "grad_norm": 2.790252685546875,
      "learning_rate": 2.906710414902625e-05,
      "loss": 1.0408,
      "step": 1570
    },
    {
      "epoch": 0.10012991539655883,
      "grad_norm": 1.9749186038970947,
      "learning_rate": 2.9060753598645217e-05,
      "loss": 1.0281,
      "step": 1580
    },
    {
      "epoch": 0.1007636490383092,
      "grad_norm": 2.2959342002868652,
      "learning_rate": 2.9054403048264184e-05,
      "loss": 1.0248,
      "step": 1590
    },
    {
      "epoch": 0.10139738268005957,
      "grad_norm": 2.1232380867004395,
      "learning_rate": 2.904805249788315e-05,
      "loss": 1.0823,
      "step": 1600
    },
    {
      "epoch": 0.10203111632180994,
      "grad_norm": 2.3029565811157227,
      "learning_rate": 2.9041701947502117e-05,
      "loss": 1.0314,
      "step": 1610
    },
    {
      "epoch": 0.10266484996356032,
      "grad_norm": 2.7153608798980713,
      "learning_rate": 2.9035351397121086e-05,
      "loss": 1.0769,
      "step": 1620
    },
    {
      "epoch": 0.10329858360531069,
      "grad_norm": 2.5098183155059814,
      "learning_rate": 2.9029000846740053e-05,
      "loss": 1.0129,
      "step": 1630
    },
    {
      "epoch": 0.10393231724706106,
      "grad_norm": 2.3358700275421143,
      "learning_rate": 2.902265029635902e-05,
      "loss": 1.0009,
      "step": 1640
    },
    {
      "epoch": 0.10456605088881143,
      "grad_norm": 2.184455394744873,
      "learning_rate": 2.9016299745977986e-05,
      "loss": 1.0316,
      "step": 1650
    },
    {
      "epoch": 0.10519978453056181,
      "grad_norm": 2.5969958305358887,
      "learning_rate": 2.9009949195596952e-05,
      "loss": 1.0711,
      "step": 1660
    },
    {
      "epoch": 0.10583351817231218,
      "grad_norm": 2.6478588581085205,
      "learning_rate": 2.900359864521592e-05,
      "loss": 0.9916,
      "step": 1670
    },
    {
      "epoch": 0.10646725181406255,
      "grad_norm": 2.3887476921081543,
      "learning_rate": 2.899724809483489e-05,
      "loss": 0.9919,
      "step": 1680
    },
    {
      "epoch": 0.10710098545581292,
      "grad_norm": 2.0973358154296875,
      "learning_rate": 2.8990897544453855e-05,
      "loss": 1.0487,
      "step": 1690
    },
    {
      "epoch": 0.1077347190975633,
      "grad_norm": 2.454986095428467,
      "learning_rate": 2.8984546994072818e-05,
      "loss": 0.9949,
      "step": 1700
    },
    {
      "epoch": 0.10836845273931367,
      "grad_norm": 3.4651479721069336,
      "learning_rate": 2.8978196443691788e-05,
      "loss": 0.9914,
      "step": 1710
    },
    {
      "epoch": 0.10900218638106404,
      "grad_norm": 1.9965912103652954,
      "learning_rate": 2.8971845893310754e-05,
      "loss": 1.024,
      "step": 1720
    },
    {
      "epoch": 0.10963592002281441,
      "grad_norm": 2.4559950828552246,
      "learning_rate": 2.896549534292972e-05,
      "loss": 1.0404,
      "step": 1730
    },
    {
      "epoch": 0.11026965366456479,
      "grad_norm": 2.047884941101074,
      "learning_rate": 2.895914479254869e-05,
      "loss": 1.0199,
      "step": 1740
    },
    {
      "epoch": 0.11090338730631516,
      "grad_norm": 2.149149179458618,
      "learning_rate": 2.8952794242167653e-05,
      "loss": 1.0024,
      "step": 1750
    },
    {
      "epoch": 0.11153712094806553,
      "grad_norm": 2.2531871795654297,
      "learning_rate": 2.8946443691786623e-05,
      "loss": 1.0232,
      "step": 1760
    },
    {
      "epoch": 0.1121708545898159,
      "grad_norm": 2.634312629699707,
      "learning_rate": 2.894009314140559e-05,
      "loss": 0.9989,
      "step": 1770
    },
    {
      "epoch": 0.11280458823156628,
      "grad_norm": 2.1846442222595215,
      "learning_rate": 2.8933742591024556e-05,
      "loss": 1.0066,
      "step": 1780
    },
    {
      "epoch": 0.11343832187331665,
      "grad_norm": 2.563927173614502,
      "learning_rate": 2.8927392040643526e-05,
      "loss": 0.9853,
      "step": 1790
    },
    {
      "epoch": 0.11407205551506702,
      "grad_norm": 2.242816209793091,
      "learning_rate": 2.892104149026249e-05,
      "loss": 1.0335,
      "step": 1800
    },
    {
      "epoch": 0.1147057891568174,
      "grad_norm": 2.1806468963623047,
      "learning_rate": 2.8914690939881455e-05,
      "loss": 0.9899,
      "step": 1810
    },
    {
      "epoch": 0.11533952279856777,
      "grad_norm": 2.064244508743286,
      "learning_rate": 2.8908340389500425e-05,
      "loss": 1.0283,
      "step": 1820
    },
    {
      "epoch": 0.11597325644031814,
      "grad_norm": 2.2729651927948,
      "learning_rate": 2.890198983911939e-05,
      "loss": 1.0092,
      "step": 1830
    },
    {
      "epoch": 0.11660699008206851,
      "grad_norm": 2.1567416191101074,
      "learning_rate": 2.8895639288738358e-05,
      "loss": 1.0146,
      "step": 1840
    },
    {
      "epoch": 0.11724072372381888,
      "grad_norm": 2.5029704570770264,
      "learning_rate": 2.8889288738357324e-05,
      "loss": 1.0459,
      "step": 1850
    },
    {
      "epoch": 0.11787445736556926,
      "grad_norm": 2.025200128555298,
      "learning_rate": 2.888293818797629e-05,
      "loss": 1.0257,
      "step": 1860
    },
    {
      "epoch": 0.11850819100731963,
      "grad_norm": 2.440765142440796,
      "learning_rate": 2.8876587637595257e-05,
      "loss": 0.9872,
      "step": 1870
    },
    {
      "epoch": 0.11914192464907,
      "grad_norm": 2.143096923828125,
      "learning_rate": 2.8870237087214227e-05,
      "loss": 0.9973,
      "step": 1880
    },
    {
      "epoch": 0.11977565829082037,
      "grad_norm": 2.647839307785034,
      "learning_rate": 2.8863886536833193e-05,
      "loss": 0.9916,
      "step": 1890
    },
    {
      "epoch": 0.12040939193257075,
      "grad_norm": 2.5948615074157715,
      "learning_rate": 2.885753598645216e-05,
      "loss": 0.998,
      "step": 1900
    },
    {
      "epoch": 0.1210431255743211,
      "grad_norm": 2.1083855628967285,
      "learning_rate": 2.8851185436071126e-05,
      "loss": 1.0509,
      "step": 1910
    },
    {
      "epoch": 0.12167685921607148,
      "grad_norm": 2.15505051612854,
      "learning_rate": 2.8844834885690093e-05,
      "loss": 0.9716,
      "step": 1920
    },
    {
      "epoch": 0.12231059285782185,
      "grad_norm": 4.598371982574463,
      "learning_rate": 2.8838484335309063e-05,
      "loss": 1.0167,
      "step": 1930
    },
    {
      "epoch": 0.12294432649957222,
      "grad_norm": 2.1678571701049805,
      "learning_rate": 2.883213378492803e-05,
      "loss": 0.996,
      "step": 1940
    },
    {
      "epoch": 0.1235780601413226,
      "grad_norm": 2.668134927749634,
      "learning_rate": 2.8825783234546995e-05,
      "loss": 1.0141,
      "step": 1950
    },
    {
      "epoch": 0.12421179378307297,
      "grad_norm": 3.168182611465454,
      "learning_rate": 2.8819432684165962e-05,
      "loss": 1.0368,
      "step": 1960
    },
    {
      "epoch": 0.12484552742482334,
      "grad_norm": 2.756594181060791,
      "learning_rate": 2.8813082133784928e-05,
      "loss": 1.0259,
      "step": 1970
    },
    {
      "epoch": 0.12547926106657373,
      "grad_norm": 2.764883279800415,
      "learning_rate": 2.8806731583403895e-05,
      "loss": 1.0121,
      "step": 1980
    },
    {
      "epoch": 0.1261129947083241,
      "grad_norm": 2.166102647781372,
      "learning_rate": 2.8800381033022865e-05,
      "loss": 1.0042,
      "step": 1990
    },
    {
      "epoch": 0.12674672835007447,
      "grad_norm": 2.974255323410034,
      "learning_rate": 2.879403048264183e-05,
      "loss": 1.0289,
      "step": 2000
    },
    {
      "epoch": 0.12738046199182484,
      "grad_norm": 2.151545524597168,
      "learning_rate": 2.8787679932260794e-05,
      "loss": 0.9816,
      "step": 2010
    },
    {
      "epoch": 0.12801419563357522,
      "grad_norm": 2.3204457759857178,
      "learning_rate": 2.8781329381879764e-05,
      "loss": 1.0045,
      "step": 2020
    },
    {
      "epoch": 0.1286479292753256,
      "grad_norm": 2.5284411907196045,
      "learning_rate": 2.877497883149873e-05,
      "loss": 1.0361,
      "step": 2030
    },
    {
      "epoch": 0.12928166291707596,
      "grad_norm": 2.2219316959381104,
      "learning_rate": 2.8768628281117697e-05,
      "loss": 1.0237,
      "step": 2040
    },
    {
      "epoch": 0.12991539655882633,
      "grad_norm": 2.8189005851745605,
      "learning_rate": 2.8762277730736666e-05,
      "loss": 0.9858,
      "step": 2050
    },
    {
      "epoch": 0.1305491302005767,
      "grad_norm": 2.0883400440216064,
      "learning_rate": 2.875592718035563e-05,
      "loss": 0.9891,
      "step": 2060
    },
    {
      "epoch": 0.13118286384232708,
      "grad_norm": 2.5032310485839844,
      "learning_rate": 2.87495766299746e-05,
      "loss": 1.008,
      "step": 2070
    },
    {
      "epoch": 0.13181659748407745,
      "grad_norm": 2.3152377605438232,
      "learning_rate": 2.8743226079593566e-05,
      "loss": 1.0075,
      "step": 2080
    },
    {
      "epoch": 0.13245033112582782,
      "grad_norm": 2.412395715713501,
      "learning_rate": 2.8736875529212532e-05,
      "loss": 1.0127,
      "step": 2090
    },
    {
      "epoch": 0.1330840647675782,
      "grad_norm": 2.468770742416382,
      "learning_rate": 2.8730524978831502e-05,
      "loss": 0.9891,
      "step": 2100
    },
    {
      "epoch": 0.13371779840932857,
      "grad_norm": 2.0966873168945312,
      "learning_rate": 2.8724174428450465e-05,
      "loss": 1.0279,
      "step": 2110
    },
    {
      "epoch": 0.13435153205107894,
      "grad_norm": 2.8442087173461914,
      "learning_rate": 2.871782387806943e-05,
      "loss": 1.0486,
      "step": 2120
    },
    {
      "epoch": 0.13498526569282931,
      "grad_norm": 1.9599123001098633,
      "learning_rate": 2.87114733276884e-05,
      "loss": 1.0183,
      "step": 2130
    },
    {
      "epoch": 0.1356189993345797,
      "grad_norm": 2.537285566329956,
      "learning_rate": 2.8705122777307368e-05,
      "loss": 0.999,
      "step": 2140
    },
    {
      "epoch": 0.13625273297633006,
      "grad_norm": 2.4373931884765625,
      "learning_rate": 2.8698772226926334e-05,
      "loss": 0.9662,
      "step": 2150
    },
    {
      "epoch": 0.13688646661808043,
      "grad_norm": 2.9809112548828125,
      "learning_rate": 2.8692421676545304e-05,
      "loss": 1.0395,
      "step": 2160
    },
    {
      "epoch": 0.1375202002598308,
      "grad_norm": 2.494171380996704,
      "learning_rate": 2.8686071126164267e-05,
      "loss": 1.0263,
      "step": 2170
    },
    {
      "epoch": 0.13815393390158118,
      "grad_norm": 2.4323501586914062,
      "learning_rate": 2.8679720575783233e-05,
      "loss": 1.0352,
      "step": 2180
    },
    {
      "epoch": 0.13878766754333155,
      "grad_norm": 2.0321784019470215,
      "learning_rate": 2.8673370025402203e-05,
      "loss": 0.9996,
      "step": 2190
    },
    {
      "epoch": 0.13942140118508192,
      "grad_norm": 2.5907537937164307,
      "learning_rate": 2.866701947502117e-05,
      "loss": 0.9712,
      "step": 2200
    },
    {
      "epoch": 0.1400551348268323,
      "grad_norm": 1.7985048294067383,
      "learning_rate": 2.8660668924640136e-05,
      "loss": 1.0333,
      "step": 2210
    },
    {
      "epoch": 0.14068886846858267,
      "grad_norm": 2.4061989784240723,
      "learning_rate": 2.8654318374259102e-05,
      "loss": 0.9777,
      "step": 2220
    },
    {
      "epoch": 0.14132260211033304,
      "grad_norm": 2.4231984615325928,
      "learning_rate": 2.864796782387807e-05,
      "loss": 0.998,
      "step": 2230
    },
    {
      "epoch": 0.1419563357520834,
      "grad_norm": 2.828348159790039,
      "learning_rate": 2.864161727349704e-05,
      "loss": 1.0593,
      "step": 2240
    },
    {
      "epoch": 0.14259006939383378,
      "grad_norm": 2.3762269020080566,
      "learning_rate": 2.8635266723116005e-05,
      "loss": 0.9813,
      "step": 2250
    },
    {
      "epoch": 0.14322380303558416,
      "grad_norm": 2.5080089569091797,
      "learning_rate": 2.862891617273497e-05,
      "loss": 1.0072,
      "step": 2260
    },
    {
      "epoch": 0.14385753667733453,
      "grad_norm": 2.520904302597046,
      "learning_rate": 2.8622565622353938e-05,
      "loss": 1.0457,
      "step": 2270
    },
    {
      "epoch": 0.1444912703190849,
      "grad_norm": 2.2867467403411865,
      "learning_rate": 2.8616215071972904e-05,
      "loss": 1.022,
      "step": 2280
    },
    {
      "epoch": 0.14512500396083525,
      "grad_norm": 2.3986265659332275,
      "learning_rate": 2.860986452159187e-05,
      "loss": 0.9993,
      "step": 2290
    },
    {
      "epoch": 0.14575873760258562,
      "grad_norm": 2.9832329750061035,
      "learning_rate": 2.860351397121084e-05,
      "loss": 0.9922,
      "step": 2300
    },
    {
      "epoch": 0.146392471244336,
      "grad_norm": 2.488196849822998,
      "learning_rate": 2.8597163420829807e-05,
      "loss": 0.9599,
      "step": 2310
    },
    {
      "epoch": 0.14702620488608636,
      "grad_norm": 2.4126174449920654,
      "learning_rate": 2.859081287044877e-05,
      "loss": 1.0038,
      "step": 2320
    },
    {
      "epoch": 0.14765993852783674,
      "grad_norm": 1.8689850568771362,
      "learning_rate": 2.858446232006774e-05,
      "loss": 0.9911,
      "step": 2330
    },
    {
      "epoch": 0.1482936721695871,
      "grad_norm": 2.317544937133789,
      "learning_rate": 2.8578111769686706e-05,
      "loss": 0.9634,
      "step": 2340
    },
    {
      "epoch": 0.14892740581133748,
      "grad_norm": 2.3033387660980225,
      "learning_rate": 2.8571761219305673e-05,
      "loss": 0.9945,
      "step": 2350
    },
    {
      "epoch": 0.14956113945308785,
      "grad_norm": 2.3790338039398193,
      "learning_rate": 2.8565410668924643e-05,
      "loss": 1.0006,
      "step": 2360
    },
    {
      "epoch": 0.15019487309483823,
      "grad_norm": 2.5691072940826416,
      "learning_rate": 2.8559060118543606e-05,
      "loss": 1.0085,
      "step": 2370
    },
    {
      "epoch": 0.1508286067365886,
      "grad_norm": 2.044067621231079,
      "learning_rate": 2.8552709568162575e-05,
      "loss": 1.0085,
      "step": 2380
    },
    {
      "epoch": 0.15146234037833897,
      "grad_norm": 2.2551167011260986,
      "learning_rate": 2.8546359017781542e-05,
      "loss": 0.9896,
      "step": 2390
    },
    {
      "epoch": 0.15209607402008934,
      "grad_norm": 2.716008424758911,
      "learning_rate": 2.854000846740051e-05,
      "loss": 0.9939,
      "step": 2400
    },
    {
      "epoch": 0.15272980766183972,
      "grad_norm": 2.8033671379089355,
      "learning_rate": 2.8533657917019478e-05,
      "loss": 1.0023,
      "step": 2410
    },
    {
      "epoch": 0.1533635413035901,
      "grad_norm": 2.4409759044647217,
      "learning_rate": 2.8527307366638445e-05,
      "loss": 1.009,
      "step": 2420
    },
    {
      "epoch": 0.15399727494534046,
      "grad_norm": 2.585149049758911,
      "learning_rate": 2.8520956816257408e-05,
      "loss": 1.0013,
      "step": 2430
    },
    {
      "epoch": 0.15463100858709083,
      "grad_norm": 2.4594602584838867,
      "learning_rate": 2.8514606265876377e-05,
      "loss": 0.9934,
      "step": 2440
    },
    {
      "epoch": 0.1552647422288412,
      "grad_norm": 3.008371353149414,
      "learning_rate": 2.8508255715495344e-05,
      "loss": 0.9783,
      "step": 2450
    },
    {
      "epoch": 0.15589847587059158,
      "grad_norm": 3.271615505218506,
      "learning_rate": 2.850190516511431e-05,
      "loss": 0.9913,
      "step": 2460
    },
    {
      "epoch": 0.15653220951234195,
      "grad_norm": 2.6596269607543945,
      "learning_rate": 2.849555461473328e-05,
      "loss": 1.0043,
      "step": 2470
    },
    {
      "epoch": 0.15716594315409232,
      "grad_norm": 2.271651268005371,
      "learning_rate": 2.8489204064352243e-05,
      "loss": 0.9946,
      "step": 2480
    },
    {
      "epoch": 0.1577996767958427,
      "grad_norm": 2.5608878135681152,
      "learning_rate": 2.848285351397121e-05,
      "loss": 0.9893,
      "step": 2490
    },
    {
      "epoch": 0.15843341043759307,
      "grad_norm": 2.437818765640259,
      "learning_rate": 2.847650296359018e-05,
      "loss": 1.002,
      "step": 2500
    },
    {
      "epoch": 0.15906714407934344,
      "grad_norm": 2.201176643371582,
      "learning_rate": 2.8470152413209146e-05,
      "loss": 0.9878,
      "step": 2510
    },
    {
      "epoch": 0.15970087772109381,
      "grad_norm": 2.8983874320983887,
      "learning_rate": 2.8463801862828112e-05,
      "loss": 1.0275,
      "step": 2520
    },
    {
      "epoch": 0.1603346113628442,
      "grad_norm": 2.3371946811676025,
      "learning_rate": 2.845745131244708e-05,
      "loss": 0.9735,
      "step": 2530
    },
    {
      "epoch": 0.16096834500459456,
      "grad_norm": 2.373133659362793,
      "learning_rate": 2.8451100762066045e-05,
      "loss": 1.0149,
      "step": 2540
    },
    {
      "epoch": 0.16160207864634493,
      "grad_norm": 2.5242488384246826,
      "learning_rate": 2.8444750211685015e-05,
      "loss": 1.0475,
      "step": 2550
    },
    {
      "epoch": 0.1622358122880953,
      "grad_norm": 2.3034913539886475,
      "learning_rate": 2.843839966130398e-05,
      "loss": 1.0262,
      "step": 2560
    },
    {
      "epoch": 0.16286954592984568,
      "grad_norm": 2.071619987487793,
      "learning_rate": 2.8432049110922948e-05,
      "loss": 1.0021,
      "step": 2570
    },
    {
      "epoch": 0.16350327957159605,
      "grad_norm": 2.419959306716919,
      "learning_rate": 2.8425698560541914e-05,
      "loss": 1.0014,
      "step": 2580
    },
    {
      "epoch": 0.16413701321334642,
      "grad_norm": 2.6636717319488525,
      "learning_rate": 2.841934801016088e-05,
      "loss": 1.0191,
      "step": 2590
    },
    {
      "epoch": 0.1647707468550968,
      "grad_norm": 2.4050989151000977,
      "learning_rate": 2.8412997459779847e-05,
      "loss": 1.0679,
      "step": 2600
    },
    {
      "epoch": 0.16540448049684717,
      "grad_norm": 2.136690855026245,
      "learning_rate": 2.8406646909398817e-05,
      "loss": 1.0221,
      "step": 2610
    },
    {
      "epoch": 0.16603821413859754,
      "grad_norm": 2.5630693435668945,
      "learning_rate": 2.8400296359017783e-05,
      "loss": 1.0207,
      "step": 2620
    },
    {
      "epoch": 0.1666719477803479,
      "grad_norm": 2.1396989822387695,
      "learning_rate": 2.8393945808636746e-05,
      "loss": 0.9896,
      "step": 2630
    },
    {
      "epoch": 0.16730568142209828,
      "grad_norm": 2.2794673442840576,
      "learning_rate": 2.8387595258255716e-05,
      "loss": 1.0022,
      "step": 2640
    },
    {
      "epoch": 0.16793941506384866,
      "grad_norm": 2.2908451557159424,
      "learning_rate": 2.8381244707874683e-05,
      "loss": 1.0341,
      "step": 2650
    },
    {
      "epoch": 0.16857314870559903,
      "grad_norm": 2.1314034461975098,
      "learning_rate": 2.837489415749365e-05,
      "loss": 1.0227,
      "step": 2660
    },
    {
      "epoch": 0.1692068823473494,
      "grad_norm": 2.249807596206665,
      "learning_rate": 2.836854360711262e-05,
      "loss": 0.9588,
      "step": 2670
    },
    {
      "epoch": 0.16984061598909977,
      "grad_norm": 1.9574499130249023,
      "learning_rate": 2.8362193056731585e-05,
      "loss": 0.9869,
      "step": 2680
    },
    {
      "epoch": 0.17047434963085015,
      "grad_norm": 2.1540513038635254,
      "learning_rate": 2.835584250635055e-05,
      "loss": 0.9518,
      "step": 2690
    },
    {
      "epoch": 0.17110808327260052,
      "grad_norm": 2.1685128211975098,
      "learning_rate": 2.8349491955969518e-05,
      "loss": 0.9666,
      "step": 2700
    },
    {
      "epoch": 0.1717418169143509,
      "grad_norm": 2.468830108642578,
      "learning_rate": 2.8343141405588485e-05,
      "loss": 1.0279,
      "step": 2710
    },
    {
      "epoch": 0.17237555055610126,
      "grad_norm": 3.2103271484375,
      "learning_rate": 2.8336790855207454e-05,
      "loss": 0.9793,
      "step": 2720
    },
    {
      "epoch": 0.17300928419785164,
      "grad_norm": 1.941463589668274,
      "learning_rate": 2.833044030482642e-05,
      "loss": 0.966,
      "step": 2730
    },
    {
      "epoch": 0.173643017839602,
      "grad_norm": 2.5897631645202637,
      "learning_rate": 2.8324089754445384e-05,
      "loss": 1.0308,
      "step": 2740
    },
    {
      "epoch": 0.17427675148135238,
      "grad_norm": 2.4741148948669434,
      "learning_rate": 2.8317739204064354e-05,
      "loss": 0.9767,
      "step": 2750
    },
    {
      "epoch": 0.17491048512310275,
      "grad_norm": 2.750504493713379,
      "learning_rate": 2.831138865368332e-05,
      "loss": 0.9974,
      "step": 2760
    },
    {
      "epoch": 0.17554421876485313,
      "grad_norm": 2.4614410400390625,
      "learning_rate": 2.8305038103302286e-05,
      "loss": 0.9948,
      "step": 2770
    },
    {
      "epoch": 0.1761779524066035,
      "grad_norm": 2.1061413288116455,
      "learning_rate": 2.8298687552921256e-05,
      "loss": 1.0109,
      "step": 2780
    },
    {
      "epoch": 0.17681168604835387,
      "grad_norm": 2.574578285217285,
      "learning_rate": 2.829233700254022e-05,
      "loss": 1.0324,
      "step": 2790
    },
    {
      "epoch": 0.17744541969010424,
      "grad_norm": 2.2413649559020996,
      "learning_rate": 2.8285986452159186e-05,
      "loss": 0.9795,
      "step": 2800
    },
    {
      "epoch": 0.17807915333185462,
      "grad_norm": 2.017630100250244,
      "learning_rate": 2.8279635901778156e-05,
      "loss": 1.0059,
      "step": 2810
    },
    {
      "epoch": 0.178712886973605,
      "grad_norm": 2.2543983459472656,
      "learning_rate": 2.8273285351397122e-05,
      "loss": 1.0059,
      "step": 2820
    },
    {
      "epoch": 0.17934662061535536,
      "grad_norm": 2.3321497440338135,
      "learning_rate": 2.826693480101609e-05,
      "loss": 0.995,
      "step": 2830
    },
    {
      "epoch": 0.17998035425710573,
      "grad_norm": 2.396773338317871,
      "learning_rate": 2.8260584250635055e-05,
      "loss": 1.0221,
      "step": 2840
    },
    {
      "epoch": 0.1806140878988561,
      "grad_norm": 2.4513938426971436,
      "learning_rate": 2.825423370025402e-05,
      "loss": 1.0184,
      "step": 2850
    },
    {
      "epoch": 0.18124782154060648,
      "grad_norm": 2.146777391433716,
      "learning_rate": 2.824788314987299e-05,
      "loss": 0.9651,
      "step": 2860
    },
    {
      "epoch": 0.18188155518235685,
      "grad_norm": 2.3244080543518066,
      "learning_rate": 2.8241532599491958e-05,
      "loss": 1.0044,
      "step": 2870
    },
    {
      "epoch": 0.18251528882410722,
      "grad_norm": 2.3014485836029053,
      "learning_rate": 2.8235182049110924e-05,
      "loss": 0.9987,
      "step": 2880
    },
    {
      "epoch": 0.1831490224658576,
      "grad_norm": 2.248777151107788,
      "learning_rate": 2.822883149872989e-05,
      "loss": 0.9934,
      "step": 2890
    },
    {
      "epoch": 0.18378275610760797,
      "grad_norm": 3.2415568828582764,
      "learning_rate": 2.8222480948348857e-05,
      "loss": 1.0313,
      "step": 2900
    },
    {
      "epoch": 0.18441648974935834,
      "grad_norm": 1.8893219232559204,
      "learning_rate": 2.8216130397967823e-05,
      "loss": 0.9856,
      "step": 2910
    },
    {
      "epoch": 0.18505022339110871,
      "grad_norm": 2.34016489982605,
      "learning_rate": 2.8209779847586793e-05,
      "loss": 0.9955,
      "step": 2920
    },
    {
      "epoch": 0.1856839570328591,
      "grad_norm": 2.4143567085266113,
      "learning_rate": 2.820342929720576e-05,
      "loss": 0.9977,
      "step": 2930
    },
    {
      "epoch": 0.18631769067460946,
      "grad_norm": 2.3814735412597656,
      "learning_rate": 2.8197078746824726e-05,
      "loss": 0.9554,
      "step": 2940
    },
    {
      "epoch": 0.18695142431635983,
      "grad_norm": 2.8147990703582764,
      "learning_rate": 2.8190728196443692e-05,
      "loss": 1.0353,
      "step": 2950
    },
    {
      "epoch": 0.1875851579581102,
      "grad_norm": 2.538766384124756,
      "learning_rate": 2.818437764606266e-05,
      "loss": 1.0264,
      "step": 2960
    },
    {
      "epoch": 0.18821889159986058,
      "grad_norm": 2.517272472381592,
      "learning_rate": 2.8178027095681625e-05,
      "loss": 0.9957,
      "step": 2970
    },
    {
      "epoch": 0.18885262524161095,
      "grad_norm": 2.5263116359710693,
      "learning_rate": 2.8171676545300595e-05,
      "loss": 0.9857,
      "step": 2980
    },
    {
      "epoch": 0.18948635888336132,
      "grad_norm": 2.447692632675171,
      "learning_rate": 2.816532599491956e-05,
      "loss": 0.9601,
      "step": 2990
    },
    {
      "epoch": 0.1901200925251117,
      "grad_norm": 2.7507495880126953,
      "learning_rate": 2.8158975444538524e-05,
      "loss": 1.0229,
      "step": 3000
    },
    {
      "epoch": 0.19075382616686207,
      "grad_norm": 2.2534189224243164,
      "learning_rate": 2.8152624894157494e-05,
      "loss": 1.0086,
      "step": 3010
    },
    {
      "epoch": 0.19138755980861244,
      "grad_norm": 3.743133068084717,
      "learning_rate": 2.814627434377646e-05,
      "loss": 1.0142,
      "step": 3020
    },
    {
      "epoch": 0.1920212934503628,
      "grad_norm": 2.549394369125366,
      "learning_rate": 2.813992379339543e-05,
      "loss": 1.0342,
      "step": 3030
    },
    {
      "epoch": 0.19265502709211318,
      "grad_norm": 2.3656203746795654,
      "learning_rate": 2.8133573243014397e-05,
      "loss": 0.9906,
      "step": 3040
    },
    {
      "epoch": 0.19328876073386356,
      "grad_norm": 2.131513833999634,
      "learning_rate": 2.812722269263336e-05,
      "loss": 1.0262,
      "step": 3050
    },
    {
      "epoch": 0.19392249437561393,
      "grad_norm": 2.7792608737945557,
      "learning_rate": 2.812087214225233e-05,
      "loss": 1.0306,
      "step": 3060
    },
    {
      "epoch": 0.1945562280173643,
      "grad_norm": 2.1477484703063965,
      "learning_rate": 2.8114521591871296e-05,
      "loss": 0.9718,
      "step": 3070
    },
    {
      "epoch": 0.19518996165911467,
      "grad_norm": 2.735358476638794,
      "learning_rate": 2.8108171041490263e-05,
      "loss": 1.0227,
      "step": 3080
    },
    {
      "epoch": 0.19582369530086505,
      "grad_norm": 2.2428958415985107,
      "learning_rate": 2.8101820491109232e-05,
      "loss": 0.9778,
      "step": 3090
    },
    {
      "epoch": 0.19645742894261542,
      "grad_norm": 3.004837989807129,
      "learning_rate": 2.8095469940728195e-05,
      "loss": 0.9515,
      "step": 3100
    },
    {
      "epoch": 0.1970911625843658,
      "grad_norm": 2.3609790802001953,
      "learning_rate": 2.8089119390347162e-05,
      "loss": 0.9797,
      "step": 3110
    },
    {
      "epoch": 0.19772489622611616,
      "grad_norm": 2.4560415744781494,
      "learning_rate": 2.8082768839966132e-05,
      "loss": 0.9679,
      "step": 3120
    },
    {
      "epoch": 0.19835862986786654,
      "grad_norm": 2.575709581375122,
      "learning_rate": 2.8076418289585098e-05,
      "loss": 1.0379,
      "step": 3130
    },
    {
      "epoch": 0.1989923635096169,
      "grad_norm": 2.2849783897399902,
      "learning_rate": 2.8070067739204065e-05,
      "loss": 0.977,
      "step": 3140
    },
    {
      "epoch": 0.19962609715136728,
      "grad_norm": 2.7122859954833984,
      "learning_rate": 2.806371718882303e-05,
      "loss": 0.9917,
      "step": 3150
    },
    {
      "epoch": 0.20025983079311765,
      "grad_norm": 2.5333452224731445,
      "learning_rate": 2.8057366638441997e-05,
      "loss": 0.98,
      "step": 3160
    },
    {
      "epoch": 0.20089356443486803,
      "grad_norm": 2.4066567420959473,
      "learning_rate": 2.8051016088060967e-05,
      "loss": 0.9778,
      "step": 3170
    },
    {
      "epoch": 0.2015272980766184,
      "grad_norm": 2.2017722129821777,
      "learning_rate": 2.8044665537679934e-05,
      "loss": 1.0101,
      "step": 3180
    },
    {
      "epoch": 0.20216103171836877,
      "grad_norm": 2.584683656692505,
      "learning_rate": 2.80383149872989e-05,
      "loss": 1.0066,
      "step": 3190
    },
    {
      "epoch": 0.20279476536011914,
      "grad_norm": 2.2302286624908447,
      "learning_rate": 2.803196443691787e-05,
      "loss": 0.987,
      "step": 3200
    },
    {
      "epoch": 0.20342849900186952,
      "grad_norm": 2.757089376449585,
      "learning_rate": 2.8025613886536833e-05,
      "loss": 0.9825,
      "step": 3210
    },
    {
      "epoch": 0.2040622326436199,
      "grad_norm": 2.398127317428589,
      "learning_rate": 2.80192633361558e-05,
      "loss": 0.9809,
      "step": 3220
    },
    {
      "epoch": 0.20469596628537026,
      "grad_norm": 3.42431378364563,
      "learning_rate": 2.801291278577477e-05,
      "loss": 0.9835,
      "step": 3230
    },
    {
      "epoch": 0.20532969992712063,
      "grad_norm": 2.54882550239563,
      "learning_rate": 2.8006562235393736e-05,
      "loss": 0.9655,
      "step": 3240
    },
    {
      "epoch": 0.205963433568871,
      "grad_norm": 2.351565361022949,
      "learning_rate": 2.8000211685012702e-05,
      "loss": 0.9603,
      "step": 3250
    },
    {
      "epoch": 0.20659716721062138,
      "grad_norm": 2.176176071166992,
      "learning_rate": 2.799386113463167e-05,
      "loss": 1.0099,
      "step": 3260
    },
    {
      "epoch": 0.20723090085237175,
      "grad_norm": 2.003251314163208,
      "learning_rate": 2.7987510584250635e-05,
      "loss": 0.9253,
      "step": 3270
    },
    {
      "epoch": 0.20786463449412212,
      "grad_norm": 3.2449004650115967,
      "learning_rate": 2.79811600338696e-05,
      "loss": 1.0147,
      "step": 3280
    },
    {
      "epoch": 0.2084983681358725,
      "grad_norm": 2.5526928901672363,
      "learning_rate": 2.797480948348857e-05,
      "loss": 1.0299,
      "step": 3290
    },
    {
      "epoch": 0.20913210177762287,
      "grad_norm": 2.267590045928955,
      "learning_rate": 2.7968458933107538e-05,
      "loss": 0.9953,
      "step": 3300
    },
    {
      "epoch": 0.20976583541937324,
      "grad_norm": 2.325261116027832,
      "learning_rate": 2.79621083827265e-05,
      "loss": 1.0463,
      "step": 3310
    },
    {
      "epoch": 0.21039956906112361,
      "grad_norm": 2.0832226276397705,
      "learning_rate": 2.795575783234547e-05,
      "loss": 1.0082,
      "step": 3320
    },
    {
      "epoch": 0.211033302702874,
      "grad_norm": 2.2474513053894043,
      "learning_rate": 2.7949407281964437e-05,
      "loss": 1.0051,
      "step": 3330
    },
    {
      "epoch": 0.21166703634462436,
      "grad_norm": 2.580388307571411,
      "learning_rate": 2.7943056731583407e-05,
      "loss": 0.9694,
      "step": 3340
    },
    {
      "epoch": 0.21230076998637473,
      "grad_norm": 2.2547616958618164,
      "learning_rate": 2.7936706181202373e-05,
      "loss": 0.949,
      "step": 3350
    },
    {
      "epoch": 0.2129345036281251,
      "grad_norm": 2.289921283721924,
      "learning_rate": 2.7930355630821336e-05,
      "loss": 0.9763,
      "step": 3360
    },
    {
      "epoch": 0.21356823726987548,
      "grad_norm": 2.0584774017333984,
      "learning_rate": 2.7924005080440306e-05,
      "loss": 0.9511,
      "step": 3370
    },
    {
      "epoch": 0.21420197091162585,
      "grad_norm": 2.243021011352539,
      "learning_rate": 2.7917654530059272e-05,
      "loss": 1.0013,
      "step": 3380
    },
    {
      "epoch": 0.21483570455337622,
      "grad_norm": 2.065838098526001,
      "learning_rate": 2.7911939034716343e-05,
      "loss": 0.9588,
      "step": 3390
    },
    {
      "epoch": 0.2154694381951266,
      "grad_norm": 2.286329984664917,
      "learning_rate": 2.790558848433531e-05,
      "loss": 0.9613,
      "step": 3400
    },
    {
      "epoch": 0.21610317183687697,
      "grad_norm": 2.2905731201171875,
      "learning_rate": 2.789923793395428e-05,
      "loss": 0.9587,
      "step": 3410
    },
    {
      "epoch": 0.21673690547862734,
      "grad_norm": 2.242253541946411,
      "learning_rate": 2.7892887383573242e-05,
      "loss": 1.0009,
      "step": 3420
    },
    {
      "epoch": 0.2173706391203777,
      "grad_norm": 2.5485353469848633,
      "learning_rate": 2.788653683319221e-05,
      "loss": 1.0111,
      "step": 3430
    },
    {
      "epoch": 0.21800437276212808,
      "grad_norm": 2.196955680847168,
      "learning_rate": 2.788018628281118e-05,
      "loss": 0.9847,
      "step": 3440
    },
    {
      "epoch": 0.21863810640387846,
      "grad_norm": 2.63417911529541,
      "learning_rate": 2.7873835732430145e-05,
      "loss": 0.9927,
      "step": 3450
    },
    {
      "epoch": 0.21927184004562883,
      "grad_norm": 3.367152690887451,
      "learning_rate": 2.786748518204911e-05,
      "loss": 1.0217,
      "step": 3460
    },
    {
      "epoch": 0.2199055736873792,
      "grad_norm": 2.2749593257904053,
      "learning_rate": 2.7861134631668078e-05,
      "loss": 0.9858,
      "step": 3470
    },
    {
      "epoch": 0.22053930732912957,
      "grad_norm": 3.132128953933716,
      "learning_rate": 2.7854784081287044e-05,
      "loss": 1.009,
      "step": 3480
    },
    {
      "epoch": 0.22117304097087995,
      "grad_norm": 2.0321030616760254,
      "learning_rate": 2.7848433530906014e-05,
      "loss": 1.0178,
      "step": 3490
    },
    {
      "epoch": 0.22180677461263032,
      "grad_norm": 2.4559614658355713,
      "learning_rate": 2.784208298052498e-05,
      "loss": 1.0137,
      "step": 3500
    },
    {
      "epoch": 0.2224405082543807,
      "grad_norm": 2.2069590091705322,
      "learning_rate": 2.7835732430143947e-05,
      "loss": 1.0019,
      "step": 3510
    },
    {
      "epoch": 0.22307424189613106,
      "grad_norm": 2.7632484436035156,
      "learning_rate": 2.7829381879762917e-05,
      "loss": 0.9874,
      "step": 3520
    },
    {
      "epoch": 0.22370797553788144,
      "grad_norm": 2.1527812480926514,
      "learning_rate": 2.782303132938188e-05,
      "loss": 1.0239,
      "step": 3530
    },
    {
      "epoch": 0.2243417091796318,
      "grad_norm": 2.3577280044555664,
      "learning_rate": 2.7816680779000846e-05,
      "loss": 0.944,
      "step": 3540
    },
    {
      "epoch": 0.22497544282138218,
      "grad_norm": 2.614461660385132,
      "learning_rate": 2.7810330228619816e-05,
      "loss": 0.9846,
      "step": 3550
    },
    {
      "epoch": 0.22560917646313255,
      "grad_norm": 2.4917800426483154,
      "learning_rate": 2.7803979678238782e-05,
      "loss": 0.9775,
      "step": 3560
    },
    {
      "epoch": 0.22624291010488293,
      "grad_norm": 2.3714282512664795,
      "learning_rate": 2.779762912785775e-05,
      "loss": 0.9953,
      "step": 3570
    },
    {
      "epoch": 0.2268766437466333,
      "grad_norm": 2.378512144088745,
      "learning_rate": 2.7791278577476715e-05,
      "loss": 0.996,
      "step": 3580
    },
    {
      "epoch": 0.22751037738838367,
      "grad_norm": 2.708212375640869,
      "learning_rate": 2.778492802709568e-05,
      "loss": 0.9986,
      "step": 3590
    },
    {
      "epoch": 0.22814411103013404,
      "grad_norm": 2.2659640312194824,
      "learning_rate": 2.7778577476714648e-05,
      "loss": 1.0228,
      "step": 3600
    },
    {
      "epoch": 0.22877784467188442,
      "grad_norm": 2.2860915660858154,
      "learning_rate": 2.7772226926333618e-05,
      "loss": 1.0092,
      "step": 3610
    },
    {
      "epoch": 0.2294115783136348,
      "grad_norm": 2.7061307430267334,
      "learning_rate": 2.7765876375952584e-05,
      "loss": 0.9896,
      "step": 3620
    },
    {
      "epoch": 0.23004531195538516,
      "grad_norm": 2.4743921756744385,
      "learning_rate": 2.7759525825571547e-05,
      "loss": 1.0129,
      "step": 3630
    },
    {
      "epoch": 0.23067904559713553,
      "grad_norm": 2.55769419670105,
      "learning_rate": 2.7753175275190517e-05,
      "loss": 0.9738,
      "step": 3640
    },
    {
      "epoch": 0.2313127792388859,
      "grad_norm": 2.038259983062744,
      "learning_rate": 2.7746824724809483e-05,
      "loss": 0.9899,
      "step": 3650
    },
    {
      "epoch": 0.23194651288063628,
      "grad_norm": 1.9794466495513916,
      "learning_rate": 2.7740474174428453e-05,
      "loss": 0.996,
      "step": 3660
    },
    {
      "epoch": 0.23258024652238665,
      "grad_norm": 2.4352834224700928,
      "learning_rate": 2.773412362404742e-05,
      "loss": 0.9977,
      "step": 3670
    },
    {
      "epoch": 0.23321398016413702,
      "grad_norm": 2.225895404815674,
      "learning_rate": 2.7727773073666383e-05,
      "loss": 0.9869,
      "step": 3680
    },
    {
      "epoch": 0.2338477138058874,
      "grad_norm": 2.2178707122802734,
      "learning_rate": 2.7721422523285353e-05,
      "loss": 1.006,
      "step": 3690
    },
    {
      "epoch": 0.23448144744763777,
      "grad_norm": 2.580091714859009,
      "learning_rate": 2.771507197290432e-05,
      "loss": 1.0195,
      "step": 3700
    },
    {
      "epoch": 0.23511518108938814,
      "grad_norm": 2.942695379257202,
      "learning_rate": 2.7708721422523285e-05,
      "loss": 0.9667,
      "step": 3710
    },
    {
      "epoch": 0.23574891473113851,
      "grad_norm": 2.540663957595825,
      "learning_rate": 2.7702370872142255e-05,
      "loss": 1.0495,
      "step": 3720
    },
    {
      "epoch": 0.2363826483728889,
      "grad_norm": 2.4146764278411865,
      "learning_rate": 2.7696020321761218e-05,
      "loss": 0.9695,
      "step": 3730
    },
    {
      "epoch": 0.23701638201463926,
      "grad_norm": 3.0230660438537598,
      "learning_rate": 2.7689669771380185e-05,
      "loss": 1.0102,
      "step": 3740
    },
    {
      "epoch": 0.23765011565638963,
      "grad_norm": 2.2321553230285645,
      "learning_rate": 2.7683319220999155e-05,
      "loss": 1.012,
      "step": 3750
    },
    {
      "epoch": 0.23828384929814,
      "grad_norm": 2.6013576984405518,
      "learning_rate": 2.767696867061812e-05,
      "loss": 1.0133,
      "step": 3760
    },
    {
      "epoch": 0.23891758293989038,
      "grad_norm": 2.1758313179016113,
      "learning_rate": 2.7670618120237087e-05,
      "loss": 1.0417,
      "step": 3770
    },
    {
      "epoch": 0.23955131658164075,
      "grad_norm": 2.350978136062622,
      "learning_rate": 2.7664267569856057e-05,
      "loss": 0.9669,
      "step": 3780
    },
    {
      "epoch": 0.24018505022339112,
      "grad_norm": 2.0350513458251953,
      "learning_rate": 2.765791701947502e-05,
      "loss": 0.9998,
      "step": 3790
    },
    {
      "epoch": 0.2408187838651415,
      "grad_norm": 2.114159345626831,
      "learning_rate": 2.765156646909399e-05,
      "loss": 0.9942,
      "step": 3800
    },
    {
      "epoch": 0.24145251750689187,
      "grad_norm": 2.6152825355529785,
      "learning_rate": 2.7645215918712956e-05,
      "loss": 1.0224,
      "step": 3810
    },
    {
      "epoch": 0.2420862511486422,
      "grad_norm": 2.397021532058716,
      "learning_rate": 2.7638865368331923e-05,
      "loss": 0.9871,
      "step": 3820
    },
    {
      "epoch": 0.24271998479039258,
      "grad_norm": 2.438300848007202,
      "learning_rate": 2.7632514817950893e-05,
      "loss": 0.9843,
      "step": 3830
    },
    {
      "epoch": 0.24335371843214296,
      "grad_norm": 2.3559112548828125,
      "learning_rate": 2.7626164267569856e-05,
      "loss": 1.003,
      "step": 3840
    },
    {
      "epoch": 0.24398745207389333,
      "grad_norm": 1.9954297542572021,
      "learning_rate": 2.7619813717188822e-05,
      "loss": 0.9888,
      "step": 3850
    },
    {
      "epoch": 0.2446211857156437,
      "grad_norm": 2.163553237915039,
      "learning_rate": 2.7613463166807792e-05,
      "loss": 0.9946,
      "step": 3860
    },
    {
      "epoch": 0.24525491935739407,
      "grad_norm": 2.172375202178955,
      "learning_rate": 2.760711261642676e-05,
      "loss": 0.96,
      "step": 3870
    },
    {
      "epoch": 0.24588865299914445,
      "grad_norm": 2.065352439880371,
      "learning_rate": 2.7600762066045725e-05,
      "loss": 1.0211,
      "step": 3880
    },
    {
      "epoch": 0.24652238664089482,
      "grad_norm": 2.4614787101745605,
      "learning_rate": 2.759441151566469e-05,
      "loss": 1.015,
      "step": 3890
    },
    {
      "epoch": 0.2471561202826452,
      "grad_norm": 2.4206457138061523,
      "learning_rate": 2.7588060965283658e-05,
      "loss": 0.9713,
      "step": 3900
    },
    {
      "epoch": 0.24778985392439556,
      "grad_norm": 2.903296709060669,
      "learning_rate": 2.7581710414902624e-05,
      "loss": 1.058,
      "step": 3910
    },
    {
      "epoch": 0.24842358756614594,
      "grad_norm": 2.8012197017669678,
      "learning_rate": 2.7575359864521594e-05,
      "loss": 1.0066,
      "step": 3920
    },
    {
      "epoch": 0.2490573212078963,
      "grad_norm": 2.4113030433654785,
      "learning_rate": 2.756900931414056e-05,
      "loss": 1.011,
      "step": 3930
    },
    {
      "epoch": 0.24969105484964668,
      "grad_norm": 2.0686609745025635,
      "learning_rate": 2.7562658763759523e-05,
      "loss": 0.9598,
      "step": 3940
    },
    {
      "epoch": 0.25032478849139705,
      "grad_norm": 2.1372992992401123,
      "learning_rate": 2.7556308213378493e-05,
      "loss": 0.977,
      "step": 3950
    },
    {
      "epoch": 0.25095852213314745,
      "grad_norm": 2.7274155616760254,
      "learning_rate": 2.754995766299746e-05,
      "loss": 0.9789,
      "step": 3960
    },
    {
      "epoch": 0.2515922557748978,
      "grad_norm": 1.9421849250793457,
      "learning_rate": 2.754360711261643e-05,
      "loss": 0.9793,
      "step": 3970
    },
    {
      "epoch": 0.2522259894166482,
      "grad_norm": 2.4166696071624756,
      "learning_rate": 2.7537256562235396e-05,
      "loss": 0.9985,
      "step": 3980
    },
    {
      "epoch": 0.25285972305839854,
      "grad_norm": 2.3929672241210938,
      "learning_rate": 2.753090601185436e-05,
      "loss": 0.9716,
      "step": 3990
    },
    {
      "epoch": 0.25349345670014894,
      "grad_norm": 2.6565544605255127,
      "learning_rate": 2.752455546147333e-05,
      "loss": 1.0006,
      "step": 4000
    },
    {
      "epoch": 0.2541271903418993,
      "grad_norm": 2.314592123031616,
      "learning_rate": 2.7518204911092295e-05,
      "loss": 0.9646,
      "step": 4010
    },
    {
      "epoch": 0.2547609239836497,
      "grad_norm": 2.0283875465393066,
      "learning_rate": 2.751185436071126e-05,
      "loss": 1.0107,
      "step": 4020
    },
    {
      "epoch": 0.25539465762540003,
      "grad_norm": 2.2830986976623535,
      "learning_rate": 2.750550381033023e-05,
      "loss": 0.9726,
      "step": 4030
    },
    {
      "epoch": 0.25602839126715043,
      "grad_norm": 2.1753225326538086,
      "learning_rate": 2.7499788314987302e-05,
      "loss": 0.9824,
      "step": 4040
    },
    {
      "epoch": 0.2566621249089008,
      "grad_norm": 2.0353281497955322,
      "learning_rate": 2.7493437764606265e-05,
      "loss": 0.9802,
      "step": 4050
    },
    {
      "epoch": 0.2572958585506512,
      "grad_norm": 2.668802261352539,
      "learning_rate": 2.748708721422523e-05,
      "loss": 0.9845,
      "step": 4060
    },
    {
      "epoch": 0.2579295921924015,
      "grad_norm": 2.2359142303466797,
      "learning_rate": 2.74807366638442e-05,
      "loss": 0.956,
      "step": 4070
    },
    {
      "epoch": 0.2585633258341519,
      "grad_norm": 2.1922872066497803,
      "learning_rate": 2.7474386113463168e-05,
      "loss": 0.976,
      "step": 4080
    },
    {
      "epoch": 0.25919705947590227,
      "grad_norm": 2.848691940307617,
      "learning_rate": 2.7468035563082134e-05,
      "loss": 0.9798,
      "step": 4090
    },
    {
      "epoch": 0.25983079311765267,
      "grad_norm": 2.465880870819092,
      "learning_rate": 2.74616850127011e-05,
      "loss": 0.9962,
      "step": 4100
    },
    {
      "epoch": 0.260464526759403,
      "grad_norm": 4.078268051147461,
      "learning_rate": 2.7455334462320067e-05,
      "loss": 0.985,
      "step": 4110
    },
    {
      "epoch": 0.2610982604011534,
      "grad_norm": 2.8106865882873535,
      "learning_rate": 2.7448983911939037e-05,
      "loss": 0.9667,
      "step": 4120
    },
    {
      "epoch": 0.26173199404290376,
      "grad_norm": 2.193296194076538,
      "learning_rate": 2.7442633361558003e-05,
      "loss": 1.0304,
      "step": 4130
    },
    {
      "epoch": 0.26236572768465416,
      "grad_norm": 2.304377317428589,
      "learning_rate": 2.743628281117697e-05,
      "loss": 0.9638,
      "step": 4140
    },
    {
      "epoch": 0.2629994613264045,
      "grad_norm": 2.5293869972229004,
      "learning_rate": 2.742993226079594e-05,
      "loss": 1.006,
      "step": 4150
    },
    {
      "epoch": 0.2636331949681549,
      "grad_norm": 2.482271432876587,
      "learning_rate": 2.7423581710414902e-05,
      "loss": 0.9648,
      "step": 4160
    },
    {
      "epoch": 0.26426692860990525,
      "grad_norm": 2.6014137268066406,
      "learning_rate": 2.741723116003387e-05,
      "loss": 1.0116,
      "step": 4170
    },
    {
      "epoch": 0.26490066225165565,
      "grad_norm": 2.0156843662261963,
      "learning_rate": 2.741088060965284e-05,
      "loss": 1.0412,
      "step": 4180
    },
    {
      "epoch": 0.265534395893406,
      "grad_norm": 1.868124008178711,
      "learning_rate": 2.7404530059271805e-05,
      "loss": 1.0266,
      "step": 4190
    },
    {
      "epoch": 0.2661681295351564,
      "grad_norm": 2.5964508056640625,
      "learning_rate": 2.739817950889077e-05,
      "loss": 1.0054,
      "step": 4200
    },
    {
      "epoch": 0.26680186317690674,
      "grad_norm": 2.343414545059204,
      "learning_rate": 2.7391828958509738e-05,
      "loss": 0.9873,
      "step": 4210
    },
    {
      "epoch": 0.26743559681865714,
      "grad_norm": 2.08260440826416,
      "learning_rate": 2.7385478408128704e-05,
      "loss": 0.9921,
      "step": 4220
    },
    {
      "epoch": 0.2680693304604075,
      "grad_norm": 3.1561474800109863,
      "learning_rate": 2.737912785774767e-05,
      "loss": 1.0004,
      "step": 4230
    },
    {
      "epoch": 0.2687030641021579,
      "grad_norm": 2.497260808944702,
      "learning_rate": 2.737277730736664e-05,
      "loss": 0.9862,
      "step": 4240
    },
    {
      "epoch": 0.26933679774390823,
      "grad_norm": 2.533987045288086,
      "learning_rate": 2.7366426756985607e-05,
      "loss": 0.9578,
      "step": 4250
    },
    {
      "epoch": 0.26997053138565863,
      "grad_norm": 2.3282663822174072,
      "learning_rate": 2.7360076206604573e-05,
      "loss": 1.0222,
      "step": 4260
    },
    {
      "epoch": 0.270604265027409,
      "grad_norm": 2.4627931118011475,
      "learning_rate": 2.735372565622354e-05,
      "loss": 0.9728,
      "step": 4270
    },
    {
      "epoch": 0.2712379986691594,
      "grad_norm": 2.3791842460632324,
      "learning_rate": 2.7347375105842506e-05,
      "loss": 0.9916,
      "step": 4280
    },
    {
      "epoch": 0.2718717323109097,
      "grad_norm": 2.3863041400909424,
      "learning_rate": 2.7341024555461476e-05,
      "loss": 0.9716,
      "step": 4290
    },
    {
      "epoch": 0.2725054659526601,
      "grad_norm": 2.5714287757873535,
      "learning_rate": 2.7334674005080443e-05,
      "loss": 0.9829,
      "step": 4300
    },
    {
      "epoch": 0.27313919959441046,
      "grad_norm": 1.8971589803695679,
      "learning_rate": 2.7328323454699406e-05,
      "loss": 0.9531,
      "step": 4310
    },
    {
      "epoch": 0.27377293323616086,
      "grad_norm": 2.3652894496917725,
      "learning_rate": 2.7321972904318375e-05,
      "loss": 1.0232,
      "step": 4320
    },
    {
      "epoch": 0.2744066668779112,
      "grad_norm": 2.3350908756256104,
      "learning_rate": 2.7315622353937342e-05,
      "loss": 0.9446,
      "step": 4330
    },
    {
      "epoch": 0.2750404005196616,
      "grad_norm": 2.4711291790008545,
      "learning_rate": 2.7309271803556308e-05,
      "loss": 0.9859,
      "step": 4340
    },
    {
      "epoch": 0.27567413416141195,
      "grad_norm": 1.8905357122421265,
      "learning_rate": 2.7302921253175278e-05,
      "loss": 0.9802,
      "step": 4350
    },
    {
      "epoch": 0.27630786780316235,
      "grad_norm": 2.152503490447998,
      "learning_rate": 2.729657070279424e-05,
      "loss": 1.0172,
      "step": 4360
    },
    {
      "epoch": 0.2769416014449127,
      "grad_norm": 2.069955825805664,
      "learning_rate": 2.7290220152413208e-05,
      "loss": 0.9637,
      "step": 4370
    },
    {
      "epoch": 0.2775753350866631,
      "grad_norm": 2.9231443405151367,
      "learning_rate": 2.7283869602032177e-05,
      "loss": 0.9655,
      "step": 4380
    },
    {
      "epoch": 0.27820906872841344,
      "grad_norm": 2.2110438346862793,
      "learning_rate": 2.7277519051651144e-05,
      "loss": 0.9853,
      "step": 4390
    },
    {
      "epoch": 0.27884280237016384,
      "grad_norm": 2.6840262413024902,
      "learning_rate": 2.727116850127011e-05,
      "loss": 0.9893,
      "step": 4400
    },
    {
      "epoch": 0.2794765360119142,
      "grad_norm": 2.8068745136260986,
      "learning_rate": 2.726481795088908e-05,
      "loss": 0.9812,
      "step": 4410
    },
    {
      "epoch": 0.2801102696536646,
      "grad_norm": 2.00561785697937,
      "learning_rate": 2.7258467400508043e-05,
      "loss": 0.9701,
      "step": 4420
    },
    {
      "epoch": 0.28074400329541493,
      "grad_norm": 2.1066250801086426,
      "learning_rate": 2.7252116850127013e-05,
      "loss": 0.9593,
      "step": 4430
    },
    {
      "epoch": 0.28137773693716533,
      "grad_norm": 2.3824000358581543,
      "learning_rate": 2.724576629974598e-05,
      "loss": 1.0155,
      "step": 4440
    },
    {
      "epoch": 0.2820114705789157,
      "grad_norm": 2.494593858718872,
      "learning_rate": 2.7239415749364946e-05,
      "loss": 0.9902,
      "step": 4450
    },
    {
      "epoch": 0.2826452042206661,
      "grad_norm": 4.455418109893799,
      "learning_rate": 2.7233065198983916e-05,
      "loss": 0.9773,
      "step": 4460
    },
    {
      "epoch": 0.2832789378624164,
      "grad_norm": 2.374818801879883,
      "learning_rate": 2.722671464860288e-05,
      "loss": 0.9757,
      "step": 4470
    },
    {
      "epoch": 0.2839126715041668,
      "grad_norm": 2.3214917182922363,
      "learning_rate": 2.7220364098221845e-05,
      "loss": 0.9903,
      "step": 4480
    },
    {
      "epoch": 0.28454640514591717,
      "grad_norm": 2.768280029296875,
      "learning_rate": 2.7214013547840815e-05,
      "loss": 1.0136,
      "step": 4490
    },
    {
      "epoch": 0.28518013878766757,
      "grad_norm": 2.5287153720855713,
      "learning_rate": 2.720766299745978e-05,
      "loss": 0.9847,
      "step": 4500
    },
    {
      "epoch": 0.2858138724294179,
      "grad_norm": 2.5321927070617676,
      "learning_rate": 2.7201312447078748e-05,
      "loss": 0.9684,
      "step": 4510
    },
    {
      "epoch": 0.2864476060711683,
      "grad_norm": 2.255659580230713,
      "learning_rate": 2.7194961896697714e-05,
      "loss": 0.977,
      "step": 4520
    },
    {
      "epoch": 0.28708133971291866,
      "grad_norm": 2.6872975826263428,
      "learning_rate": 2.718861134631668e-05,
      "loss": 1.0042,
      "step": 4530
    },
    {
      "epoch": 0.28771507335466906,
      "grad_norm": 2.101151943206787,
      "learning_rate": 2.7182260795935647e-05,
      "loss": 0.9569,
      "step": 4540
    },
    {
      "epoch": 0.2883488069964194,
      "grad_norm": 2.4000866413116455,
      "learning_rate": 2.7175910245554617e-05,
      "loss": 0.9892,
      "step": 4550
    },
    {
      "epoch": 0.2889825406381698,
      "grad_norm": 2.7204229831695557,
      "learning_rate": 2.7169559695173583e-05,
      "loss": 0.9937,
      "step": 4560
    },
    {
      "epoch": 0.28961627427992015,
      "grad_norm": 2.3743245601654053,
      "learning_rate": 2.7163209144792546e-05,
      "loss": 0.9737,
      "step": 4570
    },
    {
      "epoch": 0.2902500079216705,
      "grad_norm": 2.0039381980895996,
      "learning_rate": 2.7156858594411516e-05,
      "loss": 0.9856,
      "step": 4580
    },
    {
      "epoch": 0.2908837415634209,
      "grad_norm": 2.4298601150512695,
      "learning_rate": 2.7150508044030482e-05,
      "loss": 0.9554,
      "step": 4590
    },
    {
      "epoch": 0.29151747520517124,
      "grad_norm": 2.1136341094970703,
      "learning_rate": 2.7144157493649452e-05,
      "loss": 0.9599,
      "step": 4600
    },
    {
      "epoch": 0.29215120884692164,
      "grad_norm": 2.083838701248169,
      "learning_rate": 2.713780694326842e-05,
      "loss": 0.9749,
      "step": 4610
    },
    {
      "epoch": 0.292784942488672,
      "grad_norm": 2.115694284439087,
      "learning_rate": 2.7131456392887382e-05,
      "loss": 1.0076,
      "step": 4620
    },
    {
      "epoch": 0.2934186761304224,
      "grad_norm": 2.1778616905212402,
      "learning_rate": 2.712510584250635e-05,
      "loss": 1.0089,
      "step": 4630
    },
    {
      "epoch": 0.29405240977217273,
      "grad_norm": 2.1838760375976562,
      "learning_rate": 2.7118755292125318e-05,
      "loss": 0.9915,
      "step": 4640
    },
    {
      "epoch": 0.29468614341392313,
      "grad_norm": 2.033905267715454,
      "learning_rate": 2.7112404741744284e-05,
      "loss": 0.985,
      "step": 4650
    },
    {
      "epoch": 0.2953198770556735,
      "grad_norm": 2.114168405532837,
      "learning_rate": 2.7106054191363254e-05,
      "loss": 1.0211,
      "step": 4660
    },
    {
      "epoch": 0.2959536106974239,
      "grad_norm": 2.4453744888305664,
      "learning_rate": 2.709970364098222e-05,
      "loss": 1.0002,
      "step": 4670
    },
    {
      "epoch": 0.2965873443391742,
      "grad_norm": 2.2126893997192383,
      "learning_rate": 2.7093353090601184e-05,
      "loss": 0.9819,
      "step": 4680
    },
    {
      "epoch": 0.2972210779809246,
      "grad_norm": 2.129840135574341,
      "learning_rate": 2.7087002540220154e-05,
      "loss": 0.9784,
      "step": 4690
    },
    {
      "epoch": 0.29785481162267496,
      "grad_norm": 2.2191386222839355,
      "learning_rate": 2.708065198983912e-05,
      "loss": 0.9817,
      "step": 4700
    },
    {
      "epoch": 0.29848854526442536,
      "grad_norm": 2.50726318359375,
      "learning_rate": 2.7074301439458086e-05,
      "loss": 0.9759,
      "step": 4710
    },
    {
      "epoch": 0.2991222789061757,
      "grad_norm": 2.397005558013916,
      "learning_rate": 2.7067950889077056e-05,
      "loss": 0.9714,
      "step": 4720
    },
    {
      "epoch": 0.2997560125479261,
      "grad_norm": 2.583728075027466,
      "learning_rate": 2.706160033869602e-05,
      "loss": 0.9765,
      "step": 4730
    },
    {
      "epoch": 0.30038974618967645,
      "grad_norm": 2.479814052581787,
      "learning_rate": 2.705524978831499e-05,
      "loss": 0.9664,
      "step": 4740
    },
    {
      "epoch": 0.30102347983142685,
      "grad_norm": 2.2237017154693604,
      "learning_rate": 2.7048899237933955e-05,
      "loss": 0.968,
      "step": 4750
    },
    {
      "epoch": 0.3016572134731772,
      "grad_norm": 2.219372034072876,
      "learning_rate": 2.7042548687552922e-05,
      "loss": 0.9674,
      "step": 4760
    },
    {
      "epoch": 0.3022909471149276,
      "grad_norm": 2.5179896354675293,
      "learning_rate": 2.7036198137171892e-05,
      "loss": 1.0244,
      "step": 4770
    },
    {
      "epoch": 0.30292468075667794,
      "grad_norm": 2.0227856636047363,
      "learning_rate": 2.7029847586790855e-05,
      "loss": 0.9809,
      "step": 4780
    },
    {
      "epoch": 0.30355841439842834,
      "grad_norm": 2.179152488708496,
      "learning_rate": 2.702349703640982e-05,
      "loss": 0.9986,
      "step": 4790
    },
    {
      "epoch": 0.3041921480401787,
      "grad_norm": 1.9788833856582642,
      "learning_rate": 2.701714648602879e-05,
      "loss": 1.0,
      "step": 4800
    },
    {
      "epoch": 0.3048258816819291,
      "grad_norm": 2.0417261123657227,
      "learning_rate": 2.7010795935647757e-05,
      "loss": 0.9503,
      "step": 4810
    },
    {
      "epoch": 0.30545961532367943,
      "grad_norm": 2.280271053314209,
      "learning_rate": 2.7004445385266724e-05,
      "loss": 0.9776,
      "step": 4820
    },
    {
      "epoch": 0.30609334896542983,
      "grad_norm": 2.8256092071533203,
      "learning_rate": 2.699809483488569e-05,
      "loss": 0.9979,
      "step": 4830
    },
    {
      "epoch": 0.3067270826071802,
      "grad_norm": 2.0850412845611572,
      "learning_rate": 2.6991744284504657e-05,
      "loss": 0.9639,
      "step": 4840
    },
    {
      "epoch": 0.3073608162489306,
      "grad_norm": 2.646437883377075,
      "learning_rate": 2.6985393734123623e-05,
      "loss": 1.005,
      "step": 4850
    },
    {
      "epoch": 0.3079945498906809,
      "grad_norm": 2.949681282043457,
      "learning_rate": 2.6979043183742593e-05,
      "loss": 0.9974,
      "step": 4860
    },
    {
      "epoch": 0.3086282835324313,
      "grad_norm": 2.350914478302002,
      "learning_rate": 2.697269263336156e-05,
      "loss": 1.0057,
      "step": 4870
    },
    {
      "epoch": 0.30926201717418167,
      "grad_norm": 2.490952730178833,
      "learning_rate": 2.6966342082980522e-05,
      "loss": 1.0388,
      "step": 4880
    },
    {
      "epoch": 0.30989575081593207,
      "grad_norm": 2.136080741882324,
      "learning_rate": 2.6959991532599492e-05,
      "loss": 0.9756,
      "step": 4890
    },
    {
      "epoch": 0.3105294844576824,
      "grad_norm": 2.738816261291504,
      "learning_rate": 2.695364098221846e-05,
      "loss": 0.9866,
      "step": 4900
    },
    {
      "epoch": 0.3111632180994328,
      "grad_norm": 2.271932363510132,
      "learning_rate": 2.694729043183743e-05,
      "loss": 0.9722,
      "step": 4910
    },
    {
      "epoch": 0.31179695174118316,
      "grad_norm": 2.4065098762512207,
      "learning_rate": 2.6940939881456395e-05,
      "loss": 0.9933,
      "step": 4920
    },
    {
      "epoch": 0.31243068538293356,
      "grad_norm": 2.6142311096191406,
      "learning_rate": 2.693458933107536e-05,
      "loss": 0.9756,
      "step": 4930
    },
    {
      "epoch": 0.3130644190246839,
      "grad_norm": 2.231361150741577,
      "learning_rate": 2.6928238780694328e-05,
      "loss": 1.0374,
      "step": 4940
    },
    {
      "epoch": 0.3136981526664343,
      "grad_norm": 2.5703837871551514,
      "learning_rate": 2.6921888230313294e-05,
      "loss": 0.9856,
      "step": 4950
    },
    {
      "epoch": 0.31433188630818465,
      "grad_norm": 2.273010730743408,
      "learning_rate": 2.691553767993226e-05,
      "loss": 0.9473,
      "step": 4960
    },
    {
      "epoch": 0.31496561994993505,
      "grad_norm": 2.100154161453247,
      "learning_rate": 2.690918712955123e-05,
      "loss": 0.9999,
      "step": 4970
    },
    {
      "epoch": 0.3155993535916854,
      "grad_norm": 2.221238613128662,
      "learning_rate": 2.6902836579170197e-05,
      "loss": 0.9881,
      "step": 4980
    },
    {
      "epoch": 0.3162330872334358,
      "grad_norm": 2.129283905029297,
      "learning_rate": 2.689648602878916e-05,
      "loss": 0.9997,
      "step": 4990
    },
    {
      "epoch": 0.31686682087518614,
      "grad_norm": 2.210181474685669,
      "learning_rate": 2.689013547840813e-05,
      "loss": 0.954,
      "step": 5000
    },
    {
      "epoch": 0.31750055451693654,
      "grad_norm": 2.6119396686553955,
      "learning_rate": 2.6883784928027096e-05,
      "loss": 0.951,
      "step": 5010
    },
    {
      "epoch": 0.3181342881586869,
      "grad_norm": 2.083984375,
      "learning_rate": 2.6877434377646063e-05,
      "loss": 0.9688,
      "step": 5020
    },
    {
      "epoch": 0.3187680218004373,
      "grad_norm": 2.7602462768554688,
      "learning_rate": 2.6871083827265032e-05,
      "loss": 0.9952,
      "step": 5030
    },
    {
      "epoch": 0.31940175544218763,
      "grad_norm": 2.387117385864258,
      "learning_rate": 2.6864733276883995e-05,
      "loss": 0.9989,
      "step": 5040
    },
    {
      "epoch": 0.32003548908393803,
      "grad_norm": 2.4308533668518066,
      "learning_rate": 2.6858382726502965e-05,
      "loss": 1.0259,
      "step": 5050
    },
    {
      "epoch": 0.3206692227256884,
      "grad_norm": 2.4051294326782227,
      "learning_rate": 2.685203217612193e-05,
      "loss": 0.9788,
      "step": 5060
    },
    {
      "epoch": 0.3213029563674388,
      "grad_norm": 2.3584930896759033,
      "learning_rate": 2.6845681625740898e-05,
      "loss": 0.9575,
      "step": 5070
    },
    {
      "epoch": 0.3219366900091891,
      "grad_norm": 2.0702648162841797,
      "learning_rate": 2.6839331075359868e-05,
      "loss": 0.9455,
      "step": 5080
    },
    {
      "epoch": 0.3225704236509395,
      "grad_norm": 2.706589937210083,
      "learning_rate": 2.683298052497883e-05,
      "loss": 0.985,
      "step": 5090
    },
    {
      "epoch": 0.32320415729268986,
      "grad_norm": 2.910306215286255,
      "learning_rate": 2.6826629974597797e-05,
      "loss": 0.9601,
      "step": 5100
    },
    {
      "epoch": 0.32383789093444026,
      "grad_norm": 2.5598220825195312,
      "learning_rate": 2.6820279424216767e-05,
      "loss": 1.0027,
      "step": 5110
    },
    {
      "epoch": 0.3244716245761906,
      "grad_norm": 2.0851387977600098,
      "learning_rate": 2.6813928873835734e-05,
      "loss": 0.9824,
      "step": 5120
    },
    {
      "epoch": 0.325105358217941,
      "grad_norm": 2.4126203060150146,
      "learning_rate": 2.68075783234547e-05,
      "loss": 0.9983,
      "step": 5130
    },
    {
      "epoch": 0.32573909185969135,
      "grad_norm": 2.2933716773986816,
      "learning_rate": 2.6801227773073666e-05,
      "loss": 0.9611,
      "step": 5140
    },
    {
      "epoch": 0.32637282550144175,
      "grad_norm": 2.0498604774475098,
      "learning_rate": 2.6794877222692633e-05,
      "loss": 0.979,
      "step": 5150
    },
    {
      "epoch": 0.3270065591431921,
      "grad_norm": 2.3225576877593994,
      "learning_rate": 2.67885266723116e-05,
      "loss": 0.9921,
      "step": 5160
    },
    {
      "epoch": 0.3276402927849425,
      "grad_norm": 2.2657036781311035,
      "learning_rate": 2.678217612193057e-05,
      "loss": 1.0007,
      "step": 5170
    },
    {
      "epoch": 0.32827402642669284,
      "grad_norm": 3.0919318199157715,
      "learning_rate": 2.6775825571549536e-05,
      "loss": 0.9798,
      "step": 5180
    },
    {
      "epoch": 0.32890776006844324,
      "grad_norm": 2.391502618789673,
      "learning_rate": 2.6769475021168502e-05,
      "loss": 0.9617,
      "step": 5190
    },
    {
      "epoch": 0.3295414937101936,
      "grad_norm": 2.5995659828186035,
      "learning_rate": 2.676312447078747e-05,
      "loss": 1.0121,
      "step": 5200
    },
    {
      "epoch": 0.330175227351944,
      "grad_norm": 2.2573728561401367,
      "learning_rate": 2.6756773920406435e-05,
      "loss": 0.9946,
      "step": 5210
    },
    {
      "epoch": 0.33080896099369433,
      "grad_norm": 2.1184000968933105,
      "learning_rate": 2.6750423370025405e-05,
      "loss": 1.0086,
      "step": 5220
    },
    {
      "epoch": 0.33144269463544473,
      "grad_norm": 2.099074125289917,
      "learning_rate": 2.674407281964437e-05,
      "loss": 0.933,
      "step": 5230
    },
    {
      "epoch": 0.3320764282771951,
      "grad_norm": 2.507707118988037,
      "learning_rate": 2.6737722269263337e-05,
      "loss": 0.9689,
      "step": 5240
    },
    {
      "epoch": 0.3327101619189455,
      "grad_norm": 2.0602450370788574,
      "learning_rate": 2.6731371718882304e-05,
      "loss": 0.9531,
      "step": 5250
    },
    {
      "epoch": 0.3333438955606958,
      "grad_norm": 2.2188332080841064,
      "learning_rate": 2.672502116850127e-05,
      "loss": 1.0134,
      "step": 5260
    },
    {
      "epoch": 0.3339776292024462,
      "grad_norm": 2.154158353805542,
      "learning_rate": 2.6718670618120237e-05,
      "loss": 0.9935,
      "step": 5270
    },
    {
      "epoch": 0.33461136284419657,
      "grad_norm": 2.358680486679077,
      "learning_rate": 2.6712320067739207e-05,
      "loss": 0.9904,
      "step": 5280
    },
    {
      "epoch": 0.33524509648594697,
      "grad_norm": 2.3714752197265625,
      "learning_rate": 2.6705969517358173e-05,
      "loss": 0.9702,
      "step": 5290
    },
    {
      "epoch": 0.3358788301276973,
      "grad_norm": 2.183443307876587,
      "learning_rate": 2.6699618966977136e-05,
      "loss": 0.9814,
      "step": 5300
    },
    {
      "epoch": 0.3365125637694477,
      "grad_norm": 2.585148334503174,
      "learning_rate": 2.6693268416596106e-05,
      "loss": 0.9602,
      "step": 5310
    },
    {
      "epoch": 0.33714629741119806,
      "grad_norm": 2.1341023445129395,
      "learning_rate": 2.6686917866215072e-05,
      "loss": 0.9803,
      "step": 5320
    },
    {
      "epoch": 0.33778003105294846,
      "grad_norm": 2.952950954437256,
      "learning_rate": 2.668056731583404e-05,
      "loss": 0.9912,
      "step": 5330
    },
    {
      "epoch": 0.3384137646946988,
      "grad_norm": 1.9648027420043945,
      "learning_rate": 2.667421676545301e-05,
      "loss": 0.9821,
      "step": 5340
    },
    {
      "epoch": 0.3390474983364492,
      "grad_norm": 2.412372589111328,
      "learning_rate": 2.666786621507197e-05,
      "loss": 0.9329,
      "step": 5350
    },
    {
      "epoch": 0.33968123197819955,
      "grad_norm": 2.1162233352661133,
      "learning_rate": 2.666151566469094e-05,
      "loss": 0.9524,
      "step": 5360
    },
    {
      "epoch": 0.34031496561994995,
      "grad_norm": 2.759885787963867,
      "learning_rate": 2.6655165114309908e-05,
      "loss": 0.9729,
      "step": 5370
    },
    {
      "epoch": 0.3409486992617003,
      "grad_norm": 2.712097406387329,
      "learning_rate": 2.6648814563928874e-05,
      "loss": 1.0169,
      "step": 5380
    },
    {
      "epoch": 0.3415824329034507,
      "grad_norm": 1.9833990335464478,
      "learning_rate": 2.6642464013547844e-05,
      "loss": 0.9521,
      "step": 5390
    },
    {
      "epoch": 0.34221616654520104,
      "grad_norm": 2.1856935024261475,
      "learning_rate": 2.6636113463166807e-05,
      "loss": 0.9816,
      "step": 5400
    },
    {
      "epoch": 0.34284990018695144,
      "grad_norm": 2.630599021911621,
      "learning_rate": 2.6629762912785774e-05,
      "loss": 0.9782,
      "step": 5410
    },
    {
      "epoch": 0.3434836338287018,
      "grad_norm": 2.236259698867798,
      "learning_rate": 2.6623412362404743e-05,
      "loss": 1.0193,
      "step": 5420
    },
    {
      "epoch": 0.3441173674704522,
      "grad_norm": 2.3419270515441895,
      "learning_rate": 2.661706181202371e-05,
      "loss": 0.9384,
      "step": 5430
    },
    {
      "epoch": 0.34475110111220253,
      "grad_norm": 1.9995434284210205,
      "learning_rate": 2.6610711261642676e-05,
      "loss": 0.9782,
      "step": 5440
    },
    {
      "epoch": 0.34538483475395293,
      "grad_norm": 2.515993356704712,
      "learning_rate": 2.6604360711261646e-05,
      "loss": 0.9895,
      "step": 5450
    },
    {
      "epoch": 0.3460185683957033,
      "grad_norm": 2.5864388942718506,
      "learning_rate": 2.659801016088061e-05,
      "loss": 1.0147,
      "step": 5460
    },
    {
      "epoch": 0.3466523020374537,
      "grad_norm": 2.553481101989746,
      "learning_rate": 2.6591659610499575e-05,
      "loss": 0.9985,
      "step": 5470
    },
    {
      "epoch": 0.347286035679204,
      "grad_norm": 2.193030595779419,
      "learning_rate": 2.6585309060118545e-05,
      "loss": 0.9896,
      "step": 5480
    },
    {
      "epoch": 0.3479197693209544,
      "grad_norm": 2.0924606323242188,
      "learning_rate": 2.6578958509737512e-05,
      "loss": 0.9425,
      "step": 5490
    },
    {
      "epoch": 0.34855350296270476,
      "grad_norm": 2.1207547187805176,
      "learning_rate": 2.6572607959356478e-05,
      "loss": 1.0401,
      "step": 5500
    },
    {
      "epoch": 0.34918723660445516,
      "grad_norm": 2.2998645305633545,
      "learning_rate": 2.6566257408975445e-05,
      "loss": 0.9838,
      "step": 5510
    },
    {
      "epoch": 0.3498209702462055,
      "grad_norm": 2.48590350151062,
      "learning_rate": 2.655990685859441e-05,
      "loss": 1.0326,
      "step": 5520
    },
    {
      "epoch": 0.3504547038879559,
      "grad_norm": 2.5302016735076904,
      "learning_rate": 2.655355630821338e-05,
      "loss": 0.9616,
      "step": 5530
    },
    {
      "epoch": 0.35108843752970625,
      "grad_norm": 2.4432337284088135,
      "learning_rate": 2.6547205757832347e-05,
      "loss": 0.9928,
      "step": 5540
    },
    {
      "epoch": 0.35172217117145665,
      "grad_norm": 2.252192258834839,
      "learning_rate": 2.6540855207451314e-05,
      "loss": 0.9654,
      "step": 5550
    },
    {
      "epoch": 0.352355904813207,
      "grad_norm": 2.0626137256622314,
      "learning_rate": 2.653450465707028e-05,
      "loss": 1.0056,
      "step": 5560
    },
    {
      "epoch": 0.3529896384549574,
      "grad_norm": 2.3848471641540527,
      "learning_rate": 2.6528154106689247e-05,
      "loss": 1.0032,
      "step": 5570
    },
    {
      "epoch": 0.35362337209670774,
      "grad_norm": 2.2800145149230957,
      "learning_rate": 2.6521803556308213e-05,
      "loss": 0.9807,
      "step": 5580
    },
    {
      "epoch": 0.35425710573845814,
      "grad_norm": 3.3787589073181152,
      "learning_rate": 2.6515453005927183e-05,
      "loss": 0.9884,
      "step": 5590
    },
    {
      "epoch": 0.3548908393802085,
      "grad_norm": 2.0658695697784424,
      "learning_rate": 2.650910245554615e-05,
      "loss": 0.9838,
      "step": 5600
    },
    {
      "epoch": 0.3555245730219589,
      "grad_norm": 2.1459081172943115,
      "learning_rate": 2.6502751905165112e-05,
      "loss": 0.9714,
      "step": 5610
    },
    {
      "epoch": 0.35615830666370923,
      "grad_norm": 2.1199002265930176,
      "learning_rate": 2.6496401354784082e-05,
      "loss": 0.9642,
      "step": 5620
    },
    {
      "epoch": 0.35679204030545963,
      "grad_norm": 2.487696647644043,
      "learning_rate": 2.649005080440305e-05,
      "loss": 1.0063,
      "step": 5630
    },
    {
      "epoch": 0.35742577394721,
      "grad_norm": 2.2940542697906494,
      "learning_rate": 2.6483700254022015e-05,
      "loss": 1.0081,
      "step": 5640
    },
    {
      "epoch": 0.3580595075889604,
      "grad_norm": 2.1990561485290527,
      "learning_rate": 2.6477349703640985e-05,
      "loss": 0.9849,
      "step": 5650
    },
    {
      "epoch": 0.3586932412307107,
      "grad_norm": 2.6023218631744385,
      "learning_rate": 2.6470999153259948e-05,
      "loss": 0.959,
      "step": 5660
    },
    {
      "epoch": 0.3593269748724611,
      "grad_norm": 2.2632346153259277,
      "learning_rate": 2.6464648602878914e-05,
      "loss": 0.9185,
      "step": 5670
    },
    {
      "epoch": 0.35996070851421147,
      "grad_norm": 2.2142679691314697,
      "learning_rate": 2.6458298052497884e-05,
      "loss": 0.96,
      "step": 5680
    },
    {
      "epoch": 0.36059444215596187,
      "grad_norm": 2.4688761234283447,
      "learning_rate": 2.645194750211685e-05,
      "loss": 1.0063,
      "step": 5690
    },
    {
      "epoch": 0.3612281757977122,
      "grad_norm": 3.3397998809814453,
      "learning_rate": 2.644559695173582e-05,
      "loss": 0.9828,
      "step": 5700
    },
    {
      "epoch": 0.3618619094394626,
      "grad_norm": 2.9049816131591797,
      "learning_rate": 2.6439246401354787e-05,
      "loss": 1.0085,
      "step": 5710
    },
    {
      "epoch": 0.36249564308121296,
      "grad_norm": 2.587655782699585,
      "learning_rate": 2.643289585097375e-05,
      "loss": 0.9738,
      "step": 5720
    },
    {
      "epoch": 0.36312937672296336,
      "grad_norm": 2.3533780574798584,
      "learning_rate": 2.642654530059272e-05,
      "loss": 0.9912,
      "step": 5730
    },
    {
      "epoch": 0.3637631103647137,
      "grad_norm": 2.5258591175079346,
      "learning_rate": 2.6420194750211686e-05,
      "loss": 0.9583,
      "step": 5740
    },
    {
      "epoch": 0.3643968440064641,
      "grad_norm": 2.355693817138672,
      "learning_rate": 2.6413844199830652e-05,
      "loss": 0.9961,
      "step": 5750
    },
    {
      "epoch": 0.36503057764821445,
      "grad_norm": 2.1473240852355957,
      "learning_rate": 2.6407493649449622e-05,
      "loss": 0.9625,
      "step": 5760
    },
    {
      "epoch": 0.36566431128996485,
      "grad_norm": 2.167174816131592,
      "learning_rate": 2.6401143099068585e-05,
      "loss": 0.9823,
      "step": 5770
    },
    {
      "epoch": 0.3662980449317152,
      "grad_norm": 2.056626796722412,
      "learning_rate": 2.639479254868755e-05,
      "loss": 1.0084,
      "step": 5780
    },
    {
      "epoch": 0.3669317785734656,
      "grad_norm": 2.263904094696045,
      "learning_rate": 2.638844199830652e-05,
      "loss": 0.9556,
      "step": 5790
    },
    {
      "epoch": 0.36756551221521594,
      "grad_norm": 2.5295770168304443,
      "learning_rate": 2.6382091447925488e-05,
      "loss": 0.9504,
      "step": 5800
    },
    {
      "epoch": 0.36819924585696634,
      "grad_norm": 2.4721922874450684,
      "learning_rate": 2.6375740897544454e-05,
      "loss": 0.9703,
      "step": 5810
    },
    {
      "epoch": 0.3688329794987167,
      "grad_norm": 2.698338747024536,
      "learning_rate": 2.636939034716342e-05,
      "loss": 1.0085,
      "step": 5820
    },
    {
      "epoch": 0.3694667131404671,
      "grad_norm": 2.630523204803467,
      "learning_rate": 2.6363039796782387e-05,
      "loss": 0.9833,
      "step": 5830
    },
    {
      "epoch": 0.37010044678221743,
      "grad_norm": 2.4700939655303955,
      "learning_rate": 2.6356689246401357e-05,
      "loss": 1.0091,
      "step": 5840
    },
    {
      "epoch": 0.37073418042396783,
      "grad_norm": 2.226928949356079,
      "learning_rate": 2.6350338696020323e-05,
      "loss": 1.0396,
      "step": 5850
    },
    {
      "epoch": 0.3713679140657182,
      "grad_norm": 2.416705846786499,
      "learning_rate": 2.634398814563929e-05,
      "loss": 0.9469,
      "step": 5860
    },
    {
      "epoch": 0.3720016477074686,
      "grad_norm": 2.2262980937957764,
      "learning_rate": 2.6337637595258256e-05,
      "loss": 0.975,
      "step": 5870
    },
    {
      "epoch": 0.3726353813492189,
      "grad_norm": 2.1909263134002686,
      "learning_rate": 2.6331287044877223e-05,
      "loss": 0.9787,
      "step": 5880
    },
    {
      "epoch": 0.3732691149909693,
      "grad_norm": 2.7040016651153564,
      "learning_rate": 2.632493649449619e-05,
      "loss": 0.945,
      "step": 5890
    },
    {
      "epoch": 0.37390284863271966,
      "grad_norm": 2.4241106510162354,
      "learning_rate": 2.631858594411516e-05,
      "loss": 0.9885,
      "step": 5900
    },
    {
      "epoch": 0.37453658227447006,
      "grad_norm": 2.5355191230773926,
      "learning_rate": 2.6312235393734125e-05,
      "loss": 0.9555,
      "step": 5910
    },
    {
      "epoch": 0.3751703159162204,
      "grad_norm": 2.4981067180633545,
      "learning_rate": 2.630588484335309e-05,
      "loss": 0.9707,
      "step": 5920
    },
    {
      "epoch": 0.3758040495579708,
      "grad_norm": 2.410797595977783,
      "learning_rate": 2.6299534292972058e-05,
      "loss": 0.9498,
      "step": 5930
    },
    {
      "epoch": 0.37643778319972115,
      "grad_norm": 2.2229530811309814,
      "learning_rate": 2.6293183742591025e-05,
      "loss": 0.9622,
      "step": 5940
    },
    {
      "epoch": 0.37707151684147155,
      "grad_norm": 3.1553850173950195,
      "learning_rate": 2.628683319220999e-05,
      "loss": 1.0272,
      "step": 5950
    },
    {
      "epoch": 0.3777052504832219,
      "grad_norm": 2.428607225418091,
      "learning_rate": 2.628048264182896e-05,
      "loss": 0.9522,
      "step": 5960
    },
    {
      "epoch": 0.3783389841249723,
      "grad_norm": 2.739508628845215,
      "learning_rate": 2.6274132091447927e-05,
      "loss": 0.9749,
      "step": 5970
    },
    {
      "epoch": 0.37897271776672264,
      "grad_norm": 2.2851216793060303,
      "learning_rate": 2.626778154106689e-05,
      "loss": 0.9795,
      "step": 5980
    },
    {
      "epoch": 0.37960645140847304,
      "grad_norm": 2.146599531173706,
      "learning_rate": 2.626143099068586e-05,
      "loss": 0.978,
      "step": 5990
    },
    {
      "epoch": 0.3802401850502234,
      "grad_norm": 2.687757968902588,
      "learning_rate": 2.6255080440304827e-05,
      "loss": 0.9615,
      "step": 6000
    },
    {
      "epoch": 0.3808739186919738,
      "grad_norm": 2.401304006576538,
      "learning_rate": 2.6248729889923796e-05,
      "loss": 0.9319,
      "step": 6010
    },
    {
      "epoch": 0.38150765233372413,
      "grad_norm": 2.6312096118927,
      "learning_rate": 2.6242379339542763e-05,
      "loss": 0.9924,
      "step": 6020
    },
    {
      "epoch": 0.38214138597547453,
      "grad_norm": 2.520496368408203,
      "learning_rate": 2.6236028789161726e-05,
      "loss": 1.0026,
      "step": 6030
    },
    {
      "epoch": 0.3827751196172249,
      "grad_norm": 2.4763574600219727,
      "learning_rate": 2.6229678238780696e-05,
      "loss": 0.9557,
      "step": 6040
    },
    {
      "epoch": 0.3834088532589753,
      "grad_norm": 2.0288970470428467,
      "learning_rate": 2.6223327688399662e-05,
      "loss": 0.9498,
      "step": 6050
    },
    {
      "epoch": 0.3840425869007256,
      "grad_norm": 2.3276567459106445,
      "learning_rate": 2.621697713801863e-05,
      "loss": 0.9589,
      "step": 6060
    },
    {
      "epoch": 0.384676320542476,
      "grad_norm": 3.0233349800109863,
      "learning_rate": 2.62106265876376e-05,
      "loss": 1.0085,
      "step": 6070
    },
    {
      "epoch": 0.38531005418422637,
      "grad_norm": 2.2996909618377686,
      "learning_rate": 2.620427603725656e-05,
      "loss": 1.008,
      "step": 6080
    },
    {
      "epoch": 0.38594378782597677,
      "grad_norm": 2.214984178543091,
      "learning_rate": 2.6197925486875528e-05,
      "loss": 0.9464,
      "step": 6090
    },
    {
      "epoch": 0.3865775214677271,
      "grad_norm": 2.122485637664795,
      "learning_rate": 2.6191574936494498e-05,
      "loss": 1.0011,
      "step": 6100
    },
    {
      "epoch": 0.38721125510947746,
      "grad_norm": 2.1969258785247803,
      "learning_rate": 2.6185224386113464e-05,
      "loss": 0.9677,
      "step": 6110
    },
    {
      "epoch": 0.38784498875122786,
      "grad_norm": 2.2543444633483887,
      "learning_rate": 2.617887383573243e-05,
      "loss": 1.004,
      "step": 6120
    },
    {
      "epoch": 0.3884787223929782,
      "grad_norm": 2.36507248878479,
      "learning_rate": 2.6172523285351397e-05,
      "loss": 1.0133,
      "step": 6130
    },
    {
      "epoch": 0.3891124560347286,
      "grad_norm": 2.3757476806640625,
      "learning_rate": 2.6166172734970363e-05,
      "loss": 0.996,
      "step": 6140
    },
    {
      "epoch": 0.38974618967647895,
      "grad_norm": 2.559335470199585,
      "learning_rate": 2.6159822184589333e-05,
      "loss": 0.969,
      "step": 6150
    },
    {
      "epoch": 0.39037992331822935,
      "grad_norm": 2.4770450592041016,
      "learning_rate": 2.61534716342083e-05,
      "loss": 0.97,
      "step": 6160
    },
    {
      "epoch": 0.3910136569599797,
      "grad_norm": 2.0932118892669678,
      "learning_rate": 2.6147121083827266e-05,
      "loss": 0.9679,
      "step": 6170
    },
    {
      "epoch": 0.3916473906017301,
      "grad_norm": 2.3895809650421143,
      "learning_rate": 2.6140770533446232e-05,
      "loss": 0.9621,
      "step": 6180
    },
    {
      "epoch": 0.39228112424348044,
      "grad_norm": 2.157581090927124,
      "learning_rate": 2.61344199830652e-05,
      "loss": 0.9749,
      "step": 6190
    },
    {
      "epoch": 0.39291485788523084,
      "grad_norm": 2.647714138031006,
      "learning_rate": 2.6128069432684165e-05,
      "loss": 0.9582,
      "step": 6200
    },
    {
      "epoch": 0.3935485915269812,
      "grad_norm": 2.549042224884033,
      "learning_rate": 2.6121718882303135e-05,
      "loss": 0.9924,
      "step": 6210
    },
    {
      "epoch": 0.3941823251687316,
      "grad_norm": 2.05676531791687,
      "learning_rate": 2.61153683319221e-05,
      "loss": 0.9777,
      "step": 6220
    },
    {
      "epoch": 0.39481605881048193,
      "grad_norm": 2.597990036010742,
      "learning_rate": 2.6109017781541068e-05,
      "loss": 0.9879,
      "step": 6230
    },
    {
      "epoch": 0.39544979245223233,
      "grad_norm": 2.5945181846618652,
      "learning_rate": 2.6102667231160034e-05,
      "loss": 1.0187,
      "step": 6240
    },
    {
      "epoch": 0.3960835260939827,
      "grad_norm": 2.2339704036712646,
      "learning_rate": 2.6096316680779e-05,
      "loss": 0.9442,
      "step": 6250
    },
    {
      "epoch": 0.3967172597357331,
      "grad_norm": 2.1201157569885254,
      "learning_rate": 2.6089966130397967e-05,
      "loss": 0.9685,
      "step": 6260
    },
    {
      "epoch": 0.3973509933774834,
      "grad_norm": 2.1350791454315186,
      "learning_rate": 2.6083615580016937e-05,
      "loss": 0.9331,
      "step": 6270
    },
    {
      "epoch": 0.3979847270192338,
      "grad_norm": 2.2396140098571777,
      "learning_rate": 2.6077265029635903e-05,
      "loss": 0.9776,
      "step": 6280
    },
    {
      "epoch": 0.39861846066098416,
      "grad_norm": 1.993811011314392,
      "learning_rate": 2.6070914479254867e-05,
      "loss": 0.9339,
      "step": 6290
    },
    {
      "epoch": 0.39925219430273456,
      "grad_norm": 2.112387180328369,
      "learning_rate": 2.6064563928873836e-05,
      "loss": 0.9657,
      "step": 6300
    },
    {
      "epoch": 0.3998859279444849,
      "grad_norm": 2.3971803188323975,
      "learning_rate": 2.6058213378492803e-05,
      "loss": 0.9827,
      "step": 6310
    },
    {
      "epoch": 0.4005196615862353,
      "grad_norm": 2.601036548614502,
      "learning_rate": 2.6051862828111773e-05,
      "loss": 0.9569,
      "step": 6320
    },
    {
      "epoch": 0.40115339522798565,
      "grad_norm": 2.7897446155548096,
      "learning_rate": 2.604551227773074e-05,
      "loss": 0.961,
      "step": 6330
    },
    {
      "epoch": 0.40178712886973605,
      "grad_norm": 2.0838623046875,
      "learning_rate": 2.6039161727349702e-05,
      "loss": 0.9432,
      "step": 6340
    },
    {
      "epoch": 0.4024208625114864,
      "grad_norm": 2.9719061851501465,
      "learning_rate": 2.6032811176968672e-05,
      "loss": 1.0083,
      "step": 6350
    },
    {
      "epoch": 0.4030545961532368,
      "grad_norm": 2.1705613136291504,
      "learning_rate": 2.6026460626587638e-05,
      "loss": 0.9412,
      "step": 6360
    },
    {
      "epoch": 0.40368832979498714,
      "grad_norm": 2.8509955406188965,
      "learning_rate": 2.6020110076206605e-05,
      "loss": 0.9794,
      "step": 6370
    },
    {
      "epoch": 0.40432206343673754,
      "grad_norm": 2.5521292686462402,
      "learning_rate": 2.6013759525825575e-05,
      "loss": 0.961,
      "step": 6380
    },
    {
      "epoch": 0.4049557970784879,
      "grad_norm": 2.393019914627075,
      "learning_rate": 2.6007408975444538e-05,
      "loss": 0.9613,
      "step": 6390
    },
    {
      "epoch": 0.4055895307202383,
      "grad_norm": 2.1284282207489014,
      "learning_rate": 2.6001058425063504e-05,
      "loss": 0.9934,
      "step": 6400
    },
    {
      "epoch": 0.40622326436198863,
      "grad_norm": 2.5337812900543213,
      "learning_rate": 2.5994707874682474e-05,
      "loss": 0.9654,
      "step": 6410
    },
    {
      "epoch": 0.40685699800373903,
      "grad_norm": 2.4096899032592773,
      "learning_rate": 2.598835732430144e-05,
      "loss": 0.9831,
      "step": 6420
    },
    {
      "epoch": 0.4074907316454894,
      "grad_norm": 2.4304840564727783,
      "learning_rate": 2.5982006773920407e-05,
      "loss": 0.9521,
      "step": 6430
    },
    {
      "epoch": 0.4081244652872398,
      "grad_norm": 2.3095219135284424,
      "learning_rate": 2.5975656223539373e-05,
      "loss": 0.9566,
      "step": 6440
    },
    {
      "epoch": 0.4087581989289901,
      "grad_norm": 2.433082103729248,
      "learning_rate": 2.596930567315834e-05,
      "loss": 0.9241,
      "step": 6450
    },
    {
      "epoch": 0.4093919325707405,
      "grad_norm": 2.465911388397217,
      "learning_rate": 2.596295512277731e-05,
      "loss": 1.0152,
      "step": 6460
    },
    {
      "epoch": 0.41002566621249087,
      "grad_norm": 2.175802707672119,
      "learning_rate": 2.5956604572396276e-05,
      "loss": 0.9414,
      "step": 6470
    },
    {
      "epoch": 0.41065939985424127,
      "grad_norm": 2.319173812866211,
      "learning_rate": 2.5950254022015242e-05,
      "loss": 0.9612,
      "step": 6480
    },
    {
      "epoch": 0.4112931334959916,
      "grad_norm": 2.46710467338562,
      "learning_rate": 2.5943903471634212e-05,
      "loss": 0.9803,
      "step": 6490
    },
    {
      "epoch": 0.411926867137742,
      "grad_norm": 2.2159438133239746,
      "learning_rate": 2.5937552921253175e-05,
      "loss": 0.9694,
      "step": 6500
    },
    {
      "epoch": 0.41256060077949236,
      "grad_norm": 2.4501302242279053,
      "learning_rate": 2.593120237087214e-05,
      "loss": 0.978,
      "step": 6510
    },
    {
      "epoch": 0.41319433442124276,
      "grad_norm": 2.518714666366577,
      "learning_rate": 2.592485182049111e-05,
      "loss": 1.0192,
      "step": 6520
    },
    {
      "epoch": 0.4138280680629931,
      "grad_norm": 2.1535067558288574,
      "learning_rate": 2.5918501270110078e-05,
      "loss": 0.9884,
      "step": 6530
    },
    {
      "epoch": 0.4144618017047435,
      "grad_norm": 2.346169948577881,
      "learning_rate": 2.5912150719729044e-05,
      "loss": 1.013,
      "step": 6540
    },
    {
      "epoch": 0.41509553534649385,
      "grad_norm": 2.1694605350494385,
      "learning_rate": 2.590580016934801e-05,
      "loss": 0.9822,
      "step": 6550
    },
    {
      "epoch": 0.41572926898824425,
      "grad_norm": 2.454122304916382,
      "learning_rate": 2.5899449618966977e-05,
      "loss": 0.9843,
      "step": 6560
    },
    {
      "epoch": 0.4163630026299946,
      "grad_norm": 2.5382235050201416,
      "learning_rate": 2.5893099068585943e-05,
      "loss": 0.9603,
      "step": 6570
    },
    {
      "epoch": 0.416996736271745,
      "grad_norm": 2.427191972732544,
      "learning_rate": 2.5886748518204913e-05,
      "loss": 0.9498,
      "step": 6580
    },
    {
      "epoch": 0.41763046991349534,
      "grad_norm": 2.3373491764068604,
      "learning_rate": 2.588039796782388e-05,
      "loss": 0.9277,
      "step": 6590
    },
    {
      "epoch": 0.41826420355524574,
      "grad_norm": 2.2922303676605225,
      "learning_rate": 2.5874047417442843e-05,
      "loss": 0.9877,
      "step": 6600
    },
    {
      "epoch": 0.4188979371969961,
      "grad_norm": 2.391180992126465,
      "learning_rate": 2.5867696867061813e-05,
      "loss": 1.0343,
      "step": 6610
    },
    {
      "epoch": 0.4195316708387465,
      "grad_norm": 2.578624725341797,
      "learning_rate": 2.586134631668078e-05,
      "loss": 0.99,
      "step": 6620
    },
    {
      "epoch": 0.42016540448049683,
      "grad_norm": 2.345576047897339,
      "learning_rate": 2.585499576629975e-05,
      "loss": 0.9571,
      "step": 6630
    },
    {
      "epoch": 0.42079913812224723,
      "grad_norm": 2.491687774658203,
      "learning_rate": 2.5848645215918715e-05,
      "loss": 0.9955,
      "step": 6640
    },
    {
      "epoch": 0.4214328717639976,
      "grad_norm": 2.6161715984344482,
      "learning_rate": 2.5842294665537678e-05,
      "loss": 1.0031,
      "step": 6650
    },
    {
      "epoch": 0.422066605405748,
      "grad_norm": 2.871776819229126,
      "learning_rate": 2.5835944115156648e-05,
      "loss": 0.9342,
      "step": 6660
    },
    {
      "epoch": 0.4227003390474983,
      "grad_norm": 2.3553054332733154,
      "learning_rate": 2.5829593564775614e-05,
      "loss": 0.9985,
      "step": 6670
    },
    {
      "epoch": 0.4233340726892487,
      "grad_norm": 1.9586098194122314,
      "learning_rate": 2.582324301439458e-05,
      "loss": 0.9887,
      "step": 6680
    },
    {
      "epoch": 0.42396780633099906,
      "grad_norm": 2.67582106590271,
      "learning_rate": 2.581689246401355e-05,
      "loss": 0.9863,
      "step": 6690
    },
    {
      "epoch": 0.42460153997274946,
      "grad_norm": 1.9338499307632446,
      "learning_rate": 2.5810541913632514e-05,
      "loss": 0.9735,
      "step": 6700
    },
    {
      "epoch": 0.4252352736144998,
      "grad_norm": 3.021202564239502,
      "learning_rate": 2.580419136325148e-05,
      "loss": 0.987,
      "step": 6710
    },
    {
      "epoch": 0.4258690072562502,
      "grad_norm": 2.0804057121276855,
      "learning_rate": 2.579784081287045e-05,
      "loss": 0.9997,
      "step": 6720
    },
    {
      "epoch": 0.42650274089800055,
      "grad_norm": 2.444840669631958,
      "learning_rate": 2.5791490262489416e-05,
      "loss": 1.0022,
      "step": 6730
    },
    {
      "epoch": 0.42713647453975095,
      "grad_norm": 3.01973032951355,
      "learning_rate": 2.5785139712108383e-05,
      "loss": 0.9684,
      "step": 6740
    },
    {
      "epoch": 0.4277702081815013,
      "grad_norm": 2.1451611518859863,
      "learning_rate": 2.5778789161727353e-05,
      "loss": 0.9829,
      "step": 6750
    },
    {
      "epoch": 0.4284039418232517,
      "grad_norm": 2.2611725330352783,
      "learning_rate": 2.5772438611346316e-05,
      "loss": 0.9221,
      "step": 6760
    },
    {
      "epoch": 0.42903767546500204,
      "grad_norm": 2.618743658065796,
      "learning_rate": 2.5766088060965282e-05,
      "loss": 0.947,
      "step": 6770
    },
    {
      "epoch": 0.42967140910675244,
      "grad_norm": 2.2316160202026367,
      "learning_rate": 2.5759737510584252e-05,
      "loss": 0.9787,
      "step": 6780
    },
    {
      "epoch": 0.4303051427485028,
      "grad_norm": 2.2427353858947754,
      "learning_rate": 2.575338696020322e-05,
      "loss": 0.9779,
      "step": 6790
    },
    {
      "epoch": 0.4309388763902532,
      "grad_norm": 2.293799638748169,
      "learning_rate": 2.5747036409822188e-05,
      "loss": 1.0179,
      "step": 6800
    },
    {
      "epoch": 0.43157261003200353,
      "grad_norm": 2.1884677410125732,
      "learning_rate": 2.574068585944115e-05,
      "loss": 0.9622,
      "step": 6810
    },
    {
      "epoch": 0.43220634367375393,
      "grad_norm": 2.8132271766662598,
      "learning_rate": 2.5734335309060118e-05,
      "loss": 0.9908,
      "step": 6820
    },
    {
      "epoch": 0.4328400773155043,
      "grad_norm": 2.325813055038452,
      "learning_rate": 2.5727984758679087e-05,
      "loss": 0.9436,
      "step": 6830
    },
    {
      "epoch": 0.4334738109572547,
      "grad_norm": 2.4362828731536865,
      "learning_rate": 2.5721634208298054e-05,
      "loss": 0.9699,
      "step": 6840
    },
    {
      "epoch": 0.434107544599005,
      "grad_norm": 3.6379172801971436,
      "learning_rate": 2.571528365791702e-05,
      "loss": 0.938,
      "step": 6850
    },
    {
      "epoch": 0.4347412782407554,
      "grad_norm": 2.4353818893432617,
      "learning_rate": 2.5708933107535987e-05,
      "loss": 1.0163,
      "step": 6860
    },
    {
      "epoch": 0.43537501188250577,
      "grad_norm": 2.2664411067962646,
      "learning_rate": 2.5702582557154953e-05,
      "loss": 0.9392,
      "step": 6870
    },
    {
      "epoch": 0.43600874552425617,
      "grad_norm": 2.0883779525756836,
      "learning_rate": 2.569623200677392e-05,
      "loss": 0.9893,
      "step": 6880
    },
    {
      "epoch": 0.4366424791660065,
      "grad_norm": 2.5431132316589355,
      "learning_rate": 2.568988145639289e-05,
      "loss": 0.9891,
      "step": 6890
    },
    {
      "epoch": 0.4372762128077569,
      "grad_norm": 2.162748098373413,
      "learning_rate": 2.5683530906011856e-05,
      "loss": 0.9902,
      "step": 6900
    },
    {
      "epoch": 0.43790994644950726,
      "grad_norm": 2.2197248935699463,
      "learning_rate": 2.567718035563082e-05,
      "loss": 0.961,
      "step": 6910
    },
    {
      "epoch": 0.43854368009125766,
      "grad_norm": 2.2747271060943604,
      "learning_rate": 2.567082980524979e-05,
      "loss": 0.9875,
      "step": 6920
    },
    {
      "epoch": 0.439177413733008,
      "grad_norm": 2.5151398181915283,
      "learning_rate": 2.5664479254868755e-05,
      "loss": 0.9537,
      "step": 6930
    },
    {
      "epoch": 0.4398111473747584,
      "grad_norm": 2.4137887954711914,
      "learning_rate": 2.5658128704487725e-05,
      "loss": 0.961,
      "step": 6940
    },
    {
      "epoch": 0.44044488101650875,
      "grad_norm": 2.3919456005096436,
      "learning_rate": 2.565177815410669e-05,
      "loss": 0.9725,
      "step": 6950
    },
    {
      "epoch": 0.44107861465825915,
      "grad_norm": 2.467475175857544,
      "learning_rate": 2.5645427603725658e-05,
      "loss": 0.9774,
      "step": 6960
    },
    {
      "epoch": 0.4417123483000095,
      "grad_norm": 2.1430552005767822,
      "learning_rate": 2.5639077053344624e-05,
      "loss": 0.9741,
      "step": 6970
    },
    {
      "epoch": 0.4423460819417599,
      "grad_norm": 3.255181312561035,
      "learning_rate": 2.563272650296359e-05,
      "loss": 0.9307,
      "step": 6980
    },
    {
      "epoch": 0.44297981558351024,
      "grad_norm": 2.4745771884918213,
      "learning_rate": 2.5626375952582557e-05,
      "loss": 0.9471,
      "step": 6990
    },
    {
      "epoch": 0.44361354922526064,
      "grad_norm": 2.369776964187622,
      "learning_rate": 2.5620025402201527e-05,
      "loss": 0.9625,
      "step": 7000
    },
    {
      "epoch": 0.444247282867011,
      "grad_norm": 2.1720941066741943,
      "learning_rate": 2.5613674851820493e-05,
      "loss": 0.9822,
      "step": 7010
    },
    {
      "epoch": 0.4448810165087614,
      "grad_norm": 2.176588296890259,
      "learning_rate": 2.5607324301439456e-05,
      "loss": 0.9667,
      "step": 7020
    },
    {
      "epoch": 0.44551475015051173,
      "grad_norm": 2.2355473041534424,
      "learning_rate": 2.5600973751058426e-05,
      "loss": 0.9313,
      "step": 7030
    },
    {
      "epoch": 0.44614848379226213,
      "grad_norm": 2.2668392658233643,
      "learning_rate": 2.5594623200677393e-05,
      "loss": 0.9398,
      "step": 7040
    },
    {
      "epoch": 0.4467822174340125,
      "grad_norm": 3.0307724475860596,
      "learning_rate": 2.558827265029636e-05,
      "loss": 1.0063,
      "step": 7050
    },
    {
      "epoch": 0.4474159510757629,
      "grad_norm": 2.4516139030456543,
      "learning_rate": 2.558192209991533e-05,
      "loss": 0.9475,
      "step": 7060
    },
    {
      "epoch": 0.4480496847175132,
      "grad_norm": 2.8560214042663574,
      "learning_rate": 2.5575571549534292e-05,
      "loss": 0.9692,
      "step": 7070
    },
    {
      "epoch": 0.4486834183592636,
      "grad_norm": 2.1933791637420654,
      "learning_rate": 2.5569220999153258e-05,
      "loss": 0.9592,
      "step": 7080
    },
    {
      "epoch": 0.44931715200101396,
      "grad_norm": 2.3344075679779053,
      "learning_rate": 2.5562870448772228e-05,
      "loss": 0.9577,
      "step": 7090
    },
    {
      "epoch": 0.44995088564276436,
      "grad_norm": 2.5050251483917236,
      "learning_rate": 2.5556519898391195e-05,
      "loss": 1.0,
      "step": 7100
    },
    {
      "epoch": 0.4505846192845147,
      "grad_norm": 2.468620777130127,
      "learning_rate": 2.5550169348010164e-05,
      "loss": 1.0221,
      "step": 7110
    },
    {
      "epoch": 0.4512183529262651,
      "grad_norm": 2.447788715362549,
      "learning_rate": 2.5543818797629127e-05,
      "loss": 0.9633,
      "step": 7120
    },
    {
      "epoch": 0.45185208656801545,
      "grad_norm": 2.597842216491699,
      "learning_rate": 2.5537468247248094e-05,
      "loss": 0.9643,
      "step": 7130
    },
    {
      "epoch": 0.45248582020976585,
      "grad_norm": 2.759779214859009,
      "learning_rate": 2.5531117696867064e-05,
      "loss": 0.9535,
      "step": 7140
    },
    {
      "epoch": 0.4531195538515162,
      "grad_norm": 2.048201322555542,
      "learning_rate": 2.552476714648603e-05,
      "loss": 0.9769,
      "step": 7150
    },
    {
      "epoch": 0.4537532874932666,
      "grad_norm": 2.487860918045044,
      "learning_rate": 2.5518416596104996e-05,
      "loss": 1.0023,
      "step": 7160
    },
    {
      "epoch": 0.45438702113501694,
      "grad_norm": 2.09230375289917,
      "learning_rate": 2.5512066045723963e-05,
      "loss": 0.9629,
      "step": 7170
    },
    {
      "epoch": 0.45502075477676734,
      "grad_norm": 2.4592320919036865,
      "learning_rate": 2.550571549534293e-05,
      "loss": 0.9935,
      "step": 7180
    },
    {
      "epoch": 0.4556544884185177,
      "grad_norm": 2.6403534412384033,
      "learning_rate": 2.5499364944961896e-05,
      "loss": 0.9673,
      "step": 7190
    },
    {
      "epoch": 0.4562882220602681,
      "grad_norm": 2.436964988708496,
      "learning_rate": 2.5493014394580866e-05,
      "loss": 0.9382,
      "step": 7200
    },
    {
      "epoch": 0.45692195570201843,
      "grad_norm": 1.992918610572815,
      "learning_rate": 2.5486663844199832e-05,
      "loss": 0.9103,
      "step": 7210
    },
    {
      "epoch": 0.45755568934376883,
      "grad_norm": 2.3629024028778076,
      "learning_rate": 2.54803132938188e-05,
      "loss": 0.9615,
      "step": 7220
    },
    {
      "epoch": 0.4581894229855192,
      "grad_norm": 2.4839351177215576,
      "learning_rate": 2.5473962743437765e-05,
      "loss": 0.9894,
      "step": 7230
    },
    {
      "epoch": 0.4588231566272696,
      "grad_norm": 2.1495468616485596,
      "learning_rate": 2.546761219305673e-05,
      "loss": 0.958,
      "step": 7240
    },
    {
      "epoch": 0.4594568902690199,
      "grad_norm": 2.5453438758850098,
      "learning_rate": 2.54612616426757e-05,
      "loss": 0.9354,
      "step": 7250
    },
    {
      "epoch": 0.4600906239107703,
      "grad_norm": 2.376894235610962,
      "learning_rate": 2.5454911092294668e-05,
      "loss": 0.975,
      "step": 7260
    },
    {
      "epoch": 0.46072435755252067,
      "grad_norm": 2.2018747329711914,
      "learning_rate": 2.5448560541913634e-05,
      "loss": 0.9356,
      "step": 7270
    },
    {
      "epoch": 0.46135809119427107,
      "grad_norm": 2.404893398284912,
      "learning_rate": 2.54422099915326e-05,
      "loss": 0.9391,
      "step": 7280
    },
    {
      "epoch": 0.4619918248360214,
      "grad_norm": 2.307643413543701,
      "learning_rate": 2.5435859441151567e-05,
      "loss": 0.9704,
      "step": 7290
    },
    {
      "epoch": 0.4626255584777718,
      "grad_norm": 2.791118621826172,
      "learning_rate": 2.5429508890770533e-05,
      "loss": 0.9649,
      "step": 7300
    },
    {
      "epoch": 0.46325929211952216,
      "grad_norm": 2.181204319000244,
      "learning_rate": 2.5423158340389503e-05,
      "loss": 0.9281,
      "step": 7310
    },
    {
      "epoch": 0.46389302576127256,
      "grad_norm": 2.2379744052886963,
      "learning_rate": 2.541680779000847e-05,
      "loss": 0.9236,
      "step": 7320
    },
    {
      "epoch": 0.4645267594030229,
      "grad_norm": 3.10076904296875,
      "learning_rate": 2.5410457239627433e-05,
      "loss": 0.9951,
      "step": 7330
    },
    {
      "epoch": 0.4651604930447733,
      "grad_norm": 2.4599859714508057,
      "learning_rate": 2.5404106689246402e-05,
      "loss": 0.9666,
      "step": 7340
    },
    {
      "epoch": 0.46579422668652365,
      "grad_norm": 2.7183239459991455,
      "learning_rate": 2.539775613886537e-05,
      "loss": 0.9732,
      "step": 7350
    },
    {
      "epoch": 0.46642796032827405,
      "grad_norm": 2.3969128131866455,
      "learning_rate": 2.5391405588484335e-05,
      "loss": 0.9805,
      "step": 7360
    },
    {
      "epoch": 0.4670616939700244,
      "grad_norm": 2.4102840423583984,
      "learning_rate": 2.5385055038103305e-05,
      "loss": 0.9826,
      "step": 7370
    },
    {
      "epoch": 0.4676954276117748,
      "grad_norm": 1.9870262145996094,
      "learning_rate": 2.5378704487722268e-05,
      "loss": 0.9601,
      "step": 7380
    },
    {
      "epoch": 0.46832916125352514,
      "grad_norm": 2.388395071029663,
      "learning_rate": 2.5372353937341234e-05,
      "loss": 1.0026,
      "step": 7390
    },
    {
      "epoch": 0.46896289489527554,
      "grad_norm": 2.2882473468780518,
      "learning_rate": 2.5366003386960204e-05,
      "loss": 0.9485,
      "step": 7400
    },
    {
      "epoch": 0.4695966285370259,
      "grad_norm": 2.758145332336426,
      "learning_rate": 2.535965283657917e-05,
      "loss": 0.9512,
      "step": 7410
    },
    {
      "epoch": 0.4702303621787763,
      "grad_norm": 3.6730618476867676,
      "learning_rate": 2.535330228619814e-05,
      "loss": 0.9154,
      "step": 7420
    },
    {
      "epoch": 0.47086409582052663,
      "grad_norm": 2.664313316345215,
      "learning_rate": 2.5346951735817104e-05,
      "loss": 0.9885,
      "step": 7430
    },
    {
      "epoch": 0.47149782946227703,
      "grad_norm": 2.2438526153564453,
      "learning_rate": 2.534060118543607e-05,
      "loss": 0.9607,
      "step": 7440
    },
    {
      "epoch": 0.4721315631040274,
      "grad_norm": 2.23822021484375,
      "learning_rate": 2.533425063505504e-05,
      "loss": 0.9529,
      "step": 7450
    },
    {
      "epoch": 0.4727652967457778,
      "grad_norm": 2.8653714656829834,
      "learning_rate": 2.5327900084674006e-05,
      "loss": 0.9962,
      "step": 7460
    },
    {
      "epoch": 0.4733990303875281,
      "grad_norm": 2.7577919960021973,
      "learning_rate": 2.5321549534292973e-05,
      "loss": 0.9689,
      "step": 7470
    },
    {
      "epoch": 0.4740327640292785,
      "grad_norm": 2.4108340740203857,
      "learning_rate": 2.5315198983911942e-05,
      "loss": 0.9782,
      "step": 7480
    },
    {
      "epoch": 0.47466649767102886,
      "grad_norm": 2.227461576461792,
      "learning_rate": 2.5308848433530906e-05,
      "loss": 0.9712,
      "step": 7490
    },
    {
      "epoch": 0.47530023131277926,
      "grad_norm": 2.2530245780944824,
      "learning_rate": 2.5302497883149872e-05,
      "loss": 0.9351,
      "step": 7500
    },
    {
      "epoch": 0.4759339649545296,
      "grad_norm": 2.07965350151062,
      "learning_rate": 2.5296147332768842e-05,
      "loss": 0.9399,
      "step": 7510
    },
    {
      "epoch": 0.47656769859628,
      "grad_norm": 2.722790479660034,
      "learning_rate": 2.5289796782387808e-05,
      "loss": 0.9667,
      "step": 7520
    },
    {
      "epoch": 0.47720143223803035,
      "grad_norm": 2.2876527309417725,
      "learning_rate": 2.5283446232006775e-05,
      "loss": 0.9734,
      "step": 7530
    },
    {
      "epoch": 0.47783516587978075,
      "grad_norm": 2.294724225997925,
      "learning_rate": 2.527709568162574e-05,
      "loss": 0.9745,
      "step": 7540
    },
    {
      "epoch": 0.4784688995215311,
      "grad_norm": 1.8140308856964111,
      "learning_rate": 2.5270745131244707e-05,
      "loss": 0.9449,
      "step": 7550
    },
    {
      "epoch": 0.4791026331632815,
      "grad_norm": 2.3347582817077637,
      "learning_rate": 2.5264394580863677e-05,
      "loss": 0.9941,
      "step": 7560
    },
    {
      "epoch": 0.47973636680503184,
      "grad_norm": 2.488996982574463,
      "learning_rate": 2.5258044030482644e-05,
      "loss": 0.9596,
      "step": 7570
    },
    {
      "epoch": 0.48037010044678224,
      "grad_norm": 2.126427173614502,
      "learning_rate": 2.525169348010161e-05,
      "loss": 0.9526,
      "step": 7580
    },
    {
      "epoch": 0.4810038340885326,
      "grad_norm": 2.846975326538086,
      "learning_rate": 2.5245342929720577e-05,
      "loss": 0.9836,
      "step": 7590
    },
    {
      "epoch": 0.481637567730283,
      "grad_norm": 2.7056961059570312,
      "learning_rate": 2.5238992379339543e-05,
      "loss": 0.93,
      "step": 7600
    },
    {
      "epoch": 0.48227130137203333,
      "grad_norm": 2.1373064517974854,
      "learning_rate": 2.523264182895851e-05,
      "loss": 0.9911,
      "step": 7610
    },
    {
      "epoch": 0.48290503501378373,
      "grad_norm": 2.484464168548584,
      "learning_rate": 2.522629127857748e-05,
      "loss": 0.9887,
      "step": 7620
    },
    {
      "epoch": 0.4835387686555341,
      "grad_norm": 2.280266284942627,
      "learning_rate": 2.5219940728196446e-05,
      "loss": 0.9369,
      "step": 7630
    },
    {
      "epoch": 0.4841725022972844,
      "grad_norm": 2.5229735374450684,
      "learning_rate": 2.521359017781541e-05,
      "loss": 0.9901,
      "step": 7640
    },
    {
      "epoch": 0.4848062359390348,
      "grad_norm": 2.6133909225463867,
      "learning_rate": 2.520723962743438e-05,
      "loss": 0.9706,
      "step": 7650
    },
    {
      "epoch": 0.48543996958078517,
      "grad_norm": 2.1142420768737793,
      "learning_rate": 2.5200889077053345e-05,
      "loss": 0.9743,
      "step": 7660
    },
    {
      "epoch": 0.48607370322253557,
      "grad_norm": 2.3085877895355225,
      "learning_rate": 2.519453852667231e-05,
      "loss": 0.9625,
      "step": 7670
    },
    {
      "epoch": 0.4867074368642859,
      "grad_norm": 2.118098497390747,
      "learning_rate": 2.518818797629128e-05,
      "loss": 0.9793,
      "step": 7680
    },
    {
      "epoch": 0.4873411705060363,
      "grad_norm": 2.4793708324432373,
      "learning_rate": 2.5181837425910244e-05,
      "loss": 0.96,
      "step": 7690
    },
    {
      "epoch": 0.48797490414778666,
      "grad_norm": 2.143747568130493,
      "learning_rate": 2.517548687552921e-05,
      "loss": 0.9683,
      "step": 7700
    },
    {
      "epoch": 0.48860863778953706,
      "grad_norm": 2.1645255088806152,
      "learning_rate": 2.516913632514818e-05,
      "loss": 0.9765,
      "step": 7710
    },
    {
      "epoch": 0.4892423714312874,
      "grad_norm": 2.44954514503479,
      "learning_rate": 2.5162785774767147e-05,
      "loss": 0.9745,
      "step": 7720
    },
    {
      "epoch": 0.4898761050730378,
      "grad_norm": 2.3186774253845215,
      "learning_rate": 2.5156435224386117e-05,
      "loss": 0.8965,
      "step": 7730
    },
    {
      "epoch": 0.49050983871478815,
      "grad_norm": 1.956725001335144,
      "learning_rate": 2.5150084674005083e-05,
      "loss": 0.955,
      "step": 7740
    },
    {
      "epoch": 0.49114357235653855,
      "grad_norm": 2.2733850479125977,
      "learning_rate": 2.5143734123624046e-05,
      "loss": 0.9424,
      "step": 7750
    },
    {
      "epoch": 0.4917773059982889,
      "grad_norm": 2.6566216945648193,
      "learning_rate": 2.5137383573243016e-05,
      "loss": 1.0037,
      "step": 7760
    },
    {
      "epoch": 0.4924110396400393,
      "grad_norm": 2.3368887901306152,
      "learning_rate": 2.5131033022861982e-05,
      "loss": 0.9331,
      "step": 7770
    },
    {
      "epoch": 0.49304477328178964,
      "grad_norm": 2.3497154712677,
      "learning_rate": 2.512468247248095e-05,
      "loss": 0.9811,
      "step": 7780
    },
    {
      "epoch": 0.49367850692354004,
      "grad_norm": 2.45331072807312,
      "learning_rate": 2.511833192209992e-05,
      "loss": 0.9428,
      "step": 7790
    },
    {
      "epoch": 0.4943122405652904,
      "grad_norm": 2.7217934131622314,
      "learning_rate": 2.511198137171888e-05,
      "loss": 1.0153,
      "step": 7800
    },
    {
      "epoch": 0.4949459742070408,
      "grad_norm": 2.7175683975219727,
      "learning_rate": 2.5105630821337848e-05,
      "loss": 0.9927,
      "step": 7810
    },
    {
      "epoch": 0.49557970784879113,
      "grad_norm": 2.264707326889038,
      "learning_rate": 2.5099280270956818e-05,
      "loss": 0.9461,
      "step": 7820
    },
    {
      "epoch": 0.49621344149054153,
      "grad_norm": 2.502983331680298,
      "learning_rate": 2.5092929720575784e-05,
      "loss": 0.96,
      "step": 7830
    },
    {
      "epoch": 0.4968471751322919,
      "grad_norm": 2.0394890308380127,
      "learning_rate": 2.508657917019475e-05,
      "loss": 0.9971,
      "step": 7840
    },
    {
      "epoch": 0.4974809087740423,
      "grad_norm": 2.4115982055664062,
      "learning_rate": 2.5080228619813717e-05,
      "loss": 0.9443,
      "step": 7850
    },
    {
      "epoch": 0.4981146424157926,
      "grad_norm": 2.531599521636963,
      "learning_rate": 2.5073878069432684e-05,
      "loss": 1.0002,
      "step": 7860
    },
    {
      "epoch": 0.498748376057543,
      "grad_norm": 2.2721164226531982,
      "learning_rate": 2.506752751905165e-05,
      "loss": 0.9746,
      "step": 7870
    },
    {
      "epoch": 0.49938210969929336,
      "grad_norm": 2.5633745193481445,
      "learning_rate": 2.506117696867062e-05,
      "loss": 0.991,
      "step": 7880
    },
    {
      "epoch": 0.5000158433410438,
      "grad_norm": 2.361595630645752,
      "learning_rate": 2.5054826418289586e-05,
      "loss": 0.9838,
      "step": 7890
    },
    {
      "epoch": 0.5006495769827941,
      "grad_norm": 2.6697824001312256,
      "learning_rate": 2.5048475867908553e-05,
      "loss": 0.9829,
      "step": 7900
    },
    {
      "epoch": 0.5012833106245445,
      "grad_norm": 2.759368419647217,
      "learning_rate": 2.504212531752752e-05,
      "loss": 0.9915,
      "step": 7910
    },
    {
      "epoch": 0.5019170442662949,
      "grad_norm": 2.570490837097168,
      "learning_rate": 2.5035774767146486e-05,
      "loss": 0.988,
      "step": 7920
    },
    {
      "epoch": 0.5025507779080453,
      "grad_norm": 2.2399826049804688,
      "learning_rate": 2.5029424216765455e-05,
      "loss": 0.9732,
      "step": 7930
    },
    {
      "epoch": 0.5031845115497956,
      "grad_norm": 1.9201583862304688,
      "learning_rate": 2.5023073666384422e-05,
      "loss": 0.96,
      "step": 7940
    },
    {
      "epoch": 0.5038182451915459,
      "grad_norm": 2.467085123062134,
      "learning_rate": 2.5016723116003385e-05,
      "loss": 0.9565,
      "step": 7950
    },
    {
      "epoch": 0.5044519788332964,
      "grad_norm": 2.386364221572876,
      "learning_rate": 2.5010372565622355e-05,
      "loss": 0.9409,
      "step": 7960
    },
    {
      "epoch": 0.5050857124750467,
      "grad_norm": 2.049872636795044,
      "learning_rate": 2.500402201524132e-05,
      "loss": 0.9286,
      "step": 7970
    },
    {
      "epoch": 0.5057194461167971,
      "grad_norm": 2.298725128173828,
      "learning_rate": 2.4997671464860288e-05,
      "loss": 0.9495,
      "step": 7980
    },
    {
      "epoch": 0.5063531797585474,
      "grad_norm": 2.1721746921539307,
      "learning_rate": 2.4991320914479257e-05,
      "loss": 0.9346,
      "step": 7990
    },
    {
      "epoch": 0.5069869134002979,
      "grad_norm": 2.4207539558410645,
      "learning_rate": 2.4984970364098224e-05,
      "loss": 0.9815,
      "step": 8000
    },
    {
      "epoch": 0.5076206470420482,
      "grad_norm": 2.425496816635132,
      "learning_rate": 2.4978619813717187e-05,
      "loss": 0.9737,
      "step": 8010
    },
    {
      "epoch": 0.5082543806837986,
      "grad_norm": 2.2896180152893066,
      "learning_rate": 2.4972269263336157e-05,
      "loss": 0.9618,
      "step": 8020
    },
    {
      "epoch": 0.5088881143255489,
      "grad_norm": 2.5054945945739746,
      "learning_rate": 2.4965918712955123e-05,
      "loss": 0.9673,
      "step": 8030
    },
    {
      "epoch": 0.5095218479672994,
      "grad_norm": 2.1220502853393555,
      "learning_rate": 2.4959568162574093e-05,
      "loss": 0.9693,
      "step": 8040
    },
    {
      "epoch": 0.5101555816090497,
      "grad_norm": 2.4411680698394775,
      "learning_rate": 2.495321761219306e-05,
      "loss": 0.9646,
      "step": 8050
    },
    {
      "epoch": 0.5107893152508001,
      "grad_norm": 2.310612916946411,
      "learning_rate": 2.4946867061812022e-05,
      "loss": 0.9918,
      "step": 8060
    },
    {
      "epoch": 0.5114230488925504,
      "grad_norm": 2.341562271118164,
      "learning_rate": 2.4940516511430992e-05,
      "loss": 1.0027,
      "step": 8070
    },
    {
      "epoch": 0.5120567825343009,
      "grad_norm": 2.3291146755218506,
      "learning_rate": 2.493416596104996e-05,
      "loss": 0.9315,
      "step": 8080
    },
    {
      "epoch": 0.5126905161760512,
      "grad_norm": 2.7770841121673584,
      "learning_rate": 2.4927815410668925e-05,
      "loss": 0.9936,
      "step": 8090
    },
    {
      "epoch": 0.5133242498178016,
      "grad_norm": 2.059237241744995,
      "learning_rate": 2.4921464860287895e-05,
      "loss": 0.9605,
      "step": 8100
    },
    {
      "epoch": 0.5139579834595519,
      "grad_norm": 2.47184419631958,
      "learning_rate": 2.4915114309906858e-05,
      "loss": 0.9507,
      "step": 8110
    },
    {
      "epoch": 0.5145917171013024,
      "grad_norm": 2.4429354667663574,
      "learning_rate": 2.4908763759525824e-05,
      "loss": 0.9571,
      "step": 8120
    },
    {
      "epoch": 0.5152254507430527,
      "grad_norm": 2.4851975440979004,
      "learning_rate": 2.4902413209144794e-05,
      "loss": 0.9812,
      "step": 8130
    },
    {
      "epoch": 0.515859184384803,
      "grad_norm": 2.8480918407440186,
      "learning_rate": 2.489606265876376e-05,
      "loss": 0.9888,
      "step": 8140
    },
    {
      "epoch": 0.5164929180265534,
      "grad_norm": 2.9939262866973877,
      "learning_rate": 2.4889712108382727e-05,
      "loss": 0.9711,
      "step": 8150
    },
    {
      "epoch": 0.5171266516683038,
      "grad_norm": 2.3272111415863037,
      "learning_rate": 2.4883361558001693e-05,
      "loss": 0.9745,
      "step": 8160
    },
    {
      "epoch": 0.5177603853100542,
      "grad_norm": 2.908283233642578,
      "learning_rate": 2.487701100762066e-05,
      "loss": 0.9373,
      "step": 8170
    },
    {
      "epoch": 0.5183941189518045,
      "grad_norm": 2.2969250679016113,
      "learning_rate": 2.4870660457239626e-05,
      "loss": 0.8974,
      "step": 8180
    },
    {
      "epoch": 0.5190278525935549,
      "grad_norm": 2.2454993724823,
      "learning_rate": 2.4864309906858596e-05,
      "loss": 0.9509,
      "step": 8190
    },
    {
      "epoch": 0.5196615862353053,
      "grad_norm": 2.6299126148223877,
      "learning_rate": 2.4857959356477562e-05,
      "loss": 0.9869,
      "step": 8200
    },
    {
      "epoch": 0.5202953198770557,
      "grad_norm": 2.667656660079956,
      "learning_rate": 2.485160880609653e-05,
      "loss": 0.96,
      "step": 8210
    },
    {
      "epoch": 0.520929053518806,
      "grad_norm": 2.4179742336273193,
      "learning_rate": 2.4845258255715495e-05,
      "loss": 0.9411,
      "step": 8220
    },
    {
      "epoch": 0.5215627871605564,
      "grad_norm": 1.9466849565505981,
      "learning_rate": 2.4838907705334462e-05,
      "loss": 0.9093,
      "step": 8230
    },
    {
      "epoch": 0.5221965208023068,
      "grad_norm": 2.5564002990722656,
      "learning_rate": 2.483255715495343e-05,
      "loss": 0.9557,
      "step": 8240
    },
    {
      "epoch": 0.5228302544440572,
      "grad_norm": 2.6963958740234375,
      "learning_rate": 2.4826206604572398e-05,
      "loss": 0.972,
      "step": 8250
    },
    {
      "epoch": 0.5234639880858075,
      "grad_norm": 2.355520486831665,
      "learning_rate": 2.4819856054191364e-05,
      "loss": 0.9771,
      "step": 8260
    },
    {
      "epoch": 0.5240977217275579,
      "grad_norm": 2.6609554290771484,
      "learning_rate": 2.481350550381033e-05,
      "loss": 0.9892,
      "step": 8270
    },
    {
      "epoch": 0.5247314553693083,
      "grad_norm": 2.3215320110321045,
      "learning_rate": 2.4807154953429297e-05,
      "loss": 0.9637,
      "step": 8280
    },
    {
      "epoch": 0.5253651890110587,
      "grad_norm": 2.3937759399414062,
      "learning_rate": 2.4800804403048264e-05,
      "loss": 0.9814,
      "step": 8290
    },
    {
      "epoch": 0.525998922652809,
      "grad_norm": 2.793794870376587,
      "learning_rate": 2.4794453852667234e-05,
      "loss": 0.9675,
      "step": 8300
    },
    {
      "epoch": 0.5266326562945594,
      "grad_norm": 2.6650187969207764,
      "learning_rate": 2.47881033022862e-05,
      "loss": 0.9523,
      "step": 8310
    },
    {
      "epoch": 0.5272663899363098,
      "grad_norm": 2.366338014602661,
      "learning_rate": 2.4781752751905163e-05,
      "loss": 1.0336,
      "step": 8320
    },
    {
      "epoch": 0.5279001235780602,
      "grad_norm": 2.4870901107788086,
      "learning_rate": 2.4775402201524133e-05,
      "loss": 0.9359,
      "step": 8330
    },
    {
      "epoch": 0.5285338572198105,
      "grad_norm": 2.4459757804870605,
      "learning_rate": 2.47690516511431e-05,
      "loss": 0.9405,
      "step": 8340
    },
    {
      "epoch": 0.5291675908615608,
      "grad_norm": 2.408177375793457,
      "learning_rate": 2.476270110076207e-05,
      "loss": 0.9196,
      "step": 8350
    },
    {
      "epoch": 0.5298013245033113,
      "grad_norm": 2.4682908058166504,
      "learning_rate": 2.4756350550381035e-05,
      "loss": 0.9347,
      "step": 8360
    },
    {
      "epoch": 0.5304350581450616,
      "grad_norm": 2.088839292526245,
      "learning_rate": 2.475e-05,
      "loss": 0.9655,
      "step": 8370
    },
    {
      "epoch": 0.531068791786812,
      "grad_norm": 2.7868592739105225,
      "learning_rate": 2.474364944961897e-05,
      "loss": 1.0278,
      "step": 8380
    },
    {
      "epoch": 0.5317025254285623,
      "grad_norm": 2.6103553771972656,
      "learning_rate": 2.4737298899237935e-05,
      "loss": 0.9872,
      "step": 8390
    },
    {
      "epoch": 0.5323362590703128,
      "grad_norm": 3.1155056953430176,
      "learning_rate": 2.47309483488569e-05,
      "loss": 1.0129,
      "step": 8400
    },
    {
      "epoch": 0.5329699927120631,
      "grad_norm": 2.047562599182129,
      "learning_rate": 2.472459779847587e-05,
      "loss": 0.9279,
      "step": 8410
    },
    {
      "epoch": 0.5336037263538135,
      "grad_norm": 2.039130210876465,
      "learning_rate": 2.4718247248094834e-05,
      "loss": 1.0093,
      "step": 8420
    },
    {
      "epoch": 0.5342374599955638,
      "grad_norm": 2.3977725505828857,
      "learning_rate": 2.47118966977138e-05,
      "loss": 0.9623,
      "step": 8430
    },
    {
      "epoch": 0.5348711936373143,
      "grad_norm": 2.2456552982330322,
      "learning_rate": 2.470554614733277e-05,
      "loss": 0.9484,
      "step": 8440
    },
    {
      "epoch": 0.5355049272790646,
      "grad_norm": 2.257558822631836,
      "learning_rate": 2.4699195596951737e-05,
      "loss": 0.956,
      "step": 8450
    },
    {
      "epoch": 0.536138660920815,
      "grad_norm": 2.4910051822662354,
      "learning_rate": 2.4692845046570703e-05,
      "loss": 0.9963,
      "step": 8460
    },
    {
      "epoch": 0.5367723945625653,
      "grad_norm": 2.1713638305664062,
      "learning_rate": 2.468649449618967e-05,
      "loss": 0.9692,
      "step": 8470
    },
    {
      "epoch": 0.5374061282043158,
      "grad_norm": 2.3248255252838135,
      "learning_rate": 2.4680143945808636e-05,
      "loss": 0.9606,
      "step": 8480
    },
    {
      "epoch": 0.5380398618460661,
      "grad_norm": 2.2841947078704834,
      "learning_rate": 2.4673793395427602e-05,
      "loss": 0.9265,
      "step": 8490
    },
    {
      "epoch": 0.5386735954878165,
      "grad_norm": 2.258441686630249,
      "learning_rate": 2.4667442845046572e-05,
      "loss": 0.9221,
      "step": 8500
    },
    {
      "epoch": 0.5393073291295668,
      "grad_norm": 2.240354537963867,
      "learning_rate": 2.466109229466554e-05,
      "loss": 0.9299,
      "step": 8510
    },
    {
      "epoch": 0.5399410627713173,
      "grad_norm": 2.1794896125793457,
      "learning_rate": 2.465474174428451e-05,
      "loss": 0.9523,
      "step": 8520
    },
    {
      "epoch": 0.5405747964130676,
      "grad_norm": 2.233027458190918,
      "learning_rate": 2.464839119390347e-05,
      "loss": 0.9564,
      "step": 8530
    },
    {
      "epoch": 0.541208530054818,
      "grad_norm": 2.5071630477905273,
      "learning_rate": 2.4642040643522438e-05,
      "loss": 0.9424,
      "step": 8540
    },
    {
      "epoch": 0.5418422636965683,
      "grad_norm": 2.314746618270874,
      "learning_rate": 2.4635690093141408e-05,
      "loss": 0.9633,
      "step": 8550
    },
    {
      "epoch": 0.5424759973383187,
      "grad_norm": 2.5734763145446777,
      "learning_rate": 2.4629339542760374e-05,
      "loss": 0.9828,
      "step": 8560
    },
    {
      "epoch": 0.5431097309800691,
      "grad_norm": 2.5566670894622803,
      "learning_rate": 2.462298899237934e-05,
      "loss": 0.9851,
      "step": 8570
    },
    {
      "epoch": 0.5437434646218194,
      "grad_norm": 2.4229986667633057,
      "learning_rate": 2.4616638441998307e-05,
      "loss": 0.9264,
      "step": 8580
    },
    {
      "epoch": 0.5443771982635698,
      "grad_norm": 2.130317211151123,
      "learning_rate": 2.4610287891617273e-05,
      "loss": 0.9646,
      "step": 8590
    },
    {
      "epoch": 0.5450109319053202,
      "grad_norm": 2.4479756355285645,
      "learning_rate": 2.460393734123624e-05,
      "loss": 0.9555,
      "step": 8600
    },
    {
      "epoch": 0.5456446655470706,
      "grad_norm": 2.264420747756958,
      "learning_rate": 2.459758679085521e-05,
      "loss": 0.9507,
      "step": 8610
    },
    {
      "epoch": 0.5462783991888209,
      "grad_norm": 2.3813369274139404,
      "learning_rate": 2.4591236240474176e-05,
      "loss": 0.9515,
      "step": 8620
    },
    {
      "epoch": 0.5469121328305713,
      "grad_norm": 2.219444990158081,
      "learning_rate": 2.458488569009314e-05,
      "loss": 0.9655,
      "step": 8630
    },
    {
      "epoch": 0.5475458664723217,
      "grad_norm": 2.4741971492767334,
      "learning_rate": 2.457853513971211e-05,
      "loss": 0.989,
      "step": 8640
    },
    {
      "epoch": 0.5481796001140721,
      "grad_norm": 2.2437832355499268,
      "learning_rate": 2.4572184589331075e-05,
      "loss": 0.9801,
      "step": 8650
    },
    {
      "epoch": 0.5488133337558224,
      "grad_norm": 2.4867453575134277,
      "learning_rate": 2.4565834038950045e-05,
      "loss": 0.9699,
      "step": 8660
    },
    {
      "epoch": 0.5494470673975728,
      "grad_norm": 2.542290687561035,
      "learning_rate": 2.455948348856901e-05,
      "loss": 0.9758,
      "step": 8670
    },
    {
      "epoch": 0.5500808010393232,
      "grad_norm": 2.5071024894714355,
      "learning_rate": 2.4553132938187975e-05,
      "loss": 0.9905,
      "step": 8680
    },
    {
      "epoch": 0.5507145346810736,
      "grad_norm": 2.49065899848938,
      "learning_rate": 2.4546782387806944e-05,
      "loss": 0.9229,
      "step": 8690
    },
    {
      "epoch": 0.5513482683228239,
      "grad_norm": 2.564775228500366,
      "learning_rate": 2.454043183742591e-05,
      "loss": 0.9909,
      "step": 8700
    },
    {
      "epoch": 0.5519820019645743,
      "grad_norm": 2.39894437789917,
      "learning_rate": 2.453471634208298e-05,
      "loss": 1.0164,
      "step": 8710
    },
    {
      "epoch": 0.5526157356063247,
      "grad_norm": 2.55578351020813,
      "learning_rate": 2.4528365791701948e-05,
      "loss": 0.9644,
      "step": 8720
    },
    {
      "epoch": 0.553249469248075,
      "grad_norm": 3.0906171798706055,
      "learning_rate": 2.4522015241320918e-05,
      "loss": 0.9632,
      "step": 8730
    },
    {
      "epoch": 0.5538832028898254,
      "grad_norm": 2.3429367542266846,
      "learning_rate": 2.451566469093988e-05,
      "loss": 0.9605,
      "step": 8740
    },
    {
      "epoch": 0.5545169365315757,
      "grad_norm": 2.471719980239868,
      "learning_rate": 2.4509314140558847e-05,
      "loss": 0.9352,
      "step": 8750
    },
    {
      "epoch": 0.5551506701733262,
      "grad_norm": 2.2043917179107666,
      "learning_rate": 2.4502963590177817e-05,
      "loss": 0.9552,
      "step": 8760
    },
    {
      "epoch": 0.5557844038150765,
      "grad_norm": 2.7611799240112305,
      "learning_rate": 2.4496613039796783e-05,
      "loss": 0.9709,
      "step": 8770
    },
    {
      "epoch": 0.5564181374568269,
      "grad_norm": 2.8329553604125977,
      "learning_rate": 2.449026248941575e-05,
      "loss": 1.0254,
      "step": 8780
    },
    {
      "epoch": 0.5570518710985772,
      "grad_norm": 2.2273521423339844,
      "learning_rate": 2.4483911939034716e-05,
      "loss": 0.9753,
      "step": 8790
    },
    {
      "epoch": 0.5576856047403277,
      "grad_norm": 2.349017858505249,
      "learning_rate": 2.4477561388653683e-05,
      "loss": 0.958,
      "step": 8800
    },
    {
      "epoch": 0.558319338382078,
      "grad_norm": 2.7014811038970947,
      "learning_rate": 2.447121083827265e-05,
      "loss": 0.9591,
      "step": 8810
    },
    {
      "epoch": 0.5589530720238284,
      "grad_norm": 2.302557945251465,
      "learning_rate": 2.446486028789162e-05,
      "loss": 0.9669,
      "step": 8820
    },
    {
      "epoch": 0.5595868056655787,
      "grad_norm": 2.468170404434204,
      "learning_rate": 2.4458509737510585e-05,
      "loss": 0.9304,
      "step": 8830
    },
    {
      "epoch": 0.5602205393073292,
      "grad_norm": 2.399590015411377,
      "learning_rate": 2.4452159187129552e-05,
      "loss": 0.9342,
      "step": 8840
    },
    {
      "epoch": 0.5608542729490795,
      "grad_norm": 2.356537103652954,
      "learning_rate": 2.4445808636748518e-05,
      "loss": 0.9884,
      "step": 8850
    },
    {
      "epoch": 0.5614880065908299,
      "grad_norm": 2.56160831451416,
      "learning_rate": 2.4439458086367485e-05,
      "loss": 0.9526,
      "step": 8860
    },
    {
      "epoch": 0.5621217402325802,
      "grad_norm": 2.6135756969451904,
      "learning_rate": 2.4433107535986454e-05,
      "loss": 0.9277,
      "step": 8870
    },
    {
      "epoch": 0.5627554738743307,
      "grad_norm": 2.641507148742676,
      "learning_rate": 2.442675698560542e-05,
      "loss": 0.9595,
      "step": 8880
    },
    {
      "epoch": 0.563389207516081,
      "grad_norm": 2.318776845932007,
      "learning_rate": 2.4420406435224387e-05,
      "loss": 0.9471,
      "step": 8890
    },
    {
      "epoch": 0.5640229411578314,
      "grad_norm": 2.480666160583496,
      "learning_rate": 2.4414055884843354e-05,
      "loss": 0.964,
      "step": 8900
    },
    {
      "epoch": 0.5646566747995817,
      "grad_norm": 2.2129340171813965,
      "learning_rate": 2.440770533446232e-05,
      "loss": 0.9675,
      "step": 8910
    },
    {
      "epoch": 0.5652904084413322,
      "grad_norm": 5.935197830200195,
      "learning_rate": 2.4401354784081287e-05,
      "loss": 0.9611,
      "step": 8920
    },
    {
      "epoch": 0.5659241420830825,
      "grad_norm": 2.3162424564361572,
      "learning_rate": 2.4395004233700256e-05,
      "loss": 0.961,
      "step": 8930
    },
    {
      "epoch": 0.5665578757248328,
      "grad_norm": 2.51128888130188,
      "learning_rate": 2.4388653683319223e-05,
      "loss": 0.9732,
      "step": 8940
    },
    {
      "epoch": 0.5671916093665832,
      "grad_norm": 2.442354440689087,
      "learning_rate": 2.4382303132938186e-05,
      "loss": 0.9599,
      "step": 8950
    },
    {
      "epoch": 0.5678253430083336,
      "grad_norm": 3.0407021045684814,
      "learning_rate": 2.4375952582557156e-05,
      "loss": 0.9435,
      "step": 8960
    },
    {
      "epoch": 0.568459076650084,
      "grad_norm": 2.1753056049346924,
      "learning_rate": 2.4369602032176122e-05,
      "loss": 0.9045,
      "step": 8970
    },
    {
      "epoch": 0.5690928102918343,
      "grad_norm": 2.4869801998138428,
      "learning_rate": 2.4363251481795092e-05,
      "loss": 0.9793,
      "step": 8980
    },
    {
      "epoch": 0.5697265439335847,
      "grad_norm": 2.260756731033325,
      "learning_rate": 2.4356900931414058e-05,
      "loss": 0.9725,
      "step": 8990
    },
    {
      "epoch": 0.5703602775753351,
      "grad_norm": 2.560659646987915,
      "learning_rate": 2.435055038103302e-05,
      "loss": 0.9369,
      "step": 9000
    },
    {
      "epoch": 0.5709940112170855,
      "grad_norm": 2.471491575241089,
      "learning_rate": 2.434419983065199e-05,
      "loss": 0.9418,
      "step": 9010
    },
    {
      "epoch": 0.5716277448588358,
      "grad_norm": 2.6285452842712402,
      "learning_rate": 2.4337849280270958e-05,
      "loss": 0.9649,
      "step": 9020
    },
    {
      "epoch": 0.5722614785005862,
      "grad_norm": 2.345120906829834,
      "learning_rate": 2.4331498729889924e-05,
      "loss": 0.9267,
      "step": 9030
    },
    {
      "epoch": 0.5728952121423366,
      "grad_norm": 2.1166346073150635,
      "learning_rate": 2.4325148179508894e-05,
      "loss": 0.9611,
      "step": 9040
    },
    {
      "epoch": 0.573528945784087,
      "grad_norm": 2.6751108169555664,
      "learning_rate": 2.4318797629127857e-05,
      "loss": 0.9039,
      "step": 9050
    },
    {
      "epoch": 0.5741626794258373,
      "grad_norm": 2.6563658714294434,
      "learning_rate": 2.4312447078746823e-05,
      "loss": 0.9488,
      "step": 9060
    },
    {
      "epoch": 0.5747964130675877,
      "grad_norm": 2.176976203918457,
      "learning_rate": 2.4306096528365793e-05,
      "loss": 0.92,
      "step": 9070
    },
    {
      "epoch": 0.5754301467093381,
      "grad_norm": 2.5865519046783447,
      "learning_rate": 2.429974597798476e-05,
      "loss": 0.948,
      "step": 9080
    },
    {
      "epoch": 0.5760638803510885,
      "grad_norm": 3.123345136642456,
      "learning_rate": 2.4293395427603726e-05,
      "loss": 0.9432,
      "step": 9090
    },
    {
      "epoch": 0.5766976139928388,
      "grad_norm": 3.229445695877075,
      "learning_rate": 2.4287044877222692e-05,
      "loss": 1.0045,
      "step": 9100
    },
    {
      "epoch": 0.5773313476345892,
      "grad_norm": 2.3789899349212646,
      "learning_rate": 2.428069432684166e-05,
      "loss": 0.9502,
      "step": 9110
    },
    {
      "epoch": 0.5779650812763396,
      "grad_norm": 2.4674746990203857,
      "learning_rate": 2.4274343776460625e-05,
      "loss": 0.9842,
      "step": 9120
    },
    {
      "epoch": 0.57859881491809,
      "grad_norm": 2.4387288093566895,
      "learning_rate": 2.4267993226079595e-05,
      "loss": 0.921,
      "step": 9130
    },
    {
      "epoch": 0.5792325485598403,
      "grad_norm": 2.257199764251709,
      "learning_rate": 2.426164267569856e-05,
      "loss": 0.9545,
      "step": 9140
    },
    {
      "epoch": 0.5798662822015906,
      "grad_norm": 2.4085850715637207,
      "learning_rate": 2.425529212531753e-05,
      "loss": 0.9636,
      "step": 9150
    },
    {
      "epoch": 0.580500015843341,
      "grad_norm": 2.4960992336273193,
      "learning_rate": 2.4248941574936494e-05,
      "loss": 0.9445,
      "step": 9160
    },
    {
      "epoch": 0.5811337494850914,
      "grad_norm": 2.1886539459228516,
      "learning_rate": 2.424259102455546e-05,
      "loss": 0.8978,
      "step": 9170
    },
    {
      "epoch": 0.5817674831268418,
      "grad_norm": 2.43503737449646,
      "learning_rate": 2.423624047417443e-05,
      "loss": 0.9764,
      "step": 9180
    },
    {
      "epoch": 0.5824012167685921,
      "grad_norm": 2.5666019916534424,
      "learning_rate": 2.4229889923793397e-05,
      "loss": 0.9023,
      "step": 9190
    },
    {
      "epoch": 0.5830349504103425,
      "grad_norm": 2.7266244888305664,
      "learning_rate": 2.4223539373412363e-05,
      "loss": 0.9728,
      "step": 9200
    },
    {
      "epoch": 0.5836686840520929,
      "grad_norm": 2.3994224071502686,
      "learning_rate": 2.421718882303133e-05,
      "loss": 0.9435,
      "step": 9210
    },
    {
      "epoch": 0.5843024176938433,
      "grad_norm": 2.686389684677124,
      "learning_rate": 2.4210838272650296e-05,
      "loss": 0.9452,
      "step": 9220
    },
    {
      "epoch": 0.5849361513355936,
      "grad_norm": 2.8345890045166016,
      "learning_rate": 2.4204487722269263e-05,
      "loss": 0.9676,
      "step": 9230
    },
    {
      "epoch": 0.585569884977344,
      "grad_norm": 2.653242826461792,
      "learning_rate": 2.4198137171888233e-05,
      "loss": 0.9128,
      "step": 9240
    },
    {
      "epoch": 0.5862036186190944,
      "grad_norm": 2.6246750354766846,
      "learning_rate": 2.41917866215072e-05,
      "loss": 0.9697,
      "step": 9250
    },
    {
      "epoch": 0.5868373522608448,
      "grad_norm": 2.7675154209136963,
      "learning_rate": 2.4185436071126162e-05,
      "loss": 0.9168,
      "step": 9260
    },
    {
      "epoch": 0.5874710859025951,
      "grad_norm": 2.287811517715454,
      "learning_rate": 2.4179085520745132e-05,
      "loss": 0.9412,
      "step": 9270
    },
    {
      "epoch": 0.5881048195443455,
      "grad_norm": 2.24045467376709,
      "learning_rate": 2.4172734970364098e-05,
      "loss": 0.9231,
      "step": 9280
    },
    {
      "epoch": 0.5887385531860959,
      "grad_norm": 2.640718698501587,
      "learning_rate": 2.4166384419983068e-05,
      "loss": 0.9776,
      "step": 9290
    },
    {
      "epoch": 0.5893722868278463,
      "grad_norm": 2.47019624710083,
      "learning_rate": 2.4160033869602034e-05,
      "loss": 0.9863,
      "step": 9300
    },
    {
      "epoch": 0.5900060204695966,
      "grad_norm": 6.139875888824463,
      "learning_rate": 2.4153683319220997e-05,
      "loss": 0.9759,
      "step": 9310
    },
    {
      "epoch": 0.590639754111347,
      "grad_norm": 2.1215081214904785,
      "learning_rate": 2.4147332768839967e-05,
      "loss": 1.0115,
      "step": 9320
    },
    {
      "epoch": 0.5912734877530974,
      "grad_norm": 2.5364112854003906,
      "learning_rate": 2.4140982218458934e-05,
      "loss": 0.9544,
      "step": 9330
    },
    {
      "epoch": 0.5919072213948477,
      "grad_norm": 2.314020872116089,
      "learning_rate": 2.41346316680779e-05,
      "loss": 1.0053,
      "step": 9340
    },
    {
      "epoch": 0.5925409550365981,
      "grad_norm": 2.4064745903015137,
      "learning_rate": 2.412828111769687e-05,
      "loss": 0.9367,
      "step": 9350
    },
    {
      "epoch": 0.5931746886783484,
      "grad_norm": 2.5856144428253174,
      "learning_rate": 2.4121930567315833e-05,
      "loss": 0.966,
      "step": 9360
    },
    {
      "epoch": 0.5938084223200989,
      "grad_norm": 2.4691810607910156,
      "learning_rate": 2.41155800169348e-05,
      "loss": 0.9368,
      "step": 9370
    },
    {
      "epoch": 0.5944421559618492,
      "grad_norm": 2.605900526046753,
      "learning_rate": 2.410922946655377e-05,
      "loss": 0.9332,
      "step": 9380
    },
    {
      "epoch": 0.5950758896035996,
      "grad_norm": 2.184323310852051,
      "learning_rate": 2.4102878916172736e-05,
      "loss": 0.9554,
      "step": 9390
    },
    {
      "epoch": 0.5957096232453499,
      "grad_norm": 2.5592281818389893,
      "learning_rate": 2.4096528365791702e-05,
      "loss": 0.9484,
      "step": 9400
    },
    {
      "epoch": 0.5963433568871004,
      "grad_norm": 2.6007955074310303,
      "learning_rate": 2.4090177815410672e-05,
      "loss": 0.9611,
      "step": 9410
    },
    {
      "epoch": 0.5969770905288507,
      "grad_norm": 2.0717215538024902,
      "learning_rate": 2.4083827265029635e-05,
      "loss": 0.9318,
      "step": 9420
    },
    {
      "epoch": 0.5976108241706011,
      "grad_norm": 2.7570013999938965,
      "learning_rate": 2.40774767146486e-05,
      "loss": 0.9908,
      "step": 9430
    },
    {
      "epoch": 0.5982445578123514,
      "grad_norm": 2.4762003421783447,
      "learning_rate": 2.407112616426757e-05,
      "loss": 0.9478,
      "step": 9440
    },
    {
      "epoch": 0.5988782914541019,
      "grad_norm": 2.371011972427368,
      "learning_rate": 2.4064775613886538e-05,
      "loss": 0.9466,
      "step": 9450
    },
    {
      "epoch": 0.5995120250958522,
      "grad_norm": 2.8074452877044678,
      "learning_rate": 2.4058425063505507e-05,
      "loss": 0.9824,
      "step": 9460
    },
    {
      "epoch": 0.6001457587376026,
      "grad_norm": 2.425466299057007,
      "learning_rate": 2.405207451312447e-05,
      "loss": 0.9629,
      "step": 9470
    },
    {
      "epoch": 0.6007794923793529,
      "grad_norm": 2.6814398765563965,
      "learning_rate": 2.4045723962743437e-05,
      "loss": 0.9261,
      "step": 9480
    },
    {
      "epoch": 0.6014132260211034,
      "grad_norm": 2.535936117172241,
      "learning_rate": 2.4039373412362407e-05,
      "loss": 0.9413,
      "step": 9490
    },
    {
      "epoch": 0.6020469596628537,
      "grad_norm": 2.6451144218444824,
      "learning_rate": 2.4033022861981373e-05,
      "loss": 0.9558,
      "step": 9500
    },
    {
      "epoch": 0.602680693304604,
      "grad_norm": 3.3258724212646484,
      "learning_rate": 2.402667231160034e-05,
      "loss": 0.9451,
      "step": 9510
    },
    {
      "epoch": 0.6033144269463544,
      "grad_norm": 2.7634193897247314,
      "learning_rate": 2.4020321761219306e-05,
      "loss": 0.8918,
      "step": 9520
    },
    {
      "epoch": 0.6039481605881049,
      "grad_norm": 2.3382320404052734,
      "learning_rate": 2.4013971210838272e-05,
      "loss": 0.9797,
      "step": 9530
    },
    {
      "epoch": 0.6045818942298552,
      "grad_norm": 2.126988410949707,
      "learning_rate": 2.400762066045724e-05,
      "loss": 0.9552,
      "step": 9540
    },
    {
      "epoch": 0.6052156278716055,
      "grad_norm": 2.858214855194092,
      "learning_rate": 2.400127011007621e-05,
      "loss": 0.9283,
      "step": 9550
    },
    {
      "epoch": 0.6058493615133559,
      "grad_norm": 2.755748748779297,
      "learning_rate": 2.3994919559695175e-05,
      "loss": 0.9193,
      "step": 9560
    },
    {
      "epoch": 0.6064830951551063,
      "grad_norm": 2.8426260948181152,
      "learning_rate": 2.3988569009314138e-05,
      "loss": 0.9151,
      "step": 9570
    },
    {
      "epoch": 0.6071168287968567,
      "grad_norm": 2.162794828414917,
      "learning_rate": 2.3982218458933108e-05,
      "loss": 0.9368,
      "step": 9580
    },
    {
      "epoch": 0.607750562438607,
      "grad_norm": 2.5935282707214355,
      "learning_rate": 2.3975867908552074e-05,
      "loss": 0.9352,
      "step": 9590
    },
    {
      "epoch": 0.6083842960803574,
      "grad_norm": 2.9454729557037354,
      "learning_rate": 2.396951735817104e-05,
      "loss": 0.9458,
      "step": 9600
    },
    {
      "epoch": 0.6090180297221078,
      "grad_norm": 3.3275527954101562,
      "learning_rate": 2.396316680779001e-05,
      "loss": 1.0187,
      "step": 9610
    },
    {
      "epoch": 0.6096517633638582,
      "grad_norm": 2.6062564849853516,
      "learning_rate": 2.3956816257408974e-05,
      "loss": 0.9722,
      "step": 9620
    },
    {
      "epoch": 0.6102854970056085,
      "grad_norm": 1.9506381750106812,
      "learning_rate": 2.3950465707027943e-05,
      "loss": 0.9142,
      "step": 9630
    },
    {
      "epoch": 0.6109192306473589,
      "grad_norm": 2.500009298324585,
      "learning_rate": 2.394411515664691e-05,
      "loss": 0.9508,
      "step": 9640
    },
    {
      "epoch": 0.6115529642891093,
      "grad_norm": 2.6865975856781006,
      "learning_rate": 2.3937764606265876e-05,
      "loss": 0.9263,
      "step": 9650
    },
    {
      "epoch": 0.6121866979308597,
      "grad_norm": 2.2587108612060547,
      "learning_rate": 2.3931414055884846e-05,
      "loss": 0.9421,
      "step": 9660
    },
    {
      "epoch": 0.61282043157261,
      "grad_norm": 2.6219139099121094,
      "learning_rate": 2.3925063505503813e-05,
      "loss": 0.9479,
      "step": 9670
    },
    {
      "epoch": 0.6134541652143604,
      "grad_norm": 3.6941184997558594,
      "learning_rate": 2.3918712955122776e-05,
      "loss": 0.9544,
      "step": 9680
    },
    {
      "epoch": 0.6140878988561108,
      "grad_norm": 2.0799014568328857,
      "learning_rate": 2.3912362404741745e-05,
      "loss": 0.9503,
      "step": 9690
    },
    {
      "epoch": 0.6147216324978612,
      "grad_norm": 2.5601611137390137,
      "learning_rate": 2.3906011854360712e-05,
      "loss": 0.935,
      "step": 9700
    },
    {
      "epoch": 0.6153553661396115,
      "grad_norm": 2.697531223297119,
      "learning_rate": 2.3899661303979678e-05,
      "loss": 0.9212,
      "step": 9710
    },
    {
      "epoch": 0.6159890997813618,
      "grad_norm": 2.0188803672790527,
      "learning_rate": 2.3893310753598648e-05,
      "loss": 0.9003,
      "step": 9720
    },
    {
      "epoch": 0.6166228334231123,
      "grad_norm": 2.6894783973693848,
      "learning_rate": 2.388696020321761e-05,
      "loss": 0.9333,
      "step": 9730
    },
    {
      "epoch": 0.6172565670648626,
      "grad_norm": 3.603621482849121,
      "learning_rate": 2.3880609652836578e-05,
      "loss": 0.9718,
      "step": 9740
    },
    {
      "epoch": 0.617890300706613,
      "grad_norm": 2.2646639347076416,
      "learning_rate": 2.3874259102455547e-05,
      "loss": 0.9352,
      "step": 9750
    },
    {
      "epoch": 0.6185240343483633,
      "grad_norm": 2.4769721031188965,
      "learning_rate": 2.3867908552074514e-05,
      "loss": 0.8933,
      "step": 9760
    },
    {
      "epoch": 0.6191577679901138,
      "grad_norm": 2.1453776359558105,
      "learning_rate": 2.3861558001693484e-05,
      "loss": 0.9541,
      "step": 9770
    },
    {
      "epoch": 0.6197915016318641,
      "grad_norm": 2.4837100505828857,
      "learning_rate": 2.3855207451312447e-05,
      "loss": 0.9539,
      "step": 9780
    },
    {
      "epoch": 0.6204252352736145,
      "grad_norm": 2.5629825592041016,
      "learning_rate": 2.3848856900931413e-05,
      "loss": 0.9999,
      "step": 9790
    },
    {
      "epoch": 0.6210589689153648,
      "grad_norm": 2.5315186977386475,
      "learning_rate": 2.3842506350550383e-05,
      "loss": 0.9511,
      "step": 9800
    },
    {
      "epoch": 0.6216927025571153,
      "grad_norm": 2.2314462661743164,
      "learning_rate": 2.383615580016935e-05,
      "loss": 0.9314,
      "step": 9810
    },
    {
      "epoch": 0.6223264361988656,
      "grad_norm": 2.6850526332855225,
      "learning_rate": 2.3829805249788316e-05,
      "loss": 0.9648,
      "step": 9820
    },
    {
      "epoch": 0.622960169840616,
      "grad_norm": 2.469541549682617,
      "learning_rate": 2.3823454699407282e-05,
      "loss": 0.9149,
      "step": 9830
    },
    {
      "epoch": 0.6235939034823663,
      "grad_norm": 2.6978392601013184,
      "learning_rate": 2.381710414902625e-05,
      "loss": 0.9204,
      "step": 9840
    },
    {
      "epoch": 0.6242276371241168,
      "grad_norm": 2.297720193862915,
      "learning_rate": 2.3810753598645215e-05,
      "loss": 0.9068,
      "step": 9850
    },
    {
      "epoch": 0.6248613707658671,
      "grad_norm": 2.3910019397735596,
      "learning_rate": 2.3804403048264185e-05,
      "loss": 0.9437,
      "step": 9860
    },
    {
      "epoch": 0.6254951044076175,
      "grad_norm": 2.6271259784698486,
      "learning_rate": 2.379805249788315e-05,
      "loss": 0.9029,
      "step": 9870
    },
    {
      "epoch": 0.6261288380493678,
      "grad_norm": 3.0717432498931885,
      "learning_rate": 2.3791701947502114e-05,
      "loss": 1.0094,
      "step": 9880
    },
    {
      "epoch": 0.6267625716911183,
      "grad_norm": 2.6822967529296875,
      "learning_rate": 2.3785351397121084e-05,
      "loss": 0.9071,
      "step": 9890
    },
    {
      "epoch": 0.6273963053328686,
      "grad_norm": 2.7336204051971436,
      "learning_rate": 2.377900084674005e-05,
      "loss": 0.9177,
      "step": 9900
    },
    {
      "epoch": 0.628030038974619,
      "grad_norm": 3.177675247192383,
      "learning_rate": 2.3772650296359017e-05,
      "loss": 0.9388,
      "step": 9910
    },
    {
      "epoch": 0.6286637726163693,
      "grad_norm": 2.5609583854675293,
      "learning_rate": 2.3766299745977987e-05,
      "loss": 0.9453,
      "step": 9920
    },
    {
      "epoch": 0.6292975062581198,
      "grad_norm": 2.173006534576416,
      "learning_rate": 2.3759949195596953e-05,
      "loss": 0.9492,
      "step": 9930
    },
    {
      "epoch": 0.6299312398998701,
      "grad_norm": 2.6579909324645996,
      "learning_rate": 2.375359864521592e-05,
      "loss": 0.9026,
      "step": 9940
    },
    {
      "epoch": 0.6305649735416204,
      "grad_norm": 2.7209458351135254,
      "learning_rate": 2.3747248094834886e-05,
      "loss": 0.9007,
      "step": 9950
    },
    {
      "epoch": 0.6311987071833708,
      "grad_norm": 2.345783233642578,
      "learning_rate": 2.3740897544453853e-05,
      "loss": 0.9198,
      "step": 9960
    },
    {
      "epoch": 0.6318324408251212,
      "grad_norm": 2.953462839126587,
      "learning_rate": 2.3734546994072822e-05,
      "loss": 0.9217,
      "step": 9970
    },
    {
      "epoch": 0.6324661744668716,
      "grad_norm": 2.7734620571136475,
      "learning_rate": 2.372819644369179e-05,
      "loss": 0.9247,
      "step": 9980
    },
    {
      "epoch": 0.6330999081086219,
      "grad_norm": 2.6670899391174316,
      "learning_rate": 2.3721845893310752e-05,
      "loss": 0.9548,
      "step": 9990
    },
    {
      "epoch": 0.6337336417503723,
      "grad_norm": 2.360053777694702,
      "learning_rate": 2.371549534292972e-05,
      "loss": 0.9249,
      "step": 10000
    },
    {
      "epoch": 0.6343673753921227,
      "grad_norm": 3.006300449371338,
      "learning_rate": 2.3709144792548688e-05,
      "loss": 0.9488,
      "step": 10010
    },
    {
      "epoch": 0.6350011090338731,
      "grad_norm": 2.7210474014282227,
      "learning_rate": 2.3702794242167654e-05,
      "loss": 0.9343,
      "step": 10020
    },
    {
      "epoch": 0.6356348426756234,
      "grad_norm": 2.5297038555145264,
      "learning_rate": 2.3696443691786624e-05,
      "loss": 0.9488,
      "step": 10030
    },
    {
      "epoch": 0.6362685763173738,
      "grad_norm": 3.0011544227600098,
      "learning_rate": 2.3690093141405587e-05,
      "loss": 0.9498,
      "step": 10040
    },
    {
      "epoch": 0.6369023099591242,
      "grad_norm": 3.118892192840576,
      "learning_rate": 2.3683742591024554e-05,
      "loss": 0.9867,
      "step": 10050
    },
    {
      "epoch": 0.6375360436008746,
      "grad_norm": 2.2093753814697266,
      "learning_rate": 2.3677392040643524e-05,
      "loss": 0.9283,
      "step": 10060
    },
    {
      "epoch": 0.6381697772426249,
      "grad_norm": 2.9644699096679688,
      "learning_rate": 2.367104149026249e-05,
      "loss": 0.9281,
      "step": 10070
    },
    {
      "epoch": 0.6388035108843753,
      "grad_norm": 2.250732898712158,
      "learning_rate": 2.366469093988146e-05,
      "loss": 0.9396,
      "step": 10080
    },
    {
      "epoch": 0.6394372445261257,
      "grad_norm": 2.5887458324432373,
      "learning_rate": 2.3658340389500423e-05,
      "loss": 0.9436,
      "step": 10090
    },
    {
      "epoch": 0.6400709781678761,
      "grad_norm": 2.676168441772461,
      "learning_rate": 2.365198983911939e-05,
      "loss": 0.9286,
      "step": 10100
    },
    {
      "epoch": 0.6407047118096264,
      "grad_norm": 2.3780431747436523,
      "learning_rate": 2.364563928873836e-05,
      "loss": 0.9105,
      "step": 10110
    },
    {
      "epoch": 0.6413384454513767,
      "grad_norm": 2.314903974533081,
      "learning_rate": 2.3639288738357326e-05,
      "loss": 0.9368,
      "step": 10120
    },
    {
      "epoch": 0.6419721790931272,
      "grad_norm": 2.4001986980438232,
      "learning_rate": 2.3632938187976292e-05,
      "loss": 0.9182,
      "step": 10130
    },
    {
      "epoch": 0.6426059127348775,
      "grad_norm": 2.857478380203247,
      "learning_rate": 2.362658763759526e-05,
      "loss": 0.9705,
      "step": 10140
    },
    {
      "epoch": 0.6432396463766279,
      "grad_norm": 2.348262310028076,
      "learning_rate": 2.3620237087214225e-05,
      "loss": 0.9188,
      "step": 10150
    },
    {
      "epoch": 0.6438733800183782,
      "grad_norm": 2.8464136123657227,
      "learning_rate": 2.361388653683319e-05,
      "loss": 0.9645,
      "step": 10160
    },
    {
      "epoch": 0.6445071136601287,
      "grad_norm": 2.6954705715179443,
      "learning_rate": 2.360753598645216e-05,
      "loss": 0.9534,
      "step": 10170
    },
    {
      "epoch": 0.645140847301879,
      "grad_norm": 2.731018543243408,
      "learning_rate": 2.3601185436071127e-05,
      "loss": 0.9679,
      "step": 10180
    },
    {
      "epoch": 0.6457745809436294,
      "grad_norm": 2.4714765548706055,
      "learning_rate": 2.3594834885690094e-05,
      "loss": 0.9033,
      "step": 10190
    },
    {
      "epoch": 0.6464083145853797,
      "grad_norm": 2.6243956089019775,
      "learning_rate": 2.358848433530906e-05,
      "loss": 0.9285,
      "step": 10200
    },
    {
      "epoch": 0.6470420482271302,
      "grad_norm": 3.1798925399780273,
      "learning_rate": 2.3582133784928027e-05,
      "loss": 0.9112,
      "step": 10210
    },
    {
      "epoch": 0.6476757818688805,
      "grad_norm": 2.929896116256714,
      "learning_rate": 2.3575783234546993e-05,
      "loss": 0.9156,
      "step": 10220
    },
    {
      "epoch": 0.6483095155106309,
      "grad_norm": 2.8251278400421143,
      "learning_rate": 2.3569432684165963e-05,
      "loss": 0.9027,
      "step": 10230
    },
    {
      "epoch": 0.6489432491523812,
      "grad_norm": 3.9813013076782227,
      "learning_rate": 2.356308213378493e-05,
      "loss": 0.9102,
      "step": 10240
    },
    {
      "epoch": 0.6495769827941317,
      "grad_norm": 2.88022518157959,
      "learning_rate": 2.3556731583403896e-05,
      "loss": 0.8874,
      "step": 10250
    },
    {
      "epoch": 0.650210716435882,
      "grad_norm": 3.0530788898468018,
      "learning_rate": 2.3550381033022862e-05,
      "loss": 0.9463,
      "step": 10260
    },
    {
      "epoch": 0.6508444500776324,
      "grad_norm": 2.847869634628296,
      "learning_rate": 2.354403048264183e-05,
      "loss": 0.9419,
      "step": 10270
    },
    {
      "epoch": 0.6514781837193827,
      "grad_norm": 2.6842551231384277,
      "learning_rate": 2.35376799322608e-05,
      "loss": 0.916,
      "step": 10280
    },
    {
      "epoch": 0.6521119173611332,
      "grad_norm": 2.7051808834075928,
      "learning_rate": 2.3531329381879765e-05,
      "loss": 0.9124,
      "step": 10290
    },
    {
      "epoch": 0.6527456510028835,
      "grad_norm": 2.8838863372802734,
      "learning_rate": 2.3524978831498728e-05,
      "loss": 0.9493,
      "step": 10300
    },
    {
      "epoch": 0.6533793846446339,
      "grad_norm": 2.8712515830993652,
      "learning_rate": 2.3518628281117698e-05,
      "loss": 0.9406,
      "step": 10310
    },
    {
      "epoch": 0.6540131182863842,
      "grad_norm": 3.0717456340789795,
      "learning_rate": 2.3512277730736664e-05,
      "loss": 0.9186,
      "step": 10320
    },
    {
      "epoch": 0.6546468519281347,
      "grad_norm": 3.305572986602783,
      "learning_rate": 2.350592718035563e-05,
      "loss": 0.8732,
      "step": 10330
    },
    {
      "epoch": 0.655280585569885,
      "grad_norm": 2.6345016956329346,
      "learning_rate": 2.34995766299746e-05,
      "loss": 0.9014,
      "step": 10340
    },
    {
      "epoch": 0.6559143192116353,
      "grad_norm": 2.774024724960327,
      "learning_rate": 2.3493226079593563e-05,
      "loss": 0.9452,
      "step": 10350
    },
    {
      "epoch": 0.6565480528533857,
      "grad_norm": 2.5123300552368164,
      "learning_rate": 2.348687552921253e-05,
      "loss": 0.9112,
      "step": 10360
    },
    {
      "epoch": 0.6571817864951361,
      "grad_norm": 2.3397152423858643,
      "learning_rate": 2.34805249788315e-05,
      "loss": 0.8959,
      "step": 10370
    },
    {
      "epoch": 0.6578155201368865,
      "grad_norm": 2.8064517974853516,
      "learning_rate": 2.3474174428450466e-05,
      "loss": 0.8671,
      "step": 10380
    },
    {
      "epoch": 0.6584492537786368,
      "grad_norm": 3.03979229927063,
      "learning_rate": 2.3467823878069436e-05,
      "loss": 0.9057,
      "step": 10390
    },
    {
      "epoch": 0.6590829874203872,
      "grad_norm": 2.686368465423584,
      "learning_rate": 2.3461473327688402e-05,
      "loss": 0.9061,
      "step": 10400
    },
    {
      "epoch": 0.6597167210621376,
      "grad_norm": 3.2508182525634766,
      "learning_rate": 2.3455122777307365e-05,
      "loss": 0.9258,
      "step": 10410
    },
    {
      "epoch": 0.660350454703888,
      "grad_norm": 2.428299903869629,
      "learning_rate": 2.3448772226926335e-05,
      "loss": 0.9178,
      "step": 10420
    },
    {
      "epoch": 0.6609841883456383,
      "grad_norm": 3.343003749847412,
      "learning_rate": 2.34424216765453e-05,
      "loss": 0.9308,
      "step": 10430
    },
    {
      "epoch": 0.6616179219873887,
      "grad_norm": 3.5411925315856934,
      "learning_rate": 2.3436071126164268e-05,
      "loss": 0.9091,
      "step": 10440
    },
    {
      "epoch": 0.6622516556291391,
      "grad_norm": 2.6989705562591553,
      "learning_rate": 2.3429720575783238e-05,
      "loss": 0.9202,
      "step": 10450
    },
    {
      "epoch": 0.6628853892708895,
      "grad_norm": 3.07995867729187,
      "learning_rate": 2.34233700254022e-05,
      "loss": 0.9419,
      "step": 10460
    },
    {
      "epoch": 0.6635191229126398,
      "grad_norm": 2.901003122329712,
      "learning_rate": 2.3417019475021167e-05,
      "loss": 0.8604,
      "step": 10470
    },
    {
      "epoch": 0.6641528565543902,
      "grad_norm": 2.6294686794281006,
      "learning_rate": 2.3410668924640137e-05,
      "loss": 0.9109,
      "step": 10480
    },
    {
      "epoch": 0.6647865901961406,
      "grad_norm": 2.763125419616699,
      "learning_rate": 2.3404318374259104e-05,
      "loss": 0.9216,
      "step": 10490
    },
    {
      "epoch": 0.665420323837891,
      "grad_norm": 3.172152280807495,
      "learning_rate": 2.339796782387807e-05,
      "loss": 0.9102,
      "step": 10500
    },
    {
      "epoch": 0.6660540574796413,
      "grad_norm": 3.22257661819458,
      "learning_rate": 2.3391617273497036e-05,
      "loss": 0.8977,
      "step": 10510
    },
    {
      "epoch": 0.6666877911213916,
      "grad_norm": 2.869058132171631,
      "learning_rate": 2.3385266723116003e-05,
      "loss": 0.8761,
      "step": 10520
    },
    {
      "epoch": 0.6673215247631421,
      "grad_norm": 2.648646593093872,
      "learning_rate": 2.337891617273497e-05,
      "loss": 0.9067,
      "step": 10530
    },
    {
      "epoch": 0.6679552584048924,
      "grad_norm": 2.341895580291748,
      "learning_rate": 2.337256562235394e-05,
      "loss": 0.8921,
      "step": 10540
    },
    {
      "epoch": 0.6685889920466428,
      "grad_norm": 2.8787131309509277,
      "learning_rate": 2.3366215071972906e-05,
      "loss": 0.9096,
      "step": 10550
    },
    {
      "epoch": 0.6692227256883931,
      "grad_norm": 2.672619104385376,
      "learning_rate": 2.3359864521591872e-05,
      "loss": 0.8858,
      "step": 10560
    },
    {
      "epoch": 0.6698564593301436,
      "grad_norm": 2.6924679279327393,
      "learning_rate": 2.335351397121084e-05,
      "loss": 0.8981,
      "step": 10570
    },
    {
      "epoch": 0.6704901929718939,
      "grad_norm": 3.0086922645568848,
      "learning_rate": 2.3347163420829805e-05,
      "loss": 0.9086,
      "step": 10580
    },
    {
      "epoch": 0.6711239266136443,
      "grad_norm": 2.637820243835449,
      "learning_rate": 2.3340812870448775e-05,
      "loss": 0.9227,
      "step": 10590
    },
    {
      "epoch": 0.6717576602553946,
      "grad_norm": 2.972930431365967,
      "learning_rate": 2.333446232006774e-05,
      "loss": 0.895,
      "step": 10600
    },
    {
      "epoch": 0.6723913938971451,
      "grad_norm": 2.823545217514038,
      "learning_rate": 2.3328111769686704e-05,
      "loss": 0.9011,
      "step": 10610
    },
    {
      "epoch": 0.6730251275388954,
      "grad_norm": 2.5134923458099365,
      "learning_rate": 2.3321761219305674e-05,
      "loss": 0.9251,
      "step": 10620
    },
    {
      "epoch": 0.6736588611806458,
      "grad_norm": 2.339024543762207,
      "learning_rate": 2.331541066892464e-05,
      "loss": 0.8989,
      "step": 10630
    },
    {
      "epoch": 0.6742925948223961,
      "grad_norm": 2.344633102416992,
      "learning_rate": 2.3309060118543607e-05,
      "loss": 0.9169,
      "step": 10640
    },
    {
      "epoch": 0.6749263284641466,
      "grad_norm": 2.665719985961914,
      "learning_rate": 2.3302709568162577e-05,
      "loss": 0.8758,
      "step": 10650
    },
    {
      "epoch": 0.6755600621058969,
      "grad_norm": 2.7732512950897217,
      "learning_rate": 2.3296359017781543e-05,
      "loss": 0.9271,
      "step": 10660
    },
    {
      "epoch": 0.6761937957476473,
      "grad_norm": 2.7568600177764893,
      "learning_rate": 2.3290008467400506e-05,
      "loss": 0.896,
      "step": 10670
    },
    {
      "epoch": 0.6768275293893976,
      "grad_norm": 2.5581796169281006,
      "learning_rate": 2.3283657917019476e-05,
      "loss": 0.9112,
      "step": 10680
    },
    {
      "epoch": 0.677461263031148,
      "grad_norm": 2.9102089405059814,
      "learning_rate": 2.3277307366638442e-05,
      "loss": 0.9223,
      "step": 10690
    },
    {
      "epoch": 0.6780949966728984,
      "grad_norm": 3.00411319732666,
      "learning_rate": 2.3270956816257412e-05,
      "loss": 0.879,
      "step": 10700
    },
    {
      "epoch": 0.6787287303146488,
      "grad_norm": 3.3148491382598877,
      "learning_rate": 2.326460626587638e-05,
      "loss": 0.9119,
      "step": 10710
    },
    {
      "epoch": 0.6793624639563991,
      "grad_norm": 2.6149353981018066,
      "learning_rate": 2.325825571549534e-05,
      "loss": 0.8818,
      "step": 10720
    },
    {
      "epoch": 0.6799961975981494,
      "grad_norm": 2.787827253341675,
      "learning_rate": 2.325190516511431e-05,
      "loss": 0.9134,
      "step": 10730
    },
    {
      "epoch": 0.6806299312398999,
      "grad_norm": 2.312587022781372,
      "learning_rate": 2.3245554614733278e-05,
      "loss": 0.8685,
      "step": 10740
    },
    {
      "epoch": 0.6812636648816502,
      "grad_norm": 3.3929402828216553,
      "learning_rate": 2.3239204064352244e-05,
      "loss": 0.9143,
      "step": 10750
    },
    {
      "epoch": 0.6818973985234006,
      "grad_norm": 2.9479191303253174,
      "learning_rate": 2.3232853513971214e-05,
      "loss": 0.9011,
      "step": 10760
    },
    {
      "epoch": 0.6825311321651509,
      "grad_norm": 2.700450897216797,
      "learning_rate": 2.3226502963590177e-05,
      "loss": 0.9303,
      "step": 10770
    },
    {
      "epoch": 0.6831648658069014,
      "grad_norm": 2.6712090969085693,
      "learning_rate": 2.3220152413209144e-05,
      "loss": 0.8967,
      "step": 10780
    },
    {
      "epoch": 0.6837985994486517,
      "grad_norm": 2.721543312072754,
      "learning_rate": 2.3213801862828113e-05,
      "loss": 0.8924,
      "step": 10790
    },
    {
      "epoch": 0.6844323330904021,
      "grad_norm": 2.9241979122161865,
      "learning_rate": 2.320745131244708e-05,
      "loss": 0.9323,
      "step": 10800
    },
    {
      "epoch": 0.6850660667321524,
      "grad_norm": 2.729719638824463,
      "learning_rate": 2.3201100762066046e-05,
      "loss": 0.8984,
      "step": 10810
    },
    {
      "epoch": 0.6856998003739029,
      "grad_norm": 2.7382543087005615,
      "learning_rate": 2.3194750211685013e-05,
      "loss": 0.8867,
      "step": 10820
    },
    {
      "epoch": 0.6863335340156532,
      "grad_norm": 3.4753711223602295,
      "learning_rate": 2.318839966130398e-05,
      "loss": 0.8721,
      "step": 10830
    },
    {
      "epoch": 0.6869672676574036,
      "grad_norm": 2.6310958862304688,
      "learning_rate": 2.3182049110922946e-05,
      "loss": 0.8966,
      "step": 10840
    },
    {
      "epoch": 0.6876010012991539,
      "grad_norm": 3.0650229454040527,
      "learning_rate": 2.3175698560541915e-05,
      "loss": 0.9391,
      "step": 10850
    },
    {
      "epoch": 0.6882347349409044,
      "grad_norm": 3.2741494178771973,
      "learning_rate": 2.3169348010160882e-05,
      "loss": 0.9104,
      "step": 10860
    },
    {
      "epoch": 0.6888684685826547,
      "grad_norm": 2.612020492553711,
      "learning_rate": 2.3162997459779848e-05,
      "loss": 0.8819,
      "step": 10870
    },
    {
      "epoch": 0.6895022022244051,
      "grad_norm": 2.812664270401001,
      "learning_rate": 2.3156646909398815e-05,
      "loss": 0.9037,
      "step": 10880
    },
    {
      "epoch": 0.6901359358661554,
      "grad_norm": 2.726696014404297,
      "learning_rate": 2.315029635901778e-05,
      "loss": 0.9293,
      "step": 10890
    },
    {
      "epoch": 0.6907696695079059,
      "grad_norm": 2.367039918899536,
      "learning_rate": 2.314394580863675e-05,
      "loss": 0.919,
      "step": 10900
    },
    {
      "epoch": 0.6914034031496562,
      "grad_norm": 2.454620122909546,
      "learning_rate": 2.3137595258255717e-05,
      "loss": 0.8982,
      "step": 10910
    },
    {
      "epoch": 0.6920371367914065,
      "grad_norm": 2.831268072128296,
      "learning_rate": 2.3131244707874684e-05,
      "loss": 0.8687,
      "step": 10920
    },
    {
      "epoch": 0.6926708704331569,
      "grad_norm": 2.8648037910461426,
      "learning_rate": 2.312489415749365e-05,
      "loss": 0.9344,
      "step": 10930
    },
    {
      "epoch": 0.6933046040749073,
      "grad_norm": 2.675287961959839,
      "learning_rate": 2.3118543607112617e-05,
      "loss": 0.9088,
      "step": 10940
    },
    {
      "epoch": 0.6939383377166577,
      "grad_norm": 2.368258476257324,
      "learning_rate": 2.3112193056731583e-05,
      "loss": 0.8605,
      "step": 10950
    },
    {
      "epoch": 0.694572071358408,
      "grad_norm": 3.305015802383423,
      "learning_rate": 2.3105842506350553e-05,
      "loss": 0.9084,
      "step": 10960
    },
    {
      "epoch": 0.6952058050001584,
      "grad_norm": 2.578406572341919,
      "learning_rate": 2.309949195596952e-05,
      "loss": 0.9029,
      "step": 10970
    },
    {
      "epoch": 0.6958395386419088,
      "grad_norm": 2.9109609127044678,
      "learning_rate": 2.3093141405588482e-05,
      "loss": 0.8721,
      "step": 10980
    },
    {
      "epoch": 0.6964732722836592,
      "grad_norm": 2.3781394958496094,
      "learning_rate": 2.3086790855207452e-05,
      "loss": 0.8601,
      "step": 10990
    },
    {
      "epoch": 0.6971070059254095,
      "grad_norm": 2.4462196826934814,
      "learning_rate": 2.308044030482642e-05,
      "loss": 0.8758,
      "step": 11000
    },
    {
      "epoch": 0.6977407395671599,
      "grad_norm": 2.601332664489746,
      "learning_rate": 2.3074089754445385e-05,
      "loss": 0.9348,
      "step": 11010
    },
    {
      "epoch": 0.6983744732089103,
      "grad_norm": 2.72023868560791,
      "learning_rate": 2.3067739204064355e-05,
      "loss": 0.8834,
      "step": 11020
    },
    {
      "epoch": 0.6990082068506607,
      "grad_norm": 3.4715285301208496,
      "learning_rate": 2.3061388653683318e-05,
      "loss": 0.9089,
      "step": 11030
    },
    {
      "epoch": 0.699641940492411,
      "grad_norm": 2.7907772064208984,
      "learning_rate": 2.3055038103302288e-05,
      "loss": 0.884,
      "step": 11040
    },
    {
      "epoch": 0.7002756741341614,
      "grad_norm": 2.7843475341796875,
      "learning_rate": 2.3048687552921254e-05,
      "loss": 0.8843,
      "step": 11050
    },
    {
      "epoch": 0.7009094077759118,
      "grad_norm": 3.3096320629119873,
      "learning_rate": 2.304233700254022e-05,
      "loss": 0.8569,
      "step": 11060
    },
    {
      "epoch": 0.7015431414176622,
      "grad_norm": 2.4272050857543945,
      "learning_rate": 2.303598645215919e-05,
      "loss": 0.8574,
      "step": 11070
    },
    {
      "epoch": 0.7021768750594125,
      "grad_norm": 2.7075891494750977,
      "learning_rate": 2.3029635901778153e-05,
      "loss": 0.9298,
      "step": 11080
    },
    {
      "epoch": 0.7028106087011629,
      "grad_norm": 2.5774388313293457,
      "learning_rate": 2.302328535139712e-05,
      "loss": 0.8846,
      "step": 11090
    },
    {
      "epoch": 0.7034443423429133,
      "grad_norm": 2.2494261264801025,
      "learning_rate": 2.301693480101609e-05,
      "loss": 0.8439,
      "step": 11100
    },
    {
      "epoch": 0.7040780759846637,
      "grad_norm": 2.643949031829834,
      "learning_rate": 2.3010584250635056e-05,
      "loss": 0.8922,
      "step": 11110
    },
    {
      "epoch": 0.704711809626414,
      "grad_norm": 2.3081560134887695,
      "learning_rate": 2.3004868755292126e-05,
      "loss": 0.9226,
      "step": 11120
    },
    {
      "epoch": 0.7053455432681643,
      "grad_norm": 3.0476882457733154,
      "learning_rate": 2.2998518204911093e-05,
      "loss": 0.9106,
      "step": 11130
    },
    {
      "epoch": 0.7059792769099148,
      "grad_norm": 3.016237497329712,
      "learning_rate": 2.299216765453006e-05,
      "loss": 0.9174,
      "step": 11140
    },
    {
      "epoch": 0.7066130105516651,
      "grad_norm": 2.286888837814331,
      "learning_rate": 2.2985817104149026e-05,
      "loss": 0.9112,
      "step": 11150
    },
    {
      "epoch": 0.7072467441934155,
      "grad_norm": 2.496037006378174,
      "learning_rate": 2.2979466553767992e-05,
      "loss": 0.8402,
      "step": 11160
    },
    {
      "epoch": 0.7078804778351658,
      "grad_norm": 2.72318696975708,
      "learning_rate": 2.2973116003386962e-05,
      "loss": 0.8763,
      "step": 11170
    },
    {
      "epoch": 0.7085142114769163,
      "grad_norm": 2.3716604709625244,
      "learning_rate": 2.296676545300593e-05,
      "loss": 0.9129,
      "step": 11180
    },
    {
      "epoch": 0.7091479451186666,
      "grad_norm": 3.000404119491577,
      "learning_rate": 2.2960414902624895e-05,
      "loss": 0.916,
      "step": 11190
    },
    {
      "epoch": 0.709781678760417,
      "grad_norm": 2.4323890209198,
      "learning_rate": 2.295406435224386e-05,
      "loss": 0.8683,
      "step": 11200
    },
    {
      "epoch": 0.7104154124021673,
      "grad_norm": 2.332850694656372,
      "learning_rate": 2.2947713801862828e-05,
      "loss": 0.8764,
      "step": 11210
    },
    {
      "epoch": 0.7110491460439178,
      "grad_norm": 2.746978282928467,
      "learning_rate": 2.2941363251481797e-05,
      "loss": 0.8951,
      "step": 11220
    },
    {
      "epoch": 0.7116828796856681,
      "grad_norm": 2.3636317253112793,
      "learning_rate": 2.2935012701100764e-05,
      "loss": 0.8805,
      "step": 11230
    },
    {
      "epoch": 0.7123166133274185,
      "grad_norm": 2.7804360389709473,
      "learning_rate": 2.2928662150719727e-05,
      "loss": 0.8509,
      "step": 11240
    },
    {
      "epoch": 0.7129503469691688,
      "grad_norm": 2.5125837326049805,
      "learning_rate": 2.2922311600338697e-05,
      "loss": 0.8424,
      "step": 11250
    },
    {
      "epoch": 0.7135840806109193,
      "grad_norm": 2.574906826019287,
      "learning_rate": 2.2915961049957663e-05,
      "loss": 0.8885,
      "step": 11260
    },
    {
      "epoch": 0.7142178142526696,
      "grad_norm": 2.5433454513549805,
      "learning_rate": 2.290961049957663e-05,
      "loss": 0.8964,
      "step": 11270
    },
    {
      "epoch": 0.71485154789442,
      "grad_norm": 3.0278000831604004,
      "learning_rate": 2.29032599491956e-05,
      "loss": 0.9307,
      "step": 11280
    },
    {
      "epoch": 0.7154852815361703,
      "grad_norm": 2.723392963409424,
      "learning_rate": 2.2896909398814566e-05,
      "loss": 0.8782,
      "step": 11290
    },
    {
      "epoch": 0.7161190151779208,
      "grad_norm": 3.7283589839935303,
      "learning_rate": 2.289055884843353e-05,
      "loss": 0.8868,
      "step": 11300
    },
    {
      "epoch": 0.7167527488196711,
      "grad_norm": 2.531069278717041,
      "learning_rate": 2.28842082980525e-05,
      "loss": 0.8258,
      "step": 11310
    },
    {
      "epoch": 0.7173864824614214,
      "grad_norm": 3.033503770828247,
      "learning_rate": 2.2877857747671465e-05,
      "loss": 0.8863,
      "step": 11320
    },
    {
      "epoch": 0.7180202161031718,
      "grad_norm": 2.486292839050293,
      "learning_rate": 2.2871507197290435e-05,
      "loss": 0.8706,
      "step": 11330
    },
    {
      "epoch": 0.7186539497449222,
      "grad_norm": 2.707362413406372,
      "learning_rate": 2.28651566469094e-05,
      "loss": 0.8678,
      "step": 11340
    },
    {
      "epoch": 0.7192876833866726,
      "grad_norm": 2.932262420654297,
      "learning_rate": 2.2858806096528364e-05,
      "loss": 0.8907,
      "step": 11350
    },
    {
      "epoch": 0.7199214170284229,
      "grad_norm": 2.8879029750823975,
      "learning_rate": 2.2852455546147334e-05,
      "loss": 0.9146,
      "step": 11360
    },
    {
      "epoch": 0.7205551506701733,
      "grad_norm": 2.2523298263549805,
      "learning_rate": 2.28461049957663e-05,
      "loss": 0.8596,
      "step": 11370
    },
    {
      "epoch": 0.7211888843119237,
      "grad_norm": 2.618563652038574,
      "learning_rate": 2.2839754445385267e-05,
      "loss": 0.8481,
      "step": 11380
    },
    {
      "epoch": 0.7218226179536741,
      "grad_norm": 2.689938545227051,
      "learning_rate": 2.2833403895004237e-05,
      "loss": 0.8797,
      "step": 11390
    },
    {
      "epoch": 0.7224563515954244,
      "grad_norm": 2.373206377029419,
      "learning_rate": 2.28270533446232e-05,
      "loss": 0.9271,
      "step": 11400
    },
    {
      "epoch": 0.7230900852371748,
      "grad_norm": 2.5451743602752686,
      "learning_rate": 2.2820702794242166e-05,
      "loss": 0.8609,
      "step": 11410
    },
    {
      "epoch": 0.7237238188789252,
      "grad_norm": 2.369440793991089,
      "learning_rate": 2.2814352243861136e-05,
      "loss": 0.8667,
      "step": 11420
    },
    {
      "epoch": 0.7243575525206756,
      "grad_norm": 2.7511088848114014,
      "learning_rate": 2.2808001693480103e-05,
      "loss": 0.8864,
      "step": 11430
    },
    {
      "epoch": 0.7249912861624259,
      "grad_norm": 2.5427684783935547,
      "learning_rate": 2.280165114309907e-05,
      "loss": 0.8692,
      "step": 11440
    },
    {
      "epoch": 0.7256250198041763,
      "grad_norm": 2.4423906803131104,
      "learning_rate": 2.2795300592718035e-05,
      "loss": 0.8415,
      "step": 11450
    },
    {
      "epoch": 0.7262587534459267,
      "grad_norm": 2.6075615882873535,
      "learning_rate": 2.2788950042337002e-05,
      "loss": 0.9333,
      "step": 11460
    },
    {
      "epoch": 0.7268924870876771,
      "grad_norm": 3.1445517539978027,
      "learning_rate": 2.278259949195597e-05,
      "loss": 0.912,
      "step": 11470
    },
    {
      "epoch": 0.7275262207294274,
      "grad_norm": 2.5151641368865967,
      "learning_rate": 2.2776248941574938e-05,
      "loss": 0.8766,
      "step": 11480
    },
    {
      "epoch": 0.7281599543711778,
      "grad_norm": 2.8760788440704346,
      "learning_rate": 2.2769898391193905e-05,
      "loss": 0.9149,
      "step": 11490
    },
    {
      "epoch": 0.7287936880129282,
      "grad_norm": 2.9003918170928955,
      "learning_rate": 2.276354784081287e-05,
      "loss": 0.881,
      "step": 11500
    },
    {
      "epoch": 0.7294274216546786,
      "grad_norm": 2.431610345840454,
      "learning_rate": 2.2757197290431837e-05,
      "loss": 0.8651,
      "step": 11510
    },
    {
      "epoch": 0.7300611552964289,
      "grad_norm": 2.914886951446533,
      "learning_rate": 2.2750846740050804e-05,
      "loss": 0.8892,
      "step": 11520
    },
    {
      "epoch": 0.7306948889381792,
      "grad_norm": 2.566147565841675,
      "learning_rate": 2.2744496189669774e-05,
      "loss": 0.8826,
      "step": 11530
    },
    {
      "epoch": 0.7313286225799297,
      "grad_norm": 2.5795938968658447,
      "learning_rate": 2.273814563928874e-05,
      "loss": 0.9165,
      "step": 11540
    },
    {
      "epoch": 0.73196235622168,
      "grad_norm": 2.162437677383423,
      "learning_rate": 2.2731795088907707e-05,
      "loss": 0.8746,
      "step": 11550
    },
    {
      "epoch": 0.7325960898634304,
      "grad_norm": 2.495314121246338,
      "learning_rate": 2.2725444538526673e-05,
      "loss": 0.9113,
      "step": 11560
    },
    {
      "epoch": 0.7332298235051807,
      "grad_norm": 2.498319387435913,
      "learning_rate": 2.271909398814564e-05,
      "loss": 0.8647,
      "step": 11570
    },
    {
      "epoch": 0.7338635571469312,
      "grad_norm": 2.3378353118896484,
      "learning_rate": 2.2712743437764606e-05,
      "loss": 0.854,
      "step": 11580
    },
    {
      "epoch": 0.7344972907886815,
      "grad_norm": 3.1113674640655518,
      "learning_rate": 2.2706392887383576e-05,
      "loss": 0.8861,
      "step": 11590
    },
    {
      "epoch": 0.7351310244304319,
      "grad_norm": 2.745837450027466,
      "learning_rate": 2.2700042337002542e-05,
      "loss": 0.8759,
      "step": 11600
    },
    {
      "epoch": 0.7357647580721822,
      "grad_norm": 2.3565683364868164,
      "learning_rate": 2.2693691786621505e-05,
      "loss": 0.8615,
      "step": 11610
    },
    {
      "epoch": 0.7363984917139327,
      "grad_norm": 3.68918514251709,
      "learning_rate": 2.2687341236240475e-05,
      "loss": 0.8654,
      "step": 11620
    },
    {
      "epoch": 0.737032225355683,
      "grad_norm": 2.7015891075134277,
      "learning_rate": 2.268099068585944e-05,
      "loss": 0.8951,
      "step": 11630
    },
    {
      "epoch": 0.7376659589974334,
      "grad_norm": 2.5554611682891846,
      "learning_rate": 2.2674640135478408e-05,
      "loss": 0.8858,
      "step": 11640
    },
    {
      "epoch": 0.7382996926391837,
      "grad_norm": 2.66017746925354,
      "learning_rate": 2.2668289585097378e-05,
      "loss": 0.9296,
      "step": 11650
    },
    {
      "epoch": 0.7389334262809342,
      "grad_norm": 2.633955955505371,
      "learning_rate": 2.266193903471634e-05,
      "loss": 0.9018,
      "step": 11660
    },
    {
      "epoch": 0.7395671599226845,
      "grad_norm": 3.0893666744232178,
      "learning_rate": 2.265558848433531e-05,
      "loss": 0.8756,
      "step": 11670
    },
    {
      "epoch": 0.7402008935644349,
      "grad_norm": 2.529947280883789,
      "learning_rate": 2.2649237933954277e-05,
      "loss": 0.9133,
      "step": 11680
    },
    {
      "epoch": 0.7408346272061852,
      "grad_norm": 3.226402997970581,
      "learning_rate": 2.2642887383573243e-05,
      "loss": 0.8985,
      "step": 11690
    },
    {
      "epoch": 0.7414683608479357,
      "grad_norm": 2.901736259460449,
      "learning_rate": 2.2636536833192213e-05,
      "loss": 0.9202,
      "step": 11700
    },
    {
      "epoch": 0.742102094489686,
      "grad_norm": 3.1939687728881836,
      "learning_rate": 2.2630186282811176e-05,
      "loss": 0.8774,
      "step": 11710
    },
    {
      "epoch": 0.7427358281314363,
      "grad_norm": 2.1364517211914062,
      "learning_rate": 2.2623835732430143e-05,
      "loss": 0.9423,
      "step": 11720
    },
    {
      "epoch": 0.7433695617731867,
      "grad_norm": 2.455350399017334,
      "learning_rate": 2.2617485182049112e-05,
      "loss": 0.9302,
      "step": 11730
    },
    {
      "epoch": 0.7440032954149371,
      "grad_norm": 2.5191171169281006,
      "learning_rate": 2.261113463166808e-05,
      "loss": 0.9084,
      "step": 11740
    },
    {
      "epoch": 0.7446370290566875,
      "grad_norm": 2.5917229652404785,
      "learning_rate": 2.2604784081287045e-05,
      "loss": 0.9331,
      "step": 11750
    },
    {
      "epoch": 0.7452707626984378,
      "grad_norm": 2.761322021484375,
      "learning_rate": 2.259843353090601e-05,
      "loss": 0.894,
      "step": 11760
    },
    {
      "epoch": 0.7459044963401882,
      "grad_norm": 2.8212873935699463,
      "learning_rate": 2.2592082980524978e-05,
      "loss": 0.9054,
      "step": 11770
    },
    {
      "epoch": 0.7465382299819386,
      "grad_norm": 2.4036409854888916,
      "learning_rate": 2.2585732430143944e-05,
      "loss": 0.8636,
      "step": 11780
    },
    {
      "epoch": 0.747171963623689,
      "grad_norm": 2.576643466949463,
      "learning_rate": 2.2579381879762914e-05,
      "loss": 0.9039,
      "step": 11790
    },
    {
      "epoch": 0.7478056972654393,
      "grad_norm": 2.6737866401672363,
      "learning_rate": 2.257303132938188e-05,
      "loss": 0.8885,
      "step": 11800
    },
    {
      "epoch": 0.7484394309071897,
      "grad_norm": 2.9098410606384277,
      "learning_rate": 2.256668077900085e-05,
      "loss": 0.9211,
      "step": 11810
    },
    {
      "epoch": 0.7490731645489401,
      "grad_norm": 2.3220114707946777,
      "learning_rate": 2.2560330228619814e-05,
      "loss": 0.8356,
      "step": 11820
    },
    {
      "epoch": 0.7497068981906905,
      "grad_norm": 2.430539131164551,
      "learning_rate": 2.255397967823878e-05,
      "loss": 0.8668,
      "step": 11830
    },
    {
      "epoch": 0.7503406318324408,
      "grad_norm": 2.569017171859741,
      "learning_rate": 2.254762912785775e-05,
      "loss": 0.8361,
      "step": 11840
    },
    {
      "epoch": 0.7509743654741912,
      "grad_norm": 2.6462411880493164,
      "learning_rate": 2.2541278577476716e-05,
      "loss": 0.9202,
      "step": 11850
    },
    {
      "epoch": 0.7516080991159416,
      "grad_norm": 2.708401918411255,
      "learning_rate": 2.2534928027095683e-05,
      "loss": 0.8776,
      "step": 11860
    },
    {
      "epoch": 0.752241832757692,
      "grad_norm": 2.968353271484375,
      "learning_rate": 2.252857747671465e-05,
      "loss": 0.869,
      "step": 11870
    },
    {
      "epoch": 0.7528755663994423,
      "grad_norm": 2.632642984390259,
      "learning_rate": 2.2522226926333616e-05,
      "loss": 0.8998,
      "step": 11880
    },
    {
      "epoch": 0.7535093000411927,
      "grad_norm": 2.674168109893799,
      "learning_rate": 2.2515876375952582e-05,
      "loss": 0.8591,
      "step": 11890
    },
    {
      "epoch": 0.7541430336829431,
      "grad_norm": 2.3604800701141357,
      "learning_rate": 2.2509525825571552e-05,
      "loss": 0.9083,
      "step": 11900
    },
    {
      "epoch": 0.7547767673246935,
      "grad_norm": 2.653898239135742,
      "learning_rate": 2.2503175275190518e-05,
      "loss": 0.9229,
      "step": 11910
    },
    {
      "epoch": 0.7554105009664438,
      "grad_norm": 2.5257041454315186,
      "learning_rate": 2.249682472480948e-05,
      "loss": 0.8899,
      "step": 11920
    },
    {
      "epoch": 0.7560442346081941,
      "grad_norm": 2.627610445022583,
      "learning_rate": 2.249047417442845e-05,
      "loss": 0.8879,
      "step": 11930
    },
    {
      "epoch": 0.7566779682499446,
      "grad_norm": 2.847301483154297,
      "learning_rate": 2.2484123624047417e-05,
      "loss": 0.9163,
      "step": 11940
    },
    {
      "epoch": 0.7573117018916949,
      "grad_norm": 2.4903602600097656,
      "learning_rate": 2.2477773073666384e-05,
      "loss": 0.8452,
      "step": 11950
    },
    {
      "epoch": 0.7579454355334453,
      "grad_norm": 2.7806508541107178,
      "learning_rate": 2.2471422523285354e-05,
      "loss": 0.8932,
      "step": 11960
    },
    {
      "epoch": 0.7585791691751956,
      "grad_norm": 2.513731002807617,
      "learning_rate": 2.2465071972904317e-05,
      "loss": 0.9117,
      "step": 11970
    },
    {
      "epoch": 0.7592129028169461,
      "grad_norm": 2.8140463829040527,
      "learning_rate": 2.2458721422523287e-05,
      "loss": 0.8751,
      "step": 11980
    },
    {
      "epoch": 0.7598466364586964,
      "grad_norm": 2.67763090133667,
      "learning_rate": 2.2452370872142253e-05,
      "loss": 0.8934,
      "step": 11990
    },
    {
      "epoch": 0.7604803701004468,
      "grad_norm": 2.690945863723755,
      "learning_rate": 2.244602032176122e-05,
      "loss": 0.8866,
      "step": 12000
    },
    {
      "epoch": 0.7611141037421971,
      "grad_norm": 2.2616376876831055,
      "learning_rate": 2.243966977138019e-05,
      "loss": 0.8957,
      "step": 12010
    },
    {
      "epoch": 0.7617478373839476,
      "grad_norm": 3.2670459747314453,
      "learning_rate": 2.2433319220999152e-05,
      "loss": 0.8812,
      "step": 12020
    },
    {
      "epoch": 0.7623815710256979,
      "grad_norm": 3.1054654121398926,
      "learning_rate": 2.242696867061812e-05,
      "loss": 0.8816,
      "step": 12030
    },
    {
      "epoch": 0.7630153046674483,
      "grad_norm": 3.0049591064453125,
      "learning_rate": 2.242061812023709e-05,
      "loss": 0.877,
      "step": 12040
    },
    {
      "epoch": 0.7636490383091986,
      "grad_norm": 2.846219301223755,
      "learning_rate": 2.2414267569856055e-05,
      "loss": 0.8769,
      "step": 12050
    },
    {
      "epoch": 0.7642827719509491,
      "grad_norm": 2.5906336307525635,
      "learning_rate": 2.240791701947502e-05,
      "loss": 0.8857,
      "step": 12060
    },
    {
      "epoch": 0.7649165055926994,
      "grad_norm": 3.009115695953369,
      "learning_rate": 2.240156646909399e-05,
      "loss": 0.8995,
      "step": 12070
    },
    {
      "epoch": 0.7655502392344498,
      "grad_norm": 2.697626829147339,
      "learning_rate": 2.2395215918712954e-05,
      "loss": 0.8815,
      "step": 12080
    },
    {
      "epoch": 0.7661839728762001,
      "grad_norm": 2.483557939529419,
      "learning_rate": 2.238886536833192e-05,
      "loss": 0.9155,
      "step": 12090
    },
    {
      "epoch": 0.7668177065179506,
      "grad_norm": 2.527679204940796,
      "learning_rate": 2.238251481795089e-05,
      "loss": 0.8821,
      "step": 12100
    },
    {
      "epoch": 0.7674514401597009,
      "grad_norm": 2.5689666271209717,
      "learning_rate": 2.2376164267569857e-05,
      "loss": 0.8434,
      "step": 12110
    },
    {
      "epoch": 0.7680851738014512,
      "grad_norm": 2.4715137481689453,
      "learning_rate": 2.2369813717188827e-05,
      "loss": 0.8845,
      "step": 12120
    },
    {
      "epoch": 0.7687189074432016,
      "grad_norm": 3.100144147872925,
      "learning_rate": 2.236346316680779e-05,
      "loss": 0.8966,
      "step": 12130
    },
    {
      "epoch": 0.769352641084952,
      "grad_norm": 2.7340478897094727,
      "learning_rate": 2.2357112616426756e-05,
      "loss": 0.8572,
      "step": 12140
    },
    {
      "epoch": 0.7699863747267024,
      "grad_norm": 2.774057149887085,
      "learning_rate": 2.2350762066045726e-05,
      "loss": 0.8632,
      "step": 12150
    },
    {
      "epoch": 0.7706201083684527,
      "grad_norm": 2.8506851196289062,
      "learning_rate": 2.2344411515664692e-05,
      "loss": 0.8765,
      "step": 12160
    },
    {
      "epoch": 0.7712538420102031,
      "grad_norm": 2.364302396774292,
      "learning_rate": 2.233806096528366e-05,
      "loss": 0.8769,
      "step": 12170
    },
    {
      "epoch": 0.7718875756519535,
      "grad_norm": 2.50726580619812,
      "learning_rate": 2.2331710414902625e-05,
      "loss": 0.9206,
      "step": 12180
    },
    {
      "epoch": 0.7725213092937039,
      "grad_norm": 2.5364248752593994,
      "learning_rate": 2.2325359864521592e-05,
      "loss": 0.9021,
      "step": 12190
    },
    {
      "epoch": 0.7731550429354542,
      "grad_norm": 2.5426113605499268,
      "learning_rate": 2.2319009314140558e-05,
      "loss": 0.8984,
      "step": 12200
    },
    {
      "epoch": 0.7737887765772046,
      "grad_norm": 3.039476156234741,
      "learning_rate": 2.2312658763759528e-05,
      "loss": 0.9107,
      "step": 12210
    },
    {
      "epoch": 0.7744225102189549,
      "grad_norm": 3.534250497817993,
      "learning_rate": 2.2306308213378494e-05,
      "loss": 0.8863,
      "step": 12220
    },
    {
      "epoch": 0.7750562438607054,
      "grad_norm": 2.633138418197632,
      "learning_rate": 2.2299957662997457e-05,
      "loss": 0.8634,
      "step": 12230
    },
    {
      "epoch": 0.7756899775024557,
      "grad_norm": 2.393345832824707,
      "learning_rate": 2.2293607112616427e-05,
      "loss": 0.858,
      "step": 12240
    },
    {
      "epoch": 0.7763237111442061,
      "grad_norm": 3.1426808834075928,
      "learning_rate": 2.2287256562235394e-05,
      "loss": 0.8552,
      "step": 12250
    },
    {
      "epoch": 0.7769574447859564,
      "grad_norm": 2.3800923824310303,
      "learning_rate": 2.228090601185436e-05,
      "loss": 0.8807,
      "step": 12260
    },
    {
      "epoch": 0.7775911784277069,
      "grad_norm": 2.6557836532592773,
      "learning_rate": 2.227455546147333e-05,
      "loss": 0.8474,
      "step": 12270
    },
    {
      "epoch": 0.7782249120694572,
      "grad_norm": 2.85386323928833,
      "learning_rate": 2.2268204911092293e-05,
      "loss": 0.9008,
      "step": 12280
    },
    {
      "epoch": 0.7788586457112076,
      "grad_norm": 3.2128889560699463,
      "learning_rate": 2.2261854360711263e-05,
      "loss": 0.8916,
      "step": 12290
    },
    {
      "epoch": 0.7794923793529579,
      "grad_norm": 2.520719528198242,
      "learning_rate": 2.225550381033023e-05,
      "loss": 0.8624,
      "step": 12300
    },
    {
      "epoch": 0.7801261129947084,
      "grad_norm": 2.913175106048584,
      "learning_rate": 2.2249153259949196e-05,
      "loss": 0.8723,
      "step": 12310
    },
    {
      "epoch": 0.7807598466364587,
      "grad_norm": 2.6526832580566406,
      "learning_rate": 2.2242802709568165e-05,
      "loss": 0.914,
      "step": 12320
    },
    {
      "epoch": 0.781393580278209,
      "grad_norm": 2.6920199394226074,
      "learning_rate": 2.2236452159187132e-05,
      "loss": 0.8818,
      "step": 12330
    },
    {
      "epoch": 0.7820273139199594,
      "grad_norm": 2.4588782787323,
      "learning_rate": 2.2230101608806095e-05,
      "loss": 0.9137,
      "step": 12340
    },
    {
      "epoch": 0.7826610475617098,
      "grad_norm": 2.3281853199005127,
      "learning_rate": 2.2223751058425065e-05,
      "loss": 0.8515,
      "step": 12350
    },
    {
      "epoch": 0.7832947812034602,
      "grad_norm": 2.1753413677215576,
      "learning_rate": 2.221740050804403e-05,
      "loss": 0.9041,
      "step": 12360
    },
    {
      "epoch": 0.7839285148452105,
      "grad_norm": 2.214839458465576,
      "learning_rate": 2.2211049957662998e-05,
      "loss": 0.858,
      "step": 12370
    },
    {
      "epoch": 0.7845622484869609,
      "grad_norm": 2.517937421798706,
      "learning_rate": 2.2204699407281967e-05,
      "loss": 0.8705,
      "step": 12380
    },
    {
      "epoch": 0.7851959821287113,
      "grad_norm": 2.3792080879211426,
      "learning_rate": 2.219834885690093e-05,
      "loss": 0.9096,
      "step": 12390
    },
    {
      "epoch": 0.7858297157704617,
      "grad_norm": 2.7714734077453613,
      "learning_rate": 2.2191998306519897e-05,
      "loss": 0.8925,
      "step": 12400
    },
    {
      "epoch": 0.786463449412212,
      "grad_norm": 2.66195011138916,
      "learning_rate": 2.2185647756138867e-05,
      "loss": 0.8365,
      "step": 12410
    },
    {
      "epoch": 0.7870971830539624,
      "grad_norm": 2.673104763031006,
      "learning_rate": 2.2179297205757833e-05,
      "loss": 0.8724,
      "step": 12420
    },
    {
      "epoch": 0.7877309166957128,
      "grad_norm": 2.836909770965576,
      "learning_rate": 2.2172946655376803e-05,
      "loss": 0.8852,
      "step": 12430
    },
    {
      "epoch": 0.7883646503374632,
      "grad_norm": 2.5866730213165283,
      "learning_rate": 2.2166596104995766e-05,
      "loss": 0.8997,
      "step": 12440
    },
    {
      "epoch": 0.7889983839792135,
      "grad_norm": 2.3596248626708984,
      "learning_rate": 2.2160245554614732e-05,
      "loss": 0.8529,
      "step": 12450
    },
    {
      "epoch": 0.7896321176209639,
      "grad_norm": 2.602994680404663,
      "learning_rate": 2.2153895004233702e-05,
      "loss": 0.8724,
      "step": 12460
    },
    {
      "epoch": 0.7902658512627143,
      "grad_norm": 2.5479021072387695,
      "learning_rate": 2.214754445385267e-05,
      "loss": 0.8729,
      "step": 12470
    },
    {
      "epoch": 0.7908995849044647,
      "grad_norm": 2.7566978931427,
      "learning_rate": 2.2141193903471635e-05,
      "loss": 0.9155,
      "step": 12480
    },
    {
      "epoch": 0.791533318546215,
      "grad_norm": 2.934816360473633,
      "learning_rate": 2.21348433530906e-05,
      "loss": 0.8632,
      "step": 12490
    },
    {
      "epoch": 0.7921670521879653,
      "grad_norm": 2.772369623184204,
      "learning_rate": 2.2128492802709568e-05,
      "loss": 0.8755,
      "step": 12500
    },
    {
      "epoch": 0.7928007858297158,
      "grad_norm": 2.28131103515625,
      "learning_rate": 2.2122142252328534e-05,
      "loss": 0.8594,
      "step": 12510
    },
    {
      "epoch": 0.7934345194714661,
      "grad_norm": 2.8458595275878906,
      "learning_rate": 2.2115791701947504e-05,
      "loss": 0.8962,
      "step": 12520
    },
    {
      "epoch": 0.7940682531132165,
      "grad_norm": 2.624633550643921,
      "learning_rate": 2.210944115156647e-05,
      "loss": 0.9035,
      "step": 12530
    },
    {
      "epoch": 0.7947019867549668,
      "grad_norm": 2.282276153564453,
      "learning_rate": 2.2103090601185434e-05,
      "loss": 0.891,
      "step": 12540
    },
    {
      "epoch": 0.7953357203967173,
      "grad_norm": 2.3910818099975586,
      "learning_rate": 2.2096740050804403e-05,
      "loss": 0.8338,
      "step": 12550
    },
    {
      "epoch": 0.7959694540384676,
      "grad_norm": 2.310664415359497,
      "learning_rate": 2.209038950042337e-05,
      "loss": 0.8696,
      "step": 12560
    },
    {
      "epoch": 0.796603187680218,
      "grad_norm": 2.4116828441619873,
      "learning_rate": 2.2084038950042336e-05,
      "loss": 0.8982,
      "step": 12570
    },
    {
      "epoch": 0.7972369213219683,
      "grad_norm": 2.6020753383636475,
      "learning_rate": 2.2077688399661306e-05,
      "loss": 0.8823,
      "step": 12580
    },
    {
      "epoch": 0.7978706549637188,
      "grad_norm": 2.909609317779541,
      "learning_rate": 2.2071337849280273e-05,
      "loss": 0.8579,
      "step": 12590
    },
    {
      "epoch": 0.7985043886054691,
      "grad_norm": 2.616763114929199,
      "learning_rate": 2.206498729889924e-05,
      "loss": 0.8536,
      "step": 12600
    },
    {
      "epoch": 0.7991381222472195,
      "grad_norm": 2.706928014755249,
      "learning_rate": 2.2058636748518205e-05,
      "loss": 0.943,
      "step": 12610
    },
    {
      "epoch": 0.7997718558889698,
      "grad_norm": 3.14524507522583,
      "learning_rate": 2.2052286198137172e-05,
      "loss": 0.8753,
      "step": 12620
    },
    {
      "epoch": 0.8004055895307203,
      "grad_norm": 2.878547430038452,
      "learning_rate": 2.204593564775614e-05,
      "loss": 0.8545,
      "step": 12630
    },
    {
      "epoch": 0.8010393231724706,
      "grad_norm": 2.5283634662628174,
      "learning_rate": 2.2039585097375108e-05,
      "loss": 0.8467,
      "step": 12640
    },
    {
      "epoch": 0.801673056814221,
      "grad_norm": 2.9784443378448486,
      "learning_rate": 2.203323454699407e-05,
      "loss": 0.8443,
      "step": 12650
    },
    {
      "epoch": 0.8023067904559713,
      "grad_norm": 2.253800630569458,
      "learning_rate": 2.202688399661304e-05,
      "loss": 0.8863,
      "step": 12660
    },
    {
      "epoch": 0.8029405240977218,
      "grad_norm": 3.1222524642944336,
      "learning_rate": 2.2020533446232007e-05,
      "loss": 0.8716,
      "step": 12670
    },
    {
      "epoch": 0.8035742577394721,
      "grad_norm": 2.3476035594940186,
      "learning_rate": 2.2014182895850974e-05,
      "loss": 0.8957,
      "step": 12680
    },
    {
      "epoch": 0.8042079913812225,
      "grad_norm": 2.4197700023651123,
      "learning_rate": 2.2007832345469944e-05,
      "loss": 0.8655,
      "step": 12690
    },
    {
      "epoch": 0.8048417250229728,
      "grad_norm": 2.2097203731536865,
      "learning_rate": 2.2001481795088907e-05,
      "loss": 0.8553,
      "step": 12700
    },
    {
      "epoch": 0.8054754586647233,
      "grad_norm": 2.6053497791290283,
      "learning_rate": 2.1995131244707873e-05,
      "loss": 0.9113,
      "step": 12710
    },
    {
      "epoch": 0.8061091923064736,
      "grad_norm": 2.890845537185669,
      "learning_rate": 2.1988780694326843e-05,
      "loss": 0.8416,
      "step": 12720
    },
    {
      "epoch": 0.8067429259482239,
      "grad_norm": 2.497286558151245,
      "learning_rate": 2.198243014394581e-05,
      "loss": 0.8551,
      "step": 12730
    },
    {
      "epoch": 0.8073766595899743,
      "grad_norm": 2.685933828353882,
      "learning_rate": 2.1976079593564776e-05,
      "loss": 0.8782,
      "step": 12740
    },
    {
      "epoch": 0.8080103932317247,
      "grad_norm": 3.1706693172454834,
      "learning_rate": 2.1969729043183742e-05,
      "loss": 0.891,
      "step": 12750
    },
    {
      "epoch": 0.8086441268734751,
      "grad_norm": 2.97446608543396,
      "learning_rate": 2.196337849280271e-05,
      "loss": 0.8795,
      "step": 12760
    },
    {
      "epoch": 0.8092778605152254,
      "grad_norm": 2.386746644973755,
      "learning_rate": 2.195702794242168e-05,
      "loss": 0.8874,
      "step": 12770
    },
    {
      "epoch": 0.8099115941569758,
      "grad_norm": 3.0892152786254883,
      "learning_rate": 2.1950677392040645e-05,
      "loss": 0.9164,
      "step": 12780
    },
    {
      "epoch": 0.8105453277987262,
      "grad_norm": 2.9609169960021973,
      "learning_rate": 2.194432684165961e-05,
      "loss": 0.8844,
      "step": 12790
    },
    {
      "epoch": 0.8111790614404766,
      "grad_norm": 2.416473388671875,
      "learning_rate": 2.1937976291278578e-05,
      "loss": 0.8671,
      "step": 12800
    },
    {
      "epoch": 0.8118127950822269,
      "grad_norm": 3.1337649822235107,
      "learning_rate": 2.1931625740897544e-05,
      "loss": 0.9004,
      "step": 12810
    },
    {
      "epoch": 0.8124465287239773,
      "grad_norm": 2.6872901916503906,
      "learning_rate": 2.192527519051651e-05,
      "loss": 0.9021,
      "step": 12820
    },
    {
      "epoch": 0.8130802623657277,
      "grad_norm": 2.553256034851074,
      "learning_rate": 2.191892464013548e-05,
      "loss": 0.9136,
      "step": 12830
    },
    {
      "epoch": 0.8137139960074781,
      "grad_norm": 2.4788806438446045,
      "learning_rate": 2.1912574089754447e-05,
      "loss": 0.8966,
      "step": 12840
    },
    {
      "epoch": 0.8143477296492284,
      "grad_norm": 3.9600212574005127,
      "learning_rate": 2.1906223539373413e-05,
      "loss": 0.844,
      "step": 12850
    },
    {
      "epoch": 0.8149814632909788,
      "grad_norm": 2.7628936767578125,
      "learning_rate": 2.189987298899238e-05,
      "loss": 0.8687,
      "step": 12860
    },
    {
      "epoch": 0.8156151969327292,
      "grad_norm": 2.924243688583374,
      "learning_rate": 2.1893522438611346e-05,
      "loss": 0.8704,
      "step": 12870
    },
    {
      "epoch": 0.8162489305744796,
      "grad_norm": 2.8180248737335205,
      "learning_rate": 2.1887171888230312e-05,
      "loss": 0.8917,
      "step": 12880
    },
    {
      "epoch": 0.8168826642162299,
      "grad_norm": 2.5122156143188477,
      "learning_rate": 2.1880821337849282e-05,
      "loss": 0.8633,
      "step": 12890
    },
    {
      "epoch": 0.8175163978579802,
      "grad_norm": 2.7609663009643555,
      "learning_rate": 2.187447078746825e-05,
      "loss": 0.8686,
      "step": 12900
    },
    {
      "epoch": 0.8181501314997307,
      "grad_norm": 2.5462048053741455,
      "learning_rate": 2.1868120237087215e-05,
      "loss": 0.8854,
      "step": 12910
    },
    {
      "epoch": 0.818783865141481,
      "grad_norm": 2.608659505844116,
      "learning_rate": 2.186176968670618e-05,
      "loss": 0.8561,
      "step": 12920
    },
    {
      "epoch": 0.8194175987832314,
      "grad_norm": 2.612313985824585,
      "learning_rate": 2.1855419136325148e-05,
      "loss": 0.8667,
      "step": 12930
    },
    {
      "epoch": 0.8200513324249817,
      "grad_norm": 2.4242007732391357,
      "learning_rate": 2.1849068585944118e-05,
      "loss": 0.8598,
      "step": 12940
    },
    {
      "epoch": 0.8206850660667322,
      "grad_norm": 2.3006083965301514,
      "learning_rate": 2.1842718035563084e-05,
      "loss": 0.882,
      "step": 12950
    },
    {
      "epoch": 0.8213187997084825,
      "grad_norm": 2.4361860752105713,
      "learning_rate": 2.1836367485182047e-05,
      "loss": 0.8723,
      "step": 12960
    },
    {
      "epoch": 0.8219525333502329,
      "grad_norm": 2.368040084838867,
      "learning_rate": 2.1830016934801017e-05,
      "loss": 0.912,
      "step": 12970
    },
    {
      "epoch": 0.8225862669919832,
      "grad_norm": 3.1332263946533203,
      "learning_rate": 2.1823666384419983e-05,
      "loss": 0.917,
      "step": 12980
    },
    {
      "epoch": 0.8232200006337337,
      "grad_norm": 3.071446180343628,
      "learning_rate": 2.181731583403895e-05,
      "loss": 0.8799,
      "step": 12990
    },
    {
      "epoch": 0.823853734275484,
      "grad_norm": 3.242931842803955,
      "learning_rate": 2.181096528365792e-05,
      "loss": 0.9029,
      "step": 13000
    },
    {
      "epoch": 0.8244874679172344,
      "grad_norm": 1.9718576669692993,
      "learning_rate": 2.1804614733276883e-05,
      "loss": 0.8607,
      "step": 13010
    },
    {
      "epoch": 0.8251212015589847,
      "grad_norm": 2.3983287811279297,
      "learning_rate": 2.179826418289585e-05,
      "loss": 0.8141,
      "step": 13020
    },
    {
      "epoch": 0.8257549352007352,
      "grad_norm": 3.0049631595611572,
      "learning_rate": 2.179191363251482e-05,
      "loss": 0.8533,
      "step": 13030
    },
    {
      "epoch": 0.8263886688424855,
      "grad_norm": 2.5620384216308594,
      "learning_rate": 2.1785563082133785e-05,
      "loss": 0.8643,
      "step": 13040
    },
    {
      "epoch": 0.8270224024842359,
      "grad_norm": 2.448477029800415,
      "learning_rate": 2.1779212531752752e-05,
      "loss": 0.8804,
      "step": 13050
    },
    {
      "epoch": 0.8276561361259862,
      "grad_norm": 2.937744379043579,
      "learning_rate": 2.1772861981371718e-05,
      "loss": 0.8867,
      "step": 13060
    },
    {
      "epoch": 0.8282898697677367,
      "grad_norm": 3.1764822006225586,
      "learning_rate": 2.1766511430990685e-05,
      "loss": 0.901,
      "step": 13070
    },
    {
      "epoch": 0.828923603409487,
      "grad_norm": 2.7163829803466797,
      "learning_rate": 2.1760160880609655e-05,
      "loss": 0.9145,
      "step": 13080
    },
    {
      "epoch": 0.8295573370512374,
      "grad_norm": 2.492793560028076,
      "learning_rate": 2.175381033022862e-05,
      "loss": 0.8583,
      "step": 13090
    },
    {
      "epoch": 0.8301910706929877,
      "grad_norm": 2.485731840133667,
      "learning_rate": 2.1747459779847587e-05,
      "loss": 0.8972,
      "step": 13100
    },
    {
      "epoch": 0.8308248043347382,
      "grad_norm": 2.30157208442688,
      "learning_rate": 2.1741109229466557e-05,
      "loss": 0.8829,
      "step": 13110
    },
    {
      "epoch": 0.8314585379764885,
      "grad_norm": 3.5487449169158936,
      "learning_rate": 2.173475867908552e-05,
      "loss": 0.886,
      "step": 13120
    },
    {
      "epoch": 0.8320922716182388,
      "grad_norm": 2.290863513946533,
      "learning_rate": 2.1728408128704487e-05,
      "loss": 0.8728,
      "step": 13130
    },
    {
      "epoch": 0.8327260052599892,
      "grad_norm": 2.6882479190826416,
      "learning_rate": 2.1722057578323456e-05,
      "loss": 0.8536,
      "step": 13140
    },
    {
      "epoch": 0.8333597389017396,
      "grad_norm": 2.541968822479248,
      "learning_rate": 2.1715707027942423e-05,
      "loss": 0.882,
      "step": 13150
    },
    {
      "epoch": 0.83399347254349,
      "grad_norm": 2.4528403282165527,
      "learning_rate": 2.170935647756139e-05,
      "loss": 0.921,
      "step": 13160
    },
    {
      "epoch": 0.8346272061852403,
      "grad_norm": 2.6062867641448975,
      "learning_rate": 2.1703005927180356e-05,
      "loss": 0.8762,
      "step": 13170
    },
    {
      "epoch": 0.8352609398269907,
      "grad_norm": 2.775116443634033,
      "learning_rate": 2.1696655376799322e-05,
      "loss": 0.857,
      "step": 13180
    },
    {
      "epoch": 0.8358946734687411,
      "grad_norm": 4.059709548950195,
      "learning_rate": 2.169030482641829e-05,
      "loss": 0.878,
      "step": 13190
    },
    {
      "epoch": 0.8365284071104915,
      "grad_norm": 2.4108364582061768,
      "learning_rate": 2.168395427603726e-05,
      "loss": 0.8447,
      "step": 13200
    },
    {
      "epoch": 0.8371621407522418,
      "grad_norm": 2.4512157440185547,
      "learning_rate": 2.1677603725656225e-05,
      "loss": 0.8876,
      "step": 13210
    },
    {
      "epoch": 0.8377958743939922,
      "grad_norm": 2.454357147216797,
      "learning_rate": 2.167125317527519e-05,
      "loss": 0.8833,
      "step": 13220
    },
    {
      "epoch": 0.8384296080357426,
      "grad_norm": 2.307054042816162,
      "learning_rate": 2.1664902624894158e-05,
      "loss": 0.8363,
      "step": 13230
    },
    {
      "epoch": 0.839063341677493,
      "grad_norm": 2.806272268295288,
      "learning_rate": 2.1658552074513124e-05,
      "loss": 0.9336,
      "step": 13240
    },
    {
      "epoch": 0.8396970753192433,
      "grad_norm": 2.6884236335754395,
      "learning_rate": 2.1652201524132094e-05,
      "loss": 0.8549,
      "step": 13250
    },
    {
      "epoch": 0.8403308089609937,
      "grad_norm": 2.7010796070098877,
      "learning_rate": 2.164585097375106e-05,
      "loss": 0.8456,
      "step": 13260
    },
    {
      "epoch": 0.8409645426027441,
      "grad_norm": 2.7394349575042725,
      "learning_rate": 2.1639500423370023e-05,
      "loss": 0.8484,
      "step": 13270
    },
    {
      "epoch": 0.8415982762444945,
      "grad_norm": 2.4845588207244873,
      "learning_rate": 2.1633149872988993e-05,
      "loss": 0.9176,
      "step": 13280
    },
    {
      "epoch": 0.8422320098862448,
      "grad_norm": 2.4342076778411865,
      "learning_rate": 2.162679932260796e-05,
      "loss": 0.8639,
      "step": 13290
    },
    {
      "epoch": 0.8428657435279951,
      "grad_norm": 2.4570305347442627,
      "learning_rate": 2.1620448772226926e-05,
      "loss": 0.8664,
      "step": 13300
    },
    {
      "epoch": 0.8434994771697456,
      "grad_norm": 2.6971347332000732,
      "learning_rate": 2.1614098221845896e-05,
      "loss": 0.8574,
      "step": 13310
    },
    {
      "epoch": 0.844133210811496,
      "grad_norm": 2.6852242946624756,
      "learning_rate": 2.160774767146486e-05,
      "loss": 0.8878,
      "step": 13320
    },
    {
      "epoch": 0.8447669444532463,
      "grad_norm": 3.048811912536621,
      "learning_rate": 2.1601397121083825e-05,
      "loss": 0.9086,
      "step": 13330
    },
    {
      "epoch": 0.8454006780949966,
      "grad_norm": 3.3206746578216553,
      "learning_rate": 2.1595046570702795e-05,
      "loss": 0.8878,
      "step": 13340
    },
    {
      "epoch": 0.8460344117367471,
      "grad_norm": 2.288797378540039,
      "learning_rate": 2.158869602032176e-05,
      "loss": 0.8636,
      "step": 13350
    },
    {
      "epoch": 0.8466681453784974,
      "grad_norm": 2.6346867084503174,
      "learning_rate": 2.1582345469940728e-05,
      "loss": 0.9134,
      "step": 13360
    },
    {
      "epoch": 0.8473018790202478,
      "grad_norm": 2.3158915042877197,
      "learning_rate": 2.1575994919559698e-05,
      "loss": 0.8838,
      "step": 13370
    },
    {
      "epoch": 0.8479356126619981,
      "grad_norm": 2.255794048309326,
      "learning_rate": 2.156964436917866e-05,
      "loss": 0.8847,
      "step": 13380
    },
    {
      "epoch": 0.8485693463037486,
      "grad_norm": 2.199345588684082,
      "learning_rate": 2.156329381879763e-05,
      "loss": 0.8521,
      "step": 13390
    },
    {
      "epoch": 0.8492030799454989,
      "grad_norm": 2.3251471519470215,
      "learning_rate": 2.1556943268416597e-05,
      "loss": 0.8769,
      "step": 13400
    },
    {
      "epoch": 0.8498368135872493,
      "grad_norm": 2.3044190406799316,
      "learning_rate": 2.1550592718035564e-05,
      "loss": 0.8696,
      "step": 13410
    },
    {
      "epoch": 0.8504705472289996,
      "grad_norm": 2.651409387588501,
      "learning_rate": 2.1544242167654533e-05,
      "loss": 0.8902,
      "step": 13420
    },
    {
      "epoch": 0.8511042808707501,
      "grad_norm": 2.353825807571411,
      "learning_rate": 2.1537891617273496e-05,
      "loss": 0.8746,
      "step": 13430
    },
    {
      "epoch": 0.8517380145125004,
      "grad_norm": 2.430565118789673,
      "learning_rate": 2.1531541066892463e-05,
      "loss": 0.8583,
      "step": 13440
    },
    {
      "epoch": 0.8523717481542508,
      "grad_norm": 2.587409496307373,
      "learning_rate": 2.1525190516511433e-05,
      "loss": 0.8857,
      "step": 13450
    },
    {
      "epoch": 0.8530054817960011,
      "grad_norm": 2.4291839599609375,
      "learning_rate": 2.15188399661304e-05,
      "loss": 0.8292,
      "step": 13460
    },
    {
      "epoch": 0.8536392154377516,
      "grad_norm": 2.35835862159729,
      "learning_rate": 2.1512489415749365e-05,
      "loss": 0.9033,
      "step": 13470
    },
    {
      "epoch": 0.8542729490795019,
      "grad_norm": 2.6959002017974854,
      "learning_rate": 2.1506138865368332e-05,
      "loss": 0.8877,
      "step": 13480
    },
    {
      "epoch": 0.8549066827212523,
      "grad_norm": 2.436370849609375,
      "learning_rate": 2.14997883149873e-05,
      "loss": 0.8771,
      "step": 13490
    },
    {
      "epoch": 0.8555404163630026,
      "grad_norm": 2.4627225399017334,
      "learning_rate": 2.1493437764606265e-05,
      "loss": 0.8581,
      "step": 13500
    },
    {
      "epoch": 0.856174150004753,
      "grad_norm": 2.5257678031921387,
      "learning_rate": 2.1487087214225235e-05,
      "loss": 0.8679,
      "step": 13510
    },
    {
      "epoch": 0.8568078836465034,
      "grad_norm": 3.23891282081604,
      "learning_rate": 2.14807366638442e-05,
      "loss": 0.8945,
      "step": 13520
    },
    {
      "epoch": 0.8574416172882537,
      "grad_norm": 2.879664421081543,
      "learning_rate": 2.1474386113463167e-05,
      "loss": 0.8786,
      "step": 13530
    },
    {
      "epoch": 0.8580753509300041,
      "grad_norm": 2.537043333053589,
      "learning_rate": 2.1468035563082134e-05,
      "loss": 0.9207,
      "step": 13540
    },
    {
      "epoch": 0.8587090845717545,
      "grad_norm": 2.890453815460205,
      "learning_rate": 2.14616850127011e-05,
      "loss": 0.8696,
      "step": 13550
    },
    {
      "epoch": 0.8593428182135049,
      "grad_norm": 2.7178702354431152,
      "learning_rate": 2.145533446232007e-05,
      "loss": 0.9276,
      "step": 13560
    },
    {
      "epoch": 0.8599765518552552,
      "grad_norm": 2.5844357013702393,
      "learning_rate": 2.1448983911939037e-05,
      "loss": 0.8921,
      "step": 13570
    },
    {
      "epoch": 0.8606102854970056,
      "grad_norm": 2.468672752380371,
      "learning_rate": 2.1442633361558e-05,
      "loss": 0.9145,
      "step": 13580
    },
    {
      "epoch": 0.861244019138756,
      "grad_norm": 2.384101390838623,
      "learning_rate": 2.143628281117697e-05,
      "loss": 0.8566,
      "step": 13590
    },
    {
      "epoch": 0.8618777527805064,
      "grad_norm": 2.4920260906219482,
      "learning_rate": 2.1429932260795936e-05,
      "loss": 0.8811,
      "step": 13600
    },
    {
      "epoch": 0.8625114864222567,
      "grad_norm": 2.4117307662963867,
      "learning_rate": 2.1423581710414902e-05,
      "loss": 0.8469,
      "step": 13610
    },
    {
      "epoch": 0.8631452200640071,
      "grad_norm": 2.6276893615722656,
      "learning_rate": 2.1417231160033872e-05,
      "loss": 0.8681,
      "step": 13620
    },
    {
      "epoch": 0.8637789537057575,
      "grad_norm": 2.5244317054748535,
      "learning_rate": 2.141088060965284e-05,
      "loss": 0.8722,
      "step": 13630
    },
    {
      "epoch": 0.8644126873475079,
      "grad_norm": 2.847834348678589,
      "learning_rate": 2.14045300592718e-05,
      "loss": 0.8328,
      "step": 13640
    },
    {
      "epoch": 0.8650464209892582,
      "grad_norm": 2.293896198272705,
      "learning_rate": 2.139817950889077e-05,
      "loss": 0.8834,
      "step": 13650
    },
    {
      "epoch": 0.8656801546310086,
      "grad_norm": 2.2472548484802246,
      "learning_rate": 2.1391828958509738e-05,
      "loss": 0.8176,
      "step": 13660
    },
    {
      "epoch": 0.866313888272759,
      "grad_norm": 2.16127347946167,
      "learning_rate": 2.1385478408128704e-05,
      "loss": 0.8929,
      "step": 13670
    },
    {
      "epoch": 0.8669476219145094,
      "grad_norm": 2.3802995681762695,
      "learning_rate": 2.1379127857747674e-05,
      "loss": 0.89,
      "step": 13680
    },
    {
      "epoch": 0.8675813555562597,
      "grad_norm": 2.5778732299804688,
      "learning_rate": 2.1372777307366637e-05,
      "loss": 0.8838,
      "step": 13690
    },
    {
      "epoch": 0.86821508919801,
      "grad_norm": 2.0263118743896484,
      "learning_rate": 2.1366426756985607e-05,
      "loss": 0.8956,
      "step": 13700
    },
    {
      "epoch": 0.8688488228397605,
      "grad_norm": 2.7622334957122803,
      "learning_rate": 2.1360076206604573e-05,
      "loss": 0.8865,
      "step": 13710
    },
    {
      "epoch": 0.8694825564815108,
      "grad_norm": 2.3950839042663574,
      "learning_rate": 2.135372565622354e-05,
      "loss": 0.8786,
      "step": 13720
    },
    {
      "epoch": 0.8701162901232612,
      "grad_norm": 2.3674139976501465,
      "learning_rate": 2.134737510584251e-05,
      "loss": 0.874,
      "step": 13730
    },
    {
      "epoch": 0.8707500237650115,
      "grad_norm": 2.693100929260254,
      "learning_rate": 2.1341024555461473e-05,
      "loss": 0.9155,
      "step": 13740
    },
    {
      "epoch": 0.8713837574067619,
      "grad_norm": 2.3737471103668213,
      "learning_rate": 2.133467400508044e-05,
      "loss": 0.8689,
      "step": 13750
    },
    {
      "epoch": 0.8720174910485123,
      "grad_norm": 2.7096705436706543,
      "learning_rate": 2.132832345469941e-05,
      "loss": 0.8704,
      "step": 13760
    },
    {
      "epoch": 0.8726512246902627,
      "grad_norm": 2.3538684844970703,
      "learning_rate": 2.1321972904318375e-05,
      "loss": 0.8918,
      "step": 13770
    },
    {
      "epoch": 0.873284958332013,
      "grad_norm": 2.4297263622283936,
      "learning_rate": 2.131562235393734e-05,
      "loss": 0.8996,
      "step": 13780
    },
    {
      "epoch": 0.8739186919737634,
      "grad_norm": 3.2211902141571045,
      "learning_rate": 2.1309271803556308e-05,
      "loss": 0.8185,
      "step": 13790
    },
    {
      "epoch": 0.8745524256155138,
      "grad_norm": 2.1893880367279053,
      "learning_rate": 2.1302921253175275e-05,
      "loss": 0.8814,
      "step": 13800
    },
    {
      "epoch": 0.8751861592572642,
      "grad_norm": 2.372863292694092,
      "learning_rate": 2.129657070279424e-05,
      "loss": 0.8781,
      "step": 13810
    },
    {
      "epoch": 0.8758198928990145,
      "grad_norm": 2.2863547801971436,
      "learning_rate": 2.129022015241321e-05,
      "loss": 0.8727,
      "step": 13820
    },
    {
      "epoch": 0.8764536265407649,
      "grad_norm": 2.5442423820495605,
      "learning_rate": 2.1283869602032177e-05,
      "loss": 0.8963,
      "step": 13830
    },
    {
      "epoch": 0.8770873601825153,
      "grad_norm": 2.6264052391052246,
      "learning_rate": 2.1277519051651144e-05,
      "loss": 0.8505,
      "step": 13840
    },
    {
      "epoch": 0.8777210938242657,
      "grad_norm": 2.5679686069488525,
      "learning_rate": 2.127116850127011e-05,
      "loss": 0.8566,
      "step": 13850
    },
    {
      "epoch": 0.878354827466016,
      "grad_norm": 3.1977009773254395,
      "learning_rate": 2.1264817950889076e-05,
      "loss": 0.8616,
      "step": 13860
    },
    {
      "epoch": 0.8789885611077664,
      "grad_norm": 2.4535973072052,
      "learning_rate": 2.1258467400508046e-05,
      "loss": 0.8754,
      "step": 13870
    },
    {
      "epoch": 0.8796222947495168,
      "grad_norm": 2.4727208614349365,
      "learning_rate": 2.1252116850127013e-05,
      "loss": 0.8685,
      "step": 13880
    },
    {
      "epoch": 0.8802560283912672,
      "grad_norm": 2.5037426948547363,
      "learning_rate": 2.124576629974598e-05,
      "loss": 0.8994,
      "step": 13890
    },
    {
      "epoch": 0.8808897620330175,
      "grad_norm": 2.4167654514312744,
      "learning_rate": 2.1239415749364946e-05,
      "loss": 0.8502,
      "step": 13900
    },
    {
      "epoch": 0.8815234956747678,
      "grad_norm": 2.752596378326416,
      "learning_rate": 2.1233065198983912e-05,
      "loss": 0.8631,
      "step": 13910
    },
    {
      "epoch": 0.8821572293165183,
      "grad_norm": 2.8935704231262207,
      "learning_rate": 2.122671464860288e-05,
      "loss": 0.857,
      "step": 13920
    },
    {
      "epoch": 0.8827909629582686,
      "grad_norm": 2.723877191543579,
      "learning_rate": 2.1220364098221848e-05,
      "loss": 0.8914,
      "step": 13930
    },
    {
      "epoch": 0.883424696600019,
      "grad_norm": 2.8387231826782227,
      "learning_rate": 2.1214013547840815e-05,
      "loss": 0.879,
      "step": 13940
    },
    {
      "epoch": 0.8840584302417693,
      "grad_norm": 2.60616135597229,
      "learning_rate": 2.1207662997459778e-05,
      "loss": 0.8769,
      "step": 13950
    },
    {
      "epoch": 0.8846921638835198,
      "grad_norm": 2.5252621173858643,
      "learning_rate": 2.1201312447078748e-05,
      "loss": 0.854,
      "step": 13960
    },
    {
      "epoch": 0.8853258975252701,
      "grad_norm": 2.7542030811309814,
      "learning_rate": 2.1194961896697714e-05,
      "loss": 0.8347,
      "step": 13970
    },
    {
      "epoch": 0.8859596311670205,
      "grad_norm": 2.3920412063598633,
      "learning_rate": 2.118861134631668e-05,
      "loss": 0.8473,
      "step": 13980
    },
    {
      "epoch": 0.8865933648087708,
      "grad_norm": 2.907040596008301,
      "learning_rate": 2.118226079593565e-05,
      "loss": 0.8494,
      "step": 13990
    },
    {
      "epoch": 0.8872270984505213,
      "grad_norm": 2.2726895809173584,
      "learning_rate": 2.1175910245554613e-05,
      "loss": 0.8569,
      "step": 14000
    },
    {
      "epoch": 0.8878608320922716,
      "grad_norm": 3.1485068798065186,
      "learning_rate": 2.1169559695173583e-05,
      "loss": 0.9187,
      "step": 14010
    },
    {
      "epoch": 0.888494565734022,
      "grad_norm": 2.7925190925598145,
      "learning_rate": 2.116320914479255e-05,
      "loss": 0.8959,
      "step": 14020
    },
    {
      "epoch": 0.8891282993757723,
      "grad_norm": 2.41477632522583,
      "learning_rate": 2.1156858594411516e-05,
      "loss": 0.8585,
      "step": 14030
    },
    {
      "epoch": 0.8897620330175228,
      "grad_norm": 2.1461901664733887,
      "learning_rate": 2.1150508044030486e-05,
      "loss": 0.8871,
      "step": 14040
    },
    {
      "epoch": 0.8903957666592731,
      "grad_norm": 2.502722978591919,
      "learning_rate": 2.114415749364945e-05,
      "loss": 0.8963,
      "step": 14050
    },
    {
      "epoch": 0.8910295003010235,
      "grad_norm": 2.279609203338623,
      "learning_rate": 2.1137806943268415e-05,
      "loss": 0.8582,
      "step": 14060
    },
    {
      "epoch": 0.8916632339427738,
      "grad_norm": 2.2925477027893066,
      "learning_rate": 2.1131456392887385e-05,
      "loss": 0.8627,
      "step": 14070
    },
    {
      "epoch": 0.8922969675845243,
      "grad_norm": 2.7372395992279053,
      "learning_rate": 2.112510584250635e-05,
      "loss": 0.9259,
      "step": 14080
    },
    {
      "epoch": 0.8929307012262746,
      "grad_norm": 2.339829444885254,
      "learning_rate": 2.1118755292125318e-05,
      "loss": 0.8165,
      "step": 14090
    },
    {
      "epoch": 0.893564434868025,
      "grad_norm": 2.1894354820251465,
      "learning_rate": 2.1112404741744288e-05,
      "loss": 0.8686,
      "step": 14100
    },
    {
      "epoch": 0.8941981685097753,
      "grad_norm": 2.57075834274292,
      "learning_rate": 2.110605419136325e-05,
      "loss": 0.858,
      "step": 14110
    },
    {
      "epoch": 0.8948319021515257,
      "grad_norm": 2.095625877380371,
      "learning_rate": 2.1099703640982217e-05,
      "loss": 0.8629,
      "step": 14120
    },
    {
      "epoch": 0.8954656357932761,
      "grad_norm": 2.7350053787231445,
      "learning_rate": 2.1093353090601187e-05,
      "loss": 0.8528,
      "step": 14130
    },
    {
      "epoch": 0.8960993694350264,
      "grad_norm": 2.559835433959961,
      "learning_rate": 2.1087002540220153e-05,
      "loss": 0.9164,
      "step": 14140
    },
    {
      "epoch": 0.8967331030767768,
      "grad_norm": 2.710005521774292,
      "learning_rate": 2.108065198983912e-05,
      "loss": 0.8735,
      "step": 14150
    },
    {
      "epoch": 0.8973668367185272,
      "grad_norm": 3.192967414855957,
      "learning_rate": 2.1074301439458086e-05,
      "loss": 0.8628,
      "step": 14160
    },
    {
      "epoch": 0.8980005703602776,
      "grad_norm": 2.74300217628479,
      "learning_rate": 2.1067950889077053e-05,
      "loss": 0.8861,
      "step": 14170
    },
    {
      "epoch": 0.8986343040020279,
      "grad_norm": 2.2420387268066406,
      "learning_rate": 2.1061600338696022e-05,
      "loss": 0.8819,
      "step": 14180
    },
    {
      "epoch": 0.8992680376437783,
      "grad_norm": 2.614213228225708,
      "learning_rate": 2.105524978831499e-05,
      "loss": 0.8534,
      "step": 14190
    },
    {
      "epoch": 0.8999017712855287,
      "grad_norm": 2.972785472869873,
      "learning_rate": 2.1048899237933955e-05,
      "loss": 0.8794,
      "step": 14200
    },
    {
      "epoch": 0.9005355049272791,
      "grad_norm": 2.4139487743377686,
      "learning_rate": 2.1042548687552922e-05,
      "loss": 0.893,
      "step": 14210
    },
    {
      "epoch": 0.9011692385690294,
      "grad_norm": 2.2429096698760986,
      "learning_rate": 2.1036198137171888e-05,
      "loss": 0.8378,
      "step": 14220
    },
    {
      "epoch": 0.9018029722107798,
      "grad_norm": 2.8096301555633545,
      "learning_rate": 2.1029847586790855e-05,
      "loss": 0.8198,
      "step": 14230
    },
    {
      "epoch": 0.9024367058525302,
      "grad_norm": 2.6099541187286377,
      "learning_rate": 2.1023497036409824e-05,
      "loss": 0.8734,
      "step": 14240
    },
    {
      "epoch": 0.9030704394942806,
      "grad_norm": 2.5153512954711914,
      "learning_rate": 2.101714648602879e-05,
      "loss": 0.8778,
      "step": 14250
    },
    {
      "epoch": 0.9037041731360309,
      "grad_norm": 2.8747057914733887,
      "learning_rate": 2.1010795935647754e-05,
      "loss": 0.8962,
      "step": 14260
    },
    {
      "epoch": 0.9043379067777813,
      "grad_norm": 2.5032167434692383,
      "learning_rate": 2.1004445385266724e-05,
      "loss": 0.8774,
      "step": 14270
    },
    {
      "epoch": 0.9049716404195317,
      "grad_norm": 2.14949107170105,
      "learning_rate": 2.099809483488569e-05,
      "loss": 0.8807,
      "step": 14280
    },
    {
      "epoch": 0.905605374061282,
      "grad_norm": 2.3438291549682617,
      "learning_rate": 2.0991744284504657e-05,
      "loss": 0.8624,
      "step": 14290
    },
    {
      "epoch": 0.9062391077030324,
      "grad_norm": 2.3456084728240967,
      "learning_rate": 2.0985393734123626e-05,
      "loss": 0.9103,
      "step": 14300
    },
    {
      "epoch": 0.9068728413447827,
      "grad_norm": 2.4847991466522217,
      "learning_rate": 2.097904318374259e-05,
      "loss": 0.871,
      "step": 14310
    },
    {
      "epoch": 0.9075065749865332,
      "grad_norm": 2.5888214111328125,
      "learning_rate": 2.097269263336156e-05,
      "loss": 0.8639,
      "step": 14320
    },
    {
      "epoch": 0.9081403086282835,
      "grad_norm": 2.3263790607452393,
      "learning_rate": 2.0966342082980526e-05,
      "loss": 0.8804,
      "step": 14330
    },
    {
      "epoch": 0.9087740422700339,
      "grad_norm": 2.5214955806732178,
      "learning_rate": 2.0959991532599492e-05,
      "loss": 0.868,
      "step": 14340
    },
    {
      "epoch": 0.9094077759117842,
      "grad_norm": 2.1318461894989014,
      "learning_rate": 2.0953640982218462e-05,
      "loss": 0.8924,
      "step": 14350
    },
    {
      "epoch": 0.9100415095535347,
      "grad_norm": 2.6743383407592773,
      "learning_rate": 2.094729043183743e-05,
      "loss": 0.8454,
      "step": 14360
    },
    {
      "epoch": 0.910675243195285,
      "grad_norm": 2.736497163772583,
      "learning_rate": 2.094093988145639e-05,
      "loss": 0.8716,
      "step": 14370
    },
    {
      "epoch": 0.9113089768370354,
      "grad_norm": 2.9955356121063232,
      "learning_rate": 2.093458933107536e-05,
      "loss": 0.8188,
      "step": 14380
    },
    {
      "epoch": 0.9119427104787857,
      "grad_norm": 2.684967279434204,
      "learning_rate": 2.0928238780694328e-05,
      "loss": 0.8664,
      "step": 14390
    },
    {
      "epoch": 0.9125764441205362,
      "grad_norm": 2.632713556289673,
      "learning_rate": 2.0921888230313294e-05,
      "loss": 0.8538,
      "step": 14400
    },
    {
      "epoch": 0.9132101777622865,
      "grad_norm": 2.612499237060547,
      "learning_rate": 2.0915537679932264e-05,
      "loss": 0.8846,
      "step": 14410
    },
    {
      "epoch": 0.9138439114040369,
      "grad_norm": 2.233461618423462,
      "learning_rate": 2.0909187129551227e-05,
      "loss": 0.8382,
      "step": 14420
    },
    {
      "epoch": 0.9144776450457872,
      "grad_norm": 2.9423882961273193,
      "learning_rate": 2.0902836579170193e-05,
      "loss": 0.9039,
      "step": 14430
    },
    {
      "epoch": 0.9151113786875377,
      "grad_norm": 2.4837646484375,
      "learning_rate": 2.0896486028789163e-05,
      "loss": 0.8616,
      "step": 14440
    },
    {
      "epoch": 0.915745112329288,
      "grad_norm": 2.527708053588867,
      "learning_rate": 2.089013547840813e-05,
      "loss": 0.8249,
      "step": 14450
    },
    {
      "epoch": 0.9163788459710384,
      "grad_norm": 2.3286991119384766,
      "learning_rate": 2.0883784928027096e-05,
      "loss": 0.8725,
      "step": 14460
    },
    {
      "epoch": 0.9170125796127887,
      "grad_norm": 2.36860990524292,
      "learning_rate": 2.0877434377646062e-05,
      "loss": 0.8588,
      "step": 14470
    },
    {
      "epoch": 0.9176463132545392,
      "grad_norm": 2.852073907852173,
      "learning_rate": 2.087108382726503e-05,
      "loss": 0.8224,
      "step": 14480
    },
    {
      "epoch": 0.9182800468962895,
      "grad_norm": 3.2535648345947266,
      "learning_rate": 2.0864733276884e-05,
      "loss": 0.8909,
      "step": 14490
    },
    {
      "epoch": 0.9189137805380398,
      "grad_norm": 3.2409298419952393,
      "learning_rate": 2.0858382726502965e-05,
      "loss": 0.8586,
      "step": 14500
    },
    {
      "epoch": 0.9195475141797902,
      "grad_norm": 2.567211389541626,
      "learning_rate": 2.085203217612193e-05,
      "loss": 0.8701,
      "step": 14510
    },
    {
      "epoch": 0.9201812478215406,
      "grad_norm": 2.8753435611724854,
      "learning_rate": 2.0845681625740898e-05,
      "loss": 0.9078,
      "step": 14520
    },
    {
      "epoch": 0.920814981463291,
      "grad_norm": 2.498081922531128,
      "learning_rate": 2.0839331075359864e-05,
      "loss": 0.8225,
      "step": 14530
    },
    {
      "epoch": 0.9214487151050413,
      "grad_norm": 2.4203972816467285,
      "learning_rate": 2.083298052497883e-05,
      "loss": 0.896,
      "step": 14540
    },
    {
      "epoch": 0.9220824487467917,
      "grad_norm": 2.695007562637329,
      "learning_rate": 2.08266299745978e-05,
      "loss": 0.8604,
      "step": 14550
    },
    {
      "epoch": 0.9227161823885421,
      "grad_norm": 2.766587734222412,
      "learning_rate": 2.0820279424216767e-05,
      "loss": 0.8807,
      "step": 14560
    },
    {
      "epoch": 0.9233499160302925,
      "grad_norm": 2.131244421005249,
      "learning_rate": 2.081392887383573e-05,
      "loss": 0.8612,
      "step": 14570
    },
    {
      "epoch": 0.9239836496720428,
      "grad_norm": 2.814016819000244,
      "learning_rate": 2.08075783234547e-05,
      "loss": 0.8263,
      "step": 14580
    },
    {
      "epoch": 0.9246173833137932,
      "grad_norm": 3.009038209915161,
      "learning_rate": 2.0801227773073666e-05,
      "loss": 0.8721,
      "step": 14590
    },
    {
      "epoch": 0.9252511169555436,
      "grad_norm": 2.691915988922119,
      "learning_rate": 2.0794877222692633e-05,
      "loss": 0.9592,
      "step": 14600
    },
    {
      "epoch": 0.925884850597294,
      "grad_norm": 2.8347458839416504,
      "learning_rate": 2.0788526672311603e-05,
      "loss": 0.8898,
      "step": 14610
    },
    {
      "epoch": 0.9265185842390443,
      "grad_norm": 2.301841974258423,
      "learning_rate": 2.078217612193057e-05,
      "loss": 0.879,
      "step": 14620
    },
    {
      "epoch": 0.9271523178807947,
      "grad_norm": 2.4412460327148438,
      "learning_rate": 2.0775825571549535e-05,
      "loss": 0.841,
      "step": 14630
    },
    {
      "epoch": 0.9277860515225451,
      "grad_norm": 3.3830370903015137,
      "learning_rate": 2.0769475021168502e-05,
      "loss": 0.9042,
      "step": 14640
    },
    {
      "epoch": 0.9284197851642955,
      "grad_norm": 2.3997483253479004,
      "learning_rate": 2.0763124470787468e-05,
      "loss": 0.8512,
      "step": 14650
    },
    {
      "epoch": 0.9290535188060458,
      "grad_norm": 2.539452075958252,
      "learning_rate": 2.0756773920406438e-05,
      "loss": 0.8849,
      "step": 14660
    },
    {
      "epoch": 0.9296872524477962,
      "grad_norm": 2.6081721782684326,
      "learning_rate": 2.0750423370025404e-05,
      "loss": 0.8719,
      "step": 14670
    },
    {
      "epoch": 0.9303209860895466,
      "grad_norm": 2.444533586502075,
      "learning_rate": 2.0744072819644368e-05,
      "loss": 0.8706,
      "step": 14680
    },
    {
      "epoch": 0.930954719731297,
      "grad_norm": 2.5986897945404053,
      "learning_rate": 2.0737722269263337e-05,
      "loss": 0.8597,
      "step": 14690
    },
    {
      "epoch": 0.9315884533730473,
      "grad_norm": 2.6561858654022217,
      "learning_rate": 2.0731371718882304e-05,
      "loss": 0.8629,
      "step": 14700
    },
    {
      "epoch": 0.9322221870147976,
      "grad_norm": 2.3377206325531006,
      "learning_rate": 2.072502116850127e-05,
      "loss": 0.9112,
      "step": 14710
    },
    {
      "epoch": 0.9328559206565481,
      "grad_norm": 2.395174503326416,
      "learning_rate": 2.071867061812024e-05,
      "loss": 0.8599,
      "step": 14720
    },
    {
      "epoch": 0.9334896542982984,
      "grad_norm": 2.5609233379364014,
      "learning_rate": 2.0712320067739203e-05,
      "loss": 0.8582,
      "step": 14730
    },
    {
      "epoch": 0.9341233879400488,
      "grad_norm": 2.049445390701294,
      "learning_rate": 2.070596951735817e-05,
      "loss": 0.8735,
      "step": 14740
    },
    {
      "epoch": 0.9347571215817991,
      "grad_norm": 2.3222410678863525,
      "learning_rate": 2.069961896697714e-05,
      "loss": 0.8348,
      "step": 14750
    },
    {
      "epoch": 0.9353908552235496,
      "grad_norm": 2.484666347503662,
      "learning_rate": 2.0693268416596106e-05,
      "loss": 0.8857,
      "step": 14760
    },
    {
      "epoch": 0.9360245888652999,
      "grad_norm": 2.5750062465667725,
      "learning_rate": 2.0686917866215072e-05,
      "loss": 0.8835,
      "step": 14770
    },
    {
      "epoch": 0.9366583225070503,
      "grad_norm": 2.3875198364257812,
      "learning_rate": 2.068056731583404e-05,
      "loss": 0.8499,
      "step": 14780
    },
    {
      "epoch": 0.9372920561488006,
      "grad_norm": 2.586940050125122,
      "learning_rate": 2.0674216765453005e-05,
      "loss": 0.8461,
      "step": 14790
    },
    {
      "epoch": 0.9379257897905511,
      "grad_norm": 2.339401960372925,
      "learning_rate": 2.0667866215071975e-05,
      "loss": 0.8826,
      "step": 14800
    },
    {
      "epoch": 0.9385595234323014,
      "grad_norm": 2.671529531478882,
      "learning_rate": 2.066151566469094e-05,
      "loss": 0.8734,
      "step": 14810
    },
    {
      "epoch": 0.9391932570740518,
      "grad_norm": 2.286008596420288,
      "learning_rate": 2.0655165114309908e-05,
      "loss": 0.8883,
      "step": 14820
    },
    {
      "epoch": 0.9398269907158021,
      "grad_norm": 2.4221835136413574,
      "learning_rate": 2.0648814563928874e-05,
      "loss": 0.8792,
      "step": 14830
    },
    {
      "epoch": 0.9404607243575526,
      "grad_norm": 2.6714398860931396,
      "learning_rate": 2.064246401354784e-05,
      "loss": 0.8529,
      "step": 14840
    },
    {
      "epoch": 0.9410944579993029,
      "grad_norm": 2.8200600147247314,
      "learning_rate": 2.0636113463166807e-05,
      "loss": 0.8885,
      "step": 14850
    },
    {
      "epoch": 0.9417281916410533,
      "grad_norm": 2.4842560291290283,
      "learning_rate": 2.0629762912785777e-05,
      "loss": 0.9278,
      "step": 14860
    },
    {
      "epoch": 0.9423619252828036,
      "grad_norm": 3.5271599292755127,
      "learning_rate": 2.0623412362404743e-05,
      "loss": 0.8492,
      "step": 14870
    },
    {
      "epoch": 0.9429956589245541,
      "grad_norm": 2.208570957183838,
      "learning_rate": 2.061706181202371e-05,
      "loss": 0.8889,
      "step": 14880
    },
    {
      "epoch": 0.9436293925663044,
      "grad_norm": 2.4343254566192627,
      "learning_rate": 2.0610711261642676e-05,
      "loss": 0.8737,
      "step": 14890
    },
    {
      "epoch": 0.9442631262080547,
      "grad_norm": 2.605602741241455,
      "learning_rate": 2.0604360711261642e-05,
      "loss": 0.8484,
      "step": 14900
    },
    {
      "epoch": 0.9448968598498051,
      "grad_norm": 2.408208131790161,
      "learning_rate": 2.059801016088061e-05,
      "loss": 0.8669,
      "step": 14910
    },
    {
      "epoch": 0.9455305934915555,
      "grad_norm": 2.164874792098999,
      "learning_rate": 2.059165961049958e-05,
      "loss": 0.8732,
      "step": 14920
    },
    {
      "epoch": 0.9461643271333059,
      "grad_norm": 2.2660984992980957,
      "learning_rate": 2.0585309060118545e-05,
      "loss": 0.8615,
      "step": 14930
    },
    {
      "epoch": 0.9467980607750562,
      "grad_norm": 2.8095366954803467,
      "learning_rate": 2.057895850973751e-05,
      "loss": 0.8927,
      "step": 14940
    },
    {
      "epoch": 0.9474317944168066,
      "grad_norm": 2.587510585784912,
      "learning_rate": 2.0572607959356478e-05,
      "loss": 0.8961,
      "step": 14950
    },
    {
      "epoch": 0.948065528058557,
      "grad_norm": 2.3466150760650635,
      "learning_rate": 2.0566257408975444e-05,
      "loss": 0.8472,
      "step": 14960
    },
    {
      "epoch": 0.9486992617003074,
      "grad_norm": 2.4450926780700684,
      "learning_rate": 2.0559906858594414e-05,
      "loss": 0.8801,
      "step": 14970
    },
    {
      "epoch": 0.9493329953420577,
      "grad_norm": 2.7271716594696045,
      "learning_rate": 2.055355630821338e-05,
      "loss": 0.8825,
      "step": 14980
    },
    {
      "epoch": 0.9499667289838081,
      "grad_norm": 2.712165594100952,
      "learning_rate": 2.0547205757832344e-05,
      "loss": 0.8684,
      "step": 14990
    },
    {
      "epoch": 0.9506004626255585,
      "grad_norm": 2.710509777069092,
      "learning_rate": 2.0540855207451314e-05,
      "loss": 0.9051,
      "step": 15000
    },
    {
      "epoch": 0.9512341962673089,
      "grad_norm": 3.632862091064453,
      "learning_rate": 2.053450465707028e-05,
      "loss": 0.8984,
      "step": 15010
    },
    {
      "epoch": 0.9518679299090592,
      "grad_norm": 2.6096932888031006,
      "learning_rate": 2.0528154106689246e-05,
      "loss": 0.8504,
      "step": 15020
    },
    {
      "epoch": 0.9525016635508096,
      "grad_norm": 3.0851762294769287,
      "learning_rate": 2.0521803556308216e-05,
      "loss": 0.894,
      "step": 15030
    },
    {
      "epoch": 0.95313539719256,
      "grad_norm": 2.95367693901062,
      "learning_rate": 2.051545300592718e-05,
      "loss": 0.8818,
      "step": 15040
    },
    {
      "epoch": 0.9537691308343104,
      "grad_norm": 2.68985652923584,
      "learning_rate": 2.0509102455546146e-05,
      "loss": 0.8822,
      "step": 15050
    },
    {
      "epoch": 0.9544028644760607,
      "grad_norm": 2.3700075149536133,
      "learning_rate": 2.0502751905165115e-05,
      "loss": 0.8521,
      "step": 15060
    },
    {
      "epoch": 0.955036598117811,
      "grad_norm": 2.49988055229187,
      "learning_rate": 2.0496401354784082e-05,
      "loss": 0.8599,
      "step": 15070
    },
    {
      "epoch": 0.9556703317595615,
      "grad_norm": 2.3011393547058105,
      "learning_rate": 2.049005080440305e-05,
      "loss": 0.8381,
      "step": 15080
    },
    {
      "epoch": 0.9563040654013119,
      "grad_norm": 2.0119988918304443,
      "learning_rate": 2.048433530906012e-05,
      "loss": 0.8637,
      "step": 15090
    },
    {
      "epoch": 0.9569377990430622,
      "grad_norm": 2.187485933303833,
      "learning_rate": 2.0477984758679085e-05,
      "loss": 0.8653,
      "step": 15100
    },
    {
      "epoch": 0.9575715326848125,
      "grad_norm": 2.99790096282959,
      "learning_rate": 2.047163420829805e-05,
      "loss": 0.8583,
      "step": 15110
    },
    {
      "epoch": 0.958205266326563,
      "grad_norm": 2.6936228275299072,
      "learning_rate": 2.046528365791702e-05,
      "loss": 0.8771,
      "step": 15120
    },
    {
      "epoch": 0.9588389999683133,
      "grad_norm": 2.5065643787384033,
      "learning_rate": 2.0458933107535988e-05,
      "loss": 0.89,
      "step": 15130
    },
    {
      "epoch": 0.9594727336100637,
      "grad_norm": 2.505967855453491,
      "learning_rate": 2.0452582557154954e-05,
      "loss": 0.9155,
      "step": 15140
    },
    {
      "epoch": 0.960106467251814,
      "grad_norm": 2.3820481300354004,
      "learning_rate": 2.044623200677392e-05,
      "loss": 0.8641,
      "step": 15150
    },
    {
      "epoch": 0.9607402008935645,
      "grad_norm": 3.044936180114746,
      "learning_rate": 2.0439881456392887e-05,
      "loss": 0.8434,
      "step": 15160
    },
    {
      "epoch": 0.9613739345353148,
      "grad_norm": 2.4917218685150146,
      "learning_rate": 2.0433530906011854e-05,
      "loss": 0.8387,
      "step": 15170
    },
    {
      "epoch": 0.9620076681770652,
      "grad_norm": 3.0331599712371826,
      "learning_rate": 2.0427180355630823e-05,
      "loss": 0.8217,
      "step": 15180
    },
    {
      "epoch": 0.9626414018188155,
      "grad_norm": 3.026888847351074,
      "learning_rate": 2.042082980524979e-05,
      "loss": 0.835,
      "step": 15190
    },
    {
      "epoch": 0.963275135460566,
      "grad_norm": 2.694187879562378,
      "learning_rate": 2.0414479254868753e-05,
      "loss": 0.8904,
      "step": 15200
    },
    {
      "epoch": 0.9639088691023163,
      "grad_norm": 2.3510208129882812,
      "learning_rate": 2.0408128704487723e-05,
      "loss": 0.8662,
      "step": 15210
    },
    {
      "epoch": 0.9645426027440667,
      "grad_norm": 2.6223385334014893,
      "learning_rate": 2.040177815410669e-05,
      "loss": 0.8506,
      "step": 15220
    },
    {
      "epoch": 0.965176336385817,
      "grad_norm": 2.162346601486206,
      "learning_rate": 2.0395427603725656e-05,
      "loss": 0.882,
      "step": 15230
    },
    {
      "epoch": 0.9658100700275675,
      "grad_norm": 2.4513185024261475,
      "learning_rate": 2.0389077053344625e-05,
      "loss": 0.909,
      "step": 15240
    },
    {
      "epoch": 0.9664438036693178,
      "grad_norm": 3.206282138824463,
      "learning_rate": 2.0382726502963592e-05,
      "loss": 0.8827,
      "step": 15250
    },
    {
      "epoch": 0.9670775373110682,
      "grad_norm": 2.726104736328125,
      "learning_rate": 2.0376375952582558e-05,
      "loss": 0.8786,
      "step": 15260
    },
    {
      "epoch": 0.9677112709528185,
      "grad_norm": 2.658155679702759,
      "learning_rate": 2.0370025402201525e-05,
      "loss": 0.873,
      "step": 15270
    },
    {
      "epoch": 0.9683450045945688,
      "grad_norm": 2.4895553588867188,
      "learning_rate": 2.036367485182049e-05,
      "loss": 0.896,
      "step": 15280
    },
    {
      "epoch": 0.9689787382363193,
      "grad_norm": 2.5433578491210938,
      "learning_rate": 2.035732430143946e-05,
      "loss": 0.8935,
      "step": 15290
    },
    {
      "epoch": 0.9696124718780696,
      "grad_norm": 2.1555144786834717,
      "learning_rate": 2.0350973751058427e-05,
      "loss": 0.8739,
      "step": 15300
    },
    {
      "epoch": 0.97024620551982,
      "grad_norm": 2.538215398788452,
      "learning_rate": 2.034462320067739e-05,
      "loss": 0.8695,
      "step": 15310
    },
    {
      "epoch": 0.9708799391615703,
      "grad_norm": 2.2878780364990234,
      "learning_rate": 2.033827265029636e-05,
      "loss": 0.8747,
      "step": 15320
    },
    {
      "epoch": 0.9715136728033208,
      "grad_norm": 2.302175760269165,
      "learning_rate": 2.0331922099915327e-05,
      "loss": 0.8401,
      "step": 15330
    },
    {
      "epoch": 0.9721474064450711,
      "grad_norm": 2.255842924118042,
      "learning_rate": 2.0325571549534293e-05,
      "loss": 0.8471,
      "step": 15340
    },
    {
      "epoch": 0.9727811400868215,
      "grad_norm": 2.505828380584717,
      "learning_rate": 2.0319220999153263e-05,
      "loss": 0.8851,
      "step": 15350
    },
    {
      "epoch": 0.9734148737285718,
      "grad_norm": 2.6507742404937744,
      "learning_rate": 2.0312870448772226e-05,
      "loss": 0.8901,
      "step": 15360
    },
    {
      "epoch": 0.9740486073703223,
      "grad_norm": 2.6544458866119385,
      "learning_rate": 2.0306519898391192e-05,
      "loss": 0.8009,
      "step": 15370
    },
    {
      "epoch": 0.9746823410120726,
      "grad_norm": 2.445568799972534,
      "learning_rate": 2.0300169348010162e-05,
      "loss": 0.8641,
      "step": 15380
    },
    {
      "epoch": 0.975316074653823,
      "grad_norm": 2.560854434967041,
      "learning_rate": 2.029381879762913e-05,
      "loss": 0.8367,
      "step": 15390
    },
    {
      "epoch": 0.9759498082955733,
      "grad_norm": 2.42287015914917,
      "learning_rate": 2.0287468247248095e-05,
      "loss": 0.8513,
      "step": 15400
    },
    {
      "epoch": 0.9765835419373238,
      "grad_norm": 2.5177502632141113,
      "learning_rate": 2.028111769686706e-05,
      "loss": 0.8778,
      "step": 15410
    },
    {
      "epoch": 0.9772172755790741,
      "grad_norm": 2.231316328048706,
      "learning_rate": 2.0274767146486028e-05,
      "loss": 0.8683,
      "step": 15420
    },
    {
      "epoch": 0.9778510092208245,
      "grad_norm": 2.6936850547790527,
      "learning_rate": 2.0268416596104998e-05,
      "loss": 0.8525,
      "step": 15430
    },
    {
      "epoch": 0.9784847428625748,
      "grad_norm": 2.6311330795288086,
      "learning_rate": 2.0262066045723964e-05,
      "loss": 0.863,
      "step": 15440
    },
    {
      "epoch": 0.9791184765043253,
      "grad_norm": 2.400651454925537,
      "learning_rate": 2.025571549534293e-05,
      "loss": 0.8526,
      "step": 15450
    },
    {
      "epoch": 0.9797522101460756,
      "grad_norm": 2.466874837875366,
      "learning_rate": 2.0249364944961897e-05,
      "loss": 0.8581,
      "step": 15460
    },
    {
      "epoch": 0.980385943787826,
      "grad_norm": 2.8673195838928223,
      "learning_rate": 2.0243014394580863e-05,
      "loss": 0.9221,
      "step": 15470
    },
    {
      "epoch": 0.9810196774295763,
      "grad_norm": 3.124176263809204,
      "learning_rate": 2.023666384419983e-05,
      "loss": 0.8638,
      "step": 15480
    },
    {
      "epoch": 0.9816534110713268,
      "grad_norm": 2.4516215324401855,
      "learning_rate": 2.02303132938188e-05,
      "loss": 0.8559,
      "step": 15490
    },
    {
      "epoch": 0.9822871447130771,
      "grad_norm": 2.392122507095337,
      "learning_rate": 2.0223962743437766e-05,
      "loss": 0.8719,
      "step": 15500
    },
    {
      "epoch": 0.9829208783548274,
      "grad_norm": 2.411200761795044,
      "learning_rate": 2.0217612193056732e-05,
      "loss": 0.8719,
      "step": 15510
    },
    {
      "epoch": 0.9835546119965778,
      "grad_norm": 2.7952919006347656,
      "learning_rate": 2.02112616426757e-05,
      "loss": 0.883,
      "step": 15520
    },
    {
      "epoch": 0.9841883456383282,
      "grad_norm": 2.775820255279541,
      "learning_rate": 2.0204911092294665e-05,
      "loss": 0.8715,
      "step": 15530
    },
    {
      "epoch": 0.9848220792800786,
      "grad_norm": 2.8677566051483154,
      "learning_rate": 2.0198560541913632e-05,
      "loss": 0.8686,
      "step": 15540
    },
    {
      "epoch": 0.9854558129218289,
      "grad_norm": 3.075369358062744,
      "learning_rate": 2.01922099915326e-05,
      "loss": 0.8812,
      "step": 15550
    },
    {
      "epoch": 0.9860895465635793,
      "grad_norm": 2.4495086669921875,
      "learning_rate": 2.0185859441151568e-05,
      "loss": 0.8357,
      "step": 15560
    },
    {
      "epoch": 0.9867232802053297,
      "grad_norm": 2.692326545715332,
      "learning_rate": 2.0179508890770534e-05,
      "loss": 0.8936,
      "step": 15570
    },
    {
      "epoch": 0.9873570138470801,
      "grad_norm": 2.415086030960083,
      "learning_rate": 2.01731583403895e-05,
      "loss": 0.9026,
      "step": 15580
    },
    {
      "epoch": 0.9879907474888304,
      "grad_norm": 2.3063783645629883,
      "learning_rate": 2.0166807790008467e-05,
      "loss": 0.8609,
      "step": 15590
    },
    {
      "epoch": 0.9886244811305808,
      "grad_norm": 2.295275926589966,
      "learning_rate": 2.0160457239627437e-05,
      "loss": 0.8378,
      "step": 15600
    },
    {
      "epoch": 0.9892582147723312,
      "grad_norm": 2.639955520629883,
      "learning_rate": 2.0154106689246403e-05,
      "loss": 0.8883,
      "step": 15610
    },
    {
      "epoch": 0.9898919484140816,
      "grad_norm": 2.4646201133728027,
      "learning_rate": 2.0147756138865367e-05,
      "loss": 0.9025,
      "step": 15620
    },
    {
      "epoch": 0.9905256820558319,
      "grad_norm": 2.663939952850342,
      "learning_rate": 2.0141405588484336e-05,
      "loss": 0.842,
      "step": 15630
    },
    {
      "epoch": 0.9911594156975823,
      "grad_norm": 2.4661216735839844,
      "learning_rate": 2.0135055038103303e-05,
      "loss": 0.8337,
      "step": 15640
    },
    {
      "epoch": 0.9917931493393327,
      "grad_norm": 2.309645652770996,
      "learning_rate": 2.012870448772227e-05,
      "loss": 0.8566,
      "step": 15650
    },
    {
      "epoch": 0.9924268829810831,
      "grad_norm": 2.5306613445281982,
      "learning_rate": 2.012235393734124e-05,
      "loss": 0.8823,
      "step": 15660
    },
    {
      "epoch": 0.9930606166228334,
      "grad_norm": 1.964311957359314,
      "learning_rate": 2.0116003386960202e-05,
      "loss": 0.8876,
      "step": 15670
    },
    {
      "epoch": 0.9936943502645837,
      "grad_norm": 2.641120195388794,
      "learning_rate": 2.010965283657917e-05,
      "loss": 0.8791,
      "step": 15680
    },
    {
      "epoch": 0.9943280839063342,
      "grad_norm": 2.6402933597564697,
      "learning_rate": 2.0103302286198138e-05,
      "loss": 0.8973,
      "step": 15690
    },
    {
      "epoch": 0.9949618175480845,
      "grad_norm": 2.8615903854370117,
      "learning_rate": 2.0096951735817105e-05,
      "loss": 0.8612,
      "step": 15700
    },
    {
      "epoch": 0.9955955511898349,
      "grad_norm": 2.7606301307678223,
      "learning_rate": 2.009060118543607e-05,
      "loss": 0.9033,
      "step": 15710
    },
    {
      "epoch": 0.9962292848315852,
      "grad_norm": 2.2864296436309814,
      "learning_rate": 2.0084250635055038e-05,
      "loss": 0.8381,
      "step": 15720
    },
    {
      "epoch": 0.9968630184733357,
      "grad_norm": 2.898097276687622,
      "learning_rate": 2.0077900084674004e-05,
      "loss": 0.8979,
      "step": 15730
    },
    {
      "epoch": 0.997496752115086,
      "grad_norm": 2.329012393951416,
      "learning_rate": 2.0071549534292974e-05,
      "loss": 0.8583,
      "step": 15740
    },
    {
      "epoch": 0.9981304857568364,
      "grad_norm": 2.687086343765259,
      "learning_rate": 2.006519898391194e-05,
      "loss": 0.8893,
      "step": 15750
    },
    {
      "epoch": 0.9987642193985867,
      "grad_norm": 2.601032018661499,
      "learning_rate": 2.0058848433530907e-05,
      "loss": 0.8557,
      "step": 15760
    },
    {
      "epoch": 0.9993979530403372,
      "grad_norm": 3.1976780891418457,
      "learning_rate": 2.0052497883149876e-05,
      "loss": 0.8985,
      "step": 15770
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.129493236541748,
      "learning_rate": 2.004614733276884e-05,
      "loss": 0.8377,
      "step": 15780
    },
    {
      "epoch": 1.0006337336417503,
      "grad_norm": 2.1803390979766846,
      "learning_rate": 2.0039796782387806e-05,
      "loss": 0.8759,
      "step": 15790
    },
    {
      "epoch": 1.0012674672835007,
      "grad_norm": 2.3581480979919434,
      "learning_rate": 2.0033446232006776e-05,
      "loss": 0.8352,
      "step": 15800
    },
    {
      "epoch": 1.001901200925251,
      "grad_norm": 2.4728405475616455,
      "learning_rate": 2.0027095681625742e-05,
      "loss": 0.8558,
      "step": 15810
    },
    {
      "epoch": 1.0025349345670014,
      "grad_norm": 2.296816349029541,
      "learning_rate": 2.002074513124471e-05,
      "loss": 0.88,
      "step": 15820
    },
    {
      "epoch": 1.003168668208752,
      "grad_norm": 2.98172926902771,
      "learning_rate": 2.0014394580863675e-05,
      "loss": 0.8683,
      "step": 15830
    },
    {
      "epoch": 1.0038024018505023,
      "grad_norm": 2.286242723464966,
      "learning_rate": 2.000804403048264e-05,
      "loss": 0.8854,
      "step": 15840
    },
    {
      "epoch": 1.0044361354922526,
      "grad_norm": 3.4088854789733887,
      "learning_rate": 2.0001693480101608e-05,
      "loss": 0.8884,
      "step": 15850
    },
    {
      "epoch": 1.005069869134003,
      "grad_norm": 2.670619249343872,
      "learning_rate": 1.9995342929720578e-05,
      "loss": 0.8637,
      "step": 15860
    },
    {
      "epoch": 1.0057036027757533,
      "grad_norm": 2.427828788757324,
      "learning_rate": 1.9988992379339544e-05,
      "loss": 0.8607,
      "step": 15870
    },
    {
      "epoch": 1.0063373364175037,
      "grad_norm": 2.7728662490844727,
      "learning_rate": 1.998264182895851e-05,
      "loss": 0.866,
      "step": 15880
    },
    {
      "epoch": 1.006971070059254,
      "grad_norm": 2.250821113586426,
      "learning_rate": 1.9976291278577477e-05,
      "loss": 0.8551,
      "step": 15890
    },
    {
      "epoch": 1.0076048037010044,
      "grad_norm": 2.3570730686187744,
      "learning_rate": 1.9969940728196443e-05,
      "loss": 0.8966,
      "step": 15900
    },
    {
      "epoch": 1.008238537342755,
      "grad_norm": 2.3378310203552246,
      "learning_rate": 1.9963590177815413e-05,
      "loss": 0.8709,
      "step": 15910
    },
    {
      "epoch": 1.0088722709845053,
      "grad_norm": 2.378261089324951,
      "learning_rate": 1.995723962743438e-05,
      "loss": 0.883,
      "step": 15920
    },
    {
      "epoch": 1.0095060046262556,
      "grad_norm": 2.4943177700042725,
      "learning_rate": 1.9950889077053343e-05,
      "loss": 0.8917,
      "step": 15930
    },
    {
      "epoch": 1.010139738268006,
      "grad_norm": 2.5638809204101562,
      "learning_rate": 1.9944538526672312e-05,
      "loss": 0.8397,
      "step": 15940
    },
    {
      "epoch": 1.0107734719097563,
      "grad_norm": 2.6158382892608643,
      "learning_rate": 1.993818797629128e-05,
      "loss": 0.8722,
      "step": 15950
    },
    {
      "epoch": 1.0114072055515066,
      "grad_norm": 2.7740511894226074,
      "learning_rate": 1.9931837425910245e-05,
      "loss": 0.8181,
      "step": 15960
    },
    {
      "epoch": 1.012040939193257,
      "grad_norm": 2.800326108932495,
      "learning_rate": 1.9925486875529215e-05,
      "loss": 0.8871,
      "step": 15970
    },
    {
      "epoch": 1.0126746728350073,
      "grad_norm": 2.4141054153442383,
      "learning_rate": 1.9919136325148178e-05,
      "loss": 0.8714,
      "step": 15980
    },
    {
      "epoch": 1.013308406476758,
      "grad_norm": 3.007450819015503,
      "learning_rate": 1.9912785774767145e-05,
      "loss": 0.9273,
      "step": 15990
    },
    {
      "epoch": 1.0139421401185083,
      "grad_norm": 2.6646335124969482,
      "learning_rate": 1.9906435224386114e-05,
      "loss": 0.8629,
      "step": 16000
    },
    {
      "epoch": 1.0145758737602586,
      "grad_norm": 2.451300621032715,
      "learning_rate": 1.990008467400508e-05,
      "loss": 0.8676,
      "step": 16010
    },
    {
      "epoch": 1.015209607402009,
      "grad_norm": 2.3866002559661865,
      "learning_rate": 1.9893734123624047e-05,
      "loss": 0.8606,
      "step": 16020
    },
    {
      "epoch": 1.0158433410437593,
      "grad_norm": 2.9598777294158936,
      "learning_rate": 1.9887383573243017e-05,
      "loss": 0.8517,
      "step": 16030
    },
    {
      "epoch": 1.0164770746855096,
      "grad_norm": 2.4374582767486572,
      "learning_rate": 1.988103302286198e-05,
      "loss": 0.8751,
      "step": 16040
    },
    {
      "epoch": 1.01711080832726,
      "grad_norm": 2.178262710571289,
      "learning_rate": 1.987468247248095e-05,
      "loss": 0.8745,
      "step": 16050
    },
    {
      "epoch": 1.0177445419690103,
      "grad_norm": 2.6731536388397217,
      "learning_rate": 1.9868331922099916e-05,
      "loss": 0.8895,
      "step": 16060
    },
    {
      "epoch": 1.0183782756107609,
      "grad_norm": 2.452091932296753,
      "learning_rate": 1.9861981371718883e-05,
      "loss": 0.8885,
      "step": 16070
    },
    {
      "epoch": 1.0190120092525112,
      "grad_norm": 2.285182476043701,
      "learning_rate": 1.9855630821337853e-05,
      "loss": 0.8874,
      "step": 16080
    },
    {
      "epoch": 1.0196457428942616,
      "grad_norm": 2.6067702770233154,
      "learning_rate": 1.9849280270956816e-05,
      "loss": 0.8822,
      "step": 16090
    },
    {
      "epoch": 1.020279476536012,
      "grad_norm": 2.5393550395965576,
      "learning_rate": 1.9842929720575782e-05,
      "loss": 0.8586,
      "step": 16100
    },
    {
      "epoch": 1.0209132101777623,
      "grad_norm": 3.0290029048919678,
      "learning_rate": 1.9836579170194752e-05,
      "loss": 0.8713,
      "step": 16110
    },
    {
      "epoch": 1.0215469438195126,
      "grad_norm": 2.461786985397339,
      "learning_rate": 1.983022861981372e-05,
      "loss": 0.8633,
      "step": 16120
    },
    {
      "epoch": 1.022180677461263,
      "grad_norm": 2.3880152702331543,
      "learning_rate": 1.9823878069432685e-05,
      "loss": 0.8374,
      "step": 16130
    },
    {
      "epoch": 1.0228144111030133,
      "grad_norm": 2.765738010406494,
      "learning_rate": 1.981752751905165e-05,
      "loss": 0.8804,
      "step": 16140
    },
    {
      "epoch": 1.0234481447447639,
      "grad_norm": 3.054748296737671,
      "learning_rate": 1.9811176968670618e-05,
      "loss": 0.8653,
      "step": 16150
    },
    {
      "epoch": 1.0240818783865142,
      "grad_norm": 2.893095016479492,
      "learning_rate": 1.9804826418289584e-05,
      "loss": 0.886,
      "step": 16160
    },
    {
      "epoch": 1.0247156120282646,
      "grad_norm": 2.5597310066223145,
      "learning_rate": 1.9798475867908554e-05,
      "loss": 0.8657,
      "step": 16170
    },
    {
      "epoch": 1.025349345670015,
      "grad_norm": 2.507194995880127,
      "learning_rate": 1.979212531752752e-05,
      "loss": 0.8813,
      "step": 16180
    },
    {
      "epoch": 1.0259830793117652,
      "grad_norm": 2.3967602252960205,
      "learning_rate": 1.9785774767146487e-05,
      "loss": 0.8694,
      "step": 16190
    },
    {
      "epoch": 1.0266168129535156,
      "grad_norm": 2.410937547683716,
      "learning_rate": 1.9779424216765453e-05,
      "loss": 0.889,
      "step": 16200
    },
    {
      "epoch": 1.027250546595266,
      "grad_norm": 2.7730066776275635,
      "learning_rate": 1.977307366638442e-05,
      "loss": 0.8853,
      "step": 16210
    },
    {
      "epoch": 1.0278842802370163,
      "grad_norm": 2.665311098098755,
      "learning_rate": 1.976672311600339e-05,
      "loss": 0.8666,
      "step": 16220
    },
    {
      "epoch": 1.0285180138787668,
      "grad_norm": 2.68996524810791,
      "learning_rate": 1.9760372565622356e-05,
      "loss": 0.8801,
      "step": 16230
    },
    {
      "epoch": 1.0291517475205172,
      "grad_norm": 2.2773544788360596,
      "learning_rate": 1.975402201524132e-05,
      "loss": 0.8948,
      "step": 16240
    },
    {
      "epoch": 1.0297854811622675,
      "grad_norm": 2.771078109741211,
      "learning_rate": 1.974767146486029e-05,
      "loss": 0.9066,
      "step": 16250
    },
    {
      "epoch": 1.0304192148040179,
      "grad_norm": 2.4437310695648193,
      "learning_rate": 1.9741320914479255e-05,
      "loss": 0.8449,
      "step": 16260
    },
    {
      "epoch": 1.0310529484457682,
      "grad_norm": 2.2938568592071533,
      "learning_rate": 1.973497036409822e-05,
      "loss": 0.8186,
      "step": 16270
    },
    {
      "epoch": 1.0316866820875186,
      "grad_norm": 2.4876201152801514,
      "learning_rate": 1.972861981371719e-05,
      "loss": 0.8798,
      "step": 16280
    },
    {
      "epoch": 1.032320415729269,
      "grad_norm": 2.5379176139831543,
      "learning_rate": 1.9722269263336158e-05,
      "loss": 0.9195,
      "step": 16290
    },
    {
      "epoch": 1.0329541493710193,
      "grad_norm": 2.791412830352783,
      "learning_rate": 1.971591871295512e-05,
      "loss": 0.8848,
      "step": 16300
    },
    {
      "epoch": 1.0335878830127698,
      "grad_norm": 2.8081443309783936,
      "learning_rate": 1.970956816257409e-05,
      "loss": 0.9028,
      "step": 16310
    },
    {
      "epoch": 1.0342216166545202,
      "grad_norm": 2.5783441066741943,
      "learning_rate": 1.9703217612193057e-05,
      "loss": 0.8711,
      "step": 16320
    },
    {
      "epoch": 1.0348553502962705,
      "grad_norm": 2.6712257862091064,
      "learning_rate": 1.9696867061812023e-05,
      "loss": 0.8375,
      "step": 16330
    },
    {
      "epoch": 1.0354890839380209,
      "grad_norm": 2.5964436531066895,
      "learning_rate": 1.9690516511430993e-05,
      "loss": 0.8871,
      "step": 16340
    },
    {
      "epoch": 1.0361228175797712,
      "grad_norm": 2.4651334285736084,
      "learning_rate": 1.9684165961049956e-05,
      "loss": 0.8851,
      "step": 16350
    },
    {
      "epoch": 1.0367565512215215,
      "grad_norm": 2.528292179107666,
      "learning_rate": 1.9677815410668926e-05,
      "loss": 0.8984,
      "step": 16360
    },
    {
      "epoch": 1.037390284863272,
      "grad_norm": 2.5426888465881348,
      "learning_rate": 1.9671464860287893e-05,
      "loss": 0.872,
      "step": 16370
    },
    {
      "epoch": 1.0380240185050222,
      "grad_norm": 2.558929204940796,
      "learning_rate": 1.966511430990686e-05,
      "loss": 0.8631,
      "step": 16380
    },
    {
      "epoch": 1.0386577521467728,
      "grad_norm": 3.068251848220825,
      "learning_rate": 1.965876375952583e-05,
      "loss": 0.8895,
      "step": 16390
    },
    {
      "epoch": 1.0392914857885232,
      "grad_norm": 2.7002878189086914,
      "learning_rate": 1.9652413209144792e-05,
      "loss": 0.8645,
      "step": 16400
    },
    {
      "epoch": 1.0399252194302735,
      "grad_norm": 2.8268380165100098,
      "learning_rate": 1.9646062658763758e-05,
      "loss": 0.8466,
      "step": 16410
    },
    {
      "epoch": 1.0405589530720238,
      "grad_norm": 2.427044630050659,
      "learning_rate": 1.9639712108382728e-05,
      "loss": 0.8469,
      "step": 16420
    },
    {
      "epoch": 1.0411926867137742,
      "grad_norm": 2.712270975112915,
      "learning_rate": 1.9633361558001695e-05,
      "loss": 0.9328,
      "step": 16430
    },
    {
      "epoch": 1.0418264203555245,
      "grad_norm": 2.384371757507324,
      "learning_rate": 1.962701100762066e-05,
      "loss": 0.8405,
      "step": 16440
    },
    {
      "epoch": 1.0424601539972749,
      "grad_norm": 2.3064467906951904,
      "learning_rate": 1.9620660457239627e-05,
      "loss": 0.8796,
      "step": 16450
    },
    {
      "epoch": 1.0430938876390252,
      "grad_norm": 3.117154121398926,
      "learning_rate": 1.9614309906858594e-05,
      "loss": 0.8569,
      "step": 16460
    },
    {
      "epoch": 1.0437276212807758,
      "grad_norm": 2.681518077850342,
      "learning_rate": 1.960795935647756e-05,
      "loss": 0.8467,
      "step": 16470
    },
    {
      "epoch": 1.0443613549225261,
      "grad_norm": 2.6589419841766357,
      "learning_rate": 1.960160880609653e-05,
      "loss": 0.8981,
      "step": 16480
    },
    {
      "epoch": 1.0449950885642765,
      "grad_norm": 2.5468082427978516,
      "learning_rate": 1.9595258255715496e-05,
      "loss": 0.8999,
      "step": 16490
    },
    {
      "epoch": 1.0456288222060268,
      "grad_norm": 2.3322079181671143,
      "learning_rate": 1.958890770533446e-05,
      "loss": 0.8551,
      "step": 16500
    },
    {
      "epoch": 1.0462625558477772,
      "grad_norm": 2.5373101234436035,
      "learning_rate": 1.958255715495343e-05,
      "loss": 0.9258,
      "step": 16510
    },
    {
      "epoch": 1.0468962894895275,
      "grad_norm": 2.9862022399902344,
      "learning_rate": 1.9576206604572396e-05,
      "loss": 0.8388,
      "step": 16520
    },
    {
      "epoch": 1.0475300231312779,
      "grad_norm": 2.5495927333831787,
      "learning_rate": 1.9569856054191366e-05,
      "loss": 0.9098,
      "step": 16530
    },
    {
      "epoch": 1.0481637567730282,
      "grad_norm": 2.364136219024658,
      "learning_rate": 1.9563505503810332e-05,
      "loss": 0.8236,
      "step": 16540
    },
    {
      "epoch": 1.0487974904147788,
      "grad_norm": 2.3461313247680664,
      "learning_rate": 1.95571549534293e-05,
      "loss": 0.9095,
      "step": 16550
    },
    {
      "epoch": 1.049431224056529,
      "grad_norm": 2.747528314590454,
      "learning_rate": 1.9550804403048265e-05,
      "loss": 0.8693,
      "step": 16560
    },
    {
      "epoch": 1.0500649576982795,
      "grad_norm": 2.5940258502960205,
      "learning_rate": 1.954445385266723e-05,
      "loss": 0.9074,
      "step": 16570
    },
    {
      "epoch": 1.0506986913400298,
      "grad_norm": 2.911616563796997,
      "learning_rate": 1.9538103302286198e-05,
      "loss": 0.8764,
      "step": 16580
    },
    {
      "epoch": 1.0513324249817801,
      "grad_norm": 2.8236565589904785,
      "learning_rate": 1.9531752751905168e-05,
      "loss": 0.8737,
      "step": 16590
    },
    {
      "epoch": 1.0519661586235305,
      "grad_norm": 2.619009494781494,
      "learning_rate": 1.9525402201524134e-05,
      "loss": 0.8629,
      "step": 16600
    },
    {
      "epoch": 1.0525998922652808,
      "grad_norm": 2.4357542991638184,
      "learning_rate": 1.9519051651143097e-05,
      "loss": 0.9052,
      "step": 16610
    },
    {
      "epoch": 1.0532336259070312,
      "grad_norm": 2.5646255016326904,
      "learning_rate": 1.9512701100762067e-05,
      "loss": 0.8702,
      "step": 16620
    },
    {
      "epoch": 1.0538673595487817,
      "grad_norm": 2.262667417526245,
      "learning_rate": 1.9506350550381033e-05,
      "loss": 0.7974,
      "step": 16630
    },
    {
      "epoch": 1.054501093190532,
      "grad_norm": 2.3088486194610596,
      "learning_rate": 1.95e-05,
      "loss": 0.9184,
      "step": 16640
    },
    {
      "epoch": 1.0551348268322824,
      "grad_norm": 3.6780405044555664,
      "learning_rate": 1.949364944961897e-05,
      "loss": 0.8883,
      "step": 16650
    },
    {
      "epoch": 1.0557685604740328,
      "grad_norm": 2.8897109031677246,
      "learning_rate": 1.9487298899237933e-05,
      "loss": 0.857,
      "step": 16660
    },
    {
      "epoch": 1.0564022941157831,
      "grad_norm": 2.471345901489258,
      "learning_rate": 1.9480948348856902e-05,
      "loss": 0.8612,
      "step": 16670
    },
    {
      "epoch": 1.0570360277575335,
      "grad_norm": 2.8213489055633545,
      "learning_rate": 1.947459779847587e-05,
      "loss": 0.8968,
      "step": 16680
    },
    {
      "epoch": 1.0576697613992838,
      "grad_norm": 2.368732213973999,
      "learning_rate": 1.9468247248094835e-05,
      "loss": 0.8544,
      "step": 16690
    },
    {
      "epoch": 1.0583034950410342,
      "grad_norm": 4.443994522094727,
      "learning_rate": 1.9461896697713805e-05,
      "loss": 0.8621,
      "step": 16700
    },
    {
      "epoch": 1.0589372286827847,
      "grad_norm": 2.608466148376465,
      "learning_rate": 1.9455546147332768e-05,
      "loss": 0.889,
      "step": 16710
    },
    {
      "epoch": 1.059570962324535,
      "grad_norm": 2.5331149101257324,
      "learning_rate": 1.9449195596951734e-05,
      "loss": 0.8535,
      "step": 16720
    },
    {
      "epoch": 1.0602046959662854,
      "grad_norm": 2.6327850818634033,
      "learning_rate": 1.9442845046570704e-05,
      "loss": 0.8855,
      "step": 16730
    },
    {
      "epoch": 1.0608384296080358,
      "grad_norm": 2.5378453731536865,
      "learning_rate": 1.943649449618967e-05,
      "loss": 0.8657,
      "step": 16740
    },
    {
      "epoch": 1.061472163249786,
      "grad_norm": 2.4014482498168945,
      "learning_rate": 1.9430143945808637e-05,
      "loss": 0.9126,
      "step": 16750
    },
    {
      "epoch": 1.0621058968915364,
      "grad_norm": 2.5663225650787354,
      "learning_rate": 1.9423793395427604e-05,
      "loss": 0.8435,
      "step": 16760
    },
    {
      "epoch": 1.0627396305332868,
      "grad_norm": 2.469902276992798,
      "learning_rate": 1.941744284504657e-05,
      "loss": 0.8549,
      "step": 16770
    },
    {
      "epoch": 1.0633733641750371,
      "grad_norm": 2.5894534587860107,
      "learning_rate": 1.9411092294665536e-05,
      "loss": 0.883,
      "step": 16780
    },
    {
      "epoch": 1.0640070978167877,
      "grad_norm": 2.3223960399627686,
      "learning_rate": 1.9404741744284506e-05,
      "loss": 0.859,
      "step": 16790
    },
    {
      "epoch": 1.064640831458538,
      "grad_norm": 2.4826314449310303,
      "learning_rate": 1.9398391193903473e-05,
      "loss": 0.9055,
      "step": 16800
    },
    {
      "epoch": 1.0652745651002884,
      "grad_norm": 2.295527458190918,
      "learning_rate": 1.939204064352244e-05,
      "loss": 0.8902,
      "step": 16810
    },
    {
      "epoch": 1.0659082987420387,
      "grad_norm": 2.3034489154815674,
      "learning_rate": 1.9385690093141405e-05,
      "loss": 0.8702,
      "step": 16820
    },
    {
      "epoch": 1.066542032383789,
      "grad_norm": 2.5362675189971924,
      "learning_rate": 1.9379339542760372e-05,
      "loss": 0.8753,
      "step": 16830
    },
    {
      "epoch": 1.0671757660255394,
      "grad_norm": 2.4268558025360107,
      "learning_rate": 1.9372988992379342e-05,
      "loss": 0.8303,
      "step": 16840
    },
    {
      "epoch": 1.0678094996672898,
      "grad_norm": 2.226613998413086,
      "learning_rate": 1.9366638441998308e-05,
      "loss": 0.8665,
      "step": 16850
    },
    {
      "epoch": 1.0684432333090401,
      "grad_norm": 2.475261688232422,
      "learning_rate": 1.9360287891617275e-05,
      "loss": 0.8523,
      "step": 16860
    },
    {
      "epoch": 1.0690769669507907,
      "grad_norm": 2.1470327377319336,
      "learning_rate": 1.935393734123624e-05,
      "loss": 0.8712,
      "step": 16870
    },
    {
      "epoch": 1.069710700592541,
      "grad_norm": 2.068209648132324,
      "learning_rate": 1.9347586790855207e-05,
      "loss": 0.9059,
      "step": 16880
    },
    {
      "epoch": 1.0703444342342914,
      "grad_norm": 2.9496991634368896,
      "learning_rate": 1.9341236240474174e-05,
      "loss": 0.9039,
      "step": 16890
    },
    {
      "epoch": 1.0709781678760417,
      "grad_norm": 3.2467939853668213,
      "learning_rate": 1.9334885690093144e-05,
      "loss": 0.8326,
      "step": 16900
    },
    {
      "epoch": 1.071611901517792,
      "grad_norm": 2.3001699447631836,
      "learning_rate": 1.932853513971211e-05,
      "loss": 0.8553,
      "step": 16910
    },
    {
      "epoch": 1.0722456351595424,
      "grad_norm": 2.2307333946228027,
      "learning_rate": 1.9322184589331073e-05,
      "loss": 0.8458,
      "step": 16920
    },
    {
      "epoch": 1.0728793688012928,
      "grad_norm": 2.5125720500946045,
      "learning_rate": 1.9315834038950043e-05,
      "loss": 0.8254,
      "step": 16930
    },
    {
      "epoch": 1.073513102443043,
      "grad_norm": 2.592604875564575,
      "learning_rate": 1.930948348856901e-05,
      "loss": 0.8512,
      "step": 16940
    },
    {
      "epoch": 1.0741468360847937,
      "grad_norm": 2.167271614074707,
      "learning_rate": 1.9303132938187976e-05,
      "loss": 0.8263,
      "step": 16950
    },
    {
      "epoch": 1.074780569726544,
      "grad_norm": 2.107748508453369,
      "learning_rate": 1.9296782387806946e-05,
      "loss": 0.8357,
      "step": 16960
    },
    {
      "epoch": 1.0754143033682944,
      "grad_norm": 3.322866678237915,
      "learning_rate": 1.929043183742591e-05,
      "loss": 0.8741,
      "step": 16970
    },
    {
      "epoch": 1.0760480370100447,
      "grad_norm": 3.013709306716919,
      "learning_rate": 1.928408128704488e-05,
      "loss": 0.8897,
      "step": 16980
    },
    {
      "epoch": 1.076681770651795,
      "grad_norm": 2.4502503871917725,
      "learning_rate": 1.9277730736663845e-05,
      "loss": 0.9219,
      "step": 16990
    },
    {
      "epoch": 1.0773155042935454,
      "grad_norm": 2.275407552719116,
      "learning_rate": 1.927138018628281e-05,
      "loss": 0.87,
      "step": 17000
    },
    {
      "epoch": 1.0779492379352957,
      "grad_norm": 2.1542704105377197,
      "learning_rate": 1.926502963590178e-05,
      "loss": 0.8826,
      "step": 17010
    },
    {
      "epoch": 1.078582971577046,
      "grad_norm": 2.2860026359558105,
      "learning_rate": 1.9258679085520744e-05,
      "loss": 0.8735,
      "step": 17020
    },
    {
      "epoch": 1.0792167052187964,
      "grad_norm": 2.474022150039673,
      "learning_rate": 1.925232853513971e-05,
      "loss": 0.8567,
      "step": 17030
    },
    {
      "epoch": 1.079850438860547,
      "grad_norm": 2.800629138946533,
      "learning_rate": 1.924597798475868e-05,
      "loss": 0.8552,
      "step": 17040
    },
    {
      "epoch": 1.0804841725022973,
      "grad_norm": 2.747366189956665,
      "learning_rate": 1.9239627434377647e-05,
      "loss": 0.8514,
      "step": 17050
    },
    {
      "epoch": 1.0811179061440477,
      "grad_norm": 2.6726770401000977,
      "learning_rate": 1.9233276883996613e-05,
      "loss": 0.875,
      "step": 17060
    },
    {
      "epoch": 1.081751639785798,
      "grad_norm": 2.6488006114959717,
      "learning_rate": 1.9226926333615583e-05,
      "loss": 0.86,
      "step": 17070
    },
    {
      "epoch": 1.0823853734275484,
      "grad_norm": 2.5009994506835938,
      "learning_rate": 1.9220575783234546e-05,
      "loss": 0.8964,
      "step": 17080
    },
    {
      "epoch": 1.0830191070692987,
      "grad_norm": 2.7064993381500244,
      "learning_rate": 1.9214225232853513e-05,
      "loss": 0.8954,
      "step": 17090
    },
    {
      "epoch": 1.083652840711049,
      "grad_norm": 2.386752128601074,
      "learning_rate": 1.9207874682472482e-05,
      "loss": 0.8833,
      "step": 17100
    },
    {
      "epoch": 1.0842865743527996,
      "grad_norm": 2.473971366882324,
      "learning_rate": 1.920152413209145e-05,
      "loss": 0.8692,
      "step": 17110
    },
    {
      "epoch": 1.08492030799455,
      "grad_norm": 2.4253127574920654,
      "learning_rate": 1.9195173581710415e-05,
      "loss": 0.888,
      "step": 17120
    },
    {
      "epoch": 1.0855540416363003,
      "grad_norm": 2.992527961730957,
      "learning_rate": 1.918882303132938e-05,
      "loss": 0.8476,
      "step": 17130
    },
    {
      "epoch": 1.0861877752780507,
      "grad_norm": 2.692948579788208,
      "learning_rate": 1.9182472480948348e-05,
      "loss": 0.9307,
      "step": 17140
    },
    {
      "epoch": 1.086821508919801,
      "grad_norm": 2.6746468544006348,
      "learning_rate": 1.9176121930567318e-05,
      "loss": 0.8794,
      "step": 17150
    },
    {
      "epoch": 1.0874552425615513,
      "grad_norm": 2.465853214263916,
      "learning_rate": 1.9169771380186284e-05,
      "loss": 0.8545,
      "step": 17160
    },
    {
      "epoch": 1.0880889762033017,
      "grad_norm": 2.3530690670013428,
      "learning_rate": 1.916342082980525e-05,
      "loss": 0.8422,
      "step": 17170
    },
    {
      "epoch": 1.088722709845052,
      "grad_norm": 2.1890604496002197,
      "learning_rate": 1.9157070279424217e-05,
      "loss": 0.8861,
      "step": 17180
    },
    {
      "epoch": 1.0893564434868024,
      "grad_norm": 2.907397747039795,
      "learning_rate": 1.9150719729043184e-05,
      "loss": 0.8351,
      "step": 17190
    },
    {
      "epoch": 1.089990177128553,
      "grad_norm": 2.3735268115997314,
      "learning_rate": 1.914436917866215e-05,
      "loss": 0.8417,
      "step": 17200
    },
    {
      "epoch": 1.0906239107703033,
      "grad_norm": 3.4129703044891357,
      "learning_rate": 1.913801862828112e-05,
      "loss": 0.893,
      "step": 17210
    },
    {
      "epoch": 1.0912576444120536,
      "grad_norm": 2.5888781547546387,
      "learning_rate": 1.9131668077900086e-05,
      "loss": 0.8557,
      "step": 17220
    },
    {
      "epoch": 1.091891378053804,
      "grad_norm": 2.7348225116729736,
      "learning_rate": 1.912531752751905e-05,
      "loss": 0.8932,
      "step": 17230
    },
    {
      "epoch": 1.0925251116955543,
      "grad_norm": 2.422567367553711,
      "learning_rate": 1.911896697713802e-05,
      "loss": 0.8808,
      "step": 17240
    },
    {
      "epoch": 1.0931588453373047,
      "grad_norm": 2.6059226989746094,
      "learning_rate": 1.9112616426756986e-05,
      "loss": 0.8268,
      "step": 17250
    },
    {
      "epoch": 1.093792578979055,
      "grad_norm": 2.372164726257324,
      "learning_rate": 1.9106265876375952e-05,
      "loss": 0.8574,
      "step": 17260
    },
    {
      "epoch": 1.0944263126208056,
      "grad_norm": 2.2859694957733154,
      "learning_rate": 1.9099915325994922e-05,
      "loss": 0.8567,
      "step": 17270
    },
    {
      "epoch": 1.095060046262556,
      "grad_norm": 2.610943078994751,
      "learning_rate": 1.9093564775613885e-05,
      "loss": 0.8443,
      "step": 17280
    },
    {
      "epoch": 1.0956937799043063,
      "grad_norm": 2.640033721923828,
      "learning_rate": 1.9087214225232855e-05,
      "loss": 0.8688,
      "step": 17290
    },
    {
      "epoch": 1.0963275135460566,
      "grad_norm": 2.6159114837646484,
      "learning_rate": 1.908086367485182e-05,
      "loss": 0.861,
      "step": 17300
    },
    {
      "epoch": 1.096961247187807,
      "grad_norm": 2.63381290435791,
      "learning_rate": 1.9074513124470788e-05,
      "loss": 0.895,
      "step": 17310
    },
    {
      "epoch": 1.0975949808295573,
      "grad_norm": 2.8299145698547363,
      "learning_rate": 1.9068162574089757e-05,
      "loss": 0.8968,
      "step": 17320
    },
    {
      "epoch": 1.0982287144713077,
      "grad_norm": 2.8214919567108154,
      "learning_rate": 1.9061812023708724e-05,
      "loss": 0.8579,
      "step": 17330
    },
    {
      "epoch": 1.098862448113058,
      "grad_norm": 2.4816360473632812,
      "learning_rate": 1.9055461473327687e-05,
      "loss": 0.8617,
      "step": 17340
    },
    {
      "epoch": 1.0994961817548083,
      "grad_norm": 3.0654609203338623,
      "learning_rate": 1.9049110922946657e-05,
      "loss": 0.8641,
      "step": 17350
    },
    {
      "epoch": 1.100129915396559,
      "grad_norm": 3.3654062747955322,
      "learning_rate": 1.9042760372565623e-05,
      "loss": 0.9047,
      "step": 17360
    },
    {
      "epoch": 1.1007636490383093,
      "grad_norm": 2.617316484451294,
      "learning_rate": 1.903640982218459e-05,
      "loss": 0.8278,
      "step": 17370
    },
    {
      "epoch": 1.1013973826800596,
      "grad_norm": 3.0085818767547607,
      "learning_rate": 1.903005927180356e-05,
      "loss": 0.8861,
      "step": 17380
    },
    {
      "epoch": 1.10203111632181,
      "grad_norm": 2.615704298019409,
      "learning_rate": 1.9023708721422522e-05,
      "loss": 0.8487,
      "step": 17390
    },
    {
      "epoch": 1.1026648499635603,
      "grad_norm": 2.5358738899230957,
      "learning_rate": 1.901735817104149e-05,
      "loss": 0.8623,
      "step": 17400
    },
    {
      "epoch": 1.1032985836053106,
      "grad_norm": 2.9513516426086426,
      "learning_rate": 1.901100762066046e-05,
      "loss": 0.869,
      "step": 17410
    },
    {
      "epoch": 1.103932317247061,
      "grad_norm": 2.8544461727142334,
      "learning_rate": 1.9004657070279425e-05,
      "loss": 0.8594,
      "step": 17420
    },
    {
      "epoch": 1.1045660508888115,
      "grad_norm": 2.830078363418579,
      "learning_rate": 1.899830651989839e-05,
      "loss": 0.8476,
      "step": 17430
    },
    {
      "epoch": 1.105199784530562,
      "grad_norm": 2.095210313796997,
      "learning_rate": 1.8991955969517358e-05,
      "loss": 0.8738,
      "step": 17440
    },
    {
      "epoch": 1.1058335181723122,
      "grad_norm": 2.9346487522125244,
      "learning_rate": 1.8985605419136324e-05,
      "loss": 0.8496,
      "step": 17450
    },
    {
      "epoch": 1.1064672518140626,
      "grad_norm": 2.7456178665161133,
      "learning_rate": 1.8979254868755294e-05,
      "loss": 0.8884,
      "step": 17460
    },
    {
      "epoch": 1.107100985455813,
      "grad_norm": 2.899567127227783,
      "learning_rate": 1.897290431837426e-05,
      "loss": 0.8688,
      "step": 17470
    },
    {
      "epoch": 1.1077347190975633,
      "grad_norm": 2.4779791831970215,
      "learning_rate": 1.8966553767993227e-05,
      "loss": 0.8244,
      "step": 17480
    },
    {
      "epoch": 1.1083684527393136,
      "grad_norm": 2.250269889831543,
      "learning_rate": 1.8960203217612193e-05,
      "loss": 0.8442,
      "step": 17490
    },
    {
      "epoch": 1.109002186381064,
      "grad_norm": 2.483499526977539,
      "learning_rate": 1.895385266723116e-05,
      "loss": 0.9028,
      "step": 17500
    },
    {
      "epoch": 1.1096359200228143,
      "grad_norm": 3.365054130554199,
      "learning_rate": 1.8947502116850126e-05,
      "loss": 0.8826,
      "step": 17510
    },
    {
      "epoch": 1.1102696536645649,
      "grad_norm": 2.7556393146514893,
      "learning_rate": 1.8941151566469096e-05,
      "loss": 0.8705,
      "step": 17520
    },
    {
      "epoch": 1.1109033873063152,
      "grad_norm": 2.4261972904205322,
      "learning_rate": 1.8934801016088062e-05,
      "loss": 0.8546,
      "step": 17530
    },
    {
      "epoch": 1.1115371209480656,
      "grad_norm": 2.9269227981567383,
      "learning_rate": 1.892845046570703e-05,
      "loss": 0.8993,
      "step": 17540
    },
    {
      "epoch": 1.112170854589816,
      "grad_norm": 2.3008553981781006,
      "learning_rate": 1.8922099915325995e-05,
      "loss": 0.8239,
      "step": 17550
    },
    {
      "epoch": 1.1128045882315662,
      "grad_norm": 2.977130174636841,
      "learning_rate": 1.8915749364944962e-05,
      "loss": 0.8348,
      "step": 17560
    },
    {
      "epoch": 1.1134383218733166,
      "grad_norm": 2.4005510807037354,
      "learning_rate": 1.8909398814563928e-05,
      "loss": 0.8744,
      "step": 17570
    },
    {
      "epoch": 1.114072055515067,
      "grad_norm": 2.5957932472229004,
      "learning_rate": 1.8903048264182898e-05,
      "loss": 0.7909,
      "step": 17580
    },
    {
      "epoch": 1.1147057891568175,
      "grad_norm": 2.59590482711792,
      "learning_rate": 1.8896697713801864e-05,
      "loss": 0.8296,
      "step": 17590
    },
    {
      "epoch": 1.1153395227985679,
      "grad_norm": 2.4396519660949707,
      "learning_rate": 1.8890347163420827e-05,
      "loss": 0.8998,
      "step": 17600
    },
    {
      "epoch": 1.1159732564403182,
      "grad_norm": 2.3339858055114746,
      "learning_rate": 1.8883996613039797e-05,
      "loss": 0.876,
      "step": 17610
    },
    {
      "epoch": 1.1166069900820685,
      "grad_norm": 2.3595640659332275,
      "learning_rate": 1.8877646062658764e-05,
      "loss": 0.8637,
      "step": 17620
    },
    {
      "epoch": 1.1172407237238189,
      "grad_norm": 2.4800257682800293,
      "learning_rate": 1.8871295512277734e-05,
      "loss": 0.878,
      "step": 17630
    },
    {
      "epoch": 1.1178744573655692,
      "grad_norm": 4.659736156463623,
      "learning_rate": 1.88649449618967e-05,
      "loss": 0.8618,
      "step": 17640
    },
    {
      "epoch": 1.1185081910073196,
      "grad_norm": 2.5303587913513184,
      "learning_rate": 1.8858594411515663e-05,
      "loss": 0.8523,
      "step": 17650
    },
    {
      "epoch": 1.11914192464907,
      "grad_norm": 3.7873098850250244,
      "learning_rate": 1.8852243861134633e-05,
      "loss": 0.8727,
      "step": 17660
    },
    {
      "epoch": 1.1197756582908203,
      "grad_norm": 2.362837553024292,
      "learning_rate": 1.88458933107536e-05,
      "loss": 0.8881,
      "step": 17670
    },
    {
      "epoch": 1.1204093919325708,
      "grad_norm": 2.5137884616851807,
      "learning_rate": 1.8839542760372566e-05,
      "loss": 0.8586,
      "step": 17680
    },
    {
      "epoch": 1.1210431255743212,
      "grad_norm": 2.771871328353882,
      "learning_rate": 1.8833192209991535e-05,
      "loss": 0.9025,
      "step": 17690
    },
    {
      "epoch": 1.1216768592160715,
      "grad_norm": 2.4338860511779785,
      "learning_rate": 1.88268416596105e-05,
      "loss": 0.8513,
      "step": 17700
    },
    {
      "epoch": 1.1223105928578219,
      "grad_norm": 2.5624940395355225,
      "learning_rate": 1.8820491109229465e-05,
      "loss": 0.923,
      "step": 17710
    },
    {
      "epoch": 1.1229443264995722,
      "grad_norm": 2.4999899864196777,
      "learning_rate": 1.8814140558848435e-05,
      "loss": 0.8552,
      "step": 17720
    },
    {
      "epoch": 1.1235780601413226,
      "grad_norm": 2.3875844478607178,
      "learning_rate": 1.88077900084674e-05,
      "loss": 0.8787,
      "step": 17730
    },
    {
      "epoch": 1.124211793783073,
      "grad_norm": 2.645240068435669,
      "learning_rate": 1.8801439458086368e-05,
      "loss": 0.8414,
      "step": 17740
    },
    {
      "epoch": 1.1248455274248232,
      "grad_norm": 2.405658006668091,
      "learning_rate": 1.8795088907705334e-05,
      "loss": 0.8855,
      "step": 17750
    },
    {
      "epoch": 1.1254792610665738,
      "grad_norm": 2.8534650802612305,
      "learning_rate": 1.87887383573243e-05,
      "loss": 0.8738,
      "step": 17760
    },
    {
      "epoch": 1.1261129947083242,
      "grad_norm": 2.568936586380005,
      "learning_rate": 1.878238780694327e-05,
      "loss": 0.8872,
      "step": 17770
    },
    {
      "epoch": 1.1267467283500745,
      "grad_norm": 2.5314316749572754,
      "learning_rate": 1.8776037256562237e-05,
      "loss": 0.8787,
      "step": 17780
    },
    {
      "epoch": 1.1273804619918248,
      "grad_norm": 2.6655330657958984,
      "learning_rate": 1.8769686706181203e-05,
      "loss": 0.8298,
      "step": 17790
    },
    {
      "epoch": 1.1280141956335752,
      "grad_norm": 2.4859158992767334,
      "learning_rate": 1.8763336155800173e-05,
      "loss": 0.8719,
      "step": 17800
    },
    {
      "epoch": 1.1286479292753255,
      "grad_norm": 2.723287343978882,
      "learning_rate": 1.8756985605419136e-05,
      "loss": 0.8384,
      "step": 17810
    },
    {
      "epoch": 1.1292816629170759,
      "grad_norm": 2.543680191040039,
      "learning_rate": 1.8750635055038102e-05,
      "loss": 0.8928,
      "step": 17820
    },
    {
      "epoch": 1.1299153965588262,
      "grad_norm": 2.2303476333618164,
      "learning_rate": 1.8744284504657072e-05,
      "loss": 0.8697,
      "step": 17830
    },
    {
      "epoch": 1.1305491302005768,
      "grad_norm": 2.1328420639038086,
      "learning_rate": 1.873793395427604e-05,
      "loss": 0.8659,
      "step": 17840
    },
    {
      "epoch": 1.1311828638423271,
      "grad_norm": 2.45936918258667,
      "learning_rate": 1.8731583403895005e-05,
      "loss": 0.9004,
      "step": 17850
    },
    {
      "epoch": 1.1318165974840775,
      "grad_norm": 2.335948944091797,
      "learning_rate": 1.872523285351397e-05,
      "loss": 0.8321,
      "step": 17860
    },
    {
      "epoch": 1.1324503311258278,
      "grad_norm": 2.426929235458374,
      "learning_rate": 1.8718882303132938e-05,
      "loss": 0.8695,
      "step": 17870
    },
    {
      "epoch": 1.1330840647675782,
      "grad_norm": 2.72454571723938,
      "learning_rate": 1.8712531752751904e-05,
      "loss": 0.8416,
      "step": 17880
    },
    {
      "epoch": 1.1337177984093285,
      "grad_norm": 2.3733153343200684,
      "learning_rate": 1.8706181202370874e-05,
      "loss": 0.8626,
      "step": 17890
    },
    {
      "epoch": 1.1343515320510789,
      "grad_norm": 2.70930552482605,
      "learning_rate": 1.869983065198984e-05,
      "loss": 0.8503,
      "step": 17900
    },
    {
      "epoch": 1.1349852656928294,
      "grad_norm": 3.1865384578704834,
      "learning_rate": 1.8693480101608804e-05,
      "loss": 0.8839,
      "step": 17910
    },
    {
      "epoch": 1.1356189993345798,
      "grad_norm": 2.699266195297241,
      "learning_rate": 1.8687129551227773e-05,
      "loss": 0.8389,
      "step": 17920
    },
    {
      "epoch": 1.1362527329763301,
      "grad_norm": 2.699690818786621,
      "learning_rate": 1.868077900084674e-05,
      "loss": 0.8942,
      "step": 17930
    },
    {
      "epoch": 1.1368864666180805,
      "grad_norm": 2.7523608207702637,
      "learning_rate": 1.867442845046571e-05,
      "loss": 0.8774,
      "step": 17940
    },
    {
      "epoch": 1.1375202002598308,
      "grad_norm": 3.055050849914551,
      "learning_rate": 1.8668077900084676e-05,
      "loss": 0.8615,
      "step": 17950
    },
    {
      "epoch": 1.1381539339015811,
      "grad_norm": 2.3714544773101807,
      "learning_rate": 1.866172734970364e-05,
      "loss": 0.8738,
      "step": 17960
    },
    {
      "epoch": 1.1387876675433315,
      "grad_norm": 2.3058266639709473,
      "learning_rate": 1.865537679932261e-05,
      "loss": 0.8601,
      "step": 17970
    },
    {
      "epoch": 1.1394214011850818,
      "grad_norm": 2.1457679271698,
      "learning_rate": 1.8649026248941575e-05,
      "loss": 0.8924,
      "step": 17980
    },
    {
      "epoch": 1.1400551348268322,
      "grad_norm": 2.444162368774414,
      "learning_rate": 1.8642675698560542e-05,
      "loss": 0.8755,
      "step": 17990
    },
    {
      "epoch": 1.1406888684685828,
      "grad_norm": 2.3703927993774414,
      "learning_rate": 1.863632514817951e-05,
      "loss": 0.855,
      "step": 18000
    },
    {
      "epoch": 1.141322602110333,
      "grad_norm": 2.5366714000701904,
      "learning_rate": 1.8629974597798475e-05,
      "loss": 0.7946,
      "step": 18010
    },
    {
      "epoch": 1.1419563357520834,
      "grad_norm": 2.656322717666626,
      "learning_rate": 1.862362404741744e-05,
      "loss": 0.8101,
      "step": 18020
    },
    {
      "epoch": 1.1425900693938338,
      "grad_norm": 2.6214046478271484,
      "learning_rate": 1.861727349703641e-05,
      "loss": 0.845,
      "step": 18030
    },
    {
      "epoch": 1.1432238030355841,
      "grad_norm": 3.234645366668701,
      "learning_rate": 1.8610922946655377e-05,
      "loss": 0.8845,
      "step": 18040
    },
    {
      "epoch": 1.1438575366773345,
      "grad_norm": 2.7266387939453125,
      "learning_rate": 1.8604572396274344e-05,
      "loss": 0.8866,
      "step": 18050
    },
    {
      "epoch": 1.1444912703190848,
      "grad_norm": 2.2832651138305664,
      "learning_rate": 1.8598221845893314e-05,
      "loss": 0.8188,
      "step": 18060
    },
    {
      "epoch": 1.1451250039608352,
      "grad_norm": 3.2522759437561035,
      "learning_rate": 1.8591871295512277e-05,
      "loss": 0.8581,
      "step": 18070
    },
    {
      "epoch": 1.1457587376025855,
      "grad_norm": 2.437307357788086,
      "learning_rate": 1.8585520745131246e-05,
      "loss": 0.8526,
      "step": 18080
    },
    {
      "epoch": 1.146392471244336,
      "grad_norm": 2.4684746265411377,
      "learning_rate": 1.8579170194750213e-05,
      "loss": 0.8864,
      "step": 18090
    },
    {
      "epoch": 1.1470262048860864,
      "grad_norm": 2.291137218475342,
      "learning_rate": 1.857281964436918e-05,
      "loss": 0.8719,
      "step": 18100
    },
    {
      "epoch": 1.1476599385278368,
      "grad_norm": 2.3542230129241943,
      "learning_rate": 1.856646909398815e-05,
      "loss": 0.8472,
      "step": 18110
    },
    {
      "epoch": 1.148293672169587,
      "grad_norm": 3.056562662124634,
      "learning_rate": 1.8560118543607112e-05,
      "loss": 0.8739,
      "step": 18120
    },
    {
      "epoch": 1.1489274058113375,
      "grad_norm": 3.0200486183166504,
      "learning_rate": 1.855376799322608e-05,
      "loss": 0.8838,
      "step": 18130
    },
    {
      "epoch": 1.1495611394530878,
      "grad_norm": 2.37481427192688,
      "learning_rate": 1.854741744284505e-05,
      "loss": 0.8693,
      "step": 18140
    },
    {
      "epoch": 1.1501948730948381,
      "grad_norm": 3.048964500427246,
      "learning_rate": 1.8541066892464015e-05,
      "loss": 0.8991,
      "step": 18150
    },
    {
      "epoch": 1.1508286067365887,
      "grad_norm": 2.666844606399536,
      "learning_rate": 1.853471634208298e-05,
      "loss": 0.8688,
      "step": 18160
    },
    {
      "epoch": 1.151462340378339,
      "grad_norm": 2.5950427055358887,
      "learning_rate": 1.8528365791701948e-05,
      "loss": 0.8863,
      "step": 18170
    },
    {
      "epoch": 1.1520960740200894,
      "grad_norm": 2.752488374710083,
      "learning_rate": 1.8522015241320914e-05,
      "loss": 0.895,
      "step": 18180
    },
    {
      "epoch": 1.1527298076618397,
      "grad_norm": 2.6877801418304443,
      "learning_rate": 1.851566469093988e-05,
      "loss": 0.8681,
      "step": 18190
    },
    {
      "epoch": 1.15336354130359,
      "grad_norm": 2.6341071128845215,
      "learning_rate": 1.850931414055885e-05,
      "loss": 0.862,
      "step": 18200
    },
    {
      "epoch": 1.1539972749453404,
      "grad_norm": 2.7276620864868164,
      "learning_rate": 1.8502963590177817e-05,
      "loss": 0.8659,
      "step": 18210
    },
    {
      "epoch": 1.1546310085870908,
      "grad_norm": 2.4548943042755127,
      "learning_rate": 1.849661303979678e-05,
      "loss": 0.861,
      "step": 18220
    },
    {
      "epoch": 1.1552647422288411,
      "grad_norm": 2.392803907394409,
      "learning_rate": 1.849026248941575e-05,
      "loss": 0.8751,
      "step": 18230
    },
    {
      "epoch": 1.1558984758705915,
      "grad_norm": 2.2100913524627686,
      "learning_rate": 1.8483911939034716e-05,
      "loss": 0.8533,
      "step": 18240
    },
    {
      "epoch": 1.156532209512342,
      "grad_norm": 2.607121229171753,
      "learning_rate": 1.8477561388653686e-05,
      "loss": 0.8602,
      "step": 18250
    },
    {
      "epoch": 1.1571659431540924,
      "grad_norm": 2.2736573219299316,
      "learning_rate": 1.8471210838272652e-05,
      "loss": 0.8894,
      "step": 18260
    },
    {
      "epoch": 1.1577996767958427,
      "grad_norm": 2.547621250152588,
      "learning_rate": 1.8464860287891615e-05,
      "loss": 0.872,
      "step": 18270
    },
    {
      "epoch": 1.158433410437593,
      "grad_norm": 2.9186513423919678,
      "learning_rate": 1.8458509737510585e-05,
      "loss": 0.8624,
      "step": 18280
    },
    {
      "epoch": 1.1590671440793434,
      "grad_norm": 2.766247272491455,
      "learning_rate": 1.845215918712955e-05,
      "loss": 0.8336,
      "step": 18290
    },
    {
      "epoch": 1.1597008777210938,
      "grad_norm": 2.4727249145507812,
      "learning_rate": 1.8445808636748518e-05,
      "loss": 0.8673,
      "step": 18300
    },
    {
      "epoch": 1.160334611362844,
      "grad_norm": 2.7792086601257324,
      "learning_rate": 1.8439458086367488e-05,
      "loss": 0.8626,
      "step": 18310
    },
    {
      "epoch": 1.1609683450045947,
      "grad_norm": 2.3729028701782227,
      "learning_rate": 1.8433107535986454e-05,
      "loss": 0.8492,
      "step": 18320
    },
    {
      "epoch": 1.161602078646345,
      "grad_norm": 2.907005548477173,
      "learning_rate": 1.8426756985605417e-05,
      "loss": 0.9118,
      "step": 18330
    },
    {
      "epoch": 1.1622358122880954,
      "grad_norm": 2.5055034160614014,
      "learning_rate": 1.8420406435224387e-05,
      "loss": 0.845,
      "step": 18340
    },
    {
      "epoch": 1.1628695459298457,
      "grad_norm": 2.845799446105957,
      "learning_rate": 1.8414055884843354e-05,
      "loss": 0.8818,
      "step": 18350
    },
    {
      "epoch": 1.163503279571596,
      "grad_norm": 2.5087201595306396,
      "learning_rate": 1.840770533446232e-05,
      "loss": 0.8862,
      "step": 18360
    },
    {
      "epoch": 1.1641370132133464,
      "grad_norm": 2.717787265777588,
      "learning_rate": 1.840135478408129e-05,
      "loss": 0.8808,
      "step": 18370
    },
    {
      "epoch": 1.1647707468550967,
      "grad_norm": 2.492065668106079,
      "learning_rate": 1.8395004233700253e-05,
      "loss": 0.8499,
      "step": 18380
    },
    {
      "epoch": 1.165404480496847,
      "grad_norm": 2.311854124069214,
      "learning_rate": 1.8388653683319223e-05,
      "loss": 0.8741,
      "step": 18390
    },
    {
      "epoch": 1.1660382141385974,
      "grad_norm": 3.047236919403076,
      "learning_rate": 1.838230313293819e-05,
      "loss": 0.9209,
      "step": 18400
    },
    {
      "epoch": 1.166671947780348,
      "grad_norm": 2.7811098098754883,
      "learning_rate": 1.8375952582557155e-05,
      "loss": 0.8657,
      "step": 18410
    },
    {
      "epoch": 1.1673056814220983,
      "grad_norm": 2.44421124458313,
      "learning_rate": 1.8369602032176125e-05,
      "loss": 0.8646,
      "step": 18420
    },
    {
      "epoch": 1.1679394150638487,
      "grad_norm": 2.5213558673858643,
      "learning_rate": 1.836325148179509e-05,
      "loss": 0.8951,
      "step": 18430
    },
    {
      "epoch": 1.168573148705599,
      "grad_norm": 2.61077880859375,
      "learning_rate": 1.8356900931414055e-05,
      "loss": 0.8831,
      "step": 18440
    },
    {
      "epoch": 1.1692068823473494,
      "grad_norm": 2.523026943206787,
      "learning_rate": 1.8350550381033025e-05,
      "loss": 0.8529,
      "step": 18450
    },
    {
      "epoch": 1.1698406159890997,
      "grad_norm": 3.1554296016693115,
      "learning_rate": 1.834419983065199e-05,
      "loss": 0.8701,
      "step": 18460
    },
    {
      "epoch": 1.17047434963085,
      "grad_norm": 2.853083372116089,
      "learning_rate": 1.8337849280270957e-05,
      "loss": 0.8643,
      "step": 18470
    },
    {
      "epoch": 1.1711080832726006,
      "grad_norm": 2.527650833129883,
      "learning_rate": 1.8331498729889924e-05,
      "loss": 0.8746,
      "step": 18480
    },
    {
      "epoch": 1.171741816914351,
      "grad_norm": 2.6613471508026123,
      "learning_rate": 1.832514817950889e-05,
      "loss": 0.8367,
      "step": 18490
    },
    {
      "epoch": 1.1723755505561013,
      "grad_norm": 2.8140931129455566,
      "learning_rate": 1.8318797629127857e-05,
      "loss": 0.8474,
      "step": 18500
    },
    {
      "epoch": 1.1730092841978517,
      "grad_norm": 2.222433090209961,
      "learning_rate": 1.8312447078746827e-05,
      "loss": 0.8982,
      "step": 18510
    },
    {
      "epoch": 1.173643017839602,
      "grad_norm": 2.2239267826080322,
      "learning_rate": 1.8306096528365793e-05,
      "loss": 0.8564,
      "step": 18520
    },
    {
      "epoch": 1.1742767514813524,
      "grad_norm": 2.5317671298980713,
      "learning_rate": 1.8299745977984756e-05,
      "loss": 0.8284,
      "step": 18530
    },
    {
      "epoch": 1.1749104851231027,
      "grad_norm": 2.385650396347046,
      "learning_rate": 1.8293395427603726e-05,
      "loss": 0.8622,
      "step": 18540
    },
    {
      "epoch": 1.175544218764853,
      "grad_norm": 2.9182369709014893,
      "learning_rate": 1.8287044877222692e-05,
      "loss": 0.8282,
      "step": 18550
    },
    {
      "epoch": 1.1761779524066034,
      "grad_norm": 3.580153226852417,
      "learning_rate": 1.8280694326841662e-05,
      "loss": 0.8865,
      "step": 18560
    },
    {
      "epoch": 1.176811686048354,
      "grad_norm": 2.499303102493286,
      "learning_rate": 1.827434377646063e-05,
      "loss": 0.8873,
      "step": 18570
    },
    {
      "epoch": 1.1774454196901043,
      "grad_norm": 2.4159178733825684,
      "learning_rate": 1.8267993226079595e-05,
      "loss": 0.8416,
      "step": 18580
    },
    {
      "epoch": 1.1780791533318546,
      "grad_norm": 2.4174983501434326,
      "learning_rate": 1.826164267569856e-05,
      "loss": 0.8594,
      "step": 18590
    },
    {
      "epoch": 1.178712886973605,
      "grad_norm": 2.5623233318328857,
      "learning_rate": 1.8255292125317528e-05,
      "loss": 0.9205,
      "step": 18600
    },
    {
      "epoch": 1.1793466206153553,
      "grad_norm": 2.479419231414795,
      "learning_rate": 1.8248941574936494e-05,
      "loss": 0.8385,
      "step": 18610
    },
    {
      "epoch": 1.1799803542571057,
      "grad_norm": 2.537736654281616,
      "learning_rate": 1.8242591024555464e-05,
      "loss": 0.8954,
      "step": 18620
    },
    {
      "epoch": 1.180614087898856,
      "grad_norm": 2.4303505420684814,
      "learning_rate": 1.823624047417443e-05,
      "loss": 0.8612,
      "step": 18630
    },
    {
      "epoch": 1.1812478215406066,
      "grad_norm": 2.50948166847229,
      "learning_rate": 1.8229889923793393e-05,
      "loss": 0.8438,
      "step": 18640
    },
    {
      "epoch": 1.181881555182357,
      "grad_norm": 2.9234073162078857,
      "learning_rate": 1.8223539373412363e-05,
      "loss": 0.9025,
      "step": 18650
    },
    {
      "epoch": 1.1825152888241073,
      "grad_norm": 2.2032649517059326,
      "learning_rate": 1.821718882303133e-05,
      "loss": 0.8344,
      "step": 18660
    },
    {
      "epoch": 1.1831490224658576,
      "grad_norm": 2.774205207824707,
      "learning_rate": 1.8210838272650296e-05,
      "loss": 0.883,
      "step": 18670
    },
    {
      "epoch": 1.183782756107608,
      "grad_norm": 3.1105313301086426,
      "learning_rate": 1.8204487722269266e-05,
      "loss": 0.8419,
      "step": 18680
    },
    {
      "epoch": 1.1844164897493583,
      "grad_norm": 2.410946846008301,
      "learning_rate": 1.819813717188823e-05,
      "loss": 0.8581,
      "step": 18690
    },
    {
      "epoch": 1.1850502233911087,
      "grad_norm": 2.5972211360931396,
      "learning_rate": 1.8191786621507195e-05,
      "loss": 0.856,
      "step": 18700
    },
    {
      "epoch": 1.185683957032859,
      "grad_norm": 2.7574217319488525,
      "learning_rate": 1.8185436071126165e-05,
      "loss": 0.7993,
      "step": 18710
    },
    {
      "epoch": 1.1863176906746093,
      "grad_norm": 3.0562000274658203,
      "learning_rate": 1.817908552074513e-05,
      "loss": 0.8808,
      "step": 18720
    },
    {
      "epoch": 1.18695142431636,
      "grad_norm": 2.4858558177948,
      "learning_rate": 1.81727349703641e-05,
      "loss": 0.844,
      "step": 18730
    },
    {
      "epoch": 1.1875851579581103,
      "grad_norm": 2.396918535232544,
      "learning_rate": 1.8166384419983064e-05,
      "loss": 0.8742,
      "step": 18740
    },
    {
      "epoch": 1.1882188915998606,
      "grad_norm": 2.8653416633605957,
      "learning_rate": 1.816003386960203e-05,
      "loss": 0.8815,
      "step": 18750
    },
    {
      "epoch": 1.188852625241611,
      "grad_norm": 2.545725107192993,
      "learning_rate": 1.8153683319221e-05,
      "loss": 0.8743,
      "step": 18760
    },
    {
      "epoch": 1.1894863588833613,
      "grad_norm": 2.656344413757324,
      "learning_rate": 1.8147332768839967e-05,
      "loss": 0.8865,
      "step": 18770
    },
    {
      "epoch": 1.1901200925251116,
      "grad_norm": 2.2303168773651123,
      "learning_rate": 1.8140982218458934e-05,
      "loss": 0.8238,
      "step": 18780
    },
    {
      "epoch": 1.190753826166862,
      "grad_norm": 2.598201036453247,
      "learning_rate": 1.81346316680779e-05,
      "loss": 0.8706,
      "step": 18790
    },
    {
      "epoch": 1.1913875598086126,
      "grad_norm": 2.802952527999878,
      "learning_rate": 1.8128281117696866e-05,
      "loss": 0.8196,
      "step": 18800
    },
    {
      "epoch": 1.192021293450363,
      "grad_norm": 2.286618947982788,
      "learning_rate": 1.8121930567315833e-05,
      "loss": 0.8638,
      "step": 18810
    },
    {
      "epoch": 1.1926550270921132,
      "grad_norm": 2.356069803237915,
      "learning_rate": 1.8115580016934803e-05,
      "loss": 0.8728,
      "step": 18820
    },
    {
      "epoch": 1.1932887607338636,
      "grad_norm": 2.772132396697998,
      "learning_rate": 1.810922946655377e-05,
      "loss": 0.9296,
      "step": 18830
    },
    {
      "epoch": 1.193922494375614,
      "grad_norm": 2.8301737308502197,
      "learning_rate": 1.8102878916172736e-05,
      "loss": 0.8756,
      "step": 18840
    },
    {
      "epoch": 1.1945562280173643,
      "grad_norm": 2.7264602184295654,
      "learning_rate": 1.8096528365791702e-05,
      "loss": 0.8366,
      "step": 18850
    },
    {
      "epoch": 1.1951899616591146,
      "grad_norm": 2.452688694000244,
      "learning_rate": 1.809017781541067e-05,
      "loss": 0.8968,
      "step": 18860
    },
    {
      "epoch": 1.195823695300865,
      "grad_norm": 2.4887006282806396,
      "learning_rate": 1.8083827265029638e-05,
      "loss": 0.8893,
      "step": 18870
    },
    {
      "epoch": 1.1964574289426153,
      "grad_norm": 2.6966357231140137,
      "learning_rate": 1.8077476714648605e-05,
      "loss": 0.889,
      "step": 18880
    },
    {
      "epoch": 1.1970911625843659,
      "grad_norm": 2.2980384826660156,
      "learning_rate": 1.807112616426757e-05,
      "loss": 0.818,
      "step": 18890
    },
    {
      "epoch": 1.1977248962261162,
      "grad_norm": 2.461839199066162,
      "learning_rate": 1.8064775613886537e-05,
      "loss": 0.8989,
      "step": 18900
    },
    {
      "epoch": 1.1983586298678666,
      "grad_norm": 2.584430694580078,
      "learning_rate": 1.8058425063505504e-05,
      "loss": 0.8491,
      "step": 18910
    },
    {
      "epoch": 1.198992363509617,
      "grad_norm": 2.372666120529175,
      "learning_rate": 1.805207451312447e-05,
      "loss": 0.8246,
      "step": 18920
    },
    {
      "epoch": 1.1996260971513673,
      "grad_norm": 2.7155864238739014,
      "learning_rate": 1.804572396274344e-05,
      "loss": 0.8413,
      "step": 18930
    },
    {
      "epoch": 1.2002598307931176,
      "grad_norm": 2.3331520557403564,
      "learning_rate": 1.8039373412362407e-05,
      "loss": 0.8642,
      "step": 18940
    },
    {
      "epoch": 1.200893564434868,
      "grad_norm": 2.748534917831421,
      "learning_rate": 1.803302286198137e-05,
      "loss": 0.8454,
      "step": 18950
    },
    {
      "epoch": 1.2015272980766185,
      "grad_norm": 2.2938926219940186,
      "learning_rate": 1.802667231160034e-05,
      "loss": 0.8719,
      "step": 18960
    },
    {
      "epoch": 1.2021610317183689,
      "grad_norm": 2.6753833293914795,
      "learning_rate": 1.8020321761219306e-05,
      "loss": 0.8716,
      "step": 18970
    },
    {
      "epoch": 1.2027947653601192,
      "grad_norm": 2.7195987701416016,
      "learning_rate": 1.8013971210838272e-05,
      "loss": 0.8661,
      "step": 18980
    },
    {
      "epoch": 1.2034284990018695,
      "grad_norm": 2.635490894317627,
      "learning_rate": 1.8007620660457242e-05,
      "loss": 0.8441,
      "step": 18990
    },
    {
      "epoch": 1.20406223264362,
      "grad_norm": 2.4267468452453613,
      "learning_rate": 1.8001270110076205e-05,
      "loss": 0.8999,
      "step": 19000
    },
    {
      "epoch": 1.2046959662853702,
      "grad_norm": 2.692415952682495,
      "learning_rate": 1.799491955969517e-05,
      "loss": 0.8466,
      "step": 19010
    },
    {
      "epoch": 1.2053296999271206,
      "grad_norm": 2.4399101734161377,
      "learning_rate": 1.798856900931414e-05,
      "loss": 0.8745,
      "step": 19020
    },
    {
      "epoch": 1.205963433568871,
      "grad_norm": 2.366842269897461,
      "learning_rate": 1.7982218458933108e-05,
      "loss": 0.8616,
      "step": 19030
    },
    {
      "epoch": 1.2065971672106213,
      "grad_norm": 2.4201011657714844,
      "learning_rate": 1.7975867908552078e-05,
      "loss": 0.8747,
      "step": 19040
    },
    {
      "epoch": 1.2072309008523718,
      "grad_norm": 2.8784778118133545,
      "learning_rate": 1.796951735817104e-05,
      "loss": 0.8929,
      "step": 19050
    },
    {
      "epoch": 1.2078646344941222,
      "grad_norm": 2.0972044467926025,
      "learning_rate": 1.7963166807790007e-05,
      "loss": 0.8084,
      "step": 19060
    },
    {
      "epoch": 1.2084983681358725,
      "grad_norm": 2.4824321269989014,
      "learning_rate": 1.7956816257408977e-05,
      "loss": 0.8361,
      "step": 19070
    },
    {
      "epoch": 1.2091321017776229,
      "grad_norm": 2.326503038406372,
      "learning_rate": 1.7950465707027943e-05,
      "loss": 0.8469,
      "step": 19080
    },
    {
      "epoch": 1.2097658354193732,
      "grad_norm": 2.4767253398895264,
      "learning_rate": 1.794411515664691e-05,
      "loss": 0.8738,
      "step": 19090
    },
    {
      "epoch": 1.2103995690611236,
      "grad_norm": 2.330855369567871,
      "learning_rate": 1.793776460626588e-05,
      "loss": 0.8826,
      "step": 19100
    },
    {
      "epoch": 1.211033302702874,
      "grad_norm": 2.754472494125366,
      "learning_rate": 1.7931414055884843e-05,
      "loss": 0.8368,
      "step": 19110
    },
    {
      "epoch": 1.2116670363446245,
      "grad_norm": 2.4164650440216064,
      "learning_rate": 1.792506350550381e-05,
      "loss": 0.8833,
      "step": 19120
    },
    {
      "epoch": 1.2123007699863748,
      "grad_norm": 2.638354539871216,
      "learning_rate": 1.791871295512278e-05,
      "loss": 0.8531,
      "step": 19130
    },
    {
      "epoch": 1.2129345036281252,
      "grad_norm": 2.700751543045044,
      "learning_rate": 1.7912362404741745e-05,
      "loss": 0.8612,
      "step": 19140
    },
    {
      "epoch": 1.2135682372698755,
      "grad_norm": 2.4098851680755615,
      "learning_rate": 1.7906011854360712e-05,
      "loss": 0.888,
      "step": 19150
    },
    {
      "epoch": 1.2142019709116258,
      "grad_norm": 2.4999594688415527,
      "learning_rate": 1.7899661303979678e-05,
      "loss": 0.8422,
      "step": 19160
    },
    {
      "epoch": 1.2148357045533762,
      "grad_norm": 2.9618008136749268,
      "learning_rate": 1.7893310753598645e-05,
      "loss": 0.8286,
      "step": 19170
    },
    {
      "epoch": 1.2154694381951265,
      "grad_norm": 2.5358729362487793,
      "learning_rate": 1.7886960203217614e-05,
      "loss": 0.8976,
      "step": 19180
    },
    {
      "epoch": 1.2161031718368769,
      "grad_norm": 2.493696928024292,
      "learning_rate": 1.788060965283658e-05,
      "loss": 0.8586,
      "step": 19190
    },
    {
      "epoch": 1.2167369054786272,
      "grad_norm": 2.168745994567871,
      "learning_rate": 1.7874259102455547e-05,
      "loss": 0.8298,
      "step": 19200
    },
    {
      "epoch": 1.2173706391203778,
      "grad_norm": 2.4364912509918213,
      "learning_rate": 1.7867908552074514e-05,
      "loss": 0.8388,
      "step": 19210
    },
    {
      "epoch": 1.2180043727621281,
      "grad_norm": 2.614840030670166,
      "learning_rate": 1.786155800169348e-05,
      "loss": 0.8327,
      "step": 19220
    },
    {
      "epoch": 1.2186381064038785,
      "grad_norm": 2.704793691635132,
      "learning_rate": 1.7855207451312447e-05,
      "loss": 0.8545,
      "step": 19230
    },
    {
      "epoch": 1.2192718400456288,
      "grad_norm": 2.476407527923584,
      "learning_rate": 1.7848856900931416e-05,
      "loss": 0.8844,
      "step": 19240
    },
    {
      "epoch": 1.2199055736873792,
      "grad_norm": 2.335726022720337,
      "learning_rate": 1.7842506350550383e-05,
      "loss": 0.8917,
      "step": 19250
    },
    {
      "epoch": 1.2205393073291295,
      "grad_norm": 2.5639634132385254,
      "learning_rate": 1.7836155800169346e-05,
      "loss": 0.8543,
      "step": 19260
    },
    {
      "epoch": 1.2211730409708799,
      "grad_norm": 3.132814884185791,
      "learning_rate": 1.7829805249788316e-05,
      "loss": 0.858,
      "step": 19270
    },
    {
      "epoch": 1.2218067746126304,
      "grad_norm": 2.1783769130706787,
      "learning_rate": 1.7823454699407282e-05,
      "loss": 0.8195,
      "step": 19280
    },
    {
      "epoch": 1.2224405082543808,
      "grad_norm": 2.4934279918670654,
      "learning_rate": 1.781710414902625e-05,
      "loss": 0.897,
      "step": 19290
    },
    {
      "epoch": 1.2230742418961311,
      "grad_norm": 2.341268539428711,
      "learning_rate": 1.7810753598645218e-05,
      "loss": 0.8767,
      "step": 19300
    },
    {
      "epoch": 1.2237079755378815,
      "grad_norm": 3.262310743331909,
      "learning_rate": 1.780440304826418e-05,
      "loss": 0.8629,
      "step": 19310
    },
    {
      "epoch": 1.2243417091796318,
      "grad_norm": 2.879579544067383,
      "learning_rate": 1.7798052497883148e-05,
      "loss": 0.8591,
      "step": 19320
    },
    {
      "epoch": 1.2249754428213822,
      "grad_norm": 2.945220947265625,
      "learning_rate": 1.7791701947502118e-05,
      "loss": 0.8492,
      "step": 19330
    },
    {
      "epoch": 1.2256091764631325,
      "grad_norm": 2.9268763065338135,
      "learning_rate": 1.7785351397121084e-05,
      "loss": 0.8637,
      "step": 19340
    },
    {
      "epoch": 1.2262429101048828,
      "grad_norm": 2.4636662006378174,
      "learning_rate": 1.7779000846740054e-05,
      "loss": 0.8933,
      "step": 19350
    },
    {
      "epoch": 1.2268766437466332,
      "grad_norm": 2.84053635597229,
      "learning_rate": 1.777265029635902e-05,
      "loss": 0.8631,
      "step": 19360
    },
    {
      "epoch": 1.2275103773883838,
      "grad_norm": 2.8972620964050293,
      "learning_rate": 1.7766299745977983e-05,
      "loss": 0.8683,
      "step": 19370
    },
    {
      "epoch": 1.228144111030134,
      "grad_norm": 2.505204200744629,
      "learning_rate": 1.7759949195596953e-05,
      "loss": 0.8709,
      "step": 19380
    },
    {
      "epoch": 1.2287778446718844,
      "grad_norm": 2.2748324871063232,
      "learning_rate": 1.775359864521592e-05,
      "loss": 0.8645,
      "step": 19390
    },
    {
      "epoch": 1.2294115783136348,
      "grad_norm": 2.492271661758423,
      "learning_rate": 1.7747248094834886e-05,
      "loss": 0.856,
      "step": 19400
    },
    {
      "epoch": 1.2300453119553851,
      "grad_norm": 2.6619858741760254,
      "learning_rate": 1.7740897544453856e-05,
      "loss": 0.8604,
      "step": 19410
    },
    {
      "epoch": 1.2306790455971355,
      "grad_norm": 2.4451959133148193,
      "learning_rate": 1.773454699407282e-05,
      "loss": 0.8308,
      "step": 19420
    },
    {
      "epoch": 1.2313127792388858,
      "grad_norm": 2.5033793449401855,
      "learning_rate": 1.7728196443691785e-05,
      "loss": 0.846,
      "step": 19430
    },
    {
      "epoch": 1.2319465128806364,
      "grad_norm": 2.403775930404663,
      "learning_rate": 1.7721845893310755e-05,
      "loss": 0.877,
      "step": 19440
    },
    {
      "epoch": 1.2325802465223867,
      "grad_norm": 2.510782480239868,
      "learning_rate": 1.771549534292972e-05,
      "loss": 0.8466,
      "step": 19450
    },
    {
      "epoch": 1.233213980164137,
      "grad_norm": 2.730201005935669,
      "learning_rate": 1.7709144792548688e-05,
      "loss": 0.8663,
      "step": 19460
    },
    {
      "epoch": 1.2338477138058874,
      "grad_norm": 2.220729112625122,
      "learning_rate": 1.7702794242167654e-05,
      "loss": 0.8829,
      "step": 19470
    },
    {
      "epoch": 1.2344814474476378,
      "grad_norm": 3.053874969482422,
      "learning_rate": 1.769644369178662e-05,
      "loss": 0.8896,
      "step": 19480
    },
    {
      "epoch": 1.2351151810893881,
      "grad_norm": 2.343736410140991,
      "learning_rate": 1.769009314140559e-05,
      "loss": 0.8659,
      "step": 19490
    },
    {
      "epoch": 1.2357489147311385,
      "grad_norm": 2.540447473526001,
      "learning_rate": 1.7683742591024557e-05,
      "loss": 0.8885,
      "step": 19500
    },
    {
      "epoch": 1.2363826483728888,
      "grad_norm": 2.3777358531951904,
      "learning_rate": 1.7677392040643523e-05,
      "loss": 0.8547,
      "step": 19510
    },
    {
      "epoch": 1.2370163820146391,
      "grad_norm": 2.6682708263397217,
      "learning_rate": 1.767104149026249e-05,
      "loss": 0.8879,
      "step": 19520
    },
    {
      "epoch": 1.2376501156563897,
      "grad_norm": 2.0924692153930664,
      "learning_rate": 1.7664690939881456e-05,
      "loss": 0.852,
      "step": 19530
    },
    {
      "epoch": 1.23828384929814,
      "grad_norm": 2.769099712371826,
      "learning_rate": 1.7658340389500423e-05,
      "loss": 0.842,
      "step": 19540
    },
    {
      "epoch": 1.2389175829398904,
      "grad_norm": 2.563002109527588,
      "learning_rate": 1.7652624894157493e-05,
      "loss": 0.8723,
      "step": 19550
    },
    {
      "epoch": 1.2395513165816407,
      "grad_norm": 2.5230326652526855,
      "learning_rate": 1.7646274343776463e-05,
      "loss": 0.8572,
      "step": 19560
    },
    {
      "epoch": 1.240185050223391,
      "grad_norm": 2.6042308807373047,
      "learning_rate": 1.763992379339543e-05,
      "loss": 0.8748,
      "step": 19570
    },
    {
      "epoch": 1.2408187838651414,
      "grad_norm": 2.3688852787017822,
      "learning_rate": 1.7633573243014392e-05,
      "loss": 0.8538,
      "step": 19580
    },
    {
      "epoch": 1.2414525175068918,
      "grad_norm": 2.9757747650146484,
      "learning_rate": 1.7627222692633362e-05,
      "loss": 0.8348,
      "step": 19590
    },
    {
      "epoch": 1.2420862511486421,
      "grad_norm": 3.4173779487609863,
      "learning_rate": 1.762087214225233e-05,
      "loss": 0.8913,
      "step": 19600
    },
    {
      "epoch": 1.2427199847903925,
      "grad_norm": 2.5835812091827393,
      "learning_rate": 1.7614521591871295e-05,
      "loss": 0.8594,
      "step": 19610
    },
    {
      "epoch": 1.243353718432143,
      "grad_norm": 2.8935370445251465,
      "learning_rate": 1.7608171041490265e-05,
      "loss": 0.886,
      "step": 19620
    },
    {
      "epoch": 1.2439874520738934,
      "grad_norm": 2.8185927867889404,
      "learning_rate": 1.7601820491109228e-05,
      "loss": 0.8614,
      "step": 19630
    },
    {
      "epoch": 1.2446211857156437,
      "grad_norm": 2.5169918537139893,
      "learning_rate": 1.7595469940728194e-05,
      "loss": 0.8285,
      "step": 19640
    },
    {
      "epoch": 1.245254919357394,
      "grad_norm": 2.184476613998413,
      "learning_rate": 1.7589119390347164e-05,
      "loss": 0.8211,
      "step": 19650
    },
    {
      "epoch": 1.2458886529991444,
      "grad_norm": 2.840291738510132,
      "learning_rate": 1.758276883996613e-05,
      "loss": 0.82,
      "step": 19660
    },
    {
      "epoch": 1.2465223866408948,
      "grad_norm": 3.082122802734375,
      "learning_rate": 1.75764182895851e-05,
      "loss": 0.8907,
      "step": 19670
    },
    {
      "epoch": 1.247156120282645,
      "grad_norm": 2.6500611305236816,
      "learning_rate": 1.7570067739204063e-05,
      "loss": 0.8777,
      "step": 19680
    },
    {
      "epoch": 1.2477898539243957,
      "grad_norm": 2.8943047523498535,
      "learning_rate": 1.756371718882303e-05,
      "loss": 0.8549,
      "step": 19690
    },
    {
      "epoch": 1.248423587566146,
      "grad_norm": 2.8200085163116455,
      "learning_rate": 1.7557366638442e-05,
      "loss": 0.8571,
      "step": 19700
    },
    {
      "epoch": 1.2490573212078964,
      "grad_norm": 2.1566972732543945,
      "learning_rate": 1.7551016088060966e-05,
      "loss": 0.8536,
      "step": 19710
    },
    {
      "epoch": 1.2496910548496467,
      "grad_norm": 2.3569869995117188,
      "learning_rate": 1.7544665537679933e-05,
      "loss": 0.8931,
      "step": 19720
    },
    {
      "epoch": 1.250324788491397,
      "grad_norm": 3.0102884769439697,
      "learning_rate": 1.7538314987298902e-05,
      "loss": 0.8926,
      "step": 19730
    },
    {
      "epoch": 1.2509585221331474,
      "grad_norm": 2.519939422607422,
      "learning_rate": 1.7531964436917865e-05,
      "loss": 0.8992,
      "step": 19740
    },
    {
      "epoch": 1.2515922557748977,
      "grad_norm": 2.624147891998291,
      "learning_rate": 1.7525613886536832e-05,
      "loss": 0.8417,
      "step": 19750
    },
    {
      "epoch": 1.2522259894166483,
      "grad_norm": 2.544564723968506,
      "learning_rate": 1.75192633361558e-05,
      "loss": 0.8854,
      "step": 19760
    },
    {
      "epoch": 1.2528597230583984,
      "grad_norm": 2.6204028129577637,
      "learning_rate": 1.7512912785774768e-05,
      "loss": 0.8573,
      "step": 19770
    },
    {
      "epoch": 1.253493456700149,
      "grad_norm": 2.3584024906158447,
      "learning_rate": 1.7506562235393735e-05,
      "loss": 0.8434,
      "step": 19780
    },
    {
      "epoch": 1.2541271903418993,
      "grad_norm": 2.1316635608673096,
      "learning_rate": 1.75002116850127e-05,
      "loss": 0.8743,
      "step": 19790
    },
    {
      "epoch": 1.2547609239836497,
      "grad_norm": 2.848724126815796,
      "learning_rate": 1.7493861134631667e-05,
      "loss": 0.8265,
      "step": 19800
    },
    {
      "epoch": 1.2553946576254,
      "grad_norm": 2.6242947578430176,
      "learning_rate": 1.7487510584250637e-05,
      "loss": 0.839,
      "step": 19810
    },
    {
      "epoch": 1.2560283912671504,
      "grad_norm": 2.368906021118164,
      "learning_rate": 1.7481160033869604e-05,
      "loss": 0.8218,
      "step": 19820
    },
    {
      "epoch": 1.2566621249089007,
      "grad_norm": 3.151691198348999,
      "learning_rate": 1.747480948348857e-05,
      "loss": 0.868,
      "step": 19830
    },
    {
      "epoch": 1.257295858550651,
      "grad_norm": 2.809049129486084,
      "learning_rate": 1.7468458933107536e-05,
      "loss": 0.8619,
      "step": 19840
    },
    {
      "epoch": 1.2579295921924016,
      "grad_norm": 2.650317430496216,
      "learning_rate": 1.7462108382726503e-05,
      "loss": 0.8517,
      "step": 19850
    },
    {
      "epoch": 1.258563325834152,
      "grad_norm": 2.690563201904297,
      "learning_rate": 1.745575783234547e-05,
      "loss": 0.8365,
      "step": 19860
    },
    {
      "epoch": 1.2591970594759023,
      "grad_norm": 2.510970115661621,
      "learning_rate": 1.744940728196444e-05,
      "loss": 0.8763,
      "step": 19870
    },
    {
      "epoch": 1.2598307931176527,
      "grad_norm": 2.165473699569702,
      "learning_rate": 1.7443056731583406e-05,
      "loss": 0.8808,
      "step": 19880
    },
    {
      "epoch": 1.260464526759403,
      "grad_norm": 2.5545122623443604,
      "learning_rate": 1.743670618120237e-05,
      "loss": 0.863,
      "step": 19890
    },
    {
      "epoch": 1.2610982604011534,
      "grad_norm": 2.826457977294922,
      "learning_rate": 1.743035563082134e-05,
      "loss": 0.8948,
      "step": 19900
    },
    {
      "epoch": 1.2617319940429037,
      "grad_norm": 2.0504820346832275,
      "learning_rate": 1.7424005080440305e-05,
      "loss": 0.8477,
      "step": 19910
    },
    {
      "epoch": 1.2623657276846543,
      "grad_norm": 2.7980399131774902,
      "learning_rate": 1.741765453005927e-05,
      "loss": 0.8697,
      "step": 19920
    },
    {
      "epoch": 1.2629994613264044,
      "grad_norm": 2.5588810443878174,
      "learning_rate": 1.741130397967824e-05,
      "loss": 0.8256,
      "step": 19930
    },
    {
      "epoch": 1.263633194968155,
      "grad_norm": 2.913891077041626,
      "learning_rate": 1.7404953429297204e-05,
      "loss": 0.9118,
      "step": 19940
    },
    {
      "epoch": 1.2642669286099053,
      "grad_norm": 3.0634849071502686,
      "learning_rate": 1.739860287891617e-05,
      "loss": 0.8498,
      "step": 19950
    },
    {
      "epoch": 1.2649006622516556,
      "grad_norm": 2.1516363620758057,
      "learning_rate": 1.739225232853514e-05,
      "loss": 0.8531,
      "step": 19960
    },
    {
      "epoch": 1.265534395893406,
      "grad_norm": 2.26242733001709,
      "learning_rate": 1.7385901778154107e-05,
      "loss": 0.8886,
      "step": 19970
    },
    {
      "epoch": 1.2661681295351563,
      "grad_norm": 2.6001837253570557,
      "learning_rate": 1.7379551227773077e-05,
      "loss": 0.9005,
      "step": 19980
    },
    {
      "epoch": 1.2668018631769067,
      "grad_norm": 3.1966686248779297,
      "learning_rate": 1.7373200677392043e-05,
      "loss": 0.8976,
      "step": 19990
    },
    {
      "epoch": 1.267435596818657,
      "grad_norm": 2.4429574012756348,
      "learning_rate": 1.7366850127011006e-05,
      "loss": 0.8224,
      "step": 20000
    },
    {
      "epoch": 1.2680693304604076,
      "grad_norm": 2.374434471130371,
      "learning_rate": 1.7360499576629976e-05,
      "loss": 0.8216,
      "step": 20010
    },
    {
      "epoch": 1.268703064102158,
      "grad_norm": 2.9103875160217285,
      "learning_rate": 1.7354149026248942e-05,
      "loss": 0.8546,
      "step": 20020
    },
    {
      "epoch": 1.2693367977439083,
      "grad_norm": 2.7078206539154053,
      "learning_rate": 1.734779847586791e-05,
      "loss": 0.8274,
      "step": 20030
    },
    {
      "epoch": 1.2699705313856586,
      "grad_norm": 3.195526599884033,
      "learning_rate": 1.734144792548688e-05,
      "loss": 0.8842,
      "step": 20040
    },
    {
      "epoch": 1.270604265027409,
      "grad_norm": 2.8442342281341553,
      "learning_rate": 1.733509737510584e-05,
      "loss": 0.869,
      "step": 20050
    },
    {
      "epoch": 1.2712379986691593,
      "grad_norm": 2.970620632171631,
      "learning_rate": 1.7328746824724808e-05,
      "loss": 0.891,
      "step": 20060
    },
    {
      "epoch": 1.2718717323109097,
      "grad_norm": 2.588270425796509,
      "learning_rate": 1.7322396274343778e-05,
      "loss": 0.909,
      "step": 20070
    },
    {
      "epoch": 1.2725054659526602,
      "grad_norm": 2.593729257583618,
      "learning_rate": 1.7316045723962744e-05,
      "loss": 0.8665,
      "step": 20080
    },
    {
      "epoch": 1.2731391995944104,
      "grad_norm": 2.1608030796051025,
      "learning_rate": 1.730969517358171e-05,
      "loss": 0.8136,
      "step": 20090
    },
    {
      "epoch": 1.273772933236161,
      "grad_norm": 2.590974807739258,
      "learning_rate": 1.7303344623200677e-05,
      "loss": 0.8405,
      "step": 20100
    },
    {
      "epoch": 1.2744066668779113,
      "grad_norm": 2.8342068195343018,
      "learning_rate": 1.7296994072819644e-05,
      "loss": 0.8482,
      "step": 20110
    },
    {
      "epoch": 1.2750404005196616,
      "grad_norm": 2.4728267192840576,
      "learning_rate": 1.7290643522438613e-05,
      "loss": 0.8795,
      "step": 20120
    },
    {
      "epoch": 1.275674134161412,
      "grad_norm": 2.793043851852417,
      "learning_rate": 1.728429297205758e-05,
      "loss": 0.8697,
      "step": 20130
    },
    {
      "epoch": 1.2763078678031623,
      "grad_norm": 2.5558953285217285,
      "learning_rate": 1.7277942421676546e-05,
      "loss": 0.8653,
      "step": 20140
    },
    {
      "epoch": 1.2769416014449126,
      "grad_norm": 2.3509223461151123,
      "learning_rate": 1.7271591871295513e-05,
      "loss": 0.8635,
      "step": 20150
    },
    {
      "epoch": 1.277575335086663,
      "grad_norm": 2.665052890777588,
      "learning_rate": 1.726524132091448e-05,
      "loss": 0.8396,
      "step": 20160
    },
    {
      "epoch": 1.2782090687284136,
      "grad_norm": 2.3283259868621826,
      "learning_rate": 1.7258890770533445e-05,
      "loss": 0.8956,
      "step": 20170
    },
    {
      "epoch": 1.278842802370164,
      "grad_norm": 2.5060312747955322,
      "learning_rate": 1.7252540220152415e-05,
      "loss": 0.8686,
      "step": 20180
    },
    {
      "epoch": 1.2794765360119142,
      "grad_norm": 2.3858022689819336,
      "learning_rate": 1.7246189669771382e-05,
      "loss": 0.8874,
      "step": 20190
    },
    {
      "epoch": 1.2801102696536646,
      "grad_norm": 2.277115821838379,
      "learning_rate": 1.7239839119390345e-05,
      "loss": 0.8824,
      "step": 20200
    },
    {
      "epoch": 1.280744003295415,
      "grad_norm": 2.7516932487487793,
      "learning_rate": 1.7233488569009315e-05,
      "loss": 0.8618,
      "step": 20210
    },
    {
      "epoch": 1.2813777369371653,
      "grad_norm": 2.9875285625457764,
      "learning_rate": 1.722713801862828e-05,
      "loss": 0.831,
      "step": 20220
    },
    {
      "epoch": 1.2820114705789156,
      "grad_norm": 3.1181650161743164,
      "learning_rate": 1.7220787468247247e-05,
      "loss": 0.8492,
      "step": 20230
    },
    {
      "epoch": 1.2826452042206662,
      "grad_norm": 3.105870246887207,
      "learning_rate": 1.7214436917866217e-05,
      "loss": 0.8946,
      "step": 20240
    },
    {
      "epoch": 1.2832789378624163,
      "grad_norm": 2.4876961708068848,
      "learning_rate": 1.7208086367485184e-05,
      "loss": 0.8137,
      "step": 20250
    },
    {
      "epoch": 1.2839126715041669,
      "grad_norm": 2.838334560394287,
      "learning_rate": 1.7201735817104147e-05,
      "loss": 0.818,
      "step": 20260
    },
    {
      "epoch": 1.2845464051459172,
      "grad_norm": 2.211374521255493,
      "learning_rate": 1.7195385266723117e-05,
      "loss": 0.8584,
      "step": 20270
    },
    {
      "epoch": 1.2851801387876676,
      "grad_norm": 2.3098769187927246,
      "learning_rate": 1.7189034716342083e-05,
      "loss": 0.8332,
      "step": 20280
    },
    {
      "epoch": 1.285813872429418,
      "grad_norm": 2.3139145374298096,
      "learning_rate": 1.7182684165961053e-05,
      "loss": 0.8581,
      "step": 20290
    },
    {
      "epoch": 1.2864476060711683,
      "grad_norm": 2.540072202682495,
      "learning_rate": 1.717633361558002e-05,
      "loss": 0.8889,
      "step": 20300
    },
    {
      "epoch": 1.2870813397129186,
      "grad_norm": 2.530914545059204,
      "learning_rate": 1.7169983065198982e-05,
      "loss": 0.8527,
      "step": 20310
    },
    {
      "epoch": 1.287715073354669,
      "grad_norm": 2.4925291538238525,
      "learning_rate": 1.7163632514817952e-05,
      "loss": 0.851,
      "step": 20320
    },
    {
      "epoch": 1.2883488069964195,
      "grad_norm": 2.7224581241607666,
      "learning_rate": 1.715728196443692e-05,
      "loss": 0.822,
      "step": 20330
    },
    {
      "epoch": 1.2889825406381699,
      "grad_norm": 2.5304951667785645,
      "learning_rate": 1.7150931414055885e-05,
      "loss": 0.8587,
      "step": 20340
    },
    {
      "epoch": 1.2896162742799202,
      "grad_norm": 3.0132997035980225,
      "learning_rate": 1.7145215918712955e-05,
      "loss": 0.9013,
      "step": 20350
    },
    {
      "epoch": 1.2902500079216705,
      "grad_norm": 2.5384788513183594,
      "learning_rate": 1.7138865368331925e-05,
      "loss": 0.8829,
      "step": 20360
    },
    {
      "epoch": 1.290883741563421,
      "grad_norm": 3.921034574508667,
      "learning_rate": 1.7132514817950888e-05,
      "loss": 0.8447,
      "step": 20370
    },
    {
      "epoch": 1.2915174752051712,
      "grad_norm": 2.219083547592163,
      "learning_rate": 1.7126164267569855e-05,
      "loss": 0.855,
      "step": 20380
    },
    {
      "epoch": 1.2921512088469216,
      "grad_norm": 3.475499391555786,
      "learning_rate": 1.7119813717188824e-05,
      "loss": 0.8706,
      "step": 20390
    },
    {
      "epoch": 1.292784942488672,
      "grad_norm": 2.763294219970703,
      "learning_rate": 1.711346316680779e-05,
      "loss": 0.8685,
      "step": 20400
    },
    {
      "epoch": 1.2934186761304223,
      "grad_norm": 2.524418592453003,
      "learning_rate": 1.7107112616426757e-05,
      "loss": 0.8162,
      "step": 20410
    },
    {
      "epoch": 1.2940524097721728,
      "grad_norm": 2.834423542022705,
      "learning_rate": 1.7100762066045724e-05,
      "loss": 0.8521,
      "step": 20420
    },
    {
      "epoch": 1.2946861434139232,
      "grad_norm": 2.3189620971679688,
      "learning_rate": 1.709441151566469e-05,
      "loss": 0.8905,
      "step": 20430
    },
    {
      "epoch": 1.2953198770556735,
      "grad_norm": 2.399419069290161,
      "learning_rate": 1.708806096528366e-05,
      "loss": 0.9246,
      "step": 20440
    },
    {
      "epoch": 1.2959536106974239,
      "grad_norm": 2.769998550415039,
      "learning_rate": 1.7081710414902626e-05,
      "loss": 0.8759,
      "step": 20450
    },
    {
      "epoch": 1.2965873443391742,
      "grad_norm": 2.522960901260376,
      "learning_rate": 1.7075359864521593e-05,
      "loss": 0.8373,
      "step": 20460
    },
    {
      "epoch": 1.2972210779809246,
      "grad_norm": 2.6793859004974365,
      "learning_rate": 1.706900931414056e-05,
      "loss": 0.8359,
      "step": 20470
    },
    {
      "epoch": 1.297854811622675,
      "grad_norm": 2.3871068954467773,
      "learning_rate": 1.7062658763759526e-05,
      "loss": 0.8178,
      "step": 20480
    },
    {
      "epoch": 1.2984885452644255,
      "grad_norm": 2.3565263748168945,
      "learning_rate": 1.7056308213378492e-05,
      "loss": 0.8652,
      "step": 20490
    },
    {
      "epoch": 1.2991222789061756,
      "grad_norm": 3.560054302215576,
      "learning_rate": 1.7049957662997462e-05,
      "loss": 0.9574,
      "step": 20500
    },
    {
      "epoch": 1.2997560125479262,
      "grad_norm": 2.386625289916992,
      "learning_rate": 1.704360711261643e-05,
      "loss": 0.8629,
      "step": 20510
    },
    {
      "epoch": 1.3003897461896765,
      "grad_norm": 2.760854721069336,
      "learning_rate": 1.703725656223539e-05,
      "loss": 0.8642,
      "step": 20520
    },
    {
      "epoch": 1.3010234798314269,
      "grad_norm": 2.8615963459014893,
      "learning_rate": 1.703090601185436e-05,
      "loss": 0.8849,
      "step": 20530
    },
    {
      "epoch": 1.3016572134731772,
      "grad_norm": 2.723264694213867,
      "learning_rate": 1.7024555461473328e-05,
      "loss": 0.8273,
      "step": 20540
    },
    {
      "epoch": 1.3022909471149275,
      "grad_norm": 2.249204397201538,
      "learning_rate": 1.7018204911092294e-05,
      "loss": 0.8673,
      "step": 20550
    },
    {
      "epoch": 1.3029246807566779,
      "grad_norm": 2.7576372623443604,
      "learning_rate": 1.7011854360711264e-05,
      "loss": 0.8412,
      "step": 20560
    },
    {
      "epoch": 1.3035584143984282,
      "grad_norm": 3.0897397994995117,
      "learning_rate": 1.7005503810330227e-05,
      "loss": 0.8378,
      "step": 20570
    },
    {
      "epoch": 1.3041921480401788,
      "grad_norm": 2.68416428565979,
      "learning_rate": 1.6999153259949193e-05,
      "loss": 0.8731,
      "step": 20580
    },
    {
      "epoch": 1.3048258816819291,
      "grad_norm": 2.301266670227051,
      "learning_rate": 1.6992802709568163e-05,
      "loss": 0.8325,
      "step": 20590
    },
    {
      "epoch": 1.3054596153236795,
      "grad_norm": 2.8268535137176514,
      "learning_rate": 1.698645215918713e-05,
      "loss": 0.868,
      "step": 20600
    },
    {
      "epoch": 1.3060933489654298,
      "grad_norm": 2.6935853958129883,
      "learning_rate": 1.69801016088061e-05,
      "loss": 0.8441,
      "step": 20610
    },
    {
      "epoch": 1.3067270826071802,
      "grad_norm": 2.4575672149658203,
      "learning_rate": 1.6973751058425066e-05,
      "loss": 0.8404,
      "step": 20620
    },
    {
      "epoch": 1.3073608162489305,
      "grad_norm": 3.0418481826782227,
      "learning_rate": 1.696740050804403e-05,
      "loss": 0.8512,
      "step": 20630
    },
    {
      "epoch": 1.3079945498906809,
      "grad_norm": 3.011199712753296,
      "learning_rate": 1.6961049957663e-05,
      "loss": 0.9053,
      "step": 20640
    },
    {
      "epoch": 1.3086282835324314,
      "grad_norm": 2.115873098373413,
      "learning_rate": 1.6954699407281965e-05,
      "loss": 0.8317,
      "step": 20650
    },
    {
      "epoch": 1.3092620171741816,
      "grad_norm": 2.8932979106903076,
      "learning_rate": 1.694834885690093e-05,
      "loss": 0.8734,
      "step": 20660
    },
    {
      "epoch": 1.3098957508159321,
      "grad_norm": 2.4535021781921387,
      "learning_rate": 1.69419983065199e-05,
      "loss": 0.8525,
      "step": 20670
    },
    {
      "epoch": 1.3105294844576825,
      "grad_norm": 2.754821300506592,
      "learning_rate": 1.6935647756138864e-05,
      "loss": 0.8417,
      "step": 20680
    },
    {
      "epoch": 1.3111632180994328,
      "grad_norm": 2.5047719478607178,
      "learning_rate": 1.692929720575783e-05,
      "loss": 0.861,
      "step": 20690
    },
    {
      "epoch": 1.3117969517411832,
      "grad_norm": 2.701352119445801,
      "learning_rate": 1.69229466553768e-05,
      "loss": 0.8298,
      "step": 20700
    },
    {
      "epoch": 1.3124306853829335,
      "grad_norm": 2.360790967941284,
      "learning_rate": 1.6916596104995767e-05,
      "loss": 0.849,
      "step": 20710
    },
    {
      "epoch": 1.3130644190246838,
      "grad_norm": 2.268746852874756,
      "learning_rate": 1.6910245554614733e-05,
      "loss": 0.8492,
      "step": 20720
    },
    {
      "epoch": 1.3136981526664342,
      "grad_norm": 3.0234572887420654,
      "learning_rate": 1.69038950042337e-05,
      "loss": 0.879,
      "step": 20730
    },
    {
      "epoch": 1.3143318863081848,
      "grad_norm": 2.9817285537719727,
      "learning_rate": 1.6897544453852666e-05,
      "loss": 0.8458,
      "step": 20740
    },
    {
      "epoch": 1.314965619949935,
      "grad_norm": 2.580848217010498,
      "learning_rate": 1.6891193903471636e-05,
      "loss": 0.8402,
      "step": 20750
    },
    {
      "epoch": 1.3155993535916854,
      "grad_norm": 2.9383368492126465,
      "learning_rate": 1.6884843353090603e-05,
      "loss": 0.8438,
      "step": 20760
    },
    {
      "epoch": 1.3162330872334358,
      "grad_norm": 2.4819140434265137,
      "learning_rate": 1.687849280270957e-05,
      "loss": 0.8487,
      "step": 20770
    },
    {
      "epoch": 1.3168668208751861,
      "grad_norm": 2.8289244174957275,
      "learning_rate": 1.6872142252328535e-05,
      "loss": 0.8544,
      "step": 20780
    },
    {
      "epoch": 1.3175005545169365,
      "grad_norm": 2.5410172939300537,
      "learning_rate": 1.6865791701947502e-05,
      "loss": 0.8465,
      "step": 20790
    },
    {
      "epoch": 1.3181342881586868,
      "grad_norm": 2.774203062057495,
      "learning_rate": 1.6859441151566468e-05,
      "loss": 0.8679,
      "step": 20800
    },
    {
      "epoch": 1.3187680218004374,
      "grad_norm": 2.2808001041412354,
      "learning_rate": 1.6853090601185438e-05,
      "loss": 0.8169,
      "step": 20810
    },
    {
      "epoch": 1.3194017554421875,
      "grad_norm": 2.8961575031280518,
      "learning_rate": 1.6846740050804405e-05,
      "loss": 0.859,
      "step": 20820
    },
    {
      "epoch": 1.320035489083938,
      "grad_norm": 2.515883445739746,
      "learning_rate": 1.684038950042337e-05,
      "loss": 0.8415,
      "step": 20830
    },
    {
      "epoch": 1.3206692227256884,
      "grad_norm": 2.8364474773406982,
      "learning_rate": 1.6834038950042337e-05,
      "loss": 0.8534,
      "step": 20840
    },
    {
      "epoch": 1.3213029563674388,
      "grad_norm": 2.7715890407562256,
      "learning_rate": 1.6827688399661304e-05,
      "loss": 0.8565,
      "step": 20850
    },
    {
      "epoch": 1.3219366900091891,
      "grad_norm": 2.721116304397583,
      "learning_rate": 1.682133784928027e-05,
      "loss": 0.85,
      "step": 20860
    },
    {
      "epoch": 1.3225704236509395,
      "grad_norm": 2.504812479019165,
      "learning_rate": 1.681498729889924e-05,
      "loss": 0.8883,
      "step": 20870
    },
    {
      "epoch": 1.3232041572926898,
      "grad_norm": 2.5569167137145996,
      "learning_rate": 1.6808636748518206e-05,
      "loss": 0.8631,
      "step": 20880
    },
    {
      "epoch": 1.3238378909344402,
      "grad_norm": 2.488449811935425,
      "learning_rate": 1.680228619813717e-05,
      "loss": 0.8877,
      "step": 20890
    },
    {
      "epoch": 1.3244716245761907,
      "grad_norm": 2.3935089111328125,
      "learning_rate": 1.679593564775614e-05,
      "loss": 0.8741,
      "step": 20900
    },
    {
      "epoch": 1.325105358217941,
      "grad_norm": 2.5816001892089844,
      "learning_rate": 1.6789585097375106e-05,
      "loss": 0.8681,
      "step": 20910
    },
    {
      "epoch": 1.3257390918596914,
      "grad_norm": 2.996649980545044,
      "learning_rate": 1.6783234546994076e-05,
      "loss": 0.8861,
      "step": 20920
    },
    {
      "epoch": 1.3263728255014418,
      "grad_norm": 2.2245285511016846,
      "learning_rate": 1.6776883996613042e-05,
      "loss": 0.8419,
      "step": 20930
    },
    {
      "epoch": 1.327006559143192,
      "grad_norm": 2.3556370735168457,
      "learning_rate": 1.6770533446232005e-05,
      "loss": 0.8613,
      "step": 20940
    },
    {
      "epoch": 1.3276402927849424,
      "grad_norm": 2.8256402015686035,
      "learning_rate": 1.6764182895850975e-05,
      "loss": 0.8371,
      "step": 20950
    },
    {
      "epoch": 1.3282740264266928,
      "grad_norm": 2.108473777770996,
      "learning_rate": 1.675783234546994e-05,
      "loss": 0.818,
      "step": 20960
    },
    {
      "epoch": 1.3289077600684434,
      "grad_norm": 2.3472113609313965,
      "learning_rate": 1.6751481795088908e-05,
      "loss": 0.9007,
      "step": 20970
    },
    {
      "epoch": 1.3295414937101935,
      "grad_norm": 2.6933228969573975,
      "learning_rate": 1.6745131244707878e-05,
      "loss": 0.8554,
      "step": 20980
    },
    {
      "epoch": 1.330175227351944,
      "grad_norm": 2.9747180938720703,
      "learning_rate": 1.673878069432684e-05,
      "loss": 0.9059,
      "step": 20990
    },
    {
      "epoch": 1.3308089609936944,
      "grad_norm": 2.0676562786102295,
      "learning_rate": 1.6732430143945807e-05,
      "loss": 0.838,
      "step": 21000
    },
    {
      "epoch": 1.3314426946354447,
      "grad_norm": 2.487565279006958,
      "learning_rate": 1.6726079593564777e-05,
      "loss": 0.8596,
      "step": 21010
    },
    {
      "epoch": 1.332076428277195,
      "grad_norm": 2.6250808238983154,
      "learning_rate": 1.6719729043183743e-05,
      "loss": 0.8908,
      "step": 21020
    },
    {
      "epoch": 1.3327101619189454,
      "grad_norm": 3.1724915504455566,
      "learning_rate": 1.671337849280271e-05,
      "loss": 0.8615,
      "step": 21030
    },
    {
      "epoch": 1.3333438955606958,
      "grad_norm": 3.0328876972198486,
      "learning_rate": 1.6707027942421676e-05,
      "loss": 0.869,
      "step": 21040
    },
    {
      "epoch": 1.3339776292024461,
      "grad_norm": 2.7695062160491943,
      "learning_rate": 1.6700677392040643e-05,
      "loss": 0.8528,
      "step": 21050
    },
    {
      "epoch": 1.3346113628441967,
      "grad_norm": 2.7491159439086914,
      "learning_rate": 1.6694326841659612e-05,
      "loss": 0.88,
      "step": 21060
    },
    {
      "epoch": 1.335245096485947,
      "grad_norm": 2.396409273147583,
      "learning_rate": 1.668797629127858e-05,
      "loss": 0.8555,
      "step": 21070
    },
    {
      "epoch": 1.3358788301276974,
      "grad_norm": 2.277768135070801,
      "learning_rate": 1.6681625740897545e-05,
      "loss": 0.8006,
      "step": 21080
    },
    {
      "epoch": 1.3365125637694477,
      "grad_norm": 2.551427125930786,
      "learning_rate": 1.6675275190516515e-05,
      "loss": 0.8725,
      "step": 21090
    },
    {
      "epoch": 1.337146297411198,
      "grad_norm": 2.6654391288757324,
      "learning_rate": 1.6668924640135478e-05,
      "loss": 0.8495,
      "step": 21100
    },
    {
      "epoch": 1.3377800310529484,
      "grad_norm": 2.9731907844543457,
      "learning_rate": 1.6662574089754444e-05,
      "loss": 0.8806,
      "step": 21110
    },
    {
      "epoch": 1.3384137646946987,
      "grad_norm": 2.6106150150299072,
      "learning_rate": 1.6656223539373414e-05,
      "loss": 0.8663,
      "step": 21120
    },
    {
      "epoch": 1.3390474983364493,
      "grad_norm": 2.5548455715179443,
      "learning_rate": 1.664987298899238e-05,
      "loss": 0.8742,
      "step": 21130
    },
    {
      "epoch": 1.3396812319781994,
      "grad_norm": 2.810472249984741,
      "learning_rate": 1.6643522438611347e-05,
      "loss": 0.8568,
      "step": 21140
    },
    {
      "epoch": 1.34031496561995,
      "grad_norm": 2.6453003883361816,
      "learning_rate": 1.6637171888230314e-05,
      "loss": 0.871,
      "step": 21150
    },
    {
      "epoch": 1.3409486992617003,
      "grad_norm": 2.4018967151641846,
      "learning_rate": 1.663082133784928e-05,
      "loss": 0.8754,
      "step": 21160
    },
    {
      "epoch": 1.3415824329034507,
      "grad_norm": 2.584378719329834,
      "learning_rate": 1.6624470787468246e-05,
      "loss": 0.8988,
      "step": 21170
    },
    {
      "epoch": 1.342216166545201,
      "grad_norm": 3.163438558578491,
      "learning_rate": 1.6618120237087216e-05,
      "loss": 0.8893,
      "step": 21180
    },
    {
      "epoch": 1.3428499001869514,
      "grad_norm": 2.6415345668792725,
      "learning_rate": 1.6611769686706183e-05,
      "loss": 0.8435,
      "step": 21190
    },
    {
      "epoch": 1.3434836338287017,
      "grad_norm": 2.304776668548584,
      "learning_rate": 1.6605419136325146e-05,
      "loss": 0.891,
      "step": 21200
    },
    {
      "epoch": 1.344117367470452,
      "grad_norm": 2.5544486045837402,
      "learning_rate": 1.6599068585944116e-05,
      "loss": 0.8706,
      "step": 21210
    },
    {
      "epoch": 1.3447511011122026,
      "grad_norm": 2.418797731399536,
      "learning_rate": 1.6592718035563082e-05,
      "loss": 0.8458,
      "step": 21220
    },
    {
      "epoch": 1.345384834753953,
      "grad_norm": 2.829958438873291,
      "learning_rate": 1.6586367485182052e-05,
      "loss": 0.8369,
      "step": 21230
    },
    {
      "epoch": 1.3460185683957033,
      "grad_norm": 2.2995266914367676,
      "learning_rate": 1.6580016934801018e-05,
      "loss": 0.8653,
      "step": 21240
    },
    {
      "epoch": 1.3466523020374537,
      "grad_norm": 2.4823720455169678,
      "learning_rate": 1.657366638441998e-05,
      "loss": 0.8573,
      "step": 21250
    },
    {
      "epoch": 1.347286035679204,
      "grad_norm": 3.1245851516723633,
      "learning_rate": 1.656731583403895e-05,
      "loss": 0.8938,
      "step": 21260
    },
    {
      "epoch": 1.3479197693209544,
      "grad_norm": 2.540494918823242,
      "learning_rate": 1.6560965283657917e-05,
      "loss": 0.8266,
      "step": 21270
    },
    {
      "epoch": 1.3485535029627047,
      "grad_norm": 3.161283493041992,
      "learning_rate": 1.6554614733276884e-05,
      "loss": 0.8697,
      "step": 21280
    },
    {
      "epoch": 1.3491872366044553,
      "grad_norm": 2.6143605709075928,
      "learning_rate": 1.6548264182895854e-05,
      "loss": 0.8524,
      "step": 21290
    },
    {
      "epoch": 1.3498209702462054,
      "grad_norm": 2.326655149459839,
      "learning_rate": 1.6541913632514817e-05,
      "loss": 0.8469,
      "step": 21300
    },
    {
      "epoch": 1.350454703887956,
      "grad_norm": 2.3209786415100098,
      "learning_rate": 1.6535563082133783e-05,
      "loss": 0.8617,
      "step": 21310
    },
    {
      "epoch": 1.3510884375297063,
      "grad_norm": 2.2728917598724365,
      "learning_rate": 1.6529212531752753e-05,
      "loss": 0.8457,
      "step": 21320
    },
    {
      "epoch": 1.3517221711714567,
      "grad_norm": 3.1175215244293213,
      "learning_rate": 1.652286198137172e-05,
      "loss": 0.8841,
      "step": 21330
    },
    {
      "epoch": 1.352355904813207,
      "grad_norm": 2.8980813026428223,
      "learning_rate": 1.6516511430990686e-05,
      "loss": 0.9295,
      "step": 21340
    },
    {
      "epoch": 1.3529896384549573,
      "grad_norm": 3.1494972705841064,
      "learning_rate": 1.6510160880609656e-05,
      "loss": 0.9143,
      "step": 21350
    },
    {
      "epoch": 1.3536233720967077,
      "grad_norm": 2.2839269638061523,
      "learning_rate": 1.650381033022862e-05,
      "loss": 0.8272,
      "step": 21360
    },
    {
      "epoch": 1.354257105738458,
      "grad_norm": 2.684185743331909,
      "learning_rate": 1.6497459779847585e-05,
      "loss": 0.8841,
      "step": 21370
    },
    {
      "epoch": 1.3548908393802086,
      "grad_norm": 2.3137969970703125,
      "learning_rate": 1.6491109229466555e-05,
      "loss": 0.8582,
      "step": 21380
    },
    {
      "epoch": 1.355524573021959,
      "grad_norm": 2.8932368755340576,
      "learning_rate": 1.648475867908552e-05,
      "loss": 0.8676,
      "step": 21390
    },
    {
      "epoch": 1.3561583066637093,
      "grad_norm": 2.517500638961792,
      "learning_rate": 1.647840812870449e-05,
      "loss": 0.8494,
      "step": 21400
    },
    {
      "epoch": 1.3567920403054596,
      "grad_norm": 2.325983762741089,
      "learning_rate": 1.6472057578323454e-05,
      "loss": 0.8371,
      "step": 21410
    },
    {
      "epoch": 1.35742577394721,
      "grad_norm": 2.3272671699523926,
      "learning_rate": 1.646570702794242e-05,
      "loss": 0.8113,
      "step": 21420
    },
    {
      "epoch": 1.3580595075889603,
      "grad_norm": 2.9009616374969482,
      "learning_rate": 1.645935647756139e-05,
      "loss": 0.8676,
      "step": 21430
    },
    {
      "epoch": 1.3586932412307107,
      "grad_norm": 2.7478630542755127,
      "learning_rate": 1.6453005927180357e-05,
      "loss": 0.8597,
      "step": 21440
    },
    {
      "epoch": 1.3593269748724612,
      "grad_norm": 2.655017137527466,
      "learning_rate": 1.6446655376799323e-05,
      "loss": 0.8229,
      "step": 21450
    },
    {
      "epoch": 1.3599607085142114,
      "grad_norm": 2.766768455505371,
      "learning_rate": 1.644030482641829e-05,
      "loss": 0.8614,
      "step": 21460
    },
    {
      "epoch": 1.360594442155962,
      "grad_norm": 2.597679853439331,
      "learning_rate": 1.6433954276037256e-05,
      "loss": 0.8309,
      "step": 21470
    },
    {
      "epoch": 1.3612281757977123,
      "grad_norm": 2.475879192352295,
      "learning_rate": 1.6427603725656223e-05,
      "loss": 0.8905,
      "step": 21480
    },
    {
      "epoch": 1.3618619094394626,
      "grad_norm": 2.8958449363708496,
      "learning_rate": 1.6421253175275192e-05,
      "loss": 0.9296,
      "step": 21490
    },
    {
      "epoch": 1.362495643081213,
      "grad_norm": 2.1945345401763916,
      "learning_rate": 1.641490262489416e-05,
      "loss": 0.836,
      "step": 21500
    },
    {
      "epoch": 1.3631293767229633,
      "grad_norm": 2.798996925354004,
      "learning_rate": 1.6408552074513122e-05,
      "loss": 0.8502,
      "step": 21510
    },
    {
      "epoch": 1.3637631103647136,
      "grad_norm": 3.049884557723999,
      "learning_rate": 1.640220152413209e-05,
      "loss": 0.8823,
      "step": 21520
    },
    {
      "epoch": 1.364396844006464,
      "grad_norm": 2.865068197250366,
      "learning_rate": 1.6395850973751058e-05,
      "loss": 0.8134,
      "step": 21530
    },
    {
      "epoch": 1.3650305776482146,
      "grad_norm": 2.444263458251953,
      "learning_rate": 1.6389500423370028e-05,
      "loss": 0.8435,
      "step": 21540
    },
    {
      "epoch": 1.365664311289965,
      "grad_norm": 2.798124313354492,
      "learning_rate": 1.6383149872988994e-05,
      "loss": 0.8506,
      "step": 21550
    },
    {
      "epoch": 1.3662980449317152,
      "grad_norm": 3.370795726776123,
      "learning_rate": 1.6376799322607957e-05,
      "loss": 0.8408,
      "step": 21560
    },
    {
      "epoch": 1.3669317785734656,
      "grad_norm": 2.3705971240997314,
      "learning_rate": 1.6370448772226927e-05,
      "loss": 0.8931,
      "step": 21570
    },
    {
      "epoch": 1.367565512215216,
      "grad_norm": 2.5026400089263916,
      "learning_rate": 1.6364098221845894e-05,
      "loss": 0.8637,
      "step": 21580
    },
    {
      "epoch": 1.3681992458569663,
      "grad_norm": 2.465395927429199,
      "learning_rate": 1.635774767146486e-05,
      "loss": 0.8158,
      "step": 21590
    },
    {
      "epoch": 1.3688329794987166,
      "grad_norm": 2.587094783782959,
      "learning_rate": 1.635139712108383e-05,
      "loss": 0.8785,
      "step": 21600
    },
    {
      "epoch": 1.3694667131404672,
      "grad_norm": 2.317110538482666,
      "learning_rate": 1.6345046570702796e-05,
      "loss": 0.8814,
      "step": 21610
    },
    {
      "epoch": 1.3701004467822173,
      "grad_norm": 2.958008050918579,
      "learning_rate": 1.633869602032176e-05,
      "loss": 0.8875,
      "step": 21620
    },
    {
      "epoch": 1.3707341804239679,
      "grad_norm": 2.243514060974121,
      "learning_rate": 1.633234546994073e-05,
      "loss": 0.8444,
      "step": 21630
    },
    {
      "epoch": 1.3713679140657182,
      "grad_norm": 2.6604526042938232,
      "learning_rate": 1.6325994919559696e-05,
      "loss": 0.8683,
      "step": 21640
    },
    {
      "epoch": 1.3720016477074686,
      "grad_norm": 2.7290446758270264,
      "learning_rate": 1.6319644369178662e-05,
      "loss": 0.8694,
      "step": 21650
    },
    {
      "epoch": 1.372635381349219,
      "grad_norm": 2.6813511848449707,
      "learning_rate": 1.6313293818797632e-05,
      "loss": 0.8858,
      "step": 21660
    },
    {
      "epoch": 1.3732691149909693,
      "grad_norm": 2.6317877769470215,
      "learning_rate": 1.6306943268416595e-05,
      "loss": 0.8971,
      "step": 21670
    },
    {
      "epoch": 1.3739028486327196,
      "grad_norm": 2.575838565826416,
      "learning_rate": 1.630059271803556e-05,
      "loss": 0.8466,
      "step": 21680
    },
    {
      "epoch": 1.37453658227447,
      "grad_norm": 2.8406360149383545,
      "learning_rate": 1.629424216765453e-05,
      "loss": 0.8911,
      "step": 21690
    },
    {
      "epoch": 1.3751703159162205,
      "grad_norm": 2.8054912090301514,
      "learning_rate": 1.6287891617273498e-05,
      "loss": 0.8182,
      "step": 21700
    },
    {
      "epoch": 1.3758040495579709,
      "grad_norm": 2.1613941192626953,
      "learning_rate": 1.6281541066892467e-05,
      "loss": 0.851,
      "step": 21710
    },
    {
      "epoch": 1.3764377831997212,
      "grad_norm": 2.5448968410491943,
      "learning_rate": 1.627519051651143e-05,
      "loss": 0.8815,
      "step": 21720
    },
    {
      "epoch": 1.3770715168414716,
      "grad_norm": 2.490635395050049,
      "learning_rate": 1.6268839966130397e-05,
      "loss": 0.8937,
      "step": 21730
    },
    {
      "epoch": 1.377705250483222,
      "grad_norm": 2.272862195968628,
      "learning_rate": 1.6262489415749367e-05,
      "loss": 0.8894,
      "step": 21740
    },
    {
      "epoch": 1.3783389841249722,
      "grad_norm": 2.482025384902954,
      "learning_rate": 1.6256138865368333e-05,
      "loss": 0.8514,
      "step": 21750
    },
    {
      "epoch": 1.3789727177667226,
      "grad_norm": 2.7030630111694336,
      "learning_rate": 1.62497883149873e-05,
      "loss": 0.8393,
      "step": 21760
    },
    {
      "epoch": 1.3796064514084732,
      "grad_norm": 2.4821970462799072,
      "learning_rate": 1.6243437764606266e-05,
      "loss": 0.8845,
      "step": 21770
    },
    {
      "epoch": 1.3802401850502233,
      "grad_norm": 2.6159112453460693,
      "learning_rate": 1.6237087214225232e-05,
      "loss": 0.876,
      "step": 21780
    },
    {
      "epoch": 1.3808739186919738,
      "grad_norm": 2.810515880584717,
      "learning_rate": 1.62307366638442e-05,
      "loss": 0.8201,
      "step": 21790
    },
    {
      "epoch": 1.3815076523337242,
      "grad_norm": 2.636089324951172,
      "learning_rate": 1.622438611346317e-05,
      "loss": 0.8627,
      "step": 21800
    },
    {
      "epoch": 1.3821413859754745,
      "grad_norm": 2.702543258666992,
      "learning_rate": 1.6218035563082135e-05,
      "loss": 0.8594,
      "step": 21810
    },
    {
      "epoch": 1.3827751196172249,
      "grad_norm": 2.853327512741089,
      "learning_rate": 1.6211685012701098e-05,
      "loss": 0.8881,
      "step": 21820
    },
    {
      "epoch": 1.3834088532589752,
      "grad_norm": 2.601332902908325,
      "learning_rate": 1.6205334462320068e-05,
      "loss": 0.9012,
      "step": 21830
    },
    {
      "epoch": 1.3840425869007256,
      "grad_norm": 2.6743323802948,
      "learning_rate": 1.6198983911939034e-05,
      "loss": 0.8619,
      "step": 21840
    },
    {
      "epoch": 1.384676320542476,
      "grad_norm": 2.600015640258789,
      "learning_rate": 1.6192633361558004e-05,
      "loss": 0.8222,
      "step": 21850
    },
    {
      "epoch": 1.3853100541842265,
      "grad_norm": 2.4659361839294434,
      "learning_rate": 1.618628281117697e-05,
      "loss": 0.8741,
      "step": 21860
    },
    {
      "epoch": 1.3859437878259768,
      "grad_norm": 2.553143262863159,
      "learning_rate": 1.6179932260795937e-05,
      "loss": 0.8464,
      "step": 21870
    },
    {
      "epoch": 1.3865775214677272,
      "grad_norm": 2.7124698162078857,
      "learning_rate": 1.6173581710414903e-05,
      "loss": 0.8684,
      "step": 21880
    },
    {
      "epoch": 1.3872112551094775,
      "grad_norm": 2.1708154678344727,
      "learning_rate": 1.616723116003387e-05,
      "loss": 0.8802,
      "step": 21890
    },
    {
      "epoch": 1.3878449887512279,
      "grad_norm": 2.8574142456054688,
      "learning_rate": 1.6160880609652836e-05,
      "loss": 0.8668,
      "step": 21900
    },
    {
      "epoch": 1.3884787223929782,
      "grad_norm": 2.757763147354126,
      "learning_rate": 1.6154530059271806e-05,
      "loss": 0.8972,
      "step": 21910
    },
    {
      "epoch": 1.3891124560347285,
      "grad_norm": 2.4151723384857178,
      "learning_rate": 1.6148179508890772e-05,
      "loss": 0.8435,
      "step": 21920
    },
    {
      "epoch": 1.389746189676479,
      "grad_norm": 3.1116161346435547,
      "learning_rate": 1.6141828958509736e-05,
      "loss": 0.8971,
      "step": 21930
    },
    {
      "epoch": 1.3903799233182292,
      "grad_norm": 2.9041590690612793,
      "learning_rate": 1.6135478408128705e-05,
      "loss": 0.8789,
      "step": 21940
    },
    {
      "epoch": 1.3910136569599798,
      "grad_norm": 2.374621629714966,
      "learning_rate": 1.6129127857747672e-05,
      "loss": 0.835,
      "step": 21950
    },
    {
      "epoch": 1.3916473906017301,
      "grad_norm": 2.938483476638794,
      "learning_rate": 1.6122777307366638e-05,
      "loss": 0.8942,
      "step": 21960
    },
    {
      "epoch": 1.3922811242434805,
      "grad_norm": 2.5448098182678223,
      "learning_rate": 1.6116426756985608e-05,
      "loss": 0.8551,
      "step": 21970
    },
    {
      "epoch": 1.3929148578852308,
      "grad_norm": 2.307313919067383,
      "learning_rate": 1.611007620660457e-05,
      "loss": 0.8544,
      "step": 21980
    },
    {
      "epoch": 1.3935485915269812,
      "grad_norm": 2.6086010932922363,
      "learning_rate": 1.6103725656223537e-05,
      "loss": 0.8519,
      "step": 21990
    },
    {
      "epoch": 1.3941823251687315,
      "grad_norm": 2.7128782272338867,
      "learning_rate": 1.6097375105842507e-05,
      "loss": 0.8386,
      "step": 22000
    },
    {
      "epoch": 1.3948160588104819,
      "grad_norm": 2.359511613845825,
      "learning_rate": 1.6091024555461474e-05,
      "loss": 0.8326,
      "step": 22010
    },
    {
      "epoch": 1.3954497924522324,
      "grad_norm": 2.4165825843811035,
      "learning_rate": 1.6084674005080444e-05,
      "loss": 0.8733,
      "step": 22020
    },
    {
      "epoch": 1.3960835260939826,
      "grad_norm": 2.6370668411254883,
      "learning_rate": 1.6078323454699407e-05,
      "loss": 0.8606,
      "step": 22030
    },
    {
      "epoch": 1.3967172597357331,
      "grad_norm": 2.542999267578125,
      "learning_rate": 1.6071972904318373e-05,
      "loss": 0.9179,
      "step": 22040
    },
    {
      "epoch": 1.3973509933774835,
      "grad_norm": 2.462794542312622,
      "learning_rate": 1.6065622353937343e-05,
      "loss": 0.8279,
      "step": 22050
    },
    {
      "epoch": 1.3979847270192338,
      "grad_norm": 3.1117632389068604,
      "learning_rate": 1.605927180355631e-05,
      "loss": 0.8531,
      "step": 22060
    },
    {
      "epoch": 1.3986184606609842,
      "grad_norm": 2.9638538360595703,
      "learning_rate": 1.6052921253175276e-05,
      "loss": 0.9074,
      "step": 22070
    },
    {
      "epoch": 1.3992521943027345,
      "grad_norm": 2.5322105884552,
      "learning_rate": 1.6046570702794242e-05,
      "loss": 0.8412,
      "step": 22080
    },
    {
      "epoch": 1.3998859279444849,
      "grad_norm": 2.401740550994873,
      "learning_rate": 1.604022015241321e-05,
      "loss": 0.9145,
      "step": 22090
    },
    {
      "epoch": 1.4005196615862352,
      "grad_norm": 2.8121840953826904,
      "learning_rate": 1.6033869602032175e-05,
      "loss": 0.8286,
      "step": 22100
    },
    {
      "epoch": 1.4011533952279858,
      "grad_norm": 2.2197153568267822,
      "learning_rate": 1.6027519051651145e-05,
      "loss": 0.839,
      "step": 22110
    },
    {
      "epoch": 1.401787128869736,
      "grad_norm": 2.4902873039245605,
      "learning_rate": 1.602116850127011e-05,
      "loss": 0.8403,
      "step": 22120
    },
    {
      "epoch": 1.4024208625114865,
      "grad_norm": 2.730614423751831,
      "learning_rate": 1.6014817950889078e-05,
      "loss": 0.8582,
      "step": 22130
    },
    {
      "epoch": 1.4030545961532368,
      "grad_norm": 2.7021634578704834,
      "learning_rate": 1.6008467400508044e-05,
      "loss": 0.9033,
      "step": 22140
    },
    {
      "epoch": 1.4036883297949871,
      "grad_norm": 2.2315709590911865,
      "learning_rate": 1.600211685012701e-05,
      "loss": 0.826,
      "step": 22150
    },
    {
      "epoch": 1.4043220634367375,
      "grad_norm": 2.18527889251709,
      "learning_rate": 1.599576629974598e-05,
      "loss": 0.8606,
      "step": 22160
    },
    {
      "epoch": 1.4049557970784878,
      "grad_norm": 3.1918222904205322,
      "learning_rate": 1.5989415749364947e-05,
      "loss": 0.8683,
      "step": 22170
    },
    {
      "epoch": 1.4055895307202384,
      "grad_norm": 2.248016595840454,
      "learning_rate": 1.5983065198983913e-05,
      "loss": 0.8694,
      "step": 22180
    },
    {
      "epoch": 1.4062232643619885,
      "grad_norm": 2.436823606491089,
      "learning_rate": 1.597671464860288e-05,
      "loss": 0.8447,
      "step": 22190
    },
    {
      "epoch": 1.406856998003739,
      "grad_norm": 2.5230581760406494,
      "learning_rate": 1.5970364098221846e-05,
      "loss": 0.8681,
      "step": 22200
    },
    {
      "epoch": 1.4074907316454894,
      "grad_norm": 2.5052907466888428,
      "learning_rate": 1.5964013547840812e-05,
      "loss": 0.8726,
      "step": 22210
    },
    {
      "epoch": 1.4081244652872398,
      "grad_norm": 2.8447601795196533,
      "learning_rate": 1.5957662997459782e-05,
      "loss": 0.8692,
      "step": 22220
    },
    {
      "epoch": 1.4087581989289901,
      "grad_norm": 2.418065071105957,
      "learning_rate": 1.595131244707875e-05,
      "loss": 0.8332,
      "step": 22230
    },
    {
      "epoch": 1.4093919325707405,
      "grad_norm": 2.883918523788452,
      "learning_rate": 1.5944961896697712e-05,
      "loss": 0.8827,
      "step": 22240
    },
    {
      "epoch": 1.4100256662124908,
      "grad_norm": 2.846513509750366,
      "learning_rate": 1.593861134631668e-05,
      "loss": 0.8879,
      "step": 22250
    },
    {
      "epoch": 1.4106593998542412,
      "grad_norm": 2.607048273086548,
      "learning_rate": 1.5932260795935648e-05,
      "loss": 0.8626,
      "step": 22260
    },
    {
      "epoch": 1.4112931334959917,
      "grad_norm": 2.5888500213623047,
      "learning_rate": 1.5925910245554614e-05,
      "loss": 0.8478,
      "step": 22270
    },
    {
      "epoch": 1.411926867137742,
      "grad_norm": 2.410892963409424,
      "learning_rate": 1.5919559695173584e-05,
      "loss": 0.8803,
      "step": 22280
    },
    {
      "epoch": 1.4125606007794924,
      "grad_norm": 2.7490551471710205,
      "learning_rate": 1.5913209144792547e-05,
      "loss": 0.9103,
      "step": 22290
    },
    {
      "epoch": 1.4131943344212428,
      "grad_norm": 2.5378851890563965,
      "learning_rate": 1.5906858594411514e-05,
      "loss": 0.8804,
      "step": 22300
    },
    {
      "epoch": 1.413828068062993,
      "grad_norm": 2.5286381244659424,
      "learning_rate": 1.5900508044030483e-05,
      "loss": 0.8761,
      "step": 22310
    },
    {
      "epoch": 1.4144618017047434,
      "grad_norm": 2.2414329051971436,
      "learning_rate": 1.589415749364945e-05,
      "loss": 0.8557,
      "step": 22320
    },
    {
      "epoch": 1.4150955353464938,
      "grad_norm": 2.538004159927368,
      "learning_rate": 1.588780694326842e-05,
      "loss": 0.8442,
      "step": 22330
    },
    {
      "epoch": 1.4157292689882444,
      "grad_norm": 2.5686709880828857,
      "learning_rate": 1.5881456392887383e-05,
      "loss": 0.8723,
      "step": 22340
    },
    {
      "epoch": 1.4163630026299945,
      "grad_norm": 2.302905797958374,
      "learning_rate": 1.587510584250635e-05,
      "loss": 0.8558,
      "step": 22350
    },
    {
      "epoch": 1.416996736271745,
      "grad_norm": 2.8895297050476074,
      "learning_rate": 1.586875529212532e-05,
      "loss": 0.8379,
      "step": 22360
    },
    {
      "epoch": 1.4176304699134954,
      "grad_norm": 2.6432881355285645,
      "learning_rate": 1.5862404741744285e-05,
      "loss": 0.867,
      "step": 22370
    },
    {
      "epoch": 1.4182642035552457,
      "grad_norm": 2.906294345855713,
      "learning_rate": 1.5856054191363252e-05,
      "loss": 0.8652,
      "step": 22380
    },
    {
      "epoch": 1.418897937196996,
      "grad_norm": 2.4830338954925537,
      "learning_rate": 1.584970364098222e-05,
      "loss": 0.8919,
      "step": 22390
    },
    {
      "epoch": 1.4195316708387464,
      "grad_norm": 2.5202741622924805,
      "learning_rate": 1.5843353090601185e-05,
      "loss": 0.8374,
      "step": 22400
    },
    {
      "epoch": 1.4201654044804968,
      "grad_norm": 2.4326417446136475,
      "learning_rate": 1.583700254022015e-05,
      "loss": 0.8596,
      "step": 22410
    },
    {
      "epoch": 1.4207991381222471,
      "grad_norm": 2.666391134262085,
      "learning_rate": 1.583065198983912e-05,
      "loss": 0.8423,
      "step": 22420
    },
    {
      "epoch": 1.4214328717639977,
      "grad_norm": 3.1146984100341797,
      "learning_rate": 1.5824301439458087e-05,
      "loss": 0.9003,
      "step": 22430
    },
    {
      "epoch": 1.422066605405748,
      "grad_norm": 2.497065544128418,
      "learning_rate": 1.5817950889077054e-05,
      "loss": 0.8573,
      "step": 22440
    },
    {
      "epoch": 1.4227003390474984,
      "grad_norm": 2.642517328262329,
      "learning_rate": 1.581160033869602e-05,
      "loss": 0.8105,
      "step": 22450
    },
    {
      "epoch": 1.4233340726892487,
      "grad_norm": 2.634308338165283,
      "learning_rate": 1.5805249788314987e-05,
      "loss": 0.8754,
      "step": 22460
    },
    {
      "epoch": 1.423967806330999,
      "grad_norm": 2.552231788635254,
      "learning_rate": 1.5798899237933953e-05,
      "loss": 0.8473,
      "step": 22470
    },
    {
      "epoch": 1.4246015399727494,
      "grad_norm": 2.9076619148254395,
      "learning_rate": 1.5792548687552923e-05,
      "loss": 0.86,
      "step": 22480
    },
    {
      "epoch": 1.4252352736144998,
      "grad_norm": 2.539247989654541,
      "learning_rate": 1.578619813717189e-05,
      "loss": 0.8617,
      "step": 22490
    },
    {
      "epoch": 1.4258690072562503,
      "grad_norm": 2.379462718963623,
      "learning_rate": 1.5779847586790856e-05,
      "loss": 0.8539,
      "step": 22500
    },
    {
      "epoch": 1.4265027408980004,
      "grad_norm": 3.302354097366333,
      "learning_rate": 1.5773497036409822e-05,
      "loss": 0.84,
      "step": 22510
    },
    {
      "epoch": 1.427136474539751,
      "grad_norm": 2.637373685836792,
      "learning_rate": 1.576714648602879e-05,
      "loss": 0.864,
      "step": 22520
    },
    {
      "epoch": 1.4277702081815014,
      "grad_norm": 2.612180233001709,
      "learning_rate": 1.576079593564776e-05,
      "loss": 0.8483,
      "step": 22530
    },
    {
      "epoch": 1.4284039418232517,
      "grad_norm": 2.9222846031188965,
      "learning_rate": 1.5754445385266725e-05,
      "loss": 0.8404,
      "step": 22540
    },
    {
      "epoch": 1.429037675465002,
      "grad_norm": 2.782792329788208,
      "learning_rate": 1.5748094834885688e-05,
      "loss": 0.8282,
      "step": 22550
    },
    {
      "epoch": 1.4296714091067524,
      "grad_norm": 2.5561792850494385,
      "learning_rate": 1.5741744284504658e-05,
      "loss": 0.8524,
      "step": 22560
    },
    {
      "epoch": 1.4303051427485027,
      "grad_norm": 2.5335183143615723,
      "learning_rate": 1.5735393734123624e-05,
      "loss": 0.8344,
      "step": 22570
    },
    {
      "epoch": 1.430938876390253,
      "grad_norm": 2.808472156524658,
      "learning_rate": 1.572904318374259e-05,
      "loss": 0.8801,
      "step": 22580
    },
    {
      "epoch": 1.4315726100320036,
      "grad_norm": 3.018125534057617,
      "learning_rate": 1.572269263336156e-05,
      "loss": 0.8503,
      "step": 22590
    },
    {
      "epoch": 1.432206343673754,
      "grad_norm": 2.360102653503418,
      "learning_rate": 1.5716342082980523e-05,
      "loss": 0.9054,
      "step": 22600
    },
    {
      "epoch": 1.4328400773155043,
      "grad_norm": 2.3120713233947754,
      "learning_rate": 1.570999153259949e-05,
      "loss": 0.9054,
      "step": 22610
    },
    {
      "epoch": 1.4334738109572547,
      "grad_norm": 2.7066562175750732,
      "learning_rate": 1.570364098221846e-05,
      "loss": 0.8547,
      "step": 22620
    },
    {
      "epoch": 1.434107544599005,
      "grad_norm": 2.7420876026153564,
      "learning_rate": 1.5697290431837426e-05,
      "loss": 0.8685,
      "step": 22630
    },
    {
      "epoch": 1.4347412782407554,
      "grad_norm": 3.204678535461426,
      "learning_rate": 1.5690939881456396e-05,
      "loss": 0.8585,
      "step": 22640
    },
    {
      "epoch": 1.4353750118825057,
      "grad_norm": 2.4826366901397705,
      "learning_rate": 1.5684589331075362e-05,
      "loss": 0.8012,
      "step": 22650
    },
    {
      "epoch": 1.4360087455242563,
      "grad_norm": 2.577678918838501,
      "learning_rate": 1.5678238780694325e-05,
      "loss": 0.8634,
      "step": 22660
    },
    {
      "epoch": 1.4366424791660064,
      "grad_norm": 2.366262912750244,
      "learning_rate": 1.5671888230313295e-05,
      "loss": 0.8547,
      "step": 22670
    },
    {
      "epoch": 1.437276212807757,
      "grad_norm": 2.474954843521118,
      "learning_rate": 1.566553767993226e-05,
      "loss": 0.8897,
      "step": 22680
    },
    {
      "epoch": 1.4379099464495073,
      "grad_norm": 2.2643814086914062,
      "learning_rate": 1.5659187129551228e-05,
      "loss": 0.8578,
      "step": 22690
    },
    {
      "epoch": 1.4385436800912577,
      "grad_norm": 2.6757452487945557,
      "learning_rate": 1.5652836579170198e-05,
      "loss": 0.8397,
      "step": 22700
    },
    {
      "epoch": 1.439177413733008,
      "grad_norm": 2.346642017364502,
      "learning_rate": 1.564648602878916e-05,
      "loss": 0.8677,
      "step": 22710
    },
    {
      "epoch": 1.4398111473747583,
      "grad_norm": 2.254917621612549,
      "learning_rate": 1.5640135478408127e-05,
      "loss": 0.8431,
      "step": 22720
    },
    {
      "epoch": 1.4404448810165087,
      "grad_norm": 2.3322153091430664,
      "learning_rate": 1.5633784928027097e-05,
      "loss": 0.8675,
      "step": 22730
    },
    {
      "epoch": 1.441078614658259,
      "grad_norm": 2.4327521324157715,
      "learning_rate": 1.5627434377646064e-05,
      "loss": 0.8565,
      "step": 22740
    },
    {
      "epoch": 1.4417123483000096,
      "grad_norm": 3.1211843490600586,
      "learning_rate": 1.562108382726503e-05,
      "loss": 0.9257,
      "step": 22750
    },
    {
      "epoch": 1.44234608194176,
      "grad_norm": 2.505005359649658,
      "learning_rate": 1.5614733276883996e-05,
      "loss": 0.8448,
      "step": 22760
    },
    {
      "epoch": 1.4429798155835103,
      "grad_norm": 2.492353916168213,
      "learning_rate": 1.5608382726502963e-05,
      "loss": 0.8552,
      "step": 22770
    },
    {
      "epoch": 1.4436135492252606,
      "grad_norm": 2.29811954498291,
      "learning_rate": 1.560203217612193e-05,
      "loss": 0.8081,
      "step": 22780
    },
    {
      "epoch": 1.444247282867011,
      "grad_norm": 3.1631617546081543,
      "learning_rate": 1.55956816257409e-05,
      "loss": 0.8879,
      "step": 22790
    },
    {
      "epoch": 1.4448810165087613,
      "grad_norm": 2.302578926086426,
      "learning_rate": 1.5589331075359865e-05,
      "loss": 0.8784,
      "step": 22800
    },
    {
      "epoch": 1.4455147501505117,
      "grad_norm": 2.907860517501831,
      "learning_rate": 1.5582980524978832e-05,
      "loss": 0.8597,
      "step": 22810
    },
    {
      "epoch": 1.4461484837922622,
      "grad_norm": 2.631821632385254,
      "learning_rate": 1.55766299745978e-05,
      "loss": 0.9007,
      "step": 22820
    },
    {
      "epoch": 1.4467822174340124,
      "grad_norm": 2.932002544403076,
      "learning_rate": 1.5570279424216765e-05,
      "loss": 0.8466,
      "step": 22830
    },
    {
      "epoch": 1.447415951075763,
      "grad_norm": 2.6691479682922363,
      "learning_rate": 1.5563928873835735e-05,
      "loss": 0.8378,
      "step": 22840
    },
    {
      "epoch": 1.4480496847175133,
      "grad_norm": 2.8140954971313477,
      "learning_rate": 1.55575783234547e-05,
      "loss": 0.8662,
      "step": 22850
    },
    {
      "epoch": 1.4486834183592636,
      "grad_norm": 2.897052049636841,
      "learning_rate": 1.5551227773073664e-05,
      "loss": 0.8682,
      "step": 22860
    },
    {
      "epoch": 1.449317152001014,
      "grad_norm": 2.6791954040527344,
      "learning_rate": 1.5544877222692634e-05,
      "loss": 0.8537,
      "step": 22870
    },
    {
      "epoch": 1.4499508856427643,
      "grad_norm": 2.9215946197509766,
      "learning_rate": 1.55385266723116e-05,
      "loss": 0.8566,
      "step": 22880
    },
    {
      "epoch": 1.4505846192845147,
      "grad_norm": 2.4470884799957275,
      "learning_rate": 1.5532176121930567e-05,
      "loss": 0.8916,
      "step": 22890
    },
    {
      "epoch": 1.451218352926265,
      "grad_norm": 2.446260929107666,
      "learning_rate": 1.5525825571549537e-05,
      "loss": 0.8685,
      "step": 22900
    },
    {
      "epoch": 1.4518520865680156,
      "grad_norm": 2.413224220275879,
      "learning_rate": 1.5519475021168503e-05,
      "loss": 0.8703,
      "step": 22910
    },
    {
      "epoch": 1.452485820209766,
      "grad_norm": 2.29634165763855,
      "learning_rate": 1.5513124470787466e-05,
      "loss": 0.8682,
      "step": 22920
    },
    {
      "epoch": 1.4531195538515163,
      "grad_norm": 2.809706926345825,
      "learning_rate": 1.5506773920406436e-05,
      "loss": 0.8779,
      "step": 22930
    },
    {
      "epoch": 1.4537532874932666,
      "grad_norm": 3.1001572608947754,
      "learning_rate": 1.5500423370025402e-05,
      "loss": 0.8663,
      "step": 22940
    },
    {
      "epoch": 1.454387021135017,
      "grad_norm": 2.9339427947998047,
      "learning_rate": 1.5494072819644372e-05,
      "loss": 0.9101,
      "step": 22950
    },
    {
      "epoch": 1.4550207547767673,
      "grad_norm": 2.6194722652435303,
      "learning_rate": 1.548772226926334e-05,
      "loss": 0.852,
      "step": 22960
    },
    {
      "epoch": 1.4556544884185176,
      "grad_norm": 2.6422646045684814,
      "learning_rate": 1.54813717188823e-05,
      "loss": 0.8785,
      "step": 22970
    },
    {
      "epoch": 1.4562882220602682,
      "grad_norm": 3.3347325325012207,
      "learning_rate": 1.547502116850127e-05,
      "loss": 0.8206,
      "step": 22980
    },
    {
      "epoch": 1.4569219557020183,
      "grad_norm": 2.421231508255005,
      "learning_rate": 1.5468670618120238e-05,
      "loss": 0.8421,
      "step": 22990
    },
    {
      "epoch": 1.457555689343769,
      "grad_norm": 2.4918599128723145,
      "learning_rate": 1.5462320067739204e-05,
      "loss": 0.8615,
      "step": 23000
    },
    {
      "epoch": 1.4581894229855192,
      "grad_norm": 3.1916542053222656,
      "learning_rate": 1.5455969517358174e-05,
      "loss": 0.8556,
      "step": 23010
    },
    {
      "epoch": 1.4588231566272696,
      "grad_norm": 2.5507102012634277,
      "learning_rate": 1.5449618966977137e-05,
      "loss": 0.8563,
      "step": 23020
    },
    {
      "epoch": 1.45945689026902,
      "grad_norm": 2.1881604194641113,
      "learning_rate": 1.5443268416596103e-05,
      "loss": 0.8778,
      "step": 23030
    },
    {
      "epoch": 1.4600906239107703,
      "grad_norm": 2.680194616317749,
      "learning_rate": 1.5436917866215073e-05,
      "loss": 0.8748,
      "step": 23040
    },
    {
      "epoch": 1.4607243575525206,
      "grad_norm": 2.6492679119110107,
      "learning_rate": 1.543056731583404e-05,
      "loss": 0.8754,
      "step": 23050
    },
    {
      "epoch": 1.461358091194271,
      "grad_norm": 2.3889987468719482,
      "learning_rate": 1.5424216765453006e-05,
      "loss": 0.8647,
      "step": 23060
    },
    {
      "epoch": 1.4619918248360215,
      "grad_norm": 2.376027822494507,
      "learning_rate": 1.5417866215071973e-05,
      "loss": 0.8539,
      "step": 23070
    },
    {
      "epoch": 1.4626255584777719,
      "grad_norm": 3.2124524116516113,
      "learning_rate": 1.541151566469094e-05,
      "loss": 0.8497,
      "step": 23080
    },
    {
      "epoch": 1.4632592921195222,
      "grad_norm": 2.230825185775757,
      "learning_rate": 1.5405165114309905e-05,
      "loss": 0.8503,
      "step": 23090
    },
    {
      "epoch": 1.4638930257612726,
      "grad_norm": 2.6924891471862793,
      "learning_rate": 1.5398814563928875e-05,
      "loss": 0.8425,
      "step": 23100
    },
    {
      "epoch": 1.464526759403023,
      "grad_norm": 2.726229429244995,
      "learning_rate": 1.539246401354784e-05,
      "loss": 0.847,
      "step": 23110
    },
    {
      "epoch": 1.4651604930447732,
      "grad_norm": 2.967698812484741,
      "learning_rate": 1.5386113463166808e-05,
      "loss": 0.8569,
      "step": 23120
    },
    {
      "epoch": 1.4657942266865236,
      "grad_norm": 2.0957372188568115,
      "learning_rate": 1.5379762912785775e-05,
      "loss": 0.8118,
      "step": 23130
    },
    {
      "epoch": 1.4664279603282742,
      "grad_norm": 2.531275510787964,
      "learning_rate": 1.537341236240474e-05,
      "loss": 0.8466,
      "step": 23140
    },
    {
      "epoch": 1.4670616939700243,
      "grad_norm": 2.4572227001190186,
      "learning_rate": 1.536706181202371e-05,
      "loss": 0.8442,
      "step": 23150
    },
    {
      "epoch": 1.4676954276117748,
      "grad_norm": 2.4585418701171875,
      "learning_rate": 1.5360711261642677e-05,
      "loss": 0.8734,
      "step": 23160
    },
    {
      "epoch": 1.4683291612535252,
      "grad_norm": 2.629112720489502,
      "learning_rate": 1.5354360711261644e-05,
      "loss": 0.8489,
      "step": 23170
    },
    {
      "epoch": 1.4689628948952755,
      "grad_norm": 3.2492620944976807,
      "learning_rate": 1.534801016088061e-05,
      "loss": 0.8381,
      "step": 23180
    },
    {
      "epoch": 1.4695966285370259,
      "grad_norm": 2.463442802429199,
      "learning_rate": 1.5341659610499576e-05,
      "loss": 0.8733,
      "step": 23190
    },
    {
      "epoch": 1.4702303621787762,
      "grad_norm": 3.1570730209350586,
      "learning_rate": 1.5335309060118543e-05,
      "loss": 0.8589,
      "step": 23200
    },
    {
      "epoch": 1.4708640958205266,
      "grad_norm": 2.6074090003967285,
      "learning_rate": 1.5328958509737513e-05,
      "loss": 0.8491,
      "step": 23210
    },
    {
      "epoch": 1.471497829462277,
      "grad_norm": 2.271364688873291,
      "learning_rate": 1.532260795935648e-05,
      "loss": 0.905,
      "step": 23220
    },
    {
      "epoch": 1.4721315631040275,
      "grad_norm": 2.296325922012329,
      "learning_rate": 1.5316257408975442e-05,
      "loss": 0.8579,
      "step": 23230
    },
    {
      "epoch": 1.4727652967457778,
      "grad_norm": 2.5795516967773438,
      "learning_rate": 1.5309906858594412e-05,
      "loss": 0.8727,
      "step": 23240
    },
    {
      "epoch": 1.4733990303875282,
      "grad_norm": 2.907616138458252,
      "learning_rate": 1.530355630821338e-05,
      "loss": 0.8164,
      "step": 23250
    },
    {
      "epoch": 1.4740327640292785,
      "grad_norm": 2.5980606079101562,
      "learning_rate": 1.5297205757832348e-05,
      "loss": 0.8471,
      "step": 23260
    },
    {
      "epoch": 1.4746664976710289,
      "grad_norm": 2.706244945526123,
      "learning_rate": 1.5290855207451315e-05,
      "loss": 0.8782,
      "step": 23270
    },
    {
      "epoch": 1.4753002313127792,
      "grad_norm": 2.422941207885742,
      "learning_rate": 1.5284504657070278e-05,
      "loss": 0.8801,
      "step": 23280
    },
    {
      "epoch": 1.4759339649545296,
      "grad_norm": 2.2997562885284424,
      "learning_rate": 1.5278154106689248e-05,
      "loss": 0.8421,
      "step": 23290
    },
    {
      "epoch": 1.4765676985962801,
      "grad_norm": 2.394073963165283,
      "learning_rate": 1.5271803556308214e-05,
      "loss": 0.8368,
      "step": 23300
    },
    {
      "epoch": 1.4772014322380302,
      "grad_norm": 2.3349978923797607,
      "learning_rate": 1.526545300592718e-05,
      "loss": 0.8369,
      "step": 23310
    },
    {
      "epoch": 1.4778351658797808,
      "grad_norm": 2.5834174156188965,
      "learning_rate": 1.525910245554615e-05,
      "loss": 0.882,
      "step": 23320
    },
    {
      "epoch": 1.4784688995215312,
      "grad_norm": 2.641462802886963,
      "learning_rate": 1.5252751905165113e-05,
      "loss": 0.857,
      "step": 23330
    },
    {
      "epoch": 1.4791026331632815,
      "grad_norm": 2.2925100326538086,
      "learning_rate": 1.5246401354784081e-05,
      "loss": 0.8306,
      "step": 23340
    },
    {
      "epoch": 1.4797363668050318,
      "grad_norm": 2.8465402126312256,
      "learning_rate": 1.5240050804403048e-05,
      "loss": 0.8883,
      "step": 23350
    },
    {
      "epoch": 1.4803701004467822,
      "grad_norm": 2.1962950229644775,
      "learning_rate": 1.5233700254022016e-05,
      "loss": 0.8589,
      "step": 23360
    },
    {
      "epoch": 1.4810038340885325,
      "grad_norm": 2.4304614067077637,
      "learning_rate": 1.5227349703640984e-05,
      "loss": 0.8124,
      "step": 23370
    },
    {
      "epoch": 1.4816375677302829,
      "grad_norm": 2.3014378547668457,
      "learning_rate": 1.5220999153259949e-05,
      "loss": 0.8692,
      "step": 23380
    },
    {
      "epoch": 1.4822713013720334,
      "grad_norm": 2.8931734561920166,
      "learning_rate": 1.5214648602878915e-05,
      "loss": 0.8773,
      "step": 23390
    },
    {
      "epoch": 1.4829050350137838,
      "grad_norm": 2.4476945400238037,
      "learning_rate": 1.5208298052497883e-05,
      "loss": 0.8749,
      "step": 23400
    },
    {
      "epoch": 1.4835387686555341,
      "grad_norm": 3.2358758449554443,
      "learning_rate": 1.5201947502116851e-05,
      "loss": 0.8383,
      "step": 23410
    },
    {
      "epoch": 1.4841725022972845,
      "grad_norm": 2.3724634647369385,
      "learning_rate": 1.5195596951735818e-05,
      "loss": 0.8599,
      "step": 23420
    },
    {
      "epoch": 1.4848062359390348,
      "grad_norm": 2.88047456741333,
      "learning_rate": 1.5189246401354786e-05,
      "loss": 0.8809,
      "step": 23430
    },
    {
      "epoch": 1.4854399695807852,
      "grad_norm": 2.3816845417022705,
      "learning_rate": 1.518289585097375e-05,
      "loss": 0.8161,
      "step": 23440
    },
    {
      "epoch": 1.4860737032225355,
      "grad_norm": 2.4694936275482178,
      "learning_rate": 1.5176545300592717e-05,
      "loss": 0.8717,
      "step": 23450
    },
    {
      "epoch": 1.4867074368642859,
      "grad_norm": 2.4306998252868652,
      "learning_rate": 1.5170194750211685e-05,
      "loss": 0.848,
      "step": 23460
    },
    {
      "epoch": 1.4873411705060362,
      "grad_norm": 2.6927967071533203,
      "learning_rate": 1.5163844199830653e-05,
      "loss": 0.8678,
      "step": 23470
    },
    {
      "epoch": 1.4879749041477868,
      "grad_norm": 3.1013107299804688,
      "learning_rate": 1.5157493649449621e-05,
      "loss": 0.8649,
      "step": 23480
    },
    {
      "epoch": 1.4886086377895371,
      "grad_norm": 2.5055456161499023,
      "learning_rate": 1.5151143099068585e-05,
      "loss": 0.8402,
      "step": 23490
    },
    {
      "epoch": 1.4892423714312875,
      "grad_norm": 2.6258912086486816,
      "learning_rate": 1.5144792548687553e-05,
      "loss": 0.8827,
      "step": 23500
    },
    {
      "epoch": 1.4898761050730378,
      "grad_norm": 2.336463212966919,
      "learning_rate": 1.513844199830652e-05,
      "loss": 0.878,
      "step": 23510
    },
    {
      "epoch": 1.4905098387147881,
      "grad_norm": 3.1975908279418945,
      "learning_rate": 1.5132091447925487e-05,
      "loss": 0.895,
      "step": 23520
    },
    {
      "epoch": 1.4911435723565385,
      "grad_norm": 2.2459988594055176,
      "learning_rate": 1.5125740897544455e-05,
      "loss": 0.8634,
      "step": 23530
    },
    {
      "epoch": 1.4917773059982888,
      "grad_norm": 2.779172658920288,
      "learning_rate": 1.511939034716342e-05,
      "loss": 0.8877,
      "step": 23540
    },
    {
      "epoch": 1.4924110396400394,
      "grad_norm": 2.5844004154205322,
      "learning_rate": 1.5113039796782388e-05,
      "loss": 0.8624,
      "step": 23550
    },
    {
      "epoch": 1.4930447732817895,
      "grad_norm": 2.8583459854125977,
      "learning_rate": 1.5106689246401355e-05,
      "loss": 0.8882,
      "step": 23560
    },
    {
      "epoch": 1.49367850692354,
      "grad_norm": 2.7943239212036133,
      "learning_rate": 1.5100338696020323e-05,
      "loss": 0.8488,
      "step": 23570
    },
    {
      "epoch": 1.4943122405652904,
      "grad_norm": 2.463552474975586,
      "learning_rate": 1.509398814563929e-05,
      "loss": 0.8315,
      "step": 23580
    },
    {
      "epoch": 1.4949459742070408,
      "grad_norm": 2.715480089187622,
      "learning_rate": 1.5087637595258254e-05,
      "loss": 0.8805,
      "step": 23590
    },
    {
      "epoch": 1.4955797078487911,
      "grad_norm": 3.024787187576294,
      "learning_rate": 1.5081287044877222e-05,
      "loss": 0.9066,
      "step": 23600
    },
    {
      "epoch": 1.4962134414905415,
      "grad_norm": 2.8189034461975098,
      "learning_rate": 1.507493649449619e-05,
      "loss": 0.8567,
      "step": 23610
    },
    {
      "epoch": 1.4968471751322918,
      "grad_norm": 2.449622392654419,
      "learning_rate": 1.5068585944115158e-05,
      "loss": 0.8376,
      "step": 23620
    },
    {
      "epoch": 1.4974809087740422,
      "grad_norm": 2.6213319301605225,
      "learning_rate": 1.5062235393734125e-05,
      "loss": 0.8554,
      "step": 23630
    },
    {
      "epoch": 1.4981146424157927,
      "grad_norm": 2.628504753112793,
      "learning_rate": 1.505588484335309e-05,
      "loss": 0.8477,
      "step": 23640
    },
    {
      "epoch": 1.498748376057543,
      "grad_norm": 2.9411096572875977,
      "learning_rate": 1.5049534292972058e-05,
      "loss": 0.8582,
      "step": 23650
    },
    {
      "epoch": 1.4993821096992934,
      "grad_norm": 4.24680233001709,
      "learning_rate": 1.5043183742591024e-05,
      "loss": 0.9003,
      "step": 23660
    },
    {
      "epoch": 1.5000158433410438,
      "grad_norm": 2.046684503555298,
      "learning_rate": 1.5036833192209992e-05,
      "loss": 0.8711,
      "step": 23670
    },
    {
      "epoch": 1.500649576982794,
      "grad_norm": 3.3656888008117676,
      "learning_rate": 1.503048264182896e-05,
      "loss": 0.8898,
      "step": 23680
    },
    {
      "epoch": 1.5012833106245445,
      "grad_norm": 2.560607671737671,
      "learning_rate": 1.5024132091447927e-05,
      "loss": 0.8211,
      "step": 23690
    },
    {
      "epoch": 1.5019170442662948,
      "grad_norm": 2.3290932178497314,
      "learning_rate": 1.5017781541066891e-05,
      "loss": 0.8812,
      "step": 23700
    },
    {
      "epoch": 1.5025507779080454,
      "grad_norm": 2.330645799636841,
      "learning_rate": 1.501143099068586e-05,
      "loss": 0.8297,
      "step": 23710
    },
    {
      "epoch": 1.5031845115497955,
      "grad_norm": 2.7302839756011963,
      "learning_rate": 1.5005080440304828e-05,
      "loss": 0.872,
      "step": 23720
    },
    {
      "epoch": 1.503818245191546,
      "grad_norm": 2.5912256240844727,
      "learning_rate": 1.4998729889923794e-05,
      "loss": 0.901,
      "step": 23730
    },
    {
      "epoch": 1.5044519788332964,
      "grad_norm": 2.352454423904419,
      "learning_rate": 1.499237933954276e-05,
      "loss": 0.8819,
      "step": 23740
    },
    {
      "epoch": 1.5050857124750467,
      "grad_norm": 2.8846025466918945,
      "learning_rate": 1.4986028789161729e-05,
      "loss": 0.8442,
      "step": 23750
    },
    {
      "epoch": 1.505719446116797,
      "grad_norm": 2.3469157218933105,
      "learning_rate": 1.4979678238780693e-05,
      "loss": 0.8316,
      "step": 23760
    },
    {
      "epoch": 1.5063531797585474,
      "grad_norm": 2.3047289848327637,
      "learning_rate": 1.4973327688399661e-05,
      "loss": 0.8568,
      "step": 23770
    },
    {
      "epoch": 1.506986913400298,
      "grad_norm": 2.655388355255127,
      "learning_rate": 1.496697713801863e-05,
      "loss": 0.873,
      "step": 23780
    },
    {
      "epoch": 1.5076206470420481,
      "grad_norm": 2.874870538711548,
      "learning_rate": 1.4960626587637596e-05,
      "loss": 0.8823,
      "step": 23790
    },
    {
      "epoch": 1.5082543806837987,
      "grad_norm": 2.8615353107452393,
      "learning_rate": 1.4954276037256562e-05,
      "loss": 0.8643,
      "step": 23800
    },
    {
      "epoch": 1.5088881143255488,
      "grad_norm": 2.638477325439453,
      "learning_rate": 1.4947925486875529e-05,
      "loss": 0.849,
      "step": 23810
    },
    {
      "epoch": 1.5095218479672994,
      "grad_norm": 2.782428503036499,
      "learning_rate": 1.4941574936494497e-05,
      "loss": 0.9035,
      "step": 23820
    },
    {
      "epoch": 1.5101555816090497,
      "grad_norm": 2.560032606124878,
      "learning_rate": 1.4935224386113463e-05,
      "loss": 0.8392,
      "step": 23830
    },
    {
      "epoch": 1.5107893152508,
      "grad_norm": 2.5878520011901855,
      "learning_rate": 1.492887383573243e-05,
      "loss": 0.8269,
      "step": 23840
    },
    {
      "epoch": 1.5114230488925504,
      "grad_norm": 2.732236385345459,
      "learning_rate": 1.4922523285351398e-05,
      "loss": 0.863,
      "step": 23850
    },
    {
      "epoch": 1.5120567825343008,
      "grad_norm": 2.8749942779541016,
      "learning_rate": 1.4916172734970366e-05,
      "loss": 0.9024,
      "step": 23860
    },
    {
      "epoch": 1.5126905161760513,
      "grad_norm": 2.5166547298431396,
      "learning_rate": 1.490982218458933e-05,
      "loss": 0.844,
      "step": 23870
    },
    {
      "epoch": 1.5133242498178014,
      "grad_norm": 2.4860386848449707,
      "learning_rate": 1.4903471634208299e-05,
      "loss": 0.8826,
      "step": 23880
    },
    {
      "epoch": 1.513957983459552,
      "grad_norm": 2.7013702392578125,
      "learning_rate": 1.4897121083827265e-05,
      "loss": 0.8702,
      "step": 23890
    },
    {
      "epoch": 1.5145917171013024,
      "grad_norm": 2.5118396282196045,
      "learning_rate": 1.4890770533446232e-05,
      "loss": 0.8218,
      "step": 23900
    },
    {
      "epoch": 1.5152254507430527,
      "grad_norm": 2.5687859058380127,
      "learning_rate": 1.48844199830652e-05,
      "loss": 0.8901,
      "step": 23910
    },
    {
      "epoch": 1.515859184384803,
      "grad_norm": 2.9410359859466553,
      "learning_rate": 1.4878069432684166e-05,
      "loss": 0.8432,
      "step": 23920
    },
    {
      "epoch": 1.5164929180265534,
      "grad_norm": 2.265352964401245,
      "learning_rate": 1.4871718882303134e-05,
      "loss": 0.8675,
      "step": 23930
    },
    {
      "epoch": 1.517126651668304,
      "grad_norm": 2.4231913089752197,
      "learning_rate": 1.4865368331922099e-05,
      "loss": 0.8293,
      "step": 23940
    },
    {
      "epoch": 1.517760385310054,
      "grad_norm": 2.865910291671753,
      "learning_rate": 1.4859017781541067e-05,
      "loss": 0.8611,
      "step": 23950
    },
    {
      "epoch": 1.5183941189518046,
      "grad_norm": 2.505741596221924,
      "learning_rate": 1.4852667231160035e-05,
      "loss": 0.8765,
      "step": 23960
    },
    {
      "epoch": 1.5190278525935548,
      "grad_norm": 2.8099381923675537,
      "learning_rate": 1.4846316680779e-05,
      "loss": 0.8493,
      "step": 23970
    },
    {
      "epoch": 1.5196615862353053,
      "grad_norm": 3.351464033126831,
      "learning_rate": 1.4839966130397968e-05,
      "loss": 0.878,
      "step": 23980
    },
    {
      "epoch": 1.5202953198770557,
      "grad_norm": 2.6973745822906494,
      "learning_rate": 1.4834250635055039e-05,
      "loss": 0.8239,
      "step": 23990
    },
    {
      "epoch": 1.520929053518806,
      "grad_norm": 2.4131009578704834,
      "learning_rate": 1.4827900084674005e-05,
      "loss": 0.8756,
      "step": 24000
    },
    {
      "epoch": 1.5215627871605564,
      "grad_norm": 2.6895978450775146,
      "learning_rate": 1.4821549534292973e-05,
      "loss": 0.8656,
      "step": 24010
    },
    {
      "epoch": 1.5221965208023067,
      "grad_norm": 2.3290767669677734,
      "learning_rate": 1.481519898391194e-05,
      "loss": 0.864,
      "step": 24020
    },
    {
      "epoch": 1.5228302544440573,
      "grad_norm": 2.7777602672576904,
      "learning_rate": 1.4808848433530906e-05,
      "loss": 0.8531,
      "step": 24030
    },
    {
      "epoch": 1.5234639880858074,
      "grad_norm": 2.628242254257202,
      "learning_rate": 1.4802497883149874e-05,
      "loss": 0.881,
      "step": 24040
    },
    {
      "epoch": 1.524097721727558,
      "grad_norm": 2.9334990978240967,
      "learning_rate": 1.4796147332768839e-05,
      "loss": 0.8666,
      "step": 24050
    },
    {
      "epoch": 1.5247314553693083,
      "grad_norm": 2.812631368637085,
      "learning_rate": 1.4789796782387807e-05,
      "loss": 0.8523,
      "step": 24060
    },
    {
      "epoch": 1.5253651890110587,
      "grad_norm": 2.440183401107788,
      "learning_rate": 1.4783446232006775e-05,
      "loss": 0.8866,
      "step": 24070
    },
    {
      "epoch": 1.525998922652809,
      "grad_norm": 2.7539455890655518,
      "learning_rate": 1.477709568162574e-05,
      "loss": 0.8571,
      "step": 24080
    },
    {
      "epoch": 1.5266326562945594,
      "grad_norm": 2.922466278076172,
      "learning_rate": 1.4770745131244708e-05,
      "loss": 0.8714,
      "step": 24090
    },
    {
      "epoch": 1.52726638993631,
      "grad_norm": 2.685645580291748,
      "learning_rate": 1.4764394580863676e-05,
      "loss": 0.8317,
      "step": 24100
    },
    {
      "epoch": 1.52790012357806,
      "grad_norm": 2.880661725997925,
      "learning_rate": 1.4758044030482643e-05,
      "loss": 0.8585,
      "step": 24110
    },
    {
      "epoch": 1.5285338572198106,
      "grad_norm": 3.0589311122894287,
      "learning_rate": 1.4751693480101609e-05,
      "loss": 0.858,
      "step": 24120
    },
    {
      "epoch": 1.5291675908615607,
      "grad_norm": 3.422349691390991,
      "learning_rate": 1.4745342929720575e-05,
      "loss": 0.8777,
      "step": 24130
    },
    {
      "epoch": 1.5298013245033113,
      "grad_norm": 2.904989242553711,
      "learning_rate": 1.4738992379339544e-05,
      "loss": 0.8656,
      "step": 24140
    },
    {
      "epoch": 1.5304350581450616,
      "grad_norm": 2.392414093017578,
      "learning_rate": 1.473264182895851e-05,
      "loss": 0.8631,
      "step": 24150
    },
    {
      "epoch": 1.531068791786812,
      "grad_norm": 2.311372756958008,
      "learning_rate": 1.4726291278577476e-05,
      "loss": 0.843,
      "step": 24160
    },
    {
      "epoch": 1.5317025254285623,
      "grad_norm": 3.066293239593506,
      "learning_rate": 1.4719940728196445e-05,
      "loss": 0.8614,
      "step": 24170
    },
    {
      "epoch": 1.5323362590703127,
      "grad_norm": 2.595846652984619,
      "learning_rate": 1.4713590177815411e-05,
      "loss": 0.8597,
      "step": 24180
    },
    {
      "epoch": 1.5329699927120632,
      "grad_norm": 2.403575897216797,
      "learning_rate": 1.4707239627434377e-05,
      "loss": 0.8834,
      "step": 24190
    },
    {
      "epoch": 1.5336037263538134,
      "grad_norm": 2.8154869079589844,
      "learning_rate": 1.4700889077053346e-05,
      "loss": 0.8822,
      "step": 24200
    },
    {
      "epoch": 1.534237459995564,
      "grad_norm": 2.462430000305176,
      "learning_rate": 1.4694538526672312e-05,
      "loss": 0.8205,
      "step": 24210
    },
    {
      "epoch": 1.5348711936373143,
      "grad_norm": 2.5721607208251953,
      "learning_rate": 1.4688187976291278e-05,
      "loss": 0.8321,
      "step": 24220
    },
    {
      "epoch": 1.5355049272790646,
      "grad_norm": 3.1421420574188232,
      "learning_rate": 1.4681837425910246e-05,
      "loss": 0.8958,
      "step": 24230
    },
    {
      "epoch": 1.536138660920815,
      "grad_norm": 2.5817043781280518,
      "learning_rate": 1.4675486875529213e-05,
      "loss": 0.8398,
      "step": 24240
    },
    {
      "epoch": 1.5367723945625653,
      "grad_norm": 2.8257923126220703,
      "learning_rate": 1.4669136325148181e-05,
      "loss": 0.9036,
      "step": 24250
    },
    {
      "epoch": 1.5374061282043159,
      "grad_norm": 2.756197452545166,
      "learning_rate": 1.4662785774767146e-05,
      "loss": 0.8643,
      "step": 24260
    },
    {
      "epoch": 1.538039861846066,
      "grad_norm": 2.6006994247436523,
      "learning_rate": 1.4656435224386114e-05,
      "loss": 0.8607,
      "step": 24270
    },
    {
      "epoch": 1.5386735954878166,
      "grad_norm": 2.5407707691192627,
      "learning_rate": 1.4650084674005082e-05,
      "loss": 0.8535,
      "step": 24280
    },
    {
      "epoch": 1.5393073291295667,
      "grad_norm": 2.5947084426879883,
      "learning_rate": 1.4643734123624047e-05,
      "loss": 0.821,
      "step": 24290
    },
    {
      "epoch": 1.5399410627713173,
      "grad_norm": 3.485880136489868,
      "learning_rate": 1.4637383573243015e-05,
      "loss": 0.8947,
      "step": 24300
    },
    {
      "epoch": 1.5405747964130676,
      "grad_norm": 2.500701427459717,
      "learning_rate": 1.4631033022861981e-05,
      "loss": 0.8458,
      "step": 24310
    },
    {
      "epoch": 1.541208530054818,
      "grad_norm": 2.499978542327881,
      "learning_rate": 1.462468247248095e-05,
      "loss": 0.8603,
      "step": 24320
    },
    {
      "epoch": 1.5418422636965683,
      "grad_norm": 2.487119436264038,
      "learning_rate": 1.4618331922099916e-05,
      "loss": 0.852,
      "step": 24330
    },
    {
      "epoch": 1.5424759973383186,
      "grad_norm": 3.328895330429077,
      "learning_rate": 1.4611981371718882e-05,
      "loss": 0.8831,
      "step": 24340
    },
    {
      "epoch": 1.5431097309800692,
      "grad_norm": 3.266507148742676,
      "learning_rate": 1.460563082133785e-05,
      "loss": 0.8813,
      "step": 24350
    },
    {
      "epoch": 1.5437434646218193,
      "grad_norm": 2.4376955032348633,
      "learning_rate": 1.4599280270956817e-05,
      "loss": 0.8841,
      "step": 24360
    },
    {
      "epoch": 1.54437719826357,
      "grad_norm": 2.467430353164673,
      "learning_rate": 1.4592929720575783e-05,
      "loss": 0.7974,
      "step": 24370
    },
    {
      "epoch": 1.5450109319053202,
      "grad_norm": 2.7049858570098877,
      "learning_rate": 1.4586579170194751e-05,
      "loss": 0.8824,
      "step": 24380
    },
    {
      "epoch": 1.5456446655470706,
      "grad_norm": 3.2595016956329346,
      "learning_rate": 1.4580228619813716e-05,
      "loss": 0.843,
      "step": 24390
    },
    {
      "epoch": 1.546278399188821,
      "grad_norm": 3.0116946697235107,
      "learning_rate": 1.4573878069432684e-05,
      "loss": 0.8868,
      "step": 24400
    },
    {
      "epoch": 1.5469121328305713,
      "grad_norm": 2.482111930847168,
      "learning_rate": 1.4567527519051652e-05,
      "loss": 0.8615,
      "step": 24410
    },
    {
      "epoch": 1.5475458664723218,
      "grad_norm": 2.4941530227661133,
      "learning_rate": 1.4561176968670619e-05,
      "loss": 0.871,
      "step": 24420
    },
    {
      "epoch": 1.548179600114072,
      "grad_norm": 2.6303212642669678,
      "learning_rate": 1.4554826418289585e-05,
      "loss": 0.8701,
      "step": 24430
    },
    {
      "epoch": 1.5488133337558225,
      "grad_norm": 2.4012653827667236,
      "learning_rate": 1.4548475867908552e-05,
      "loss": 0.8804,
      "step": 24440
    },
    {
      "epoch": 1.5494470673975727,
      "grad_norm": 2.800872564315796,
      "learning_rate": 1.454212531752752e-05,
      "loss": 0.8785,
      "step": 24450
    },
    {
      "epoch": 1.5500808010393232,
      "grad_norm": 2.439667224884033,
      "learning_rate": 1.4535774767146486e-05,
      "loss": 0.846,
      "step": 24460
    },
    {
      "epoch": 1.5507145346810736,
      "grad_norm": 2.639329433441162,
      "learning_rate": 1.4529424216765453e-05,
      "loss": 0.8253,
      "step": 24470
    },
    {
      "epoch": 1.551348268322824,
      "grad_norm": 2.119981527328491,
      "learning_rate": 1.452307366638442e-05,
      "loss": 0.8865,
      "step": 24480
    },
    {
      "epoch": 1.5519820019645743,
      "grad_norm": 3.376983880996704,
      "learning_rate": 1.4516723116003389e-05,
      "loss": 0.9029,
      "step": 24490
    },
    {
      "epoch": 1.5526157356063246,
      "grad_norm": 2.667585611343384,
      "learning_rate": 1.4510372565622354e-05,
      "loss": 0.8823,
      "step": 24500
    },
    {
      "epoch": 1.5532494692480752,
      "grad_norm": 2.490994930267334,
      "learning_rate": 1.4504022015241322e-05,
      "loss": 0.8756,
      "step": 24510
    },
    {
      "epoch": 1.5538832028898253,
      "grad_norm": 2.395345687866211,
      "learning_rate": 1.4497671464860288e-05,
      "loss": 0.8554,
      "step": 24520
    },
    {
      "epoch": 1.5545169365315759,
      "grad_norm": 2.5726776123046875,
      "learning_rate": 1.4491320914479255e-05,
      "loss": 0.8801,
      "step": 24530
    },
    {
      "epoch": 1.5551506701733262,
      "grad_norm": 2.3606295585632324,
      "learning_rate": 1.4484970364098223e-05,
      "loss": 0.8238,
      "step": 24540
    },
    {
      "epoch": 1.5557844038150765,
      "grad_norm": 2.9291675090789795,
      "learning_rate": 1.4478619813717189e-05,
      "loss": 0.8692,
      "step": 24550
    },
    {
      "epoch": 1.5564181374568269,
      "grad_norm": 2.322585105895996,
      "learning_rate": 1.4472269263336157e-05,
      "loss": 0.8585,
      "step": 24560
    },
    {
      "epoch": 1.5570518710985772,
      "grad_norm": 2.604480743408203,
      "learning_rate": 1.4465918712955122e-05,
      "loss": 0.8298,
      "step": 24570
    },
    {
      "epoch": 1.5576856047403278,
      "grad_norm": 3.40154767036438,
      "learning_rate": 1.445956816257409e-05,
      "loss": 0.8787,
      "step": 24580
    },
    {
      "epoch": 1.558319338382078,
      "grad_norm": 2.4637298583984375,
      "learning_rate": 1.4453217612193058e-05,
      "loss": 0.8383,
      "step": 24590
    },
    {
      "epoch": 1.5589530720238285,
      "grad_norm": 3.4629740715026855,
      "learning_rate": 1.4446867061812023e-05,
      "loss": 0.8657,
      "step": 24600
    },
    {
      "epoch": 1.5595868056655786,
      "grad_norm": 2.3302948474884033,
      "learning_rate": 1.4440516511430991e-05,
      "loss": 0.8639,
      "step": 24610
    },
    {
      "epoch": 1.5602205393073292,
      "grad_norm": 2.698559045791626,
      "learning_rate": 1.443416596104996e-05,
      "loss": 0.89,
      "step": 24620
    },
    {
      "epoch": 1.5608542729490795,
      "grad_norm": 2.6988415718078613,
      "learning_rate": 1.4427815410668924e-05,
      "loss": 0.8695,
      "step": 24630
    },
    {
      "epoch": 1.5614880065908299,
      "grad_norm": 2.33427357673645,
      "learning_rate": 1.4421464860287892e-05,
      "loss": 0.8756,
      "step": 24640
    },
    {
      "epoch": 1.5621217402325802,
      "grad_norm": 3.0923779010772705,
      "learning_rate": 1.4415114309906858e-05,
      "loss": 0.8346,
      "step": 24650
    },
    {
      "epoch": 1.5627554738743306,
      "grad_norm": 2.7980260848999023,
      "learning_rate": 1.4408763759525827e-05,
      "loss": 0.8869,
      "step": 24660
    },
    {
      "epoch": 1.5633892075160811,
      "grad_norm": 3.083371162414551,
      "learning_rate": 1.4402413209144793e-05,
      "loss": 0.8483,
      "step": 24670
    },
    {
      "epoch": 1.5640229411578312,
      "grad_norm": 2.4025604724884033,
      "learning_rate": 1.439606265876376e-05,
      "loss": 0.8718,
      "step": 24680
    },
    {
      "epoch": 1.5646566747995818,
      "grad_norm": 2.2981715202331543,
      "learning_rate": 1.4389712108382728e-05,
      "loss": 0.8817,
      "step": 24690
    },
    {
      "epoch": 1.5652904084413322,
      "grad_norm": 2.715946674346924,
      "learning_rate": 1.4383361558001692e-05,
      "loss": 0.8116,
      "step": 24700
    },
    {
      "epoch": 1.5659241420830825,
      "grad_norm": 3.623058319091797,
      "learning_rate": 1.437701100762066e-05,
      "loss": 0.9217,
      "step": 24710
    },
    {
      "epoch": 1.5665578757248328,
      "grad_norm": 2.3409385681152344,
      "learning_rate": 1.4370660457239629e-05,
      "loss": 0.8281,
      "step": 24720
    },
    {
      "epoch": 1.5671916093665832,
      "grad_norm": 2.5679619312286377,
      "learning_rate": 1.4364309906858595e-05,
      "loss": 0.9071,
      "step": 24730
    },
    {
      "epoch": 1.5678253430083338,
      "grad_norm": 2.381565809249878,
      "learning_rate": 1.4357959356477561e-05,
      "loss": 0.8269,
      "step": 24740
    },
    {
      "epoch": 1.5684590766500839,
      "grad_norm": 2.7134082317352295,
      "learning_rate": 1.435160880609653e-05,
      "loss": 0.8756,
      "step": 24750
    },
    {
      "epoch": 1.5690928102918344,
      "grad_norm": 2.8314106464385986,
      "learning_rate": 1.4345258255715496e-05,
      "loss": 0.8198,
      "step": 24760
    },
    {
      "epoch": 1.5697265439335846,
      "grad_norm": 2.9722416400909424,
      "learning_rate": 1.4338907705334462e-05,
      "loss": 0.8758,
      "step": 24770
    },
    {
      "epoch": 1.5703602775753351,
      "grad_norm": 2.716586112976074,
      "learning_rate": 1.4332557154953429e-05,
      "loss": 0.8377,
      "step": 24780
    },
    {
      "epoch": 1.5709940112170855,
      "grad_norm": 2.3921194076538086,
      "learning_rate": 1.4326206604572397e-05,
      "loss": 0.8834,
      "step": 24790
    },
    {
      "epoch": 1.5716277448588358,
      "grad_norm": 2.7017602920532227,
      "learning_rate": 1.4319856054191365e-05,
      "loss": 0.8381,
      "step": 24800
    },
    {
      "epoch": 1.5722614785005862,
      "grad_norm": 2.413205862045288,
      "learning_rate": 1.431350550381033e-05,
      "loss": 0.8505,
      "step": 24810
    },
    {
      "epoch": 1.5728952121423365,
      "grad_norm": 2.483851432800293,
      "learning_rate": 1.4307154953429298e-05,
      "loss": 0.8379,
      "step": 24820
    },
    {
      "epoch": 1.573528945784087,
      "grad_norm": 2.701836585998535,
      "learning_rate": 1.4300804403048264e-05,
      "loss": 0.8908,
      "step": 24830
    },
    {
      "epoch": 1.5741626794258372,
      "grad_norm": 3.209691047668457,
      "learning_rate": 1.429445385266723e-05,
      "loss": 0.8463,
      "step": 24840
    },
    {
      "epoch": 1.5747964130675878,
      "grad_norm": 2.6619067192077637,
      "learning_rate": 1.4288103302286199e-05,
      "loss": 0.8239,
      "step": 24850
    },
    {
      "epoch": 1.5754301467093381,
      "grad_norm": 2.6832427978515625,
      "learning_rate": 1.4281752751905165e-05,
      "loss": 0.8635,
      "step": 24860
    },
    {
      "epoch": 1.5760638803510885,
      "grad_norm": 2.604170083999634,
      "learning_rate": 1.4275402201524133e-05,
      "loss": 0.8477,
      "step": 24870
    },
    {
      "epoch": 1.5766976139928388,
      "grad_norm": 2.5369327068328857,
      "learning_rate": 1.42690516511431e-05,
      "loss": 0.8679,
      "step": 24880
    },
    {
      "epoch": 1.5773313476345892,
      "grad_norm": 2.4963667392730713,
      "learning_rate": 1.4262701100762066e-05,
      "loss": 0.8309,
      "step": 24890
    },
    {
      "epoch": 1.5779650812763397,
      "grad_norm": 3.1787562370300293,
      "learning_rate": 1.4256350550381034e-05,
      "loss": 0.8639,
      "step": 24900
    },
    {
      "epoch": 1.5785988149180898,
      "grad_norm": 2.579044818878174,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 0.8692,
      "step": 24910
    },
    {
      "epoch": 1.5792325485598404,
      "grad_norm": 3.295654058456421,
      "learning_rate": 1.4243649449618967e-05,
      "loss": 0.8315,
      "step": 24920
    },
    {
      "epoch": 1.5798662822015905,
      "grad_norm": 2.390190601348877,
      "learning_rate": 1.4237298899237935e-05,
      "loss": 0.8547,
      "step": 24930
    },
    {
      "epoch": 1.580500015843341,
      "grad_norm": 2.705277919769287,
      "learning_rate": 1.42309483488569e-05,
      "loss": 0.8679,
      "step": 24940
    },
    {
      "epoch": 1.5811337494850914,
      "grad_norm": 3.160998821258545,
      "learning_rate": 1.4224597798475868e-05,
      "loss": 0.8995,
      "step": 24950
    },
    {
      "epoch": 1.5817674831268418,
      "grad_norm": 2.2054107189178467,
      "learning_rate": 1.4218247248094835e-05,
      "loss": 0.8649,
      "step": 24960
    },
    {
      "epoch": 1.5824012167685921,
      "grad_norm": 2.821397066116333,
      "learning_rate": 1.4211896697713803e-05,
      "loss": 0.8846,
      "step": 24970
    },
    {
      "epoch": 1.5830349504103425,
      "grad_norm": 2.645394802093506,
      "learning_rate": 1.420554614733277e-05,
      "loss": 0.8757,
      "step": 24980
    },
    {
      "epoch": 1.583668684052093,
      "grad_norm": 2.5387015342712402,
      "learning_rate": 1.4199195596951736e-05,
      "loss": 0.8411,
      "step": 24990
    },
    {
      "epoch": 1.5843024176938432,
      "grad_norm": 3.0822088718414307,
      "learning_rate": 1.4192845046570704e-05,
      "loss": 0.8365,
      "step": 25000
    },
    {
      "epoch": 1.5849361513355937,
      "grad_norm": 2.529374122619629,
      "learning_rate": 1.418649449618967e-05,
      "loss": 0.8469,
      "step": 25010
    },
    {
      "epoch": 1.5855698849773439,
      "grad_norm": 3.1131224632263184,
      "learning_rate": 1.4180143945808637e-05,
      "loss": 0.9237,
      "step": 25020
    },
    {
      "epoch": 1.5862036186190944,
      "grad_norm": 2.5964760780334473,
      "learning_rate": 1.4173793395427605e-05,
      "loss": 0.8525,
      "step": 25030
    },
    {
      "epoch": 1.5868373522608448,
      "grad_norm": 2.7635679244995117,
      "learning_rate": 1.4167442845046571e-05,
      "loss": 0.841,
      "step": 25040
    },
    {
      "epoch": 1.5874710859025951,
      "grad_norm": 2.682137966156006,
      "learning_rate": 1.4161092294665538e-05,
      "loss": 0.9011,
      "step": 25050
    },
    {
      "epoch": 1.5881048195443455,
      "grad_norm": 2.425232410430908,
      "learning_rate": 1.4154741744284506e-05,
      "loss": 0.8625,
      "step": 25060
    },
    {
      "epoch": 1.5887385531860958,
      "grad_norm": 3.122539758682251,
      "learning_rate": 1.4148391193903472e-05,
      "loss": 0.9197,
      "step": 25070
    },
    {
      "epoch": 1.5893722868278464,
      "grad_norm": 2.680910110473633,
      "learning_rate": 1.4142040643522439e-05,
      "loss": 0.8469,
      "step": 25080
    },
    {
      "epoch": 1.5900060204695965,
      "grad_norm": 2.5286035537719727,
      "learning_rate": 1.4135690093141405e-05,
      "loss": 0.8196,
      "step": 25090
    },
    {
      "epoch": 1.590639754111347,
      "grad_norm": 2.6953177452087402,
      "learning_rate": 1.4129339542760373e-05,
      "loss": 0.8219,
      "step": 25100
    },
    {
      "epoch": 1.5912734877530974,
      "grad_norm": 2.637305974960327,
      "learning_rate": 1.4122988992379341e-05,
      "loss": 0.8494,
      "step": 25110
    },
    {
      "epoch": 1.5919072213948477,
      "grad_norm": 2.4116125106811523,
      "learning_rate": 1.4116638441998306e-05,
      "loss": 0.8605,
      "step": 25120
    },
    {
      "epoch": 1.592540955036598,
      "grad_norm": 2.6713311672210693,
      "learning_rate": 1.4110287891617274e-05,
      "loss": 0.832,
      "step": 25130
    },
    {
      "epoch": 1.5931746886783484,
      "grad_norm": 2.852051019668579,
      "learning_rate": 1.4103937341236242e-05,
      "loss": 0.8533,
      "step": 25140
    },
    {
      "epoch": 1.593808422320099,
      "grad_norm": 2.7446842193603516,
      "learning_rate": 1.4097586790855207e-05,
      "loss": 0.8532,
      "step": 25150
    },
    {
      "epoch": 1.5944421559618491,
      "grad_norm": 2.939211845397949,
      "learning_rate": 1.4091236240474175e-05,
      "loss": 0.8917,
      "step": 25160
    },
    {
      "epoch": 1.5950758896035997,
      "grad_norm": 2.575002908706665,
      "learning_rate": 1.4084885690093141e-05,
      "loss": 0.8529,
      "step": 25170
    },
    {
      "epoch": 1.5957096232453498,
      "grad_norm": 2.5333268642425537,
      "learning_rate": 1.4078535139712108e-05,
      "loss": 0.8896,
      "step": 25180
    },
    {
      "epoch": 1.5963433568871004,
      "grad_norm": 2.5002214908599854,
      "learning_rate": 1.4072184589331076e-05,
      "loss": 0.8439,
      "step": 25190
    },
    {
      "epoch": 1.5969770905288507,
      "grad_norm": 2.6969525814056396,
      "learning_rate": 1.4065834038950042e-05,
      "loss": 0.8696,
      "step": 25200
    },
    {
      "epoch": 1.597610824170601,
      "grad_norm": 2.8540802001953125,
      "learning_rate": 1.405948348856901e-05,
      "loss": 0.8788,
      "step": 25210
    },
    {
      "epoch": 1.5982445578123514,
      "grad_norm": 2.9358530044555664,
      "learning_rate": 1.4053132938187975e-05,
      "loss": 0.8718,
      "step": 25220
    },
    {
      "epoch": 1.5988782914541018,
      "grad_norm": 2.5932607650756836,
      "learning_rate": 1.4046782387806943e-05,
      "loss": 0.835,
      "step": 25230
    },
    {
      "epoch": 1.5995120250958523,
      "grad_norm": 2.380373239517212,
      "learning_rate": 1.4040431837425912e-05,
      "loss": 0.8675,
      "step": 25240
    },
    {
      "epoch": 1.6001457587376025,
      "grad_norm": 2.556755781173706,
      "learning_rate": 1.4034081287044876e-05,
      "loss": 0.8824,
      "step": 25250
    },
    {
      "epoch": 1.600779492379353,
      "grad_norm": 2.7480013370513916,
      "learning_rate": 1.4027730736663844e-05,
      "loss": 0.8479,
      "step": 25260
    },
    {
      "epoch": 1.6014132260211034,
      "grad_norm": 3.24558424949646,
      "learning_rate": 1.4021380186282812e-05,
      "loss": 0.8448,
      "step": 25270
    },
    {
      "epoch": 1.6020469596628537,
      "grad_norm": 2.7497165203094482,
      "learning_rate": 1.4015029635901779e-05,
      "loss": 0.8496,
      "step": 25280
    },
    {
      "epoch": 1.602680693304604,
      "grad_norm": 2.390010356903076,
      "learning_rate": 1.4008679085520745e-05,
      "loss": 0.8789,
      "step": 25290
    },
    {
      "epoch": 1.6033144269463544,
      "grad_norm": 2.2774407863616943,
      "learning_rate": 1.4002328535139712e-05,
      "loss": 0.8219,
      "step": 25300
    },
    {
      "epoch": 1.603948160588105,
      "grad_norm": 2.4345390796661377,
      "learning_rate": 1.399597798475868e-05,
      "loss": 0.858,
      "step": 25310
    },
    {
      "epoch": 1.604581894229855,
      "grad_norm": 2.7962069511413574,
      "learning_rate": 1.3989627434377646e-05,
      "loss": 0.8529,
      "step": 25320
    },
    {
      "epoch": 1.6052156278716057,
      "grad_norm": 2.468231678009033,
      "learning_rate": 1.3983276883996613e-05,
      "loss": 0.8755,
      "step": 25330
    },
    {
      "epoch": 1.6058493615133558,
      "grad_norm": 2.731889009475708,
      "learning_rate": 1.3976926333615581e-05,
      "loss": 0.8555,
      "step": 25340
    },
    {
      "epoch": 1.6064830951551063,
      "grad_norm": 2.550109624862671,
      "learning_rate": 1.3970575783234547e-05,
      "loss": 0.8816,
      "step": 25350
    },
    {
      "epoch": 1.6071168287968567,
      "grad_norm": 2.455436944961548,
      "learning_rate": 1.3964225232853514e-05,
      "loss": 0.8573,
      "step": 25360
    },
    {
      "epoch": 1.607750562438607,
      "grad_norm": 2.760565996170044,
      "learning_rate": 1.3957874682472482e-05,
      "loss": 0.8365,
      "step": 25370
    },
    {
      "epoch": 1.6083842960803574,
      "grad_norm": 2.877558946609497,
      "learning_rate": 1.3951524132091448e-05,
      "loss": 0.8462,
      "step": 25380
    },
    {
      "epoch": 1.6090180297221077,
      "grad_norm": 2.446840763092041,
      "learning_rate": 1.3945173581710415e-05,
      "loss": 0.8258,
      "step": 25390
    },
    {
      "epoch": 1.6096517633638583,
      "grad_norm": 2.6740775108337402,
      "learning_rate": 1.3938823031329383e-05,
      "loss": 0.8549,
      "step": 25400
    },
    {
      "epoch": 1.6102854970056084,
      "grad_norm": 2.422863245010376,
      "learning_rate": 1.393247248094835e-05,
      "loss": 0.8665,
      "step": 25410
    },
    {
      "epoch": 1.610919230647359,
      "grad_norm": 2.582218647003174,
      "learning_rate": 1.3926121930567317e-05,
      "loss": 0.8547,
      "step": 25420
    },
    {
      "epoch": 1.6115529642891093,
      "grad_norm": 2.5022504329681396,
      "learning_rate": 1.3919771380186282e-05,
      "loss": 0.8341,
      "step": 25430
    },
    {
      "epoch": 1.6121866979308597,
      "grad_norm": 2.3172943592071533,
      "learning_rate": 1.391342082980525e-05,
      "loss": 0.8719,
      "step": 25440
    },
    {
      "epoch": 1.61282043157261,
      "grad_norm": 2.8941521644592285,
      "learning_rate": 1.3907070279424218e-05,
      "loss": 0.893,
      "step": 25450
    },
    {
      "epoch": 1.6134541652143604,
      "grad_norm": 2.858907461166382,
      "learning_rate": 1.3900719729043183e-05,
      "loss": 0.9177,
      "step": 25460
    },
    {
      "epoch": 1.614087898856111,
      "grad_norm": 2.490572214126587,
      "learning_rate": 1.3894369178662151e-05,
      "loss": 0.8366,
      "step": 25470
    },
    {
      "epoch": 1.614721632497861,
      "grad_norm": 2.4106924533843994,
      "learning_rate": 1.3888018628281118e-05,
      "loss": 0.8452,
      "step": 25480
    },
    {
      "epoch": 1.6153553661396116,
      "grad_norm": 2.8611772060394287,
      "learning_rate": 1.3881668077900084e-05,
      "loss": 0.8649,
      "step": 25490
    },
    {
      "epoch": 1.6159890997813617,
      "grad_norm": 2.789951801300049,
      "learning_rate": 1.3875317527519052e-05,
      "loss": 0.8753,
      "step": 25500
    },
    {
      "epoch": 1.6166228334231123,
      "grad_norm": 2.787933349609375,
      "learning_rate": 1.3868966977138019e-05,
      "loss": 0.8318,
      "step": 25510
    },
    {
      "epoch": 1.6172565670648626,
      "grad_norm": 2.5602571964263916,
      "learning_rate": 1.3862616426756987e-05,
      "loss": 0.8866,
      "step": 25520
    },
    {
      "epoch": 1.617890300706613,
      "grad_norm": 2.973036527633667,
      "learning_rate": 1.3856265876375953e-05,
      "loss": 0.8095,
      "step": 25530
    },
    {
      "epoch": 1.6185240343483633,
      "grad_norm": 2.76798415184021,
      "learning_rate": 1.384991532599492e-05,
      "loss": 0.8322,
      "step": 25540
    },
    {
      "epoch": 1.6191577679901137,
      "grad_norm": 2.642317295074463,
      "learning_rate": 1.3843564775613888e-05,
      "loss": 0.8988,
      "step": 25550
    },
    {
      "epoch": 1.6197915016318643,
      "grad_norm": 2.5867183208465576,
      "learning_rate": 1.3837214225232852e-05,
      "loss": 0.8305,
      "step": 25560
    },
    {
      "epoch": 1.6204252352736144,
      "grad_norm": 2.76914644241333,
      "learning_rate": 1.383086367485182e-05,
      "loss": 0.8629,
      "step": 25570
    },
    {
      "epoch": 1.621058968915365,
      "grad_norm": 2.5805420875549316,
      "learning_rate": 1.3824513124470789e-05,
      "loss": 0.8658,
      "step": 25580
    },
    {
      "epoch": 1.6216927025571153,
      "grad_norm": 2.692185640335083,
      "learning_rate": 1.3818162574089755e-05,
      "loss": 0.8233,
      "step": 25590
    },
    {
      "epoch": 1.6223264361988656,
      "grad_norm": 2.358825206756592,
      "learning_rate": 1.3811812023708722e-05,
      "loss": 0.8546,
      "step": 25600
    },
    {
      "epoch": 1.622960169840616,
      "grad_norm": 2.6242451667785645,
      "learning_rate": 1.3805461473327688e-05,
      "loss": 0.8639,
      "step": 25610
    },
    {
      "epoch": 1.6235939034823663,
      "grad_norm": 2.431313991546631,
      "learning_rate": 1.3799110922946656e-05,
      "loss": 0.8785,
      "step": 25620
    },
    {
      "epoch": 1.6242276371241169,
      "grad_norm": 2.8159542083740234,
      "learning_rate": 1.3792760372565622e-05,
      "loss": 0.7891,
      "step": 25630
    },
    {
      "epoch": 1.624861370765867,
      "grad_norm": 2.936877965927124,
      "learning_rate": 1.3786409822184589e-05,
      "loss": 0.9305,
      "step": 25640
    },
    {
      "epoch": 1.6254951044076176,
      "grad_norm": 2.5513455867767334,
      "learning_rate": 1.3780059271803557e-05,
      "loss": 0.8469,
      "step": 25650
    },
    {
      "epoch": 1.6261288380493677,
      "grad_norm": 3.1206443309783936,
      "learning_rate": 1.3773708721422525e-05,
      "loss": 0.8966,
      "step": 25660
    },
    {
      "epoch": 1.6267625716911183,
      "grad_norm": 2.7283780574798584,
      "learning_rate": 1.376735817104149e-05,
      "loss": 0.8537,
      "step": 25670
    },
    {
      "epoch": 1.6273963053328686,
      "grad_norm": 2.912442445755005,
      "learning_rate": 1.3761007620660458e-05,
      "loss": 0.8321,
      "step": 25680
    },
    {
      "epoch": 1.628030038974619,
      "grad_norm": 2.6549925804138184,
      "learning_rate": 1.3754657070279424e-05,
      "loss": 0.8564,
      "step": 25690
    },
    {
      "epoch": 1.6286637726163693,
      "grad_norm": 2.4229066371917725,
      "learning_rate": 1.3748306519898391e-05,
      "loss": 0.8184,
      "step": 25700
    },
    {
      "epoch": 1.6292975062581196,
      "grad_norm": 2.3320107460021973,
      "learning_rate": 1.3741955969517359e-05,
      "loss": 0.8928,
      "step": 25710
    },
    {
      "epoch": 1.6299312398998702,
      "grad_norm": 2.6918632984161377,
      "learning_rate": 1.3735605419136325e-05,
      "loss": 0.8155,
      "step": 25720
    },
    {
      "epoch": 1.6305649735416203,
      "grad_norm": 2.9306581020355225,
      "learning_rate": 1.3729254868755292e-05,
      "loss": 0.8689,
      "step": 25730
    },
    {
      "epoch": 1.631198707183371,
      "grad_norm": 3.3423376083374023,
      "learning_rate": 1.3722904318374258e-05,
      "loss": 0.9018,
      "step": 25740
    },
    {
      "epoch": 1.6318324408251212,
      "grad_norm": 2.596595525741577,
      "learning_rate": 1.3716553767993226e-05,
      "loss": 0.8514,
      "step": 25750
    },
    {
      "epoch": 1.6324661744668716,
      "grad_norm": 2.461076498031616,
      "learning_rate": 1.3710203217612195e-05,
      "loss": 0.8325,
      "step": 25760
    },
    {
      "epoch": 1.633099908108622,
      "grad_norm": 2.9615890979766846,
      "learning_rate": 1.370385266723116e-05,
      "loss": 0.8916,
      "step": 25770
    },
    {
      "epoch": 1.6337336417503723,
      "grad_norm": 2.669727325439453,
      "learning_rate": 1.3697502116850127e-05,
      "loss": 0.8094,
      "step": 25780
    },
    {
      "epoch": 1.6343673753921228,
      "grad_norm": 2.486431837081909,
      "learning_rate": 1.3691151566469095e-05,
      "loss": 0.8375,
      "step": 25790
    },
    {
      "epoch": 1.635001109033873,
      "grad_norm": 2.9024698734283447,
      "learning_rate": 1.368480101608806e-05,
      "loss": 0.8569,
      "step": 25800
    },
    {
      "epoch": 1.6356348426756235,
      "grad_norm": 2.424361228942871,
      "learning_rate": 1.3678450465707028e-05,
      "loss": 0.8704,
      "step": 25810
    },
    {
      "epoch": 1.6362685763173737,
      "grad_norm": 2.562859296798706,
      "learning_rate": 1.3672099915325995e-05,
      "loss": 0.8749,
      "step": 25820
    },
    {
      "epoch": 1.6369023099591242,
      "grad_norm": 2.5408935546875,
      "learning_rate": 1.3665749364944963e-05,
      "loss": 0.8766,
      "step": 25830
    },
    {
      "epoch": 1.6375360436008746,
      "grad_norm": 2.340787172317505,
      "learning_rate": 1.365939881456393e-05,
      "loss": 0.8486,
      "step": 25840
    },
    {
      "epoch": 1.638169777242625,
      "grad_norm": 2.8259739875793457,
      "learning_rate": 1.3653048264182896e-05,
      "loss": 0.8437,
      "step": 25850
    },
    {
      "epoch": 1.6388035108843753,
      "grad_norm": 2.995422601699829,
      "learning_rate": 1.3646697713801864e-05,
      "loss": 0.8787,
      "step": 25860
    },
    {
      "epoch": 1.6394372445261256,
      "grad_norm": 2.642387866973877,
      "learning_rate": 1.3640347163420829e-05,
      "loss": 0.8547,
      "step": 25870
    },
    {
      "epoch": 1.6400709781678762,
      "grad_norm": 2.322148084640503,
      "learning_rate": 1.3633996613039797e-05,
      "loss": 0.8316,
      "step": 25880
    },
    {
      "epoch": 1.6407047118096263,
      "grad_norm": 2.615710973739624,
      "learning_rate": 1.3627646062658765e-05,
      "loss": 0.8497,
      "step": 25890
    },
    {
      "epoch": 1.6413384454513769,
      "grad_norm": 2.725409507751465,
      "learning_rate": 1.3621295512277731e-05,
      "loss": 0.8954,
      "step": 25900
    },
    {
      "epoch": 1.6419721790931272,
      "grad_norm": 2.8374686241149902,
      "learning_rate": 1.3614944961896698e-05,
      "loss": 0.8404,
      "step": 25910
    },
    {
      "epoch": 1.6426059127348775,
      "grad_norm": 2.8518996238708496,
      "learning_rate": 1.3608594411515666e-05,
      "loss": 0.8363,
      "step": 25920
    },
    {
      "epoch": 1.643239646376628,
      "grad_norm": 2.3164727687835693,
      "learning_rate": 1.3602243861134632e-05,
      "loss": 0.8135,
      "step": 25930
    },
    {
      "epoch": 1.6438733800183782,
      "grad_norm": 2.6492726802825928,
      "learning_rate": 1.3595893310753599e-05,
      "loss": 0.8416,
      "step": 25940
    },
    {
      "epoch": 1.6445071136601288,
      "grad_norm": 3.329648733139038,
      "learning_rate": 1.3589542760372565e-05,
      "loss": 0.8557,
      "step": 25950
    },
    {
      "epoch": 1.645140847301879,
      "grad_norm": 2.3031017780303955,
      "learning_rate": 1.3583192209991533e-05,
      "loss": 0.8439,
      "step": 25960
    },
    {
      "epoch": 1.6457745809436295,
      "grad_norm": 2.6054530143737793,
      "learning_rate": 1.3576841659610501e-05,
      "loss": 0.8488,
      "step": 25970
    },
    {
      "epoch": 1.6464083145853796,
      "grad_norm": 2.510424852371216,
      "learning_rate": 1.3570491109229466e-05,
      "loss": 0.8961,
      "step": 25980
    },
    {
      "epoch": 1.6470420482271302,
      "grad_norm": 2.668912410736084,
      "learning_rate": 1.3564140558848434e-05,
      "loss": 0.8208,
      "step": 25990
    },
    {
      "epoch": 1.6476757818688805,
      "grad_norm": 2.6936299800872803,
      "learning_rate": 1.35577900084674e-05,
      "loss": 0.8326,
      "step": 26000
    },
    {
      "epoch": 1.6483095155106309,
      "grad_norm": 2.4339916706085205,
      "learning_rate": 1.3551439458086367e-05,
      "loss": 0.8515,
      "step": 26010
    },
    {
      "epoch": 1.6489432491523812,
      "grad_norm": 2.7430737018585205,
      "learning_rate": 1.3545088907705335e-05,
      "loss": 0.8928,
      "step": 26020
    },
    {
      "epoch": 1.6495769827941316,
      "grad_norm": 2.405402421951294,
      "learning_rate": 1.3538738357324302e-05,
      "loss": 0.8525,
      "step": 26030
    },
    {
      "epoch": 1.6502107164358821,
      "grad_norm": 2.414141893386841,
      "learning_rate": 1.3532387806943268e-05,
      "loss": 0.8367,
      "step": 26040
    },
    {
      "epoch": 1.6508444500776323,
      "grad_norm": 2.822408676147461,
      "learning_rate": 1.3526037256562236e-05,
      "loss": 0.8491,
      "step": 26050
    },
    {
      "epoch": 1.6514781837193828,
      "grad_norm": 2.70902156829834,
      "learning_rate": 1.3519686706181203e-05,
      "loss": 0.8558,
      "step": 26060
    },
    {
      "epoch": 1.6521119173611332,
      "grad_norm": 2.2245066165924072,
      "learning_rate": 1.351333615580017e-05,
      "loss": 0.8558,
      "step": 26070
    },
    {
      "epoch": 1.6527456510028835,
      "grad_norm": 2.776991605758667,
      "learning_rate": 1.3506985605419135e-05,
      "loss": 0.8457,
      "step": 26080
    },
    {
      "epoch": 1.6533793846446339,
      "grad_norm": 2.2074639797210693,
      "learning_rate": 1.3500635055038104e-05,
      "loss": 0.8521,
      "step": 26090
    },
    {
      "epoch": 1.6540131182863842,
      "grad_norm": 2.8250420093536377,
      "learning_rate": 1.3494284504657072e-05,
      "loss": 0.8572,
      "step": 26100
    },
    {
      "epoch": 1.6546468519281348,
      "grad_norm": 2.6809000968933105,
      "learning_rate": 1.3487933954276036e-05,
      "loss": 0.8396,
      "step": 26110
    },
    {
      "epoch": 1.6552805855698849,
      "grad_norm": 2.4115350246429443,
      "learning_rate": 1.3481583403895005e-05,
      "loss": 0.8625,
      "step": 26120
    },
    {
      "epoch": 1.6559143192116355,
      "grad_norm": 2.484724521636963,
      "learning_rate": 1.3475232853513973e-05,
      "loss": 0.8473,
      "step": 26130
    },
    {
      "epoch": 1.6565480528533856,
      "grad_norm": 3.099447011947632,
      "learning_rate": 1.3468882303132939e-05,
      "loss": 0.8552,
      "step": 26140
    },
    {
      "epoch": 1.6571817864951361,
      "grad_norm": 2.763742446899414,
      "learning_rate": 1.3462531752751905e-05,
      "loss": 0.8649,
      "step": 26150
    },
    {
      "epoch": 1.6578155201368865,
      "grad_norm": 2.8538522720336914,
      "learning_rate": 1.3456181202370872e-05,
      "loss": 0.8941,
      "step": 26160
    },
    {
      "epoch": 1.6584492537786368,
      "grad_norm": 2.8525757789611816,
      "learning_rate": 1.344983065198984e-05,
      "loss": 0.8454,
      "step": 26170
    },
    {
      "epoch": 1.6590829874203872,
      "grad_norm": 2.59674072265625,
      "learning_rate": 1.3443480101608806e-05,
      "loss": 0.8826,
      "step": 26180
    },
    {
      "epoch": 1.6597167210621375,
      "grad_norm": 2.904322385787964,
      "learning_rate": 1.3437129551227773e-05,
      "loss": 0.8718,
      "step": 26190
    },
    {
      "epoch": 1.660350454703888,
      "grad_norm": 2.5588114261627197,
      "learning_rate": 1.3430779000846741e-05,
      "loss": 0.8669,
      "step": 26200
    },
    {
      "epoch": 1.6609841883456382,
      "grad_norm": 2.6852221488952637,
      "learning_rate": 1.3424428450465707e-05,
      "loss": 0.8668,
      "step": 26210
    },
    {
      "epoch": 1.6616179219873888,
      "grad_norm": 2.7906546592712402,
      "learning_rate": 1.3418077900084674e-05,
      "loss": 0.8599,
      "step": 26220
    },
    {
      "epoch": 1.6622516556291391,
      "grad_norm": 2.5782599449157715,
      "learning_rate": 1.3411727349703642e-05,
      "loss": 0.8599,
      "step": 26230
    },
    {
      "epoch": 1.6628853892708895,
      "grad_norm": 3.3942525386810303,
      "learning_rate": 1.3405376799322608e-05,
      "loss": 0.8815,
      "step": 26240
    },
    {
      "epoch": 1.6635191229126398,
      "grad_norm": 2.8793365955352783,
      "learning_rate": 1.3399026248941575e-05,
      "loss": 0.8322,
      "step": 26250
    },
    {
      "epoch": 1.6641528565543902,
      "grad_norm": 2.70548939704895,
      "learning_rate": 1.3392675698560543e-05,
      "loss": 0.8692,
      "step": 26260
    },
    {
      "epoch": 1.6647865901961407,
      "grad_norm": 2.5477848052978516,
      "learning_rate": 1.338632514817951e-05,
      "loss": 0.8376,
      "step": 26270
    },
    {
      "epoch": 1.6654203238378908,
      "grad_norm": 3.25447154045105,
      "learning_rate": 1.3379974597798476e-05,
      "loss": 0.8568,
      "step": 26280
    },
    {
      "epoch": 1.6660540574796414,
      "grad_norm": 2.4479129314422607,
      "learning_rate": 1.3374259102455548e-05,
      "loss": 0.8193,
      "step": 26290
    },
    {
      "epoch": 1.6666877911213915,
      "grad_norm": 2.7410874366760254,
      "learning_rate": 1.3367908552074513e-05,
      "loss": 0.8151,
      "step": 26300
    },
    {
      "epoch": 1.667321524763142,
      "grad_norm": 2.63145112991333,
      "learning_rate": 1.336155800169348e-05,
      "loss": 0.8347,
      "step": 26310
    },
    {
      "epoch": 1.6679552584048924,
      "grad_norm": 2.4044859409332275,
      "learning_rate": 1.3355207451312447e-05,
      "loss": 0.8266,
      "step": 26320
    },
    {
      "epoch": 1.6685889920466428,
      "grad_norm": 2.5726664066314697,
      "learning_rate": 1.3348856900931414e-05,
      "loss": 0.8408,
      "step": 26330
    },
    {
      "epoch": 1.6692227256883931,
      "grad_norm": 3.136880397796631,
      "learning_rate": 1.3342506350550382e-05,
      "loss": 0.8987,
      "step": 26340
    },
    {
      "epoch": 1.6698564593301435,
      "grad_norm": 3.96025013923645,
      "learning_rate": 1.3336155800169348e-05,
      "loss": 0.8662,
      "step": 26350
    },
    {
      "epoch": 1.670490192971894,
      "grad_norm": 2.4713616371154785,
      "learning_rate": 1.3329805249788316e-05,
      "loss": 0.8705,
      "step": 26360
    },
    {
      "epoch": 1.6711239266136442,
      "grad_norm": 3.1307218074798584,
      "learning_rate": 1.3323454699407283e-05,
      "loss": 0.8244,
      "step": 26370
    },
    {
      "epoch": 1.6717576602553947,
      "grad_norm": 2.5050792694091797,
      "learning_rate": 1.331710414902625e-05,
      "loss": 0.8845,
      "step": 26380
    },
    {
      "epoch": 1.672391393897145,
      "grad_norm": 2.3347365856170654,
      "learning_rate": 1.3310753598645217e-05,
      "loss": 0.8443,
      "step": 26390
    },
    {
      "epoch": 1.6730251275388954,
      "grad_norm": 2.6031010150909424,
      "learning_rate": 1.3304403048264182e-05,
      "loss": 0.8761,
      "step": 26400
    },
    {
      "epoch": 1.6736588611806458,
      "grad_norm": 2.605767250061035,
      "learning_rate": 1.329805249788315e-05,
      "loss": 0.83,
      "step": 26410
    },
    {
      "epoch": 1.6742925948223961,
      "grad_norm": 2.674783945083618,
      "learning_rate": 1.3291701947502118e-05,
      "loss": 0.8888,
      "step": 26420
    },
    {
      "epoch": 1.6749263284641467,
      "grad_norm": 2.803154706954956,
      "learning_rate": 1.3285351397121083e-05,
      "loss": 0.848,
      "step": 26430
    },
    {
      "epoch": 1.6755600621058968,
      "grad_norm": 2.5226473808288574,
      "learning_rate": 1.3279000846740051e-05,
      "loss": 0.8762,
      "step": 26440
    },
    {
      "epoch": 1.6761937957476474,
      "grad_norm": 2.473724603652954,
      "learning_rate": 1.3272650296359018e-05,
      "loss": 0.8522,
      "step": 26450
    },
    {
      "epoch": 1.6768275293893975,
      "grad_norm": 2.2013092041015625,
      "learning_rate": 1.3266299745977986e-05,
      "loss": 0.8798,
      "step": 26460
    },
    {
      "epoch": 1.677461263031148,
      "grad_norm": 2.4215564727783203,
      "learning_rate": 1.3259949195596952e-05,
      "loss": 0.8512,
      "step": 26470
    },
    {
      "epoch": 1.6780949966728984,
      "grad_norm": 2.781961679458618,
      "learning_rate": 1.3253598645215919e-05,
      "loss": 0.8764,
      "step": 26480
    },
    {
      "epoch": 1.6787287303146488,
      "grad_norm": 2.5581703186035156,
      "learning_rate": 1.3247248094834887e-05,
      "loss": 0.8082,
      "step": 26490
    },
    {
      "epoch": 1.679362463956399,
      "grad_norm": 2.5283820629119873,
      "learning_rate": 1.3240897544453853e-05,
      "loss": 0.8733,
      "step": 26500
    },
    {
      "epoch": 1.6799961975981494,
      "grad_norm": 2.4887213706970215,
      "learning_rate": 1.323454699407282e-05,
      "loss": 0.8695,
      "step": 26510
    },
    {
      "epoch": 1.6806299312399,
      "grad_norm": 2.769251585006714,
      "learning_rate": 1.3228196443691788e-05,
      "loss": 0.8537,
      "step": 26520
    },
    {
      "epoch": 1.6812636648816501,
      "grad_norm": 2.1193065643310547,
      "learning_rate": 1.3221845893310754e-05,
      "loss": 0.8025,
      "step": 26530
    },
    {
      "epoch": 1.6818973985234007,
      "grad_norm": 2.7170538902282715,
      "learning_rate": 1.321549534292972e-05,
      "loss": 0.8533,
      "step": 26540
    },
    {
      "epoch": 1.6825311321651508,
      "grad_norm": 2.752826690673828,
      "learning_rate": 1.3209144792548689e-05,
      "loss": 0.8408,
      "step": 26550
    },
    {
      "epoch": 1.6831648658069014,
      "grad_norm": 2.96510648727417,
      "learning_rate": 1.3202794242167655e-05,
      "loss": 0.9021,
      "step": 26560
    },
    {
      "epoch": 1.6837985994486517,
      "grad_norm": 2.6673460006713867,
      "learning_rate": 1.3196443691786621e-05,
      "loss": 0.8489,
      "step": 26570
    },
    {
      "epoch": 1.684432333090402,
      "grad_norm": 2.2680160999298096,
      "learning_rate": 1.3190093141405588e-05,
      "loss": 0.8477,
      "step": 26580
    },
    {
      "epoch": 1.6850660667321524,
      "grad_norm": 2.535893201828003,
      "learning_rate": 1.3183742591024556e-05,
      "loss": 0.8259,
      "step": 26590
    },
    {
      "epoch": 1.6856998003739028,
      "grad_norm": 2.669093370437622,
      "learning_rate": 1.3177392040643524e-05,
      "loss": 0.8515,
      "step": 26600
    },
    {
      "epoch": 1.6863335340156533,
      "grad_norm": 2.183600664138794,
      "learning_rate": 1.3171041490262489e-05,
      "loss": 0.8531,
      "step": 26610
    },
    {
      "epoch": 1.6869672676574035,
      "grad_norm": 2.722769021987915,
      "learning_rate": 1.3164690939881457e-05,
      "loss": 0.8746,
      "step": 26620
    },
    {
      "epoch": 1.687601001299154,
      "grad_norm": 2.5916380882263184,
      "learning_rate": 1.3158340389500425e-05,
      "loss": 0.8571,
      "step": 26630
    },
    {
      "epoch": 1.6882347349409044,
      "grad_norm": 2.7352235317230225,
      "learning_rate": 1.315198983911939e-05,
      "loss": 0.8175,
      "step": 26640
    },
    {
      "epoch": 1.6888684685826547,
      "grad_norm": 2.7156476974487305,
      "learning_rate": 1.3145639288738358e-05,
      "loss": 0.8958,
      "step": 26650
    },
    {
      "epoch": 1.689502202224405,
      "grad_norm": 2.56710147857666,
      "learning_rate": 1.3139288738357324e-05,
      "loss": 0.8289,
      "step": 26660
    },
    {
      "epoch": 1.6901359358661554,
      "grad_norm": 2.46662974357605,
      "learning_rate": 1.313293818797629e-05,
      "loss": 0.8191,
      "step": 26670
    },
    {
      "epoch": 1.690769669507906,
      "grad_norm": 2.5977208614349365,
      "learning_rate": 1.3126587637595259e-05,
      "loss": 0.8668,
      "step": 26680
    },
    {
      "epoch": 1.691403403149656,
      "grad_norm": 2.691500425338745,
      "learning_rate": 1.3120237087214225e-05,
      "loss": 0.8749,
      "step": 26690
    },
    {
      "epoch": 1.6920371367914067,
      "grad_norm": 2.8061280250549316,
      "learning_rate": 1.3113886536833193e-05,
      "loss": 0.8397,
      "step": 26700
    },
    {
      "epoch": 1.6926708704331568,
      "grad_norm": 2.873795747756958,
      "learning_rate": 1.3107535986452158e-05,
      "loss": 0.917,
      "step": 26710
    },
    {
      "epoch": 1.6933046040749073,
      "grad_norm": 3.0553886890411377,
      "learning_rate": 1.3101185436071126e-05,
      "loss": 0.8891,
      "step": 26720
    },
    {
      "epoch": 1.6939383377166577,
      "grad_norm": 2.645271062850952,
      "learning_rate": 1.3094834885690094e-05,
      "loss": 0.8784,
      "step": 26730
    },
    {
      "epoch": 1.694572071358408,
      "grad_norm": 2.7967498302459717,
      "learning_rate": 1.308848433530906e-05,
      "loss": 0.8679,
      "step": 26740
    },
    {
      "epoch": 1.6952058050001584,
      "grad_norm": 2.5386922359466553,
      "learning_rate": 1.3082133784928027e-05,
      "loss": 0.8721,
      "step": 26750
    },
    {
      "epoch": 1.6958395386419087,
      "grad_norm": 2.8854031562805176,
      "learning_rate": 1.3075783234546995e-05,
      "loss": 0.8797,
      "step": 26760
    },
    {
      "epoch": 1.6964732722836593,
      "grad_norm": 3.40995454788208,
      "learning_rate": 1.3069432684165962e-05,
      "loss": 0.8866,
      "step": 26770
    },
    {
      "epoch": 1.6971070059254094,
      "grad_norm": 2.815756320953369,
      "learning_rate": 1.3063082133784928e-05,
      "loss": 0.833,
      "step": 26780
    },
    {
      "epoch": 1.69774073956716,
      "grad_norm": 2.2986326217651367,
      "learning_rate": 1.3056731583403895e-05,
      "loss": 0.8319,
      "step": 26790
    },
    {
      "epoch": 1.6983744732089103,
      "grad_norm": 2.2835934162139893,
      "learning_rate": 1.3050381033022863e-05,
      "loss": 0.8418,
      "step": 26800
    },
    {
      "epoch": 1.6990082068506607,
      "grad_norm": 2.664490222930908,
      "learning_rate": 1.304403048264183e-05,
      "loss": 0.8544,
      "step": 26810
    },
    {
      "epoch": 1.699641940492411,
      "grad_norm": 2.5323472023010254,
      "learning_rate": 1.3037679932260796e-05,
      "loss": 0.8539,
      "step": 26820
    },
    {
      "epoch": 1.7002756741341614,
      "grad_norm": 2.334961175918579,
      "learning_rate": 1.3031329381879764e-05,
      "loss": 0.8316,
      "step": 26830
    },
    {
      "epoch": 1.700909407775912,
      "grad_norm": 2.7987523078918457,
      "learning_rate": 1.302497883149873e-05,
      "loss": 0.835,
      "step": 26840
    },
    {
      "epoch": 1.701543141417662,
      "grad_norm": 2.121335506439209,
      "learning_rate": 1.3018628281117697e-05,
      "loss": 0.8564,
      "step": 26850
    },
    {
      "epoch": 1.7021768750594126,
      "grad_norm": 2.3184456825256348,
      "learning_rate": 1.3012277730736665e-05,
      "loss": 0.8277,
      "step": 26860
    },
    {
      "epoch": 1.7028106087011627,
      "grad_norm": 2.5705435276031494,
      "learning_rate": 1.3005927180355631e-05,
      "loss": 0.8346,
      "step": 26870
    },
    {
      "epoch": 1.7034443423429133,
      "grad_norm": 2.4951517581939697,
      "learning_rate": 1.2999576629974598e-05,
      "loss": 0.8361,
      "step": 26880
    },
    {
      "epoch": 1.7040780759846637,
      "grad_norm": 2.384519577026367,
      "learning_rate": 1.2993226079593566e-05,
      "loss": 0.8737,
      "step": 26890
    },
    {
      "epoch": 1.704711809626414,
      "grad_norm": 2.64089298248291,
      "learning_rate": 1.2986875529212532e-05,
      "loss": 0.8731,
      "step": 26900
    },
    {
      "epoch": 1.7053455432681643,
      "grad_norm": 3.173902750015259,
      "learning_rate": 1.29805249788315e-05,
      "loss": 0.8929,
      "step": 26910
    },
    {
      "epoch": 1.7059792769099147,
      "grad_norm": 2.6951637268066406,
      "learning_rate": 1.2974174428450465e-05,
      "loss": 0.8517,
      "step": 26920
    },
    {
      "epoch": 1.7066130105516653,
      "grad_norm": 2.3016300201416016,
      "learning_rate": 1.2967823878069433e-05,
      "loss": 0.9065,
      "step": 26930
    },
    {
      "epoch": 1.7072467441934154,
      "grad_norm": 2.5321874618530273,
      "learning_rate": 1.2961473327688401e-05,
      "loss": 0.8206,
      "step": 26940
    },
    {
      "epoch": 1.707880477835166,
      "grad_norm": 2.8224503993988037,
      "learning_rate": 1.2955122777307366e-05,
      "loss": 0.8976,
      "step": 26950
    },
    {
      "epoch": 1.7085142114769163,
      "grad_norm": 2.7181825637817383,
      "learning_rate": 1.2948772226926334e-05,
      "loss": 0.8719,
      "step": 26960
    },
    {
      "epoch": 1.7091479451186666,
      "grad_norm": 2.795041084289551,
      "learning_rate": 1.29424216765453e-05,
      "loss": 0.8415,
      "step": 26970
    },
    {
      "epoch": 1.709781678760417,
      "grad_norm": 2.700767993927002,
      "learning_rate": 1.2936071126164267e-05,
      "loss": 0.8664,
      "step": 26980
    },
    {
      "epoch": 1.7104154124021673,
      "grad_norm": 2.6688666343688965,
      "learning_rate": 1.2929720575783235e-05,
      "loss": 0.8433,
      "step": 26990
    },
    {
      "epoch": 1.711049146043918,
      "grad_norm": 2.7893340587615967,
      "learning_rate": 1.2923370025402202e-05,
      "loss": 0.8238,
      "step": 27000
    },
    {
      "epoch": 1.711682879685668,
      "grad_norm": 2.3358983993530273,
      "learning_rate": 1.291701947502117e-05,
      "loss": 0.8442,
      "step": 27010
    },
    {
      "epoch": 1.7123166133274186,
      "grad_norm": 3.013233184814453,
      "learning_rate": 1.2910668924640136e-05,
      "loss": 0.8385,
      "step": 27020
    },
    {
      "epoch": 1.7129503469691687,
      "grad_norm": 2.6875245571136475,
      "learning_rate": 1.2904318374259103e-05,
      "loss": 0.9122,
      "step": 27030
    },
    {
      "epoch": 1.7135840806109193,
      "grad_norm": 2.7844226360321045,
      "learning_rate": 1.289796782387807e-05,
      "loss": 0.8668,
      "step": 27040
    },
    {
      "epoch": 1.7142178142526696,
      "grad_norm": 2.9396204948425293,
      "learning_rate": 1.2891617273497035e-05,
      "loss": 0.8233,
      "step": 27050
    },
    {
      "epoch": 1.71485154789442,
      "grad_norm": 3.191638708114624,
      "learning_rate": 1.2885266723116003e-05,
      "loss": 0.8799,
      "step": 27060
    },
    {
      "epoch": 1.7154852815361703,
      "grad_norm": 2.5335874557495117,
      "learning_rate": 1.2878916172734972e-05,
      "loss": 0.835,
      "step": 27070
    },
    {
      "epoch": 1.7161190151779206,
      "grad_norm": 2.2017500400543213,
      "learning_rate": 1.2872565622353938e-05,
      "loss": 0.8264,
      "step": 27080
    },
    {
      "epoch": 1.7167527488196712,
      "grad_norm": 2.545053005218506,
      "learning_rate": 1.2866215071972904e-05,
      "loss": 0.8665,
      "step": 27090
    },
    {
      "epoch": 1.7173864824614213,
      "grad_norm": 2.7925093173980713,
      "learning_rate": 1.2859864521591871e-05,
      "loss": 0.8551,
      "step": 27100
    },
    {
      "epoch": 1.718020216103172,
      "grad_norm": 2.729670286178589,
      "learning_rate": 1.2853513971210839e-05,
      "loss": 0.8404,
      "step": 27110
    },
    {
      "epoch": 1.7186539497449222,
      "grad_norm": 2.7374887466430664,
      "learning_rate": 1.2847163420829805e-05,
      "loss": 0.817,
      "step": 27120
    },
    {
      "epoch": 1.7192876833866726,
      "grad_norm": 2.5447089672088623,
      "learning_rate": 1.2840812870448772e-05,
      "loss": 0.8479,
      "step": 27130
    },
    {
      "epoch": 1.719921417028423,
      "grad_norm": 2.4800939559936523,
      "learning_rate": 1.283446232006774e-05,
      "loss": 0.8569,
      "step": 27140
    },
    {
      "epoch": 1.7205551506701733,
      "grad_norm": 2.646939754486084,
      "learning_rate": 1.2828111769686708e-05,
      "loss": 0.848,
      "step": 27150
    },
    {
      "epoch": 1.7211888843119239,
      "grad_norm": 2.546436071395874,
      "learning_rate": 1.2821761219305673e-05,
      "loss": 0.8408,
      "step": 27160
    },
    {
      "epoch": 1.721822617953674,
      "grad_norm": 2.867563009262085,
      "learning_rate": 1.2815410668924641e-05,
      "loss": 0.8315,
      "step": 27170
    },
    {
      "epoch": 1.7224563515954245,
      "grad_norm": 3.6940081119537354,
      "learning_rate": 1.2809060118543607e-05,
      "loss": 0.8586,
      "step": 27180
    },
    {
      "epoch": 1.7230900852371747,
      "grad_norm": 2.5565271377563477,
      "learning_rate": 1.2802709568162574e-05,
      "loss": 0.8883,
      "step": 27190
    },
    {
      "epoch": 1.7237238188789252,
      "grad_norm": 2.6750800609588623,
      "learning_rate": 1.2796359017781542e-05,
      "loss": 0.8481,
      "step": 27200
    },
    {
      "epoch": 1.7243575525206756,
      "grad_norm": 2.656125783920288,
      "learning_rate": 1.2790008467400508e-05,
      "loss": 0.8206,
      "step": 27210
    },
    {
      "epoch": 1.724991286162426,
      "grad_norm": 2.987668514251709,
      "learning_rate": 1.2783657917019475e-05,
      "loss": 0.8788,
      "step": 27220
    },
    {
      "epoch": 1.7256250198041763,
      "grad_norm": 2.47194766998291,
      "learning_rate": 1.2777307366638441e-05,
      "loss": 0.8446,
      "step": 27230
    },
    {
      "epoch": 1.7262587534459266,
      "grad_norm": 2.38346529006958,
      "learning_rate": 1.277095681625741e-05,
      "loss": 0.8249,
      "step": 27240
    },
    {
      "epoch": 1.7268924870876772,
      "grad_norm": 2.9582066535949707,
      "learning_rate": 1.2764606265876377e-05,
      "loss": 0.8788,
      "step": 27250
    },
    {
      "epoch": 1.7275262207294273,
      "grad_norm": 2.504410743713379,
      "learning_rate": 1.2758255715495342e-05,
      "loss": 0.9024,
      "step": 27260
    },
    {
      "epoch": 1.7281599543711779,
      "grad_norm": 2.3275909423828125,
      "learning_rate": 1.275190516511431e-05,
      "loss": 0.8342,
      "step": 27270
    },
    {
      "epoch": 1.7287936880129282,
      "grad_norm": 2.501330852508545,
      "learning_rate": 1.2745554614733278e-05,
      "loss": 0.901,
      "step": 27280
    },
    {
      "epoch": 1.7294274216546786,
      "grad_norm": 2.7562415599823,
      "learning_rate": 1.2739204064352243e-05,
      "loss": 0.9021,
      "step": 27290
    },
    {
      "epoch": 1.730061155296429,
      "grad_norm": 2.76656436920166,
      "learning_rate": 1.2732853513971211e-05,
      "loss": 0.8928,
      "step": 27300
    },
    {
      "epoch": 1.7306948889381792,
      "grad_norm": 2.6161179542541504,
      "learning_rate": 1.2726502963590178e-05,
      "loss": 0.896,
      "step": 27310
    },
    {
      "epoch": 1.7313286225799298,
      "grad_norm": 2.548675537109375,
      "learning_rate": 1.2720152413209146e-05,
      "loss": 0.8316,
      "step": 27320
    },
    {
      "epoch": 1.73196235622168,
      "grad_norm": 2.499586343765259,
      "learning_rate": 1.2713801862828112e-05,
      "loss": 0.8691,
      "step": 27330
    },
    {
      "epoch": 1.7325960898634305,
      "grad_norm": 2.6982133388519287,
      "learning_rate": 1.2707451312447079e-05,
      "loss": 0.8648,
      "step": 27340
    },
    {
      "epoch": 1.7332298235051806,
      "grad_norm": 2.9364194869995117,
      "learning_rate": 1.2701100762066047e-05,
      "loss": 0.8255,
      "step": 27350
    },
    {
      "epoch": 1.7338635571469312,
      "grad_norm": 3.108941078186035,
      "learning_rate": 1.2694750211685012e-05,
      "loss": 0.8847,
      "step": 27360
    },
    {
      "epoch": 1.7344972907886815,
      "grad_norm": 3.342897415161133,
      "learning_rate": 1.268839966130398e-05,
      "loss": 0.8579,
      "step": 27370
    },
    {
      "epoch": 1.7351310244304319,
      "grad_norm": 2.4596762657165527,
      "learning_rate": 1.2682049110922948e-05,
      "loss": 0.8538,
      "step": 27380
    },
    {
      "epoch": 1.7357647580721822,
      "grad_norm": 2.9989829063415527,
      "learning_rate": 1.2675698560541914e-05,
      "loss": 0.8683,
      "step": 27390
    },
    {
      "epoch": 1.7363984917139326,
      "grad_norm": 2.5074901580810547,
      "learning_rate": 1.266934801016088e-05,
      "loss": 0.8407,
      "step": 27400
    },
    {
      "epoch": 1.7370322253556831,
      "grad_norm": 2.452561378479004,
      "learning_rate": 1.2662997459779849e-05,
      "loss": 0.8682,
      "step": 27410
    },
    {
      "epoch": 1.7376659589974333,
      "grad_norm": 2.622479200363159,
      "learning_rate": 1.2656646909398815e-05,
      "loss": 0.8498,
      "step": 27420
    },
    {
      "epoch": 1.7382996926391838,
      "grad_norm": 2.4109890460968018,
      "learning_rate": 1.2650296359017782e-05,
      "loss": 0.8086,
      "step": 27430
    },
    {
      "epoch": 1.7389334262809342,
      "grad_norm": 2.571476459503174,
      "learning_rate": 1.2643945808636748e-05,
      "loss": 0.838,
      "step": 27440
    },
    {
      "epoch": 1.7395671599226845,
      "grad_norm": 3.10499906539917,
      "learning_rate": 1.2637595258255716e-05,
      "loss": 0.8292,
      "step": 27450
    },
    {
      "epoch": 1.7402008935644349,
      "grad_norm": 2.4314653873443604,
      "learning_rate": 1.2631244707874684e-05,
      "loss": 0.8285,
      "step": 27460
    },
    {
      "epoch": 1.7408346272061852,
      "grad_norm": 2.5102930068969727,
      "learning_rate": 1.2624894157493649e-05,
      "loss": 0.8775,
      "step": 27470
    },
    {
      "epoch": 1.7414683608479358,
      "grad_norm": 2.5883584022521973,
      "learning_rate": 1.2618543607112617e-05,
      "loss": 0.8358,
      "step": 27480
    },
    {
      "epoch": 1.742102094489686,
      "grad_norm": 2.7564077377319336,
      "learning_rate": 1.2612193056731584e-05,
      "loss": 0.8449,
      "step": 27490
    },
    {
      "epoch": 1.7427358281314365,
      "grad_norm": 2.3763935565948486,
      "learning_rate": 1.260584250635055e-05,
      "loss": 0.8389,
      "step": 27500
    },
    {
      "epoch": 1.7433695617731866,
      "grad_norm": 2.541858434677124,
      "learning_rate": 1.2599491955969518e-05,
      "loss": 0.86,
      "step": 27510
    },
    {
      "epoch": 1.7440032954149371,
      "grad_norm": 2.741389274597168,
      "learning_rate": 1.2593141405588485e-05,
      "loss": 0.8355,
      "step": 27520
    },
    {
      "epoch": 1.7446370290566875,
      "grad_norm": 2.8777427673339844,
      "learning_rate": 1.2586790855207451e-05,
      "loss": 0.8493,
      "step": 27530
    },
    {
      "epoch": 1.7452707626984378,
      "grad_norm": 2.5583746433258057,
      "learning_rate": 1.2580440304826419e-05,
      "loss": 0.9016,
      "step": 27540
    },
    {
      "epoch": 1.7459044963401882,
      "grad_norm": 3.0328238010406494,
      "learning_rate": 1.2574089754445386e-05,
      "loss": 0.89,
      "step": 27550
    },
    {
      "epoch": 1.7465382299819385,
      "grad_norm": 2.823709726333618,
      "learning_rate": 1.2567739204064354e-05,
      "loss": 0.8765,
      "step": 27560
    },
    {
      "epoch": 1.747171963623689,
      "grad_norm": 2.6342971324920654,
      "learning_rate": 1.2561388653683318e-05,
      "loss": 0.8449,
      "step": 27570
    },
    {
      "epoch": 1.7478056972654392,
      "grad_norm": 2.2052996158599854,
      "learning_rate": 1.2555038103302286e-05,
      "loss": 0.8776,
      "step": 27580
    },
    {
      "epoch": 1.7484394309071898,
      "grad_norm": 2.2942769527435303,
      "learning_rate": 1.2548687552921255e-05,
      "loss": 0.8808,
      "step": 27590
    },
    {
      "epoch": 1.7490731645489401,
      "grad_norm": 2.5887365341186523,
      "learning_rate": 1.254233700254022e-05,
      "loss": 0.8581,
      "step": 27600
    },
    {
      "epoch": 1.7497068981906905,
      "grad_norm": 2.760465383529663,
      "learning_rate": 1.2535986452159187e-05,
      "loss": 0.87,
      "step": 27610
    },
    {
      "epoch": 1.7503406318324408,
      "grad_norm": 2.072669267654419,
      "learning_rate": 1.2529635901778154e-05,
      "loss": 0.8344,
      "step": 27620
    },
    {
      "epoch": 1.7509743654741912,
      "grad_norm": 2.511692523956299,
      "learning_rate": 1.2523285351397122e-05,
      "loss": 0.8303,
      "step": 27630
    },
    {
      "epoch": 1.7516080991159417,
      "grad_norm": 2.595860481262207,
      "learning_rate": 1.2516934801016088e-05,
      "loss": 0.8506,
      "step": 27640
    },
    {
      "epoch": 1.7522418327576919,
      "grad_norm": 2.866135358810425,
      "learning_rate": 1.2510584250635055e-05,
      "loss": 0.862,
      "step": 27650
    },
    {
      "epoch": 1.7528755663994424,
      "grad_norm": 2.65924072265625,
      "learning_rate": 1.2504233700254023e-05,
      "loss": 0.8663,
      "step": 27660
    },
    {
      "epoch": 1.7535093000411925,
      "grad_norm": 3.0331225395202637,
      "learning_rate": 1.249788314987299e-05,
      "loss": 0.8543,
      "step": 27670
    },
    {
      "epoch": 1.754143033682943,
      "grad_norm": 2.6213669776916504,
      "learning_rate": 1.2491532599491956e-05,
      "loss": 0.8724,
      "step": 27680
    },
    {
      "epoch": 1.7547767673246935,
      "grad_norm": 2.6030006408691406,
      "learning_rate": 1.2485182049110924e-05,
      "loss": 0.8616,
      "step": 27690
    },
    {
      "epoch": 1.7554105009664438,
      "grad_norm": 3.06889009475708,
      "learning_rate": 1.247883149872989e-05,
      "loss": 0.8642,
      "step": 27700
    },
    {
      "epoch": 1.7560442346081941,
      "grad_norm": 2.444356918334961,
      "learning_rate": 1.2472480948348857e-05,
      "loss": 0.8575,
      "step": 27710
    },
    {
      "epoch": 1.7566779682499445,
      "grad_norm": 2.5951380729675293,
      "learning_rate": 1.2466130397967825e-05,
      "loss": 0.8943,
      "step": 27720
    },
    {
      "epoch": 1.757311701891695,
      "grad_norm": 3.1246533393859863,
      "learning_rate": 1.2459779847586791e-05,
      "loss": 0.8646,
      "step": 27730
    },
    {
      "epoch": 1.7579454355334452,
      "grad_norm": 3.334885835647583,
      "learning_rate": 1.2453429297205758e-05,
      "loss": 0.8319,
      "step": 27740
    },
    {
      "epoch": 1.7585791691751957,
      "grad_norm": 2.680899143218994,
      "learning_rate": 1.2447078746824724e-05,
      "loss": 0.8364,
      "step": 27750
    },
    {
      "epoch": 1.759212902816946,
      "grad_norm": 2.637118101119995,
      "learning_rate": 1.2440728196443692e-05,
      "loss": 0.8114,
      "step": 27760
    },
    {
      "epoch": 1.7598466364586964,
      "grad_norm": 3.0818397998809814,
      "learning_rate": 1.2434377646062659e-05,
      "loss": 0.8098,
      "step": 27770
    },
    {
      "epoch": 1.7604803701004468,
      "grad_norm": 2.284667491912842,
      "learning_rate": 1.2428027095681625e-05,
      "loss": 0.8513,
      "step": 27780
    },
    {
      "epoch": 1.7611141037421971,
      "grad_norm": 2.6282386779785156,
      "learning_rate": 1.2421676545300593e-05,
      "loss": 0.9068,
      "step": 27790
    },
    {
      "epoch": 1.7617478373839477,
      "grad_norm": 2.5621531009674072,
      "learning_rate": 1.2415325994919561e-05,
      "loss": 0.8682,
      "step": 27800
    },
    {
      "epoch": 1.7623815710256978,
      "grad_norm": 3.005084753036499,
      "learning_rate": 1.2408975444538526e-05,
      "loss": 0.8892,
      "step": 27810
    },
    {
      "epoch": 1.7630153046674484,
      "grad_norm": 2.900221824645996,
      "learning_rate": 1.2402624894157494e-05,
      "loss": 0.8916,
      "step": 27820
    },
    {
      "epoch": 1.7636490383091985,
      "grad_norm": 2.368758201599121,
      "learning_rate": 1.239627434377646e-05,
      "loss": 0.8703,
      "step": 27830
    },
    {
      "epoch": 1.764282771950949,
      "grad_norm": 2.5341796875,
      "learning_rate": 1.2389923793395427e-05,
      "loss": 0.837,
      "step": 27840
    },
    {
      "epoch": 1.7649165055926994,
      "grad_norm": 2.8877556324005127,
      "learning_rate": 1.2383573243014395e-05,
      "loss": 0.8934,
      "step": 27850
    },
    {
      "epoch": 1.7655502392344498,
      "grad_norm": 2.850707530975342,
      "learning_rate": 1.2377222692633362e-05,
      "loss": 0.8482,
      "step": 27860
    },
    {
      "epoch": 1.7661839728762,
      "grad_norm": 2.7641196250915527,
      "learning_rate": 1.237087214225233e-05,
      "loss": 0.8784,
      "step": 27870
    },
    {
      "epoch": 1.7668177065179504,
      "grad_norm": 2.641474962234497,
      "learning_rate": 1.2364521591871295e-05,
      "loss": 0.8518,
      "step": 27880
    },
    {
      "epoch": 1.767451440159701,
      "grad_norm": 2.403414726257324,
      "learning_rate": 1.2358171041490263e-05,
      "loss": 0.8746,
      "step": 27890
    },
    {
      "epoch": 1.7680851738014511,
      "grad_norm": 2.6992199420928955,
      "learning_rate": 1.235182049110923e-05,
      "loss": 0.8693,
      "step": 27900
    },
    {
      "epoch": 1.7687189074432017,
      "grad_norm": 3.48589825630188,
      "learning_rate": 1.2345469940728196e-05,
      "loss": 0.859,
      "step": 27910
    },
    {
      "epoch": 1.769352641084952,
      "grad_norm": 2.9324560165405273,
      "learning_rate": 1.2339119390347164e-05,
      "loss": 0.8761,
      "step": 27920
    },
    {
      "epoch": 1.7699863747267024,
      "grad_norm": 2.4745917320251465,
      "learning_rate": 1.2332768839966132e-05,
      "loss": 0.8514,
      "step": 27930
    },
    {
      "epoch": 1.7706201083684527,
      "grad_norm": 2.423896312713623,
      "learning_rate": 1.2326418289585098e-05,
      "loss": 0.8247,
      "step": 27940
    },
    {
      "epoch": 1.771253842010203,
      "grad_norm": 2.952864170074463,
      "learning_rate": 1.2320067739204065e-05,
      "loss": 0.8386,
      "step": 27950
    },
    {
      "epoch": 1.7718875756519537,
      "grad_norm": 3.106694221496582,
      "learning_rate": 1.2313717188823031e-05,
      "loss": 0.887,
      "step": 27960
    },
    {
      "epoch": 1.7725213092937038,
      "grad_norm": 2.625999927520752,
      "learning_rate": 1.2307366638442e-05,
      "loss": 0.8723,
      "step": 27970
    },
    {
      "epoch": 1.7731550429354543,
      "grad_norm": 2.0978317260742188,
      "learning_rate": 1.2301016088060966e-05,
      "loss": 0.8462,
      "step": 27980
    },
    {
      "epoch": 1.7737887765772045,
      "grad_norm": 2.764357566833496,
      "learning_rate": 1.2294665537679932e-05,
      "loss": 0.8251,
      "step": 27990
    },
    {
      "epoch": 1.774422510218955,
      "grad_norm": 2.482067823410034,
      "learning_rate": 1.22883149872989e-05,
      "loss": 0.8773,
      "step": 28000
    },
    {
      "epoch": 1.7750562438607054,
      "grad_norm": 2.437126636505127,
      "learning_rate": 1.2281964436917867e-05,
      "loss": 0.8645,
      "step": 28010
    },
    {
      "epoch": 1.7756899775024557,
      "grad_norm": 2.685197353363037,
      "learning_rate": 1.2275613886536833e-05,
      "loss": 0.8424,
      "step": 28020
    },
    {
      "epoch": 1.776323711144206,
      "grad_norm": 2.995262861251831,
      "learning_rate": 1.2269263336155801e-05,
      "loss": 0.8489,
      "step": 28030
    },
    {
      "epoch": 1.7769574447859564,
      "grad_norm": 2.443641424179077,
      "learning_rate": 1.2262912785774768e-05,
      "loss": 0.8294,
      "step": 28040
    },
    {
      "epoch": 1.777591178427707,
      "grad_norm": 2.588101387023926,
      "learning_rate": 1.2256562235393734e-05,
      "loss": 0.8443,
      "step": 28050
    },
    {
      "epoch": 1.778224912069457,
      "grad_norm": 2.8355584144592285,
      "learning_rate": 1.2250211685012702e-05,
      "loss": 0.8562,
      "step": 28060
    },
    {
      "epoch": 1.7788586457112077,
      "grad_norm": 2.8003079891204834,
      "learning_rate": 1.2243861134631669e-05,
      "loss": 0.86,
      "step": 28070
    },
    {
      "epoch": 1.7794923793529578,
      "grad_norm": 3.8390467166900635,
      "learning_rate": 1.2237510584250635e-05,
      "loss": 0.8334,
      "step": 28080
    },
    {
      "epoch": 1.7801261129947084,
      "grad_norm": 2.4970390796661377,
      "learning_rate": 1.2231160033869601e-05,
      "loss": 0.8344,
      "step": 28090
    },
    {
      "epoch": 1.7807598466364587,
      "grad_norm": 2.5405006408691406,
      "learning_rate": 1.222480948348857e-05,
      "loss": 0.8857,
      "step": 28100
    },
    {
      "epoch": 1.781393580278209,
      "grad_norm": 2.6454875469207764,
      "learning_rate": 1.2218458933107538e-05,
      "loss": 0.8486,
      "step": 28110
    },
    {
      "epoch": 1.7820273139199594,
      "grad_norm": 2.799910545349121,
      "learning_rate": 1.2212108382726502e-05,
      "loss": 0.8604,
      "step": 28120
    },
    {
      "epoch": 1.7826610475617097,
      "grad_norm": 2.4604275226593018,
      "learning_rate": 1.220575783234547e-05,
      "loss": 0.833,
      "step": 28130
    },
    {
      "epoch": 1.7832947812034603,
      "grad_norm": 2.572711706161499,
      "learning_rate": 1.2199407281964437e-05,
      "loss": 0.841,
      "step": 28140
    },
    {
      "epoch": 1.7839285148452104,
      "grad_norm": 2.7492997646331787,
      "learning_rate": 1.2193056731583403e-05,
      "loss": 0.8591,
      "step": 28150
    },
    {
      "epoch": 1.784562248486961,
      "grad_norm": 2.5655412673950195,
      "learning_rate": 1.2186706181202371e-05,
      "loss": 0.879,
      "step": 28160
    },
    {
      "epoch": 1.7851959821287113,
      "grad_norm": 3.0554592609405518,
      "learning_rate": 1.2180355630821338e-05,
      "loss": 0.8249,
      "step": 28170
    },
    {
      "epoch": 1.7858297157704617,
      "grad_norm": 2.5704259872436523,
      "learning_rate": 1.2174005080440306e-05,
      "loss": 0.9029,
      "step": 28180
    },
    {
      "epoch": 1.786463449412212,
      "grad_norm": 2.570387840270996,
      "learning_rate": 1.2167654530059272e-05,
      "loss": 0.8261,
      "step": 28190
    },
    {
      "epoch": 1.7870971830539624,
      "grad_norm": 3.060649871826172,
      "learning_rate": 1.2161303979678239e-05,
      "loss": 0.8426,
      "step": 28200
    },
    {
      "epoch": 1.787730916695713,
      "grad_norm": 3.0876147747039795,
      "learning_rate": 1.2154953429297207e-05,
      "loss": 0.862,
      "step": 28210
    },
    {
      "epoch": 1.788364650337463,
      "grad_norm": 2.851020097732544,
      "learning_rate": 1.2148602878916172e-05,
      "loss": 0.8663,
      "step": 28220
    },
    {
      "epoch": 1.7889983839792136,
      "grad_norm": 2.4335174560546875,
      "learning_rate": 1.214225232853514e-05,
      "loss": 0.8731,
      "step": 28230
    },
    {
      "epoch": 1.7896321176209637,
      "grad_norm": 2.81691575050354,
      "learning_rate": 1.2135901778154108e-05,
      "loss": 0.8532,
      "step": 28240
    },
    {
      "epoch": 1.7902658512627143,
      "grad_norm": 3.3854830265045166,
      "learning_rate": 1.2129551227773074e-05,
      "loss": 0.8796,
      "step": 28250
    },
    {
      "epoch": 1.7908995849044647,
      "grad_norm": 2.5945632457733154,
      "learning_rate": 1.212320067739204e-05,
      "loss": 0.8339,
      "step": 28260
    },
    {
      "epoch": 1.791533318546215,
      "grad_norm": 2.651207208633423,
      "learning_rate": 1.2116850127011007e-05,
      "loss": 0.827,
      "step": 28270
    },
    {
      "epoch": 1.7921670521879653,
      "grad_norm": 2.540637493133545,
      "learning_rate": 1.2110499576629975e-05,
      "loss": 0.8307,
      "step": 28280
    },
    {
      "epoch": 1.7928007858297157,
      "grad_norm": 2.8932244777679443,
      "learning_rate": 1.2104149026248942e-05,
      "loss": 0.8395,
      "step": 28290
    },
    {
      "epoch": 1.7934345194714663,
      "grad_norm": 2.6452159881591797,
      "learning_rate": 1.2097798475867908e-05,
      "loss": 0.8598,
      "step": 28300
    },
    {
      "epoch": 1.7940682531132164,
      "grad_norm": 3.4923131465911865,
      "learning_rate": 1.2091447925486876e-05,
      "loss": 0.8626,
      "step": 28310
    },
    {
      "epoch": 1.794701986754967,
      "grad_norm": 2.3761374950408936,
      "learning_rate": 1.2085097375105843e-05,
      "loss": 0.8277,
      "step": 28320
    },
    {
      "epoch": 1.7953357203967173,
      "grad_norm": 2.2377092838287354,
      "learning_rate": 1.207874682472481e-05,
      "loss": 0.8431,
      "step": 28330
    },
    {
      "epoch": 1.7959694540384676,
      "grad_norm": 2.8181426525115967,
      "learning_rate": 1.2072396274343777e-05,
      "loss": 0.8657,
      "step": 28340
    },
    {
      "epoch": 1.796603187680218,
      "grad_norm": 2.3129987716674805,
      "learning_rate": 1.2066045723962744e-05,
      "loss": 0.8234,
      "step": 28350
    },
    {
      "epoch": 1.7972369213219683,
      "grad_norm": 2.6507458686828613,
      "learning_rate": 1.205969517358171e-05,
      "loss": 0.871,
      "step": 28360
    },
    {
      "epoch": 1.797870654963719,
      "grad_norm": 2.876398801803589,
      "learning_rate": 1.2053344623200678e-05,
      "loss": 0.8424,
      "step": 28370
    },
    {
      "epoch": 1.798504388605469,
      "grad_norm": 2.676187753677368,
      "learning_rate": 1.2046994072819645e-05,
      "loss": 0.8272,
      "step": 28380
    },
    {
      "epoch": 1.7991381222472196,
      "grad_norm": 2.4469408988952637,
      "learning_rate": 1.2040643522438611e-05,
      "loss": 0.8322,
      "step": 28390
    },
    {
      "epoch": 1.7997718558889697,
      "grad_norm": 2.7274038791656494,
      "learning_rate": 1.2034292972057578e-05,
      "loss": 0.8442,
      "step": 28400
    },
    {
      "epoch": 1.8004055895307203,
      "grad_norm": 3.0347049236297607,
      "learning_rate": 1.2027942421676546e-05,
      "loss": 0.8877,
      "step": 28410
    },
    {
      "epoch": 1.8010393231724706,
      "grad_norm": 2.9697628021240234,
      "learning_rate": 1.2021591871295514e-05,
      "loss": 0.8826,
      "step": 28420
    },
    {
      "epoch": 1.801673056814221,
      "grad_norm": 2.3470678329467773,
      "learning_rate": 1.2015241320914479e-05,
      "loss": 0.9023,
      "step": 28430
    },
    {
      "epoch": 1.8023067904559713,
      "grad_norm": 2.681562662124634,
      "learning_rate": 1.2008890770533447e-05,
      "loss": 0.8127,
      "step": 28440
    },
    {
      "epoch": 1.8029405240977217,
      "grad_norm": 2.8639516830444336,
      "learning_rate": 1.2002540220152415e-05,
      "loss": 0.8315,
      "step": 28450
    },
    {
      "epoch": 1.8035742577394722,
      "grad_norm": 2.500218391418457,
      "learning_rate": 1.199618966977138e-05,
      "loss": 0.8323,
      "step": 28460
    },
    {
      "epoch": 1.8042079913812223,
      "grad_norm": 2.3223018646240234,
      "learning_rate": 1.1989839119390348e-05,
      "loss": 0.8326,
      "step": 28470
    },
    {
      "epoch": 1.804841725022973,
      "grad_norm": 2.397714376449585,
      "learning_rate": 1.1983488569009314e-05,
      "loss": 0.8761,
      "step": 28480
    },
    {
      "epoch": 1.8054754586647233,
      "grad_norm": 2.9996538162231445,
      "learning_rate": 1.1977138018628282e-05,
      "loss": 0.8718,
      "step": 28490
    },
    {
      "epoch": 1.8061091923064736,
      "grad_norm": 2.7322261333465576,
      "learning_rate": 1.1970787468247249e-05,
      "loss": 0.8657,
      "step": 28500
    },
    {
      "epoch": 1.806742925948224,
      "grad_norm": 2.534008026123047,
      "learning_rate": 1.1964436917866215e-05,
      "loss": 0.8322,
      "step": 28510
    },
    {
      "epoch": 1.8073766595899743,
      "grad_norm": 2.9942705631256104,
      "learning_rate": 1.1958086367485183e-05,
      "loss": 0.833,
      "step": 28520
    },
    {
      "epoch": 1.8080103932317249,
      "grad_norm": 2.3368263244628906,
      "learning_rate": 1.1951735817104148e-05,
      "loss": 0.8276,
      "step": 28530
    },
    {
      "epoch": 1.808644126873475,
      "grad_norm": 2.2614808082580566,
      "learning_rate": 1.1945385266723116e-05,
      "loss": 0.8622,
      "step": 28540
    },
    {
      "epoch": 1.8092778605152255,
      "grad_norm": 2.733582019805908,
      "learning_rate": 1.1939034716342084e-05,
      "loss": 0.8783,
      "step": 28550
    },
    {
      "epoch": 1.8099115941569757,
      "grad_norm": 2.6393423080444336,
      "learning_rate": 1.193268416596105e-05,
      "loss": 0.8369,
      "step": 28560
    },
    {
      "epoch": 1.8105453277987262,
      "grad_norm": 2.6464743614196777,
      "learning_rate": 1.1926333615580017e-05,
      "loss": 0.8255,
      "step": 28570
    },
    {
      "epoch": 1.8111790614404766,
      "grad_norm": 2.3591701984405518,
      "learning_rate": 1.1919983065198985e-05,
      "loss": 0.8406,
      "step": 28580
    },
    {
      "epoch": 1.811812795082227,
      "grad_norm": 3.376006841659546,
      "learning_rate": 1.1913632514817952e-05,
      "loss": 0.8302,
      "step": 28590
    },
    {
      "epoch": 1.8124465287239773,
      "grad_norm": 2.8830761909484863,
      "learning_rate": 1.1907281964436918e-05,
      "loss": 0.8524,
      "step": 28600
    },
    {
      "epoch": 1.8130802623657276,
      "grad_norm": 3.1243584156036377,
      "learning_rate": 1.1900931414055884e-05,
      "loss": 0.8222,
      "step": 28610
    },
    {
      "epoch": 1.8137139960074782,
      "grad_norm": 2.5740859508514404,
      "learning_rate": 1.1894580863674852e-05,
      "loss": 0.849,
      "step": 28620
    },
    {
      "epoch": 1.8143477296492283,
      "grad_norm": 3.039827823638916,
      "learning_rate": 1.1888230313293819e-05,
      "loss": 0.8434,
      "step": 28630
    },
    {
      "epoch": 1.8149814632909789,
      "grad_norm": 2.6736254692077637,
      "learning_rate": 1.1881879762912785e-05,
      "loss": 0.8733,
      "step": 28640
    },
    {
      "epoch": 1.8156151969327292,
      "grad_norm": 2.5341811180114746,
      "learning_rate": 1.1875529212531753e-05,
      "loss": 0.8448,
      "step": 28650
    },
    {
      "epoch": 1.8162489305744796,
      "grad_norm": 2.3054206371307373,
      "learning_rate": 1.186917866215072e-05,
      "loss": 0.8363,
      "step": 28660
    },
    {
      "epoch": 1.81688266421623,
      "grad_norm": 2.6021625995635986,
      "learning_rate": 1.1862828111769686e-05,
      "loss": 0.8653,
      "step": 28670
    },
    {
      "epoch": 1.8175163978579802,
      "grad_norm": 2.9645230770111084,
      "learning_rate": 1.1856477561388654e-05,
      "loss": 0.8578,
      "step": 28680
    },
    {
      "epoch": 1.8181501314997308,
      "grad_norm": 2.764861583709717,
      "learning_rate": 1.1850127011007621e-05,
      "loss": 0.8544,
      "step": 28690
    },
    {
      "epoch": 1.818783865141481,
      "grad_norm": 3.359833002090454,
      "learning_rate": 1.1843776460626587e-05,
      "loss": 0.8459,
      "step": 28700
    },
    {
      "epoch": 1.8194175987832315,
      "grad_norm": 2.9985907077789307,
      "learning_rate": 1.1837425910245555e-05,
      "loss": 0.8818,
      "step": 28710
    },
    {
      "epoch": 1.8200513324249816,
      "grad_norm": 3.0331432819366455,
      "learning_rate": 1.1831075359864522e-05,
      "loss": 0.8272,
      "step": 28720
    },
    {
      "epoch": 1.8206850660667322,
      "grad_norm": 2.538884162902832,
      "learning_rate": 1.182472480948349e-05,
      "loss": 0.8376,
      "step": 28730
    },
    {
      "epoch": 1.8213187997084825,
      "grad_norm": 2.427180767059326,
      "learning_rate": 1.1818374259102455e-05,
      "loss": 0.8266,
      "step": 28740
    },
    {
      "epoch": 1.8219525333502329,
      "grad_norm": 2.671396493911743,
      "learning_rate": 1.1812023708721423e-05,
      "loss": 0.8517,
      "step": 28750
    },
    {
      "epoch": 1.8225862669919832,
      "grad_norm": 2.476979970932007,
      "learning_rate": 1.1805673158340391e-05,
      "loss": 0.8507,
      "step": 28760
    },
    {
      "epoch": 1.8232200006337336,
      "grad_norm": 2.7034804821014404,
      "learning_rate": 1.1799322607959356e-05,
      "loss": 0.8972,
      "step": 28770
    },
    {
      "epoch": 1.8238537342754841,
      "grad_norm": 2.814953327178955,
      "learning_rate": 1.1792972057578324e-05,
      "loss": 0.8315,
      "step": 28780
    },
    {
      "epoch": 1.8244874679172343,
      "grad_norm": 2.721956968307495,
      "learning_rate": 1.178662150719729e-05,
      "loss": 0.8557,
      "step": 28790
    },
    {
      "epoch": 1.8251212015589848,
      "grad_norm": 2.7565457820892334,
      "learning_rate": 1.1780270956816258e-05,
      "loss": 0.8681,
      "step": 28800
    },
    {
      "epoch": 1.8257549352007352,
      "grad_norm": 3.214315176010132,
      "learning_rate": 1.1773920406435225e-05,
      "loss": 0.8274,
      "step": 28810
    },
    {
      "epoch": 1.8263886688424855,
      "grad_norm": 2.4384682178497314,
      "learning_rate": 1.1767569856054191e-05,
      "loss": 0.8608,
      "step": 28820
    },
    {
      "epoch": 1.8270224024842359,
      "grad_norm": 3.0443477630615234,
      "learning_rate": 1.176121930567316e-05,
      "loss": 0.8377,
      "step": 28830
    },
    {
      "epoch": 1.8276561361259862,
      "grad_norm": 2.5649125576019287,
      "learning_rate": 1.1754868755292126e-05,
      "loss": 0.8633,
      "step": 28840
    },
    {
      "epoch": 1.8282898697677368,
      "grad_norm": 2.7749171257019043,
      "learning_rate": 1.1748518204911092e-05,
      "loss": 0.8299,
      "step": 28850
    },
    {
      "epoch": 1.828923603409487,
      "grad_norm": 2.9794840812683105,
      "learning_rate": 1.174216765453006e-05,
      "loss": 0.9023,
      "step": 28860
    },
    {
      "epoch": 1.8295573370512375,
      "grad_norm": 2.4236412048339844,
      "learning_rate": 1.1735817104149027e-05,
      "loss": 0.86,
      "step": 28870
    },
    {
      "epoch": 1.8301910706929876,
      "grad_norm": 2.3630809783935547,
      "learning_rate": 1.1729466553767993e-05,
      "loss": 0.8274,
      "step": 28880
    },
    {
      "epoch": 1.8308248043347382,
      "grad_norm": 2.8502602577209473,
      "learning_rate": 1.1723116003386961e-05,
      "loss": 0.8566,
      "step": 28890
    },
    {
      "epoch": 1.8314585379764885,
      "grad_norm": 2.8858933448791504,
      "learning_rate": 1.1716765453005928e-05,
      "loss": 0.8792,
      "step": 28900
    },
    {
      "epoch": 1.8320922716182388,
      "grad_norm": 2.3939437866210938,
      "learning_rate": 1.1710414902624894e-05,
      "loss": 0.8556,
      "step": 28910
    },
    {
      "epoch": 1.8327260052599892,
      "grad_norm": 2.286801338195801,
      "learning_rate": 1.170406435224386e-05,
      "loss": 0.8403,
      "step": 28920
    },
    {
      "epoch": 1.8333597389017395,
      "grad_norm": 2.609551429748535,
      "learning_rate": 1.1697713801862829e-05,
      "loss": 0.8288,
      "step": 28930
    },
    {
      "epoch": 1.83399347254349,
      "grad_norm": 2.948904514312744,
      "learning_rate": 1.1691363251481795e-05,
      "loss": 0.8793,
      "step": 28940
    },
    {
      "epoch": 1.8346272061852402,
      "grad_norm": 2.6748228073120117,
      "learning_rate": 1.1685012701100762e-05,
      "loss": 0.8658,
      "step": 28950
    },
    {
      "epoch": 1.8352609398269908,
      "grad_norm": 2.473170042037964,
      "learning_rate": 1.167866215071973e-05,
      "loss": 0.884,
      "step": 28960
    },
    {
      "epoch": 1.8358946734687411,
      "grad_norm": 2.5052146911621094,
      "learning_rate": 1.1672311600338698e-05,
      "loss": 0.8649,
      "step": 28970
    },
    {
      "epoch": 1.8365284071104915,
      "grad_norm": 2.6304616928100586,
      "learning_rate": 1.1665961049957662e-05,
      "loss": 0.8387,
      "step": 28980
    },
    {
      "epoch": 1.8371621407522418,
      "grad_norm": 2.7219631671905518,
      "learning_rate": 1.165961049957663e-05,
      "loss": 0.8988,
      "step": 28990
    },
    {
      "epoch": 1.8377958743939922,
      "grad_norm": 2.4843146800994873,
      "learning_rate": 1.1653259949195597e-05,
      "loss": 0.9246,
      "step": 29000
    },
    {
      "epoch": 1.8384296080357427,
      "grad_norm": 3.0108489990234375,
      "learning_rate": 1.1646909398814563e-05,
      "loss": 0.8569,
      "step": 29010
    },
    {
      "epoch": 1.8390633416774929,
      "grad_norm": 2.6927149295806885,
      "learning_rate": 1.1640558848433532e-05,
      "loss": 0.8925,
      "step": 29020
    },
    {
      "epoch": 1.8396970753192434,
      "grad_norm": 2.3802480697631836,
      "learning_rate": 1.1634208298052498e-05,
      "loss": 0.873,
      "step": 29030
    },
    {
      "epoch": 1.8403308089609935,
      "grad_norm": 2.349808931350708,
      "learning_rate": 1.1627857747671466e-05,
      "loss": 0.8701,
      "step": 29040
    },
    {
      "epoch": 1.8409645426027441,
      "grad_norm": 2.2092981338500977,
      "learning_rate": 1.1621507197290431e-05,
      "loss": 0.8186,
      "step": 29050
    },
    {
      "epoch": 1.8415982762444945,
      "grad_norm": 2.4648988246917725,
      "learning_rate": 1.1615156646909399e-05,
      "loss": 0.8177,
      "step": 29060
    },
    {
      "epoch": 1.8422320098862448,
      "grad_norm": 2.7736353874206543,
      "learning_rate": 1.1608806096528367e-05,
      "loss": 0.8753,
      "step": 29070
    },
    {
      "epoch": 1.8428657435279951,
      "grad_norm": 2.4156875610351562,
      "learning_rate": 1.1602455546147332e-05,
      "loss": 0.8295,
      "step": 29080
    },
    {
      "epoch": 1.8434994771697455,
      "grad_norm": 2.9577431678771973,
      "learning_rate": 1.15961049957663e-05,
      "loss": 0.8554,
      "step": 29090
    },
    {
      "epoch": 1.844133210811496,
      "grad_norm": 2.754025936126709,
      "learning_rate": 1.1589754445385268e-05,
      "loss": 0.8488,
      "step": 29100
    },
    {
      "epoch": 1.8447669444532462,
      "grad_norm": 3.0326991081237793,
      "learning_rate": 1.1583403895004234e-05,
      "loss": 0.8823,
      "step": 29110
    },
    {
      "epoch": 1.8454006780949967,
      "grad_norm": 2.486250162124634,
      "learning_rate": 1.1577053344623201e-05,
      "loss": 0.8724,
      "step": 29120
    },
    {
      "epoch": 1.846034411736747,
      "grad_norm": 2.6597096920013428,
      "learning_rate": 1.1570702794242167e-05,
      "loss": 0.9037,
      "step": 29130
    },
    {
      "epoch": 1.8466681453784974,
      "grad_norm": 2.589803695678711,
      "learning_rate": 1.1564352243861135e-05,
      "loss": 0.8572,
      "step": 29140
    },
    {
      "epoch": 1.8473018790202478,
      "grad_norm": 2.5550684928894043,
      "learning_rate": 1.1558001693480102e-05,
      "loss": 0.8541,
      "step": 29150
    },
    {
      "epoch": 1.8479356126619981,
      "grad_norm": 2.6684257984161377,
      "learning_rate": 1.1551651143099068e-05,
      "loss": 0.8639,
      "step": 29160
    },
    {
      "epoch": 1.8485693463037487,
      "grad_norm": 2.8504035472869873,
      "learning_rate": 1.1545300592718036e-05,
      "loss": 0.8686,
      "step": 29170
    },
    {
      "epoch": 1.8492030799454988,
      "grad_norm": 2.498378276824951,
      "learning_rate": 1.1538950042337001e-05,
      "loss": 0.8558,
      "step": 29180
    },
    {
      "epoch": 1.8498368135872494,
      "grad_norm": 2.721175193786621,
      "learning_rate": 1.153259949195597e-05,
      "loss": 0.891,
      "step": 29190
    },
    {
      "epoch": 1.8504705472289995,
      "grad_norm": 3.054283618927002,
      "learning_rate": 1.1526248941574937e-05,
      "loss": 0.8614,
      "step": 29200
    },
    {
      "epoch": 1.85110428087075,
      "grad_norm": 2.848400354385376,
      "learning_rate": 1.1519898391193904e-05,
      "loss": 0.9053,
      "step": 29210
    },
    {
      "epoch": 1.8517380145125004,
      "grad_norm": 2.5727338790893555,
      "learning_rate": 1.151354784081287e-05,
      "loss": 0.8603,
      "step": 29220
    },
    {
      "epoch": 1.8523717481542508,
      "grad_norm": 2.723116159439087,
      "learning_rate": 1.1507197290431838e-05,
      "loss": 0.865,
      "step": 29230
    },
    {
      "epoch": 1.853005481796001,
      "grad_norm": 2.679413080215454,
      "learning_rate": 1.1500846740050805e-05,
      "loss": 0.8511,
      "step": 29240
    },
    {
      "epoch": 1.8536392154377515,
      "grad_norm": 2.501922607421875,
      "learning_rate": 1.1494496189669771e-05,
      "loss": 0.8705,
      "step": 29250
    },
    {
      "epoch": 1.854272949079502,
      "grad_norm": 2.846342086791992,
      "learning_rate": 1.1488145639288738e-05,
      "loss": 0.8653,
      "step": 29260
    },
    {
      "epoch": 1.8549066827212521,
      "grad_norm": 2.5116167068481445,
      "learning_rate": 1.1481795088907706e-05,
      "loss": 0.858,
      "step": 29270
    },
    {
      "epoch": 1.8555404163630027,
      "grad_norm": 2.471748113632202,
      "learning_rate": 1.1475444538526674e-05,
      "loss": 0.8653,
      "step": 29280
    },
    {
      "epoch": 1.856174150004753,
      "grad_norm": 2.7862443923950195,
      "learning_rate": 1.1469093988145639e-05,
      "loss": 0.8848,
      "step": 29290
    },
    {
      "epoch": 1.8568078836465034,
      "grad_norm": 2.4571621417999268,
      "learning_rate": 1.1462743437764607e-05,
      "loss": 0.8467,
      "step": 29300
    },
    {
      "epoch": 1.8574416172882537,
      "grad_norm": 2.8706791400909424,
      "learning_rate": 1.1456392887383573e-05,
      "loss": 0.821,
      "step": 29310
    },
    {
      "epoch": 1.858075350930004,
      "grad_norm": 3.327789783477783,
      "learning_rate": 1.145004233700254e-05,
      "loss": 0.866,
      "step": 29320
    },
    {
      "epoch": 1.8587090845717547,
      "grad_norm": 3.182969808578491,
      "learning_rate": 1.1443691786621508e-05,
      "loss": 0.8408,
      "step": 29330
    },
    {
      "epoch": 1.8593428182135048,
      "grad_norm": 3.1398613452911377,
      "learning_rate": 1.1437341236240474e-05,
      "loss": 0.8508,
      "step": 29340
    },
    {
      "epoch": 1.8599765518552553,
      "grad_norm": 2.9264023303985596,
      "learning_rate": 1.1430990685859442e-05,
      "loss": 0.8321,
      "step": 29350
    },
    {
      "epoch": 1.8606102854970055,
      "grad_norm": 2.5868451595306396,
      "learning_rate": 1.1424640135478409e-05,
      "loss": 0.8547,
      "step": 29360
    },
    {
      "epoch": 1.861244019138756,
      "grad_norm": 2.971071720123291,
      "learning_rate": 1.1418289585097375e-05,
      "loss": 0.8314,
      "step": 29370
    },
    {
      "epoch": 1.8618777527805064,
      "grad_norm": 2.461514711380005,
      "learning_rate": 1.1411939034716343e-05,
      "loss": 0.8798,
      "step": 29380
    },
    {
      "epoch": 1.8625114864222567,
      "grad_norm": 2.806814670562744,
      "learning_rate": 1.1405588484335308e-05,
      "loss": 0.8217,
      "step": 29390
    },
    {
      "epoch": 1.863145220064007,
      "grad_norm": 2.1778860092163086,
      "learning_rate": 1.1399237933954276e-05,
      "loss": 0.8772,
      "step": 29400
    },
    {
      "epoch": 1.8637789537057574,
      "grad_norm": 2.4758219718933105,
      "learning_rate": 1.1392887383573244e-05,
      "loss": 0.851,
      "step": 29410
    },
    {
      "epoch": 1.864412687347508,
      "grad_norm": 2.47721266746521,
      "learning_rate": 1.138653683319221e-05,
      "loss": 0.8321,
      "step": 29420
    },
    {
      "epoch": 1.865046420989258,
      "grad_norm": 2.794344186782837,
      "learning_rate": 1.1380186282811177e-05,
      "loss": 0.8994,
      "step": 29430
    },
    {
      "epoch": 1.8656801546310087,
      "grad_norm": 2.353719711303711,
      "learning_rate": 1.1373835732430144e-05,
      "loss": 0.8425,
      "step": 29440
    },
    {
      "epoch": 1.866313888272759,
      "grad_norm": 4.273412227630615,
      "learning_rate": 1.1367485182049112e-05,
      "loss": 0.8205,
      "step": 29450
    },
    {
      "epoch": 1.8669476219145094,
      "grad_norm": 2.627105236053467,
      "learning_rate": 1.1361134631668078e-05,
      "loss": 0.8597,
      "step": 29460
    },
    {
      "epoch": 1.8675813555562597,
      "grad_norm": 2.5123770236968994,
      "learning_rate": 1.1354784081287045e-05,
      "loss": 0.8601,
      "step": 29470
    },
    {
      "epoch": 1.86821508919801,
      "grad_norm": 2.9843242168426514,
      "learning_rate": 1.1348433530906013e-05,
      "loss": 0.8559,
      "step": 29480
    },
    {
      "epoch": 1.8688488228397606,
      "grad_norm": 2.5074245929718018,
      "learning_rate": 1.1342082980524979e-05,
      "loss": 0.887,
      "step": 29490
    },
    {
      "epoch": 1.8694825564815107,
      "grad_norm": 2.99664568901062,
      "learning_rate": 1.1335732430143945e-05,
      "loss": 0.8768,
      "step": 29500
    },
    {
      "epoch": 1.8701162901232613,
      "grad_norm": 2.9431698322296143,
      "learning_rate": 1.1329381879762914e-05,
      "loss": 0.9125,
      "step": 29510
    },
    {
      "epoch": 1.8707500237650114,
      "grad_norm": 2.5613579750061035,
      "learning_rate": 1.132303132938188e-05,
      "loss": 0.8368,
      "step": 29520
    },
    {
      "epoch": 1.871383757406762,
      "grad_norm": 2.9078283309936523,
      "learning_rate": 1.1316680779000846e-05,
      "loss": 0.8562,
      "step": 29530
    },
    {
      "epoch": 1.8720174910485123,
      "grad_norm": 2.9888978004455566,
      "learning_rate": 1.1310330228619815e-05,
      "loss": 0.8546,
      "step": 29540
    },
    {
      "epoch": 1.8726512246902627,
      "grad_norm": 2.5666842460632324,
      "learning_rate": 1.1303979678238781e-05,
      "loss": 0.8562,
      "step": 29550
    },
    {
      "epoch": 1.873284958332013,
      "grad_norm": 2.714390277862549,
      "learning_rate": 1.1297629127857747e-05,
      "loss": 0.8493,
      "step": 29560
    },
    {
      "epoch": 1.8739186919737634,
      "grad_norm": 2.3856112957000732,
      "learning_rate": 1.1291278577476714e-05,
      "loss": 0.8509,
      "step": 29570
    },
    {
      "epoch": 1.874552425615514,
      "grad_norm": 2.392064094543457,
      "learning_rate": 1.1284928027095682e-05,
      "loss": 0.8271,
      "step": 29580
    },
    {
      "epoch": 1.875186159257264,
      "grad_norm": 2.681485891342163,
      "learning_rate": 1.127857747671465e-05,
      "loss": 0.8882,
      "step": 29590
    },
    {
      "epoch": 1.8758198928990146,
      "grad_norm": 2.848389148712158,
      "learning_rate": 1.1272226926333615e-05,
      "loss": 0.8935,
      "step": 29600
    },
    {
      "epoch": 1.8764536265407648,
      "grad_norm": 2.2966277599334717,
      "learning_rate": 1.1265876375952583e-05,
      "loss": 0.8748,
      "step": 29610
    },
    {
      "epoch": 1.8770873601825153,
      "grad_norm": 2.5542728900909424,
      "learning_rate": 1.1259525825571551e-05,
      "loss": 0.8779,
      "step": 29620
    },
    {
      "epoch": 1.8777210938242657,
      "grad_norm": 2.5221219062805176,
      "learning_rate": 1.1253175275190516e-05,
      "loss": 0.8376,
      "step": 29630
    },
    {
      "epoch": 1.878354827466016,
      "grad_norm": 2.458489179611206,
      "learning_rate": 1.1246824724809484e-05,
      "loss": 0.8495,
      "step": 29640
    },
    {
      "epoch": 1.8789885611077664,
      "grad_norm": 2.8223378658294678,
      "learning_rate": 1.124047417442845e-05,
      "loss": 0.84,
      "step": 29650
    },
    {
      "epoch": 1.8796222947495167,
      "grad_norm": 2.5213096141815186,
      "learning_rate": 1.1234123624047418e-05,
      "loss": 0.8871,
      "step": 29660
    },
    {
      "epoch": 1.8802560283912673,
      "grad_norm": 2.6368298530578613,
      "learning_rate": 1.1227773073666385e-05,
      "loss": 0.8187,
      "step": 29670
    },
    {
      "epoch": 1.8808897620330174,
      "grad_norm": 2.59069561958313,
      "learning_rate": 1.1221422523285351e-05,
      "loss": 0.8694,
      "step": 29680
    },
    {
      "epoch": 1.881523495674768,
      "grad_norm": 2.6255335807800293,
      "learning_rate": 1.121507197290432e-05,
      "loss": 0.8563,
      "step": 29690
    },
    {
      "epoch": 1.8821572293165183,
      "grad_norm": 2.7000532150268555,
      "learning_rate": 1.1208721422523286e-05,
      "loss": 0.8988,
      "step": 29700
    },
    {
      "epoch": 1.8827909629582686,
      "grad_norm": 2.7455570697784424,
      "learning_rate": 1.1202370872142252e-05,
      "loss": 0.8724,
      "step": 29710
    },
    {
      "epoch": 1.883424696600019,
      "grad_norm": 2.6401729583740234,
      "learning_rate": 1.119602032176122e-05,
      "loss": 0.8647,
      "step": 29720
    },
    {
      "epoch": 1.8840584302417693,
      "grad_norm": 3.2321841716766357,
      "learning_rate": 1.1189669771380185e-05,
      "loss": 0.8548,
      "step": 29730
    },
    {
      "epoch": 1.88469216388352,
      "grad_norm": 2.462458848953247,
      "learning_rate": 1.1183319220999153e-05,
      "loss": 0.8252,
      "step": 29740
    },
    {
      "epoch": 1.88532589752527,
      "grad_norm": 2.64363431930542,
      "learning_rate": 1.1176968670618121e-05,
      "loss": 0.8731,
      "step": 29750
    },
    {
      "epoch": 1.8859596311670206,
      "grad_norm": 2.4831109046936035,
      "learning_rate": 1.1170618120237088e-05,
      "loss": 0.8282,
      "step": 29760
    },
    {
      "epoch": 1.8865933648087707,
      "grad_norm": 2.819014310836792,
      "learning_rate": 1.1164267569856054e-05,
      "loss": 0.8543,
      "step": 29770
    },
    {
      "epoch": 1.8872270984505213,
      "grad_norm": 2.735586643218994,
      "learning_rate": 1.115791701947502e-05,
      "loss": 0.8928,
      "step": 29780
    },
    {
      "epoch": 1.8878608320922716,
      "grad_norm": 2.676351308822632,
      "learning_rate": 1.1151566469093989e-05,
      "loss": 0.8363,
      "step": 29790
    },
    {
      "epoch": 1.888494565734022,
      "grad_norm": 3.3084356784820557,
      "learning_rate": 1.1145215918712955e-05,
      "loss": 0.8927,
      "step": 29800
    },
    {
      "epoch": 1.8891282993757723,
      "grad_norm": 2.741264581680298,
      "learning_rate": 1.1139500423370026e-05,
      "loss": 0.8896,
      "step": 29810
    },
    {
      "epoch": 1.8897620330175227,
      "grad_norm": 3.2904562950134277,
      "learning_rate": 1.1133149872988992e-05,
      "loss": 0.8403,
      "step": 29820
    },
    {
      "epoch": 1.8903957666592732,
      "grad_norm": 2.8190019130706787,
      "learning_rate": 1.112679932260796e-05,
      "loss": 0.8479,
      "step": 29830
    },
    {
      "epoch": 1.8910295003010233,
      "grad_norm": 2.525298595428467,
      "learning_rate": 1.1120448772226927e-05,
      "loss": 0.8232,
      "step": 29840
    },
    {
      "epoch": 1.891663233942774,
      "grad_norm": 2.619234561920166,
      "learning_rate": 1.1114098221845893e-05,
      "loss": 0.8662,
      "step": 29850
    },
    {
      "epoch": 1.8922969675845243,
      "grad_norm": 2.645672082901001,
      "learning_rate": 1.1107747671464861e-05,
      "loss": 0.8248,
      "step": 29860
    },
    {
      "epoch": 1.8929307012262746,
      "grad_norm": 2.6325337886810303,
      "learning_rate": 1.1101397121083828e-05,
      "loss": 0.8613,
      "step": 29870
    },
    {
      "epoch": 1.893564434868025,
      "grad_norm": 3.0437326431274414,
      "learning_rate": 1.1095046570702794e-05,
      "loss": 0.8874,
      "step": 29880
    },
    {
      "epoch": 1.8941981685097753,
      "grad_norm": 2.8761472702026367,
      "learning_rate": 1.108869602032176e-05,
      "loss": 0.8634,
      "step": 29890
    },
    {
      "epoch": 1.8948319021515259,
      "grad_norm": 2.5559158325195312,
      "learning_rate": 1.1082345469940729e-05,
      "loss": 0.856,
      "step": 29900
    },
    {
      "epoch": 1.895465635793276,
      "grad_norm": 3.3587539196014404,
      "learning_rate": 1.1075994919559697e-05,
      "loss": 0.8582,
      "step": 29910
    },
    {
      "epoch": 1.8960993694350265,
      "grad_norm": 2.6032023429870605,
      "learning_rate": 1.1069644369178661e-05,
      "loss": 0.8764,
      "step": 29920
    },
    {
      "epoch": 1.8967331030767767,
      "grad_norm": 2.5269546508789062,
      "learning_rate": 1.106329381879763e-05,
      "loss": 0.8334,
      "step": 29930
    },
    {
      "epoch": 1.8973668367185272,
      "grad_norm": 3.4996492862701416,
      "learning_rate": 1.1056943268416598e-05,
      "loss": 0.8398,
      "step": 29940
    },
    {
      "epoch": 1.8980005703602776,
      "grad_norm": 2.7919986248016357,
      "learning_rate": 1.1050592718035562e-05,
      "loss": 0.8536,
      "step": 29950
    },
    {
      "epoch": 1.898634304002028,
      "grad_norm": 2.5718865394592285,
      "learning_rate": 1.104424216765453e-05,
      "loss": 0.8501,
      "step": 29960
    },
    {
      "epoch": 1.8992680376437783,
      "grad_norm": 2.647106409072876,
      "learning_rate": 1.1037891617273497e-05,
      "loss": 0.8449,
      "step": 29970
    },
    {
      "epoch": 1.8999017712855286,
      "grad_norm": 2.853158473968506,
      "learning_rate": 1.1031541066892465e-05,
      "loss": 0.8878,
      "step": 29980
    },
    {
      "epoch": 1.9005355049272792,
      "grad_norm": 2.7341203689575195,
      "learning_rate": 1.1025190516511432e-05,
      "loss": 0.8648,
      "step": 29990
    },
    {
      "epoch": 1.9011692385690293,
      "grad_norm": 3.0254013538360596,
      "learning_rate": 1.1018839966130398e-05,
      "loss": 0.8589,
      "step": 30000
    },
    {
      "epoch": 1.9018029722107799,
      "grad_norm": 2.562361478805542,
      "learning_rate": 1.1012489415749366e-05,
      "loss": 0.8739,
      "step": 30010
    },
    {
      "epoch": 1.9024367058525302,
      "grad_norm": 3.0566136837005615,
      "learning_rate": 1.100613886536833e-05,
      "loss": 0.8634,
      "step": 30020
    },
    {
      "epoch": 1.9030704394942806,
      "grad_norm": 2.6550464630126953,
      "learning_rate": 1.0999788314987299e-05,
      "loss": 0.8552,
      "step": 30030
    },
    {
      "epoch": 1.903704173136031,
      "grad_norm": 2.5343403816223145,
      "learning_rate": 1.0993437764606267e-05,
      "loss": 0.8158,
      "step": 30040
    },
    {
      "epoch": 1.9043379067777813,
      "grad_norm": 2.6804521083831787,
      "learning_rate": 1.0987087214225233e-05,
      "loss": 0.822,
      "step": 30050
    },
    {
      "epoch": 1.9049716404195318,
      "grad_norm": 2.7965056896209717,
      "learning_rate": 1.09807366638442e-05,
      "loss": 0.8562,
      "step": 30060
    },
    {
      "epoch": 1.905605374061282,
      "grad_norm": 2.272228956222534,
      "learning_rate": 1.0974386113463168e-05,
      "loss": 0.8791,
      "step": 30070
    },
    {
      "epoch": 1.9062391077030325,
      "grad_norm": 2.8638007640838623,
      "learning_rate": 1.0968035563082134e-05,
      "loss": 0.8411,
      "step": 30080
    },
    {
      "epoch": 1.9068728413447826,
      "grad_norm": 3.238093137741089,
      "learning_rate": 1.0961685012701101e-05,
      "loss": 0.8863,
      "step": 30090
    },
    {
      "epoch": 1.9075065749865332,
      "grad_norm": 2.3447415828704834,
      "learning_rate": 1.0955334462320067e-05,
      "loss": 0.8709,
      "step": 30100
    },
    {
      "epoch": 1.9081403086282835,
      "grad_norm": 3.082036256790161,
      "learning_rate": 1.0948983911939035e-05,
      "loss": 0.874,
      "step": 30110
    },
    {
      "epoch": 1.9087740422700339,
      "grad_norm": 2.5880088806152344,
      "learning_rate": 1.0942633361558002e-05,
      "loss": 0.8587,
      "step": 30120
    },
    {
      "epoch": 1.9094077759117842,
      "grad_norm": 2.576596975326538,
      "learning_rate": 1.0936282811176968e-05,
      "loss": 0.8649,
      "step": 30130
    },
    {
      "epoch": 1.9100415095535346,
      "grad_norm": 2.73068904876709,
      "learning_rate": 1.0929932260795936e-05,
      "loss": 0.853,
      "step": 30140
    },
    {
      "epoch": 1.9106752431952851,
      "grad_norm": 2.9898862838745117,
      "learning_rate": 1.0923581710414903e-05,
      "loss": 0.8755,
      "step": 30150
    },
    {
      "epoch": 1.9113089768370353,
      "grad_norm": 2.4975030422210693,
      "learning_rate": 1.091723116003387e-05,
      "loss": 0.8547,
      "step": 30160
    },
    {
      "epoch": 1.9119427104787858,
      "grad_norm": 2.584000825881958,
      "learning_rate": 1.0910880609652837e-05,
      "loss": 0.898,
      "step": 30170
    },
    {
      "epoch": 1.9125764441205362,
      "grad_norm": 2.8231945037841797,
      "learning_rate": 1.0904530059271804e-05,
      "loss": 0.8298,
      "step": 30180
    },
    {
      "epoch": 1.9132101777622865,
      "grad_norm": 2.556502103805542,
      "learning_rate": 1.089817950889077e-05,
      "loss": 0.8394,
      "step": 30190
    },
    {
      "epoch": 1.9138439114040369,
      "grad_norm": 2.533679962158203,
      "learning_rate": 1.0891828958509738e-05,
      "loss": 0.8282,
      "step": 30200
    },
    {
      "epoch": 1.9144776450457872,
      "grad_norm": 2.392970323562622,
      "learning_rate": 1.0885478408128705e-05,
      "loss": 0.8282,
      "step": 30210
    },
    {
      "epoch": 1.9151113786875378,
      "grad_norm": 2.81426739692688,
      "learning_rate": 1.0879127857747673e-05,
      "loss": 0.8955,
      "step": 30220
    },
    {
      "epoch": 1.915745112329288,
      "grad_norm": 2.277414321899414,
      "learning_rate": 1.0872777307366638e-05,
      "loss": 0.8567,
      "step": 30230
    },
    {
      "epoch": 1.9163788459710385,
      "grad_norm": 2.288482427597046,
      "learning_rate": 1.0866426756985606e-05,
      "loss": 0.8214,
      "step": 30240
    },
    {
      "epoch": 1.9170125796127886,
      "grad_norm": 2.868942975997925,
      "learning_rate": 1.0860076206604574e-05,
      "loss": 0.8318,
      "step": 30250
    },
    {
      "epoch": 1.9176463132545392,
      "grad_norm": 2.6535706520080566,
      "learning_rate": 1.0853725656223539e-05,
      "loss": 0.7765,
      "step": 30260
    },
    {
      "epoch": 1.9182800468962895,
      "grad_norm": 3.179180860519409,
      "learning_rate": 1.0847375105842507e-05,
      "loss": 0.8716,
      "step": 30270
    },
    {
      "epoch": 1.9189137805380398,
      "grad_norm": 3.8175158500671387,
      "learning_rate": 1.0841024555461473e-05,
      "loss": 0.8662,
      "step": 30280
    },
    {
      "epoch": 1.9195475141797902,
      "grad_norm": 2.8832204341888428,
      "learning_rate": 1.0834674005080441e-05,
      "loss": 0.85,
      "step": 30290
    },
    {
      "epoch": 1.9201812478215405,
      "grad_norm": 2.4109673500061035,
      "learning_rate": 1.0828323454699408e-05,
      "loss": 0.8448,
      "step": 30300
    },
    {
      "epoch": 1.920814981463291,
      "grad_norm": 2.906843423843384,
      "learning_rate": 1.0821972904318374e-05,
      "loss": 0.8735,
      "step": 30310
    },
    {
      "epoch": 1.9214487151050412,
      "grad_norm": 2.607823133468628,
      "learning_rate": 1.0815622353937342e-05,
      "loss": 0.8239,
      "step": 30320
    },
    {
      "epoch": 1.9220824487467918,
      "grad_norm": 2.3171229362487793,
      "learning_rate": 1.0809271803556309e-05,
      "loss": 0.8426,
      "step": 30330
    },
    {
      "epoch": 1.9227161823885421,
      "grad_norm": 3.0034351348876953,
      "learning_rate": 1.0802921253175275e-05,
      "loss": 0.8553,
      "step": 30340
    },
    {
      "epoch": 1.9233499160302925,
      "grad_norm": 2.2278096675872803,
      "learning_rate": 1.0796570702794243e-05,
      "loss": 0.7897,
      "step": 30350
    },
    {
      "epoch": 1.9239836496720428,
      "grad_norm": 2.6598784923553467,
      "learning_rate": 1.0790220152413208e-05,
      "loss": 0.8113,
      "step": 30360
    },
    {
      "epoch": 1.9246173833137932,
      "grad_norm": 2.562654733657837,
      "learning_rate": 1.0783869602032176e-05,
      "loss": 0.804,
      "step": 30370
    },
    {
      "epoch": 1.9252511169555437,
      "grad_norm": 2.8371682167053223,
      "learning_rate": 1.0777519051651144e-05,
      "loss": 0.8689,
      "step": 30380
    },
    {
      "epoch": 1.9258848505972939,
      "grad_norm": 3.284414052963257,
      "learning_rate": 1.077116850127011e-05,
      "loss": 0.8214,
      "step": 30390
    },
    {
      "epoch": 1.9265185842390444,
      "grad_norm": 2.841559410095215,
      "learning_rate": 1.0764817950889077e-05,
      "loss": 0.8812,
      "step": 30400
    },
    {
      "epoch": 1.9271523178807946,
      "grad_norm": 2.8569953441619873,
      "learning_rate": 1.0758467400508043e-05,
      "loss": 0.8126,
      "step": 30410
    },
    {
      "epoch": 1.9277860515225451,
      "grad_norm": 2.8193519115448,
      "learning_rate": 1.0752116850127012e-05,
      "loss": 0.8903,
      "step": 30420
    },
    {
      "epoch": 1.9284197851642955,
      "grad_norm": 2.655332088470459,
      "learning_rate": 1.0745766299745978e-05,
      "loss": 0.8755,
      "step": 30430
    },
    {
      "epoch": 1.9290535188060458,
      "grad_norm": 2.7595231533050537,
      "learning_rate": 1.0739415749364944e-05,
      "loss": 0.8944,
      "step": 30440
    },
    {
      "epoch": 1.9296872524477962,
      "grad_norm": 2.531388998031616,
      "learning_rate": 1.0733065198983913e-05,
      "loss": 0.9295,
      "step": 30450
    },
    {
      "epoch": 1.9303209860895465,
      "grad_norm": 2.6724624633789062,
      "learning_rate": 1.072671464860288e-05,
      "loss": 0.8395,
      "step": 30460
    },
    {
      "epoch": 1.930954719731297,
      "grad_norm": 2.3012027740478516,
      "learning_rate": 1.0720364098221845e-05,
      "loss": 0.843,
      "step": 30470
    },
    {
      "epoch": 1.9315884533730472,
      "grad_norm": 2.4885308742523193,
      "learning_rate": 1.0714013547840814e-05,
      "loss": 0.8586,
      "step": 30480
    },
    {
      "epoch": 1.9322221870147978,
      "grad_norm": 2.7189698219299316,
      "learning_rate": 1.070766299745978e-05,
      "loss": 0.8473,
      "step": 30490
    },
    {
      "epoch": 1.932855920656548,
      "grad_norm": 2.768721342086792,
      "learning_rate": 1.0701312447078746e-05,
      "loss": 0.8469,
      "step": 30500
    },
    {
      "epoch": 1.9334896542982984,
      "grad_norm": 2.9637563228607178,
      "learning_rate": 1.0694961896697715e-05,
      "loss": 0.8425,
      "step": 30510
    },
    {
      "epoch": 1.9341233879400488,
      "grad_norm": 3.044433355331421,
      "learning_rate": 1.0688611346316681e-05,
      "loss": 0.8655,
      "step": 30520
    },
    {
      "epoch": 1.9347571215817991,
      "grad_norm": 2.8821017742156982,
      "learning_rate": 1.0682260795935649e-05,
      "loss": 0.8267,
      "step": 30530
    },
    {
      "epoch": 1.9353908552235497,
      "grad_norm": 2.581129312515259,
      "learning_rate": 1.0675910245554614e-05,
      "loss": 0.7996,
      "step": 30540
    },
    {
      "epoch": 1.9360245888652998,
      "grad_norm": 2.5481772422790527,
      "learning_rate": 1.0669559695173582e-05,
      "loss": 0.8705,
      "step": 30550
    },
    {
      "epoch": 1.9366583225070504,
      "grad_norm": 2.8851377964019775,
      "learning_rate": 1.066320914479255e-05,
      "loss": 0.8338,
      "step": 30560
    },
    {
      "epoch": 1.9372920561488005,
      "grad_norm": 2.6491005420684814,
      "learning_rate": 1.0656858594411515e-05,
      "loss": 0.8138,
      "step": 30570
    },
    {
      "epoch": 1.937925789790551,
      "grad_norm": 2.915966033935547,
      "learning_rate": 1.0650508044030483e-05,
      "loss": 0.8546,
      "step": 30580
    },
    {
      "epoch": 1.9385595234323014,
      "grad_norm": 3.076747179031372,
      "learning_rate": 1.0644157493649451e-05,
      "loss": 0.8478,
      "step": 30590
    },
    {
      "epoch": 1.9391932570740518,
      "grad_norm": 2.9440340995788574,
      "learning_rate": 1.0637806943268417e-05,
      "loss": 0.8618,
      "step": 30600
    },
    {
      "epoch": 1.9398269907158021,
      "grad_norm": 2.848396062850952,
      "learning_rate": 1.0631456392887384e-05,
      "loss": 0.877,
      "step": 30610
    },
    {
      "epoch": 1.9404607243575525,
      "grad_norm": 2.9242002964019775,
      "learning_rate": 1.062510584250635e-05,
      "loss": 0.8154,
      "step": 30620
    },
    {
      "epoch": 1.941094457999303,
      "grad_norm": 2.5895490646362305,
      "learning_rate": 1.0618755292125318e-05,
      "loss": 0.8232,
      "step": 30630
    },
    {
      "epoch": 1.9417281916410531,
      "grad_norm": 2.3967320919036865,
      "learning_rate": 1.0612404741744285e-05,
      "loss": 0.8361,
      "step": 30640
    },
    {
      "epoch": 1.9423619252828037,
      "grad_norm": 3.614881992340088,
      "learning_rate": 1.0606054191363251e-05,
      "loss": 0.8562,
      "step": 30650
    },
    {
      "epoch": 1.942995658924554,
      "grad_norm": 2.853235960006714,
      "learning_rate": 1.059970364098222e-05,
      "loss": 0.8673,
      "step": 30660
    },
    {
      "epoch": 1.9436293925663044,
      "grad_norm": 2.587618827819824,
      "learning_rate": 1.0593353090601184e-05,
      "loss": 0.8342,
      "step": 30670
    },
    {
      "epoch": 1.9442631262080547,
      "grad_norm": 2.589139223098755,
      "learning_rate": 1.0587002540220152e-05,
      "loss": 0.8238,
      "step": 30680
    },
    {
      "epoch": 1.944896859849805,
      "grad_norm": 2.761319398880005,
      "learning_rate": 1.058065198983912e-05,
      "loss": 0.9029,
      "step": 30690
    },
    {
      "epoch": 1.9455305934915557,
      "grad_norm": 2.9679367542266846,
      "learning_rate": 1.0574301439458087e-05,
      "loss": 0.8667,
      "step": 30700
    },
    {
      "epoch": 1.9461643271333058,
      "grad_norm": 2.465158224105835,
      "learning_rate": 1.0567950889077053e-05,
      "loss": 0.8314,
      "step": 30710
    },
    {
      "epoch": 1.9467980607750563,
      "grad_norm": 2.822359085083008,
      "learning_rate": 1.0561600338696021e-05,
      "loss": 0.8562,
      "step": 30720
    },
    {
      "epoch": 1.9474317944168065,
      "grad_norm": 2.899162530899048,
      "learning_rate": 1.0555249788314988e-05,
      "loss": 0.9011,
      "step": 30730
    },
    {
      "epoch": 1.948065528058557,
      "grad_norm": 3.0702383518218994,
      "learning_rate": 1.0548899237933954e-05,
      "loss": 0.8865,
      "step": 30740
    },
    {
      "epoch": 1.9486992617003074,
      "grad_norm": 2.4017529487609863,
      "learning_rate": 1.054254868755292e-05,
      "loss": 0.9004,
      "step": 30750
    },
    {
      "epoch": 1.9493329953420577,
      "grad_norm": 3.1608290672302246,
      "learning_rate": 1.0536198137171889e-05,
      "loss": 0.8887,
      "step": 30760
    },
    {
      "epoch": 1.949966728983808,
      "grad_norm": 2.6730051040649414,
      "learning_rate": 1.0529847586790857e-05,
      "loss": 0.8198,
      "step": 30770
    },
    {
      "epoch": 1.9506004626255584,
      "grad_norm": 2.855455160140991,
      "learning_rate": 1.0523497036409822e-05,
      "loss": 0.8834,
      "step": 30780
    },
    {
      "epoch": 1.951234196267309,
      "grad_norm": 2.398484706878662,
      "learning_rate": 1.051714648602879e-05,
      "loss": 0.8581,
      "step": 30790
    },
    {
      "epoch": 1.951867929909059,
      "grad_norm": 2.7167575359344482,
      "learning_rate": 1.0510795935647756e-05,
      "loss": 0.8604,
      "step": 30800
    },
    {
      "epoch": 1.9525016635508097,
      "grad_norm": 2.2668910026550293,
      "learning_rate": 1.0504445385266723e-05,
      "loss": 0.8826,
      "step": 30810
    },
    {
      "epoch": 1.95313539719256,
      "grad_norm": 2.5223891735076904,
      "learning_rate": 1.049809483488569e-05,
      "loss": 0.8449,
      "step": 30820
    },
    {
      "epoch": 1.9537691308343104,
      "grad_norm": 2.675240993499756,
      "learning_rate": 1.0491744284504657e-05,
      "loss": 0.8618,
      "step": 30830
    },
    {
      "epoch": 1.9544028644760607,
      "grad_norm": 3.0656442642211914,
      "learning_rate": 1.0485393734123625e-05,
      "loss": 0.835,
      "step": 30840
    },
    {
      "epoch": 1.955036598117811,
      "grad_norm": 2.7411437034606934,
      "learning_rate": 1.0479043183742592e-05,
      "loss": 0.8352,
      "step": 30850
    },
    {
      "epoch": 1.9556703317595616,
      "grad_norm": 2.624750852584839,
      "learning_rate": 1.0472692633361558e-05,
      "loss": 0.8564,
      "step": 30860
    },
    {
      "epoch": 1.9563040654013117,
      "grad_norm": 2.7975118160247803,
      "learning_rate": 1.0466342082980526e-05,
      "loss": 0.8277,
      "step": 30870
    },
    {
      "epoch": 1.9569377990430623,
      "grad_norm": 2.6049811840057373,
      "learning_rate": 1.0459991532599491e-05,
      "loss": 0.8757,
      "step": 30880
    },
    {
      "epoch": 1.9575715326848124,
      "grad_norm": 3.265878438949585,
      "learning_rate": 1.0453640982218459e-05,
      "loss": 0.9072,
      "step": 30890
    },
    {
      "epoch": 1.958205266326563,
      "grad_norm": 2.4466707706451416,
      "learning_rate": 1.0447290431837427e-05,
      "loss": 0.8315,
      "step": 30900
    },
    {
      "epoch": 1.9588389999683133,
      "grad_norm": 2.9929866790771484,
      "learning_rate": 1.0440939881456392e-05,
      "loss": 0.8792,
      "step": 30910
    },
    {
      "epoch": 1.9594727336100637,
      "grad_norm": 2.4831812381744385,
      "learning_rate": 1.043458933107536e-05,
      "loss": 0.8098,
      "step": 30920
    },
    {
      "epoch": 1.960106467251814,
      "grad_norm": 2.8898754119873047,
      "learning_rate": 1.0428238780694326e-05,
      "loss": 0.8662,
      "step": 30930
    },
    {
      "epoch": 1.9607402008935644,
      "grad_norm": 2.511672019958496,
      "learning_rate": 1.0421888230313295e-05,
      "loss": 0.8298,
      "step": 30940
    },
    {
      "epoch": 1.961373934535315,
      "grad_norm": 2.485612630844116,
      "learning_rate": 1.0415537679932261e-05,
      "loss": 0.8755,
      "step": 30950
    },
    {
      "epoch": 1.962007668177065,
      "grad_norm": 2.5882534980773926,
      "learning_rate": 1.0409187129551227e-05,
      "loss": 0.8636,
      "step": 30960
    },
    {
      "epoch": 1.9626414018188156,
      "grad_norm": 3.0242679119110107,
      "learning_rate": 1.0402836579170196e-05,
      "loss": 0.8299,
      "step": 30970
    },
    {
      "epoch": 1.963275135460566,
      "grad_norm": 2.648902654647827,
      "learning_rate": 1.0396486028789162e-05,
      "loss": 0.8313,
      "step": 30980
    },
    {
      "epoch": 1.9639088691023163,
      "grad_norm": 3.015584945678711,
      "learning_rate": 1.0390135478408128e-05,
      "loss": 0.891,
      "step": 30990
    },
    {
      "epoch": 1.9645426027440667,
      "grad_norm": 2.9314918518066406,
      "learning_rate": 1.0383784928027097e-05,
      "loss": 0.8683,
      "step": 31000
    },
    {
      "epoch": 1.965176336385817,
      "grad_norm": 2.5823795795440674,
      "learning_rate": 1.0377434377646063e-05,
      "loss": 0.8583,
      "step": 31010
    },
    {
      "epoch": 1.9658100700275676,
      "grad_norm": 2.4185826778411865,
      "learning_rate": 1.037108382726503e-05,
      "loss": 0.8691,
      "step": 31020
    },
    {
      "epoch": 1.9664438036693177,
      "grad_norm": 3.079667568206787,
      "learning_rate": 1.0364733276883998e-05,
      "loss": 0.8451,
      "step": 31030
    },
    {
      "epoch": 1.9670775373110683,
      "grad_norm": 2.5286920070648193,
      "learning_rate": 1.0358382726502964e-05,
      "loss": 0.8571,
      "step": 31040
    },
    {
      "epoch": 1.9677112709528184,
      "grad_norm": 2.419084072113037,
      "learning_rate": 1.035203217612193e-05,
      "loss": 0.8625,
      "step": 31050
    },
    {
      "epoch": 1.968345004594569,
      "grad_norm": 2.4118287563323975,
      "learning_rate": 1.0345681625740897e-05,
      "loss": 0.8936,
      "step": 31060
    },
    {
      "epoch": 1.9689787382363193,
      "grad_norm": 2.5404787063598633,
      "learning_rate": 1.0339331075359865e-05,
      "loss": 0.8579,
      "step": 31070
    },
    {
      "epoch": 1.9696124718780696,
      "grad_norm": 2.496903896331787,
      "learning_rate": 1.0332980524978833e-05,
      "loss": 0.879,
      "step": 31080
    },
    {
      "epoch": 1.97024620551982,
      "grad_norm": 3.15539288520813,
      "learning_rate": 1.0326629974597798e-05,
      "loss": 0.8501,
      "step": 31090
    },
    {
      "epoch": 1.9708799391615703,
      "grad_norm": 2.511687755584717,
      "learning_rate": 1.0320279424216766e-05,
      "loss": 0.8238,
      "step": 31100
    },
    {
      "epoch": 1.971513672803321,
      "grad_norm": 3.0100133419036865,
      "learning_rate": 1.0313928873835734e-05,
      "loss": 0.8221,
      "step": 31110
    },
    {
      "epoch": 1.972147406445071,
      "grad_norm": 2.4924685955047607,
      "learning_rate": 1.0307578323454699e-05,
      "loss": 0.8582,
      "step": 31120
    },
    {
      "epoch": 1.9727811400868216,
      "grad_norm": 2.3177943229675293,
      "learning_rate": 1.0301227773073667e-05,
      "loss": 0.8418,
      "step": 31130
    },
    {
      "epoch": 1.9734148737285717,
      "grad_norm": 2.6159627437591553,
      "learning_rate": 1.0294877222692633e-05,
      "loss": 0.8526,
      "step": 31140
    },
    {
      "epoch": 1.9740486073703223,
      "grad_norm": 2.336392402648926,
      "learning_rate": 1.0288526672311601e-05,
      "loss": 0.8456,
      "step": 31150
    },
    {
      "epoch": 1.9746823410120726,
      "grad_norm": 2.2396535873413086,
      "learning_rate": 1.0282176121930568e-05,
      "loss": 0.8625,
      "step": 31160
    },
    {
      "epoch": 1.975316074653823,
      "grad_norm": 3.084343910217285,
      "learning_rate": 1.0275825571549534e-05,
      "loss": 0.8383,
      "step": 31170
    },
    {
      "epoch": 1.9759498082955733,
      "grad_norm": 2.465334415435791,
      "learning_rate": 1.0269475021168502e-05,
      "loss": 0.9269,
      "step": 31180
    },
    {
      "epoch": 1.9765835419373237,
      "grad_norm": 3.3042802810668945,
      "learning_rate": 1.0263124470787467e-05,
      "loss": 0.8373,
      "step": 31190
    },
    {
      "epoch": 1.9772172755790742,
      "grad_norm": 2.5802900791168213,
      "learning_rate": 1.0256773920406435e-05,
      "loss": 0.8569,
      "step": 31200
    },
    {
      "epoch": 1.9778510092208244,
      "grad_norm": 4.019853591918945,
      "learning_rate": 1.0250423370025403e-05,
      "loss": 0.8721,
      "step": 31210
    },
    {
      "epoch": 1.978484742862575,
      "grad_norm": 2.5555858612060547,
      "learning_rate": 1.0244072819644368e-05,
      "loss": 0.8763,
      "step": 31220
    },
    {
      "epoch": 1.9791184765043253,
      "grad_norm": 2.3216519355773926,
      "learning_rate": 1.0237722269263336e-05,
      "loss": 0.8588,
      "step": 31230
    },
    {
      "epoch": 1.9797522101460756,
      "grad_norm": 2.5932135581970215,
      "learning_rate": 1.0231371718882304e-05,
      "loss": 0.8563,
      "step": 31240
    },
    {
      "epoch": 1.980385943787826,
      "grad_norm": 3.415790557861328,
      "learning_rate": 1.022502116850127e-05,
      "loss": 0.846,
      "step": 31250
    },
    {
      "epoch": 1.9810196774295763,
      "grad_norm": 2.792184591293335,
      "learning_rate": 1.0218670618120237e-05,
      "loss": 0.8401,
      "step": 31260
    },
    {
      "epoch": 1.9816534110713269,
      "grad_norm": 2.6751859188079834,
      "learning_rate": 1.0212320067739204e-05,
      "loss": 0.8387,
      "step": 31270
    },
    {
      "epoch": 1.982287144713077,
      "grad_norm": 2.4678914546966553,
      "learning_rate": 1.0205969517358172e-05,
      "loss": 0.8629,
      "step": 31280
    },
    {
      "epoch": 1.9829208783548276,
      "grad_norm": 2.6356191635131836,
      "learning_rate": 1.0199618966977138e-05,
      "loss": 0.8689,
      "step": 31290
    },
    {
      "epoch": 1.9835546119965777,
      "grad_norm": 2.9187896251678467,
      "learning_rate": 1.0193268416596105e-05,
      "loss": 0.8878,
      "step": 31300
    },
    {
      "epoch": 1.9841883456383282,
      "grad_norm": 2.5562479496002197,
      "learning_rate": 1.0186917866215073e-05,
      "loss": 0.8408,
      "step": 31310
    },
    {
      "epoch": 1.9848220792800786,
      "grad_norm": 2.703686237335205,
      "learning_rate": 1.0180567315834039e-05,
      "loss": 0.8493,
      "step": 31320
    },
    {
      "epoch": 1.985455812921829,
      "grad_norm": 2.1673338413238525,
      "learning_rate": 1.0174216765453006e-05,
      "loss": 0.8105,
      "step": 31330
    },
    {
      "epoch": 1.9860895465635793,
      "grad_norm": 2.5201470851898193,
      "learning_rate": 1.0167866215071974e-05,
      "loss": 0.8294,
      "step": 31340
    },
    {
      "epoch": 1.9867232802053296,
      "grad_norm": 2.3104374408721924,
      "learning_rate": 1.016151566469094e-05,
      "loss": 0.8621,
      "step": 31350
    },
    {
      "epoch": 1.9873570138470802,
      "grad_norm": 3.216722249984741,
      "learning_rate": 1.0155165114309907e-05,
      "loss": 0.8598,
      "step": 31360
    },
    {
      "epoch": 1.9879907474888303,
      "grad_norm": 2.560537338256836,
      "learning_rate": 1.0148814563928875e-05,
      "loss": 0.8198,
      "step": 31370
    },
    {
      "epoch": 1.9886244811305809,
      "grad_norm": 2.6960928440093994,
      "learning_rate": 1.0142464013547841e-05,
      "loss": 0.8219,
      "step": 31380
    },
    {
      "epoch": 1.9892582147723312,
      "grad_norm": 2.7863097190856934,
      "learning_rate": 1.013611346316681e-05,
      "loss": 0.8427,
      "step": 31390
    },
    {
      "epoch": 1.9898919484140816,
      "grad_norm": 2.6292552947998047,
      "learning_rate": 1.0129762912785774e-05,
      "loss": 0.8473,
      "step": 31400
    },
    {
      "epoch": 1.990525682055832,
      "grad_norm": 2.515061855316162,
      "learning_rate": 1.0123412362404742e-05,
      "loss": 0.8465,
      "step": 31410
    },
    {
      "epoch": 1.9911594156975823,
      "grad_norm": 3.3715667724609375,
      "learning_rate": 1.011706181202371e-05,
      "loss": 0.8204,
      "step": 31420
    },
    {
      "epoch": 1.9917931493393328,
      "grad_norm": 3.2477011680603027,
      "learning_rate": 1.0110711261642675e-05,
      "loss": 0.8755,
      "step": 31430
    },
    {
      "epoch": 1.992426882981083,
      "grad_norm": 2.758781909942627,
      "learning_rate": 1.0104360711261643e-05,
      "loss": 0.8773,
      "step": 31440
    },
    {
      "epoch": 1.9930606166228335,
      "grad_norm": 3.004657030105591,
      "learning_rate": 1.009801016088061e-05,
      "loss": 0.8795,
      "step": 31450
    },
    {
      "epoch": 1.9936943502645836,
      "grad_norm": 2.3909401893615723,
      "learning_rate": 1.0091659610499576e-05,
      "loss": 0.897,
      "step": 31460
    },
    {
      "epoch": 1.9943280839063342,
      "grad_norm": 2.6620898246765137,
      "learning_rate": 1.0085309060118544e-05,
      "loss": 0.8516,
      "step": 31470
    },
    {
      "epoch": 1.9949618175480845,
      "grad_norm": 2.3336384296417236,
      "learning_rate": 1.007895850973751e-05,
      "loss": 0.895,
      "step": 31480
    },
    {
      "epoch": 1.995595551189835,
      "grad_norm": 3.046694040298462,
      "learning_rate": 1.0072607959356479e-05,
      "loss": 0.8882,
      "step": 31490
    },
    {
      "epoch": 1.9962292848315852,
      "grad_norm": 2.322707414627075,
      "learning_rate": 1.0066257408975445e-05,
      "loss": 0.8522,
      "step": 31500
    },
    {
      "epoch": 1.9968630184733356,
      "grad_norm": 2.7349605560302734,
      "learning_rate": 1.0059906858594411e-05,
      "loss": 0.8384,
      "step": 31510
    },
    {
      "epoch": 1.9974967521150861,
      "grad_norm": 2.9782931804656982,
      "learning_rate": 1.005355630821338e-05,
      "loss": 0.8131,
      "step": 31520
    },
    {
      "epoch": 1.9981304857568363,
      "grad_norm": 2.4835660457611084,
      "learning_rate": 1.0047205757832344e-05,
      "loss": 0.8382,
      "step": 31530
    },
    {
      "epoch": 1.9987642193985868,
      "grad_norm": 3.7549209594726562,
      "learning_rate": 1.0040855207451312e-05,
      "loss": 0.8232,
      "step": 31540
    },
    {
      "epoch": 1.9993979530403372,
      "grad_norm": 2.478084087371826,
      "learning_rate": 1.003450465707028e-05,
      "loss": 0.8426,
      "step": 31550
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.940671682357788,
      "learning_rate": 1.0028154106689247e-05,
      "loss": 0.8215,
      "step": 31560
    },
    {
      "epoch": 2.0006337336417506,
      "grad_norm": 2.4948368072509766,
      "learning_rate": 1.0021803556308213e-05,
      "loss": 0.82,
      "step": 31570
    },
    {
      "epoch": 2.0012674672835007,
      "grad_norm": 2.5021824836730957,
      "learning_rate": 1.001545300592718e-05,
      "loss": 0.8218,
      "step": 31580
    },
    {
      "epoch": 2.0019012009252513,
      "grad_norm": 2.663914203643799,
      "learning_rate": 1.0009102455546148e-05,
      "loss": 0.8019,
      "step": 31590
    },
    {
      "epoch": 2.0025349345670014,
      "grad_norm": 2.2803006172180176,
      "learning_rate": 1.0002751905165114e-05,
      "loss": 0.8536,
      "step": 31600
    },
    {
      "epoch": 2.003168668208752,
      "grad_norm": 2.865565776824951,
      "learning_rate": 9.99640135478408e-06,
      "loss": 0.8581,
      "step": 31610
    },
    {
      "epoch": 2.003802401850502,
      "grad_norm": 4.6186089515686035,
      "learning_rate": 9.990050804403049e-06,
      "loss": 0.9175,
      "step": 31620
    },
    {
      "epoch": 2.0044361354922526,
      "grad_norm": 2.56821346282959,
      "learning_rate": 9.983700254022017e-06,
      "loss": 0.8557,
      "step": 31630
    },
    {
      "epoch": 2.0050698691340028,
      "grad_norm": 2.3846418857574463,
      "learning_rate": 9.977349703640982e-06,
      "loss": 0.8423,
      "step": 31640
    },
    {
      "epoch": 2.0057036027757533,
      "grad_norm": 2.450451135635376,
      "learning_rate": 9.97099915325995e-06,
      "loss": 0.8739,
      "step": 31650
    },
    {
      "epoch": 2.006337336417504,
      "grad_norm": 2.6635820865631104,
      "learning_rate": 9.964648602878916e-06,
      "loss": 0.9085,
      "step": 31660
    },
    {
      "epoch": 2.006971070059254,
      "grad_norm": 2.785722255706787,
      "learning_rate": 9.958298052497883e-06,
      "loss": 0.904,
      "step": 31670
    },
    {
      "epoch": 2.0076048037010046,
      "grad_norm": 2.4105846881866455,
      "learning_rate": 9.951947502116851e-06,
      "loss": 0.8577,
      "step": 31680
    },
    {
      "epoch": 2.0082385373427547,
      "grad_norm": 2.2884178161621094,
      "learning_rate": 9.945596951735817e-06,
      "loss": 0.8219,
      "step": 31690
    },
    {
      "epoch": 2.0088722709845053,
      "grad_norm": 2.795210838317871,
      "learning_rate": 9.939246401354785e-06,
      "loss": 0.8568,
      "step": 31700
    },
    {
      "epoch": 2.0095060046262554,
      "grad_norm": 2.3419997692108154,
      "learning_rate": 9.93289585097375e-06,
      "loss": 0.8458,
      "step": 31710
    },
    {
      "epoch": 2.010139738268006,
      "grad_norm": 2.855912685394287,
      "learning_rate": 9.926545300592718e-06,
      "loss": 0.8862,
      "step": 31720
    },
    {
      "epoch": 2.0107734719097565,
      "grad_norm": 2.827962875366211,
      "learning_rate": 9.920194750211686e-06,
      "loss": 0.8798,
      "step": 31730
    },
    {
      "epoch": 2.0114072055515066,
      "grad_norm": 2.5441927909851074,
      "learning_rate": 9.913844199830651e-06,
      "loss": 0.8406,
      "step": 31740
    },
    {
      "epoch": 2.012040939193257,
      "grad_norm": 2.7781357765197754,
      "learning_rate": 9.90749364944962e-06,
      "loss": 0.8428,
      "step": 31750
    },
    {
      "epoch": 2.0126746728350073,
      "grad_norm": 2.846371650695801,
      "learning_rate": 9.901143099068587e-06,
      "loss": 0.8478,
      "step": 31760
    },
    {
      "epoch": 2.013308406476758,
      "grad_norm": 2.532940626144409,
      "learning_rate": 9.894792548687552e-06,
      "loss": 0.9146,
      "step": 31770
    },
    {
      "epoch": 2.013942140118508,
      "grad_norm": 2.4211578369140625,
      "learning_rate": 9.88844199830652e-06,
      "loss": 0.8166,
      "step": 31780
    },
    {
      "epoch": 2.0145758737602586,
      "grad_norm": 2.5826032161712646,
      "learning_rate": 9.882091447925487e-06,
      "loss": 0.8661,
      "step": 31790
    },
    {
      "epoch": 2.0152096074020087,
      "grad_norm": 2.292523145675659,
      "learning_rate": 9.875740897544455e-06,
      "loss": 0.8254,
      "step": 31800
    },
    {
      "epoch": 2.0158433410437593,
      "grad_norm": 2.295640230178833,
      "learning_rate": 9.869390347163421e-06,
      "loss": 0.8378,
      "step": 31810
    },
    {
      "epoch": 2.01647707468551,
      "grad_norm": 2.331758737564087,
      "learning_rate": 9.863039796782388e-06,
      "loss": 0.8268,
      "step": 31820
    },
    {
      "epoch": 2.01711080832726,
      "grad_norm": 2.920112371444702,
      "learning_rate": 9.856689246401356e-06,
      "loss": 0.8356,
      "step": 31830
    },
    {
      "epoch": 2.0177445419690105,
      "grad_norm": 2.579237937927246,
      "learning_rate": 9.85033869602032e-06,
      "loss": 0.8278,
      "step": 31840
    },
    {
      "epoch": 2.0183782756107607,
      "grad_norm": 2.715029239654541,
      "learning_rate": 9.843988145639289e-06,
      "loss": 0.8362,
      "step": 31850
    },
    {
      "epoch": 2.0190120092525112,
      "grad_norm": 2.709301471710205,
      "learning_rate": 9.837637595258257e-06,
      "loss": 0.8468,
      "step": 31860
    },
    {
      "epoch": 2.0196457428942614,
      "grad_norm": 2.726694107055664,
      "learning_rate": 9.831287044877223e-06,
      "loss": 0.8587,
      "step": 31870
    },
    {
      "epoch": 2.020279476536012,
      "grad_norm": 2.8964571952819824,
      "learning_rate": 9.82493649449619e-06,
      "loss": 0.8173,
      "step": 31880
    },
    {
      "epoch": 2.0209132101777625,
      "grad_norm": 2.5018367767333984,
      "learning_rate": 9.818585944115158e-06,
      "loss": 0.8446,
      "step": 31890
    },
    {
      "epoch": 2.0215469438195126,
      "grad_norm": 2.51885724067688,
      "learning_rate": 9.812235393734124e-06,
      "loss": 0.7988,
      "step": 31900
    },
    {
      "epoch": 2.022180677461263,
      "grad_norm": 2.8700804710388184,
      "learning_rate": 9.80588484335309e-06,
      "loss": 0.8612,
      "step": 31910
    },
    {
      "epoch": 2.0228144111030133,
      "grad_norm": 2.907999277114868,
      "learning_rate": 9.799534292972057e-06,
      "loss": 0.8322,
      "step": 31920
    },
    {
      "epoch": 2.023448144744764,
      "grad_norm": 3.1271629333496094,
      "learning_rate": 9.793183742591025e-06,
      "loss": 0.8707,
      "step": 31930
    },
    {
      "epoch": 2.024081878386514,
      "grad_norm": 2.550534725189209,
      "learning_rate": 9.786833192209993e-06,
      "loss": 0.8574,
      "step": 31940
    },
    {
      "epoch": 2.0247156120282646,
      "grad_norm": 3.2592391967773438,
      "learning_rate": 9.780482641828958e-06,
      "loss": 0.8134,
      "step": 31950
    },
    {
      "epoch": 2.0253493456700147,
      "grad_norm": 2.8612864017486572,
      "learning_rate": 9.774132091447926e-06,
      "loss": 0.8798,
      "step": 31960
    },
    {
      "epoch": 2.0259830793117652,
      "grad_norm": 3.251443386077881,
      "learning_rate": 9.767781541066892e-06,
      "loss": 0.8887,
      "step": 31970
    },
    {
      "epoch": 2.026616812953516,
      "grad_norm": 2.504801034927368,
      "learning_rate": 9.761430990685859e-06,
      "loss": 0.8272,
      "step": 31980
    },
    {
      "epoch": 2.027250546595266,
      "grad_norm": 2.4828920364379883,
      "learning_rate": 9.755080440304827e-06,
      "loss": 0.8695,
      "step": 31990
    },
    {
      "epoch": 2.0278842802370165,
      "grad_norm": 2.583432197570801,
      "learning_rate": 9.748729889923793e-06,
      "loss": 0.8279,
      "step": 32000
    },
    {
      "epoch": 2.0285180138787666,
      "grad_norm": 2.691772699356079,
      "learning_rate": 9.74237933954276e-06,
      "loss": 0.8567,
      "step": 32010
    },
    {
      "epoch": 2.029151747520517,
      "grad_norm": 2.6064019203186035,
      "learning_rate": 9.736028789161728e-06,
      "loss": 0.8561,
      "step": 32020
    },
    {
      "epoch": 2.0297854811622673,
      "grad_norm": 2.5317108631134033,
      "learning_rate": 9.729678238780694e-06,
      "loss": 0.811,
      "step": 32030
    },
    {
      "epoch": 2.030419214804018,
      "grad_norm": 2.6015007495880127,
      "learning_rate": 9.723327688399663e-06,
      "loss": 0.8439,
      "step": 32040
    },
    {
      "epoch": 2.0310529484457684,
      "grad_norm": 3.3928983211517334,
      "learning_rate": 9.716977138018627e-06,
      "loss": 0.9043,
      "step": 32050
    },
    {
      "epoch": 2.0316866820875186,
      "grad_norm": 2.6701912879943848,
      "learning_rate": 9.710626587637595e-06,
      "loss": 0.8305,
      "step": 32060
    },
    {
      "epoch": 2.032320415729269,
      "grad_norm": 2.7932074069976807,
      "learning_rate": 9.704276037256564e-06,
      "loss": 0.792,
      "step": 32070
    },
    {
      "epoch": 2.0329541493710193,
      "grad_norm": 3.0653719902038574,
      "learning_rate": 9.697925486875528e-06,
      "loss": 0.9064,
      "step": 32080
    },
    {
      "epoch": 2.03358788301277,
      "grad_norm": 2.5523688793182373,
      "learning_rate": 9.691574936494496e-06,
      "loss": 0.8491,
      "step": 32090
    },
    {
      "epoch": 2.03422161665452,
      "grad_norm": 2.6535770893096924,
      "learning_rate": 9.685224386113463e-06,
      "loss": 0.8854,
      "step": 32100
    },
    {
      "epoch": 2.0348553502962705,
      "grad_norm": 2.6813557147979736,
      "learning_rate": 9.678873835732431e-06,
      "loss": 0.8186,
      "step": 32110
    },
    {
      "epoch": 2.0354890839380206,
      "grad_norm": 2.5239109992980957,
      "learning_rate": 9.672523285351397e-06,
      "loss": 0.8659,
      "step": 32120
    },
    {
      "epoch": 2.036122817579771,
      "grad_norm": 2.4886648654937744,
      "learning_rate": 9.666172734970364e-06,
      "loss": 0.8487,
      "step": 32130
    },
    {
      "epoch": 2.0367565512215218,
      "grad_norm": 2.718506097793579,
      "learning_rate": 9.659822184589332e-06,
      "loss": 0.8508,
      "step": 32140
    },
    {
      "epoch": 2.037390284863272,
      "grad_norm": 2.466327428817749,
      "learning_rate": 9.653471634208298e-06,
      "loss": 0.8705,
      "step": 32150
    },
    {
      "epoch": 2.0380240185050225,
      "grad_norm": 3.716684341430664,
      "learning_rate": 9.647121083827265e-06,
      "loss": 0.8169,
      "step": 32160
    },
    {
      "epoch": 2.0386577521467726,
      "grad_norm": 3.0303425788879395,
      "learning_rate": 9.640770533446233e-06,
      "loss": 0.8626,
      "step": 32170
    },
    {
      "epoch": 2.039291485788523,
      "grad_norm": 3.004307985305786,
      "learning_rate": 9.6344199830652e-06,
      "loss": 0.8908,
      "step": 32180
    },
    {
      "epoch": 2.0399252194302733,
      "grad_norm": 2.6027982234954834,
      "learning_rate": 9.628069432684166e-06,
      "loss": 0.865,
      "step": 32190
    },
    {
      "epoch": 2.040558953072024,
      "grad_norm": 2.3190860748291016,
      "learning_rate": 9.621718882303134e-06,
      "loss": 0.8257,
      "step": 32200
    },
    {
      "epoch": 2.0411926867137744,
      "grad_norm": 2.133406162261963,
      "learning_rate": 9.6153683319221e-06,
      "loss": 0.8311,
      "step": 32210
    },
    {
      "epoch": 2.0418264203555245,
      "grad_norm": 2.8660476207733154,
      "learning_rate": 9.609017781541067e-06,
      "loss": 0.8644,
      "step": 32220
    },
    {
      "epoch": 2.042460153997275,
      "grad_norm": 3.1482462882995605,
      "learning_rate": 9.602667231160033e-06,
      "loss": 0.8784,
      "step": 32230
    },
    {
      "epoch": 2.043093887639025,
      "grad_norm": 3.003432512283325,
      "learning_rate": 9.596316680779001e-06,
      "loss": 0.8246,
      "step": 32240
    },
    {
      "epoch": 2.043727621280776,
      "grad_norm": 2.8939144611358643,
      "learning_rate": 9.58996613039797e-06,
      "loss": 0.8651,
      "step": 32250
    },
    {
      "epoch": 2.044361354922526,
      "grad_norm": 2.835519552230835,
      "learning_rate": 9.583615580016934e-06,
      "loss": 0.8364,
      "step": 32260
    },
    {
      "epoch": 2.0449950885642765,
      "grad_norm": 2.4912924766540527,
      "learning_rate": 9.577265029635902e-06,
      "loss": 0.8409,
      "step": 32270
    },
    {
      "epoch": 2.0456288222060266,
      "grad_norm": 3.1027538776397705,
      "learning_rate": 9.57091447925487e-06,
      "loss": 0.8692,
      "step": 32280
    },
    {
      "epoch": 2.046262555847777,
      "grad_norm": 3.115509033203125,
      "learning_rate": 9.564563928873835e-06,
      "loss": 0.8919,
      "step": 32290
    },
    {
      "epoch": 2.0468962894895277,
      "grad_norm": 3.027238130569458,
      "learning_rate": 9.558213378492803e-06,
      "loss": 0.8442,
      "step": 32300
    },
    {
      "epoch": 2.047530023131278,
      "grad_norm": 2.9323227405548096,
      "learning_rate": 9.55186282811177e-06,
      "loss": 0.8169,
      "step": 32310
    },
    {
      "epoch": 2.0481637567730284,
      "grad_norm": 2.9195809364318848,
      "learning_rate": 9.545512277730736e-06,
      "loss": 0.8609,
      "step": 32320
    },
    {
      "epoch": 2.0487974904147785,
      "grad_norm": 2.8511128425598145,
      "learning_rate": 9.539161727349704e-06,
      "loss": 0.8941,
      "step": 32330
    },
    {
      "epoch": 2.049431224056529,
      "grad_norm": 2.694361448287964,
      "learning_rate": 9.53281117696867e-06,
      "loss": 0.9358,
      "step": 32340
    },
    {
      "epoch": 2.0500649576982792,
      "grad_norm": 2.6311612129211426,
      "learning_rate": 9.526460626587639e-06,
      "loss": 0.8919,
      "step": 32350
    },
    {
      "epoch": 2.05069869134003,
      "grad_norm": 2.823739767074585,
      "learning_rate": 9.520110076206603e-06,
      "loss": 0.8428,
      "step": 32360
    },
    {
      "epoch": 2.0513324249817804,
      "grad_norm": 2.8226027488708496,
      "learning_rate": 9.513759525825572e-06,
      "loss": 0.8091,
      "step": 32370
    },
    {
      "epoch": 2.0519661586235305,
      "grad_norm": 3.3098580837249756,
      "learning_rate": 9.50740897544454e-06,
      "loss": 0.8322,
      "step": 32380
    },
    {
      "epoch": 2.052599892265281,
      "grad_norm": 2.6651220321655273,
      "learning_rate": 9.501058425063504e-06,
      "loss": 0.866,
      "step": 32390
    },
    {
      "epoch": 2.053233625907031,
      "grad_norm": 2.544436454772949,
      "learning_rate": 9.494707874682473e-06,
      "loss": 0.8223,
      "step": 32400
    },
    {
      "epoch": 2.0538673595487817,
      "grad_norm": 2.882798910140991,
      "learning_rate": 9.48835732430144e-06,
      "loss": 0.8613,
      "step": 32410
    },
    {
      "epoch": 2.054501093190532,
      "grad_norm": 2.8619461059570312,
      "learning_rate": 9.482006773920407e-06,
      "loss": 0.857,
      "step": 32420
    },
    {
      "epoch": 2.0551348268322824,
      "grad_norm": 2.784872531890869,
      "learning_rate": 9.475656223539374e-06,
      "loss": 0.8157,
      "step": 32430
    },
    {
      "epoch": 2.0557685604740326,
      "grad_norm": 3.49385142326355,
      "learning_rate": 9.46930567315834e-06,
      "loss": 0.8637,
      "step": 32440
    },
    {
      "epoch": 2.056402294115783,
      "grad_norm": 2.8335158824920654,
      "learning_rate": 9.46359017781541e-06,
      "loss": 0.8623,
      "step": 32450
    },
    {
      "epoch": 2.0570360277575337,
      "grad_norm": 2.720266819000244,
      "learning_rate": 9.457239627434379e-06,
      "loss": 0.7941,
      "step": 32460
    },
    {
      "epoch": 2.057669761399284,
      "grad_norm": 2.581134796142578,
      "learning_rate": 9.450889077053343e-06,
      "loss": 0.8566,
      "step": 32470
    },
    {
      "epoch": 2.0583034950410344,
      "grad_norm": 3.284817934036255,
      "learning_rate": 9.444538526672311e-06,
      "loss": 0.8434,
      "step": 32480
    },
    {
      "epoch": 2.0589372286827845,
      "grad_norm": 3.273329973220825,
      "learning_rate": 9.43818797629128e-06,
      "loss": 0.906,
      "step": 32490
    },
    {
      "epoch": 2.059570962324535,
      "grad_norm": 2.705925941467285,
      "learning_rate": 9.431837425910246e-06,
      "loss": 0.8361,
      "step": 32500
    },
    {
      "epoch": 2.060204695966285,
      "grad_norm": 2.5701632499694824,
      "learning_rate": 9.425486875529212e-06,
      "loss": 0.8517,
      "step": 32510
    },
    {
      "epoch": 2.0608384296080358,
      "grad_norm": 2.5040438175201416,
      "learning_rate": 9.41913632514818e-06,
      "loss": 0.8717,
      "step": 32520
    },
    {
      "epoch": 2.0614721632497863,
      "grad_norm": 2.918408155441284,
      "learning_rate": 9.412785774767147e-06,
      "loss": 0.8554,
      "step": 32530
    },
    {
      "epoch": 2.0621058968915364,
      "grad_norm": 2.923323392868042,
      "learning_rate": 9.406435224386113e-06,
      "loss": 0.8042,
      "step": 32540
    },
    {
      "epoch": 2.062739630533287,
      "grad_norm": 2.8587088584899902,
      "learning_rate": 9.40008467400508e-06,
      "loss": 0.8258,
      "step": 32550
    },
    {
      "epoch": 2.063373364175037,
      "grad_norm": 2.7311928272247314,
      "learning_rate": 9.393734123624048e-06,
      "loss": 0.8786,
      "step": 32560
    },
    {
      "epoch": 2.0640070978167877,
      "grad_norm": 2.8658945560455322,
      "learning_rate": 9.387383573243016e-06,
      "loss": 0.8141,
      "step": 32570
    },
    {
      "epoch": 2.064640831458538,
      "grad_norm": 2.6389529705047607,
      "learning_rate": 9.38103302286198e-06,
      "loss": 0.8624,
      "step": 32580
    },
    {
      "epoch": 2.0652745651002884,
      "grad_norm": 2.7876408100128174,
      "learning_rate": 9.374682472480949e-06,
      "loss": 0.789,
      "step": 32590
    },
    {
      "epoch": 2.0659082987420385,
      "grad_norm": 2.4847066402435303,
      "learning_rate": 9.368331922099915e-06,
      "loss": 0.8069,
      "step": 32600
    },
    {
      "epoch": 2.066542032383789,
      "grad_norm": 3.1867411136627197,
      "learning_rate": 9.361981371718882e-06,
      "loss": 0.8225,
      "step": 32610
    },
    {
      "epoch": 2.0671757660255397,
      "grad_norm": 2.9552299976348877,
      "learning_rate": 9.35563082133785e-06,
      "loss": 0.8321,
      "step": 32620
    },
    {
      "epoch": 2.0678094996672898,
      "grad_norm": 2.3914682865142822,
      "learning_rate": 9.349280270956816e-06,
      "loss": 0.8484,
      "step": 32630
    },
    {
      "epoch": 2.0684432333090403,
      "grad_norm": 2.9683711528778076,
      "learning_rate": 9.342929720575784e-06,
      "loss": 0.8627,
      "step": 32640
    },
    {
      "epoch": 2.0690769669507905,
      "grad_norm": 3.1968953609466553,
      "learning_rate": 9.33657917019475e-06,
      "loss": 0.8174,
      "step": 32650
    },
    {
      "epoch": 2.069710700592541,
      "grad_norm": 2.3124477863311768,
      "learning_rate": 9.330228619813717e-06,
      "loss": 0.8435,
      "step": 32660
    },
    {
      "epoch": 2.070344434234291,
      "grad_norm": 3.3299789428710938,
      "learning_rate": 9.323878069432685e-06,
      "loss": 0.8838,
      "step": 32670
    },
    {
      "epoch": 2.0709781678760417,
      "grad_norm": 2.335707664489746,
      "learning_rate": 9.31752751905165e-06,
      "loss": 0.8814,
      "step": 32680
    },
    {
      "epoch": 2.0716119015177923,
      "grad_norm": 2.8436672687530518,
      "learning_rate": 9.311176968670618e-06,
      "loss": 0.8687,
      "step": 32690
    },
    {
      "epoch": 2.0722456351595424,
      "grad_norm": 2.358853340148926,
      "learning_rate": 9.304826418289586e-06,
      "loss": 0.7736,
      "step": 32700
    },
    {
      "epoch": 2.072879368801293,
      "grad_norm": 2.9721157550811768,
      "learning_rate": 9.298475867908551e-06,
      "loss": 0.8791,
      "step": 32710
    },
    {
      "epoch": 2.073513102443043,
      "grad_norm": 3.136425018310547,
      "learning_rate": 9.29212531752752e-06,
      "loss": 0.8236,
      "step": 32720
    },
    {
      "epoch": 2.0741468360847937,
      "grad_norm": 3.0074729919433594,
      "learning_rate": 9.285774767146486e-06,
      "loss": 0.8652,
      "step": 32730
    },
    {
      "epoch": 2.074780569726544,
      "grad_norm": 3.2617673873901367,
      "learning_rate": 9.279424216765454e-06,
      "loss": 0.8685,
      "step": 32740
    },
    {
      "epoch": 2.0754143033682944,
      "grad_norm": 2.8145337104797363,
      "learning_rate": 9.27307366638442e-06,
      "loss": 0.8646,
      "step": 32750
    },
    {
      "epoch": 2.0760480370100445,
      "grad_norm": 2.7206928730010986,
      "learning_rate": 9.266723116003387e-06,
      "loss": 0.8258,
      "step": 32760
    },
    {
      "epoch": 2.076681770651795,
      "grad_norm": 2.6495847702026367,
      "learning_rate": 9.260372565622355e-06,
      "loss": 0.8492,
      "step": 32770
    },
    {
      "epoch": 2.0773155042935456,
      "grad_norm": 2.903535842895508,
      "learning_rate": 9.254022015241321e-06,
      "loss": 0.8497,
      "step": 32780
    },
    {
      "epoch": 2.0779492379352957,
      "grad_norm": 2.7373504638671875,
      "learning_rate": 9.247671464860288e-06,
      "loss": 0.8617,
      "step": 32790
    },
    {
      "epoch": 2.0785829715770463,
      "grad_norm": 2.5936665534973145,
      "learning_rate": 9.241320914479256e-06,
      "loss": 0.8247,
      "step": 32800
    },
    {
      "epoch": 2.0792167052187964,
      "grad_norm": 2.7215735912323,
      "learning_rate": 9.234970364098222e-06,
      "loss": 0.8378,
      "step": 32810
    },
    {
      "epoch": 2.079850438860547,
      "grad_norm": 2.6712162494659424,
      "learning_rate": 9.228619813717189e-06,
      "loss": 0.8643,
      "step": 32820
    },
    {
      "epoch": 2.080484172502297,
      "grad_norm": 3.4336156845092773,
      "learning_rate": 9.222269263336157e-06,
      "loss": 0.8525,
      "step": 32830
    },
    {
      "epoch": 2.0811179061440477,
      "grad_norm": 2.8220884799957275,
      "learning_rate": 9.215918712955123e-06,
      "loss": 0.8386,
      "step": 32840
    },
    {
      "epoch": 2.0817516397857982,
      "grad_norm": 2.6985697746276855,
      "learning_rate": 9.20956816257409e-06,
      "loss": 0.8498,
      "step": 32850
    },
    {
      "epoch": 2.0823853734275484,
      "grad_norm": 2.6638641357421875,
      "learning_rate": 9.203217612193058e-06,
      "loss": 0.8469,
      "step": 32860
    },
    {
      "epoch": 2.083019107069299,
      "grad_norm": 3.172086477279663,
      "learning_rate": 9.196867061812024e-06,
      "loss": 0.8467,
      "step": 32870
    },
    {
      "epoch": 2.083652840711049,
      "grad_norm": 2.638733148574829,
      "learning_rate": 9.190516511430992e-06,
      "loss": 0.8323,
      "step": 32880
    },
    {
      "epoch": 2.0842865743527996,
      "grad_norm": 2.7377727031707764,
      "learning_rate": 9.184165961049957e-06,
      "loss": 0.8408,
      "step": 32890
    },
    {
      "epoch": 2.0849203079945497,
      "grad_norm": 2.547668933868408,
      "learning_rate": 9.177815410668925e-06,
      "loss": 0.893,
      "step": 32900
    },
    {
      "epoch": 2.0855540416363003,
      "grad_norm": 3.3610055446624756,
      "learning_rate": 9.171464860287893e-06,
      "loss": 0.9029,
      "step": 32910
    },
    {
      "epoch": 2.0861877752780504,
      "grad_norm": 2.65177059173584,
      "learning_rate": 9.165114309906858e-06,
      "loss": 0.8576,
      "step": 32920
    },
    {
      "epoch": 2.086821508919801,
      "grad_norm": 2.9155988693237305,
      "learning_rate": 9.158763759525826e-06,
      "loss": 0.8633,
      "step": 32930
    },
    {
      "epoch": 2.0874552425615516,
      "grad_norm": 2.628908157348633,
      "learning_rate": 9.152413209144792e-06,
      "loss": 0.826,
      "step": 32940
    },
    {
      "epoch": 2.0880889762033017,
      "grad_norm": 2.8456387519836426,
      "learning_rate": 9.146062658763759e-06,
      "loss": 0.8147,
      "step": 32950
    },
    {
      "epoch": 2.0887227098450523,
      "grad_norm": 2.48864483833313,
      "learning_rate": 9.139712108382727e-06,
      "loss": 0.8501,
      "step": 32960
    },
    {
      "epoch": 2.0893564434868024,
      "grad_norm": 3.0591793060302734,
      "learning_rate": 9.133361558001693e-06,
      "loss": 0.8227,
      "step": 32970
    },
    {
      "epoch": 2.089990177128553,
      "grad_norm": 3.0353736877441406,
      "learning_rate": 9.127011007620662e-06,
      "loss": 0.858,
      "step": 32980
    },
    {
      "epoch": 2.090623910770303,
      "grad_norm": 2.263519525527954,
      "learning_rate": 9.120660457239628e-06,
      "loss": 0.8428,
      "step": 32990
    },
    {
      "epoch": 2.0912576444120536,
      "grad_norm": 2.9454057216644287,
      "learning_rate": 9.114309906858594e-06,
      "loss": 0.8823,
      "step": 33000
    },
    {
      "epoch": 2.0918913780538038,
      "grad_norm": 2.645754098892212,
      "learning_rate": 9.107959356477563e-06,
      "loss": 0.8106,
      "step": 33010
    },
    {
      "epoch": 2.0925251116955543,
      "grad_norm": 3.246812343597412,
      "learning_rate": 9.101608806096527e-06,
      "loss": 0.88,
      "step": 33020
    },
    {
      "epoch": 2.093158845337305,
      "grad_norm": 2.680722713470459,
      "learning_rate": 9.095258255715495e-06,
      "loss": 0.9062,
      "step": 33030
    },
    {
      "epoch": 2.093792578979055,
      "grad_norm": 2.6758053302764893,
      "learning_rate": 9.088907705334463e-06,
      "loss": 0.888,
      "step": 33040
    },
    {
      "epoch": 2.0944263126208056,
      "grad_norm": 2.4547221660614014,
      "learning_rate": 9.08255715495343e-06,
      "loss": 0.8257,
      "step": 33050
    },
    {
      "epoch": 2.0950600462625557,
      "grad_norm": 3.8114676475524902,
      "learning_rate": 9.076206604572396e-06,
      "loss": 0.8929,
      "step": 33060
    },
    {
      "epoch": 2.0956937799043063,
      "grad_norm": 2.843329429626465,
      "learning_rate": 9.069856054191363e-06,
      "loss": 0.8861,
      "step": 33070
    },
    {
      "epoch": 2.0963275135460564,
      "grad_norm": 2.8854029178619385,
      "learning_rate": 9.063505503810331e-06,
      "loss": 0.8618,
      "step": 33080
    },
    {
      "epoch": 2.096961247187807,
      "grad_norm": 2.7044153213500977,
      "learning_rate": 9.057154953429297e-06,
      "loss": 0.8146,
      "step": 33090
    },
    {
      "epoch": 2.0975949808295575,
      "grad_norm": 2.942235231399536,
      "learning_rate": 9.050804403048264e-06,
      "loss": 0.8914,
      "step": 33100
    },
    {
      "epoch": 2.0982287144713077,
      "grad_norm": 2.94850492477417,
      "learning_rate": 9.044453852667232e-06,
      "loss": 0.8517,
      "step": 33110
    },
    {
      "epoch": 2.098862448113058,
      "grad_norm": 2.678344964981079,
      "learning_rate": 9.0381033022862e-06,
      "loss": 0.8621,
      "step": 33120
    },
    {
      "epoch": 2.0994961817548083,
      "grad_norm": 2.9762766361236572,
      "learning_rate": 9.031752751905165e-06,
      "loss": 0.8335,
      "step": 33130
    },
    {
      "epoch": 2.100129915396559,
      "grad_norm": 3.1243739128112793,
      "learning_rate": 9.025402201524133e-06,
      "loss": 0.8463,
      "step": 33140
    },
    {
      "epoch": 2.100763649038309,
      "grad_norm": 2.739650011062622,
      "learning_rate": 9.0190516511431e-06,
      "loss": 0.8511,
      "step": 33150
    },
    {
      "epoch": 2.1013973826800596,
      "grad_norm": 3.2095000743865967,
      "learning_rate": 9.012701100762066e-06,
      "loss": 0.8641,
      "step": 33160
    },
    {
      "epoch": 2.10203111632181,
      "grad_norm": 3.0186002254486084,
      "learning_rate": 9.006350550381034e-06,
      "loss": 0.9298,
      "step": 33170
    },
    {
      "epoch": 2.1026648499635603,
      "grad_norm": 2.8340418338775635,
      "learning_rate": 9e-06,
      "loss": 0.8675,
      "step": 33180
    },
    {
      "epoch": 2.103298583605311,
      "grad_norm": 2.519106149673462,
      "learning_rate": 8.993649449618968e-06,
      "loss": 0.8114,
      "step": 33190
    },
    {
      "epoch": 2.103932317247061,
      "grad_norm": 2.857163667678833,
      "learning_rate": 8.987298899237933e-06,
      "loss": 0.805,
      "step": 33200
    },
    {
      "epoch": 2.1045660508888115,
      "grad_norm": 2.6719136238098145,
      "learning_rate": 8.980948348856901e-06,
      "loss": 0.8523,
      "step": 33210
    },
    {
      "epoch": 2.1051997845305617,
      "grad_norm": 2.5336058139801025,
      "learning_rate": 8.97459779847587e-06,
      "loss": 0.8324,
      "step": 33220
    },
    {
      "epoch": 2.1058335181723122,
      "grad_norm": 2.922205686569214,
      "learning_rate": 8.968247248094834e-06,
      "loss": 0.874,
      "step": 33230
    },
    {
      "epoch": 2.1064672518140624,
      "grad_norm": 2.7803614139556885,
      "learning_rate": 8.961896697713802e-06,
      "loss": 0.862,
      "step": 33240
    },
    {
      "epoch": 2.107100985455813,
      "grad_norm": 5.418496608734131,
      "learning_rate": 8.95554614733277e-06,
      "loss": 0.8377,
      "step": 33250
    },
    {
      "epoch": 2.1077347190975635,
      "grad_norm": 3.0039470195770264,
      "learning_rate": 8.949195596951735e-06,
      "loss": 0.8435,
      "step": 33260
    },
    {
      "epoch": 2.1083684527393136,
      "grad_norm": 2.4317362308502197,
      "learning_rate": 8.942845046570703e-06,
      "loss": 0.8398,
      "step": 33270
    },
    {
      "epoch": 2.109002186381064,
      "grad_norm": 3.076982259750366,
      "learning_rate": 8.93649449618967e-06,
      "loss": 0.8564,
      "step": 33280
    },
    {
      "epoch": 2.1096359200228143,
      "grad_norm": 2.814842700958252,
      "learning_rate": 8.930143945808638e-06,
      "loss": 0.8121,
      "step": 33290
    },
    {
      "epoch": 2.110269653664565,
      "grad_norm": 2.8447234630584717,
      "learning_rate": 8.923793395427604e-06,
      "loss": 0.8751,
      "step": 33300
    },
    {
      "epoch": 2.110903387306315,
      "grad_norm": 2.527824640274048,
      "learning_rate": 8.91744284504657e-06,
      "loss": 0.8495,
      "step": 33310
    },
    {
      "epoch": 2.1115371209480656,
      "grad_norm": 2.6652281284332275,
      "learning_rate": 8.911092294665539e-06,
      "loss": 0.82,
      "step": 33320
    },
    {
      "epoch": 2.1121708545898157,
      "grad_norm": 2.37910795211792,
      "learning_rate": 8.904741744284503e-06,
      "loss": 0.8241,
      "step": 33330
    },
    {
      "epoch": 2.1128045882315662,
      "grad_norm": 2.803347587585449,
      "learning_rate": 8.898391193903472e-06,
      "loss": 0.8638,
      "step": 33340
    },
    {
      "epoch": 2.113438321873317,
      "grad_norm": 2.6529197692871094,
      "learning_rate": 8.89204064352244e-06,
      "loss": 0.841,
      "step": 33350
    },
    {
      "epoch": 2.114072055515067,
      "grad_norm": 2.8244545459747314,
      "learning_rate": 8.885690093141406e-06,
      "loss": 0.8205,
      "step": 33360
    },
    {
      "epoch": 2.1147057891568175,
      "grad_norm": 2.703451156616211,
      "learning_rate": 8.879339542760373e-06,
      "loss": 0.847,
      "step": 33370
    },
    {
      "epoch": 2.1153395227985676,
      "grad_norm": 2.6728408336639404,
      "learning_rate": 8.87298899237934e-06,
      "loss": 0.7835,
      "step": 33380
    },
    {
      "epoch": 2.115973256440318,
      "grad_norm": 2.884124517440796,
      "learning_rate": 8.866638441998307e-06,
      "loss": 0.8133,
      "step": 33390
    },
    {
      "epoch": 2.1166069900820683,
      "grad_norm": 2.4251840114593506,
      "learning_rate": 8.860287891617273e-06,
      "loss": 0.8655,
      "step": 33400
    },
    {
      "epoch": 2.117240723723819,
      "grad_norm": 3.105337619781494,
      "learning_rate": 8.85393734123624e-06,
      "loss": 0.8665,
      "step": 33410
    },
    {
      "epoch": 2.1178744573655695,
      "grad_norm": 2.5176284313201904,
      "learning_rate": 8.847586790855208e-06,
      "loss": 0.8444,
      "step": 33420
    },
    {
      "epoch": 2.1185081910073196,
      "grad_norm": 2.5018670558929443,
      "learning_rate": 8.841236240474176e-06,
      "loss": 0.8585,
      "step": 33430
    },
    {
      "epoch": 2.11914192464907,
      "grad_norm": 2.7882184982299805,
      "learning_rate": 8.834885690093141e-06,
      "loss": 0.8309,
      "step": 33440
    },
    {
      "epoch": 2.1197756582908203,
      "grad_norm": 2.822845458984375,
      "learning_rate": 8.828535139712109e-06,
      "loss": 0.8477,
      "step": 33450
    },
    {
      "epoch": 2.120409391932571,
      "grad_norm": 2.5407814979553223,
      "learning_rate": 8.822184589331075e-06,
      "loss": 0.8202,
      "step": 33460
    },
    {
      "epoch": 2.121043125574321,
      "grad_norm": 2.7302582263946533,
      "learning_rate": 8.815834038950042e-06,
      "loss": 0.8893,
      "step": 33470
    },
    {
      "epoch": 2.1216768592160715,
      "grad_norm": 2.720384359359741,
      "learning_rate": 8.80948348856901e-06,
      "loss": 0.8459,
      "step": 33480
    },
    {
      "epoch": 2.1223105928578216,
      "grad_norm": 2.9206180572509766,
      "learning_rate": 8.803132938187976e-06,
      "loss": 0.8761,
      "step": 33490
    },
    {
      "epoch": 2.122944326499572,
      "grad_norm": 2.6998844146728516,
      "learning_rate": 8.796782387806943e-06,
      "loss": 0.8204,
      "step": 33500
    },
    {
      "epoch": 2.1235780601413228,
      "grad_norm": 2.4355125427246094,
      "learning_rate": 8.790431837425911e-06,
      "loss": 0.865,
      "step": 33510
    },
    {
      "epoch": 2.124211793783073,
      "grad_norm": 2.7456583976745605,
      "learning_rate": 8.784081287044877e-06,
      "loss": 0.8537,
      "step": 33520
    },
    {
      "epoch": 2.1248455274248235,
      "grad_norm": 3.225964307785034,
      "learning_rate": 8.777730736663845e-06,
      "loss": 0.8457,
      "step": 33530
    },
    {
      "epoch": 2.1254792610665736,
      "grad_norm": 3.1366989612579346,
      "learning_rate": 8.77138018628281e-06,
      "loss": 0.8189,
      "step": 33540
    },
    {
      "epoch": 2.126112994708324,
      "grad_norm": 2.798830032348633,
      "learning_rate": 8.765029635901778e-06,
      "loss": 0.8345,
      "step": 33550
    },
    {
      "epoch": 2.1267467283500743,
      "grad_norm": 2.4421095848083496,
      "learning_rate": 8.758679085520746e-06,
      "loss": 0.8457,
      "step": 33560
    },
    {
      "epoch": 2.127380461991825,
      "grad_norm": 2.951885461807251,
      "learning_rate": 8.752328535139711e-06,
      "loss": 0.8421,
      "step": 33570
    },
    {
      "epoch": 2.1280141956335754,
      "grad_norm": 4.388088703155518,
      "learning_rate": 8.74597798475868e-06,
      "loss": 0.8741,
      "step": 33580
    },
    {
      "epoch": 2.1286479292753255,
      "grad_norm": 3.074901819229126,
      "learning_rate": 8.739627434377646e-06,
      "loss": 0.8218,
      "step": 33590
    },
    {
      "epoch": 2.129281662917076,
      "grad_norm": 2.539100408554077,
      "learning_rate": 8.733276883996614e-06,
      "loss": 0.9057,
      "step": 33600
    },
    {
      "epoch": 2.1299153965588262,
      "grad_norm": 2.549123525619507,
      "learning_rate": 8.72692633361558e-06,
      "loss": 0.8721,
      "step": 33610
    },
    {
      "epoch": 2.130549130200577,
      "grad_norm": 2.663403034210205,
      "learning_rate": 8.720575783234547e-06,
      "loss": 0.8589,
      "step": 33620
    },
    {
      "epoch": 2.131182863842327,
      "grad_norm": 2.4248199462890625,
      "learning_rate": 8.714225232853515e-06,
      "loss": 0.8227,
      "step": 33630
    },
    {
      "epoch": 2.1318165974840775,
      "grad_norm": 2.865206003189087,
      "learning_rate": 8.707874682472481e-06,
      "loss": 0.8747,
      "step": 33640
    },
    {
      "epoch": 2.1324503311258276,
      "grad_norm": 2.833751916885376,
      "learning_rate": 8.701524132091448e-06,
      "loss": 0.864,
      "step": 33650
    },
    {
      "epoch": 2.133084064767578,
      "grad_norm": 2.5796329975128174,
      "learning_rate": 8.695173581710416e-06,
      "loss": 0.8706,
      "step": 33660
    },
    {
      "epoch": 2.1337177984093287,
      "grad_norm": 2.3376708030700684,
      "learning_rate": 8.688823031329382e-06,
      "loss": 0.7975,
      "step": 33670
    },
    {
      "epoch": 2.134351532051079,
      "grad_norm": 2.7108798027038574,
      "learning_rate": 8.682472480948349e-06,
      "loss": 0.8374,
      "step": 33680
    },
    {
      "epoch": 2.1349852656928294,
      "grad_norm": 3.78113055229187,
      "learning_rate": 8.676121930567317e-06,
      "loss": 0.891,
      "step": 33690
    },
    {
      "epoch": 2.1356189993345795,
      "grad_norm": 3.0049641132354736,
      "learning_rate": 8.669771380186283e-06,
      "loss": 0.7995,
      "step": 33700
    },
    {
      "epoch": 2.13625273297633,
      "grad_norm": 2.8540821075439453,
      "learning_rate": 8.66342082980525e-06,
      "loss": 0.8371,
      "step": 33710
    },
    {
      "epoch": 2.1368864666180802,
      "grad_norm": 2.635695219039917,
      "learning_rate": 8.657070279424216e-06,
      "loss": 0.8647,
      "step": 33720
    },
    {
      "epoch": 2.137520200259831,
      "grad_norm": 2.543973684310913,
      "learning_rate": 8.650719729043184e-06,
      "loss": 0.8354,
      "step": 33730
    },
    {
      "epoch": 2.1381539339015814,
      "grad_norm": 2.4131274223327637,
      "learning_rate": 8.644369178662152e-06,
      "loss": 0.8759,
      "step": 33740
    },
    {
      "epoch": 2.1387876675433315,
      "grad_norm": 2.5911858081817627,
      "learning_rate": 8.638018628281117e-06,
      "loss": 0.8415,
      "step": 33750
    },
    {
      "epoch": 2.139421401185082,
      "grad_norm": 3.041149616241455,
      "learning_rate": 8.631668077900085e-06,
      "loss": 0.8712,
      "step": 33760
    },
    {
      "epoch": 2.140055134826832,
      "grad_norm": 2.597323417663574,
      "learning_rate": 8.625317527519053e-06,
      "loss": 0.8263,
      "step": 33770
    },
    {
      "epoch": 2.1406888684685828,
      "grad_norm": 2.3290159702301025,
      "learning_rate": 8.618966977138018e-06,
      "loss": 0.8596,
      "step": 33780
    },
    {
      "epoch": 2.141322602110333,
      "grad_norm": 2.8535969257354736,
      "learning_rate": 8.612616426756986e-06,
      "loss": 0.8245,
      "step": 33790
    },
    {
      "epoch": 2.1419563357520834,
      "grad_norm": 3.394605875015259,
      "learning_rate": 8.606265876375953e-06,
      "loss": 0.8315,
      "step": 33800
    },
    {
      "epoch": 2.142590069393834,
      "grad_norm": 2.649286985397339,
      "learning_rate": 8.599915325994919e-06,
      "loss": 0.8489,
      "step": 33810
    },
    {
      "epoch": 2.143223803035584,
      "grad_norm": 2.8033745288848877,
      "learning_rate": 8.593564775613887e-06,
      "loss": 0.8074,
      "step": 33820
    },
    {
      "epoch": 2.1438575366773347,
      "grad_norm": 2.5931968688964844,
      "learning_rate": 8.587214225232854e-06,
      "loss": 0.8441,
      "step": 33830
    },
    {
      "epoch": 2.144491270319085,
      "grad_norm": 2.8933515548706055,
      "learning_rate": 8.580863674851822e-06,
      "loss": 0.8128,
      "step": 33840
    },
    {
      "epoch": 2.1451250039608354,
      "grad_norm": 2.787978172302246,
      "learning_rate": 8.574513124470786e-06,
      "loss": 0.8494,
      "step": 33850
    },
    {
      "epoch": 2.1457587376025855,
      "grad_norm": 3.082817316055298,
      "learning_rate": 8.568162574089755e-06,
      "loss": 0.8721,
      "step": 33860
    },
    {
      "epoch": 2.146392471244336,
      "grad_norm": 2.834235668182373,
      "learning_rate": 8.561812023708723e-06,
      "loss": 0.9138,
      "step": 33870
    },
    {
      "epoch": 2.147026204886086,
      "grad_norm": 2.8340532779693604,
      "learning_rate": 8.555461473327687e-06,
      "loss": 0.872,
      "step": 33880
    },
    {
      "epoch": 2.1476599385278368,
      "grad_norm": 2.641805648803711,
      "learning_rate": 8.549110922946655e-06,
      "loss": 0.8649,
      "step": 33890
    },
    {
      "epoch": 2.1482936721695873,
      "grad_norm": 2.4840314388275146,
      "learning_rate": 8.542760372565624e-06,
      "loss": 0.8321,
      "step": 33900
    },
    {
      "epoch": 2.1489274058113375,
      "grad_norm": 2.745907783508301,
      "learning_rate": 8.53640982218459e-06,
      "loss": 0.8988,
      "step": 33910
    },
    {
      "epoch": 2.149561139453088,
      "grad_norm": 2.807779550552368,
      "learning_rate": 8.530059271803556e-06,
      "loss": 0.8356,
      "step": 33920
    },
    {
      "epoch": 2.150194873094838,
      "grad_norm": 2.2940304279327393,
      "learning_rate": 8.523708721422523e-06,
      "loss": 0.846,
      "step": 33930
    },
    {
      "epoch": 2.1508286067365887,
      "grad_norm": 2.555326223373413,
      "learning_rate": 8.517358171041491e-06,
      "loss": 0.8117,
      "step": 33940
    },
    {
      "epoch": 2.151462340378339,
      "grad_norm": 2.720136880874634,
      "learning_rate": 8.511007620660457e-06,
      "loss": 0.8357,
      "step": 33950
    },
    {
      "epoch": 2.1520960740200894,
      "grad_norm": 3.2635915279388428,
      "learning_rate": 8.504657070279424e-06,
      "loss": 0.8405,
      "step": 33960
    },
    {
      "epoch": 2.1527298076618395,
      "grad_norm": 3.143953561782837,
      "learning_rate": 8.498306519898392e-06,
      "loss": 0.8553,
      "step": 33970
    },
    {
      "epoch": 2.15336354130359,
      "grad_norm": 2.999406337738037,
      "learning_rate": 8.491955969517358e-06,
      "loss": 0.8939,
      "step": 33980
    },
    {
      "epoch": 2.1539972749453407,
      "grad_norm": 2.7401540279388428,
      "learning_rate": 8.485605419136325e-06,
      "loss": 0.8599,
      "step": 33990
    },
    {
      "epoch": 2.154631008587091,
      "grad_norm": 3.09228515625,
      "learning_rate": 8.479254868755293e-06,
      "loss": 0.8193,
      "step": 34000
    },
    {
      "epoch": 2.1552647422288413,
      "grad_norm": 2.487647533416748,
      "learning_rate": 8.47290431837426e-06,
      "loss": 0.8519,
      "step": 34010
    },
    {
      "epoch": 2.1558984758705915,
      "grad_norm": 2.5669009685516357,
      "learning_rate": 8.466553767993226e-06,
      "loss": 0.8938,
      "step": 34020
    },
    {
      "epoch": 2.156532209512342,
      "grad_norm": 2.792703628540039,
      "learning_rate": 8.460203217612194e-06,
      "loss": 0.8147,
      "step": 34030
    },
    {
      "epoch": 2.157165943154092,
      "grad_norm": 2.7277419567108154,
      "learning_rate": 8.45385266723116e-06,
      "loss": 0.831,
      "step": 34040
    },
    {
      "epoch": 2.1577996767958427,
      "grad_norm": 2.5407471656799316,
      "learning_rate": 8.447502116850127e-06,
      "loss": 0.9001,
      "step": 34050
    },
    {
      "epoch": 2.158433410437593,
      "grad_norm": 2.5304975509643555,
      "learning_rate": 8.441151566469093e-06,
      "loss": 0.8568,
      "step": 34060
    },
    {
      "epoch": 2.1590671440793434,
      "grad_norm": 2.6548068523406982,
      "learning_rate": 8.434801016088061e-06,
      "loss": 0.8359,
      "step": 34070
    },
    {
      "epoch": 2.159700877721094,
      "grad_norm": 2.6402792930603027,
      "learning_rate": 8.42845046570703e-06,
      "loss": 0.8482,
      "step": 34080
    },
    {
      "epoch": 2.160334611362844,
      "grad_norm": 2.866055727005005,
      "learning_rate": 8.422099915325994e-06,
      "loss": 0.813,
      "step": 34090
    },
    {
      "epoch": 2.1609683450045947,
      "grad_norm": 2.7662527561187744,
      "learning_rate": 8.415749364944962e-06,
      "loss": 0.8624,
      "step": 34100
    },
    {
      "epoch": 2.161602078646345,
      "grad_norm": 2.684007167816162,
      "learning_rate": 8.409398814563929e-06,
      "loss": 0.8262,
      "step": 34110
    },
    {
      "epoch": 2.1622358122880954,
      "grad_norm": 2.975128412246704,
      "learning_rate": 8.403048264182895e-06,
      "loss": 0.8369,
      "step": 34120
    },
    {
      "epoch": 2.1628695459298455,
      "grad_norm": 2.7936501502990723,
      "learning_rate": 8.396697713801863e-06,
      "loss": 0.8321,
      "step": 34130
    },
    {
      "epoch": 2.163503279571596,
      "grad_norm": 2.8056576251983643,
      "learning_rate": 8.39034716342083e-06,
      "loss": 0.8212,
      "step": 34140
    },
    {
      "epoch": 2.1641370132133466,
      "grad_norm": 2.7871932983398438,
      "learning_rate": 8.383996613039798e-06,
      "loss": 0.8428,
      "step": 34150
    },
    {
      "epoch": 2.1647707468550967,
      "grad_norm": 2.755164384841919,
      "learning_rate": 8.377646062658764e-06,
      "loss": 0.8741,
      "step": 34160
    },
    {
      "epoch": 2.1654044804968473,
      "grad_norm": 2.3302478790283203,
      "learning_rate": 8.37129551227773e-06,
      "loss": 0.821,
      "step": 34170
    },
    {
      "epoch": 2.1660382141385974,
      "grad_norm": 2.9590935707092285,
      "learning_rate": 8.364944961896699e-06,
      "loss": 0.822,
      "step": 34180
    },
    {
      "epoch": 2.166671947780348,
      "grad_norm": 2.926208972930908,
      "learning_rate": 8.358594411515664e-06,
      "loss": 0.8231,
      "step": 34190
    },
    {
      "epoch": 2.167305681422098,
      "grad_norm": 2.4958653450012207,
      "learning_rate": 8.352243861134632e-06,
      "loss": 0.8477,
      "step": 34200
    },
    {
      "epoch": 2.1679394150638487,
      "grad_norm": 2.5485706329345703,
      "learning_rate": 8.3458933107536e-06,
      "loss": 0.8425,
      "step": 34210
    },
    {
      "epoch": 2.1685731487055993,
      "grad_norm": 2.389279842376709,
      "learning_rate": 8.339542760372566e-06,
      "loss": 0.8472,
      "step": 34220
    },
    {
      "epoch": 2.1692068823473494,
      "grad_norm": 2.782473087310791,
      "learning_rate": 8.333192209991533e-06,
      "loss": 0.8025,
      "step": 34230
    },
    {
      "epoch": 2.1698406159891,
      "grad_norm": 2.603726863861084,
      "learning_rate": 8.326841659610499e-06,
      "loss": 0.8309,
      "step": 34240
    },
    {
      "epoch": 2.17047434963085,
      "grad_norm": 3.05026912689209,
      "learning_rate": 8.320491109229467e-06,
      "loss": 0.854,
      "step": 34250
    },
    {
      "epoch": 2.1711080832726006,
      "grad_norm": 2.926060199737549,
      "learning_rate": 8.314140558848434e-06,
      "loss": 0.8531,
      "step": 34260
    },
    {
      "epoch": 2.1717418169143508,
      "grad_norm": 2.714998960494995,
      "learning_rate": 8.3077900084674e-06,
      "loss": 0.8465,
      "step": 34270
    },
    {
      "epoch": 2.1723755505561013,
      "grad_norm": 2.9494714736938477,
      "learning_rate": 8.301439458086368e-06,
      "loss": 0.8327,
      "step": 34280
    },
    {
      "epoch": 2.1730092841978514,
      "grad_norm": 2.6873655319213867,
      "learning_rate": 8.295088907705336e-06,
      "loss": 0.8623,
      "step": 34290
    },
    {
      "epoch": 2.173643017839602,
      "grad_norm": 2.52358078956604,
      "learning_rate": 8.288738357324301e-06,
      "loss": 0.8339,
      "step": 34300
    },
    {
      "epoch": 2.1742767514813526,
      "grad_norm": 2.5482025146484375,
      "learning_rate": 8.282387806943269e-06,
      "loss": 0.8766,
      "step": 34310
    },
    {
      "epoch": 2.1749104851231027,
      "grad_norm": 2.680833578109741,
      "learning_rate": 8.276037256562236e-06,
      "loss": 0.8219,
      "step": 34320
    },
    {
      "epoch": 2.1755442187648533,
      "grad_norm": 3.30179762840271,
      "learning_rate": 8.269686706181202e-06,
      "loss": 0.8736,
      "step": 34330
    },
    {
      "epoch": 2.1761779524066034,
      "grad_norm": 3.105015993118286,
      "learning_rate": 8.26333615580017e-06,
      "loss": 0.8946,
      "step": 34340
    },
    {
      "epoch": 2.176811686048354,
      "grad_norm": 2.704594373703003,
      "learning_rate": 8.256985605419137e-06,
      "loss": 0.8022,
      "step": 34350
    },
    {
      "epoch": 2.177445419690104,
      "grad_norm": 2.993556022644043,
      "learning_rate": 8.250635055038103e-06,
      "loss": 0.8764,
      "step": 34360
    },
    {
      "epoch": 2.1780791533318546,
      "grad_norm": 2.5749826431274414,
      "learning_rate": 8.24428450465707e-06,
      "loss": 0.9109,
      "step": 34370
    },
    {
      "epoch": 2.1787128869736048,
      "grad_norm": 2.631108522415161,
      "learning_rate": 8.237933954276038e-06,
      "loss": 0.8227,
      "step": 34380
    },
    {
      "epoch": 2.1793466206153553,
      "grad_norm": 2.9478600025177,
      "learning_rate": 8.231583403895006e-06,
      "loss": 0.8202,
      "step": 34390
    },
    {
      "epoch": 2.179980354257106,
      "grad_norm": 3.202535629272461,
      "learning_rate": 8.22523285351397e-06,
      "loss": 0.8346,
      "step": 34400
    },
    {
      "epoch": 2.180614087898856,
      "grad_norm": 2.783267021179199,
      "learning_rate": 8.218882303132938e-06,
      "loss": 0.8816,
      "step": 34410
    },
    {
      "epoch": 2.1812478215406066,
      "grad_norm": 2.802579164505005,
      "learning_rate": 8.212531752751907e-06,
      "loss": 0.8115,
      "step": 34420
    },
    {
      "epoch": 2.1818815551823567,
      "grad_norm": 2.8304922580718994,
      "learning_rate": 8.206181202370871e-06,
      "loss": 0.8612,
      "step": 34430
    },
    {
      "epoch": 2.1825152888241073,
      "grad_norm": 2.472987651824951,
      "learning_rate": 8.19983065198984e-06,
      "loss": 0.8389,
      "step": 34440
    },
    {
      "epoch": 2.1831490224658574,
      "grad_norm": 2.821131706237793,
      "learning_rate": 8.193480101608806e-06,
      "loss": 0.842,
      "step": 34450
    },
    {
      "epoch": 2.183782756107608,
      "grad_norm": 2.78975772857666,
      "learning_rate": 8.187129551227774e-06,
      "loss": 0.8062,
      "step": 34460
    },
    {
      "epoch": 2.1844164897493585,
      "grad_norm": 2.7390339374542236,
      "learning_rate": 8.18077900084674e-06,
      "loss": 0.8796,
      "step": 34470
    },
    {
      "epoch": 2.1850502233911087,
      "grad_norm": 2.6048104763031006,
      "learning_rate": 8.174428450465707e-06,
      "loss": 0.8256,
      "step": 34480
    },
    {
      "epoch": 2.1856839570328592,
      "grad_norm": 2.829010486602783,
      "learning_rate": 8.168077900084675e-06,
      "loss": 0.8456,
      "step": 34490
    },
    {
      "epoch": 2.1863176906746093,
      "grad_norm": 2.8788914680480957,
      "learning_rate": 8.16172734970364e-06,
      "loss": 0.821,
      "step": 34500
    },
    {
      "epoch": 2.18695142431636,
      "grad_norm": 2.6847548484802246,
      "learning_rate": 8.155376799322608e-06,
      "loss": 0.8664,
      "step": 34510
    },
    {
      "epoch": 2.18758515795811,
      "grad_norm": 2.853341817855835,
      "learning_rate": 8.149026248941576e-06,
      "loss": 0.8318,
      "step": 34520
    },
    {
      "epoch": 2.1882188915998606,
      "grad_norm": 2.7001609802246094,
      "learning_rate": 8.142675698560542e-06,
      "loss": 0.859,
      "step": 34530
    },
    {
      "epoch": 2.188852625241611,
      "grad_norm": 2.710967540740967,
      "learning_rate": 8.136325148179509e-06,
      "loss": 0.8504,
      "step": 34540
    },
    {
      "epoch": 2.1894863588833613,
      "grad_norm": 2.666382074356079,
      "learning_rate": 8.129974597798477e-06,
      "loss": 0.8219,
      "step": 34550
    },
    {
      "epoch": 2.190120092525112,
      "grad_norm": 2.6481289863586426,
      "learning_rate": 8.123624047417443e-06,
      "loss": 0.8596,
      "step": 34560
    },
    {
      "epoch": 2.190753826166862,
      "grad_norm": 3.1936676502227783,
      "learning_rate": 8.11727349703641e-06,
      "loss": 0.8774,
      "step": 34570
    },
    {
      "epoch": 2.1913875598086126,
      "grad_norm": 2.4771199226379395,
      "learning_rate": 8.110922946655376e-06,
      "loss": 0.8499,
      "step": 34580
    },
    {
      "epoch": 2.1920212934503627,
      "grad_norm": 2.34818172454834,
      "learning_rate": 8.104572396274344e-06,
      "loss": 0.8097,
      "step": 34590
    },
    {
      "epoch": 2.1926550270921132,
      "grad_norm": 3.191710948944092,
      "learning_rate": 8.09822184589331e-06,
      "loss": 0.8525,
      "step": 34600
    },
    {
      "epoch": 2.1932887607338634,
      "grad_norm": 2.6513288021087646,
      "learning_rate": 8.091871295512277e-06,
      "loss": 0.864,
      "step": 34610
    },
    {
      "epoch": 2.193922494375614,
      "grad_norm": 2.943063259124756,
      "learning_rate": 8.085520745131245e-06,
      "loss": 0.8681,
      "step": 34620
    },
    {
      "epoch": 2.1945562280173645,
      "grad_norm": 2.5408732891082764,
      "learning_rate": 8.079170194750212e-06,
      "loss": 0.8703,
      "step": 34630
    },
    {
      "epoch": 2.1951899616591146,
      "grad_norm": 2.796081781387329,
      "learning_rate": 8.072819644369178e-06,
      "loss": 0.8405,
      "step": 34640
    },
    {
      "epoch": 2.195823695300865,
      "grad_norm": 3.2743446826934814,
      "learning_rate": 8.066469093988146e-06,
      "loss": 0.9027,
      "step": 34650
    },
    {
      "epoch": 2.1964574289426153,
      "grad_norm": 2.7804675102233887,
      "learning_rate": 8.060118543607113e-06,
      "loss": 0.835,
      "step": 34660
    },
    {
      "epoch": 2.197091162584366,
      "grad_norm": 3.2585153579711914,
      "learning_rate": 8.053767993226079e-06,
      "loss": 0.8644,
      "step": 34670
    },
    {
      "epoch": 2.197724896226116,
      "grad_norm": 2.942138910293579,
      "learning_rate": 8.047417442845047e-06,
      "loss": 0.8551,
      "step": 34680
    },
    {
      "epoch": 2.1983586298678666,
      "grad_norm": 2.68495512008667,
      "learning_rate": 8.041066892464014e-06,
      "loss": 0.8796,
      "step": 34690
    },
    {
      "epoch": 2.1989923635096167,
      "grad_norm": 3.3451104164123535,
      "learning_rate": 8.034716342082982e-06,
      "loss": 0.8449,
      "step": 34700
    },
    {
      "epoch": 2.1996260971513673,
      "grad_norm": 2.668016195297241,
      "learning_rate": 8.028365791701947e-06,
      "loss": 0.8307,
      "step": 34710
    },
    {
      "epoch": 2.200259830793118,
      "grad_norm": 2.4795045852661133,
      "learning_rate": 8.022015241320915e-06,
      "loss": 0.8473,
      "step": 34720
    },
    {
      "epoch": 2.200893564434868,
      "grad_norm": 3.222069025039673,
      "learning_rate": 8.016299745977985e-06,
      "loss": 0.8689,
      "step": 34730
    },
    {
      "epoch": 2.2015272980766185,
      "grad_norm": 2.593980073928833,
      "learning_rate": 8.009949195596952e-06,
      "loss": 0.8205,
      "step": 34740
    },
    {
      "epoch": 2.2021610317183686,
      "grad_norm": 2.4819209575653076,
      "learning_rate": 8.003598645215918e-06,
      "loss": 0.8583,
      "step": 34750
    },
    {
      "epoch": 2.202794765360119,
      "grad_norm": 2.533775568008423,
      "learning_rate": 7.997248094834886e-06,
      "loss": 0.845,
      "step": 34760
    },
    {
      "epoch": 2.2034284990018693,
      "grad_norm": 2.868596315383911,
      "learning_rate": 7.990897544453853e-06,
      "loss": 0.8616,
      "step": 34770
    },
    {
      "epoch": 2.20406223264362,
      "grad_norm": 3.077077865600586,
      "learning_rate": 7.98454699407282e-06,
      "loss": 0.8392,
      "step": 34780
    },
    {
      "epoch": 2.2046959662853705,
      "grad_norm": 2.5865819454193115,
      "learning_rate": 7.978196443691787e-06,
      "loss": 0.8826,
      "step": 34790
    },
    {
      "epoch": 2.2053296999271206,
      "grad_norm": 2.7068967819213867,
      "learning_rate": 7.971845893310754e-06,
      "loss": 0.8455,
      "step": 34800
    },
    {
      "epoch": 2.205963433568871,
      "grad_norm": 3.2197539806365967,
      "learning_rate": 7.965495342929722e-06,
      "loss": 0.8768,
      "step": 34810
    },
    {
      "epoch": 2.2065971672106213,
      "grad_norm": 2.8832004070281982,
      "learning_rate": 7.959144792548686e-06,
      "loss": 0.835,
      "step": 34820
    },
    {
      "epoch": 2.207230900852372,
      "grad_norm": 2.7444658279418945,
      "learning_rate": 7.952794242167654e-06,
      "loss": 0.8174,
      "step": 34830
    },
    {
      "epoch": 2.207864634494122,
      "grad_norm": 2.1440911293029785,
      "learning_rate": 7.946443691786623e-06,
      "loss": 0.8201,
      "step": 34840
    },
    {
      "epoch": 2.2084983681358725,
      "grad_norm": 2.7789523601531982,
      "learning_rate": 7.940093141405589e-06,
      "loss": 0.8344,
      "step": 34850
    },
    {
      "epoch": 2.209132101777623,
      "grad_norm": 2.7163262367248535,
      "learning_rate": 7.933742591024555e-06,
      "loss": 0.8449,
      "step": 34860
    },
    {
      "epoch": 2.209765835419373,
      "grad_norm": 2.5029852390289307,
      "learning_rate": 7.927392040643522e-06,
      "loss": 0.8393,
      "step": 34870
    },
    {
      "epoch": 2.210399569061124,
      "grad_norm": 3.1269514560699463,
      "learning_rate": 7.92104149026249e-06,
      "loss": 0.8323,
      "step": 34880
    },
    {
      "epoch": 2.211033302702874,
      "grad_norm": 3.055830955505371,
      "learning_rate": 7.914690939881456e-06,
      "loss": 0.854,
      "step": 34890
    },
    {
      "epoch": 2.2116670363446245,
      "grad_norm": 2.621537446975708,
      "learning_rate": 7.908340389500423e-06,
      "loss": 0.8163,
      "step": 34900
    },
    {
      "epoch": 2.2123007699863746,
      "grad_norm": 2.583829164505005,
      "learning_rate": 7.901989839119391e-06,
      "loss": 0.8708,
      "step": 34910
    },
    {
      "epoch": 2.212934503628125,
      "grad_norm": 2.597137212753296,
      "learning_rate": 7.895639288738359e-06,
      "loss": 0.8405,
      "step": 34920
    },
    {
      "epoch": 2.2135682372698753,
      "grad_norm": 2.993842840194702,
      "learning_rate": 7.889288738357324e-06,
      "loss": 0.8505,
      "step": 34930
    },
    {
      "epoch": 2.214201970911626,
      "grad_norm": 3.1050212383270264,
      "learning_rate": 7.882938187976292e-06,
      "loss": 0.8262,
      "step": 34940
    },
    {
      "epoch": 2.2148357045533764,
      "grad_norm": 2.5556938648223877,
      "learning_rate": 7.876587637595258e-06,
      "loss": 0.8668,
      "step": 34950
    },
    {
      "epoch": 2.2154694381951265,
      "grad_norm": 3.2698044776916504,
      "learning_rate": 7.870237087214225e-06,
      "loss": 0.8768,
      "step": 34960
    },
    {
      "epoch": 2.216103171836877,
      "grad_norm": 3.56655216217041,
      "learning_rate": 7.863886536833193e-06,
      "loss": 0.8825,
      "step": 34970
    },
    {
      "epoch": 2.2167369054786272,
      "grad_norm": 2.807155132293701,
      "learning_rate": 7.85753598645216e-06,
      "loss": 0.9131,
      "step": 34980
    },
    {
      "epoch": 2.217370639120378,
      "grad_norm": 2.5681889057159424,
      "learning_rate": 7.851185436071126e-06,
      "loss": 0.8154,
      "step": 34990
    },
    {
      "epoch": 2.218004372762128,
      "grad_norm": 2.8616273403167725,
      "learning_rate": 7.844834885690092e-06,
      "loss": 0.8825,
      "step": 35000
    },
    {
      "epoch": 2.2186381064038785,
      "grad_norm": 2.7072927951812744,
      "learning_rate": 7.83848433530906e-06,
      "loss": 0.8848,
      "step": 35010
    },
    {
      "epoch": 2.2192718400456286,
      "grad_norm": 2.403444528579712,
      "learning_rate": 7.832133784928028e-06,
      "loss": 0.8516,
      "step": 35020
    },
    {
      "epoch": 2.219905573687379,
      "grad_norm": 2.8663065433502197,
      "learning_rate": 7.825783234546993e-06,
      "loss": 0.8288,
      "step": 35030
    },
    {
      "epoch": 2.2205393073291297,
      "grad_norm": 2.726041555404663,
      "learning_rate": 7.819432684165961e-06,
      "loss": 0.8541,
      "step": 35040
    },
    {
      "epoch": 2.22117304097088,
      "grad_norm": 2.631343364715576,
      "learning_rate": 7.81308213378493e-06,
      "loss": 0.8117,
      "step": 35050
    },
    {
      "epoch": 2.2218067746126304,
      "grad_norm": 3.089984178543091,
      "learning_rate": 7.806731583403894e-06,
      "loss": 0.8503,
      "step": 35060
    },
    {
      "epoch": 2.2224405082543806,
      "grad_norm": 2.617410898208618,
      "learning_rate": 7.800381033022862e-06,
      "loss": 0.8306,
      "step": 35070
    },
    {
      "epoch": 2.223074241896131,
      "grad_norm": 3.0924623012542725,
      "learning_rate": 7.794030482641829e-06,
      "loss": 0.8481,
      "step": 35080
    },
    {
      "epoch": 2.2237079755378812,
      "grad_norm": 3.0441412925720215,
      "learning_rate": 7.787679932260797e-06,
      "loss": 0.8077,
      "step": 35090
    },
    {
      "epoch": 2.224341709179632,
      "grad_norm": 2.6430211067199707,
      "learning_rate": 7.781329381879763e-06,
      "loss": 0.8277,
      "step": 35100
    },
    {
      "epoch": 2.2249754428213824,
      "grad_norm": 2.4706521034240723,
      "learning_rate": 7.77497883149873e-06,
      "loss": 0.8552,
      "step": 35110
    },
    {
      "epoch": 2.2256091764631325,
      "grad_norm": 2.8706471920013428,
      "learning_rate": 7.768628281117698e-06,
      "loss": 0.8314,
      "step": 35120
    },
    {
      "epoch": 2.226242910104883,
      "grad_norm": 2.553779125213623,
      "learning_rate": 7.762277730736663e-06,
      "loss": 0.8496,
      "step": 35130
    },
    {
      "epoch": 2.226876643746633,
      "grad_norm": 3.233527660369873,
      "learning_rate": 7.75592718035563e-06,
      "loss": 0.8697,
      "step": 35140
    },
    {
      "epoch": 2.2275103773883838,
      "grad_norm": 2.8312320709228516,
      "learning_rate": 7.749576629974599e-06,
      "loss": 0.8587,
      "step": 35150
    },
    {
      "epoch": 2.228144111030134,
      "grad_norm": 2.642282485961914,
      "learning_rate": 7.743226079593565e-06,
      "loss": 0.8501,
      "step": 35160
    },
    {
      "epoch": 2.2287778446718844,
      "grad_norm": 2.799483060836792,
      "learning_rate": 7.736875529212532e-06,
      "loss": 0.8533,
      "step": 35170
    },
    {
      "epoch": 2.229411578313635,
      "grad_norm": 2.962406873703003,
      "learning_rate": 7.7305249788315e-06,
      "loss": 0.8624,
      "step": 35180
    },
    {
      "epoch": 2.230045311955385,
      "grad_norm": 2.453733444213867,
      "learning_rate": 7.724174428450466e-06,
      "loss": 0.8781,
      "step": 35190
    },
    {
      "epoch": 2.2306790455971357,
      "grad_norm": 3.7048962116241455,
      "learning_rate": 7.717823878069433e-06,
      "loss": 0.9023,
      "step": 35200
    },
    {
      "epoch": 2.231312779238886,
      "grad_norm": 2.7152888774871826,
      "learning_rate": 7.711473327688399e-06,
      "loss": 0.8221,
      "step": 35210
    },
    {
      "epoch": 2.2319465128806364,
      "grad_norm": 2.9898030757904053,
      "learning_rate": 7.705122777307367e-06,
      "loss": 0.8462,
      "step": 35220
    },
    {
      "epoch": 2.2325802465223865,
      "grad_norm": 3.2307815551757812,
      "learning_rate": 7.698772226926334e-06,
      "loss": 0.8282,
      "step": 35230
    },
    {
      "epoch": 2.233213980164137,
      "grad_norm": 2.703540802001953,
      "learning_rate": 7.6924216765453e-06,
      "loss": 0.8131,
      "step": 35240
    },
    {
      "epoch": 2.233847713805887,
      "grad_norm": 3.1089203357696533,
      "learning_rate": 7.686071126164268e-06,
      "loss": 0.8059,
      "step": 35250
    },
    {
      "epoch": 2.2344814474476378,
      "grad_norm": 2.5372231006622314,
      "learning_rate": 7.679720575783235e-06,
      "loss": 0.8389,
      "step": 35260
    },
    {
      "epoch": 2.2351151810893883,
      "grad_norm": 2.9640326499938965,
      "learning_rate": 7.673370025402201e-06,
      "loss": 0.8281,
      "step": 35270
    },
    {
      "epoch": 2.2357489147311385,
      "grad_norm": 2.7457361221313477,
      "learning_rate": 7.667019475021169e-06,
      "loss": 0.8429,
      "step": 35280
    },
    {
      "epoch": 2.236382648372889,
      "grad_norm": 2.6401944160461426,
      "learning_rate": 7.660668924640136e-06,
      "loss": 0.8912,
      "step": 35290
    },
    {
      "epoch": 2.237016382014639,
      "grad_norm": 2.4340479373931885,
      "learning_rate": 7.654318374259102e-06,
      "loss": 0.9046,
      "step": 35300
    },
    {
      "epoch": 2.2376501156563897,
      "grad_norm": 2.6768620014190674,
      "learning_rate": 7.64796782387807e-06,
      "loss": 0.8599,
      "step": 35310
    },
    {
      "epoch": 2.23828384929814,
      "grad_norm": 2.716705083847046,
      "learning_rate": 7.641617273497037e-06,
      "loss": 0.8471,
      "step": 35320
    },
    {
      "epoch": 2.2389175829398904,
      "grad_norm": 2.463608741760254,
      "learning_rate": 7.635266723116005e-06,
      "loss": 0.8438,
      "step": 35330
    },
    {
      "epoch": 2.2395513165816405,
      "grad_norm": 2.678210973739624,
      "learning_rate": 7.62891617273497e-06,
      "loss": 0.837,
      "step": 35340
    },
    {
      "epoch": 2.240185050223391,
      "grad_norm": 2.7579121589660645,
      "learning_rate": 7.6225656223539375e-06,
      "loss": 0.8469,
      "step": 35350
    },
    {
      "epoch": 2.2408187838651417,
      "grad_norm": 2.7840993404388428,
      "learning_rate": 7.616215071972905e-06,
      "loss": 0.8588,
      "step": 35360
    },
    {
      "epoch": 2.241452517506892,
      "grad_norm": 2.3203790187835693,
      "learning_rate": 7.609864521591871e-06,
      "loss": 0.8434,
      "step": 35370
    },
    {
      "epoch": 2.2420862511486424,
      "grad_norm": 2.6534218788146973,
      "learning_rate": 7.6035139712108385e-06,
      "loss": 0.8248,
      "step": 35380
    },
    {
      "epoch": 2.2427199847903925,
      "grad_norm": 2.4789767265319824,
      "learning_rate": 7.597163420829805e-06,
      "loss": 0.8716,
      "step": 35390
    },
    {
      "epoch": 2.243353718432143,
      "grad_norm": 2.993574380874634,
      "learning_rate": 7.590812870448772e-06,
      "loss": 0.8907,
      "step": 35400
    },
    {
      "epoch": 2.243987452073893,
      "grad_norm": 2.6249892711639404,
      "learning_rate": 7.58446232006774e-06,
      "loss": 0.8414,
      "step": 35410
    },
    {
      "epoch": 2.2446211857156437,
      "grad_norm": 4.733240604400635,
      "learning_rate": 7.578111769686706e-06,
      "loss": 0.846,
      "step": 35420
    },
    {
      "epoch": 2.245254919357394,
      "grad_norm": 2.4899871349334717,
      "learning_rate": 7.571761219305673e-06,
      "loss": 0.84,
      "step": 35430
    },
    {
      "epoch": 2.2458886529991444,
      "grad_norm": 2.2409651279449463,
      "learning_rate": 7.565410668924641e-06,
      "loss": 0.8418,
      "step": 35440
    },
    {
      "epoch": 2.246522386640895,
      "grad_norm": 3.108464479446411,
      "learning_rate": 7.559060118543607e-06,
      "loss": 0.8213,
      "step": 35450
    },
    {
      "epoch": 2.247156120282645,
      "grad_norm": 2.718740940093994,
      "learning_rate": 7.552709568162575e-06,
      "loss": 0.8895,
      "step": 35460
    },
    {
      "epoch": 2.2477898539243957,
      "grad_norm": 2.7494919300079346,
      "learning_rate": 7.5463590177815405e-06,
      "loss": 0.8564,
      "step": 35470
    },
    {
      "epoch": 2.248423587566146,
      "grad_norm": 2.710554838180542,
      "learning_rate": 7.540008467400508e-06,
      "loss": 0.8149,
      "step": 35480
    },
    {
      "epoch": 2.2490573212078964,
      "grad_norm": 2.8134350776672363,
      "learning_rate": 7.533657917019476e-06,
      "loss": 0.8631,
      "step": 35490
    },
    {
      "epoch": 2.2496910548496465,
      "grad_norm": 2.689194917678833,
      "learning_rate": 7.5273073666384415e-06,
      "loss": 0.8437,
      "step": 35500
    },
    {
      "epoch": 2.250324788491397,
      "grad_norm": 2.7394723892211914,
      "learning_rate": 7.52095681625741e-06,
      "loss": 0.913,
      "step": 35510
    },
    {
      "epoch": 2.2509585221331476,
      "grad_norm": 2.678751230239868,
      "learning_rate": 7.514606265876375e-06,
      "loss": 0.8905,
      "step": 35520
    },
    {
      "epoch": 2.2515922557748977,
      "grad_norm": 2.8931612968444824,
      "learning_rate": 7.508255715495343e-06,
      "loss": 0.8774,
      "step": 35530
    },
    {
      "epoch": 2.2522259894166483,
      "grad_norm": 2.7704455852508545,
      "learning_rate": 7.501905165114311e-06,
      "loss": 0.8498,
      "step": 35540
    },
    {
      "epoch": 2.2528597230583984,
      "grad_norm": 2.5036814212799072,
      "learning_rate": 7.495554614733277e-06,
      "loss": 0.8593,
      "step": 35550
    },
    {
      "epoch": 2.253493456700149,
      "grad_norm": 2.668700933456421,
      "learning_rate": 7.489204064352244e-06,
      "loss": 0.8776,
      "step": 35560
    },
    {
      "epoch": 2.254127190341899,
      "grad_norm": 2.4803667068481445,
      "learning_rate": 7.482853513971211e-06,
      "loss": 0.8498,
      "step": 35570
    },
    {
      "epoch": 2.2547609239836497,
      "grad_norm": 2.4157555103302,
      "learning_rate": 7.476502963590178e-06,
      "loss": 0.8399,
      "step": 35580
    },
    {
      "epoch": 2.2553946576254003,
      "grad_norm": 3.027026414871216,
      "learning_rate": 7.470152413209145e-06,
      "loss": 0.8154,
      "step": 35590
    },
    {
      "epoch": 2.2560283912671504,
      "grad_norm": 2.29903507232666,
      "learning_rate": 7.463801862828112e-06,
      "loss": 0.8285,
      "step": 35600
    },
    {
      "epoch": 2.256662124908901,
      "grad_norm": 2.6320676803588867,
      "learning_rate": 7.457451312447079e-06,
      "loss": 0.8596,
      "step": 35610
    },
    {
      "epoch": 2.257295858550651,
      "grad_norm": 2.5681002140045166,
      "learning_rate": 7.451100762066045e-06,
      "loss": 0.8961,
      "step": 35620
    },
    {
      "epoch": 2.2579295921924016,
      "grad_norm": 2.668449878692627,
      "learning_rate": 7.4447502116850135e-06,
      "loss": 0.8462,
      "step": 35630
    },
    {
      "epoch": 2.2585633258341518,
      "grad_norm": 2.375713586807251,
      "learning_rate": 7.43839966130398e-06,
      "loss": 0.8799,
      "step": 35640
    },
    {
      "epoch": 2.2591970594759023,
      "grad_norm": 2.2742931842803955,
      "learning_rate": 7.432049110922947e-06,
      "loss": 0.8733,
      "step": 35650
    },
    {
      "epoch": 2.2598307931176524,
      "grad_norm": 2.5855391025543213,
      "learning_rate": 7.425698560541914e-06,
      "loss": 0.7896,
      "step": 35660
    },
    {
      "epoch": 2.260464526759403,
      "grad_norm": 2.4798901081085205,
      "learning_rate": 7.41934801016088e-06,
      "loss": 0.8477,
      "step": 35670
    },
    {
      "epoch": 2.2610982604011536,
      "grad_norm": 2.7802584171295166,
      "learning_rate": 7.412997459779848e-06,
      "loss": 0.8273,
      "step": 35680
    },
    {
      "epoch": 2.2617319940429037,
      "grad_norm": 3.4630162715911865,
      "learning_rate": 7.406646909398815e-06,
      "loss": 0.8266,
      "step": 35690
    },
    {
      "epoch": 2.2623657276846543,
      "grad_norm": 3.0217320919036865,
      "learning_rate": 7.400296359017782e-06,
      "loss": 0.8627,
      "step": 35700
    },
    {
      "epoch": 2.2629994613264044,
      "grad_norm": 2.647895336151123,
      "learning_rate": 7.393945808636748e-06,
      "loss": 0.8779,
      "step": 35710
    },
    {
      "epoch": 2.263633194968155,
      "grad_norm": 2.6572630405426025,
      "learning_rate": 7.387595258255716e-06,
      "loss": 0.8769,
      "step": 35720
    },
    {
      "epoch": 2.264266928609905,
      "grad_norm": 2.4307687282562256,
      "learning_rate": 7.381244707874683e-06,
      "loss": 0.8789,
      "step": 35730
    },
    {
      "epoch": 2.2649006622516556,
      "grad_norm": 2.5153560638427734,
      "learning_rate": 7.374894157493649e-06,
      "loss": 0.8216,
      "step": 35740
    },
    {
      "epoch": 2.2655343958934058,
      "grad_norm": 3.0328667163848877,
      "learning_rate": 7.368543607112617e-06,
      "loss": 0.8651,
      "step": 35750
    },
    {
      "epoch": 2.2661681295351563,
      "grad_norm": 2.8254177570343018,
      "learning_rate": 7.362193056731584e-06,
      "loss": 0.8761,
      "step": 35760
    },
    {
      "epoch": 2.266801863176907,
      "grad_norm": 3.0603764057159424,
      "learning_rate": 7.355842506350551e-06,
      "loss": 0.824,
      "step": 35770
    },
    {
      "epoch": 2.267435596818657,
      "grad_norm": 2.7975988388061523,
      "learning_rate": 7.3494919559695176e-06,
      "loss": 0.8103,
      "step": 35780
    },
    {
      "epoch": 2.2680693304604076,
      "grad_norm": 3.0705790519714355,
      "learning_rate": 7.343141405588484e-06,
      "loss": 0.8923,
      "step": 35790
    },
    {
      "epoch": 2.2687030641021577,
      "grad_norm": 3.1620078086853027,
      "learning_rate": 7.336790855207451e-06,
      "loss": 0.8672,
      "step": 35800
    },
    {
      "epoch": 2.2693367977439083,
      "grad_norm": 2.9315185546875,
      "learning_rate": 7.3304403048264185e-06,
      "loss": 0.8146,
      "step": 35810
    },
    {
      "epoch": 2.269970531385659,
      "grad_norm": 3.0837295055389404,
      "learning_rate": 7.324089754445386e-06,
      "loss": 0.8566,
      "step": 35820
    },
    {
      "epoch": 2.270604265027409,
      "grad_norm": 3.3791277408599854,
      "learning_rate": 7.317739204064352e-06,
      "loss": 0.8675,
      "step": 35830
    },
    {
      "epoch": 2.2712379986691595,
      "grad_norm": 2.626213788986206,
      "learning_rate": 7.3113886536833195e-06,
      "loss": 0.8761,
      "step": 35840
    },
    {
      "epoch": 2.2718717323109097,
      "grad_norm": 2.717442750930786,
      "learning_rate": 7.305038103302287e-06,
      "loss": 0.8331,
      "step": 35850
    },
    {
      "epoch": 2.2725054659526602,
      "grad_norm": 2.7258169651031494,
      "learning_rate": 7.298687552921253e-06,
      "loss": 0.8776,
      "step": 35860
    },
    {
      "epoch": 2.2731391995944104,
      "grad_norm": 2.939143419265747,
      "learning_rate": 7.2923370025402205e-06,
      "loss": 0.8559,
      "step": 35870
    },
    {
      "epoch": 2.273772933236161,
      "grad_norm": 2.9075238704681396,
      "learning_rate": 7.285986452159187e-06,
      "loss": 0.8602,
      "step": 35880
    },
    {
      "epoch": 2.274406666877911,
      "grad_norm": 3.2214810848236084,
      "learning_rate": 7.279635901778155e-06,
      "loss": 0.8451,
      "step": 35890
    },
    {
      "epoch": 2.2750404005196616,
      "grad_norm": 2.9984560012817383,
      "learning_rate": 7.2732853513971214e-06,
      "loss": 0.8819,
      "step": 35900
    },
    {
      "epoch": 2.275674134161412,
      "grad_norm": 2.638834238052368,
      "learning_rate": 7.266934801016088e-06,
      "loss": 0.8462,
      "step": 35910
    },
    {
      "epoch": 2.2763078678031623,
      "grad_norm": 4.135647296905518,
      "learning_rate": 7.260584250635055e-06,
      "loss": 0.8267,
      "step": 35920
    },
    {
      "epoch": 2.276941601444913,
      "grad_norm": 2.780587673187256,
      "learning_rate": 7.254233700254022e-06,
      "loss": 0.8012,
      "step": 35930
    },
    {
      "epoch": 2.277575335086663,
      "grad_norm": 2.7109732627868652,
      "learning_rate": 7.24788314987299e-06,
      "loss": 0.841,
      "step": 35940
    },
    {
      "epoch": 2.2782090687284136,
      "grad_norm": 3.0121214389801025,
      "learning_rate": 7.241532599491956e-06,
      "loss": 0.818,
      "step": 35950
    },
    {
      "epoch": 2.2788428023701637,
      "grad_norm": 2.42964768409729,
      "learning_rate": 7.235182049110923e-06,
      "loss": 0.8679,
      "step": 35960
    },
    {
      "epoch": 2.2794765360119142,
      "grad_norm": 2.7458531856536865,
      "learning_rate": 7.22883149872989e-06,
      "loss": 0.8042,
      "step": 35970
    },
    {
      "epoch": 2.2801102696536644,
      "grad_norm": 2.61177134513855,
      "learning_rate": 7.222480948348857e-06,
      "loss": 0.8506,
      "step": 35980
    },
    {
      "epoch": 2.280744003295415,
      "grad_norm": 3.2450859546661377,
      "learning_rate": 7.216130397967824e-06,
      "loss": 0.8881,
      "step": 35990
    },
    {
      "epoch": 2.2813777369371655,
      "grad_norm": 3.3855834007263184,
      "learning_rate": 7.209779847586791e-06,
      "loss": 0.8551,
      "step": 36000
    },
    {
      "epoch": 2.2820114705789156,
      "grad_norm": 2.919193983078003,
      "learning_rate": 7.203429297205758e-06,
      "loss": 0.86,
      "step": 36010
    },
    {
      "epoch": 2.282645204220666,
      "grad_norm": 2.9624006748199463,
      "learning_rate": 7.197078746824725e-06,
      "loss": 0.8669,
      "step": 36020
    },
    {
      "epoch": 2.2832789378624163,
      "grad_norm": 2.603806257247925,
      "learning_rate": 7.190728196443692e-06,
      "loss": 0.8869,
      "step": 36030
    },
    {
      "epoch": 2.283912671504167,
      "grad_norm": 2.9609909057617188,
      "learning_rate": 7.184377646062659e-06,
      "loss": 0.865,
      "step": 36040
    },
    {
      "epoch": 2.284546405145917,
      "grad_norm": 2.2071287631988525,
      "learning_rate": 7.1780270956816255e-06,
      "loss": 0.8277,
      "step": 36050
    },
    {
      "epoch": 2.2851801387876676,
      "grad_norm": 2.7223920822143555,
      "learning_rate": 7.171676545300593e-06,
      "loss": 0.8191,
      "step": 36060
    },
    {
      "epoch": 2.2858138724294177,
      "grad_norm": 3.1551849842071533,
      "learning_rate": 7.16532599491956e-06,
      "loss": 0.8514,
      "step": 36070
    },
    {
      "epoch": 2.2864476060711683,
      "grad_norm": 3.116177797317505,
      "learning_rate": 7.158975444538527e-06,
      "loss": 0.8366,
      "step": 36080
    },
    {
      "epoch": 2.287081339712919,
      "grad_norm": 2.6532371044158936,
      "learning_rate": 7.152624894157494e-06,
      "loss": 0.8532,
      "step": 36090
    },
    {
      "epoch": 2.287715073354669,
      "grad_norm": 2.611295461654663,
      "learning_rate": 7.14627434377646e-06,
      "loss": 0.8174,
      "step": 36100
    },
    {
      "epoch": 2.2883488069964195,
      "grad_norm": 2.7600934505462646,
      "learning_rate": 7.139923793395428e-06,
      "loss": 0.8664,
      "step": 36110
    },
    {
      "epoch": 2.2889825406381696,
      "grad_norm": 2.6924707889556885,
      "learning_rate": 7.133573243014395e-06,
      "loss": 0.8436,
      "step": 36120
    },
    {
      "epoch": 2.28961627427992,
      "grad_norm": 2.77048921585083,
      "learning_rate": 7.127222692633362e-06,
      "loss": 0.8002,
      "step": 36130
    },
    {
      "epoch": 2.2902500079216703,
      "grad_norm": 3.1908118724823,
      "learning_rate": 7.120872142252328e-06,
      "loss": 0.8387,
      "step": 36140
    },
    {
      "epoch": 2.290883741563421,
      "grad_norm": 2.8662524223327637,
      "learning_rate": 7.114521591871296e-06,
      "loss": 0.8232,
      "step": 36150
    },
    {
      "epoch": 2.291517475205171,
      "grad_norm": 4.128607749938965,
      "learning_rate": 7.108171041490263e-06,
      "loss": 0.904,
      "step": 36160
    },
    {
      "epoch": 2.2921512088469216,
      "grad_norm": 2.506612539291382,
      "learning_rate": 7.101820491109229e-06,
      "loss": 0.8458,
      "step": 36170
    },
    {
      "epoch": 2.292784942488672,
      "grad_norm": 2.6603176593780518,
      "learning_rate": 7.095469940728197e-06,
      "loss": 0.8531,
      "step": 36180
    },
    {
      "epoch": 2.2934186761304223,
      "grad_norm": 2.9628868103027344,
      "learning_rate": 7.089119390347163e-06,
      "loss": 0.8961,
      "step": 36190
    },
    {
      "epoch": 2.294052409772173,
      "grad_norm": 3.0537023544311523,
      "learning_rate": 7.082768839966131e-06,
      "loss": 0.8547,
      "step": 36200
    },
    {
      "epoch": 2.294686143413923,
      "grad_norm": 2.629493236541748,
      "learning_rate": 7.076418289585098e-06,
      "loss": 0.8394,
      "step": 36210
    },
    {
      "epoch": 2.2953198770556735,
      "grad_norm": 5.144898414611816,
      "learning_rate": 7.070067739204064e-06,
      "loss": 0.885,
      "step": 36220
    },
    {
      "epoch": 2.295953610697424,
      "grad_norm": 2.598743200302124,
      "learning_rate": 7.063717188823031e-06,
      "loss": 0.8525,
      "step": 36230
    },
    {
      "epoch": 2.296587344339174,
      "grad_norm": 2.3736062049865723,
      "learning_rate": 7.057366638441999e-06,
      "loss": 0.82,
      "step": 36240
    },
    {
      "epoch": 2.297221077980925,
      "grad_norm": 2.9203436374664307,
      "learning_rate": 7.051016088060966e-06,
      "loss": 0.8674,
      "step": 36250
    },
    {
      "epoch": 2.297854811622675,
      "grad_norm": 2.302089214324951,
      "learning_rate": 7.044665537679932e-06,
      "loss": 0.8648,
      "step": 36260
    },
    {
      "epoch": 2.2984885452644255,
      "grad_norm": 4.647731304168701,
      "learning_rate": 7.0383149872988996e-06,
      "loss": 0.8705,
      "step": 36270
    },
    {
      "epoch": 2.2991222789061756,
      "grad_norm": 3.0287234783172607,
      "learning_rate": 7.031964436917867e-06,
      "loss": 0.8662,
      "step": 36280
    },
    {
      "epoch": 2.299756012547926,
      "grad_norm": 2.865359306335449,
      "learning_rate": 7.025613886536833e-06,
      "loss": 0.8184,
      "step": 36290
    },
    {
      "epoch": 2.3003897461896763,
      "grad_norm": 2.8037195205688477,
      "learning_rate": 7.0192633361558005e-06,
      "loss": 0.8468,
      "step": 36300
    },
    {
      "epoch": 2.301023479831427,
      "grad_norm": 2.577805280685425,
      "learning_rate": 7.012912785774767e-06,
      "loss": 0.8554,
      "step": 36310
    },
    {
      "epoch": 2.3016572134731774,
      "grad_norm": 2.9320788383483887,
      "learning_rate": 7.006562235393734e-06,
      "loss": 0.8991,
      "step": 36320
    },
    {
      "epoch": 2.3022909471149275,
      "grad_norm": 3.3775601387023926,
      "learning_rate": 7.0002116850127015e-06,
      "loss": 0.891,
      "step": 36330
    },
    {
      "epoch": 2.302924680756678,
      "grad_norm": 2.690244197845459,
      "learning_rate": 6.993861134631668e-06,
      "loss": 0.857,
      "step": 36340
    },
    {
      "epoch": 2.3035584143984282,
      "grad_norm": 2.47300124168396,
      "learning_rate": 6.987510584250635e-06,
      "loss": 0.8102,
      "step": 36350
    },
    {
      "epoch": 2.304192148040179,
      "grad_norm": 2.6514780521392822,
      "learning_rate": 6.981160033869602e-06,
      "loss": 0.8895,
      "step": 36360
    },
    {
      "epoch": 2.304825881681929,
      "grad_norm": 3.257770538330078,
      "learning_rate": 6.97480948348857e-06,
      "loss": 0.8606,
      "step": 36370
    },
    {
      "epoch": 2.3054596153236795,
      "grad_norm": 2.9675652980804443,
      "learning_rate": 6.968458933107536e-06,
      "loss": 0.8712,
      "step": 36380
    },
    {
      "epoch": 2.3060933489654296,
      "grad_norm": 2.562605142593384,
      "learning_rate": 6.9621083827265035e-06,
      "loss": 0.8621,
      "step": 36390
    },
    {
      "epoch": 2.30672708260718,
      "grad_norm": 2.8348262310028076,
      "learning_rate": 6.95575783234547e-06,
      "loss": 0.8088,
      "step": 36400
    },
    {
      "epoch": 2.3073608162489307,
      "grad_norm": 3.065904140472412,
      "learning_rate": 6.949407281964437e-06,
      "loss": 0.8781,
      "step": 36410
    },
    {
      "epoch": 2.307994549890681,
      "grad_norm": 2.5145504474639893,
      "learning_rate": 6.9430567315834044e-06,
      "loss": 0.8546,
      "step": 36420
    },
    {
      "epoch": 2.3086282835324314,
      "grad_norm": 2.7915241718292236,
      "learning_rate": 6.936706181202371e-06,
      "loss": 0.8428,
      "step": 36430
    },
    {
      "epoch": 2.3092620171741816,
      "grad_norm": 2.6844887733459473,
      "learning_rate": 6.930355630821338e-06,
      "loss": 0.834,
      "step": 36440
    },
    {
      "epoch": 2.309895750815932,
      "grad_norm": 2.770524263381958,
      "learning_rate": 6.9240050804403046e-06,
      "loss": 0.8297,
      "step": 36450
    },
    {
      "epoch": 2.3105294844576822,
      "grad_norm": 2.634777307510376,
      "learning_rate": 6.917654530059272e-06,
      "loss": 0.8973,
      "step": 36460
    },
    {
      "epoch": 2.311163218099433,
      "grad_norm": 2.8559412956237793,
      "learning_rate": 6.911303979678239e-06,
      "loss": 0.8672,
      "step": 36470
    },
    {
      "epoch": 2.311796951741183,
      "grad_norm": 2.6081786155700684,
      "learning_rate": 6.9049534292972055e-06,
      "loss": 0.8555,
      "step": 36480
    },
    {
      "epoch": 2.3124306853829335,
      "grad_norm": 2.5081264972686768,
      "learning_rate": 6.898602878916173e-06,
      "loss": 0.8247,
      "step": 36490
    },
    {
      "epoch": 2.313064419024684,
      "grad_norm": 3.044435977935791,
      "learning_rate": 6.89225232853514e-06,
      "loss": 0.8509,
      "step": 36500
    },
    {
      "epoch": 2.313698152666434,
      "grad_norm": 2.605125904083252,
      "learning_rate": 6.885901778154107e-06,
      "loss": 0.8443,
      "step": 36510
    },
    {
      "epoch": 2.3143318863081848,
      "grad_norm": 2.692660093307495,
      "learning_rate": 6.879551227773074e-06,
      "loss": 0.8357,
      "step": 36520
    },
    {
      "epoch": 2.314965619949935,
      "grad_norm": 3.1726200580596924,
      "learning_rate": 6.87320067739204e-06,
      "loss": 0.834,
      "step": 36530
    },
    {
      "epoch": 2.3155993535916854,
      "grad_norm": 2.638993978500366,
      "learning_rate": 6.866850127011008e-06,
      "loss": 0.8416,
      "step": 36540
    },
    {
      "epoch": 2.316233087233436,
      "grad_norm": 2.282440423965454,
      "learning_rate": 6.860499576629975e-06,
      "loss": 0.845,
      "step": 36550
    },
    {
      "epoch": 2.316866820875186,
      "grad_norm": 2.2133123874664307,
      "learning_rate": 6.854149026248942e-06,
      "loss": 0.8595,
      "step": 36560
    },
    {
      "epoch": 2.3175005545169367,
      "grad_norm": 3.07348370552063,
      "learning_rate": 6.8477984758679085e-06,
      "loss": 0.8336,
      "step": 36570
    },
    {
      "epoch": 2.318134288158687,
      "grad_norm": 3.4510743618011475,
      "learning_rate": 6.841447925486875e-06,
      "loss": 0.864,
      "step": 36580
    },
    {
      "epoch": 2.3187680218004374,
      "grad_norm": 2.7023837566375732,
      "learning_rate": 6.835097375105843e-06,
      "loss": 0.8632,
      "step": 36590
    },
    {
      "epoch": 2.3194017554421875,
      "grad_norm": 2.572709798812866,
      "learning_rate": 6.8287468247248094e-06,
      "loss": 0.8357,
      "step": 36600
    },
    {
      "epoch": 2.320035489083938,
      "grad_norm": 2.6495723724365234,
      "learning_rate": 6.822396274343777e-06,
      "loss": 0.8706,
      "step": 36610
    },
    {
      "epoch": 2.320669222725688,
      "grad_norm": 2.681915044784546,
      "learning_rate": 6.816045723962743e-06,
      "loss": 0.8789,
      "step": 36620
    },
    {
      "epoch": 2.3213029563674388,
      "grad_norm": 2.5756466388702393,
      "learning_rate": 6.809695173581711e-06,
      "loss": 0.8413,
      "step": 36630
    },
    {
      "epoch": 2.3219366900091893,
      "grad_norm": 2.7186641693115234,
      "learning_rate": 6.803344623200678e-06,
      "loss": 0.8881,
      "step": 36640
    },
    {
      "epoch": 2.3225704236509395,
      "grad_norm": 2.573389768600464,
      "learning_rate": 6.796994072819644e-06,
      "loss": 0.8634,
      "step": 36650
    },
    {
      "epoch": 2.32320415729269,
      "grad_norm": 3.6237354278564453,
      "learning_rate": 6.790643522438611e-06,
      "loss": 0.9052,
      "step": 36660
    },
    {
      "epoch": 2.32383789093444,
      "grad_norm": 3.4912402629852295,
      "learning_rate": 6.784292972057579e-06,
      "loss": 0.8347,
      "step": 36670
    },
    {
      "epoch": 2.3244716245761907,
      "grad_norm": 2.535918951034546,
      "learning_rate": 6.777942421676546e-06,
      "loss": 0.8436,
      "step": 36680
    },
    {
      "epoch": 2.325105358217941,
      "grad_norm": 2.576215982437134,
      "learning_rate": 6.771591871295512e-06,
      "loss": 0.8314,
      "step": 36690
    },
    {
      "epoch": 2.3257390918596914,
      "grad_norm": 3.3668713569641113,
      "learning_rate": 6.76524132091448e-06,
      "loss": 0.84,
      "step": 36700
    },
    {
      "epoch": 2.3263728255014415,
      "grad_norm": 3.518245220184326,
      "learning_rate": 6.758890770533446e-06,
      "loss": 0.8308,
      "step": 36710
    },
    {
      "epoch": 2.327006559143192,
      "grad_norm": 3.1003854274749756,
      "learning_rate": 6.752540220152413e-06,
      "loss": 0.7943,
      "step": 36720
    },
    {
      "epoch": 2.3276402927849427,
      "grad_norm": 2.55725359916687,
      "learning_rate": 6.746189669771381e-06,
      "loss": 0.7787,
      "step": 36730
    },
    {
      "epoch": 2.328274026426693,
      "grad_norm": 2.464235544204712,
      "learning_rate": 6.739839119390347e-06,
      "loss": 0.8193,
      "step": 36740
    },
    {
      "epoch": 2.3289077600684434,
      "grad_norm": 2.8892159461975098,
      "learning_rate": 6.733488569009314e-06,
      "loss": 0.8655,
      "step": 36750
    },
    {
      "epoch": 2.3295414937101935,
      "grad_norm": 2.7674596309661865,
      "learning_rate": 6.727138018628282e-06,
      "loss": 0.8932,
      "step": 36760
    },
    {
      "epoch": 2.330175227351944,
      "grad_norm": 2.489149332046509,
      "learning_rate": 6.720787468247248e-06,
      "loss": 0.8532,
      "step": 36770
    },
    {
      "epoch": 2.330808960993694,
      "grad_norm": 2.869220495223999,
      "learning_rate": 6.714436917866215e-06,
      "loss": 0.8306,
      "step": 36780
    },
    {
      "epoch": 2.3314426946354447,
      "grad_norm": 2.7889323234558105,
      "learning_rate": 6.708086367485182e-06,
      "loss": 0.8271,
      "step": 36790
    },
    {
      "epoch": 2.332076428277195,
      "grad_norm": 3.1662111282348633,
      "learning_rate": 6.70173581710415e-06,
      "loss": 0.7977,
      "step": 36800
    },
    {
      "epoch": 2.3327101619189454,
      "grad_norm": 2.5895934104919434,
      "learning_rate": 6.695385266723116e-06,
      "loss": 0.8158,
      "step": 36810
    },
    {
      "epoch": 2.333343895560696,
      "grad_norm": 2.2550346851348877,
      "learning_rate": 6.6890347163420835e-06,
      "loss": 0.8341,
      "step": 36820
    },
    {
      "epoch": 2.333977629202446,
      "grad_norm": 2.958711862564087,
      "learning_rate": 6.68268416596105e-06,
      "loss": 0.7885,
      "step": 36830
    },
    {
      "epoch": 2.3346113628441967,
      "grad_norm": 2.638951063156128,
      "learning_rate": 6.676333615580016e-06,
      "loss": 0.916,
      "step": 36840
    },
    {
      "epoch": 2.335245096485947,
      "grad_norm": 3.257436513900757,
      "learning_rate": 6.6699830651989845e-06,
      "loss": 0.8734,
      "step": 36850
    },
    {
      "epoch": 2.3358788301276974,
      "grad_norm": 3.0470492839813232,
      "learning_rate": 6.663632514817951e-06,
      "loss": 0.8574,
      "step": 36860
    },
    {
      "epoch": 2.336512563769448,
      "grad_norm": 2.8293070793151855,
      "learning_rate": 6.657281964436918e-06,
      "loss": 0.8462,
      "step": 36870
    },
    {
      "epoch": 2.337146297411198,
      "grad_norm": 3.0200130939483643,
      "learning_rate": 6.650931414055885e-06,
      "loss": 0.8411,
      "step": 36880
    },
    {
      "epoch": 2.3377800310529486,
      "grad_norm": 2.995882272720337,
      "learning_rate": 6.644580863674852e-06,
      "loss": 0.8505,
      "step": 36890
    },
    {
      "epoch": 2.3384137646946987,
      "grad_norm": 2.776723623275757,
      "learning_rate": 6.638230313293819e-06,
      "loss": 0.9046,
      "step": 36900
    },
    {
      "epoch": 2.3390474983364493,
      "grad_norm": 2.5319418907165527,
      "learning_rate": 6.631879762912786e-06,
      "loss": 0.866,
      "step": 36910
    },
    {
      "epoch": 2.3396812319781994,
      "grad_norm": 2.5818164348602295,
      "learning_rate": 6.625529212531753e-06,
      "loss": 0.8226,
      "step": 36920
    },
    {
      "epoch": 2.34031496561995,
      "grad_norm": 2.665658473968506,
      "learning_rate": 6.61917866215072e-06,
      "loss": 0.8248,
      "step": 36930
    },
    {
      "epoch": 2.3409486992617,
      "grad_norm": 3.260784149169922,
      "learning_rate": 6.6128281117696874e-06,
      "loss": 0.8402,
      "step": 36940
    },
    {
      "epoch": 2.3415824329034507,
      "grad_norm": 2.7754173278808594,
      "learning_rate": 6.606477561388654e-06,
      "loss": 0.8539,
      "step": 36950
    },
    {
      "epoch": 2.3422161665452013,
      "grad_norm": 2.689917802810669,
      "learning_rate": 6.60012701100762e-06,
      "loss": 0.8184,
      "step": 36960
    },
    {
      "epoch": 2.3428499001869514,
      "grad_norm": 3.16351318359375,
      "learning_rate": 6.5937764606265876e-06,
      "loss": 0.8246,
      "step": 36970
    },
    {
      "epoch": 2.343483633828702,
      "grad_norm": 2.756734848022461,
      "learning_rate": 6.587425910245555e-06,
      "loss": 0.8502,
      "step": 36980
    },
    {
      "epoch": 2.344117367470452,
      "grad_norm": 2.735008955001831,
      "learning_rate": 6.581075359864522e-06,
      "loss": 0.8426,
      "step": 36990
    },
    {
      "epoch": 2.3447511011122026,
      "grad_norm": 2.7078068256378174,
      "learning_rate": 6.5747248094834885e-06,
      "loss": 0.8155,
      "step": 37000
    },
    {
      "epoch": 2.3453848347539528,
      "grad_norm": 2.693049669265747,
      "learning_rate": 6.568374259102455e-06,
      "loss": 0.7808,
      "step": 37010
    },
    {
      "epoch": 2.3460185683957033,
      "grad_norm": 2.5338327884674072,
      "learning_rate": 6.562023708721423e-06,
      "loss": 0.8206,
      "step": 37020
    },
    {
      "epoch": 2.3466523020374535,
      "grad_norm": 2.5552852153778076,
      "learning_rate": 6.5556731583403895e-06,
      "loss": 0.8196,
      "step": 37030
    },
    {
      "epoch": 2.347286035679204,
      "grad_norm": 2.510488748550415,
      "learning_rate": 6.549322607959357e-06,
      "loss": 0.8592,
      "step": 37040
    },
    {
      "epoch": 2.3479197693209546,
      "grad_norm": 2.8338871002197266,
      "learning_rate": 6.542972057578323e-06,
      "loss": 0.8717,
      "step": 37050
    },
    {
      "epoch": 2.3485535029627047,
      "grad_norm": 2.553335428237915,
      "learning_rate": 6.536621507197291e-06,
      "loss": 0.8862,
      "step": 37060
    },
    {
      "epoch": 2.3491872366044553,
      "grad_norm": 2.8668935298919678,
      "learning_rate": 6.530270956816258e-06,
      "loss": 0.8492,
      "step": 37070
    },
    {
      "epoch": 2.3498209702462054,
      "grad_norm": 2.743921995162964,
      "learning_rate": 6.523920406435224e-06,
      "loss": 0.8125,
      "step": 37080
    },
    {
      "epoch": 2.350454703887956,
      "grad_norm": 2.828352928161621,
      "learning_rate": 6.5175698560541915e-06,
      "loss": 0.8144,
      "step": 37090
    },
    {
      "epoch": 2.351088437529706,
      "grad_norm": 3.146714210510254,
      "learning_rate": 6.511219305673158e-06,
      "loss": 0.8419,
      "step": 37100
    },
    {
      "epoch": 2.3517221711714567,
      "grad_norm": 3.0893442630767822,
      "learning_rate": 6.504868755292126e-06,
      "loss": 0.8568,
      "step": 37110
    },
    {
      "epoch": 2.3523559048132068,
      "grad_norm": 2.6177642345428467,
      "learning_rate": 6.4985182049110924e-06,
      "loss": 0.8196,
      "step": 37120
    },
    {
      "epoch": 2.3529896384549573,
      "grad_norm": 2.799280881881714,
      "learning_rate": 6.492167654530059e-06,
      "loss": 0.857,
      "step": 37130
    },
    {
      "epoch": 2.353623372096708,
      "grad_norm": 2.5466554164886475,
      "learning_rate": 6.485817104149026e-06,
      "loss": 0.8179,
      "step": 37140
    },
    {
      "epoch": 2.354257105738458,
      "grad_norm": 3.0127005577087402,
      "learning_rate": 6.479466553767993e-06,
      "loss": 0.8634,
      "step": 37150
    },
    {
      "epoch": 2.3548908393802086,
      "grad_norm": 2.464712381362915,
      "learning_rate": 6.473116003386961e-06,
      "loss": 0.8381,
      "step": 37160
    },
    {
      "epoch": 2.3555245730219587,
      "grad_norm": 2.854705810546875,
      "learning_rate": 6.466765453005927e-06,
      "loss": 0.815,
      "step": 37170
    },
    {
      "epoch": 2.3561583066637093,
      "grad_norm": 3.169792890548706,
      "learning_rate": 6.460414902624894e-06,
      "loss": 0.8573,
      "step": 37180
    },
    {
      "epoch": 2.35679204030546,
      "grad_norm": 2.6374406814575195,
      "learning_rate": 6.454064352243862e-06,
      "loss": 0.8847,
      "step": 37190
    },
    {
      "epoch": 2.35742577394721,
      "grad_norm": 2.8339505195617676,
      "learning_rate": 6.447713801862828e-06,
      "loss": 0.8433,
      "step": 37200
    },
    {
      "epoch": 2.3580595075889605,
      "grad_norm": 2.6493020057678223,
      "learning_rate": 6.441363251481795e-06,
      "loss": 0.8093,
      "step": 37210
    },
    {
      "epoch": 2.3586932412307107,
      "grad_norm": 2.718031644821167,
      "learning_rate": 6.435012701100762e-06,
      "loss": 0.8231,
      "step": 37220
    },
    {
      "epoch": 2.3593269748724612,
      "grad_norm": 3.748894453048706,
      "learning_rate": 6.428662150719729e-06,
      "loss": 0.8505,
      "step": 37230
    },
    {
      "epoch": 2.3599607085142114,
      "grad_norm": 3.3117527961730957,
      "learning_rate": 6.422311600338696e-06,
      "loss": 0.849,
      "step": 37240
    },
    {
      "epoch": 2.360594442155962,
      "grad_norm": 2.8448004722595215,
      "learning_rate": 6.415961049957664e-06,
      "loss": 0.8484,
      "step": 37250
    },
    {
      "epoch": 2.361228175797712,
      "grad_norm": 2.688570976257324,
      "learning_rate": 6.40961049957663e-06,
      "loss": 0.8417,
      "step": 37260
    },
    {
      "epoch": 2.3618619094394626,
      "grad_norm": 2.904914379119873,
      "learning_rate": 6.4032599491955965e-06,
      "loss": 0.8816,
      "step": 37270
    },
    {
      "epoch": 2.362495643081213,
      "grad_norm": 2.5726497173309326,
      "learning_rate": 6.396909398814565e-06,
      "loss": 0.8343,
      "step": 37280
    },
    {
      "epoch": 2.3631293767229633,
      "grad_norm": 3.038166046142578,
      "learning_rate": 6.390558848433531e-06,
      "loss": 0.8414,
      "step": 37290
    },
    {
      "epoch": 2.363763110364714,
      "grad_norm": 2.538501024246216,
      "learning_rate": 6.384208298052498e-06,
      "loss": 0.8157,
      "step": 37300
    },
    {
      "epoch": 2.364396844006464,
      "grad_norm": 3.6455795764923096,
      "learning_rate": 6.377857747671465e-06,
      "loss": 0.886,
      "step": 37310
    },
    {
      "epoch": 2.3650305776482146,
      "grad_norm": 2.746394157409668,
      "learning_rate": 6.371507197290432e-06,
      "loss": 0.8331,
      "step": 37320
    },
    {
      "epoch": 2.3656643112899647,
      "grad_norm": 2.639763355255127,
      "learning_rate": 6.365156646909399e-06,
      "loss": 0.8499,
      "step": 37330
    },
    {
      "epoch": 2.3662980449317152,
      "grad_norm": 2.6394381523132324,
      "learning_rate": 6.358806096528366e-06,
      "loss": 0.8047,
      "step": 37340
    },
    {
      "epoch": 2.3669317785734654,
      "grad_norm": 2.664428234100342,
      "learning_rate": 6.352455546147333e-06,
      "loss": 0.8639,
      "step": 37350
    },
    {
      "epoch": 2.367565512215216,
      "grad_norm": 2.4676029682159424,
      "learning_rate": 6.3461049957663e-06,
      "loss": 0.8096,
      "step": 37360
    },
    {
      "epoch": 2.3681992458569665,
      "grad_norm": 2.2983508110046387,
      "learning_rate": 6.3397544453852675e-06,
      "loss": 0.8409,
      "step": 37370
    },
    {
      "epoch": 2.3688329794987166,
      "grad_norm": 2.694138765335083,
      "learning_rate": 6.333403895004234e-06,
      "loss": 0.8593,
      "step": 37380
    },
    {
      "epoch": 2.369466713140467,
      "grad_norm": 2.8414900302886963,
      "learning_rate": 6.3270533446232e-06,
      "loss": 0.8148,
      "step": 37390
    },
    {
      "epoch": 2.3701004467822173,
      "grad_norm": 2.346611261367798,
      "learning_rate": 6.320702794242168e-06,
      "loss": 0.8324,
      "step": 37400
    },
    {
      "epoch": 2.370734180423968,
      "grad_norm": 2.9294300079345703,
      "learning_rate": 6.314352243861135e-06,
      "loss": 0.8382,
      "step": 37410
    },
    {
      "epoch": 2.371367914065718,
      "grad_norm": 2.8490970134735107,
      "learning_rate": 6.308001693480102e-06,
      "loss": 0.8458,
      "step": 37420
    },
    {
      "epoch": 2.3720016477074686,
      "grad_norm": 2.581912040710449,
      "learning_rate": 6.301651143099069e-06,
      "loss": 0.8395,
      "step": 37430
    },
    {
      "epoch": 2.3726353813492187,
      "grad_norm": 2.609616279602051,
      "learning_rate": 6.295300592718035e-06,
      "loss": 0.8116,
      "step": 37440
    },
    {
      "epoch": 2.3732691149909693,
      "grad_norm": 2.5890555381774902,
      "learning_rate": 6.288950042337003e-06,
      "loss": 0.8681,
      "step": 37450
    },
    {
      "epoch": 2.37390284863272,
      "grad_norm": 2.9706478118896484,
      "learning_rate": 6.28259949195597e-06,
      "loss": 0.8233,
      "step": 37460
    },
    {
      "epoch": 2.37453658227447,
      "grad_norm": 3.1116790771484375,
      "learning_rate": 6.276248941574937e-06,
      "loss": 0.8538,
      "step": 37470
    },
    {
      "epoch": 2.3751703159162205,
      "grad_norm": 2.6117067337036133,
      "learning_rate": 6.269898391193903e-06,
      "loss": 0.8217,
      "step": 37480
    },
    {
      "epoch": 2.3758040495579706,
      "grad_norm": 2.8275630474090576,
      "learning_rate": 6.263547840812871e-06,
      "loss": 0.8483,
      "step": 37490
    },
    {
      "epoch": 2.376437783199721,
      "grad_norm": 3.31113600730896,
      "learning_rate": 6.257197290431838e-06,
      "loss": 0.9235,
      "step": 37500
    },
    {
      "epoch": 2.3770715168414718,
      "grad_norm": 3.552129030227661,
      "learning_rate": 6.250846740050804e-06,
      "loss": 0.8491,
      "step": 37510
    },
    {
      "epoch": 2.377705250483222,
      "grad_norm": 2.9424145221710205,
      "learning_rate": 6.245131244707875e-06,
      "loss": 0.8667,
      "step": 37520
    },
    {
      "epoch": 2.3783389841249725,
      "grad_norm": 2.6639506816864014,
      "learning_rate": 6.238780694326842e-06,
      "loss": 0.853,
      "step": 37530
    },
    {
      "epoch": 2.3789727177667226,
      "grad_norm": 2.636646032333374,
      "learning_rate": 6.2324301439458084e-06,
      "loss": 0.8691,
      "step": 37540
    },
    {
      "epoch": 2.379606451408473,
      "grad_norm": 2.678823232650757,
      "learning_rate": 6.226079593564776e-06,
      "loss": 0.8073,
      "step": 37550
    },
    {
      "epoch": 2.3802401850502233,
      "grad_norm": 3.0674803256988525,
      "learning_rate": 6.219729043183743e-06,
      "loss": 0.8697,
      "step": 37560
    },
    {
      "epoch": 2.380873918691974,
      "grad_norm": 2.881732225418091,
      "learning_rate": 6.21337849280271e-06,
      "loss": 0.792,
      "step": 37570
    },
    {
      "epoch": 2.381507652333724,
      "grad_norm": 2.6954219341278076,
      "learning_rate": 6.207027942421677e-06,
      "loss": 0.7987,
      "step": 37580
    },
    {
      "epoch": 2.3821413859754745,
      "grad_norm": 2.7615268230438232,
      "learning_rate": 6.200677392040643e-06,
      "loss": 0.8485,
      "step": 37590
    },
    {
      "epoch": 2.382775119617225,
      "grad_norm": 2.4428319931030273,
      "learning_rate": 6.194326841659611e-06,
      "loss": 0.8358,
      "step": 37600
    },
    {
      "epoch": 2.3834088532589752,
      "grad_norm": 3.121534585952759,
      "learning_rate": 6.187976291278578e-06,
      "loss": 0.8675,
      "step": 37610
    },
    {
      "epoch": 2.384042586900726,
      "grad_norm": 3.0096185207366943,
      "learning_rate": 6.181625740897545e-06,
      "loss": 0.8215,
      "step": 37620
    },
    {
      "epoch": 2.384676320542476,
      "grad_norm": 2.6623435020446777,
      "learning_rate": 6.175275190516511e-06,
      "loss": 0.8805,
      "step": 37630
    },
    {
      "epoch": 2.3853100541842265,
      "grad_norm": 2.2132890224456787,
      "learning_rate": 6.168924640135478e-06,
      "loss": 0.8279,
      "step": 37640
    },
    {
      "epoch": 2.3859437878259766,
      "grad_norm": 2.6148366928100586,
      "learning_rate": 6.162574089754446e-06,
      "loss": 0.8739,
      "step": 37650
    },
    {
      "epoch": 2.386577521467727,
      "grad_norm": 2.6921839714050293,
      "learning_rate": 6.156223539373412e-06,
      "loss": 0.8287,
      "step": 37660
    },
    {
      "epoch": 2.3872112551094773,
      "grad_norm": 2.376580238342285,
      "learning_rate": 6.14987298899238e-06,
      "loss": 0.8771,
      "step": 37670
    },
    {
      "epoch": 2.387844988751228,
      "grad_norm": 2.6102051734924316,
      "learning_rate": 6.143522438611346e-06,
      "loss": 0.8766,
      "step": 37680
    },
    {
      "epoch": 2.3884787223929784,
      "grad_norm": 2.7875638008117676,
      "learning_rate": 6.137171888230314e-06,
      "loss": 0.8235,
      "step": 37690
    },
    {
      "epoch": 2.3891124560347285,
      "grad_norm": 3.47528338432312,
      "learning_rate": 6.130821337849281e-06,
      "loss": 0.8726,
      "step": 37700
    },
    {
      "epoch": 2.389746189676479,
      "grad_norm": 3.123469114303589,
      "learning_rate": 6.124470787468247e-06,
      "loss": 0.8898,
      "step": 37710
    },
    {
      "epoch": 2.3903799233182292,
      "grad_norm": 2.5580570697784424,
      "learning_rate": 6.118120237087214e-06,
      "loss": 0.8234,
      "step": 37720
    },
    {
      "epoch": 2.39101365695998,
      "grad_norm": 2.89261794090271,
      "learning_rate": 6.1117696867061816e-06,
      "loss": 0.8496,
      "step": 37730
    },
    {
      "epoch": 2.39164739060173,
      "grad_norm": 2.8949873447418213,
      "learning_rate": 6.105419136325149e-06,
      "loss": 0.8714,
      "step": 37740
    },
    {
      "epoch": 2.3922811242434805,
      "grad_norm": 3.240344285964966,
      "learning_rate": 6.099068585944115e-06,
      "loss": 0.8563,
      "step": 37750
    },
    {
      "epoch": 2.3929148578852306,
      "grad_norm": 2.570479154586792,
      "learning_rate": 6.092718035563082e-06,
      "loss": 0.8445,
      "step": 37760
    },
    {
      "epoch": 2.393548591526981,
      "grad_norm": 3.1170592308044434,
      "learning_rate": 6.086367485182049e-06,
      "loss": 0.8416,
      "step": 37770
    },
    {
      "epoch": 2.3941823251687318,
      "grad_norm": 2.912977457046509,
      "learning_rate": 6.080016934801016e-06,
      "loss": 0.8757,
      "step": 37780
    },
    {
      "epoch": 2.394816058810482,
      "grad_norm": 2.6232821941375732,
      "learning_rate": 6.0736663844199835e-06,
      "loss": 0.8243,
      "step": 37790
    },
    {
      "epoch": 2.3954497924522324,
      "grad_norm": 2.9364869594573975,
      "learning_rate": 6.06731583403895e-06,
      "loss": 0.8266,
      "step": 37800
    },
    {
      "epoch": 2.3960835260939826,
      "grad_norm": 2.6884684562683105,
      "learning_rate": 6.060965283657917e-06,
      "loss": 0.8543,
      "step": 37810
    },
    {
      "epoch": 2.396717259735733,
      "grad_norm": 2.79386043548584,
      "learning_rate": 6.0546147332768845e-06,
      "loss": 0.8126,
      "step": 37820
    },
    {
      "epoch": 2.3973509933774833,
      "grad_norm": 2.739331007003784,
      "learning_rate": 6.048264182895851e-06,
      "loss": 0.8886,
      "step": 37830
    },
    {
      "epoch": 2.397984727019234,
      "grad_norm": 2.9520394802093506,
      "learning_rate": 6.041913632514818e-06,
      "loss": 0.8431,
      "step": 37840
    },
    {
      "epoch": 2.398618460660984,
      "grad_norm": 2.8354616165161133,
      "learning_rate": 6.035563082133785e-06,
      "loss": 0.7751,
      "step": 37850
    },
    {
      "epoch": 2.3992521943027345,
      "grad_norm": 2.805717706680298,
      "learning_rate": 6.029212531752753e-06,
      "loss": 0.7848,
      "step": 37860
    },
    {
      "epoch": 2.399885927944485,
      "grad_norm": 3.1016135215759277,
      "learning_rate": 6.022861981371719e-06,
      "loss": 0.8719,
      "step": 37870
    },
    {
      "epoch": 2.400519661586235,
      "grad_norm": 2.709334135055542,
      "learning_rate": 6.0165114309906864e-06,
      "loss": 0.8499,
      "step": 37880
    },
    {
      "epoch": 2.4011533952279858,
      "grad_norm": 2.696004867553711,
      "learning_rate": 6.010160880609653e-06,
      "loss": 0.8182,
      "step": 37890
    },
    {
      "epoch": 2.401787128869736,
      "grad_norm": 2.6817595958709717,
      "learning_rate": 6.003810330228619e-06,
      "loss": 0.8662,
      "step": 37900
    },
    {
      "epoch": 2.4024208625114865,
      "grad_norm": 3.240766763687134,
      "learning_rate": 5.997459779847587e-06,
      "loss": 0.889,
      "step": 37910
    },
    {
      "epoch": 2.403054596153237,
      "grad_norm": 2.5559544563293457,
      "learning_rate": 5.991109229466554e-06,
      "loss": 0.8929,
      "step": 37920
    },
    {
      "epoch": 2.403688329794987,
      "grad_norm": 2.637526273727417,
      "learning_rate": 5.984758679085521e-06,
      "loss": 0.8215,
      "step": 37930
    },
    {
      "epoch": 2.4043220634367377,
      "grad_norm": 2.743807554244995,
      "learning_rate": 5.9784081287044875e-06,
      "loss": 0.8133,
      "step": 37940
    },
    {
      "epoch": 2.404955797078488,
      "grad_norm": 2.3128738403320312,
      "learning_rate": 5.972057578323455e-06,
      "loss": 0.8208,
      "step": 37950
    },
    {
      "epoch": 2.4055895307202384,
      "grad_norm": 3.1583502292633057,
      "learning_rate": 5.965707027942422e-06,
      "loss": 0.8986,
      "step": 37960
    },
    {
      "epoch": 2.4062232643619885,
      "grad_norm": 2.6546802520751953,
      "learning_rate": 5.9593564775613885e-06,
      "loss": 0.8528,
      "step": 37970
    },
    {
      "epoch": 2.406856998003739,
      "grad_norm": 3.1881227493286133,
      "learning_rate": 5.953005927180356e-06,
      "loss": 0.8679,
      "step": 37980
    },
    {
      "epoch": 2.407490731645489,
      "grad_norm": 2.927773952484131,
      "learning_rate": 5.946655376799323e-06,
      "loss": 0.8579,
      "step": 37990
    },
    {
      "epoch": 2.40812446528724,
      "grad_norm": 2.8754987716674805,
      "learning_rate": 5.94030482641829e-06,
      "loss": 0.8699,
      "step": 38000
    },
    {
      "epoch": 2.4087581989289903,
      "grad_norm": 2.534306764602661,
      "learning_rate": 5.933954276037257e-06,
      "loss": 0.8647,
      "step": 38010
    },
    {
      "epoch": 2.4093919325707405,
      "grad_norm": 2.949822187423706,
      "learning_rate": 5.927603725656223e-06,
      "loss": 0.8618,
      "step": 38020
    },
    {
      "epoch": 2.410025666212491,
      "grad_norm": 2.655266284942627,
      "learning_rate": 5.9212531752751905e-06,
      "loss": 0.8761,
      "step": 38030
    },
    {
      "epoch": 2.410659399854241,
      "grad_norm": 2.834102153778076,
      "learning_rate": 5.914902624894158e-06,
      "loss": 0.8193,
      "step": 38040
    },
    {
      "epoch": 2.4112931334959917,
      "grad_norm": 2.925302267074585,
      "learning_rate": 5.908552074513125e-06,
      "loss": 0.8514,
      "step": 38050
    },
    {
      "epoch": 2.411926867137742,
      "grad_norm": 2.613494634628296,
      "learning_rate": 5.9022015241320914e-06,
      "loss": 0.888,
      "step": 38060
    },
    {
      "epoch": 2.4125606007794924,
      "grad_norm": 2.7685883045196533,
      "learning_rate": 5.895850973751058e-06,
      "loss": 0.8212,
      "step": 38070
    },
    {
      "epoch": 2.4131943344212425,
      "grad_norm": 2.8213794231414795,
      "learning_rate": 5.889500423370026e-06,
      "loss": 0.8331,
      "step": 38080
    },
    {
      "epoch": 2.413828068062993,
      "grad_norm": 2.457139253616333,
      "learning_rate": 5.883149872988992e-06,
      "loss": 0.8567,
      "step": 38090
    },
    {
      "epoch": 2.4144618017047437,
      "grad_norm": 2.38500714302063,
      "learning_rate": 5.87679932260796e-06,
      "loss": 0.8093,
      "step": 38100
    },
    {
      "epoch": 2.415095535346494,
      "grad_norm": 2.4186294078826904,
      "learning_rate": 5.870448772226926e-06,
      "loss": 0.8333,
      "step": 38110
    },
    {
      "epoch": 2.4157292689882444,
      "grad_norm": 3.149343967437744,
      "learning_rate": 5.864098221845894e-06,
      "loss": 0.8416,
      "step": 38120
    },
    {
      "epoch": 2.4163630026299945,
      "grad_norm": 2.847587823867798,
      "learning_rate": 5.857747671464861e-06,
      "loss": 0.8814,
      "step": 38130
    },
    {
      "epoch": 2.416996736271745,
      "grad_norm": 3.1820127964019775,
      "learning_rate": 5.851397121083827e-06,
      "loss": 0.8462,
      "step": 38140
    },
    {
      "epoch": 2.417630469913495,
      "grad_norm": 3.7016170024871826,
      "learning_rate": 5.845046570702794e-06,
      "loss": 0.8883,
      "step": 38150
    },
    {
      "epoch": 2.4182642035552457,
      "grad_norm": 2.4118542671203613,
      "learning_rate": 5.838696020321761e-06,
      "loss": 0.8449,
      "step": 38160
    },
    {
      "epoch": 2.418897937196996,
      "grad_norm": 2.8371284008026123,
      "learning_rate": 5.832345469940729e-06,
      "loss": 0.8322,
      "step": 38170
    },
    {
      "epoch": 2.4195316708387464,
      "grad_norm": 2.8368260860443115,
      "learning_rate": 5.825994919559695e-06,
      "loss": 0.8726,
      "step": 38180
    },
    {
      "epoch": 2.420165404480497,
      "grad_norm": 2.600804567337036,
      "learning_rate": 5.819644369178662e-06,
      "loss": 0.8634,
      "step": 38190
    },
    {
      "epoch": 2.420799138122247,
      "grad_norm": 2.692333221435547,
      "learning_rate": 5.813293818797629e-06,
      "loss": 0.8187,
      "step": 38200
    },
    {
      "epoch": 2.4214328717639977,
      "grad_norm": 3.070568323135376,
      "learning_rate": 5.806943268416596e-06,
      "loss": 0.8222,
      "step": 38210
    },
    {
      "epoch": 2.422066605405748,
      "grad_norm": 2.5047202110290527,
      "learning_rate": 5.8005927180355636e-06,
      "loss": 0.8545,
      "step": 38220
    },
    {
      "epoch": 2.4227003390474984,
      "grad_norm": 2.940779685974121,
      "learning_rate": 5.79424216765453e-06,
      "loss": 0.8388,
      "step": 38230
    },
    {
      "epoch": 2.423334072689249,
      "grad_norm": 2.81213641166687,
      "learning_rate": 5.787891617273497e-06,
      "loss": 0.7926,
      "step": 38240
    },
    {
      "epoch": 2.423967806330999,
      "grad_norm": 3.2443017959594727,
      "learning_rate": 5.7815410668924645e-06,
      "loss": 0.8147,
      "step": 38250
    },
    {
      "epoch": 2.4246015399727496,
      "grad_norm": 2.690934896469116,
      "learning_rate": 5.775190516511431e-06,
      "loss": 0.8325,
      "step": 38260
    },
    {
      "epoch": 2.4252352736144998,
      "grad_norm": 3.5499727725982666,
      "learning_rate": 5.768839966130398e-06,
      "loss": 0.9062,
      "step": 38270
    },
    {
      "epoch": 2.4258690072562503,
      "grad_norm": 2.63761305809021,
      "learning_rate": 5.762489415749365e-06,
      "loss": 0.8447,
      "step": 38280
    },
    {
      "epoch": 2.4265027408980004,
      "grad_norm": 2.715660810470581,
      "learning_rate": 5.756138865368332e-06,
      "loss": 0.8464,
      "step": 38290
    },
    {
      "epoch": 2.427136474539751,
      "grad_norm": 2.690744638442993,
      "learning_rate": 5.749788314987299e-06,
      "loss": 0.8603,
      "step": 38300
    },
    {
      "epoch": 2.427770208181501,
      "grad_norm": 2.831278085708618,
      "learning_rate": 5.743437764606266e-06,
      "loss": 0.8429,
      "step": 38310
    },
    {
      "epoch": 2.4284039418232517,
      "grad_norm": 3.17922306060791,
      "learning_rate": 5.737087214225233e-06,
      "loss": 0.8763,
      "step": 38320
    },
    {
      "epoch": 2.4290376754650023,
      "grad_norm": 2.490821361541748,
      "learning_rate": 5.730736663844199e-06,
      "loss": 0.8637,
      "step": 38330
    },
    {
      "epoch": 2.4296714091067524,
      "grad_norm": 2.7682673931121826,
      "learning_rate": 5.7243861134631675e-06,
      "loss": 0.845,
      "step": 38340
    },
    {
      "epoch": 2.430305142748503,
      "grad_norm": 2.962007999420166,
      "learning_rate": 5.718035563082134e-06,
      "loss": 0.8875,
      "step": 38350
    },
    {
      "epoch": 2.430938876390253,
      "grad_norm": 2.7591679096221924,
      "learning_rate": 5.711685012701101e-06,
      "loss": 0.8625,
      "step": 38360
    },
    {
      "epoch": 2.4315726100320036,
      "grad_norm": 2.8247482776641846,
      "learning_rate": 5.705334462320068e-06,
      "loss": 0.8305,
      "step": 38370
    },
    {
      "epoch": 2.4322063436737538,
      "grad_norm": 2.573369264602661,
      "learning_rate": 5.698983911939035e-06,
      "loss": 0.8749,
      "step": 38380
    },
    {
      "epoch": 2.4328400773155043,
      "grad_norm": 2.905754804611206,
      "learning_rate": 5.692633361558002e-06,
      "loss": 0.9497,
      "step": 38390
    },
    {
      "epoch": 2.4334738109572545,
      "grad_norm": 2.647095203399658,
      "learning_rate": 5.6862828111769686e-06,
      "loss": 0.842,
      "step": 38400
    },
    {
      "epoch": 2.434107544599005,
      "grad_norm": 3.00248122215271,
      "learning_rate": 5.679932260795936e-06,
      "loss": 0.8344,
      "step": 38410
    },
    {
      "epoch": 2.4347412782407556,
      "grad_norm": 2.783287763595581,
      "learning_rate": 5.673581710414902e-06,
      "loss": 0.8522,
      "step": 38420
    },
    {
      "epoch": 2.4353750118825057,
      "grad_norm": 2.469388961791992,
      "learning_rate": 5.66723116003387e-06,
      "loss": 0.8325,
      "step": 38430
    },
    {
      "epoch": 2.4360087455242563,
      "grad_norm": 2.8016433715820312,
      "learning_rate": 5.660880609652837e-06,
      "loss": 0.8392,
      "step": 38440
    },
    {
      "epoch": 2.4366424791660064,
      "grad_norm": 2.655672788619995,
      "learning_rate": 5.654530059271803e-06,
      "loss": 0.8797,
      "step": 38450
    },
    {
      "epoch": 2.437276212807757,
      "grad_norm": 2.82928466796875,
      "learning_rate": 5.6481795088907705e-06,
      "loss": 0.8438,
      "step": 38460
    },
    {
      "epoch": 2.437909946449507,
      "grad_norm": 3.605372428894043,
      "learning_rate": 5.641828958509738e-06,
      "loss": 0.8653,
      "step": 38470
    },
    {
      "epoch": 2.4385436800912577,
      "grad_norm": 2.84846568107605,
      "learning_rate": 5.635478408128705e-06,
      "loss": 0.8789,
      "step": 38480
    },
    {
      "epoch": 2.439177413733008,
      "grad_norm": 3.137552261352539,
      "learning_rate": 5.6291278577476715e-06,
      "loss": 0.8215,
      "step": 38490
    },
    {
      "epoch": 2.4398111473747583,
      "grad_norm": 2.6132867336273193,
      "learning_rate": 5.622777307366638e-06,
      "loss": 0.8325,
      "step": 38500
    },
    {
      "epoch": 2.440444881016509,
      "grad_norm": 2.9522998332977295,
      "learning_rate": 5.616426756985606e-06,
      "loss": 0.8379,
      "step": 38510
    },
    {
      "epoch": 2.441078614658259,
      "grad_norm": 3.1242551803588867,
      "learning_rate": 5.6100762066045725e-06,
      "loss": 0.8625,
      "step": 38520
    },
    {
      "epoch": 2.4417123483000096,
      "grad_norm": 3.22979998588562,
      "learning_rate": 5.60372565622354e-06,
      "loss": 0.8804,
      "step": 38530
    },
    {
      "epoch": 2.4423460819417597,
      "grad_norm": 2.792060375213623,
      "learning_rate": 5.597375105842506e-06,
      "loss": 0.7932,
      "step": 38540
    },
    {
      "epoch": 2.4429798155835103,
      "grad_norm": 2.4443199634552,
      "learning_rate": 5.5910245554614734e-06,
      "loss": 0.8499,
      "step": 38550
    },
    {
      "epoch": 2.443613549225261,
      "grad_norm": 3.2409770488739014,
      "learning_rate": 5.584674005080441e-06,
      "loss": 0.8596,
      "step": 38560
    },
    {
      "epoch": 2.444247282867011,
      "grad_norm": 3.104027509689331,
      "learning_rate": 5.578323454699407e-06,
      "loss": 0.8949,
      "step": 38570
    },
    {
      "epoch": 2.4448810165087616,
      "grad_norm": 2.700423002243042,
      "learning_rate": 5.571972904318374e-06,
      "loss": 0.8133,
      "step": 38580
    },
    {
      "epoch": 2.4455147501505117,
      "grad_norm": 2.623112678527832,
      "learning_rate": 5.565622353937341e-06,
      "loss": 0.8217,
      "step": 38590
    },
    {
      "epoch": 2.4461484837922622,
      "grad_norm": 2.7149081230163574,
      "learning_rate": 5.559271803556309e-06,
      "loss": 0.8371,
      "step": 38600
    },
    {
      "epoch": 2.4467822174340124,
      "grad_norm": 2.870577335357666,
      "learning_rate": 5.552921253175275e-06,
      "loss": 0.8665,
      "step": 38610
    },
    {
      "epoch": 2.447415951075763,
      "grad_norm": 2.8244426250457764,
      "learning_rate": 5.546570702794242e-06,
      "loss": 0.8677,
      "step": 38620
    },
    {
      "epoch": 2.448049684717513,
      "grad_norm": 2.9409098625183105,
      "learning_rate": 5.540220152413209e-06,
      "loss": 0.8308,
      "step": 38630
    },
    {
      "epoch": 2.4486834183592636,
      "grad_norm": 2.3130621910095215,
      "learning_rate": 5.533869602032176e-06,
      "loss": 0.8428,
      "step": 38640
    },
    {
      "epoch": 2.449317152001014,
      "grad_norm": 2.5488622188568115,
      "learning_rate": 5.527519051651144e-06,
      "loss": 0.8435,
      "step": 38650
    },
    {
      "epoch": 2.4499508856427643,
      "grad_norm": 2.747440814971924,
      "learning_rate": 5.52116850127011e-06,
      "loss": 0.8464,
      "step": 38660
    },
    {
      "epoch": 2.450584619284515,
      "grad_norm": 2.544642448425293,
      "learning_rate": 5.514817950889077e-06,
      "loss": 0.8673,
      "step": 38670
    },
    {
      "epoch": 2.451218352926265,
      "grad_norm": 2.6591691970825195,
      "learning_rate": 5.508467400508044e-06,
      "loss": 0.834,
      "step": 38680
    },
    {
      "epoch": 2.4518520865680156,
      "grad_norm": 2.867614269256592,
      "learning_rate": 5.502116850127011e-06,
      "loss": 0.8652,
      "step": 38690
    },
    {
      "epoch": 2.4524858202097657,
      "grad_norm": 2.4319913387298584,
      "learning_rate": 5.495766299745978e-06,
      "loss": 0.8569,
      "step": 38700
    },
    {
      "epoch": 2.4531195538515163,
      "grad_norm": 2.7297143936157227,
      "learning_rate": 5.489415749364945e-06,
      "loss": 0.8456,
      "step": 38710
    },
    {
      "epoch": 2.4537532874932664,
      "grad_norm": 3.3243696689605713,
      "learning_rate": 5.483065198983912e-06,
      "loss": 0.8406,
      "step": 38720
    },
    {
      "epoch": 2.454387021135017,
      "grad_norm": 2.962092638015747,
      "learning_rate": 5.476714648602879e-06,
      "loss": 0.8212,
      "step": 38730
    },
    {
      "epoch": 2.4550207547767675,
      "grad_norm": 2.887617826461792,
      "learning_rate": 5.470364098221846e-06,
      "loss": 0.8497,
      "step": 38740
    },
    {
      "epoch": 2.4556544884185176,
      "grad_norm": 2.8128201961517334,
      "learning_rate": 5.464013547840813e-06,
      "loss": 0.8092,
      "step": 38750
    },
    {
      "epoch": 2.456288222060268,
      "grad_norm": 2.6539316177368164,
      "learning_rate": 5.4576629974597794e-06,
      "loss": 0.8489,
      "step": 38760
    },
    {
      "epoch": 2.4569219557020183,
      "grad_norm": 2.4726383686065674,
      "learning_rate": 5.4513124470787475e-06,
      "loss": 0.8797,
      "step": 38770
    },
    {
      "epoch": 2.457555689343769,
      "grad_norm": 2.7127203941345215,
      "learning_rate": 5.444961896697714e-06,
      "loss": 0.8395,
      "step": 38780
    },
    {
      "epoch": 2.458189422985519,
      "grad_norm": 2.9029510021209717,
      "learning_rate": 5.438611346316681e-06,
      "loss": 0.8236,
      "step": 38790
    },
    {
      "epoch": 2.4588231566272696,
      "grad_norm": 2.594113826751709,
      "learning_rate": 5.432260795935648e-06,
      "loss": 0.8255,
      "step": 38800
    },
    {
      "epoch": 2.4594568902690197,
      "grad_norm": 3.0718634128570557,
      "learning_rate": 5.425910245554614e-06,
      "loss": 0.837,
      "step": 38810
    },
    {
      "epoch": 2.4600906239107703,
      "grad_norm": 2.270411968231201,
      "learning_rate": 5.419559695173582e-06,
      "loss": 0.8439,
      "step": 38820
    },
    {
      "epoch": 2.460724357552521,
      "grad_norm": 3.096133232116699,
      "learning_rate": 5.413209144792549e-06,
      "loss": 0.8449,
      "step": 38830
    },
    {
      "epoch": 2.461358091194271,
      "grad_norm": 3.079469680786133,
      "learning_rate": 5.406858594411516e-06,
      "loss": 0.8651,
      "step": 38840
    },
    {
      "epoch": 2.4619918248360215,
      "grad_norm": 3.0784356594085693,
      "learning_rate": 5.400508044030482e-06,
      "loss": 0.858,
      "step": 38850
    },
    {
      "epoch": 2.4626255584777716,
      "grad_norm": 2.674445152282715,
      "learning_rate": 5.39415749364945e-06,
      "loss": 0.8302,
      "step": 38860
    },
    {
      "epoch": 2.463259292119522,
      "grad_norm": 3.1710591316223145,
      "learning_rate": 5.387806943268417e-06,
      "loss": 0.8469,
      "step": 38870
    },
    {
      "epoch": 2.463893025761273,
      "grad_norm": 3.322340250015259,
      "learning_rate": 5.381456392887383e-06,
      "loss": 0.844,
      "step": 38880
    },
    {
      "epoch": 2.464526759403023,
      "grad_norm": 3.4591991901397705,
      "learning_rate": 5.375105842506351e-06,
      "loss": 0.7832,
      "step": 38890
    },
    {
      "epoch": 2.4651604930447735,
      "grad_norm": 2.564816474914551,
      "learning_rate": 5.368755292125318e-06,
      "loss": 0.8865,
      "step": 38900
    },
    {
      "epoch": 2.4657942266865236,
      "grad_norm": 2.8324708938598633,
      "learning_rate": 5.362404741744285e-06,
      "loss": 0.8567,
      "step": 38910
    },
    {
      "epoch": 2.466427960328274,
      "grad_norm": 2.711094856262207,
      "learning_rate": 5.3560541913632516e-06,
      "loss": 0.8448,
      "step": 38920
    },
    {
      "epoch": 2.4670616939700243,
      "grad_norm": 2.898175001144409,
      "learning_rate": 5.349703640982218e-06,
      "loss": 0.9012,
      "step": 38930
    },
    {
      "epoch": 2.467695427611775,
      "grad_norm": 2.7000362873077393,
      "learning_rate": 5.343353090601186e-06,
      "loss": 0.8558,
      "step": 38940
    },
    {
      "epoch": 2.468329161253525,
      "grad_norm": 2.7497711181640625,
      "learning_rate": 5.3370025402201525e-06,
      "loss": 0.8856,
      "step": 38950
    },
    {
      "epoch": 2.4689628948952755,
      "grad_norm": 3.245018243789673,
      "learning_rate": 5.33065198983912e-06,
      "loss": 0.8623,
      "step": 38960
    },
    {
      "epoch": 2.469596628537026,
      "grad_norm": 2.954113483428955,
      "learning_rate": 5.324301439458086e-06,
      "loss": 0.8682,
      "step": 38970
    },
    {
      "epoch": 2.4702303621787762,
      "grad_norm": 3.1898908615112305,
      "learning_rate": 5.3179508890770535e-06,
      "loss": 0.8043,
      "step": 38980
    },
    {
      "epoch": 2.470864095820527,
      "grad_norm": 2.794020891189575,
      "learning_rate": 5.311600338696021e-06,
      "loss": 0.8668,
      "step": 38990
    },
    {
      "epoch": 2.471497829462277,
      "grad_norm": 2.304337501525879,
      "learning_rate": 5.305249788314987e-06,
      "loss": 0.8305,
      "step": 39000
    },
    {
      "epoch": 2.4721315631040275,
      "grad_norm": 3.1650238037109375,
      "learning_rate": 5.2988992379339545e-06,
      "loss": 0.8565,
      "step": 39010
    },
    {
      "epoch": 2.4727652967457776,
      "grad_norm": 2.8849143981933594,
      "learning_rate": 5.292548687552921e-06,
      "loss": 0.794,
      "step": 39020
    },
    {
      "epoch": 2.473399030387528,
      "grad_norm": 2.7646162509918213,
      "learning_rate": 5.286198137171889e-06,
      "loss": 0.8297,
      "step": 39030
    },
    {
      "epoch": 2.4740327640292783,
      "grad_norm": 2.5725507736206055,
      "learning_rate": 5.2798475867908555e-06,
      "loss": 0.8682,
      "step": 39040
    },
    {
      "epoch": 2.474666497671029,
      "grad_norm": 2.807884931564331,
      "learning_rate": 5.273497036409822e-06,
      "loss": 0.8189,
      "step": 39050
    },
    {
      "epoch": 2.4753002313127794,
      "grad_norm": 3.150136947631836,
      "learning_rate": 5.267146486028789e-06,
      "loss": 0.8488,
      "step": 39060
    },
    {
      "epoch": 2.4759339649545296,
      "grad_norm": 2.522869110107422,
      "learning_rate": 5.2607959356477564e-06,
      "loss": 0.8333,
      "step": 39070
    },
    {
      "epoch": 2.47656769859628,
      "grad_norm": 2.8466384410858154,
      "learning_rate": 5.254445385266724e-06,
      "loss": 0.8314,
      "step": 39080
    },
    {
      "epoch": 2.4772014322380302,
      "grad_norm": 2.892557382583618,
      "learning_rate": 5.24809483488569e-06,
      "loss": 0.8339,
      "step": 39090
    },
    {
      "epoch": 2.477835165879781,
      "grad_norm": 2.6295816898345947,
      "learning_rate": 5.241744284504657e-06,
      "loss": 0.7864,
      "step": 39100
    },
    {
      "epoch": 2.478468899521531,
      "grad_norm": 2.733743906021118,
      "learning_rate": 5.235393734123624e-06,
      "loss": 0.7955,
      "step": 39110
    },
    {
      "epoch": 2.4791026331632815,
      "grad_norm": 2.822272300720215,
      "learning_rate": 5.229043183742591e-06,
      "loss": 0.8431,
      "step": 39120
    },
    {
      "epoch": 2.4797363668050316,
      "grad_norm": 3.1117970943450928,
      "learning_rate": 5.222692633361558e-06,
      "loss": 0.8229,
      "step": 39130
    },
    {
      "epoch": 2.480370100446782,
      "grad_norm": 2.72512149810791,
      "learning_rate": 5.216342082980525e-06,
      "loss": 0.8485,
      "step": 39140
    },
    {
      "epoch": 2.4810038340885328,
      "grad_norm": 2.6584975719451904,
      "learning_rate": 5.209991532599492e-06,
      "loss": 0.8386,
      "step": 39150
    },
    {
      "epoch": 2.481637567730283,
      "grad_norm": 2.7272117137908936,
      "learning_rate": 5.203640982218459e-06,
      "loss": 0.8027,
      "step": 39160
    },
    {
      "epoch": 2.4822713013720334,
      "grad_norm": 3.0449061393737793,
      "learning_rate": 5.197290431837426e-06,
      "loss": 0.8613,
      "step": 39170
    },
    {
      "epoch": 2.4829050350137836,
      "grad_norm": 3.3159027099609375,
      "learning_rate": 5.190939881456393e-06,
      "loss": 0.8486,
      "step": 39180
    },
    {
      "epoch": 2.483538768655534,
      "grad_norm": 2.0924696922302246,
      "learning_rate": 5.1845893310753595e-06,
      "loss": 0.8346,
      "step": 39190
    },
    {
      "epoch": 2.4841725022972843,
      "grad_norm": 2.8503663539886475,
      "learning_rate": 5.178238780694328e-06,
      "loss": 0.8288,
      "step": 39200
    },
    {
      "epoch": 2.484806235939035,
      "grad_norm": 3.2673206329345703,
      "learning_rate": 5.171888230313294e-06,
      "loss": 0.8892,
      "step": 39210
    },
    {
      "epoch": 2.485439969580785,
      "grad_norm": 2.4739794731140137,
      "learning_rate": 5.165537679932261e-06,
      "loss": 0.8502,
      "step": 39220
    },
    {
      "epoch": 2.4860737032225355,
      "grad_norm": 2.6512067317962646,
      "learning_rate": 5.159187129551228e-06,
      "loss": 0.8303,
      "step": 39230
    },
    {
      "epoch": 2.486707436864286,
      "grad_norm": 2.464306116104126,
      "learning_rate": 5.152836579170194e-06,
      "loss": 0.8967,
      "step": 39240
    },
    {
      "epoch": 2.487341170506036,
      "grad_norm": 2.8238525390625,
      "learning_rate": 5.146486028789162e-06,
      "loss": 0.8505,
      "step": 39250
    },
    {
      "epoch": 2.4879749041477868,
      "grad_norm": 3.0184154510498047,
      "learning_rate": 5.140135478408129e-06,
      "loss": 0.8365,
      "step": 39260
    },
    {
      "epoch": 2.488608637789537,
      "grad_norm": 3.1293210983276367,
      "learning_rate": 5.133784928027096e-06,
      "loss": 0.8536,
      "step": 39270
    },
    {
      "epoch": 2.4892423714312875,
      "grad_norm": 2.4961578845977783,
      "learning_rate": 5.127434377646062e-06,
      "loss": 0.855,
      "step": 39280
    },
    {
      "epoch": 2.489876105073038,
      "grad_norm": 3.3020756244659424,
      "learning_rate": 5.12108382726503e-06,
      "loss": 0.9049,
      "step": 39290
    },
    {
      "epoch": 2.490509838714788,
      "grad_norm": 2.3895046710968018,
      "learning_rate": 5.114733276883997e-06,
      "loss": 0.8125,
      "step": 39300
    },
    {
      "epoch": 2.4911435723565387,
      "grad_norm": 3.122274398803711,
      "learning_rate": 5.108382726502963e-06,
      "loss": 0.8802,
      "step": 39310
    },
    {
      "epoch": 2.491777305998289,
      "grad_norm": 2.9055209159851074,
      "learning_rate": 5.102032176121931e-06,
      "loss": 0.8693,
      "step": 39320
    },
    {
      "epoch": 2.4924110396400394,
      "grad_norm": 2.9495761394500732,
      "learning_rate": 5.095681625740898e-06,
      "loss": 0.8162,
      "step": 39330
    },
    {
      "epoch": 2.4930447732817895,
      "grad_norm": 2.892637014389038,
      "learning_rate": 5.089331075359865e-06,
      "loss": 0.8664,
      "step": 39340
    },
    {
      "epoch": 2.49367850692354,
      "grad_norm": 3.0498158931732178,
      "learning_rate": 5.082980524978832e-06,
      "loss": 0.8068,
      "step": 39350
    },
    {
      "epoch": 2.49431224056529,
      "grad_norm": 2.525094509124756,
      "learning_rate": 5.076629974597798e-06,
      "loss": 0.8299,
      "step": 39360
    },
    {
      "epoch": 2.494945974207041,
      "grad_norm": 2.371584177017212,
      "learning_rate": 5.070279424216765e-06,
      "loss": 0.8261,
      "step": 39370
    },
    {
      "epoch": 2.4955797078487914,
      "grad_norm": 3.0860400199890137,
      "learning_rate": 5.063928873835733e-06,
      "loss": 0.8248,
      "step": 39380
    },
    {
      "epoch": 2.4962134414905415,
      "grad_norm": 3.367868661880493,
      "learning_rate": 5.0575783234547e-06,
      "loss": 0.8711,
      "step": 39390
    },
    {
      "epoch": 2.496847175132292,
      "grad_norm": 2.998342752456665,
      "learning_rate": 5.051227773073666e-06,
      "loss": 0.8824,
      "step": 39400
    },
    {
      "epoch": 2.497480908774042,
      "grad_norm": 2.505561351776123,
      "learning_rate": 5.044877222692634e-06,
      "loss": 0.8445,
      "step": 39410
    },
    {
      "epoch": 2.4981146424157927,
      "grad_norm": 2.8540425300598145,
      "learning_rate": 5.038526672311601e-06,
      "loss": 0.8511,
      "step": 39420
    },
    {
      "epoch": 2.498748376057543,
      "grad_norm": 2.8736374378204346,
      "learning_rate": 5.032176121930567e-06,
      "loss": 0.8512,
      "step": 39430
    },
    {
      "epoch": 2.4993821096992934,
      "grad_norm": 2.7856955528259277,
      "learning_rate": 5.0258255715495346e-06,
      "loss": 0.8506,
      "step": 39440
    },
    {
      "epoch": 2.5000158433410435,
      "grad_norm": 2.883904218673706,
      "learning_rate": 5.019475021168501e-06,
      "loss": 0.8322,
      "step": 39450
    },
    {
      "epoch": 2.500649576982794,
      "grad_norm": 2.8943986892700195,
      "learning_rate": 5.013124470787469e-06,
      "loss": 0.8628,
      "step": 39460
    },
    {
      "epoch": 2.5012833106245447,
      "grad_norm": 2.9517621994018555,
      "learning_rate": 5.0067739204064355e-06,
      "loss": 0.8435,
      "step": 39470
    },
    {
      "epoch": 2.501917044266295,
      "grad_norm": 2.923880100250244,
      "learning_rate": 5.000423370025402e-06,
      "loss": 0.8168,
      "step": 39480
    },
    {
      "epoch": 2.5025507779080454,
      "grad_norm": 2.784749984741211,
      "learning_rate": 4.994072819644369e-06,
      "loss": 0.8104,
      "step": 39490
    },
    {
      "epoch": 2.5031845115497955,
      "grad_norm": 2.8206098079681396,
      "learning_rate": 4.987722269263336e-06,
      "loss": 0.8631,
      "step": 39500
    },
    {
      "epoch": 2.503818245191546,
      "grad_norm": 2.9590272903442383,
      "learning_rate": 4.981371718882304e-06,
      "loss": 0.8793,
      "step": 39510
    },
    {
      "epoch": 2.5044519788332966,
      "grad_norm": 3.2754034996032715,
      "learning_rate": 4.97502116850127e-06,
      "loss": 0.8522,
      "step": 39520
    },
    {
      "epoch": 2.5050857124750467,
      "grad_norm": 2.4719157218933105,
      "learning_rate": 4.9686706181202375e-06,
      "loss": 0.8475,
      "step": 39530
    },
    {
      "epoch": 2.505719446116797,
      "grad_norm": 3.1434905529022217,
      "learning_rate": 4.962320067739204e-06,
      "loss": 0.8239,
      "step": 39540
    },
    {
      "epoch": 2.5063531797585474,
      "grad_norm": 3.4856014251708984,
      "learning_rate": 4.955969517358171e-06,
      "loss": 0.8533,
      "step": 39550
    },
    {
      "epoch": 2.506986913400298,
      "grad_norm": 2.5884997844696045,
      "learning_rate": 4.9496189669771385e-06,
      "loss": 0.8689,
      "step": 39560
    },
    {
      "epoch": 2.507620647042048,
      "grad_norm": 3.1066434383392334,
      "learning_rate": 4.943268416596105e-06,
      "loss": 0.8773,
      "step": 39570
    },
    {
      "epoch": 2.5082543806837987,
      "grad_norm": 2.9257681369781494,
      "learning_rate": 4.936917866215072e-06,
      "loss": 0.8583,
      "step": 39580
    },
    {
      "epoch": 2.508888114325549,
      "grad_norm": 3.0205841064453125,
      "learning_rate": 4.9305673158340394e-06,
      "loss": 0.8467,
      "step": 39590
    },
    {
      "epoch": 2.5095218479672994,
      "grad_norm": 2.944838523864746,
      "learning_rate": 4.924216765453006e-06,
      "loss": 0.8768,
      "step": 39600
    },
    {
      "epoch": 2.51015558160905,
      "grad_norm": 2.621676206588745,
      "learning_rate": 4.917866215071973e-06,
      "loss": 0.8244,
      "step": 39610
    },
    {
      "epoch": 2.5107893152508,
      "grad_norm": 2.487067461013794,
      "learning_rate": 4.9115156646909396e-06,
      "loss": 0.822,
      "step": 39620
    },
    {
      "epoch": 2.51142304889255,
      "grad_norm": 4.085541248321533,
      "learning_rate": 4.905165114309907e-06,
      "loss": 0.853,
      "step": 39630
    },
    {
      "epoch": 2.5120567825343008,
      "grad_norm": 2.603052854537964,
      "learning_rate": 4.898814563928874e-06,
      "loss": 0.834,
      "step": 39640
    },
    {
      "epoch": 2.5126905161760513,
      "grad_norm": 2.7340002059936523,
      "learning_rate": 4.892464013547841e-06,
      "loss": 0.8733,
      "step": 39650
    },
    {
      "epoch": 2.5133242498178014,
      "grad_norm": 2.5255115032196045,
      "learning_rate": 4.886113463166808e-06,
      "loss": 0.8356,
      "step": 39660
    },
    {
      "epoch": 2.513957983459552,
      "grad_norm": 2.803746223449707,
      "learning_rate": 4.879762912785774e-06,
      "loss": 0.8573,
      "step": 39670
    },
    {
      "epoch": 2.514591717101302,
      "grad_norm": 3.222841501235962,
      "learning_rate": 4.873412362404742e-06,
      "loss": 0.8228,
      "step": 39680
    },
    {
      "epoch": 2.5152254507430527,
      "grad_norm": 2.7860989570617676,
      "learning_rate": 4.867061812023709e-06,
      "loss": 0.8268,
      "step": 39690
    },
    {
      "epoch": 2.5158591843848033,
      "grad_norm": 3.0868377685546875,
      "learning_rate": 4.860711261642676e-06,
      "loss": 0.8712,
      "step": 39700
    },
    {
      "epoch": 2.5164929180265534,
      "grad_norm": 2.86033034324646,
      "learning_rate": 4.8543607112616425e-06,
      "loss": 0.864,
      "step": 39710
    },
    {
      "epoch": 2.517126651668304,
      "grad_norm": 3.0268356800079346,
      "learning_rate": 4.84801016088061e-06,
      "loss": 0.8205,
      "step": 39720
    },
    {
      "epoch": 2.517760385310054,
      "grad_norm": 2.8999972343444824,
      "learning_rate": 4.841659610499577e-06,
      "loss": 0.8439,
      "step": 39730
    },
    {
      "epoch": 2.5183941189518046,
      "grad_norm": 2.570376396179199,
      "learning_rate": 4.8353090601185435e-06,
      "loss": 0.8142,
      "step": 39740
    },
    {
      "epoch": 2.5190278525935548,
      "grad_norm": 2.447441816329956,
      "learning_rate": 4.828958509737511e-06,
      "loss": 0.8453,
      "step": 39750
    },
    {
      "epoch": 2.5196615862353053,
      "grad_norm": 2.4213056564331055,
      "learning_rate": 4.822607959356477e-06,
      "loss": 0.806,
      "step": 39760
    },
    {
      "epoch": 2.5202953198770555,
      "grad_norm": 2.6826727390289307,
      "learning_rate": 4.816257408975445e-06,
      "loss": 0.8629,
      "step": 39770
    },
    {
      "epoch": 2.520929053518806,
      "grad_norm": 2.806424617767334,
      "learning_rate": 4.809906858594412e-06,
      "loss": 0.816,
      "step": 39780
    },
    {
      "epoch": 2.5215627871605566,
      "grad_norm": 2.169771432876587,
      "learning_rate": 4.803556308213378e-06,
      "loss": 0.8262,
      "step": 39790
    },
    {
      "epoch": 2.5221965208023067,
      "grad_norm": 2.5808300971984863,
      "learning_rate": 4.797205757832345e-06,
      "loss": 0.8158,
      "step": 39800
    },
    {
      "epoch": 2.5228302544440573,
      "grad_norm": 2.711733818054199,
      "learning_rate": 4.790855207451313e-06,
      "loss": 0.8708,
      "step": 39810
    },
    {
      "epoch": 2.5234639880858074,
      "grad_norm": 2.9863181114196777,
      "learning_rate": 4.78450465707028e-06,
      "loss": 0.8396,
      "step": 39820
    },
    {
      "epoch": 2.524097721727558,
      "grad_norm": 2.833620548248291,
      "learning_rate": 4.778154106689246e-06,
      "loss": 0.8613,
      "step": 39830
    },
    {
      "epoch": 2.5247314553693085,
      "grad_norm": 2.866244316101074,
      "learning_rate": 4.771803556308214e-06,
      "loss": 0.8532,
      "step": 39840
    },
    {
      "epoch": 2.5253651890110587,
      "grad_norm": 2.9970831871032715,
      "learning_rate": 4.765453005927181e-06,
      "loss": 0.8496,
      "step": 39850
    },
    {
      "epoch": 2.525998922652809,
      "grad_norm": 2.75365948677063,
      "learning_rate": 4.759102455546147e-06,
      "loss": 0.8726,
      "step": 39860
    },
    {
      "epoch": 2.5266326562945594,
      "grad_norm": 2.6907339096069336,
      "learning_rate": 4.752751905165115e-06,
      "loss": 0.8838,
      "step": 39870
    },
    {
      "epoch": 2.52726638993631,
      "grad_norm": 2.536811351776123,
      "learning_rate": 4.746401354784081e-06,
      "loss": 0.8472,
      "step": 39880
    },
    {
      "epoch": 2.52790012357806,
      "grad_norm": 2.9884204864501953,
      "learning_rate": 4.740050804403048e-06,
      "loss": 0.8546,
      "step": 39890
    },
    {
      "epoch": 2.5285338572198106,
      "grad_norm": 2.724186420440674,
      "learning_rate": 4.733700254022016e-06,
      "loss": 0.8342,
      "step": 39900
    },
    {
      "epoch": 2.5291675908615607,
      "grad_norm": 2.599587917327881,
      "learning_rate": 4.727349703640982e-06,
      "loss": 0.8506,
      "step": 39910
    },
    {
      "epoch": 2.5298013245033113,
      "grad_norm": 2.4307494163513184,
      "learning_rate": 4.720999153259949e-06,
      "loss": 0.8464,
      "step": 39920
    },
    {
      "epoch": 2.530435058145062,
      "grad_norm": 2.537114143371582,
      "learning_rate": 4.714648602878916e-06,
      "loss": 0.8794,
      "step": 39930
    },
    {
      "epoch": 2.531068791786812,
      "grad_norm": 2.7339975833892822,
      "learning_rate": 4.708298052497884e-06,
      "loss": 0.8472,
      "step": 39940
    },
    {
      "epoch": 2.531702525428562,
      "grad_norm": 3.0438930988311768,
      "learning_rate": 4.70194750211685e-06,
      "loss": 0.8591,
      "step": 39950
    },
    {
      "epoch": 2.5323362590703127,
      "grad_norm": 3.246574878692627,
      "learning_rate": 4.6955969517358176e-06,
      "loss": 0.8814,
      "step": 39960
    },
    {
      "epoch": 2.5329699927120632,
      "grad_norm": 2.645700693130493,
      "learning_rate": 4.689246401354784e-06,
      "loss": 0.8689,
      "step": 39970
    },
    {
      "epoch": 2.5336037263538134,
      "grad_norm": 2.573824405670166,
      "learning_rate": 4.682895850973751e-06,
      "loss": 0.8714,
      "step": 39980
    },
    {
      "epoch": 2.534237459995564,
      "grad_norm": 2.688525676727295,
      "learning_rate": 4.6765453005927185e-06,
      "loss": 0.8524,
      "step": 39990
    },
    {
      "epoch": 2.534871193637314,
      "grad_norm": 3.354011058807373,
      "learning_rate": 4.670194750211685e-06,
      "loss": 0.8814,
      "step": 40000
    },
    {
      "epoch": 2.5355049272790646,
      "grad_norm": 2.798316717147827,
      "learning_rate": 4.663844199830652e-06,
      "loss": 0.8285,
      "step": 40010
    },
    {
      "epoch": 2.536138660920815,
      "grad_norm": 2.5798580646514893,
      "learning_rate": 4.657493649449619e-06,
      "loss": 0.8324,
      "step": 40020
    },
    {
      "epoch": 2.5367723945625653,
      "grad_norm": 2.6690404415130615,
      "learning_rate": 4.651143099068586e-06,
      "loss": 0.8408,
      "step": 40030
    },
    {
      "epoch": 2.537406128204316,
      "grad_norm": 2.423863410949707,
      "learning_rate": 4.644792548687553e-06,
      "loss": 0.8487,
      "step": 40040
    },
    {
      "epoch": 2.538039861846066,
      "grad_norm": 3.134857416152954,
      "learning_rate": 4.63844199830652e-06,
      "loss": 0.8573,
      "step": 40050
    },
    {
      "epoch": 2.5386735954878166,
      "grad_norm": 2.545856475830078,
      "learning_rate": 4.632091447925487e-06,
      "loss": 0.7951,
      "step": 40060
    },
    {
      "epoch": 2.5393073291295667,
      "grad_norm": 2.859879493713379,
      "learning_rate": 4.625740897544454e-06,
      "loss": 0.8756,
      "step": 40070
    },
    {
      "epoch": 2.5399410627713173,
      "grad_norm": 2.9890189170837402,
      "learning_rate": 4.6193903471634215e-06,
      "loss": 0.8656,
      "step": 40080
    },
    {
      "epoch": 2.5405747964130674,
      "grad_norm": 3.0905542373657227,
      "learning_rate": 4.613039796782388e-06,
      "loss": 0.8673,
      "step": 40090
    },
    {
      "epoch": 2.541208530054818,
      "grad_norm": 2.821104049682617,
      "learning_rate": 4.606689246401354e-06,
      "loss": 0.8121,
      "step": 40100
    },
    {
      "epoch": 2.5418422636965685,
      "grad_norm": 2.576822519302368,
      "learning_rate": 4.6003386960203224e-06,
      "loss": 0.8384,
      "step": 40110
    },
    {
      "epoch": 2.5424759973383186,
      "grad_norm": 2.5983619689941406,
      "learning_rate": 4.593988145639289e-06,
      "loss": 0.8425,
      "step": 40120
    },
    {
      "epoch": 2.543109730980069,
      "grad_norm": 2.924154758453369,
      "learning_rate": 4.587637595258256e-06,
      "loss": 0.8384,
      "step": 40130
    },
    {
      "epoch": 2.5437434646218193,
      "grad_norm": 2.9813637733459473,
      "learning_rate": 4.5812870448772226e-06,
      "loss": 0.8025,
      "step": 40140
    },
    {
      "epoch": 2.54437719826357,
      "grad_norm": 2.780712366104126,
      "learning_rate": 4.574936494496189e-06,
      "loss": 0.8589,
      "step": 40150
    },
    {
      "epoch": 2.5450109319053205,
      "grad_norm": 2.8351731300354004,
      "learning_rate": 4.568585944115157e-06,
      "loss": 0.8356,
      "step": 40160
    },
    {
      "epoch": 2.5456446655470706,
      "grad_norm": 3.144171714782715,
      "learning_rate": 4.5622353937341235e-06,
      "loss": 0.8307,
      "step": 40170
    },
    {
      "epoch": 2.5462783991888207,
      "grad_norm": 3.3635623455047607,
      "learning_rate": 4.555884843353091e-06,
      "loss": 0.837,
      "step": 40180
    },
    {
      "epoch": 2.5469121328305713,
      "grad_norm": 2.716581344604492,
      "learning_rate": 4.549534292972057e-06,
      "loss": 0.8118,
      "step": 40190
    },
    {
      "epoch": 2.547545866472322,
      "grad_norm": 2.61409854888916,
      "learning_rate": 4.543183742591025e-06,
      "loss": 0.8452,
      "step": 40200
    },
    {
      "epoch": 2.548179600114072,
      "grad_norm": 2.7491769790649414,
      "learning_rate": 4.536833192209992e-06,
      "loss": 0.8324,
      "step": 40210
    },
    {
      "epoch": 2.5488133337558225,
      "grad_norm": 2.9822046756744385,
      "learning_rate": 4.530482641828958e-06,
      "loss": 0.8393,
      "step": 40220
    },
    {
      "epoch": 2.5494470673975727,
      "grad_norm": 2.705129861831665,
      "learning_rate": 4.5241320914479255e-06,
      "loss": 0.8874,
      "step": 40230
    },
    {
      "epoch": 2.550080801039323,
      "grad_norm": 3.079045295715332,
      "learning_rate": 4.517781541066893e-06,
      "loss": 0.8852,
      "step": 40240
    },
    {
      "epoch": 2.550714534681074,
      "grad_norm": 2.6077239513397217,
      "learning_rate": 4.512066045723962e-06,
      "loss": 0.8208,
      "step": 40250
    },
    {
      "epoch": 2.551348268322824,
      "grad_norm": 2.4949569702148438,
      "learning_rate": 4.50571549534293e-06,
      "loss": 0.8697,
      "step": 40260
    },
    {
      "epoch": 2.551982001964574,
      "grad_norm": 3.161440849304199,
      "learning_rate": 4.499364944961897e-06,
      "loss": 0.8389,
      "step": 40270
    },
    {
      "epoch": 2.5526157356063246,
      "grad_norm": 3.77083158493042,
      "learning_rate": 4.493014394580864e-06,
      "loss": 0.8732,
      "step": 40280
    },
    {
      "epoch": 2.553249469248075,
      "grad_norm": 2.7746541500091553,
      "learning_rate": 4.486663844199831e-06,
      "loss": 0.83,
      "step": 40290
    },
    {
      "epoch": 2.5538832028898253,
      "grad_norm": 2.6035633087158203,
      "learning_rate": 4.480313293818797e-06,
      "loss": 0.8747,
      "step": 40300
    },
    {
      "epoch": 2.554516936531576,
      "grad_norm": 3.113834857940674,
      "learning_rate": 4.473962743437765e-06,
      "loss": 0.847,
      "step": 40310
    },
    {
      "epoch": 2.555150670173326,
      "grad_norm": 2.5481481552124023,
      "learning_rate": 4.467612193056732e-06,
      "loss": 0.8515,
      "step": 40320
    },
    {
      "epoch": 2.5557844038150765,
      "grad_norm": 3.250194787979126,
      "learning_rate": 4.461261642675699e-06,
      "loss": 0.8554,
      "step": 40330
    },
    {
      "epoch": 2.556418137456827,
      "grad_norm": 2.8423314094543457,
      "learning_rate": 4.454911092294665e-06,
      "loss": 0.7825,
      "step": 40340
    },
    {
      "epoch": 2.5570518710985772,
      "grad_norm": 3.1126487255096436,
      "learning_rate": 4.4485605419136326e-06,
      "loss": 0.8581,
      "step": 40350
    },
    {
      "epoch": 2.557685604740328,
      "grad_norm": 2.650101900100708,
      "learning_rate": 4.4422099915326e-06,
      "loss": 0.8346,
      "step": 40360
    },
    {
      "epoch": 2.558319338382078,
      "grad_norm": 3.2832088470458984,
      "learning_rate": 4.435859441151566e-06,
      "loss": 0.8162,
      "step": 40370
    },
    {
      "epoch": 2.5589530720238285,
      "grad_norm": 3.131999969482422,
      "learning_rate": 4.4295088907705336e-06,
      "loss": 0.8896,
      "step": 40380
    },
    {
      "epoch": 2.5595868056655786,
      "grad_norm": 2.730438709259033,
      "learning_rate": 4.4231583403895e-06,
      "loss": 0.8438,
      "step": 40390
    },
    {
      "epoch": 2.560220539307329,
      "grad_norm": 2.9468390941619873,
      "learning_rate": 4.416807790008468e-06,
      "loss": 0.8331,
      "step": 40400
    },
    {
      "epoch": 2.5608542729490793,
      "grad_norm": 2.6312029361724854,
      "learning_rate": 4.4104572396274345e-06,
      "loss": 0.8212,
      "step": 40410
    },
    {
      "epoch": 2.56148800659083,
      "grad_norm": 3.126162052154541,
      "learning_rate": 4.404106689246401e-06,
      "loss": 0.8475,
      "step": 40420
    },
    {
      "epoch": 2.5621217402325804,
      "grad_norm": 2.6725761890411377,
      "learning_rate": 4.397756138865368e-06,
      "loss": 0.8591,
      "step": 40430
    },
    {
      "epoch": 2.5627554738743306,
      "grad_norm": 3.273669481277466,
      "learning_rate": 4.3914055884843355e-06,
      "loss": 0.8662,
      "step": 40440
    },
    {
      "epoch": 2.563389207516081,
      "grad_norm": 2.3401076793670654,
      "learning_rate": 4.385055038103303e-06,
      "loss": 0.8258,
      "step": 40450
    },
    {
      "epoch": 2.5640229411578312,
      "grad_norm": 2.811309814453125,
      "learning_rate": 4.378704487722269e-06,
      "loss": 0.8275,
      "step": 40460
    },
    {
      "epoch": 2.564656674799582,
      "grad_norm": 3.052502155303955,
      "learning_rate": 4.3723539373412365e-06,
      "loss": 0.8621,
      "step": 40470
    },
    {
      "epoch": 2.5652904084413324,
      "grad_norm": 2.563324213027954,
      "learning_rate": 4.366003386960204e-06,
      "loss": 0.81,
      "step": 40480
    },
    {
      "epoch": 2.5659241420830825,
      "grad_norm": 2.8341917991638184,
      "learning_rate": 4.35965283657917e-06,
      "loss": 0.8739,
      "step": 40490
    },
    {
      "epoch": 2.5665578757248326,
      "grad_norm": 2.69366455078125,
      "learning_rate": 4.3533022861981374e-06,
      "loss": 0.861,
      "step": 40500
    },
    {
      "epoch": 2.567191609366583,
      "grad_norm": 2.45440936088562,
      "learning_rate": 4.346951735817104e-06,
      "loss": 0.8285,
      "step": 40510
    },
    {
      "epoch": 2.5678253430083338,
      "grad_norm": 3.2310268878936768,
      "learning_rate": 4.340601185436072e-06,
      "loss": 0.8574,
      "step": 40520
    },
    {
      "epoch": 2.568459076650084,
      "grad_norm": 2.4929146766662598,
      "learning_rate": 4.3342506350550384e-06,
      "loss": 0.8423,
      "step": 40530
    },
    {
      "epoch": 2.5690928102918344,
      "grad_norm": 3.1401448249816895,
      "learning_rate": 4.327900084674005e-06,
      "loss": 0.8478,
      "step": 40540
    },
    {
      "epoch": 2.5697265439335846,
      "grad_norm": 2.5193443298339844,
      "learning_rate": 4.321549534292972e-06,
      "loss": 0.8713,
      "step": 40550
    },
    {
      "epoch": 2.570360277575335,
      "grad_norm": 2.876349925994873,
      "learning_rate": 4.3151989839119386e-06,
      "loss": 0.8765,
      "step": 40560
    },
    {
      "epoch": 2.5709940112170857,
      "grad_norm": 2.729442834854126,
      "learning_rate": 4.308848433530907e-06,
      "loss": 0.8498,
      "step": 40570
    },
    {
      "epoch": 2.571627744858836,
      "grad_norm": 3.103882312774658,
      "learning_rate": 4.302497883149873e-06,
      "loss": 0.8723,
      "step": 40580
    },
    {
      "epoch": 2.572261478500586,
      "grad_norm": 2.6083950996398926,
      "learning_rate": 4.29614733276884e-06,
      "loss": 0.892,
      "step": 40590
    },
    {
      "epoch": 2.5728952121423365,
      "grad_norm": 3.042767286300659,
      "learning_rate": 4.289796782387807e-06,
      "loss": 0.8556,
      "step": 40600
    },
    {
      "epoch": 2.573528945784087,
      "grad_norm": 2.545905351638794,
      "learning_rate": 4.283446232006774e-06,
      "loss": 0.8775,
      "step": 40610
    },
    {
      "epoch": 2.574162679425837,
      "grad_norm": 2.5035219192504883,
      "learning_rate": 4.277095681625741e-06,
      "loss": 0.8288,
      "step": 40620
    },
    {
      "epoch": 2.5747964130675878,
      "grad_norm": 3.00024676322937,
      "learning_rate": 4.270745131244708e-06,
      "loss": 0.8601,
      "step": 40630
    },
    {
      "epoch": 2.575430146709338,
      "grad_norm": 3.0185203552246094,
      "learning_rate": 4.264394580863675e-06,
      "loss": 0.8616,
      "step": 40640
    },
    {
      "epoch": 2.5760638803510885,
      "grad_norm": 3.1342625617980957,
      "learning_rate": 4.258044030482642e-06,
      "loss": 0.8872,
      "step": 40650
    },
    {
      "epoch": 2.576697613992839,
      "grad_norm": 2.463709592819214,
      "learning_rate": 4.251693480101609e-06,
      "loss": 0.8413,
      "step": 40660
    },
    {
      "epoch": 2.577331347634589,
      "grad_norm": 3.035291910171509,
      "learning_rate": 4.245342929720576e-06,
      "loss": 0.8858,
      "step": 40670
    },
    {
      "epoch": 2.5779650812763397,
      "grad_norm": 3.161457061767578,
      "learning_rate": 4.2389923793395425e-06,
      "loss": 0.8552,
      "step": 40680
    },
    {
      "epoch": 2.57859881491809,
      "grad_norm": 2.5913875102996826,
      "learning_rate": 4.23264182895851e-06,
      "loss": 0.8102,
      "step": 40690
    },
    {
      "epoch": 2.5792325485598404,
      "grad_norm": 2.8506596088409424,
      "learning_rate": 4.226291278577477e-06,
      "loss": 0.8068,
      "step": 40700
    },
    {
      "epoch": 2.5798662822015905,
      "grad_norm": 3.167202949523926,
      "learning_rate": 4.219940728196444e-06,
      "loss": 0.8643,
      "step": 40710
    },
    {
      "epoch": 2.580500015843341,
      "grad_norm": 2.9542622566223145,
      "learning_rate": 4.213590177815411e-06,
      "loss": 0.8328,
      "step": 40720
    },
    {
      "epoch": 2.5811337494850912,
      "grad_norm": 2.8248157501220703,
      "learning_rate": 4.207239627434377e-06,
      "loss": 0.9184,
      "step": 40730
    },
    {
      "epoch": 2.581767483126842,
      "grad_norm": 2.527556896209717,
      "learning_rate": 4.200889077053345e-06,
      "loss": 0.8561,
      "step": 40740
    },
    {
      "epoch": 2.5824012167685924,
      "grad_norm": 2.8386969566345215,
      "learning_rate": 4.194538526672312e-06,
      "loss": 0.8468,
      "step": 40750
    },
    {
      "epoch": 2.5830349504103425,
      "grad_norm": 2.73649263381958,
      "learning_rate": 4.188187976291279e-06,
      "loss": 0.8444,
      "step": 40760
    },
    {
      "epoch": 2.583668684052093,
      "grad_norm": 3.0261502265930176,
      "learning_rate": 4.181837425910245e-06,
      "loss": 0.9102,
      "step": 40770
    },
    {
      "epoch": 2.584302417693843,
      "grad_norm": 2.7353355884552,
      "learning_rate": 4.175486875529213e-06,
      "loss": 0.8358,
      "step": 40780
    },
    {
      "epoch": 2.5849361513355937,
      "grad_norm": 2.9923031330108643,
      "learning_rate": 4.16913632514818e-06,
      "loss": 0.8501,
      "step": 40790
    },
    {
      "epoch": 2.585569884977344,
      "grad_norm": 2.1724088191986084,
      "learning_rate": 4.162785774767146e-06,
      "loss": 0.8356,
      "step": 40800
    },
    {
      "epoch": 2.5862036186190944,
      "grad_norm": 2.7008931636810303,
      "learning_rate": 4.156435224386114e-06,
      "loss": 0.7981,
      "step": 40810
    },
    {
      "epoch": 2.5868373522608445,
      "grad_norm": 2.9987874031066895,
      "learning_rate": 4.15008467400508e-06,
      "loss": 0.8638,
      "step": 40820
    },
    {
      "epoch": 2.587471085902595,
      "grad_norm": 2.9497132301330566,
      "learning_rate": 4.143734123624048e-06,
      "loss": 0.8538,
      "step": 40830
    },
    {
      "epoch": 2.5881048195443457,
      "grad_norm": 2.2277419567108154,
      "learning_rate": 4.137383573243015e-06,
      "loss": 0.8341,
      "step": 40840
    },
    {
      "epoch": 2.588738553186096,
      "grad_norm": 3.022839069366455,
      "learning_rate": 4.131033022861981e-06,
      "loss": 0.835,
      "step": 40850
    },
    {
      "epoch": 2.5893722868278464,
      "grad_norm": 3.0049521923065186,
      "learning_rate": 4.124682472480948e-06,
      "loss": 0.8755,
      "step": 40860
    },
    {
      "epoch": 2.5900060204695965,
      "grad_norm": 2.897542715072632,
      "learning_rate": 4.1183319220999156e-06,
      "loss": 0.8372,
      "step": 40870
    },
    {
      "epoch": 2.590639754111347,
      "grad_norm": 2.4707283973693848,
      "learning_rate": 4.111981371718883e-06,
      "loss": 0.8363,
      "step": 40880
    },
    {
      "epoch": 2.5912734877530976,
      "grad_norm": 2.4430434703826904,
      "learning_rate": 4.105630821337849e-06,
      "loss": 0.8628,
      "step": 40890
    },
    {
      "epoch": 2.5919072213948477,
      "grad_norm": 2.717702627182007,
      "learning_rate": 4.099280270956816e-06,
      "loss": 0.8277,
      "step": 40900
    },
    {
      "epoch": 2.592540955036598,
      "grad_norm": 2.5925521850585938,
      "learning_rate": 4.092929720575784e-06,
      "loss": 0.8623,
      "step": 40910
    },
    {
      "epoch": 2.5931746886783484,
      "grad_norm": 2.796790599822998,
      "learning_rate": 4.08657917019475e-06,
      "loss": 0.8649,
      "step": 40920
    },
    {
      "epoch": 2.593808422320099,
      "grad_norm": 2.720961332321167,
      "learning_rate": 4.0802286198137175e-06,
      "loss": 0.8644,
      "step": 40930
    },
    {
      "epoch": 2.594442155961849,
      "grad_norm": 3.2202847003936768,
      "learning_rate": 4.073878069432684e-06,
      "loss": 0.8503,
      "step": 40940
    },
    {
      "epoch": 2.5950758896035997,
      "grad_norm": 2.6433801651000977,
      "learning_rate": 4.067527519051651e-06,
      "loss": 0.856,
      "step": 40950
    },
    {
      "epoch": 2.59570962324535,
      "grad_norm": 2.8996694087982178,
      "learning_rate": 4.0611769686706185e-06,
      "loss": 0.8567,
      "step": 40960
    },
    {
      "epoch": 2.5963433568871004,
      "grad_norm": 2.7949557304382324,
      "learning_rate": 4.054826418289585e-06,
      "loss": 0.8415,
      "step": 40970
    },
    {
      "epoch": 2.596977090528851,
      "grad_norm": 2.7329983711242676,
      "learning_rate": 4.048475867908552e-06,
      "loss": 0.8183,
      "step": 40980
    },
    {
      "epoch": 2.597610824170601,
      "grad_norm": 3.0609285831451416,
      "learning_rate": 4.042125317527519e-06,
      "loss": 0.8431,
      "step": 40990
    },
    {
      "epoch": 2.598244557812351,
      "grad_norm": 2.7837846279144287,
      "learning_rate": 4.035774767146487e-06,
      "loss": 0.8312,
      "step": 41000
    },
    {
      "epoch": 2.5988782914541018,
      "grad_norm": 2.9227705001831055,
      "learning_rate": 4.029424216765453e-06,
      "loss": 0.8328,
      "step": 41010
    },
    {
      "epoch": 2.5995120250958523,
      "grad_norm": 2.4857981204986572,
      "learning_rate": 4.0230736663844204e-06,
      "loss": 0.8532,
      "step": 41020
    },
    {
      "epoch": 2.6001457587376025,
      "grad_norm": 3.0481770038604736,
      "learning_rate": 4.016723116003387e-06,
      "loss": 0.8924,
      "step": 41030
    },
    {
      "epoch": 2.600779492379353,
      "grad_norm": 3.018254518508911,
      "learning_rate": 4.010372565622354e-06,
      "loss": 0.8716,
      "step": 41040
    },
    {
      "epoch": 2.601413226021103,
      "grad_norm": 2.911862373352051,
      "learning_rate": 4.004022015241321e-06,
      "loss": 0.8724,
      "step": 41050
    },
    {
      "epoch": 2.6020469596628537,
      "grad_norm": 2.622368812561035,
      "learning_rate": 3.997671464860288e-06,
      "loss": 0.8448,
      "step": 41060
    },
    {
      "epoch": 2.6026806933046043,
      "grad_norm": 2.839287757873535,
      "learning_rate": 3.991320914479255e-06,
      "loss": 0.8504,
      "step": 41070
    },
    {
      "epoch": 2.6033144269463544,
      "grad_norm": 2.745464324951172,
      "learning_rate": 3.9849703640982215e-06,
      "loss": 0.8108,
      "step": 41080
    },
    {
      "epoch": 2.603948160588105,
      "grad_norm": 2.614140748977661,
      "learning_rate": 3.978619813717189e-06,
      "loss": 0.8458,
      "step": 41090
    },
    {
      "epoch": 2.604581894229855,
      "grad_norm": 3.0560126304626465,
      "learning_rate": 3.972269263336156e-06,
      "loss": 0.8686,
      "step": 41100
    },
    {
      "epoch": 2.6052156278716057,
      "grad_norm": 2.7776238918304443,
      "learning_rate": 3.9659187129551225e-06,
      "loss": 0.8216,
      "step": 41110
    },
    {
      "epoch": 2.6058493615133558,
      "grad_norm": 2.788489818572998,
      "learning_rate": 3.95956816257409e-06,
      "loss": 0.8384,
      "step": 41120
    },
    {
      "epoch": 2.6064830951551063,
      "grad_norm": 3.163379192352295,
      "learning_rate": 3.953217612193057e-06,
      "loss": 0.8253,
      "step": 41130
    },
    {
      "epoch": 2.6071168287968565,
      "grad_norm": 3.246673107147217,
      "learning_rate": 3.946867061812024e-06,
      "loss": 0.8569,
      "step": 41140
    },
    {
      "epoch": 2.607750562438607,
      "grad_norm": 2.2924230098724365,
      "learning_rate": 3.940516511430991e-06,
      "loss": 0.8494,
      "step": 41150
    },
    {
      "epoch": 2.6083842960803576,
      "grad_norm": 2.6832425594329834,
      "learning_rate": 3.934165961049957e-06,
      "loss": 0.8493,
      "step": 41160
    },
    {
      "epoch": 2.6090180297221077,
      "grad_norm": 3.0088629722595215,
      "learning_rate": 3.927815410668925e-06,
      "loss": 0.8071,
      "step": 41170
    },
    {
      "epoch": 2.6096517633638583,
      "grad_norm": 2.8031301498413086,
      "learning_rate": 3.921464860287892e-06,
      "loss": 0.852,
      "step": 41180
    },
    {
      "epoch": 2.6102854970056084,
      "grad_norm": 2.532287836074829,
      "learning_rate": 3.915114309906859e-06,
      "loss": 0.8446,
      "step": 41190
    },
    {
      "epoch": 2.610919230647359,
      "grad_norm": 2.6870832443237305,
      "learning_rate": 3.9087637595258254e-06,
      "loss": 0.8151,
      "step": 41200
    },
    {
      "epoch": 2.6115529642891095,
      "grad_norm": 2.9892337322235107,
      "learning_rate": 3.902413209144792e-06,
      "loss": 0.8571,
      "step": 41210
    },
    {
      "epoch": 2.6121866979308597,
      "grad_norm": 2.6263997554779053,
      "learning_rate": 3.89606265876376e-06,
      "loss": 0.8072,
      "step": 41220
    },
    {
      "epoch": 2.61282043157261,
      "grad_norm": 2.7529187202453613,
      "learning_rate": 3.889712108382726e-06,
      "loss": 0.8435,
      "step": 41230
    },
    {
      "epoch": 2.6134541652143604,
      "grad_norm": 2.935476779937744,
      "learning_rate": 3.883361558001694e-06,
      "loss": 0.8306,
      "step": 41240
    },
    {
      "epoch": 2.614087898856111,
      "grad_norm": 3.040442705154419,
      "learning_rate": 3.87701100762066e-06,
      "loss": 0.8255,
      "step": 41250
    },
    {
      "epoch": 2.614721632497861,
      "grad_norm": 2.829387903213501,
      "learning_rate": 3.870660457239628e-06,
      "loss": 0.8514,
      "step": 41260
    },
    {
      "epoch": 2.6153553661396116,
      "grad_norm": 2.8791494369506836,
      "learning_rate": 3.864309906858595e-06,
      "loss": 0.8536,
      "step": 41270
    },
    {
      "epoch": 2.6159890997813617,
      "grad_norm": 2.471245527267456,
      "learning_rate": 3.857959356477561e-06,
      "loss": 0.8587,
      "step": 41280
    },
    {
      "epoch": 2.6166228334231123,
      "grad_norm": 3.653731107711792,
      "learning_rate": 3.851608806096528e-06,
      "loss": 0.827,
      "step": 41290
    },
    {
      "epoch": 2.617256567064863,
      "grad_norm": 3.193222761154175,
      "learning_rate": 3.845258255715496e-06,
      "loss": 0.8224,
      "step": 41300
    },
    {
      "epoch": 2.617890300706613,
      "grad_norm": 2.616304397583008,
      "learning_rate": 3.838907705334463e-06,
      "loss": 0.8435,
      "step": 41310
    },
    {
      "epoch": 2.618524034348363,
      "grad_norm": 2.943950891494751,
      "learning_rate": 3.832557154953429e-06,
      "loss": 0.862,
      "step": 41320
    },
    {
      "epoch": 2.6191577679901137,
      "grad_norm": 2.6928250789642334,
      "learning_rate": 3.826206604572396e-06,
      "loss": 0.8872,
      "step": 41330
    },
    {
      "epoch": 2.6197915016318643,
      "grad_norm": 3.100832462310791,
      "learning_rate": 3.819856054191363e-06,
      "loss": 0.8416,
      "step": 41340
    },
    {
      "epoch": 2.6204252352736144,
      "grad_norm": 2.7440545558929443,
      "learning_rate": 3.8135055038103307e-06,
      "loss": 0.8184,
      "step": 41350
    },
    {
      "epoch": 2.621058968915365,
      "grad_norm": 3.8376567363739014,
      "learning_rate": 3.807154953429297e-06,
      "loss": 0.8685,
      "step": 41360
    },
    {
      "epoch": 2.621692702557115,
      "grad_norm": 2.4817373752593994,
      "learning_rate": 3.800804403048264e-06,
      "loss": 0.8602,
      "step": 41370
    },
    {
      "epoch": 2.6223264361988656,
      "grad_norm": 2.6614153385162354,
      "learning_rate": 3.794453852667231e-06,
      "loss": 0.8503,
      "step": 41380
    },
    {
      "epoch": 2.622960169840616,
      "grad_norm": 2.573307991027832,
      "learning_rate": 3.7881033022861986e-06,
      "loss": 0.8312,
      "step": 41390
    },
    {
      "epoch": 2.6235939034823663,
      "grad_norm": 2.6551060676574707,
      "learning_rate": 3.7817527519051654e-06,
      "loss": 0.8938,
      "step": 41400
    },
    {
      "epoch": 2.624227637124117,
      "grad_norm": 2.3586440086364746,
      "learning_rate": 3.7754022015241323e-06,
      "loss": 0.875,
      "step": 41410
    },
    {
      "epoch": 2.624861370765867,
      "grad_norm": 3.2034943103790283,
      "learning_rate": 3.769051651143099e-06,
      "loss": 0.8332,
      "step": 41420
    },
    {
      "epoch": 2.6254951044076176,
      "grad_norm": 2.694762706756592,
      "learning_rate": 3.7627011007620664e-06,
      "loss": 0.8235,
      "step": 41430
    },
    {
      "epoch": 2.6261288380493677,
      "grad_norm": 2.652904510498047,
      "learning_rate": 3.7563505503810332e-06,
      "loss": 0.8218,
      "step": 41440
    },
    {
      "epoch": 2.6267625716911183,
      "grad_norm": 2.9198999404907227,
      "learning_rate": 3.75e-06,
      "loss": 0.8639,
      "step": 41450
    },
    {
      "epoch": 2.6273963053328684,
      "grad_norm": 3.264998197555542,
      "learning_rate": 3.743649449618967e-06,
      "loss": 0.8202,
      "step": 41460
    },
    {
      "epoch": 2.628030038974619,
      "grad_norm": 2.6549904346466064,
      "learning_rate": 3.7372988992379342e-06,
      "loss": 0.8387,
      "step": 41470
    },
    {
      "epoch": 2.6286637726163695,
      "grad_norm": 2.6365623474121094,
      "learning_rate": 3.730948348856901e-06,
      "loss": 0.8199,
      "step": 41480
    },
    {
      "epoch": 2.6292975062581196,
      "grad_norm": 2.724306106567383,
      "learning_rate": 3.724597798475868e-06,
      "loss": 0.8732,
      "step": 41490
    },
    {
      "epoch": 2.62993123989987,
      "grad_norm": 2.7211530208587646,
      "learning_rate": 3.7182472480948348e-06,
      "loss": 0.7843,
      "step": 41500
    },
    {
      "epoch": 2.6305649735416203,
      "grad_norm": 2.6013011932373047,
      "learning_rate": 3.711896697713802e-06,
      "loss": 0.8864,
      "step": 41510
    },
    {
      "epoch": 2.631198707183371,
      "grad_norm": 2.8516366481781006,
      "learning_rate": 3.705546147332769e-06,
      "loss": 0.8489,
      "step": 41520
    },
    {
      "epoch": 2.6318324408251215,
      "grad_norm": 2.7879371643066406,
      "learning_rate": 3.699195596951736e-06,
      "loss": 0.8538,
      "step": 41530
    },
    {
      "epoch": 2.6324661744668716,
      "grad_norm": 2.8790462017059326,
      "learning_rate": 3.692845046570703e-06,
      "loss": 0.8384,
      "step": 41540
    },
    {
      "epoch": 2.6330999081086217,
      "grad_norm": 3.072171211242676,
      "learning_rate": 3.68649449618967e-06,
      "loss": 0.8403,
      "step": 41550
    },
    {
      "epoch": 2.6337336417503723,
      "grad_norm": 3.067115068435669,
      "learning_rate": 3.6801439458086367e-06,
      "loss": 0.857,
      "step": 41560
    },
    {
      "epoch": 2.634367375392123,
      "grad_norm": 2.638152837753296,
      "learning_rate": 3.673793395427604e-06,
      "loss": 0.8367,
      "step": 41570
    },
    {
      "epoch": 2.635001109033873,
      "grad_norm": 2.6676084995269775,
      "learning_rate": 3.667442845046571e-06,
      "loss": 0.8371,
      "step": 41580
    },
    {
      "epoch": 2.6356348426756235,
      "grad_norm": 2.5932869911193848,
      "learning_rate": 3.6610922946655377e-06,
      "loss": 0.8586,
      "step": 41590
    },
    {
      "epoch": 2.6362685763173737,
      "grad_norm": 2.747781753540039,
      "learning_rate": 3.654741744284505e-06,
      "loss": 0.8226,
      "step": 41600
    },
    {
      "epoch": 2.6369023099591242,
      "grad_norm": 2.9486305713653564,
      "learning_rate": 3.6483911939034714e-06,
      "loss": 0.8344,
      "step": 41610
    },
    {
      "epoch": 2.637536043600875,
      "grad_norm": 3.006467580795288,
      "learning_rate": 3.6420406435224387e-06,
      "loss": 0.8326,
      "step": 41620
    },
    {
      "epoch": 2.638169777242625,
      "grad_norm": 2.7379748821258545,
      "learning_rate": 3.6356900931414055e-06,
      "loss": 0.8351,
      "step": 41630
    },
    {
      "epoch": 2.638803510884375,
      "grad_norm": 2.7884538173675537,
      "learning_rate": 3.629339542760373e-06,
      "loss": 0.8672,
      "step": 41640
    },
    {
      "epoch": 2.6394372445261256,
      "grad_norm": 3.082608699798584,
      "learning_rate": 3.6229889923793396e-06,
      "loss": 0.8341,
      "step": 41650
    },
    {
      "epoch": 2.640070978167876,
      "grad_norm": 2.9073970317840576,
      "learning_rate": 3.616638441998307e-06,
      "loss": 0.8439,
      "step": 41660
    },
    {
      "epoch": 2.6407047118096263,
      "grad_norm": 2.512812376022339,
      "learning_rate": 3.6102878916172733e-06,
      "loss": 0.829,
      "step": 41670
    },
    {
      "epoch": 2.641338445451377,
      "grad_norm": 2.7553250789642334,
      "learning_rate": 3.6039373412362406e-06,
      "loss": 0.8222,
      "step": 41680
    },
    {
      "epoch": 2.641972179093127,
      "grad_norm": 2.7527871131896973,
      "learning_rate": 3.5975867908552075e-06,
      "loss": 0.8162,
      "step": 41690
    },
    {
      "epoch": 2.6426059127348775,
      "grad_norm": 3.0607171058654785,
      "learning_rate": 3.5912362404741747e-06,
      "loss": 0.8436,
      "step": 41700
    },
    {
      "epoch": 2.643239646376628,
      "grad_norm": 2.721360921859741,
      "learning_rate": 3.5848856900931416e-06,
      "loss": 0.8185,
      "step": 41710
    },
    {
      "epoch": 2.6438733800183782,
      "grad_norm": 2.863053321838379,
      "learning_rate": 3.5785351397121084e-06,
      "loss": 0.8599,
      "step": 41720
    },
    {
      "epoch": 2.644507113660129,
      "grad_norm": 2.3688552379608154,
      "learning_rate": 3.5721845893310753e-06,
      "loss": 0.857,
      "step": 41730
    },
    {
      "epoch": 2.645140847301879,
      "grad_norm": 2.482800245285034,
      "learning_rate": 3.565834038950042e-06,
      "loss": 0.8399,
      "step": 41740
    },
    {
      "epoch": 2.6457745809436295,
      "grad_norm": 2.8073339462280273,
      "learning_rate": 3.5594834885690094e-06,
      "loss": 0.9102,
      "step": 41750
    },
    {
      "epoch": 2.6464083145853796,
      "grad_norm": 4.2187981605529785,
      "learning_rate": 3.5531329381879763e-06,
      "loss": 0.8169,
      "step": 41760
    },
    {
      "epoch": 2.64704204822713,
      "grad_norm": 3.278779983520508,
      "learning_rate": 3.5467823878069435e-06,
      "loss": 0.831,
      "step": 41770
    },
    {
      "epoch": 2.6476757818688803,
      "grad_norm": 2.544809341430664,
      "learning_rate": 3.5404318374259104e-06,
      "loss": 0.8064,
      "step": 41780
    },
    {
      "epoch": 2.648309515510631,
      "grad_norm": 2.6774067878723145,
      "learning_rate": 3.5340812870448772e-06,
      "loss": 0.8292,
      "step": 41790
    },
    {
      "epoch": 2.6489432491523814,
      "grad_norm": 2.7037250995635986,
      "learning_rate": 3.527730736663844e-06,
      "loss": 0.8842,
      "step": 41800
    },
    {
      "epoch": 2.6495769827941316,
      "grad_norm": 2.8566198348999023,
      "learning_rate": 3.5213801862828114e-06,
      "loss": 0.8403,
      "step": 41810
    },
    {
      "epoch": 2.650210716435882,
      "grad_norm": 3.019425868988037,
      "learning_rate": 3.5150296359017782e-06,
      "loss": 0.8349,
      "step": 41820
    },
    {
      "epoch": 2.6508444500776323,
      "grad_norm": 2.639019250869751,
      "learning_rate": 3.5086790855207455e-06,
      "loss": 0.8309,
      "step": 41830
    },
    {
      "epoch": 2.651478183719383,
      "grad_norm": 2.6751201152801514,
      "learning_rate": 3.5023285351397123e-06,
      "loss": 0.888,
      "step": 41840
    },
    {
      "epoch": 2.6521119173611334,
      "grad_norm": 2.846169948577881,
      "learning_rate": 3.4959779847586788e-06,
      "loss": 0.8747,
      "step": 41850
    },
    {
      "epoch": 2.6527456510028835,
      "grad_norm": 2.7598214149475098,
      "learning_rate": 3.489627434377646e-06,
      "loss": 0.8531,
      "step": 41860
    },
    {
      "epoch": 2.6533793846446336,
      "grad_norm": 2.9544520378112793,
      "learning_rate": 3.483276883996613e-06,
      "loss": 0.835,
      "step": 41870
    },
    {
      "epoch": 2.654013118286384,
      "grad_norm": 3.133829355239868,
      "learning_rate": 3.47692633361558e-06,
      "loss": 0.8705,
      "step": 41880
    },
    {
      "epoch": 2.6546468519281348,
      "grad_norm": 2.933501958847046,
      "learning_rate": 3.470575783234547e-06,
      "loss": 0.8264,
      "step": 41890
    },
    {
      "epoch": 2.655280585569885,
      "grad_norm": 2.533017873764038,
      "learning_rate": 3.4642252328535143e-06,
      "loss": 0.8511,
      "step": 41900
    },
    {
      "epoch": 2.6559143192116355,
      "grad_norm": 2.773550271987915,
      "learning_rate": 3.4578746824724807e-06,
      "loss": 0.8531,
      "step": 41910
    },
    {
      "epoch": 2.6565480528533856,
      "grad_norm": 2.8110547065734863,
      "learning_rate": 3.451524132091448e-06,
      "loss": 0.863,
      "step": 41920
    },
    {
      "epoch": 2.657181786495136,
      "grad_norm": 2.523122787475586,
      "learning_rate": 3.445173581710415e-06,
      "loss": 0.8574,
      "step": 41930
    },
    {
      "epoch": 2.6578155201368867,
      "grad_norm": 2.3544232845306396,
      "learning_rate": 3.438823031329382e-06,
      "loss": 0.8503,
      "step": 41940
    },
    {
      "epoch": 2.658449253778637,
      "grad_norm": 2.4958913326263428,
      "learning_rate": 3.432472480948349e-06,
      "loss": 0.8338,
      "step": 41950
    },
    {
      "epoch": 2.659082987420387,
      "grad_norm": 2.621098756790161,
      "learning_rate": 3.4261219305673162e-06,
      "loss": 0.8224,
      "step": 41960
    },
    {
      "epoch": 2.6597167210621375,
      "grad_norm": 2.856780767440796,
      "learning_rate": 3.419771380186283e-06,
      "loss": 0.8421,
      "step": 41970
    },
    {
      "epoch": 2.660350454703888,
      "grad_norm": 2.996995687484741,
      "learning_rate": 3.41342082980525e-06,
      "loss": 0.8219,
      "step": 41980
    },
    {
      "epoch": 2.660984188345638,
      "grad_norm": 2.834684133529663,
      "learning_rate": 3.407070279424217e-06,
      "loss": 0.8287,
      "step": 41990
    },
    {
      "epoch": 2.661617921987389,
      "grad_norm": 2.5793373584747314,
      "learning_rate": 3.4007197290431836e-06,
      "loss": 0.8138,
      "step": 42000
    },
    {
      "epoch": 2.662251655629139,
      "grad_norm": 2.946108102798462,
      "learning_rate": 3.394369178662151e-06,
      "loss": 0.8562,
      "step": 42010
    },
    {
      "epoch": 2.6628853892708895,
      "grad_norm": 3.4368808269500732,
      "learning_rate": 3.3880186282811178e-06,
      "loss": 0.8509,
      "step": 42020
    },
    {
      "epoch": 2.66351912291264,
      "grad_norm": 2.399559259414673,
      "learning_rate": 3.381668077900085e-06,
      "loss": 0.8306,
      "step": 42030
    },
    {
      "epoch": 2.66415285655439,
      "grad_norm": 2.8577184677124023,
      "learning_rate": 3.3753175275190515e-06,
      "loss": 0.8703,
      "step": 42040
    },
    {
      "epoch": 2.6647865901961407,
      "grad_norm": 3.614372968673706,
      "learning_rate": 3.3689669771380187e-06,
      "loss": 0.8439,
      "step": 42050
    },
    {
      "epoch": 2.665420323837891,
      "grad_norm": 2.8265109062194824,
      "learning_rate": 3.3626164267569856e-06,
      "loss": 0.8378,
      "step": 42060
    },
    {
      "epoch": 2.6660540574796414,
      "grad_norm": 3.06919002532959,
      "learning_rate": 3.356265876375953e-06,
      "loss": 0.8705,
      "step": 42070
    },
    {
      "epoch": 2.6666877911213915,
      "grad_norm": 2.8469197750091553,
      "learning_rate": 3.3499153259949197e-06,
      "loss": 0.8583,
      "step": 42080
    },
    {
      "epoch": 2.667321524763142,
      "grad_norm": 3.1532225608825684,
      "learning_rate": 3.343564775613887e-06,
      "loss": 0.861,
      "step": 42090
    },
    {
      "epoch": 2.6679552584048922,
      "grad_norm": 2.8854570388793945,
      "learning_rate": 3.3372142252328534e-06,
      "loss": 0.8195,
      "step": 42100
    },
    {
      "epoch": 2.668588992046643,
      "grad_norm": 2.7299673557281494,
      "learning_rate": 3.3308636748518207e-06,
      "loss": 0.8676,
      "step": 42110
    },
    {
      "epoch": 2.6692227256883934,
      "grad_norm": 2.93932843208313,
      "learning_rate": 3.3245131244707875e-06,
      "loss": 0.8701,
      "step": 42120
    },
    {
      "epoch": 2.6698564593301435,
      "grad_norm": 3.265314817428589,
      "learning_rate": 3.3181625740897544e-06,
      "loss": 0.8486,
      "step": 42130
    },
    {
      "epoch": 2.670490192971894,
      "grad_norm": 2.8753676414489746,
      "learning_rate": 3.3118120237087217e-06,
      "loss": 0.8374,
      "step": 42140
    },
    {
      "epoch": 2.671123926613644,
      "grad_norm": 2.6647789478302,
      "learning_rate": 3.3054614733276885e-06,
      "loss": 0.8761,
      "step": 42150
    },
    {
      "epoch": 2.6717576602553947,
      "grad_norm": 2.781189203262329,
      "learning_rate": 3.2991109229466554e-06,
      "loss": 0.8245,
      "step": 42160
    },
    {
      "epoch": 2.6723913938971453,
      "grad_norm": 2.899906635284424,
      "learning_rate": 3.2927603725656222e-06,
      "loss": 0.8525,
      "step": 42170
    },
    {
      "epoch": 2.6730251275388954,
      "grad_norm": 2.622049331665039,
      "learning_rate": 3.2864098221845895e-06,
      "loss": 0.8325,
      "step": 42180
    },
    {
      "epoch": 2.6736588611806456,
      "grad_norm": 3.176743745803833,
      "learning_rate": 3.2800592718035563e-06,
      "loss": 0.8382,
      "step": 42190
    },
    {
      "epoch": 2.674292594822396,
      "grad_norm": 2.9644906520843506,
      "learning_rate": 3.2737087214225236e-06,
      "loss": 0.8616,
      "step": 42200
    },
    {
      "epoch": 2.6749263284641467,
      "grad_norm": 2.7962818145751953,
      "learning_rate": 3.2673581710414905e-06,
      "loss": 0.8062,
      "step": 42210
    },
    {
      "epoch": 2.675560062105897,
      "grad_norm": 2.8669986724853516,
      "learning_rate": 3.2610076206604573e-06,
      "loss": 0.8573,
      "step": 42220
    },
    {
      "epoch": 2.6761937957476474,
      "grad_norm": 2.7746267318725586,
      "learning_rate": 3.254657070279424e-06,
      "loss": 0.8579,
      "step": 42230
    },
    {
      "epoch": 2.6768275293893975,
      "grad_norm": 3.4543771743774414,
      "learning_rate": 3.2483065198983914e-06,
      "loss": 0.8494,
      "step": 42240
    },
    {
      "epoch": 2.677461263031148,
      "grad_norm": 2.7731778621673584,
      "learning_rate": 3.2419559695173583e-06,
      "loss": 0.8281,
      "step": 42250
    },
    {
      "epoch": 2.6780949966728986,
      "grad_norm": 2.554147720336914,
      "learning_rate": 3.235605419136325e-06,
      "loss": 0.8505,
      "step": 42260
    },
    {
      "epoch": 2.6787287303146488,
      "grad_norm": 3.3062829971313477,
      "learning_rate": 3.2292548687552924e-06,
      "loss": 0.8587,
      "step": 42270
    },
    {
      "epoch": 2.679362463956399,
      "grad_norm": 3.365666151046753,
      "learning_rate": 3.222904318374259e-06,
      "loss": 0.8841,
      "step": 42280
    },
    {
      "epoch": 2.6799961975981494,
      "grad_norm": 2.514954090118408,
      "learning_rate": 3.216553767993226e-06,
      "loss": 0.8431,
      "step": 42290
    },
    {
      "epoch": 2.6806299312399,
      "grad_norm": 3.1169512271881104,
      "learning_rate": 3.210203217612193e-06,
      "loss": 0.8157,
      "step": 42300
    },
    {
      "epoch": 2.68126366488165,
      "grad_norm": 3.0733468532562256,
      "learning_rate": 3.2038526672311602e-06,
      "loss": 0.8547,
      "step": 42310
    },
    {
      "epoch": 2.6818973985234007,
      "grad_norm": 2.835583448410034,
      "learning_rate": 3.197502116850127e-06,
      "loss": 0.8104,
      "step": 42320
    },
    {
      "epoch": 2.682531132165151,
      "grad_norm": 3.998216152191162,
      "learning_rate": 3.1911515664690944e-06,
      "loss": 0.8883,
      "step": 42330
    },
    {
      "epoch": 2.6831648658069014,
      "grad_norm": 3.0670456886291504,
      "learning_rate": 3.184801016088061e-06,
      "loss": 0.8281,
      "step": 42340
    },
    {
      "epoch": 2.683798599448652,
      "grad_norm": 2.543902635574341,
      "learning_rate": 3.178450465707028e-06,
      "loss": 0.8267,
      "step": 42350
    },
    {
      "epoch": 2.684432333090402,
      "grad_norm": 2.6691014766693115,
      "learning_rate": 3.172099915325995e-06,
      "loss": 0.8258,
      "step": 42360
    },
    {
      "epoch": 2.685066066732152,
      "grad_norm": 2.628750801086426,
      "learning_rate": 3.165749364944962e-06,
      "loss": 0.8788,
      "step": 42370
    },
    {
      "epoch": 2.6856998003739028,
      "grad_norm": 3.5455119609832764,
      "learning_rate": 3.159398814563929e-06,
      "loss": 0.83,
      "step": 42380
    },
    {
      "epoch": 2.6863335340156533,
      "grad_norm": 2.9687142372131348,
      "learning_rate": 3.153048264182896e-06,
      "loss": 0.8675,
      "step": 42390
    },
    {
      "epoch": 2.6869672676574035,
      "grad_norm": 2.807422161102295,
      "learning_rate": 3.1466977138018627e-06,
      "loss": 0.8115,
      "step": 42400
    },
    {
      "epoch": 2.687601001299154,
      "grad_norm": 2.386840581893921,
      "learning_rate": 3.1403471634208296e-06,
      "loss": 0.8349,
      "step": 42410
    },
    {
      "epoch": 2.688234734940904,
      "grad_norm": 2.9839348793029785,
      "learning_rate": 3.133996613039797e-06,
      "loss": 0.8523,
      "step": 42420
    },
    {
      "epoch": 2.6888684685826547,
      "grad_norm": 2.4985668659210205,
      "learning_rate": 3.1276460626587637e-06,
      "loss": 0.8377,
      "step": 42430
    },
    {
      "epoch": 2.6895022022244053,
      "grad_norm": 3.0886974334716797,
      "learning_rate": 3.121295512277731e-06,
      "loss": 0.8239,
      "step": 42440
    },
    {
      "epoch": 2.6901359358661554,
      "grad_norm": 2.5512542724609375,
      "learning_rate": 3.114944961896698e-06,
      "loss": 0.8317,
      "step": 42450
    },
    {
      "epoch": 2.690769669507906,
      "grad_norm": 2.5051944255828857,
      "learning_rate": 3.1085944115156647e-06,
      "loss": 0.8214,
      "step": 42460
    },
    {
      "epoch": 2.691403403149656,
      "grad_norm": 2.570065498352051,
      "learning_rate": 3.1022438611346315e-06,
      "loss": 0.8767,
      "step": 42470
    },
    {
      "epoch": 2.6920371367914067,
      "grad_norm": 2.5177762508392334,
      "learning_rate": 3.095893310753599e-06,
      "loss": 0.8052,
      "step": 42480
    },
    {
      "epoch": 2.692670870433157,
      "grad_norm": 2.3742454051971436,
      "learning_rate": 3.0895427603725657e-06,
      "loss": 0.8694,
      "step": 42490
    },
    {
      "epoch": 2.6933046040749073,
      "grad_norm": 2.6160595417022705,
      "learning_rate": 3.083192209991533e-06,
      "loss": 0.8337,
      "step": 42500
    },
    {
      "epoch": 2.6939383377166575,
      "grad_norm": 2.5356719493865967,
      "learning_rate": 3.0768416596105e-06,
      "loss": 0.8939,
      "step": 42510
    },
    {
      "epoch": 2.694572071358408,
      "grad_norm": 2.5395872592926025,
      "learning_rate": 3.0704911092294666e-06,
      "loss": 0.8558,
      "step": 42520
    },
    {
      "epoch": 2.6952058050001586,
      "grad_norm": 2.604363441467285,
      "learning_rate": 3.0641405588484335e-06,
      "loss": 0.8283,
      "step": 42530
    },
    {
      "epoch": 2.6958395386419087,
      "grad_norm": 3.2535390853881836,
      "learning_rate": 3.0577900084674003e-06,
      "loss": 0.8622,
      "step": 42540
    },
    {
      "epoch": 2.6964732722836593,
      "grad_norm": 2.786888837814331,
      "learning_rate": 3.0514394580863676e-06,
      "loss": 0.8256,
      "step": 42550
    },
    {
      "epoch": 2.6971070059254094,
      "grad_norm": 2.494231700897217,
      "learning_rate": 3.0450889077053345e-06,
      "loss": 0.8296,
      "step": 42560
    },
    {
      "epoch": 2.69774073956716,
      "grad_norm": 3.4981842041015625,
      "learning_rate": 3.0387383573243017e-06,
      "loss": 0.8327,
      "step": 42570
    },
    {
      "epoch": 2.6983744732089106,
      "grad_norm": 3.0783140659332275,
      "learning_rate": 3.0323878069432686e-06,
      "loss": 0.8728,
      "step": 42580
    },
    {
      "epoch": 2.6990082068506607,
      "grad_norm": 2.828517198562622,
      "learning_rate": 3.0260372565622354e-06,
      "loss": 0.8479,
      "step": 42590
    },
    {
      "epoch": 2.699641940492411,
      "grad_norm": 2.51509165763855,
      "learning_rate": 3.0196867061812023e-06,
      "loss": 0.8333,
      "step": 42600
    },
    {
      "epoch": 2.7002756741341614,
      "grad_norm": 2.7179102897644043,
      "learning_rate": 3.0133361558001696e-06,
      "loss": 0.8589,
      "step": 42610
    },
    {
      "epoch": 2.700909407775912,
      "grad_norm": 2.7667720317840576,
      "learning_rate": 3.0069856054191364e-06,
      "loss": 0.844,
      "step": 42620
    },
    {
      "epoch": 2.701543141417662,
      "grad_norm": 2.4842820167541504,
      "learning_rate": 3.0006350550381037e-06,
      "loss": 0.8522,
      "step": 42630
    },
    {
      "epoch": 2.7021768750594126,
      "grad_norm": 3.4810988903045654,
      "learning_rate": 2.9942845046570705e-06,
      "loss": 0.9043,
      "step": 42640
    },
    {
      "epoch": 2.7028106087011627,
      "grad_norm": 2.874061107635498,
      "learning_rate": 2.987933954276037e-06,
      "loss": 0.8782,
      "step": 42650
    },
    {
      "epoch": 2.7034443423429133,
      "grad_norm": 2.7440602779388428,
      "learning_rate": 2.9815834038950042e-06,
      "loss": 0.8508,
      "step": 42660
    },
    {
      "epoch": 2.704078075984664,
      "grad_norm": 3.0922062397003174,
      "learning_rate": 2.975232853513971e-06,
      "loss": 0.8401,
      "step": 42670
    },
    {
      "epoch": 2.704711809626414,
      "grad_norm": 2.532071113586426,
      "learning_rate": 2.9688823031329384e-06,
      "loss": 0.8522,
      "step": 42680
    },
    {
      "epoch": 2.705345543268164,
      "grad_norm": 3.1454529762268066,
      "learning_rate": 2.9625317527519052e-06,
      "loss": 0.8332,
      "step": 42690
    },
    {
      "epoch": 2.7059792769099147,
      "grad_norm": 3.0784754753112793,
      "learning_rate": 2.9561812023708725e-06,
      "loss": 0.8705,
      "step": 42700
    },
    {
      "epoch": 2.7066130105516653,
      "grad_norm": 2.968663215637207,
      "learning_rate": 2.949830651989839e-06,
      "loss": 0.858,
      "step": 42710
    },
    {
      "epoch": 2.7072467441934154,
      "grad_norm": 3.0908362865448,
      "learning_rate": 2.943480101608806e-06,
      "loss": 0.8505,
      "step": 42720
    },
    {
      "epoch": 2.707880477835166,
      "grad_norm": 3.9149320125579834,
      "learning_rate": 2.937129551227773e-06,
      "loss": 0.8452,
      "step": 42730
    },
    {
      "epoch": 2.708514211476916,
      "grad_norm": 2.8512463569641113,
      "learning_rate": 2.9307790008467403e-06,
      "loss": 0.8687,
      "step": 42740
    },
    {
      "epoch": 2.7091479451186666,
      "grad_norm": 2.423619270324707,
      "learning_rate": 2.924428450465707e-06,
      "loss": 0.8373,
      "step": 42750
    },
    {
      "epoch": 2.709781678760417,
      "grad_norm": 3.1170575618743896,
      "learning_rate": 2.9180779000846744e-06,
      "loss": 0.8802,
      "step": 42760
    },
    {
      "epoch": 2.7104154124021673,
      "grad_norm": 2.7729294300079346,
      "learning_rate": 2.911727349703641e-06,
      "loss": 0.857,
      "step": 42770
    },
    {
      "epoch": 2.711049146043918,
      "grad_norm": 2.5929503440856934,
      "learning_rate": 2.9053767993226077e-06,
      "loss": 0.8642,
      "step": 42780
    },
    {
      "epoch": 2.711682879685668,
      "grad_norm": 3.3897857666015625,
      "learning_rate": 2.899026248941575e-06,
      "loss": 0.876,
      "step": 42790
    },
    {
      "epoch": 2.7123166133274186,
      "grad_norm": 2.858595371246338,
      "learning_rate": 2.892675698560542e-06,
      "loss": 0.8533,
      "step": 42800
    },
    {
      "epoch": 2.7129503469691687,
      "grad_norm": 2.673072576522827,
      "learning_rate": 2.8869602032176123e-06,
      "loss": 0.8342,
      "step": 42810
    },
    {
      "epoch": 2.7135840806109193,
      "grad_norm": 2.525751829147339,
      "learning_rate": 2.880609652836579e-06,
      "loss": 0.8431,
      "step": 42820
    },
    {
      "epoch": 2.7142178142526694,
      "grad_norm": 3.083343982696533,
      "learning_rate": 2.8742591024555464e-06,
      "loss": 0.8606,
      "step": 42830
    },
    {
      "epoch": 2.71485154789442,
      "grad_norm": 2.585792064666748,
      "learning_rate": 2.8679085520745133e-06,
      "loss": 0.8579,
      "step": 42840
    },
    {
      "epoch": 2.7154852815361705,
      "grad_norm": 2.970215082168579,
      "learning_rate": 2.86155800169348e-06,
      "loss": 0.8638,
      "step": 42850
    },
    {
      "epoch": 2.7161190151779206,
      "grad_norm": 3.2591135501861572,
      "learning_rate": 2.855207451312447e-06,
      "loss": 0.8151,
      "step": 42860
    },
    {
      "epoch": 2.716752748819671,
      "grad_norm": 2.421322822570801,
      "learning_rate": 2.8488569009314142e-06,
      "loss": 0.8766,
      "step": 42870
    },
    {
      "epoch": 2.7173864824614213,
      "grad_norm": 2.8024685382843018,
      "learning_rate": 2.842506350550381e-06,
      "loss": 0.8545,
      "step": 42880
    },
    {
      "epoch": 2.718020216103172,
      "grad_norm": 3.0821924209594727,
      "learning_rate": 2.8361558001693484e-06,
      "loss": 0.8648,
      "step": 42890
    },
    {
      "epoch": 2.7186539497449225,
      "grad_norm": 2.8985581398010254,
      "learning_rate": 2.8298052497883152e-06,
      "loss": 0.8727,
      "step": 42900
    },
    {
      "epoch": 2.7192876833866726,
      "grad_norm": 2.6658098697662354,
      "learning_rate": 2.8234546994072817e-06,
      "loss": 0.8388,
      "step": 42910
    },
    {
      "epoch": 2.7199214170284227,
      "grad_norm": 2.8085498809814453,
      "learning_rate": 2.817104149026249e-06,
      "loss": 0.8648,
      "step": 42920
    },
    {
      "epoch": 2.7205551506701733,
      "grad_norm": 3.216850996017456,
      "learning_rate": 2.8107535986452158e-06,
      "loss": 0.8055,
      "step": 42930
    },
    {
      "epoch": 2.721188884311924,
      "grad_norm": 2.320643186569214,
      "learning_rate": 2.804403048264183e-06,
      "loss": 0.856,
      "step": 42940
    },
    {
      "epoch": 2.721822617953674,
      "grad_norm": 2.85740327835083,
      "learning_rate": 2.79805249788315e-06,
      "loss": 0.8489,
      "step": 42950
    },
    {
      "epoch": 2.7224563515954245,
      "grad_norm": 2.7986671924591064,
      "learning_rate": 2.791701947502117e-06,
      "loss": 0.8335,
      "step": 42960
    },
    {
      "epoch": 2.7230900852371747,
      "grad_norm": 2.6614134311676025,
      "learning_rate": 2.7853513971210836e-06,
      "loss": 0.8255,
      "step": 42970
    },
    {
      "epoch": 2.7237238188789252,
      "grad_norm": 2.8981704711914062,
      "learning_rate": 2.779000846740051e-06,
      "loss": 0.8983,
      "step": 42980
    },
    {
      "epoch": 2.724357552520676,
      "grad_norm": 2.826338052749634,
      "learning_rate": 2.7726502963590177e-06,
      "loss": 0.875,
      "step": 42990
    },
    {
      "epoch": 2.724991286162426,
      "grad_norm": 2.963724136352539,
      "learning_rate": 2.766299745977985e-06,
      "loss": 0.8639,
      "step": 43000
    },
    {
      "epoch": 2.725625019804176,
      "grad_norm": 2.5665781497955322,
      "learning_rate": 2.759949195596952e-06,
      "loss": 0.8669,
      "step": 43010
    },
    {
      "epoch": 2.7262587534459266,
      "grad_norm": 2.681715726852417,
      "learning_rate": 2.753598645215919e-06,
      "loss": 0.8472,
      "step": 43020
    },
    {
      "epoch": 2.726892487087677,
      "grad_norm": 3.5545616149902344,
      "learning_rate": 2.7472480948348855e-06,
      "loss": 0.8472,
      "step": 43030
    },
    {
      "epoch": 2.7275262207294273,
      "grad_norm": 2.8110315799713135,
      "learning_rate": 2.7408975444538524e-06,
      "loss": 0.8532,
      "step": 43040
    },
    {
      "epoch": 2.728159954371178,
      "grad_norm": 2.7099902629852295,
      "learning_rate": 2.7345469940728197e-06,
      "loss": 0.8411,
      "step": 43050
    },
    {
      "epoch": 2.728793688012928,
      "grad_norm": 2.7507214546203613,
      "learning_rate": 2.7281964436917865e-06,
      "loss": 0.8182,
      "step": 43060
    },
    {
      "epoch": 2.7294274216546786,
      "grad_norm": 2.7979838848114014,
      "learning_rate": 2.721845893310754e-06,
      "loss": 0.863,
      "step": 43070
    },
    {
      "epoch": 2.730061155296429,
      "grad_norm": 2.9840798377990723,
      "learning_rate": 2.7154953429297206e-06,
      "loss": 0.8544,
      "step": 43080
    },
    {
      "epoch": 2.7306948889381792,
      "grad_norm": 3.2300267219543457,
      "learning_rate": 2.709144792548688e-06,
      "loss": 0.8692,
      "step": 43090
    },
    {
      "epoch": 2.73132862257993,
      "grad_norm": 3.2028167247772217,
      "learning_rate": 2.7027942421676543e-06,
      "loss": 0.8542,
      "step": 43100
    },
    {
      "epoch": 2.73196235622168,
      "grad_norm": 2.6828651428222656,
      "learning_rate": 2.6964436917866216e-06,
      "loss": 0.8326,
      "step": 43110
    },
    {
      "epoch": 2.7325960898634305,
      "grad_norm": 2.914863109588623,
      "learning_rate": 2.6900931414055885e-06,
      "loss": 0.8273,
      "step": 43120
    },
    {
      "epoch": 2.7332298235051806,
      "grad_norm": 2.6447904109954834,
      "learning_rate": 2.6837425910245557e-06,
      "loss": 0.787,
      "step": 43130
    },
    {
      "epoch": 2.733863557146931,
      "grad_norm": 2.7546980381011963,
      "learning_rate": 2.6773920406435226e-06,
      "loss": 0.8577,
      "step": 43140
    },
    {
      "epoch": 2.7344972907886813,
      "grad_norm": 2.6715052127838135,
      "learning_rate": 2.67104149026249e-06,
      "loss": 0.8127,
      "step": 43150
    },
    {
      "epoch": 2.735131024430432,
      "grad_norm": 2.6251113414764404,
      "learning_rate": 2.6646909398814563e-06,
      "loss": 0.8431,
      "step": 43160
    },
    {
      "epoch": 2.7357647580721824,
      "grad_norm": 2.9794490337371826,
      "learning_rate": 2.658340389500423e-06,
      "loss": 0.8776,
      "step": 43170
    },
    {
      "epoch": 2.7363984917139326,
      "grad_norm": 2.884871006011963,
      "learning_rate": 2.6519898391193904e-06,
      "loss": 0.8247,
      "step": 43180
    },
    {
      "epoch": 2.737032225355683,
      "grad_norm": 3.072835683822632,
      "learning_rate": 2.6456392887383573e-06,
      "loss": 0.8566,
      "step": 43190
    },
    {
      "epoch": 2.7376659589974333,
      "grad_norm": 2.7857720851898193,
      "learning_rate": 2.6392887383573245e-06,
      "loss": 0.8356,
      "step": 43200
    },
    {
      "epoch": 2.738299692639184,
      "grad_norm": 2.6489531993865967,
      "learning_rate": 2.6329381879762914e-06,
      "loss": 0.8238,
      "step": 43210
    },
    {
      "epoch": 2.7389334262809344,
      "grad_norm": 3.6442084312438965,
      "learning_rate": 2.6265876375952582e-06,
      "loss": 0.8848,
      "step": 43220
    },
    {
      "epoch": 2.7395671599226845,
      "grad_norm": 2.686789035797119,
      "learning_rate": 2.620237087214225e-06,
      "loss": 0.8248,
      "step": 43230
    },
    {
      "epoch": 2.7402008935644346,
      "grad_norm": 3.0092411041259766,
      "learning_rate": 2.6138865368331924e-06,
      "loss": 0.859,
      "step": 43240
    },
    {
      "epoch": 2.740834627206185,
      "grad_norm": 2.9072177410125732,
      "learning_rate": 2.6075359864521592e-06,
      "loss": 0.8472,
      "step": 43250
    },
    {
      "epoch": 2.7414683608479358,
      "grad_norm": 2.5366737842559814,
      "learning_rate": 2.6011854360711265e-06,
      "loss": 0.8753,
      "step": 43260
    },
    {
      "epoch": 2.742102094489686,
      "grad_norm": 3.1879539489746094,
      "learning_rate": 2.5948348856900933e-06,
      "loss": 0.8773,
      "step": 43270
    },
    {
      "epoch": 2.7427358281314365,
      "grad_norm": 2.891400098800659,
      "learning_rate": 2.58848433530906e-06,
      "loss": 0.827,
      "step": 43280
    },
    {
      "epoch": 2.7433695617731866,
      "grad_norm": 3.398583173751831,
      "learning_rate": 2.582133784928027e-06,
      "loss": 0.8452,
      "step": 43290
    },
    {
      "epoch": 2.744003295414937,
      "grad_norm": 3.036820650100708,
      "learning_rate": 2.575783234546994e-06,
      "loss": 0.8578,
      "step": 43300
    },
    {
      "epoch": 2.7446370290566877,
      "grad_norm": 2.8962090015411377,
      "learning_rate": 2.569432684165961e-06,
      "loss": 0.8432,
      "step": 43310
    },
    {
      "epoch": 2.745270762698438,
      "grad_norm": 2.796205997467041,
      "learning_rate": 2.563082133784928e-06,
      "loss": 0.8241,
      "step": 43320
    },
    {
      "epoch": 2.745904496340188,
      "grad_norm": 3.687605619430542,
      "learning_rate": 2.5567315834038953e-06,
      "loss": 0.8907,
      "step": 43330
    },
    {
      "epoch": 2.7465382299819385,
      "grad_norm": 3.0547327995300293,
      "learning_rate": 2.5503810330228617e-06,
      "loss": 0.8311,
      "step": 43340
    },
    {
      "epoch": 2.747171963623689,
      "grad_norm": 2.699671983718872,
      "learning_rate": 2.544030482641829e-06,
      "loss": 0.8669,
      "step": 43350
    },
    {
      "epoch": 2.747805697265439,
      "grad_norm": 3.059216260910034,
      "learning_rate": 2.537679932260796e-06,
      "loss": 0.8581,
      "step": 43360
    },
    {
      "epoch": 2.74843943090719,
      "grad_norm": 2.756748676300049,
      "learning_rate": 2.531329381879763e-06,
      "loss": 0.8359,
      "step": 43370
    },
    {
      "epoch": 2.74907316454894,
      "grad_norm": 3.0842137336730957,
      "learning_rate": 2.52497883149873e-06,
      "loss": 0.8313,
      "step": 43380
    },
    {
      "epoch": 2.7497068981906905,
      "grad_norm": 3.026503562927246,
      "learning_rate": 2.5186282811176972e-06,
      "loss": 0.7951,
      "step": 43390
    },
    {
      "epoch": 2.750340631832441,
      "grad_norm": 2.45682954788208,
      "learning_rate": 2.5122777307366637e-06,
      "loss": 0.8387,
      "step": 43400
    },
    {
      "epoch": 2.750974365474191,
      "grad_norm": 2.6819796562194824,
      "learning_rate": 2.505927180355631e-06,
      "loss": 0.8711,
      "step": 43410
    },
    {
      "epoch": 2.7516080991159417,
      "grad_norm": 2.827767848968506,
      "learning_rate": 2.499576629974598e-06,
      "loss": 0.8775,
      "step": 43420
    },
    {
      "epoch": 2.752241832757692,
      "grad_norm": 2.5050060749053955,
      "learning_rate": 2.4932260795935646e-06,
      "loss": 0.8586,
      "step": 43430
    },
    {
      "epoch": 2.7528755663994424,
      "grad_norm": 2.1950392723083496,
      "learning_rate": 2.486875529212532e-06,
      "loss": 0.8067,
      "step": 43440
    },
    {
      "epoch": 2.7535093000411925,
      "grad_norm": 2.81184983253479,
      "learning_rate": 2.4805249788314988e-06,
      "loss": 0.8403,
      "step": 43450
    },
    {
      "epoch": 2.754143033682943,
      "grad_norm": 2.6173458099365234,
      "learning_rate": 2.4741744284504656e-06,
      "loss": 0.8343,
      "step": 43460
    },
    {
      "epoch": 2.7547767673246932,
      "grad_norm": 2.8167884349823,
      "learning_rate": 2.4678238780694325e-06,
      "loss": 0.8662,
      "step": 43470
    },
    {
      "epoch": 2.755410500966444,
      "grad_norm": 3.3392531871795654,
      "learning_rate": 2.4614733276883997e-06,
      "loss": 0.8475,
      "step": 43480
    },
    {
      "epoch": 2.7560442346081944,
      "grad_norm": 3.219956159591675,
      "learning_rate": 2.4551227773073666e-06,
      "loss": 0.8356,
      "step": 43490
    },
    {
      "epoch": 2.7566779682499445,
      "grad_norm": 2.5249252319335938,
      "learning_rate": 2.448772226926334e-06,
      "loss": 0.8122,
      "step": 43500
    },
    {
      "epoch": 2.757311701891695,
      "grad_norm": 2.7582101821899414,
      "learning_rate": 2.4424216765453007e-06,
      "loss": 0.8388,
      "step": 43510
    },
    {
      "epoch": 2.757945435533445,
      "grad_norm": 2.608989715576172,
      "learning_rate": 2.4360711261642676e-06,
      "loss": 0.8351,
      "step": 43520
    },
    {
      "epoch": 2.7585791691751957,
      "grad_norm": 3.0554492473602295,
      "learning_rate": 2.4297205757832344e-06,
      "loss": 0.8651,
      "step": 43530
    },
    {
      "epoch": 2.7592129028169463,
      "grad_norm": 3.419814109802246,
      "learning_rate": 2.4233700254022017e-06,
      "loss": 0.8336,
      "step": 43540
    },
    {
      "epoch": 2.7598466364586964,
      "grad_norm": 2.3266093730926514,
      "learning_rate": 2.4170194750211685e-06,
      "loss": 0.8112,
      "step": 43550
    },
    {
      "epoch": 2.7604803701004466,
      "grad_norm": 2.661130905151367,
      "learning_rate": 2.410668924640136e-06,
      "loss": 0.8233,
      "step": 43560
    },
    {
      "epoch": 2.761114103742197,
      "grad_norm": 2.4771618843078613,
      "learning_rate": 2.4043183742591027e-06,
      "loss": 0.8501,
      "step": 43570
    },
    {
      "epoch": 2.7617478373839477,
      "grad_norm": 2.8588690757751465,
      "learning_rate": 2.3979678238780695e-06,
      "loss": 0.8405,
      "step": 43580
    },
    {
      "epoch": 2.762381571025698,
      "grad_norm": 2.481048822402954,
      "learning_rate": 2.3916172734970364e-06,
      "loss": 0.8559,
      "step": 43590
    },
    {
      "epoch": 2.7630153046674484,
      "grad_norm": 2.4774563312530518,
      "learning_rate": 2.3852667231160032e-06,
      "loss": 0.8357,
      "step": 43600
    },
    {
      "epoch": 2.7636490383091985,
      "grad_norm": 2.5366439819335938,
      "learning_rate": 2.3789161727349705e-06,
      "loss": 0.8352,
      "step": 43610
    },
    {
      "epoch": 2.764282771950949,
      "grad_norm": 2.4438157081604004,
      "learning_rate": 2.3725656223539373e-06,
      "loss": 0.8157,
      "step": 43620
    },
    {
      "epoch": 2.7649165055926996,
      "grad_norm": 2.664863348007202,
      "learning_rate": 2.3662150719729046e-06,
      "loss": 0.861,
      "step": 43630
    },
    {
      "epoch": 2.7655502392344498,
      "grad_norm": 3.0082507133483887,
      "learning_rate": 2.3598645215918715e-06,
      "loss": 0.8252,
      "step": 43640
    },
    {
      "epoch": 2.7661839728762,
      "grad_norm": 3.1113951206207275,
      "learning_rate": 2.3535139712108383e-06,
      "loss": 0.8603,
      "step": 43650
    },
    {
      "epoch": 2.7668177065179504,
      "grad_norm": 2.927729606628418,
      "learning_rate": 2.347163420829805e-06,
      "loss": 0.849,
      "step": 43660
    },
    {
      "epoch": 2.767451440159701,
      "grad_norm": 2.576345205307007,
      "learning_rate": 2.3408128704487724e-06,
      "loss": 0.856,
      "step": 43670
    },
    {
      "epoch": 2.768085173801451,
      "grad_norm": 2.69169282913208,
      "learning_rate": 2.3344623200677393e-06,
      "loss": 0.8406,
      "step": 43680
    },
    {
      "epoch": 2.7687189074432017,
      "grad_norm": 3.220391273498535,
      "learning_rate": 2.3281117696867066e-06,
      "loss": 0.8707,
      "step": 43690
    },
    {
      "epoch": 2.769352641084952,
      "grad_norm": 3.119357109069824,
      "learning_rate": 2.3217612193056734e-06,
      "loss": 0.8536,
      "step": 43700
    },
    {
      "epoch": 2.7699863747267024,
      "grad_norm": 2.4672441482543945,
      "learning_rate": 2.31541066892464e-06,
      "loss": 0.8616,
      "step": 43710
    },
    {
      "epoch": 2.770620108368453,
      "grad_norm": 2.8395473957061768,
      "learning_rate": 2.309060118543607e-06,
      "loss": 0.8658,
      "step": 43720
    },
    {
      "epoch": 2.771253842010203,
      "grad_norm": 3.0126953125,
      "learning_rate": 2.302709568162574e-06,
      "loss": 0.8195,
      "step": 43730
    },
    {
      "epoch": 2.7718875756519537,
      "grad_norm": 2.5601882934570312,
      "learning_rate": 2.2963590177815412e-06,
      "loss": 0.8623,
      "step": 43740
    },
    {
      "epoch": 2.7725213092937038,
      "grad_norm": 2.6203174591064453,
      "learning_rate": 2.290008467400508e-06,
      "loss": 0.8496,
      "step": 43750
    },
    {
      "epoch": 2.7731550429354543,
      "grad_norm": 2.8988325595855713,
      "learning_rate": 2.2836579170194754e-06,
      "loss": 0.7715,
      "step": 43760
    },
    {
      "epoch": 2.7737887765772045,
      "grad_norm": 2.957658529281616,
      "learning_rate": 2.277307366638442e-06,
      "loss": 0.8546,
      "step": 43770
    },
    {
      "epoch": 2.774422510218955,
      "grad_norm": 2.5548501014709473,
      "learning_rate": 2.270956816257409e-06,
      "loss": 0.8703,
      "step": 43780
    },
    {
      "epoch": 2.775056243860705,
      "grad_norm": 2.8251099586486816,
      "learning_rate": 2.264606265876376e-06,
      "loss": 0.8771,
      "step": 43790
    },
    {
      "epoch": 2.7756899775024557,
      "grad_norm": 3.564741849899292,
      "learning_rate": 2.258255715495343e-06,
      "loss": 0.8654,
      "step": 43800
    },
    {
      "epoch": 2.7763237111442063,
      "grad_norm": 2.467503070831299,
      "learning_rate": 2.25190516511431e-06,
      "loss": 0.8416,
      "step": 43810
    },
    {
      "epoch": 2.7769574447859564,
      "grad_norm": 3.329017400741577,
      "learning_rate": 2.2455546147332773e-06,
      "loss": 0.8402,
      "step": 43820
    },
    {
      "epoch": 2.777591178427707,
      "grad_norm": 3.195568323135376,
      "learning_rate": 2.2392040643522437e-06,
      "loss": 0.8603,
      "step": 43830
    },
    {
      "epoch": 2.778224912069457,
      "grad_norm": 2.505110263824463,
      "learning_rate": 2.2328535139712106e-06,
      "loss": 0.8489,
      "step": 43840
    },
    {
      "epoch": 2.7788586457112077,
      "grad_norm": 2.506500005722046,
      "learning_rate": 2.226502963590178e-06,
      "loss": 0.8804,
      "step": 43850
    },
    {
      "epoch": 2.779492379352958,
      "grad_norm": 3.995828151702881,
      "learning_rate": 2.2201524132091447e-06,
      "loss": 0.8465,
      "step": 43860
    },
    {
      "epoch": 2.7801261129947084,
      "grad_norm": 2.308173418045044,
      "learning_rate": 2.213801862828112e-06,
      "loss": 0.8583,
      "step": 43870
    },
    {
      "epoch": 2.7807598466364585,
      "grad_norm": 2.8869223594665527,
      "learning_rate": 2.207451312447079e-06,
      "loss": 0.8591,
      "step": 43880
    },
    {
      "epoch": 2.781393580278209,
      "grad_norm": 2.646571397781372,
      "learning_rate": 2.2011007620660457e-06,
      "loss": 0.8752,
      "step": 43890
    },
    {
      "epoch": 2.7820273139199596,
      "grad_norm": 3.0873801708221436,
      "learning_rate": 2.1947502116850125e-06,
      "loss": 0.8804,
      "step": 43900
    },
    {
      "epoch": 2.7826610475617097,
      "grad_norm": 2.820861339569092,
      "learning_rate": 2.18839966130398e-06,
      "loss": 0.8404,
      "step": 43910
    },
    {
      "epoch": 2.7832947812034603,
      "grad_norm": 3.3297674655914307,
      "learning_rate": 2.1820491109229467e-06,
      "loss": 0.8123,
      "step": 43920
    },
    {
      "epoch": 2.7839285148452104,
      "grad_norm": 3.005509853363037,
      "learning_rate": 2.175698560541914e-06,
      "loss": 0.8565,
      "step": 43930
    },
    {
      "epoch": 2.784562248486961,
      "grad_norm": 2.634942054748535,
      "learning_rate": 2.169348010160881e-06,
      "loss": 0.8073,
      "step": 43940
    },
    {
      "epoch": 2.7851959821287116,
      "grad_norm": 3.4168879985809326,
      "learning_rate": 2.1629974597798476e-06,
      "loss": 0.8541,
      "step": 43950
    },
    {
      "epoch": 2.7858297157704617,
      "grad_norm": 2.331369161605835,
      "learning_rate": 2.1566469093988145e-06,
      "loss": 0.8607,
      "step": 43960
    },
    {
      "epoch": 2.786463449412212,
      "grad_norm": 2.8193023204803467,
      "learning_rate": 2.1502963590177813e-06,
      "loss": 0.839,
      "step": 43970
    },
    {
      "epoch": 2.7870971830539624,
      "grad_norm": 3.0526819229125977,
      "learning_rate": 2.1439458086367486e-06,
      "loss": 0.8772,
      "step": 43980
    },
    {
      "epoch": 2.787730916695713,
      "grad_norm": 2.8319478034973145,
      "learning_rate": 2.1375952582557155e-06,
      "loss": 0.8757,
      "step": 43990
    },
    {
      "epoch": 2.788364650337463,
      "grad_norm": 2.9123029708862305,
      "learning_rate": 2.1312447078746827e-06,
      "loss": 0.8448,
      "step": 44000
    },
    {
      "epoch": 2.7889983839792136,
      "grad_norm": 2.547610282897949,
      "learning_rate": 2.124894157493649e-06,
      "loss": 0.8586,
      "step": 44010
    },
    {
      "epoch": 2.7896321176209637,
      "grad_norm": 2.6014018058776855,
      "learning_rate": 2.1185436071126164e-06,
      "loss": 0.831,
      "step": 44020
    },
    {
      "epoch": 2.7902658512627143,
      "grad_norm": 3.23124098777771,
      "learning_rate": 2.1121930567315833e-06,
      "loss": 0.8444,
      "step": 44030
    },
    {
      "epoch": 2.790899584904465,
      "grad_norm": 3.0216360092163086,
      "learning_rate": 2.1058425063505506e-06,
      "loss": 0.8619,
      "step": 44040
    },
    {
      "epoch": 2.791533318546215,
      "grad_norm": 2.648266315460205,
      "learning_rate": 2.0994919559695174e-06,
      "loss": 0.8581,
      "step": 44050
    },
    {
      "epoch": 2.792167052187965,
      "grad_norm": 2.9650564193725586,
      "learning_rate": 2.0931414055884847e-06,
      "loss": 0.885,
      "step": 44060
    },
    {
      "epoch": 2.7928007858297157,
      "grad_norm": 3.0502490997314453,
      "learning_rate": 2.0867908552074515e-06,
      "loss": 0.8513,
      "step": 44070
    },
    {
      "epoch": 2.7934345194714663,
      "grad_norm": 3.020110845565796,
      "learning_rate": 2.0804403048264184e-06,
      "loss": 0.9209,
      "step": 44080
    },
    {
      "epoch": 2.7940682531132164,
      "grad_norm": 3.320706605911255,
      "learning_rate": 2.0740897544453852e-06,
      "loss": 0.8753,
      "step": 44090
    },
    {
      "epoch": 2.794701986754967,
      "grad_norm": 3.208019256591797,
      "learning_rate": 2.067739204064352e-06,
      "loss": 0.8721,
      "step": 44100
    },
    {
      "epoch": 2.795335720396717,
      "grad_norm": 2.4308724403381348,
      "learning_rate": 2.0613886536833194e-06,
      "loss": 0.8253,
      "step": 44110
    },
    {
      "epoch": 2.7959694540384676,
      "grad_norm": 2.5210213661193848,
      "learning_rate": 2.0550381033022862e-06,
      "loss": 0.8493,
      "step": 44120
    },
    {
      "epoch": 2.796603187680218,
      "grad_norm": 2.802359104156494,
      "learning_rate": 2.0486875529212535e-06,
      "loss": 0.8562,
      "step": 44130
    },
    {
      "epoch": 2.7972369213219683,
      "grad_norm": 2.5932984352111816,
      "learning_rate": 2.04233700254022e-06,
      "loss": 0.8295,
      "step": 44140
    },
    {
      "epoch": 2.797870654963719,
      "grad_norm": 2.831704616546631,
      "learning_rate": 2.035986452159187e-06,
      "loss": 0.7923,
      "step": 44150
    },
    {
      "epoch": 2.798504388605469,
      "grad_norm": 3.2195563316345215,
      "learning_rate": 2.029635901778154e-06,
      "loss": 0.8704,
      "step": 44160
    },
    {
      "epoch": 2.7991381222472196,
      "grad_norm": 2.902249813079834,
      "learning_rate": 2.0232853513971213e-06,
      "loss": 0.8457,
      "step": 44170
    },
    {
      "epoch": 2.7997718558889697,
      "grad_norm": 2.2679903507232666,
      "learning_rate": 2.016934801016088e-06,
      "loss": 0.8412,
      "step": 44180
    },
    {
      "epoch": 2.8004055895307203,
      "grad_norm": 2.6569647789001465,
      "learning_rate": 2.0105842506350554e-06,
      "loss": 0.8402,
      "step": 44190
    },
    {
      "epoch": 2.8010393231724704,
      "grad_norm": 2.9951252937316895,
      "learning_rate": 2.004233700254022e-06,
      "loss": 0.8441,
      "step": 44200
    },
    {
      "epoch": 2.801673056814221,
      "grad_norm": 3.086716413497925,
      "learning_rate": 1.997883149872989e-06,
      "loss": 0.8497,
      "step": 44210
    },
    {
      "epoch": 2.8023067904559715,
      "grad_norm": 2.746760368347168,
      "learning_rate": 1.991532599491956e-06,
      "loss": 0.871,
      "step": 44220
    },
    {
      "epoch": 2.8029405240977217,
      "grad_norm": 2.5495986938476562,
      "learning_rate": 1.985182049110923e-06,
      "loss": 0.8316,
      "step": 44230
    },
    {
      "epoch": 2.803574257739472,
      "grad_norm": 2.8293519020080566,
      "learning_rate": 1.97883149872989e-06,
      "loss": 0.8341,
      "step": 44240
    },
    {
      "epoch": 2.8042079913812223,
      "grad_norm": 2.7445435523986816,
      "learning_rate": 1.972480948348857e-06,
      "loss": 0.8195,
      "step": 44250
    },
    {
      "epoch": 2.804841725022973,
      "grad_norm": 2.809274435043335,
      "learning_rate": 1.966130397967824e-06,
      "loss": 0.8558,
      "step": 44260
    },
    {
      "epoch": 2.8054754586647235,
      "grad_norm": 3.102858304977417,
      "learning_rate": 1.9597798475867907e-06,
      "loss": 0.8653,
      "step": 44270
    },
    {
      "epoch": 2.8061091923064736,
      "grad_norm": 2.828329563140869,
      "learning_rate": 1.953429297205758e-06,
      "loss": 0.8395,
      "step": 44280
    },
    {
      "epoch": 2.8067429259482237,
      "grad_norm": 2.6139204502105713,
      "learning_rate": 1.947078746824725e-06,
      "loss": 0.8067,
      "step": 44290
    },
    {
      "epoch": 2.8073766595899743,
      "grad_norm": 2.6283087730407715,
      "learning_rate": 1.940728196443692e-06,
      "loss": 0.8796,
      "step": 44300
    },
    {
      "epoch": 2.808010393231725,
      "grad_norm": 2.5715365409851074,
      "learning_rate": 1.934377646062659e-06,
      "loss": 0.8608,
      "step": 44310
    },
    {
      "epoch": 2.808644126873475,
      "grad_norm": 2.9230709075927734,
      "learning_rate": 1.9280270956816258e-06,
      "loss": 0.8298,
      "step": 44320
    },
    {
      "epoch": 2.8092778605152255,
      "grad_norm": 2.6444497108459473,
      "learning_rate": 1.9216765453005926e-06,
      "loss": 0.8169,
      "step": 44330
    },
    {
      "epoch": 2.8099115941569757,
      "grad_norm": 2.9175615310668945,
      "learning_rate": 1.91532599491956e-06,
      "loss": 0.8641,
      "step": 44340
    },
    {
      "epoch": 2.8105453277987262,
      "grad_norm": 2.5395970344543457,
      "learning_rate": 1.9089754445385267e-06,
      "loss": 0.8679,
      "step": 44350
    },
    {
      "epoch": 2.811179061440477,
      "grad_norm": 2.604578971862793,
      "learning_rate": 1.9026248941574936e-06,
      "loss": 0.8177,
      "step": 44360
    },
    {
      "epoch": 2.811812795082227,
      "grad_norm": 3.1178905963897705,
      "learning_rate": 1.8962743437764607e-06,
      "loss": 0.8276,
      "step": 44370
    },
    {
      "epoch": 2.812446528723977,
      "grad_norm": 2.738837480545044,
      "learning_rate": 1.8899237933954275e-06,
      "loss": 0.84,
      "step": 44380
    },
    {
      "epoch": 2.8130802623657276,
      "grad_norm": 2.7313034534454346,
      "learning_rate": 1.8835732430143948e-06,
      "loss": 0.8421,
      "step": 44390
    },
    {
      "epoch": 2.813713996007478,
      "grad_norm": 2.737657308578491,
      "learning_rate": 1.8772226926333614e-06,
      "loss": 0.8766,
      "step": 44400
    },
    {
      "epoch": 2.8143477296492283,
      "grad_norm": 2.775763511657715,
      "learning_rate": 1.8708721422523287e-06,
      "loss": 0.8451,
      "step": 44410
    },
    {
      "epoch": 2.814981463290979,
      "grad_norm": 2.5750603675842285,
      "learning_rate": 1.8645215918712958e-06,
      "loss": 0.8626,
      "step": 44420
    },
    {
      "epoch": 2.815615196932729,
      "grad_norm": 3.1484367847442627,
      "learning_rate": 1.8581710414902624e-06,
      "loss": 0.854,
      "step": 44430
    },
    {
      "epoch": 2.8162489305744796,
      "grad_norm": 2.8172812461853027,
      "learning_rate": 1.8518204911092295e-06,
      "loss": 0.818,
      "step": 44440
    },
    {
      "epoch": 2.81688266421623,
      "grad_norm": 2.7511956691741943,
      "learning_rate": 1.8454699407281965e-06,
      "loss": 0.8606,
      "step": 44450
    },
    {
      "epoch": 2.8175163978579802,
      "grad_norm": 3.144153118133545,
      "learning_rate": 1.8391193903471634e-06,
      "loss": 0.8669,
      "step": 44460
    },
    {
      "epoch": 2.818150131499731,
      "grad_norm": 2.983449697494507,
      "learning_rate": 1.8327688399661304e-06,
      "loss": 0.8811,
      "step": 44470
    },
    {
      "epoch": 2.818783865141481,
      "grad_norm": 2.9638075828552246,
      "learning_rate": 1.8264182895850975e-06,
      "loss": 0.8596,
      "step": 44480
    },
    {
      "epoch": 2.8194175987832315,
      "grad_norm": 3.0051910877227783,
      "learning_rate": 1.8200677392040643e-06,
      "loss": 0.8259,
      "step": 44490
    },
    {
      "epoch": 2.8200513324249816,
      "grad_norm": 3.025891065597534,
      "learning_rate": 1.8137171888230314e-06,
      "loss": 0.9083,
      "step": 44500
    },
    {
      "epoch": 2.820685066066732,
      "grad_norm": 2.662964105606079,
      "learning_rate": 1.8073666384419985e-06,
      "loss": 0.84,
      "step": 44510
    },
    {
      "epoch": 2.8213187997084823,
      "grad_norm": 2.8747494220733643,
      "learning_rate": 1.8010160880609653e-06,
      "loss": 0.8145,
      "step": 44520
    },
    {
      "epoch": 2.821952533350233,
      "grad_norm": 3.0512936115264893,
      "learning_rate": 1.7946655376799324e-06,
      "loss": 0.8371,
      "step": 44530
    },
    {
      "epoch": 2.8225862669919835,
      "grad_norm": 2.742995023727417,
      "learning_rate": 1.7883149872988994e-06,
      "loss": 0.8321,
      "step": 44540
    },
    {
      "epoch": 2.8232200006337336,
      "grad_norm": 2.632800340652466,
      "learning_rate": 1.7819644369178663e-06,
      "loss": 0.8312,
      "step": 44550
    },
    {
      "epoch": 2.823853734275484,
      "grad_norm": 2.6724398136138916,
      "learning_rate": 1.7756138865368331e-06,
      "loss": 0.8516,
      "step": 44560
    },
    {
      "epoch": 2.8244874679172343,
      "grad_norm": 2.806551218032837,
      "learning_rate": 1.7692633361558002e-06,
      "loss": 0.8527,
      "step": 44570
    },
    {
      "epoch": 2.825121201558985,
      "grad_norm": 2.828425884246826,
      "learning_rate": 1.762912785774767e-06,
      "loss": 0.8307,
      "step": 44580
    },
    {
      "epoch": 2.8257549352007354,
      "grad_norm": 2.902355909347534,
      "learning_rate": 1.7565622353937341e-06,
      "loss": 0.861,
      "step": 44590
    },
    {
      "epoch": 2.8263886688424855,
      "grad_norm": 2.52494740486145,
      "learning_rate": 1.7502116850127012e-06,
      "loss": 0.8494,
      "step": 44600
    },
    {
      "epoch": 2.8270224024842356,
      "grad_norm": 3.0487968921661377,
      "learning_rate": 1.743861134631668e-06,
      "loss": 0.8766,
      "step": 44610
    },
    {
      "epoch": 2.827656136125986,
      "grad_norm": 2.6036717891693115,
      "learning_rate": 1.737510584250635e-06,
      "loss": 0.84,
      "step": 44620
    },
    {
      "epoch": 2.8282898697677368,
      "grad_norm": 3.158217668533325,
      "learning_rate": 1.7311600338696022e-06,
      "loss": 0.8611,
      "step": 44630
    },
    {
      "epoch": 2.828923603409487,
      "grad_norm": 2.9746575355529785,
      "learning_rate": 1.724809483488569e-06,
      "loss": 0.841,
      "step": 44640
    },
    {
      "epoch": 2.8295573370512375,
      "grad_norm": 2.6961746215820312,
      "learning_rate": 1.718458933107536e-06,
      "loss": 0.8191,
      "step": 44650
    },
    {
      "epoch": 2.8301910706929876,
      "grad_norm": 2.9523115158081055,
      "learning_rate": 1.7121083827265031e-06,
      "loss": 0.8512,
      "step": 44660
    },
    {
      "epoch": 2.830824804334738,
      "grad_norm": 3.6735284328460693,
      "learning_rate": 1.70575783234547e-06,
      "loss": 0.8757,
      "step": 44670
    },
    {
      "epoch": 2.8314585379764887,
      "grad_norm": 2.699333906173706,
      "learning_rate": 1.699407281964437e-06,
      "loss": 0.8123,
      "step": 44680
    },
    {
      "epoch": 2.832092271618239,
      "grad_norm": 3.0829248428344727,
      "learning_rate": 1.6930567315834039e-06,
      "loss": 0.8558,
      "step": 44690
    },
    {
      "epoch": 2.832726005259989,
      "grad_norm": 2.7122890949249268,
      "learning_rate": 1.6867061812023707e-06,
      "loss": 0.8195,
      "step": 44700
    },
    {
      "epoch": 2.8333597389017395,
      "grad_norm": 2.4622104167938232,
      "learning_rate": 1.6803556308213378e-06,
      "loss": 0.8463,
      "step": 44710
    },
    {
      "epoch": 2.83399347254349,
      "grad_norm": 3.097330093383789,
      "learning_rate": 1.6740050804403049e-06,
      "loss": 0.849,
      "step": 44720
    },
    {
      "epoch": 2.8346272061852402,
      "grad_norm": 2.6783385276794434,
      "learning_rate": 1.6676545300592717e-06,
      "loss": 0.8538,
      "step": 44730
    },
    {
      "epoch": 2.835260939826991,
      "grad_norm": 3.0377562046051025,
      "learning_rate": 1.6613039796782388e-06,
      "loss": 0.8636,
      "step": 44740
    },
    {
      "epoch": 2.835894673468741,
      "grad_norm": 3.3161752223968506,
      "learning_rate": 1.6549534292972058e-06,
      "loss": 0.8543,
      "step": 44750
    },
    {
      "epoch": 2.8365284071104915,
      "grad_norm": 2.613820791244507,
      "learning_rate": 1.6486028789161727e-06,
      "loss": 0.8684,
      "step": 44760
    },
    {
      "epoch": 2.837162140752242,
      "grad_norm": 2.8992300033569336,
      "learning_rate": 1.6422523285351398e-06,
      "loss": 0.8156,
      "step": 44770
    },
    {
      "epoch": 2.837795874393992,
      "grad_norm": 3.3960485458374023,
      "learning_rate": 1.6359017781541068e-06,
      "loss": 0.8255,
      "step": 44780
    },
    {
      "epoch": 2.8384296080357427,
      "grad_norm": 2.6414272785186768,
      "learning_rate": 1.6295512277730737e-06,
      "loss": 0.8528,
      "step": 44790
    },
    {
      "epoch": 2.839063341677493,
      "grad_norm": 2.8402352333068848,
      "learning_rate": 1.6232006773920407e-06,
      "loss": 0.8753,
      "step": 44800
    },
    {
      "epoch": 2.8396970753192434,
      "grad_norm": 2.745323657989502,
      "learning_rate": 1.6168501270110078e-06,
      "loss": 0.8523,
      "step": 44810
    },
    {
      "epoch": 2.8403308089609935,
      "grad_norm": 3.092233419418335,
      "learning_rate": 1.6104995766299746e-06,
      "loss": 0.8428,
      "step": 44820
    },
    {
      "epoch": 2.840964542602744,
      "grad_norm": 2.49971342086792,
      "learning_rate": 1.6041490262489415e-06,
      "loss": 0.8471,
      "step": 44830
    },
    {
      "epoch": 2.8415982762444942,
      "grad_norm": 2.839139223098755,
      "learning_rate": 1.5977984758679086e-06,
      "loss": 0.8591,
      "step": 44840
    },
    {
      "epoch": 2.842232009886245,
      "grad_norm": 2.8627376556396484,
      "learning_rate": 1.5914479254868756e-06,
      "loss": 0.8463,
      "step": 44850
    },
    {
      "epoch": 2.8428657435279954,
      "grad_norm": 2.7078118324279785,
      "learning_rate": 1.5850973751058425e-06,
      "loss": 0.854,
      "step": 44860
    },
    {
      "epoch": 2.8434994771697455,
      "grad_norm": 3.208611488342285,
      "learning_rate": 1.5787468247248095e-06,
      "loss": 0.8553,
      "step": 44870
    },
    {
      "epoch": 2.844133210811496,
      "grad_norm": 2.497361660003662,
      "learning_rate": 1.5723962743437766e-06,
      "loss": 0.917,
      "step": 44880
    },
    {
      "epoch": 2.844766944453246,
      "grad_norm": 2.7630538940429688,
      "learning_rate": 1.5660457239627434e-06,
      "loss": 0.8476,
      "step": 44890
    },
    {
      "epoch": 2.8454006780949967,
      "grad_norm": 2.84561824798584,
      "learning_rate": 1.5596951735817105e-06,
      "loss": 0.8354,
      "step": 44900
    },
    {
      "epoch": 2.8460344117367473,
      "grad_norm": 3.13228702545166,
      "learning_rate": 1.5533446232006776e-06,
      "loss": 0.8075,
      "step": 44910
    },
    {
      "epoch": 2.8466681453784974,
      "grad_norm": 3.144894599914551,
      "learning_rate": 1.5469940728196444e-06,
      "loss": 0.8436,
      "step": 44920
    },
    {
      "epoch": 2.8473018790202476,
      "grad_norm": 2.7837979793548584,
      "learning_rate": 1.5406435224386115e-06,
      "loss": 0.8657,
      "step": 44930
    },
    {
      "epoch": 2.847935612661998,
      "grad_norm": 2.696106195449829,
      "learning_rate": 1.5342929720575785e-06,
      "loss": 0.8532,
      "step": 44940
    },
    {
      "epoch": 2.8485693463037487,
      "grad_norm": 2.89276123046875,
      "learning_rate": 1.5279424216765454e-06,
      "loss": 0.8427,
      "step": 44950
    },
    {
      "epoch": 2.849203079945499,
      "grad_norm": 3.174062728881836,
      "learning_rate": 1.5215918712955122e-06,
      "loss": 0.8406,
      "step": 44960
    },
    {
      "epoch": 2.8498368135872494,
      "grad_norm": 2.943113088607788,
      "learning_rate": 1.5152413209144793e-06,
      "loss": 0.8361,
      "step": 44970
    },
    {
      "epoch": 2.8504705472289995,
      "grad_norm": 3.0741775035858154,
      "learning_rate": 1.5088907705334462e-06,
      "loss": 0.8717,
      "step": 44980
    },
    {
      "epoch": 2.85110428087075,
      "grad_norm": 2.756613254547119,
      "learning_rate": 1.5025402201524132e-06,
      "loss": 0.8113,
      "step": 44990
    },
    {
      "epoch": 2.8517380145125006,
      "grad_norm": 3.0950446128845215,
      "learning_rate": 1.4961896697713803e-06,
      "loss": 0.8774,
      "step": 45000
    },
    {
      "epoch": 2.8523717481542508,
      "grad_norm": 2.7287662029266357,
      "learning_rate": 1.4898391193903471e-06,
      "loss": 0.8817,
      "step": 45010
    },
    {
      "epoch": 2.853005481796001,
      "grad_norm": 2.885697603225708,
      "learning_rate": 1.4834885690093142e-06,
      "loss": 0.8632,
      "step": 45020
    },
    {
      "epoch": 2.8536392154377515,
      "grad_norm": 2.464651107788086,
      "learning_rate": 1.4771380186282812e-06,
      "loss": 0.8231,
      "step": 45030
    },
    {
      "epoch": 2.854272949079502,
      "grad_norm": 2.646409511566162,
      "learning_rate": 1.470787468247248e-06,
      "loss": 0.8102,
      "step": 45040
    },
    {
      "epoch": 2.854906682721252,
      "grad_norm": 3.320239543914795,
      "learning_rate": 1.4644369178662152e-06,
      "loss": 0.8605,
      "step": 45050
    },
    {
      "epoch": 2.8555404163630027,
      "grad_norm": 2.8456766605377197,
      "learning_rate": 1.4580863674851822e-06,
      "loss": 0.828,
      "step": 45060
    },
    {
      "epoch": 2.856174150004753,
      "grad_norm": 2.8592443466186523,
      "learning_rate": 1.451735817104149e-06,
      "loss": 0.8603,
      "step": 45070
    },
    {
      "epoch": 2.8568078836465034,
      "grad_norm": 3.4897913932800293,
      "learning_rate": 1.4453852667231161e-06,
      "loss": 0.8596,
      "step": 45080
    },
    {
      "epoch": 2.857441617288254,
      "grad_norm": 3.38356876373291,
      "learning_rate": 1.439034716342083e-06,
      "loss": 0.8293,
      "step": 45090
    },
    {
      "epoch": 2.858075350930004,
      "grad_norm": 3.1862387657165527,
      "learning_rate": 1.4326841659610498e-06,
      "loss": 0.8233,
      "step": 45100
    },
    {
      "epoch": 2.8587090845717547,
      "grad_norm": 2.6491949558258057,
      "learning_rate": 1.426333615580017e-06,
      "loss": 0.8807,
      "step": 45110
    },
    {
      "epoch": 2.8593428182135048,
      "grad_norm": 2.7106688022613525,
      "learning_rate": 1.419983065198984e-06,
      "loss": 0.8599,
      "step": 45120
    },
    {
      "epoch": 2.8599765518552553,
      "grad_norm": 2.7648165225982666,
      "learning_rate": 1.4136325148179508e-06,
      "loss": 0.8569,
      "step": 45130
    },
    {
      "epoch": 2.8606102854970055,
      "grad_norm": 2.7497060298919678,
      "learning_rate": 1.4072819644369179e-06,
      "loss": 0.8879,
      "step": 45140
    },
    {
      "epoch": 2.861244019138756,
      "grad_norm": 3.517719030380249,
      "learning_rate": 1.400931414055885e-06,
      "loss": 0.8459,
      "step": 45150
    },
    {
      "epoch": 2.861877752780506,
      "grad_norm": 3.1209280490875244,
      "learning_rate": 1.3945808636748518e-06,
      "loss": 0.8805,
      "step": 45160
    },
    {
      "epoch": 2.8625114864222567,
      "grad_norm": 2.5097548961639404,
      "learning_rate": 1.3882303132938188e-06,
      "loss": 0.8126,
      "step": 45170
    },
    {
      "epoch": 2.8631452200640073,
      "grad_norm": 2.933612108230591,
      "learning_rate": 1.381879762912786e-06,
      "loss": 0.8475,
      "step": 45180
    },
    {
      "epoch": 2.8637789537057574,
      "grad_norm": 2.630721092224121,
      "learning_rate": 1.3755292125317528e-06,
      "loss": 0.8314,
      "step": 45190
    },
    {
      "epoch": 2.864412687347508,
      "grad_norm": 2.995075225830078,
      "learning_rate": 1.3691786621507198e-06,
      "loss": 0.8509,
      "step": 45200
    },
    {
      "epoch": 2.865046420989258,
      "grad_norm": 2.6767237186431885,
      "learning_rate": 1.3628281117696869e-06,
      "loss": 0.8841,
      "step": 45210
    },
    {
      "epoch": 2.8656801546310087,
      "grad_norm": 2.9026076793670654,
      "learning_rate": 1.3564775613886535e-06,
      "loss": 0.8416,
      "step": 45220
    },
    {
      "epoch": 2.8663138882727592,
      "grad_norm": 3.1162021160125732,
      "learning_rate": 1.3501270110076206e-06,
      "loss": 0.897,
      "step": 45230
    },
    {
      "epoch": 2.8669476219145094,
      "grad_norm": 2.6967155933380127,
      "learning_rate": 1.3437764606265876e-06,
      "loss": 0.8392,
      "step": 45240
    },
    {
      "epoch": 2.8675813555562595,
      "grad_norm": 3.022716999053955,
      "learning_rate": 1.3374259102455545e-06,
      "loss": 0.8311,
      "step": 45250
    },
    {
      "epoch": 2.86821508919801,
      "grad_norm": 2.626854181289673,
      "learning_rate": 1.3310753598645216e-06,
      "loss": 0.8671,
      "step": 45260
    },
    {
      "epoch": 2.8688488228397606,
      "grad_norm": 3.017885684967041,
      "learning_rate": 1.3247248094834886e-06,
      "loss": 0.8455,
      "step": 45270
    },
    {
      "epoch": 2.8694825564815107,
      "grad_norm": 2.513137102127075,
      "learning_rate": 1.3183742591024555e-06,
      "loss": 0.8334,
      "step": 45280
    },
    {
      "epoch": 2.8701162901232613,
      "grad_norm": 2.4639344215393066,
      "learning_rate": 1.3120237087214225e-06,
      "loss": 0.8614,
      "step": 45290
    },
    {
      "epoch": 2.8707500237650114,
      "grad_norm": 2.9571189880371094,
      "learning_rate": 1.3056731583403896e-06,
      "loss": 0.7817,
      "step": 45300
    },
    {
      "epoch": 2.871383757406762,
      "grad_norm": 2.6103758811950684,
      "learning_rate": 1.2993226079593564e-06,
      "loss": 0.8691,
      "step": 45310
    },
    {
      "epoch": 2.8720174910485126,
      "grad_norm": 2.9471054077148438,
      "learning_rate": 1.2929720575783235e-06,
      "loss": 0.8671,
      "step": 45320
    },
    {
      "epoch": 2.8726512246902627,
      "grad_norm": 3.304462432861328,
      "learning_rate": 1.2866215071972906e-06,
      "loss": 0.8626,
      "step": 45330
    },
    {
      "epoch": 2.873284958332013,
      "grad_norm": 2.666170597076416,
      "learning_rate": 1.2802709568162574e-06,
      "loss": 0.8569,
      "step": 45340
    },
    {
      "epoch": 2.8739186919737634,
      "grad_norm": 2.9636175632476807,
      "learning_rate": 1.2739204064352245e-06,
      "loss": 0.8616,
      "step": 45350
    },
    {
      "epoch": 2.874552425615514,
      "grad_norm": 2.6517465114593506,
      "learning_rate": 1.2675698560541913e-06,
      "loss": 0.8239,
      "step": 45360
    },
    {
      "epoch": 2.875186159257264,
      "grad_norm": 2.9409852027893066,
      "learning_rate": 1.2612193056731584e-06,
      "loss": 0.8562,
      "step": 45370
    },
    {
      "epoch": 2.8758198928990146,
      "grad_norm": 2.6177544593811035,
      "learning_rate": 1.2548687552921252e-06,
      "loss": 0.8673,
      "step": 45380
    },
    {
      "epoch": 2.8764536265407648,
      "grad_norm": 2.4820282459259033,
      "learning_rate": 1.2485182049110923e-06,
      "loss": 0.8341,
      "step": 45390
    },
    {
      "epoch": 2.8770873601825153,
      "grad_norm": 3.362034320831299,
      "learning_rate": 1.2421676545300594e-06,
      "loss": 0.8381,
      "step": 45400
    },
    {
      "epoch": 2.877721093824266,
      "grad_norm": 3.109985828399658,
      "learning_rate": 1.2358171041490262e-06,
      "loss": 0.8537,
      "step": 45410
    },
    {
      "epoch": 2.878354827466016,
      "grad_norm": 2.8300795555114746,
      "learning_rate": 1.2294665537679933e-06,
      "loss": 0.786,
      "step": 45420
    },
    {
      "epoch": 2.878988561107766,
      "grad_norm": 2.8153910636901855,
      "learning_rate": 1.2231160033869603e-06,
      "loss": 0.8387,
      "step": 45430
    },
    {
      "epoch": 2.8796222947495167,
      "grad_norm": 3.149477481842041,
      "learning_rate": 1.2174005080440306e-06,
      "loss": 0.8648,
      "step": 45440
    },
    {
      "epoch": 2.8802560283912673,
      "grad_norm": 3.208317995071411,
      "learning_rate": 1.2110499576629977e-06,
      "loss": 0.8263,
      "step": 45450
    },
    {
      "epoch": 2.8808897620330174,
      "grad_norm": 2.716461658477783,
      "learning_rate": 1.2046994072819645e-06,
      "loss": 0.8497,
      "step": 45460
    },
    {
      "epoch": 2.881523495674768,
      "grad_norm": 3.067765474319458,
      "learning_rate": 1.1983488569009316e-06,
      "loss": 0.8552,
      "step": 45470
    },
    {
      "epoch": 2.882157229316518,
      "grad_norm": 2.766263961791992,
      "learning_rate": 1.1919983065198984e-06,
      "loss": 0.8645,
      "step": 45480
    },
    {
      "epoch": 2.8827909629582686,
      "grad_norm": 2.8957083225250244,
      "learning_rate": 1.1856477561388653e-06,
      "loss": 0.8641,
      "step": 45490
    },
    {
      "epoch": 2.883424696600019,
      "grad_norm": 2.806272029876709,
      "learning_rate": 1.1792972057578323e-06,
      "loss": 0.8731,
      "step": 45500
    },
    {
      "epoch": 2.8840584302417693,
      "grad_norm": 3.3685312271118164,
      "learning_rate": 1.1729466553767994e-06,
      "loss": 0.8411,
      "step": 45510
    },
    {
      "epoch": 2.88469216388352,
      "grad_norm": 2.9448232650756836,
      "learning_rate": 1.1665961049957662e-06,
      "loss": 0.8414,
      "step": 45520
    },
    {
      "epoch": 2.88532589752527,
      "grad_norm": 2.858128786087036,
      "learning_rate": 1.1602455546147333e-06,
      "loss": 0.8202,
      "step": 45530
    },
    {
      "epoch": 2.8859596311670206,
      "grad_norm": 3.1077239513397217,
      "learning_rate": 1.1538950042337004e-06,
      "loss": 0.8328,
      "step": 45540
    },
    {
      "epoch": 2.8865933648087707,
      "grad_norm": 2.720735788345337,
      "learning_rate": 1.1475444538526672e-06,
      "loss": 0.8481,
      "step": 45550
    },
    {
      "epoch": 2.8872270984505213,
      "grad_norm": 2.6278562545776367,
      "learning_rate": 1.1411939034716343e-06,
      "loss": 0.8352,
      "step": 45560
    },
    {
      "epoch": 2.8878608320922714,
      "grad_norm": 3.3468618392944336,
      "learning_rate": 1.1348433530906013e-06,
      "loss": 0.889,
      "step": 45570
    },
    {
      "epoch": 2.888494565734022,
      "grad_norm": 2.9256908893585205,
      "learning_rate": 1.1284928027095682e-06,
      "loss": 0.7901,
      "step": 45580
    },
    {
      "epoch": 2.8891282993757725,
      "grad_norm": 2.6403794288635254,
      "learning_rate": 1.1221422523285353e-06,
      "loss": 0.8859,
      "step": 45590
    },
    {
      "epoch": 2.8897620330175227,
      "grad_norm": 3.296971559524536,
      "learning_rate": 1.1157917019475023e-06,
      "loss": 0.8493,
      "step": 45600
    },
    {
      "epoch": 2.8903957666592732,
      "grad_norm": 2.510329008102417,
      "learning_rate": 1.109441151566469e-06,
      "loss": 0.8825,
      "step": 45610
    },
    {
      "epoch": 2.8910295003010233,
      "grad_norm": 2.8702030181884766,
      "learning_rate": 1.103090601185436e-06,
      "loss": 0.8617,
      "step": 45620
    },
    {
      "epoch": 2.891663233942774,
      "grad_norm": 2.900217056274414,
      "learning_rate": 1.096740050804403e-06,
      "loss": 0.8687,
      "step": 45630
    },
    {
      "epoch": 2.8922969675845245,
      "grad_norm": 2.840610980987549,
      "learning_rate": 1.09038950042337e-06,
      "loss": 0.8422,
      "step": 45640
    },
    {
      "epoch": 2.8929307012262746,
      "grad_norm": 3.004215717315674,
      "learning_rate": 1.084038950042337e-06,
      "loss": 0.8348,
      "step": 45650
    },
    {
      "epoch": 2.8935644348680247,
      "grad_norm": 3.2094101905822754,
      "learning_rate": 1.077688399661304e-06,
      "loss": 0.8384,
      "step": 45660
    },
    {
      "epoch": 2.8941981685097753,
      "grad_norm": 3.542473793029785,
      "learning_rate": 1.071337849280271e-06,
      "loss": 0.8634,
      "step": 45670
    },
    {
      "epoch": 2.894831902151526,
      "grad_norm": 2.522249698638916,
      "learning_rate": 1.064987298899238e-06,
      "loss": 0.8524,
      "step": 45680
    },
    {
      "epoch": 2.895465635793276,
      "grad_norm": 2.8694891929626465,
      "learning_rate": 1.058636748518205e-06,
      "loss": 0.8266,
      "step": 45690
    },
    {
      "epoch": 2.8960993694350265,
      "grad_norm": 2.9655256271362305,
      "learning_rate": 1.0522861981371719e-06,
      "loss": 0.8457,
      "step": 45700
    },
    {
      "epoch": 2.8967331030767767,
      "grad_norm": 2.4193620681762695,
      "learning_rate": 1.045935647756139e-06,
      "loss": 0.8347,
      "step": 45710
    },
    {
      "epoch": 2.8973668367185272,
      "grad_norm": 2.785855770111084,
      "learning_rate": 1.039585097375106e-06,
      "loss": 0.8298,
      "step": 45720
    },
    {
      "epoch": 2.898000570360278,
      "grad_norm": 2.6779849529266357,
      "learning_rate": 1.0332345469940729e-06,
      "loss": 0.8769,
      "step": 45730
    },
    {
      "epoch": 2.898634304002028,
      "grad_norm": 2.8085341453552246,
      "learning_rate": 1.0268839966130397e-06,
      "loss": 0.8519,
      "step": 45740
    },
    {
      "epoch": 2.899268037643778,
      "grad_norm": 2.4297080039978027,
      "learning_rate": 1.0205334462320068e-06,
      "loss": 0.7991,
      "step": 45750
    },
    {
      "epoch": 2.8999017712855286,
      "grad_norm": 2.627711057662964,
      "learning_rate": 1.0141828958509736e-06,
      "loss": 0.8435,
      "step": 45760
    },
    {
      "epoch": 2.900535504927279,
      "grad_norm": 2.8460073471069336,
      "learning_rate": 1.0078323454699407e-06,
      "loss": 0.8361,
      "step": 45770
    },
    {
      "epoch": 2.9011692385690293,
      "grad_norm": 2.879467248916626,
      "learning_rate": 1.0014817950889077e-06,
      "loss": 0.8559,
      "step": 45780
    },
    {
      "epoch": 2.90180297221078,
      "grad_norm": 2.935006618499756,
      "learning_rate": 9.951312447078746e-07,
      "loss": 0.8579,
      "step": 45790
    },
    {
      "epoch": 2.90243670585253,
      "grad_norm": 3.207252264022827,
      "learning_rate": 9.887806943268417e-07,
      "loss": 0.8461,
      "step": 45800
    },
    {
      "epoch": 2.9030704394942806,
      "grad_norm": 2.4501476287841797,
      "learning_rate": 9.824301439458087e-07,
      "loss": 0.8469,
      "step": 45810
    },
    {
      "epoch": 2.903704173136031,
      "grad_norm": 2.6607353687286377,
      "learning_rate": 9.760795935647756e-07,
      "loss": 0.8227,
      "step": 45820
    },
    {
      "epoch": 2.9043379067777813,
      "grad_norm": 2.5625505447387695,
      "learning_rate": 9.697290431837426e-07,
      "loss": 0.8582,
      "step": 45830
    },
    {
      "epoch": 2.904971640419532,
      "grad_norm": 2.736812114715576,
      "learning_rate": 9.633784928027097e-07,
      "loss": 0.8732,
      "step": 45840
    },
    {
      "epoch": 2.905605374061282,
      "grad_norm": 2.7436389923095703,
      "learning_rate": 9.570279424216765e-07,
      "loss": 0.8299,
      "step": 45850
    },
    {
      "epoch": 2.9062391077030325,
      "grad_norm": 2.659606695175171,
      "learning_rate": 9.506773920406436e-07,
      "loss": 0.8289,
      "step": 45860
    },
    {
      "epoch": 2.9068728413447826,
      "grad_norm": 2.7041707038879395,
      "learning_rate": 9.443268416596107e-07,
      "loss": 0.8366,
      "step": 45870
    },
    {
      "epoch": 2.907506574986533,
      "grad_norm": 2.9213008880615234,
      "learning_rate": 9.379762912785774e-07,
      "loss": 0.865,
      "step": 45880
    },
    {
      "epoch": 2.9081403086282833,
      "grad_norm": 2.8087501525878906,
      "learning_rate": 9.316257408975445e-07,
      "loss": 0.8743,
      "step": 45890
    },
    {
      "epoch": 2.908774042270034,
      "grad_norm": 2.85392165184021,
      "learning_rate": 9.252751905165115e-07,
      "loss": 0.8591,
      "step": 45900
    },
    {
      "epoch": 2.9094077759117845,
      "grad_norm": 2.8534278869628906,
      "learning_rate": 9.189246401354784e-07,
      "loss": 0.8428,
      "step": 45910
    },
    {
      "epoch": 2.9100415095535346,
      "grad_norm": 2.9126763343811035,
      "learning_rate": 9.125740897544453e-07,
      "loss": 0.8549,
      "step": 45920
    },
    {
      "epoch": 2.910675243195285,
      "grad_norm": 2.8033926486968994,
      "learning_rate": 9.062235393734124e-07,
      "loss": 0.8467,
      "step": 45930
    },
    {
      "epoch": 2.9113089768370353,
      "grad_norm": 2.663816452026367,
      "learning_rate": 8.998729889923794e-07,
      "loss": 0.8595,
      "step": 45940
    },
    {
      "epoch": 2.911942710478786,
      "grad_norm": 2.704450845718384,
      "learning_rate": 8.935224386113463e-07,
      "loss": 0.8342,
      "step": 45950
    },
    {
      "epoch": 2.9125764441205364,
      "grad_norm": 3.089927911758423,
      "learning_rate": 8.871718882303134e-07,
      "loss": 0.8707,
      "step": 45960
    },
    {
      "epoch": 2.9132101777622865,
      "grad_norm": 2.7851696014404297,
      "learning_rate": 8.808213378492803e-07,
      "loss": 0.8924,
      "step": 45970
    },
    {
      "epoch": 2.9138439114040366,
      "grad_norm": 3.045440912246704,
      "learning_rate": 8.744707874682472e-07,
      "loss": 0.8616,
      "step": 45980
    },
    {
      "epoch": 2.914477645045787,
      "grad_norm": 2.8979146480560303,
      "learning_rate": 8.681202370872143e-07,
      "loss": 0.8329,
      "step": 45990
    },
    {
      "epoch": 2.915111378687538,
      "grad_norm": 2.697223424911499,
      "learning_rate": 8.617696867061812e-07,
      "loss": 0.8648,
      "step": 46000
    },
    {
      "epoch": 2.915745112329288,
      "grad_norm": 2.838048219680786,
      "learning_rate": 8.554191363251482e-07,
      "loss": 0.813,
      "step": 46010
    },
    {
      "epoch": 2.9163788459710385,
      "grad_norm": 2.865135669708252,
      "learning_rate": 8.490685859441152e-07,
      "loss": 0.8033,
      "step": 46020
    },
    {
      "epoch": 2.9170125796127886,
      "grad_norm": 2.9298858642578125,
      "learning_rate": 8.427180355630822e-07,
      "loss": 0.8439,
      "step": 46030
    },
    {
      "epoch": 2.917646313254539,
      "grad_norm": 2.818157196044922,
      "learning_rate": 8.36367485182049e-07,
      "loss": 0.8511,
      "step": 46040
    },
    {
      "epoch": 2.9182800468962897,
      "grad_norm": 2.4521634578704834,
      "learning_rate": 8.300169348010161e-07,
      "loss": 0.8316,
      "step": 46050
    },
    {
      "epoch": 2.91891378053804,
      "grad_norm": 3.2254955768585205,
      "learning_rate": 8.23666384419983e-07,
      "loss": 0.8669,
      "step": 46060
    },
    {
      "epoch": 2.91954751417979,
      "grad_norm": 2.7669200897216797,
      "learning_rate": 8.173158340389501e-07,
      "loss": 0.8544,
      "step": 46070
    },
    {
      "epoch": 2.9201812478215405,
      "grad_norm": 3.1027987003326416,
      "learning_rate": 8.109652836579171e-07,
      "loss": 0.8134,
      "step": 46080
    },
    {
      "epoch": 2.920814981463291,
      "grad_norm": 2.5815157890319824,
      "learning_rate": 8.04614733276884e-07,
      "loss": 0.8439,
      "step": 46090
    },
    {
      "epoch": 2.9214487151050412,
      "grad_norm": 2.963738203048706,
      "learning_rate": 7.982641828958511e-07,
      "loss": 0.8489,
      "step": 46100
    },
    {
      "epoch": 2.922082448746792,
      "grad_norm": 2.8458497524261475,
      "learning_rate": 7.919136325148179e-07,
      "loss": 0.8347,
      "step": 46110
    },
    {
      "epoch": 2.922716182388542,
      "grad_norm": 3.1943588256835938,
      "learning_rate": 7.855630821337849e-07,
      "loss": 0.8454,
      "step": 46120
    },
    {
      "epoch": 2.9233499160302925,
      "grad_norm": 2.890021800994873,
      "learning_rate": 7.79212531752752e-07,
      "loss": 0.8301,
      "step": 46130
    },
    {
      "epoch": 2.923983649672043,
      "grad_norm": 2.805877447128296,
      "learning_rate": 7.728619813717189e-07,
      "loss": 0.837,
      "step": 46140
    },
    {
      "epoch": 2.924617383313793,
      "grad_norm": 2.876908540725708,
      "learning_rate": 7.665114309906859e-07,
      "loss": 0.8384,
      "step": 46150
    },
    {
      "epoch": 2.9252511169555437,
      "grad_norm": 2.536625385284424,
      "learning_rate": 7.601608806096529e-07,
      "loss": 0.862,
      "step": 46160
    },
    {
      "epoch": 2.925884850597294,
      "grad_norm": 3.3264901638031006,
      "learning_rate": 7.538103302286198e-07,
      "loss": 0.8386,
      "step": 46170
    },
    {
      "epoch": 2.9265185842390444,
      "grad_norm": 2.5751137733459473,
      "learning_rate": 7.474597798475867e-07,
      "loss": 0.8313,
      "step": 46180
    },
    {
      "epoch": 2.9271523178807946,
      "grad_norm": 2.8561649322509766,
      "learning_rate": 7.411092294665538e-07,
      "loss": 0.8639,
      "step": 46190
    },
    {
      "epoch": 2.927786051522545,
      "grad_norm": 3.0654683113098145,
      "learning_rate": 7.347586790855208e-07,
      "loss": 0.8189,
      "step": 46200
    },
    {
      "epoch": 2.9284197851642952,
      "grad_norm": 2.681244373321533,
      "learning_rate": 7.284081287044877e-07,
      "loss": 0.8652,
      "step": 46210
    },
    {
      "epoch": 2.929053518806046,
      "grad_norm": 3.1727640628814697,
      "learning_rate": 7.220575783234548e-07,
      "loss": 0.8743,
      "step": 46220
    },
    {
      "epoch": 2.9296872524477964,
      "grad_norm": 2.600116014480591,
      "learning_rate": 7.157070279424217e-07,
      "loss": 0.8564,
      "step": 46230
    },
    {
      "epoch": 2.9303209860895465,
      "grad_norm": 2.7042036056518555,
      "learning_rate": 7.093564775613886e-07,
      "loss": 0.8442,
      "step": 46240
    },
    {
      "epoch": 2.930954719731297,
      "grad_norm": 3.324641227722168,
      "learning_rate": 7.030059271803556e-07,
      "loss": 0.8761,
      "step": 46250
    },
    {
      "epoch": 2.931588453373047,
      "grad_norm": 3.2773983478546143,
      "learning_rate": 6.966553767993226e-07,
      "loss": 0.8341,
      "step": 46260
    },
    {
      "epoch": 2.9322221870147978,
      "grad_norm": 3.176007032394409,
      "learning_rate": 6.903048264182896e-07,
      "loss": 0.8742,
      "step": 46270
    },
    {
      "epoch": 2.9328559206565483,
      "grad_norm": 2.8535423278808594,
      "learning_rate": 6.839542760372566e-07,
      "loss": 0.8589,
      "step": 46280
    },
    {
      "epoch": 2.9334896542982984,
      "grad_norm": 2.5570709705352783,
      "learning_rate": 6.776037256562236e-07,
      "loss": 0.8443,
      "step": 46290
    },
    {
      "epoch": 2.9341233879400486,
      "grad_norm": 2.6890625953674316,
      "learning_rate": 6.712531752751905e-07,
      "loss": 0.8399,
      "step": 46300
    },
    {
      "epoch": 2.934757121581799,
      "grad_norm": 3.203068494796753,
      "learning_rate": 6.649026248941575e-07,
      "loss": 0.8396,
      "step": 46310
    },
    {
      "epoch": 2.9353908552235497,
      "grad_norm": 3.036547899246216,
      "learning_rate": 6.585520745131244e-07,
      "loss": 0.8304,
      "step": 46320
    },
    {
      "epoch": 2.9360245888653,
      "grad_norm": 3.1156694889068604,
      "learning_rate": 6.522015241320915e-07,
      "loss": 0.8958,
      "step": 46330
    },
    {
      "epoch": 2.9366583225070504,
      "grad_norm": 3.033546209335327,
      "learning_rate": 6.458509737510585e-07,
      "loss": 0.8593,
      "step": 46340
    },
    {
      "epoch": 2.9372920561488005,
      "grad_norm": 2.7855942249298096,
      "learning_rate": 6.395004233700254e-07,
      "loss": 0.8581,
      "step": 46350
    },
    {
      "epoch": 2.937925789790551,
      "grad_norm": 2.7770352363586426,
      "learning_rate": 6.331498729889925e-07,
      "loss": 0.8329,
      "step": 46360
    },
    {
      "epoch": 2.9385595234323016,
      "grad_norm": 3.2011823654174805,
      "learning_rate": 6.267993226079593e-07,
      "loss": 0.8152,
      "step": 46370
    },
    {
      "epoch": 2.9391932570740518,
      "grad_norm": 3.260265827178955,
      "learning_rate": 6.204487722269263e-07,
      "loss": 0.8745,
      "step": 46380
    },
    {
      "epoch": 2.939826990715802,
      "grad_norm": 2.6345181465148926,
      "learning_rate": 6.140982218458933e-07,
      "loss": 0.8162,
      "step": 46390
    },
    {
      "epoch": 2.9404607243575525,
      "grad_norm": 2.3409996032714844,
      "learning_rate": 6.077476714648603e-07,
      "loss": 0.837,
      "step": 46400
    },
    {
      "epoch": 2.941094457999303,
      "grad_norm": 2.3997907638549805,
      "learning_rate": 6.013971210838273e-07,
      "loss": 0.8736,
      "step": 46410
    },
    {
      "epoch": 2.941728191641053,
      "grad_norm": 3.067079544067383,
      "learning_rate": 5.950465707027943e-07,
      "loss": 0.8609,
      "step": 46420
    },
    {
      "epoch": 2.9423619252828037,
      "grad_norm": 2.883216619491577,
      "learning_rate": 5.886960203217613e-07,
      "loss": 0.8638,
      "step": 46430
    },
    {
      "epoch": 2.942995658924554,
      "grad_norm": 2.4917449951171875,
      "learning_rate": 5.823454699407281e-07,
      "loss": 0.8165,
      "step": 46440
    },
    {
      "epoch": 2.9436293925663044,
      "grad_norm": 2.498180389404297,
      "learning_rate": 5.759949195596952e-07,
      "loss": 0.8272,
      "step": 46450
    },
    {
      "epoch": 2.944263126208055,
      "grad_norm": 2.66601300239563,
      "learning_rate": 5.696443691786621e-07,
      "loss": 0.8882,
      "step": 46460
    },
    {
      "epoch": 2.944896859849805,
      "grad_norm": 3.0624632835388184,
      "learning_rate": 5.632938187976291e-07,
      "loss": 0.8158,
      "step": 46470
    },
    {
      "epoch": 2.9455305934915557,
      "grad_norm": 2.5882930755615234,
      "learning_rate": 5.569432684165962e-07,
      "loss": 0.8647,
      "step": 46480
    },
    {
      "epoch": 2.946164327133306,
      "grad_norm": 2.699270725250244,
      "learning_rate": 5.505927180355631e-07,
      "loss": 0.8667,
      "step": 46490
    },
    {
      "epoch": 2.9467980607750563,
      "grad_norm": 2.7507622241973877,
      "learning_rate": 5.442421676545301e-07,
      "loss": 0.8421,
      "step": 46500
    },
    {
      "epoch": 2.9474317944168065,
      "grad_norm": 2.9945220947265625,
      "learning_rate": 5.37891617273497e-07,
      "loss": 0.8369,
      "step": 46510
    },
    {
      "epoch": 2.948065528058557,
      "grad_norm": 2.457679271697998,
      "learning_rate": 5.31541066892464e-07,
      "loss": 0.7657,
      "step": 46520
    },
    {
      "epoch": 2.948699261700307,
      "grad_norm": 2.9673330783843994,
      "learning_rate": 5.25190516511431e-07,
      "loss": 0.8179,
      "step": 46530
    },
    {
      "epoch": 2.9493329953420577,
      "grad_norm": 2.5875906944274902,
      "learning_rate": 5.18839966130398e-07,
      "loss": 0.8794,
      "step": 46540
    },
    {
      "epoch": 2.9499667289838083,
      "grad_norm": 3.535942792892456,
      "learning_rate": 5.12489415749365e-07,
      "loss": 0.8336,
      "step": 46550
    },
    {
      "epoch": 2.9506004626255584,
      "grad_norm": 2.589657783508301,
      "learning_rate": 5.061388653683319e-07,
      "loss": 0.7926,
      "step": 46560
    },
    {
      "epoch": 2.951234196267309,
      "grad_norm": 2.4184327125549316,
      "learning_rate": 4.997883149872989e-07,
      "loss": 0.8968,
      "step": 46570
    },
    {
      "epoch": 2.951867929909059,
      "grad_norm": 2.8268959522247314,
      "learning_rate": 4.934377646062658e-07,
      "loss": 0.9036,
      "step": 46580
    },
    {
      "epoch": 2.9525016635508097,
      "grad_norm": 2.5253589153289795,
      "learning_rate": 4.870872142252329e-07,
      "loss": 0.8623,
      "step": 46590
    },
    {
      "epoch": 2.9531353971925602,
      "grad_norm": 2.317197799682617,
      "learning_rate": 4.807366638441999e-07,
      "loss": 0.8533,
      "step": 46600
    },
    {
      "epoch": 2.9537691308343104,
      "grad_norm": 2.7128708362579346,
      "learning_rate": 4.743861134631668e-07,
      "loss": 0.8682,
      "step": 46610
    },
    {
      "epoch": 2.9544028644760605,
      "grad_norm": 2.8307952880859375,
      "learning_rate": 4.6803556308213377e-07,
      "loss": 0.868,
      "step": 46620
    },
    {
      "epoch": 2.955036598117811,
      "grad_norm": 2.7063848972320557,
      "learning_rate": 4.616850127011008e-07,
      "loss": 0.7993,
      "step": 46630
    },
    {
      "epoch": 2.9556703317595616,
      "grad_norm": 2.9383716583251953,
      "learning_rate": 4.553344623200678e-07,
      "loss": 0.8381,
      "step": 46640
    },
    {
      "epoch": 2.9563040654013117,
      "grad_norm": 2.6528522968292236,
      "learning_rate": 4.4898391193903474e-07,
      "loss": 0.8116,
      "step": 46650
    },
    {
      "epoch": 2.9569377990430623,
      "grad_norm": 2.5851240158081055,
      "learning_rate": 4.426333615580017e-07,
      "loss": 0.8411,
      "step": 46660
    },
    {
      "epoch": 2.9575715326848124,
      "grad_norm": 2.7288260459899902,
      "learning_rate": 4.362828111769687e-07,
      "loss": 0.8837,
      "step": 46670
    },
    {
      "epoch": 2.958205266326563,
      "grad_norm": 3.2861361503601074,
      "learning_rate": 4.2993226079593566e-07,
      "loss": 0.8604,
      "step": 46680
    },
    {
      "epoch": 2.9588389999683136,
      "grad_norm": 2.9087398052215576,
      "learning_rate": 4.235817104149026e-07,
      "loss": 0.8323,
      "step": 46690
    },
    {
      "epoch": 2.9594727336100637,
      "grad_norm": 2.9460971355438232,
      "learning_rate": 4.1723116003386963e-07,
      "loss": 0.8534,
      "step": 46700
    },
    {
      "epoch": 2.960106467251814,
      "grad_norm": 2.7008864879608154,
      "learning_rate": 4.108806096528366e-07,
      "loss": 0.8352,
      "step": 46710
    },
    {
      "epoch": 2.9607402008935644,
      "grad_norm": 3.410799026489258,
      "learning_rate": 4.0453005927180354e-07,
      "loss": 0.8843,
      "step": 46720
    },
    {
      "epoch": 2.961373934535315,
      "grad_norm": 2.9412803649902344,
      "learning_rate": 3.9817950889077055e-07,
      "loss": 0.8532,
      "step": 46730
    },
    {
      "epoch": 2.962007668177065,
      "grad_norm": 3.5819008350372314,
      "learning_rate": 3.918289585097375e-07,
      "loss": 0.8573,
      "step": 46740
    },
    {
      "epoch": 2.9626414018188156,
      "grad_norm": 2.538530111312866,
      "learning_rate": 3.8547840812870446e-07,
      "loss": 0.8431,
      "step": 46750
    },
    {
      "epoch": 2.9632751354605658,
      "grad_norm": 2.622143507003784,
      "learning_rate": 3.7912785774767147e-07,
      "loss": 0.8481,
      "step": 46760
    },
    {
      "epoch": 2.9639088691023163,
      "grad_norm": 2.7881829738616943,
      "learning_rate": 3.727773073666385e-07,
      "loss": 0.8962,
      "step": 46770
    },
    {
      "epoch": 2.964542602744067,
      "grad_norm": 2.6315183639526367,
      "learning_rate": 3.6642675698560544e-07,
      "loss": 0.8241,
      "step": 46780
    },
    {
      "epoch": 2.965176336385817,
      "grad_norm": 3.170604705810547,
      "learning_rate": 3.600762066045724e-07,
      "loss": 0.8301,
      "step": 46790
    },
    {
      "epoch": 2.9658100700275676,
      "grad_norm": 2.65863037109375,
      "learning_rate": 3.537256562235394e-07,
      "loss": 0.8443,
      "step": 46800
    },
    {
      "epoch": 2.9664438036693177,
      "grad_norm": 2.894221782684326,
      "learning_rate": 3.4737510584250636e-07,
      "loss": 0.8271,
      "step": 46810
    },
    {
      "epoch": 2.9670775373110683,
      "grad_norm": 2.524667978286743,
      "learning_rate": 3.410245554614733e-07,
      "loss": 0.8449,
      "step": 46820
    },
    {
      "epoch": 2.9677112709528184,
      "grad_norm": 3.1195836067199707,
      "learning_rate": 3.346740050804403e-07,
      "loss": 0.8558,
      "step": 46830
    },
    {
      "epoch": 2.968345004594569,
      "grad_norm": 2.8123817443847656,
      "learning_rate": 3.283234546994073e-07,
      "loss": 0.8302,
      "step": 46840
    },
    {
      "epoch": 2.968978738236319,
      "grad_norm": 3.5115885734558105,
      "learning_rate": 3.2197290431837424e-07,
      "loss": 0.8427,
      "step": 46850
    },
    {
      "epoch": 2.9696124718780696,
      "grad_norm": 3.0395631790161133,
      "learning_rate": 3.1562235393734125e-07,
      "loss": 0.8262,
      "step": 46860
    },
    {
      "epoch": 2.97024620551982,
      "grad_norm": 2.728419303894043,
      "learning_rate": 3.092718035563082e-07,
      "loss": 0.8239,
      "step": 46870
    },
    {
      "epoch": 2.9708799391615703,
      "grad_norm": 2.6443140506744385,
      "learning_rate": 3.029212531752752e-07,
      "loss": 0.8451,
      "step": 46880
    },
    {
      "epoch": 2.971513672803321,
      "grad_norm": 2.8750221729278564,
      "learning_rate": 2.9657070279424217e-07,
      "loss": 0.8331,
      "step": 46890
    },
    {
      "epoch": 2.972147406445071,
      "grad_norm": 2.8744654655456543,
      "learning_rate": 2.902201524132092e-07,
      "loss": 0.8455,
      "step": 46900
    },
    {
      "epoch": 2.9727811400868216,
      "grad_norm": 2.3484866619110107,
      "learning_rate": 2.8386960203217613e-07,
      "loss": 0.8292,
      "step": 46910
    },
    {
      "epoch": 2.9734148737285717,
      "grad_norm": 2.527174472808838,
      "learning_rate": 2.775190516511431e-07,
      "loss": 0.8417,
      "step": 46920
    },
    {
      "epoch": 2.9740486073703223,
      "grad_norm": 2.7375736236572266,
      "learning_rate": 2.711685012701101e-07,
      "loss": 0.833,
      "step": 46930
    },
    {
      "epoch": 2.9746823410120724,
      "grad_norm": 3.2493252754211426,
      "learning_rate": 2.6481795088907706e-07,
      "loss": 0.8484,
      "step": 46940
    },
    {
      "epoch": 2.975316074653823,
      "grad_norm": 2.8419687747955322,
      "learning_rate": 2.58467400508044e-07,
      "loss": 0.8787,
      "step": 46950
    },
    {
      "epoch": 2.9759498082955735,
      "grad_norm": 3.066663980484009,
      "learning_rate": 2.52116850127011e-07,
      "loss": 0.8854,
      "step": 46960
    },
    {
      "epoch": 2.9765835419373237,
      "grad_norm": 2.7333195209503174,
      "learning_rate": 2.45766299745978e-07,
      "loss": 0.869,
      "step": 46970
    },
    {
      "epoch": 2.9772172755790742,
      "grad_norm": 3.861436367034912,
      "learning_rate": 2.39415749364945e-07,
      "loss": 0.8319,
      "step": 46980
    },
    {
      "epoch": 2.9778510092208244,
      "grad_norm": 2.343092441558838,
      "learning_rate": 2.3306519898391197e-07,
      "loss": 0.8371,
      "step": 46990
    },
    {
      "epoch": 2.978484742862575,
      "grad_norm": 2.5719523429870605,
      "learning_rate": 2.2671464860287893e-07,
      "loss": 0.8091,
      "step": 47000
    },
    {
      "epoch": 2.9791184765043255,
      "grad_norm": 2.6711621284484863,
      "learning_rate": 2.2036409822184588e-07,
      "loss": 0.8894,
      "step": 47010
    },
    {
      "epoch": 2.9797522101460756,
      "grad_norm": 2.911644697189331,
      "learning_rate": 2.140135478408129e-07,
      "loss": 0.8469,
      "step": 47020
    },
    {
      "epoch": 2.9803859437878257,
      "grad_norm": 2.9629197120666504,
      "learning_rate": 2.0766299745977985e-07,
      "loss": 0.8737,
      "step": 47030
    },
    {
      "epoch": 2.9810196774295763,
      "grad_norm": 2.8153884410858154,
      "learning_rate": 2.0131244707874683e-07,
      "loss": 0.8256,
      "step": 47040
    },
    {
      "epoch": 2.981653411071327,
      "grad_norm": 2.6489508152008057,
      "learning_rate": 1.949618966977138e-07,
      "loss": 0.8439,
      "step": 47050
    },
    {
      "epoch": 2.982287144713077,
      "grad_norm": 3.7953150272369385,
      "learning_rate": 1.8861134631668077e-07,
      "loss": 0.8526,
      "step": 47060
    },
    {
      "epoch": 2.9829208783548276,
      "grad_norm": 5.076497554779053,
      "learning_rate": 1.8226079593564778e-07,
      "loss": 0.9017,
      "step": 47070
    },
    {
      "epoch": 2.9835546119965777,
      "grad_norm": 2.8824961185455322,
      "learning_rate": 1.7591024555461473e-07,
      "loss": 0.8307,
      "step": 47080
    },
    {
      "epoch": 2.9841883456383282,
      "grad_norm": 2.9505767822265625,
      "learning_rate": 1.695596951735817e-07,
      "loss": 0.8247,
      "step": 47090
    },
    {
      "epoch": 2.984822079280079,
      "grad_norm": 2.520331859588623,
      "learning_rate": 1.632091447925487e-07,
      "loss": 0.8398,
      "step": 47100
    },
    {
      "epoch": 2.985455812921829,
      "grad_norm": 2.578209638595581,
      "learning_rate": 1.5685859441151566e-07,
      "loss": 0.8936,
      "step": 47110
    },
    {
      "epoch": 2.986089546563579,
      "grad_norm": 2.859278440475464,
      "learning_rate": 1.5050804403048267e-07,
      "loss": 0.8823,
      "step": 47120
    },
    {
      "epoch": 2.9867232802053296,
      "grad_norm": 3.2214293479919434,
      "learning_rate": 1.4415749364944962e-07,
      "loss": 0.8772,
      "step": 47130
    },
    {
      "epoch": 2.98735701384708,
      "grad_norm": 2.6372573375701904,
      "learning_rate": 1.3780694326841658e-07,
      "loss": 0.8378,
      "step": 47140
    },
    {
      "epoch": 2.9879907474888303,
      "grad_norm": 2.9489192962646484,
      "learning_rate": 1.314563928873836e-07,
      "loss": 0.8355,
      "step": 47150
    },
    {
      "epoch": 2.988624481130581,
      "grad_norm": 2.6588973999023438,
      "learning_rate": 1.2510584250635054e-07,
      "loss": 0.8688,
      "step": 47160
    },
    {
      "epoch": 2.989258214772331,
      "grad_norm": 3.3162219524383545,
      "learning_rate": 1.1875529212531754e-07,
      "loss": 0.8491,
      "step": 47170
    },
    {
      "epoch": 2.9898919484140816,
      "grad_norm": 2.787442684173584,
      "learning_rate": 1.1240474174428451e-07,
      "loss": 0.8593,
      "step": 47180
    },
    {
      "epoch": 2.990525682055832,
      "grad_norm": 3.9017393589019775,
      "learning_rate": 1.0605419136325149e-07,
      "loss": 0.861,
      "step": 47190
    },
    {
      "epoch": 2.9911594156975823,
      "grad_norm": 2.69879412651062,
      "learning_rate": 9.970364098221846e-08,
      "loss": 0.8907,
      "step": 47200
    }
  ],
  "logging_steps": 10,
  "max_steps": 47340,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 9.956159932858368e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
