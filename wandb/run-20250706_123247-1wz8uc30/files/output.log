                                                                                                                                                       
{'loss': 2.8217, 'grad_norm': 1.5122922658920288, 'learning_rate': 2.7e-06, 'epoch': 0.02}
{'loss': 2.7743, 'grad_norm': 1.4283167123794556, 'learning_rate': 5.7000000000000005e-06, 'epoch': 0.04}
{'loss': 2.8079, 'grad_norm': 1.1534563302993774, 'learning_rate': 8.7e-06, 'epoch': 0.06}
{'loss': 2.8419, 'grad_norm': 1.145107626914978, 'learning_rate': 1.1700000000000001e-05, 'epoch': 0.08}
{'loss': 2.8089, 'grad_norm': 1.4811506271362305, 'learning_rate': 1.47e-05, 'epoch': 0.09}
{'loss': 2.6719, 'grad_norm': 1.4661641120910645, 'learning_rate': 1.77e-05, 'epoch': 0.11}
{'loss': 2.6148, 'grad_norm': 1.5954077243804932, 'learning_rate': 2.07e-05, 'epoch': 0.13}
{'loss': 2.5002, 'grad_norm': 1.6718047857284546, 'learning_rate': 2.37e-05, 'epoch': 0.15}
{'loss': 2.2763, 'grad_norm': 2.0976016521453857, 'learning_rate': 2.6700000000000002e-05, 'epoch': 0.17}
{'loss': 2.1405, 'grad_norm': 2.4598937034606934, 'learning_rate': 2.97e-05, 'epoch': 0.19}
{'loss': 1.9092, 'grad_norm': 3.1058998107910156, 'learning_rate': 2.9965801139962003e-05, 'epoch': 0.21}
{'loss': 1.6633, 'grad_norm': 2.673539638519287, 'learning_rate': 2.9927802406586447e-05, 'epoch': 0.23}
{'loss': 1.505, 'grad_norm': 2.30532169342041, 'learning_rate': 2.9889803673210895e-05, 'epoch': 0.24}
{'loss': 1.3487, 'grad_norm': 1.9867956638336182, 'learning_rate': 2.985180493983534e-05, 'epoch': 0.26}
{'loss': 1.268, 'grad_norm': 1.9549055099487305, 'learning_rate': 2.9813806206459787e-05, 'epoch': 0.28}
{'loss': 1.1507, 'grad_norm': 1.6918312311172485, 'learning_rate': 2.977580747308423e-05, 'epoch': 0.3}
{'loss': 1.1946, 'grad_norm': 1.311674952507019, 'learning_rate': 2.973780873970868e-05, 'epoch': 0.32}
{'loss': 1.0589, 'grad_norm': 1.5074299573898315, 'learning_rate': 2.9699810006333124e-05, 'epoch': 0.34}
{'loss': 1.0959, 'grad_norm': 1.4673599004745483, 'learning_rate': 2.9661811272957568e-05, 'epoch': 0.36}
{'loss': 1.1185, 'grad_norm': 1.431644082069397, 'learning_rate': 2.9623812539582012e-05, 'epoch': 0.38}
{'loss': 1.1082, 'grad_norm': 1.3599650859832764, 'learning_rate': 2.958581380620646e-05, 'epoch': 0.39}
{'loss': 1.147, 'grad_norm': 1.4507381916046143, 'learning_rate': 2.9547815072830904e-05, 'epoch': 0.41}
{'loss': 1.0732, 'grad_norm': 1.9149643182754517, 'learning_rate': 2.9509816339455352e-05, 'epoch': 0.43}
{'loss': 1.087, 'grad_norm': 1.4159175157546997, 'learning_rate': 2.94718176060798e-05, 'epoch': 0.45}
{'loss': 1.0854, 'grad_norm': 1.3608853816986084, 'learning_rate': 2.9433818872704244e-05, 'epoch': 0.47}
{'loss': 1.1339, 'grad_norm': 1.5391136407852173, 'learning_rate': 2.9395820139328692e-05, 'epoch': 0.49}
{'loss': 1.0842, 'grad_norm': 1.4978657960891724, 'learning_rate': 2.9357821405953137e-05, 'epoch': 0.51}
{'loss': 1.0972, 'grad_norm': 1.544613003730774, 'learning_rate': 2.9319822672577584e-05, 'epoch': 0.53}
{'loss': 1.0726, 'grad_norm': 1.7075390815734863, 'learning_rate': 2.9281823939202025e-05, 'epoch': 0.54}
{'loss': 1.0729, 'grad_norm': 1.352433204650879, 'learning_rate': 2.9243825205826473e-05, 'epoch': 0.56}
{'loss': 1.0646, 'grad_norm': 1.2649198770523071, 'learning_rate': 2.9205826472450917e-05, 'epoch': 0.58}
{'loss': 1.075, 'grad_norm': 1.542443037033081, 'learning_rate': 2.9167827739075365e-05, 'epoch': 0.6}
{'loss': 1.0608, 'grad_norm': 1.3501951694488525, 'learning_rate': 2.912982900569981e-05, 'epoch': 0.62}
{'loss': 1.0022, 'grad_norm': 1.068137288093567, 'learning_rate': 2.9091830272324257e-05, 'epoch': 0.64}
{'loss': 1.014, 'grad_norm': 1.2291693687438965, 'learning_rate': 2.9053831538948702e-05, 'epoch': 0.66}
{'loss': 1.0943, 'grad_norm': 1.2145438194274902, 'learning_rate': 2.901583280557315e-05, 'epoch': 0.68}
{'loss': 1.0119, 'grad_norm': 1.3407833576202393, 'learning_rate': 2.8977834072197594e-05, 'epoch': 0.69}
{'loss': 1.05, 'grad_norm': 1.331926941871643, 'learning_rate': 2.893983533882204e-05, 'epoch': 0.71}
{'loss': 1.0667, 'grad_norm': 1.1505169868469238, 'learning_rate': 2.8901836605446486e-05, 'epoch': 0.73}
{'loss': 1.0084, 'grad_norm': 1.3949841260910034, 'learning_rate': 2.886383787207093e-05, 'epoch': 0.75}
{'loss': 1.0419, 'grad_norm': 1.364060640335083, 'learning_rate': 2.8825839138695378e-05, 'epoch': 0.77}
{'loss': 1.0281, 'grad_norm': 1.9917964935302734, 'learning_rate': 2.8787840405319823e-05, 'epoch': 0.79}
{'loss': 1.0702, 'grad_norm': 1.3003700971603394, 'learning_rate': 2.874984167194427e-05, 'epoch': 0.81}
{'loss': 1.0751, 'grad_norm': 1.2376962900161743, 'learning_rate': 2.8711842938568715e-05, 'epoch': 0.83}
{'loss': 1.1048, 'grad_norm': 1.1719191074371338, 'learning_rate': 2.8673844205193162e-05, 'epoch': 0.84}
{'loss': 0.9649, 'grad_norm': 1.2421810626983643, 'learning_rate': 2.8635845471817607e-05, 'epoch': 0.86}
{'loss': 1.0582, 'grad_norm': 1.759553074836731, 'learning_rate': 2.8597846738442055e-05, 'epoch': 0.88}
{'loss': 1.0234, 'grad_norm': 1.1709141731262207, 'learning_rate': 2.85598480050665e-05, 'epoch': 0.9}
{'loss': 1.0449, 'grad_norm': 1.70639967918396, 'learning_rate': 2.8521849271690943e-05, 'epoch': 0.92}
{'loss': 1.0385, 'grad_norm': 1.389583706855774, 'learning_rate': 2.8483850538315388e-05, 'epoch': 0.94}
{'loss': 1.0512, 'grad_norm': 1.0916582345962524, 'learning_rate': 2.8445851804939835e-05, 'epoch': 0.96}
{'loss': 1.0553, 'grad_norm': 1.2524338960647583, 'learning_rate': 2.840785307156428e-05, 'epoch': 0.98}
{'loss': 1.083, 'grad_norm': 1.2214463949203491, 'learning_rate': 2.8369854338188728e-05, 'epoch': 0.99}
{'loss': 1.0492, 'grad_norm': 1.4286704063415527, 'learning_rate': 2.8331855604813175e-05, 'epoch': 1.01}
{'loss': 1.0304, 'grad_norm': 1.0586684942245483, 'learning_rate': 2.829385687143762e-05, 'epoch': 1.03}
  File "/home/aneek/LLM-Adapters/finetune.py", line 438, in <module>
    fire.Fire(train)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/aneek/LLM-Adapters/finetune.py", line 367, in train
    trainer.train(resume_from_checkpoint=resume_from_checkpoint)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 2206, in train
    return inner_training_loop(
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 3797, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/accelerate/accelerator.py", line 2549, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/aneek/LLM-Adapters/finetune.py", line 438, in <module>
[rank0]:     fire.Fire(train)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/fire/core.py", line 135, in Fire
[rank0]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/fire/core.py", line 468, in _Fire
[rank0]:     component, remaining_args = _CallAndUpdateTrace(
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
[rank0]:     component = fn(*varargs, **kwargs)
[rank0]:   File "/home/aneek/LLM-Adapters/finetune.py", line 367, in train
[rank0]:     trainer.train(resume_from_checkpoint=resume_from_checkpoint)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 2206, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 3797, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/accelerate/accelerator.py", line 2549, in backward
[rank0]:     self.scaler.scale(loss).backward(**kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: KeyboardInterrupt
