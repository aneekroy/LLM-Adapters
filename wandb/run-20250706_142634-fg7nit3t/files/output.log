                                                                                                                                                                             
{'loss': 3.7489, 'grad_norm': 1.923857569694519, 'learning_rate': 2.7e-06, 'epoch': 0.0}
{'loss': 3.6966, 'grad_norm': 1.8410136699676514, 'learning_rate': 5.7000000000000005e-06, 'epoch': 0.0}
{'loss': 3.6541, 'grad_norm': 1.8053408861160278, 'learning_rate': 8.7e-06, 'epoch': 0.0}
{'loss': 3.6125, 'grad_norm': 1.7470875978469849, 'learning_rate': 1.1700000000000001e-05, 'epoch': 0.0}
{'loss': 3.6082, 'grad_norm': 1.780455470085144, 'learning_rate': 1.47e-05, 'epoch': 0.0}
{'loss': 3.4817, 'grad_norm': 1.9736688137054443, 'learning_rate': 1.77e-05, 'epoch': 0.0}
{'loss': 3.4115, 'grad_norm': 2.216304302215576, 'learning_rate': 2.07e-05, 'epoch': 0.0}
{'loss': 3.2363, 'grad_norm': 2.282683849334717, 'learning_rate': 2.37e-05, 'epoch': 0.01}
{'loss': 3.0155, 'grad_norm': 2.539881706237793, 'learning_rate': 2.6700000000000002e-05, 'epoch': 0.01}
{'loss': 2.7112, 'grad_norm': 2.882103443145752, 'learning_rate': 2.97e-05, 'epoch': 0.01}
{'loss': 2.3597, 'grad_norm': 3.6152679920196533, 'learning_rate': 2.999885883347422e-05, 'epoch': 0.01}
{'loss': 2.0195, 'grad_norm': 3.4373068809509277, 'learning_rate': 2.9997590870667794e-05, 'epoch': 0.01}
{'loss': 1.7689, 'grad_norm': 3.4663655757904053, 'learning_rate': 2.999632290786137e-05, 'epoch': 0.01}
{'loss': 1.5074, 'grad_norm': 2.8785533905029297, 'learning_rate': 2.999505494505495e-05, 'epoch': 0.01}
{'loss': 1.3963, 'grad_norm': 3.0561392307281494, 'learning_rate': 2.999378698224852e-05, 'epoch': 0.01}
{'loss': 1.2544, 'grad_norm': 2.4781136512756348, 'learning_rate': 2.9992519019442096e-05, 'epoch': 0.01}
{'loss': 1.1792, 'grad_norm': 1.5301223993301392, 'learning_rate': 2.9991251056635673e-05, 'epoch': 0.01}
{'loss': 1.1491, 'grad_norm': 1.8542166948318481, 'learning_rate': 2.9989983093829247e-05, 'epoch': 0.01}
{'loss': 1.1646, 'grad_norm': 1.6610890626907349, 'learning_rate': 2.9988715131022825e-05, 'epoch': 0.01}
{'loss': 1.1544, 'grad_norm': 2.1555159091949463, 'learning_rate': 2.9987447168216402e-05, 'epoch': 0.01}
{'loss': 1.1033, 'grad_norm': 2.3460493087768555, 'learning_rate': 2.9986179205409976e-05, 'epoch': 0.01}
{'loss': 1.1693, 'grad_norm': 1.714970350265503, 'learning_rate': 2.998491124260355e-05, 'epoch': 0.01}
{'loss': 1.1716, 'grad_norm': 2.0583057403564453, 'learning_rate': 2.9983643279797127e-05, 'epoch': 0.01}
{'loss': 1.0499, 'grad_norm': 2.896580457687378, 'learning_rate': 2.99823753169907e-05, 'epoch': 0.02}
{'loss': 1.0745, 'grad_norm': 2.7867376804351807, 'learning_rate': 2.9981107354184278e-05, 'epoch': 0.02}
{'loss': 1.0821, 'grad_norm': 1.9681144952774048, 'learning_rate': 2.9979839391377855e-05, 'epoch': 0.02}
{'loss': 1.1185, 'grad_norm': 2.3436529636383057, 'learning_rate': 2.997857142857143e-05, 'epoch': 0.02}
{'loss': 1.118, 'grad_norm': 1.637357473373413, 'learning_rate': 2.9977303465765003e-05, 'epoch': 0.02}
{'loss': 1.093, 'grad_norm': 2.4294345378875732, 'learning_rate': 2.997603550295858e-05, 'epoch': 0.02}
{'loss': 1.1071, 'grad_norm': 1.8233188390731812, 'learning_rate': 2.9974767540152154e-05, 'epoch': 0.02}
{'loss': 1.1163, 'grad_norm': 2.371264934539795, 'learning_rate': 2.997349957734573e-05, 'epoch': 0.02}
{'loss': 1.1217, 'grad_norm': 2.733307361602783, 'learning_rate': 2.997223161453931e-05, 'epoch': 0.02}
{'loss': 1.0899, 'grad_norm': 2.645132303237915, 'learning_rate': 2.9970963651732883e-05, 'epoch': 0.02}
{'loss': 1.1462, 'grad_norm': 2.8646364212036133, 'learning_rate': 2.9969695688926457e-05, 'epoch': 0.02}
{'loss': 1.0752, 'grad_norm': 1.9506622552871704, 'learning_rate': 2.9968427726120034e-05, 'epoch': 0.02}
{'loss': 1.1216, 'grad_norm': 1.9959063529968262, 'learning_rate': 2.996715976331361e-05, 'epoch': 0.02}
{'loss': 1.1178, 'grad_norm': 2.6307599544525146, 'learning_rate': 2.9965891800507185e-05, 'epoch': 0.02}
{'loss': 1.0468, 'grad_norm': 2.2508034706115723, 'learning_rate': 2.9964623837700762e-05, 'epoch': 0.02}
{'loss': 1.0905, 'grad_norm': 2.7943167686462402, 'learning_rate': 2.996335587489434e-05, 'epoch': 0.02}
{'loss': 1.0697, 'grad_norm': 2.8371286392211914, 'learning_rate': 2.9962087912087914e-05, 'epoch': 0.03}
{'loss': 1.1115, 'grad_norm': 2.2414746284484863, 'learning_rate': 2.9960819949281487e-05, 'epoch': 0.03}
{'loss': 1.1398, 'grad_norm': 2.441978931427002, 'learning_rate': 2.9959551986475065e-05, 'epoch': 0.03}
{'loss': 1.0956, 'grad_norm': 2.214005947113037, 'learning_rate': 2.995828402366864e-05, 'epoch': 0.03}
{'loss': 1.1344, 'grad_norm': 1.7509965896606445, 'learning_rate': 2.9957016060862216e-05, 'epoch': 0.03}
{'loss': 1.1099, 'grad_norm': 2.2508363723754883, 'learning_rate': 2.9955748098055793e-05, 'epoch': 0.03}
{'loss': 1.0948, 'grad_norm': 1.8511617183685303, 'learning_rate': 2.9954480135249367e-05, 'epoch': 0.03}
{'loss': 1.0824, 'grad_norm': 2.8791468143463135, 'learning_rate': 2.995321217244294e-05, 'epoch': 0.03}
{'loss': 1.0977, 'grad_norm': 2.1496474742889404, 'learning_rate': 2.9951944209636518e-05, 'epoch': 0.03}
{'loss': 1.0977, 'grad_norm': 2.4462859630584717, 'learning_rate': 2.9950676246830092e-05, 'epoch': 0.03}
{'loss': 1.112, 'grad_norm': 2.4883439540863037, 'learning_rate': 2.994940828402367e-05, 'epoch': 0.03}
{'loss': 1.082, 'grad_norm': 2.156167507171631, 'learning_rate': 2.9948140321217247e-05, 'epoch': 0.03}
{'loss': 1.1258, 'grad_norm': 2.7546238899230957, 'learning_rate': 2.994687235841082e-05, 'epoch': 0.03}
{'loss': 1.0541, 'grad_norm': 2.319725513458252, 'learning_rate': 2.9945604395604398e-05, 'epoch': 0.03}
{'loss': 1.0368, 'grad_norm': 2.181256055831909, 'learning_rate': 2.9944336432797972e-05, 'epoch': 0.03}
{'loss': 1.0996, 'grad_norm': 2.0872867107391357, 'learning_rate': 2.9943068469991546e-05, 'epoch': 0.03}
{'loss': 1.0802, 'grad_norm': 2.2840702533721924, 'learning_rate': 2.9941800507185123e-05, 'epoch': 0.04}
{'loss': 1.0912, 'grad_norm': 2.139181613922119, 'learning_rate': 2.99405325443787e-05, 'epoch': 0.04}
{'loss': 1.103, 'grad_norm': 2.405700922012329, 'learning_rate': 2.9939264581572274e-05, 'epoch': 0.04}
{'loss': 1.1094, 'grad_norm': 2.267124652862549, 'learning_rate': 2.993799661876585e-05, 'epoch': 0.04}
{'loss': 1.0672, 'grad_norm': 2.132690668106079, 'learning_rate': 2.9936728655959425e-05, 'epoch': 0.04}
{'loss': 1.0358, 'grad_norm': 2.7049736976623535, 'learning_rate': 2.9935460693153e-05, 'epoch': 0.04}
{'loss': 1.1158, 'grad_norm': 2.200362205505371, 'learning_rate': 2.9934192730346576e-05, 'epoch': 0.04}
{'loss': 1.0483, 'grad_norm': 3.0896058082580566, 'learning_rate': 2.9932924767540154e-05, 'epoch': 0.04}
{'loss': 1.0438, 'grad_norm': 2.057285785675049, 'learning_rate': 2.9931656804733728e-05, 'epoch': 0.04}
{'loss': 1.0758, 'grad_norm': 1.861235499382019, 'learning_rate': 2.9930388841927305e-05, 'epoch': 0.04}
{'loss': 1.1063, 'grad_norm': 1.8059064149856567, 'learning_rate': 2.9929120879120882e-05, 'epoch': 0.04}
{'loss': 1.0735, 'grad_norm': 2.1444075107574463, 'learning_rate': 2.9927852916314456e-05, 'epoch': 0.04}
{'loss': 1.0615, 'grad_norm': 1.776524543762207, 'learning_rate': 2.992658495350803e-05, 'epoch': 0.04}
{'loss': 1.0188, 'grad_norm': 1.7674813270568848, 'learning_rate': 2.9925316990701607e-05, 'epoch': 0.04}
{'loss': 1.057, 'grad_norm': 1.946736216545105, 'learning_rate': 2.9924049027895184e-05, 'epoch': 0.04}
{'loss': 1.0587, 'grad_norm': 3.6336352825164795, 'learning_rate': 2.992278106508876e-05, 'epoch': 0.04}
{'loss': 1.0507, 'grad_norm': 1.932765245437622, 'learning_rate': 2.9921513102282336e-05, 'epoch': 0.05}
{'loss': 1.0603, 'grad_norm': 2.2574141025543213, 'learning_rate': 2.992024513947591e-05, 'epoch': 0.05}
{'loss': 1.0726, 'grad_norm': 2.0130090713500977, 'learning_rate': 2.9918977176669483e-05, 'epoch': 0.05}
{'loss': 1.0338, 'grad_norm': 2.298815965652466, 'learning_rate': 2.991770921386306e-05, 'epoch': 0.05}
{'loss': 1.0488, 'grad_norm': 2.7454466819763184, 'learning_rate': 2.9916441251056638e-05, 'epoch': 0.05}
{'loss': 1.0293, 'grad_norm': 1.7255909442901611, 'learning_rate': 2.9915173288250212e-05, 'epoch': 0.05}
{'loss': 1.0776, 'grad_norm': 2.20395565032959, 'learning_rate': 2.991390532544379e-05, 'epoch': 0.05}
{'loss': 1.0773, 'grad_norm': 1.9848946332931519, 'learning_rate': 2.9912637362637366e-05, 'epoch': 0.05}
{'loss': 1.0379, 'grad_norm': 4.2597856521606445, 'learning_rate': 2.9911369399830937e-05, 'epoch': 0.05}
{'loss': 1.0216, 'grad_norm': 2.5099823474884033, 'learning_rate': 2.9910101437024514e-05, 'epoch': 0.05}
{'loss': 1.0571, 'grad_norm': 2.933833122253418, 'learning_rate': 2.990883347421809e-05, 'epoch': 0.05}
{'loss': 1.0511, 'grad_norm': 2.326343536376953, 'learning_rate': 2.9907565511411665e-05, 'epoch': 0.05}
{'loss': 1.0293, 'grad_norm': 3.0488710403442383, 'learning_rate': 2.9906297548605243e-05, 'epoch': 0.05}
{'loss': 1.0322, 'grad_norm': 2.8434324264526367, 'learning_rate': 2.990502958579882e-05, 'epoch': 0.05}
{'loss': 1.0609, 'grad_norm': 2.3477351665496826, 'learning_rate': 2.990376162299239e-05, 'epoch': 0.05}
{'loss': 1.0903, 'grad_norm': 2.3671858310699463, 'learning_rate': 2.9902493660185968e-05, 'epoch': 0.06}
{'loss': 1.0456, 'grad_norm': 2.3755502700805664, 'learning_rate': 2.9901225697379545e-05, 'epoch': 0.06}
{'loss': 1.0458, 'grad_norm': 1.836952567100525, 'learning_rate': 2.989995773457312e-05, 'epoch': 0.06}
{'loss': 1.0137, 'grad_norm': 1.9712382555007935, 'learning_rate': 2.9898689771766696e-05, 'epoch': 0.06}
{'loss': 1.0592, 'grad_norm': 2.475912094116211, 'learning_rate': 2.9897421808960273e-05, 'epoch': 0.06}
{'loss': 1.0579, 'grad_norm': 2.569138288497925, 'learning_rate': 2.9896153846153844e-05, 'epoch': 0.06}
{'loss': 1.0119, 'grad_norm': 2.8032686710357666, 'learning_rate': 2.989488588334742e-05, 'epoch': 0.06}
{'loss': 1.0552, 'grad_norm': 2.2918684482574463, 'learning_rate': 2.9893617920541e-05, 'epoch': 0.06}
  File "/home/aneek/LLM-Adapters/finetune.py", line 438, in <module>
    fire.Fire(train)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/aneek/LLM-Adapters/finetune.py", line 367, in train
    trainer.train(resume_from_checkpoint=resume_from_checkpoint)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 2206, in train
    return inner_training_loop(
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 3749, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 3836, in compute_loss
    outputs = model(**inputs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1637, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1464, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/accelerate/utils/operations.py", line 818, in forward
    return model_forward(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/accelerate/utils/operations.py", line 806, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/peft/peft_model.py", line 1757, in forward
    return self.base_model(
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 193, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 552, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 440, in forward
    layer_outputs = decoder_layer(
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/modeling_layers.py", line 83, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 287, in forward
    hidden_states = self.input_layernorm(hidden_states)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 63, in forward
    variance = hidden_states.pow(2).mean(-1, keepdim=True)
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/aneek/LLM-Adapters/finetune.py", line 438, in <module>
[rank0]:     fire.Fire(train)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/fire/core.py", line 135, in Fire
[rank0]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/fire/core.py", line 468, in _Fire
[rank0]:     component, remaining_args = _CallAndUpdateTrace(
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
[rank0]:     component = fn(*varargs, **kwargs)
[rank0]:   File "/home/aneek/LLM-Adapters/finetune.py", line 367, in train
[rank0]:     trainer.train(resume_from_checkpoint=resume_from_checkpoint)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 2206, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 3749, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/trainer.py", line 3836, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1637, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1464, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/accelerate/utils/operations.py", line 818, in forward
[rank0]:     return model_forward(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/accelerate/utils/operations.py", line 806, in __call__
[rank0]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/peft/peft_model.py", line 1757, in forward
[rank0]:     return self.base_model(
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 193, in forward
[rank0]:     return self.model.forward(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/utils/generic.py", line 943, in wrapper
[rank0]:     output = func(self, *args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 552, in forward
[rank0]:     outputs: BaseModelOutputWithPast = self.model(
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/utils/generic.py", line 943, in wrapper
[rank0]:     output = func(self, *args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 440, in forward
[rank0]:     layer_outputs = decoder_layer(
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/modeling_layers.py", line 83, in __call__
[rank0]:     return super().__call__(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 287, in forward
[rank0]:     hidden_states = self.input_layernorm(hidden_states)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/aneek/miniconda3/envs/prune-net/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 63, in forward
[rank0]:     variance = hidden_states.pow(2).mean(-1, keepdim=True)
[rank0]: KeyboardInterrupt
