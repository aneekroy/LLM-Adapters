Traceback (most recent call last):
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/utils/hub.py", line 470, in cached_files
    hf_hub_download(
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/aneek/models/Llama-3.2-1B-Instruct'. Use `repo_type` argument if needed.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/home/aneek/LLM-Adapters/ensemble_eval_multi_lora_commonsense.py", line 196, in <module>
    main()
  File "/home/aneek/LLM-Adapters/ensemble_eval_multi_lora_commonsense.py", line 151, in main
    tokenizer, base = load_base(args)
  File "/home/aneek/LLM-Adapters/ensemble_eval_multi_lora_commonsense.py", line 67, in load_base
    tok = Tok.from_pretrained(args.base_model, use_fast=True, trust_remote_code=True)
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 982, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 814, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/utils/hub.py", line 312, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/utils/hub.py", line 522, in cached_files
    resolved_files = [
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/utils/hub.py", line 523, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/utils/hub.py", line 140, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir=cache_dir, revision=revision)
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/aneek/models/Llama-3.2-1B-Instruct'. Use `repo_type` argument if needed.
Traceback (most recent call last):
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/utils/hub.py", line 470, in cached_files
    hf_hub_download(
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/aneek/models/Llama-3.2-1B-Instruct'. Use `repo_type` argument if needed.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/home/aneek/LLM-Adapters/ensemble_eval_multi_lora_commonsense.py", line 196, in <module>
    main()
  File "/home/aneek/LLM-Adapters/ensemble_eval_multi_lora_commonsense.py", line 151, in main
    tokenizer, base = load_base(args)
  File "/home/aneek/LLM-Adapters/ensemble_eval_multi_lora_commonsense.py", line 67, in load_base
    tok = Tok.from_pretrained(args.base_model, use_fast=True, trust_remote_code=True)
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 982, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 814, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/utils/hub.py", line 312, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/utils/hub.py", line 522, in cached_files
    resolved_files = [
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/utils/hub.py", line 523, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/transformers/utils/hub.py", line 140, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir=cache_dir, revision=revision)
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/aneek/miniconda3/envs/llama4env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/aneek/models/Llama-3.2-1B-Instruct'. Use `repo_type` argument if needed.